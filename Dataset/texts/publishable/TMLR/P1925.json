{
  "Abstract": "Massive backpropagated models can outperform humans on a variety of tasks but sufferfrom high power consumption and poor generalization. Local learning, which focuses onupdating subsets of a models parameters at a time, has emerged as a promising techniqueto address these issues. Recently, a novel local learning algorithm called Forward-Forwardhas received widespread attention due to its innovative approach to learning. Unfortunately,its application has been limited to smaller datasets due to scalability issues. To this end,we propose The Trifecta, a collection of three simple techniques that drastically improvethe Forward-Forward algorithm on deeper networks. Our experiments demonstrate thatour models are on par with similarly structured, backpropagation-based models in bothtraining speed and test accuracy on simple datasets. Specifically, we achieve around 84%accuracy on CIFAR-10, a notable improvement (25%) over the original FF algorithm.",
  "Introduction": "Despite its pervasiveness, backpropagation is not flawless; it has inherent challenges that have puzzled re-searchers for decades. One such challenge is that it usually produces overfitted models, models that memorizethe training set and fail to generalize to unseen data (Erhan et al., 2010). Over the years, researchers havefound ways to mitigate the issue by introducing dropout (Srivastava et al., 2014) or simply increasing thedataset size through augmentation techniques or by collecting more data. However, with modern datasetsreaching near-internet size (Gao et al., 2020; Schuhmann et al., 2022; Carlini et al., 2021), this provisionalsolution cannot be scaled further. Moreover, contemporary models are tricky to scale both locally (Nagelet al., 2021) due to memory issues and across machines (Huang et al., 2019) due to communication overhead.The need for sequential forward and backward passes leads to the phenomenon known as backward locking,hampering the efficient distribution of computation (Huo et al., 2018). Lastly, backpropagation struggleswith vanishing or exploding gradients, which affect learning and stability, especially in deeper networks(Hochreiter & Schmidhuber, 1997; Glorot & Bengio, 2010; Bengio et al., 1994). Research has unveiled sev-eral mitigations (He et al., 2016; Ioffe & Szegedy, 2015; He et al., 2015; Glorot & Bengio, 2010), but thisstill remains an issue in large models (Chowdhery et al., 2022). The combination of these systematic issues and the biological implausibility of backpropagation (Lillicrapet al., 2016) has prompted the machine learning community to explore alternative learning algorithms. Onesuch approach is local learning (Bengio et al., 2006; Hinton et al., 2006; Belilovsky et al., 2019; N kland,",
  "Published in Transactions on Machine Learning Research (10/2024)": "Single Sample Training. A notable advantage of layer normalization over batch normalization lies inthe fact that there is no constraint on batch sizes. In a biological setting, which FF endeavors to resemble,learning by normalizing over a batch of training samples is implausible. However, comparable to evaluationusing batch normalization, this can be solved by using rolling averages for the full training process. Somepreliminary experiments have shown that this does not significantly degrade accuracy. Specifically, whenusing the same settings as in with the full TFF, the resulting model achieves an accuracy of 73.3%,only 2% lower than originally.",
  "Background": "The Forward-Forward algorithm is a biologically plausible alternative to backpropagation. Instead of min-imizing the cross-entropy between the models outputs and a one-hot encoding of labels, it replaces thebackward pass from backpropagation with a second forward pass, hence the name. The first forward passcontains positive (real) data, and the second pass contains negative (bogus) data. The main idea is thateach layer of the network should learn to distinguish this data as well as possible. To this end, Hinton (2022)considers the magnitude of hidden activations, which they call goodness. This goodness should be high forpositive samples and low for negative samples. Generally, the l2 norm is used to determine the goodness,but variants exist.",
  "Negative Sampling": "At the core of the learning algorithm lies learning the distinction between positive and negative samples.Consequently, the nature and quality of negative samples strongly influence the learning process. While onecan imagine a multitude of ways to approach this, we focus on creating negative and positive samples in asupervised manner. Specifically by adding the label to the input. In this case, a positive sample is a pair inwhich the input and label match. Conversely, a negative sample has labels and inputs that dont match. Todistinguish matching pairs, the model has to learn the structure of the inputs. In some cases, one should becareful about how this pair is created. Consider a CNN-based model where the label is encoded in a limitednumber of pixels; then, not all parts of the model could access this data. Due to the lack of backward signal,this may seriously hamper learning. While such a contrastive approach has some drawbacks, often rooted inlower computational efficiency, it is much more flexible. For instance, one could focus on the negative pairsthat the model often mispredicts or are important to the task.",
  "ilog(1 + exp(gnegi ))": "Here, gpos and gneg refer to the vector of the goodness values of the positive and negative samples in eachbatch, respectively, and is a threshold. This threshold defines how strongly the positive and negativesamples should be separated. Finally, i represents an entry in a batch. This function will produce low lossvalues if both the goodness of positive samples is well above the threshold and well below the same thresholdfor negative samples.",
  "Evaluation": "During evaluation, the goodness across one or multiple layers determines the certainty in the prediction ofthe provided sample. In the supervised setting, a forward pass with each label must be performed to attainthe logits of the n-way classification. This can be inefficient, especially in scenarios with numerous classes,such as ImageNet (Deng et al., 2009). Alternatively, the features of the last layer in the network can beused to train a classifier; this mitigates the linear complexity of n-way classification 2. Due to the novelty ofthe algorithm, different existing training and evaluation strategies are largely unexplored. Consequently, FFoften exhibits lower accuracy and performance compared to other local learning techniques. Nevertheless,this presents a promising avenue for investigating learning algorithms and gaining insights into the learningprocess of deep networks in general. Appendix H discusses some experiments and ideas in this area. This work focuses on the simplest version of the Forward-Forward algorithm. We encourage the reader torefer to the original work (Hinton, 2022) for a more complete intuition and connections to other learningalgorithms.",
  "The Trifecta": "The main contribution of this work is The Trifecta, which aims to solve three observed weaknesses in theForward-Forward algorithm: the loss function, the normalization, and the lack of error signals.As ourexperimentation reveals, there is a positive synergy between the three components; combining the threeyields higher gains than their separate improvements.",
  "Symmetric Loss Function": "Consistent with (Lee & Song, 2023), we find that the proposed loss function in FF is not equal for falsepositives and false negatives of the same scale, leading to gradients that do not directly point toward theminimum loss. Additionally, it is not possible to have negative goodness which means the separation ofthe negative goodness is limited for many threshold values. The original paper (Hinton, 2022) noted thatminimizing goodness, instead of maximizing it for positive samples, yields higher accuracy. This suggeststhat it is easier for the network to distinguish negative examples very well according to how bogus they are.Appendix C provides a comprehensive mathematical analysis of these issues. To address these issues Lee& Song (2023) proposes the SymBa loss function that relies on directly optimizing for the separation gap,shown below.",
  "ilog(1 + exp(gposi gnegi))": "This resolves the aforementioned issue. This approach works best when the positive and negative samplesare highly correlated, which is the case with the supervised approach. Additionally, this function can beimproved by scaling the separation with a constant term , this penalizes wrong classifications further, akinto focal loss (Lin et al., 2018). Our work, therefore, opts to employ this loss function. In Appendix D wefurther discuss the advantages and disadvantages of this loss function.",
  "Batch Normalization": "Prior to each layer, Hinton (2022) suggests normalizing the length of the activations, akin to a simplifiedform of layer normalization (LN) 3. This serves two purposes: it prevents prediction information from leaking 2In the supervised approach, a label must still be provided during evaluation. The original work (Hinton, 2022) uses aneutral label which is an average of all labels. Limited experiments show that simply using a random label works slightlybetter in practice.3In this work we refer to this reduced form of normalization that does not subtract the mean simply as layernorm. Allexperiments in this regard are verified to hold for both this reduced version and the standard PyTorch layernorm implementation(PyTorch Contributors, 2023).",
  "Overlapping Local Updates": "As a means to introduce error signals into FF, the original work (Hinton, 2022) proposes a recurrent net-work that uses the goodness of the subsequent and previous layers. However, for simple classification, thisrequires the network to be evaluated several times. Therefore, we propose using another technique fromrecent research (Huo et al., 2018; Laskin et al., 2021) that uses semi-local error signals within the contextof backpropagation.This approach involves training layers in groups in an alternating and overlappingpattern. Using group size 1 simply results in greedy learning while using group size 2 leads to OverlappingLocal Updates (OLU). A more thorough explanation and further implementation details can be found inAppendix F.",
  "Experimental Setup": "Conforming to Hinton (2022), all experimentation in this work concentrates on image classification to measurethe capabilities of the Forward-Forward algorithm, specifically on the MNIST, Fashion-MNIST, SVHN,CIFAR-10, and CIFAR-100 datasets (LeCun & Cortes, 2010; Xiao et al., 2017; Netzer et al., 2011; Krizhevsky,2009). These datasets provide a good difficulty mix to assess the behavior of the algorithm in differentscenarios.",
  "Training and Encoding": "In this work, we opt to exclusively employ the supervised approach to the Forward-Forward algorithmas described in Hinton (2022).Further, we opt to use CNNs due to their ubiquitous nature in imageclassification. This combination poses some problems as the one-hot label encoding (1D) cannot be triviallyconcatenated to an image (3D). Additionally, the encoding must be concatenated in such a way that amajority of convolutions can differentiate the labels 4. To this end, this work encodes the label into anadditional channel of identical size as the images, this encoding is achieved through a learned matrix in thecase of OLU and a random one without. In the case of CIFAR-10, the one-hot vector is multiplied by a10 1024 embedding matrix (E), and this result is transformed into a 32 32 matrix and concatenated asa fourth input channel (shown below). In practice, we found this to work well.",
  "Architecture": "Selecting the appropriate architecture is a crucial consideration in deep learning. However, most designprinciples and architectural rules-of-thumb stem from backpropagation and are not blindly applicable tounexplored learning algorithms. For instance, in a local learning setting, bottleneck layers most often resultin information loss rather than compact representations in our experience. Currently, the Forward-Forwardalgorithm has only been used to train small fully-connected networks and non-weight-sharing local receptivefields. In line with other work within local learning (N kland, 2016b; Belilovsky et al., 2019), this workemploys VGG-like CNNs (Simonyan & Zisserman, 2015). However, some specific changes are made to adaptit to Forward-Forward. First, we omit the (linear) classification head because goodness is simply determinedfrom an aggregation of activations, no matter the dimensionality 5. Second, we use a different ordering ofoperations within each layer. Specifically, we first perform normalization, followed by the convolution, thenon-linearity, and finally an optional maxpool. This is simply an extension of Hinton (2022) and permutationsof this have not been explored further in this work 6. This work studies three architectures: two CNNsconsisting of 6 and 12 layers and an FCN with 6 layers. These networks respectively consist of 2.8 million,8.5 million, and 25.1 million parameters (depending on the input). The CNN architectures are depicted in. The FCN is simply a stack of hidden layers with a dimension of 2048.",
  "Evaluation Metrics": "The Forward-Forward algorithm allows for more fine-grained metrics to evaluate any given network comparedto backpropagation. In essence, each layer has the same objective, in contrast to backpropagation whereonly the last layer has an objective, which can be used to examine the relations between layers. For instance,it is possible to inspect how well a single layer is able to classify based solely on its own goodness. We expectthis accuracy to increase with depth, and failure to do so may indicate an issue with the previous layers.The same principle can be applied to any per-layer metric, such as the separation, loss, or gradient stabilityto gain insight into the behavior of the model. An important metric in evaluating learning algorithms istraining accuracy. This indicates whether the algorithm is overfitting or simply fails to represent the dataset.However, in the supervised setting, this is impossible to determine efficiently without sampling all labels,which would slow down training time by the number of labels. 4Consider the case where the first ten pixels of the image are used to encode the label. The majority of output neurons haveno information about the label, leaving them incapable of separating positive from negative.5Preliminary experiments in this work have shown that the goodness mechanism, in general, does not operate well whenusing very few features, such as in a classification head.6For simplicity, we do not apply any techniques such as the peer-normalization proposed in Hinton (2022) in all networks.",
  "Accuracy": "To measure the actual accuracy of the network, Hinton (2022) reports that taking the average goodness of alllayers except the first leads to the highest accuracy. Our experiments show this to be true in tiny networkstrained with the original algorithm but when using The Trifecta on deeper networks, accuracy drasticallydegrades. This is because, on complex datasets, the earlier layers are significantly worse than the later onesand affect the stability of the ensemble. Therefore, all reported single-valued accuracies (, )are simply based on the goodness of the last layer. Nevertheless, in deep networks, aggregating the last fewlayers can be fruitful. By aggregating the last 3 layers for goodness-based classification, we can improve testaccuracy by a few decimal points. More information about evaluation can be found in Appendix H.",
  "Results": "We perform and study two series of experiments to evaluate the effectiveness of The Trifecta. The first is anablation of the three components of The Trifecta. The second is a study into the impact of the architectureand training time. Where possible, we compare results to the vanilla (original) Forward-Forward algorithm(Hinton, 2022), feedback alignment (Lillicrap et al., 2016), PEPITA (Dellaferrera & Kreiman, 2022), and abackpropagation baseline. This baseline is trained on the deep architecture with an altered layer structureto accommodate backpropagation 7. Further, a linear classification head is appended. Note that this simplyserves as a baseline and is far from SOTA. The same hyperparameters and architecture are used on all datasetsfor fair comparison. We describe each model with the shorthand notation [algorithm]/[architecture]. Theconsidered algorithms consist of Vanilla FF (VFF), Trifecta FF (TFF), feedback alignment (FA), PEPITA(PEP), and backpropagation (BP). The architectures are the shallow CNN (s), the deep CNN (d), the FCN(f), and finally a tiny network consisting of 3 or fewer layers (t). For instance, VFF/s denotes the originalFF algorithm on the shallow architecture.",
  "Ablation of Trifecta Components": "shows the accuracy achieved by multiple interplays of The Trifecta components. We perform thisablation with the shallow architecture on the CIFAR-10 dataset as it is the most challenging and, therefore,differentiates the algorithms best. The first row illustrates the importance of the loss function in our recre-ation of the original FF algorithm. This highlights the importance of as a hyperparameter. The value = 2 is chosen in line with other implementations (Lee & Song, 2023; Pezeshki, 2023) and = 10 is anoptimized value we found for our network on CIFAR-10. Additionally, utilizing the improved loss functionfurther improves the accuracy and establishes this component as the most crucial part of the Trifecta. Thenext rows show that the addition of each component yields significant gains to the final accuracy. The fullTrifecta (the bottom right entry) achieves the highest accuracy by a significant margin. The last columnshows that the combination of all techniques outperforms the sum of parts, the differences compared to onlySymBa are 6.60% for OLU, 9.29% for BN, and their combination is 16.01%.",
  "Convergence Properties": "We train the TFF/s and the TFF/d architectures for 200 and 500 iterations respectively on four imageclassification datasets (). We compare our results at several stages of training to the available resultsfrom Hinton (2022) and a backpropagation baseline. When using backpropagation to fit trivial datasets, suchas MNIST, unnecessarily deep networks are inadvisable as they lead to slower convergence and instability.Our experiments confirm this observation as the deep model (99.58%) only slightly outperforms the shallowmodel (99.53%). A similar trend can be observed for the f-MNIST and SVHN datasets. On the other hand,the more challenging CIFAR-10 dataset exhibits the same behavior up to 100 epochs, where the shallowmodel (75.23%) convincingly outperforms the deep model (71.12%).However, the longer training timeallows the deep model (83.51%) to surpass the shallow model (80.01%). A more detailed explanation of thisphenomenon can be found in Appendix G. Lastly, the learning speed of our algorithm is very competitive, 7Backpropagation, trained on the shallow architecture, shows slightly worse convergence properties and is therefore notshown. Further, on our setup, backpropagation converges after 100 epochs, showing no further improvement on longer training.",
  ": The evolution of test accuracy (left) and separation (right) throughout the training of the best-performing CIFAR-10 TFF/d model presented in": "We can split the empirical impact of The Trifecta on the Forward-Forward algorithm into two categories.First, the progressive improvement is the amount by which each layer improves compared to its predecessor.Ideally, appending additional layers continues to improve accuracy. Second, the learning speed is the amountby which each layer improves over time. Trivially, the improvement of each layer must be as quick as possibleto reduce training time.",
  "Progressive Improvement": "The primary advantage of The Trifecta lies in its ability to improve the accuracy upon going deeper intothe network. In contrast to the original FF, which is unable to learn further than a few layers, especiallywhen using CNNs (). In this work, we were able to scale to 12 layers on multiple datasets withconsiderable gains per layer. Above this threshold, our experiments did not show worthwhile improvement.Further, the progressive improvement of later layers is closely related to the learning rate. illustratesthe accuracy and the separation throughout training. These plots reveal the impact of a lower learning rateon the separation and stability of the later layers, as can be seen at epochs 200 and 400 when the learningrate schedule changes. This is also reflected in the accuracy (although difficult to see on the plot), as thedifference between layers 6 and 12 at epoch 200 is 2.6% and 3.7% at epoch 500. This work has only scratchedthe surface in terms of exploring the impact of learning rate schedules. More exotic learning schedules, whichmay involve freezing certain layers, may be highly fruitful to further improve accuracy.",
  "Learning Speed": "Another strength of The Trifecta is its ability to learn more quickly.Each component in The Trifectanot only impacts the final accuracy of the model but also the speed at which this accuracy is achieved.All components contribute to this in their own way, leading to the strong synergy within The Trifecta.Specifically, our shallow CNN model is able to cross the cape of 97% on MNIST and 50% on CIFAR-10 inless than 10 epochs. Again, the learning speed is closely related to the learning rate of the layers. We areconfident that finding techniques to increase the learning rate, will further improve the speed of FF. The focus of this work is to improve the accuracy of the FF algorithm in a supervised image classificationsetting using slight modifications. However, outside of The Trifecta and the original Forward-Forward paper(Hinton, 2022), not much has yet been explored and several ideas for improving the algorithm still remainuntried and await further investigation. We strongly believe that future work in the following two areas ismost important for the future of FF: finding better evaluation strategies and improving general understandingof the learning characteristics.",
  "Evaluation Strategies": "Ultimately, FF is still held back due to several issues. In particular, performing non-binary classification usingthis algorithm is prohibitively slow as each class needs to be sampled separately to achieve high accuracy.Using FF for tasks with many classes, such as classifying ImageNet (Deng et al., 2009), is therefore stronglyinadvisable. Nonetheless, alternate classification techniques that avoid sampling all classes separately byusing candidate lists or one-pass classification (Hinton, 2022) seem promising but currently lead to loweraccuracy and higher instability, respectively. Solving this would be a huge boost toward the viability ofForward-Forward in more tasks.",
  "General Understanding": "There are several gaps in knowledge about FF which we believe to be nontransferable from backpropagation.For instance, performing a thorough architecture search to determine which patterns and blocks work best.Bottleneck layers are not suitable for Forward-Forward but inverted bottlenecks (Sandler et al., 2018) maybe. Additionally, techniques such as residual connections (He et al., 2016) remain an intriguing avenue ofresearch that may push the algorithm to scale to increasingly complex tasks (touched upon in Appendix I).Further, an investigation using well-established visual explanation techniques such as (Selvaraju et al., 2019)",
  "Closing Remarks": "We proposed The Trifecta, three simple techniques that improve the accuracy and learning speed of FF-based methods. This enables the training of deeper FF-based architectures that are competitive with theirbackpropagated counterparts. We note that, within our proposed improvements, there remain several openquestions. We encourage further research to scrutinize our findings as well as explore how to further improvethe FF algorithm. Continued research into alternate learning algorithms may reveal generally applicableknowledge that will aid our understanding of machine learning and intelligence itself.",
  "Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult.IEEE Transactions on Neural Networks, 5(2):157166, 1994. doi: 10.1109/72.279181": "Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy layer-wise training of deepnetworks. In B. Schlkopf, J. Platt, and T. Hoffman (eds.), Advances in Neural Information ProcessingSystems, volume 19. MIT Press, 2006. URL Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, AdamRoberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting trainingdata from large language models, 2021. URL Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, VinodkumarPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, MichaelIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, HenrykMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, ShivaniAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, AitorLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, DouglasEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. In ArXiv,2022. Giorgia Dellaferrera and Gabriel Kreiman. Error-driven input modulation: Solving the credit assignmentproblem without a backward pass. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning,volume 162 of Proceedings of Machine Learning Research, pp. 49374955. PMLR, 1723 Jul 2022. URL Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255,2009. doi: 10.1109/CVPR.2009.5206848.",
  "Stephen Grossberg. Competitive learning: From interactive activation to adaptive resonance. CognitiveScience, 11(1):2363, 1987. ISSN 0364-0213. doi: URL": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision(ICCV), pp. 10261034, 2015. doi: 10.1109/ICCV.2015.123. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016. doi:10.1109/CVPR.2016.90.",
  "Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation, 9:173580, 121997. doi: 10.1162/neco.1997.9.8.1735": "Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee,Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efficient training of giant neuralnetworks using pipeline parallelism. In ArXiv, 2019. Zhouyuan Huo, Bin Gu, qian Yang, and Heng Huang. Decoupled parallel backpropagation with convergenceguarantee. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conferenceon Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 20982106. PMLR,1015 Jul 2018. URL Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducinginternal covariate shift. In Proceedings of the 32nd International Conference on International Conferenceon Machine Learning - Volume 37, ICML15, pp. 448456. JMLR.org, 2015. Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex Graves, David Silver,and Koray Kavukcuoglu. Decoupled neural interfaces using synthetic gradients. In Doina Precup andYee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70of Proceedings of Machine Learning Research, pp. 16271635. PMLR, 0611 Aug 2017.URL",
  "Alex Krizhevsky. Learning multiple layers of features from tiny images. pp. 3233, 2009. URL ~kriz/learning-features-2009-TR.pdf": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutionalneural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger (eds.), Advances in NeuralInformation Processing Systems, volume 25. Curran Associates, Inc., 2012. URL Michael Laskin, Luke Metz, Seth Nabarro, Mark Saroufim, Badreddine Noune, Carlo Luschi, Jascha Sohl-Dickstein, and Pieter Abbeel. Parallel training of deep networks with local updates. In ArXiv, 2021.",
  "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Ng. Reading digits innatural images with unsupervised feature learning. NIPS, 01 2011": "Arild N kland. Direct feedback alignment provides learning in deep neural networks. In D. Lee, M. Sugiyama,U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-ume 29. Curran Associates, Inc., 2016a. URL Arild N kland. Direct feedback alignment provides learning in deep neural networks. In D. Lee, M. Sugiyama,U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-ume 29. Curran Associates, Inc., 2016b. URL",
  "David E. Rumelhart and James L. McClelland. Learning Internal Representations by Error Propagation, pp.318362. 1987": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:Inverted residuals and linear bottlenecks. In 2018 IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 45104520, 2018. doi: 10.1109/CVPR.2018.00474. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kun-durthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An openlarge-scale dataset for training next generation image-text models. In ArXiv, 2022. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-CAM: Visual explanations from deep networks via gradient-based localization. Inter-national Journal of Computer Vision, 128(2):336359, oct 2019. doi: 10.1007/s11263-019-01228-7. URL",
  "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.In ArXiv, 2015": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958, 2014. URL C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-binovich. Going deeper with convolutions. In 2015 IEEE Conference on Computer Vision and PatternRecognition (CVPR), pp. 19, Los Alamitos, CA, USA, jun 2015. IEEE Computer Society. doi: 10.1109/CVPR.2015.7298594. URL",
  "BHyperparameter Discussion": "Similar to backpropagation, training deeper networks requires more epochs. Specifically, we opted for 200epochs for the shallow network and 500 for the deep network. Further, in line with Pezeshki (2023); Lowe(2023), we found the Forward-Forward algorithm is quite sensitive to the learning rate. Specifically, higherlearning rates tend to lead to instability, and lower learning rates simply lead to slow convergence. We havefound 103 to be a sensible default. For optimal results, however, we found a learning rate schedule to behighly beneficial. In our experiments, we train the deep network with a learning rate of 103 for the first200 epochs, then 5 104 for the next 200 epochs and 104 for the last 100. The shallow network uses 103 for the first 150 epochs and 104 during the last 50. In terms of batch size, we found 1024 (512 positiveand 512 negative) to work well but changing this slightly does not seem to noticeably influence accuracy.Further, both discussed loss functions have a hyperparameter. For the original loss function, we mention thevalue of in each experiment explicitly. The SymBa loss has a scaling parameter that penalizes incorrectclassification, we use a scale of 4 as recommended by Lee & Song (2023). We find that values in the range[2; 8] for this parameter do not impact accuracy, only the separation. In terms of data augmentation, arandom rotation and random crop are performed. Lastly, all networks are initialized using uniform Kaiminginitialization He et al. (2015).",
  "LF F (gpos, gneg) ( gpos) + (gneg )(1)": "This study inspects whether the loss function handles positive and negative samples equally.As statedearlier, in reality, the loss function is computed for a batch of positive and negative samples (shown insection 3) but we will use gpos and gneg as the goodness of a single positive and negative sample respectively.",
  "LF F ( + pos, + neg)?= LF F ( pos, neg)(2)": "Intuitively, the left-hand side corresponds to the loss of correctly classifying a positive sample and wronglyclassifying the negative one (false positive). The right-hand side represents the inverse, where the positivesample is misclassified (false negative). As the respective errors are the same, we expect the loss function tobe the same. When filling in Equation 2 with Equation 1, we attain the following.",
  "pos LF F ( pos, neg)(5)": "In essence, the first part computes what the partial derivative of the negative goodness is upon a falsepositive. The second computes the same but for the positive goodness upon a false negative. If any one ofthese is misclassified, derivatives of equal value are desired. To verify whether this holds for the loss function,the partial derivatives need to be computed first.",
  "Again, this is the case iff pos = neg, when positive and negative samples are equally separated from thethreshold": "Consequences.The loss function only satisfies our two criteria given a very specific condition: whenpos = neg. Intuitively, one would expect this to hold. However, if pos increases, so does, exp(pos)/1 + exp(pos)and similarly for neg, especially for small , which occurs most of the time. Therefore, if the separationof either positive or negative increases, that derivative will start to dominate the gradient, leading to anunstable equilibrium. Such imbalance can occur simply due to the variance in batches or due to it being simpler to generate highor low goodness for the optimizer. Another factor that can cause this imbalance is that goodness must bepositive. This sets a hard limit for the separation of negative samples, namely neg . In reality, thislimit is never even reached due to the optimization process. Achieving low goodness values necessitatesa delicate equilibrium, rendering the network highly responsive to any minor alteration.Since the lossfunctions constituent terms are aggregated, the gradients stemming from positive samples severely disturbthis balance. Conversely, this is not an issue for the positive samples as small disturbances from the negativegradients will not impact them as much. Scaling the threshold to be high enough to circumvent this behavioris not possible without repercussions; a high threshold will induce lots of instability in the network as allweights need to be higher, in some cases this may even lead to the inability to converge at all. This issue is exacerbated in weight-sharing architectures such as CNNs, given that it is more difficult tocontrol the exact output of multiple neurons given a single weight change. This can be observed in thefollowing experiment where we simply employ the shallow CNN in combination with an unaltered version ofFF (VFF/s). clearly shows that the accuracy declines upon going deeper into the network, whichis a large issue in many cases. Finally, due to the abovementioned issues, finding a correct value for is complicated. In limited experiments,we found the value that achieves the highest accuracy to be highly dependent on the architecture and dataset.However, all statements made or observed trends for a given (such as ) are verified to hold withinthe range [1;10].",
  "LSB(gpos, gneg) (gpos gneg)(9)": "This function suffers from a flaw that makes it possible to lower the loss function without learning newfeatures.As the loss is simply calculated by the separation, scaling all parameters uniformly (given analready existing separation) will lower the loss, not by learning but by simply amplifying the features.Mathematically, we can note the following.",
  "= ReLU(Fi(x, i)) = = ReLU(Fi(x, i))(10)": "This simply denotes that if the parameters of a layer i are able to achieve a separation , an arbitraryseparation can be achieved by choosing accordingly. Notice this only holds, if Fi is a linear operation,which is the case for a convolutional or a dense layer. Moreover, ReLU only disregards negative featureswhich has no impact on scaling the positive features (like sigmoid would). In theory, this uncontrolled scaling can have devastating effects on the stability of a network. However,empirically, we find that this behavior is negligible in all experiments performed. We verify this using thethree experiments listed below. To ensure the conjectured behavior has the highest chance to be observed,we perform these experiments on the model that achieves the highest accuracy on CIFAR-10, shown in. This model has the most layers and is trained the longest. Plot the Mean of Positive and Negative Goodness Throughout Training for Each Layer. Thisexperiment provides a comprehensive view of the evolution of the goodness which can be used to indirectlyassess the stability of specific layers through training. shows that some layers have significantlyhigher goodness than others, especially the second layer.Regardless, knowing that this is the squaredsum of thousands of features, all values remain within the expected range for neural networks and do notdemonstrate this uncontrolled scaling. Measure the Maximal Weight for Each Layer After Training.The magnitude of weights is onlyan indirect indication of stability or other common metrics. Nevertheless, low maximal weight ensures thatthe network suffers less from phenomena such as overfitting or instability. This experiment, depicted in, reveals that the highest weight has a value of 0.3 which occurs in the first layer. After the second",
  "ENormalization Deep Dive": "This appendix accompanies section 4 and provides additional plots and insights about the impact of nor-malization. Specifically, the statements made in that section will be substantiated further here. In contrast to the majority of other experiments in this work, this section uses the FCN architecture consistingof 6 layers that all have a hidden dimension of 2048. This choice is made to ensure that any trends throughoutlayers are the product of the learning algorithm and not the architecture (for example side effects of amaxpool). The observations made in this section are also verified to hold for CNNs, but not shown. No Normalization Leads to Feature Recycling. We empirically demonstrate the feature recyclingphenomenon as denoted in (Hinton, 2022). The recycling behavior stems from the reuse of features thatalready produce high/low goodness depending on the sample. Any subsequent layer can therefore simplyperform an identity operation or even a permutation and produce low loss without learning anything new. The following plots () demonstrate that networks trained without normalization, have substantiallylower gradients, especially after a few layers. Compared to the other normalization strategies, the right-hand side of shows that the gradients of the network without normalization are several orders ofmagnitude lower. Further, the left-hand side shows that the magnitude of the gradients decreases throughoutlayers. Layer Normalization Leads to Information Loss.Information loss is often quantified in terms ofmutual information. Specifically, if an embedding has low mutual information with the input, it is saidto have lost information. Informally, mutual information conveys how much information is shared betweentwo distributions. Or alternatively, how much information does sampling one distribution reveal about theother and vice-versa. Consider a very narrow (bottleneck) layer that only has very few features, generally,it will be impossible to redetermine the original input exactly. However, this phenomenon occurs not onlywhen reducing dimensionality but at many locations. For instance: a ReLU operation loses all negativeinformation, and numeric approximation (which occurs at almost every floating point computation) losesinformation too. The mutual information between the input and the label is generally low, from the label alone, the exact inputcannot be determined. Therefore, at some point, the network will lose information. It has been shown thatmodern networks retain most information until the classification head aggressively narrows dimensionality(Shwartz-Ziv & Tishby, 2017). This allows later layers to manipulate data to become even more separable",
  ": Layerwise maximal weight (left) and maximal goodness (right) for layernorm, batchnorm, and nonormalization. The metrics are from the same run as depicted in": "Maximal values are good for putting bounds on the variance of a network but give a narrow view of thedistribution.Simply looking at the weights themselves () gives a qualitative insight into thedifferences of distribution: batchnorm has drastically less spread out weights. : A visual representation of the weights at layer 6 of the VFF+SymBa/f model.The y-axiscorresponds to the output neurons and the x-axis to the input neurons. Additionally, the values are clampedwithin [-0.3; 0.3] for visual clarity. The metrics are from the same run as depicted in .",
  "This appendix will provide an overview of the role and implementation of Overlapping Local Updates (OLU)within FF. Prior to this, a detailed explanation of OLU is given": "Details of OLU. Intuitively, OLU is a form of truncated backpropagation where two layers are updatedat once.At each step in the learning process, instead of simply updating the current layer given thelocal objective function, the previous layer is also updated. In the context of FF, there are two possibleimplementations. Both approaches are depicted in .",
  "Approach 2: Update both the current and previous layer at each step": "In terms of accuracy, both techniques are equal. The discrepancy lies in their runtime; we found the firstapproach to be faster than the second on the setup we used. Therefore, we opted for the first approach.However, we expect, with some tuning, both techniques can perform equally fast. : A visual representation of two possible OLU implementations. The red lines represent errorsignals. At even iterations, even objectives are optimized and at uneven iterations, uneven objectives areoptimized. Alternatives to OLU. In the case of this work, OLU was inspired by the concept that neurons not onlysend signals to other neurons within their layer but also to neighboring neurons, such as the previous layer.OLU takes the simplest approach from a contemporary research standpoint and allows a full error signalto be locally propagated. However, more biologically plausible alternatives exist, in which random synapticfeedback can be used to locally update neurons (Lillicrap et al., 2016). We encourage future work in thisarea to explore this technique.",
  "GArchitecture Discussion": "The main role of this appendix is to depict and describe the two main architectures used throughout thiswork.However, before that, a collection of empirical observations will be provided that led us towardthis design. These observations will be contrasted with backpropagation and outline the differences andsimilarities to this algorithm. During the experimentation phase of this work, the viability of training several architectures with FF wastested. This manual search focussed on simple CNNs, meaning only combining convolutions and maxpooloperations. This exploration entails more hyperparameters than might be expected. For instance, techni-cally FF is proposed as a layer-wise learning algorithm. However, it can be used as a general algorithm totrain entire blocks, such as (He et al., 2016; Szegedy et al., 2015), or simply a subnetwork. However, thiswork limits itself to layerwise training and defers this avenue of research to future work. When training simple CNNs, two parameters of the network are of importance: the width and the depth.Varying these parameters directly influences the learning speed (time to reach a threshold in accuracy) aswell as the final accuracy (accuracy convergence after long training). The learning speed difference betweenshallow and deep networks is a well-known phenomenon in backpropagation (especially in non-residualarchitectures). This stems from the increasing instability of gradients (He et al., 2016). In local learning, asno global gradients are used, this issue does not arise. The accuracy of the local algorithms is mostly determined by the width (the number of features or channels) ofthe layers. There exist several architectures that intentionally reduce the number of features of certain layers,such as autoencoders (Rumelhart & McClelland, 1987). These networks can be trained with backpropagationwhich will result in these narrow layers containing a compressed representation of the input. However, inlocal learning regimes, this bottleneck simply leads to information loss as the local objective struggles toretain useful downstream information. Conversely, scaling the width of a layer severely impacts the learningspeed, intuitively, more features lead to a more challenging optimization process as local minima are harderto find. Therefore, the dimensionality of the features after each block is of paramount importance to thefinal accuracy. When using local learning on CNNs, this balance in network width can be balanced through lossy operationssuch as maxpool operations. Therefore, their locations severely impact the learning characteristics of thenetwork. This disparity causes a large difference between the shallow and deep networks used in this work.As can be seen on , the shallow network is not simply a subset of the deep network. Empirically, asdiscussed in section 6, we confirm the expected tradeoff between learning speed and final accuracy betweenthese architectures. As explained in section 5, each layer of our architecture deviates from the ordinary order of operations.Particularly, each layer first normalizes, followed by the convolution and the non-linearity. Lastly, an optionalmax pool is performed before the calculation of the goodness, which is explicitly shown in . Thisfigure shows the progression of channels through the architecture, where C is the number of input channels.",
  "HEvaluation Strategies": "As originally discussed in section 3, there are several methods to evaluate with a network trained in asupervised FF setting. This appendix explores some such methods and compares their effect on the finalaccuracy. Goodness-Based Classification. This evaluation method stands out as the prominent classification tech-nique to achieve the highest possible accuracy with FF (Hinton, 2022). This stems from the fact that thistechnique leverages the exact purpose the network was trained for. It evaluates the unseen example with allpossible labels in the dataset and selects the label with the highest goodness, as this is the most positive.The main disadvantage, however, is that it requires an evaluation of the network for each possible label,which is prohibitively expensive in most cases.",
  ": The two CNN architectures used throughout this work. The shallow network (left) has 2.8 millionparameters and the deep network (right) has 8.5 million parameters": "Feature-Based Classification. This method does not necessitate sampling each class individually. Instead,it utilizes a neutral label, which is the average of all class labels, to generate features that represent all classes.These features are then used by an auxiliary network to make a single prediction encompassing all classes.In line with (Hinton, 2022), we find that this method generally suffers from instability and therefore does notachieve the accuracy of the aforementioned technique. We have performed several experiments to remedythis without success: freeze the whole network except the auxiliary head, use normal negative labels insteadof neutral ones, use very low learning rates, and use a classifier consisting of multiple layers. Ensemble Classification. The fact that the objective of each layer is the same can be leveraged in bothaforementioned techniques.Unlike ordinary backpropagated networks, which only use the last layer togenerate a prediction, any layer or combination of layers of the network can be used for evaluation. Inessence, the goodness or features from multiple layers can be aggregated into an ensemble that is moreresilient to noise and generally achieves higher accuracy. This aggregation can be a straightforward averageor even a small classifier of any kind. To predict, the highest value of the mean goodness is used, instead ofa single goodness value, which results in more stability. This is depicted in . In this work, we usethe average of the goodness to make the prediction. With this technique, we are able to augment the testaccuracy of all our deep models by a few decimals, shown in . The shallow models do not benefit fromaveraging the last few layers as the accuracy of the last layer is significantly higher than its predecessors.",
  "only last layer99.58 0.0691.38 0.3894.31 0.0783.51 0.78three last layers99.62 0.0291.51 0.2794.36 0.0483.72 0.80": ": A depiction of the two prominent evaluation techniques for Forward-Forward. The black arrowrepresents data flow and the red arrows represent gradient flow. Goodness-based classification (right) requiresthe network to be evaluated N times according to the amount of label. The head in this figure can rangefrom a simple average to a small classifier. motivation between residual connections is twofold: it provides a sensible (indirect) initialization for eachlayer, namely the identity matrix, and allows the backward gradients to bypass each layer. This results in asmoother convergence, especially for deeper networks. Within FF, both motivations no longer hold. First, adding the identity to each layer is not desirable as thismay discourage the network from learning new features. Second, there is no need for backward gradientsto bypass layers, as there is no full backpropagation. Additionally, the models used within this work aretoo shallow enough to fully benefit from residuals, even if they were trained with backprop. Interestingly,preliminary experiments using TFF/s and TFF/d on CIFAR-10 with residual connections paint a moreneutral picture. Specifically, early during training (the first 30 epochs), the use of residual connection yieldsimproved results. However, after 100 epochs the accuracy of the small and deep models are respectively 4%and 2% lower. Therefore, we believe residual connections to hold some promise, if modified in a sensiblemanner to the requirements of FF."
}