{
  "Abstract": "We introduce a novel representation of monotone set functions called Extended Deep Sub-modular functions (EDSFs), which are neural network-representable. EDSFs serve as anextension of Deep Submodular Functions (DSFs), inheriting crucial properties from DSFswhile addressing innate limitations. It is known that DSFs can represent a limiting subsetof submodular functions. In contrast, we establish that EDSFs possess the capability torepresent all monotone submodular functions, a notable enhancement compared to DSFs.Furthermore, our findings demonstrate that EDSFs can represent any monotone set function,indicating the family of EDSFs is equivalent to the family of all monotone set functions.Additionally, we prove that EDSFs maintain the concavity inherent in DSFs when the com-ponents of the input vector are non-negative real numbersan essential feature in certaincombinatorial optimization problems. Through extensive experiments, we demonstrate thatEDSFs exhibit significantly lower empirical generalization error in representing and learningcoverage and cut functions compared to existing baselines, such as DSFs, Deep Sets, and SetTransformers.",
  "Introduction": "Submodular functions have found extensive applications in various fields of study, including modeling influencein social networks Kempe et al. (2003), energy functions in probabilistic models Gillenwater et al. (2012),and clustering Narasimhan et al. (2005). In particular in economics, a wide range of scenarios incorporatethe concept of diminishing marginal return, where acquiring more goods results in diminishing the overallsatisfaction or the so-called utility McLaughlin & Chernew (2001); Kimball et al. (2024). There are several challenges associated with the widespread use of these functions in recent machine learningapplications. To improve the modeling of submodular functions in these applications, efforts have been madeto represent submodular functions using differentiable functions, such as neural networks. This representation enables the solution of key submodular optimization problems through the utilization of gradient-basedmethods and convex optimization. To illustrate this, suppose we want to maximize a submodular function funder certain constraints. In the exhaustive-search approach, we would have to check all 2n subsets, which isexponential in the size of the ground set S. However, with a differentiable representation of f, we can utilizefirst-order information of the function, such as gradients, and project onto the constraint set to solve theoptimization problem more efficiently. There have been previous efforts to represent submodular functions using neural networks. For instance,in Bilmes & Bai (2017), the authors introduced a neural network architecture called Deep SubmodularFunctions (DSFs), consisting of feedforward layers with non-negative weights and normalized non-decreasingconcave activation functions. Functions in this class exhibit interesting properties, such as the concavity ofthe function when the components of the input vector are all non-negative real numbers. By increasing thenumber of layers in the DSFs architecture, the family expands, indicating that there are functions in DSFswith n + 1 layers that are not present in DSFs with n layers. However, as stated by the authors, DSFs cannotrepresent all monotone submodular functions, which highly restricts their applicability to many machinelearning problems. In this paper, we introduce a novel neural network architecture, called Extended Deep Submodular Functions(EDSFs), which not only have the capability to represent any monotone submodular functions but canalso represent any monotone set functions. Moreover, same as in DSFs, when the components of the inputvector are all non-negative real numbers, EDSFs are concave, an important feature applicable in variouscombinatorial optimization settings. In addition, our experiments demonstrate that EDSFs are able to learnone of the most complicated monotone submodular functions, i.e., coverage functions, with significantly lowerempirical generalization error compared to DSFs. We define EDSFs as the minimum of r DSFs. Although inour proofs, exponential number of DSFs are needed to represent any monotone submodular function, in ourexperiments we observed that we can learn coverage functions and monotone cut functions using much fewerDSFs1. The rest of the paper is organized as follows: In , we formally define DSFs and state some of theirimportant properties. In , we introduce our augmented architecture to represent all monotone(submodular) set functions and provide a proof. In , we demonstrate the superior performance ofEDSFs in learning coverage functions through numerical evaluations.",
  "Related Works": "The exploration of neural networks for modeling submodular functions is relatively sparse in the existingliterature. A notable contribution is the introduction of DSFs Bilmes & Bai (2017); Dolhansky & Bilmes(2016). Building on this work, the authors in Bai et al. (2018) address the maximization of DSFs undermatroid constraints, using gradient-based methods to solve the optimization problem. Their work providestheoretical guarantees, establishing a suitable approximation factor given the problems constraints. Morerecently, a novel architectural approach has been proposed in De & Chakrabarti (2022), which not onlypreserves submodularity but also extends its applicability to a more generalized form, accommodating-submodular functions. This signifies a notable advancement in the landscape of neural network-basedmodeling of submodular functions, expanding the scope of potential applications and providing a platformfor exploring more nuanced and versatile representations within this domain. There are other works thatattempt to represent monotone set functions using neural networks. For example, in Weissteiner et al. (2021),the authors introduce a neural network architecture that represents all monotone set functions and exploretheir use in designing auctions. Authors in Zaheer et al. (2017) introduces Deepsets, a method for learning permutation-invariant set functions,offering a flexible architecture that can handle set-based tasks. While their work provides a powerful way tomodel general set functions, they do not explicitly address the estimation of submodular functions, whichrequire handling the diminishing returns property. This limits its direct applicability to submodular functionestimation, as their focus is more on tasks like point cloud classification and set expansion rather thanoptimizing or estimating submodular structures. Inspired by this work, Lee et al. (2019) introduces the Set",
  "All of the codes associated with experiments are available at": "Transformer, an attention-based neural network architecture designed for processing set-structured data.While the authors demonstrate its effectiveness on various set-input tasks, its performance was not specificallyevaluated on estimating set functions such as submodular functions. Wagstaff et al. (2019) talks more aboutthe theoretical side of works like Deepsets and Set Transformer, providing insights into the limitations ofrepresenting permutation-invariant functions on sets. While their work doesnt specifically address submodularfunctions, it provides a general framework for understanding the representational power of neural networkson set-structured data. However, their focus is primarily on theoretical limitations rather than practicalalgorithms for learning specific classes of set functions, leaving open questions about how these insightstranslate to the efficient learning of submodular functions in particular. One of the works related to learning general submodular functions is Feldman & Vondrk (2016). Theauthors prove tight bounds on approximating submodular functions by juntas, showing that any submodularfunction can be -approximated by an O( 1",
  "log 1": ")-junta. Although their work gives a good insight intolearning submodular functions, it mainly focuses on the theoretical existence of these estimators anddoesnt fully provide a practical way of learning them. The authors in Balcan & Harvey (2018) investigatesubmodular functions from a learning theory perspective, developing algorithms for learning these functionsand establishing lower bounds on their learnability. Moreover, the authors in Feldman & Kothari (2014)attempt to approximate and learn coverage functions in polynomial time. There are some works that talk about the combinatorial and submodular optimization. For example, Sakaue(2021) introduces a differentiable version of the greedy algorithm for monotone submodular maximization,called Smoothed Greedy. One of the strengths of this work is that it maintains theoretical guarantees similarto the original greedy algorithm Vondrak (2008) while enabling gradient-based learning. However, they workdoesnt directly address the estimation of submodular functions and only focusing on maximizing a (given)single submodular function. In our work we aim to estimate any given submodular function and use thisestimation for downstream tasks related to submodular maximization such as social welfare maximization. inanother work, Wilder et al. (2019) introduces a framework called decision-focused learning that aims tobridge the gap between predictive models and combinatorial optimization. Their approach involves end-to-endtraining of machine learning models to directly optimize decision quality, rather than prediction accuracy.However, in their work, the target function can be described by an unknown parameter and the aim is toperform optimization and learning together, whereas our work aims to learn the submodular function itselffrom data. Lin & Bilmes (2012) introduces the concept of submodular shell mixtures for learning submodular functions ina structured prediction setting. While proposed method achieves strong results in document summarization,it reliance on predefined shell and approximate inference may restrict the class of learnable functions. Theselimitations highlight the ongoing challenges in developing versatile methods for learning submodular functions.",
  "f(A v) f(A) f(B v) f(B).(4)": "In the remainder of the paper, without loss of generality, we will focus on normalized monotone set/submodularfunctions. If the function is not normalized, we can simply subtract the value of f() from the function so asto make it normalized. Note that this transformation also maintains the submodularity of the function. Definition 2.5. (Sum of Concave Composed with Modular Functions Bilmes & Bai (2017)) Assume a finiteset S (nodes or input features) with cardinality n. Given a set of m1, m2, . . . , mk (mi : 2S R+) modularfunctions and 1, 2, . . . , k (i : R+ R+) being their corresponding non-negative, non-decreasing,normalized (i.e., i(0) = 0, i), concave functions, and an arbitrary modular function m : 2S R, theSCMM, g : 2S R, derived by these functions is defined as,",
  "i=1i(mi(A)) + m(A).(5)": "In the rest of the paper, since we only consider the monotone set functions, we pull our attention into monotoneSCMMs, which the range of modular functions is only the positive real values, namely, m : 2S R+. Based upon recent developments, it has been discovered that the aforementioned functions given in Equation 5exhibit inherent submodular characteristics Bilmes & Bai (2017). Looking ahead, we can view these functionsas a single-layer neural networks equipped with nonlinear activation functions that exhibit concavity properties,similar to a linear mixture of inputs followed by a stepwise activation function such as ReLU (Rectified LinearUnit) after computing the overall output values. By employing this insight, we can expand the horizons of submodular functions by leveraging them acrossmultiple tiers. We now introduce a pivotal concept that facilitates the utilization of advanced artificialintelligence tools, specifically deep learning. Definition 2.6. (Deep Submodular Function Bilmes & Bai (2017)2) Assume we intend to define L (i.e.,depth of the network) layer DSF. If L = 1 we use an SCMM (without last summation). Therefore, inthis case we have a single-layer n k network (with non-negative weights) with submodular outputs. ForL = l > 1 we first assume the output of l 1 layers as a given n k DSF. Lets denote the output nodes ofthe given DSF by B = {1, 2, . . . , k}. Now we want to add one layer at the end of the given DSF. Weappend a k m fully connected layer with non-negative weights and corresponding normalized non-decreasingconcave functions 1, 2, . . . , m for each newly added node. Then we define the outputs of the network asC = {1, 2, . . . , m}",
  "and bv R+ is a bias parameter of the node. In this scheme, we have new layer added to the DSF. Therefore,we introduced a n m DSF with L = l layers. As an example, a 3-layer DSF is shown in": "With this understanding, one can guarantee that the outcome of this architecture forms a submodularfunction, as it is shown in Bilmes & Bai (2017). An established finding regarding SCMMs states that anySCMM employing an arbitrary activation function can be represented as a two-layer SCMM with only themin activation function Bilmes & Bai (2017). Despite these findings, there exist notable limitations when it comes to DSFs. While DSFs possess a range ofcapabilities, they are unable to encompass all submodular functions. This implies that there will always besubmodular functions that cannot be represented using any number of layers in DSFs Bilmes & Bai (2017).To address this limitation, in , we extend DSFs by adding a limited number of components in thenetwork architecture to represent all submodular functions.",
  "We also define the following important submodular problem which we need later in our experiments": "Definition 2.10. (Submodular Welfare Maximization) For a collection of n users with v1, v2, . . . , vn : 2S Ras their (estimated) submodular valuation functions on each subset of the finite set S with |S| = s, thesubmodular welfare maximization problem aims to maximize the social welfare function, i.e., the sum of allvaluation functions when we partitioned set of items S and assigned to the users, and is formally defined as",
  "uiBAi w(u)": "Example 2.12. As an example of the coverage function, for a ground set of size 4, we illustrate the coveragefunction with universe size 10 and with uniform weight of 1 for all of the elements in the . In thisexample we have f({1, 3}) = 7 and f{3, 4}) = 8. Alternatively, coverage functions can be described as non-negative linear combinations of monotone disjunc-tions. There are a natural subclass of submodular functions and arise in a number of applications Feldman &Kothari (2014).",
  "h(A) = min {f1(A), f2(A), . . . , fr(A)} ,A S.(11)": "In the rest of this section, we aim to demonstrate that we can represent any monotone submodular functionf using some g EDSFs. The idea behind the proof is to leverage the relationship between the minimum ofsubmodular functions and the intersection of their polymatroids. The main result of this paper is summarizedin the following theorem.Theorem 3.2. Family of monotone set functions is exactly equal to the family of Extended Deep Submodularfunctions (EDSFs). In the following, we will go to prove this result step by step. First, we assume that a submodular functionf : 2S R is given. For convenience, we use these notations to simplify our discussion.Definition 3.3. We define:",
  "where wj = cA for any j A and wj = cj for any j / A, and wj represents the corresponding weightof some neural network": "Using these definitions, in the first step, we will show that each gA can be represented using a simply two-layerDSF.Lemma 3.4. For any A S, we can design a two-layer DSF that represents the gA function, therefore thegA function is a submodular function. Proof. To represent the function gA using a DSF, we have constructed a network architecture as illustratedin . This architecture comprises only two layers. The first layer contains two nodes, one of whichemploys the minimum function as its activation layer. The second layer consists of a single output noderesponsible for computing the sum of the inputs. All edges connected to the lower node possess a weight of cA. Conversely, each edge linked to the upper nodebears a weight corresponding to the value cj, specifically f(j) according to the definition.",
  "We next introduce a useful lemma": ": The simple architecture for the representation of gA as a DSF. The input of the network is a0-1 vector corresponding to the subset B. The subset B could have non-empty intersection with subsetA = {a1, . . . , ak} and S \\ A = {j1, . . . , jnk}.",
  "Proof. To show the equality of two aforementioned sets, we first show PgA LA, then we show LA PgA,which completes the proof": "For the first part, for any x PgA, we have for all B S, x(B) gA(B), therefore, for any B A, we havex(B) cA, in fact in this scenario gA(B) = 0 or cA, that implies x(A) cA. Furthermore, for any j / Awe have xj f(j) = cj. Therefore, each point in the polymatroid has the conditions to be in the LA, thatmeans, x LA. For the reverse part, for any x LA, we have x(A) cA, which implies that B A, B = , x(B) cA =gA(B). For any one-member subset of S, for example B = {j}, j / A we have xj cj = gA(B). Furthermore,for any arbitrary B S, we could write B = (A B) (B \\ A). We have two cases,",
  "Incorporating this component into DSF, enhances its capability of representing submodular functionseffectively": "In general, note that the min operator does not maintain the submodularity of the input functions. However,during the proof of the Theorem 3.9, we have used some techniques to assure that the output function wouldbe submodular, as it can be seen in the following. Moving towards the last stage, we present an architecture that is built upon the provided submodular function.In the initial layers, for any subset Ai S, we have crafted the functions gAi using the architecture outlinedin . Subsequently, we applied a min-component to all of these constructed gAi functions, resulting ina composite function denoted as g. Therefore, the corresponding EDSF is generated.",
  "where w = f(S), which is the maximum value function f can take (because f is monotone). Now we definefunction g as follows:g(A) = minBS {gB(A)} .(25)": "3Refer to Remark 5.2 for more discussion about Theorem 3.9. Moreover, in the review process, a simpler proof was alsosuggested by the anonymous reviewer, which for the sake of completeness have been presented in Appendix C. For any A S we can see that gB(A) = f(A) if B = A. Now suppose B = A. If A \\ B = , gB(A) f(S) f(A), because of monotonicity. On the other hand if A \\ B = we know that A B. In this case we havegB(A) = f(B) f(A) , because of monotonicity. Hence we can see that gA(A) gB(A) for any B S.Therefore, g(A) = f(A) for all A S.",
  "Theorem 3.12. Given g EDSFs, g is a concave function with respect to the input vector, if the inputvector components are all non-negative real numbers": "Proof. Since all the DSF functions are concave in this setting Bai et al. (2018) (See Corollary 1), and g isthe minimum of a number of DSFs and we know that minimum of concave functions are concave, we canconclude that g is also concave. We can exploit the above-mentioned property to solve certain combinatorial optimization problems, such asthe social welfare maximization problem, using gradient-based methods in an efficient manner. This providesa powerful tool to handle some combinatorial problems. Applications in this context are discussed in .",
  "Experimental Results": "In the following section, we showcase a series of experiments aimed to demonstrating the positive outcomesand advantages derived from the application of Extended Deep Submodular Functions (EDSFs) in themodeling of submodular functions. Additionally, we highlight their efficacy in efficiently addressing andsolving various combinatorial optimization problems. Through these experiments, we aim to provide aclear and comprehensive understanding of how EDSFs contribute to improved outcomes in the domain ofsubmodular function modeling and the optimization of complex combinatorial scenarios.",
  "As outlined in , coverage functions constitute a crucial and intricate subset of monotone submodularfunctions, posing challenges in accurate learning from their instances": "Our experimental findings indicate that, in contrast to Deep Submodular Functions (DSFs), Extended DeepSubmodular Functions (EDSFs) shown to be effective in efficiently learning these complex functions, exhibitingmuch lower empirical generalization error, compared to DSFs. To perform our experiments, from each coverage function (defined in Definition 2.11), we generate a randomdataset D = (Xi, yi)di=1 where Xi is a random subset from the ground set S, and yi is the value of thecoverage function. To create a coverage function for our experiments, we define the universe size, the numberof items (subsets), and the probability that each element in the universe independently belongs to each subset.Additionally, the weights in the coverage function are kept constant with a value of 1. For each experimentwe generate a dataset, allocating 80% for training and 20% for testing. The learning setup for all experiments in this section is the same. The optimizer used is Adam with alearning rate of 0.01. Each model is trained on a dataset of 1024 samples for 10,000 epochs. The employed",
  "cost function is the L1-loss function. Additionally, the weights for all EDSF and DSF neural networks areinitialized using a Gaussian distribution with a mean of 0 and a variance of 0.01": "In our first experiment, both an EDSF and DSF were trained to learn a coverage function with a universesize of 100, 16 subsets (items), and probability of 0.2. The architectures of the EDSF and DSF are quitesimilar, with the main difference being the min-component at the end of the EDSF. Each model consistsof three fully-connected layers with 64 neurons, using (x) = min(, x) (a minimum linear unit) as theactivation function, where = 95. Additionally, we compared the performance of EDSF against otherbaselines, including the methods of Zaheer et al. (2017) and Lee et al. (2019), in learning the target coveragefunction. The neural networks were trained using the setup described above. presents the train andtest losses for each model, showing the mean and standard deviation across 20 runs. As shown, the EDSFdemonstrates a significant improvement over the other baselines.",
  ". 5 hidden layers with respectively 32, 64, 128, 64, and 32 neurons, alpha set to 400: test loss is 81.4188": "These experimental results demonstrate the superiority of EDSF compared to DSF in learning the coveragefunctions. Additional DSF architectures (including larger ones) were also tested, and the corresponding plotsare presented in Appendix A, Figures 11 and 12. At the end of our first set of experiments, we conducted additional tests to examine the effects of variousactivation functions on the performance of DSFs during training. We tested three activation functions:",
  ". a(x) = (x) 0.5": "The resulting plots for the loss functions, training, and testing performance are provided in Appendix A,Figures 13-15. As shown, none of these activation functions improved DSF performance in learning the coverage function. Additionally, as discussed in , Remark 5.4, the only effective activation functionfor training EDSFs is MiLU, while none of these activation functions performed well when learning thecoverage function with EDSFs. In our second experiment, we tested different values of r, the number of DSFs used before the min function, toexamine the effect of the EDSF network size on the generalization error of the training. The target functionis a coverage function with a universe size of 500, 16 items, and a probability of 0.2. All experimental setupsare the same as in our first experiment, with the only difference being the last layer, which now reflects thevalue of r, meaning that our EDSF has 4 layers with 64, 64, 64, and r neurons, respectively. presentsthe test loss for various values of r, showing the mean and standard deviation across 20 runs.",
  "Learning Cut Functions": "We conducted several experiments to learn modified graph cut functions (we choose modified version tomaintain the monotonicity), which is a well-known submodular function. Firstly, we generated the randomgraph using Erdos-Renyi model (with probability 0.2 and 50 vertices), then, for any set of vertices X, weconsidered the function f(X) = |cut(X)| +",
  "aX deg(a) which is a monotone submodular function": "The architecture used to train the EDSF on the data is identical to the one used for learning coveragefunctions. It consists of three fully-connected layers with 64 neurons, each using (x) = min(, x) (a minimumlinear unit) as the activation function, where = 95. In the last layer, we incorporated a min-componentwith r = 64. Similar to the coverage function experiment, we employed DSF, Deep Sets, and Set Transformer to learn thecut function and compare their performance with our proposed EDSF. The DSF used in this experiment isidentical to that used for learning coverage functions, consisting of three layers with 64 neurons each, exceptfor the alpha parameter, which is set to 450. For training our network, we used the Adam optimizer with alearning rate of 0.01, employed the L1-loss function as our cost function, and trained each network for 10,000epochs. Additionally, the weights for the neural networks were initialized using a Gaussian distribution witha mean of 0 and a variance of 0.01. presents the train and test losses for each model, showing themean and standard deviation across 20 runs",
  "Social Welfare Maximization": "In these experiments, we leveraged the previously discussed concave property inherent in EDSFs to addressthe widely recognized combinatorial optimization problem of maximizing social welfare, as delineated in. In this specific context, each of the valuation functions, denoted as v1, v2, . . . , vn, is assumed to bean EDSF or a DSF, acquired through learning from samples collected from users. Our optimization problem,expressed in Equation 9, can be reformulated as follows,",
  "aij .(27)": "As we mentioned, each of the vis are EDSF or DSF, so they are concave, hence, the relaxed problem is clearlya convex problem, e.g., maximizing a concave function with convex constraints, and can be solved usingconvex optimization techniques such as projected gradient ascent. The projection step consists of m distinctprojections on probability simplex for each item. The algorithm pseudo-code is shown in Algorithm 1. In this set of experiments, we assume there are 3 users (bidders), each with a coverage function as theirtrue submodular valuation function. Subsequently, our network is trained to learn each bidders valuationfunction. Based on these learned networks, gradient ascent is then used to find a semi-optimal allocationthat maximizes social welfare. Finally, the estimated social welfare is calculated based on this maximizingallocation with the true valuation functions. The learning setup for all the experiments in this section is the same. The optimizer is Adam with a learningrate of 0.01. Each model is trained with a dataset of size 64 for 10,000 epochs. The employed cost function",
  "end for": "is the L1-loss function. Additionally, the weights for all neural networks are initialized with a Gaussiandistribution with a mean of 0 and a variance of 0.01. Finally, for the gradient ascent part, the learning rate() is 0.001. For our first experiment, we assume that the coverage function for each bidder has a universe size of 60 and8 items. However, the coverage probabilities are 0.1, 0.3, and 0.5 for the three bidders, respectively. Thelearning model is an EDSF with 4 layers of 64 neurons, using MiLU with = 95 as its activation function.Three neural networks are trained, one for each bidder, following the learning setup described above. shows the predicted social welfare and the optimal social welfare (calculated using brute force on the truevaluation functions) for 10 different experiments.",
  "Average115.6127.390.8288": ": Experiments when value functions are coverage functions, with 60 universe size, and 8 items, withcoverage probabilities (p) 0.1, 0.3, and 0.5 for three bidders respectively, and the learning model is EDSF. Inthe phase of learning EDSFs we have used the same setting as mentioned in the . The number ofbidders (n) is 3 and the learning rate for the gradient ascent () is 0.001. The second experiment is the same as the first experiment, except that our learning model is a DSF with 3layers of 64 neurons, using the MiLU activation function with = 95. The corresponding results can be seenin . As observed, the average efficiency in this experiment is significantly lower than in the experimentwith the EDSF model. Furthermore, we conducted an experiment to compare the performance of vanilla neural networks versusEDSFs in estimating the optimal allocation for maximizing social welfare. By \"vanilla neural network,\" wemean that weights can be negative, and activation functions may be non-concave. In this experiment, a",
  "Average8412666.7279": ": Experiments when value functions are coverage functions, with 60 universe size, and 8 items, withcoverage probabilities (p) 0.1, 0.3, and 0.5 for three bidders respectively, and the learning model is DSF.In the phase of learning DSFs we have used the same setting as mentioned in the . The number ofbidders (n) is 3 and the learning rate for the gradient ascent () is 0.001. fully-connected neural network with 3 layers, each having 64 neurons and using the ReLU activation function,was employed to learn the bidders valuation functions. The networks were trained using the setup describedabove, and the results are shown in . We observe that the mean efficiency is about 77%, which ismuch lower than the EDSF efficiency of (90%).",
  "Average76.1398.277.55": ": Experiments when value functions are coverage functions, with 50 universe size, and 8 items,with probablities 0.1, 0.3, and 0.5 for three bidders respectively, and the learning model is vanilla neuralnetwork.The number of bidders (n) is 3 and the learning rate for the gradient ascent () is 0.001 Additionally, experiments with larger universe sizes of 500 and 1000 were conducted, and the correspondingresults, comparing the performance of EDSF and DSF in predicting optimal social welfare, are shown inTables 7 and 8, respectively. Other settings for these experiments are the same as those in Tables 4 and 5.",
  "Average894.0618.0904.098.9668.35": ": Experiments comparing EDSF and DSF efficiency in the maximizing social welfare problem, withcoverage function as value function, 500 universe size, with probabilities 0.1, 0.3, and 0.5 for three bidders,respectively. In the phase of learning EDSFs and DSF we have used the same setting as mentioned in the. The number of bidders (n) is 3 and the learning rate for the gradient ascent () is 0.001",
  "Average1752.41648.41783.098.292.60": ": Experiments comparing EDSF and DSF efficiency in the maximizing social welfare problem, withcoverage function as value function, 1000 universe size, with probabilities 0.1, 0.3, and 0.5 for three bidders,respectively. In the phase of learning EDSFs and DSF we have used the same setting as mentioned in the. The number of bidders (n) is 3 and the learning rate for the gradient ascent () is 0.001. Finally, we conducted a set of experiments to demonstrate the optimal learned social welfare based on thetrained EDSFs and DSFs. All details of this experiment are the same as in Tables 4 and 5, except for theuniverse size, which is set to 1000. Results are presented in . As observed, the optimal learned socialwelfare for EDSFs is close to the optimal social welfare, indicating a good estimation of the true valuationfunctions. However, the optimal learned social welfare for DSFs is significantly lower than the optimal value.Additionally, it is worth noting that the gradient ascent algorithm, when using EDSF networks, achievesreasonably good performance and shows only a slight difference from the optimal learned social welfare forEDSFs. Observing the results, it becomes apparent that the utilization of Extended Deep Submodular Functions(EDSFs) in the context of the social welfare maximization problem yields significantly higher efficiency whencompared to Deep Submodular Functions (DSFs) and also vanilla neural networks. This stark differencein efficiency underscores the potential advantages and superior performance that our proposed framework,leveraging EDSFs, can bring to the modeling of user valuations within the realm of this NP-hard combinatorialoptimization problem.",
  "Avg2318.62005.72372.42084.32380.897.784.5": ": Experiments comparing EDSF and DSF efficiency in the maximizing social welfare problem, withcoverage function as value function, 1000 universe size, with probabilities 0.1, 0.3, and 0.5 for three bidders,respectively. In the phase of learning EDSFs and DSF we have used the same setting as mentioned in the. The number of bidders (n) is 3 and the learning rate for the gradient ascent () is 0.001. that the assumption on the exponentiality of the number of DSFs is just used as a sufficient condition for ourproofs. In our experiments, esspecialy in with different numbers of DSFs tested, we observed thatusing much fewer number of DSFs than exponential order is enough to attain good generalization in practice.For example, in the problem of learning coverage functions, we used 64 DSFs to represent a function with16 and 50 items as input (note that 64 216 and 64 250). We have also used 64 DSFs in learning cutfunctions of graphs with 50 vertices (64 250). Remark 5.2. Regarding Theorem 3.9, we would like to note that the architecture of EDSFs was derived throughan analysis of polymatroids, which also forms the basis of proving Theorem 3.9. We believe that includingTheorem 3.9, along with its proof based on polymatroid theory, is essential for a deeper understanding ofEDSFs. Additionally, during the review process, a simpler proof of Theorem 3.9 was suggested by one of thereviewers, which we have included in Appendix C for completeness. Remark 5.3. There exists some approximation algorithm to find the near-optimal social welfare in the problemsetting mentioned above, like the one proposed in Vondrak (2008), namely, Continuous Greedy. We haveconducted multiple experiments in order to compare the Continuous Greedy and Gradient Ascent algorithmas shown in . As can be seen from the results, the average performance of both methods (Gradient Ascent and ContinuousGreedy) is very similar, leading us to conclude that there may be a shared intuition underlying both algorithms.The intuition behind the continuous greedy algorithm is that the current distribution will shift towards themost increasing direction to find the optimal distribution for sampling. This concept is reminiscent of movingin the direction of the gradient in concave functions to maximize them. Overall, it appears that the GradientAscent algorithm closely resembles the continuous dual of the continuous greedy algorithm. Exploring thetheoretical connection between Gradient Ascent and Continuous Greedy can be an interesting direction forfuture work.",
  "However, in the following, we would like to mention that the continuous greedy algorithm has a majorlimitation": "As it can be seen in the , the running time of the Continuous Greedy algorithm is much larger thanthe Gradient Ascent in practice. It seems the reason behind this large difference is that the CG algorithmrequires to sample from the function in order of O((mn)5) times to estimate the expectations in the algorithm.It makes it so computationally complex in practice. However, since the size of the neural network for theproposed method in practice is much smaller than the theoretical requirements, using GA is much faster thanthe CG algorithm, as it can be seen in the .",
  "better than 1 1": "e for large k. Note that the proof of the approximation factor for maximizing DSFs usinggradient ascent (including pipage rounding as the last step) is exactly applicable to EDSFs. The intuition isthat since for each input x n, only one of the DSFs is active, then for functions that can be representedby an EDSF using the minimum over a finite number of DSFs, we can achieve a better approximation factorthan 1 1",
  "e. Note that this is only interesting for EDSFs that we can represent using minimum over polynomialnumber of DSFs": "Remark 5.4. As shown in Appendix A, 5-7, the loss function for EDSF exhibits significant fluctuations andspikes when learning coverage functions. We hypothesized that this was due to the MiLU activation function,which has sharp edges that could complicate backpropagation and hinder the learning process. To test thisconjecture, we conducted additional experiments with EDSF using different, smoother activation functions,such as log(1 + x). As seen in Figures 16-18, these loss functions became much smoother, with no visiblefluctuations. However, these smoother activation functions did not generalize as effectively as MiLU and wereunable to learn the coverage function as well. Remark 5.5. Finally, we conclude our discussion by analyzing why DSFs produce constant outputs aftertraining, particularly when learning coverage functions, as shown in Figures 9-10. To test the expressive powerof DSFs, especially for coverage functions, we ran additional experiments with much larger DSFs. The results,presented in Figures 11-12, indicate that DSFs still output constant values. We hypothesize that this issuearises when neurons in a layer reach the saturation point of their activation functiona limitation observedacross common concave activation functions. Beyond this saturation point, DSFs produce a constant output.Using larger DSFs exacerbates this issue, while smaller DSFs, as seen in , can sometimes avoid it.",
  "Conclusions": "In this research, we introduce a novel concept called Extended Deep Submodular Functions (EDSFs), buildingupon the foundation of Deep Submodular Functions (DSFs). DSFs, a subset of monotone submodularfunctions, provide a structured framework for representing specific types (a subset) of submodular functions.However, the scope of DSFs is limited to a strict subset of monotone submodular functions. EDSFs, onthe other hand, serve as a natural extension, expanding the family of DSFs to encompass all monotoneset/submodular functions. Our proofs are rooted in the properties of polymatroids, offering insights intothe relationship between polymatroids and submodular functions. Additionally, we highlight the concavenature of EDSFs, a characteristic that is proved to be valuable in addressing and efficiently solving variouscombinatorial optimization problems. To validate the efficacy of EDSFs, we conducted experiments in threedistinct settings, namely, learning coverage functions, learning modified cut functions, and maximizing social welfare. The results consistently demonstrated the superior performance of EDSFs compared to DSFs inthese experimental scenarios. In conclusion, our findings suggest that EDSFs provide a more comprehensiveand effective solution for modeling monotone set/submodular functions using neural networks. The extendedscope and enhanced performance make EDSFs a promising direction for further exploration in various machinelearning and optimization domains.",
  "Broader Impact": "This paper presents work whose goal is to advance the field of Machine Learning. There arent many straightpotential societal consequences of our work, none which we feel must be specifically highlighted here. Ashwinkumar Badanidiyuru and Jan Vondrk. Fast algorithms for maximizing submodular functions. InProceedings of the Twenty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 14971514.Society for Industrial and Applied Mathematics, January 2014. ISBN 978-1-61197-338-9 978-1-61197-340-2. doi: 10.1137/1.9781611973402.110. URL Wenruo Bai, William Stafford Noble, and Jeff A Bilmes. Submodular Maximization via Gradient Ascent:The Case of Deep Submodular Functions. In Advances in Neural Information Processing Systems, vol-ume 31. Curran Associates, Inc., 2018. URL Maria-Florina Balcan and Nicholas J. A. Harvey. Submodular Functions: Learnability, Structure, andOptimization. SIAM Journal on Computing, 47(3):703754, January 2018. ISSN 0097-5397, 1095-7111.doi: 10.1137/120888909. URL",
  "Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. Near-optimal map inference for determinantal pointprocesses. Advances in Neural Information Processing Systems, 25, 2012": "Michel X. Goemans, Nicholas J. A. Harvey, Satoru Iwata, and Vahab Mirrokni. Approximating SubmodularFunctions Everywhere. In Proceedings of the Twentieth Annual ACM-SIAM Symposium on DiscreteAlgorithms, pp. 535544. Society for Industrial and Applied Mathematics, January 2009. ISBN 978-0-89871-680-1 978-1-61197-306-8. doi: 10.1137/1.9781611973068.59. URL David Kempe, Jon Kleinberg, and va Tardos. Maximizing the spread of influence through a social network.In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and datamining, pp. 137146, 2003.",
  "Mukund Narasimhan, Nebojsa Jojic, and Jeff A Bilmes. Q-clustering. Advances in Neural InformationProcessing Systems, 18, 2005": "Shinsaku Sakaue. Differentiable greedy algorithm for monotone submodular maximization: Guarantees,gradient estimators, and applications. In International Conference on Artificial Intelligence and Statistics,pp. 2836. PMLR, 2021. Jan Vondrak. Optimal approximation for the submodular welfare problem in the value oracle model. InProceedings of the fortieth annual ACM symposium on Theory of computing, pp. 6774, Victoria BritishColumbia Canada, May 2008. ACM. ISBN 978-1-60558-047-0. doi: 10.1145/1374376.1374389. URL Edward Wagstaff, Fabian Fuchs, Martin Engelcke, Ingmar Posner, and Michael A Osborne. On the limitationsof representing functions on sets. In International Conference on Machine Learning, pp. 64876494. PMLR,2019.",
  "Jakob Weissteiner, Jakob Heiss, Julien Siems, and Sven Seuken. Monotone-value neural networks: Exploitingpreference monotonicity in combinatorial assignment. arXiv preprint arXiv:2109.15117, 2021": "Bryan Wilder, Bistra Dilkina, and Milind Tambe. Melding the data-decisions pipeline: Decision-focusedlearning for combinatorial optimization. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 33, pp. 16581665, 2019. Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexan-der J Smola. Deep sets. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur-ran Associates, Inc., 2017. URL",
  "ADetails of Experiments on Learning Coverage Functions": "To set the hyperparameterssuch as the number of layers, neurons, and the value of alphawe performedan exhaustive search across a defined parameter space. The search space included varying the number oflayers from 2 to 6, the number of neurons per layer from 32 to 2048 (in powers of 2), and testing differentvalues of alpha, starting from small values around 5 up to larger values in the order of 1000. While manydifferent hyperparameter configurations worked well for the EDSF architecture, none of the candidates yieldedpromising results for DSFs in terms of output or loss. Additionally, for all weight initialization in our DSF",
  "and EDSF networks across all experiments in the paper, we used a Gaussian distribution with a mean of 0and a variance of 0.1": "For our first experiment (see ), the architecture for the Deep Set model consists of three layers with 64neurons each for and three layers with 64 neurons each for . The architecture used for the Set Transformermodel includes 4 attention heads and a hidden dimension of 128. To demonstrate the loss function and outputs of EDSFs and DSFs in learning coverage functions, we testedthree different coverage functions with probabilities of 0.1, 0.3, and 0.5. All three coverage functions had auniverse size of 100 and 16 items. The DSF and EDSF used for these experiments had the same architectureas described in . For each experiment, our network was trained for 10,000 epochs using the Adamoptimizer with a learning rate of 0.01. The employed cost function, consistent with all other experiments,was the L1-loss function. As shown in Figures 5-10, the DSF fails to learn the patterns of the target coveragefunctions and outputs a constant value. In contrast, the EDSF can closely follow the patterns of the targetfunctions. To demonstrate the expressive power of DSFs, we employed larger architectures for DSFs in learning thecoverage function. The DSFs used in these experiments had 4 and 6 layers, each with 2048 neurons, andutilized the MiLU activation function with = 95. The target function was a coverage function with auniverse size of 100, 16 items, and a probability of 0.5. Consistent with all other experiments, the networkwas trained for 10,000 epochs using the Adam optimizer with a learning rate of 0.01 and the L1-loss functionas the cost function. As shown in Figures 11 and 12, the DSF still failed to learn the target coverage function,producing a constant output instead. Additional experiments using various concave activation functions forDSFs were also conducted to learn the aforementioned coverage function, with results presented in Figures13-15. The learning setup for these experiments was consistent with the other experiments. Moreover, theDSF used for these experiments had 3 layers with 64 neurons each. Finally, to test our conjecture described in Remark 5.4 regarding the loss function for EDSFs when learningcoverage functions with many spikes and fluctuations, we experimented with smoother concave activationfunctions. As shown in the corresponding plots in Figures 16-18, the loss functions exhibit no visible spikesor fluctuations. For these experiments, the target coverage function had a universe size of 100, 31 items,and probability of 0.2. Moreover, The EDSF used for these experiments had 4 layers with 64 neurons each,similar to , and learning setup was consistent with other experiments.",
  "BDetails of Experiments on Learning Cut Functions": "For setting hyperparameters in learning the cut function, similar to learning the coverage function, weconducted an exhaustive search and tried different values for the number of layers, neurons, and values ofalpha. The search space for these parameters was the same as that used in learning coverage functions,as mentioned in Appendix A. Many different candidates for the DSF encountered the same problem ofoutputting a constant value; however, a few of them could approximately identify the pattern of the targetcut function. It is worth mentioning that they could not learn the cut function as well as the EDSF, andmost of the hyperparameters worked well for the EDSF. For our first experiment in .2 (see ), the architecture for the Deep Set model consists of threelayers with 64 neurons each for and three layers with 64 neurons each for . The architecture used for theSet Transformer model includes 4 attention heads and a hidden dimension of 128. We can observe the loss function and the true vs. predicted values for the train and test samples for theEDSF and DSF in Figures 19 and 20, respectively. The setup for these experiments is the same as the setupdescribed in .2, .",
  "This proof provided kindly by one of the reviewers during the review process": "the paper, however, without using the analysis of polymatroids there is no intuition at hand about how weconstructed the architecture of the EDSFs to respresent all of the monotone submodular functions, whichmakes is possible to generalize the architecture to all monotone set functions as a more powerful result. iteration number 0.86 3.86 6.86 9.86 12.86 15.86 18.86 21.86 24.86 27.86 30.86 33.86 36.86 loss",
  "Test Samples": "Value of Test Samples True vs Predicted Value predicttruth : Learning coverage function with probability 0.5, universe size 100, and 16 items. We have usedDSF and shown Training loss, Truth vs. Predicted values for train and test samples. Other settings are sameas .1 . : Learning coverage function with DSF having more number of neurons. The used DSF has 4 layerseach having 2048 neurons and MiLU activation function with = 95. we can see that it still outputs constantwhen learning coverage function with universe size of 100 and probability of 0.5. : Learning coverage function with DSF having more number of neurons. The used DSF has 6 layerseach having 2048 neurons and MiLU activation function with = 95. we can see that it still outputs constantwhen learning coverage function with universe size of 100 and probability of 0.5. : Learning coverage function with DSF having log(1 + x) as activation function. The used DSFhas 3 layers each having 64 neurons and log(1 + x) as activation function. We can see that it still outputsconstant when learning coverage function with universe size of 100 and probability of 0.5. : Learning coverage function with DSF having tanh(x) as activation function. The used DSF has 3layers each having 64 neurons and tanh(x) as activation function. We can see that it still outputs constantwhen learning coverage function with universe size of 100 and probability of 0.5. : Learning coverage function with DSF having (x) 0.5 as activation function. The used DSFhas 3 layers each having 64 neurons and (x) 0.5 as activation function. We can see that it still outputsconstant when learning coverage function with universe size of 100 and probability of 0.5. iteration number 80.503 81.503 82.503 83.503 84.503 loss"
}