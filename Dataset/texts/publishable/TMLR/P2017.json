{
  "Abstract": "Federated Learning (FL) is a distributed machine learning approach to learn models ondecentralized heterogeneous data, without the need for clients to share their data. Manyexisting FL approaches assume that all clients have equal importance and construct a globalobjective based on all clients. We consider a version of FL we call Prioritized FL, wherethe goal is to learn a weighted mean objective of a subset of clients, designated as priorityclients. An important question arises: How do we choose well-aligned non-priority clients toparticipate in the federation, while discarding misaligned clients? We present FedALIGN(Federated Adaptive Learning with Inclusion of Global Needs) to address this challenge.The algorithm employs a matching strategy that chooses non-priority clients based on howsimilar the models loss is on their data compared to the global data, thereby ensuringthe use of non-priority client gradients only when it is benecial for priority clients. Thisapproach ensures mutual benets as non-priority clients are motivated to join when themodel performs satisfactorily on their data, and priority clients can utilize their updatesand computational resources when their goals align. We present a convergence analysis thatquanties the trade-o between client selection and speed of convergence. Our algorithmshows faster convergence and higher test accuracy than baselines for various synthetic andbenchmark datasets.",
  "Introduction": "Federated Learning (FL) (McMahan et al., 2017; Kairouz et al., 2021) is a distributed learning setting wherea set of clients, with highly heterogeneous data, limited computation, and communication capabilities, areconnected to a central server. For N clients, the goal of FL is for the central server to learn a model thatminimizes",
  "k=1 pkFk(w),(1)": "where w Rm is the model parameter. The objective function Fk(w) is the local objective function of thekth client, and pk is the fraction of data at the kth client. A key constraint of FL is that the server wishes tominimize F(w) without directly observing the data at each of the N clients. A popular method to solve (1) in the FL setting is Federated Averaging (FedAvg) (Brendan McMahan et al.,2017). In FedAvg, E iterations of Stochastic Gradient Descent (SGD) are performed locally at the clientsbefore each communication round. After the tth local iteration, if t corresponds to a communication round(i.e., t (mod E) = 0), a participating client k sends the updated model parameter wk",
  "E[F(wT )] F (C + )/T,(3)": "where C is a constant. Intuitively, a more severe misalignment between the local and global objectives leadsto a larger and to worse training performance. In particular, this suggests that FL systems that operatewith a set of clients with reasonably aligned objectives may perform better. Prioritizing Clients in the Model. The idea of selecting a subset of clients to include in each communicationround in order to tackle heterogeneity is not new (Fu et al., 2022; Karimireddy et al., 2021; Li et al., 2018).In addition to reducing heterogeneity, client selection strategies have been proposed in the FL literatureto accelerate convergence rates (Cho et al., 2020; Chen et al., 2020), promote fairness (Sultana et al.,2022), and bolster robustness (Nguyen et al., 2021). These methods emphasize specic clients during theaggregation process based on the task at hand, yet they cannot account for real-world applications withintrinsic prioritization. Specically, in various real-world applications, some clients are inherently prioritized, compared to others. Insubscription-based services, for instance, it is often the case that some clients have a paid subscription, whileothers use a free version of the service. One example is online streaming services, which may provide perkssuch as ad-free content only to paid subscribers. In such cases, the central server may want to optimize amodel (e.g., a recommendation system) that prioritizes the paid subscribers, while still taking advantage ofthe data provided by all clients. Another example are internet service providers, which have a set of payingclients, but may also provide free-access Wi-Fi hotspots. In such cases, the free service may be provided inexchange for some input from the clients, such as data or computational resources. These clients are likely tohave highly heterogeneous data, and may not be willing to directly share their data, which ts naturally intoa FL setting. In these cases, the global objective function F(w) should only take into consideration the priority clients, andit is important to understand whether the non-priority clients can be leveraged to improve the prioritizedglobal objective. Is it possible to incentivize well-aligned non-priority clients to participate and contributeto model improvement, while discarding the updates from misaligned clients? To formalize this question,we propose the Prioritized Federated Learning (PFL) setting. We discuss additional motivating scenarios forPFL in",
  "kP pkFk(w). The remaining clients are designated as": "non-priority clients. Non-priority clients, if chosen wisely and incorporated into the learning process, canpotentially accelerate the models convergence through additional gradient steps. To facilitate this, we proposeto devise an algorithm capable of discerning well-aligned non-priority clients and aggregating their updatesalong with those from the priority clients. The critical challenge here lies in nding the right balance: includingmore non-priority clients might expedite the convergence but could also introduce bias into the global model,especially if these clients are not well-aligned with the primary objective. Conversely, implementing morestringent selection criteria ensures only the most well-aligned clients contribute, but at the expense of fasterconvergence from additional updates. Against this backdrop, our paper will address two questions that arisein this setting: (1) What criterion should we use to eectively select well-aligned non-priority clients withoutcompromising the integrity of our learning objective? (2) Can we establish a theoretical framework thatquanties the trade-o between accelerated convergence and potential bias introduced by including updatesfrom non-priority clients? FedALIGN and Our Contributions. To address the questions proposed for this PFL setting, we proposeFedALIGN (Federated Adaptive Learning with Inclusion of Global Needs). FedALIGNs strategy for non-priority client inclusion can be combined with a variety of existing algorithms. In every communication",
  "Published in Transactions on Machine Learning Research (09/2024)": "Notably, gradient comparisons are more eective than loss comparisons since gradient alignment is a betterindicator of model alignment at a given time (Zeng et al., 2021). Moreover, absolute loss based methods likethose proposed by Cho et al. (2020) oer an advantage in terms of eciency, as they eliminate the bottleneckcaused by repeatedly determining the client with the lowest loss. Sorting clients based on absolute loss values,rather than comparison with relative loss values like FedALIGN, is computationally more ecient. It is thusimportant to acknowledge that other methods have their own advantages and may be more useful in dierentcontexts. For example, absolute loss-based methods Cho et al. (2020); Chen et al. (2020); Ghadikolaei et al.(2019) can be particularly eective in scenarios where rapid convergence is critical, such as in time-sensitiveapplications where communication is not the bottleneck, but computation is. Thus, while our approach oerssignicant benets in terms of communication eciency (since we do not require server level decisions) in thePFL setting, specic methods may prove more advantageous in other scenarios such computational eciency.",
  "T+ T .(4)": "The factor T captures the reduction in heterogeneity achieved by FedALIGN. The more non-priorityclients are included in all communication rounds till T, the lower the value of T . This is balanced with abias term T , which captures the misalignment in the selected non-priority clients. The terms T and T exhibit an inverse relationship and depend on the desired proximity between global andlocal losses. Specically when T = 1 and T = 0 (we show in that this choice achieved by ignoringall non-priority clients), we recover (3), the convergence rate for FedAvg. The tradeo between T and Tenables us to ne-tune these terms, favoring a rapid convergence initially, and gradually adjusting the bias asneeded. Indeed experiments implementing FedALIGN shows that exploiting this trade-o leads to fasterconvergence and better performance over baseline algorithms, both on synthetic data and many benchmarkdatasets (FMNIST, EMNIST and CIFAR10). Prioritization and Fairness in AI. In our exploration of PFL, it is crucial to talk about fairness. At theheart of this discussion is the fact that PFL is developed to model real-world scenarios such as subscription-based services where a large fraction of the population (non-subscribers) are inherently excluded from thefederation, since the goal of the server is to optimize the model for the priority clients only. In this context,FedALIGN can be seen as a way to attain higher fairness in the PFL setting. To explain this note if one wereto utilize FedAvg in PFL, there would be two possible options: (i) using FedAvg only on the priority clientsand none of the non-priority clients (say FedAvg-None) or (ii) using FedAvg on all priority and non-priorityclients (say FedAvg-All). A natural denition of fairness in this case is the fraction of the overall population of clients that has accessto a model that is useful to them. This denition of fairness used is inspired by many works Zafar et al.(2017); Mohri et al. (2019); Li et al. (2020b), but is adapted to be more relevant to the PFL setting. Underthis denition, (i) would lead to little fairness (but better model performance for priority clients), and (ii)would lead to higher fairness, but worse model performance, again for priority clients. In real-world settings(ii) will not be accepted by priority clients, since they do not stand to benet by including non-priority clients.Therefore FedALIGN attains higher fairness in the PFL setting (although not as high as a full FedAvg-All,which is not realistic). The priority clients stand to benet from this (as they will ultimately receive a bettermodel) and will agree to the model being shared with non-priority clients, and non-priority clients stand tobenet (at least some fraction of them) from gaining access to a well-trained model, even though they are notsubscribers/priority-clients. Thus FedALIGN naturally leads to the most realistically fair algorithm for thesubscription based setting we model.",
  "of local updates. In each communication round, each client that participates in that round, independentlyruns E local epochs. Aggregation happens when t (mod E) = 0": "Other Settings with Client Prioritization. The need to prioritize a subset of clients may arise in contextsother than subscription services discussed in . For instance, existing FL settings that focus onoptimizing objectives dened by their clients may face challenges when integrating additional clients. Whilethese new clients may be valuable to the FL setting, their inclusion often necessitates a shift in the learningobjective. FedALIGN provides a partial solution to this predicament, enabling the inclusion of well-alignedclients without necessitating alterations to the learning objective. Another context that naturally leads to prioritization of some clients, is when some clients are slow, unreliable,or resource-poor. This leads to the straggler eect, which can signicantly hamper the overall learningprocess (Wang et al., 2020a; Hard et al., 2019; Nishio and Yonetani, 2019). The straggler eect occurs becausethe global model update must wait until slower clients complete their local computations, and the overallupdate time is when the overall computation time is dictated by the slowest participants. In many scenariosit is usually known which clients are likely to be slow. An example is households with multiple smart devices,where models are trained with a few powerful devices like smart phones and laptops, with additional lesspowerful devices like IOT devices (with potentially dierent data) also available for computation. In this caseit may be fruitful to exclude stragglers from the global objective and only include their updates if they arewell-aligned to this objective. Moreover in our supplementary material we extend our convergence analysis tothe case where non-priority clients are free to participate in aggregation as they want to, naturally modellingstragglers who may only be able to provide updates in a few rounds. Selsh Federated Learning. To the best of our knowledge, only one work (Anonymous, 2022) has exploreda model similar to ours, which they call Selsh Federated Learning. In the Selsh paradigm, it is assumedthat non-priority clients always contribute to the federation irrespective of what global model they receive, anassumption that may not hold true in practice, since clients need to be incentivized to participate. Secondly,the previous model assumes that the number of priority clients are much smaller than the total clients. Thisassumption can be overly restrictive, limiting the models applicability in varied scenarios. Furthermore,their algorithm exhibits convergence only under highly stringent learning rates, thereby further conningits practical utility. Furthermore, regarding client participation, the model requires involvement from allnon-priority clients in each communication round. This expectation is impractical and signicantly impactscommunication eciency. Lastly, it presumes identical data distribution among priority clients, whichessentially aligns it with the framework of Personalized FL, rendering the premise of their approach somewhattrivial. Our work, in contrast, imposes no such assumptions, but instead looks at a more pragmatic scenarioas outlined in the introduction. While an other study (Chayti et al., 2021) has investigated scenarios whereclients not involved in the global objective can still contribute to its improvement, these works necessitate fullparticipation. These approaches are incompatible with federated learning contexts. Our model and algorithmnaturally addresses these challenges. Personalized FL. Various FL strategies have been explored to create personalized models. One commonapproach involves adapting a global FL model to each local clients specic requirements (Wang et al., 2020b;Yang et al., 2021; Chai et al., 2020; Smith et al., 2018), while another leverages inter-client relationships tone-tune personalized models, often achieved by introducing adjustable regularization terms into each clientslocal objective (Kairouz et al., 2019; Karimireddy et al., 2021; Cheng et al., 2021). In these scenarios, auniversally applicable model is usually maintained, which is subsequently rened to suit individual clients",
  "FedALIGN (Our Method)737": "While client selection as a concept is similar to our problem setting, the methods used for client selection inclassical settings often use updates from all the clients while making decisions, which is unrealistic in thePFL setting. In FL, there are two kinds of communication: downstream from server to clients and upstreamfrom clients to server Sattler et al. (2020). In FedALIGN, the server broadcasts downstream to all clients,while only aligned non-priority clients communicate upstream. This is dierent from current methods, whichtypically require full communication from all clients. This makes FedALIGN communication ecient in thePFL setting. For example, Wang et al. (2020a) presents a loss-based selection algorithm by picking clientswith the highest loss. This however requires you to make server level decisions on which client has the highestloss, implying that the non-priority clients would all be participating in every communication round, whichis unrealistic in the PFL setting, and FedALIGN is specically designed to not require all non-non-priorityclients to participate. Similarly, there are other works involving gradient-based selection criteria (Fu et al.,2023), which also require such server-level decisions.",
  "FedALIGN and Convergence": "In this section we describe FedALIGN and prove its convergence guarantee. Our results are presented for thefull client participation setting, though the analysis can be easily extended to the case where a subset ofclients (both priority and non-priority) are picked uniformly at random. Further we can extend the result fora more general case, where non-priority clients follow an arbitrary participation rule or are given the choiceto participate in any given round. This is relevant in the stragglers setting, for example. These cases areconsidered in the supplementary material.",
  "FedALIGN": "FedALIGN introduces a simple decision rule to select clients based on the relative discrepancy betweenlocal and global loss. Non-priority clients are conditionally included in the aggregation process during acommunication round corresponding to the tth local round, if for the provided global model wt, and a chosenthreshold t , the absolute value of the dierence between a non-priority clients local loss and the global lossis below t; i.e., |F(wt) Fk(wt)| t. This method ensures that updates contributing to the global modeldo not diverge signicantly from the overarching learning objective, which is determined by the priorityclients. By strategically aligning the updates from non-priority clients with the global learning objective,FedALIGN can eectively harness the collective learning power of the entire network, while maintaining focuson the primary learning task. The exact steps taken are provided in Algorithm 1. In practice, during a communication round, along with the model parameter wt, the server sends the value ofaccuracy on the global data. The non-priority clients send back updates, only if the accuracy (accuracy is usedas a proxy for loss) of the received model on its local data is high enough. This is done to ensure that clientsonly communicate when they are well-aligned. The server then decides to aggregate it to the global model ifit is well aligned, based on how close the accuracies are; i.e., if the full condition |F(wt) Fk(wt)| t issatised.",
  "Convergence Analysis": "Before stating the main theoretical result on the convergence of FedALIGN, we present key assumptionsand notation. We make the following assumptions on local objective functions F1, . . . , Fn. Assumptions 1and 2 are standard in optimization, while Assumptions 3 and 4 are standard in distributed learning and FLliterature (Li et al., 2020a; Cho et al., 2020; Zhang et al., 2013; Stich, 2019).",
  "| < t} is an indicator random variable for whether a non-priority client is": "included in a given communication round. The alignment of non-priority clients, T , eectively measures theaverage inclusion of non-priority clients over T 1 time steps. Lower values of T correspond to a greaternumber of clients being included in aggregation. Specically the convergence rate improves to O(T /T) ineach time step. This kind of improvement due to client selection strategies has been considered before (Choet al., 2020).",
  "This term is an average aggregation of the bias introduced in the rst T 1 time steps": "The parameters T and T eect the convergence of the learning model under consideration, in oppositeways. The choice of t at each time step plays a crucial role in controlling this balance. If t is set too low, itrisks under-utilizing the potential of well-aligned clients, thereby causing a slower rate of convergence. Onthe other hand, if t is set too high, it introduces a high bias in the aggregated model updates, which cannegatively impact the models performance. Optimality of FedALIGN. The above dichotomy highlights the ne-tune (Cheng et al., 2021) aspect of theFedALIGN algorithm. We can thus set t such that it is gradually reduced in the later rounds. This strategyensures that the algorithm initially benets from the contributions of a larger number of clients (albeitpotentially introducing some bias), but as the rounds progress and the model is closer to convergence, the biascan gradually be eliminated by reducing t. This enables the algorithm to leverage the power of well-aligned",
  "T E[F(wT )] = F .(9)": "Corollary 2 is a straightforward consequence of Theorem 1 and is proved in the supplementary material.Further, in the supplementary material, we conduct experiments that set T to be distinct monotonicallydecreasing functions of T. We use our benchmark datasets to evaluate the eect of dierent decay rates of Ton convergence. Consistency of our result with FedAvg on priority clients. If we set t = 0 for all time steps inFedALIGN, T (the alignment of non-priority clients) becomes 1, implying that all priority clients are includedin the aggregation. At the same time, T (the tunable bias) becomes 0, indicating that no bias is introducedin the model updates. Under these conditions, FedALIGN is identical to FedAvg on just the priority clients,and the convergence bound is precisely the result in Li et al. (2020a). It shows how FedALIGN buildsupon FedAvg by introducing additional exibility and control through the t parameter, which allows us tomanage the trade-o between bias and speed of convergence. We also highlight that we can modify otheralgorithms like FedPROX (Kairouz et al., 2019) to obtain a similar control. We show experimental results inour supplementary material to validate this.",
  "Experiments": "We conduct experiments on various benchmark datasets: FMNIST, balanced EMNIST and CIFAR-10 (Xiaoet al., 2017). The datasets were preprocessed following the methods developed in Brendan McMahan et al.(2017). For FMNIST we use a logistic regression model, for EMNIST we use a two-layer neural networkand for the CIFAR-10 dataset we use a CNN. The dataset is generated by distributing uni-class shards toeach client, following the steps in Brendan McMahan et al. (2017). More details about the architectures aredeferred to the supplementary material. In addition, in order to assess the eect of noisy and irrelevant datain the non-priority clients, we modify the Synth(1, 1) dataset considered in Li et al. (2018). We use a logisticregression model for this experiment. We evaluate FedALIGN (built on FedAvg) against two baselines: (1) FedAvg on only the priority clients and(2) FedAvg on all clients. Further in our supplementary material we consider FedALIGN built on FedPROX(Li et al., 2018). We consider the full participation case, where all clients are sent a global model by theserver and consider the case of partial participation (where only a randomly sampled subset of clients arechosen in each round) in the supplementary material. Experiment results on benchmark data. For the learning rate (), we adopted a value of 0.1 for bothFMNIST and EMNIST datasets, whereas for CIFAR-10, we utilized a lower learning rate of 0.01. The selectionof these learning rates was made after conducting a grid search of dierent learning rates on small sets of thedata. The selected learning rates resulted in stable convergence and exhibited the best performance. Allexperiments were conducted with 5 dierent seeds for randomness, setting the local epoch E = 5. We expandon the impact of varying epoch choices in the supplementary material. The initial 10% of communicationrounds were dedicated as warm-up rounds during which only priority clients contributed to the aggregationprocess. This design was to ensure the development of a reasonably working model prior to the inclusionof non-priority clients. We found by optimizing over a grid of dierent s that the selection = 0.2 andeventually reducing it to 0, performs the best. The results depicted in show the superiority ofFedALIGN over the baseline methods. This is evident in the post-warm-up phase, where there is a noticeableacceleration in convergence rates. Interestingly, it was observed that the inclusion of all clients often ledto a degradation in overall performance, further arming the importance of selecting well-aligned clients.We further explore alternative congurations of priority client selections with varying data fractions in thesupplementary material.",
  ":Test accuracy for benchmark datasets, under full participation, with 2 heterogeneous priority": "clients and N = 60 clients, with E = 5. After the rst 20 warm-up rounds, a clear increase in convergencerate is observed for FedALIGN (green), while a deterioration is observed for All (orange) (FedAvg with allclients). FedALIGN also achieves a higher nal accuracy. Experimental results on synthetic data. We simulate scenarios to analyze the performance of non-priorityclients with varying degrees of alignment. For this purpose, we utilize the Synth(1, 1) dataset as referredto in Li et al. (2018). We create a global dataset and distibute this data to non-priority clients and addtwo forms of noise: (1) Discrepancies in label assignments for data points, and (2) the addition of irrelevantdata points, which are generated by independent distributions. We congure the noise to ensure that thenon-priority clients experience varying degrees of noise, thereby inuencing their alignment with the objective.The detailed methodology for the generation of this noise is provided in the supplementary materials. Weapply a logistic regression model to this data, maintaining a learning rate of 0.1, with local models usedto nd the best and stable learning rates, as in the prior experiments. We consider 3 dierent averagenoise/discrepancy levels across non-priority clients: medium, low and high. We found that = 0.2 is the bestchoice for the selection parameter for low and medium noise, while = 0.4 is the best choice for the high noiseexperiment. From , it becomes evident that FedALIGN consistently outperforms baselines, underthis more complicated form of misalignment in the non-priotity clients. This underscores the robustness ofFedALIGN even in challenging conditions.",
  "Concluding Remarks": "We proposed Prioritized Federated Learning, a FL setting that models the real-world situation where someclients are given priority from the point of view of model training, but other clients may choose to participatein the federation as well. We introduced a simple algorithm (FedALIGN), that smartly selects non-priorityclients to help achieve the goals of priority clients. This creates a win-win situation where non-priorityclients that align well with the system obtain a strong global model that can enhance their local tasks. Theconvergence analysis of FedALIGN highlights an important trade-o: using more priority clients can speed upconvergence but introduces a bias. We show that this bias can be eliminated in later rounds. Our extensiveexperiments support these ndings. Generalized PFL. One can conceptualize traditional FL and PFL as two ends of a spectrum. On one end,traditional FL involves weighing the objectives generated by both priority and non-priority clients accordingto their respective data fractions. This ensures that each client, regardless of priority status, contributesproportionally to the overall model training. On the other end of the spectrum, PFL focuses exclusively onthe objectives generated by priority clients, eectively assigning a weight of zero to the non-priority clients.This approach prioritizes the specic needs and data characteristics of the priority clients, ensuring that themodel is tailored to their unique requirements. An intermediate approach would involve assigning varyingweights to the objectives generated by priority and non-priority clients, with a greater emphasis on thepriority clients. By adjusting these weights, we can control the inuence of each client group on the modeltraining process. This allows for a exible and nuanced approach, balancing the benets of penalization for",
  ":Training loss and test accuracy for the Synth(1,1) dataset, under full participation, with 10": "heterogeneous priority clients and N = 20 clients, with E = 5. The rst graph considers the case where thereis medium noise, the second considers the low noise case and the third considers the high noise case. Afterthe rst 20 warm-up rounds, a clear improvement is observed for FedALIGN (green) for all these cases.",
  "priority clients with the broader generalization capabilities of traditional FL. Exploring this spectrum andadjusting the weights accordingly presents an exciting direction for future research": "On the willingness of non-priority clients to participate. We argue that the receipt of a well-performingglobal model is one of many incentives for non-priority clients to contribute to the aggregation. In oursupplementary material, we experimentally show numerous instances where the globally trained modeloutperforms locally trained counterparts. During instances where the received model does not surpass theperformance of a locally trained model, primarily due to the latters ne-tuned adaptation to specic clientcharacteristics, we posit that globally trained models help rene local models. This concept of leveragingexternal data for local ne-tuning has been the subject of extensive research in Personalized FL (Tan et al.,2022). Furthermore, we assert that access to a global model that demonstrates satisfactory performance onlocal client data is invariably benecial. It contributes to the suite of tools available to the local client topotentially improve overall performance. A compelling future direction is thus to formalize these incentivesas utility functions.",
  "Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,": "Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. DOliveira, HubertEichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri Gascn, Badih Ghazi,Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson,Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konecn, Aleksandra Korolova,Farinaz Koushanfar, Sanmi Koyejo, Tancrde Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, RichardNock, Ayfer zgr, Rasmus Pagh, Hang Qi, Daniel Ramage, Ramesh Raskar, Mariana Raykova, DawnSong, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramr, PraneethVepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advancesand open problems in federated learning. Foundations and Trends in Machine Learning, 14(1-2):1210,June 2021. ISSN 1935-8237. doi: 10.1561/2200000083. Publisher Copyright: 2021 Georg Thieme Verlag.All rights reserved."
}