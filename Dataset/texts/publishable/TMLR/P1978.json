{
  "Abstract": "Object-centric scene decompositions are important representations for downstream tasks infields such as computer vision and robotics. The recently proposed Slot Attention module,already leveraged by several derivative works for image segmentation and object trackingin videos, is a deep learning component which performs unsupervised object-centric scenedecomposition on input images. It is based on an attention architecture, in which latentslot vectors, which hold compressed information on objects, attend to localized perceptualfeatures from the input image. In this paper, we demonstrate that design decisions onnormalizing the aggregated values in the attention architecture have considerable impact onthe capabilities of Slot Attention to generalize to a higher number of slots and objects as seenduring training. We propose and investigate alternatives to the original normalization schemewhich increase the generalization capabilities of Slot Attention to varying slot and objectcounts, resulting in performance gains on the task of unsupervised image segmentation. Thenewly proposed normalizations represent minimal and easy to implement modifications ofthe usual Slot Attention module, changing the value aggregation mechanism from a weightedmean operation to a scaled weighted sum operation.",
  "Introduction": "Object-wise scene decompositions are ubiquitous in computer vision, robotics, and related disciplines such asreinforcement learning, since the state and actions in the environment are naturally represented in relation toobjects. Over recent years, unsupervised learning of object-centric representations from unlabelled imagesand video has attracted significant interest in the machine learning community (Greff et al., 2017; Engelckeet al., 2020; Locatello et al., 2020). The Slot Attention architecture (Locatello et al., 2020) decomposes atwo-dimensional RGB input image object-wise using an attention mechanism which updates slots, holdinginformation about objects, in a recurrent manner. In (Locatello et al., 2020), it was used for set predictionand image segmentation tasks on relatively simple renderings of 2D or 3D scenes, such as CLEVR (Johnsonet al., 2017). Later in (Seitzer et al., 2023), Slot Attention has also been successfully applied to the task ofunsupervised segmentation of more realistic images (MOVi, (Greff et al., 2022)). Detecting and trackingobjects in videos using Slot Attention is described in (Kipf et al., 2022; Elsayed et al., 2022). Despite itsempirical success, a theoretical explanation of its inner workings and the inductive biases which lead to",
  "the emergence of object-wise decompositions in Slot Attention is still under active research (Chang et al.,2022b;a)": "In this paper we investigate design choices on the normalization of aggregated attention values in SlotAttention. We find that the normalization proposed in (Locatello et al., 2020) leads to suboptimal foregroundsegmentation performance during inference with higher number of objects or slots than used for trainingthe model. We also investigate two alternative normalization approaches, give theoretical insights on theirbehavior, and assess their performance in relation to the original Slot Attention baseline. We demonstratethat these different approaches for normalizing the aggregated values can have a significant impact on thegeneralization of Slot Attention to a varying number of slots and objects during inference.",
  "Slot Attention": "The Slot Attention module (Locatello et al., 2020) is given a set of N input tokens xn RDinput, n {1, ..., N}and iteratively refines a set of K slots k RDslot, k {1, ..., K}. In an object-wise scene decompositionscenario, slots correspond to latent variables holding information on objects, while the input tokens arelocalized image features, e.g., computed by a convolutional neural network. Slots bind to input tokens via adot-product attention mechanism (Luong et al., 2015). Learned linear maps k and q extract D-dimensionalkeys and queries from the layer-normalized (Ba et al., 2016) input tokens xn := LayerNorm(xn) and layer-normalized slots k := LayerNorm(k), respectively. In our case, we always have Dinput = D and the layernormalization modules that produce xn and k do not share parameters.",
  "uk :=ukNn=1 n,k(4)": "This normalization scheme is termed weighted mean. Locatello et al. (2020) discuss two ablations of thisnormalization. The weighted sum scheme normalizes the update code by multiplication with a constant, i.e.uk := 1 C uk. In the ablation study in (Locatello et al., 2020), the value chosen for C is not discussed, and itmust be assumed that C = 1 was chosen. The second ablation of (Locatello et al., 2020) is termed layernormalization and uses a layer normalization module that is shared across slots for normalization. Concretely,the normalized update code is computed as uk := LayerNorm(uk). We refer to Appendix B for an exactdefinition of layer normalization. For each slot k, the aggregated value uk is used to update the latent representation k via a gated recurrentunit (GRU) (Cho et al., 2014) and a residual multilayer perceptron with newk:= update(k, uk).",
  "Slot Attention and von Mises-Fisher Mixture Model Parameter Estimation": "Many works (Locatello et al., 2020; Chang et al., 2022b;a; Kirilenko et al., 2023) compare Slot Attention toexpectation maximization (Dempster et al., 1977; Bishop, 2006) (EM) in Gaussian mixture models, i.e. tosoft k-means clustering. We, however, connect it with expectation maximization in a mixture model of vonMises-Fisher (vMF) distributions (Banerjee et al., 2003), since Slot Attention uses a bilinear form on slotsand inputs as a scoring function instead of the negative Euclidean distance. In this section, we make theparallel between Slot Attention and EM explicit by performing EM parameter estimation in a vMF mixturemodel and relating each step to the corresponding step in Slot Attention. We will then view the weightedmean, layer norm, and weighted sum normalization variants in the context of this analogy and compare them.",
  "Relating Slot Attention to EM": "We consider a case in which N points xn are given on the unit (d 1)-sphere in Rd. We estimate the meandirections 1, ..., K of K vMF components, along with the mixture coefficients 1, ..., K. We assume thatthe vMF distributions have fixed concentration, i.e., = 1. We interpret the parameters k to relate toslots in Slot Attention and the points xn to relate to the perceptual input features of the module. Theconcentration parameter can be understood as an analogue to the temperature",
  "n,k :=k exp(xn k)Kk=1 k exp(xn k)(5)": "The resulting matrix NK corresponds to the attention matrix in Slot Attention. Equation (5)closely resembles the computation of the attention matrix in Slot Attention with some differences: While theinputs xn in Slot Attention do not necessarily lie on the unit sphere, we do remind the reader that they arelayer-normalized and therefore lie on ellipsoids. Similarly, the slots are layer-normalized before the attentionstep. In contrast to equation 5, Slot Attention uses key and query maps instead of directly forming a dotproduct between xn and k. I.e., the dot products are formed between kn and qk. While the Slot Attention architecture does not explicitly model the mixture parameters k, it may encodesome weighting in the layer-normalized slots. Indeed, we show in Appendix B that the keys kn are containedin some (D 1)-dimensional affine subspace A RD which may be written uniquely as A = a + V where Vis a (D 1)-dimensional linear space and a V is perpendicular to V . If pV : RD V is the orthogonalprojection onto V and pa : RD a is the orthogonal projection onto the span of a, we may decomposeany x RD orthogonally as x = pV (x) + pa(x). For any key vector kn = k(xn) A we therefore havekn = a + pV (kn). The attention value n,k in Slot Attention may now be writen as:",
  "Comparing Normalizations in EM Analogy": "In the following paragraphs, we discuss how the update normalizations discussed previously compare inthe context of our EM analogy and, in particular, whether the normalized update codes uk hold sufficientinformation to recover the quantities from equations (7) and (8). Weighted MeanIn the weighted mean case, the aggregated values uk can hold sufficient information toextract the quantities in (7) and (8) if D = Dinput holds. Assuming that the value map is the identity, theright-hand side of (7) may be computed as uk/uk2. However, it is not clear how k could be computedfrom uk. Indeed, we show in Proposition 1 that there can be no general formula as for the weighted sum casethat generalizes without exception across slot-counts. We provide a proof in Appendix C by constructingsome explicit slot settings which demonstrate that a hypothetical function f can not map every update codeto a corresponding unique scalar. Proposition 1. Consider Slot Attention with weighted mean normalization and any fixed model parametersand fixed input data x1, ..., xN with N 1. Then, there exists no function f : RD R such that it holds",
  "for arbitrary K 1, arbitrary slots 1, ..., K and resulting normalized update codes u1, ..., uk": "Layer NormalizationWhile, at least in some cases, it is possible to recover the quantity in (7) fromupdate codes in the layer-normalization variant, these update codes still do not contain sufficient informationto infer the quantity in (8). Indeed, the reader may verify that the same argument we presented in the proofof Proposition 1 also holds for the layer norm variant. Weighted SumIn the weighted sum case, we may, as for the weighted mean normalization, obtain theright-hand side of equation (7) via uk/uk2 if the value map is the identity. In contrast to the previouslydiscussed normalizations, we may also recover information on the column sums Nn=1 n,k, which appear inequation (8). We make this rigorous in Proposition 2 and provide a proof in Appendix D, where we exploitthe fact that the values vn lie in a lower-dimensional subspace. Proposition 2. Consider Slot Attention with weighted sum normalization and fixed model parameters. Letthe number of input tokens N be fixed. Assume that Dinput = D holds. For almost all (w.r.t. Lebesguemeasure) parameters of the inputs layernorm module and the value map v, there exists a map f : RD R(which may depend on these parameters) such that",
  "Published in Transactions on Machine Learning Research (09/2024)": "EvaluationDuring evaluation, we also utilize 3 Slot Attention iterations. Since the alpha masks that areproduced by our model are of shape 28 28, we cannot directly compare them to ground truth segmentations,which are of shape 224 224. We follow the approach of (Seitzer et al., 2023) and bi-linearly upscale thealpha masks to shape 224 224 before deriving segmentations, which we then compare to the ground-truth.",
  "Weighted Sum Normalization with Fixed Scaling": "As detailed in the above discussion, in contrast to the weighted mean normalization, the weighted sumnormalization may preserve information on the fraction k of input tokens assigned to the slot in the slotupdate code uk. While (Locatello et al., 2020) report worse performance of the weighted sum normalizationcompared to the weighted mean normalization, the value chosen for C is not further discussed. As detailedin our experiments, we observe that the weighted sum normalization can outperform the weighted meannormalization for C = N, where N is the number of input tokens. For image inputs, the number of tokensis relatively large, e.g. N = 1282 = 16, 384 for feature maps of CLEVR renderings. We aim to avoidunreasonably large values in the update code uk, which may lead to numerical instabilities, such as vanishinggradients. With C = N, it holds that uk is bounded with |uk,d| maxn |vn,d| d {1, ..., D}, which isindependent of N. Indeed, we have the following chain of inequalities:",
  "n=1n,k maxn|vn,d| maxn|vn,d|(11)": "Here, we first use the triangle inequality, followed by the crude estimate |vn,d| maxn |vn,d| for all n. Finally,we used the fact that Nn=1 n,k N holds, since is row-stochastic with N rows. While this provides anargument for our choice of C which is a suitable heuristics across tasks, we hypothesize that task-specifictuning of this hyperparameter may be beneficial.",
  "Weighted Sum Normalization with Batch Scaling": "Instead of heuristically choosing a scaling parameter C in the weighted sum normalization as above, we alsoinvestigate an approach in which the scaling factor is learned via a form of batch normalization (Ioffe &Szegedy, 2015) during training. Concretely, we measure the magnitude of unnormalized update vectors in thefirst Slot Attention iteration of each forward pass by computing their batch statistics. These batch statisticsare used during the subsequent iterations of the forward pass to scale the update codes. While other worksusing batch normalization in recurrent networks do not share statistics across time (Cooijmans et al., 2017;Laurent et al., 2016), we find that our approach greatly simplifies varying the number of iterations duringinference. In contrast to typical implementations of batch normalization, we propose to reduce all axes duringthe computation of the statistics (i.e., the batch axis, the slot axis, and the layer axis). Reducing the slotaxis is necessary to preserve slot-permutation equivariance, which is a desirable property in object-centriclearning (Locatello et al., 2020). Reducing the layer axis leads to scalar batch statistics, yielding a methodthat more closely aligns with the normalization approaches we have discussed so far.",
  "Experiments": "We investigate the proposed normalizations on unsupervised object discovery tasks. To this end, we trainautoencoders on the CLEVR (Johnson et al., 2017) and MOVi-C (Greff et al., 2022) datasets, utilizingautoencoder architectures that have been described in (Locatello et al., 2020) and (Seitzer et al., 2023),respectively. Additional results for a property prediction task can be found in Appendix I. We providevisualizations of scene segmentations in Appendix H. We will give a brief overview on our experimental setupin the following and refer to the supplementary material for more details1. Model VariantsWe refer to the standard normalization (weighted mean) as the baseline and to theLayerNorm-based ablation from (Locatello et al., 2020) as the layer normalization. We term the methoddetailed in Sec. 4.1 the weighted sum normalization, and the method from Sec. 4.2 the batch normalization.In some experiments, we will train models on filtered training sets (CLEVR6 and MOVi-C6) containingonly a limited number of objects. For clarity, we annotate each model variant with a tuple (O, K), where Odenotes the maximum number of objects seen in the training set and K denotes the number of slot latentsused during training. CLEVR DatasetWe use an extended version of the CLEVR dataset that is provided in the Multi-objectDatasets repository (Kabra et al., 2019). It consists of 100,000 2D renderings of 3D scenes depicting up to 10objects whose shapes are geometric primitives. Each scene is annotated with a ground truth segmentation,which we use for evaluation. Following Locatello et al. (2020), we use 70,000 images for training and furtheradopt the approach of (Locatello et al., 2020; Greff et al., 2019; Burgess et al., 2019) by cropping the imagesto highlight objects in the center. In contrast to (Locatello et al., 2020), we also augment the data duringtraining via random horizontal flips. As in (Locatello et al., 2020), we also consider a subset of the CLEVRdataset, only consisting of images containing at most 6 objects. We refer to this dataset as CLEVR6 and willdenote the original dataset by CLEVR10. MOVi-C DatasetCompared to CLEVR, MOVi-C represents a significant step-up in perceptual complexity.It contains 10,986 video sequences, each consisting of 24 frames. We use 250 of these video sequences forvalidation and hold out 999 sequences for testing. Each clip shows 3 to 10 highly textured 3D-scanned objectsfrom the Google Scanned Objects repository (Downs et al., 2022) flying into view and colliding. In ourexperiments, we only consider single RGB frames from the dataset and discard any temporal relation betweenthem. Once again, we introduce a filtered dataset, which we denote by MOVi-C6 and which consists of framesof clips that contain at most 6 objects. For sake of clarity, we refer to the original dataset as MOVi-C10. MOVi-D DatasetThe MOVi-D dataset consists of scenes that are visually similar to those from the MOVi-C dataset. However, the scenes contain up to 23 (10 to 20 static, 1 to 3 moving) objects, thereby presenting agreater challenge to object-centric method. Structurally, the dataset resembles MOVi-C, consisting of 11,000",
  "Object Discovery on CLEVR": "In this subsection, we investigate our proposed normalization approaches on an object discovery task on theCLEVR dataset. In a first set of experiments, we follow the exact training procedure detailed in (Locatelloet al., 2020) and illustrate how the different normalization methods behave as the number of slot latents K ischanged during inference. The effect of choosing large numbers of slot latents during training is studied in asecond set of experiments. Throughout these experiments, we additionally scrutinize the impact of the objectcount on model performance. Training With 7 SlotsIn this first set of experiments, we follow (Locatello et al., 2020) as closely aspossible and train the previously described CNN-based architecture on the CLEVR6 dataset with 7 slots. Wecompare the baseline and layer normalizations proposed in (Locatello et al., 2020) to the methods discussedin . For each variant, we perform 5 training runs with different seeds. The trained models areevaluated on the CLEVR6 and CLEVR10 test sets.",
  ": Qualitative results on a CLEVR10 images, showing reconstructions and (soft) segmentations. Themodels are trained on CLEVR6 with 7 slots and evaluated with 21 slots": "The baseline and layer norm variants lead to object-centric behavior for all 5 seeds. For the weighted sumnormalization and the batch norm variant, however, we encounter two runs each in which the autoencodersdecompose the input spatially instead of object-wise. Following (Locatello et al., 2020), we omit these runs inour analysis. In Subfigures 1a and 1b, we illustrate how the foreground segmentation performance changes as we varythe number of slots during evaluation. We note that in the baseline and layer norm variants, segmentationperformance deteriorates when they are presented with more than nine slots, while our proposed normalizationsappear to generalize well to these changes. In particular, we observe that both of our proposed normalizationsoutperform the baseline when the autoencoders are evaluated with 11 slots, as is done in (Locatello et al.,2020). We show qualitative results of the different model variants at a high slot count in and referto Appendix H for more visualizations.",
  ": Dependence of segmentation performance on slot and object count. Models are trained on CLEVR6with 11 slots": "We study how performance depends on the number of objects in Subfigure 1c. Here, we evaluate each modelvariant with 11 slot latents on the CLEVR10 test set and plot average foreground segmentation performancedependent on the object count. Overall, we observe that our proposed normalizations outperform the baseline,independently of the object count. Also note that, across all model variants, segmentation performance trendsdownwards as the number of objects in the scene increases. Subfigures 1d-1f illustrate the behavior of the overall segmentation performance when background pixels aretaken into consideration. It appears that our proposed methods outperform the other two variants w.r.t. thismetric, although the variablity across runs is large. Excess Slots During TrainingWhile we have so far only discussed experiments in which we increase thenumber of slot latents during inference, we will now outline an experiment in which the Slot Attention moduleis also provided with excess slots during training: Concretely, we train the autoencoders on the CLEVR6dataset with 11 slot latents. We annotate the resulting variants with the tuple (6, 11) to underline that theywere trained with 11 slots on scenes consisting of at most 6 objects. To limit computational expenses, weperform only three runs per model variant. We observe object-centric behavior in all runs for the baseline,the layer norm variant, and the weighted sum variant. For the batch normalization, we encounter one runin which the scenes are deconstructed spatially in vertical stripes. As before, we exclude this run from ouranalysis and are therefore left with only two runs for this variant. Considering the breadth of our otherexperiments and the small variability observed in this experiment, we deem this loss of information acceptable. In this setting, the studied variants seem to perform more comparably than before w.r.t.foregroundsegmentation performance (Subfigures 3a-3c). Notwithstanding, we again note that the performance of thebaseline and layer norm variants starts to suffer as we add additional slot latents during inference. In contrast,the performance of the two proposed variants remains more stable. In general, the foreground segmentationperformance is lower than during training with few slots (Subfigures 1a-1c). All models trained with ourproposed methods learn to segment the background into a single slot, leading to high overall segmentationperformance (Subfigures 3d-3f). One model using the baseline normalization exhibits this behavior, whilenone of the models using layer normalization do so.",
  "Object Discovery on MOVi-C": "To further support the validity of our proposed normalizations, we run additional experiments, using theDinosaur (Seitzer et al., 2023) framework. As previously discussed, we train the MLP-based architecturedescribed in (Seitzer et al., 2023) on the MOVi-C dataset. While it still is a synthetic dataset, this representsa significant step-up in complexity compared to CLEVR, approaching the complexity of real-world scenes. Training on MOVi-C10In our first set of experiments, we closely follow the setup detailed in (Seitzeret al., 2023) and train the autoencoders on the full MOVi-C10 dataset, using 11 slots. As in the previoussubsection, we investigate how the foreground segmentation performance develops as we vary the number ofslot latents during inference. For each model variant, we perform 5 training runs with different seeds. Forsake of consistency with the other experiments, we evaluate the trained models both on the MOVi-C10 testset and on the filtered MOVi-C6 dataset. In Subfigures 4a and 4b, we observe that our proposed normalizations generally lead to improved foregroundsegmentation performance compared to the baseline and layer normalization across all slot counts. Inparticular, both proposed normalizations outperform the baseline and layer normalization variant with 11slots on the MOVi-C10 dataset, as can be observed in Subfigures 4a and 4b. As in our previous experiments,we note that foreground segmentation performance starts to suffer as the baseline variant is provided withexcess slots during inference. While our proposed methods exhibit a similar behavior in these experiments,we note that the deterioration progresses at a slower rate. Additionally, we observe in Subfigure 4c thatperformance is improved across smaller object counts. The behavior of overall segmentation performance is shown in Subfigures 4d-4f. In line with our previousobservations, we note that performance suffers for all variants when excess slots are present during inference.While baseline, layer normalization and batch normalization yield comparable results w.r.t. this metric, theweighted sum variant performs noticably better. Training on MOVi-C6While the authors of (Seitzer et al., 2023) trained autoencoders exclusively onthe MOVi-C10 dataset, we will also investigate an approach that resembles the one described in (Locatelloet al., 2020), and which we adopted in Subsection 5.1. Namely, we train models on the filtered MOVi-C6dataset with 7 slots and subsequently evaluate them on both MOVi-C6 and MOVi-C10. This approach may",
  ": Dependence of segmentation performance on slot and object count. Models are trained on MOVi-C6with 7 slots": "be particularly interesting to practitioners, as reducing the number of slot latents during training can serveto greatly reduce computational effort. To limit computational expenses, we again only perform three runsper model variant. We illustrate in Subfigures 5a-5c the behavior of the foreground segmentation performance. As in our previousexperiments, we observe that both of our proposed normalizations outperform the baseline when evaluatedon the MOVi-C10 test set with 11 slots. Interestingly, it can be noted that performance also improves overthe models trained on the MOVi-C10 dataset with 11 slots (). Again, we point out that excess slotlatents during inference lead to a substantial deterioration of foreground segmentation performance in thebaseline and layer norm models. In Subfigures 5d-5f, we plot the behavior of overall segmentation quality. Compared to the previous experiment,varying slot count has a lesser impact on overall segmentation performance for all methods. While thedifferences seem unsubstantial, the baseline appears to perform best w.r.t. this metric. Excess Slots During TrainingIn line with the experiments on the CLEVR dataset, we turn to a set ofexperiments that investigates the impact of excess slots during training. Similar to the corresponding setup inSubsection 5.1, we train the models on the filtered MOVi-C6 dataset, but provide them with 11 slots duringtraining. We generally observe that both the weighted sum normalization and the batch normalization leadto improved foreground segmentations compared to the baseline and layer normalization, especially at higherslot count, as can be concluded from Subfigures 6a and 6b. Subfigure 6c additionally demonstrates onceagain that our proposed normalizations appear to perform at least as well as the baseline across all objectcounts. In Subfigures 6d-6f we find that increased slot count during inference harms overall segmentationquality. The weighted sum variant performs best, although the variablity in ARI appears large. Evaluation on MOVi-DWe now investigate how the previously described models (which were trained onMOVi-C) transfer to the MOVi-D dataset. We recall that scenes of the MOVi-D dataset may contain up to23 objects. Hence, the MOVi-D dataset allows us to study how the trained models behave when slot- andobject-count are increased significantly during inference. We evaluate the models with 24 slots. In ,we show the MOVi-D zero-shot performance of all 12 model variants we trained on MOVi-C. The batch normvariant performs best w.r.t. our main metric, the F-ARI. This holds true both when comparing all 12 variantsto each other, and when considering the subsets of models obtained by only considering variants with any fixed",
  ": Dependence of segmentation performance on slot and object count. Models are trained on MOVi-C6with 11 slots": "annotation (O, K). Compared to their batch normalization counterparts, the weighted sum models performsimilarly w.r.t. foreground ARI, performing only slightly worse. Both the weighted sum normalization andthe batch normalization produce markedly better foreground segmentations than the baseline and layer normvariants. With respect to overall segmentation quality (ARI), none of the normalization variants consistentlyoutperform the others, which is in line with our observations on MOVi-C. For any fixed normalization method, the (6, 7) variant performs better w.r.t. F-ARI than the (10, 11) and(6, 11) variants. This illustrates that, firstly, training on filtered training sets with few objects can improveperformance during inference on scenes with many objects; secondly, avoiding excess slots during trainingappears to be important, which is an observation that has been made in (Locatello et al., 2020) and whichwe also note when comparing Figures 1 and 3. This effect underlines the importance of strong slot-countgeneralization capabilities. We posit that training at low object- and slot-counts reduces the number of\"unoccupied\" slots during training, thereby tightening the representational slot-bottleneck, which has beenpostulated to encourage object-centricness (Locatello et al., 2020; Stange et al., 2023).",
  "Related Work": "In this work, we follow a longer tradition of using autoencoders to obtain semantic scene decompositions (Greffet al., 2016; 2017; 2019; Crawford & Pineau, 2019; Burgess et al., 2019; Lin et al., 2020; Engelcke et al., 2020;Locatello et al., 2020). An early work in this area is Neural-EM (Greff et al., 2017), which performs expectationmaximization at the image level. It is proceeded by IODINE (Greff et al., 2019) and MONet (Burgess et al.,2019), which learn variational autoencoders. Slot Attention (Locatello et al., 2020) has already been used inmany derivative works to scale object-centric learning to increasingly complex datasets. In particular, motioncues (Kipf et al., 2022; Elsayed et al., 2022; Wu et al., 2023), high-level image features (Seitzer et al., 2023)and powerful decoder models (Singh et al., 2022a;b) were found to be useful for obtaining desired behavioron real-world datasets. Several previous works have discussed generalization capabilities of object-centric representations (Dittadiet al., 2022; Seitzer et al., 2023). That the number of slot latents has influence on model performance has beennoted by Seitzer et al. (2023) where the authors illustrate that varying the number of slots has a significantimpact on segmentation quality, determining whether objects are split into constituent parts or discovered in",
  "VariantF-ARI ()2 loss ()ARI ()": "Baseline (10, 11)0.647 0.0152.143 0.0040.172 0.011Layer Norm (10, 11)0.660 0.0122.146 0.0020.171 0.004Weighted Sum (10, 11)0.722 0.0052.223 0.0090.204 0.004Batch Norm (10, 11)0.725 0.0062.149 0.0040.182 0.011 Baseline (6, 11)0.640 0.0092.220 0.0010.176 0.003Layer Norm (6, 11)0.639 0.0112.220 0.0040.176 0.005Weighted Sum (6, 11)0.709 0.0442.309 0.0140.205 0.147Batch Norm (6, 11)0.711 0.0052.219 0.0060.175 0.007 Baseline (6, 7)0.741 0.0052.370 0.0090.253 0.018Layer Norm (6, 7)0.742 0.0192.365 0.0070.253 0.007Weighted Sum (6, 7)0.797 0.0232.475 0.0380.242 0.156Batch Norm (6, 7)0.809 0.0052.374 0.0020.297 0.030 : Zero-shot performance on MOVi-D of models trained on MOVi-C. Evaluated with 24 slots. We showthe median maximum deviation across multiple runs. For each metric, we underline the most advantagousvariant in each section and mark the most advantagous variant across all sections in bold. their entirety. To date, only few works investigate the role of attention normalization in the performance ofSlot Attention. The first ablation evaluated for Slot Attention in the original paper (Locatello et al., 2020) isalmost identical to the approach we discuss in Subsection 4.1, differing from our method crucially in that theweighted sums are not scaled by a constant, which appears to result in poor training behavior. In a secondablation, the authors modify the first ablation by normalizing the update codes via layer normalization (Baet al., 2016). Comparable performance is reported to the original method. Normalization in Slot Attention has been investigated in (Zhang et al., 2023), where the authors proposeto use the Sinkhorn-Knopp iteration to normalize attention matrices. In contrast to our work, they do notinvestigate the impact on generalization capabilities and substantially increase the complexity of the SAmodule. More broadly, the use of the Sinkhorn-Knopp algorithm for normalization in multi-head attentionmodules has been scrutinized before by Sander et al. (2022).",
  "Conclusion": "Allowing models to dynamically find suitable levels of segmentation coarseness is an important problemin unsupervised object-centric representation learning. In Slot Attention it has been found that excessslots oftentimes split objects into parts or distribute responsibility for individual pixels among several slots.Hence, finding a reasonable number of slots has been crucial during training and inference. In this work,we discussed approaches for making the Slot Attention module more robust with respect to this choice. Westudied normalizations of the slot-update vectors and analysed how they impact Slot Attentions ability toscale to different numbers of slots and objects during inference. On the theoretical side, we motivated thisphenomenon via an analogy between Slot Attention and parameter estimation in vMF mixture models. Inexperiments, we demonstrated that our proposed normalization schemes increase the generalization capabilityof Slot Attention to varying number of slots and objects during inference. With these insights, we hope tocontribute to increase performance of numerous existing and future applications of Slot Attention.",
  "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,2016. URL": "Arindam Banerjee, Inderjit S. Dhillon, Joydeep Ghosh, and Suvrit Sra. Generative model-based clusteringof directional data. In Lise Getoor, Ted E. Senator, Pedro M. Domingos, and Christos Faloutsos (eds.),Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and DataMining, Washington, DC, USA, August 24 - 27, 2003, pp. 1928. ACM, 2003. doi: 10.1145/956750.956757.URL",
  "Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006": "Christopher P. Burgess, Loc Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matthew M.Botvinick, and Alexander Lerchner. MONet: Unsupervised scene decomposition and representation. CoRR,abs/1901.11390, 2019. URL Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In International Conference onComputer Vision (ICCV), pp. 96309640. IEEE, 2021.doi: 10.1109/ICCV48922.2021.00951.URL Michael Chang, Thomas L. Griffiths, and Sergey Levine. Object representations as fixed points: Trainingiterative refinement algorithms with implicit differentiation. In Alice H. Oh, Alekh Agarwal, DanielleBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems (NeurIPS),2022a. URL",
  "Michael Chang, Sergey Levine, and Thomas L. Griffiths. Object-centric learning as nested optimization.In ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality, 2022b. URL": "Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties ofneural machine translation: Encoder-decoder approaches. In Dekai Wu, Marine Carpuat, Xavier Carreras,and Eva Maria Vecchi (eds.), Workshop on Syntax, Semantics and Structure in Statistical Translation(SSST@EMNLP), pp. 103111. Association for Computational Linguistics, 2014. doi: 10.3115/v1/W14-4012.URL Tim Cooijmans, Nicolas Ballas, Csar Laurent, aglar Glehre, and Aaron C. Courville. Recurrent batchnormalization. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France,April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL Eric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional neuralnetworks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-FirstInnovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposiumon Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27- February 1, 2019, pp. 34123420. AAAI Press, 2019.doi: 10.1609/AAAI.V33I01.33013412.URL A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EMalgorithm. Journal of the Royal Statistical Society. Series B (Methodological), 39(1):138, 1977. ISSN00359246. URL Andrea Dittadi, Samuele S. Papa, Michele De Vita, Bernhard Schlkopf, Ole Winther, and Francesco Locatello.Generalization and robustness implications in object-centric learning. In Kamalika Chaudhuri, StefanieJegelka, Le Song, Csaba Szepesvri, Gang Niu, and Sivan Sabato (eds.), International Conference onMachine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings ofMachine Learning Research, pp. 52215285. PMLR, 2022. URL",
  "Ronald Aylmer Fisher. Dispersion on a sphere. Proceedings of the Royal Society of London. Series A.Mathematical and Physical Sciences, 217(1130):295305, 1953. doi: 10.1098/rspa.1953.0064. URL": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks.In Yee Whye Teh and D. Mike Titterington (eds.), Proceedings of the Thirteenth International Conferenceon Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15,2010, volume 9 of JMLR Proceedings, pp. 249256. JMLR.org, 2010. URL Klaus Greff, Antti Rasmus, Mathias Berglund, Tele Hotloo Hao, Harri Valpola, and Jrgen Schmidhu-ber. Tagger: Deep unsupervised perceptual grouping. In Daniel D. Lee, Masashi Sugiyama, Ulrikevon Luxburg, Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information ProcessingSystems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,Barcelona, Spain, pp. 44844492, 2016.URL Klaus Greff,Sjoerd van Steenkiste,and Jrgen Schmidhuber.Neural expectation maximiza-tion.In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus,S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Sys-tems (NeurIPS), pp. 66916701, 2017.URL Klaus Greff, Raphal Lopez Kaufman, Rishabh Kabra, Nick Watters, Chris Burgess, Daniel Zoran, LoicMatthey, Matthew M. Botvinick, and Alexander Lerchner. Multi-object representation learning withiterative variational inference. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), InternationalConference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pp.24242433. PMLR, 2019. URL Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet,Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun,Issam H. Laradji, Hsueh-Ti Derek Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, A. Cengizztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, VincentSitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, FangchengZhong, and Andrea Tagliasacchi. Kubric: A scalable dataset generator. In Conference on Computer Visionand Pattern Recognition (CVPR), pp. 37393751. IEEE, 2022. doi: 10.1109/CVPR52688.2022.00373. URL",
  "Daniil Kirilenko, Alexey Kovalev, and Aleksandr Panov. Object-centric learning with slot mixture models. 2023.URL": "Csar Laurent, Gabriel Pereyra, Philemon Brakel, Ying Zhang, and Yoshua Bengio. Batch normalizedrecurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and SignalProcessing, ICASSP 2016, Shanghai, China, March 20-25, 2016, pp. 26572661. IEEE, 2016. doi: 10.1109/ICASSP.2016.7472159. URL Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong Jiang, andSungjin Ahn. SPACE: unsupervised object-oriented scene representation via spatial attention and decom-position. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,April 26-30, 2020. OpenReview.net, 2020. URL Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, JakobUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In HugoLarochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advancesin Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based neuralmachine translation. In Llus Mrquez, Chris Callison-Burch, Jian Su, Daniele Pighin, and Yuval Marton(eds.), Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP2015, Lisbon, Portugal, September 17-21, 2015, pp. 14121421. The Association for ComputationalLinguistics, 2015. doi: 10.18653/v1/d15-1166. URL Razvan Pascanu, Toms Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks.In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA,16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pp. 13101318. JMLR.org,2013. URL",
  "Ross Wightman. Pytorch image models. 2019.Accessed: 2023-05-24": "Ziyi Wu, Nikita Dvornik, Klaus Greff, Thomas Kipf, and Animesh Garg. Slotformer: Unsupervised visualdynamics simulation with object-centric models. In The Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.URL Yan Zhang, David W. Zhang, Simon Lacoste-Julien, Gertjan J. Burghouts, and Cees G. M. Snoek. Unlockingslot attention by changing optimal transport costs. In Andreas Krause, Emma Brunskill, KyunghyunCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference onMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings ofMachine Learning Research, pp. 4193141951. PMLR, 2023. URL",
  "AReconstruction Loss": "In this subsection, we investigate how the 2 reconstruction loss is impacted by the choice of normalization.As in previous figures, we explore how this objective varies as object and slot-count is varied during inference.For each experiment provided in the main paper we provide a corresponding figure on the 2 loss. CLEVR (6, 7)We first consider the experiment in which we trained a model with 7 slots on CLEVR6.We note in Subfigures 7a and 7b that all normalization approaches suffer under excess slots during inference.The layer norm variant generally performs best in this context, while the baseline and weighted sum variantsperform comparably to each other. In c, we note that reconstruction quality decreases with increasingobject count for all variants. It can be observed that this deterioration progresses fastest for the baseline,slowest with batch normalization, and at a seemingly similar rate for the layer norm and weighted sumvariants.",
  ": Dependence of reconstruction quality on slot and object count. Models are trained on CLEVR6with 7 slots. Note the non-zero y-intercept": "CLEVR (6, 11) illustrates the behavior of reconstruction losses for models trained on CLEVR6with excess slots. We find that, analogous to our observations in Subsection 5.1, varying slot count duringinference has a lesser effect than in the previous experiment (compare Subfigures 8a and 8b to Subfigures 7aand 7b). Generally, the weighted sum variant has the highest reconstruction loss, while the layer norm varianthas the lowest. The baseline and the batch norm variant perform comparably. As before, we observe inSubfigure 8c that reconstruction loss increases with increasing object count.",
  ": Dependence of reconstruction quality on slot and object count. Models are trained on CLEVR6with 11 slots": "Training on MOVi-CWe now shift our focus to the Dinosaur models. shows the reconstructionloss for the models trained on MOVi-C10 with 11 slots. Contrary to previous observations, we note inSubfigures 9a and 9c that reconstruction losses improve as the slot count increases. Generally, it appears thatthe weighted sum variant performs worst w.r.t. the 2 loss, while the other variants perform comparably toeach other. We observe analogous behavior in Figures 10 and 11.",
  "Dxn E[xn]122(16)": "Given these definitions, we present the following lemma, giving an expression for the affine hull of the imageof the layer normalization module.Lemma 1. The vector xk E[xk]1 is the orthogonal projection of xk onto the orthogonal complement of thespan of the all-ones vector 1 RDinput. Hence, the image of LayerNorm is contained in a (Dinput 1)-dimensional affine subspace of RDinput. More concretely, the affine hull of the image of a LayerNorm modulewith parameters , RDinput is given via:",
  "Lemma 2. For any affine subspace A RDinput, there exists a unique vector a A and a unique vectorspaceV RDinput such that a V is orthogonal to V and we have A = a + V": "Proof. Given any affine subspace A RDinput, let a A be the orthogonal projection of the origin onto A.By definition of the orthogonal projection, for any other a A, we have (a 0)(a a) = 0. Moreover,V := Aa is a linear subspace of RDinput with a+V = A. For any arbitrary v V , we may write v = a afor some a A and we haveav = (a 0)(a a) = 0(24)",
  "Hence, a V holds. We have so far shown the existence of some vector space V and a V withA = a + V": "We now show that this decomposition A = a + V is unique. Assume that there exists another vector spaceV RDinput and a V with a + V = a + V . We will show that V = V and a = a must hold.Rearranging, we find V = (a a) + V and V = V (a a). From the first equation, we may concludea a V . Since a a is a vector in V , which is closed under subtraction, we find V (a a) = V .Hence, substituting into the previous equation, we conclude V = V . We now show that we must also havea = a. We compute:(a a)(a a) = a(a a) a(a a)(25) and recall that a a V holds. By assumption, we have a, a V and therefore a(a a) = 0 anda(a a) = 0. Hence, (a a)(a a) = 0 and we conclude from the positive-definiteles of the scalarproduct that a = a does indeed hold. Finally, we show that for almost all parameters of the value map v and the layer normalization module, thetranslation vector a from the previous lemma does not vanish. Again, this will be crucial to show that afunction satisfying Proposition 2 exists almost always. Lemma 3. Denote D := Dinput and let v : RD RD be a linear map that is parametrized via a matrixB RDD which acts, say, via left-multiplication. Consider parameters , RD of the layer normalization.Consider the affine hull of the image of the composition of v and the layer normalization:",
  "A := aff(v LayerNorm)RD(26)": "As detailed in Lemma 2, we may write A uniquely as a + V , where a and V depend on the parameters of vand the layer normalization. In the following, we abuse notation by not making this dependence explicit. Foralmost all parameters (B, , ) (w.r.t. the Lebesgue measure on RDD RD RD), we have a = 0.",
  "CProof of Proposition 1": "Proof. Assume by contradiction that such a function f : RD R exists. Consider now the setting in whichwe have K := 1 and 1 := 0. Denote the resulting attention matrix by (1) and the resulting update codeu1 by w(1)1 . Consider also the setting in which we have K := 2 and 1 = 2 := 0. Denote the resultingattention matrix by (2) and the resulting update codes u1, u2 by w(2)1 , w(2)2 . One may now easily verifythat all entries of (1) are 1 and that all entries of (2) are 1/2. Hence, we have",
  "DProof of Proposition 2": "Proof. We recall from Lemma 3 that for almost all parameters of the value map v and the layer normalizationmodule, we may choose a vector a RD \\ {0} and a vector space W RD such that a W holds andfor any input xn, the corresponding values vn lie in a + W. Given fixed parameters (that do not lie in thenullset outlined in Lemma 3), fix such a vector a.",
  "While we closely follow the descriptions of (Locatello et al., 2020), we use a re-implementation of the methodin our experiments": "DataAs in (Locatello et al., 2020), we use the extended CLEVR dataset that is provided in (Kabra et al.,2019). This version of the dataset consists of 100,000 images in total, each being of dimension 320 240. Wefollow (Locatello et al., 2020; Greff et al., 2019; Burgess et al., 2019) in the pre-processing of this data: Weuse 70,000 images for training and hold out 15,000 for validation and testing. We perform a square centercrop of size 192 to increase the space occupied by objects. The cropped images are then bilinearly scaled toshape 128 128. The corresponding ground-truth segmentation masks are pre-processed analogously, usingnearest-neighbor interpolation in place of bi-linear interpolation. In contrast to (Locatello et al., 2020), weaugment the data by performing random horizontal flips. Before feeding it to the autoencoder, the RGB datais scaled to the interval . ArchitectureWe follow the autoencoder architecture described in (Locatello et al., 2020) as closely aspossible. Conceptually, we divide the autoencoder into three distinct entities: The encoder processes theinput image and produces a set of tokens. The Slot Attention module processes these tokens and yields alatent slot representation. The decoder decodes the latent representation into a reconstruction of the input.",
  "TypeIn ShapeOut ShapeActivationComment": "Spatial Broadcast648 8 64-for single slotPos. Embed8 8 648 8 64--Transposed Conv 5 58 8 6416 16 64ReLUstride:2padding:2out padding: 1Transposed Conv 5 516 16 6432 32 64ReLUas aboveTransposed Conv 5 532 32 6464 64 64ReLUas aboveTransposed Conv 5 564 64 64128 128 64ReLUas aboveTransposed Conv 5 5128 128 64128 128 64ReLUstride:1padding:2Transposed Conv 3 3128 128 64128 128 4-stride:1padding:1",
  ": Encoder network for experiments on CLEVR": "We implement the positional embedding as in (Locatello et al., 2020). Namely, to positionally embed a tensorX of shape W H C, we construct a W H 4 tensor P in which each of the four channels is a linearramp spanning between 0 and 1, either progressing horizontally or vertically and in either of the two possible",
  "directions (e.g. left or right). In order to embed the feature Xi,j,:, we learn an affine layer R4 RC andcompute the embedded feature:Xi,j,: + affine(Pi,j,:)(38)": "The resulting set of input tokens is then processed by the Slot Attention module, which we implement asdescribed in (Locatello et al., 2020). The key, query, and value maps q, k, v use the common dimensionD = 64. Slots are also 64-dimensional. The residual MLP that is used to update the slot latents has a singlehidden layer of size 128.",
  ": Decoder network for experiments on CLEVR": "The 4 channels of the output of the decoder are split into RGB channels and an unnormalized alpha channel.The alpha channels are normalized via a softmax operation across all slots, and the RGB reconstructions areblended to produce an entire reconstruction. TrainingWe closely follow the training procedure of (Locatello et al., 2020).Namely, we train theautoencoder with an 2 reconstruction loss, utilize 3 Slot Attention iterations during training, and use anAdam (Kingma & Ba, 2015) optimizer. The models are trained for 500,000 steps. As the authors of (Locatelloet al., 2020), we linearly warm up the learning rate over the course of the first 10,000 steps, after which itattains a peak value of 4 (0.5)0.1 104. Subsequently, we decay it over the course of the remaining steps,with a half life of 100,000 steps. We use a batch size of 64.",
  "While we closely follow the descriptions of (Seitzer et al., 2023), we use a re-implementation of the method inour experiments": "DataWe use the MOVi-C dataset from (Greff et al., 2022). In total, it contains 10,986 video sequences, eachconsisting of 24 frames. We hold out 250 of the sequence for validation and 999 for testing. Following (Seitzeret al., 2023), we pre-process the frames from the dataset by bicubicly resizing them to shape 224 224. Weuse a pretrained vision transformer to pre-compute image features from these resized frames. Specifically, weuse the model vit_base_patch8_224_dino from the timm (Wightman, 2019) repository. After dropping theclass token, this model provides a 28 28 768 feature map for each frame. We save these feature maps at",
  "half precision (16 bit floating point) and use them during training. Alongside, we save the resized frames andanalogously resized (via nearest-neighbor interpolation) ground truth segmentations": "ArchitectureWe closely follow the MLP-based autoencoder architecture detailed in (Seitzer et al., 2023).The goal of this autoencoder is to encode and decode the pre-computed ViT features. Importantly, it doesnot operate on RGB images. As before, we conceptually divide the autoencoder into the encoder, the SlotAttention module, and the decoder. The encoder processes each ViT feature separately via a shared MLP. In contrast to the encoder we usedin the experiments on the CLEVR dataset, no positional embedding is employed, as the ViT features stillcontain a sufficient amount of positional information (see the discussions in (Seitzer et al., 2023)). We providea detailed description of this architecture in .",
  ": Encoder network for experiments on MOVi-C": "The set of tokens that is produced by the encoder is processed by the Slot Attention module to produce alatent slot representation. Slots, keys, values, and queries are 128-dimensional. The residual MLP that isused to update the slot latents has a single hidden layer of dimension 512. In order to produce a reconstruction of the ViT features, an MLP-based decoder architecture is used.Conceptually, the decoder resembles the one we used in the experiments on the CLEVR dataset: Eachslot is decoded separately into a partial reconstruction and an unnormalized alpha channel. A completereconstruction is formed by normalizing the alpha channels across slots and blending the partial reconstructions.Each slot is broadcasted spatially before decoding and a positional embedding is added. Once again, thedecoder consists of an MLP that operates on each spatial feature separately. We provide a detailed descriptionof the architecture in",
  "Xi,j,: + Pi,j,:(39)": "TrainingWe follow the training procedure detailed in (Seitzer et al., 2023).Namely, we train theautoencoder via an 2 reconstruction loss and use 3 Slot Attention iterations during training. The batch sizeis 64 and we employ an Adam optimizer. As before, a learning rate schedule consisting of a linear increaseand exponential decay is used. Here, the peak learning rate is 4 104, which is reached after 10,000 steps.Afterwards, the learning rate decays with a half life of 100,000 steps. The models are trained for 500,000steps in total.",
  "HVisual Results": "In Tables 6 and 7, we present visual results from object discovery on the CLEVR10 and MOVi-C10datasets, respectively. On CLEVR, we show both reconstructions and segmentations, while we only providesegmentations on MOVi-C, as the reconstructions are high-dimensional ViT features. In both cases, weperform inference with a high slot count (21).",
  "IProperty Prediction": "We further illustrate the proposed normalizations on a property prediction task on the CLEVR10 dataset.We closely follow the experimental setup detailed by Locatello et al. (2020), using three seeds per variant.We train the models on the CLEVR10 dataset using 10 slots. In , we plot how the foreground ARIvaries as the number of slots is modified during inference. Consistent with our observations on the objectdiscovery task, we find that the weighted mean and layer norm variants suffer from excess slots, while theproposed normalizations are robust to them. Overall, we find that the batch norm variant performs bestw.r.t. foreground segmentation quality. # Slots 0.0 0.2 0.4 0.6 0.8 F-ARI BaselineLayer NormWeighted SumBatch Norm",
  ": F-ARI () for property prediction on CLEVR10": "In , we additionally show the mean average prediction at various distance thresholds, as definedin (Locatello et al., 2020). We observe a similar behavior as before, namely that the proposed variantsare robust to high slot counts during inference, while the weighted mean and layer norm variants are not.However, we generally find that the weighted mean variant appears to perform best at low slot counts. # Slots 0.0 0.2 0.4 0.6 0.8 mAP-1.0 BaselineLayer NormWeighted SumBatch Norm",
  "JAblation of Normalization Schemes": "In this section, we study two alternative normalizations methods. Firstly, we consider the weighted sumvariant with the scaling parameter chosen as C = 1. We refer to this method as \"unnormalized\". Secondly,we consider a variant which ablates the normalization across the batch axis from the batch normalizedvariant. I.e., we compute mean and variance for each instance separately across only the slot and layer axes.Hence, we also do not have to keep a moving average of the normalizing statistics for inference. Insteadthe normalization behaves identically during training and inference. As in the batch normalized variant,two scalar values and are learned. We refer to this variant as \"K-D-Layer Norm\", to reflect that we areperforming a layer normalization across the slot (K) and layer (D) axes. We perform experiments on theproperty prediction task on CLEVR with three seeds per variant. The unnormalized variant fails to obtain object-centric behavior, as becomes apparent from the low foregroundARI across all slot counts, as shown in . In , we observe that the mean average precisionsuffers correspondingly. This underscores the importance of scaling the update codes appropriately to achievecompetitive performance. While the K-D-Layer Norm performs better, we find that as the other baselinevariants, it is not robust to high slot counts. Moreover, on this specific task, it seems to generally underperformin terms of mAP when compared to the other object-centric variants. # Slots 0.0 0.2 0.4 0.6 0.8 F-ARI BaselineLayer NormWeighted SumBatch NormK-D-Layer NormUnnormalized"
}