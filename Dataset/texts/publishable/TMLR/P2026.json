{
  "Abstract": "Recent research shows that large language models are susceptible to privacy attacks thatinfer aspects of the training data. However, it is unclear if simpler generative models, liketopic models, share similar vulnerabilities.In this work, we propose an attack againsttopic models that can confidently identify members of the training data in Latent DirichletAllocation. Our results suggest that the privacy risks associated with generative modelingare not restricted to large neural models. Additionally, to mitigate these vulnerabilities, weexplore differentially private (DP) topic modeling. We propose a framework for private topicmodeling that incorporates DP vocabulary selection as a pre-processing step, and show thatit improves privacy while having limited effects on practical utility.",
  "Introduction": "Deep learning models propensity to memorize training data presents many privacy concerns. Notably, largelanguage models are particularly susceptible to privacy attacks that exploit memorization (Carlini et al.,2021; 2023). In this paper, we aim to investigate whether simpler probabilistic models, such as topic models,raise similar concerns. Our specific focus lies on examining probabilistic topic models like Latent DirichletAllocation (LDA) (Blei et al., 2003). Topic modeling is an unsupervised machine learning (ML) method that aims to identify underlying themesor topics within a corpus. Despite the recent successes of large language models (LLMs), Probabilistic topicmodels are still widely used due to their interpretability and straightforward implementation for text analysis.Furthermore, topic models continue to be developed for a variety of applications and downstream tasks likedocument classification, summarization, or generation (Boyd-Graber et al., 2017; Wang et al., 2019). Researchers apply topic models across a variety of domains where privacy concerns arise. For example, manystudies in the medical domain use topic models to analyze datasets with sensitive attributes (Mustakim et al.,2021). Additionally, topic models are widely used for various government or national defense applications.For instance, researchers applied LDA to better understand Twitter activity aimed at discrediting NATOduring the Trident Juncture Exercises in 2018 (Uyheng et al., 2020). To ensure ethical and responsible useof ML, it is crucial to consider the privacy implications associated with topic modeling. Probabilistic topic models like LDA serve as a basic Bayesian generative model for text. Neural topic modelsand language models can learn complex generative processes for text based on observed patterns in the",
  "Published in Transactions on Machine Learning Research (09/2024)": "Matthew Hoffman, Francis Bach, and David Blei.Online Learning for Latent Dirichlet Allocation.InJ. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta (eds.), Advances in Neural InformationProcessing Systems, volume 23. Curran Associates, Inc., 2010. URL Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V.Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. Resolving Individuals ContributingTrace Amounts of DNA to Highly Complex Mixtures Using High-Density SNP Genotyping Microarrays.PLoS Genet, 4(8):e1000167, 08 2008. doi: doi:10.1371/journal.pgen.1000167. URL Alexander Hoyle, Pranav Goel, Andrew Hian-Cheong, Denis Peskov, Jordan Boyd-Graber, and Philip Resnik.Is Automated Topic Model Evaluation Broken? The Incoherence of Coherence. In M. Ranzato, A. Beygelz-imer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information ProcessingSystems, volume 34, pp. 20182033. Curran Associates, Inc., 2021. URL T. Huang, S.Y. Zhao, H. Chen, J. Liu, and Y. Xu. Improving Parameter Estimation and Defensive Abilityof Latent Dirichlet Allocation Model Training Under Rnyi Differential Privacy. Journal of ComputerScience and Technology, 37(6):13821397, 2022. doi: 10.1007/s11390-022-2425-x. Tao Huang and Hong Chen. Improving Privacy Guarantee and Efficiency of Latent Dirichlet AllocationModel Training Under Differential Privacy. In Findings of the Association for Computational Linguistics:EMNLP 2021, pp. 143152, Punta Cana, Dominican Republic, November 2021. Association for Compu-tational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.14. URL Aleksandra Korolova, Krishnaram Kenthapadi, Nina Mishra, and Alexandros Ntoulas. Releasing SearchQueries and Clicks Privately. In Proceedings of the 18th International Conference on World Wide Web,WWW 09, pp. 171180, New York, NY, USA, 2009. Association for Computing Machinery.ISBN9781605584874. doi: 10.1145/1526709.1526733. URL David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. Optimizingsemantic coherence in topic models. In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 11, pp. 262272, USA, 2011. Association for Computational Linguistics.ISBN 9781937284114. M. Mustakim, Retantyo Wardoyo, Khabib Mustofa, Gandes Retno Rahayu, and Ida Rosyidah.LatentDirichlet Allocation for Medical Records Topic Modeling: Systematic Literature Review. In 2021 SixthInternational Conference on Informatics and Computing (ICIC), pp. 17, 2021. doi: 10.1109/ICIC54025.2021.9632993.",
  "Definition 2.1 (Topic Model) For a vocabulary size of V and k topics, a topic model kV is amatrix whose rows sum to 1 representing each topic as a distribution over words": "We let fG(D) denote the process of learning a model parameterized by latent variables in G on a corpusD with M documents drawn from the underlying data distribution D. The function fG incorporates thelearning algorithm to estimate Gs latent variables and returns the learned topic model . Our definitionassumes that the primary objective of topic modeling is to estimate . While G is also associated with otherlatent variables that may define the document generation process, best achieves the goals associated withtopic modeling by summarizing the relevant themes in the corpus.",
  "Latent Dirichlet Allocation": "Blei et al. (2003) define LDA where each document is assumed to contain a mixture of topics. LDA assumesthat each document d D is represented by a k-dimensional document-topic distribution Dirichlet(),and the entire corpus is represented by k V -dimensional topic-word distributions Dirichlet(). Fur-thermore, it follows a simple bag-of-words approach where each word in a document is generated by firstsampling a topic from the documents topic distribution and then sampling a word from the chosen topicsdistribution over words. The process of estimating presents a Bayesian inference problem typically solved using collapsed Gibbssampling or variational inference (Griffiths & Steyvers, 2004; Hoffman et al., 2010). Each entry z,w repre-sents the probability of drawing word w from topic z. The entire topic model represents the likelihood ofeach word appearing in each topic, which can be used to estimate the document-topic distribution for anygiven document.",
  "Memorization and Privacy": "Understanding memorization in machine learning is critical for trustworthy deployment. Neural models largeparameter space and learning method introduce training data memorization which increases with modelsize or observation duplication (Carlini et al., 2023; Feldman & Zhang, 2020). Memorization introducesvulnerabilities to attacks on privacy, such as membership inference, attribute inference or data extractionattacks (Shokri et al., 2017; Song & Raghunathan, 2020; Carlini et al., 2021). Schofield et al. (2017) note that document duplication in topic models concentrates the duplicated documentstopic distribution over less topics, and briefly refer to this phenomenon as memorization. Their findingsare consistent with studies on memorization and text duplication in large language models (Carlini et al.,2023). In this work, we investigate the memorization and privacy of topic models using membership inferenceattacks (MIA).",
  "Membership Inference Attacks": "In an MIA the adversary learns if a specific observation appeared in the models training data (Homer et al.,2008; Shokri et al., 2017). These attacks serve as the foundation for other attacks like data extraction, andcould violate privacy alone if the adversary learns that an individual contributed to a dataset with a sensitiveglobal attribute (i.e. a dataset containing only patients with disease X). Prior to attacking ML models, researchers explored exploiting aggregate statistics released in genome-wideassociation studies (GWAS) (Homer et al., 2008). Shokri et al. (2017) introduced one of the first MIAs onML models. While most MIAs exploit deep learning models, few studies investigate topic models. Huanget al. (2022) propose three simple MIAs to evaluate their privatized LDA learning algorithm. However,they evaluate their attacks using precision and recall and fail to show that their attack confidently identifiesmembers of the training data. To evaluate an MIAs ability to confidently infer membership, we examine the attacks true positive rate(TPR) at low false positive rates (FPR) (Carlini et al., 2022). Receiver operator characteristic (ROC) andarea under the curve (AUC) analysis may be useful, but we must be wary of the axes. To accurately capturethe TPR at the low FPRs of interest, we visualize ROC curves using log-scaled axes.",
  "Threat Model": "We consider a threat model where the adversary has black-box access to the learned topic-word distribution, but not other latent variables captured by G. The adversary can not observe intermediate word counts,learning method, or hyper-parameters used while learning . We also assume that the adversary has queryaccess to the underlying data distribution D, which allows them to train shadow models on datasets drawnfrom D. Under the typical query model, we could consider queries as a request to predict a document-topic distribution given a document. However, would hold little meaning without access to . Furthermore, a cleveradversary could query many specially selected documents to reconstruct . Therefore, we remove this levelof abstraction and assume that the adversary is given direct access to as the output of our topic model.",
  "where p(obs|Tx(d)) is the probability density function of obs under Tx(d) (Carlini et al., 2022)": "However, the ratio in Equation 1 is intractable because it requires integrating over all with all possibleD with and without d. Instead, the adversary applies a carefully chosen query statistic : (, d) R onall to reduce the problem to 1-dimension. The test can proceed by calculating the probability density of(obs, d) under estimated normal distributions from Tin(d) = {(, d) | fG(D {d}), D D} andTout(d) = {(, d) | fG(D\\{d}), D D}:",
  "where and 2 are the mean and variance of Tin(d) and Tout(d)": "In the online LiRA, the adversary must train N shadow topic models with and without d to estimate Tin(d)and Tout(d). Appendix A contains the full online LiRA algorithm. To ease the computational burden oftraining N shadow topic models for each target document, Carlini et al. (2022) propose an offline variantof the LiRA. In the offline LiRA, the adversary can evaluate any d after learning a collection of N shadowtopic models once by comparing (obs, d) to a normal estimated from Tout(d) with a one-sided hypothesistest. Appendix B contains the full offline LiRA algorithm. Designing is crucial to the attacks success. In supervised learning scenarios, the adversary can directlyquery the model and use the loss of an observation to inform the statistic (Carlini et al., 2022). However,because topic models present an unsupervised learning tasks, the adversary encounters the challenge ofidentifying an informative statistic that can be computed efficiently using . Hence, must be carefullytailored to topic models for the attack to be effective.",
  "Designing an Effective Query Statistic": "Effective model query statistics for the LiRA framework must satisfy the following criteria: the statisticshould increase for d when d is included in the training data (Tin(d) > Tout(d)) to enable the offlineLiRA, and the estimated distributions Tin(d) and Tout(d) are approximately normal to allow for parametricmodeling. Ideally, the statistic should be related to how the model may memorize the training data toprovide an intuitive interpretation for attack performance. First, we consider statistics from the simple MIAs on LDA presented by Huang et al. (2022). They proposethree statistics derived from the target documents estimated topic distribution : the entropy of , thestandard deviation of , and the maximum value in . The intuition behind their chosen statistics comesfrom the observation that a documents topic distribution tends to concentrate in a few topics when included(or duplicated) in the training data (Schofield et al., 2017). However, Huang et al. (2022) apply globalthresholds to these statistics instead of using them as query statistics to derive the LiRA test statistic as inEquation 2. We propose an attack that leverages the LiRA framework with an improved query statistic to directly exploitLDAs generative process. Our query statistic is a heuristic for the target documents log-likelihood underLDA. To compute this statistic on for a given document d, the adversary maximizes the log-likelihood ofthe document over all possible document-topic distributions such that:",
  "z z = 1. In practice, we use SciPys out-of-the-box optimizationmethods to estimate (, d) (Virtanen et al., 2020)": "Compared to the statistics proposed by Huang et al. (2022), our statistic more directly exploits LDAs gener-ative processes and is based on the observation that a documents likelihood under the target model increaseswhen included in the training data. Therefore, using allows us to better reason about memorization intopic models. We verify our criteria for the proposed statistic in Appendix C. Additionally, Appendix Ccontains an evaluation of other candidate statistics where we show that our statistic significantly outperformsother statistics based on the attacks in Huang et al. (2022). The LiRA with statistic directly exploits memorization and per-example hardness. The attack accountsfor the natural differences in on various documents by estimating the likelihood ratio under distributionsbased on Tin(d) and Tout(d). This enables the LiRA to outperform simple attacks like in Huang et al.(2022) that use global thresholds on the other query statistics based on . Therefore, we propose the LiRAwith statistic as a stronger alternative to attack topic models.",
  "Memorization and Per-Example Hardness": "To verify aspects of memorization and per-example hardness for probabilistic topic models, we estimate andvisualize Tin(d) and Tout(d) in an experiment similar to Carlini et al. (2022) and Feldman & Zhang (2020).Consistent with their findings, we show that outlying and hard-to-fit observations tend to have a large effecton the learned model when included in the training set for LDA. The histograms in display theestimated distributions for various types of documents. When included in the training data for , longer and outlying documents increase (, d). We note thatthe word probability z,w in certain topics dramatically changes for words that appear infrequently in D orwords that occur many times in d. Together, these factors affect the learned topic model and shift (, d). The simple fact that the model is more likely to generate specific documents after inclusion in the trainingdata hints toward memorization. Because the generative process for documents in LDA is extremely simplecompared to neural language models, they do not learn verbatim sequences of words. Instead, topic modelsability to memorize\" the training data is due to learning based on word co-occurrences. Long documents are inherently harder to fit than shorter documents.Because the log-likelihood for adocument d is the sum of independent log-probabilities for generating each word in d, the magnitude of(, d) is naturally higher for longer documents. Additionally, the model may struggle to capture coherenttopic structures for longer documents because they tend to contain words that span a wider range of topics. The LiRA turns our observations on memorization and per-example hardness into a MIA. When the wordsin a document become more common in the training set, will reflect the new word co-occurrences in a fewspecific topics which tends to increase (, d). This effect is amplified when the document contains manyrare words from the vocabulary set. Therefore, the attacker can easily differentiate between Tin(d) andTout(d) for long and outlying documents.",
  "Attack Evaluation Set-Up": "We evaluate our attack against three datasets: TweetRumors, 20Newsgroup and NIPS1. We apply standardtext pre-processing procedures for each dataset: removal of all non-alphabetic characters, tokenization, re-moval of stop words, removal of tokens longer than 15 characters, removal of tokens shorter than 3 characters,and lemmentization.2 Appendix D provides the basic data profile for each dataset after pre-processing.",
  "TweetRumors12.8%0.19%0.19%0.18%NIPS (k = 10)44.9%1.69%1.69%1.31%20Newsgroup21.1%0.57%0.57%0.53%": "To initiate the attack, we randomly sample half of the data to learn and release obs. Next, we train Nshadow topic models by repeatedly sampling half of the data to simulate sampling from D. Like in Carliniet al. (2022), this setup creates some overlap between shadow model training data and target model trainingdata which presents a strong assumption made to accommodate the smaller size of our datasets. We do notexpect performance to significantly decrease using disjoint datasets as observed by Carlini et al. (2022). We compare our LiRA against each of the attacks presented by Huang et al. (2022). To replicate theirattacks, we randomly sample half of the dataset to learn obs. Using obs, we estimate each documentstopic-distribution and compute the maximum posterior, standard deviation, and entropy on . We directlythreshold the each of these statistics to evaluate membership for every document in the dataset. For each experiment, we learn using scikit-learns implementation of LDA with default learning param-eters.3 For TweetSet and 20Newsgroup we set the number of topics k to 5 and 20 respectively based onthe datas known distribution across topics: TweetSet contains tweets collected on 5 major news events and20Newsgroup contains newsgroup documents across 20 different newsgroups. For NIPS, we vary k for exper-imentation purposes. In practice, k is known by the attacker who can observe the dimensions of . We learnN = 128 shadow models, replicate each experiment 10 times and report our results across all iterations. We interpret as a predicted membership score where a higher value indicates that the document is morelikely to be a member of the training data. We empirically estimate our attacks performance by evaluatingthe TPR at all FPRs and plot the attacks ROC curve on log-scaled axes.",
  "Attack Evaluation Results": "First, we compare our online attack performance against the attacks in Huang et al. (2022) at an FPR of0.1% in . displays the ROC curves for the online and offline variant of our attacks. and demonstrate that our LiRA outperforms each of attacks in Huang et al. (2022). Because the LiRA considers the likelihood-ratio of d, it accounts for per-example hardness and dominatesthe attacks by Huang et al. (2022) at all FPRs. As noted by Carlini et al. (2022), attacks that directly applyglobal thresholds do not consider document level differences on the learned model and fail to confidentlyidentify members of the training data. Stronger attacks, like our LiRA, should be used to empirically evaluatethe privacy associated with topic models. To understand how the number of topics in influence attack performance, we vary the number of topicsk on NIPS and attack the resulting model. shows the the attack performance as we vary k. As kincreases, we see that TPR increases at an FPR of 0.1%. Additionally, we note minor differences betweenonline and offline attack performance. The number of parameters in grows linearly by the length of the vocabulary set as k increases. Withmore topics and more parameters, documents word co-occurrence structure is typically better representedin the documents learned topic distribution. Consequently, increasing k enhances the impact of includingthe document in the training data resulting in better attack performance.",
  "Private Topic Modeling": "Designing private ML solutions is an active field of research. In topic modeling, the literature is largelyfocused on differential privacy (DP). DP acts as a direct defense against privacy attacks like MIAs by limitingthe effect one data point can have on the learned model. Furthermore, DP provides a clear, quantifiablemethod for reasoning about privacy loss.",
  "Differential Privacy": "DP provides strong theoretical guarantees of individual-level privacy by requiring the output of some dataanalysis to be indistinguishable (with respect to a small multiplicity factor) between any two adjacentdatasets x and x. Definition 4.1 (Differential Privacy (Dwork et al., 2006b;a)) Let M : X n R be a randomizedalgorithm. For any 0 and , we say that M is (, )-differentially private if for all adjacentdatabases x, x X n and every S R",
  "Pr[M(x) S] e Pr[M(x) S] +": "The notion of adjacency is critical. For any text dataset, we say that two corpora D and D are author-leveladjacent if they differ by one authors documents. This notion enforces DPs promise of individual levelprivacy by considering adjacency at the user level. If two corpora D and D differ by one document, then adjacency is at the document-level. This relaxed notionof adjacency can satisfy author-level adjacency if we assume that each document has a unique author. Forcorpora the most relaxed notion of adjacency is word-level adjacency. Two corpora D and D are word-leveladjacent if they differ by one word in one document. To achieve differential privacy we must add noise whose magnitude is scaled to the worst-case sensitivity ofthe target statistics from all adjacent datasets. Researchers have explored word-level adjacency to controlsensitivity, but this approach offers weak privacy guarantees. For instance, most DP collapsed Gibbs samplingalgorithms for learning LDA rely on perturbing word-count statistics by adding noise scaled to word-leveladjacency (Zhao et al., 2021; Huang & Chen, 2021; Huang et al., 2022).",
  "DP Vocabulary Selection": "Releasing the topic-word distribution without ensuring the privacy of the accompanying vocabulary setcan lead to privacy violations because the vocabulary set is derived directly from the data. Topic modelswith comprehensive vocabulary sets tend to include many infrequently used words from the corpus whichmakes them more vulnerable to MIAs (as shown in section 3.4). Furthermore, even if the values in areprivate, the overall release of the topic-word distribution can not satisfy DP because the vocabulary setaccompanied by is not private. To illustrate this point, consider a scenario where we have a corpus D with a single document d. If we applyDP topic modeling to release without changing the vocabulary set, then we release all of the words in d.This clearly compromises the privacy of the document in D. If we chose not to release the vocabulary set, loses practical interpretability. To address this issue, it is necessary to explore methods for differentiallyprivate vocabulary selection. DP vocabulary selection can be formalized as the DP Set-Union (DPSU) (Gopi et al., 2020). If we assumeeach author i contributes a subset Wi U of terms in their documents, DPSU seeks to design a (, )-DPalgorithm to release a vocabulary set S iWi such that the size of S is as large as possible. Researchersfirst approached this problem with the intent to release the approximate counts of as many items as possiblein iWi (Korolova et al., 2009; Wilson et al., 2020). Their algorithms guarantee privacy by constructing ahistogram of iWi, adding noise to each word frequency in the histogram, and releasing all word frequenciesthat fall above a certain threshold. These implementations upper-bound the sensitivity based on the maxi-mum the number of words contributed by all users (0 = maxi |Wi|) to account for users contributing morethan one word. However, most users contribute significantly less than 0 so this method wastes sensitivity.Gopi et al. (2020) propose algorithms that build weighted histograms with update policies that help controlsensitivity. Intuitively, the update policy stops increasing the weights of words in the histogram after theyreach some cutoff which is set using the parameter . Carvalho et al. (2022) propose an improved algo-rithm for DPSU which is well fit for vocabulary selection because it allows for one user to contribute thesame term multiple times.",
  "Theorem 4.1 (Privacy Guarantee of FDPTM) If M1 for selecting the vocabulary set satisfies (1, 1)-DP and M2 for topic modeling satisfies (2, 2)-DP, then the overall release of satisfies (1+2, 1+2)-DP": "Implementing FDPTM requires a few key considerations. First, the process depends on carefully tuning theprivacy parameters and other parameters within M1 and M2. Second, the data curator must ensure thatM1 and M2 satisfy the same notion of differential privacy and adjacency. Overall, FDPTM is a modular framework for releasing meaningful topic-word distributions. Although ourevaluations focus on LDA, other topic models for specific uses can easily fit into the proposed procedure.",
  "FDPTM Evaluation Set-Up": "We empirically evaluate our FDPTM using the TweetRumors dataset with k = 5. We ensure author-levelprivacy via document-level adjacency under the assumption that each document has a unique author. Thisassumption necessary for controlling sensitivity and is common to most DP NLP solutions. We accomplishthis by using the DPSU solution proposed by Carvalho et al. (2022) for vocabulary selection and DP LDAlearning algorithm by Zhu et al. (2016) because they satisfy the same notion of privacy. For DPSU, we fixthe privacy parameter = 105 and set the parameter = 3 to control the cut-off value like in Carvalhoet al. (2022). To evaluate utility, we first vary the privacy loss parameter for DPSU and analyze utility when LDA is non-private. We study the interaction between DP vocabulary selection and DP LDA by fixing the privacy lossparameter for one mechanism and varying the privacy loss parameter for the other mechanism. Specifically,we first fix 2 = 3 for DP LDA and vary 1 for DPSU. Then, we fix 1 = 3 and vary 2. We vary our privacyloss parameters in intervals across common choices for DP ML solutions. Typically, > 1 provides veryweak theoretical privacy guarantees, but tend to be effective for providing empirical privacy.",
  "D(v(t)l ),(4)": "where D(v) is the document frequency of word v (i.e. the number of documents where word v occurs),D(v, v) is the co-document frequency of words v and v (i.e. the number of documents where v and v bothoccur), and V (t) = {v(t)1 , ..., v(t)M } is list of the M most probable words in topic t. A high topic coherencescore suggests that the top M terms have higher co-document frequencies which makes the topic easier tounderstand because the terms are more semantically similar or coherent. In our analysis, we select eachtopics top M = 10 words and report the average across each topic. We also test FDPTM against the online LiRA to empirically evaluate the privacy of FDPTM. For eachattack we learn 64 shadow models. Like before, we conduct attacks while varying 1 {1, 5, 10} and fixing2 = 5. Then, we fix 1 = 5 and vary 2. We repeat each experiment 10 times and report the results acrosseach iteration.",
  "displays topic coherence as we increase 1. displays topic coherence as we vary either and hold the other constant. We include the average vocabulary size as we increase 1 in Appendix G": "Topic coherence remains stable after = 3, suggesting that increasing the privacy loss for DPSU may notaffect topic coherence after a certain point. When comparing coherence while increasing 2, we see that1 decreases coherence regardless of 2. In , coherence plateaus around 1 = 3 while increasing 2continues to boost coherence. Consequently, increasing 1 yields diminishing utility returns faster.",
  "Conclusions": "In our work, we show that topic models exhibit aspects of memorization and successfully implement strongMIAs against them. Although probabilistic topic models do not memorize verbatim sequences of text likelanguage models, they do memorize word frequency and occurrence. In some instances, knowledge of thefrequency of certain terms in a document constitutes a privacy violation regardless of their word ordering. To combat MIAs and memorization, we propose a modular framework for implementing better private topicmodels using differential privacy. Overall, the addition of DP vocabulary selection to the DP topic modelingworkflow is important for guaranteeing private, interpretable releases of . Not only does DP vocabularyselection allow for fully DP releases of , it also provides an effective defense against MIAs. We highlight the greater need for continued development in the field of privacy-preserving ML. If simpleprobabilistic topics models for text memorize their training data, then its only inevitable that LLMs andneural topic models memorize their training data. As ML models continue to become more sophisticatedand widely used, privacy concerns become increasingly relevant.",
  "Limitations": "While our results provide insights on topic models privacy, there are a few limitations of our methodologyand experiments. First, because we rely on a BoW document representation, we only identify that documentswith certain word frequencies are part of the training set. In practice, this may cause the attack to realizemore false positives. The datasets used in our attack evaluations did not include documents with overlappingword combinations, and most reasonable documents contain very few sensible alternative word combinationswhich eases the implications of this limitation. Additionally, our utility assessments for FDPTM are constrained by the limitations of automated topicevaluation methods.While different measures give us different insights into the topic model, the bestevaluations are typically based on human inspection (Hoyle et al., 2021). Other utility metrics that evaluatetopic models with metrics that estimate model fit, like perplexity, tend to be negatively correlated withhuman-measured topic interpretability (Chang et al., 2009). Therefore, we use topic coherence because it isassociated with the human interpretability of topics. Finally, while our results are convincing, we conduct them with the same learning process (fG based onLDA via Gibbs-sampling) and hyperparameters for the the shadow and target models with access to theentire topic-word distribution . In many practical use cases, researchers may not use LDA or release allof . A mismatch between shadow and topic models fG may impact attack accuracy, but we suspect to alesser extent than in neural models. Additionally, we believe that clever adversaries can reconstruct givenpartial or alternative releases of . Determining the affect that different fG and releases of have on attackperformance is left for future research.",
  "Ethics Statement": "The privacy attacks presented in this paper are for research purposes only. The methodology and code shouldbe used responsibly with respect for privacy considerations. Each of the datasets used in our experimentsare open access and do not pose a direct threat to the privacy of the users who contributed to them.",
  "Acknowledgement": "N.M. is supported through the MIT Lincoln Laboratory (MITLL) Military Fellowship and by NSF grant BCS-2218803. W.Z. is supported by a Computing Innovation Fellowship from the Computing Research Association(CRA) and the Computing Community Consortium (CCC). S.V. is supported by NSF grant BCS-2218803and a Simons Investigator Award.The authors would like to acknowledge the MITLL SupercomputingCenter for providing compute and consulting resources that contributed to our results.",
  "Jordan L Boyd-Graber, Yuening Hu, David Mimno, and et al. Applications of Topic Models, volume 11.Now Publishers Incorporated, 2017": "Nicholas Carlini, Florian Tramr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, AdamRoberts, Tom Brown, Dawn Song, lfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting TrainingData from Large Language Models. In 30th USENIX Security Symposium (USENIX Security 21), pp.26332650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramr. MembershipInference Attacks From First Principles. In 2022 IEEE Symposium on Security and Privacy (SP), pp.18971914, 2022. doi: 10.1109/SP46214.2022.9833649. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.Quantifying memorization across neural language models. In The Eleventh International Conference onLearning Representations, 2023. URL Ricardo Silva Carvalho, Ke Wang, and Lovedeep Singh Gondara. Incorporating Item Frequency for Differen-tially Private Set Union. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.95049511, Jun. 2022. doi: 10.1609/aaai.v36i9.21183. URL Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-graber, and David Blei.Reading TeaLeaves: How Humans Interpret Topic Models. In Y. Bengio, D. Schuurmans, J. Lafferty, C. Williams,and A. Culotta (eds.), Advances in Neural Information Processing Systems, volume 22. CurranAssociates,Inc.,2009.URL Chris Decarolis, Mukul Ram, Seyed Esmaeili, Yu-Xiang Wang, and Furong Huang.An End-to-EndDifferentially Private Latent Dirichlet Allocation Using a Spectral Algorithm.In Hal Daum IIIand Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, vol-ume 119 of Proceedings of Machine Learning Research, pp. 24212431. PMLR, 1318 Jul 2020.URL Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our Data, Our-selves: Privacy Via Distributed Noise Generation. In Serge Vaudenay (ed.), Advances in Cryptology -EUROCRYPT 2006, pp. 486503, Berlin, Heidelberg, 2006a. Springer Berlin Heidelberg. ISBN 978-3-540-34547-3. Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity in PrivateData Analysis. In Lecture notes in computer science, Lecture Notes in Computer Science, pp. 265284,Berlin, Heidelberg, 2006b. Springer Berlin Heidelberg. ISBN 3540327312. Vitaly Feldman and Chiyuan Zhang. What Neural Networks Memorize and Why: Discovering the LongTail via Influence Estimation. In Proceedings of the 34th International Conference on Neural InformationProcessing Systems, NIPS20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Sivakanth Gopi, Pankaj Gulhane, Janardhan Kulkarni, Judy Hanwen Shen, Milad Shokouhi, and SergeyYekhanin. Differentially Private Set Union. In Hal Daum III and Aarti Singh (eds.), Proceedings of the37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Re-search, pp. 36273636. PMLR, 1318 Jul 2020. URL",
  "New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450370899. doi: 10.1145/3372297.3417270. URL": "Abhradeep Guha Thakurta and Adam Smith. Differentially Private Feature Selection via Stability Argu-ments, and the Robustness of the Lasso. In Shai Shalev-Shwartz and Ingo Steinwart (eds.), Proceedings ofthe 26th Annual Conference on Learning Theory, volume 30 of Proceedings of Machine Learning Research,pp. 819850, Princeton, NJ, USA, 1214 Jun 2013. PMLR. URL Joshua Uyheng, Thomas Magelinski, Ramon Villa-Cox, Christine Sowa, and Kathleen M. Carley. Inter-operable Pipelines for Social Cyber-Security: Assessing Twitter Information Operations during NATOTrident Juncture 2018. Comput. Math. Organ. Theory, 26(4):465483, dec 2020. ISSN 1381-298X. doi:10.1007/s10588-019-09298-1. URL Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Ev-geni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stfan J. van der Walt, MatthewBrett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, RobertKern, Eric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde,Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald,Antnio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors.SciPy 1.0:Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261272, 2020. doi:10.1038/s41592-019-0686-2. Han Wang, Jayashree Sharma, Shuya Feng, Kai Shu, and Yuan Hong.A Model-Agnostic Approach toDifferentially Private Topic Mining. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pp. 18351845, 2022. Wenlin Wang, Zhe Gan, Hongteng Xu, Ruiyi Zhang, Guoyin Wang, Dinghan Shen, Changyou Chen,and Lawrence Carin.Topic-Guided Variational Auto-Encoder for Text Generation.In Proceedingsof the 2019 Conference of the North American Chapter of the Association for Computational Linguis-tics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 166177, Minneapolis,Minnesota, June 2019. Association for Computational Linguistics.doi: 10.18653/v1/N19-1015.URL Royce J. Wilson, Celia Yuxin Zhang, William Lam, Damien Desfontaines, Daniel Simmons-Marengo, andBryant Gipson. Differentially Private SQL with Bounded User Contribution. In Proceedings on PrivacyEnhancing Technologies, volume 2020, pp. 230250, 2020. doi: 10.2478/popets-2020-0025. Fangyuan Zhao, Xuebin Ren, Shusen Yang, Qing Han, Peng Zhao, and Xinyu Yang. Latent dirichlet allo-cation model training with differential privacy. IEEE transactions on information forensics and security,16:12901305, 2021. ISSN 1556-6013.",
  "Additionally, we compare the proposed statistic to alternative statistics based on the attacks in Huang et al.(2022) and show that our proposed statistic outperforms the alternatives": "Huang et al. (2022) propose three statistics: the entropy of , the standard deviation of , and the maximumvalue in where is a documents estimated topic distribution Huang et al. (2022). Each statistic requiresestimating the documents topic distribution . They estimate using sampling techniques, but for theseexperiments, we choose to estimate by performing the optimization from Eq. 3 for increased efficiency andconsistency with the proposed statistic in Eq. 3. To tailor each statistic to fit within the LiRA framework, we make some minor modifications to the statisticsproposed by Huang et al. (2022). Because entropy naturally decreases when the document is included inthe training data, we use the negative entropy to satisfy the first requirement. Additionally, because themaximum value in is bound between and tends to concentrate toward 1 for outliers, we apply the logittransformation to promote adherence to the normality requirement, as illustrated by Carlini et al. (2022). To evaluate the query statistic selection criteria, we fit an online attack using N = 256 shadow modelsusing each of our candidate statistics. We sample 1000 documents from each dataset to extract Tin(d)and Tout(d) for each candidate statistic. We assess the first requirement by evaluating the KL-divergencebetween two normal distributions estimated from Tin(d) and Tout(d). A positive KL-divergence betweenNin(d) = N(mean(Tin(d)), var(Tin(d))) and Nout(d) = N(mean(Tout(d)), var(Tout(d))) indicates that thestatistic is greater when d is included in the training data. displays a boxplot for the KL divergencesfor each statistic. We note that all KL-divergences in are greater than 0 which indicates that Tin(d)> Tout(d). We evaluate the normality requirement by performing a Shipiro-Wilk test for each Tin(d) and Tout(d) withcontrol for the False Discovery Rate (FDR) using the Benjamini-Hochberg Procedure (Shapiro & Wilk, 1965;Benjamini & Hochberg, 1995). The null hypothesis is that the data is drawn from a normal distribution. contains the the number of tests rejected with FDR control for each statistic at a significance levelof .05. Our analysis indicates that the distributions of Tin/out(d) for our statistic tend to be normal moreoften than the alternatives. We can expect to encounter fewer rejections as we increase N (i.e. increase thesample size of Tin/out(d)).",
  "AuthorsNotion of DPNotion of AdjacencyLearning MethodOther Technical Details": "Zhu et al. (2016) Zhu et al. (2016)-DPdocument-levelCGSZhao et al. (2021) Zhao et al. (2021)-DPword-levelCGSHuang and Chen (2021) Huang & Chen (2021)(, )-DPword-levelCGSsub-samplingHuang et al. (2022) Huang et al. (2022)Rnyi-DPword-levelCGSPark et al. (2018) Park et al. (2016)(, )-DPdocument-levelVImoments accountant and sub-samplingDecarolis et al. (2020) Decarolis et al. (2020)Rnyi-DPdocument-levelSAlocal sensitivity from propose-test-releaseWang et al. (2022) Wang et al. (2022)Rnyi-DPauthor-levelPHpain-free smooth sensitivity Finally, in , we evaluate the attack performance of all statistics across each dataset. We show thatthe LiRA with our proposed statistic dominates the other candidate statistics at all FPRs. While each of thecandidate statistics satisfies requirement 1, the long-whiskers and outliers associated with the logit maximumposterior statistic in suggest that it would outperform other statistics because the model can moreeasily differentiate between Nin(d) and Nout(d). However, the statistics lack of normality seems to hinderperformance.",
  "FProof of Theorem 4.1": "First, let M1 be a (1, 1)-DP vocabulary selection algorithm that returns a private vocabulary set S, andM2 be a (2, 2)-DP topic modeling algorithm that returns a private topic-word distribution . Now, let PREbe a function that takes a corpus D and applies standard pre-processing procedures, and SAN be a function",
  "M2(SAN(PRE(D), M1(PRE(D)))))": "Definition F.1 (k-Stability (Thakurta & Smith, 2013)) A function f : U R is k-stable on inputD if adding or removing any k elements from D does not change the value of f, that is, f(D) = f(D) forall D such that DD k. We say f is stable on D if it is (at least) 1-stable on D, and unstable otherwise.",
  "Lemma F.1 (Composition with Stable Functions (Thakurta & Smith, 2013)) Let f be a stablefunction, and let M be an (, )-DP algorithm. Then, their composition M(f(x)) satisfies (, )-DP": "The functions PRE and SAN are stable algorithms because each document is processed independently of theothers based on a standard set of rules. Simply, adding or removing a document from the corpus does notaffect the functions behavior on other documents,. Therefore, PRE and SAN are stable functions. Via our definition of M1, and using the fact that PRE is a stable function, then the first term in Mx,M1(PRE(D)), satisfies (1, 1)-DP. The second term of Mx applies M2 which directly depends on M1 andthe stable functions PRE and SAN. Therefore, via adaptive composition, Mx is (1 + 2, 1 + 2)-DP (Dworket al., 2006a)."
}