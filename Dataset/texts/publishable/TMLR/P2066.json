{
  "Abstract": "In this paper, we propose a novel optimization algorithm for training machine learning mod-els called Input Normalized Stochastic Gradient Descent (INSGD), inspired by the Normal-ized Least Mean Squares (NLMS) algorithm used in adaptive filtering. When training com-plex models on large datasets, choosing optimizer parameters, particularly the learning rate,is crucial to avoid divergence. Our algorithm updates the network weights using stochasticgradient descent with 1 and 2-based normalizations applied to the learning rate, similarto NLMS. However, unlike existing normalization methods, we exclude the error term fromthe normalization process and instead normalize the update term using the input vector tothe neuron. Our experiments demonstrate that our optimization algorithm achieves higheraccuracy levels compared to different initialization settings. We evaluate the efficiency ofour training algorithm on benchmark datasets using a toy neural network and several ma-ture modern deep networks including ResNet-20, ResNet-50, MobileNetV3, WResNet-18,and Vision Transformer. Our INSGD algorithm improves ResNet-20s CIFAR-10 test ac-curacy from 92.57% to 92.67%, MobileNetV3s CIFAR-10 test accuracy from 90.83% to91.13%, WResNet-18 on CIFAR-100 from 78.24% to 78.47%, and ResNet-50s accuracy onImageNet-1K validation dataset from 75.60% to 75.92%.",
  "Introduction": "Deep Neural Networks (DNNs) have gained immense popularity and have been extensively applied acrossvarious research fields due to their convenience and ease of use in many machine learning tasks LeCunet al. (1995); He et al. (2016); Krizhevsky et al. (2017); Simonyan & Zisserman (2014); Long et al. (2015).Researchers from different domains can readily utilize DNN models for their work, as these models can adapttheir parameters to find the best possible solutions for a wide range of problems, particularly in supervisedlearning scenarios. The parameters of a DNN model are updated using various optimization algorithms, andresearchers have proposed different algorithms that offer fresh perspectives and address different conditionsRuder (2016). It is important to note that different optimization algorithms can yield different results in agiven problem depending on the task at hand. Stochastic Gradient Descent (SGD) is a widely adopted optimization algorithm for supervised learning inDNN models. It is a simple, efficient, and parallelizable algorithm that can produce very accurate resultson large-scale datasets appropriate initial conditions Bottou (2010). Another popular algorithm Adam canoutperform the SGD in some cases Kingma & Ba (2014). However, its initial learning rate is crucial. Arelatively high value can lead to divergence. Optimization algorithms always play a significant role in trainingDNN models, but ensuring convergence of weights and finding the optimal solution for a given problem isnot always guaranteed.Therefore, the evaluation of an optimization algorithm should also consider itslimitations.Robustness to variability is a crucial attribute expected from any optimization algorithm.",
  "Published in Transactions on Machine Learning Research (08/2024)": "Priya Goyal, Piotr Dollr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, AndrewTulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour.arXiv preprint arXiv:1706.02677, 2017. Osman Gunay, Behet Ugur Toreyin, Kivanc Kose, and A. Enis Cetin. Entropy-functional-based onlineadaptive decision fusion framework with application to wildfire detection in video. IEEE Transactions onImage Processing, 21(5):28532865, 2012. doi: 10.1109/TIP.2012.2183141.",
  "Stochastic Gradient Descent": "Stochastic Gradient Descent (SGD) is an iterative optimization method commonly used in machine learningto update the weights of a neural network model. It calculates the gradient of the weights based on theobjective function defined to measure the error in the training phase and estimates the new set of weightsusing the gradients with a predefined step size. The SGD with a convex loss function can converge to theoptimal or sub-optimal set of weights with the correct initial settings Li & Orabona (2019). The gradualconvergence provided by gradient descent boosts optimizing the weights for any type of machine learningmodel. Assume a pair of (x, y) composed of an arbitrary input x and an output y. Given a set of weights w Wwhere W stands for the space of possible weights, a machine learning model predicts the output using a non-linear function f(x, w) and the optimal weights, w, to minimize the objective (loss) function L(y, f(x, w)):",
  "w = arg minwW L(y, f(x, w)).(1)": "Due to the highly complex and non-linear nature of machine learning models, it is impossible to find aclosed-form solution for the optimization problem given in Eq. (1) ada (2008).The stochastic gradientdescent algorithm is introduced to avoid extensive computation and give an iterative method to estimate theoptimal weights. The formula for SGD is given as:",
  "w(k + 1) = w(k) w(k)L(yi, f(xi, w)),(2)": "where w(j) represents the weights at jth step, w(k)L is the gradient of the objective function calculatedusing a single training example (xi, yi), and is determined by the step size and the learning rate. Although SGD is a simple algorithm that can be applied to various tasks, it faces challenges related totuning and scalability, which hinder its ability to converge quickly in deep learning algorithms. If the initialweights are not properly defined, or without preconditioned gradients that consider curvature information,the algorithm can get trapped in a local minima Le et al. (2011); Hinton & Salakhutdinov (2006).Toestimate the minimum of the objective function more effectively, a deeper understanding of the error surfaceis required. In addition to using gradients, the exploitation of second-order derivatives can lead to fasterconvergence. However, this requires calculating the Hessian matrix of the objective function. Calculating thesecond derivative with respect to each weight is computationally expensive and can lead to memory issues indeep networks. The Hessian matrix and its approximations are also utilized in the Normalized Least MeanSquares (NLMS)-type methods, which will be discussed in the following subsection.",
  "Normalized Least Mean Squares (NLMS)": "As pointed out above, the NLMS is widely used to estimate the weights of an adaptive filter, which is abasic linear neuron. In minimum mean square error filtering, assume u, the input to a system, is a 1 Mrandom vector with zero mean and a positive-definite covariance matrix Ru and d, the desired output of thesystem, is a scalar random variable with zero mean and a finite variance 2d. The linear estimation problemis defined as the solution ofminw E |d uw|2 ,(3)",
  "J(w) = 2d RTduw wT Rdu + wT Rduw,(5)": "where Rdu = E[du] is the cross-covariance matrix of d and u. The closed-form solution to such a problemin (3) can be found using the linear estimation theory as Ruwo = Rdu; however, it may not be possible toobtain a closed-form solution for problems with criteria other than the mean-square-error criterion. The Least Mean Squares (LMS) algorithm Widrow et al. (1960) computes the stochastic gradient and updatesthe weight vector iteratively to find a solution for the problem in Eq. (3). The weight vector can be updatedusing the following iterative process:",
  "w(j) = w(j 1) + uT (j)e(j),(6)": "where u(j) is j-th observation of the random vector u and e(j) = d(j) uT w(j 1) is the error vector attime j. The updating term is obtained as the negative of the stochastic gradient of the mean squared errorfunction defined in Eq. (4) with respect to the weights. The NLMS algorithm has been shown to achieve a better convergence rate compared to LMS by incorporatinga different step-size parameter for each component ui of the vector u Sayed (2008). The LMS algorithmcan encounter scalability issues when the input signal is large or when the step-size parameter is too large.Since the LMS algorithm uses a gradient-based approach to update the filter coefficients, and if the step-sizeparameter is too large, the filter coefficients can diverge. To address this, normalization is introduced to theupdate term:",
  "and the NLMS converges to the Wiener filter solution of the optimization problem in (3) as long as 0 < < 2Theodoridis et al. (2010); Yamada et al. (2002)": "Another interpretation of the NLMS algorithm is based on the fact that the error e(j) = d(j) uT w shouldbe minimized by selecting an appropriate weight vector w. The equation d(j) = uT w is a hyperplane inthe M dimensional weight space w RM. When the vector w(j 1) is projected onto the hyperplanee(j) = d(j) uT w, we obtain the update equation:",
  "||u(j)||22u(j).(8)": "As shown in , the error is minimized by selecting the next weight vector on the hyperplane d(j) = uT w.The orthogonal projection operation described in Eq. (8) minimizes the Euclidean distance between the vectorw(j 1) and the hyperplane d(j) = uT w Combettes (1993); Trussell & Civanlar (1984); Cetin et al. (1997;2013) The weights converge to the intersection of the hyperplanes as shown in , provided that theintersection of the hyperplanes is non-empty Combettes (1993); Cetin et al. (2013).",
  "||u(j)||1u(j),(9)": "where ||u(j)||1 is the 1 norm of the vector uj Gunay et al. (2012); Sayin et al. (2014); Arikan et al. (1994;1995); Aydin et al. (1999). The 1-norm-based method is usually more robust to outliers in input. This paper describes a new optimization algorithm inspired by Normalized LMS. It is called InputNormalized-SGD (INSGD) and utilizes the same approach as in NLMS. INSGD provides a better solu-tion to the variability issue that may cause divergence or inconsistent results and obtains better accuracyresults on benchmark datasets. By adapting the concepts of NLMS to deep learning, we can potentiallyimprove the convergence behavior and overall performance of DNN models.",
  "Related Works": "Machine learning models commonly utilize the backpropagation method for optimization Rumelhart et al.(1986); LeCun et al. (1988). Stochastic Gradient Descent (SGD) is a widely used optimization algorithm withvarious modifications in the machine learning community. While SGD can provide convergence with properinitialization, researchers have identified both positive and negative aspects of SGD and have attempted toenhance it according to their specific objectives Ruder (2016). One issue with SGD is that it updates weightsbased solely on the instantaneous gradient, which may lead to a lack of global information and oscillations.Another challenge is using a constant learning rate for all weights in the model. As training progresses, certainweights become more important than others, requiring different step sizes to ensure effective learning. In recent years, the Adaptive Gradient (AdaGrad) algorithm aims to enhance optimization by adaptivelyadjusting the learning rate for each weight based on the cumulative sum of past and current squared gradi-ents Duchi et al. (2011). This adaptive approach allows for finer adjustments of the learning rate, ensuringlarger updates for weights with smaller gradients and vice versa. AdaGrads formulas are:",
  "where represents the momentum parameter. RMSProp strikes a balance between AdaGrads adaptabilityand momentums stability, leading to improved optimization performance": "Another widely used optimization algorithm is Adaptive Moment Estimation (Adam) Kingma & Ba (2014).It builds upon the concepts of momentum and the divisor factor used in RMSProp. In addition to maintainingan exponentially weighted moving average of the squared gradients like RMSProp, Adam also incorporatesthe notion of momentum by keeping track of an exponentially weighted moving average of the gradientsthemselves. This combination of momentum and the divisor factor makes Adam more adaptive and robust",
  "w(k)L + w(k)L(17)": "where t is the parameter vector at time step t, is the base learning rate, L(t) is the gradient of theloss function with respect to t, is a small constant to prevent division by zero. The limitation is theperformance of LARS compared to other adaptive methods can vary and is influenced by factors such as thedataset, model architecture, and hyperparameters. Another adaptive learning algorithm proposed by Singh et al. (2015) presented a Layer-Specific AdaptiveLearning Rate (LSALR), where the parameters in the same layer share similar gradients. Therefore, thelearning rate of the entire layer should be similar but different layers should have different learning rates.The work is described to adjust the learning rate to escape from the saddle points and it uses the 2 normin gradients:",
  "w(k)L.(18)": "Eq. (18) allows the learning rate increases when the gradients are small. The aim is to correct the updateterm when the gradients are small in the high error low curvature saddle points. Therefore, the algorithmescapes from saddle points with a large learning rate. Similarly, it scales the learning rate to stability if thegradients are too large. The use of the log function provides the scaling under different conditions. In summary, Adam, AdaGrad, and RMSProp are optimization algorithms that address the limitations ofstandard stochastic gradient descent (SGD). These algorithms improve the convergence speed in variousscenarios. While they incorporate normalization parameters, the update terms in these algorithms are stillinput-dependent and gradually decrease over iterations. On the other hand, our approach uses a normaliza-tion term based on the layers input. its details will be presented in the following section.",
  "+ ||xk||2 xk,(20)": "where the error ei = di wi,0 xk, the update parameter = 1, and is a small number to avoid the divisionby 0. This selection of weights reduces Fk(W) and it is the same as the gradient descent with a new step sizedetermined by the length of the input vector. It is also the well-known NLMS algorithm used in adaptivefiltering and signal processing as shown in Sec 1.2, Eq. (7). The NLMS algorithm converges for 0 < < 2when the input is a wide-sense stationary random process. Inspired by the NLMS algorithm we can continueupdating the neurons of the inner layers of the network in the same manner.",
  "+ ||xk||2xk.(24)": "By employing the solution described in Eq. (24), the NLMS algorithm can be adapted to optimize the weightsin the final layer to minimize various cost functions. However, extending the INSGD algorithm to deepernetworks with multiple layers poses a challenge in its derivation. We adopt similar assumptions to thoseused in the backpropagation algorithm to derive the INSGD algorithm for each weight in a deep-learningmodel. These assumptions provide a foundation for developing the INSGD algorithm, allowing effectivelyoptimizing the weights across the layers of the deep learning model. In addition to the final layer, we incorporate the input feature maps of each layer to apply the gradientterm with normalization to the neurons using the backpropagation algorithm. This enables the optimizerto propagate the gradients and update the weights layer-wisely throughout the network. By leveraging theinformation from the input feature maps, we enhance the training process by ensuring that the gradients are",
  "+ ||xk||22,(25)": "where xk is the vector of inputs to the neuron and wk are the weights of the neurons. Note that we dropi in the weight notation that represents the neuron since the algorithm is applicable to every neuron. Forconvenience, we also change the notation for the learning rate from to . A description of how the INSGDoptimizer algorithm works for any layer of a typical deep network is shown in .",
  ": INSGD algorithm for different layers. It utilizes the input to each layer to update the weights": "Power normalization in NLMS is used to introduce a memory into the recursion so that the input poweris estimated using the input data in the remote past.In the INSGD algorithm, we introduce an inputmomentum term to estimate the power of the dataset, enabling power normalization.By replacing thedenominator term with the estimated input power, we emphasize the significance of power estimation in ouralgorithm. Furthermore, the utilization of input momentum allows capturing the norm of all the inputs.Denoted as P, the input momentum term accumulates the squared 2 norm of the input instances:",
  "Pk = Pk1 + (1 )||xk||22.(26)": "While estimating the input power is crucial, the normalization factor can grow excessively, resulting ininfinitesimally small updates. To address this, we draw inspiration from the Layer Specific Adaptive LearningRate (LSALR) approach Singh et al. (2015) and employ the logarithm function and moving-average methodto stabilize the normalization factor.However, the use of the logarithm function introduces the risk ofnegative values. If the power is too low, the function could yield a negative value, reversing the direction ofthe update. To mitigate this, we employ a function with the rectified linear unit, which avoids the issue ofnegative values. Adding a regularizer may not be sufficient to resolve this problem, hence the choice of therectified linear unit function. The function is designed as follows:",
  "Models Architecture": "In this study, we conduct experiments using six different networks to evaluate the performance of the INSGDalgorithm in the classification tasks of CIFAR-10, CIFAR-100, and ImageNet-1K. We make modificationsto the network architectures and initialization settings to assess the impact of the INSGD algorithm. Inthis study, we employ several networks for the classification tasks. Specifically, we utilize ResNet-20 Heet al. (2016), MobileNetV3 Howard et al. (2019) and Vision Transformer (ViT) Dosovitskiy et al. (2020) forCIFAR-10, WResNet-18 Zagoruyko & Komodakis (2016) for CIFAR-100, ResNet-50 and MobileNetV3 forImageNet-1K. Additionally, we design a custom CNN architecture specifically for CIFAR-10, which consists",
  "Conv18 32 323 3, 8Conv216 16 163 3, 16, stride = 2Conv332 8 83 3, 32, stride = 2Conv464 4 43 3, 64, stride = 2Dropout64 4 4p = 0.2Flatten1024-Output10Linear": "On large benchmark datasets, traditional optimization algorithms often struggle to find the optimum resultsif the learning rate is not properly chosen. In such cases, these algorithms may diverge and fail to convergeto the desired solution. However, the INSGD algorithm offers a solution by providing flexibility in learningrate selection, thereby improving the chances of reaching the global optimum. By adapting the learning ratedynamically based on the input and gradient information, INSGD enhances the optimization process andincreases the likelihood of achieving superior results on large-scale datasets.",
  "CIFAR-10 Classification": "We conduct a series of experiments using the CIFAR-10 dataset which consists of 10 classes, initially em-ploying the custom-designed CNN and ResNet-20 models for training. In certain experiments, we makemodifications to the custom network to explore the algorithms capabilities. All the experiments are re-peated 7 times with different seeds to verify the results. These experiments aim to assess the algorithmsperformance under various conditions.",
  "SGD0.192.570.11%Adam0.00191.340.01%Adagrad0.189.410.01%Adadelta0.189.240.01%INSGD-10.192.670.13%INSGD-20.192.600.11%": "In addition to showcasing the testing accuracy results, understanding the behavior of each optimizer through-out the training process is crucial. visually represents the progression of testing set errors for eachoptimizer over 200 epochs. By training the ResNet-20 model with each optimizer, we can observe the corre-sponding testing set error depicted in the plot. This visualization offers valuable insights into the convergencespeed and overall behavior of each optimizer, enabling a comprehensive analysis of their performance andeffectiveness. As depicted in , the INSGD algorithm with both norms consistently exhibits lower error rates inthe testing set, which indicates the superior performance and effectiveness of INSGD in optimizing themodels parameters and minimizing the testing set errors. The ability of INSGD to adaptively adjust thelearning rates for each individual parameter contributes to its remarkable performance in achieving lowererrors during the training process. We can also observe that the behavior of INSGD closely aligns with thatof SGD, indicating their compatibility and similarity in optimization behavior. As seen in , only SGD closely matches the performance of our optimizer in this training scenario.Therefore, we concentrate on comparing INSGD with SGD for training ResNet-20 on CIFAR-10. We explorethe impact of varying batch sizes on the normalization factor to understand how input size affects the trainingprocess. Analyzing the results across different batch sizes is crucial due to the trade-off between time andmemory usage. While larger datasets may benefit from larger batch sizes to expedite training time, it isimportant to consider the increased memory requirements. presents the accuracy results of otheralgorithms and INSGD when training the model with different batch sizes. To accommodate the increasedbatch size, we adjust the learning rate similar to the linear scaling rule described in Goyal et al. (2017). demonstrates that the INSGD optimizer maintains high performance across different batch sizes.Similar to NLMS, we anticipate that INSGD will be effective for real-time processing (online learning),highlighting its potential for superior performance in online learning scenarios. To enhance the diversity of models utilized in our experiments, we incorporate the MobileNetV3 model forcomparative analysis. A batch size of 256 is used in MobileNetV3 training. The mean and standard deviationof 7 experiments with different seeds are shown in . As depicted, the results clearly demonstrate thatthe INSGD algorithm outperforms other conventional optimization algorithms in terms of performance. This",
  "SGD0.0179.080.34%INSGD-10.0178.830.43%INSGD-20.0178.920.31%": "The toy network, used as a simplified representation of the model, plays a crucial role in evaluating theeffectiveness of our algorithm. The results obtained from training the toy network confirm the robustness ofINSGD to variability, as it shows consistent accuracy results regardless of the network architecture or thelearning rate used. Given the overlap in the experiments conducted with the custom network and ResNet-20,we opted not to replicate the ResNet-20 experiments using the toy network. This decision was made to avoidredundancy in our findings and to focus on exploring the direct impact of INSGD. We conducted experiments on a modified network architecture to explore the impact of INSGDs inputnormalization. Given that INSGD normalizes the input, we hypothesized that the batch normalization layermight become redundant. To test this hypothesis, we implemented it on a toy network, as its smaller scaleallows for easier amplification and verification of this claim and on a ResNet-20. shows the accuracyresults of networks trained without the batch normalization layer. From , it is evident that the absence of a batch normalization layer can be managed effectively bythe optimization offered by INSGD. In particular, SGD diverges during training of the toy network withoutbatch normalization, whereas INSGD achieves performance levels close to optimal settings. Normalization layers, such as Batch Normalization, are used during forward propagation and are trainablecomponents of neural networks. While they help stabilize the learning process and improve convergence bynormalizing the input to each layer, they do not directly influence the update terms of the convolutional",
  "ResNet-20SGD0.190.140.18%INSGD-10.190.550.26%INSGD-20.190.700.15%": "layers. Consequently, the use of INSGD allows for normalization during backpropagation as well, furtherstabilizing the optimization process and potentially enhancing model performance and training efficiency. Beyond CNNs, we trained a Vision Transformer (ViT) model on CIFAR-10 using different optimizers andsettings. We hypothesized that the partitioning of an image into patches could influence how the optimizerconverges during training. presents the results of the ViT under various training configurations. Tofacilitate a warm startup, we employed Cosine Annealing for learning rate scheduling and trained the modelfor 300 epochs. The base setting utilized a batch size of 256. Additionally, we evaluated the performance ofoptimizers without the layer normalization layer.",
  "%Adam0.000585.130.45%INSGD-10.0583.500.18%INSGD-20.0583.320.48%": "We observed that the INSGD optimizer can achieve convergence and results close to optimal when trainingthe Vision Transformer (ViT). However, when trained with a warm startup, Adam outperforms both INSGDand SGD. This suggests that while INSGD can effectively handle the training of ViT models and achievecompetitive results, Adam may offer superior performance under certain conditions, particularly when thelearning rate is scheduled with a warm restart. Further investigation into the interplay between optimizerchoice, warm startup strategies, and model architecture could provide deeper insights into the optimaltraining procedures for ViT models.",
  "CIFAR-100 Experiment": "We further extend our research by conducting experiments on the CIFAR-100 dataset. CIFAR-100 is a morechallenging dataset compared to CIFAR-10 as it contains 100 classes instead of 10, requiring models to havea higher level of discrimination and classification capability. The increased class diversity in CIFAR-100poses additional difficulty in achieving high accuracy and generalization performance. To ensure adequaterepresentation of each class in the training process, we opted to increase the batch size to 256 for thisparticular experiment. Before our study, Wide ResNet-18 was recognized for its convergence capabilities andsatisfactory results Zagoruyko & Komodakis (2016). In alignment with the settings outlined in the Wide",
  "SGD0.125677.220.43%93.790.43%INSGD-10.125678.150.43%94.540.43%INSGD-20.125677.890.43%93.980.43%": "The results presented in provide compelling evidence of the effectiveness of the INSGD algorithm inachieving improved convergence on complex datasets. The superior performance of INSGD, as evidenced byits higher Top-1 and Top-5 accuracy, establishes its utility in training sophisticated models on challengingdatasets. These findings underscore the algorithms capability to handle intricate data distributions andoptimize model performance, thereby showcasing its potential for advancing the state-of-the-art in deeplearning.",
  "ImageNet-1K Results": "In this section, we present the test accuracy results on the ImageNet-1K dataset. We utilize the ResNet-50model, as discussed in .2. The training process is conducted using the official PyTorch ImageNet-1Ktraining code Ima (2022). Specifically, we employ the SGD and INSGD optimizers with a weight decay of0.0001 and a momentum of 0.9.",
  "Adam0.00166.710.08%87.650.15%SGD0.175.600.14%92.670.12%INSGD-10.175.920.43%92.750.10%INSGD-20.175.900.14%92.810.04%": "The results presented in highlight the improved top-1 accuracy achieved by the INSGD algorithm onthe ImageNet-1K dataset. This improvement is particularly significant considering the scale of the dataset,demonstrating the effectiveness of INSGD in handling large and complex datasets. By leveraging the inputnormalization factor, INSGD enables the model to converge more effectively by aligning the gradient directionand appropriate magnitude. Using different seeds not only confirms but also strengthens the evidence ofimprovement brought by the INSGD algorithm. It is essential to know that Adam may perform bettertraining ResNet-50 however, it looks like similar settings with the INSGD and SGD, it doesnt achieve thesame performance. The power estimation obtained through momentum in INSGD indicates that the optimization algorithmcan benefit from considering the entire input sequence. It suggests that the algorithm can capture long-term dependencies and utilize them for better optimization performance. Furthermore, it is worth notingthat the batch size used in our experiments is relatively small compared to the number of images in thedataset. Exploring the algorithms behavior with larger batch sizes would be an interesting avenue for futureinvestigation.",
  "Conclusion": "In this paper, we proposed a novel neural network training method called INSGD, which incorporates ideasfrom the widely used NLMS algorithm in adaptive filtering. INSGD introduces a normalization step tothe weight update term that normalizes the update term using only the input vector to the neurons. Thenormalization can be performed using both the l1 and l2 norms. To evaluate the effectiveness of INSGD, we conducted experiments on various datasets using different mod-els. Notably, our algorithm consistently demonstrated improvements in testing accuracy across multipledatasets. For example, on the CIFAR-10 dataset, INSGD achieved a significant boost in accuracy comparedto traditional stochastic gradient algorithms. We observed similar positive outcomes on other datasets, suchas CIFAR-100 and ImageNet-1K, when employing different models like ResNet-20 and ResNet-50. Traditional optimization algorithms often lack flexibility when it comes to selecting hyperparameters, whichcan limit their effectiveness. However, the INSGD (Input Normalized Stochastic Gradient Descent) algorithm",
  "O. Arikan, A. Enis Cetin, and E. Erzin. Adaptive filtering for non-gaussian stable processes. IEEE SignalProcessing Letters, 1(11):163165, 1994. doi: 10.1109/97.335063": "O. Arikan, M. Belge, A.E. Cetin, and E. Erzin.Adaptive filtering approaches for non-gaussian stableprocesses. In 1995 International Conference on Acoustics, Speech, and Signal Processing, volume 2, pp.14001403 vol.2, 1995. doi: 10.1109/ICASSP.1995.480503. G. Aydin, O. Arikan, and A.E. Cetin. Robust adaptive filtering algorithms for /spl alpha/-stable randomprocesses. IEEE Transactions on Circuits and Systems II: Analog and Digital Signal Processing, 46(2):198202, 1999. doi: 10.1109/82.752953. Lon Bottou. Large-scale machine learning with stochastic gradient descent.In Proceedings of COMP-STAT2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010Keynote, Invited and Contributed Papers, pp. 177186. Springer, 2010.",
  "A Enis Cetin, Omer N Gerek, and Yasemin Yardimci. Equiripple fir filter design by the fft algorithm. IEEESignal Processing Magazine, 14(2):6064, 1997": "A Enis Cetin, Alican Bozkurt, Osman Gunay, Yusuf Hakan Habiboglu, Kivanc Kose, Ibrahim Onaran,Mohammad Tofighi, and Rasim Akin Sevimli. Projections onto convex sets (pocs) based optimization bylifting. In 2013 IEEE Global Conference on Signal and Information Processing, pp. 623623. IEEE, 2013. SC Chan and Yi Zhou. Convergence behavior of nlms algorithm for gaussian inputs: Solutions using gen-eralized abelian integral functions and step size selection. Journal of Signal Processing Systems, 59(3):255265, 2010.",
  "Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech, and time series. The handbookof brain theory and neural networks, 3361(10):1995, 1995": "Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes.In The 22nd international conference on artificial intelligence and statistics, pp. 983992. PMLR, 2019. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-tion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 34313440,2015."
}