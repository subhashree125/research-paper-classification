{
  "Abstract": "In this work we systematically review the recent advancements in software engineering withlanguage models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 relatedworks. Unlike previous works, we integrate software engineering (SE) with natural languageprocessing (NLP) by discussing the perspectives of both sides: SE applies language modelsfor development automation, while NLP adopts SE tasks for language model evaluation. Webreak down code processing models into general language models represented by the GPTfamily and specialized models that are specifically pretrained on code, often with tailoredobjectives. We discuss the relations and differences between these models, and highlightthe historical transition of code modeling from statistical models and RNNs to pretrainedTransformers and LLMs, which is exactly the same course that had been taken by NLP.We also go beyond programming and review LLMs application in other software engineer-ing activities including requirement engineering, testing, deployment, and operations in anendeavor to provide a global view of NLP in SE, and identify key challenges and poten-tial future directions in this domain. We keep the survey open and updated on GitHub at",
  "Introduction": "Language modeling has advanced remarkably in recent years with the advent of pretrained Transform-ers (Vaswani et al., 2017) such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018). As largelanguage models (LLMs) scaled to hundreds of billions of parameters and started to display early signs ofartificial general intelligence (Brown et al., 2020; Chowdhery et al., 2023; OpenAI, 2023), their applicationshave also transcended text processing. Pioneered by Codex (Chen et al., 2021b), LLMs have achieved impres-sive results in code processing, giving rise to commercial products such as GitHub Copilot1 and open-sourcemulti-billion code models such as StarCoder (Li et al., 2023i) and Code LLaMA (Rozire et al., 2023). The application of pretrained Transformers in software engineering, however, can be traced back to datesbefore decoder-only autoregressive models became dominant (Feng et al., 2020; Liu et al., 2020), and alsogoes beyond code processing (Arora et al., 2023; Feng et al., 2024; Ma et al., 2024). However, this domainis yet to witness a comprehensive review.In an attempt to bridge the gap between natural languageprocessing (NLP) community and software engineering (SE) community on the topic of language modelapplications, we undertake a panoramic survey of language models for SE in this work, covering 70+ models,40+ downstream tasks, 180+ datasets, and 900 related works.Putting the emphasis on code languagemodels, we analyze their development, discuss their differences from general language models, and highlightthe integration of code-specific features such as abstract syntax trees or data flows, as well as the latesttechniques adapted from NLP. Related to our work, we are aware of several surveys on similar topics, with three works concurrent to us (Houet al., 2023; Zheng et al., 2023b; She et al., 2023). These works, however, focus either on NLP side (Zanet al., 2023; Xu & Zhu, 2022) or SE side (Niu et al., 2023; Hou et al., 2023; Zheng et al., 2023b; She et al.,2023), and do not cover models, tasks, and challenges from the other side. For example, Zan et al. (2023)focus on LLMs for text-to-code generation, while giving little discussion of other applications in softwareengineering community. Hou et al. (2023) and She et al. (2023), in contrast, comprehensively review worksfrom SE venues such as ASE and ICSE, but cite only a handful of works from deep learning and NLP venuessuch as ACL, EMNLP, NeurIPS, and ICLR. Thus, building on these works, we endeavor to unite the perspectives from both communities, and highlightthe relationship between NLP and SE: SE applies language models to various tasks for automation, whileNLP adopts tasks from SE to evaluate language models. We observe that advanced topics from languagemodeling have been recently introduced into code processing, including instruction tuning (Honovich et al.,2023; Xu et al., 2024; Luo et al., 2023), infilling objectives (Tay et al., 2023b; Li et al., 2023i; Rozire et al.,2023), recontemplation of scaling laws (Hoffmann et al., 2022; Gunasekar et al., 2023; Li et al., 2023j),architectural improvements (Shazeer, 2019; Su et al., 2024; Dao et al., 2022), and autonomous agents (Qianet al., 2023; Hong et al., 2023), while in return SE requirements, represented by programming (Chen et al.,2021b), are providing real-world testbeds for these technologies and driving the development of LLMs forwardinto production and deployment. We believe a systematic review of these advancements would benefit bothcommunities. Furthermore, unlike the existing reviews that focus on programming-related tasks, we are alsothe first to explicitly go beyond programming and provide a global view of LLM applications in the full lifecycle of software development, covering distinct stages such as requirement engineering, deployment, andoperations. The rest of this work is organized following the taxonomy presented in .In we firstcontextualize the downstream tasks in software engineering, highlighting the focus on programming relatedtasks.Then, in we provide the preliminaries of language modeling and Transformer models,and in we discuss the plethora of LLMs that have demonstrated coding ability. In wereview the specialized and often smaller models by their architecture, with special attention on the recentapplication of infilling objectives, instruction tuning, reinforcement learning, and engineering improvements.Then, in , we discuss unique features of code that are not available to natural languages but havebeen utilized to aid code processing. In , we review the most recent integration between LLMs andsoftware development, before finally concluding this work in and highlighting the current challengesin code processing.",
  "Raw LM": "LaMDA (Thoppilan et al., 2022), PaLM (Chowdhery et al., 2023), GPT-NeoX (Black et al., 2022), BLOOM (Scao et al., 2022), LLaMA (Touvronet al., 2023a), GPT-4 (OpenAI, 2023), LLaMA 2 (Touvron et al., 2023b),Phi-1.5 (Li et al., 2023j), Baichuan 2 (Yang et al., 2023a), Qwen (Baiet al., 2023), Mistral (Jiang et al., 2023a), Gemini (Anil et al., 2023a),DeepSeek (DeepSeek-AI et al., 2024), Mixtral (Jiang et al., 2024a),Gemma (Mesnard et al., 2024), Yi (Young et al., 2024), Phi-3 (Abdin et al.,2024)...",
  "Encoder-Decoder": "PyMT5 (Clement et al., 2020), T5-code (Mas-tropaolo et al., 2021), DOBF (Lachaux et al., 2021),PLBART (Ahmad et al., 2021), CodeT5 (Wang et al.,2021f), SPT-Code (Niu et al., 2022), AlphaCode (Li et al.,2022h), NatGen (Chakraborty et al., 2022a), ERNIE-Code (Chai et al., 2023), CodeT5+ (Wang et al., 2023h),AST-T5 (Gong et al., 2024)",
  "Downstream Tasks in Software Engineering": "Over the past decade, the SE community has found applications of language models in various downstreamtasks. CodeXGLUE (Lu et al., 2021) consolidates most of the code-related tasks into a single benchmark,while HumanEval Chen et al. (2021b) brought NL-to-code synthesis into the spotlight in the NLP community,which has since become a standard task for evaluating LLMs (). However, other tasks, especiallythose not directly related to coding, have remained understudied. In this section, we first briefly introduce each of the traditional SE tasks and the application of pretrainedlanguage models in them in 2.1, and provide a comprehensive list of related works for each task. Then,we review the evaluation metrics in 2.2 and investigate program synthesis in more detail in 2.3. Lastly, we",
  "Published in Transactions on Machine Learning Research (09/2024)": ": Benchmarks for code retrieval, code reasoning, type inference, and clone detection/code search,. JSis short for JavaScript.These benchmarks include a large number of automatically constructed samples,and a small set of human-annotated samples.These are general-domain reasoning benchmarks, and onlya subset therein concern programming, algorithms, and other topics related to computer science.Theseare project counts (or, in the case of Cassano et al. (2023c), file counts).Yee & Guha (2023) proposeto measure project-level type check rate instead of type prediction accuracy for TypeScript.These are21K/24K Java/Python methods for 576 programming problems.",
  "Downstream Tasks in SE": "The custom in software engineering is to categorize downstream tasks according to their input/output modal-ity, such as NL-to-PL (also referred to as text-to-code) tasks and PL-to-NL (i.e. code-to-text) tasks (Luet al., 2021). However, as we are dedicated to the full life cycle of software development, we adopt a differenttaxonomy in this work, and classify downstream tasks according to the stages in software development:requirement engineering (2.1.1), development (2.1.2), testing and analysis (2.1.3), deployment and opera-tions (2.1.4), and lastly several novel tasks recently proposed for evaluating LLMs (2.1.5). We note thatthis taxonomy is interleaved with the understanding-generation dichotomy in NLP, since each category maycontain both understanding and generation tasks, as discussed in 2.1.6.",
  "Requirement engineering refers to the specification, analysis, and validation of software requirements, soft-ware modeling, and UI/UX design. Related works are listed in": "- Requirement analysis refers to the process of extracting, summarizing, classifying, and validating softwarerequirements. Most early works in this field rely on statistical NLP technologies such as POS (Part-of-Speech) tagging and dependency parsing to extract actions from requirements, while more recent worksutilize LLMs to directly classify, summarize, or extract requirement clauses. Zhao et al. (2020) provide afine-grained survey on requirement analysis. - UI/UX Design, short for User Interface and User Experience design, is a fundamental step in softwaredevelopment. While many works on UI design utilize computer vision technologies for layout design (Chenget al., 2023; Kulkarni et al., 2023; Feng et al., 2023b), we only focus on the intersection of this task and NLPtechnologies, such as using language models to generate markup languages. - Model generation aims to generate software models in modeling languages such as UML (Unified ModelingLanguage). Abdelnabi et al. (2021a) and Ahmed et al. (2022) provide comprehensive reviews on this taskbefore the rise of LLMs.",
  "RequirementAnalysis": "ARSENAL (Ghosh et al., 2016),CAR (Ferrari et al., 2014),RETA (Arora et al., 2015a),NARCIA (Arora et al., 2015b),FENL (Bakar et al., 2016),SNACC (Zhang & El-Gohary,2017), REGICE (Arora et al.,2017), AUR-BoW (Lu & Liang,2017), ALERTme (Guzman et al.,2017), SAFE (Johann et al., 2017),OCLgen (Wang et al., 2018b),SEMIOS (Mezghani et al., 2018),Tiwari et al. (2019) Sainani et al. (2020), RE-BERT (de Arajo & Marcacini,2021), White et al. (2023), Ro-driguez et al. (2023), Jain et al.(2023), Arora et al. (2023), Ro-nanki et al. (2023), CORE-QQA (Abualhaija et al., 2023),CompliAT (Arora et al., 2024)",
  "pix2code (Bel-tramelli,2018), Aroluet al. (2019),sketch2code (Robin-son, 2019)": "PixelHelp (Li et al., 2020c),HTLM (Aghajanyan et al., 2022),Screen2Words (Wang et al., 2021b),MarkupLM (Li et al., 2022b),HTMLBERT (Xie et al., 2021a),DOM-LM (Deng et al., 2022b),WebFormer (Wang et al., 2022b),PromptMaker (Jiang et al., 2022),Wang et al. (2023a), Gur et al.(2023), MenuCraft (Kargaranet al., 2023), Wang et al. (2023d),GPTDroid (Liu et al., 2023m),ViCT (Soselia et al., 2023), UIgrammar (Lu et al., 2023c),Canvil (Feng et al., 2024)",
  "Code Search": "- NL-to-code search, also referred to as text-to-code search, aims to retrieve relevant code given naturallanguage queries, or to mine parallel text-code pairs from an unannotated corpus.This task is usuallyperformed by computing a similarity metric between the embedding of query and candidate code, andthe contextual embeddings produced by bidirectional language models - such as BERT - has proven to beextremely helpful. Grazia & Pradel (2023) and Xie et al. (2023a) provide comprehensive reviews on thistopic. - Code-to-code search is a similar task where the input is an existing code snippet, often in a differentprogramming language from the target. Code-to-code search can be reformulated as finding clones of thequery in the pool of targets, and is thus equivalent to clone detection to some extent. - API mining refers to the process of finding similar APIs in different libraries, potentially in differentprogramming languages. API mining is traditionally tackled by computing similarity metrics between sourceand target APIs using information retrieval models, but as generative models become ever more capable, itis also worth exploring to directly generate the target API as a sequence-to-sequence task. Another closelyrelated task is idiom mining (Allamanis & Sutton, 2014), where the objective is to discover commonly usedcode patterns, which exposes the potential need for new APIs (Sivaraman et al., 2022).",
  "Lu et al. (2015),CodeHow (Lvet al., 2015),RACS (Li et al.,2016), Code-Matcher (Liu et al.,2022a)": "CODE-NN (Iyer et al.,2016), DeepCS (Gu et al.,2018), Yin et al. (2018),NCS (Sachdev et al.,2018), UNIF (Cambroneroet al., 2019), HECS (Liet al., 2020b), CARLCS-CNN (Shuai et al., 2020),DGMS (Ling et al., 2021),NJACS (Hu et al., 2020),TabCS (Xu et al., 2021a),GraphSearchNet (Liu et al.,2023i), TranCS (Sun et al.,2022) TranS3 (Wang et al., 2020e), Hey-man & Cutsem (2020), Corder (Buiet al., 2021b), SAN-CS (Fang et al.,2021), SST (Gu et al., 2021), Mu-CoS (Du et al., 2021), MEM (Salzaet al., 2023), CDCS (Chai et al., 2022),CodeRetriever (Li et al., 2022e), Co-CoSoDa (Shi et al., 2023b), Li et al.(2022a), CCT-LM (Sorokin et al., 2023)",
  "Euphony (Leeet al., 2018),Neo (Feng et al.,2018)": "LPN (Ling et al., 2016),NSPS (Parisotto et al., 2017),DeepCoder (Balog et al.,2017), RobustFill (Devlinet al., 2017a), (Yin & Neu-big, 2017), ASN (Rabinovichet al., 2017), NGDS (Kalyanet al., 2018), Bunel et al.(2018), ReCode (Hayatiet al., 2018), AutoPan-das (Bavishi et al., 2019),Wei et al. (2019), Plot-Coder (Chen et al., 2021c),Huang et al. (2022a) TreeGen (Sun et al., 2020), RED-CODER (Parvez et al., 2021), Jig-saw (Jain et al., 2022), JuPyT5 (Chan-del et al., 2022), CodeT (Chen et al.,2023a), TiCoder (Lahiri et al., 2022),AceCoder (Li et al., 2023e), Self-Debugging (Chen et al., 2023f),ClarifyGPT (Mu et al., 2023b),MANGO (Chen et al., 2024)",
  "Text-to-SQL": "Seq2SQL (Zhong et al.,2017), SQLNet (Xu et al.,2017), Suhr et al. (2018),TypeSQL (Yu et al., 2018a),Coarse2Fine (Dong & Lapata,2018), Finegan-Dollak et al.(2018), SyntaxSQLNet (Yuet al., 2018b), GNN (Boginet al., 2019), TREQS (Wanget al., 2020b) SQLova (Hwang et al., 2019), IRNet (Guo et al.,2019), Zhang et al. (2019b), RAT-SQL (Wanget al., 2020a), Bertrand-DR (Kelkar et al., 2020),RYANSQL (Choi et al., 2021), TaBERT (Yinet al., 2020), Photon (Zeng et al., 2020), Hy-draNet (Lyu et al., 2020), GAZP (Zhong et al.,2020), GraPPa (Yu et al., 2021), SmBoP (Ru-bin & Berant, 2021), NQG-T5 (Shaw et al.,2021), StruG (Deng et al., 2021), SLSQL (Leiet al., 2020), GAP (Shi et al., 2021), Wang et al.(2021a), GP (Zhao et al., 2021), LGESQL (Caoet al., 2021), Picard (Scholak et al., 2021), H-NeurSyn (Yang et al., 2021), UnifiedSKG (Xieet al., 2022b), CodexDB (Trummer, 2022),T5QL (Arcadinho et al., 2022), TKK (Gaoet al., 2022), Graphix-T5 (Li et al., 2023f),RESDSQL (Li et al., 2023c), Liu et al. (2023a),DIN-SQL (Pourreza & Rafiei, 2023), Chang &Fosler-Lussier (2023), Nan et al. (2023), SQL-PaLM (Sun et al., 2023a), Guo et al. (2023a),DAIL-SQL (Gao et al., 2023a)",
  ": NLP applications in the development stage of SE": "of retrieval models. Statistical machine translation (SMT) and neural machine translation (NMT) modelshave been widely adopted for this task, often with enhanced decoders that leverage the unique grammaticalrules of programming languages (Yin & Neubig, 2017; Rabinovich et al., 2017). Pretrained language modelsbased on Transformer architecture, however, changed the game by directly generating the source code in",
  "the autoregressive language modeling style, even without task-specific finetuning (Chen et al., 2021b). Wediscuss this task in more detail in .3": "- Code completion aims to complete a piece of code given its prefix, and remains to date one of the mostpopular applications of code language models in IDEs. This is essentially language modeling applied to code(which we dub code modeling), and related technologies have been progressively introduced: n-gram, RNN,and Transformer. However, due to the structured nature of programming languages, many early works foundgrammar-aided statistical models to perform better (Bielik et al., 2016; Hellendoorn & Devanbu, 2017), andneural models only became dominant after 2018. - Code infilling is another recently proposed task, after fill-in-the-middle pretraining (Bavarian et al., 2022)became popular. It is a generalization of code completion, where not only the left context but also the rightcontext is given. - Text-to-SQL is a special case of program synthesis, where the model is tasked to generate SQL commandsfrom natural language queries. It has been a topic of special interest in the NLP community (as can beseen from ), and one hypothesis for this is that SQLs declarative nature and restricted grammarmake it easier to optimize compared with imperative programming languages such as C and Python (Marcuset al., 2019; 2020). We refer to Kumar et al. (2022); Deng et al. (2022a); Qin et al. (2022a); Katsogiannis-Meimarakis & Koutrika (2023) for surveys on this topic.",
  "Code Editing": "- Code translation aims to translate a piece of code (usually a function or method) into another programminglanguage. The relation between code translation and cross-lingual code search is similar to the one betweenprogram synthesis and text-to-code retrieval, and SMT/MNT models have also been widely applied to thistask. One of the important applications of code translation is migrating old projects written in obsoletelanguages. However, we are yet to witness such applications at scale in the LLM era, as the context windowof even the most powerful language models are quite limited in the face of such projects. Malyala et al.(2023) provide a short survey on this task from the SE perspective. - Program repair, also known as bug fix, aims to fix a piece of buggy code. Like code translation, it is atraditional sequence-to-sequence generation task, and surveys are abundant on this topic (Gazzola et al.,2018; Monperrus, 2018; Zhong et al., 2022; Zhang et al., 2023d; Huang et al., 2023a).",
  "Code Suggestion": "- Type prediction aims to predict the type of variables in dynamic programming languages such as Pythonand JavaScript. It has been used as a pretraining objective for code language models (Wang et al., 2022e),where it is often simplified as a binary tagging task to predict which tokens in the code are identifiers (Wanget al., 2021d;f). - Identifier prediction is the task of predicting identifier names in the code. As these names are deemedto contain important semantic information, this task has been utilized for code summarization (Allamaniset al., 2016b), as well as pretraining code models (Wang et al., 2021f; Niu et al., 2022). A special case ofidentifier prediction is method name prediction. - Cloze test is a recently proposed task for code understanding, after the rise of BERT-style pretraining. Dueto the unique semantics of programming languages, several keywords are often selected for this test, such asmin and max (Feng et al., 2020).",
  "Code Explanation": "- Code summarization, comment generation, and code documentation all aim to generate explanations forcode to facilitate understanding of the code, but with slightly different emphasis.Code summarizationgenerates a natural language summary of the given code, while comment generation generates comments.These two terms are often used interchangeably in the literature, and their intended audience is usuallydevelopers. Code documentation, on the other hand, is more structured, includes more detailed information",
  "Prophet (Long &Rinard, 2016),TBar (Liu et al.,2019b), Refac-tory (Hu et al.,2019), PyTER (Oh& Oh, 2022)": "sk_p (Pu et al., 2016), Deep-Fix (Gupta et al., 2017),SSC (Devlin et al., 2017b),SynFix (Bhatia et al., 2018),Codit (Chakraborty et al.,2022b), Tufano et al. (2019b),Sequencer (Chen et al.,2021e), Tufano et al. (2019a),Vasic et al. (2019), DrRe-pair (Yasunaga & Liang,2020), CoCoNuT (Lutel-lier et al., 2020), DL-Fix (Li et al., 2020d), Re-view4Repair (Huq et al.,2022), DEAR (Li et al.,2022g) CURE (Jiang et al., 2021a), DeepDe-bug (Drain et al., 2021), BIFI (Ya-sunaga & Liang, 2021), Recoder (Zhuet al., 2021), TFix (Berabi et al.,2021), Modit (Chakraborty & Ray,2021), Fan et al. (2023), AlphaRe-pair (Xia & Zhang, 2022), RING (Joshiet al., 2023), Xia et al. (2023b), Vul-Repair (Fu et al., 2022), CodeT5-DLR (Bui et al., 2022), ConversationalAPR (Xia & Zhang, 2023), Jiang et al.(2023b), Paul et al. (2023), Cao et al.(2023), TypeFix (Peng et al., 2023b),Maniple (Parasaram et al., 2024)",
  "Code Sum-marization": "CODE-NN (Iyer et al., 2016),TL-CodeSum (Hu et al.,2018b), Code2Seq (Alonet al., 2019a), Fernan-des et al. (2019), Wanet al. (2018), AST-AttendGRU (LeClair et al.,2019), Wei et al. (2019),Haque et al. (2020), DMA-COS (Xie et al., 2021b),Bansal et al. (2021), CoCo-SUM (Wang et al., 2021e),MLCS (Zhou et al., 2023c) TranS3 (Wang et al., 2020e), Ah-mad et al. (2020), Corder (Bui et al.,2021b), SiT (Wu et al., 2021), SG-Trans (Gao et al., 2023c), Codex-D (Chen et al., 2021b), M2TS (Gao& Lyu, 2022), AST-Trans (Tang et al.,2022), CoSS (Shi et al., 2023a), Sunet al. (2023b), Yuan et al. (2023a),use-seq (Su & McMillan, 2023a), Su& McMillan (2023b), CSA-Trans (Oh& Yoo, 2024), Haldar & Hockenmaier(2024)",
  "Testing": "- Unit test generation aims to generate unit tests for a given program. Prior to the rise of Codex and othercode LLMs, almost all works in this area employed non-neural methods (see ). In the age of LLMs,however, this task is ever more important, as research has shown that the current unit tests for evaluatingLLMs program synthesis capability may be insufficient (Liu et al., 2023f). Wang et al. (2023b) provide acomprehensive survey on software testing with LLMs. - Assertion generation is a subtask of unit testing. Given a program and a partial unit test, this task aimsto generate assertions (also known as oracles in software engineering) within the unit test. This task hasgenerally went unnoticed by the NLP community, as the program synthesis task used for evaluating LLMsoften concern standalone, competition-style methods, for which the simple assertion of the equality betweenprogram output and expected answer suffices. - Mutant generation aims to generate mutants of a given program for the purpose of mutation testing, andrelates closely to unit test generation and assertion generation. A mutant that is not detected by a given setof unit tests and assertions indicates that either additional test cases or better assertions are required (Fraser& Arcuri, 2011). Recently, masking out tokens in the source code and sampling them from the output ofa masked language model has become a common method for this task. Ojdanic et al. (2021; 2023) giveempirical comparisons between different mutation methods. - Fuzzing is another software testing task, where the objective is to generate a large set of inputs covering asmany corner cases as possible. While many recent works on fuzzing target deep learning libraries, few haveutilized language models to conduct this process (see ).",
  "Analysis": "- Defect detection, or vulnerability detection, predicts whether the input code is buggy or not, and is astandard single-sentence classification task. Nong et al. (2023); Steenhoek et al. (2023a); Bi et al. (2023);Harzevili et al. (2023); Zhou et al. (2024) provide surveys on this task. - Malware detection is a similar task to defect detection.However, malware differs from other types ofvulnerable code in that the bugs therein are malicious - i.e. they are intentionally injected. We refer to Sahin& Bahtiyar (2020) and Gopinath & Sethuraman (2023) for reviews on non-Transformer based methods forthis task. - Clone detection predicts whether or not two pieces of code are clones of each other. In software engineeringthere exist four types of code clones, and the most challenging type to identify is semantic clones, i.e.syntactically dissimilar code that have the same functionality. As this task can be viewed as a two-sentenceclassification task, BERT-style language models have been widely applied to it. Svajlenko & Roy (2020) andZhang & Sakurai (2021) provide comprehensive reviews on non-deep-learning based methods for this task. - Code classification aims to predict the functionality of a piece of code within a predefined set of labels.A very similar task is author identification, which predicts the author of the input code. Both tasks arestandard single-sentence classification tasks, and traditional machine learning methods have been widelyadopted in them (Kalgutkar et al., 2019), while pretrained language models have seen almost no application.",
  "EvoSuite (Fraser & Arcuri, 2011),EvoSuiteR (Shamshiri, 2015), Dy-naMOSA (Panichella et al., 2018),LambdaTester (Selakovic et al.,2018), TSE (Shimmi & Rahimi,2022), Nessie (Arteca et al., 2022)": "AthenaTest (Tufano et al.,2021a), FSLM (Barei et al.,2022), TestPilot (Schfer et al.,2023), A3Test (Alagarsamyet al., 2023), TeCo (Nie et al.,2023), CodaMosa (Lemieux et al.,2023), ChatTester (Yuan et al.,2023b), ChatUniTest (Xie et al.,2023b), Tang et al. (2023a), PBT-GPT (Vikram et al., 2023), Shinet al. (2023), MuTAP (Dakhelet al., 2023), Zhang et al. (2023f),RLSQM (Steenhoek et al., 2023b),Guilherme & Vincenzi (2023),CoverUp (Pizzorno & Berger,2024), TELPA (Yang et al., 2024a)",
  "Fuzzing": "SymFuzz (Cha et al., 2015),AFLFast (Bhme et al., 2019),FairFuzz (Lemieux & Sen,2018), AFLGo (Bhme et al.,2017), Angora (Chen & Chen,2018), TensorFuzz (Odena et al.,2019), Audee (Guo et al., 2020),LEMON (Wang et al., 2020f),DocTer (Xie et al., 2022a), Free-Fuzz (Wei et al., 2022a), Spec-Fuzzer (Molina et al., 2022),Muffin (Gu et al., 2022), Deep-REL (Deng et al., 2022c),NNSmith (Liu et al., 2023e),Fuzz (Yang et al., 2023c)",
  "Ray et al. (2016),Bugram (Wanget al., 2016a),NAR-Miner (Bianet al., 2018),(Pearce et al.,2022)": "Wang et al. (2016b), VulDeeP-ecker (Li et al., 2018b), Lin et al.(2018a), DeepBugs (Pradel &Sen, 2018), Russell et al. (2018),SySeVR (Li et al., 2022j), De-vign (Zhou et al., 2019), Liet al. (2019c), Lin et al. (2021),VulDeeLocator (Li et al., 2022i),VulDeePecker (Zou et al., 2021),ReVeal (Chakraborty et al.,2022c), BugLab (Allamaniset al., 2021), IVDetect (Li et al.,2021a), ReGVD (Nguyen et al.,2022) GREAT (Hellendoorn et al., 2020),VulBERTa (Hanif & Maffeis, 2022),LineVul (Fu & Tantithamthavorn,2022), DeepDevVuln (Chan et al.,2023), Yuan et al. (2023a), (Zhanget al., 2023a), CausalVul (Rahmanet al., 2023), Gao et al. (2023d),Ullah et al. (2023), LLM4Vuln (Sunet al., 2024), MuCoLD (Mao et al.,2024), Steenhoek et al. (2024)",
  "DeepLog (Du et al., 2017), Luet al. (2018), LogAnomaly (Menget al., 2019), LogRobust (Zhanget al., 2019c), LogDTL (Nguyenet al., 2021), UniParser (Liuet al., 2022e)": "NuLog (Nedelkoski et al., 2020a),Logsy (Nedelkoski et al., 2020b),LogBERT (Guo et al., 2021b),NeuralLog (Le & Zhang, 2021),LogStamp (Tao et al., 2022),LogPPT (Le & Zhang, 2023a),Le & Zhang (2023b), Semlog (Yuet al., 2023b), LogPrompt (Liuet al., 2023k), LogGPT (Qi et al.,2023), Mudgal & Wouhaybi (2023),LILAC (Jiang et al., 2023d), LLM-Parser (Ma et al., 2024)",
  ": NLP applications in the deployment & operations stage of SE": "Compiler optimization is the task of optimizing compiled code to increase its efficiency. Many early worksapplied reinforcement learning and other machine learning technologies to this task by optimizing taskscheduling or pass ordering (Wang & OBoyle, 2018; Leather & Cummins, 2020), and it is only recentlythat researchers started to view it as a sequence-to-sequence generation task with the help of powerfulLLMs (Cummins et al., 2023). - Decompilation is the reverse process of compilation, and an important topic in reverse engineering. In thistask a model takes a compiled program - represented in assembly code or binary machine code - and aimsto output the original source program in high-level languages such as C. - Obfuscation refers to the process of renaming identifiers (e.g. variables, methods, and classes), for exampleto generic names like var_1, var_2 or x, y. It is an important technique in virus detection, intellectualproperty protection, and code size reduction (Collberg & Thomborson, 2002; Murad et al., 2010; Vasilescuet al., 2017). Deobfuscation refers to the reverse process, where meaningful identifier names are recoveredfrom obfuscated programs.Obfuscation can be easily achieved statically, but deobfuscation has been asubject of more interest in recent years. It plays a significant role in decompiling, and has also been adoptedas a pretraining objective for code language models (Lachaux et al., 2021; Ding et al., 2022a; Liu et al.,2022d). - Log analysis aims to analyze the system logs produced by software products, for example parsing logs intostructured templates or finding anomalies from raw logs. Zhu et al. (2019) provide a survey on traditionalmethods for this task up to 2018, and Chen et al. (2021d) give an empirical comparison between neural",
  "Operations": "- Code review aims to automate the process of peer code review, and includes many subtasks, such as reviewnecessity prediction, review comment generation, code refinement, and review decision prediction. - Commit message generation aims to automatically generate commit messages for code changes. This tasktakes the code before and after change as input, and output the description for the change. This can beviewed as the dual task of program repair, as many code changes and their accompanying commit messagesconcern bug fixing. Tao et al. (2021) provide a survey on methods and datasets for this task up to 2021.",
  "LLM Evaluation": "Apart from the previously covered stages in SE, we observe that programming-related evaluation of LLMshas been gaining momentum in both the NLP and the SE community since the release of Codex. Thus, wealso list several novel tasks that appeared recently for this purpose. We discuss this topic in more detail in.",
  ": Programming-related evaluation of LLMs": "- Code reasoning requires a model to reason about code or algorithms, and answer related questions which arewritten in multiple-choice format or free-form QA format, which may range from conceptual understanding tonumerical calculation and complexity analysis. It often comes as a subset of composite evaluation benchmarkssuch as MMLU (Hendrycks et al., 2021b). - Coding for reasoning is a special case of reasoning where LLMs solve complex reasoning problems bygenerating code that will be executed by external interpreters. This task abstracts the reasoning processfrom numerical calculations, and is thus of special interest in evaluating LLMs. More recently research hasalso shown that asking the LLM itself to simulate the execution of generated code can also help its reasoningprocess.",
  "NLP Point-of-View": "Unlike software engineering, evaluation tasks in NLP are generally categorized into understanding and gen-eration.The former, represented by GLUE (Wang et al., 2018a) and SuperGLUE (Wang et al., 2019),emphasizes the comprehension of input text, and is typically formalized as classification, regression, se-quence tagging, or span extraction. The later, on the other hand, involves autoregressive generation of text,such as machine translation and summarization. Among the previously listed tasks, code synthesis, code translation, code repair, deobfuscation, unit testgeneration, assertion generation, mutant generation, code summarization, code review, identifier prediction,and commit message geneartion are sequence-to-sequence generation tasks. Formally, each instance of thesetasks has a source sequence x (e.g. a piece of source code) and a target sequence y (e.g. its correspondingsummarization), and the language model is tasked to maximize the conditional probability given by (5),where can be either a decoder-only model or an encoder-decoder model. In the former case, x and y areconcatenated. In the later case, x is processed by the encoder and y is processed by the decoder. Code completion and code infilling are also generation tasks, but differ from sequence-to-sequence tasks wherethe input and output are related by different sequences. In these two tasks, the target is a continuation orinfill of the input. They correlate closely to the language modeling objectives given in Equation (3) and (5).Similarly, cloze test takes the same form as Equation (4) but is usually considered an understanding task,as its output is usually a single token and does not involve autoregressive generation. Defect detection, malware detection, clone detection, code classification, and author identification are se-quence classification tasks. In these tasks, a set of labels Y is defined over the input, and each instance isassigned a label y Y (e.g. for defect detection Y = {0, 1}, while for author identification a possible Y is{Alice, Bob, John, others}). The model is then tasked to maximize",
  "i=1p(yi|x).(2)": "The last two tasks - code retrieval and code search - also belong to understanding tasks. In these tasks, eachsource sequence x is paired with a positive target sequence y and a set of negative targets y {y1, , yk}.The models task is to find a similarity metric s such that s(x, y) is larger than s(x, y).",
  "Code LLMs for Low-Resource, Low-Level, and Domain-Specific Languages": "In NLP, human languages are categorized into high-, middle-, and low-resource languages based on theamount of available data in each language. High-resource languages such as English are extensively studied,while low-resource languages such as Swahili and Yoruba often rely on transfer learning from other languagesto improve performance due to data scarcity (Conneau et al., 2020; Xue et al., 2021; Zhang et al., 2023g). This phenomenon also exists in code modeling, as most works listed in -8 target the most popularprogramming languages such as C, Java, and Python. However, a major difference between NLP and SEis that in NLP, low-resource languages are often spoken by few people or even endangered, while in SElow-resource languages are often designed for specific domains and purposes, and thus have an active usercommunity. Verilog, a hardware description language, is one such example, for which code modeling research",
  "is quite abundant (Thakur et al., 2023a;b; Lu et al., 2023b; Liu et al., 2023g; Tsai et al., 2023; Thorat et al.,2023; Liu et al., 2023h; Nadimi & Zheng, 2024)": "From a different perspective, most of these popular languages are imperative languages, while few works havestudied NLP and LLM applications in declarative or functional languages, except those concerning SQL andone recent work on Haskell (van Dam et al., 2024). However, research has shown that declarative languagesare more aligned for optimization since they describe the what and not how in programming (Marcuset al., 2019; 2020), thus they are worth more attention in the future research. Similar to NLP, many works in code modeling have studied the transfer of model capability across languages.Chen et al. (2022), for example, investigate the performance of multilingual language models on Ruby(which is one of the six languages in the popular CodeSearchNet pertaining corpus (Husain et al., 2019),but relatively low-resourced compared with the other five), finding that multilingual models have lowerperformance-to-time ratio compared with monolingual ones. Cassano et al. (2023a), on the other hand,propose MutiPL-T, an approach for translating high-resource training data into low-resource languages forfine-tuning. However, such research is yet scarce compared with the NLP community, but with the recentadvent of colossal, multilingual datasets such as The Stack v2 (Lozhkov et al., 2024) we expect to see moreof it in the future.",
  "Evaluation Metrics": "Of the tasks mentioned in .1, the understanding tasks are similar in form to natural languageunderstanding tasks (Wang et al., 2018a; 2019) and evaluated likewise by metrics such as accuracy, F1 andMean Reciprocal Rank (MRR), while short generation tasks such as identifier prediction is also evaluatedby accuracy of exact matches. Code-to-text tasks are evaluated with common metrics for text generationsuch as BLEU (Papineni et al., 2002). Evaluation of tasks involving code generation, on the other hand, is more complicated. Most early worksevaluate syntactical correctness, i.e. the percentage of generations that can be successfully parsed. Chenet al. (2018) argue against such metrics and suggest reference match instead, which is the percentage ofgenerations that are exactly the same as the references. Ren et al. (2020) propose CodeBLUE, a variant ofBLEU that takes code syntax and semantics into account by evaluating the overlap of abstract syntax tree(AST) and data flow. As code generation models became more capable over the years, however, these metrics based on content-overlap have been found to be inadequate (Rozire et al., 2020; Hendrycks et al., 2021a; Austin et al., 2021),since functionally equivalent snippets of code can differ dramatically in their lexical forms. Consequently,researchers have turned their attention to functional correctness. One popular example of such metrics ispass@k, proposed by Kulal et al. (2019) and refined by Chen et al. (2021b), which is an unbiased estimatorof the models chance in passing all unit tests of a program with any of k generated samples. This metriccan be generalized to passn@k (Li et al., 2022h), which limits the number of model submissions to n butallows filtering by unit tests given in the input from k samples.",
  "Program Synthesis": "While dozens of evaluation tasks exist in software engineering, they have generally stayed out of the focusof the NLP community until very recently. The only exception is program synthesis, which has become astandard evaluation task for LLMs since the advent of HumanEval in 2021. Looking back at this task, weidentify four changes in program synthesis over the years: shift of coding paradigms (from example-based tointention-based), generalization in languages (from domain-specific languages to general-purpose languages),simplification of model architectures (from grammar-guided decoders to general-purpose language models),and application of execution-based feedback. Many of the early methods for program synthesis are example-based (Menon et al., 2013), which meansthey induce programs from input-output examples, often in domain-specific languages (DSLs) such as Flash-",
  "Fill (Devlin et al., 2017a) and Karel3 (Bunel et al., 2018), as these languages are usually simple in syntaxand structure": "As code generation models became more capable over the years, researchers started to pay attention toprogram synthesis in general-purpose programming languages as well. Hearthstone (Ling et al., 2016) andCONCODE (Iyer et al., 2018) are two of the early datasets, representing Python and Java respectively. Eachexample in Hearthstone is the description of a card in the game and its corresponding class implementation,while examples in CONCODE are simply Java methods paired with their natural-language documentationcrawled from public GitHub repositories. Synthesizing programs from their corresponding natural languagedescriptions has since then become a standard practice in program synthesis, and has led to some of themost widely used benchmarks, such as HumanEval (Chen et al., 2021b), which has even been translatedinto multiple languages (Cassano et al., 2023b; Zheng et al., 2023a; Muennighoff et al., 2024). Some recentbenchmarks use general-purpose languages but focus on specific domains, such as data science (Bavishi et al.,2019; Lai et al., 2023) or Jupyter notebooks (Agashe et al., 2019), while several math reasoning benchmarkshave also been converted to programming tasks, including MathQA-Python (Amini et al., 2019; Austin et al.,2021) and GSM8K-Python (Cobbe et al., 2021; Chowdhery et al., 2023; Wang et al., 2023h). Many early works argue that simply treating program synthesis as a text generation task does not utilizethe underlying syntax of programming languages, and thus often use syntax-enhanced decoders to inject thetarget syntax as prior knowledge (Yin & Neubig, 2017). LLMs, however, have demonstrated that pretrainedlanguage models are capable of generating syntactically correct programs without loss of generality. Underthis setting, researchers start to execute the generated programs and provide feedback to the generationmodel to inject the prior knowledge of code instead. This has recently led to the popularity of interactivecoding, which we discuss in more detail in .1.",
  "Repository-Level Coding": "Most tasks discussed in .1 are limited to a single file or even a single function, as cross-file codemodeling poses challenges that are beyond the capability of most existing language models.Recently,however, position interpolation techniques (Chen et al., 2023d; Rozire et al., 2023; Peng et al., 2023a) haveextended the context window of LLMs to hundreds of thousands of tokens, making it possible to contextualizecoding activities within entire repositories. Several works have studied code completion (Shrivastava et al.,2023b; Ding et al., 2022b; Zhang et al., 2023b; Shrivastava et al., 2023a; Phan et al., 2024; Wu et al., 2024b)and generation (Liao et al., 2023; Zan et al., 2024) leveraging repository-level context, and correspondingbenchmarks have been proposed Liu et al. (2023j); Ding et al. (2023); Li et al. (2024a). Bairi et al. (2023)investigate the more challenging tasks of repository-level API migration and temporal editing, while Jimenezet al. (2023) introduce a related benchmark, SWE-bench.",
  "Language Modeling Preliminaries": "As code is ultimately a subset of natural languages, language models have been extensively used to tacklethe tasks listed in . Before diving into the language models themselves, we first briefly reviewthe preliminaries of Transformer-based language modeling in this section following the common choices oftraining objectives, and also some implementation designs.",
  "Causal Language Modeling": "Unidirectional language models (also known as causal language models4) factor the probability of a sentenceinto the product of each tokens conditional probability with the chain rule. A piece of input text x = 3FlashFill is used in Microsoft Excel for string transformation. Karel is a simple programming language for educationalpurpose.4The training objective of such language models is Causal Language Modeling (CLM), but also referred to as Next TokenPrediction.",
  "i=1p(xi|x1:i1),(3)": "where x1:i1 is a shorthand for tokens before xi in the input, and is the parameters of the model. WithTransformer decoders such as GPT (Radford et al., 2018; 2019; Brown et al., 2020) and LLaMA (Touvronet al., 2023a;b), the conditional probability in (3) is modeled by adding an attention mask to the attentionmatrix of each Transformer block, ensuring that xi can only attend to previous tokens. During training, thecross entropy loss on all tokens in the input is calculated in parallel, while at inference time each new tokenis generated autoregressively. For further details about the Transformer architecture we refer to Vaswaniet al. (2017).",
  "Masked Language Modeling": "Unlike causal language models, bidirectional language models are trained to acquire a better contextualrepresentation of text rather than generating text autoregressively.In the vanilla Transformer, the en-coder part is allowed to attend to a tokens left as well as right context for this purpose. BERT (Devlinet al., 2019) takes one step further and pretrains only a Transformer encoder. A set M of randomly cho-sen tokens in the input are replaced by a special token [MASK] to obtain a noisy input x, for example[[CLS], x1, [MASK], x3, [MASK], x5, [EOS]]5, and the model is trained to recover the original tokens by maxi-mizing",
  "mMp(m|x).(4)": "While this objective requires the model to have a deep understanding of the input text to reconstruct it,it suffers from low training efficiency, since only a small set of tokens (usually 15%) are masked (and thustrained on). To address this issue, Clark et al. (2020) propose ELECTRA, which is trained to discriminatewhether or not each token in the input has been replaced by a BERT-like model instead, thereby computingloss on all input tokens.",
  "Denoising Objectives": "GPT-style causal LM and BERT-style bidirectional LM each has its own strengths and weaknesses. WhileGPT can be used for autoregressive generation, it lacks a bidirectional representation of input text, and isthus unsuitable for sequence-to-sequence (seq2seq) generation tasks such as translation and summarization.BERT, on the other hand, can produce bidirectional representations, but is pretrained only for mask filling,not generation. The vanilla Transformer encoder-decoder architecture combines the respective advantages of GPT and BERT.T5 (Raffel et al., 2020) is such a model pretrained with span corruption, which can be regarded as a variantof MLM. During pretraining, spans of text in the input are replaced with sentinel tokens, which play thesame role as [MASK] in BERT. The noisy input is first processed by the encoder with bidirectional attention,and the masked spans are then generated autoregressively by the decoder. Formally, if k spans are sampledfor corruption in input x, the noisy input x is then constructed by replacing each span with a special token<extra_id_i>, for i = 1, 2, , k, and the target y is constructed by concatenating all spans prependedwith corresponding sentinels: [<extra_id_1>, span1, , <extra_id_k>, spank]. The model is then trainedwith a standard seq2seq objective, by maximizing",
  "i=1p(yi|x, y1:i1).(5)": "5Both [CLS] and [EOS] are artificial tokens added to the input text. [CLS] is added at the beginning and its representationis used for sentence classification, while [EOS] indicates end of sentence. The original BERT also uses another special token[SEP], which is not in common use in LLMs, and we refer to Devlin et al. (2019) for details.",
  ": Major implementation changes in LLM over the past few years": "Lester et al. (2021) show that models pretrained with such objectives can be adapted for autoregressivelanguage modeling with extra pretraining using the prefix language modeling objective, i.e. splitting thetext into two parts, processing the first part with encoder and generating the second part with decoder. Tay et al. (2023b) argue that span corruption is also closely related to CLM, since one can mask out the wholeinput text as a single span and train the decoder to generate it autoregressively. Inspired by this relation, theypropose UL2, which is the combination of many span corruption objectives that differ in corruption rate andspan length. Applying it to both encoder-decoder models and decoder-only models, they find that encoder-decoder models perform better under the same computation budget constraint. Other researches have alsofound that such encoder-decoder models generally perform better than causal decoder-only models (Wanget al., 2022c; Soltan et al., 2022).",
  "Auxiliary Objectives": "Language modeling objectives, such as previously discussed CLM and MLM, mainly train the model to cap-ture token-level information and are ineffective at modeling document structures. Thus, auxiliary objectivesare often added to help the models learn such global information. BERT is pretrained with next sentenceprediction (NSP) along with MLM, which is formulated as a binary classification task to predict whether twosegments in the input are neighboring in the original corpus. Lan et al. (2020) propose a more challengingsentence-order prediction (SOP) task, where the negative samples are constructed by swapping the order oftwo neighboring sentences instead of sampling a random sentence from other documents. Relatedly, Raffel et al. (2020) mix supervised downstream samples such as GLUE (Wang et al., 2018a) intoT5s pretraining dataset to conduct multi-task pretraining. However, it is worth noting that since they unifyall tasks into a text-to-text format, the training objective is the same for their self-supervised pretrainingand supervised downstream tasks, i.e. conditional generation of the target sequence.",
  "and Chowdhery et al. (2023) observes limited performance degradation when applying this design to largermodels": "As self-attention alone cannot capture the position information of each input token, the vanilla Transformeradds a non-learnable vector thats dependent on the absolute position in the input sequence to the embeddingof each token, which is known as cosine position embedding. Later works such as GPT and BERT uselearnable embeddings instead, while T5 adopts relative position embedding (Shaw et al., 2018), where theinformation of relative position between query and key is injected in the computation of self-attention instead.A more recent scheme RoPE (Su et al., 2024) multiplies the keys and queries by a position-dependent rotationmatrix, and is later shown to enable position interpolation for processing of longer sequences (Chen et al.,2023d; Rozire et al., 2023; Peng et al., 2023a). Alternatively, Press et al. (2022) propose ALiBi, whichdirectly attenuates the attention scores according to the relative position between key and query. RoPE ispopularized by its application in PaLM (Chowdhery et al., 2023) and GPT-NeoX (Black et al., 2022), whileALiBi is adopted by BLOOM (Scao et al., 2022). We refer to Zhao et al. (2023) for a survey on positionembedding and interpolation in Transformers. Apart from position embeddings, another issue in Transformer that has long troubled researchers is the factthat the complexity of self-attention scales quadratically with the input sequence length. Some works suchas Reformer (Kitaev et al., 2020), Linformer (Wang et al., 2020c), Performer (Choromanski et al., 2021) andcosFormer (Qin et al., 2022b) use approximate attention to reduce this complexity, but they mostly come atthe cost of degraded performance (Tay et al., 2023a; Lin et al., 2022). Other works tackle this issue from anengineering point-of-view. MQA (Shazeer, 2019) shares the same set of keys and values across all attentionheads to optimize memory-to-arithmetic ratio and significantly improves inference speed at small costs ofmodel performance. Its variant Grouped-Query Attention (GQA, Ainslie et al., 2023) takes a middle-groundapproach by dividing attention heads into groups and sharing the same set of keys/values within each group.Orthogonally, Dao et al. (2022) introduce FlashAttention, which is an exact but improved implementation ofself-attention that optimizes IO operations on the accelerating device via tiling to improve memory efficiency. Another important technique for improving LLMs training efficiency is mixed precision training (Micikevi-cius et al., 2018), which stores model weights and activations in lower precision to save memory consumption.Early works use FP16 format for this low precision, while BF16 has now become more popular (Chowdheryet al., 2023; Scao et al., 2022), as it can represent the same range of floating point numbers as FP32, thuspreventing numerical overflowing and underflowing during training.",
  "General Language Models for Code": "Since language models scaled to hundreds of billions of parameters (Brown et al., 2020; Chowdhery et al.,2023), many of them have demonstrated non-trivial coding capability, even if they are not specifically de-signed or trained for code. Pioneered by Codex, researchers have also found continual pretraining on codeto significantly benefit language models performance on code6. 6While some works refer to this process as finetuning on code\", it is still self-supervised in nature. Thus we choose toadopt the term extra/additional/continual pretraining\" in this work to avoid confusion with supervised in-task finetuning orinstruction finetuning.",
  "Off-the-Shelf Language Models": ": Pass@k performance of raw language mod-els (top) and language models with extra training oncode (bottom) on HumanEval (0-shot) and MBPP (3-shot), ordered chronologically. For Phi-1.5 we considerPhi-1.5-web version, and for Code LLaMA we considerits Python version. 1 Chen et al. (2021b); 2 Chowdh-ery et al. (2023); 3 Austin et al. (2021); 4 Scao et al.(2022); 5 Touvron et al. (2023a); 6 OpenAI (2023);7 Bubeck et al. (2023); 8 Touvron et al. (2023b); 9 Liet al. (2023j);10 Yang et al. (2023a);11 Bai et al.(2023); 12 Jiang et al. (2023a); 13 Anil et al. (2023a);14 DeepSeek-AI et al. (2024); 15 Jiang et al. (2024a);16 Dai et al. (2024a); 17 Anil et al. (2023b); 18 Rozireet al. (2023).",
  "Codex112B28.872.3PaLM-Coder2540B36.088.447.080.8PaLM 2*17S37.688.450.086.8Code LLaMA1834B53.794.756.2Code-Qwen1114B45.151.4": "Large language models are often pretrained on tril-lions of tokens following the scaling laws (Kaplanet al., 2020;Hoffmann et al., 2022), and suchan amount of text data is often a diverse com-posite with a non-negligible part of code.ThePile (Gao et al., 2021), for example, includes 95GBof code crawled from GitHub out of its 800GB rawdataset, while the multilingual pretraining datasetROOTS (Laurenon et al., 2022) also contains163GB of code spanning 13 programming languagesin its 1.6TB compound. As two of the largest open-source pretraining datasets, they have supportedmany language models with coding ability. GPT-J (Wang & Komatsuzaki, 2021), for example, is re-ported by Chen et al. (2021b) to demonstrate non-trivial performance on HumanEval, while Scao et al.(2022) report similar results for GPT-NeoX (Blacket al., 2022) and BLOOM. LLaMA (Touvron et al.,2023a), whose pretraining dataset includes 328GBcode from GitHub, achieves 23.7 pass@1 perfor-mance on HumanEval, and its successor LLaMA2 (Touvron et al., 2023b), achieves an even higherscore of 29.9. Closed-source models, on the other hand, performgenerally better. LaMDA (Thoppilan et al., 2022)and PaLM (Chowdhery et al., 2023), whose pre-training dataset contains 12.5% and 5% code respec-tively, achieve 14.0 and 26.2 pass@1 performanceon HumanEval, while GPT-4 (OpenAI, 2023) set astaggering record of 67.0 (and an early version is re-ported by Bubeck et al. (2023) to be 82) that untilrecently has remained higher than any specializedmodels pretrained or instruction-finetuned for code. More recently, the general trend has been to trainsmaller models with larger datasets, following the re-vised scaling law (Hoffmann et al., 2022). Baichuan2 (Yang et al., 2023a), for example, is a 13B modeltrained on 2.6T tokens, while Qwen (Bai et al., 2023) is a 14B model trained on 3T tokens. They achieve17.1 and 32.3 pass@1 on HumanEval, respectively. Li et al. (2023j), however, demonstrate that models assmall as 1.3B can acquire coding capability thats comparable to much larger models while also maintaining areasonable performance on general text processing and even manifesting some emergent abilities (Wei et al.,2022c) such as chain-of-though reasoning (Wei et al., 2022d). Their model, Phi-1.5, is trained on 21B tokensof textbook data generated by ChatGPT, and 100B tokens of filtered web data from Stack Overflow andRefined Web (Penedo et al., 2023), and attains 41.4 pass@1 performance on HumanEval. Another rising trend is mixture-of-expert models (Lepikhin et al., 2021; Fedus et al., 2022; Du et al., 2022).Notably, Jiang et al. (2024a) recently introduce Mixtral 8x7B, where only 13B parameters are activated foreach token during inference, but achieving 40.2 on HumanEval and surpassing much larger dense modelssuch as LLaMA 2. Similarly, Dai et al. (2024a) present DeepSeekMoE, where only 2.8B parameters areactivated in a 16B model, scoring 26.8 on HumanEval.",
  "Language Models with Additional Pretraining on Code": "Along with the seminal benchmark HumanEval, Chen et al. (2021b) kick-started the age of LLM for codewith Codex, which are GPT-3 checkpoints pretrained on 100B additional code tokens and one of the earliestmulti-billion models for code. Following their work, other researchers have also specialized their LLMs oncode with additional pretraining. Chowdhery et al. (2023) train PaLM on 7.8B additional code tokens toobtain PaLM-Coder, setting new state-of-the-art on HumanEval and MBPP () that are only brokenlater by its successor PaLM 2-S*, the smallest version of PaLM 2 (Anil et al., 2023b) further trained onan undisclosed amount of code. Similarly, Lewkowycz et al. (2022) train PaLM on 38.5B tokens of arXivpapers and mathematical content, while Rozire et al. (2023) train LLaMA 2 (Touvron et al., 2023b) on morethan 500B code tokens to acquire Code LLaMA, whose performance on HumanEval surpasses all previousLMs except GPT-4 (). Liu et al. (2023b) further train Code LLaMA with multi-task finetuning(MFT) to introduce CodeFuse-CodeLLaMA, achieving 74.4 pass@1 on HumanEval and surpassing even theperformance of GPT-4 published in OpenAI (2023). While almost all of these models are Transformer decoders pretrained with CLM, several architecturalmodifications have been introduced along the way, as we noted in .5. All these models use pre-norm,and GPT-J introduces parallel attention, which is later adopted by PaLM, GPT-NeoX, and Phi-1.5. PaLMintroduces MQA and RoPE into LLMs, and RoPE is now employed by most language models, includingGPT-NeoX, two generations of LLaMA, Qwen, and the 7B version of Baichuan 2. BLOOM and the 13Bversion of Baichuan 2, however, use ALiBi for position embeddings, while LLaMA 2 and Code LLaMA adoptGQA instead of MHA or MQA. In , we show that specialized models pretrained exclusively on codehave also followed these advancements closely.",
  "Specialized Language Models for Code": "As pretrained Transformers such as GPT and BERT achieved remarkable success in natural language pro-cessing, such model architectures, learning paradigms, and training objectives were soon adopted by thesoftware engineering community to produce specialized models for code understanding and generation. Inthis section, we first review common datasets used for pretraining code language models (.1), andthen dive into the complex family of code LMs by their model architecture: encoder-only models (Sec-tion 5.2), encoder-decoder models (.3), decoder-only models (.4), UniLM (.5), : Statistics of several pretraining datasets forcode models: size in bytes, number of files, and num-ber of programming languages.In CodeSearchNeteach file is a function.For Pile and ROOTS weonly consider their code composite.a Husain et al.(2019); b Gao et al. (2021); c Biderman et al. (2022);d e Kocetkov et al. (2023); f Laurenonet al. (2022); g Lozhkov et al. (2024).",
  "CodeSearchNeta206.56The Pilebc9519-CodeParrotd1K11530The Stacke313631730ROOTSf1631513The Stack v2g32K3K619": "and diffusion models (.6). Lastly, in Sec-tion 5.7 we also illustrate the current trend of ap-plying more recent techniques in NLP, such as in-struction tuning (Wei et al., 2022b; Sanh et al.,2022; Chung et al., 2022) and reinforcement learn-ing (Ouyang et al., 2022) to code processing. Anoverview of these pretrained models are provided in.",
  "Training Dataset for Code": "While text data for pretraining language modelsare often crawled from the web and must undergometiculous and often aggressive preprocessing (Raf-fel et al., 2020), code data come naturally as wholedocuments from public GitHub repositories. Evenbetter, they come with readily available quality in-dicators such as the count of stars or forks (althoughAllal et al. (2023) suggest that star count correlatespoorly with downstream performance). As a result,many large-scale code pretraining datasets havebeen introduced, including CodeSearchNet (Husain",
  "et al., 2019), CodeParrot (Tunstall et al., 2022), and the Stack (Kocetkov et al., 2023), totaling 20GB, 50GBand 3TB of code documents respectively ()": "While these datasets are meant for training code models, it should be noted that code is ultimately a specialform of natural language, as the vocabulary of most programming languages is a small subset of English.Besides, high-quality code is often interleaved with natural language comments or documentations, whichalso enables models to acquire certain knowledge of general text representation. In fact, of the 6.5M functionsin CodeSearchNet, 2.3M are paired with natural language documentation, allowing models to train explicitlyon such bimodal data. Compared with natural language, another byproduct of scraping code from GitHub is commit histories,which consist of code before commit, code after commit, and a short message describing the commit, whichcan loosely serve as an instruction for language models. Muennighoff et al. (2024) utilize this feature andconstruct a 2GB dataset CommitPackFT containing 742K samples of instruction data for code, obviatingthe need of extensive human labor thats required to construct natural language instructions (Sanh et al.,2022; Wang et al., 2022g). Apart from bimodal training and instruction finetuning, another recent trend in constructing code datasetis synthesizing data with powerful models such as ChatGPT. While this method is originally proposed forgenerating instruction data in natural language (Wang et al., 2023g; Honovich et al., 2023), Gunasekar et al.(2023) take one step further and synthesize 1B tokens of Python textbooks and coding exercises to pretraina 1.3B model, achieving state-of-the-art results on HumanEval thats comparable to much larger modelstrained on significantly larger datasets.",
  "Encoders": "Pretrained Transformer encoders such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019d), andELECTRA (Clark et al., 2020) have attained impressive results on natural language understanding tasks,and these methods were soon introduced into code processing after their advent.Kanade et al. (2020)replicate the training procedure of BERT on a code corpus to produce CuBERT, showcasing its superiorperformance over LSTM (Hochreiter & Schmidhuber, 1997) and non-pretrained Transformers. Feng et al.(2020), on the other hand, train CodeBERT with MLM and ELECTRAs RTD on CodeSearchNet. Theyalso utilize the explicit text-code pairs in CodeSearchNet, and use them respectively as the first and secondsegment in BERTs input. When using CodeBERT to initialize the encoder part of a vanilla Transformer forsequence-to-sequence generation tasks such as code summarization, they observe a moderate performancegain over non-pretrained baselines. Apart from these standard training objectives, many auxiliary objectives specifically designed for code havealso been introduced. GraphCodeBERT (Guo et al., 2021a) and SynCoBERT (Wang et al., 2021d) bothextract graphs from the source code (data flow graph and abstract syntax tree, respectively) and train themodels to predict the typological relations between the nodes, while SynCoBERT and Code-MVP (Wanget al., 2022e) also add type inference to their pretraining stage in the form of tagging. Another commonobjective is contrastive learning: SynCoBERT and Code-MVP contrast between different views of the input(such as code, comment, AST, and transformed code), while DISCO (Ding et al., 2022a) constructs positivesample pairs by semantic-preserving transformations such as obfuscation, and negative pairs by injectingartificial bugs.",
  "Encoder-Decoders": "In NLP, pretrained Transformer encoder-decoders such as T5 (Raffel et al., 2020) and BART (Lewis et al.,2020) have also left a notable mark in the past few years advancement in language modeling.T5, forexample, unifies all textual tasks into a sequence to sequence format and sets new records on GLUE (Wanget al., 2018a) and SuperGLUE (Wang et al., 2019). Compared with encoder-only models, encoder-decodersare naturally more powerful as they can be used for conditional text generation, while their encoder partcan always be taken out to perform tasks that require an encoder-only architecture, such as regression (Tayet al., 2023b). Inspired by these advantages of encoder-decoder architecture, many such models have been proposed forcode processing. PyMT5 (Clement et al., 2020) and Mastropaolo et al. (2021) replicate the pretraining andmulti-task finetuning process of T5 on code corpus, while Ahmad et al. (2021) introduce PLBART, a BARTpretrained on 655GB combined data of Java, Pyhton, and natural language. Lachaux et al. (2021) arguethat MLM could be too easy a task for programming languages as identifier names often occur multiple timesin a single context window, and propose a deobfuscation pretraining objective, where the model is trained toconvert obfuscated code back to its original form. Related to this method, we note that meaningful variablenames have also been found to have a positive impact on the code generation process of large languagemodels (Chen et al., 2023e). Building on these early works, Wang et al. (2021f) propose CodeT5, which is pretrained alternatively with1) T5s original span corruption; 2) identifier tagging (where each token in the code input is tagged aseither identifier or non-identifier); 3) masked identifier prediction (a special form of span corruption whereall identifiers are masked); and 4) text-to-code & code-to-text generation. Its successor, CodeT5+ (Wanget al., 2023h), take inspiration from UL2 (Tay et al., 2023b) and introduce causal language modeling (CLM)into pretraining, along with additional contrastive objectives based on text-code matching. AlphaCode (Li et al., 2022h) is also trained with multiple objectives, where the encoder is trained withMLM and the decoder is trained with CLM, with architecture modifications such as shallow-encoder & deep-decoder, multi-query attention (Shazeer, 2019), and being much larger than CodeT5 (up to 41B parameters).NatGen (Chakraborty et al., 2022a), on the other hand, is pretrained with a \"naturalization\" objective similarto deobfuscation: semantically equivalent but unnatural code is generated by predefined operations such asloop transformation, dead code injection, and variable renaming, and the model is pretrained to translate",
  "GPT-4-67.0/82-PaLM 2*S37.650.0Code LLaMA34B53.756.2Phi-1.51.3B41.443.5": "these unnatural code back to its original form. Wenote that some of these models are built on previ-ous works. For example, NatGen is initialized withCodeT5, while the largest version of CodeT5+ isinitialized from a decoder-only model, CodeGen (Ni-jkamp et al., 2023b). Apart from these general pretraining objectives, sev-eral works have also trained Transformer encoder-decoders with a focus on code translation, which isa natural application of Transformer models in codeas the Transformer architecture was originally pro-posed by Vaswani et al. (2017) for machine trans-lation (MT). However, unlike natural languages,where parallel corpus across two or more humanlanguages exist in abundance, there is little par-allel data for code.To tackle this issue, Rozireet al. (2020) propose Transcoder, which first pre-trains an encoder with XLM (Conneau & Lample,2019), and then initializes a vanilla Transformerwith this encoder and continue to pretrain it withDenoising Auto-Encoding (DAE, Lewis et al., 2020)and back translation (Sennrich et al., 2016), while itsfollow-up work (Szafraniec et al., 2023) also utilizelanguage-independent intermediate representationsto enhance this process, which we discuss in moredetail in . Apart from training data and objectives, these mod-els mostly keep to the original architectures pro-posed by the NLP community, as shown in Ta-ble 3.Models based on BART, for example, usepost-normalization and learnable absolute position embeddings, while those based on T5 use its simplifiedrelative position embeddings and pre-normalization.",
  "Decoders": "After the monumental debut of GPT-3 (Brown et al., 2020) and the discovery of in-context learning, decoder-only Transformer models have become dominant in language modeling (Rae et al., 2021; Hoffmann et al.,2022; Chowdhery et al., 2023; Scao et al., 2022; Touvron et al., 2023a;b, inter alia). Many models similarlypretrained with CLM have also emerged in code processing, such as GPT-C (Svyatkovskiy et al., 2020),CodeGPT (Lu et al., 2021), PolyCoder (Xu et al., 2022), CodeGen (Nijkamp et al., 2023b), PyCodeGPT (Zanet al., 2022), Pangu-Coder (Christopoulou et al., 2022), CodeGeeX (Zheng et al., 2023a), Jam (Su et al.,2023a), Phi-1 (Gunasekar et al., 2023), and CodeFuse (Di et al., 2023). Of these models, several alternativetraining objectives have been experimented with, such as MLM and Masked CLM7 in Pangu-Coder, but arefound to underperform compared with CLM-only training. Zan et al. (2022) also propose continual trainingon sketches, where the model learns to first generate a sketch of a program and then the actual code. Notably,Gunasekar et al. (2023) present Phi-1, a 1.3B small model trained on a dataset of only 7B tokens consistingof 6B tokens from StackOverflow and 1B synthetic data generated by ChatGPT but achieving 50.6 pass@1on HumanEval and 55.5 pass@1 on MBPP, comparable to much larger (both in model size and training datasize) models such as Code LLaMA or PaLM 2. 7In their paper, MLM is conducted by replacing tokens in the input with <mask> and predicting it from only the left context,while Masked CLM is performed by adding a <mask> in the input and predicting the next token from it. Both tasks do notchange the attention mask patterns of the model.",
  "UniLMs": "Following UniLM (Dong et al., 2019) in NLP, several works in code processing have also pretrained thisfourth family of Transformer models on code. CugLM (Liu et al., 2020) is trained with both CLM and MLM+ NSP via alternating attention masks, while UniXcoder is trained with CLM, MLM, Span Corruption(in Prefix LM style) along with auxiliary objectives including contrastive learning and text-code mutualgeneration. Both two models, however, are relatively small in size, and whether or not this architecture issuitable for code processing is yet to be explored at scale.",
  "Diffusion Models": "Currently the Transformer architecture dominates text generation, but several works (Li et al., 2022d; Linet al., 2023) have also adopted Diffusion Models (Ho et al., 2020) from computer vision for text generation.Recently CodeFusion (Singh et al., 2023) also introduces diffusion models into code modeling, and demon-strates that a 75M diffusion model can outperform StarCoder, CodeT5+, and GPT-3 on three code synthesisdatasets.",
  "Instruction Finetuning and Reinforcement Learning for Code": "In natural language processing, training models on a diverse set of tasks with instruction prefix, knownas instruction finetuning, has been shown to unlock the ability of cross-task generalization (Ouyang et al.,2022; Chung et al., 2022; Iyer et al., 2022). At first, these instruction data samples are manually compiledor crowd-sourced (Wei et al., 2022b; Sanh et al., 2022), but later researches find LLM-generated instructionsto be sufficient (Wang et al., 2023g; Honovich et al., 2023). Following these works in natural language, researchers from the code community have applied instructiontuning to their models as well. Wang et al. (2023h) finetune CodeT5+ with 20K instruction data generatedby InstructGPT (Ouyang et al., 2022) to obtain InstructCodeT5+. WizardCoder (Luo et al., 2023) followsthe methods of WizardLM (Xu et al., 2024) to evolve 20K code Alpaca (Taori et al., 2023) samples into a78K dataset and uses it to finetune StarCoder. Pangu-Coder 2 (Shen et al., 2023) also uses WizardLMsEvol-Instruct to generate 68K instruction samples from 20K code Alpaca, but also introduces reinforcementlearning via Rank Responses to align Test & Teacher Feedback (RRTF). OctoCoder (Muennighoff et al.,2024), on the other hand, takes a different path and uses Git commit histories as instruction data to finetuneStarCoder and CodeGeeX2. More recently, CodeFuse (Liu et al., 2023b) also employs multitask-finetuningand explicitly introduces multiple downstream tasks into their instruction data. The performance of theseinstruction finetuned code models can also be found in . In NLP, another technology closely related to instruction finetuning is reinforcement learning from humanfeedback (RLHF), which has played a significant role in aligning LLMs with human values (Ouyang et al.,2022; Bai et al., 2022). The merit of reinforcement learning is that it can incorporate non-differentiable",
  "Code Features for Language Models": "A major difference between programming languages and natural languages is that the former is artificiallydefined to be precise and unambiguous, and need to be compiled (or interpreted) without error beforeexecution. This allows for a much larger flexibility in designing pretraining objectives on code, beside lexicalmanipulations such as CLM, MLM, and Span Corruption. A similar trend can be observed in the last yearsbefore neural networks were introduced into mainstream NLP literature (Sutskever et al., 2014; Bahdanauet al., 2015), when researchers in the MT community utilized alternative views of text such as syntacticfeatures to improve the performance of SMT systems (Galley et al., 2006; Chiang, 2007). These features,however, are not universally applicable or even agreed upon, and often result in highly complicated systems(for example, the size of English part-of-speech taggings label set may range from dozens to hundreds). Programming languages, however, fare much better in these aspects. Each mainstream programming lan-guage, such as C, Python, and Java, comes with readily available compiler toolkits that allow for easy andaccurate extraction of semantic information such as Abstract Syntax Tree (AST), language-independentIntermediate Representation (IR), and auxiliary information such as type of each token and control/dataflow graph (CFG/DFG). Thus, in the context of Transformer-based language modeling for code, many workshave incorporated these features into their training procedure.",
  "Abstract Syntax Tree and Intermediate Representation": "AST is one of the most common intermediate results of the compiling process, where a program is parsed intoa tree of operations and their operands. Before the popularization of Transformer in the code processingcommunity, there had been works such as InferCode (Bui et al., 2021a) that processes these representa-tions with special network architectures like Tree-Based CNN and conducts self-supervised pretraining bypredicting subtrees. TreeBERT (Jiang et al., 2021b) is one of the first attempts to take AST into the Transformer-basedpretraining-finetuning framework. Its a Transformer encoder-decoder pretrained with Tree MLM and NodeOrder Prediction, where the encoder takes a set of constituent paths in the AST as input (with each tokenbeing a path, which is the concatenation of its nodes representations) while the decoder takes the code asinput. Tree MLM is then performed by masking certain nodes in a path representation and its correspondingcode tokens in the decoder input, while Node Order Prediction is accomplished by swapping nodes in a pathand predicting it with a [CLS] token similar to BERT. The method used by TreeBERT, however, is complicated and does not scale well. Later works mostly opt tofirst process AST into a text sequence and treat it like a normal part of the input. Wang et al. (2021d), forexample, process AST with depth-first traversal and concatenate it with code and comment, and then trainSynCoBERT (which, unlike TreeBERT, is actually a BERT-like encoder-only model) with four objectives:1) MLM; 2) identifier tagging; 3) AST edge prediction (predicting whether there exists an edge betweentwo AST nodes from the dot product of these nodes representations); and 4) contrastive learning over i)code and AST pairs, as well as ii) text and code-AST pairs. Similarly, SPT-Code (Niu et al., 2022), aTransformer encoder-decoder, takes the concatenation of code, sequentialized AST, and text as input, and",
  "Control Flow and Data Flow": "While AST and IR have proved to be useful information in certain tasks such as code translation, they arestatic by nature, just like the source code, and may fail to capture semantic properties of code that are onlyrevealed at runtime (Wang & Su, 2020). Such semantics, however, are contained in dynamic features such ascontrol flow and data flow. Similar to AST, specialized networks were used to process such information beforethe rise of pretrained Transformers, such as Message Passing Neural Network used by ProGraML (Cumminset al., 2021). Unlike AST, however, even after pretrained Transformers became dominant few works havelooked in this direction. GraphCodeBERT (Guo et al., 2021a) is one of such works, which creates special tokens and position embed-dings for variables in the flow graph, and concatenates the variable sequence after text and source code toconstruct model input, with tailored attention masks on the code and variable segments: tokens from codesegment and variable segment can attend to each other if and only if the variable is identified from the codetoken, and for tokens within the variable segment, vi is allowed to attend to vj if there is a direct edge fromvj to vi in the dataflow. The model is then pretrained with MLM in combination with edge prediction andnode alignment, both of which are accomplished by binary classification from the dot product of two tokensrepresentations (one from code segment and one from variable segment for node alignment, and both fromvariable segment for edge prediction).",
  "Type": "Apart from AST, IR, and data flow, type information has also been used to aid language models in processingcode. CugLM (Liu et al., 2020), for example, uses type information during finetuning to aid in the predictionof tokens for unidirectional MLM (i.e. MLM with unidirectional attention mask): the type of a masked tokenis first predicted from the final Transformer layers representation, and then the token itself is predicted basedon both the hidden representation and predicted type. In contrast, both CodeT5 (Wang et al., 2021f) andSynCoBERT (Wang et al., 2021d) include identifier tagging in their pretraining objectives, which can beviewed as coarse-grained type prediction.",
  "Program Transformation": "As we have shown in .3, function-preserving program transformations have proven to be importanttechniques in pretraining code language models. Obfuscation is one instance of program transformation, andothers include loop transformation (for-while), condition transformation (if-switch), dead code injection (e.g.if True:pass), and statement swapping. DOBF (Lachaux et al., 2021) and NatGen (Chakraborty et al.,2022a) are two code language models pretrained to recover the original program from such transformations,while Wang et al. (2022a) also apply program transformations during the finetuning stage of language modelsto make them more robust to transformed test samples.",
  "LLMs Extended with Coding Tools": "Research in the NLP community has shown that LLMs can learn to use external tools such as calculators,MT systems, and search engines (Thoppilan et al., 2022; Schick et al., 2023).As such, interpreter hasbeen used to augment LLMs in complex reasoning tasks. PAL (Gao et al., 2023b) and PoT (Chen et al.,2023e) both extend Codex with Python interpreters for numerical calculations, while ViperGPT (Surs et al.,2023) extends it further by calling vision APIs to extract information from visual input and answer relatedquestions. However, LLMs do not always produce code that can be interpreted and executed, and there has also beena line of works that explore the possibility of emulating the interpreter with LLMs themselves. Nye et al.(2021) trains LLMs to emulate the execution of programs by outputting program states at each step. Morerecently, Li et al. (2023a) propose Chain-of-Code, where a real interpreter executes the generated code untilan error occurs, whereupon the LLM takes over to simulate execution. Chae et al. (2024) similarly proposeThink-and-Execute framework, where an LLM generates pseudo code and then simulates its execution tosolve reasoning tasks. Apart from alleviating the burden of numerical calculation in abstract reasoning tasks, interpreter (togetherwith unit tests) also provides feedback on the process of code generation itself, allowing for interactivegeneration and refinement of code. Such works include Self-Edit (Zhang et al., 2023c), LeTI (Wang et al.,2023f), OpenCodeInterpreter (Zheng et al., 2024), ProCoder (Bi et al., 2024), Cycle (Ding et al., 2024), andSOAP (Huang et al., 2024), which run model-generated code against unit tests to provide feedback for furtherrefinement. Alternatively, CodeT (Chen et al., 2023a), TiCoder (Barei et al., 2022), and CONLINE (Heet al., 2024) also utilize the LLM itself to generate unit tests, while Self-Refine (Madaan et al., 2023) sendsthe generated code to an LLM instead of an interpreter for feedback. Zhou et al. (2023a) show that OpenAIsinterpreter plugin8 allows GPT-4 to self-debug, while InterCode (Yang et al., 2023d) provides a benchmarkfor evaluating interactive coding. In .7 we have also shown that the execution results on unit testsserve as natural supervision signals for reinforcement learning on code. A topic closely related to tool using in LLM research is planning as intelligent agents, which has been shownto enhance LLMs capability both theoretically and empirically (Feng et al., 2023a). Ruan et al. (2023) findthat LLMs can plan to solve complex tasks using external SQL generators and Python generators, whileCodePlan (Bairi et al., 2023) demonstrates they can perform repository-level coding via adaptive planning. Another stream of works use LLMs to create multi-agent systems for code generation, such as self-collaboration (Dong et al., 2023), ChatDev (Qian et al., 2023), MetaGPT (Hong et al., 2023), LCG (Linet al., 2024), MAGIS (Tao et al., 2024), and SoA (Ishibashi & Nishimura, 2024). In these frameworks,multiple LLMs are prompted to play distinct roles such as programmer, reviewer, and manager. These rolesinteract with each other, breakdown code generation into different phases (e.g. designing, coding, testing,and documenting), and collaborate to complete complex tasks.",
  "With the increase in LLMs interactive coding capability, researchers have also started to integrate theminto each and every process of software development": "Auto code completion is one of the earliest applications of language models in software development, asthey require only the ability to predict the next token. Even before language models scaled to billions ofparameters, there had been integration of completion systems such as Pythia (Svyatkovskiy et al., 2019) andIntelliCode (Svyatkovskiy et al., 2020) into popular IDEs. Recently, however, the application of code language models have transcended simple code completion.GitHub Copilot is arguably one of the most popular AI code assistants, with diverse features includingcode generation, vulnerability detection, and license management9, while CodeFuse (Di et al., 2023) alsointegrates code generation, code translation, code commenting, and testcase generation into a single IDEextension.As code language models become larger, however, their client-side deployment and real-timeperformance also raise new challenges. As LLMs continue to advance, building applications on top of them is also evolving into a consequentialtask itself. Many open-source frameworks for such applications have been released, including LangChain10,AutoGPT11, and WorkGPT12. These frameworks provide abstractions over language models for developers,and are actively revolutionizing the entire process of software development even as this survey is beingfinalized.",
  "Analysis of LLM-Generated Code": "As AI code assistants become prevalent, many recent works have also focused on examining AI-generatedcode from different aspects, including correctness (Nguyen & Nadi, 2022; Yetistiren et al., 2023), bugs (Jesseet al., 2023; Tambon et al., 2024), vulnerabilities (Asare et al., 2023; Sandoval et al., 2023; Perry et al.,2023; Hamer et al., 2024), syntactic robustness (Sarker et al., 2024), efficiency (Niu et al., 2024), andhallucinations (Liu et al., 2024). Asare et al. (2023) and Sandoval et al. (2023)s studies show that AI codeassistants do not introduce extra security risks compared with human programmers, and Hamer et al. (2024)also find that ChatGPTs responses to security-related questions contain less vulnerabilities than answersfrom StackOverflow. Contrarily, Perry et al. (2023) find that users write significantly less secure code whenassisted by AI. In terms of code complexity, Nguyen & Nadi (2022) find GitHub Copilot to generate low-complexity code with no statistically significant differences between languages, whereas Liu et al. (2023n)find ChatGPT to generate the most complex code in C and the least complex code in Python. Overall, AIcode assistants are still in their nascent stage and constantly evolving, and their implications on softwaredevelopment are yet to be investigated systematically.",
  "Conclusion and Challenges": "In this work, we systematically reviewed the history of pretrained Transformer language models in codeprocessing and other software engineering tasks. The advancement in code modeling generally follows thehistory course of NLP, evolving from SMT models, to NMT models, and then to finetuning pretrained Trans-formers and lastly to few-shot application of LLMs and even autonomous agents in real-world production.Unlike natural languages, the nature of code makes it easy to extract auxiliary information from alternativeviews, and to utilize interpreter and unit tests for automatic feedback.",
  "Sallam Abualhaija, Marcello Ceci, and Lionel C. Briand. Legal requirements analysis. CoRR, abs/2311.13871,2023. doi: 10.48550/ARXIV.2311.13871. URL": "Mohammed Abuhamad, Tamer AbuHmed, Aziz Mohaisen, and DaeHun Nyang. Large-scale and language-oblivious code authorship identification. In David Lie, Mohammad Mannan, Michael Backes, and Xi-aoFeng Wang (eds.), Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communica-tions Security, CCS 2018, Toronto, ON, Canada, October 15-19, 2018, pp. 101114. ACM, 2018. doi:10.1145/3243734.3243738. URL Rajas Agashe, Srinivasan Iyer, and Luke Zettlemoyer. Juice: A large scale distantly supervised dataset foropen domain context-based code generation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan(eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong",
  "Kong, China, November 3-7, 2019, pp. 54355445. Association for Computational Linguistics, 2019. doi:10.18653/V1/D19-1546. URL": "Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettle-moyer. HTLM: hyper-text pre-training and prompting of language models. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,2022. URL Lakshya A Agrawal, Aditya Kanade, Navin Goyal, Shuvendu K. Lahiri, and Sriram K. Rajamani. Guidinglanguage models of code with global context using monitors. CoRR, abs/2306.10763, 2023. doi: 10.48550/ARXIV.2306.10763. URL Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. A transformer-based approachfor source code summarization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL2020, Online, July 5-10, 2020, pp. 49985007. Association for Computational Linguistics, 2020.doi:10.18653/v1/2020.acl-main.449. URL Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang.Unified pre-training forprogram understanding and generation. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, DilekHakkani-Tr, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.),Proceedings of the 2021 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pp. 26552668. Association for Computational Linguistics, 2021.doi: 10.18653/v1/2021.naacl-main.211.URL Wasi Uddin Ahmad, Md Golam Rahman Tushar, Saikat Chakraborty, and Kai-Wei Chang. AVATAR: Aparallel corpus for java-python program translation. In Anna Rogers, Jordan L. Boyd-Graber, and NaoakiOkazaki (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada,July 9-14, 2023, pp. 22682281. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL.143. URL Sharif Ahmed, Arif Ahmed, and Nasir U. Eisty. Automatic transformation of natural to unified modelinglanguage: A systematic review. In Juyeon Jo, Yeong-Tae Song, Lin Deng, and Junghwan John Rhee(eds.), 20th IEEE/ACIS International Conference on Software Engineering Research, Management andApplications, SERA 2022, Las Vegas, NV, USA, May 25-27, 2022, pp. 112119. IEEE, 2022. doi: 10.1109/SERA54885.2022.9806783. URL Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrn, and Sumit Sang-hai. GQA: training generalized multi-query transformer models from multi-head checkpoints. In HoudaBouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods inNatural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 48954901. Associationfor Computational Linguistics, 2023. URL",
  "Saranya Alagarsamy, Chakkrit Tantithamthavorn, and Aldeida Aleti.A3test: Assertion-augmented au-tomated test case generation. CoRR, abs/2302.10352, 2023. doi: 10.48550/ARXIV.2302.10352. URL": "Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muoz Fer-randis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Car-olyn Jane Anderson, Yangtian Zi, Joel Lamy-Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Ab-ulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garca del Ro, Qian Liu,Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, SourabMangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau,Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. San-tacoder: dont reach for the stars! CoRR, abs/2301.03988, 2023. doi: 10.48550/arXiv.2301.03988. URL",
  "Miltiadis Allamanis, Earl T. Barr, Ren Just, and Charles Sutton. Tailored mutants fit bugs better. CoRR,abs/1611.02516, 2016a. URL": "Miltiadis Allamanis, Hao Peng, and Charles Sutton. A convolutional attention network for extreme sum-marization of source code. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24,2016, volume 48 of JMLR Workshop and Conference Proceedings, pp. 20912100. JMLR.org, 2016b. URL Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs withgraphs.In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, and Zheng Gao. Typilus: neural type hints. In Alastair F.Donaldson and Emina Torlak (eds.), Proceedings of the 41st ACM SIGPLAN International Conferenceon Programming Language Design and Implementation, PLDI 2020, London, UK, June 15-20, 2020, pp.91105. ACM, 2020. doi: 10.1145/3385412.3385997. URL Miltiadis Allamanis,Henry Jackson-Flux,and Marc Brockschmidt.Self-supervised bug detectionand repair.In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, andJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:An-nual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,2021, virtual, pp. 2786527876, 2021.URL Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. A general path-based representation for predictingprogram properties. In Jeffrey S. Foster and Dan Grossman (eds.), Proceedings of the 39th ACM SIGPLANConference on Programming Language Design and Implementation, PLDI 2018, Philadelphia, PA, USA,June 18-22, 2018, pp. 404419. ACM, 2018. doi: 10.1145/3192366.3192412. URL",
  "Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. code2vec: learning distributed representationsof code. Proc. ACM Program. Lang., 3(POPL):40:140:29, 2019b. doi: 10.1145/3290353. URL": "Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In Proceedingsof the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,volume 119 of Proceedings of Machine Learning Research, pp. 245256. PMLR, 2020.URL Hussein Alrubaye, Mohamed Wiem Mkaouer, Igor Khokhlov, Leon Reznik, Ali Ouni, and Jason Mcgoff.Learning to recommend third-party library migration opportunities at the API level. Appl. Soft Comput.,90:106140, 2020.doi: 10.1016/J.ASOC.2020.106140.URL Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-jishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms.In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human Language Technolo-gies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers),pp. 23572367. Association for Computational Linguistics, 2019.doi:10.18653/v1/n19-1245.URL Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap,Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Ben-jamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer,Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, MaximKrikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anas White, Anders Andreassen,Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Syg-nowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023a.doi: 10.48550/ARXIV.2312.11805. URL Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robin-son, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernndez brego,Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, KevinBrooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowd-hery, Clment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Daz, Nan Du,Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebas-tian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023b. doi:10.48550/arXiv.2305.10403. URL",
  "Owura Asare, Meiyappan Nagappan, and N. Asokan. Is githubs copilot as bad as humans at introducingvulnerabilities in code?Empir. Softw. Eng., 28(6):129, 2023. doi: 10.1007/S10664-023-10380-1. URL": "Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Ud-din Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian Ding, VarunKumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng Qian, Murali Kr-ishna Ramanathan, and Ramesh Nallapati. Multi-lingual evaluation of code generation models. In TheEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net, 2023. URL Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton.Program synthesis with largelanguage models. CoRR, abs/2108.07732, 2021. URL Batuhan Arolu, Bta Rmeysa Mete, Eyyp Yldz, Yaz Nalakan, Alper Sezen, Mustafa Datekin, andTolga Ensari. Automatic html code generation from mock-up images using machine learning techniques.In 2019 Scientific Meeting on Electrical-Electronics & Biomedical Engineering and Computer Science(EBBT), pp. 14, 2019. doi: 10.1109/EBBT.2019.8741736.",
  "Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR, abs/1607.06450,2016. URL": "Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q. Feldman, and Carolyn JaneAnderson.Studenteval: A benchmark of student-written prompts for large language models of code.CoRR, abs/2306.04556, 2023.doi: 10.48550/ARXIV.2306.04556.URL Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning toalign and translate. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on LearningRepresentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.URL Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In 5th InternationalConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, ConferenceTrack Proceedings. OpenReview.net, 2017. URL Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, FeiHuang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu,Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, JianhongTu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang,Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, XingxuanZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu.Qwen technical report.CoRR, abs/2309.16609, 2023.doi: 10.48550/arXiv.2309.16609.URL Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, TomConerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, ScottJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown,Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. Training a helpful andharmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862, 2022. doi:10.48550/ARXIV.2204.05862. URL Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Vageesh D. C, Arun Iyer, Suresh Parthasarathy,Sriram K. Rajamani, Balasubramanyan Ashok, and Shashank Shet. Codeplan: Repository-level codingusing llms and planning. CoRR, abs/2309.12499, 2023. doi: 10.48550/ARXIV.2309.12499. URL Noor Hasrina Bakar, Zarinah Mohd Kasirun, Norsaremah Salleh, and Hamid Abdullah Jalab. Extractingfeatures from online software reviews to aid requirements reuse. Appl. Soft Comput., 49:12971315, 2016.doi: 10.1016/J.ASOC.2016.07.048. URL Vipin Balachandran. Reducing human effort and improving quality in peer code reviews using automaticstatic analysis and reviewer recommendation.In David Notkin, Betty H. C. Cheng, and Klaus Pohl(eds.), 35th International Conference on Software Engineering, ICSE 13, San Francisco, CA, USA, May18-26, 2013, pp. 931940. IEEE Computer Society, 2013. doi: 10.1109/ICSE.2013.6606642. URL Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian Nowozin, and Daniel Tarlow. Deepcoder:Learning to write programs. In 5th International Conference on Learning Representations, ICLR 2017,Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL Upul Bandara and Gamini Wijayarathna. Deep neural networks for source code author identification. InMinho Lee, Akira Hirose, Zeng-Guang Hou, and Rhee Man Kil (eds.), Neural Information Processing - 20thInternational Conference, ICONIP 2013, Daegu, Korea, November 3-7, 2013. Proceedings, Part II, volume",
  "Pratyay Banerjee, Kuntal Kumar Pal, Fish Wang, and Chitta Baral. Variable name recovery in decompiledbinary code using constrained masked language modeling. CoRR, abs/2103.12801, 2021. URL": "Aakash Bansal, Sakib Haque, and Collin McMillan. Project-level encoding for neural source code summa-rization of subroutines. In 29th IEEE/ACM International Conference on Program Comprehension, ICPC2021, Madrid, Spain, May 20-21, 2021, pp. 253264. IEEE, 2021. doi: 10.1109/ICPC52881.2021.00032.URL Patrick Barei, Beatriz Souza, Marcelo dAmorim, and Michael Pradel. Code generation tools (almost)for free? A study of few-shot, pre-trained language models on code. CoRR, abs/2206.01335, 2022. doi:10.48550/arXiv.2206.01335. URL Mike Barnett, Christian Bird, Joo Brunet, and Shuvendu K. Lahiri. Helping developers help themselves:Automatic decomposition of code review changesets. In Antonia Bertolino, Gerardo Canfora, and Se-bastian G. Elbaum (eds.), 37th IEEE/ACM International Conference on Software Engineering, ICSE2015, Florence, Italy, May 16-24, 2015, Volume 1, pp. 134144. IEEE Computer Society, 2015.doi:10.1109/ICSE.2015.35. URL Antonio Valerio Miceli Barone and Rico Sennrich. A parallel corpus of python functions and documentationstrings for automated code documentation and code generation. In Greg Kondrak and Taro Watanabe(eds.), Proceedings of the Eighth International Joint Conference on Natural Language Processing, IJCNLP2017, Taipei, Taiwan, November 27 - December 1, 2017, Volume 2: Short Papers, pp. 314319. AsianFederation of Natural Language Processing, 2017. URL Ezio Bartocci, Leonardo Mariani, Dejan Nickovic, and Drishti Yadav. Property-based mutation testing. InIEEE Conference on Software Testing, Verification and Validation, ICST 2023, Dublin, Ireland, April16-20, 2023, pp. 222233. IEEE, 2023. doi: 10.1109/ICST57152.2023.00029. URL Nauman Bashir, Muhammad Bilal, Misbah Liaqat, Mohsen Marjani, Nadia Malik, and Mohsin Ali. Modelingclass diagram using nlp in object-oriented designing. In 2021 National Computing Colleges Conference(NCCC), pp. 16, 2021. doi: 10.1109/NCCC49330.2021.9428817. Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek, andMark Chen. Efficient training of language models to fill in the middle. CoRR, abs/2207.14255, 2022. doi:10.48550/arXiv.2207.14255. URL Rohan Bavishi, Caroline Lemieux, Roy Fox, Koushik Sen, and Ion Stoica.Autopandas: neural-backedgenerators for program synthesis.Proc. ACM Program. Lang., 3(OOPSLA):168:1168:27, 2019.doi:10.1145/3360594. URL Tony Beltramelli. pix2code: Generating code from a graphical user interface screenshot. In Proceedings ofthe ACM SIGCHI Symposium on Engineering Interactive Computing Systems, EICS 2018, Paris, France,June 19-22, 2018, pp. 3:13:6. ACM, 2018. doi: 10.1145/3220134.3220135. URL Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. Neural code comprehension: A learnablerepresentation of code semantics. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman,Nicol Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31:Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,Montral, Canada, pp. 35893601, 2018. URL",
  "Yingzhou Bi, Jiangtao Huang, Penghui Liu, and Lianmei Wang. Benchmarking software vulnerability de-tection techniques: A survey. CoRR, abs/2303.16362, 2023. doi: 10.48550/ARXIV.2303.16362. URL": "Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin Lu, Zili Zhang, Yulei Sui,Xuanhua Shi, and Hai Jin. Iterative refinement of project-level code context for precise code generationwith compiler feedback. CoRR, abs/2403.16792, 2024. doi: 10.48550/ARXIV.2403.16792. URL Pan Bian, Bin Liang, Wenchang Shi, Jianjun Huang, and Yan Cai. Nar-miner: discovering negative as-sociation rules from code for bug detection.In Gary T. Leavens, Alessandro Garcia, and Corina S.Pasareanu (eds.), Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Con-ference and Symposium on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, LakeBuena Vista, FL, USA, November 04-09, 2018, pp. 411422. ACM, 2018. doi: 10.1145/3236024.3236032.URL Benjamin Bichsel, Veselin Raychev, Petar Tsankov, and Martin T. Vechev. Statistical deobfuscation of an-droid applications. In Edgar R. Weippl, Stefan Katzenbeisser, Christopher Kruegel, Andrew C. Myers, andShai Halevi (eds.), Proceedings of the 2016 ACM SIGSAC Conference on Computer and CommunicationsSecurity, Vienna, Austria, October 24-28, 2016, pp. 343355. ACM, 2016. doi: 10.1145/2976749.2978422.URL",
  "Stella Biderman, Kieran Bicheno, and Leo Gao. Datasheet for the pile. CoRR, abs/2201.07311, 2022. URL": "Pavol Bielik, Veselin Raychev, and Martin T. Vechev. PHOG: probabilistic model for code. In Maria-FlorinaBalcan and Kilian Q. Weinberger (eds.), Proceedings of the 33nd International Conference on MachineLearning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop andConference Proceedings, pp. 29332942. JMLR.org, 2016. URL Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth, Shivanshu Purohit,Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.GPT-NeoX-20B: An open-sourceautoregressive language model. In Angela Fan, Suzana Ilic, Thomas Wolf, and Matthias Gall (eds.),Proceedings of BigScience Episode #5 Workshop on Challenges & Perspectives in Creating Large Lan-guage Models, pp. 95136, virtual+Dublin, May 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.bigscience-1.9. URL",
  "Casper Boone, Niels de Bruin, Arjan Langerak, and Fabian Stelmach. Dltpy: Deep learning type inferenceof python function signatures using natural language context. CoRR, abs/1912.00680, 2019. URL": "David Bingham Brown, Michael Vaughn, Ben Liblit, and Thomas W. Reps. The care and feeding of wild-caught mutants. In Eric Bodden, Wilhelm Schfer, Arie van Deursen, and Andrea Zisman (eds.), Proceed-ings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017, Pader-born, Germany, September 4-8, 2017, pp. 511522. ACM, 2017. doi: 10.1145/3106237.3106280. URL Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, JeffreyWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, andDario Amodei.Language models are few-shot learners.In Hugo Larochelle, MarcAurelio Ranzato,Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Pro-cessing Systems 33:Annual Conference on Neural Information Processing Systems 2020, NeurIPS2020, December 6-12, 2020, virtual, 2020. URL Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from examples to improve code completionsystems. In Hans van Vliet and Valrie Issarny (eds.), Proceedings of the 7th joint meeting of the EuropeanSoftware Engineering Conference and the ACM SIGSOFT International Symposium on Foundations ofSoftware Engineering, 2009, Amsterdam, The Netherlands, August 24-28, 2009, pp. 213222. ACM, 2009.doi: 10.1145/1595696.1595728. URL Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, PeterLee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tlio Ribeiro, andYi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712,2023. doi: 10.48550/arXiv.2303.12712. URL Nghi Bui, Yue Wang, and Steven C. H. Hoi. Detect-localize-repair: A unified framework for learning to debugwith codet5. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association forComputational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp.812823. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.57. URL",
  "Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug.CoRR, abs/2304.05128, 2023f.doi: 10.48550/arXiv.2304.05128.URL": "Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. Comments as natural logicpivots: Improve code generation via comment perspective. CoRR, abs/2404.07549, 2024. doi: 10.48550/ARXIV.2404.07549. URL Yizheng Chen, Zhoujie Ding, Lamya Alowain, Xinyun Chen, and David A. Wagner. Diversevul: A newvulnerable source code dataset for deep learning based vulnerability detection.In Proceedings of the26th International Symposium on Research in Attacks, Intrusions and Defenses, RAID 2023, Hong Kong,China, October 16-18, 2023, pp. 654668. ACM, 2023g. doi: 10.1145/3607199.3607242. URL",
  "Zhuangbin Chen, Jinyang Liu, Wenwei Gu, Yuxin Su, and Michael R. Lyu.Experience report: Deeplearning-based system log analysis for anomaly detection. CoRR, abs/2107.05908, 2021d. URL": "Zimin Chen, Steve Kommrusch, Michele Tufano, Louis-Nol Pouchet, Denys Poshyvanyk, and Martin Mon-perrus. Sequencer: Sequence-to-sequence learning for end-to-end program repair. IEEE Trans. SoftwareEng., 47(9):19431959, 2021e. doi: 10.1109/TSE.2019.2940179. URL Chin-Yi Cheng, Forrest Huang, Gang Li, and Yang Li. Play: Parametrically conditioned layout generationusing latent diffusion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, SivanSabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp.54495471. PMLR, 2023. URL",
  "David Chiang. Hierarchical phrase-based translation. Comput. Linguistics, 33(2):201228, 2007. doi: 10.1162/coli.2007.33.2.201. URL": "Muslim Chochlov, Gul Aftab Ahmed, James Vincent Patten, Guoxian Lu, Wei Hou, David Gregg, andJim Buckley.Using a nearest-neighbour, bert-based approach for scalable clone detection.In IEEEInternational Conference on Software Maintenance and Evolution, ICSME 2022, Limassol, Cyprus,October 3-7, 2022, pp. 582591. IEEE, 2022.doi:10.1109/ICSME55016.2022.00080.URL DongHyun Choi, Myeongcheol Shin, EungGyun Kim, and Dong Ryeol Shin. RYANSQL: recursively applyingsketch-based slot fillings for complex text-to-sql in cross-domain databases. Comput. Linguistics, 47(2):309332, 2021. doi: 10.1162/coli\\_a\\_00403. URL",
  "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse,and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL": "Henry Coles, Thomas Laurent, Christopher Henard, Mike Papadakis, and Anthony Ventresque.PIT: apractical mutation testing tool for java (demo).In Andreas Zeller and Abhik Roychoudhury (eds.),Proceedings of the 25th International Symposium on Software Testing and Analysis, ISSTA 2016, Saar-brcken, Germany, July 18-20, 2016, pp. 449452. ACM, 2016. doi: 10.1145/2931037.2948707. URL Christian S. Collberg and Clark Thomborson. Watermarking, tamper-proofing, and obfuscation - tools forsoftware protection. IEEE Transactions on Software Engineering, 28(8):735746, August 2002. ISSN 0098-5589. doi: 10.1109/TSE.2002.1027797. Funding Information: The authors are grateful for the extensiveand insightful comments of two anonymous referees. This material is based upon work supported by theUS National Science Foundation under Grant No. 0073483. Alexis Conneau and Guillaume Lample.Cross-lingual language model pretraining.In Hanna M.Wallach,Hugo Larochelle,Alina Beygelzimer,Florence dAlch-Buc,Emily B. Fox,and Ro-man Garnett (eds.), Advances in Neural Information Processing Systems 32:Annual Conferenceon Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-ver, BC, Canada, pp. 70577067, 2019.URL Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingualrepresentation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL2020, Online, July 5-10, 2020, pp. 84408451. Association for Computational Linguistics, 2020.doi:10.18653/v1/2020.acl-main.747. URL Luis Fernando Cortes-Coy, Mario Linares Vsquez, Jairo Aponte, and Denys Poshyvanyk. On automaticallygenerating commit messages via summarization of source code changes.In 14th IEEE InternationalWorking Conference on Source Code Analysis and Manipulation, SCAM 2014, Victoria, BC, Canada,September 28-29, 2014, pp. 275284. IEEE Computer Society, 2014. doi: 10.1109/SCAM.2014.14. URL Viktor Csuvik and Lszl Vidcs. Fixjs: A dataset of bug-fixing javascript commits. In 19th IEEE/ACMInternational Conference on Mining Software Repositories, MSR 2022, Pittsburgh, PA, USA, May 23-24, 2022, pp. 712716. ACM, 2022. doi: 10.1145/3524842.3528480. URL Haotian Cui, Chenglong Wang, Junjie Huang, Jeevana Priya Inala, Todd Mytkowicz, Bo Wang, JianfengGao, and Nan Duan. Codeexp: Explanatory code document generation. In Yoav Goldberg, ZornitsaKozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022,Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 23422354. Association for ComputationalLinguistics, 2022. doi: 10.18653/V1/2022.FINDINGS-EMNLP.174. URL",
  "Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. URL": "Adailton Ferreira de Arajo and Ricardo Marcondes Marcacini. RE-BERT: automatic extraction of softwarerequirements from app reviews using BERT language model. In Chih-Cheng Hung, Jiman Hong, AlessioBechini, and Eunjee Song (eds.), SAC 21: The 36th ACM/SIGAPP Symposium on Applied Computing,Virtual Event, Republic of Korea, March 22-26, 2021, pp. 13211327. ACM, 2021. doi: 10.1145/3412841.3442006. URL Nelson Tavares de Sousa and Wilhelm Hasselbring.Javabert: Training a transformer-based model forthe java programming language. In 36th IEEE/ACM International Conference on Automated SoftwareEngineering, ASE 2021 - Workshops, Melbourne, Australia, November 15-19, 2021, pp. 9095. IEEE, 2021.doi: 10.1109/ASEW52652.2021.00028. URL DeepSeek-AI, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, HonghuiDing, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan,Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, ErhangLi, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu,Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, TianPei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, ZhihongShao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi",
  "Xiang Deng, Prashant Shiralkar, Colin Lockard, Binxuan Huang, and Huan Sun.DOM-LM: learn-ing generalizable representations for HTML documents. CoRR, abs/2201.10608, 2022b. URL": "Yinlin Deng, Chenyuan Yang, Anjiang Wei, and Lingming Zhang. Fuzzing deep-learning libraries via au-tomated relational API inference. In Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (eds.),Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on theFoundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, pp.4456. ACM, 2022c. doi: 10.1145/3540250.3549085. URL Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming Zhang. Large languagemodels are zero-shot fuzzers: Fuzzing deep-learning libraries via large language models. In Ren Justand Gordon Fraser (eds.), Proceedings of the 32nd ACM SIGSOFT International Symposium on SoftwareTesting and Analysis, ISSTA 2023, Seattle, WA, USA, July 17-21, 2023, pp. 423435. ACM, 2023. doi:10.1145/3597926.3598067. URL Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and PushmeetKohli.Robustfill: Neural program learning under noisy I/O.In Doina Precup and Yee Whye Teh(eds.), Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 990998. PMLR,2017a. URL",
  "Jacob Devlin, Jonathan Uesato, Rishabh Singh, and Pushmeet Kohli. Semantic code repair using neuro-symbolic transformation networks. CoRR, abs/1710.11054, 2017b. URL": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidi-rectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio(eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June2-7, 2019, Volume 1 (Long and Short Papers), pp. 41714186. Association for Computational Linguistics,2019. doi: 10.18653/v1/n19-1423. URL Peng Di, Jianguo Li, Hang Yu, Wei Jiang, Wenting Cai, Yang Cao, Chaoyu Chen, Dajun Chen, HongweiChen, Liang Chen, Gang Fan, Jie Gong, Zi Gong, Wen Hu, Tingting Guo, Zhichao Lei, Ting Li, ZhengLi, Ming Liang, Cong Liao, Bingchang Liu, Jiachen Liu, Zhiwei Liu, Shaojun Lu, Min Shen, GuangpeiWang, Huan Wang, Zhi Wang, Zhaogui Xu, Jiawei Yang, Qing Ye, Gehao Zhang, Yu Zhang, Zelin Zhao,Xunjin Zheng, Hailian Zhou, Lifu Zhu, and Xianying Zhu. Codefuse-13b: A pretrained multi-lingual codelarge language model. CoRR, abs/2310.06266, 2023. doi: 10.48550/ARXIV.2310.06266. URL Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and Shuvendu K. Lahiri. TOGA: A neural method fortest oracle generation. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 21302141. ACM, 2022. doi: 10.1145/3510003.3510141.URL Yangruibo Ding, Luca Buratti, Saurabh Pujar, Alessandro Morari, Baishakhi Ray, and Saikat Chakraborty.Towards learning (dis)-similarity of source code from program contrasts. In Smaranda Muresan, PreslavNakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp.63006312. Association for Computational Linguistics, 2022a. doi: 10.18653/v1/2022.acl-long.436. URL Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati,Parminder Bhatia, Dan Roth, and Bing Xiang.Cocomic: Code completion by jointly modeling in-file and cross-file context.CoRR, abs/2212.10007, 2022b.doi:10.48550/ARXIV.2212.10007.URL Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali KrishnaRamanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. Crosscodeeval: A diverseand multilingual benchmark for cross-file code completion. CoRR, abs/2310.11248, 2023. doi: 10.48550/ARXIV.2310.11248. URL",
  "Yangruibo Ding, Marcus J. Min, Gail E. Kaiser, and Baishakhi Ray. CYCLE: learning to self-refine the codegeneration. CoRR, abs/2403.18746, 2024. doi: 10.48550/ARXIV.2403.18746. URL": "Brendan Dolan-Gavitt, Patrick Hulin, Engin Kirda, Tim Leek, Andrea Mambretti, William K. Robertson,Frederick Ulrich, and Ryan Whelan. LAVA: large-scale automated vulnerability addition. In IEEE Sym-posium on Security and Privacy, SP 2016, San Jose, CA, USA, May 22-26, 2016, pp. 110121. IEEEComputer Society, 2016. doi: 10.1109/SP.2016.15. URL Jinhao Dong, Yiling Lou, Qihao Zhu, Zeyu Sun, Zhilin Li, Wenjie Zhang, and Dan Hao.FIRA: fine-grained graph-based code change representation for automated commit message generation.In 44thIEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA,May 25-27, 2022, pp. 970981. ACM, 2022. doi: 10.1145/3510003.3510069. URL",
  "Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li.Self-collaboration code generation via chatgpt.CoRR,abs/2304.07590, 2023.doi: 10.48550/ARXIV.2304.07590.URL": "Shihan Dou, Junjie Shan, Haoxiang Jia, Wenhao Deng, Zhiheng Xi, Wei He, Yueming Wu, Tao Gui, YangLiu, and Xuanjing Huang. Towards understanding the capability of large language models on code clonedetection: A survey. CoRR, abs/2308.01191, 2023. doi: 10.48550/arXiv.2308.01191. URL Shihan Dou, Yan Liu, Haoxiang Jia, Limao Xiong, Enyu Zhou, Wei Shen, Junjie Shan, Caishuang Huang,Xiao Wang, Xiaoran Fan, Zhiheng Xi, Yuhao Zhou, Tao Ji, Rui Zheng, Qi Zhang, Xuanjing Huang,and Tao Gui. Stepcoder: Improve code generation with reinforcement learning from compiler feedback.CoRR, abs/2402.01391, 2024.doi: 10.48550/ARXIV.2402.01391.URL",
  "Iddo Drori and Nakul Verma. Solving linear algebra by program synthesis. CoRR, abs/2111.08171, 2021.URL": "Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, LindaChen, Sunny Tran, Newman Cheng, Roman Wang, Nikhil Singh, Taylor L. Patti, Jayson Lynch, AviShporer, Nakul Verma, Eugene Wu, and Gilbert Strang. A neural network solves, explains, and generatesuniversity math problems by program synthesis and few-shot learning at human level. Proceedings of theNational Academy of Sciences (PNAS), 119(32), 2022. Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and Dongmei Zhang.Is a single modelenough? mucos: A multi-model ensemble learning approach for semantic code search. In Gianluca De-martini, Guido Zuccon, J. Shane Culpepper, Zi Huang, and Hanghang Tong (eds.), CIKM 21: The30th ACM International Conference on Information and Knowledge Management, Virtual Event, Queens-land, Australia, November 1 - 5, 2021, pp. 29942998. ACM, 2021. doi: 10.1145/3459637.3482127. URL Min Du and Feifei Li. Spell: Streaming parsing of system event logs. In Francesco Bonchi, Josep Domingo-Ferrer, Ricardo Baeza-Yates, Zhi-Hua Zhou, and Xindong Wu (eds.), IEEE 16th International Conferenceon Data Mining, ICDM 2016, December 12-15, 2016, Barcelona, Spain, pp. 859864. IEEE ComputerSociety, 2016. doi: 10.1109/ICDM.2016.0103. URL",
  "Alessio Ferrari, Sallam Abualhaija, and Chetan Arora.Model generation from requirements with llms:an exploratory study. CoRR, abs/2404.06371, 2024. doi: 10.48550/ARXIV.2404.06371. URL": "Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam,Rui Zhang, and Dragomir R. Radev. Improving text-to-sql evaluation methodology. In Iryna Gurevychand Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Association for Computa-tional Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp.351360. Association for Computational Linguistics, 2018.doi: 10.18653/v1/P18-1033.URL Georgia Frantzeskou, Stephen G. MacDonell, Efstathios Stamatatos, Stelios Georgiou, and Stefanos Gritzalis.The significance of user-defined identifiers in java source code authorship identification. Comput. Syst.Sci. Eng., 26(2), 2011. Gordon Fraser and Andrea Arcuri. Evosuite: automatic test suite generation for object-oriented software.In Tibor Gyimthy and Andreas Zeller (eds.), SIGSOFT/FSE11 19th ACM SIGSOFT Symposium onthe Foundations of Software Engineering (FSE-19) and ESEC11: 13th European Software Engineering",
  "Gordon Fraser and Andrea Arcuri. A large-scale evaluation of automated unit test generation using evosuite.ACM Trans. Softw. Eng. Methodol., 24(2):8:18:42, 2014. doi: 10.1145/2685612. URL": "Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih,Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. In TheEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net, 2023. URL Lingyue Fu, Huacan Chai, Shuang Luo, Kounianhua Du, Weiming Zhang, Longteng Fan, Jiayi Lei, RentingRui, Jianghao Lin, Yuchen Fang, Yifan Liu, Jingkuan Wang, Siyuan Qi, Kangning Zhang, Weinan Zhang,and Yong Yu. Codeapex: A bilingual programming evaluation benchmark for large language models.CoRR, abs/2309.01940, 2023. doi: 10.48550/arXiv.2309.01940. URL Michael Fu and Chakkrit Tantithamthavorn. Linevul: A transformer-based line-level vulnerability prediction.In 19th IEEE/ACM International Conference on Mining Software Repositories, MSR 2022, Pittsburgh,PA, USA, May 23-24, 2022, pp. 608620. ACM, 2022.doi: 10.1145/3524842.3528452.URL Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, and Dinh Q. Phung. Vulrepair: a t5-basedautomated software vulnerability repair.In Abhik Roychoudhury, Cristian Cadar, and Miryung Kim(eds.), Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium onthe Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022,pp. 935947. ACM, 2022. doi: 10.1145/3540250.3549098. URL Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and IgnacioThayer. Scalable inference and training of context-rich syntactic translation models. In Nicoletta Calzolari,Claire Cardie, and Pierre Isabelle (eds.), ACL 2006, 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of theConference, Sydney, Australia, 17-21 July 2006. The Association for Computer Linguistics, 2006. doi:10.3115/1220175.1220296. URL Yujian Gan, Xinyun Chen, Qiuping Huang, Matthew Purver, John R. Woodward, Jinxia Xie, and Peng-sheng Huang.Towards robustness of text-to-sql models against synonym substitution.In ChengqingZong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Asso-ciation for Computational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 25052515. Association for Computational Linguistics, 2021a. doi: 10.18653/V1/2021.ACL-LONG.195. URL Yujian Gan, Xinyun Chen, and Matthew Purver.Exploring underexplored limitations of cross-domaintext-to-sql generalization. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tauYih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 89268931.Association for Computational Linguistics, 2021b.doi: 10.18653/V1/2021.EMNLP-MAIN.702.URL Yujian Gan, Xinyun Chen, Qiuping Huang, and Matthew Purver. Measuring and improving compositionalgeneralization in text-to-sql via component alignment. In Marine Carpuat, Marie-Catherine de Marneffe,and Ivn Vladimir Meza Ruz (eds.), Findings of the Association for Computational Linguistics: NAACL2022, Seattle, WA, United States, July 10-15, 2022, pp. 831843. Association for Computational Linguis-tics, 2022. doi: 10.18653/V1/2022.FINDINGS-NAACL.62. URL",
  "Zeyu Gao, Hao Wang, Yuchen Zhou, Wenyu Zhu, and Chao Zhang. How far have we gone in vulnerabilitydetection using large language models. CoRR, abs/2311.12420, 2023d. doi: 10.48550/ARXIV.2311.12420.URL": "Luca Gazzola, Daniela Micucci, and Leonardo Mariani. Automatic software repair: a survey. In MichelChaudron, Ivica Crnkovic, Marsha Chechik, and Mark Harman (eds.), Proceedings of the 40th InternationalConference on Software Engineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, pp. 1219.ACM, 2018. doi: 10.1145/3180155.3182526. URL Mingyang Geng, Shangwen Wang, Dezun Dong, Haotian Wang, Ge Li, Zhi Jin, Xiaoguang Mao, and XiangkeLiao. Large language models are few-shot summarizers: Multi-intent comment generation via in-contextlearning. In Proceedings of the 46th IEEE/ACM International Conference on Software Engineering, ICSE2024, Lisbon, Portugal, April 14-20, 2024, pp. 39:139:13. ACM, 2024. doi: 10.1145/3597503.3608134.URL Shalini Ghosh, Daniel Elenius, Wenchao Li, Patrick Lincoln, Natarajan Shankar, and Wilfried Steiner. AR-SENAL: automatic requirements specification extraction from natural language. In Sanjai Rayadurgamand Oksana Tkachuk (eds.), NASA Formal Methods - 8th International Symposium, NFM 2016, Min-neapolis, MN, USA, June 7-9, 2016, Proceedings, volume 9690 of Lecture Notes in Computer Science,pp. 4146. Springer, 2016.doi:10.1007/978-3-319-40648-0\\_4.URL",
  "M. Gopinath and Sibi Chakkaravarthy Sethuraman. A comprehensive survey on deep learning based malwaredetection techniques. Comput. Sci. Rev., 47:100529, 2023. doi: 10.1016/J.COSREV.2022.100529. URL": "Claire Le Goues, Neal J. Holtschulte, Edward K. Smith, Yuriy Brun, Premkumar T. Devanbu, StephanieForrest, and Westley Weimer.The manybugs and introclass benchmarks for automated repair of Cprograms. IEEE Trans. Software Eng., 41(12):12361256, 2015. doi: 10.1109/TSE.2015.2454513. URL Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-ishnan, MarcAurelio Ranzato, Francisco Guzmn, and Angela Fan. The flores-101 evaluation benchmarkfor low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522538,2022. doi: 10.1162/TACL\\_A\\_00474. URL",
  "Dejan Grubisic, Chris Cummins, Volker Seeker, and Hugh Leather. Compiler generated feedback for largelanguage models. CoRR, abs/2403.14714, 2024a. doi: 10.48550/ARXIV.2403.14714. URL": "Dejan Grubisic, Volker Seeker, Gabriel Synnaeve, Hugh Leather, John M. Mellor-Crummey, and ChrisCummins. Priority sampling of large language models for compilers. In Proceedings of the 4th Workshopon Machine Learning and Systems, EuroMLSys 2024, Athens, Greece, 22 April 2024, pp. 9197. ACM,2024b. doi: 10.1145/3642970.3655831. URL Alex Gu, Baptiste Rozire, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida I. Wang.Cruxeval: A benchmark for code reasoning, understanding and execution. CoRR, abs/2401.03065, 2024.doi: 10.48550/ARXIV.2401.03065. URL Jian Gu, Zimin Chen, and Martin Monperrus. Multimodal representation for neural code search. In IEEEInternational Conference on Software Maintenance and Evolution, ICSME 2021, Luxembourg, September27 - October 1, 2021, pp. 483494. IEEE, 2021. doi: 10.1109/ICSME52107.2021.00049. URL Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. Muffin: Testing deep learning libraries via neuralarchitecture fuzzing. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 14181430. ACM, 2022. doi: 10.1145/3510003.3510092.URL Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. Deepam: Migrate apis with multi-modalsequence to sequence learning. In Carles Sierra (ed.), Proceedings of the Twenty-Sixth International JointConference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pp. 36753681. ijcai.org, 2017. doi: 10.24963/IJCAI.2017/514. URL Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In Michel Chaudron, Ivica Crnkovic,Marsha Chechik, and Mark Harman (eds.), Proceedings of the 40th International Conference on SoftwareEngineering, ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018, pp. 933944. ACM, 2018. doi:10.1145/3180155.3180167. URL Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Yulei Sui, Pan Zhou, and Lichao Sun.Codeip: A grammar-guided multi-bit watermark for large language models of code. CoRR, abs/2404.15639,2024. doi: 10.48550/ARXIV.2404.15639. URL Vitor Guilherme and Auri Vincenzi. An initial investigation of chatgpt unit test generation capability. InAwdren L. Fonto, Dbora M. B. Paiva, Hudson Borges, Maria Istela Cagnin, Patrcia Gomes Fernandes,Vanessa Borges, Silvana M. Melo, Vinicius H. S. Durelli, and Edna Dias Canedo (eds.), 8th BrazilianSymposium on Systematic and Automated Software Testing, SAST 2023, Campo Grande, MS, Brazil,",
  "September 25-29, 2023, pp. 1524. ACM, 2023. doi: 10.1145/3624032.3624035. URL": "Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harki-rat Singh Behl, Xin Wang, Sbastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, andYuanzhi Li. Textbooks are all you need. CoRR, abs/2306.11644, 2023. doi: 10.48550/arXiv.2306.11644.URL Tuge Gnes and Fatma Basak Aydemir. Automated goal model extraction from user stories using NLP.In Travis D. Breaux, Andrea Zisman, Samuel Fricker, and Martin Glinz (eds.), 28th IEEE InternationalRequirements Engineering Conference, RE 2020, Zurich, Switzerland, August 31 - September 4, 2020,pp. 382387. IEEE, 2020. doi: 10.1109/RE48521.2020.00052. URL Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang, and Ting Wang. Retrieval-augmented gpt-3.5-based text-to-sql framework with sample-aware prompting and dynamic revision chain.In Biao Luo, Long Cheng, Zheng-Guang Wu, Hongyi Li, and Chaojie Li (eds.), Neural Information Pro-cessing - 30th International Conference, ICONIP 2023, Changsha, China, November 20-23, 2023, Pro-ceedings, Part VI, volume 14452 of Lecture Notes in Computer Science, pp. 341356. Springer, 2023a. doi:10.1007/978-981-99-8076-5\\_25. URL Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Alexey Svy-atkovskiy, Shengyu Fu, Michele Tufano, Shao Kun Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan,Jian Yin, Daxin Jiang, and Ming Zhou. Graphcodebert: Pre-training code representations with data flow.In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May3-7, 2021. OpenReview.net, 2021a. URL Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin.Unixcoder: Unified cross-modal pre-training for code representation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio(eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 72127225. Association for Compu-tational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.499. URL Daya Guo, Canwen Xu, Nan Duan, Jian Yin, and Julian J. McAuley. Longcoder: A long-range pre-trainedlanguage model for code completion. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, BarbaraEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning,ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine LearningResearch, pp. 1209812107. PMLR, 2023b. URL Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu,Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language modelmeets programming - the rise of code intelligence. CoRR, abs/2401.14196, 2024. doi: 10.48550/ARXIV.2401.14196. URL Haixuan Guo, Shuhan Yuan, and Xintao Wu. Logbert: Log anomaly detection via BERT. In InternationalJoint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021, pp. 18. IEEE,2021b. doi: 10.1109/IJCNN52387.2021.9534113. URL Jiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-Guang Lou, Ting Liu, and Dongmei Zhang.To-wards complex text-to-sql in cross-domain database with intermediate representation. In Anna Korhonen,David R. Traum, and Llus Mrquez (eds.), Proceedings of the 57th Conference of the Association forComputational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Pa-pers, pp. 45244535. Association for Computational Linguistics, 2019. doi: 10.18653/v1/p19-1444. URL",
  "Anshul Gupta and Neel Sundaresan. Intelligent code reviews using deep learning, 2018": "Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish K. Shevade. Deepfix: Fixing common C languageerrors by deep learning. In Satinder Singh and Shaul Markovitch (eds.), Proceedings of the Thirty-FirstAAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pp.13451351. AAAI Press, 2017. doi: 10.1609/aaai.v31i1.10742. URL Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sha-ran Narang, Noah Fiedel, and Aleksandra Faust.Understanding HTML with large language mod-els.In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Com-putational Linguistics:EMNLP 2023, Singapore, December 6-10, 2023, pp. 28032821. Associationfor Computational Linguistics, 2023.doi: 10.18653/V1/2023.FINDINGS-EMNLP.185.URL Emitza Guzman, Mohamed Ibrahim, and Martin Glinz. A little bird told me: Mining tweets for requirementsand software evolution. In Ana Moreira, Joo Arajo, Jane Hayes, and Barbara Paech (eds.), 25th IEEEInternational Requirements Engineering Conference, RE 2017, Lisbon, Portugal, September 4-8, 2017, pp.1120. IEEE Computer Society, 2017. doi: 10.1109/RE.2017.88. URL Pter Gyimesi, Bla Vancsics, Andrea Stocco, Davood Mazinanian, rpd Beszdes, Rudolf Ferenc, andAli Mesbah. Bugsjs: a benchmark of javascript bugs. In 12th IEEE Conference on Software Testing,Validation and Verification, ICST 2019, Xian, China, April 22-27, 2019, pp. 90101. IEEE, 2019. doi:10.1109/ICST.2019.00019. URL",
  "Rajarshi Haldar and Julia Hockenmaier.Analyzing the performance of large language models on codesummarization. CoRR, abs/2404.08018, 2024. doi: 10.48550/ARXIV.2404.08018. URL": "Patrick Haller, Jonas Golde, and Alan Akbik. PECC: problem extraction and coding challenges. In NicolettaCalzolari, Min-Yen Kan, Vronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.),Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resourcesand Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy, pp. 1269012699. ELRA andICCL, 2024. URL Sivana Hamer, Marcelo dAmorim, and Laurie Williams.Just another copy and paste?comparing thesecurity vulnerabilities of chatgpt generated code and stackoverflow answers.CoRR, abs/2403.15600,2024. doi: 10.48550/ARXIV.2403.15600. URL Quinn Hanam, Fernando Santos De Mattos Brito, and Ali Mesbah. Discovering bug patterns in javascript.In Thomas Zimmermann, Jane Cleland-Huang, and Zhendong Su (eds.), Proceedings of the 24th ACMSIGSOFT International Symposium on Foundations of Software Engineering, FSE 2016, Seattle, WA,USA, November 13-18, 2016, pp. 144156. ACM, 2016.doi: 10.1145/2950290.2950308.URL Hazim Hanif and Sergio Maffeis. Vulberta: Simplified source code pre-training for vulnerability detection. InInternational Joint Conference on Neural Networks, IJCNN 2022, Padua, Italy, July 18-23, 2022, pp. 18. IEEE, 2022. doi: 10.1109/IJCNN55064.2022.9892280. URL Yiyang Hao, Ge Li, Yongqiang Liu, Xiaowei Miao, He Zong, Siyuan Jiang, Yang Liu, and He Wei. Aixbench:A code generation benchmark dataset. CoRR, abs/2206.13179, 2022. doi: 10.48550/arXiv.2206.13179.URL",
  "Moshe Hazoom, Vibhor Malik, and Ben Bogin. Text-to-sql in the wild: A naturally-occurring dataset basedon stack exchange data. CoRR, abs/2106.05006, 2021. URL": "Jingxuan He, Pesho Ivanov, Petar Tsankov, Veselin Raychev, and Martin T. Vechev. Debin: Predicting debuginformation in stripped binaries. In David Lie, Mohammad Mannan, Michael Backes, and XiaoFeng Wang(eds.), Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security,CCS 2018, Toronto, ON, Canada, October 15-19, 2018, pp. 16671680. ACM, 2018. doi: 10.1145/3243734.3243866. URL Pinjia He, Jieming Zhu, Zibin Zheng, and Michael R. Lyu. Drain: An online log parsing approach withfixed depth tree. In Ilkay Altintas and Shiping Chen (eds.), 2017 IEEE International Conference on WebServices, ICWS 2017, Honolulu, HI, USA, June 25-30, 2017, pp. 3340. IEEE, 2017. doi: 10.1109/ICWS.2017.13. URL",
  "Shilin He, Jieming Zhu, Pinjia He, and Michael R. Lyu. Loghub: A large collection of system log datasetstowards automated log analytics. CoRR, abs/2008.06448, 2020. URL": "Xinyi He, Jiaru Zou, Yun Lin, Mengyu Zhou, Shi Han, Zejian Yuan, and Dongmei Zhang. CONLINE: com-plex code generation and refinement with online searching and correctness testing. CoRR, abs/2403.13583,2024. doi: 10.48550/ARXIV.2403.13583. URL Yichen He, Liran Wang, Kaiyi Wang, Yupeng Zhang, Hang Zhang, and Zhoujun Li.COME: commitmessage generation with modification embedding. In Ren Just and Gordon Fraser (eds.), Proceedingsof the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2023,Seattle, WA, USA, July 17-21, 2023, pp. 792803. ACM, 2023a. doi: 10.1145/3597926.3598096. URL Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, ShumingShi, and Xing Wang.Exploring human-like translation strategy with large language models.CoRR,abs/2305.04118, 2023b. doi: 10.48550/ARXIV.2305.04118. URL Joy He-Yueya, Gabriel Poesia, Rose E. Wang, and Noah D. Goodman. Solving math word problems bycombining language models with symbolic solvers. CoRR, abs/2304.09102, 2023. doi: 10.48550/ARXIV.2304.09102. URL",
  "Geert Heyman and Tom Van Cutsem. Neural code search revisited: Enhancing code snippet retrieval throughnatural language intent. CoRR, abs/2008.12193, 2020. URL": "Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. Devanbu. On the naturalnessof software. In Martin Glinz, Gail C. Murphy, and Mauro Pezz (eds.), 34th International Conferenceon Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland, pp. 837847. IEEE ComputerSociety, 2012. doi: 10.1109/ICSE.2012.6227135. URL Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle,MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in NeuralInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020.URL",
  "Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):17351780, 1997.doi: 10.1162/neco.1997.9.8.1735. URL": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, EricNoland, Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero,Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. An empirical analysis ofcompute-optimal large language model training. In Sanmi Koyejo, S. Mohamed, A. Agarwal, DanielleBelgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: AnnualConference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,November 28 - December 9, 2022, 2022. URL Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, StevenKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Metaprogramming for multi-agent collaborative framework.CoRR, abs/2308.00352, 2023.doi: 10.48550/ARXIV.2308.00352. URL Yang Hong, Chakkrit Tantithamthavorn, Patanamon Thongtanunam, and Aldeida Aleti. Commentfinder: asimpler, faster, more accurate code review comments recommendation. In Abhik Roychoudhury, CristianCadar, and Miryung Kim (eds.), Proceedings of the 30th ACM Joint European Software EngineeringConference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022, Singapore,Singapore, November 14-18, 2022, pp. 507519. ACM, 2022. doi: 10.1145/3540250.3549119. URL Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning languagemodels with (almost) no human labor. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki(eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1440914428. Association for Com-putational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.806. URL",
  "Iman Hosseini and Brendan Dolan-Gavitt. Beyond the C: retargetable decompilation using neural machinetranslation. CoRR, abs/2212.08950, 2022. doi: 10.48550/ARXIV.2212.08950. URL": "Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy,and Haoyu Wang. Large language models for software engineering: A systematic literature review. CoRR,abs/2308.10620, 2023.doi: 10.48550/ARXIV.2308.10620.URL Gang Hu, Min Peng, Yihan Zhang, Qianqian Xie, and Mengting Yuan. Neural joint attention code searchover structure embeddings for software q&a sites. J. Syst. Softw., 170:110773, 2020. doi: 10.1016/J.JSS.2020.110773. URL Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin.Deep code comment generation.In Foutse Khomh,Chanchal K. Roy, and Janet Siegmund (eds.), Proceedings of the 26th Conference on Program Com-prehension, ICPC 2018, Gothenburg, Sweden, May 27-28, 2018, pp. 200210. ACM, 2018a.doi:10.1145/3196321.3196334. URL Xing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi Jin. Summarizing source code with transferred APIknowledge. In Jrme Lang (ed.), Proceedings of the Twenty-Seventh International Joint Conference onArtificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 22692275. ijcai.org, 2018b.doi: 10.24963/ijcai.2018/314. URL",
  "Dong Huang, Jianbo Dai, Han Weng, Puzhen Wu, Yuhao Qing, Jie M Zhang, Heming Cui, and ZhijiangGuo. Soap: Enhancing efficiency of generated code via self-optimization. 2024. URL": "Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu, Daxin Jiang, Ming Zhou, and Nan Duan.Cosqa: 20, 000+ web queries for code search and question answering.In Chengqing Zong, Fei Xia,Wenjie Li, and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Associationfor Computational Linguistics and the 11th International Joint Conference on Natural Language Pro-cessing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 56905700. Association for Computational Linguistics, 2021.doi:10.18653/v1/2021.acl-long.442.URL Junjie Huang, Chenglong Wang, Jipeng Zhang, Cong Yan, Haotian Cui, Jeevana Priya Inala, Colin B.Clement, Nan Duan, and Jianfeng Gao.Execution-based evaluation for data science code generationmodels. CoRR, abs/2211.09374, 2022b. doi: 10.48550/ARXIV.2211.09374. URL",
  "Kai Huang, Zhengzi Xu, Su Yang, Hongyu Sun, Xuejun Li, Zheng Yan, and Yuqing Zhang. A survey onautomated program repair techniques. CoRR, abs/2303.18184, 2023a. doi: 10.48550/ARXIV.2303.18184.URL": "Qiao Huang, Xin Xia, Zhenchang Xing, David Lo, and Xinyu Wang. API method recommendation withoutworrying about the task-api knowledge gap. In Marianne Huchard, Christian Kstner, and Gordon Fraser(eds.), Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering,ASE 2018, Montpellier, France, September 3-7, 2018, pp. 293304. ACM, 2018. doi: 10.1145/3238147.3238191. URL Yuan Huang, Nan Jia, Hao-Jie Zhou, Xiangping Chen, Zibin Zheng, and Mingdong Tang. Learning human-written commit messages to document code changes. J. Comput. Sci. Technol., 35(6):12581277, 2020.doi: 10.1007/S11390-020-0496-0. URL Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu,Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He.C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. CoRR, abs/2305.08322, 2023b. doi:10.48550/arXiv.2305.08322. URL Faria Huq, Masum Hasan, Md. Mahim Anjum Haque, Sazan Mahbub, Anindya Iqbal, and Toufique Ahmed.Review4repair: Code review aided automatic program repairing. Inf. Softw. Technol., 143:106765, 2022.doi: 10.1016/J.INFSOF.2021.106765. URL",
  "Yufan Jiang, Qiaozhi He, Xiaomin Zhuang, and Zhihua Wu. Code comparison tuning for code large languagemodels. CoRR, abs/2403.19121, 2024b. doi: 10.48550/ARXIV.2403.19121. URL": "Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu,and Michael R. Lyu. Lilac: Log parsing using llms with adaptive parsing cache. CoRR, abs/2310.01796,2023d. doi: 10.48550/ARXIV.2310.01796. URL Zhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li, Yintong Huo, Jiazhen Gu, Zhuangbin Chen, JiemingZhu, and Michael R. Lyu. A large-scale benchmark for log parsing. CoRR, abs/2308.10828, 2023e. doi:10.48550/ARXIV.2308.10828. URL Mingsheng Jiao, Tingrui Yu, Xuan Li, Guanjie Qiu, Xiaodong Gu, and Beijun Shen. On the evaluation ofneural code translation: Taxonomy and benchmark. CoRR, abs/2308.08961, 2023. doi: 10.48550/ARXIV.2308.08961. URL Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and KarthikNarasimhan. Swe-bench: Can language models resolve real-world github issues? CoRR, abs/2310.06770,2023. doi: 10.48550/ARXIV.2310.06770. URL Timo Johann, Christoph Stanik, Alireza M. Alizadeh B., and Walid Maalej. SAFE: A simple approach forfeature extraction from app descriptions and app reviews. In Ana Moreira, Joo Arajo, Jane Hayes, andBarbara Paech (eds.), 25th IEEE International Requirements Engineering Conference, RE 2017, Lisbon,Portugal, September 4-8, 2017, pp. 2130. IEEE Computer Society, 2017. doi: 10.1109/RE.2017.71. URL Harshit Joshi, Jos Pablo Cambronero Snchez, Sumit Gulwani, Vu Le, Gust Verbruggen, and Ivan Radicek.Repair is nearly generation: Multilingual program repair with llms. In Brian Williams, Yiling Chen, andJennifer Neville (eds.), Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposiumon Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14,2023, pp. 51315140. AAAI Press, 2023. doi: 10.1609/AAAI.V37I4.25642. URL",
  "Tae-Hwan Jung. Commitbert: Commit message generation using pre-trained programming language model.CoRR, abs/2105.14242, 2021. URL": "Ren Just. The major mutation framework: efficient and scalable mutation analysis for java. In Corina S.Pasareanu and Darko Marinov (eds.), International Symposium on Software Testing and Analysis, ISSTA14, San Jose, CA, USA - July 21 - 26, 2014, pp. 433436. ACM, 2014. doi: 10.1145/2610384.2628053.URL Ren Just, Darioush Jalali, and Michael D. Ernst. Defects4j: a database of existing faults to enable controlledtesting studies for java programs. In Corina S. Pasareanu and Darko Marinov (eds.), International Sympo-sium on Software Testing and Analysis, ISSTA 14, San Jose, CA, USA - July 21 - 26, 2014, pp. 437440.ACM, 2014. doi: 10.1145/2610384.2628055. URL",
  "Ahmed Khanfir, Renzo Degiovanni, Mike Papadakis, and Yves Le Traon. Efficient mutation testing viapre-trained language models.CoRR, abs/2301.03543, 2023a.doi: 10.48550/arXiv.2301.03543.URL": "Ahmed Khanfir, Anil Koyuncu, Mike Papadakis, Maxime Cordy, Tegawend F. Bissyand, Jacques Klein,and Yves Le Traon. ibir: Bug-report-driven fault injection. ACM Trans. Softw. Eng. Methodol., 32(2):33:133:31, 2023b. doi: 10.1145/3542946. URL Kisub Kim, Dongsun Kim, Tegawend F. Bissyand, Eunjong Choi, Li Li, Jacques Klein, and Yves Le Traon.Facoy: a code-to-code search engine. In Michel Chaudron, Ivica Crnkovic, Marsha Chechik, and MarkHarman (eds.), Proceedings of the 40th International Conference on Software Engineering, ICSE 2018,Gothenburg, Sweden, May 27 - June 03, 2018, pp. 946957. ACM, 2018. doi: 10.1145/3180155.3180187.URL Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th Inter-national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.OpenReview.net, 2020. URL Denis Kocetkov, Raymond Li, Loubna Ben allal, Jia LI, Chenghao Mou, Yacine Jernite, Margaret Mitchell,Carlos Muoz Ferrandis, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro Von Werra, and Harmde Vries.The stack: 3 TB of permissively licensed source code.Transactions on Machine LearningResearch, 2023. ISSN 2835-8856. URL Li Kuang, Cong Zhou, and Xiaoxian Yang.Code comment generation based on graph neural net-work enhanced transformer model for code understanding in open-source software ecosystems.Au-tom. Softw. Eng., 29(2):43, 2022. doi: 10.1007/S10515-022-00341-1. URL Sumith Kulal, Panupong Pasupat, Kartik Chandra, Mina Lee, Oded Padon, Alex Aiken, and Percy Liang.Spoc: Search-based pseudocode to code. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Flo-rence dAlch-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information ProcessingSystems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, Decem-ber 8-14, 2019, Vancouver, BC, Canada, pp. 1188311894, 2019. URL Chinmay Kulkarni, Stefania Druga, Minsuk Chang, Alex Fiannaca, Carrie J. Cai, and Michael Terry. Aword is worth a thousand pictures: Prompts as AI design material. CoRR, abs/2303.12647, 2023. doi:10.48550/ARXIV.2303.12647. URL Ayush Kumar, Parth Nagarkar, Prabhav Nalhe, and Sanjeev Vijayakumar. Deep learning driven naturallanguages text to SQL query conversion: A survey. CoRR, abs/2208.04415, 2022. doi: 10.48550/ARXIV.2208.04415. URL Deeptimahanti Deva Kumar and Ratna Sanyal. Static uml model generator from analysis of requirements(sugar). In 2008 Advanced Software Engineering and Its Applications, pp. 7784, 2008. doi: 10.1109/ASEA.2008.25.",
  "Max Landauer, Sebastian Onder, Florian Skopik, and Markus Wurzenberger. Deep learning for anomalydetection in log data: A survey. CoRR, abs/2207.03820, 2022. doi: 10.48550/ARXIV.2207.03820. URL": "Chris Lattner and Vikram S. Adve.LLVM: A compilation framework for lifelong program analysis &transformation. In 2nd IEEE / ACM International Symposium on Code Generation and Optimization(CGO 2004), 20-24 March 2004, San Jose, CA, USA, pp. 7588. IEEE Computer Society, 2004. doi:10.1109/CGO.2004.1281665. URL Hugo Laurenon,Lucile Saulnier,Thomas Wang,Christopher Akiki,Albert Villanova del Moral,Teven Le Scao, Leandro von Werra, Chenghao Mou, Eduardo Gonzlez Ponferrada, Huu Nguyen,Jrg Frohberg, Mario Sasko, Quentin Lhoest, Angelina McMillan-Major, Grard Dupont, Stella Bi-derman, Anna Rogers, Loubna Ben Allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, So-maieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush,Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Muoz, Jian Zhu, Daniel van Strien, ZaidAlyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, MananDey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, HieuTran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Alexan-dra Sasha Luccioni, and Yacine Jernite.The bigscience ROOTS corpus:A 1.6tb composite multi-lingual dataset.In NeurIPS, 2022.URL",
  "HungLe,YueWang,AkhileshDeepakGotmare,SilvioSavarese,andStevenChu-HongHoi.Coderl:Masteringcodegenerationthroughpretrainedmodelsanddeepreinforcementlearn-ing.InNeurIPS,2022.URL": "Van-Hoang Le and Hongyu Zhang. Log-based anomaly detection without log parsing. In 36th IEEE/ACMInternational Conference on Automated Software Engineering, ASE 2021, Melbourne, Australia, November15-19, 2021, pp. 492504. IEEE, 2021. doi: 10.1109/ASE51524.2021.9678773. URL Van-Hoang Le and Hongyu Zhang. Log parsing with prompt-based few-shot learning. In 45th IEEE/ACMInternational Conference on Software Engineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023,pp. 24382449. IEEE, 2023a.doi: 10.1109/ICSE48619.2023.00204.URL Van-Hoang Le and Hongyu Zhang. Log parsing: How far can chatgpt go?In 38th IEEE/ACM Interna-tional Conference on Automated Software Engineering, ASE 2023, Luxembourg, September 11-15, 2023,pp. 16991704. IEEE, 2023b.doi: 10.1109/ASE56229.2023.00206.URL Hugh Leather and Chris Cummins. Machine learning in compilers: Past, present and future. In Forum forSpecification and Design Languages, FDL 2020, Kiel, Germany, September 15-17, 2020, pp. 18. IEEE,2020. doi: 10.1109/FDL50818.2020.9232934. URL Alexander LeClair, Siyuan Jiang, and Collin McMillan. A neural model for generating natural languagesummaries of program subroutines. In Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (eds.), Proceedingsof the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May25-31, 2019, pp. 795806. IEEE / ACM, 2019. doi: 10.1109/ICSE.2019.00087. URL Changyoon Lee, Yeon Seonwoo, and Alice Oh. CS1QA: A dataset for assisting code-based question answeringin an introductory programming course.In Marine Carpuat, Marie-Catherine de Marneffe, and IvnVladimir Meza Ruz (eds.), Proceedings of the 2022 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA,United States, July 10-15, 2022, pp. 20262040. Association for Computational Linguistics, 2022. doi:10.18653/V1/2022.NAACL-MAIN.148. URL Chia-Hsuan Lee, Oleksandr Polozov, and Matthew Richardson. Kaggledbqa: Realistic evaluation of text-to-sql parsers.In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings ofthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-tual Event, August 1-6, 2021, pp. 22612273. Association for Computational Linguistics, 2021.doi:10.18653/V1/2021.ACL-LONG.176. URL Woosuk Lee, Kihong Heo, Rajeev Alur, and Mayur Naik.Accelerating search-based program synthesisusing learned probabilistic models.In Jeffrey S. Foster and Dan Grossman (eds.), Proceedings of the39th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2018,Philadelphia, PA, USA, June 18-22, 2018, pp. 436449. ACM, 2018. doi: 10.1145/3192366.3192410. URL Wenqiang Lei, Weixin Wang, Zhixin Ma, Tian Gan, Wei Lu, Min-Yen Kan, and Tat-Seng Chua.Re-examining the role of schema linking in text-to-sql.In Bonnie Webber, Trevor Cohn, Yulan He, andYang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-cessing, EMNLP 2020, Online, November 16-20, 2020, pp. 69436954. Association for ComputationalLinguistics, 2020. doi: 10.18653/V1/2020.EMNLP-MAIN.564. URL",
  "Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. Acecoder: Utilizing existing code to enhance codegeneration, 2023e": "Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, and Zhi Jin. Evocodebench: An evolving code generationbenchmark aligned with real-world code repositories.CoRR, abs/2404.00599, 2024a.doi: 10.48550/ARXIV.2404.00599. URL Jian Li, Yue Wang, Michael R. Lyu, and Irwin King. Code completion with neural attention and pointernetworks. In Jrme Lang (ed.), Proceedings of the Twenty-Seventh International Joint Conference onArtificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 41594165. ijcai.org, 2018a.doi: 10.24963/ijcai.2018/578. URL Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei Huang, Wenyu Du, LuoSi, and Yongbin Li. Graphix-t5: Mixing pre-trained transformers with graph-aware layers for text-to-sqlparsing. In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), Thirty-Seventh AAAI Conferenceon Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of ArtificialIntelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI2023, Washington, DC, USA, February 7-14, 2023, pp. 1307613084. AAAI Press, 2023f. doi: 10.1609/AAAI.V37I11.26536. URL",
  "Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. Diffusion-lm im-proves controllable text generation. In NeurIPS, 2022d. URL": "Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang, Bolun Yao, Weizhen Qi, Daxin Jiang,Weizhu Chen, and Nan Duan.Coderetriever: A large scale contrastive pre-training method for codesearch. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates,December 7-11, 2022, pp. 28982910. Association for Computational Linguistics, 2022e. doi: 10.18653/v1/2022.emnlp-main.187. URL",
  "Ruigang Liang, Ying Cao, Peiwei Hu, and Kai Chen.Neutron:an attention-based neural decom-piler. Cybersecur., 4(1):5, 2021. doi: 10.1186/S42400-021-00070-0. URL": "Dianshu Liao, Shidong Pan, Xiaoyu Sun, Xiaoxue Ren, Qing Huang, Zhenchang Xing, Huan Jin, and QinyingLi. A3-codgen: A repository-level code generation framework for code reuse with local-aware, global-aware,and third-party-library-aware. CoRR, abs/2312.05772, 2023. doi: 10.48550/ARXIV.2312.05772. URL Derrick Lin, James Koppel, Angela Chen, and Armando Solar-Lezama. Quixbugs: a multi-lingual programrepair benchmark set based on the quixey challenge. In Gail C. Murphy (ed.), Proceedings Companion of the2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications:Software for Humanity, SPLASH 2017, Vancouver, BC, Canada, October 23 - 27, 2017, pp. 5556. ACM,2017. doi: 10.1145/3135932.3135941. URL",
  "Feng Lin, Dong Jae Kim, and Tse-Husn Chen. When llm-based code generation meets the software de-velopment process.CoRR, abs/2403.15852, 2024.doi:10.48550/ARXIV.2403.15852.URL": "Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Yang Xiang, Olivier Y. de Vel, and Paul Montague. Cross-projecttransfer representation learning for vulnerable function discovery. IEEE Trans. Ind. Informatics, 14(7):32893297, 2018a. doi: 10.1109/TII.2018.2821768. URL Guanjun Lin, Wei Xiao, Jun Zhang, and Yang Xiang. Deep learning-based vulnerable function detection: Abenchmark. In Jianying Zhou, Xiapu Luo, Qingni Shen, and Zhen Xu (eds.), Information and Commu-nications Security - 21st International Conference, ICICS 2019, Beijing, China, December 15-17, 2019,Revised Selected Papers, volume 11999 of Lecture Notes in Computer Science, pp. 219232. Springer, 2019.doi: 10.1007/978-3-030-41579-2\\_13. URL Guanjun Lin, Jun Zhang, Wei Luo, Lei Pan, Olivier Y. de Vel, Paul Montague, and Yang Xiang. Softwarevulnerability discovery via learning multi-domain knowledge bases. IEEE Trans. Dependable Secur. Com-put., 18(5):24692485, 2021. doi: 10.1109/TDSC.2019.2954088. URL",
  "Aiwei Liu, Xuming Hu, Lijie Wen, and Philip S. Yu. A comprehensive evaluation of chatgpts zero-shottext-to-sql capability. CoRR, abs/2303.13547, 2023a. doi: 10.48550/ARXIV.2303.13547. URL": "Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei, Ming Liang, Dajun Chen, MinShen, Hailian Zhou, Hang Yu, and Jianguo Li. Mftcoder: Boosting code llms with multitask fine-tuning.CoRR, abs/2311.02303, 2023b. doi: 10.48550/ARXIV.2311.02303. URL Chao Liu, Xin Xia, David Lo, Zhiwei Liu, Ahmed E. Hassan, and Shanping Li. Codematcher: Searchingcode based on sequential semantics of important query words. ACM Trans. Softw. Eng. Methodol., 31(1):12:112:37, 2022a. doi: 10.1145/3465403. URL Chenxiao Liu and Xiaojun Wan. Codeqa: A question answering dataset for source code comprehension.In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings ofthe Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana, DominicanRepublic, 16-20 November, 2021, pp. 26182632. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.FINDINGS-EMNLP.223. URL Fang Liu, Ge Li, Yunfei Zhao, and Zhi Jin. Multi-task learning based pre-trained language model for codecompletion. In 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020,Melbourne, Australia, September 21-25, 2020, pp. 473485. IEEE, 2020. doi: 10.1145/3324884.3416591.URL Fang Liu, Ge Li, Zhiyi Fu, Shuai Lu, Yiyang Hao, and Zhi Jin. Learning to recommend method names withglobal context. In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022,Pittsburgh, PA, USA, May 25-27, 2022, pp. 12941306. ACM, 2022b. doi: 10.1145/3510003.3510154. URL Fang Liu, Jia Li, and Li Zhang. Syntax and domain aware model for unsupervised program translation. In45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Melbourne, Australia,May 14-20, 2023, pp. 755767. IEEE, 2023c. doi: 10.1109/ICSE48619.2023.00072. URL",
  "Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, and Deheng Ye. RLTF: reinforcementlearning from unit test feedback. CoRR, abs/2307.04349, 2023d. doi: 10.48550/ARXIV.2307.04349. URL": "Jiawei Liu, Jinkun Lin, Fabian Ruffy, Cheng Tan, Jinyang Li, Aurojit Panda, and Lingming Zhang. Nn-smith: Generating diverse and valid test cases for deep learning compilers. In Tor M. Aamodt, NatalieD. Enright Jerger, and Michael M. Swift (eds.), Proceedings of the 28th ACM International Conferenceon Architectural Support for Programming Languages and Operating Systems, Volume 2, ASPLOS 2023,Vancouver, BC, Canada, March 25-29, 2023, pp. 530543. ACM, 2023e. doi: 10.1145/3575693.3575707.URL Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgptreally correct? rigorous evaluation of large language models for code generation. CoRR, abs/2305.01210,2023f. doi: 10.48550/arXiv.2305.01210. URL Kui Liu, Dongsun Kim, Tegawend F. Bissyand, Tae-young Kim, Kisub Kim, Anil Koyuncu, Suntae Kim,and Yves Le Traon.Learning to spot and refactor inconsistent method names.In Joanne M. Atlee,Tevfik Bultan, and Jon Whittle (eds.), Proceedings of the 41st International Conference on SoftwareEngineering, ICSE 2019, Montreal, QC, Canada, May 25-31, 2019, pp. 112. IEEE / ACM, 2019a. doi:10.1109/ICSE.2019.00019. URL Kui Liu, Anil Koyuncu, Dongsun Kim, and Tegawend F. Bissyand.Tbar: revisiting template-basedautomated program repair. In Dongmei Zhang and Anders Mller (eds.), Proceedings of the 28th ACMSIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2019, Beijing, China, July15-19, 2019, pp. 3142. ACM, 2019b. doi: 10.1145/3293882.3330577. URL Mingjie Liu, Nathaniel Ross Pinckney, Brucek Khailany, and Haoxing Ren. Verilogeval: Evaluating largelanguage models for verilog code generation. CoRR, abs/2309.07544, 2023g. doi: 10.48550/ARXIV.2309.07544. URL Qin Liu, Zihe Liu, Hongming Zhu, Hongfei Fan, Bowen Du, and Yu Qian. Generating commit messagesfrom diffs using pointer-generator network. In Margaret-Anne D. Storey, Bram Adams, and Sonia Haiduc(eds.), Proceedings of the 16th International Conference on Mining Software Repositories, MSR 2019, 26-27 May 2019, Montreal, Canada, pp. 299309. IEEE / ACM, 2019c. doi: 10.1109/MSR.2019.00056. URL Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang, and Zhiyao Xie. Rtlcoder: Outperform-ing GPT-3.5 in design RTL generation with our open-source dataset and lightweight solution. CoRR,abs/2312.08617, 2023h. doi: 10.48550/ARXIV.2312.08617. URL Shangqing Liu, Cuiyun Gao, Sen Chen, Lun Yiu Nie, and Yang Liu. ATOM: commit message generationbased on abstract syntax tree and hybrid ranking. IEEE Trans. Software Eng., 48(5):18001817, 2022c.doi: 10.1109/TSE.2020.3038681. URL Shangqing Liu, Xiaofei Xie, Jing Kai Siow, Lei Ma, Guozhu Meng, and Yang Liu. Graphsearchnet: Enhancinggnns via capturing global dependencies for semantic code search. IEEE Trans. Software Eng., 49(4):28392855, 2023i. doi: 10.1109/TSE.2022.3233901. URL",
  "Francesca Lucchetti and Arjun Guha. Activation steering for robust type prediction in codellms. CoRR,abs/2404.01903, 2024.doi: 10.48550/ARXIV.2404.01903.URL": "Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, QingweiLin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. CoRR,abs/2306.08568, 2023. doi: 10.48550/arXiv.2306.08568. URL Thibaud Lutellier, Hung Viet Pham, Lawrence Pang, Yitong Li, Moshi Wei, and Lin Tan. Coconut: com-bining context-aware neural translation models using ensemble for program repair.In Sarfraz Khur-shid and Corina S. Pasareanu (eds.), ISSTA 20: 29th ACM SIGSOFT International Symposium onSoftware Testing and Analysis, Virtual Event, USA, July 18-22, 2020, pp. 101114. ACM, 2020. doi:10.1145/3395363.3397369. URL Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei Zhang, and Jianjun Zhao. Codehow: Ef-fective code search based on API understanding and extended boolean model (E). In Myra B. Cohen, LarsGrunske, and Michael Whalen (eds.), 30th IEEE/ACM International Conference on Automated SoftwareEngineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015, pp. 260270. IEEE Computer Society,2015. doi: 10.1109/ASE.2015.42. URL",
  "Qin Lyu, Kaushik Chakrabarti, Shobhit Hathi, Souvik Kundu, Jianwen Zhang, and Zheng Chen. Hybridranking network for text-to-sql.CoRR, abs/2008.04759, 2020.URL": "Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Chen, and Shaowei Wang. Llmparser: An exploratorystudy on using large language models for log parsing. In Proceedings of the 46th IEEE/ACM InternationalConference on Software Engineering, ICSE 2024, Lisbon, Portugal, April 14-20, 2024, pp. 99:199:13.ACM, 2024. doi: 10.1145/3597503.3639150. URL Marcos Macedo, Yuan Tian, Filipe Roseiro Cgo, and Bram Adams. Exploring the impact of the outputformat on the evaluation of large language models for code translation. CoRR, abs/2403.17214, 2024. doi:10.48550/ARXIV.2403.17214. URL Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, UriAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Ma-jumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-refine: Iter-ative refinement with self-feedback.In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko,Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: An-nual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,USA, December 10 - 16, 2023, 2023. URL Fernanda Madeiral, Simon Urli, Marcelo de Almeida Maia, and Martin Monperrus. BEARS: an extensiblejava bug benchmark for automatic program repair studies. In Xinyu Wang, David Lo, and Emad Shihab(eds.), 26th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER2019, Hangzhou, China, February 24-27, 2019, pp. 468478. IEEE, 2019. doi: 10.1109/SANER.2019.8667991. URL",
  "Yubo Mai, Zhipeng Gao, Xing Hu, Lingfeng Bao, Yu Liu, and Jianling Sun. Are human rules necessary?generating reusable apis with cot reasoning and in-context learning. 2024. URL": "Rabee Sohail Malik, Jibesh Patra, and Michael Pradel. Nl2type: inferring javascript function types fromnatural language information. In Joanne M. Atlee, Tevfik Bultan, and Jon Whittle (eds.), Proceedingsof the 41st International Conference on Software Engineering, ICSE 2019, Montreal, QC, Canada, May25-31, 2019, pp. 304315. IEEE / ACM, 2019. doi: 10.1109/ICSE.2019.00045. URL Aniketh Malyala, Katelyn Zhou, Baishakhi Ray, and Saikat Chakraborty. On ml-based program translation:Perils and promises. In 45th IEEE/ACM International Conference on Software Engineering: New Ideasand Emerging Results, NIER@ICSE, Melbourne, Australia, May 14-20, 2023, pp. 6065. IEEE, 2023. doi:10.1109/ICSE-NIER58687.2023.00017. URL",
  "Ryan Marcus, Parimarjan Negi, Hongzi Mao, Nesime Tatbul, Mohammad Alizadeh, and Tim Kraska. Bao:Learning to steer query optimizers. CoRR, abs/2004.03814, 2020. URL": "Antonio Mastropaolo, Simone Scalabrino, Nathan Cooper, David Nader-Palacio, Denys Poshyvanyk, RoccoOliveto, and Gabriele Bavota. Studying the usage of text-to-text transfer transformer to support code-related tasks. In 43rd IEEE/ACM International Conference on Software Engineering, ICSE 2021, Madrid,Spain, 22-30 May 2021, pp. 336347. IEEE, 2021. doi: 10.1109/ICSE43902.2021.00041. URL George Mathew and Kathryn T. Stolee. Cross-language code search using static and dynamic analyses.In Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta (eds.), ESEC/FSE21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundationsof Software Engineering, Athens, Greece, August 23-28, 2021, pp. 205217. ACM, 2021. doi: 10.1145/3468264.3468538. URL Deep Mehta, Kartik Rawool, Subodh Gujar, and Bowen Xu. Automated devops pipeline generation for coderepositories using large language models. CoRR, abs/2312.13225, 2023. doi: 10.48550/ARXIV.2312.13225.URL Weibin Meng, Ying Liu, Yichen Zhu, Shenglin Zhang, Dan Pei, Yuqing Liu, Yihao Chen, Ruizhi Zhang,Shimin Tao, Pei Sun, and Rong Zhou. Loganomaly: Unsupervised detection of sequential and quantitativeanomalies in unstructured logs. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth International JointConference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 47394745.ijcai.org, 2019. doi: 10.24963/IJCAI.2019/658. URL Aditya Krishna Menon, Omer Tamuz, Sumit Gulwani, Butler W. Lampson, and Adam Kalai. A machinelearning framework for programming by example. In Proceedings of the 30th International Conferenceon Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshopand Conference Proceedings, pp. 187195. JMLR.org, 2013. URL",
  "Martin Monperrus. Automatic software repair: A bibliography. ACM Comput. Surv., 51(1):17:117:24, 2018.doi: 10.1145/3105906. URL": "Martin Monperrus, Matias Martinez, He Ye, Fernanda Madeiral, Thomas Durieux, and Zhongxing Yu.Megadiff: A dataset of 600k java source code changes categorized by diff size. CoRR, abs/2108.04631,2021. URL Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. Convolutional neural networks over tree structures forprogramming language processing. In Dale Schuurmans and Michael P. Wellman (eds.), Proceedings of theThirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA, pp.12871293. AAAI Press, 2016. doi: 10.1609/aaai.v30i1.10139. URL Fangwen Mu, Xiao Chen, Lin Shi, Song Wang, and Qing Wang. Developer-intent driven code commentgeneration.In 45th IEEE/ACM International Conference on Software Engineering, ICSE 2023, Mel-bourne, Australia, May 14-20, 2023, pp. 768780. IEEE, 2023a.doi: 10.1109/ICSE48619.2023.00073.URL",
  "Priyanka Mudgal and Rita H. Wouhaybi. An assessment of chatgpt on log data. CoRR, abs/2309.07938,2023. doi: 10.48550/ARXIV.2309.07938. URL": "Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, SwayamSingh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code largelanguage models.In The Twelfth International Conference on Learning Representations, 2024.URL Khurram Murad, Syed Noor-ul-Hassan Shirazi, Yousaf Bin Zikria, and Nassar Ikram. Evading virus detectionusing code obfuscation.In Tai-Hoon Kim, Young-Hoon Lee, Byeong Ho Kang, and Dominik Slezak(eds.), Future Generation Information Technology - Second International Conference, FGIT 2010, JejuIsland, Korea, December 13-15, 2010. Proceedings, volume 6485 of Lecture Notes in Computer Science,pp. 394401. Springer, 2010. doi: 10.1007/978-3-642-17569-5\\_39. URL",
  "Bardia Nadimi and Hao Zheng. A multi-expert large language model architecture for verilog code generation.CoRR, abs/2404.08029, 2024.doi: 10.48550/ARXIV.2404.08029.URL": "Kawser Wazed Nafi, Tonny Shekha Kar, Banani Roy, Chanchal K. Roy, and Kevin A. Schneider. CLCDSA:cross language code clone detection using syntactical features and API documentation. In 34th IEEE/ACMInternational Conference on Automated Software Engineering, ASE 2019, San Diego, CA, USA, November11-15, 2019, pp. 10261037. IEEE, 2019. doi: 10.1109/ASE.2019.00099. URL Tasuku Nakagawa, Yoshiki Higo, and Shinji Kusumoto. NIL: large-scale detection of large-variance clones.In Diomidis Spinellis, Georgios Gousios, Marsha Chechik, and Massimiliano Di Penta (eds.), ESEC/FSE21: 29th ACM Joint European Software Engineering Conference and Symposium on the Foundationsof Software Engineering, Athens, Greece, August 23-28, 2021, pp. 830841. ACM, 2021. doi: 10.1145/3468264.3468564. URL Linyong Nan, Yilun Zhao, Weijin Zou, Narutatsu Ri, Jaesung Tae, Ellen Zhang, Arman Cohan, and DragomirRadev. Enhancing few-shot text-to-sql capabilities of large language models: A study on prompt designstrategies. CoRR, abs/2305.12586, 2023. doi: 10.48550/arXiv.2305.12586. URL Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, and Odej Kao. Self-supervisedlog parsing.In Yuxiao Dong, Dunja Mladenic, and Craig Saunders (eds.), Machine Learning andKnowledge Discovery in Databases: Applied Data Science Track - European Conference, ECML PKDD2020, Ghent, Belgium, September 14-18, 2020, Proceedings, Part IV, volume 12460 of Lecture Notes inComputer Science, pp. 122138. Springer, 2020a.doi: 10.1007/978-3-030-67667-4\\_8.URL Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, and Odej Kao.Self-attentiveclassification-based anomaly detection in unstructured logs.In Claudia Plant, Haixun Wang, Al-fredo Cuzzocrea, Carlo Zaniolo, and Xindong Wu (eds.), 20th IEEE International Conference on DataMining, ICDM 2020, Sorrento, Italy, November 17-20, 2020, pp. 11961201. IEEE, 2020b.doi:10.1109/ICDM50108.2020.00148. URL Anh Tuan Nguyen, Tung Thanh Nguyen, and Tien N. Nguyen. Lexical statistical machine translation forlanguage migration. In Bertrand Meyer, Luciano Baresi, and Mira Mezini (eds.), Joint Meeting of theEuropean Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations ofSoftware Engineering, ESEC/FSE13, Saint Petersburg, Russian Federation, August 18-26, 2013, pp. 651654. ACM, 2013. doi: 10.1145/2491411.2494584. URL",
  "Daniel Nichols, Joshua Hoke Davis, Zhaojun Xie, Arjun Rajaram, and Abhinav Bhatele. Can large languagemodels write parallel code?CoRR, abs/2401.12554, 2024.doi: 10.48550/ARXIV.2401.12554.URL": "Lun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam, Yang Liu, and Zenglin Xu. Coregen: Contextualizedcode representation learning for commit message generation. Neurocomputing, 459:97107, 2021. doi:10.1016/J.NEUCOM.2021.05.039. URL Pengyu Nie, Rahul Banerjee, Junyi Jessy Li, Raymond J. Mooney, and Milos Gligoric.Learning deepsemantics for test completion. In 45th IEEE/ACM International Conference on Software Engineering,ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 21112123. IEEE, 2023. doi: 10.1109/ICSE48619.2023.00178. URL Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2: Lessonsfor training llms on programming and natural languages. CoRR, abs/2305.02309, 2023a. doi: 10.48550/ARXIV.2305.02309. URL",
  "Changan Niu, Ting Zhang, Chuanyi Li, Bin Luo, and Vincent Ng. On evaluating the efficiency of sourcecode generated by llms. CoRR, abs/2404.06041, 2024. doi: 10.48550/ARXIV.2404.06041. URL": "Yu Nong, Rainy Sharma, Abdelwahab Hamou-Lhadj, Xiapu Luo, and Haipeng Cai. Open science in softwareengineering: A study on deep learning-based vulnerability detection. IEEE Trans. Software Eng., 49(4):19832005, 2023. doi: 10.1109/TSE.2022.3207149. URL Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Showyour work: Scratchpads for intermediate computation with language models. CoRR, abs/2112.00114, 2021.URL Augustus Odena, Catherine Olsson, David G. Andersen, and Ian J. Goodfellow. Tensorfuzz: Debuggingneural networks with coverage-guided fuzzing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, LongBeach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 49014911. PMLR,2019. URL",
  "Saeyoon Oh and Shin Yoo. Csa-trans: Code structure aware transformer for ast. CoRR, abs/2404.05767,2024. doi: 10.48550/ARXIV.2404.05767. URL": "Wonseok Oh and Hakjoo Oh. Pyter: effective program repair for python type errors. In Abhik Roychoud-hury, Cristian Cadar, and Miryung Kim (eds.), Proceedings of the 30th ACM Joint European SoftwareEngineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2022,Singapore, Singapore, November 14-18, 2022, pp. 922934. ACM, 2022. doi: 10.1145/3540250.3549130.URL",
  "OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/arXiv.2303.08774. URL": "Long Ouyang,Jeffrey Wu,Xu Jiang,Diogo Almeida,Carroll L. Wainwright,Pamela Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,Fraser Kelton,Luke Miller,Maddie Simens,Amanda Askell,Peter Welinder,Paul F. Chris-tiano,Jan Leike,and Ryan Lowe.Training language models to follow instructions with hu-man feedback.In NeurIPS, 2022.URL Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, MicheleMerler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. Understanding theeffectiveness of large language models in code translation. CoRR, abs/2308.03109, 2023. doi: 10.48550/ARXIV.2308.03109. URL Xinglu Pan, Chenxiao Liu, Yanzhen Zou, Tao Xie, and Bing Xie. MESIA: understanding and leveraging sup-plementary nature of method-level comments for automatic comment generation. CoRR, abs/2403.17357,2024. doi: 10.48550/ARXIV.2403.17357. URL",
  "Irene Vlassi Pandi, Earl T. Barr, Andrew D. Gordon, and Charles Sutton. Opttyper: Probabilistic typeinference by optimising logical and natural constraints.CoRR, abs/2004.00348, 2020.URL": "Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella.Automated test case generation asa many-objective optimisation problem with dynamic selection of the targets.IEEE Trans. SoftwareEng., 44(2):122158, 2018. doi: 10.1109/TSE.2017.2663435. URL Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th Annual Meeting of the Association for ComputationalLinguistics, July 6-12, 2002, Philadelphia, PA, USA, pp. 311318. ACL, 2002. doi: 10.3115/1073083.1073135. URL Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, andSergey Mechtaev. The fact selection problem in llm-based program repair. CoRR, abs/2404.05520, 2024.doi: 10.48550/ARXIV.2404.05520. URL Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli.Neuro-symbolic program synthesis. In 5th International Conference on Learning Representations, ICLR2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL Md. Rizwan Parvez, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Building language models fortext with named entities. In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20,2018, Volume 1: Long Papers, pp. 23732383. Association for Computational Linguistics, 2018.doi:10.18653/v1/P18-1221. URL",
  "Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extensionof large language models. CoRR, abs/2309.00071, 2023a. doi: 10.48550/ARXIV.2309.00071. URL": "Yun Peng, Cuiyun Gao, Zongjie Li, Bowei Gao, David Lo, Qirun Zhang, and Michael R. Lyu. Static inferencemeets deep learning: A hybrid type inference approach for python. In 44th IEEE/ACM 44th InternationalConference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 20192030.ACM, 2022. doi: 10.1145/3510003.3510038. URL Yun Peng, Shuzheng Gao, Cuiyun Gao, Yintong Huo, and Michael R. Lyu. Domain knowledge matters:Improving prompts with fix templates for repairing python type errors. CoRR, abs/2306.01394, 2023b.doi: 10.48550/ARXIV.2306.01394. URL",
  "Yun Peng, Chaozheng Wang, Wenxuan Wang, Cuiyun Gao, and Michael R. Lyu. Generative type inferencefor python. CoRR, abs/2307.09163, 2023c. doi: 10.48550/ARXIV.2307.09163. URL": "Daniel Perez and Shigeru Chiba. Cross-language clone detection by learning over abstract syntax trees. InMargaret-Anne D. Storey, Bram Adams, and Sonia Haiduc (eds.), Proceedings of the 16th InternationalConference on Mining Software Repositories, MSR 2019, 26-27 May 2019, Montreal, Canada, pp. 518528.IEEE / ACM, 2019. doi: 10.1109/MSR.2019.00078. URL Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh. Do users write more insecure code withAI assistants?In Weizhi Meng, Christian Damsgaard Jensen, Cas Cremers, and Engin Kirda (eds.),Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, CCS 2023,Copenhagen, Denmark, November 26-30, 2023, pp. 27852799. ACM, 2023. doi: 10.1145/3576915.3623157.URL Jannik Pewny and Thorsten Holz. Evilcoder: automated bug insertion. In Stephen Schwab, William K.Robertson, and Davide Balzarotti (eds.), Proceedings of the 32nd Annual Conference on Computer SecurityApplications, ACSAC 2016, Los Angeles, CA, USA, December 5-9, 2016, pp. 214225. ACM, 2016. URL",
  "Juan Altmayer Pizzorno and Emery D. Berger. Coverup: Coverage-guided llm-based test generation. CoRR,abs/2403.16218, 2024.doi: 10.48550/ARXIV.2403.16218.URL": "Serena Elisa Ponta, Henrik Plate, Antonino Sabetta, Michele Bezzi, and Cdric Dangremont. A manually-curated dataset of fixes to vulnerabilities of open-source software. In Margaret-Anne D. Storey, BramAdams, and Sonia Haiduc (eds.), Proceedings of the 16th International Conference on Mining SoftwareRepositories, MSR 2019, 26-27 May 2019, Montreal, Canada, pp. 383387. IEEE / ACM, 2019. doi:10.1109/MSR.2019.00064. URL",
  "Michael Pradel and Koushik Sen. Deepbugs: a learning approach to name-based bug detection. Proc. ACMProgram. Lang., 2(OOPSLA):147:1147:25, 2018.doi: 10.1145/3276517.URL": "Michael Pradel, Parker Schuh, and Koushik Sen.Typedevil: Dynamic type inconsistency analysis forjavascript. In Antonia Bertolino, Gerardo Canfora, and Sebastian G. Elbaum (eds.), 37th IEEE/ACMInternational Conference on Software Engineering, ICSE 2015, Florence, Italy, May 16-24, 2015, Volume1, pp. 314324. IEEE Computer Society, 2015. doi: 10.1109/ICSE.2015.51. URL Michael Pradel, Georgios Gousios, Jason Liu, and Satish Chandra. Typewriter: neural type prediction withsearch-based validation. In Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (eds.), ESEC/FSE20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering, Virtual Event, USA, November 8-13, 2020, pp. 209220. ACM, 2020. doi: 10.1145/3368089.3409715. URL",
  "Julian Aron Prenner and Romain Robbes.Runbugrun - an executable dataset for automated programrepair. CoRR, abs/2304.01102, 2023. doi: 10.48550/ARXIV.2304.01102. URL": "Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enablesinput length extrapolation. In The Tenth International Conference on Learning Representations, ICLR2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay. sk_p: a neural programcorrector for moocs.In Eelco Visser (ed.), Companion Proceedings of the 2016 ACM SIGPLAN In-ternational Conference on Systems, Programming, Languages and Applications: Software for Humanity,SPLASH 2016, Amsterdam, Netherlands, October 30 - November 4, 2016, pp. 3940. ACM, 2016. doi:10.1145/2984043.2989222. URL Ruchir Puri,David S. Kung,Geert Janssen,Wei Zhang,Giacomo Domeniconi,Vladimir Zolo-tov, Julian Dolby, Jie Chen, Mihir R. Choudhury, Lindsey Decker, Veronika Thost, Luca Bu-ratti, Saurabh Pujar, Shyam Ramji, Ulrich Finkler, Susan Malaika, and Frederick Reiss.Co-denet:A large-scale AI for code dataset for learning a diversity of coding tasks.In Joaquin",
  "Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing SystemsTrack on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021,virtual,2021.URL": "Jiaxing Qi, Shaohan Huang, Zhongzhi Luan, Carol J. Fung, Hailong Yang, and Depei Qian. Loggpt: Ex-ploring chatgpt for log-based anomaly detection. CoRR, abs/2309.01189, 2023. doi: 10.48550/ARXIV.2309.01189. URL Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun.Communicative agents for software development. CoRR, abs/2307.07924, 2023. doi: 10.48550/ARXIV.2307.07924. URL Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao,Jian Sun, Luo Si, Fei Huang, and Yongbin Li.A survey on text-to-sql parsing: Concepts, methods,and future directions. CoRR, abs/2208.13629, 2022a. doi: 10.48550/ARXIV.2208.13629. URL Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong,and Yiran Zhong. cosformer: Rethinking softmax in attention. In The Tenth International Conference onLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022b. URL Maxim Rabinovich, Mitchell Stern, and Dan Klein.Abstract syntax networks for code generation andsemantic parsing. In Regina Barzilay and Min-Yen Kan (eds.), Proceedings of the 55th Annual Meetingof the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4,Volume 1: Long Papers, pp. 11391149. Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1105. URL",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, H. Francis Song, JohnAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, JacobMenick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato,John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant M. Jayaku-mar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Lau-rent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya,Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, NikolaiGrigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama,Cyprien de Masson dAutume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark,Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew J. Johnson, Blake A. Hechtman,Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Simon Osindero, Laura Rimell, ChrisDyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher. CoRR,abs/2112.11446, 2021. URL Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.J. Mach. Learn. Res., 21:140:1140:67, 2020. URL",
  "Anthony Saieva, Saikat Chakraborty, and Gail E. Kaiser. On contrastive learning of semantic similarityforcode to code search. CoRR, abs/2305.03843, 2023. doi: 10.48550/ARXIV.2305.03843. URL": "Abhishek Sainani, Preethu Rose Anish, Vivek Joshi, and Smita Ghaisas. Extracting and classifying require-ments from software engineering contracts. In Travis D. Breaux, Andrea Zisman, Samuel Fricker, andMartin Glinz (eds.), 28th IEEE International Requirements Engineering Conference, RE 2020, Zurich,Switzerland, August 31 - September 4, 2020, pp. 147157. IEEE, 2020. doi: 10.1109/RE48521.2020.00026.URL Vaibhav Saini, Farima Farmahinifarahani, Yadong Lu, Pierre Baldi, and Cristina V. Lopes. Oreo: detectionof clones in the twilight zone. In Gary T. Leavens, Alessandro Garcia, and Corina S. Pasareanu (eds.),Proceedings of the 2018 ACM Joint Meeting on European Software Engineering Conference and Symposiumon the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA,November 04-09, 2018, pp. 354365. ACM, 2018. doi: 10.1145/3236024.3236026. URL Hitesh Sajnani, Vaibhav Saini, Jeffrey Svajlenko, Chanchal K. Roy, and Cristina V. Lopes. Sourcerercc:scaling code clone detection to big-code. In Laura K. Dillon, Willem Visser, and Laurie A. Williams(eds.), Proceedings of the 38th International Conference on Software Engineering, ICSE 2016, Austin,TX, USA, May 14-22, 2016, pp. 11571168. ACM, 2016. doi: 10.1145/2884781.2884877. URL Pasquale Salza, Christoph Schwizer, Jian Gu, and Harald C. Gall. On the effectiveness of transfer learningfor code search. IEEE Trans. Software Eng., 49(4):18041822, 2023. doi: 10.1109/TSE.2022.3192755.URL Gustavo Sandoval, Hammond Pearce, Teo Nys, Ramesh Karri, Siddharth Garg, and Brendan Dolan-Gavitt.Lost at C: A user study on the security implications of large language model code assistants. In Joseph A.Calandrino and Carmela Troncoso (eds.), 32nd USENIX Security Symposium, USENIX Security 2023,Anaheim, CA, USA, August 9-11, 2023, pp. 22052222. USENIX Association, 2023. URL Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-fin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya SharmaSharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, JonathanChang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, ThibaultFvry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, andAlexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth",
  "Laboni Sarker, Mara Downing, Achintya Desai, and Tevfik Bultan. Syntactic robustness for llm-based codegeneration. CoRR, abs/2404.01535, 2024. doi: 10.48550/ARXIV.2404.01535. URL": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagn,Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, Jonathan Tow, Alexander M. Rush, StellaBiderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benot Sagot, Niklas Muen-nighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, HugoLaurenon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi,Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, ChenghaoMou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, andet al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022.doi: 10.48550/arXiv.2211.05100. URL Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettle-moyer, Nicola Cancedda, and Thomas Scialom.Toolformer: Language models can teach themselvesto use tools.In Thirty-seventh Conference on Neural Information Processing Systems, 2023.URL Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: parsing incrementally for constrainedauto-regressive decoding from language models. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia,and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November,2021, pp. 98959901. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.779. URL",
  "Max Schfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. An empirical evaluation of using large languagemodels for automated unit test generation, 2023": "Marija Selakovic, Michael Pradel, Rezwana Karim, and Frank Tip. Test generation for higher-order functionsin dynamic languages. Proc. ACM Program. Lang., 2(OOPSLA):161:1161:27, 2018. doi: 10.1145/3276531.URL Sachith Seneviratne, Ridwan Shariffdeen, Sanka Rasnayaka, and Nuran Kasthuriarachchi. Self-supervisedvision transformers for malware detection. IEEE Access, 10:103121103135, 2022. doi: 10.1109/ACCESS.2022.3206445. URL Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models withmonolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguis-tics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Com-puter Linguistics, 2016. doi: 10.18653/v1/p16-1009. URL Ramin Shahbazi and Fatemeh Hendijani Fard.Apicontext2com:Code comment generation by incor-porating pre-defined API documentation.In 31st IEEE/ACM International Conference on ProgramComprehension, ICPC 2023, Melbourne, Australia, May 15-16, 2023, pp. 1324. IEEE, 2023.doi:10.1109/ICPC58990.2023.00012. URL",
  "Noam Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019.URL": "Dongdong She, Kexin Pei, Dave Epstein, Junfeng Yang, Baishakhi Ray, and Suman Jana. NEUZZ: efficientfuzzing with neural program smoothing. In 2019 IEEE Symposium on Security and Privacy, SP 2019,San Francisco, CA, USA, May 19-23, 2019, pp. 803817. IEEE, 2019. doi: 10.1109/SP.2019.00052. URL Dongdong She, Rahul Krishna, Lu Yan, Suman Jana, and Baishakhi Ray. Mtfuzz: fuzzing with a multi-task neural network. In Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (eds.), ESEC/FSE20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundations ofSoftware Engineering, Virtual Event, USA, November 8-13, 2020, pp. 737749. ACM, 2020. doi: 10.1145/3368089.3409723. URL Xinyu She, Yue Liu, Yanjie Zhao, Yiling He, Li Li, Chakkrit Tantithamthavorn, Zhan Qin, and Haoyu Wang.Pitfalls in language models for code intelligence: A taxonomy and survey. CoRR, abs/2310.17903, 2023.doi: 10.48550/ARXIV.2310.17903. URL Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, JichuanJi, Jingyang Zhao, Yuenan Guo, and Qianxiang Wang. Pangu-coder2: Boosting large language modelsfor code with ranking feedback. CoRR, abs/2307.14936, 2023. doi: 10.48550/arXiv.2307.14936. URL Chaochen Shi, Borui Cai, Yao Zhao, Longxiang Gao, Keshav Sood, and Yong Xiang. Coss: Leveragingstatement semantics for code summarization. IEEE Trans. Software Eng., 49(6):34723486, 2023a. doi:10.1109/TSE.2023.3256362. URL Ensheng Shi, Yanlin Wang, Wei Tao, Lun Du, Hongyu Zhang, Shi Han, Dongmei Zhang, and Hongbin Sun.RACE: retrieval-augmented commit message generation. In Yoav Goldberg, Zornitsa Kozareva, and YueZhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 55205530. Association forComputational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.372. URL",
  "Jiho Shin, Sepehr Hashtroudi, Hadi Hemmati, and Song Wang. Domain adaptation for deep unit test casegeneration. CoRR, abs/2308.08033, 2023. doi: 10.48550/ARXIV.2308.08033. URL": "Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan K. Reddy. Execution-based code generationusing deep reinforcement learning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856.URL Disha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Repofusion:Training code models to understand your repository.CoRR, abs/2306.10998, 2023a.doi: 10.48550/ARXIV.2306.10998. URL Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large lan-guage models of code. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, SivanSabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp.3169331715. PMLR, 2023b. URL Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and Yan Lei. Improving code search with co-attentive representation learning. In ICPC 20: 28th International Conference on Program Comprehension,Seoul, Republic of Korea, July 13-15, 2020, pp. 196207. ACM, 2020. doi: 10.1145/3387904.3389269. URL",
  "Mukul Singh, Jos Cambronero, Sumit Gulwani, Vu Le, Carina Negreanu, and Gust Verbruggen. Codefusion:A pre-trained diffusion model for code generation, 2023": "Jing Kai Siow, Cuiyun Gao, Lingling Fan, Sen Chen, and Yang Liu. CORE: automating review recom-mendation for code changes. In Kostas Kontogiannis, Foutse Khomh, Alexander Chatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui Zhou (eds.), 27th IEEE International Conference on Software Analysis,Evolution and Reengineering, SANER 2020, London, ON, Canada, February 18-21, 2020, pp. 284295.IEEE, 2020. doi: 10.1109/SANER48275.2020.9054794. URL Aishwarya Sivaraman, Rui Abreu, Andrew Scott, Tobi Akomolede, and Satish Chandra. Mining idiomsin the wild. In 44th IEEE/ACM International Conference on Software Engineering: Software Engineer-ing in Practice, ICSE (SEIP) 2022, Pittsburgh, PA, USA, May 22-24, 2022, pp. 187196. IEEE, 2022.doi: 10.1109/ICSE-SEIP55303.2022.9794062. URL Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael Hamza, Haidar Khan, CharithPeris, Stephen Rawls, Andy Rosenbaum, Anna Rumshisky, Chandana Satya Prakash, Mukund Sridhar,Fabian Triefenbach, Apurv Verma, Gkhan Tr, and Prem Natarajan. Alexatm 20b: Few-shot learningusing a large-scale multilingual seq2seq model. CoRR, abs/2208.01448, 2022. doi: 10.48550/arXiv.2208.01448. URL Nikita Sorokin, Dmitry Abulkhanov, Sergey I. Nikolenko, and Valentin Malykh. Cct-code: Cross-consistencytraining for multilingual clone detection and code search. CoRR, abs/2305.11626, 2023. doi: 10.48550/ARXIV.2305.11626. URL",
  "Davit Soselia, Khalid Saifullah, and Tianyi Zhou. Learning ui-to-code reverse generator using visual criticwithout rendering. CoRR, abs/2305.14637, 2023. doi: 10.48550/ARXIV.2305.14637. URL": "Benjamin Steenhoek, Md Mahbubur Rahman, Richard Jiles, and Wei Le.An empirical study of deeplearning models for vulnerability detection. In 45th IEEE/ACM International Conference on SoftwareEngineering, ICSE 2023, Melbourne, Australia, May 14-20, 2023, pp. 22372248. IEEE, 2023a.doi:10.1109/ICSE48619.2023.00188. URL Benjamin Steenhoek, Michele Tufano, Neel Sundaresan, and Alexey Svyatkovskiy. Reinforcement learningfrom automatic feedback for high-quality unit test generation. CoRR, abs/2310.02368, 2023b. doi: 10.48550/ARXIV.2310.02368. URL Benjamin Steenhoek, Md Mahbubur Rahman, Monoshi Kumar Roy, Mirza Sanjida Alam, Earl T. Barr, andWei Le. A comprehensive study of the capabilities of large language models for vulnerability detection.CoRR, abs/2403.17218, 2024.doi: 10.48550/ARXIV.2403.17218.URL",
  "Ddac Surs, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning.CoRR, abs/2303.08128, 2023.doi: 10.48550/ARXIV.2303.08128.URL": "Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. InZoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.), Ad-vances in Neural Information Processing Systems 27: Annual Conference on Neural Information Process-ing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 31043112, 2014. URL",
  "Hanzhuo Tan, Qi Luo, Jing Li, and Yuqun Zhang. Llm4decompile: Decompiling binary code with largelanguage models. CoRR, abs/2403.05286, 2024. doi: 10.48550/ARXIV.2403.05286. URL": "Shin Hwei Tan, Jooyong Yi, Yulis, Sergey Mechtaev, and Abhik Roychoudhury.Codeflaws:a pro-gramming competition benchmark for evaluating automated program repair tools. In Sebastin Uchi-tel, Alessandro Orso, and Martin P. Robillard (eds.), Proceedings of the 39th International Confer-ence on Software Engineering, ICSE 2017, Buenos Aires, Argentina, May 20-28, 2017 - CompanionVolume, pp. 180182. IEEE Computer Society, 2017.doi:10.1109/ICSE-C.2017.76.URL Lappoon R. Tang and Raymond J. Mooney. Automated construction of database interfaces: Intergratingstatistical and relational learning for semantic parsing. In Hinrich Schtze and Keh-Yih Su (eds.), JointSIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,EMNLP 2000, Hong Kong, October 7-8, 2000, pp. 133141. Association for Computational Linguistics,2000. doi: 10.3115/1117794.1117811. URL",
  "Yutian Tang, Zhijie Liu, Zhichao Zhou, and Xiapu Luo.Chatgpt vs SBST: A comparative assessmentof unit test suite generation. CoRR, abs/2307.00588, 2023a. doi: 10.48550/ARXIV.2307.00588. URL": "Ze Tang, Xiaoyu Shen, Chuanyi Li, Jidong Ge, Liguo Huang, Zheling Zhu, and Bin Luo. Ast-trans: Codesummarization with efficient tree-structured attention. In 44th IEEE/ACM 44th International Conferenceon Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 150162. ACM, 2022.doi: 10.1145/3510003.3510224. URL Zilu Tang, Mayank Agarwal, Alex Shypula, Bailin Wang, Derry Wijaya, Jie Chen, and Yoon Kim. Explain-then-translate: An analysis on improving program translation with self-generated explanations. CoRR,abs/2311.07070, 2023b. doi: 10.48550/ARXIV.2311.07070. URL Shimin Tao, Weibin Meng, Yimeng Chen, Yichen Zhu, Ying Liu, Chunning Du, Tao Han, Yongpeng Zhao,Xiangguang Wang, and Hao Yang. Logstamp: Automatic online log parsing based on sequence labelling.SIGMETRICS Perform. Evaluation Rev., 49(4):9398, 2022. doi: 10.1145/3543146.3543168. URL",
  "Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput.Surv., 55(6):109:1109:28, 2023a. doi: 10.1145/3530811. URL": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, DaraBahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: unifyinglanguage learning paradigms. In The Eleventh International Conference on Learning Representations,ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023b. URL",
  "CodeGemma Team.Codegemma: Open code models based on gemma, 2024.URL": "Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce, Benjamin Tan, Ramesh Karri, BrendanDolan-Gavitt, and Siddharth Garg. Benchmarking large language models for automated verilog RTL codegeneration. In Design, Automation & Test in Europe Conference & Exhibition, DATE 2023, Antwerp,Belgium, April 17-19, 2023, pp. 16. IEEE, 2023a.doi: 10.23919/DATE56975.2023.10137086.URL Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin Tan, Brendan Dolan-Gavitt, Ramesh Karri,and Siddharth Garg. Verigen: A large language model for verilog code generation. CoRR, abs/2308.00708,2023b. doi: 10.48550/ARXIV.2308.00708. URL Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, AminGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, IgorKrivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, MarkDiaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, RaviRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, RachelBernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le.Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022. URL Kiran Thorat, Jiahui Zhao, Yaotian Liu, Hongwu Peng, Xi Xie, Bin Lei, Jeff Zhang, and Caiwen Ding.Advanced large language model (llm)-driven verilog development: Enhancing power, performance, andarea optimization in code synthesis. CoRR, abs/2312.01022, 2023. doi: 10.48550/ARXIV.2312.01022.URL Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan, Yesai Wu, Zhiyuan Liu, and MaosongSun. Debugbench: Evaluating debugging capability of large language models. CoRR, abs/2401.04621,2024. doi: 10.48550/ARXIV.2401.04621. URL",
  "Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit test casegeneration with transformers and focal context, 2021a": "Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, and Neel Sundaresan.Generating accurate assertstatements for unit test cases using pretrained transformers. In IEEE/ACM International Conferenceon Automation of Software Test, AST@ICSE 2022, Pittsburgh, PA, USA, May 21-22, 2022, pp. 5464.ACM/IEEE, 2022a. doi: 10.1145/3524481.3527220. URL Rosalia Tufano, Luca Pascarella, Michele Tufano, Denys Poshyvanyk, and Gabriele Bavota. Towards au-tomating code review activities. In 43rd IEEE/ACM International Conference on Software Engineering,ICSE 2021, Madrid, Spain, 22-30 May 2021, pp. 163174. IEEE, 2021b. doi: 10.1109/ICSE43902.2021.00027. URL Rosalia Tufano, Simone Masiero, Antonio Mastropaolo, Luca Pascarella, Denys Poshyvanyk, and GabrieleBavota. Using pre-trained models to boost code review automation. In 44th IEEE/ACM 44th InternationalConference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 22912302.ACM, 2022b. doi: 10.1145/3510003.3510621. URL Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Natural Language Processing with Transformers:Building Language Applications with Hugging Face. OReilly Media, Incorporated, 2022. ISBN 1098103246.URL Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce, Ayse K. Coskun, and Gianluca Stringhini. Canlarge language models identify and reason about security vulnerabilities? not yet. CoRR, abs/2312.12575,2023. doi: 10.48550/ARXIV.2312.12575. URL Tim van Dam, Frank van der Heijden, Philippe de Bekker, Berend Nieuwschepen, Marc Otten, and MalihehIzadi. Investigating the performance of language models for completing code in functional programminglanguages: a haskell case study. CoRR, abs/2403.15185, 2024. doi: 10.48550/ARXIV.2403.15185. URL Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural program repairby jointly learning to localize and repair. In 7th International Conference on Learning Representations,ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL",
  "Vasudev Vikram, Caroline Lemieux, and Rohan Padhye. Can large language models write good property-based tests? CoRR, abs/2307.04346, 2023. doi: 10.48550/ARXIV.2307.04346. URL": "Johannes Villmow, Jonas Depoix, and Adrian Ulges. ConTest: A unit test completion benchmark featuringcontext. In Royi Lachmy, Ziyu Yao, Greg Durrett, Milos Gligoric, Junyi Jessy Li, Ray Mooney, Gra-ham Neubig, Yu Su, Huan Sun, and Reut Tsarfaty (eds.), Proceedings of the 1st Workshop on NaturalLanguage Processing for Programming (NLP4Prog 2021), pp. 1725, Online, August 2021. Associationfor Computational Linguistics. doi: 10.18653/v1/2021.nlp4prog-1.2. URL Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S. Yu. Improvingautomatic source code summarization via deep reinforcement learning. In Marianne Huchard, ChristianKstner, and Gordon Fraser (eds.), Proceedings of the 33rd ACM/IEEE International Conference onAutomated Software Engineering, ASE 2018, Montpellier, France, September 3-7, 2018, pp. 397407.ACM, 2018. doi: 10.1145/3238147.3238206. URL Yao Wan, Guanghua Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Pan Zhou, Hai Jin, and Lichao Sun.Does your neural code completion model use my code?a membership inference approach.CoRR,abs/2404.14296, 2024.doi: 10.48550/ARXIV.2404.14296.URL Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: Amulti-task benchmark and analysis platform for natural language understanding. In Tal Linzen, GrzegorzChrupala, and Afra Alishahi (eds.), Proceedings of the Workshop: Analyzing and Interpreting NeuralNetworks for NLP, BlackboxNLP@EMNLP 2018, Brussels, Belgium, November 1, 2018, pp. 353355.Association for Computational Linguistics, 2018a. doi: 10.18653/v1/w18-5446. URL Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understandingsystems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B.Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Con-ference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-ver, BC, Canada, pp. 32613275, 2019.URL",
  "Ben Wang and Aran Komatsuzaki.GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. May 2021": "Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2words: Automaticmobile UI summarization with multimodal learning. In Jeffrey Nichols, Ranjitha Kumar, and MichaelNebeling (eds.), UIST 21: The 34th Annual ACM Symposium on User Interface Software and Technology,Virtual Event, USA, October 10-14, 2021, pp. 498510. ACM, 2021b. doi: 10.1145/3472749.3474765. URL Bryan Wang, Gang Li, and Yang Li. Enabling conversational interaction with mobile UI using large languagemodels. In Albrecht Schmidt, Kaisa Vnnen, Tesh Goyal, Per Ola Kristensson, Anicia Peters, StefanieMueller, Julie R. Williamson, and Max L. Wilson (eds.), Proceedings of the 2023 CHI Conference onHuman Factors in Computing Systems, CHI 2023, Hamburg, Germany, April 23-28, 2023, pp. 432:1432:17. ACM, 2023a. doi: 10.1145/3544548.3580895. URL Chunhui Wang, Fabrizio Pastore, and Lionel C. Briand. Automated generation of constraints from use casespecifications to support system testing. In 11th IEEE International Conference on Software Testing,Verification and Validation, ICST 2018, Vsters, Sweden, April 9-13, 2018, pp. 2333. IEEE ComputerSociety, 2018b. doi: 10.1109/ICST.2018.00013. URL Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong, Wei Dong, and Xiangke Liao. Bridging pre-trained models and downstream tasks for source code understanding. In 44th IEEE/ACM 44th Interna-tional Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 287298. ACM, 2022a. doi: 10.1145/3510003.3510062. URL Haoye Wang, Xin Xia, David Lo, Qiang He, Xinyu Wang, and John Grundy.Context-aware retrieval-based deep commit message generation. ACM Trans. Softw. Eng. Methodol., 30(4):56:156:30, 2021c. doi:10.1145/3464689. URL Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing Wang. Software testing withlarge language models: Survey, landscape, and vision. CoRR, abs/2307.07221, 2023b. doi: 10.48550/ARXIV.2307.07221. URL Ke Wang and Zhendong Su. Blended, precise semantic program embeddings. In Proceedings of the 41stACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2020, pp.121134, New York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450376136. doi:10.1145/3385412.3385999. URL Liran Wang, Xunzhu Tang, Yichen He, Changyu Ren, Shuhua Shi, Chaoran Yan, and Zhoujun Li. Delv-ing into commit-issue correlation to enhance commit message generation models. In 38th IEEE/ACMInternational Conference on Automated Software Engineering, ASE 2023, Luxembourg, September 11-15,2023, pp. 710722. IEEE, 2023c. doi: 10.1109/ASE56229.2023.00050. URL",
  "Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linearcomplexity. CoRR, abs/2006.04768, 2020c. URL": "Song Wang, Devin Chollak, Dana Movshovitz-Attias, and Lin Tan. Bugram: bug detection with n-gramlanguage models. In David Lo, Sven Apel, and Sarfraz Khurshid (eds.), Proceedings of the 31st IEEE/ACMInternational Conference on Automated Software Engineering, ASE 2016, Singapore, September 3-7, 2016,pp. 708719. ACM, 2016a. doi: 10.1145/2970276.2970341. URL Song Wang, Taiyue Liu, and Lin Tan.Automatically learning semantic features for defect prediction.In Laura K. Dillon, Willem Visser, and Laurie A. Williams (eds.), Proceedings of the 38th InternationalConference on Software Engineering, ICSE 2016, Austin, TX, USA, May 14-22, 2016, pp. 297308. ACM,2016b. doi: 10.1145/2884781.2884804. URL Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, JulienLaunay, and Colin Raffel. What language model architecture and pretraining objective works best forzero-shot generalization?In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvri, GangNiu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 2296422984. PMLR, 2022c. URL Wenhan Wang, Ge Li, Bo Ma, Xin Xia, and Zhi Jin.Detecting code clones with graph neural net-work and flow-augmented abstract syntax tree.In Kostas Kontogiannis, Foutse Khomh, AlexanderChatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui Zhou (eds.), 27th IEEE International Confer-ence on Software Analysis, Evolution and Reengineering, SANER 2020, London, ON, Canada, Febru-ary 18-21, 2020, pp. 261271. IEEE, 2020d.doi: 10.1109/SANER48275.2020.9054857.URL",
  "Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang.Syncobert: Syntax-guided multi-modal contrastive pre-training for code representation, 2021d": "Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yitong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang, andQun Liu. Compilable neural code generation with compiler feedback. In Smaranda Muresan, PreslavNakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL2022, Dublin, Ireland, May 22-27, 2022, pp. 919. Association for Computational Linguistics, 2022d. doi:10.18653/V1/2022.FINDINGS-ACL.2. URL Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou, Li Li, Hao Wu, and Jin Liu. CODE-MVP: learning to represent source code from multiple views with contrastive pre-training. In MarineCarpuat, Marie-Catherine de Marneffe, and Ivn Vladimir Meza Ruz (eds.), Findings of the Associationfor Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pp. 10661077. Association for Computational Linguistics, 2022e. doi: 10.18653/v1/2022.findings-naacl.80. URL",
  "Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, and Heng Ji. Leti: Learning to generate from textualinteractions. CoRR, abs/2305.10314, 2023f. doi: 10.48550/ARXIV.2305.10314. URL": "Xuheng Wang, Xu Zhang, Liqun Li, Shilin He, Hongyu Zhang, Yudong Liu, Lingling Zheng, Yu Kang, Qing-wei Lin, Yingnong Dang, Saravanakumar Rajmohan, and Dongmei Zhang. SPINE: a scalable log parserwith feedback guidance. In Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (eds.), Proceedingsof the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundationsof Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, pp. 11981208.ACM, 2022f. doi: 10.1145/3540250.3549176. URL Yanlin Wang, Ensheng Shi, Lun Du, Xiaodi Yang, Yuxuan Hu, Shi Han, Hongyu Zhang, and DongmeiZhang. Cocosum: Contextual code summarization with multi-relational graph neural network. CoRR,abs/2107.01933, 2021e. URL Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, AtharvaNaik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Gi-annis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia,Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-hit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Sa-van Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit,and Xudong Shen. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLPtasks.In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Confer-ence on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United ArabEmirates, December 7-11, 2022, pp. 50855109. Association for Computational Linguistics, 2022g. URL Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jor-dan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023,pp. 1348413508. Association for Computational Linguistics, 2023g. doi: 10.18653/v1/2023.acl-long.754.URL Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified pre-trainedencoder-decoder models for code understanding and generation.In Marie-Francine Moens, XuanjingHuang, Lucia Specia, and Scott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empiri-cal Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican",
  "Republic, 7-11 November, 2021, pp. 86968708. Association for Computational Linguistics, 2021f. doi:10.18653/v1/2021.emnlp-main.685. URL": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+:Open code large language models for code understanding and generation. CoRR, abs/2305.07922, 2023h.doi: 10.48550/arXiv.2305.07922. URL Zan Wang, Ming Yan, Junjie Chen, Shuang Liu, and Dongdi Zhang. Deep learning library testing via effectivemodel generation.In Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (eds.), ESEC/FSE20: 28th ACM Joint European Software Engineering Conference and Symposium on the Foundationsof Software Engineering, Virtual Event, USA, November 8-13, 2020, pp. 788799. ACM, 2020f.doi:10.1145/3368089.3409761. URL",
  "Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig.Execution-based evaluation for open-domain code generation. CoRR, abs/2212.10481, 2022h. doi: 10.48550/ARXIV.2212.10481. URL": "Zhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F. Xu, and Graham Neubig. Mconala: A benchmark forcode generation from multiple natural languages. In Andreas Vlachos and Isabelle Augenstein (eds.), Find-ings of the Association for Computational Linguistics: EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pp.265273. Association for Computational Linguistics, 2023i. doi: 10.18653/V1/2023.FINDINGS-EACL.20.URL Cody Watson, Michele Tufano, Kevin Moran, Gabriele Bavota, and Denys Poshyvanyk. On learning mean-ingful assert statements for unit test cases. In Gregg Rothermel and Doo-Hwan Bae (eds.), ICSE 20: 42ndInternational Conference on Software Engineering, Seoul, South Korea, 27 June - 19 July, 2020, pp. 13981409. ACM, 2020. doi: 10.1145/3377811.3380429. URL Anjiang Wei, Yinlin Deng, Chenyuan Yang, and Lingming Zhang. Free lunch for testing: Fuzzing deep-learning libraries from open source.In 44th IEEE/ACM 44th International Conference on SoftwareEngineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022, pp. 9951007. ACM, 2022a.doi:10.1145/3510003.3510041. URL Bolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. Code generation as a dual task of code summariza-tion. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B. Fox,and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Confer-ence on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-ver, BC, Canada, pp. 65596569, 2019.URL Huihui Wei and Ming Li. Supervised deep features for software functional clone detection by exploitinglexical and syntactical information in source code. In Carles Sierra (ed.), Proceedings of the Twenty-SixthInternational Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25,2017, pp. 30343040. ijcai.org, 2017. doi: 10.24963/ijcai.2017/423. URL Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,2022b. URL Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Ma-chine Learning Research, 2022c. ISSN 2835-8856. URL Certification.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,QuocV.Le,andDennyZhou.Chain-of-thoughtpromptingelicitsreasoninginlargelan-guage models.In NeurIPS, 2022d.URL": "Jiayi Wei, Maruth Goyal, Greg Durrett, and Isil Dillig.Lambdanet: Probabilistic type inference usinggraph neural networks. In 8th International Conference on Learning Representations, ICLR 2020, AddisAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL Jiayi Wei, Greg Durrett, and Isil Dillig.Typet5: Seq2seq type inference using static analysis.In TheEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net, 2023. URL Yuanbo Wen, Qi Guo, Qiang Fu, Xiaqing Li, Jianxing Xu, Yanlin Tang, Yongwei Zhao, Xing Hu, Zidong Du,Ling Li, Chao Wang, Xuehai Zhou, and Yunji Chen. Babeltower: Learning to auto-parallelized programtranslation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvri, Gang Niu, and SivanSabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 2368523700. PMLR, 2022.URL Martin Weyssow, Houari A. Sahraoui, and Eugene Syriani.Recommending metamodel concepts duringmodeling activities with pre-trained language models. Softw. Syst. Model., 21(3):10711089, 2022. doi:10.1007/S10270-022-00975-5. URL Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and Douglas C. Schmidt.Chatgpt promptpatterns for improving code quality, refactoring, requirements elicitation, and software design. CoRR,abs/2303.07839, 2023.doi: 10.48550/ARXIV.2303.07839.URL Martin White, Christopher Vendome, Mario Linares Vsquez, and Denys Poshyvanyk.Toward deeplearning software repositories. In Massimiliano Di Penta, Martin Pinzger, and Romain Robbes (eds.),12th IEEE/ACM Working Conference on Mining Software Repositories, MSR 2015, Florence, Italy,May 16-17, 2015, pp. 334345. IEEE Computer Society, 2015.doi:10.1109/MSR.2015.38.URL Martin White, Michele Tufano, Christopher Vendome, and Denys Poshyvanyk. Deep learning code frag-ments for code clone detection.In David Lo, Sven Apel, and Sarfraz Khurshid (eds.), Proceedingsof the 31st IEEE/ACM International Conference on Automated Software Engineering, ASE 2016, Sin-gapore, September 3-7, 2016, pp. 8798. ACM, 2016.doi:10.1145/2970276.2970326.URL Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi Qi, Jack Phan, Qijin Tay, Constance Tan, FionaWee, Jodie Ethelda Tan, Yuheng Yieh, Brian Goh, Ferdian Thung, Hong Jin Kang, Thong Hoang, DavidLo, and Eng Lieh Ouh. Bugsinpy: a database of existing bugs in python programs to enable controlledtesting and debugging studies. In Prem Devanbu, Myra B. Cohen, and Thomas Zimmermann (eds.),ESEC/FSE 20: 28th ACM Joint European Software Engineering Conference and Symposium on theFoundations of Software Engineering, Virtual Event, USA, November 8-13, 2020, pp. 15561560. ACM,2020. doi: 10.1145/3368089.3417943. URL",
  "Jie JW Wu and Fatemeh H Fard. Benchmarking the communication competence of code generation for llmsand llm agent. arXiv preprint arXiv:2406.00215, 2024. URL": "Ming Wu, Pengcheng Wang, Kangqi Yin, Haoyu Cheng, Yun Xu, and Chanchal K. Roy. Lvmapper: Alarge-variance clone detector using sequencing alignment approach. IEEE Access, 8:2798627997, 2020.doi: 10.1109/ACCESS.2020.2971545. URL Mingyuan Wu, Ling Jiang, Jiahong Xiang, Yuqun Zhang, Guowei Yang, Huixin Ma, Sen Nie, Shi Wu,Heming Cui, and Lingming Zhang. Evaluating and improving neural program-smoothing-based fuzzing.In 44th IEEE/ACM 44th International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA,USA, May 25-27, 2022, pp. 847858. ACM, 2022. doi: 10.1145/3510003.3510089. URL Chunqiu Steven Xia and Lingming Zhang. Less training, more repairing please: revisiting automated programrepair via zero-shot learning. In Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (eds.), Proceed-ings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundationsof Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18, 2022, pp. 959971.ACM, 2022. doi: 10.1145/3540250.3549101. URL",
  "Chunqiu Steven Xia and Lingming Zhang.Conversational automated program repair.CoRR,abs/2301.13246, 2023.doi: 10.48550/ARXIV.2301.13246.URL": "Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel, and Lingming Zhang. Fuzz4all: Univer-sal fuzzing with large language models. CoRR, abs/2308.04748, 2023a. doi: 10.48550/ARXIV.2308.04748.URL Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. Automated program repair in the era of large pre-trained language models. In 45th IEEE/ACM International Conference on Software Engineering, ICSE2023, Melbourne, Australia, May 14-20, 2023, pp. 14821494. IEEE, 2023b. doi: 10.1109/ICSE48619.2023.00129. URL",
  "Rui Xie, Zhengran Zeng, Zhuohao Yu, Chang Gao, Shikun Zhang, and Wei Ye. Codeshell technical report.CoRR, abs/2403.15747, 2024.doi: 10.48550/ARXIV.2403.15747.URL": "Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-ShengWu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, LukeZettlemoyer, and Tao Yu. Unifiedskg: Unifying and multi-tasking structured knowledge grounding withtext-to-text language models. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedingsof the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi,United Arab Emirates, December 7-11, 2022, pp. 602631. Association for Computational Linguistics,2022b. doi: 10.18653/v1/2022.emnlp-main.39. URL",
  "Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and Jianwei Yin. Chatunitest: a chatgpt-basedautomated unit test generation tool. CoRR, abs/2305.04764, 2023b. doi: 10.48550/ARXIV.2305.04764.URL": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, andDaxin Jiang. WizardLM: Empowering large pre-trained language models to follow complex instructions.In The Twelfth International Conference on Learning Representations, 2024. URL Frank F. Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of largelanguage models of code. In Swarat Chaudhuri and Charles Sutton (eds.), MAPS@PLDI 2022: 6th ACMSIGPLAN International Symposium on Machine Programming, San Diego, CA, USA, 13 June 2022, pp.110. ACM, 2022. doi: 10.1145/3520312.3534862. URL Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan, Yan Lei, and Zhou Xu. Two-stage attention-based model for code search with textual and structural features. In 28th IEEE International Conferenceon Software Analysis, Evolution and Reengineering, SANER 2021, Honolulu, HI, USA, March 9-12, 2021,pp. 342353. IEEE, 2021a. doi: 10.1109/SANER50967.2021.00039. URL",
  "Yichen Xu and Yanqiao Zhu. A survey on pretrained language models for neural code intelligence. CoRR,abs/2212.10079, 2022. doi: 10.48550/arXiv.2212.10079. URL": "Zhaogui Xu, Xiangyu Zhang, Lin Chen, Kexin Pei, and Baowen Xu. Python probabilistic type inferencewith natural language support. In Thomas Zimmermann, Jane Cleland-Huang, and Zhendong Su (eds.),Proceedings of the 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,FSE 2016, Seattle, WA, USA, November 13-18, 2016, pp. 607618. ACM, 2016. doi: 10.1145/2950290.2950343. URL",
  "Zhifeng Xu, Xianjin Fang, and Gaoming Yang. Malbert: A novel pre-training method for malware detection.Comput. Secur., 111:102458, 2021b. doi: 10.1016/J.COSE.2021.102458. URL": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. In Kristina Toutanova,Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tr, Iz Beltagy, Steven Bethard, Ryan Cotterell,Tanmoy Chakraborty, and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT2021, Online, June 6-11, 2021, pp. 483498. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.naacl-main.41. URL",
  "Mohammad A. Yahya and Dae-Kyoo Kim. Cross-language source code clone detection using deep learningwith infercode. CoRR, abs/2205.04913, 2022. doi: 10.48550/arXiv.2205.04913. URL": "Shuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao Jiang. Are the code snippets what we aresearching for? A benchmark and an empirical study on code search with natural-language queries. InKostas Kontogiannis, Foutse Khomh, Alexander Chatzigeorgiou, Marios-Eleftherios Fokaefs, and MinghuiZhou (eds.), 27th IEEE International Conference on Software Analysis, Evolution and Reengineering,SANER 2020, London, ON, Canada, February 18-21, 2020, pp. 344354. IEEE, 2020.doi: 10.1109/SANER48275.2020.9054840. URL Weixiang Yan, Yuchen Tian, Yunzhe Li, Qian Chen, and Wen Wang. Codetransocean: A comprehensivemultilingual benchmark for code translation. CoRR, abs/2310.04951, 2023. doi: 10.48550/ARXIV.2310.04951. URL",
  "Wei Yang, Peng Xu, and Yanshuai Cao. Hierarchical neural data synthesis for semantic parsing. CoRR,abs/2112.02212, 2021. URL": "Xianjun Yang, Kexun Zhang, Haifeng Chen, Linda R. Petzold, William Yang Wang, and Wei Cheng. Zero-shot detection of machine-generated codes. CoRR, abs/2310.05103, 2023e. doi: 10.48550/ARXIV.2310.05103. URL Yanping Yang, Ling Xu, Meng Yan, Zhou Xu, and Zhongyang Deng. A naming pattern based approachfor method name recommendation. In IEEE 33rd International Symposium on Software Reliability En-gineering, ISSRE 2022, Charlotte, NC, USA, October 31 - Nov. 3, 2022, pp. 344354. IEEE, 2022. doi:10.1109/ISSRE55969.2022.00041. URL Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. Staqc: A systematically mined question-codedataset from stack overflow. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagio-tis G. Ipeirotis (eds.), Proceedings of the 2018 World Wide Web Conference on World Wide Web, WWW2018, Lyon, France, April 23-27, 2018, pp. 16931703. ACM, 2018. doi: 10.1145/3178876.3186081. URL Michihiro Yasunaga and Percy Liang. Graph-based, self-supervised program repair from diagnostic feedback.In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 1079910808. PMLR, 2020.URL",
  "Ying Yin, Yuhai Zhao, Yiming Sun, and Chen Chen. Automatic code review by learning the structureinformation of code graph. Sensors, 23(5):2551, 2023. doi: 10.3390/s23052551. URL": "Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu,Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang,Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, YudongLiu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation modelsby 01.ai. CoRR, abs/2403.04652, 2024. doi: 10.48550/ARXIV.2403.04652. URL Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, and Qianxiang Wang. Neural detection of semantic codeclones via tree-based convolution. In Yann-Gal Guhneuc, Foutse Khomh, and Federica Sarro (eds.),Proceedings of the 27th International Conference on Program Comprehension, ICPC 2019, Montreal, QC,Canada, May 25-31, 2019, pp. 7080. IEEE / ACM, 2019a. doi: 10.1109/ICPC.2019.00021. URL Hao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao Xie, andQianxiang Wang.Codereval: A benchmark of pragmatic code generation with generative pre-trainedmodels. CoRR, abs/2302.00288, 2023a. doi: 10.48550/arXiv.2302.00288. URL",
  "Chunyan Zhang, Junchao Wang, Qinglei Zhou, Ting Xu, Ke Tang, Hairen Gui, and Fudong Liu. A surveyof automatic source code summarization. Symmetry, 14(3):471, 2022. doi: 10.3390/SYM14030471. URL": "Dejiao Zhang, Wasi Uddin Ahmad, Ming Tan, Hantian Ding, Ramesh Nallapati, Dan Roth, Xiaofei Ma, andBing Xiang. Code representation learning at scale. CoRR, abs/2402.01935, 2024a. doi: 10.48550/ARXIV.2402.01935. URL Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and WeizhuChen. Repocoder: Repository-level code completion through iterative retrieval and generation. CoRR,abs/2303.12570, 2023b. doi: 10.48550/ARXIV.2303.12570. URL",
  "Haibo Zhang and Kouichi Sakurai. A survey of software clone detection from security perspective. IEEEAccess, 9:4815748173, 2021. doi: 10.1109/ACCESS.2021.3065872. URL": "Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, Kaixuan Wang, and Xudong Liu. A novel neural sourcecode representation based on abstract syntax tree. In Joanne M. Atlee, Tevfik Bultan, and Jon Whittle(eds.), Proceedings of the 41st International Conference on Software Engineering, ICSE 2019, Montreal,QC, Canada, May 25-31, 2019, pp. 783794. IEEE / ACM, 2019a. doi: 10.1109/ICSE.2019.00086. URL Jiansong Zhang and Nora M. El-Gohary. Integrating semantic nlp and logic reasoning into a unified system forfully-automated code checking. Automation in Construction, 73:4557, 2017. ISSN 0926-5805. doi: URL",
  "Jingxuan Zhang, He Jiang, Zhilei Ren, and Xin Chen. Recommending apis for API related questions instack overflow.IEEE Access, 6:62056219, 2018.doi: 10.1109/ACCESS.2017.2777845.URL": "Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation. InAnna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,July 9-14, 2023, pp. 769787. Association for Computational Linguistics, 2023c. doi: 10.18653/V1/2023.ACL-LONG.45. URL",
  "Quanjun Zhang, Chunrong Fang, Yuxiang Ma, Weisong Sun, and Zhenyu Chen.A survey of learning-based automated program repair. CoRR, abs/2301.03270, 2023d. doi: 10.48550/arXiv.2301.03270. URL": "Rui Zhang, Tao Yu, Heyang Er, Sungrok Shim, Eric Xue, Xi Victoria Lin, Tianze Shi, Caiming Xiong,Richard Socher, and Dragomir R. Radev. Editing-based SQL query generation for cross-domain context-dependent questions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,2019, pp. 53375348. Association for Computational Linguistics, 2019b. doi: 10.18653/v1/D19-1537. URL Shudan Zhang, Hanlin Zhao, Xiao Liu, Qinkai Zheng, Zehan Qi, Xiaotao Gu, Xiaohan Zhang, Yuxiao Dong,and Jie Tang. Naturalcodebench: Examining coding performance mismatch on humaneval and naturaluser prompts. 2024b. URL Tianzhu Zhang, Han Qiu, Gabriele Castellano, Myriana Rifai, Chung Shue Chen, and Fabio Pianese. Systemlog parsing: A survey. IEEE Trans. Knowl. Data Eng., 35(8):85968614, 2023e. doi: 10.1109/TKDE.2022.3222417. URL Xinyu Zhang, Siddharth Muralee, Sourag Cherupattamoolayil, and Aravind Machiry. On the effectiveness oflarge language models for github workflows. CoRR, abs/2403.12446, 2024c. doi: 10.48550/ARXIV.2403.12446. URL Xu Zhang, Yong Xu, Qingwei Lin, Bo Qiao, Hongyu Zhang, Yingnong Dang, Chunyu Xie, Xinsheng Yang,Qian Cheng, Ze Li, Junjie Chen, Xiaoting He, Randolph Yao, Jian-Guang Lou, Murali Chintalapati,Furao Shen, and Dongmei Zhang. Robust log-based anomaly detection on unstable log data. In MarlonDumas, Dietmar Pfahl, Sven Apel, and Alessandra Russo (eds.), Proceedings of the ACM Joint Meeting onEuropean Software Engineering Conference and Symposium on the Foundations of Software Engineering,ESEC/SIGSOFT FSE 2019, Tallinn, Estonia, August 26-30, 2019, pp. 807817. ACM, 2019c. doi: 10.1145/3338906.3338931. URL",
  "Ziyin Zhang, Lizhen Xu, Zhaokun Jiang, Hongkun Hao, and Rui Wang. Multiple-choice questions are efficientand robust llm evaluators. 2024d. URL": "Gang Zhao and Jeff Huang.Deepsim: deep learning code functional similarity.In Gary T. Leavens,Alessandro Garcia, and Corina S. Pasareanu (eds.), Proceedings of the 2018 ACM Joint Meeting on Eu-ropean Software Engineering Conference and Symposium on the Foundations of Software Engineering,ESEC/SIGSOFT FSE 2018, Lake Buena Vista, FL, USA, November 04-09, 2018, pp. 141151. ACM,2018. doi: 10.1145/3236024.3236068. URL",
  "Liang Zhao, Hexin Cao, and Yunsong Zhao. GP: context-free grammar pre-training for text-to-sql parsers.CoRR, abs/2101.09901, 2021. URL": "Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, and Ting Liu. Length extrapolation of transformers:A survey from the perspective of position encoding. CoRR, abs/2312.17044, 2023. doi: 10.48550/ARXIV.2312.17044. URL Liping Zhao, Waad Alhoshan, Alessio Ferrari, Keletso J. Letsholo, Muideen A. Ajagbe, Erol-ValeriuChioasca, and Riza Theresa Batista-Navarro. Natural language processing (NLP) for requirements en-gineering: A systematic mapping study. CoRR, abs/2004.01099, 2020. URL Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, AndiWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code generationwith multilingual benchmarking on humaneval-x. In Proceedings of the 29th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, KDD 23, pp. 56735684, New York, NY, USA, 2023a. Associationfor Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599790. URL Tianyu Zheng, Ge Zhang, Tianhao Shen, Xueling Liu, Bill Yuchen Lin, Jie Fu, Wenhu Chen, and Xiang Yue.Opencodeinterpreter: Integrating code generation with execution and refinement. CoRR, abs/2402.14658,2024. doi: 10.48550/ARXIV.2402.14658. URL Yunhui Zheng, Saurabh Pujar, Burn L. Lewis, Luca Buratti, Edward A. Epstein, Bo Yang, Jim Laredo,Alessandro Morari, and Zhong Su. D2A: A dataset built for ai-based vulnerability detection methods usingdifferential analysis. In 43rd IEEE/ACM International Conference on Software Engineering: Software En-gineering in Practice, ICSE (SEIP) 2021, Madrid, Spain, May 25-28, 2021, pp. 111120. IEEE, 2021. doi:10.1109/ICSE-SEIP52600.2021.00020. URL Zibin Zheng, Kaiwen Ning, Jiachi Chen, Yanlin Wang, Wenqing Chen, Lianghong Guo, and Weicheng Wang.Towards an understanding of large language models in software engineering tasks. CoRR, abs/2308.11396,2023b. doi: 10.48550/ARXIV.2308.11396. URL",
  "Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from naturallanguage using reinforcement learning. CoRR, abs/1709.00103, 2017. URL": "Victor Zhong, Mike Lewis, Sida I. Wang, and Luke Zettlemoyer.Grounded adaptation for zero-shotexecutable semantic parsing.In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Pro-ceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020,Online, November 16-20, 2020, pp. 68696882. Association for Computational Linguistics, 2020.doi:10.18653/V1/2020.EMNLP-MAIN.558. URL",
  "ATISHemphill et al. (1990); Dahlet al. (1994)11508": "1996GeoQueryZelle & Mooney (1996)8772000RestaurantsTang & Mooney (2000)3782014-09MASLi & Jagadish (2014)1962017-02YelpYaghmazadeh et al. (2017)1282017-02IMDbYaghmazadeh et al. (2017)1312017-04ScholarIyer et al. (2017)8162017-08WikiSQLZhong et al. (2017)806542018-06AdvisingFinegan-Dollak et al. (2018)45702018-09SpiderYu et al. (2018c)101812019-06SParCYu et al. (2019c)127262019-07MIMICSQLWang et al. (2020b)100002019-09CoSQLYu et al. (2019b)155982020-05Criteria-to-SQLYu et al. (2020)20032020-10SquallShi et al. (2020)112762020-10Spider-RealisticDeng et al. (2021)5082021-06Spider-SynGan et al. (2021a)80342021-06SEDEHazoom et al. (2021)120232021-06KaggleDBQALee et al. (2021)4002021-09Spider-DKGan et al. (2021b)5352022-05Spider-SSGan et al. (2022)80342022-05Spider-CGGan et al. (2022)455992023-05BIRDLi et al. (2023g)12751",
  "ProgramRepair": "2014-07Defects4JJust et al. (2014)357Java2015-12ManyBugsGoues et al. (2015)185C2015-12IntroClassGoues et al. (2015)998C2016-11BugAIDHanam et al. (2016)105KJS2017-02DeepFixGupta et al. (2017)6971C2017-05CodeflawsTan et al. (2017)3902C2017-10QuixBugsLin et al. (2017)80Java, Python2018-05Bugs.jarSaha et al. (2018)1158Java2018-12BFPChakraborty et al. (2022b)124KJava2019-01BearsMadeiral et al. (2019)251Java2019-01unnamedTufano et al. (2019a)21.8KJava2019-04BugsJSGyimesi et al. (2019)453JS2019-05BugSwarmTomassi et al. (2019)1827/1264Java/Python2019-05CPatMinerNguyen et al. (2019)17KJava2019-05ManySStuBs4JKarampatsis & Sutton (2020)154KJava2019-11RefactoryHu et al. (2019)1783Python",
  "Defect(Vulnerability)Detection": "2018-01CGDLi et al. (2018b)62KC, C++2018-07Draper VDISCRussell et al. (2018)12.8MC, C++2018-07SySeVRLi et al. (2022j)15591C, C++2018-04unnamedLin et al. (2018a)32988C, C++2019-02unnamedPonta et al. (2019)624Java2019-09DevignZhou et al. (2019)48687C2019-11unnamedLin et al. (2021)170KC, C++2019-12GREATHellendoorn et al. (2020)2.8MPython2020-01MVDZou et al. (2021)182KC, C++2020-02unnamedLin et al. (2019)1471C2020-09ReVealChakraborty et al. (2022c)18KC2020-09Big-VulFan et al. (2020)265KC, C++2021-02D2AZheng et al. (2021)1.3MC, C++2021-05PyPIBugsAllamanis et al. (2021)2374Python2021-07CVEfixesBhandari et al. (2021)5495272021-08CrossVulNikitopoulos et al. (2021)2747640+2023-04DiverseVulChen et al. (2023g)349KC, C++2023-06VulnPatchPairsRisse & Bhme (2023)26KC2023-11VulBenchGao et al. (2023d)455C"
}