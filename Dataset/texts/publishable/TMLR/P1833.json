{
  "Abstract": "Vision-based imitation learning has shown promising capabilities of endowing robots withvarious motion skills given visual observation. However, current visuomotor policies failto adapt to drastic changes in their visual observations. We present Perception Stitchingthat enables strong zero-shot adaptation to large visual changes by directly stitching novelcombinations of visual encoders. Our key idea is to enforce modularity of visual encodersby aligning the latent visual features among different visuomotor policies.Our methoddisentangles the perceptual knowledge with the downstream motion skills and allows thereuse of the visual encoders by directly stitching them to a policy network trained withpartially different visual conditions. We evaluate our method in various simulated and real-world manipulation tasks. While baseline methods failed at all attempts, our method couldachieve zero-shot success in real-world visuomotor tasks. Our quantitative and qualitativeanalysis of the learned features of the policy network provides more insights into the highperformance of our proposed method. : Perception Stitching: Policy A was trained with an in-hand camera and a front-view camera. Policy B wastrained with a close-up camera and a side-view camera. Perception Stitching enables zero-shot stitching of the original PolicyA and B by reusing their relevant components for each sensing configuration to form a Policy C. Policy C can maintainstrong zero-shot transfer performance with an in-hand camera and a side-view camera.",
  "Introduction": "Despite recent advances in vision-based imitation learning for acquiring diverse motor skills (Chi et al., 2023;Fu et al., 2024), a significant challenge in deploying visuomotor policies in real-world settings is ensuringthe perceptual configurations are identical during training and policy execution. With the growth of hybridrobot datasets where robots are trained to perform similar tasks across the world, it remains difficult to sharetheir learned experiences, even under the same task but with different visual observations. Such a challengeoften stems from the unique configurations and perspectives each camera setup brings, which, while enrichingthe dataset, complicates the sharing of learned experiences across different systems. Different institutionsworldwide usually place different sensors at different perspectives when collecting their datasets. In these",
  "cases, previously collected datasets or trained policies are not interchangeable, and new training data mustbe collected in each environment": "Instead, what if we could directly stitch the perception encoder trained in one visuomotor policy to therest of the components of another visuomotor policy? This approach would enable zero-shot transfer of thetrained visuomotor policies to a novel combination of perceptual configurations. To address the challengesassociated with zero-shot transfer across different settings, previous efforts have aimed to learn an invariantfeature space or universal representation for quick adaptation to new environments, with approaches rangingfrom extensive pre-training with video data (Nair et al., 2022), employing contrastive learning between twopolicies to find a common feature space (Gupta et al., 2017), and concentrating on low-dimensional dataover vision-based observations (Jian et al., 2023). In particular, recent studies have proposed to achieve fastpolicy transfer (Jian et al., 2023) by aligning the latent representations of different perception encoders withrelative representation (Moschella et al., 2022). However, it remains unclear how to scale similar approachesfrom few-shot transfer to zero-shot transfer and high-dimensional observations. We present Perception Stitching (PeS) () to enable zero-shot perception encoder transfer forvisuomotor robot policies. PeS advances the previous studies through a novel training scheme under variouscamera configurations and effectively processing high-dimensional image data.Our approach can trainmodular perception encoders for specific visual configurations (e.g. camera parameters and positions) andreuse the trained perception encoders in a novel environment in a plug-and-go manner. In the simulation,we evaluate PeS in five different robotic manipulation tasks, each with seven unique visual configurations. Itconstantly shows significant performance improvement compared to four baseline methods and two ablationstudies. We also evaluate PeS in four real-world manipulation tasks. While the baseline struggles to getany successful attempts, PeS achieves pronounced success rates, indicating that directly stitching modularperception encoders in the real world has been turned from impossible to possible by this work. Additionally,we contribute quantitative and qualitative analysis to provide more insights on the high performance of ourproposed method.",
  "Related Work": "Learning Visuomotor Policy for Robotic Manipulation A wide range of previous work has focused onlearning visuomotor policy for robotic manipulation (Levine et al., 2016; Finn et al., 2016; 2017b; Kalashnikovet al., 2018; Srinivas et al., 2018; Ebert et al., 2018; Zhu et al., 2018; Rafailov et al., 2021; Jain et al., 2019;Hmlinen et al., 2019; Florence et al., 2022; Brohan et al., 2022; 2023; Padalkar et al., 2023; Sermanetet al., 2018). Certain works have investigated the impact of camera placements (Zaky et al., 2020; Hsuet al., 2022), design of the hardware and software (Zhao et al., 2023; Fu et al., 2024; Kim et al., 2023), novelnetwork architectures and optimization techniques (Dasari & Gupta, 2021; Kim et al., 2021; Zhu et al., 2023;Abolghasemi et al., 2019; Ramachandruni et al., 2020; Brohan et al., 2023; 2022; Padalkar et al., 2023; Chiet al., 2023; Li et al., 2023). In this work, we show that perception encoders trained with Behavior Cloning(BC) (Pomerleau, 1988) often lack the flexibility for module reuse. Our Perception Stitching (PeS) methodenables perceptual knowledge reuse and facilitates zero-shot transfer between diverse visual configurations,advancing existing research on fast adaptable visuomotor policy design. Robot Transfer Learning Transfer learning has long been considered a primary challenge in robotics(Tan et al., 2018; Taylor & Stone, 2009). In the context of reinforcement learning, many previous worktransfer different components such as policies (Devin et al., 2017; Konidaris & Barto, 2007; Fernndez &Veloso, 2006), parameters (Finn et al., 2017a; Killian et al., 2017; Doshi-Velez & Konidaris, 2016), features(Barreto et al., 2017; Gupta et al., 2017), experience samples (Lazaric et al., 2008), value functions (Liu et al.,2021; Zhang & Zavlanos, 2020; Tirinzoni et al., 2018), and reward functions (Konidaris & Barto, 2006). Inimitation learning, additional studies have made progress via domain adaptation (Kim et al., 2020; Yu et al.,2018), querying unlabeled datasets (Du et al., 2023), abstracting and transferring concepts (Lzaro-Gredillaet al., 2019; Shao et al., 2021), or conditioning on other information such as language instructions (Stepputtiset al., 2020; Lynch & Sermanet, 2020; Jang et al., 2022) and goal images (Pathak et al., 2018). Our workfocuses on neural network policy sub-module reuse through direct stitching to achieve zero-shot transfer. Sim-to-real transfer is another important topic within transfer learning (Sadeghi et al., 2018; James et al.,2019; Zhang et al., 2019; Tobin et al., 2018; Mehta et al., 2020; James et al., 2017; Nguyen et al., 2018; Rusu",
  "Published in Transactions on Machine Learning Research (11/2024)": "Alessandro Lazaric, Marcello Restelli, and Andrea Bonarini. Transfer of samples in batch reinforcementlearning. In Proceedings of the 25th international conference on Machine learning, pp. 544551, 2008. Miguel Lzaro-Gredilla, Dianhuan Lin, J Swaroop Guntupalli, and Dileep George. Beyond imitation: Zero-shot task transfer on robots by learning concepts as cognitive programs. Science Robotics, 4(26):eaav3150,2019. Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, andLawrence D Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541551, 1989.",
  "Perception Stitching: Learning Reusable Perception Network Module": "Perception Stitching (PeS) () is a compositional vision-based robot learning framework that allowszero-shot transfer of perceptual knowledge between different camera configurations. It is designed to becompatible with conventional behavior cloning algorithms (Bain & Sammut, 1995; Ross et al., 2011; Torabiet al., 2018; Chi et al., 2023), various visual encoder structures such as CNN (LeCun et al., 1989) and ResNet",
  "Modular Visuomotor Policy Design": "Consider an environment E with observation oE. We denote the observations from two camera views as oE1and oE2 , and the robot proprioception of the end effector position, orientation, and gripper open width as oEp .For an MLP-based policy, we denote it as E(aE | oE) parameterized by a function E(oE). For an RNN-based policy, we denote it as E(aE, hEt+1, cEt+1 | oE, hEt , cEt ) parameterized by a function E(oE, hEt , cEt ),where hEt is the hidden state of the RNN network at time step t and cEt is the cell state. The visuomotorpolicy can be decomposed into the visual encoder gE1 for camera 1, the visual encoder gE2 for camera 2, andthe action decoder f E. We can represent the MLP-based policy as Eq. 1 and the RNN-based policy as Eq. 2.",
  "E(oE, hEt , cEt ) = E(oE1 , oE2 , oEp , hEt , cEt ) = f E(gE1 (oE1 ), gE2 (oE2 ), oEp , hEt , cEt ),(2)": "Without loss of generality, we demonstrate the perception stitching process with the MLP-based policy. Withtwo visuomotor policies f E1(gE11 (oE11 ), gE12 (oE12 ), oE1p ) and f E2(gE21 (oE21 ), gE22 (oE22 ), oE2p ) in two environmentsE1 and E2 with different cameras, we define perception stitching as constructing another visuomotor policynetwork f E1(gE11 (oE31 ), gE22 (oE32 ), oE3p ) by initializing the visual encoder 1 with parameters from gE11 , visualencoder 2 with parameters from gE22 , and action decoder with parameters from f E1. Then this stitchedpolicy is zero-shot transferred to the new environment E3 with the same perception configuration 1 as thatof E1 and perception configuration 2 as that of E2. For example, as shown in (left half), we trainpolicy E1 in E1 with an in-hand camera and a side-view camera, and policy E2 in E2 with a side-viewcamera and a front view camera. Now, if we need a policy for E3 with an in-hand camera and a front-viewcamera, we stitch the visual encoder gE22to the gE11and f E1 to form a stitched policy that directly works inE3. Note that though we formalize PeS set up with dual-camera settings, PeS is not constrained with onlytwo cameras, as demonstrated in our experiments in the .2. In addition, although we choose theaction decoder 1 for the experiments in section 4, the choice of which action decoder to be used doesnt affectthe performance of PeS, and the experiment results of comparing the influence of the two action decodersare presented in Appendix D.",
  "Latent Space Alignment for Transferable Representation": "Relative Representation Simply stitching one portion of a neural network to another neural networkusually cannot yield optimal performance in the target environment due to the misalignment of latent space(Jian et al., 2023; Devin et al., 2017). Previous works have observed an approximate isometric transformationrelationship between the latent representations trained with different random seeds (Moschella et al., 2022)and different robot kinematics (Jian et al., 2023). These isometric transformations include rotation, reflecting,rescaling, and translation. In this work, we observe a similar phenomenon in the latent spaces of the visualencoders. Hence, we calculate a relative representation at the latent space (Jian et al., 2023; Moschella et al.,2022) to align the latent features from different policy modules. As shown in , we first collect a set A of anchor images a(j) from the dataset D for the behavior cloning:A = {a(j)} D. By applying the visual encoder g to both the input image s(i) (s(i) S) and the anchorimage a(j), we obtain their embedded forms es(i) = g(s(i)) and ea(j) = g(a(j)). We want to project the embedded input images to a coordinate system consisting of the embedded anchorimages, and if this coordinate system is invariant to isometric transformations, we can alleviate the latentspace misalignment issue. Therefore, we calculate a similarity score r = sim (es(i), ea(j)) between an embed-ded task state and an embedded anchor state where sim : Rd Rd R. Then the relative representation(Jian et al., 2023; Moschella et al., 2022) of the input image s(i) with respect to the anchor set A is givenby:rs = (sim(es(i), ea(1)), sim(es(i), ea(2)), . . . , sim(es(i), ea(|A|)))(3)",
  "l=1,l=k |cov (zk, zl)| ,(5)": "where Z is the dimensionality of the latent space. Overall, this disentanglement loss calculates the covarianceof a latent representation feature zk with all other features zl (l = k), sums up these absolute values,normalizes it with Z 1, and calculates the mean over all the features zk (k = 1, 2, ..., Z). By encouragingdifferent features at the latent space to be independent with each other, we encourage them to capturedifferent underlying factors hidden in the observation (e.g. object color, position etc.). The disentangledrepresentation has been used in many applications in supervised learning (Tran et al., 2017; Kim & Mnih,2018; Chen et al., 2018; Higgins et al., 2017; Quessard et al., 2020; Higgins et al., 2018; Zbontar et al., 2021;Ermolov et al., 2021; Bardes et al., 2021), and we empirically find it significantly improves the performanceof PeS in difficult tasks.",
  "i=1 zi2,(8)": "where the weights 1 = 0.001, 2 = 0.001, zi is the ith latent representation, and N is the batch size. TheseL1 and L2 regularizations have been used in previous work (Nair et al., 2022) to avoid the state-distributionshift failure in imitation learning (Ross et al., 2011) by limiting the latent space dimension. We find itimproves the zero-shot transfer performance in some cases but generally doesnt perform as well as thedisentanglement loss.",
  "Implementation Details": "Each input image is a (84, 84, 3) RGB image.The visual encoder consists of a ResNet-18 network (Heet al., 2016), followed by a spatial-softmax layer (Finn et al., 2016; Mandlekar et al., 2021), and then a256-dimensional last layer at the module interface. We use 256 anchors to match the dimension of the latentrepresentation. The eight-dimensional proprioception consists of the end effector position (3D), end effectorquaternion (4D), and the gripper open width (1D). It is embedded by a 64-dimensional linear layer and thenconcatenated with two latent representations of the two images. We use the RNN-based action decoder forthe Can and Door Open task and the MLP-based action decoder for other tasks. We list all the parametersof the neural network in the Appendix A with our code base. The actions are output by a Gaussian MixtureModel in the last layer of the policy network where the eight-dimensional action vector is the delta valueof the 8D proprioception. Before the perception stitching and zero-shot transfer, all policies are trained to100% success rates with BC (Pomerleau, 1988). The dataset of each task contains 200 trajectories of theexpert demonstrations. The pseudo code of PeS is presented in Appendix B B.",
  "Experiments": "Our experiments aim to (1) evaluate the effectiveness of PeS on enhancing zero-shot policy transfer onthe stitched policies and (2) understand the mechanisms behind the performance advantage of PeS. Ourexperiments consist of five parts. First, we evaluate the zero-shot transfer performance of PeS for train-ing double-camera-view visuomotor policies in five simulated manipulation tasks with seven variations ofcamera configurations. Next, we apply PeS to single-camera-view and triple-camera-view policies to testits generalizability to arbitrary camera number settings. We then assess the performance of PeS in fourreal-world manipulation tasks with three different camera configurations. In addition, we analyze the latentrepresentations of the visual encoders both quantitatively and qualitatively to understand the effectiveness ofPeS in latent space alignment. Lastly, we adopt Gradient-weighted Class Activation Mapping (Grad-CAM)(Selvaraju et al., 2017) to visualize the regions that the visuomotor policies focus on, offering an intuitiveinsight into the success of PeS.",
  "PeS92.72.5094.71.8989.34.1196.01.6388.70.9493.00.0384.76.6091.3": ": Zero-Shot Transfer Success Rates in basic Simulation tasks. We test all methods across six different visualconfigurations and also calculate the average performance. In the two basic tasks, Push and Lift, PeS and its two ablationmethods can all get about a 90% success rate on average. The Cannistraci et al. 2024 (linear) baseline has 70% - 80% of successrate, while the Cannistraci et al. 2024 (non-linear) and Devin et al. 2017 baselines perform poorly with about 20% - 50% ofsuccess rate.",
  "PeS58.74.1168.70.9470.70.9452.73.4064.74.9948.73.4056.75.7360.1": ": Zero-Shot Transfer Success Rates in difficult Simulation tasks. The three difficult tasks include Can, Stack,and Door Open. We test all methods across six different visual configurations and also calculate the average performance. Thethree baselines achieve about 0% - 40% of success rates in these tasks. In the Can and Stack tasks, PeS achieves 75% - 95% ofsuccess rates, while the ablation methods achieves about 30% - 65% of success rates. In the Door task, PeS achieves 60.7% ofsuccess rates, while the two ablation methods only have 20% - 30% of success rates. but are optimized for the accuracy of behavior cloning, leading to unaligned latent space and hurting theperformance. Devin et al. 2017 baseline achieves very low success rates between 18% to 33%, indicatingthat the unaligned latent representations cause the failure of the stitched policy for zero-shot transfer. Tab. 2 reports the performance in more difficult tasks: Can, Stack, and Door Open. PeS achieves 60% to93% of success rates, which has about 30% to 60% of advantage to the base policy 1 and 2. The success ratesof all four baselines are lower than 43%. This result suggests that previous methods cannot solve the zero-shot transfer problem for difficult tasks with satisfactory performance, which usually involve complicatedvisual background (e.g., Can), long-horizon multi-stage motions (e.g., Stack), and articulated objects (e.g.,Door Open).Among these baselines, we find that RT-1 sometimes can achieve very good performancewhen the visual variation is moderate. For example, in the Can-Mask task and the Door Open-Noise task,",
  "Simulation Experiments with Single Camera View and Triple Camera Views": ": Perception Stitching with Single Cam-era. The original two policies are trained with only onecamera. The other visual encoder takes in a black image.The corresponding anchors are selected with our proposedmethod 3. The visual encoder of the black image uses thesame anchor images as the other encoder of this policy. While most visuomotor policies adopt two cameras intheir applications, we also conduct experiments for poli-cies with only one camera and policies with three camerasto verify the generalizability of PeS to arbitrary cameranumber settings. Single Camera View We train two policies with twodifferent cameras separately.Each policy still has twovisual encoders, but one visual encoder takes in a blackimage and the other encoder takes in the images from thesingle camera. For both encoders of the policy, they usethe same anchor images set collected by the only camera,because we assume that each policy is trained separatelyand only has access to the dataset of one camera. In the perception stitching process, we stitch the visualencoder of policy 2 to the visual encoder and action de-coder of policy 1, as shown in , and assemblea stitched policy for double camera inputs. We suggestthat this is the most reasonable setting for the perceptionstitching of single-camera policies. In contrast, if we re-move the black image encoder and stitch the only visionencoder of the policy 1 to the action decoder of the policy2, applying this reassembled policy in either environment1 or 2 wont lead to better performance compared withthe original policy trained to a maximum success rate inthat environment. As shown in table 3, PeS outperforms the other methodsin the three difficult tasks and achieves close-to-optimalperformance in the two basic tasks. It has the highestaverage success rate over the five tasks. Ideally, the decoder should learn to completely ignore the latent representations from the encoder with emptyinput if we train the policy with a large amount of expert data and many epochs. In practice, however,the policy is trained with only 200 expert demonstrations of data and limited epochs. The policy will learnto assign more importance to the encoder with RGB image input and less importance to the encoder withempty input, but it will not fully ignore that encoder with empty input. Therefore, stitching another encoderto replace the encoder with empty input will have less influence on the policy performance compared with thedouble camera experiments, but the disturbance caused by this new encoder will still exist due to the latent",
  "PushLiftCanStackDoor OpenAverage": "Devin et al. 201773.311.4772.70.9466.76.1814.03.2722.72.5049.8Cannistraci et al. 2024 (linear)89.34.1186.01.6372.83.2714.76.1839.33.4060.4Cannistraci et al. 2024 (non-linear)60.71.8880.73.4024.02.830.70.9441.34.9941.5PeS (-w/o disent. loss)88.74.1194.01.6391.03.4026.04.3232.73.7766.5PeS (w. l1 & l2 loss)94.72.4990.02.8386.71.8958.71.8840.70.9474.2PeS91.32.4992.60.9494.60.9480.04.9072.73.7786.2 : Zero-Shot Transfer Success Rates of single camera policies. PeS achieves optimal performance in the threedifficult tasks (Can, Stack, Door Open), and close-to optimal performance in the two basic tasks (Push, Lift). Its averagesuccess rate outperforms all the baselines and the ablation methods.",
  "MaskZoom inBlurredNoiseFisheyeCamera PositionAverage": "100% success rate94.70.9496.70.9490.01.6396.71.8997.32.4980.04.9092.654% success rate98.70.9495.33.7746.01.6385.30.9452.72.4984.03.2777.06% success rate49.36.8048.74.1176.78.0698.70.9444.73.4098.01.6369.4 : Anchors from Failure Trajectories. All the experiments are carried out with the Stack task. We chose anchordatasets from three different datasets with K-means algorithm: (1) The dataset is collected by an expert agent, and it contains200 success trajectories. (2) The dataset is collected by an semi-trained policy which has around 50% of success rate. It isused to collect 200 trajectories, among which 54% are successful trajectories and 46% are failure trajectories. (3) The datasetis collected by a poorly behaved policy with a close to zero success rate. It is used to collect 200 trajectories, among whichonly 6% are successful trajectories and 94% are failure trajectories. The average success rate decreases when the percentage ofsuccessful trajectories in the dataset for anchor selection decreases.",
  "PeS100.085.080.045.0Devin et al. 20170.00.00.00.0": ": Zero-Shot Transfer Success Rates in Real World. We report the successrates of three manipulation tasks in real world. Each success rate is calculated with 20games. PeS achieves a 100% success rate in the easiest Reach task, 85% in the Push task,80% in the Lift task, and 45% in the hardest Stack task. In comparison, the baselinemethod doesnt have any success case in all these tasks. Results As shown in Tab. 5,PeS achieves 100% of successrate in Reach, 85% in Push,80% in Lift, and 45% in Stack,while Devin et al. 2017 base-line gets 0% of success ratesin all the experiments. In theexperiments, we observe thatPeS policies show more accu-rate motions compared with the baseline policies. In comparison, Devin et al. 2017 baseline policies cannotoutput actions accurately enough to accomplish the tasks, although they have some attempts to completethe tasks in some cases. Take the Lift task as an example, the robot trained with the baseline method reachessome positions close to the cube but always fails to grasp it. In the most difficult Stack task, PeS robot canstill achieve many success cases, while the baseline robot cannot output meaningful motions. Please refer tothe supplementary material for the experiment videos. In summary, we find that the performance advantage of PeS is more pronounced in the real world than inthe simulation. We assume that this is because there are more noises and disturbances in the real world. Tothe best of our knowledge, PeS is the first method that enables vision-based zero-shot transfer in the realworld via reassembling neural network components.",
  "To understand the mechanism behind the high success rate of PeS, we perform visualization and quantitativeanalysis of the latent representations of the visual encoders": "We choose the Push-Camera Position experiment in the simulation and visualize the corresponding latentrepresentations to have an intuitive understanding of the mechanism of PeS. We first reduced the 256Drepresentations to 3D with PCA(Hotelling, 1933) for visualization.Since the side view encoder of thepolicy 2 is stitched to the policy 1 at the position of its original front view encoder, we compare the latentrepresentations of these two encoders. As shown in (a), the latent representations trained with PeS have",
  "(b) Side View": ": Attention Heatmap. (a) In the in-hand view, our policy paysattention to the cube on the table, while the baseline policy pays meaninglessattention to the upper region of the image. (b) In the front view image, ourpolicy pays attention to the robot end effector, while the baseline methodhas slightly more attention to the two sides of the image. We visualize the attention map fromthe policy modules to further explainwhy certain policies work well whileothers fail.To this end, we modifythe Gradient-weighted Class ActivationMapping (Grad-CAM) (Selvaraju et al.,2017) approach to highlight the regionsthat the policies pay attention to. Grad-CAM is widely applied to neural classi-fication models with convolutional lay-ers. To adapt the Grad-CAM from im-age classification to robot learning, wereplace the before-softmax score yc forclass c of the image classification net-works with the log-likelihood l(a) of the robot action a in the training dataset. We denote the kth featuremap activation output from the last convolutional layer as Ak. Then the backpropagated gradient of l(a)with respect to Ak is computed as l(a)",
  ".(10)": "We apply ReLU because we are only interested in the features that have a positive influence on the actions.The intensity of these pixels should be increased in order to increase the log-likelihood l(a) (Selvaraju et al.,2017). This LaGrad-CAM is a heatmap of the same size as the convolutional feature maps Ak. We upsampleit to the input image size with bilinear interpolation to get the final attention heatmap of the input image(Selvaraju et al., 2017). A larger value on this heatmap means this pixel contributes to a larger gradient ofthe log-likelihood of the robot action. We choose the Lift-Camera Position experiment and visualize the attention heatmap of the stitched policieswith PeS and the baseline separately. As shown in (a), from the in-hand camera, our policy is focusingon the cube between the two gripper fingers, which is intuitively what humans pay attention to in a Lifttask. In comparison, the Devin et al. 2017 baseline policy has more attention to the upper middle part ofthe image, which is the desk surface, and is not informative for this task. In (b), our policy is payingattention to the robot end effector from the side-view camera, while the baseline policy has more attentionto the two sides of the image, while these regions have no crucial object. To sum up, the attention mapsfrom the Grad-CAM suggest that the stitched policy with PeS performs better than the baseline because itcan pay attention to the crucial regions for accomplishing the task, while the baseline policy cannot. Thisresult provides an intuitive explanation of the good performance of PeS. Videos of the attention heatmapsduring the manipulation process can be found in the supplementary material.",
  "Discussion": "Conclusion We present Perception Stitching (PeS), a method for zero-shot visuomotor policies transfer vialatent spaces alignment. PeS aligns the latent spaces of different visual encoders by enforcing the relativerepresentations invariant to isometric transformations and, therefore, allows the trained visual encoders tobe reused in a plug-and-go manner. Our evaluation covers 35 simulation experiments and 4 real-world exper-iments with a variety of manipulation tasks and camera configurations. The results demonstrate significantperformance improvement with PeS for zero-shot transfer, and its advantage is especially pronounced in real-world tasks. Moreover, we conduct quantitative and qualitative analyses to understand the mechanism of thesuperior performance of PeS. We hope that this work can inspire further exploration of the compositionalityand modularity in robot learning. Limitations and Future Work There is no additional training required during transfer, but to obtainthe anchor states our framework still requires the robot to replay the trajectories in the previous datasetto collect a new dataset and then select anchors with the corresponding indices. Although this is feasible,as we have demonstrated in our real-world experiments, the trajectories replaying process in the real worldusually takes longer time than collecting a new dataset by random sampling. Future work can develop analternative algorithm for more efficient anchor selection. We also acknowledge that PeS cannot yet achieveperfect performance on some tasks that require very high precision or have very long horizons. One majorcause of the potential failure is that the PeS can only enforce an approximate invariance of the latentrepresentations but not a strict invariance, which leads to the loss of some accuracy of the stitched policy.It is an interesting topic for future work to further refine the latent space alignment. In addition, the policy trained with the Behavior Cloning algorithm usually cannot recover from the erroraccumulation very well in long-horizon tasks. Although our paper focuses on imitation learning, one excitingfuture direction is to explore our techniques in a reinforcement learning setup (Ricciardi et al., 2024) orexplore more advanced imitation learning that handles error accumulation, since these approaches haveshown some capabilities of recovery from failures in recent works. Another promising direction is to explorethe combination of PeS with other robot transfer learning techniques that focus beyond vision encodertransfer, such as embodiment transfer and scale to large robot learning datasets.",
  "Adrien Bardes, Jean Ponce, and Yann LeCun.Vicreg: Variance-invariance-covariance regularization forself-supervised learning. arXiv preprint arXiv:2105.04906, 2021": "Andr Barreto, Will Dabney, Rmi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and DavidSilver. Successor features for transfer in reinforcement learning. Advances in neural information processingsystems, 30, 2017. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, KeerthanaGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, TianliDing, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transferweb knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca, and Emanuele Rodol. From bricks tobridges: Product of invariances to enhance latent space communication. arXiv preprint arXiv:2310.01211,2023.",
  "Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglementin variational autoencoders. Advances in neural information processing systems, 31, 2018": "Yutian Chen, Abram L Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew Hoffman, andNando de Freitas. Modular meta-learning with shrinkage. Advances in Neural Information ProcessingSystems, 33:28582869, 2020. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023.",
  "Coline Devin. Compositionality and Modularity for Robot Learning. PhD thesis, EECS Department, Uni-versity of California, Berkeley, Dec 2020. URL": "Coline Devin, Abhishek Gupta, Trevor Darrell, Pieter Abbeel, and Sergey Levine. Learning modular neuralnetwork policies for multi-task and multi-robot transfer. In 2017 IEEE international conference on roboticsand automation (ICRA), pp. 21692176. IEEE, 2017. Finale Doshi-Velez and George Konidaris. Hidden parameter markov decision processes: A semiparametricregression approach for discovering latent task parametrizations. In IJCAI: proceedings of the conference,volume 2016, pp. 1432. NIH Public Access, 2016.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016": "Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, ShakirMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variationalframework. ICLR (Poster), 3, 2017. Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and AlexanderLerchner. Towards a definition of disentangled representations. arXiv preprint arXiv:1812.02230, 2018. Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly, AndrewSenior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al.Deep neural networks for acous-tic modeling in speech recognition: The shared views of four research groups. IEEE Signal processingmagazine, 29(6):8297, 2012.",
  "Marcel Hussing, Jorge A Mendez, Anisha Singrodia, Cassandra Kent, and Eric Eaton. Robotic manipulationdatasets for offline compositional reinforcement learning. arXiv preprint arXiv:2307.07091, 2023": "Divye Jain, Andrew Li, Shivam Singhal, Aravind Rajeswaran, Vikash Kumar, and Emanuel Todorov. Learn-ing deep visuomotor policies for dexterous hand manipulation. In 2019 international conference on roboticsand automation (ICRA), pp. 36363643. IEEE, 2019. Stephen James, Andrew J Davison, and Edward Johns. Transferring end-to-end visuomotor control fromsimulation to real world for a multi-stage task. In Conference on Robot Learning, pp. 334343. PMLR,2017. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, SergeyLevine, Raia Hadsell, and Konstantinos Bousmalis.Sim-to-real via sim-to-sim: Data-efficient roboticgrasping via randomized-to-canonical adaptation networks. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1262712637, 2019. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, andChelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on RobotLearning, pp. 9911002. PMLR, 2022.",
  "Pingcheng Jian, Easop Lee, Zachary Bell, Michael M Zavlanos, and Boyuan Chen. Policy stitching: Learningtransferable robot policies. In Conference on Robot Learning, pp. 37893808. PMLR, 2023": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learningfor vision-based robotic manipulation. arXiv preprint arXiv:1806.10293, 2018. Taylor W Killian, Samuel Daulton, George Konidaris, and Finale Doshi-Velez. Robust and efficient transferlearning with hidden parameter markov decision processes. Advances in neural information processingsystems, 30, 2017. Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Transformer-based deep imitation learning fordual-arm robot manipulation. In 2021 IEEE/RSJ International Conference on Intelligent Robots andSystems (IROS), pp. 89658972. IEEE, 2021. Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo, and Yasuo Kuniyoshi. Training robots without robots:deep imitation learning for master-to-robot policy transfer. IEEE Robotics and Automation Letters, 8(5):29062913, 2023.",
  "Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. R3m: A universalvisual representation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022": "Phuong DH Nguyen, Tobias Fischer, Hyung Jin Chang, Ugo Pattacini, Giorgio Metta, and Yiannis Demiris.Transferring visuomotor learning from simulation to the real world for robotics manipulation tasks. In2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 66676674.IEEE, 2018. Abhishek Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khaz-atsky, Anant Rai, Anikait Singh, Anthony Brohan, et al. Open x-embodiment: Robotic learning datasetsand rt-x models. arXiv preprint arXiv:2310.08864, 2023. Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shel-hamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In Proceedings ofthe IEEE conference on computer vision and pattern recognition workshops, pp. 20502053, 2018.",
  "Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning fromimages with latent space models. In Learning for Dynamics and Control, pp. 11541168. PMLR, 2021": "Kartik Ramachandruni, Madhu Babu, Anima Majumder, Samrat Dutta, and Swagat Kumar. Attentivetask-net: Self supervised task-attention network for imitation learning using video demonstration. In 2020IEEE International Conference on Robotics and Automation (ICRA), pp. 47604766. IEEE, 2020. Antonio Pio Ricciardi, Valentino Maiorca, Luca Moschella, Riccardo Marin, and Emanuele Rodol. Zero-shot stitching in reinforcement learning using relative representations. arXiv preprint arXiv:2404.12917,2024. Stphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured pre-diction to no-regret online learning. In Proceedings of the fourteenth international conference on artificialintelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, KorayKavukcuoglu, Razvan Pascanu, and Raia Hadsell.Progressive neural networks.arXiv preprintarXiv:1606.04671, 2016. Andrei A Rusu, Matej Veerk, Thomas Rothrl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. In Conference on robot learning, pp. 262270.PMLR, 2017. Fereshteh Sadeghi, Alexander Toshev, Eric Jang, and Sergey Levine. Sim2real viewpoint invariant visualservoing by recurrent control. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 46914699, 2018.",
  "Stefan Schaal. Learning from demonstration. Advances in neural information processing systems, 9, 1996": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. InProceedings of the IEEE international conference on computer vision, pp. 618626, 2017. Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, andGoogle Brain. Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE internationalconference on robotics and automation (ICRA), pp. 11341141. IEEE, 2018. Lin Shao, Toki Migimatsu, Qiang Zhang, Karen Yang, and Jeannette Bohg. Concept2robot: Learning ma-nipulation concepts from instructions and human demonstrations. The International Journal of RoboticsResearch, 40(12-14):14191434, 2021. Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning networks:Learning generalizable representations for visuomotor control. In International Conference on MachineLearning, pp. 47324741. PMLR, 2018. Simon Stepputtis, Joseph Campbell, Mariano Phielipp, Stefan Lee, Chitta Baral, and Heni Ben Amor.Language-conditioned imitation learning for robot manipulation tasks. Advances in Neural InformationProcessing Systems, 33:1313913150, 2020.",
  "Youssef Zaky, Gaurav Paruthi, Bryan Tripp, and James Bergstra. Active perception and representation forrobotic manipulation. arXiv preprint arXiv:2003.06734, 2020": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learningvia redundancy reduction. In International conference on machine learning, pp. 1231012320. PMLR, 2021. Jingwei Zhang, Lei Tai, Peng Yun, Yufeng Xiong, Ming Liu, Joschka Boedecker, and Wolfram Burgard.Vr-goggles for robots: Real-to-sim domain adaptation for visual control. IEEE Robotics and AutomationLetters, 4(2):11481155, 2019. Yan Zhang and Michael M Zavlanos. Transfer reinforcement learning under unobserved contextual informa-tion. In 2020 ACM/IEEE 11th International Conference on Cyber-Physical Systems (ICCPS), pp. 7586.IEEE, 2020.",
  ": MLP-based policy Hyperparameters": "For the baseline methods, we demonstrate the detailed structures of the MLP-base policy network in (a), and the RNN-based policy network in (b). The hyperparameters of the MLP-base networkand the RNN-based network are listed in TABLE 6 and TABLE 7 separately. The policy network of the Perception Stitching method shares all the network structures and hyperparametersas that of the baseline method. The only difference is that it calculates the relative representations of theimages after the 256-unit linear layer, as shown in and equation 3.",
  ": RNN-based policy Hyperparameters": "Then we replay the expert trajectories in D1 to collect another expert dataset D2 in the environment E2.We collect the anchor set A1 in the dataset D1 via the K-means algorithm Hartigan & Wong (1979). Thenwe collect the anchor set A2 which has the same indices in D2 as A1 in D1. For the environment E3 with twovisual configurations oE11and oE22 , we assemble an anchor set A3 = {a(j)3 } consists of images of oE11from A1and images of oE22from A2. In the next step, we train the two polices f E1(gE11 (oE11 ), gE12 (oE12 ), oE1l , A1) andf E2(gE21 (oE21 ), gE22 (oE22 ), oE2l , A2) in E1 and E2 with the Behavior Cloning algorithm Schaal (1996) separately.Then we initialize a stitched policy f E1(gE11 (oE11 ), gE22 (oE22 ), oE1l , A3). This stitched policy is tested in thetask T in E3. Its performance is measured by its success rate.",
  "CQuantitative Analysis of the Module Interface": "The cosine and L2 pairwise distance shown in measures the similarity between two latent represen-tations. For a group of states SE,T = {siE,T } in the environment E of task T, they are observed from twocameras and get two groups of observed images O1,E,T = {oi1,E,T } and O2,E,T = {oi2,E,T }. We obtain theaverage pairwise distance between the latent representations of two visual encoders by calculating the meanof the pairwise distances across all input states:",
  "This section aims to answer the question: Is there any difference between choosing which action decoder(action decoder 1 v.s. action decoder 2)?": "We carry out the zero-shot transfer in the Stack task with six different visual configurations.For eachexperiment, we try action decoder 1 and action decoder 2, and report their success rates side-by-side in . We notice that the average success rates of decoder 1 and decoder 2 across the six visual configurationsare close to each other for all the methods, and the maximum difference is within 15%. We believe that thisdifference is generated by the randomness of the testing process, but not the systematic advantage of one",
  "Decoder 283.34.1186.04.3294.02.8392.70.9495.30.9468.76.1885.0": ": Action Decoder 1 V.S. Action Decoder 2. All the experiments are carried out with the Stack task. For the PeSmethod and other baseline and ablation methods, the difference in the average success rates of using action decoder 1 or actiondecoder 2 is within 15%. The choice of the action decoder does not have distinct influence on the zero-shot transfer successrates.",
  "decoder to another decoder. This result suggests that the choice of the action decoder during the zero-shottransfer process does not make a distinct difference for all the methods on average": "If we look at the success rates of each visual configuration, we can see that the success rates difference of thePeS method is within 11%. The choice of decoder does not affect the reassembled policys performance withPeS, and both decoders can lead to very satisfying success rates over 85%. However, in some experimentswith the baselines and the ablation methods (e.g. PeS (w. l1 & l2 loss)-Fisheye), we can see a huge successrate difference. It indicates that the choice of different decoders has a random influence on the performanceof the baselines and ablation methods for different visual configurations, but it doesnt affect the averagesuccess rates drastically.",
  "This section aims to answer the question: Does the data collection process require some success trajectory?Or even failure/exploratory trajectories are still useful?": "We use the Stack task to carry out the experiments. We collect a dataset with 200 successful trajectories andcollect an anchor set 1 from this fully successful dataset. Then we train a policy to about 50% of trainingsuccess rates and execute it to collect 200 trajectories during testing. 54% of these trajectories are successfuland the rest 46% are failed. We collect an anchor set 2 from this semi-successful dataset. We also train apolicy for only 1 minute so that it has a very low success rate. We use this poorly trained policy to collecta dataset of 200 trajectories with only 6% of them being successful. We collect an anchor set 3 from thisdataset in which most trajectories fail. All the anchors are collected with the k-means algorithm as shownin . We use these three different anchor sets for training the policies across the six visual configurations. The dataset used for training is still the same dataset collected by an expert agent with 200 successful trajectories,and the only difference is that the anchors are selected from data sets with different success rates. reports the success rates. On average, the success rate decreases when the proportion of successful trajectoriesdecreases for the anchor selection. In addition, we notice that the performance of the trained policy becomesunstable when the failure trajectory number during anchor selection increases. When there are no failuretrajectories, the policy success rates are stably above 80%. In contrast, with the failure trajectories numberincreases, although there are still some cases where the trained policy can get over 90% of success rates,there appear more cases where the policy gets around 40% to 50% of success rates. These low-performanceexperiments make the average success rate decrease. Since we need to use an expert dataset for training the policies, and it is a small data set with only 200trajectories that are easy to collect, we encourage the users of the PeS method to directly collect the anchorset from the training data set with K-means algorithm.Our experiment result shows that this anchorselection method can lead to more stable zero-shot transfer performance and a higher average success rate.",
  "FLatent Representations Visualization": "We visualize the latent representations of the Lift, Can, Stack and Door tasks.Among all the visualconfiguration changing experiments, we choose the camera position variation experiments. We first reducedthe 256D representations to 3D with PCA (Hotelling, 1933) for visualization. Since the side view encoderof the policy 2 is stitched to the policy 1 at the position of its original front view encoder, we compare thelatent representations of these two encoders. In these visualizations, the red data points are the first 5 steps of images in each of the 200 games in thedataset of a certain task. The blue data points are the last 5 steps of images, and the green data points arethe 5 steps of images in the middle of each trajectory. Therefore, we have 1000 data points for each colorwhich represent the starting, middle, and ending stages of a task. The visualization results show that PeS can better align the latent space and force an approximate invarianceof the latent representations. In contrast, the Devin et al. (2017) baseline without adopting the relativerepresentation and the disentanglement regularization leads to different latent representations between thetwo encoders, and they have an approximately isometric transformation relationship. These visualizationssupport our conclusions in the paper.",
  "GImpact of Anchor Number": "To study the impact of number of anchors on transfer performance, we picked a challenging task, Stack, andreported the success rates with standard errors over 3 random seeds on all the different visual configurationsin . We found that when the anchor number is too small, the latent space of the visual encoderdoes not have enough capacity to capture effective visual representations while maintaining an approximateinvariance at the same time. Therefore, the zero-shot transfer performance drops drastically. On the otherhand, when the number of anchors becomes too large, there are more redundant anchors which are similarto each other. This will make disentangling the features at the latent space with the disentanglement loss",
  ": Success rates of the Stack task with different anchor numbers": "We also investigated the impact of the anchor number on computational efficiency. We picked the Stack taskand trained the policies on a NVIDIA A6000 GPU with the batch size of 32. As shown in , a largeranchor number will lead to a larger model size, longer anchor searching time, and longer training time foreach epoch.",
  ": Success rates of the Stack task with different disentanglement loss weight": "We have also studied the effect of different weights of disentanglement loss. We picked the Stack task andtested different weights on all the various visual configurations. When the weight is close to zero, the transferperformance gets close to that of the PeS (w/o disent. loss) ablation method and is significantly lower thanthe PeS full method.When the weight becomes too large, the optimization process leans too much todisentangling the latent features than imitating the expert behaviors. Therefore, the weaker imitation ofthe expert agent will also cause the performance drop. We empirically found that the optimal weight ofthe disentanglement loss in our tasks should be around the range of 0.002 to 0.02. The success rates withstandard errors over 3 random seeds are shown in .",
  "IImpact of Replay Trajectory Deviations": "One limitation of PeS is that it requires a trajectory replay to collect the anchor images. However, in real-world applications, it is usually hard to accurately replay the exact trajectories. In order to understandhow the trajectory deviation errors in replay could influence the performance of PeS, we conduct additionalexperiments to introduce trajectory deviations with different amplitudes. We use the Stack task in the simulation to test the effect of trajectory deviations. During the trajectoryreplaying, we add a random horizontal vector to every position point on the trajectory except for the gripperclose or open action point. The length of the random vector to cause deviation is sampled from a uniformdistribution within a certain range, and we test out the range options of 0 to1 cm, 0 to 3 cm, and 0 to 5 cm.The direction of the deviation vector is randomly sampled within the x-y plane.",
  ": Success rates of the Stack task with different replay trajectory deviation amplitudes": "The experiment results are shown in the table below. When the trajectory deviation is within the range of1 cm, The average performance of PeS drops by about 10%, but it can still achieve 81.4% of average successrate, which is much higher than all the baselines that dont require trajectory replay and all the ablationmethods that require trajectory replay without deviation. When the trajectory deviation goes up to therange of 3cm, PeS can achieve an average success rate of 31.6%, which is on par with the best performingbaselines RT1 (29.5%) and Cannistraci et al. 2024 (linear) (34.6%). When the trajectory deviation goes upto a very large range of 5cm, the average success rate of PeS drops to 13.6%."
}