{
  "Abstract": "Many graph neural networks have been developed to learn graph representations in eitherEuclidean or hyperbolic space, with all nodes representations embedded in a single space.However, a graph can have hyperbolic and Euclidean geometries at different regions of thegraph. Thus, it is sub-optimal to indifferently embed an entire graph into a single space.In this paper, we explore and analyze two notions of local hyperbolicity, describing theunderlying local geometry: geometric (Gromov) and model-based, to determine the preferredspace of embedding for each node. The two hyperbolicities distributions are aligned usingthe Wasserstein metric such that the calculated geometric hyperbolicity guides the choiceof the learned model hyperbolicity. As such our model Joint Space Graph Neural Network(JSGNN) can leverage both Euclidean and hyperbolic spaces during learning by allowingnode-specific geometry space selection. We evaluate our model on both node classificationand link prediction tasks and observe promising performance compared to baseline models.",
  "Introduction": "Graph neural networks (GNNs) are neural networks that learn from graph-structured data. Many workssuch as Graph Convolutional Network (GCN) (Kipf & Welling, 2016), Graph Attention Network (GAT)(Velikovi et al., 2018), GraphSAGE (Hamilton et al., 2017) and their variants operate on the Euclideanspace and have been applied in many areas such as recommender systems (Ying et al., 2018; Chen et al.,2022a), chemistry (Gilmer et al., 2017) and financial systems (Sawhney et al., 2021). Despite their remarkableaccomplishments, their performances are still limited by the representation ability of Euclidean space. Theyare unable to achieve the best performance in situations when the data exhibit non-Euclidean characteristicssuch as scale-free, tree-like, or hierarchical structures (Yang et al., 2022). As such, hyperbolic spaces have gained traction in research as they have been proven to better embedtree-like, hierarchical structures compared to the Euclidean geometry (Bachmann et al., 2019; Cho et al.,2019). Intuitively, encoding non-Euclidean structures such as trees in the Euclidean space would result inmore considerable distortion since the number of nodes in a tree increases exponentially with the depth ofthe tree while the Euclidean space only grows polynomially (Zhu et al., 2020). In such cases, the hyperbolicgeometry serves as an alternative to learning those structures with comparably smaller distortion as thehyperbolic space has the exponential growth property (Yang et al., 2022). As such, hyperbolic versions of",
  "GNNs such as HGCN (Chami et al., 2019), HGNN (Liu et al., 2019), HGAT (Zhang et al., 2021a) and LGCN(Zhang et al., 2021b) have been proposed": "Nevertheless, real-world graphs are often complex. They are neither solely made up of Euclidean nor non-Euclidean structures alone but a mixture of geometrical structures. Consider a localized version of geometrichyperbolicity, a concept from geometry group theory measuring how tree-like the underlying space is for eachnode in the graph (refer to .1 for more details). We observe a mixture of local geometric hyperbolicityvalues in most of the benchmark datasets we employ for our experiments as seen in . This impliesthat the graphs contain a mixture of geometries and thus, it is not ideal to embed the graphs into a singlegeometry space, regardless of Euclidean or hyperbolic as it inevitably leads to undesired structural inductivebiases and distortions (Yang et al., 2022). Taking a graph containing both lattice-like and tree-like structures as an example, c and fshows that 15 of the blue-colored nodes in the tree structure are calculated to have 2-hop local geometrichyperbolicity value of zero, while 12 of the purple nodes have a value of one and the other 3 purple nodes(at the center of the lattice) have a value of two (the smaller the hyperbolicity value, the more hyperbolic).This localized metric can therefore serve as an indication during learning on which of the two spaces is moresuitable to embed the respective nodes.",
  "(f)": ": Example graphs. (a) Lattice-like graph. (b) A tree. (c) A combined graph containing both latticeand tree structure. (d-f) The histograms reflect the geometric hyperbolicity in the respective graphs. Here we address this mixture of geometry in a graph and propose Joint Space Graph Neural Network (JSGNN)that performs learning on a joint space consisting of both Euclidean and hyperbolic geometries. To achievethis, we first update all the node features in both Euclidean and hyperbolic spaces independently, givingrise to two sets of updated node features. Then, we employ exponential and logarithmic maps to bridge thetwo spaces and an attention mechanism is used as a form of model hyperbolicity, taking into account theunderlying structure around each node and the corresponding node features. The learned model hyperbolicityis guided by geometric hyperbolicity and is used to softly decide the most suitable embedding space foreach node and to reduce the two sets of updated features into only one set. Ideally, a node should be eitherhyperbolic or Euclidean and not both simultaneously, thus, we also introduce an additional loss term toachieve this non-uniform characteristic. Related works are discussed in Appendix E.",
  "Hyperbolic geometry": "A hyperbolic space is a non-Euclidean space with constant negative curvature. There are different butequivalent models to describe the same hyperbolic geometry. In this paper, we work with the Poincar ballmodel, in which all points are inside a ball. The hyperbolic space with constant negative curvature c isdenoted by (Dnc , gcx). It consists of the n-dimensional hyperbolic manifold Dnc = {x Rn : cx < 1} with theRiemannian metric gcx = (cx)2gE, where cx = 2/(1 cx2) and gE = In is the Euclidean metric. At each x Dnc , there is a tangent space TxDnc , which can be viewed as the first-order approximation of thehyperbolic manifold at x (Bachmann et al., 2019). The tangent space is then useful to perform Euclideanoperations that we are familiar with but are undefined in hyperbolic spaces. A hyperbolic space and thetangent space at a point are connected through the exponential map expcx : TxDnc Dnc and logarithmic maplogcx : Dnc TxDnc , specifically defined as follows:",
  "Published in Transactions on Machine Learning Research (12/2024)": "in different spaces and employs the -stereographic model in each of the spaces. The Cartesian productenables a combinatorial construction of the mixed curvature space, thus the representations are first learnedindependently in the respective spaces and then concatenated (Sun et al., 2022). In terms of implementation,this is similar to our framework but instead of concatenation, we introduce a space selection mechanismguided by hyperbolicity to fuse the representations. On the other hand, GIL proposes a feature interaction scheme to leverage different spaces and a probabilityassembling module to combine the classification probabilities for obtaining the final prediction. The featureinteraction scheme is where the node features in the respective two spaces are enhanced based on thedistance similarity of the two sets of spatial embeddings. The larger the distance between the different spatialembeddings, the more significant the portion of features from the other space is summed to itself as seen in. However, this may introduce noise to the respective spaces, which we explain further below. Our approach differs from GIL (Zhu et al., 2020) in some key aspects. Firstly, we leverage the distribution ofgeometric hyperbolicity to guide our model to learn to decide if each node better embedded in a Euclidean orhyperbolic space instead of performing feature interaction learning. This is done by aligning the distribution ofthe learned model hyperbolicity and geometric hyperbolicity using the Wasserstein distance. Our motivationis that if a node can be best embedded in one of the two spaces, encoding it in another space other thanthe optimal one would result in comparably larger distortions. Minimal information would be present inthe sub-optimal space to help enhance the representation in the better space. Hence, promoting featureinteraction could possibly introduce more noise to the corresponding spaces. The ideal situation is then tolearn normalized selection weights that are non-uniform for each node so that we select for each node asingle, comparably better spaces output embedding. To achieve this, we introduce an additional loss termthat promotes non-uniformity. Lastly, we do not require probability assembling since we only have one set ofoutput features at the end of the selection process.",
  "where Unif represents the uniform distribution": "In order for these invariants to be useful for graphs, we require them to be almost identical for graphs withsimilar structures. We shall see that this is indeed the case. Before stating the result, we need a few moreconcepts. Let G be the space of weighted, undirected simple graphs. Though for most experiments, the given graphs areunweighted. However, aggregation mechanisms such as attention essentially generate weights for the edges.Therefore, for both theoretical and practical reasons, it makes sense to expand the graph domain to includeweighted graphs. For each G = (V, E) G, it has a canonical path metric dG, and dG makes G into a metric space includingnon-vertex points on the edges. For > 0, there is the subspace G of G consisting of graphs whose edgeweights are greater than . On the other hand, there is a metric on the space G and G, called the Gromov-Hausdorff metric (Bridson &Haefliger (1999) p.72). To define it, we first introduce the Hausdorff distance. Let X and Y be two subsets ofa metric space (M, d). Then the Hausdorff distance dH(X, Y ) between X and Y is",
  "where d(x, Y ) = infyY d(x, y), d(X, y) = infxX d(x, y). The Hausdorff distance measures in the worst case,how far away a point in X is away from Y and vice versa": "In general, we want to also compare spaces that do not a priori belong to a common ambient space. Forthis, if X, Y are two compact metric spaces, then their Gromov-Hausdorff distance dGH(X, Y ) is definedas the infimum of all numbers dH(f(X), g(Y )) for all metric spaces M and all isometric embeddingsf : X M, g : Y M. Intuitively, the Gromov-Hausdorff distance measures how far X and Y are frombeing isometric. The following is proved in the Appendix.Proposition 1. Suppose G and its subspaces have the Gromov-Hausdorff metric. Then G, is Lipschitzcontinuous w.r.t. G G and G,1 is continuous w.r.t. G G for any > 0. Consider a graph G. We fix either G, or G,1 as a measure of hyperbolicity, and apply to each localneighborhood of G. To be more precise, it is studied (Chen et al., 2020b; Rong et al., 2020) that manypopular GNN models have a shallow structure. It is customary to have a 2-layer network possibly due tooversmoothing (Chen et al., 2020a; Chamberlain et al., 2021b; Zeng et al., 2021) and oversquashing (Toppinget al., 2022) phenomena. In such models, each node only aggregates information in a small neighborhood. Therefore, if we fix a small k and let Gv be the subgraph of the k-hop neighborhood of v V , then it is moreappropriate to study the hyperbolicity v, either Gv, or Gv,1, of Gv. For our experiments, the former isutilized. We call v the geometric hyperbolicity at node v. The collection V = {v : v V } allows us toobtain an empirical distribution G of geometric hyperbolicity on the sample space R0. For instance, we can build histograms to acquire the distributions as observed in . In Cora, we observethat a substantial number of nodes have small (local) hyperbolicity, despite its high global hyperbolicity value",
  "(b)": ": Comparison between JSGNN and GIL (Zhu et al., 2020) in leveraging Euclidean and hyperbolicspaces. Both models utilizes GAT in .2 and HGAT in .3 for message passing in Euclideanand hyperbolic space respectively. (a) Soft space selection mechanism of JSGNN where trainable selectionweights v,R, v,D are non-uniform, effectively selecting the better of the two spaces considered. (b) Featureinteraction mechanism of GIL where , R are trainable weights and dD, dR are the hyperbolic distance(cf. (8)) and Euclidean distance respectively. The node embeddings of both spaces in GIL are adjustedbased on distance, potentially introducing more noise to the branches as there is minimal information in thesub-optimal space to enhance the representation in the better space. (Chami et al., 2019; Liu et al., 2022a). Meanwhile, Airport is argued to be globally hyperbolic, but a largeproportion of nodes has large local hyperbolicity. However, this is not a contradiction as we are consideringthe local structures of the graph. We call G the distribution of geometric hyperbolicity. It depends only on Gand k.",
  "Space selection and model hyperbolicity": "In this section, we describe the backbone of our model and introduce the notion of model hyperbolicity. Ourmodel consists of two branches, one using Euclidean geometry and the other using hyperbolic geometry. Weprimarily use GAT (cf. .2) for the Euclidean model and HGAT (cf. .3) for the hyperbolicmodel. Other pairs of Euclidean or hyperbolic models (e.g., GCN and HGCN) can also be applied to thecorresponding branches. In .3, we show the experimental results on two variants of JSGNN. After the respective message propagation, we would have two sets of updated node embeddings, the Euclideanembedding ZR and the hyperbolic embedding ZD. The two sets of embeddings are combined into a singleembedding Z = {zv, v V } through an attention mechanism that serves as a space selection procedure. Theattention mechanism is performed in a Euclidean space. Thus, the hyperbolic embeddings are first mappedinto the tangent space using the logarithmic map. Mathematically, the normalized attention score indicatingwhether a node should be embedded in the hyperbolic space v,D or Euclidean space v,R is as follows:",
  "exp(wv,R) + exp(wv,D),(14)": "where q refers to the learnable space selection attention vector, M is a learnable weight matrix, b denotes alearnable bias and v,D +v,R = 1, for all v V . The weights v,D and v,R are conditioned to be non-uniformas illustrated in .4. The two sets of space-specific node embeddings can then be combined via aconvex combination using the learned weights as follows:",
  "This gives one layer of the model architecture of JSGNN, as illustrated in": "The parameter v,R, v V controls whether the combined output, consisting of both hyperbolic and Euclideancomponents, should rely more on the hyperbolic components or not. We call v,R the model hyperbolicity atthe node v. The notion of model hyperbolicity depends on node features as well as the explicit GNN model.Similar to geometric hyperbolicity, the collection G = {v,R : v V } gives rise to an empirical distributionG on . We call G the distribution of model hyperbolicity. To motivate the next subsection, from (15), we notice that the output depends smoothly on v,R. If wewish to have a similar output for nodes with similar neighborhood structures and features, we want theirselection weights to have similar values. On the other hand, we have seen (cf. Proposition 1) that geometrichyperbolicities, which can be computed given G, are similar for nodes with similar neighborhoods. It suggeststhat we may use geometric hyperbolicities to guide the choice of model hyperbolicities.",
  "We have introduced geometric and model hyperbolicities in the previous subsections. In this subsection, weexplore the interconnections between these two notions": "Let be the parameters of a proposed GNN model. We assume that the model has the pipeline shown in. Given node features {hv, v V } and model parameters , the model generates (embedding) features{zv, v V } and selection weights or model hyperbolicity {v,R, v V } in the intermediate stage. For eachv V , there is a combination function v such that the final output {yv, v V } satisfies yv = v(zv, v).",
  ": The model pipeline is shown in the (blue) dashed box, while the geometric hyperbolicity can becomputed independently of the model": "In principle, we want to compare {v,R, v V } and {v, v V } so that the geometric hyperbolicity guides thechoice of model hyperbolicity. However, comparing pairwise v and v for each v V may lead to overfitting.An alternative is to compare their respective distributions G and G, or even coarser statistics (e.g., mean)of G and G (cf. ). The latter may lead to underfitting. We perform an ablation study on the differentcomparison methods in .5. We advocate choosing the middle ground by comparing the distributions G and G. The former can becomputed readily as long as the ambient graph G is given, while the latter is a part of the model that plays acrucial role in feature aggregation at each node. Therefore, G can be pre-determined but not G. We proposeto use the known G to constrain G and thus the model parameters . A widely used comparison tool is theWasserstein metric.",
  ": Different ways of comparing geometric and model hyperbolicities": "Definition 3 (Wasserstein distance). Given p 1, the p-Wasserstein distance metric Villani (2008) measuresthe difference between two different probability distributions Gao et al. (2021). Let (G, G) be the set of alljoint distributions for random variables x and y where x G and y G. Then the p-Wasserstein distancebetween G and G is as follows:",
  "Wp(G, G) =inf(G,G) E(x,y)x yp1/p.(16)": "To compute the Wasserstein distance exactly is costly given that the solution of an optimal transportproblem is required (Rowland et al., 2019; Chen et al., 2022b). However, for one-dimensional distributions,the p-Wasserstein distance can be computed by ordering the samples from the two distributions and thencomputing the average p-distance between the ordered samples (Kolouri et al., 2019; Rowland et al., 2019). In ideal circumstances, considering the distributions do not lose much information. We first notice that forboth v,R and v, a smaller value means more hyperbolic in an appropriate sense. Suppose v,R is increasing",
  "Non-uniformity of selection weights": "A node is considered to be more suitable to be embedded in the hyperbolic space when v,D > v,R. Meanwhilewhen v,D v,R, the node is considered to be Euclidean. Nevertheless, to align with our motivation that eachnode can be better embedded in one of the two spaces and the less suitable space would result in distortionin representation, we require JSGNN to learn non-uniform attention weights, meaning that each pair ofattention weights (v,D, v,R) should significantly deviate from the uniform distribution. This is because softselection without a non-uniformity constraint may result in the assignment of nodes to be partially Euclideanand partially hyperbolic with v,R v,D 0.5. Hence, we include an additional component to the standardloss function encouraging non-uniform learned weights as follows:",
  "Loverall = Ltask + nuLnu + wasW2(G, G),(18)": "where Ltask is the task-specific loss, while nu and was are balancing factors. For the node classificationtask, Ltask refers to the cross-entropy loss over all labeled nodes while for link prediction, it refers to thecross-entropy loss with negative sampling. This completes the description of the JSGNN model. We speculate that the non-uniform component Lnu should push the model hyperbolicities towards the twoextremes 0 and 1. On the other hand, as we have seen in .3, to compute W2(G, G), we need toorder (v)vV , (v,R)vV respectively, and compute their pairwise differences. Therefore, W2(G, G) alignsthe shapes of G and G.",
  "GraphSAGE76.59 1.0665.26 2.9177.90 0.7191.25 0.2284.08 0.2589.62 0.18CurvGN81.58 0.5171.14 0.6778.17 0.4891.60 0.2584.18 0.3786.65 0.14CGNN82.15 0.6071.31 1.1678.34 0.8391.96 0.2784.29 0.3486.86 0.16": "HGNN79.28 0.7770.00 0.7477.45 1.4089.73 0.8480.27 0.2188.27 0.51HGCN78.68 0.7767.25 1.4576.72 0.9291.57 0.2883.68 0.5286.83 0.31HGAT78.81 1.4968.16 1.3477.43 1.2090.27 0.8181.29 0.7986.27 0.47LGCN78.93 0.7968.59 0.6478.08 0.6592.55 0.5785.03 0.2889.59 0.11-GCN (D16)78.64 0.8367.17 0.7378.01 0.6789.16 0.5984.57 0.2088.37 0.37DeepHGCN80.66 0.8972.11 0.6078.13 1.6788.51 1.5280.45 0.3686.90 0.42 -GCN (D16 R16)78.71 1.0266.96 1.1377.67 0.7488.85 0.8884.04 0.8185.59 0.53GIL79.97 1.9367.54 1.2376.62 0.8191.90 0.8482.39 0.9087.39 0.21JSGNN (GCN+HGCN)81.79 0.8070.55 1.0978.38 0.7493.26 0.9284.95 0.3189.68 0.60JSGNN (GAT+HGAT)82.94 0.5571.26 1.1378.57 0.9093.10 0.8685.10 0.6490.53 0.32",
  "Baselines": "For JSGNN, we consider two variants: GAT as the Euclidean model with HGAT as the hyperbolic model,and GCN as the Euclidean model with HGCN as the hyperbolic model. We compare against the followingmodels: (a) Euclidean methods: GCN (Kipf & Welling, 2016), GraphSAGE (Hamilton et al., 2017) andGAT (Velikovi et al., 2018); (b) hyperbolic models: HGCN (Chami et al., 2019), HGNN (Liu et al.,2019), HGAT (Zhang et al., 2021a), LGCN (Zhang et al., 2021b), DeepHGCN (Liu et al., 2024) and -GCN(D16) (Bachmann et al., 2019), which is a constant (negative) curvature graph neural network based onthe -stereographic model; (c) CurvGN (Ye et al., 2020) and CGNN (Li et al., 2021), which utilizes graphcurvature information to filter messages differently based upon different local structures; (d) mixed models:GIL (Zhu et al., 2020) and -GCN (D16 R16), which similar to JSGNN, leverages multiple spaces or a mixedcurvature space. -GCN (D16 R16) employs a two-component product space of negative and zero curvature.",
  "Node classification": "For the node classification task, each of the nodes in a dataset belongs to one of the C classes in the dataset.With the final set of node representations, we aim to predict the labels of nodes that are in the testing set. To test the performance of each model under both semi-supervised and fully-supervised settings, two datasplits are used in the node classification task for the Cora, Citeseer and Pubmed datasets. In the first split, wefollowed the standard split for semi-supervised settings used in Kipf & Welling (2016); Velikovi et al. (2018);Monti et al. (2017); Chamberlain et al. (2021b); Zhu et al. (2020); Chamberlain et al. (2021a); Hamilton et al.(2017); Liu et al. (2022b); Feng et al. (2020). The train set consists of 20 train examples per class while the",
  "-GCN (D16)97.04 0.1095.56 0.5489.08 1.1592.39 0.73DeepHGCN95.80 0.8894.37 1.4390.24 1.8890.38 1.76": "-GCN (D16 R16)96.97 0.1094.31 0.4188.38 0.6289.47 1.56GIL95.83 0.3094.41 0.5790.78 1.7490.67 1.98JSGNN (GCN+HGCN)97.08 0.0495.69 0.2290.59 1.7590.73 1.44JSGNN (GAT+HGAT)97.40 0.1497.16 0.4490.33 1.6190.88 1.54 validation set and test set consist of 500 samples and 1,000 samples, respectively.1 Meanwhile, in the secondsplit, all labels are utilized and the percentages of training, validation, and test sets are set as 60/20/20%. Forthe Photo and CS datasets, the labeled nodes are also split into three sets where 60% of the nodes made upthe training set, and the rest of the nodes were divided equally to form the validation and test sets. Airportand Disease datasets were split in similar settings as Zhu et al. (2020). In and , the mean accuracy with standard deviation is reported for node classification, exceptfor the case of Airport and Disease datasets where the mean F1 score is reported. Our empirical resultsdemonstrate that JSGNN frequently outperforms the baselines, especially HGAT and GAT which are thebuilding blocks of JSGNN. Even though the performance of the variant JSGNN (GCN+HGCN) is oftenslightly lower than JSGNN (GAT+HGAT), we have similarly observed it to consistently outperform itsbuilding blocks GCN and HGCN. This is not necessarily observed for other mixed space models. Thus,this shows the superiority of not only using both Euclidean and hyperbolic spaces but also our method ofincorporating the two spaces for graph learning as compared to GIL and -GCN (D16 R16). We also observe that Euclidean models such as GCN, GAT, GraphSAGE, CurvGN and CGNN perform betterthan hyperbolic models in general on the Cora, Citeseer, and Pubmed datasets for both splits. Meanwhile,hyperbolic models achieve better results on the CS, Photo, Airport, and Disease datasets. This means thatEuclidean features are more significant for representing Cora, Citeseer and Pubmed datasets while hyperbolicfeatures are more significant for the others. Nevertheless, JSGNN is able to perform relatively well across alldatasets. We note that JSGNN exceeds the performance of single-space baselines on all datasets except forDisease. This can be explained by the fact that Disease consists of a perfect tree and thus, does not exhibitdifferent hyperbolicities in the graph. However, JSGNN still outperforms 3 hyperbolic benchmarks and allthe other mixed models. We also particularly note that the difference in results between single-space models using only the Euclideanembedding space and hyperbolic models is not significant. This means that many of the node labels can bepotentially predicted even without the best representation from the right space. This might be the reasonwhy the gain in performance for the node classification task is not exceptional from embedding nodes in thebetter space. Nevertheless, we still see improvements in predictions for cases where there is a mixture of localhyperbolicities. Moreover, embedding nodes in a more suitable space can benefit other tasks that requiremore accurate representations such as link prediction. 1Note that the top results on used different data splits(either semi-supervised settings with a larger number of training samples or fully-supervised settings such as the 60/20/20%split) which give much higher accuracies",
  "MethodCoraCiteseerPubmedAirportDisease": "GCN88.22 1.0190.60 1.1087.63 3.2591.79 1.4861.60 3.76GAT85.47 2.2885.31 1.8985.30 1.4693.70 0.6561.23 2.75GraphSAGE88.94 0.8191.61 1.0088.42 1.1491.63 0.8168.31 2.94CurvGN94.40 2.1395.38 2.3394.55 0.8993.90 0.3795.47 0.81CGNN94.26 1.3696.54 0.7894.71 3.1495.13 0.9795.35 1.22 HGNN91.48 0.3893.63 0.1492.95 0.3596.31 0.3082.98 0.98HGCN93.72 0.2696.72 1.6996.68 0.0497.55 0.0887.14 1.34HGAT94.06 0.1195.60 0.2095.78 0.0597.86 0.0886.61 1.67LGCN93.10 0.3093.40 0.7095.45 0.0897.88 0.1995.99 0.58-GCN (D16)92.43 0.6394.38 0.5194.89 0.0796.78 0.1993.58 0.31 -GCN (D16 R16)91.32 0.3892.87 0.2793.53 0.0697.17 0.1090.15 0.77GIL98.04 1.6499.95 0.0992.50 0.5097.20 1.04100.00 0.00JSGNN (GCN+HGCN)98.83 0.9299.97 0.0997.67 0.0398.94 0.91100.00 0.00JSGNN (GAT+HGAT)99.43 0.2199.98 0.0596.95 0.0399.26 1.2399.97 0.08",
  "Link prediction": "We employ the Fermi-Dirac decoder with a distance function to model the probability of an edge based on ourfinal output embedding, similar to Zhu et al. (2020); Sun et al. (2022); Chami et al. (2019). The probabilitythat an edge exists is given by P(evj E | ) = (e(d(xi,xj)r)/t + 1)1 where r, t > 0 are hyperparametersand d is the distance function. The edges of the datasets are randomly split into 85/5/10% for training,validation, and testing. The average ROC AUC for link prediction is recorded in . We observe thatJSGNN (GAT+HGAT) performs better than the baselines in most cases. For the link prediction task, wenotice that hyperbolic models consistently outperform Euclidean models by a significant margin. Moreover,Euclidean methods such as CurvGN and CGNN benefit from using topological information during learning.Empirical results also suggest that predicting the existence of edges seems to benefit from dual space models,i.e., GIL and JSGNN, except for the case of -GCN (D16 R16). This finding is similar to that reported in Bachmann et al. (2019); Xiong et al. (2022) where despite slightlydifferent settings, the constant (negative) curvature -stereographic model frequently outperforms the -GCNleveraging on the product of multiple constant curvature spaces. We hypothesize that the simple concatenationto combine the embeddings of the two different component spaces in -GCN (D16 R16) might be insufficientand might have resulted in noise from the other space being passed to the negatively curved space which wasperforming well standalone as seen in -GCN (D16). As such, our aim to learn to select the better space foreach node is potentially capable of offering better representations with reduced distortions.",
  "Without the non-uniformity loss and Wasserstein distance (w/o NU & W2): Only guided by thecross entropy loss, i.e., nu = 0, was = 0 (cf. (18))": "summarizes the results of our study, from which we observe that all variants of JSGNN (GAT+HGAT)with some components discarded perform worse than the full model. Moreover, JSGNN (GAT+HGAT)without W2 always achieves better results than JSGNN (GAT+HGAT) without NU and W2, signifyingthe importance of selecting the better of the two spaces instead of combining the features with relativelyuniform weights. Similarly, JSGNN (GAT+HGAT) without NU performs better than JSGNN (GAT+HGAT)without NU and W2 in most cases, suggesting that incorporating geometric hyperbolicity through distributionalignment does help to improve the model. A comparison of the time taken for each variant is provided inAppendix B. To further analyze our model, we present a study regarding our method of incorporating the guidance ofgeometric hyperbolicity through distribution alignment. The result is as seen in . We test and analyzeempirically different variants of our model based on the different comparisons shown in . Pairwisematch indicates minimizing the mean squared error between elements of G and V (without sorting) whilemean match minimizes the squared loss between the means of G and V . We observe that comparing thedistributions of G and G consistently outperforms comparing their mean, demonstrating the insufficiency ofutilising coarse statistics for supervision. Secondly, pairwise matching gave better results than mean matching,though still lower than distribution matching, suggesting the importance of fine-scale information yet, a needto avoid potential overfitting.",
  "Analysis of hyperbolicities": "We have speculated the effects of different components of our proposed model at the end of .4.To verify that our model can learn model hyperbolicity that is non-uniform and similar in distribution asgeometric hyperbolicity, we analyze the learned model hyperbolicities (v,R)vV of JSGNN (GAT+HGAT)and the model w/o NU & W2 for the node classification task. Specifically, we extract the learned values fromthe first two layers of JSGNN and its variant for ten separate runs. The learned values from the first twolayers were then averaged before determining W2(G, Unif) and W2(G, G). In , it can be inferred that JSGNNs learned model hyperbolicity is always less uniform than that of themodel w/o NU & W2 given JSGNNs larger W2(G, Unif) score, demonstrating a divergence from uniformdistribution. Meanwhile, for most cases, JSGNNs W2(G, G) is smaller than that of the model w/o NU &W2, suggesting that the shape between G and G of JSGNN is relatively more similar. At times, JSGNNsW2(G, G) is larger than the model w/o NU & W2, suggesting a tradeoff between NU and W2 as we choosethe optimal combination for the models best performance.",
  "Conclusion": "In this paper, we have explored the learning of GNNs in a joint space setting given that different regions ofa graph can have different geometrical characteristics. In these situations, it would be beneficial to embeddifferent regions of the graph in different spaces that are better suited for their underlying structures, to",
  ": Analysis of hyperbolicities on different datasets. (a) W2(G, Unif). (b) W2(G, G)": "reduce the distortions incurred while learning node representations. Our method JSGNN utilizes a softattention mechanism with non-uniformity constraint and distribution alignment between model and geometrichyperbolicities to select the best space-specific feature for each node. This indirectly finds the space that isbest suited for each node. Experimental results of node classification and link prediction demonstrate theeffectiveness of JSGNN against various baselines. In future work, we aim to further improve our model withan adaptive mechanism to determine the appropriate, node-level specific neighborhood to account for eachnodes hyperbolicity. Limitations are discussed in Appendix F.",
  "Acknowledgements": "The first author is supported by Shopee Singapore Private Limited under the Economic Development BoardIndustrial Postgraduate Programme (EDB IPP). The programme is a collaboration between Shopee andNanyang Technological University, Singapore. This research is supported in part by the Singapore Ministryof Education Academic Research Fund Tier 2 grant MOE-T2EP20220-0002. This work is carried out duringthe first authors participation in the EDB IPP. Aaron B. Adcock, Blair D. Sullivan, and Michael W. Mahoney. Tree-like structure in large social andinformation networks. In 2013 IEEE 13th International Conference on Data Mining, pp. 110, 2013. doi:10.1109/ICDM.2013.77.",
  "M. Bridson and A. Haefliger. Metric Spaces of Non-Positive Curvature. Springer, 1999": "Benjamin Paul Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Dong Xiaowen,and Michael M Bronstein. Beltrami flow and neural diffusion on graphs. Proceedings of the Thirty-fifthConference on Neural Information Processing Systems (NeurIPS), 2021a. Benjamin Paul Chamberlain, James Rowbottom, Maria I. Gorinova, Stefan D Webb, Emanuele Rossi, andMichael M. Bronstein. GRAND: Graph neural diffusion. In The Symbiosis of Deep Learning and DifferentialEquations, 2021b.",
  "Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutionalnetworks on node classification. In International Conference on Learning Representations, 2020. URL": "Mark Rowland, Jiri Hron, Yunhao Tang, Krzysztof Choromanski, Tamas Sarlos, and Adrian Weller. Orthogonalestimation of wasserstein distances. In Proceedings of the 22nd International Conference on ArtificialIntelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp. 186195. PMLR,1618 Apr 2019. Ramit Sawhney, Shivam Agarwal, Arnav Wadhwa, and Rajiv Shah. Exploring the scale-free nature of stockmarkets: Hyperbolic graph learning for algorithmic trading. In Proceedings of the Web Conference, WWW21, pp. 1122, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383127.doi: 10.1145/3442381.3450095. Li Sun, Zhongbao Zhang, Junda Ye, Hao Peng, Jiawei Zhang, Sen Su, and Philip S. Yu. A self-supervisedmixed-curvature graph neural network. In Association for the Advancement of Artificial Intelligence(AAAI), 2022. Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M. Bronstein.Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference onLearning Representations, 2022. URL",
  "Ze Ye, Kin Sum Liu, Tengfei Ma, Jie Gao, and Chao Chen. Curvature graph network. In Proceedings of the8th International Conference on Learning Representations (ICLR 2020), April 2020": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. Graphconvolutional neural networks for web-scale recommender systems. In Proceedings of the 24th ACMSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 18, pp. 974983, 2018.ISBN 9781450355520. Hanqing Zeng, Muhan Zhang, Yinglong Xia, Ajitesh Srivastava, Andrey Malevich, Rajgopal Kannan, ViktorPrasanna, Long Jin, and Ren Chen. Decoupling the depth and scope of graph neural networks. In Advancesin Neural Information Processing Systems, 2021.",
  "Cora2708542971433Citeseer3327473263703Pubmed19717443383500Aiport31881863144Disease1044104321000Photo76501190818745CS1833381894156805": "For all models, the hidden units are set to 16. We set the early stopping patience to 100 epochs with amaximum limit of 2000 epochs. The hyperparameter settings for the baselines are the same as Zhu et al.(2020) if given. The only difference is that the hyperparameter h-drop for GIL in Zhu et al. (2020) (whichdetermines the dropout to the weight associated with the hyperbolic space embedding) is set to 0 for alldatasets as setting a large value essentially explicitly chooses one single space. Else, the hyperparameters arechosen to yield the best performance. For JSGNN, we perform a grid search on the following search spaces:Learning rate: [0.01, 0.005]; Dropout probability: [0.0, 0.1, 0.5, 0.6]; Number of layers: ; nu andwas: [1.0, 0.5, 0.2, 0.1, 0.01, 0.005]; q (cf. (11)): . The Wasserstein-2 distance is employed in allvariants of JSGNN.",
  "BComplexity, run-time and model size": "We first incur an overhead to compute the geometric hyperbolicity v for each node. We follow the approachas described below2 rather than using the naive implementation with a time complexity of O(V 4). Instead ofconsidering all possible 4-tuples in the graph, we heuristically sample K 4-tuples from each nodes two-hopsubgraph. Consequently, the complexity is O(N) [O(N 2) + O(K Esubgraph)] O(NKd2) where d denotesthe average degree in the graph, K denotes the number of samples and Esubgraph represents the number ofedges in the two-hop subgraph of each node. Assuming the graph is sparse, d is small. This runtime can befurther optimized by parallelization, as each nodes calculation is independent. Also, notice that this stepis performed only once during pre-processing, and the step is model-agnostic. For a dataset, the computed{v}vV can be stored and re-used for different trainings and also for different base models. Moreover, if thedataset is updated, we only need to adjust v for nodes v whose neighborhood structures are changed duringthe update, which can be done efficiently. For training of the model, we usually use two base models Me (e.g., GCN) and Mh (e.g., HGCN) for handlingEuclidean and hyperbolic structures respectively. The complexity of the message passing in our model isO(Ce + Ch), where Ce and Ch are the message passing complexities of Me and Mh respectively. Regarding the implementation of the extra loss terms, calculating the Wasserstein distance for one-dimensionaldistributions requires a time complexity of O(N log N) where N is the number of nodes in the graph, primarilydominated by the time needed to sort the distributions. Meanwhile, obtaining the non-uniformity loss has atime complexity of O(N).",
  "CDistributions of hyperbolicity": "We include that tabulates the percentage of nodes whose geometric hyperbolicity v is (approximately)the prescribed value. Here, the hyperbolicity is taken in a neighborhood of each node to account for the localnature of message passing. We see, for example, for the Cora dataset, there are more than 52.4% of nodeswhose v is at least 1. Hence, an Euclidean model is preferred. On the other hand, there are 23.0% of nodeswhose neighborhood is tree-like, and this is why a hybrid model can further improve the performance.",
  "DProof of Proposition 1": "Proof. We first consider G,. For two graphs G1 = (V1, E1) and G2 = (V2, E2), let f1 : G1 M, f2 : G2 M be isometeric embeddings into a metric space (M, d) such that dGH(G1, G2) = dH(f1(G1), f2(G2)). DenotedGH(G1, G2) by . For x, y, z, t in G1, there are x, y, z, t G2 such that d(f1(x), f2(x)), d(f1(y), f2(y)),",
  "Therefore, G1, G2, + 4. By the same argument swapping the role of G1 and G2, we have G2, G1, + 4. Therefore |G1, G2,| 4 and G, is Lipschitz continuous w.r.t. G": "The proof of the continuity of G,1 is more involved. Consider G1 and G2 in G. Let f1, f2, (M, d), be asearlier and assume , for example, = for is smaller than all the numerical constants in the rest ofthe proof. We adopt the following convention: for any non-vertex point of a graph, its degree is 2. By subdividing theedges of G1 and G2 if necessary, we may assume that the length of each edge e in E1 or E2 satisfies /2 e < .As a consequence, for (u, v) in E1 (resp. E2), dG1(u, v) (resp. dG2(u, v)) is the same as the length of (u, v).We define a map : G1 G2 as follows. For v G1, there is a v in G2 such that dGH(f1(v), f2(v)) .Then we set (v) = v. The map is injective on the vertex set V1. Indeed, for u = v V1, dG1(u, , v) /2and hence dG2((u), (v)) /2 2 > 0. The strategy is to modify by a small perturbation such that theresulting function : G1 G2 is a homeomorphism that is almost an isometry. For v V1, let Nv be the 5 neighborhood of v. It is a star graph and its number of branches is the sameas the degree of v, say k. Let v1, . . . , vk be the endpoints of Nv. The convex hull (of shortest paths) Cv of{(v1), . . . , (vk)} in G2 is also a star graph. This is because Cv is contained in the 7 neighborhood of (v)and it contains at most 1 vertex in V2.",
  ": Illustration of": "For each v V1, we now enlarge the neighborhood and consider its /6-neighborhood N v. It does not containanother vertex and hence is also a star graph. Moreover, if v = u V1, then N v N u = for otherwisedG1(u, v) /3, which is impossible. We may similarly consider the /6-neighborhoods Cu, Cv of (u) and(v). Both Cu and Cv do not contain any vertex in V2 with degree = 2. As N v and Cv are star graphs with the same number of branches, there is an isometry (also denoted by) : N v Cv such that dG2((w), (w)) 2. By disjointedness of /6 neighborhoods, we may combine allthe maps above together to obtain : vV1N v vV1Cv. For the rest of G1, consider any edge (u, v) E1. Without loss of generality, let u1 and v1 be the leavesof N u and N v contained in (u, v). We claim that the shortest open path connecting (u1) and (v1) isdisjoint from vV1Cv. For otherwise, dG1(u1, v1) 2/3, while dG2((u1), (u2)) dG2((u1), (u2))4 /2 + 2/6 4. Therefore, 2/3 2 5/6 4, which is impossible as . Let Pu,v and Qu,v be the shortest paths connecting u1, v1 and (u1), (v1) respectively (illustrated in ).Then the length of Pu,v and Qu,v differ at most by 4. We may further extend : Pu,v Qu,v by alinear scaling such that dG2((w), (w)) 3 for w Pu,v. For different edges (u, v), (u, v), it is apparentQu,v Qu,v are disjoint, as the minimal distance between points on Pu,v and Pu,v is at least /3. Therefore,we obtain a continuous injection : G1 G2, which maps homeomorphically onto its image. We claim that is onto. If not, there is a vertex v V2 that is not in (V1) but it has a neighboring vertexu = (u). However, this implies that the degree of u is strictly larger than that of u, which is impossible aswe have shown.",
  ": Illustration of Pu,v and Qu,v": "We are ready to estimate |G1,1 G2,1|. Let |Gi| be the total edge weights of Gi, i = 1, 2. For convenience,we denote a typical tuple (u, v, w, t) G41 as a vector v, and ((u), (v), (w), (t)) by (v). The map :G41 G42, v (v) inherits the properties of its counterpart , which is a piecewise linear homeomorphism.In particular, its Jacobian J(v) is defined almost everywhere. Using Definition 2, we have:",
  ",": "the same bounds holds for |G1|/|G2|. Both upper and lower bounds can be arbitrarily close to 1 if is smallenough. Similarly, by (20), J(v) as a fourth power of can also be made arbitrarily close to 1. In conjunctionwith (21) and (22), |G1,1 G2,1| can be arbitrarily small if is chosen to be small enough. This proves thatG,1 is continuous in G.",
  "In this appendix, models that utilize multiple spaces and advanced topological information such as curvatureare reviewed": "CurvGN (Ye et al., 2020) and Curvature Graph Neural Network (CGNN) (Li et al., 2021) learn to reweighmessages propagated between nodes, in Euclidean space, using curvature information. Curvature measureshow easily information flows between two nodes. These works assume that the edges with low curvaturesindicate the class boundaries, thus low weights are assigned when the edges are of low curvature. In our work,we use Gromov hyperbolicity instead of Olliviers Ricci curvature to choose the appropriate space and we donot reweigh the edges. To the best of our knowledge, the closest works to ours are Geometry Interaction Learning (GIL) (Zhu et al.,2020) and -GCN (Bachmann et al., 2019). -GCN utilizes the (Cartesian) product space to model data",
  "FLimitations": "The paper does not have a comprehensive numerical study on extremely large datasets. Therefore, we do nothave a definite answer to how JSGNN will perform on these graphs. However, the datasets being studied inour paper are diverse enough in terms of geometric properties. We have theoretically analyzed the complexity,which depends on the complexities of the hyperbolic and Euclidean base models. Therefore, its scalabilityalso depends on the based models. For example, GCN, GAT, HGCN, and HGAT all scale well with graphsize. Notice that message passing is a local operation, and hence model performance on graphs of mediumsizes should be illustrative enough."
}