{
  "Abstract": "Federated learning is a paradigm of distributed machine learning in which multiple clientscoordinate with a central server to learn a model, without sharing their own training data.Standard federated optimization methods such as Federated Averaging (FedAvg) ensure bal-ance among the clients by using the same stepsize for local updates on all clients. However,this means that all clients need to respect the global geometry of the function which couldyield slow convergence. In this work, we propose locally adaptive federated learning algo-rithms, that leverage the local geometric information for each client function. We show thatsuch locally adaptive methods with uncoordinated stepsizes across all clients can be particu-larly efficient in interpolated (overparameterized) settings, and analyze their convergence inthe presence of heterogeneous data for convex and strongly convex settings. We validate ourtheoretical claims by performing illustrative experiments for both i.i.d. non-i.i.d. cases. Ourproposed algorithms match the optimization performance of tuned FedAvg in the convexsetting, outperform FedAvg as well as state-of-the-art adaptive federated algorithms likeFedAMS for non-convex experiments, and come with superior generalization performance.",
  "Introduction": "Federated Learning (FL) (Kairouz et al., 2021) has become popular as a collaborative learning paradigmwhere multiple clients jointly train a machine learning model without sharing their local data.Despitethe recent success of FL, state-of-the-art federated optimization methods like FedAvg (McMahan et al.,2017) still face various challenges in practical scenarios such as not being able to adapt according to thetraining dynamicsFedAvg using vanilla SGD updates with constant stepsizes maybe unsuitable for heavy-tail stochastic gradient noise distributions, arising frequently in training large-scale models such as ViT(Dosovitskiy et al., 2021).Such settings benefit from adaptive stepsizes, which use some optimizationstatistics (e.g., loss history, gradient norm). In the centralized setting, adaptive methods such as Adam (Kingma & Ba, 2014) and AdaGrad (Duchiet al., 2011) have succeeded in obtaining superior empirical performance over SGD for various machinelearning tasks. However, extending adaptive methods to the federated setting remains a challenging task,and majority of the recently proposed adaptive federated methods such as FedAdam (Reddi et al., 2021)and FedAMS (Wang et al., 2022a) consider only server-side adaptivity, i.e., essentially adaptivity only in theaggregation step. Some methods like Local-AMSGrad (Chen et al., 2020) and Local-AdaAlter (Xie et al.,2019) do consider local (client-side) adaptivity, but they perform some form of stepsize aggregation in thecommunication round, thereby using the same stepsize on all clients.",
  "Published in Transactions on Machine Learning Research (10/2024)": "in practice. Therefore, we choose (a) as our aggregation formula and this has some added advantages forthe associated proofs. We feel that a reason behind the good performance of FedSPS-Global is the lowinter-client and intra-client variance of the stepsizes explained in this is the reason behind whysimple averaging of the local stepsizes work.Proposition 2 (Global SPS stepsize aggregation formula). Using the definitions of the three aggregationformulae for synchronous SPS stepsizes (a), (b), and (c), as defined above in Section D.2.2, we have thefollowing inequalities",
  "(a) Can local adaptivity for federated optimization be useful (faster convergence)?(b) Can we design such a locally adaptive federated optimization algorithm that provably converges?": "To answer (a), we shall see a concrete case in Example 1 along with an illustration in , showinglocally adaptive stepsizes can substantially speed up convergence. For designing a fully locally adaptivemethod for federated optimization, we need an adaptive stepsize that would be optimal for each clientfunction. Inspired by the Polyak stepsize (Polyak, 1987) which is designed for gradient descent on convexfunctions, Loizou et al. (2021) recently proposed the stochastic Polyak step-size (SPS) for SGD. SPS comeswith strong convergence guarantees, and needs less tuning compared to other adaptive methods like Adamand AdaGrad.We propose the FedSPS algorithm by incorporating the SPS stepsize in the local clientupdates.We obtain exact convergence of our locally adaptive FedSPS when interpolation condition issatisfied (overparameterized case common in deep learning problems), and convergence to a neighbourhoodfor the general case. Reminiscing the fact that Li et al. (2020b) showed FedAvg needs decaying stepsizes toconverge under heterogeneity, we extend our method to a decreasing stepsize version FedDecSPS (followingideas from DecSPS (Orvieto et al., 2022)), that provides exact convergence in practice, for the generalnon-interpolating setting without the aforementioned small stepsize assumption. Finally, we experimentallyobserve that the optimization performance of FedSPS is always on par or better than that of tuned FedAvgand FedAMS, and FedDecSPS is particularly efficient in non-interpolating settings.",
  "Contributions. We summarize our contributions as follows:": "We show that local adaptivity can lead to substantially faster convergence. We design the firstfully locally adaptive method for federated learning called FedSPS, and prove sublinear and linearconvergence to the optimum, for convex and strongly convex cases, respectively, under interpolation(Theorem 3). This is in contrast to existing adaptive federated methods such as FedAdam andFedAMS, both of which employ adaptivity only for server aggregation. For real-world FL scenarios (such as when the interpolation condition is not satisfied due to clientheterogeneity), we propose a practically motivated algorithm FedDecSPS that enjoys local adaptivityand exact convergence also in the non-interpolating regime due to decreasing stepsizes. We empirically verify our theoretical claims by performing relevant illustrative experiments to showthat our method requires less tuning compared to state-of-the-art algorithms which need extensivegrid search. We also obtain competitive performance (both optimization as well as generalization)of the proposed FedSPS and FedDecSPS compared to tuned FedAvg and FedAMS for the convexand non-convex cases in i.i.d. as well as non-i.i.d. settings.",
  "Additional related Work": "Adaptive gradient methods and SPS. Recently, adaptive stepsize methods that use some optimizationstatistics have become popular for deep learning applications. Such methods, including Adam (Kingma &Ba, 2014) and AdaGrad (Duchi et al., 2011), work well in practice and also theoretically convergent underdifferent levels of smoothness (Rodomanov et al., 2024). A further adaptive method with sound theoreticalguarantees is the Polyak stepsize (Polyak, 1987), which has been recently extended to the stochastic settingby Loizou et al. (2021) and termed stochastic Polyak stepsize (SPS). Extensions of SPS have been proposedfor solving structured non-convex problems (Gower et al., 2021a) and in the update rule of stochastic mirror",
  ",(1)": "where the components fi : Rd R are distributed among n local clients and are given in stochastic formfi(x) := EDi[Fi(x, )], where Di denotes the distribution of over parameter space i on client i [n].Standard empirical risk minimization is an important special case of this problem, when each Di presentsa finite number mi of elements {i1, . . . , imi}. Then fi can be rewritten as fi(x) =1mimij=1 Fi(x, ij). Wedo not make any restrictive assumptions on the data distributions Di, so our analysis covers the case ofheterogeneous (non-i.i.d.) data where Di = Dj, i = j and the local minima xi := arg minxRd fi(x), can bedifferent from the global minimizer of (1).",
  "Background and Motivation": "Federated averaging. A common approach to solving (1) in the distributed setting is FedAvg (McMahanet al., 2017) also known as Local SGD (Stich, 2018). This involves the clients performing a local step of SGDin each iteration, and the clients communicate with a central server after every iterationstheir iteratesare averaged on the server, and sent back to all clients. FedAvg corresponds to the special case of Algorithm1 with constant stepsizes it 0 (Line 4).",
  ", where F 1 := infD1,xRd F1(x, ), b > 0 is an upper bound on the": "stepsize that controls the size of neighbourhood (b trades-off adaptivity for accuracy), and c > 0is a constant scaling factor. Instead of using the optimal function values of each stochastic functionas in the original SPS paper (Loizou et al., 2021), we use the lower bound on the function values1 F 1 , which is easier to obtain for many practical tasks as shown in (Orvieto et al., 2022). Itshould also be noted that using the lower bound on function values lead to the same convergenceresult as in original SPS using optimal function values, as shown in Theorem 2 of Orvieto et al.(2022)).",
  "Stepsize": "Constant Stepsize: 1,2 = 0.50 Global SPS: 1,2 = SPS(f) Ours (1) Ours (2) : Illustration for Example 1, showing local adaptivity can improve convergence. We run SGD with constant,global SPS, and locally adaptive SPS stepsizes (with c = 0.5, b = 1.0), for functions f1(x) = x2, f2(x) = 1",
  "Proposed Method": "Motivated by the previous example on the benefit of local adaptivity, we now turn to design such a locallyadaptive federated optimization algorithm with provable convergence guarantees. As stated before, we needan adaptive stepsize that is optimal for each client function, and we choose the SPS stepsize for this purpose.In the following, we describe our proposed method FedSPS. FedSPS. We propose a fully locally (i.e., client-side) adaptive federated optimization algorithm FedSPS(Algorithm 1) with asynchronous stepsizes, i.e., the stepsizes are different across the clients, and also acrossthe local steps for a particular client. The FedSPS stepsize for a client i and local iteration t will be givenby",
  ": end for": "We provide a theoretical justification of this choice of aggregation formula, and compare it with other possiblechoices in Appendix D.2.2. As it is apparent from (41), we need to use stale quantities from the last round,for calculating the stepsize for the current round. This is justified by the empirical observation that stepsizesdo not vary too much between consecutive rounds, and is a reasonable assumption that has also been madein previous work on adaptive distributed optimization Xie et al. (2019). Due to the staleness in update ofthe stepsize in this method, we need to provide a starting stepsize 0 to the algorithm, and this stepsize isused in all clients till the first communication round happens. FedSPS-Global can be thought of more as aheuristic method using cheap stepsize computations, and offering similar empirical performance as FedSPS.",
  "i=1i ,(5)": "that will appear in our complexity estimates, and thus we implicitly assume that 2f < (finite optimalobjective difference). Moreover, 2f also acts as our measure of heterogeneity between clients. This is inline with previous works on federated optimization in non-i.i.d. setting, such as Li et al. (2020b) that used := f Eif i as the heterogeneity measure. We can relate 2f to the more standard measures of function het-",
  "Convergence of fully locally adaptive FedSPS": "In this section we provide the convergence guarantees of FedSPS on sums of convex (or strongly convex)functions. We do not make any restriction on b, and thus denote this as the fully locally adaptive settingthat is of most interest to us. The primary theoretical challenge was to extend the error-feedback framework(that originally works for equal stepsizes) (Mania et al., 2017; Stich & Karimireddy, 2020) to work for fullyun-coordinated local stepsizes, and we do this for the first time in our work. All proofs are provided inAppendix B.",
  "nni=1 xit": "The convergence criterion of the first result (6) is non-standard, as it involves the average of all iterates xiton the left hand side, and not the average xt more commonly used. However, note that every -th iterationthese quantities are the same, and thus our result implies convergence of1T n(T 1)/t=0ni=1 E[fi(xt) i ].Moreover, in the interpolation case all i f . Remark 4 (Minimal need for hyperparamter tuning). The parameter is a user selected input parameterto determine the number of local steps, and b trades-off adaptivity (potentially faster convergence for largeb) and accuracy (higher for small b). Moreover, as c only depends on the input parameter and not onproperties of the function (e.g. L or ), it is also a free parameter. The algorithm provably converges (up tothe indicated accuracy) for any choice of these parameters. The lower bounds i can be set to zero for many",
  "(Special case II) Exact convergence of FedSPS in the small stepsize regime: Theorem 3 showsconvergence of FedSPS to a neighborhood of the solution. Decreasing b (smaller than1": "2cL) can improvethe accuracy, but the error is at least (2f) even when b 0. This issue is also persistent in the originalwork on SPSmax (Loizou et al., 2021, Corr. 3.3). However, we remark when the stepsize upper bound b ischosen extremely smallnot allowing for adaptivity-then FedSPS becomes identical to constant stepsizeFedAvg. This is not reflected in Theorem 3 that cannot recover the exact convergence known for FedAvg.We address this in the next theorem, proving exact convergence of small stepsize FedSPS (equivalent toanalysis of FedAvg with 2f assumption).",
  ".(10)": "This theorem shows that by choosing an appropriately small b, any arbitrary target accuracy > 0 canbe obtained. We are only aware of Li et al. (2020b) that studies FedAvg under similar assumptions as us( := f Eif i measuring heterogeneity). However, their analysis additionally required bounded stochasticgradients and their convergence rates are weaker (e.g., not recovering linear convergence under interpolationwhen 2f = 0).",
  "Decreasing FedSPS for exact convergence": "In the previous section, we have proved that FedSPS converges in the interpolation setting irrespective ofthe value of the stepsize parameter b. However, many practical federated learning scenarios such as thoseinvolving heterogeneous clients constitute the non-interpolating setting (f > 0). Here, we need to choosea small value of b to ensure convergence, trading-off adaptivity for achieving exact convergence. We mightrecall that Li et al. (2020b) proved choosing a decaying stepsize is necessary for convergence of FedAvgunder heterogeneity. In this section, we draw inspiration from the decreasing SPS stepsize DecSPS (Orvieto",
  "(d) Optimal c versus": ": Sensitivity analysis of FedSPS to hyperparameters for convex logistic regression on the MNIST dataset(i.i.d.)without client sampling.(a) Comparing the effect of varying b on FedSPS and varying on FedAvgconvergenceFedAvg is more sensitive to changes in , while FedSPS is insensitive changes in to b. (b) Effect ofvarying b on FedSPS stepsize adaptivityadaptivity is lost if b is chosen too small. (c) Small c works well inpractice ( = 5). (d) Optimal c versus , showing that there is no dependence.",
  "(d) Stepsizes for (c)": ": Comparison for convex logistic regression. (a) MNIST dataset (i.i.d. without client sampling). (b) w8adataset (i.i.d. with client sampling). (c) MNIST dataset (non-i.i.d. with client sampling). (d) Average stepsize acrossall clients for FedSPS and FedDecSPS corresponding to (c). Performance of FedSPS matches that of FedAvg withbest tuned local learning rate for the i.i.d. cases, and outperforms in the non-i.i.d. case.",
  "Experiments": "Experimental setup. For all federated training experiments we have 500 communication rounds (the no.of communication rounds being T/ as per our notation), 5 local steps on each client ( = 5, unless otherwisespecified for some ablation experiments), and a batch size of 20 (|B| = 20). We perform experiments in thei.i.d. as well as non-i.i.d. settings. Results are reported for both settings without client sampling (10 clients)and with client sampling (10 clients sampled uniformly at random from 100 clients with participation fraction0.1, and data split among all 100 clients) i.e., n = 10 active clients throughout. The i.i.d. experiments involverandomly shuffling the data and equally splitting the data between clients. For non-i.i.d. experiments with the MNIST dataset (LeCun et al., 2010), we assign every client samples fromexactly two classes of the dataset, the splits being non-overlapping and balanced with each client having samenumber of samples (Li et al., 2020b). For non-i.i.d. experiments with the CIFAR-10/CIFAR-100 datasets,we use the Dirichlet distribution over classes following the proposal in Hsu et al. (2019). The degree of",
  "(b) MNIST (non-i.i.d.)": ": Non-convex MNIST experiments with client sampling. (a) Non-convex case of LeNet on MNIST dataset(i.i.d.). (b) Non-convex case of LeNet on MNIST dataset (non-i.i.d.). First column represents training loss, secondcolumn is test accuracy. Convergence of FedSPS is very close to that of FedAvg with the best possible tuned locallearning rate. Moreover, FedSPS converges better than FedAMS for the non-convex MNIST case (both i.i.d. andnon-i.i.d.), and also offers superior generalization performance than FedAMS.",
  "Test Accuracy": "FedSPSFedAMSFedAvg : Non-convex CIFAR-10 (i.i.d.) experiments with client sampling and ResNet18 architecture. First columnrepresents training loss, second column is test accuracy. FedSPS converges better than FedAvg and FedAMS, andalso offers superior generalization performance. (with client sampling), in Figures 4, 5, and . For the upper bound on stepsizes, we use the smoothingtechnique for rest of the experiments, as suggested by Loizou et al. (2021) for avoiding sudden fluctuationsin the stepsize. For a client i [n] and iteration t, the adaptive iteration-dependent upper bound is given byib,t = 2|B|/miib,t1, where |B| is the batch-size, mi is the number of data examples on that client and we fixb,0 = 1. In (MNIST), we find that the convergence of FedSPS is very close to that of FedAvg withthe best possible tuned local learning rate, while outperforming FedAMS. In (CIFAR-10), FedSPSoutperforms tuned FedAvg and FedAMS in terms of both training loss and test accuracy. We report the",
  "Conclusion": "In this paper, we show that locally adaptive federated optimization can lead to faster convergence by harness-ing the geometric information of local objective functions. This is especially beneficial in the interpolatingsetting, which arises commonly for overparameterized deep learning problems. We propose a locally adaptivefederated optimization algorithm FedSPS, by incorporating the stochastic Polyak stepsize in local steps, andprove sublinear and linear convergence to a neighbourhood for convex and strongly convex cases, respectively.We further extend our method to the decreasing stepsize version FedDecSPS, that enables exact convergenceeven in practical non-interpolating FL settings without compromising adaptivity. We perform relevant il-lustrative experiments to show that our proposed method is relatively insensitive to the hyperparametersinvolved, thereby requiring less tuning compared to other state-of-the-art federated algorithms. Moreover,our methods perform as good or better than tuned FedAvg and FedAMS for convex as well as non-convexexperiments in i.i.d. and non-i.i.d. settings. This work was supported by the Helmholtz Associations Initiative and Networking Fund on theHAICORE@FZJ partition. Sebastian Stich thanks the partial financial support from a Google ResearchScholar Award 2023. Nicolas Loizou acknowledges support from CISCO Research.",
  "Xiangyi Chen, Xiaoyun Li, and Ping Li. Toward communication efficient adaptive gradient method. InProceedings of the 2020 ACM-IMS on Foundations of Data Science Conference, pp. 119128, 2020": "Ryan DOrazio, Nicolas Loizou, Issam Laradji, and Ioannis Mitliagkas. Stochastic mirror descent: Con-vergence analysis and adaptive variants via the mirror stochastic polyak stepsize.arXiv preprintarXiv:2110.15412, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In InternationalConference on Learning Representations, 2021. URL",
  "Xiaowen Jiang, Anton Rodomanov, and Sebastian U Stich. Stabilized proximal-point methods for federatedoptimization. arXiv preprint arXiv:2407.07084, 2024b": "Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and openproblems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210, 2021. Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian UStich, and Ananda Theertha Suresh.Breaking the centralized barrier for cross-device federatedlearning.In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan(eds.),Advances in Neural Information Processing Systems,volume 34,pp. 2866328676. Cur-ran Associates, Inc., 2021.URL",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian U. Stich. A unified theoryof decentralized SGD with changing topology and local updates. In 37th International Conference onMachine Learning (ICML). PMLR, 2020. Anastasiia Koloskova, Sebastian U Stich, and Martin Jaggi. Sharper convergence guarantees for asynchronoussgd for distributed and federated learning. Advances in Neural Information Processing Systems, 35:1720217215, 2022.",
  "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fe-davg on non-iid data. In International Conference on Learning Representations, 2020b. URL": "Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive stepsizes.In The 22nd international conference on artificial intelligence and statistics, pp. 983992. PMLR, 2019. Nicolas Loizou, Sharan Vaswani, Issam Hadj Laradji, and Simon Lacoste-Julien. Stochastic polyak step-size for sgd: An adaptive learning rate for fast convergence. In International Conference on ArtificialIntelligence and Statistics, pp. 13061314. PMLR, 2021.",
  "Yura Malitsky and Konstantin Mishchenko.Adaptive gradient descent without descent. arXiv preprintarXiv:1910.09529, 2019": "Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan Ramchandran, and Michael IJordan. Perturbed iterate analysis for asynchronous stochastic optimization. SIAM Journal on Optimiza-tion, 27(4):22022229, 2017. Brendan McMahan,Eider Moore,Daniel Ramage,Seth Hampson,and Blaise Aguera y Arcas.Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence andstatistics, pp. 12731282. PMLR, 2017. Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Linear convergence in federated learning:Tackling client heterogeneity and sparse gradients. Advances in Neural Information Processing Systems,34:1460614619, 2021.",
  "Boris T Polyak. Introduction to optimization. optimization software. Inc., Publications Division, New York,1:32, 1987": "Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konen, SanjivKumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference onLearning Representations, 2021. URL Anton Rodomanov, Xiaowen Jiang, and Sebastian Stich. Universality of adagrad stepsizes for stochasticoptimization: Inexact oracle, acceleration and variance reduction. arXiv preprint arXiv:2406.06398, 2024.",
  "i=1it[Fi(xit, it) i ] + 2Rt ,": "while the remaining terms are the same (up to a small change in the constant in front of Rt) as in the convexcase. The increased constant can be handled by imposing a slightly stronger condition on c (here we usedc 4 2, compared to c 2 2 in the convex case). The rest of the proof follows by the same steps, withone additional departure: when summing up the inequalities over the iterations t = 0 to T 1 we need touse an appropriate weighing to benefit from the linear decrease. This technique is standard in the analysis(illustrated in Stich (2019) and carried out in the setting a residual Rt in (Stich & Karimireddy, 2020, Proofof Theorem 16) and with a residual Rt in a distributed setting in (Koloskova et al., 2020, Proof of Theorem2, in particular Lemma 15).",
  "E xt+1 x2 (1 b) xt x2 b[f(xt) f ] + 4bL E[Rt] + 4L2b 2f": "(the constant in front of E[Rt] increased by a factor of two because of the estimate in (38) and can becontrolled by the slightly stronger condition on b). From there, the proof is very standard and followsexactly the template outlined in e.g. (Stich & Karimireddy, 2020, Proof of Theorem 16) or (Koloskova et al.,2020, Proof of Theorem 2, in particular Lemma 15).",
  "= 2L2f": "Gradient variance.We recall the definitions 2 =1nni=1 EiFi(x, i) fi(x)2 and 2f =1nni=1 (fi(x) i ).The global optimal point is denoted by x, local optimum on client i by xi andthe local optima of the functions on worker i by xi. Note that fi(xi ) = 0, Fi(xi, i) = 0 and we makeuse of these facts in our proof.",
  "D.1.2FedSPS-Normalized experimental details": "The implementation is done according to Algorithm 2. All remarks we made about the choice of scalingparameter c, upper bound on stepsize b, and i in the previous section on FedSPS also remain same here. shows a comparison of FedSPS and FedSPS-Normalized for the convex case of logistic regressionon MNIST dataset. For the homogeneous case (a) normalization has no effectthis is as expected sincethe client and server side correction factors are equal and balance out. We expect to observe some changesfor the heterogeneous setting (b), but here the FedSPS-Normalized actually performs very slightly worsethan FedSPS. We conclude that normalization does not lead to any significant performance gain, and canintuitively attribute this to the fact the FedSPS stepsizes have low inter-client variance ().",
  "D.2.3FedSPS-Global experimental details": "The implementation is done according to Algorithm 33. All remarks we made about the choice of scalingparameter c, upper bound on stepsize b, and i in the previous section on FedSPS also remain same here.As mentioned before, this method needs an extra hyperparameter 0, that is the stepsize across all clientsuntil the first communication round. Empirically we found that setting 0 = 00, i.e. the local SPS stepsize forclient 0 at iteration 0, works quite well in practice. This is explained by the fact that the SPS stepsizes havelow inter-client and intra-client variance (). Experimentally we find that FedSPS-Global convergesalmost identically to the locally adaptive FedSPS ( and ). 3Note that for any experiment in the Appendix that involves FedSPS-Global, we refer to FedSPS (from the main paper) asFedSPS-Local in the legends of plots, for the sake of clarity between the two approaches.",
  "i=1fi(x) f(x)22 2 .(43)": "This assumption is frequently used in the analysis of FedAvg Koloskova et al. (2020), as well as adaptivefederated methods Reddi et al. (2021); Wang et al. (2022a).The local variance denotes randomness instochastic gradients of clients, while the global variance represents heterogeneity between clients. Note that = 0 corresponds to the i.i.d. setting. We further note that Equation (43) corresponds to the varianceAssumption in Loizou et al. (2021) (their Equation (8) with = 1, = 2) that they also required for theanalysis in the non-convex setting.",
  "where T := min0tT 1 E f(xt)2 and F0 := f(x0) f": "The proof of this theorem again relies on the assumption that the stepsize is very small, but otherwise followsthe template of earlier work Li et al. (2019); Koloskova et al. (2020) and precisely recovers their result. Forthe sake of completeness we still add a proof in the Appendix, but do not claim novelty here. The theoremstates that when using a constant stepsize, the algorithms reach a neighborhood of the solution, where theneighbourhood is dependent on the local variance and global variance , and for the i.i.d. case of = 0the neighbourhood is smaller and less dependent on number of local steps . By choosing an appropriatelysmall b, any arbitrary target accuracy can be reached. A limitation of our result is that it only applies to the small stepsize regime, when the adaptivity is governedby the stepsize bound on b. However, when comparing to other theoretical work on adaptive federatedlearning algorithms we observe that related work has similar (or stronger) limitations, as e.g. both Wanget al. (2022a) (FedAMS) and Reddi et al. (2021) (FedAdam) require an uniform bound on the stochasticgradients (we do not need) and also require effective stepsizes smaller than1L similar to our case.",
  "(b) Training loss, test accuracy and stepsizes for FedSPS (non-i.i.d.)": ": Visualization of FedSPS stepsize statistics for convex case of logistic regression on MNIST dataset (i.i.d.and non-i.i.d.). Left column represents training loss, middle column test accuracy, and right column stepsize plots.intra-client refers to selecting any one client and plotting the mean and standard deviation of stepsizes for thatclient across local steps, for a particular communication round. inter-client refers plotting the mean and standarddeviation of stepsizes across all clients, for a particular communication round. We extend (a) beyond the best andworst stepsizes and report the performance for all stepszies in .",
  "(d) = 100": ": Additional experiment on the effect of varying c on convergence of FedSPS for different values of . Inpractice, there is no square law relation between c and , and any small value of c 0.5 works well. : Test accuracy for different values of l for convex case of logistic regression on MNIST dataset (i.i.d.)extending the results of (a) for multiple values of local stepsizes. The best and worst stepsize values arehighlighted in bold. We see that there is a substantial gap between the best other stepsizes.",
  "F.2Effect of varying c on convergence": "We provide additional experiments on the effect of varying the FedSPS scale parameter c on convergence,for different number of local steps {10, 20, 50, 100} (convex case logistic regression on i.i.d. MNISTdataset without client sampling) in . Similarly as before, we observe that smaller c results in betterconvergence, and any c {0.01, 0.1, 0.5} works well. Moreover, the effect of varying c on the convergence issame for all clarifying that in practice there is no square law relation between c and , unlike as suggestedby the theory.",
  "F.3Additional convex experiments": "Additional convex experiments for the rest of the LIBSVM datasets (i.i.d.) have been shown in .The first column represents the training loss and the second column the test accuracy. The third columnrepresents the FedSPS stepsize statistics as described before in Section F.1. We can make similar observationsas before in the main paperthe proposed FedSPS methods (without tuning) converge as well as or slightlybetter than FedAvg and FedAdam with the best tuned hyperparameters. The stepsize plots again highlightthe low inter-client and intra-client stepsize variance.",
  "F.4Hyperparameters tuning for FedAvg and FedAMS": "Here we provide more details on hyperparameters for each dataset and model needed by FedAvg and FedAMS.We perform a grid search for the client learning rate l {0.0001, 0.001, 0.01, 0.1, 1.0}, and server learningrate {0.001, 0.01, 0.1, 1.0}. Moreover, for FedAMS we also choose 1 = 0.9, 2 = 0.99, and the max"
}