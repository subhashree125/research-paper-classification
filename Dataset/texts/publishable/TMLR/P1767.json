{
  "This paper analyzes 1 regularized linear regression under the challenging scenario of havingonly adversarially corrupted data for training.Firstly, we prove existing deterministic": "adversarial attacks (e.g., FGSM and variants) focusing on maximizing the loss function canbe easily handled with a few samples for support recovery. Hence, we consider a more general,challenging stochastic adversary which can be conditionally dependent on uncorrupted dataand show existing models attacking support (Goodfellow et al., 2015; Madry et al., 2018) orHuber model (Prasad et al., 2020) are particular cases of our adversarial model. This enablesus to show the counter-intuitive result that an adversary can inuence sample complexityby corrupting the irrelevant features, i.e., non-support. Secondly, as any adversariallyrobust algorithm has limitations, our theoretical analysis identies that the dependence(covariance) between adversarial perturbations and uncorrupted data plays a critical rolein dening the regimes under which this challenging adversary or Lasso can dominate overeach other. Thirdly, we derive a necessary condition for support recovery for any algorithm(not restrictive to Lasso), which corroborates our theoretical ndings for Lasso. Fourthly, weidentify the fundamental limits and address critical scientic questions of which parameters(i.e., mutual incoherence, the maximum and minimum eigenvalue of the covariance matrix,and the budget of adversarial perturbations) play a role in the high or low probability ofsuccess of the Lasso algorithm. Also, the derived sample complexity is logarithmic withrespect to the size of the regression parameter vector. Our theoretical claims are validatedby empirical analysis.",
  "Introduction": "A well-known instance of the failure of machine learning (ML) models is when they are confronted withadversarial attacks. The vulnerability of ML models to possibly small perturbations imperceptible to thehuman eye such as one-pixel attacks (Su et al., 2019) may produce inaccurate predictions with high condence(Szegedy et al., 2014; Goodfellow et al., 2015; Madry et al., 2018). Hence, demystifying empirical failure withtheoretical analysis to design certiable learning algorithms has been an active area of research recently. Inthis work, we consider the support recovery problem under adversarial attacks and theoretically analyze thebehavior of the widely accepted Lasso algorithm. We begin by providing theoretical evidence of the limitations of existing adversarial attack models in thecontext of the support recovery problem. We propose a more comprehensive and demanding adversarialmodel that addresses these limitations. Existing gradient descent-based methods, such as FGSM (Goodfellowet al., 2015) and various others (Madry et al., 2018; Szegedy et al., 2014; Xing et al., 2021; Yin et al., 2019;Awasthi et al., 2020; Qin et al., 2021) target the support (the entries corresponding to non-zero coecients)",
  "Published in Transactions on Machine Learning Research (12/2024)": "where y(j) R, x(j) Rp, w Rp, and (x(j)) is the adversarial perturbation which can be anysub-Gaussian random variable possibly dependent on x(j) (See Appendix E.3 for an example). Let S = S(w)denote the support of w, i.e., the set of indices corresponding to non-zero entries of w. Let k = |S| denotethe cardinality of the support, and hence p k = |Sc|. We argue that our proposed model is more generalthan other existing models in the literature, including the Huber model in Lemma 3. Our theoretical andempirical analysis (Appendix E.4) draws interesting inferences using this model. We assume only adversariallycorrupted data, that is, n independent samples of",
  "samples,": "whereas existing attacks only need (log(p)) samples (in .2). Here, k denotes the number of elementsin the support. Furthermore, it is worth noting that under a specic condition, our adversary can be sopowerful that achieving successful support recovery becomes impossible, even with innite samples. The attack in existing methods is just one possible attack model that ts our assumptions; hence, ouradversarial model is more general. For instance, approaches by Goodfellow et al. (2015); Madry et al. (2018);Szegedy et al. (2014); Xing et al. (2021); Yin et al. (2019); Schmidt et al. (2018); Awasthi et al. (2020);Qin et al. (2021) consider deterministic attacks within an -ball, which is sub-Gaussian since any boundedrandom variable is known to be sub-Gaussian. Also, note that stochastic attacks are more general as anydeterministic value is a random variable with a Dirac delta probability density function. Furthermore, it isworth noting that the -Huber model (Prasad et al., 2020; Diakonikolas et al., 2019) corrupting only < 1fraction of samples is a particular case of our adversarial model. In the presence of a more challenging adversary, our analysis (as demonstrated in Lemma 12) reveals anintriguing aspect: the adversary can impact the sample complexity by manipulating not only the relevantfeatures or support (i.e., entries corresponding to non-zero coecients of the regression parameter vector)but also the irrelevant features or non-support (i.e., entries corresponding to zero coecients). This is acounter-intuitive result as the existing methods (Madry et al., 2018; Szegedy et al., 2014; Xing et al., 2021;Yin et al., 2019; Awasthi et al., 2020; Qin et al., 2021) attack only the support (Lemma 6). We also addresscritical scientic questions of which parameters (i.e., mutual incoherence (after Eq (8)), the maximum andminimum eigenvalue (Lemma 19) of the covariance matrix, and the budget of adversarial perturbations (Eq(15))) play a role in the high or low probability of success of Lasso. The adversary can also increase the samplecomplexity by designing the covariance matrix of perturbations in the support such that its eigenvectorcorresponding to the maximum eigenvalue is parallel to the regression parameter vector (discussed after Eq(15)). We derive a necessary condition for support recovery that applies to any algorithm, extending beyond thescope of Lasso. Notably, we observe that this condition aligns with our separate analysis of the Lassoalgorithm, strengthening our ndings in that context. With slight abuse of terminology, both conditionsessentially indicate that support recovery is impossible when the absolute value of the coecient in theparameter vector (for an entry in the support) is not large enough as compared to the entries in the covariancematrix between the adversarial perturbation and the uncorrupted data . This aspect of our main result inTheorem 10 is discussed in .2 and veried empirically. Please refer to for a demonstrationof possible and impossible support recovery scenarios in the cases of b = 0 and b = 0 respectively. For moredetailed experiments on the three cases of the regularization parameter, in Theorem 10, characterizingpossible and impossible support recovery, please refer to Appendix E.4. Some initial attempts have been made in the sparse regression literature (Herman & Strohmer, 2010), butwe believe that the particular theoretical framework did not allow prior work to make important ndingssuch as noting that when adversarial perturbations correlate with uncorrupted data, then support recoveryis impossible. In addition, there is prior literature in robust optimization (Bertsimas & Den Hertog, 2020),where there is uncertainty in the input coecients of an optimization problem. Their goal is to provideguarantees that the unperturbed optimal solution is still feasible (although not optimal) in the problem withperturbed inputs. Bertsimas & Den Hertog (2020) treats canonical problems such as linear programming,for instance. While we also focus on an optimization problem, Bertsimas & Den Hertog (2020) provides a",
  "Notation.We use a lowercase letter, e.g., a to denote a scalar, a lowercase bold letter such as a to denote a": "vector, and an uppercase bold letter such as A to denote a matrix. A vector 1m or 0m represents a vector ofones or zeros respectively, of size m. We denote a set with a calligraphic letter, e.g., P. Also, [n] denotes theset {1, 2, . . . , n}. For a vector, ai denotes the ith entry of the vector a. For a matrix A Rpq, we representthe sub-matrix with rows P [p] and columns Q [q] as APQ R|P||Q|.",
  "1p . Similarly ||A|| denotes": "the entrywise norm of a matrix A and ||A||2 denotes the spectral norm. We denote the induced norm using |||A||| = maxi[m] ||ai||1 for a matrix A Rmk, where ai denotes its ith row. Minimum andmaximum eigenvalues of a matrix A are denoted by min (A), max (A). A function f(m) = (g(m)) impliesthat there exists a constant c1 such that f(m) c1g(m), m m0. Similarly, f(m) = O(g(m)) denotes thatthere exists a constant c2 such that f(m) c2g(m), m m0. For a vector w Rp, S(w) = {i [p], wi = 0}denotes the support of w and similarly Sc(w) = [p] \\ S(w) denotes the non-support.",
  "for j [n] are available for training, which": "makes the problem challenging as compared to existing works which assume availability of uncorruptedfeatures x(j). Let X Rnp be the collection of n samples x(j) for j [n]. For brevity, we may dropthe superscript (j) later. Let the population covariance matrix of X be denoted by x. We assume thatx",
  "e": "Adversarial perturbations are typically bounded by some budget for each sample (Zhai et al., 2019; Baldaet al., 2019; Yin et al., 2019; Cohen et al., 2019) so that the underlying model is learnable. We model thisxed budget as a parameter in our analysis, and hence, we provide performance guarantees as a function ofthis parameter. We choose adversarial perturbations to be sub-Gaussian random variables, which generalizesthe regime of bounded perturbations. Adversarial perturbations can also be dependent on the uncorruptedregressors, x for each sample. To clarify, (x(i)) can be dependent on x(i), where i denotes the ith sample,but (x(i)) is independent of x(j), if i = j, where j denotes another sample. Note that the adversary doesnot have control over the uncorrupted regressors x, but has access to x, which can be used to design (x)in an arbitrary manner (that may make the support recovery problem more challenging). Assumption 1. (Adversarial Perturbation) (x) is a zero-mean sub-Gaussian random vector with pa-rameters (, r2), which can be conditionally dependent on uncorrupted data. Here is the populationcovariance matrix of the adversarial perturbation ((x)) and r2 is the variance proxy parameter for somer R.",
  "Existing attacks are special cases of our adversarial model": "Note that a random variable is more general than a deterministic one, i.e., a deterministic quantity is arandom variable with a Dirac delta probability density function. Hence, existing deterministic adversarialattacks (Goodfellow et al., 2015; Madry et al., 2018; Szegedy et al., 2014; Xing et al., 2021; Yin et al., 2019;Awasthi et al., 2020; Qin et al., 2021) are a special case of our adversarial model specied in Assumption 1. Lemma 2. Existing adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Szegedy et al., 2014;Xing et al., 2021; Yin et al., 2019; Awasthi et al., 2020; Qin et al., 2021) are a special case of our adversarialmodel specied in Assumption 1.",
  "(xSc) as well, to make the learning task of estimating S and Sc tougher, which is formalized in the followinglemma": "Lemma 5. If the adversary attacks only the support entries (xS) or the non-support entries (xSc) withnon-zero-mean adversarial perturbation, then the learner can guess the support trivially with probability atleast 1 O (1/p) if n = (log(p)). The proof of the above lemma relies on the fact that the sample mean of feature vector x(j) for j [n] willbe close to the population mean. If only the support is attacked, the sample mean is away from zero forentries in the support, and is zero for entries in the non-support. Hence, the learner can guess the support byjust computing the sample mean. Lemma 6. The support can be trivially estimated in n = (log(p)) samples with high probability underexisting adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018; Szegedy et al., 2014; Xing et al.,2021; Yin et al., 2019; Awasthi et al., 2020; Qin et al., 2021) aiming to maximize a per-sample loss function.",
  "nX|X and the population covariance matrix be denoted by x. For the uniqueness": "of the solution to the problem stated in Eq. (2), we need a submatrix of the sample covariance matrix tobe positive denite. But as X is assumed to be random, we assume the population covariance matrix ofX to be positive denite as done in prior literature (Wainwright, 2009; Ravikumar et al., 2011; 2010; 2009;Daneshmand et al., 2014).",
  "SS, is Dmin 0. The minimum eigenvalue of [x + x]SS/2": "is denoted by Fmin, which inuences the sample complexity as discussed in Lemma 13. Similarly, the maximumeigenvalues of matrices {x, , x} are denoted by {Cmax, Dmax, Fmax}, whose inuence on samplecomplexity can be seen in Lemma 15. Further, we make the assumption of mutual incoherence, which impliesthat the regressors in the non-support Sc do not have a strong correlation with regressors in the support S.",
  "recovery, even in the population regime, where (intuitively speaking) the learner has access to an innitenumber of samples": "In the above lemma, we prove the necessary condition by considering a case where support recovery isimpossible if the required condition is disobeyed. The proof carefully constructs a specic case where theuncorrupted data as well as the adversarial perturbations are both Gaussian, and then uses the fact thatGaussian distributions are fully identiable from its rst and second order moments to argue for impossibility. With slight abuse of terminology, the above lemma implies that the coecients in the parameter vector mustbe suciently large compared to the adversarial budget for successful detection.With a brief discussion of",
  "log(p)/n": "The rst two claims of the Theorem 10 imply that we can uniquely recover the true support with highprobability, assuming we have a sucient number of samples if we choose a regularization parameter greaterthan a certain threshold. Note that choosing a very large value of is not desirable as it would also increasethe upper bound for || wS w",
  "condition for all methods": "In statement 5 with b = 0, the situation mentioned above can be avoided by increasing the value of nappropriately despite any eorts from the adversary. This helps us to identify dierent regimes under whichwe can provide theoretical guarantees of Lasso for successful support recovery under adversarial attacks andalso the case which may be favorable to the adversary (See .2).",
  "the non-zeros, i.e., wi = 0 for all i S(w)": "Remark 11. For clarity of exposition, we focus on sub-Gaussian data. Our proofs can be extended toheavy-tailed data by using m-order moment concentration instead of sub-Gaussian concentration, as donein (Ravikumar et al., 2011) for another machine learning problem. This will lead to a sample complexitypolynomial in p, instead of logarithmic in p.",
  "l(( wS, 0)) + z = 0p1(4)": "where z || w||1 belongs to the sub-dierential set of the 1 norm at w. In the context of the primal-dualwitness framework (Ravikumar et al., 2010; 2011; 2009; Daneshmand et al., 2014), wS and z are referred asthe primal and dual variables respectively. As z belongs to the sub-dierential set of the 1 norm, we canclaim that ||z|| 1 by norm duality but for strict dual feasibility we need ||zSc|| < 1 as stated in Lemma1 of (Wainwright, 2009). In order to ensure this condition, we need to rst derive zSc from the rst orderstationary condition in Eq. (4) which is a pdimensional vector equation and can be written for elements inS and Sc separately to derive zSc. The nal expression is (See Appendix D.1 for more details):",
  "------": "This carefully constructed decomposition of zSct1 has given us the freedom to study the eect of the adversarialperturbation on the non-support entries and the support entries by analyzing m1 and m2 respectively. Theterm m4 in Eq. (7) can be bounded using the mutual incoherence Assumption 8. We propose Lemma 12 andLemma 13 to bound the terms m1, m2, and m3 as discussed below.",
  "------ increases, then the learner is": "forced to choose a smaller value of in Lemma 12 for the adversarial perturbation in Sc which increases thesample complexity. This also demonstrates the counterintuitive point that an adversary can inuence thesample complexity by an attack on the non-support (See Lemma 6). The next step is to bound the term m2in Eq. (7) using the following lemma.Lemma 13. If n =",
  "V": "Hence, in order to bound the trace of M(), we focus on the eigenvalues of V. We observe that V is amatrix with only two non-zero eigenvalues. This helps us to derive improved bounds as compared to the caseof all 2k eigenvalues being non-zero. More details can be seen in Appendix D.5.",
  "+ /16 + /8 + /16 = 1 3/4(12)": "In this sub-section, we derived an upper bound for the innity norm of zSct1 which will be used later to boundthe innity norm of zSc dened in Eq. (5) to ensure strict dual feasibility. More importantly, our analysisalso sheds light on the dependence of the sample complexity on parameters such as mutual incoherence,minimum eigenvalue, and other constants like Gmax and Hmax, which helps us to study critical scienticlimitations or behavior of the Lasso algorithm under adversarial attacks.",
  "Choosing regularization parameter ()": "In this subsection, we continue the discussion on strict dual feasibility and focus on how the adversary aectsthe regularization parameter. We start from zSct2 in Eq. (6), which is a (p k) dimensional random vector.Using properties of norms, whose details are mentioned in Appendix D.6 and using the bound derived in Eq.(12), we arrive at:",
  "< 1(16)": "In this sub-section, we have veried the strict dual feasibility condition by proving that ||zSc|| < 1 as > 0in the above equation. This ensures that KKT conditions are met, which proves the rst claim of Theorem10, i.e., S( w) S(w). It should be noted that we derive the lower bound constraint on in order to provide",
  "= 0 specied in Lemma 9 holds true for this case": "In this subsection, we completed the proof of Theorem 10 and discussed the critical regimes which may befavorable to the adversary or the learning algorithm. We also discussed the counter-intuitive result of howthe adversarial perturbation in Sc can aect the guarantees for wS indirectly by inuencing the lower boundon the regularization parameter.",
  "Concluding Remarks": "Sparsity, support recovery and weight recovery are tightly related concepts. In our paper, support relates tothe zero/nonzero weights in the sparse regression vector (nonzero entries corresponding to relevant featuresfor prediction) and weight recovery relates to estimating a vector that is close (in distance) to the trueregression vector (See e.g., Theorem 10, Eq. (3)). Our initial analysis for sparse regression already highlights some fundamental issues that not only pertain tosample complexity, e.g., impossibility. Our initial results could be later extended to other machine learningproblems where sparsity as well as support and weight recovery is relevant. This includes for instance,Gaussian graphical models (Ravikumar et al., 2011), Ising models (Ravikumar et al., 2010), non-parametricregression (Ravikumar et al., 2009) and diusion networks (Daneshmand et al., 2014). Besides the aforementioned models, we believe our results could potentially motivate future work on neuralnetworks. Indeed, there has been a recent interest on sparse neural networks (Liu & Wang, 2023) as well asweight recovery for two-layer neural networks (Bakshi et al., 2019)."
}