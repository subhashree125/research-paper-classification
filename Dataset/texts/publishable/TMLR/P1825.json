{
  "Abstract": "Large-width asymptotic properties of neural networks (NNs) with Gaussian distributedweights have been extensively investigated in the literature, with major results characterizingtheir large-width asymptotic behavior in terms of Gaussian processes and their large-widthtraining dynamics in terms of the neural tangent kernel (NTK). In this paper, we studylarge-width asymptotics and training dynamics of -Stable ReLU-NNs, namely NNs withReLU activation function and -Stable distributed weights, with (0, 2). For (0, 2],-Stable distributions form a broad class of heavy tails distributions, with the special case = 2 corresponding to the Gaussian distribution. Firstly, we show that if the NNs widthgoes to innity, then a rescaled -Stable ReLU-NN converges weakly (in distribution) toan -Stable process, which generalizes the Gaussian process. As a dierence with respectto the Gaussian setting, our result shows that the activation function aects the scalingof the -Stable NN; more precisely, in order to achieve the innite-width -Stable process,the ReLU activation requires an additional logarithmic term in the scaling with respectto sub-linear activations. Secondly, we characterize the large-width training dynamics of-Stable ReLU-NNs in terms an innite-width random kernel, which is referred to as the-Stable NTK, and we show that the gradient descent achieves zero training error at linearrate, for a suciently large width, with high probability. Dierently from the NTK arisingin the Gaussian setting, the -Stable NTK is a random kernel; more precisely, the random-ness of the -Stable ReLU-NN at initialization does not vanish in the large-width trainingdynamics.",
  "Introduction": "There exists a vast literature on the interplay between Gaussian processes and the large-width asymptoticbehaviour of Gaussian neural networks (NNs), namely NNs with Gaussian distributed weights (Neal, 1996;Der and Lee, 2006; Garriga-Alonso et al., 2018; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2018;Yang, 2019;a;b; Bracale et al., 2021; Eldan et al., 2021; Klukowski, 2022; Yang and Hu, 2021; Basteri andTrevisan, 2022; Favaro et al., 2023; Hanin, 2023; Trevisan, 2023; Hanin, 2024). To dene a Gaussian NN,consider the following elements: i) for d, k 1 let X be a d k NNs input, such that xj = (xj1, . . . , xjd)T isthe j-th input (column vector); ii) let be an activation function; iii) for m 1 let W = (w(0)1 , . . . , w(0)m , w)be the NNs weights, such that w(0)i= (w(0)i1 , . . . , w(0)id ) and w = (w1, . . . , wm) with the w(0)ij s and the wis",
  "i=1wi(w(0)i, xj)j = 1, . . . , k": "Neal (1996) rst investigated the large-width behaviour of fm(W, X, ), which follows by a straightforwardapplication of the central limit theorem (CLT). In particular, it is well-known that if m +, thenthe rescaled Gaussian -NN m1/2fm(W, X, ) converges weakly (or in distribution) to a Gaussian processwith covariance function X, such that X,[r, s] = 2E[(w(0)i, xr(w(0)i, xs]. Some extensions of thisinnite-width limit are available for deep NNs (Matthews et al., 2018), more general NNs architectures(Yang, 2019a;b), and innite-dimensional inputs (Bracale et al., 2021; Eldan et al., 2021; Favaro et al.,2023). The large-width training dynamics of Gaussian NNs has been also extensively investigated in the literature,with the training being performed through the gradient descent (Jacot et al., 2018; Arora et al., 2019; Duet al., 2019; Lee et al., 2019). In particular, consider the Gaussian ReLU-NN fm(W, X) = fm(W, X, ReLU),and setfm(W, X) :=1",
  "m1/2 fm(W, X)": "Let (X, Y ) be the training set, where Y = (y1, . . . , yk) is the (training) output such that yj correspondsto the j-th input xj. By considering a random initialization W(0) for the NNs weights, and assuming asquared-error loss, the gradient ow of W(t) leads to the training dynamics of fm(W(t), X), that is for anyt 0d fm(W(t), X)",
  "dt= ( fm(W(t), X) Y )mHm(W(t), X),(2)": "where m > 0 is the learning rate, and Hm(W(t), X) is a k k random matrix whose (j, j) entry is fm(W(t), xj)/W, fm(W(t), xj)/W. By assuming m = 1, Jacot et al. (2018) rst characterized thelarge-width training dynamics of fm(W(t), X), showing that: i) if m + then Hm(W(0), X) convergesin probability to a deterministic matrix H(X, X); ii) the gradient descent achieves zero training error atlinear rate, i.e.Y fm(W(t), X)22 exp(0t)Y fm(W(0), X)22 for m suciently large, with high probability. The limiting matrix H(X, X) is refereed to as the neuraltangent kernel (NTK). See Yang (2019) and Yang and Littwin (2021) for extensions to deep NNs and generalarchitectures.",
  "Our contributions": "In this paper, we study large-width asymptotics and training dynamics of -Stable ReLU-NNs, namely NNswith a ReLU activation function and -Stable distributed weights. For (0, 2], -Stable distributionsform a broad class of heavy tails distributions, with the special case = 2 corresponding to the Gaussiandistribution; see Samoradnitsky and Taqqu (1994) and references therein for an overview on -Stable dis-tributions. According to the denition (1), we denote by fm(W, X, ; ) the -Stable -NN, namely a NNof the form (1) with the weighs W distributed according to the -Stable distribution with (0, 2), thusexcluding the Gaussian case = 2. In particular, fm(W, X; ) = fm(W, X, ReLU; ) denotes the -StableReLU-NN.",
  "Related work": "Neal (1996) considered -Stable distributions to initialize NNs weights, showing that while all Gaussianweights vanish in the innite-width limit, some -Stable weights retain a non-negligible contribution. Sucha dierent behaviour may be attribute to the diversity of the NNs path properties as (0, 2] varies, whichmakes -Stable NNs more exible than Gaussian NNs; see . Further works demonstrating practical",
  "Published in Transactions on Machine Learning Research (11/2024)": "Proof. Since the distribution of H2() is absolutely continuous in the space of symmetric positive semi-denite matrices then we can write that P(det( H2()) = 0) = 0. Moreover, since H2() is positive semi-denite, then P(min( H2()) > 0) = 1. Thus, for every > 0, the exists 2 > 0 such that P(min( H2()) >2) > 1 . Now, we are in the position of proving Proposition 3.1. Let > 0 be a xed number. By Lemmas A.5and A.7, there exist 1 > 0 and 2 > 0 such that, for i = 1, 2, P(min( Hi ()) > i) 1 /2. Since theminimum eigenvalue map is continuous with respect to Frobenius norm then, by Portmanteau theorem, fori = 1, 2,lim infmP(min( H(i)m (W(0), X)) > i) P(min( Hi ()) > i) 1 /2. Let 0 = 1 + 2. Since the minimum eigenvalue of a sum of symmetric, positive semi-denite matricesis greater than or equal to the sum of the eigenvalues of the two matrices (see Horn and Johnson (1985)Theorem 4.3.1), then we can write that",
  "Large-width asymptotics": "We extend the main results of Favaro et al. (2020; 2021) to the ReLU activation function, which is arguablyone of the most popular activation function in the eld of NNs. In particular, we show that if m +, thenthe rescaled -Stable ReLU-NN (m log m)1/fm(W, X; ) converges weakly to an -Stable process. ForNNs with a single input, i.e. k = 1, the large-width limit follows by a direct application of the generalizedCLT for heavy tails distributions (Uchaikin and Zolotarev, 2011; Bordino et al., 2022), whereas for k > 1it requires to develop an alternative strategy that may be of independent interest in the context of multidi-mensional -Stable distributions (Samoradnitsky and Taqqu, 1994, Chapter 1 and Chapter 2). Dierentlyfrom the Gaussian setting, the large-width asymptotic behaviour of -Stable NNs shows how the choice ofthe activation function aects the scaling of the NN. More precisely, in order to achieve the innite-width-Stable process, the use of the ReLU activation in place of a sub-linear activation results in a change ofthe scaling m1/ of the NN through the additional (log m)1/ term. See also Bordino et al. (2022) fora detailed discussion of this peculiar phenomenon in the context of -Stable ReLU-NN with a single input(k = 1).",
  "(m log m)1/ fm(W, X; ).(3)": "In analogy with (2), we dene the training dynamics of fm(W(t), X; ), with a learning rate m and a k krandom matrix Hm(W(t), X; ) whose (j, j) entry is fm(W(t), xj; )/W, fm(W(t), xj; )/W. Byassuming the learning rate m = (log m)2/, we show that: i) if m + then (log m)2/Hm(W(0), X; )converges weakly to an (/2)-Stable (almost surely) positive denite random matrix H(X, X; ); ii) andfor every > 0 the gradient descent achieves zero training error at linear rate, for m suciently large,with probability 1 .The limiting random matrix H(X, X; ) is refereed to as the -Stable NTK.Dierently from the NTK that arises from the Gaussian setting, the -Stable NTK is a random kernel.More precisely, the randomness of the -Stable ReLU-NN at initialization does not vanish in the large-widthtraining dynamics.",
  "Organization of the paper": "The paper is organized as follows. In we characterize its large-width asymptotic behaviour of-Stable ReLU-NNs in terms of the innite-width -Stable process. In we characterize the large-width training dynamics of -Stable ReLU-NNs in terms of the -Stable NTK, and we show that the gradientdescent achieves zero training error at linear rate, for a suciently large width, with high probability. contains a discussion of our results with respect to some directions of future work. Proofs are deferred tothe appendix.",
  "Large-width asymptotics of -Stable ReLU-NNs": "We study the large-width asymptotic behaviour of -Stable ReLU-NNs. The section is organized as follows:i) we recall the denition of multidimensional -Stable distribution (.1); ii) we dene the -StableReLU-NN and characterize its large-width asymptotic behaviour in terms of the innite-width -Stableprocess (.2); iii) we present some numerical illustrations of the large-width behaviour of -StableReLU-NNs (Section ??). The main result of this section is Theorem 2.1, whose proof is deferred to AppendixA.1.",
  "Multidimensional -Stable distribution": "For (0, 2], a random variable S R is distributed according to a symmetric and centered 1-dimensional-Stable distribution with scale > 0 if its characteristic function is E(exp{izS}) = exp {|z|}, andwe write S St(, ). If the stability parameter = 2 then S is distributed as a Gaussian distributionwith mean 0 and variance 2. Let Sk1 be the unit sphere in Rk, with k 1, and let be a symmetricnite measure on Sk1. For (0, 2], we say that a random variable S Rk is distributed according toa symmetric and centered k-dimensional -Stable distribution with spectral measure if its characteristicfunction is",
  "We deal mostly with k-dimensional -Stable distributions with discrete spectral measure, that is () =": "1in isi() with n N, i R and si Sk1, for i = 1, . . . , n (Samoradnitsky and Taqqu, 1994,Chapter 2). All the random variables are dened on a common probability space, say (, F, P), unlessotherwise stated. We make use of the following characterization of the spectral measure of -stable distributions (Samoradnit-sky and Taqqu, 1994, Chapter 2): if S Stk(, ), then for every Borel set B of Sk1 such that (B) = 0,",
  "The innite-width -Stable process": "To dene a generic ReLU NN, let us consider the following elements: i) for d, k 1 let X be the d kNNs input, such that xj = (xj1, . . . , xjd)T is the j-th input (column vector); ii) for m 1 let W =(w(0)1 , . . . , w(0)m , w) be the NNs weights, such that w(0)i= (w(0)i1 , . . . , w(0)id ) and w = (w1, . . . , wm). A ReLU-NN of width m isfm(W, X; ) = (fm(W, x1; ), . . . , fm(W, xk; )),(6)",
  "i=1wiw(0)i, xjI(w(0)i, xj > 0)j = 1, . . . , k,": "with I() being the indicator function. We denote by W(0) = (w(0)1 (0), . . . , w(0)m (0), w(0)) the NNs weightsat random initialization. If the NNs weights w(0)ij s and the wis are initialized as i.i.d. -Stable randomvariables, with (0, 2) and > 0, then fm(W(0), X; ) denes an -Stable ReLU-NN of width m.",
  ",": "where Bu = {v Rd : v, xj > 0 if uj = 1, v, xj 0 if uj = 0, j = 1, . . . , k}, ei is a d-dimensional vectorsatisfying eij = 1 if j = i, and eij = 0 if j = i (i, j = 1, . . . , d), and C/2 is the constant dened in (4).",
  "m1/ fm(W, X, ; )w f(X),(7)": "with f(X) being an -Stable process with spectral measure X,. Theorem 2.1 extends Favaro et al. (2021,Theorem 1.2) to the ReLU activation function. Theorem 2.1 shows that the use of the ReLU activationin place of a bounded activation results in a change of the scaling m1/ in (7), through the inclusion ofthe (log m)1/ term. This is a critical dierence between the -Stable setting and Gaussian setting, asin the latter the choice of the activation function does not aect the scaling m1/2 required to achievethe innite-width Gaussian process. For k = 1, we refer to Bordino et al. (2022) for a detailed analysis ofinnitely wide limits of -Stable NNs with general classes of sub-linear, linear and super-linear activationfunctions. Remark 2.1. The need of the additional log(m) can be claried by considering the -Stable ReLU-NNwith a single input, i.e. k = 1, where the proof of Theorem 2.1 reduces to a straightforward application ofthe generalized CLT for heavy tails distributions (Uchaikin and Zolotarev, 2011; Bordino et al., 2022). Inparticular, we refer to Theorem 2.1. and Theorem 2.6 of Bordino et al. (2022), which show how the log termarises from the tail behaviour of the product of -Stable random variable wiw(0)is, which denes the NN; seeCline (1986) and references therein. The log term is expected to hold for any activation that has a lineargrowth. To demonstrate numerically Theorem 2.1, we sample random neural networks according to 3 for variousvalues of width m and stability index . We evaluate these networks on a ne uniform grid of points in2. displays the results, which show that the function samples remain well-behaved as m growslarger.",
  "Large-width training dynamics of -Stable ReLU-NNs": "We study the large-width training dynamics of -Stable ReLU-NNs. The section is organized as follows: wedene the training dynamics of the -Stable ReLU-NN and characterize its large-width asymptotic behaviourin terms of the -Stable NTK (.1); ii) we show that the gradient descent achieves zero trainingerror at linear rate, for a suciently large width, with high probability (.2). The main results ofthis section are Theorem 3.1 and Theorem 3.2, whose proofs are deferred to Appendix A.2 and AppendixA.4, respectively.",
  "where H(X, X; ) is a k k random matrix that is positive semi-denite and distributed according to an(/2)-Stable distribution with spectral measure = 1 + 2.H(X, X; ) is refereed to as the -StableNTK": "Sketch of the proof of Theorem 3.1. We can see ( H(1)m (W(0), X), H(2)m (W(0), X)) as a random vector ofdimension 2k2, with k 1, whose elements are sums of independent and identically distributed randomvectors. The proof relies on the analysis of the tail behavior of these summands, and it exploits a charac-terization of the multivariate -Stable distribution as limiting distribution of the sum of independent andidentically distributed random vectors that exhibit specic tail properties. We refer to Appendix A.2 for thedetails. It turns out that the (/2)-Stable distributions of the limiting random matrices H1() and H2() areabsolutely continuous in suitable subspaces of the space of symmetric and positive semi-denite matrices;see Lemma A.4 and Lemma A.5 for details on the distribution of the random matrix H1(), and LemmaA.6 and Lemma A.7 for details on the distribution of the random matrix H2(). This is applied in thenext theorem to show that the minimum eigenvalues of H(1)m (W(0), X) and of H(2)m (W(0), X) are boundedaway from zero, uniformly in m, for m suciently large, with arbitrarily high probability. Accordingly,the minimum eigenvalue of Hm(W(0), X) = H(1)m (W(0), X) + H(2)m (W(0), X) is bounded away from zero,uniformly in m, for m suciently large, with arbitrarily high probability. We denote by min() the minimumeigenvalue. Proposition 3.1. For any (0, 2), let Hm(W, X), H(1)m (W, X) and H(2)m (W, X) be the random matricesas in Theorem 3.1. For every > 0 there exist strictly positive numbers 0, 1 and 2 such that, for msuciently large,min( H(i)m (W(0), X)) > ii = 1, 2,",
  "Zero training error at linear rate": "Under the training dynamics (8), we show that for every > 0 the gradient descent achieves zero trainingerror at linear rate, for m suciently large, with probability 1 . In order to prove this result we combineProposition 3.1 with the next proposition, which shows that, if m is suciently large, then with highprobability the minumum eigenvalue of the random matrix Hm(W(t), X) remains bounded away from zero.We denote by F and 2 the Frobenius and operator norms of symmetric and positive semi-denitematrices, respectively. Proposition 3.2. Let (0, 1) and c > 0. For k 1 let the NNs inputs x1, . . . , xk be linearly independentand such that xj = 1. For any (0, 2), let Hm(W, X) and H(2)m (W, X) be the random matrices as inTheorem 3.1. For every > 0 the following properties hold for every t 0, with probability at least 1 ,for m suciently large:",
  "In the next theorem we summarize the main nding on the large-width training dynamics of -Stable ReLUNNs": "Theorem 3.2. For k 1 let the NNs inputs x1, . . . , xk be linearly independent and such that xj = 1.For any (0, 2), under the training dynamics (8), if the learning rate m = (log m)2/ then for every > 0 there exists 0 > 0 such that, for m suciently large and any t > 0, with probability at least 1 itholds true thatY fm(W(t), X; )22 exp(0t)Y fm(W(0), X; )22.",
  "Discussion": "In this paper, we investigated large-width asymptotics and training dynamics of -Stable ReLU-NNs, namelyNNs with a ReLU activation function and -Stable distributed weights. With regards to the large-widthasymptotics, our result (Theorem 2.1) extends the main result of Favaro et al. (2020; 2021) to the ReLUactivation function, showing the need of an additional logarithmic term in the scaling of the NN to achieve theinnite-width -Stable process. With regards to the large-width training dynamics, our results (Theorem 3.1and Theorem 3.2) extends some of the main results of Jacot et al. (2018) to -Stable ReLU-NNs, showingthat randomness of the -Stable ReLU-NN at initialization does not vanish in the large-width trainingdynamics. It remains open to establish a large-width equivalence between training an -Stable ReLU-NN and performinga kernel regression with the -Stable NTK. For Gaussian NN, Jacot et al. (2018) showed that during trainingt > 0, if m is suciently large then the uctuations of the squared Frobenious norm Hm(W(t), X) Hm(W(0), X)2F are vanishing. This suggested to replace mHm(W(t), X) with the NTK H(X, X) in thedynamics (2), and writedf (t, X)",
  "dt= (f (t, X) Y )H(X, X)": "This is the dynamics of a kernel regression under gradient ow, for which at t + the prediction fora generic test point x Rd is of the form f (x) = Y H(X, X)1H(X, x)T . In particular, the predictionof the Gaussian NN fm(W(t), x) at t +, for m suciently large, is equivalent to the kernel regressionprediction f (x) (Arora et al., 2019). Within the -Stable setting, it is not clear whether the uctuationsof Hm(W(t), X) = H(1)m (W(t), X) + H(2)m (W(t), X) during the training vanish, as m . Proposition 3.2shows that the uctuations of H(2)m (W(t), X) vanish, as m . Such a result is based on the fact that forevery > 0 it holds that",
  "j=1w(l)i,jf (l1)j(X, m)I(f (l1)j(X, m) > 0)": "with f (1)i,m(X; ) = f (1)i(X; ), is a deep -Stable ReLU-NN of depth D and width m. If the NNs widthgrows sequentially over the NNs layers, i.e. m + one layer at a time, it is easy to extend Theorem 2.1to f (l)i,m(X; ). Under the same assumption on the growth of m, we expect the analysis of the large-widthtraining dynamics to follow along lines similar to that of Theorem 3.1 and Theorem 3.2, though computationsmay be more involved. A more challenging task would to extend our results to deep -Stable ReLU-NNsunder the assumptions that the NNs width grows jointly over the NNs layers, i.e. m + simultaneouslyover the layers The authors wish to thank the Action Editor, Professor Murat A. Erdogdu, and three anonymous Refereesfor their helpful suggestions.Stefano Favaro was funded by the European Research Council under theHorizon 2020 research and innovation programme, grant 817257. Stefano Favaro also gratefully acknowledgesupport from the Italian Ministry of Education, University and Research, Dipartimenti di Eccellenza\" grant2023-2027.",
  "C1(B),": "for every Borel set B of Sk1 such that 1(B) = 0 (see Appendix B). Let T : Sk1 k and C :Rk \\ {0} Sk1 be dened as T(u) = [u, xjI(u, xj > 0]j and C(v) = v/v, respectively. Fix a Borelset B of Sk1 such that 1(B) = 0. This condition implies that",
  "PuT H1()u 0 lim supmPuT H(1)m u 0= 1": "Let A be a countable dense subset of Sk1. Then, with probability one, aT H1()a 0 for every a A. Bycontinuity, this implies that the same property holds true with probability one for every u Sk1, whichproves that H1() is almost surely positive semi-denite. By eventually modifying H1() on a null set, weobtain a positive semi-denite random matrix.",
  "as r +. The proof that H2() is positive semi-denite can be done by following the same line ofreasoning as in the proof of Lemma A.1": "Lemma A.3. As m +, the probability distribution of ( H(1)m , H(1)m ) converges weakly to the law ofindependent stable random matrices, with spectral measures 1 and 2 as in (14) and (15), respectively. Proof. Since H(1)mand H(2)mconverge marginally to /2-stable random matrices, by the properties of themultivariate stable distributions it is sucient to show that they converge to stochastically independentrandom matrices. By Theorem B.1, we know that",
  "To simplify the notation, we set in this section:w := w(0), w(0) := w(0)(0), W := W(0),H(1)m:=H(1)m (W(0), X) and H(2)m := H(2)m (W(0), X), with H(1)m (W, X) and H(2)m (W, X) dened in (12) and (13)": "From (11), Hm(W(0), X)) is the sum of two positive semi-denite random matrices, H(1)mand H(2)m . Thefollowing results show that for every > 0, there exist 1 > 0 and 2 > 0 such that, for m suciently large,with probability at least 1 min( H(i)m ) > i. with the large-width behaviour of H(i)m being characterized in Lemma A.1 and Lemma A.2, through an(/2)-Stable limiting random matrix Hi () with spectral measure i of the form (14) and (15). To provethat the minumum eigenvales of H(1)mand H(2)mare bounded away from zero, we rst need to inspect thecharacteristics of the distributions of H1() and of H2(). This is the content of Lemma A.4 and of LemmaA.6. Then, the results concerning the minumum eigenvalues of H(1)m and H(2)m are given in Lemma A.5 andLemma A.7. Lemma A.4. Under the assumptions of Theorem 3.2, the distribution of the random matrix H1() isabsolutely continuous in the subspace of the symmetric positive semi-denite matrices with zero entries inthe positions (j, j) such that xj, xj = 0, with j, j {1, . . . , k}, with the topology of Frobenius norm.",
  "j,jujujsj,jxj, xj|/2": "Since x1, . . . , xk are linearly independent, then for every u1, . . . , uk, P(Au11 . . . , Aukk ) > 0. To prove it,assume, without loss of generality, that ui = 1 for every i. Since x1, . . . , xk are linearly independent, then wecan complete the matrix X = [x1 . . . xk] by adding k d columns in such a way that the completed matrixX is non-singular. For every d-dimensional vector v such that v1 > 0, . . . , vk > 0 there exists a vector u suchthat u = ( XT )1v. Thus,",
  "|s, u|/22(du) = 0,": "where 2 is the spectral measure (15), Sk21 is the unit sphere in the space of the k k symmetric positivesemi-denite matrices, with the Frobenius norm. For every u {0, 1}k, let Bu = {v Rd : v, xj > 0 if uj =1, v, xj 0 if uj = 0}. Moreover, for every i = 1, . . . , k, let ei be a d-dimensional random vector satisfyingeij = 1 for j = i and eij = 0 for j = i. Finally, let C/2 be the constant dened in (4). Then",
  "k ,": "for every j = 1, . . . , k, and for every W such that W W(0)()F (log m)2/.We will prove, bycontradiction, that for every N1 N2 N3, W(t) W(0)F < (log m)2/ for every t > 0. In thefollowing we will write W(s) in the place of W(s)() and always assume that belongs to N1 N2 N3.Suppose that there exists t such that W(t) W(0)F (log m)2/, and let",
  "which, for m large, contradicts W(t0) W(0)F (log m)2/": "Now, we are in the position of proving Proposition 3.2. Let m N and N F be such that P(N) > 1 and the properties mentioned in Lemma A.8, Lemma A.9, Lemma A.10 and Lemma A.11 hold true for every N. Therefore, by means of Lemma A.8 and of Lemma A.9, it is sucient to show that",
  "2uT au + eiuT x 1 iuT xI(||x|| 1)(dx)(16)": "where is a measure on Rk \\ {0} satisfying(||x||2 1)(dx) < , a is a k k positive semi-denite,symmetric matrix and b is a vector. The measure is called the Lvy measure of and (a, b, ) are calledthe characteristics of the innitely divisible distribution. We will write i.d.(a, b, ). Other kinds oftruncation can be used for the term iuT x. This aects only the vector of centering constants b. An i.i.d.array of random vectors is a collection of random vectors {nj, j mn, n 1} such that, for every n,",
  "(iii) mnE(n1I(||n1|| < h)) b(h)": "Inside the class of innitely divisible distribution, we can distinguish the subclass of stable distributions. Ak-dimensional random vector has stable distribution if, for every independent random vectors 1 and 2with 1d= 2d= and every a, b R, there exists c R and d Rk such that a1 + b2d= c + d. This isequivalent to the condition: for every n 1,",
  "+ + nd= n1/ + dn(17)": "where (0, 2], 1, . . . , n are i.i.d. copies of and dn is a vector. The random vector is said to be strictlystable if (17) holds with dn = 0. A stable vector is strictly stable if and only if all its components arestrictly stable. The coecient is called the index of stability of and the law of is called -stable. Astable vector is symmetric stable if P( A) = P( A) for every Borel set A. A symmetric stablevector is strictly stable. The class of stable distributions coincides with the class of limit laws of sequences((nk=1 Xk bn)/an), where (Xn) are i.i.d. random variables. A stable distribution is innitely divisible. Thus its characteristic function admits the Lvy representation(16). If = 2, then the Lvy measure is the null measure and, therefore, the stable distribution coincideswith the multivariate normal distribution with covariance matrix a and mean vector b. If < 2, then a = 0(the zero matrix) and the -stability implies that there exists a measure on the unit sphere Sk1 such that(dx) = r(+1)dr(ds), where r = ||x|| and s = x/||x||. Substituting in (16), we obtain",
  "|||| A= Cx(A)(18)": "for every Borel set A of S such that (A) = 0. Moreover, the distribution of a random vector belongsto the domain of attraction of the Stk(, ) distribution, with (0, 2) and simmetric nite measure onSk1, if and only if (18) holds (see e.g. Davydov et al. (2008, Theorem 4.3))."
}