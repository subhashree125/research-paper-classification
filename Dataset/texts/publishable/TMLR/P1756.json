{
  "Abstract": "In K-armed dueling bandits, the learner receives preference feedback between arms, and theregret of an arm is defined in terms of its suboptimality to a winner arm. The non-stationaryvariant of the problem, motivated by concerns of changing user preferences, has receivedrecent interest (Saha & Gupta, 2022; Buening & Saha, 2023; Suk & Agarwal, 2023). Thegoal here is to design algorithms with low dynamic regret, ideally without foreknowledge ofthe amount of change. The notion of regret here is tied to a notion of winner arm, most typically taken to be aso-called Condorcet winner or a Borda winner. However, the aforementioned results mostlyfocus on the Condorcet winner. In comparison, the Borda version of this problem hasreceived less attention which is the focus of this work. We establish the first optimal andadaptive dynamic regret upper bound O(L1/3K1/3T 2/3), where L is the unknown number ofsignificant Borda winner switches. We also introduce a novel weighted Borda score framework which generalizes both the Bordaand Condorcet problems. This framework surprisingly allows a Borda-style regret analysis ofthe Condorcet problem and establishes improved bounds over the theoretical state-of-art inregimes with a large number of arms or many spurious changes in Condorcet winner. Such ageneralization was not known and could be of independent interest.",
  "Introduction": "In K-armed dueling bandits problem, a learner relies on relative feedback between arms, as opposed to thereward feedback model of the more well-studied multi-armed bandit problem (see Sui et al., 2018; Bengset al., 2021, for surveys). This problem has application in information retrieval, recommendation systems,etc, where relative feedback is easy to elicit, while real-valued feedback is difficult to obtain or interpret.For example, the availability of implicit user feedback comparing the output of two information retrievalalgorithms allows one to automatically tune the parameters of these algorithms using the framework ofdueling bandits (Radlinski et al., 2008; Liu, 2009). Formally, at round t [T], the learner pulls a pair of arms and observes relative feedback between these armsindicating which was preferred. The feedback is drawn randomly according to a pairwise mean preferencematrix and the regret is measured in terms of the sub-optimality of choices to a winner arm. Unlike classicalMAB, there are different contending notions of winner arm in dueling bandits, and the underlying theorydepends critically on the chosen notion. Most early work in dueling bandits considered the Condorcet winner(CW) (Urvoy et al., 2013; Ailon et al., 2014; Zoghi et al., 2014; 2015b; Komiyama et al., 2015) which is anarm that stochastically beats every other arm. An alternative line of works focuses on the Borda winner",
  "Published in Transactions on Machine Learning Research (12/2024)": "showed similar guarantees in this problem for procedures inspired by stochastic bandit algorithms (Garivier &Moulines, 2011; Kocsis & Szepesvri, 2006). Recently, Auer et al. (2018; 2019); Chen et al. (2019) establishedthe first adaptive and optimal dynamic regret guarantees, without requiring parameter knowledge of thenumber of changes. Alternative characterizations of the change in rewards in terms of a total variation quantity was first introducedin Besbes et al. (2019) with minimax rates quantified therein and adaptive rates attained in Chen et al. (2019).There have also been characterizations of non-stationarity in terms of drift parameters (Jia et al., 2023;Krishnamurthy & Gopalan, 2021). Yet another characterization, in terms of the number of best arm switchesS was studied in Abbasi-Yadkori et al. (2023), establishing an adaptive regret rate of SKT. Around thesame time, Suk & Kpotufe (2022) introduced the aforementioned notion of significant shifts for the switchingbandit problem and adaptively achieved rates of the form SKT in terms of S significant shifts in therewards, which serves as the inspiration for our Definition 1.",
  "Question #1. Can we attain adaptive and optimal Borda dynamic regret in terms of a Borda notion ofsignificant winner switches?": "Now, turning back to the Condorcet problem, we again note that the O( SCKT) rate was only establishedunder SST and STI, which assumes a linear ordering on arms and monotonicity/transitivity conditionson preferences. While such assumptions are well-studied in earlier dueling bandit works (Yue & Joachims,2011; Yue et al., 2012), theyre arguably unrealistic as arm preferences may not even be totally orderedin application (e.g., cyclic preferences in a tournament). Despite this, Suk & Agarwal (2023) showed the SCKT rate is in fact unattainable without SSTSTI.",
  "ST) in terms ofthe coarser count S SC of total CW switches. However, when compared to the optimal stationary regretrate": "KT, its unclear if the dependence of K in this result is optimal and also whether an intermediatenotion of significant non-stationarity (counting between SC and S switches in CW) can be learned adaptively.In other words, one hopes for a regret rate of O(SintKT) in terms of some count Sint [ SC, S] capturingthe learnable significant non-stationarity, thus resolving the gap in the findings of Suk & Agarwal (2023) andBuening & Saha (2023).",
  "In fact, we argue this is a difficult problem. Even in the stationary dueling bandit problem, the minimaxoptimal regret rate of": "KT is only known to be achievable using a sparring reduction to adversarial banditalgorithms (Dudik et al., 2015; Balsubramani et al., 2016; Saha & Gaillard, 2022). On the other hand,the adaptive procedures of Buening & Saha (2023); Suk & Agarwal (2023) crucially rely on stochasticelimination-style algorithms. Thus, its unclear how to simultaneously attain a sharp dependence on K andhandle unknown non-stationarity. 1The celebrated Arrows impossibility theorem from social choice theory (Arrow, 1950) shows no single winner notion satisfiesa class of reasonable axiomatic requirements.2measured to a time-varying sequence of Condorcet winners.3 These assume the preference matrix admits a total ordering on arms which obey monotonicity and transitivity conditions.",
  ": GIC vs. SST, STI": "Our second contribution is to make progress in answering the above question. In particular, we introduce a newcount Sapprox of approximate CW changes and show an adaptive Condorcet regret bound of S1/3approxK2/3T 2/3 under a weaker general identifiability condition (GIC) (Condition 1). GIC mandates that theres an armstochastically dominating any other arm with the largest margin, and is weaker than SSTSTI (see for comparison). This yields an improvement over the K",
  "ST rate of Buening & Saha (2023) under GICwhen the number of arms K is large or when Sapprox S": "Both our discussion for non-stationary Borda and Condorcet dueling bandits can also go through an alternativenon-stationarity measure VT measuring the total variation of preferences over time. For the Borda setting, wegive the first optimal dynamic regret bound in terms of VT (Theorem 2). For the Condorcet setting, we givethe first adaptive dynamic regret bound, which was only shown previously under the restrictive SSTSTIassumption (Buening & Saha, 2023; Suk & Agarwal, 2023). Both our answers to Questions #1 and 2 involve a new unified regret minimization framework, called weightedBorda scores, for dueling bandits. This framework generalizes both the Condorcet and Borda problems and,in particular, allows for the Condorcet regret to be recast as a Borda-like regret. Such a relationship was notknown before and may be of independent interest. We note that, within this framework, showing a Condorcetregret bound involves additional complications beyond the Borda-style regret analysis as the reference weight(a notion generalizing the uniform weight of the Borda score; cf. ) on arms is unknown and mustbe efficiently estimated. Further details on the challenges of this Condorcet regret analysis are found in.",
  "Setup Non-stationary Dueling Bandits": "We consider K-armed dueling bandits with horizon T. At round t [T], the pairwise preference matrix isdenoted by Pt KK, where (i, j)-th entry Pt(i, j) encodes the likelihood of observing a preference forarm i in a direct comparison with arm j. In the stationary dueling bandit problem, the preference matricesPt P are unchanging in time, whereas in the non-stationary problem, the preference matrices Pt maychange arbitrarily from round to round. At round t, the learner selects a pair of actions (it, jt) [K] [K]and observes the feedback Ot(it, jt) Ber(Pt(it, jt)) where Pt(it, jt) is the underlying preference of arm itover jt. We next outline the two main formulations of dueling bandits (Borda and Condorcet).",
  "Non-Stationarity Measures": "Following the discussion in , key in works on non-stationary (dueling) bandits is a measure ofnon-stationarity which captures the difficulty of the problem. Speaking plainly, the higher the amount ofnon-stationarity the more difficult the problem is, and so larger regret rates are expected. It is thus crucialthat such a measure of change properly captures the difficulty of the problem. However, the only other workdealing with Borda dynamic regret (Saha & Gupta, 2022) relies on coarse non-stationarity measures, such asthe aggregate number L .= Tt=2 111{Pt = Pt1} or magnitude VT .= Tt=2maxa,a[K]|Pt(a, a) Pt1(a, a)| of",
  "changes (see )": "To contrast, in non-stationary MAB and Condorcet dueling bandits, it is now recognized (Suk & Kpotufe,2022; Buening & Saha, 2023; Suk & Agarwal, 2023) that tighter non-stationary measures, so-called significantshifts, which capture only those changes which are detrimental to performance can in fact be adaptivelylearned.The main intuition behind a significant shift is that, for any notion of regret (e.g., MAB, Condorcet,Borda), one can track when each arm accrues significant regret (i.e., surpassing the minimax rate) and definea significant shift as occurring when every arm has accrued significant regret. Thus, our Definition 1 has asimilar form as Definition 1 of Suk & Kpotufe (2022) or as the corresponding notion for Condorcet winnerdueling bandits (Buening & Saha, 2023). The key difference here is that we use the Borda notion of regretand compare to the Borda minimax regret rate (T 2/3 vs. T 1/2).",
  "s=s1Bs (a) K1/3 (s2 s1)2/3.(2)": "Define significant BW switches (abbrev. SBS) as follows: let 0 = 1 and the (i + 1)-th sig. shift i+1 isrecursively defined as the smallest t > i such that for each arm a [K], [s1, s2] [i, t] such that arm ahas significant regret over [s1, s2]. We refer to the interval of rounds [i, i+1) as an SBS phase4. Let SB bethe number of SBS phases elapsed in T rounds. By convention, let SB+1.= T + 1. Remark 1. If there is a fixed Borda winner aBt aB, then it does not incur significant regret. Thus, thenumber SB of SBS is smaller than the number SB of Borda winner switches, meaning Definition 1 is a tightermeasure of non-stationarity.Notation. As is common in works on non-stationary bandits (see Appendix J), well often conflate counts ofchanges (e.g., L, SB) with the number of phases induced, appearing in regret rates, which is always onegreater than the corresponding count. Note that this only affects constants in the regret rates.",
  "Dynamic Regret Lower Bounds": "We briefly summarize the minimax dynamic regret rates for the Borda and Condorcet problems. Fullstatements and proofs are deferred to Appendices B and C.We note the arguments are fairly standard andmostly follow previous lower bound constructions for Borda (Theorem 16 Saha et al., 2021) and non-stationarydueling bandits (Theorems 5.1, 5.2 Saha & Gupta, 2022). Borda Dynamic RegretAs the minimax Borda regret rate over n stationary rounds is K1/3n2/3 (Sahaet al., 2021), it follows that the minimax dynamic regret rate over L stationary phases of length T/L isLK1/3(T/L)2/3 = K1/3T 2/3L1/3 (Theorem 7). In fact, the number of SBS phases (Definition 1) may replaceL here. To our knowledge, this establishes the first lower bound on Borda dynamic regret.",
  "Algorithmic Design": "To minimize the weighted Borda dynamic regret, we rely on estimating bt(a, w) for weights w belonging to areference set of weights W. In what follows, well discuss the procedures in terms of a fixed W. In the end,W can be flexibly specialized for each of the Borda and Condorcet regret problems (see Definition 4). Following the high-level idea of prior works on non-stationary dueling bandits (Suk & Agarwal, 2023; Buening& Saha, 2023), well first design a base algorithm which works well in mildly non-stationary environmentswhere there is no approximate winner change (Definition 4) and then randomly schedule different instancesof this base algorithm to allow for detection of unknown non-stationarity. One key deviation from the aforementioned prior non-stationary works is that we avoid using a successiveelimination base algorithm. This is necessary since accurate estimation of the (weighted) Borda score bt(a, w),calls for some baseline uniform exploration, which rules out any hard elimination strategy.",
  "ws (a, a) C log(T) F([s1, s2]).(4)": "In the above C > 0 is a universal constant free of any problem-dependent parameters whose value can bedetermined from the analysis and the eviction threshold F(I) for interval I is determined using the estimationerror bounds on s2s=s1 bs(a, w) (see Definition 4 in Appendix E for exact formulas for F(I)).",
  "Novel Exploration Schedule.We choose qt as a mixture of exploring the candidate set At and uniformlyexploring all arms:qt (1 t) Unif{At} + t Unif{W},(5)": "where Unif{W} is a further uniform mixture of playing according to the distributions in W. The learningrate t must then be carefully set to ensure both sufficient exploration (to reliably estimate the WBS) andsafe regret. As before, the values of t (see Definition 4) will depend on the Borda vs. Condorcet setting. However, prior works on stationary Borda dueling bandits must set t using knowledge of the horizon T(Jamieson et al., 2015; Saha et al., 2021; Wu et al., 2023). In our non-stationary problem, this amounts tosetting t based on knowledge of the underlying non-stationarity (Saha & Gupta, 2022). Thus, a key difficulty in targeting the regret rate of Theorem 2 without knowledge of non-stationarity isthat the optimal oracle learning rate t depends on the unknown phase lengths (i+1 i for Borda scores;i+1 i for Condorcet). To circumvent this, we employ a time-varying learning rate t which dependson the current number of rounds elapsed. To our knowledge, such an idea has only been used in works onadversarial bandit for analyzing EXP3 (Seldin et al., 2013; Maillard, 2011).",
  "if t < T then restart from Line 2 (i.e. start a new episode). ;": "For the non-stationary setting, we use a hierarchical algorithm METABOSSE (Algorithm 2) to schedulemultiples copies of the base algorithm BOSSE(tstart, m) (Algorithm 1) at random times tstart and durations m. Going into more detail, METABOSSE proceeds in episodes, starting each episode by running a starter instanceof BOSSE. A running base algorithm may further activates its own base algorithms of varying durations(Line 12 of Algorithm 1), called replays according to a random schedule decided by the Bernoullis Bs,m (seeLine 6 of Algorithm 2). We refer to the base algorithm playing at round t as the active base algorithm. The candidate arm set At is pruned by the active base algorithm at round t, and globally shared between allrunning base algorithms. In addition, all other variables, i.e. the -th episode start time t, round count t,and replay schedule {Bs,m}s,m, are shared between base algorithms. In sharing these global variables, any replay can trigger a new episode: every time an arm is evicted by areplay, it is also evicted from the global arm set Aglobal, essentially the active arm set for the entire episode.A new episode is triggered when Aglobal becomes empty, i.e., there is no safe arm left to play.",
  "For further intuition on the hierarchical schedule and management of base algorithms, we defer the reader to of Suk & Kpotufe (2022). We focus here on highlighting the novelties of this algorithm": "Each Active Base Alg. Chooses Own Learning RateEach base algorithm BOSSE(tstart, m) determinesits own time-varying learning rate using its starting round tstart. Then, the global learning rate t at round tis set by the base algorithm active at round t (Line 3 of Algorithm 1). Furthermore, the history of globallearning rates {t}t (globally accessible by any base algorithm) is used to determine the eviction thresholds(16) and (17) over intervals [s1, s2] of rounds where multiple base algorithms may be active. This ensures reliable detection of critical segments [s1, s2] of time where an arm has large (weighted) Bordaregret. For such a critical segment, a core argument of the analysis is that an ideal replay, i.e., an instance ofBOSSE(s1, m) for m = s2 s1, is scheduled with high probability. However, such an ideal replay must bescheduled for a duration commensurate with s2 s1, and hence must also use a commensurate learning rate. A Different Replay Scheduling Rate.In order to properly detect critical segments while also safeguardingT 2/3 regret, a different replay scheduling rate (Line 12 of Algorithm 1) is required. For this, we find thatinstantiating a base algorithm of length m at time t in episode [t, t+1) with probability m1/3 (s t)2/3",
  "balances regret minimization and detection of changes in winner": "Remark 2. For the Borda setting, the total complexity of our algorithm is O(KT 2) versus O(KT) for DEX3(Saha & Gupta, 2022), but the latter algorithm requires knowledge of non-stationarity. For the Condorcetsetting, we have a complexity of O(K2T 2) which matches that of ANACONDA (Buening & Saha, 2023),the only other adaptive algorithm with regret guarantees for Condorcet winner. We emphasize, regardless ofsetting, it remains open whether higher runtime can be avoided while getting adaptive and optimal regret.",
  "Our goal then is to establish a dynamic regret upper bound which depends optimally on the number of SBS,and does not require knowledge of the underlying non-stationarity": "Adaptive and Optimal Regret Upper Bound.Intuitively, if one knows the SBS i, then, in each SBSphase [i, i+1), one can achieve a tight regret bound of order K1/3 (i+1 i)2/3 by learning the last safearm, or the last arm to incur significant Borda regret in said phase. We show that in fact such a rate can beattained, over all SBS phases, without any knowledge of the non-stationarity.",
  "Condition 1. (General Identifiability Condition) At each round t, there exists an arm at such thatat argmaxa[K] Pt(a, a) for all arms a [K]": "While GIC requires the CW to beat every other arm with the largest margin, it is far broader than SSTSTIin not requiring any ordering on non-winner arms, allowing for cycles or arbitrary preference relations. TheGIC was previously studied in utility dueling bandits by Zimmert & Seldin (2018) (see also Bengs et al., 2021,.1). Unlike these prior works, we do not require the winner arm at to be unique.",
  "Continuing the discussion of Subsection 1.1, our regret upper bounds (Theorems 2 and 5) are shown byappealing to a new framework which generalizes the Borda and Condorcet regret minimization tasks": "Key to this is the new idea of a weighted Borda score (WBS), which measures the preference of an arm a overa reference distribution or weight of arms w K, where K denotes the probability simplex on [K]. Moreprecisely, we define a WBS with respect to weight w = (w1, . . . , wK) as",
  "Notions of weighted Borda winner (maximizer of WBS) and weighted Borda dynamic regret (analogous to (1))follow suit. Taking w to be Unif{[K]}, this recovers the Borda score and dynamic regret": "If a Condorcet winner aCt exists, then taking w to be the point-mass weight w(aCt ) on aCt also allows us tocapture the Condorcet regret. Indeed, one observes that the CW aCt maximizes the score bt(a, w(aCt )), andthe regret in terms of weighted Borda scores of arm a w.r.t. weight w(aCt ) becomes",
  "Recasting the Condorcet regret objective as a Borda-like regret quantity will be key to bypassing the need forSSTSTI, while still capturing an enhanced measure of non-stationarity": "Changing and Unknown Weights (Key Difficulty).The challenge with this reformulation of theCondorcet problem is that the reference weight w(aCt ) is unknown since the CW aCt is. Furthermore, in themore difficult non-stationary problem, the identity of aCt may change at unknown times meaning so can theweight w(aCt ). This makes it difficult to even estimate the weighted Borda score bt(a, wt(aCt )) compared tothe usual Borda task where the weight w is fixed over time.",
  "Proof Outline for Theorem 2": "We follow a general proof strategy seen in prior works on non-stationary (dueling) bandits (Suk & Kpotufe,2022; Buening & Saha, 2023; Suk & Agarwal, 2023). The full proof can be found in Appendix H for thegeneralized weigted Borda problem with a fixed weight (introduced in ). Estimators Concentrate around True Gaps.Let Bt (a, a) be the estimated weighted Borda scoreinduced by (3) for the uniform weight w Unif{[K]}. We first assert using Freedmans inequality (Lemma 15)that our estimated (cumulative) weighted Borda scores s2s=s1 Bt (a, a) concentrate about the true valuess2s=s1 Bs (a, a). This means our arm elimination criterion (4) is valid in the sense that arm a being evictedmeans it has significant regret (2). Thus, with high probability, each episode [t, t+1) aligns with the SBS phases (Definition 1) in the sense thata restart only occurs if an SBS has occurred within the episode (Lemma 20). Thus, it suffices to bound theregret in each episode [t, t+1) by the minimax regret rate over each overlapping SBS phase:",
  "Per-Episode Regret of the Last Safe Arm: by definition of the safe arm and SBS (Definition 1)this arm has safe regret K1/3(i+1 i)2/3 on each SBS phase [i, i+1)": "Per-episode Regret of the Played Arms to Last Global Arm: this part of the analysis isanalogous to Appendix B.1 of Suk & Kpotufe (2022), Appendix A.2 of Buening & Saha (2023), orAppendix C.3 of Suk & Agarwal (2023). First, note the play distribution qt (5) defaults to uniformexploration at a frequency which is safe for K1/3 (t+1 t)2/3 regret. Thus, it suffices to boundregret when qt explores the arms in the candidate arm set At. Now, a, by definition, is any arm a",
  "a[K] Bt (a, a) 111{it = a}. Thus, it suffices tobound the relative regrets": "t Bt (a, a) 111{it = a}, by carefully considering intervals of time where ais also retained. In particular, we first bound this sum on an interval from t to t(a), defined as theround at which a is evicted from Aglobal, and then intervals within replays of BOSSE which bring aback into At. Regret on the first kind of interval is bounded using our elimination criterion (4) andconcentration. Bounding the regret on the second kind of interval uses the key fact that sufficientlyfew replays are scheduled. Per-Episode Regret of the Last Global Arm to the Last Safe Arm: here, well use a badsegment analysis similar to Appendix B.2 of Suk & Kpotufe (2022) or Appendix C.4 of Suk & Agarwal(2023). Roughly, a bad segment [s1, s2] will be such that s2s=s1 Bs (at, a) (s2 s1)2/3 K1/3. Onsuch a bad segment [s1, s2], a perfect replay is roughly defined as a replay whose scheduled durationoverlaps most of [s1, s2]. Then, the key fact is that the randomized scheduling of replays (Line 12 ofAlgorithm 1) ensures that a perfect replay is scheduled for some bad segment (thus evicting a fromAglobal before too many bad segments elapse, giving us a way of bounding s2s=s1 Bs (at, a) withhigh probability.",
  "Leaving the details of the analysis to Appendix I, we instead focus here on highlighting the key challenges inshowing our Condorcet dynamic regret upper bound": "Recasting Condorcet Regret as a weighted Borda Regret.As discussed in , we emphasizeour main analysis novelty is in reformulating the Condorcet regret as a weighted Borda-like regret. Thisallows us to take the analysis route of prior works on non-stationary MAB (Suk & Kpotufe, 2022) withoutsuffering from difficulties endemic to preference feedback, as seen in Buening & Saha (2023); Suk & Agarwal(2023). To our knowledge, this is the first work on dueling bandits to make use of such a trick. Keeping Track of All Unknown Weights.As mentioned earlier in , a crucial difficulty withmaking use of the weighted Borda framework for Condorcet dueling bandits is that the reference weights ware unknown. Furthermore, the space of all possible weights K is combinatorially large and thus one cannothope to maintain estimates for all weights in K without an intractable union bound. We show in fact thatsuch accurate estimation can be bypassed because the worst-case regret is always attained at a point-massweight on some arm a [K] due to regret being linear in the weight vector.",
  "Conclusion and Future Questions": "Weve achieved the first optimal and adaptive dynamic Borda regret upper bound. Additionally, weveintroduced the weighted Borda framework which revealed new preference models where faster Condorcetregret rates are attainable, adaptively, in terms of new tighter measures of non-stationarity. In the Condorcetsetting, its still unclear if the bounds of Theorem 5 or the KSCT rate of Buening & Saha (2023) are tightfor the GIC class and, more challengingly, whether the KSCT rate can be improved outside of the GIC class.Outside of the Condorcet Winner assumption, tracking changing von-Neumann winners in non-stationarydueling bandits appears a challenging, but quite interesting direction. More broadly, on the theoretical side, future work can also independently study this weighted Borda framework(with generic known or unknown weights) and characterize the instance-dependent regret rates, which remainunclear even in stationary settings. Itd also be interesting to explore instance-dependent, or logarithmic, regret bounds for the non-stationarydueling bandit problem. We note that even in the non-stationary MAB problem, its been shown thatlogarithmic instance-dependent dynamic regret bounds are unattainable in general without knowledge ofnon-stationarity (Lattimore & Szepesvri, 2020, Theorem 31.2). Thus, it remains open what is an appropriateinstance-dependent dynamic regret rate for this problem. Future work can also implement Algorithm 2 on real datasets and compare with other algorithms for non-stationary dueling bandits. One challenge here is the high computational cost (O(KT 2) computations over Trounds) which makes our procedure somewhat impractical. We note a recent work (Li et al., 2024) exploresways to improve the computational complexity while preserving optimal regret guarantees in non-stationarybandits.",
  "Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an unknownnumber of distribution changes.Conference on Learning Theory, pp. 138158, 2019.URL": "Akshay Balsubramani, Zohar Karnin, Robert E. Schapire, and Masrour Zoghi. Instance-dependent regretbounds for dueling bandits. In 29th Annual Conference on Learning Theory, volume 49 of Proceedingsof Machine Learning Research, pp. 336360. PMLR, 2326 Jun 2016. URL Gbor Bartk, Dean P. Foster, Dvid Pl, Alexander Rakhlin, and Csaba Szepesvri. Partial monitor-ingclassification, regret bounds, and algorithms. Mathematics of Operations Research, 39(4):967997,2014. ISSN 0364765X, 15265471. URL",
  "Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual banditalgorithms with supervised learning guarantees. AISTATS, 2011. URL": "Jasmin Brandt, Bjrn Haddenhorst, Viktor Bengs, and Eyke Hllermeier. Finding optimal arms in non-stochastic combinatorial bandits with semi-bandit feedback and finite budget. Advances in Neural Infor-mation Processing Systems, 2022. URL Thomas Kleine Buening and Aadirupa Saha. Anaconda: An improved dynamic regret algorithm for adaptivenon-stationary dueling bandits. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent (eds.),Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume 206of Proceedings of Machine Learning Research, pp. 38543878. PMLR, 2527 Apr 2023. URL Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary contextualbandits: efficient, optimal, and parameter-free. In 32nd Annual Conference on Learning Theory, 2019.URL",
  "Miroslav Dudik, Katja Hofmann, Robert E. Schapire, Aleksandrs Slivkins, and Masrour Zoghi. ContextualDueling Bandits.In Proceedings of the 28th Conference on Learning Theory, 2015.URL": "Moein Falahatgar, Yi Hao, Alon Orlitsky, Venkatadheeraj Pichapati, and Vaishakh Ravindrakumar. Maxingand ranking with few assumptions. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur-ran Associates, Inc., 2017. URL Pratik Gajane, Tanguy Urvoy, and Fabrice Clrot. A relative exponential weighing algorithm for adversarialutility-based dueling bandits. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32ndInternational Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of",
  "JMLR Workshop and Conference Proceedings, pp. 218227. JMLR.org, 2015. URL": "Aurlien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit problems. InProceedings of the 22nd International Conference on Algorithmic Learning Theory, pp. 174188. ALT 2011,Springer, 2011. URL Bjrn Haddenhorst, Viktor Bengs, and Eyke Hllermeier. Identification of the generalized condorcet winnerin multi-dueling bandits. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2590425916. Cur-ran Associates, Inc., 2021. URL Reinhard Heckel, Max Simchowitz, Kannan Ramchandran, and Martin Wainwright. Approximate rankingfrom pairwise comparisons. In Proceedings of the Twenty-First International Conference on ArtificialIntelligence and Statistics, volume 84 of Proceedings of Machine Learning Research, pp. 10571066. PMLR,0911 Apr 2018. URL",
  "Patrick Kolpaczki, Viktor Bengs, and Eyke Hllermeier. Non-stationary dueling bandits. arXiv preprintarXiv:2202.00935, 2022. URL": "Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi Nakagawa. Regret Lower Bound and OptimalAlgorithm in Dueling Bandit Problem. In Proceedings of the 28th Conference on Learning Theory, 2015.URL Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa.Copeland Dueling Bandit Problem: RegretLower Bound, Optimal Algorithm, and Computationally Efficient Algorithm.In Proceedings of the33rd International Conference on Machine Learning, 2016. URL",
  "Siddartha Ramamohan, Arun Rajkumar, and Shivani Agarwal.Dueling Bandits :Beyond Con-dorcet Winners to General Tournament Solutions.In Advances in Neural Information Process-ing Systems 29, 2016.URL": "Aadirupa Saha and Pierre Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning fromrelative preferences. In International Conference on Machine Learning, pp. 1901119026. PMLR, 2022.URL Aadirupa Saha and Shubham Gupta. Optimal and efficient dynamic regret algorithms for non-stationarydueling bandits. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore,Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 1902719049. PMLR, 2022.URL Aadirupa Saha, Tomer Koren, and Yishay Mansour. Adversarial dueling bandits. In Marina Meila and TongZhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 92359244. PMLR,2021. URL Markus Schulze. A new monotonic, clone-independent, reversal symmetric, and condorcet-consistent single-winner election method. Social Choice and Welfare, 36(2):267303, 2011. ISSN 01761714, 1432217X. URL Yevgeny Seldin, Csaba Szepesvri, Peter Auer, and Yasin Abbasi-Yadkori. Evaluation and analysis of theperformance of the exp3 algorithm in stochastic environments. In Marc Peter Deisenroth, Csaba Szepesvri,and Jan Peters (eds.), Proceedings of the Tenth European Workshop on Reinforcement Learning, volume 24of Proceedings of Machine Learning Research, pp. 103116, Edinburgh, Scotland, 30 Jun01 Jul 2013.PMLR. URL Yanan Sui, Masrour Zoghi, Katja Hofmann, and Yisong Yue. Advancements in dueling bandits. In JrmeLang (ed.), Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,IJCAI 2018, July 13-19, 2018, Stockholm, Sweden, pp. 55025510. ijcai.org, 2018. URL",
  "Yisong Yue and Thorsten Joachims.Beat the mean bandit.In Proceedings of the 28th InternationalConference on Machine Learning, 2011. URL": "Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem.Journal of Computer and System Sciences, 78(5):15381556, 2012. ISSN 0022-0000. URL JCSS Special Issue: Cloud Computing2011. Julian Zimmert and Yevgeny Seldin. Factored bandits. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Cur-ran Associates, Inc., 2018. URL Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten de Rijke. Relative Upper Confidence Boundfor the K-Armed Dueling Bandit Problem. In Proceedings of the 31st International Conference on MachineLearning, 2014. URL",
  "a[K]Pt(a, a) wa,": "with respect to weight w .= (w1, . . . , wK) K. Our goal is to minimize the analogous notion of dynamicregret with respect to the quantities bt(a, w) in two situations: (1) the setting of known weights where thevector w is known to the learner and (2) the setting of unknown and changing weights where wt changesover time t and are unknown to the learner. These two scenarios will respectively capture the Borda andCondorcet regret problems. To avoid confusion, throughout this appendix well sometimes refer to the usualBorda score bt(a, Unif{[K]}) as the uniform Borda score (resp. regret, winner). Throughout this appendix, well rewrite the definitions, theorems, and proofs of the body in terms of thisgeneral framework (with arbitrary weights wt) in order to unify presentation across the Borda and Condorcetsettings. We first summarize the key results.",
  ": Summary of Results": "(a) In Appendix B.1, we first establish a minimax regret rate of K1/3T 2/3 for a known reference weightunder the stationary setting. Interestingly, the minimax regret is the same as that of the usual Bordaregret, thus showing the generic known weight problem is no harder than the Borda problem. (b) Then, in Appendix B.2, we establish the minimax regret rate of K1/3 L1/3KnownT 2/3 in the non-stationarydueling bandit problem where the underlying preference model and (known) reference weights areallowed to change, and LKnown counts the significant changes in weighted winner (Definition 1). (c) We next consider the unknown weight setting and show in Appendix C.1 that its impossible fora single algorithm to simultaneously obtain sublinear regret for both the uniform and Condorcetwinner reference weights, thus ruling out getting both T 2/3 Borda regret and",
  "s=s1ws (a) K1/3 (s2 s1)2/3.(8)": "Define significant winner switches w.r.t. the known weight w (abbreviated as SKW) as follows: the0-th sig. shift is defined as 0 = 1 and the (i + 1)-th sig. shift i+1 is recursively defined as the smallest t > isuch that for each arm a [K], [s1, s2] [i, t] such that arm a has significant weighted Borda regret over[s1, s2]. We refer to the interval of rounds [i, i+1) as an SKW phase. Let LKnown be the number of SKWphases elapsed in T rounds5.Remark 3. Definition 1 and subsequent results (Theorem 7 and Theorem 12) for a known weight w can betrivially generalized to the setting where theres a fixed and known sequence of weights {wt}Tt=1. For simplicityof presentation, we focus on the fixed weight wt w, which preserves the essence of the theory. We introduce a similar generalization of Definition 4 for the weighted Borda problem with unknown weights.In particular, we define a notion of significant shifts which tracks when an SKW occurs for any weight w.Definition 2 (Significant Winner Switches w.r.t. Unknown Weightings (SUW)). Define an arm a as havingsignificant worst-case weighted Borda regret over [s1, s2] if",
  "s=s1ws (a) K2/3 (s2 s1)2/3.(9)": "We then define weighted significant winner switches 0, 1, . . . in an analogous manner to Definition 1. LetLUnknown be the number of weighted SUW phases.Remark 1. Under GIC, LUnknown is less than the number of changes in the winner at (Condition 1). Although SUW are a stronger notion of shift than SKW (i.e., LUnknown LKnown), the SUW can be learnedin a more general setting where one aims to minimize dynamic regret w.r.t. any sequence of changing weights{wt}Tt=1 which are aligned with the SUW, defined as follows.",
  "B.1Stationary Regret Lower Bound": "We first show that, in the stationary setting, the problem of minimizing weighted Borda regret w.r.t. anyfixed and known weight w is as hard as the uniform Borda problem (i.e., K1/3 T 2/3 minimax regret).Theorem 6 (Lower Bound on Regret in Stochastic Setting). For any algorithm and any reference weightwt w, there exists a preference matrix P, with K 3, such that the expected regret w.r.t. wt is:",
  "E[Regret(w)] (K1/3 T 2/3)": "Proof. We construct an environment similar to the lower bound construction for the uniform Borda score(Saha et al., 2021, Lemma 14). As in said result, it will in fact suffice to construct an environment forcing aregret lower bound (w.r.t. weight w) of order (min{T , K/2}) for (0, 0.05). Taking := (K/T)1/3",
  "K2/3 K = T 2/3K1/3 to conclude": "Suppose K is even; if K is odd, we may reduce to a dueling bandit problem with K 1 arms by settingthe gaps (a, a) to zero for some fixed arm a [K] and any other arm a [K]. First observe that for anyreference weight w, there must exist a set of arms I of size at most K/2 such that its mass under w isw(I) 1/2. This follows since either the set of arms {1, . . . , K/2} or {K/2 + 1, . . . , K} has mass at least1/2 under the distribution w(). We then partition the K arms into a good set G := [K]\\I and bad setB := I. Without loss of generality, suppose G = {1, . . . , K/2}. Well consider K/2 + 1 different environmentsE0, E1, . . . , EK/2 where in Ea for a [K/2], arm a will maximize the weighted Borda score b(, w).",
  "The alternative environments E1, . . . , EK/2 will be small perturbations of E0 where an arm a G will havethe highest weighted Borda score by a margin of": "Environment Ea for a [K/2]:Let the preference matrix P be identical to that of environment E0 exceptin the entries P(a, a) = 0.9 + for a B. Thus, in this environment the weighted Borda scores are identicalto that of E0 except for b(a, w) = (0.4 + ) w(B), meaning arm a is the winner w.r.t. weight w. Let EEa[], PEa() denote the expectation and probability measure of the induced distributions on observationsand decisions under environment Ea. Now, the expected regret on environment Ea is lower bounded by",
  "a{1,...,K/2}KL(E0, Ea).(11)": "We now aim to bound this last KL term. Letting Ht be the history of randomness, observations, and decisionstill round t: Ht := {a} {(is, js, Os(is, js))}st for t 1 and H0 := {a}. Let Pta denote the marginaldistribution over the round t data (it, jt, Ot(it, jt)) under the realization of environment Ea. By the chainrule for KL, we have",
  "B.2Dynamic Regret Lower Bound": "Now, for the known weight setting, the K1/3T 2/3 lower bound of Theorem 6 can naturally be extended to adynamic regret lower bound of order L1/3KnownT 2/3K1/3 over LKnown SKW phases. The proof techniques areroutine, similar to arguments already used for showing dynamic regret lower bounds for Condorcet regret(Saha & Gupta, 2022, ), and will not be done in detail here to avoid redundancy. Theorem 7 (Fixed Weight Dynamic Regret Lower Bound in Terms of SKW). For L [T], let P(L) be theclass of environments with SKW LKnown(w) L. For any algorithm, we have that the minimax dynamicregret w.r.t. known weight w over class P(L) is lower bounded by",
  "supEP(L)EE[Regret(w)] (L1/3T 2/3K1/3)": "Proof. (Sketch) We can take the lower bound construction of Theorem 6 over a horizon of length T/L toforce a regret of K1/3(T/L)2/3. Repeating this construction with a new randomly chosen winner arm everyT/L rounds forces a total regret of K1/3L1/3T 2/3 while satisfying LKnown(w) L. Theorem 8 (Fixed Weight Dynamic Regret Lower Bound in Terms of Total Variation). For V [0, T] R,let P(V ) be the class of environments with total variation VT at most V . For any algorithm, we have theminimax dynamic regret w.r.t. known weight w is lower bounded by",
  "supEP(V )EE[Regret(w)] (V 1/4T 3/4K1/4 + K1/3T 2/3)": "Proof. (Sketch) The argument is analogous to the proof of Theorem 5.2 in Saha & Gupta (2022). First,assume T 1/4 V 3/4 K1/4 1. Then, using the previous lower bound construction forcing regret of orderL1/3 T 2/3 K1/3, we can use the specialization L T 1/4 V 3/4 K1/4 which ensures the total variation VTat mostL (K L/T)1/3 = L4/3 K1/3 T 1/3 V",
  "C.2Stationary Regret Lower Bound under GIC": "Under GIC (Condition 1), we next focus on deriving regret lower bounds in stochastic environments wherethere is a fixed winner arm a at across all rounds. Even under GIC, the best rate we can achieve for allweights is T 2/3. In particular, so long as an algorithm attains optimal uniform Borda regret it cannot achieve",
  "Set T large enough so that < 0.05": "We first sketch out the intuition behind the proof. In P1, arm 1 is both the Condorcet winner and the Bordawinner, while in P2, arm 2 is. The argument will go as follows: first, the algorithm must identify the winnerarm and whether were in P1 or P2 since otherwise it pays more than C T 2/3 uniform Borda regret. Theonly way to identify the winner arm, however, is to play arm 3 at least (T 2/3) times. This forces a (T 2/3)Condorcet winner regret.",
  "As a consequence, Theorem 10 prohibits using Condorcet regret minimizing algorithms in this setting, andhence new algorithmic techniques are required for efficiently learning winners under GIC": "We next investigate the dependence on K in the minimax regret rate. We show that, to achieve optimalregret for all (unknown) weights adaptively, we need to suffer a higher dependence on K of K2/3, comparedto the known and fixed weight setting with dependence K1/3 (see Theorem 6).Theorem 11 (Lower Bound on Adaptive Regret under GIC). Fix any algorithm. Then, for K > 3, thereexists a dueling bandit problem P KK satisfying Condition 1 such that there is a fixed weight w K",
  "with:E[Regret(w)] (min{K2/3T 2/3, T})": "Proof. We first sketch the argument. Well consider a variant of the environments E0, E1, . . . , EK/2 introducedin the proof of Theorem 6. Recall there is a good set G := {1, . . . , K/2} and a bad set B := {K/2 + 1, . . . , K}of arms. In every environment, the good arms (resp. bad arms) have a gap of zero when compared to eachother. In E0, each good arm has a gap of 0.4 to each bad arm. Now, we introduce a further refinement of thisconstruction and define the environment Ea,a for a G and a B as identical to the environment E0 exceptP(a, a) = 0.9 + . Now, the sample complexity of finding the winner arm a in environment Ea,a will be K2",
  "C.3Dynamic Regret Lower Bounds": "By analogous arguments as in the previous section, except now using the stationary lower bound constructionof Theorem 11 as a base template, we have the dynamic regret is lower bounded by L1/3 T 2/3 K2/3 forenvironments SUW count LUnknown L. For total variation budget V , we claim the regret lower bound is of order V 1/4 T 3/4 K1/2 + K2/3 T 2/3.In particular, we note that if V = O(K2/3T 1/3), then V 1/4K3/4K1/2 = O(K2/3T 2/3) so that it sufficesto use the stationary regret bound of Theorem 11.Otherwise, we have V = (K2/3T 1/3) and L .=V 3/4T 1/4K1/2 = (1) so that a regret lower bound of order L1/3T 2/3K2/3 = (V 1/4T 3/4K1/2) holds. Atthe same time L (K2/T)1/3 V so that the constructed environment has total variation at most V .",
  "D.1Known Weights": "We first state formally the upper bound on dynamic regret w.r.t. known weight w in terms of the SKW(Definition 1). In particular, we show matching upper bounds, up to log terms of the lower bounds ofAppendix B.2. As a reminder, taking w to be the uniform weight, we recover the Borda dynamic regret andso the below results generalize Theorem 2 and Corollary 3.",
  "ERegret Analysis Preliminaries and Estimation Bounds": "Throughout the regret upper bound analyses c1, c2, . . . will denote positive constants not depending on T orany distributional parameters. We first recall a version of Freedmans inequality, which has become standardin adaptive non-stationary bandit analyses (Suk & Agarwal, 2023; Buening & Saha, 2023; Suk & Kpotufe,2022). Lemma 15 (Theorem 1 of Beygelzimer et al. (2011)). Let X1, . . . , Xn R be a martingale difference sequencewith respect to some filtration {F0, F1, . . .}. Assume for all t that Xt R a.s. and that ni=1 E[X2i |Fi1] Vna.s. for some constant Vn only depending on n. Then for any (0, 1) and [0, 1/R], with probability atleast 1 , we have:n",
  "where F := {Ft}Tt=1 is the canonical filtration generated by observations and randomness of elapsed rounds.Then, E1 occurs with probability at least 1 1/T 2": "Proof. First, note for any arm a [K] and weight w, the random variable bt(a, w) E[bt(a, w)|Ft1] isa martingale difference bounded above by maxt[s1,s2] K 1t . Then, in light of Lemma 15, it suffices tocompute the variance of bt(a). We have for arm a active at round t (i.e., a At):",
  "t.(15)": "Now, combining (14) and (15) with (13) yields the following parameter specifications for use in the known vs.unknown weight setting.Definition 4 (Parameter Specifications). We define two parameter specifications for use in Algorithm 1(for known vs. unknown weights). We define the known weight specification w.r.t. weight w K viaW .= {w}, t .= min{K1/3 t1/3, 1},",
  ".(17)": "We next establish an elementary helper lemma which asserts that the intervals of rounds [s1, s2] over whichwe evict an arm a in (4) must be at least (K) rounds in length for the fixed weight specification and at least(K2) rounds in length for the unknown weight specification. This will serve useful in further simplifying theconcentration bound of (13) throughout the regret analysis, as needed.",
  "Proof. We first note the regret bound is vacuous for T < K; so, assume T K": "WLOG suppose arms are evicted in the order 1, 2, . . . , K at respective times t1 t2 tK (if arm a isnot evicted, let ta := T + 1. Then, we can first decompose the regret depending on whether we play an activearm or explore other arms with probability t:",
  "Using the fixed specification for t = (K/t)1/3 from Definition 4, we have first term on the RHS above (ourexploration cost) is of order K1/3 T 2/3": "Now, in light of our concentration bound Proposition 16, it suffices to bound the regret on the highprobabilitygood event E1 since the total regret is negligible outside of this event. For the second expectation, using ourfixed-weight concentration bound (14) we get that the regret of playing arm a as a candidate is at most (onthe good event E1):",
  "Now, in light of our concentration bound (14) from Proposition 16, it suffices to bound the regret on thehighprobability good event E1 since the total regret is negligible outside of this event": "We then follow the recipe of the proof of Theorem 19 (see Appendix F) except now using the concentrationbound of Proposition 16 with (15) and the unknown weight specification t .= K2/3/t1/3. First, note that itsclear the exploration cost Tt=1 t for the unknown weight specification is order K2/3 T 2/3.",
  "HDynamic Regret Analysis for Known Weights": "Proof of Theorem 12The broad outline of our regret analysis will be similar to prior works on adaptivenon-stationary (dueling) bandits (Suk & Agarwal, 2023; Buening & Saha, 2023; Suk & Kpotufe, 2022). Ourfirst goal is to show that episodes [t, t+1) align with the SKW phases, in the sense that a new episode istriggered only when an SKW has occurred.",
  "Lemma 20. On event E1, for each episode [t, t+1) with t+1 T (i.e., an episode which concludes with arestart), there exists an SKW i [t, t+1)": "Proof. We first note that bt(a, w) is an unbiased estimator for bt(a, w) in the sense that E[bt(a, w)|Ft1] =bt(a, w) if a At. Then, by concentration (Proposition 16) and our eviction criteria (4), we have that arm abeing evicted from At over the interval [s1, s2] using the eviction threshold (16) implies",
  "H.1Decomposing the Regret": "Let at denote the last safe arm at round t, or the last arm to incur significant weighted Borda regret w.r.t. win the unique phase [i, i+1) containing round t, per Definition 1. Furthermore, let a denote the last globalarm of episode [t, t+1) or the last arm to be evicted from Aglobal in said episode. Then, we can decomposethe per-episode regret:",
  "H.2Bounding the per-Episode Regret of the Safe Arm": "By the definition of SKW (Definition 1), the safe arm at is fixed for t [i, i+1) and has dynamic regretupper bounded by K1/3 (s2 s1)2/3 on any subinterval [s1, s2] [i, i+1). In particular, letting [s1, s2] bethe intersection [i, i+1) [t, t+1) (which is necessarily an interval), we have that the dynamic regret of aton episode [t, t+1) is at most order (21).",
  "H.3Bounding per-Episode Regret of Active Arms to Last Global Arm": "This will follow a similar argument as our regret analysis in fixed winner environments (Theorem 18). Recallthat the global learning rate t is the learning rate ttstart set by the base algorithm which is active at roundt. Note that t is a random variable which depends on the scheduling of base algorithms in episode [t, t+1). We first observe that the play distribution qt plays an arm according to weight w with probability t andplays an arm chosen from At uniformly at random with probability 1 t. Let a be a random draw from thedistribution qt be the play distribution at round t, which is measurable w.r.t. Ft1. Then, we may rewritethe regret as",
  ".(22)": "Bounding the Regret of Extra Exploration.We bound the first expectation on the above RHS. Recallthat ttstart denotes the learning rate t set by a base algorithm initiated at round tstart. Now, we cancoarsely bound the sum of the ts by the sum of all possible ttstart, weighted by the probabilities of thescheduling of each base algorithm. So, we have",
  "the above becomes order log(T) K1/3 (t+1 t)2/3. This is of the right order with respect to (21) by thesub-additivity of the function x x2/3": "Bounding the Regret of Active Arms.Next, we bound the second expectation on the RHS of (22).This follows a similar argument to Appendix F (the proof of Theorem 18). Supposing WLOG that the armsare evicted from Aglobal in the order 1, 2, . . . , K at respective times t1 tK , then we have the secondexpectation on the RHS of (22) can be written as",
  "Definition 5": "(i) For each scheduled and activated BOSSE(s, m), let the round M(s, m) be the minimum of two quantities:(a) the last round in [s, s + m] when arm a is retained by BOSSE(s, m) and all of its children, and(b) the last round that BOSSE(s, m) is active and not permanently interrupted. Call the interval[s, M(s, m)] the active interval of BOSSE(s, m). (ii) Call a replay BOSSE(s, m) proper if there is no other scheduled replay BOSSE(s, m) such that[s, s + m] (s, s + m) where BOSSE(s, m) will become active again after round s + m. In otherwords, a proper replay is not scheduled inside the scheduled range of rounds of another replay. LetProper(t, t+1) be the set of proper replays scheduled to start before round t+1. (iii) Call a scheduled replay BOSSE(s, m) subproper if it is non-proper and if each of its ancestorreplays (i.e., previously scheduled replays whose durations have not concluded) BOSSE(s, m) satisfiesM(s, m) < s. In other words, a subproper replay either permanently interrupts its parent or does not,but is scheduled after its parent (and all its ancestors) stops playing arm a. Let SubProper(t, t+1)be the set of all subproper replays scheduled before round t+1. Equipped with this language, we now show some basic claims which essentially reduce analyzing thecomplicated hierarchy of replays to analyzing the active intervals of replays in Proper(t, t+1) SubProper(t, t+1).",
  "are mutually disjoint": "Proof. Clearly, the classes of replays Proper(t, t+1) and SubProper(t, t+1) are disjoint. Next, weshow the respective active intervals [s, M(s, m)] and [s, M(s, m)] of any two BOSSE(s, m), BOSSE(s, m) Proper(t, t+1) SubProper(t, t+1) are disjoint. There are three cases here: 1. Proper replay vs. subproper replay: a subproper replay can only be scheduled after the roundM(s, m) of the most recent proper replay BOSSE(s, m) (which is necessarily an ancestor). Thus, theactive intervals of proper replays and subproper replays are disjoint. 2. Two distinct proper replays: two such replays can only permanently interrupt each other, and sinceM(s, m) always occurs before the permanent interruption of BOSSE(s, m), we have the active intervalsof two such replays are disjoint.",
  ",(24)": "where the sum is over all replays BOSSE(s, m), i.e. s {t + 1, . . . , t+1 1} and m {2, 4, . . . , 2log(T )}. Itthen remains to bound the contributed relative regret of each BOSSE(s, m) in the interval [s ta, M(s, m)],which will follow similarly to the previous steps. Fix s, m and suppose ta + 1 M(s, m) since otherwiseBOSSE(s, m) contributes no regret in (24).",
  "H.4Bounding per-Episode Regret of the Last Global Arm to the Last Safe Arm": "Now, it remains to bound Et+11t=twt (at, a). For this, well use a bad segment analysis similar to Suk& Agarwal (2023); Buening & Saha (2023); Suk & Kpotufe (2022). Roughly a bad segment [s1, s2] willbe such that s2s=s1 ws (at, a) (s2 s1)2/3 K1/3. On such a bad segment [s1, s2], a perfect replay isroughly defined as a replay whose scheduled duration overlaps most of [s1, s2]. Then, the key fact is thatthe randomized scheduling of replays (Line 12 of Algorithm 1) ensures that a perfect replay is scheduled forsome bad segment (thus evicting a from Aglobal before too many bad segments elapse, giving us a way ofbounding s2s=s1 ws (at, a) with high probability. We next formally define such notions. In what follows, bad segments will be defined with respect to afixed arm a and conditional on the episode start time t. In particular, this will hold for a = a which willultimately be used to bound wt (at, a) across the episode [t, t+1). The following definition only depends ona fixed arm a and the episode start time t and, conditional on these quantities, are deterministic given theenvironment.Definition 6. Fix the episode start time t, and let [i, i+1) be any phase intersecting [t, T). For any arm a,define rounds si,0(a), si,1(a), si,2(a) . . . [t i, i+1) recursively as follows: let si,0(a) := t i and definesi,j(a) as the smallest round in (si,j1(a), i+1) such that arm a satisfies for some fixed c10 > 0:",
  "The following fact, which may be considered an analogue of Lemma 17, will serve useful.Fact 22. A bad segment [si,j(a), si,j+1(a)) satisfies si,j+1(a) si,j(a) K/8": "Now, note a bad segment [si,j(a), si,j+1(a)) only contributes order K1/3 (si,j+1(a) si,j(a))2/3 regret of ato at. At the same time, we claim that a well-timed replay (see Definition 7 below) running from si,j(a) tosi,j+1(a) will in fact be capable of evicting arm a using (4). This will allow us to reduce the problem tostudying the number and lengths of bad segments which elapse before one is detected by such a replay.",
  "log(T) K1/3 (si,j+1(a) si,j(a))2/3": "Next, we note that any perfect replay BOSSE(tstart, m) will not evict at since otherwise it incurs significantregret within SKW phase [i, i+1) (see also the proof of Lemma 20). The same applies for any child basealgorithm of a perfect replay. Next, we argue that arm a must be evicted from Aglobal at some round in [si,j(a), si,j+1(a)]. If a At forsome round t [si,j(a), si,j+1(a)] we are already done. Otherwise, suppose a At for all t [si,j(a), si,j+1(a)]and so we must have E[wt (at, a)|Ft1] = wt (at, a) for all such rounds t. Now, if a perfect replay is scheduled,then we must have a global learning rate t K1/3 (si,j+1(a) si,j(a))1/3 for all t [si,j(a), si,j+1(a)]since any child base algorithm of a perfect replay can only set a larger learning rate by Definition 4. Now, noting [si,j(a), si,j+1(a)) and the second half of the bad segment [si,j(a), si,j+1(a)) have commensuratelengths up to constants, this means, if a perfect replay w.r.t. [si,j(a), si,j+1(a)) is scheduled, by similarcalculations to earlier (and using Fact 22) we must haveK",
  "Then, by our eviction criterion (4) and concentration, we have that arm a will be evicted over [si,j(a), si,j+1(a)]for large enough constant c10 in Definition 6": "It remains to show that, for any arm a, a perfect replay is scheduled w.h.p. before too much regret is incurredon the elapsed bad segments w.r.t. a. In particular, this will hold for the last global arm a, allowing us tobound the remaining expectation E[t+11t=twt (at, a)]. To show this, well define a bad round s(a) > t which will roughly be the latest time that arm a can beevicted by a perfect replay before there is too much regret. Well then show that a perfect replay with respectto some bad segment is indeed scheduled before the bad round is reached.",
  "2n si,j+1(a) si,j(a) > 2n1": "Plainly, mi,j is a dyadic approximation of the bad segment length. Next, recall that the Bernoulli Bt,m decideswhether BOSSE(t, m) is scheduled at round t (see Line 6 of Algorithm 2). If for some t [si,j(a), si,j(a)],Bt,mi,j = 1, i.e.a perfect replay is scheduled, then a will be evicted from Aglobal by round si,j+1(a)(Proposition 24). We will show this happens with high probability via concentration on the sum",
  "t=si,j(a)Bt,mi,j,": "Note that the random variable X(a, t) only depends on the replay scheduling probabilities {Bs,m}s,m givena fixed arm a and episode start time t, since the bad round s(a) is also fixed given these quantities. Thismeans that X(a, t) is an independent sum of Bernoulli random variables Bt,mi,j, conditional on t. Then, amultiplicative Chernoff bound over the randomness of X(a, t), conditional on t yields",
  "log(T),": "for c12 > 0 large enough, where the last inequality follows from (27) in the definition of the bad round s(a)(Definition 8). Taking a further union bound over the choice of arm a [K] gives us that X(a, t) > 1 for allchoices of arm a (define this as the good event E2(t)) with probability at least 1 K/T 3. Thus, under eventE2(t), all arms a will be evicted before round s(a) with high probability. Recall on the event E1 the concentration bounds of Proposition 16 hold. Then, on E1 E2(t), letting a = ain the preceding arguments we must have t+1 1 s(a) Thus, by the definition of the bad round s(a)(Definition 8), we must have:",
  "[si,j(a),si,j+1(a)):si,j+1(a)<t+11(si,j+1(a) si,j(a))2/3 c12 log(T) (t+1 t)2/3.(28)": "Thus, by (26) in the definition of bad segments (Definition 6), over the bad segments [si,j(a), si,j+1(a)) whichelapse before the end of the episode t+1 1, the regret of a to at is at most order log2(T)K1/3 (t+1 t)2/3. Over each non-bad critical segment [si,j(a), si,j+1(a)), the regret of playing arm a to at is at mostlog(T) K1/3 (i+1 i)2/3 and there is at most one non-bad critical segment per phase [i, i+1) (followsfrom Definition 6).",
  "I.1Analysis Overview for Theorem 14": "The proof of Theorem 14 will broadly follow the same outline as the regret analysis for known weights, butwith replacements of the eviction threshold by (s2 s1)2/3 K2/3 (see the unknown weight specification inDefinition 4), and bounding the variance of estimation by quantities of this order. The key difficulty forunknown weights is that the evaluation weights wt may change at the unknown SUW shifts i. Importantly, Algorithm 2 can only estimate aggregate gaps s2s=s1 wt (a, a) over intervals [s1, s2] with respectto a fixed and unchanging weight w. Thus, the main novelty in this analysis is to carefully partition the regretanalysis along intervals [s1, s2] lying within a phase [i, i+1). In fact, such a strategy is already inherentto the bad segment argument done in Appendix H.4 to bound t+11t=twt (at, a) for a known and knownweight w. So, well repeat a similar such bad segment analysis, but for different arms and even for differentbase algorithms. This strategy is similar to the approach taken in the Condorcet winner dynamic regret analysis of Buening& Saha (2023, see Appendix A.3 therein), who rely on such a tactic to avoid decomposing the regret usingtriangle inequalities as done in the original non-stationary MAB analysis of Suk & Kpotufe (2022). Our needfor this strategy is different, as we must constrain ourselves to only being able to detect that an arm a is badover segments [s1, s2] of rounds where we can use a single reference weight w.",
  "Lemma 25. On event E1, for each episode [t, t+1) with t+1 T (i.e., an episode which concludes with arestart), there exists an SUW shift i [t, t+1)": "Proof. This follows in an analogous manner as Lemma 20, where we note that significant SUW regret occursif, for some weight w K, we have s2s=s1 ws (a) K2/3 (s2 s1)2/3. Thus, an arm being evicted fromAglobal implies it has significant SUW regret meaning a new episode is triggered only when an SUW hasoccurred.",
  "I.3Generic Bad Segment Analysis": "Well first define a generic good event E3 over which the bad segment-type argument (as seen in Appendix H.4)holds for any arm a and any episode start time t. Before we can define such an event, well genericallyredefine the necessary mathematical objects of Appendix H.4 for the unknown weight setting. We note thateverything that follows in this subsection is independent of any observations or decisions made by Algorithm 2and depend only on the possible random choices of replay schedules (see Line 12 of Algorithm 2), which maybe instantiated independently and obliviously to the algorithms actual behavior. In what follows, fix a starting round tinit [T] and an arm a [K]. Define the random variables Btinits,m Ber(m1/3 (s tinit)2/3) for m = 2, 4, . . . , 2log(T ) and s = tinit +1, . . . , T, which are the replay schedulersof Line 12 in Algorithm 2 if tinit was the start of an episode. Also, fix a round tstart from which we will begindefining bad segments; the variable tstart will serve useful when we analyze bad segments for different basealgorithms BOSSE(tstart, m). The following definitions will then be relative to a fixed tinit, tstart, and arm a.Definition 9. (Generic Bad Segment) Fix an SUW phase [i, i+1) intersecting [tstart, T). Define roundssi,0, si,1, si,2 . . . [tstart i, i+1) recursively as follows: let si,0 := tstart i and define si,j as the smallestround in (si,j1, i+1) such that arm a satisfies for some fixed c14 > 0:",
  "where mi,j is the dyadic approximation w.r.t. [si,j(a), si,j+1(a)) in the same sense as defined in Appendix H.4": "We are now prepared to define the good event E3, where all bad segment arguments hold, based on thefollowing lemma.Lemma 26. Let E3 be the event over which for all a [K] and rounds tstart, tinit [T] with tstart tinit,X(a, tstart, tinit) > 1. Then, over the randomness of all possible replay schedules, P(E3) > 1 T 3. Proof. This follows from repeating the multiplicative Chernoff bound as used in Appendix H.4 for eachX(a, tstart, tinit) and then taking union bounds over arms a [K] and rounds tinit [T]. We note thatcrucially all potential replay schedules {Btinits,m }s,m,tinit are independent across different s, m, tinit. Next, we show that, under event E3, the regret of a fixed arm a to at on the active interval [s, M(s, m, a)] (seeDefinition 5 in Appendix H.3) of any scheduled base algorithm BOSSE(s, m) will be boundable by running acustomized bad segment analysis using the notions defined above.Notation 27. (Active Interval of First Ancestor Base Algorithm) For the first ancestor base algorithmBOSSE(t, T + 1 t) (i.e., that which is instantiated first in episode [t, t+1) per Line 7 of Algorithm 2),well let M(t, T + 1 t, a) denote the last round when a is retained by BOSSE(t, T + 1 t) and all of itschildren.Definition 11. For an interval of rounds I, Let Phases(I) .= {i [LUnknown] : [i, i+1) I = }, i.e.,denote those phases intersecting I. Let S(I) := |Phases(I)| be the number of intersecting phases and letL(I, i) := |[i, i+1) I| be the intersections length for phase [i, i+1).Lemma 28. (Generic Bad Segment Analysis for BOSSE(s, m)) For any scheduled BOSSE(s, m), lettingI := [s, M(s, m, a)] be its active interval, we have on event E3 E1:",
  "s=si,j1s+ K maxs[si,j,si,j+1] 1s c18 K2/3 (si,j+1 si,j)2/3": "Combining the above points with our concentration bound (15) and eviction criterion (4) give us that arm awill be evicted from At by round si,j+1. By Lemma 28, we have that a perfect replay with respect to arm a,tstart = s, and tinit = t will be scheduled before the bad round s(a, s, t) on event E3. By (30), we then musthave:",
  "(i,j)(si,j+1 si,j)2/3 c15 log(T) (M(s, m, a) t)2/3,": "where the sum is over pairs of indices (i, j) representing these generic bad segments. Thus, the desired regretbound follows by similar arguments to Appendix H.4. Note that bounding the regret over the bad segmentsof multiple phases [i, i+1) is needed only if the number of intersecting phases S(I) > 1. Otherwise, we canavoid a bad segment analysis and follow the proof steps of Appendix F to directly bound the regret as orderK2/3 m2/3.",
  "I.4Decomposing the Regret Along Different Base Algorithms": "Equipped with the generic bad segment analysis for any base algorithm BOSSE(s, m), were now ready tobound the per-episode regret. It will suffice to decompose the regret along active intervals [s, M(s, m, a)] ofdifferent base algorithms BOSSE(s, m) in a similar fashion to Appendix H.3, within each of which we canplug in the regret bound of Lemma 28 and then carefully integrate with respect to the randomness of replayscheduling.",
  "I.6Proof of Theorem 5": "Although our main regret upper bound for CW (Theorem 5) does not directly follow from Theorem 14,the analysis will follow a nearly identical structure while substituting the SUW phases [i, i+1) with theapproximate winner phases [i, i+1) (Definition 4). First, we transform the Condorcet dynamic regret to a weighted Borda dynamic regret. Let wt = w(at)where at is the approximate winner arm of the unique approximate winner phase [, i+1) containing round t.Let Sapprox+1 .= T + 1. Now, the Condorcet dynamic regret may be re-written using Definition 4:",
  "The key facts will be crucial in showing these claims": "Fact 29. An SUW cannot occur within an approximate winner phase. In other words, supposing ai is theapproximate winner arm of phase [i, i+1) (such that (6) holds for a = ai and for all s [i, i+1), a [K])then we must have that ai satisfies for all [s1, s2] [i, i+1):",
  "Fact 31 (Analogue of Lemma 25). On event E1, for each episode [t, t+1) with t+1 T (i.e., an episodewhich concludes with a restart), there exists an approximate winner change i [t, t+1)": "Proof. This follows from Fact 29, as an SUW cannot occur within an approximate winner phase. This meansthat since a restart implies an SUW has occurred, an approximate winner change must have also occurred. Next, using Fact 31, we bound the regret E[Tt=1 wtt (it) + wtt (jt)] using the generic bad segment analysisof Appendix I.3 except replacing the SUW phases [i, i+1) with the approximate winner phases [i, i+1).Fact 29 ensures that the analogue of Lemma 28 will hold as the approximate winner arm ai of phase icannot be evicted by a perfect replay corresponding to a generic bad segment in phase [i, i+1). Then, theproof steps of Appendix I.4 and Appendix I.5 follow mutatis mutandis while using Fact 30 to get the totalvariation bound.",
  "Dueling bandits.The stochastic dueling bandit problem was first proposed by Yue & Joachims (2011);": "Yue et al. (2012), which provided an algorithm achieving an instance-dependent O(K log T) regret under theSSTSTI condition. Urvoy et al. (2013) studied this problem under the broader Condorcet winner conditionand achieved an instance-dependent O(K2 log T) regret bound, which was further improved by Zoghi et al.(2014) and Komiyama et al. (2015) to O(K2 + K log T). Finally, Saha & Gaillard (2022) showed it is possibleto achieve an optimal instance-dependent bound of O(K log T) and instance-independent bound of O(",
  "KT)under the Condorcet condition. These works all assume a stationary environment": "Works on adversarial dueling bandits (Gajane et al., 2015; Saha et al., 2021) allow for changing preferencesand thus are closer to this work. However, these works focus on the static regret objective against the bestarm in hindsight and whereas we consider the dynamic regret. Other than the earlier mentioned works on dynamic regret minimization in dueling bandits, Kolpaczki et al.(2022) studies weak dynamic regret minimization but uses procedures requiring knowledge of non-stationarity. Borda Regret Minimization.The only works studying Borda regret (in stochastic or adversarial settings)are Saha et al. (2021); Saha & Gupta (2022); Wu et al. (2023). Of these, only Saha & Gupta (2022) establishesdynamic Borda regret bounds, which require knowledge of the underlying non-stationarity, and are suboptimalin light of our optimal regret bound (Theorem 2). Hilgendorf (2018) studies weak Borda regret where thelearner only incurs the Borda regret of the better of the two arms paid. Other Notions of Winner.Other alternative notions of winner and objectives, beyond Condorcet andBorda, have been proposed, such as Copeland winner (Zoghi et al., 2015a; Komiyama et al., 2016; Wu & Liu,2016) and von Neumann winner (Dudik et al., 2015; Balsubramani et al., 2016)."
}