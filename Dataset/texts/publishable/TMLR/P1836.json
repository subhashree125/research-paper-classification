{
  "Abstract": "Federated learning is a framework for training machine learning models from clients withmultiple local data sets without access to the data in its aggregate. Instead, a shared modelis jointly learned through an interactive process between a centralized server that combineslocally learned model gradients or weights from the client. However, the lack of data trans-parency naturally raises concerns about model security. Recently, several state-of-the-artbackdoor attacks have been proposed, which achieve high attack success rates while simul-taneously being difficult to detect, leading to compromised federated learning models. Inthis paper, motivated by differences in the logits of models trained with and without thepresence of backdoor attacks, we propose a defense method that can prevent backdoor at-tacks from influencing the model while maintaining the accuracy of the original classificationtask. TAG leverages a small validation data set to estimate the most considerable changea benign clients local training can make to the shared model, which can be used to filterclients from updating the shared model. Experimental results on multiple data sets showthat TAG defends against backdoor attacks even when 40 percent of user submissions toupdate the shared model are malicious.",
  "Introduction": "Federated learning (FL) is a promising solution for constructing machine learning models from numerouslocal data sources that cannot be directly exchanged or aggregated (Yang et al., 2019; Kairouz et al., 2021).These limitations become particularly crucial in contexts where data privacy and security are prominentconcerns (Li et al., 2020), with healthcare being a prime example. Additionally, FL has garnered significantattention from companies that opt to offload computing workloads onto local devices. Furthermore, FLallows for non-independent and non-identically distributed local data sets.Hence, a shared and robustglobal model is often unattainable without collaborative learning. Within this FL framework, local entities,called clients, contribute their locally acquired model gradients or weights to be intelligently combined bysome centralized entity, the server, resulting in a shared machine-learning model. However, concerns have arisen regarding the potential vulnerabilities inherent in FL. The lack of control orknowledge concerning the local training procedures allows malicious users to craft updates that compromisethe global model for all participating clients. One insidious threat, especially for classification models, is the",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Model performance under DBA with Neurotoxin (NT) backdoor attacks with various percentagesof malicious updates under imbalanced local data sets on CIFAR100. With scaling coefficients < 2, TAGis able to prevent the backdoor attacks with 10 and 20 percent malicious updates. With imbalanced localdata sets on CIFAR100, TAG is not able to prevent the backdoor attack against 40% malicious updates.",
  "Related Work": "Federated LearningFederated learning (FL) is an emerging machine learning paradigm with great suc-cess in many fields (Bonawitz et al., 2019; Hard et al., 2018; Ryffel et al., 2018). At its core, FL operatesthrough iterative rounds of model improvement. In each round, the global model is distributed to partici-pating users, and a subset of these users is selected to update a local copy of the model. These chosen userstrain their models on their respective local data sets. The resulting models are shared to safeguard dataprivacy and aggregated to construct a new global model. The resulting models are shared to safeguard dataprivacy and aggregated to construct a new global model. Backdoor AttackRecently, the FL setting has become a target for various backdoor attacks. In Xie et al.(2020), the authors highlighted how the multi-user nature of FL could be exploited to create more potentand persistent backdoor attacks. Distributing the backdoor trigger among a few malicious users effectivelyinduced the desired behavior in the global model at higher rates and extended periods after the attack hadceased. Another notable contribution in this domain is the projection method known as Neurotoxin (Zhanget al., 2022). This approach projects the attackers updates onto dimensions with small absolute weight vectorvalues, claiming that benign users update such weights less frequently, leading to longer-lasting successfulattacks. Our research rigorously evaluates our proposed methods effectiveness against both attacks.",
  "Trusted Aggregation (TAG)": "This section begins with an observation in .1 that model performance on clean data changes depend-ing on whether the model was backdoor attacked. Next, .2 introduces our framework for excludingusers from updating the global model depending on their local models performance on some clean data.Finally, .3 provides a smoothing procedure to strengthen our defense further.",
  "Motivation": ": Output distributions (kernel density estimation based) conditional on the class label for a backdoormodel (black) and a clean model (red). Note the obvious difference between the distributions of the backdoorand clean models for the target label class. Our proposed method, Trusted Aggregation (TAG), is motivated by the observation that the distributionsof model logits generated by malicious users significantly differ from those produced by benign users. We usethe term logits to refer to the output of a classification model before the softmax operation is used to produceprobabilities. Note that each element of the logit vector corresponds to precisely one class, establishing aunique node-class association for the logits. In targeted backdoor attacks, malicious users aim to create an additional learned association between aparticular manipulation of input data (the trigger) and a specific class label (the target). The additionallearned association effectively transforms the task into a (m + 1)-way classification problem. We intuitivelybelieve this difference in the models task can be exploited to identify models trained with a backdoor attack.In , we demonstrate that this learned association can lead to a distributional change in the logitson non-attacked inputs, particularly for the target class. Our insight suggests that models containing abackdoor may produce distinct distributions of logits when provided with clean data. Consequently, if we",
  "Detection Framework": "Our detection framework assumes the presence of a small, clean validation data set, which serves as thegatekeeper for updates to the global model. This data set can either belong to a trusted existing user or becollected by the centralized server and treated as a new user. We will refer to this trustworthy validationdata set as the trusted user. Our detection method leverages the trusted user to evaluate incoming model weights and determine whethereach contribution can participate in the global model update process. The core idea is to detect user modelswith unusually distributed outputs using a clean data set from the trusted user. Our method can be easilyextended when multiple trustworthy data sets or users are available. Refer to for an overview ofour proposed detection framework.",
  "Threshold": ": Diagram representation of our trusted aggregation detection framework. A distance score (v(j))is calculated for each selected users model (U (j)) based on the distributional distances between the usersmodel and the global model. A threshold () is computed based on the distributional distances between thetrusted users model (U (T )) and the global model (G). If the distance score of a user is greater than thethreshold, it will be excluded from the aggregation. See more details in .2.",
  ". The validation data is utilized to update a copy of the current global model simultaneously with thelocal training of other users": "2. When models are returned by the subset of users selected to participate in the shared model updatepotentially, logits are generated and stored for the validation data. These logits are denoted as o(G),o(T ), and o(j) for the global, validation, and j-th user models, respectively. 3. Subsequently, we compute the distance between each user from the current global model. Specifi-cally, for each class c, we calculate the class-conditional distributional distance D between empiricaldistributions for the current global models logits and the users logits using some distributionaldifference function.",
  "Do(j)c , o(G)c(1)": "Users with distance values surpassing this threshold should be excluded from the update process.Forestimation of r, we compute maxcDo(T )c, o(G)cfor our trusted user. Note r involves the maximum of allbenign users. Since the validated user is non-malicious, their distance vector serves as a good representationof other non-malicious users. However, we scale by 1 since the actual maximum will be at least aslarge as our observed trusted user.A user with a maximum distance smaller than the threshold r = maxcDo(T )c, o(G)cis considered a benign user. In comparison, a user with a maximum distancelarger than or equal to the threshold will be removed. However, this naive threshold is precarious, and dueto its instability, a lucky malicious user can get past it in some rounds. To overcome this limitation, wepropose a specific smoothing procedure in .3",
  "Proposition 1. If vc Uniform(0, bc) for all classes c [1, . . . , m] and for all benign users, then E [v] b E [2v] where v = maxc [vc] and b = maxc [bc]": "For example, when v(j)care each uniformly distributed for all users, Proposition 1 suggests that, on average, = maxcv(T )c, should equal = b for some . Yet, we acknowledge that it may be unreasonableto assume that class conditional distances are Uniform as many training hyper-parameters and even modelchoice will impact the distance distributions. A better approach could be to allow the data to determine thescaling factor, i.e., selecting the scaling factor based on the distribution of logits observed in the experiments.However, in our experimental results in , we often found that simply setting = 2 outperformsexisting methods, so we did not further explore data-dependent scaling factors.See Section B.2, for asensitivity analysis on how our scaling coefficient impacts model performance for non-iid users. We presentthese results to assist in understanding reasonable magnitudes for as many distributions may not requirelarge scaling values.",
  "Global-Min Mean Smoothing (GMMS)": "Stabilizing the threshold value is essential for maintaining the security of our method. While a straightforwardapproach to achieving this stability is through a smoothing technique, such as a moving average, it comeswith challenges. The naive threshold value experiences fluctuations in the early communication rounds asthe model learns quickly to relate inputs and output classes. We need to understand when past behavior ofr is relevant while ensuring stability throughout the process. Conventional smoothing methods, which rely on several previous values, can lead to an overly high thresholdin the initial rounds. A falsely high cutoff, in turn, could create vulnerabilities that attackers could exploit.To address this concern, we introduce our Global-Min Mean Smoothing approach, which combines thebenefits of both a stable threshold and a rapidly adjusting threshold early in training. The foundation of our approach lies in using the lowest observed value of r (Global Min) as the startingpoint for the (Mean) smoothing window. Here, r represents the naive threshold estimation up to round n.The smoothed threshold n for round n is determined using Algorithm 2.",
  ": Comparison of the global min-meansmoothing with the naive threshold and vari-ous smoothing methods": "In scenarios where exhibits rapid initial decreases, weshould observe new global minimums, which serve as a resetpoint for the threshold smoothing. Starting our smoothingfrom the global minimum allows us to maintain the originalthreshold sequences decreasing behaviors. However, whenthe original sequence is not experiencing a decline, previousvalues are utilized to smooth the threshold, effectively pre-venting lucky malicious users from evading a volatile thresh-old. visually compares our Global-Min Mean Smooth-ing with the base (naive) threshold and various conventionalsmoothing methods. It is evident that our Global Min-MeanSmoothing not only captures the early behavior of the naivethreshold but also significantly improves stability, renderingit a robust choice for safeguarding the global model againstbackdoor attacks.",
  "Trim Meanbeta.2.2.2": ": Default arguments for all experiments unless otherwise specified. For all experiments, alternativevalues for did not prevent the backdoor attacks. Any values modified for Neurotoxin attacks are shown inparentheses. ModelOur experiments employ the ResNet18 model (He et al., 2016), a well-established classifier. Ad-ditionally, to showcase the robustness and generalizability of our approach, we reproduce the main resultsusing the VGG16 model (Simonyan & Zisserman, 2014) in Section A.2. Importantly, we assume that allusers, including potential malicious actors, have complete control over various aspects of local training. Forsimplicity, we use two sets of hyper-parameters for benign and malicious users. The malicious users willpoison (add their backdoor trigger) and change the training label to the target class for a given proportionof their local data. They intend their model to associate the trigger with the target class and transfer suchbehavior to future global models. Attack and DefenseTo assess the effectiveness of TAG, we operate in a scenario where the backdoorattack is particularly potent. We mandate that the same set of malicious users are included every round inthe subset of selected users responsible for updating the global model. Moreover, all attacks start in the firstcommunication round. This approach circumvents the randomness associated with selecting users, allowingmalicious users to influence the global model repeatedly. Furthermore, the guaranteed benign validationuser is excluded from participating in global model updates. These decisions are made to showcase TAGsability to thwart even backdoor attacks against the global model in the most substantial attack settings.",
  "Comparison of Defense Methods Against Backdoor Attacks": "To thoroughly evaluate TAGs performance, we explore settings where 10, 20, and 40 percent of the returninguser models are malicious in each communication round. provides a visual representation of theperformance of various methods against backdoor attacks on three different data sets. We assess the successof each method in terms of attack success rate while ensuring that classification accuracy, , remainshigh. For our primary results, we use scaling coefficients () of 2, 2, and 1.1 for the CIFAR10, CIFAR100,and STL10 data sets, respectively. Our findings reveal that TAG effectively neutralizes the backdoor attackin each case without significantly compromising the classification accuracy of the original task. Other methods, such as coordinate-wise Median, Trim-mean, and FLTrust, fail to thwart backdoor attacks,with or without Neurotoxin, at all considered strength levels. FLTrust, while capable of delaying attacksuccess in some settings, ultimately falls short in preventing backdoor attacks. The critical difference betweenTAG and the baseline methods is that while the baseline approaches differentiate between malicious andbenign users based on update gradients, our approach compares task performance based on model outputs.For backdoor attacks, the loss is typically a combination of the original task loss and the backdoor loss:Loss = Original Task Loss+Backdoor Loss, where is usually close to zero. As a result, the gradients ofmalicious users may appear similar to benign users, but our method can still detect differences between theresulting models. However, different local minima can result in similar model outputs in highly non-convexloss landscapes. In such cases, our method may not be as effective at filtering out models that are moreeasily detected by gradient-based defenses. Yet, in none of our experimental settings were gradient-baseddefenses successful in defending against any of the attacks we considered. We remark that our method couldfilter out suspicious users before other defenses, making it possible to combine with other strategies furtherto enhance the robustness of models against targeted backdoor attacks. Our supplemental experiment, detailed in Section A.1, supports that TAG does not hinder performancefor the original classification task, even in the absence of backdoor attacks.This comparison with Fe-dAvg highlights the minimal impact on the classification tasks performance when using TAG as a defense",
  "Necessity Of Threshold Smoothing": "We revisit the last attack on the STL10 data set to highlight the importance of our proposed global-minmean smoothing technique. Recall that our smoothing is intended to improve the stability of our estimatedthreshold while preserving its behavior in the initial rounds, which is not conserved by other smoothingtechniques. Suppose we repeat the backdoor attack where 40 percent of the user subset is malicious eachiteration and omit the global-min mean smoothing. In that case, our method can no longer prevent thebackdoor attack with Neurotoxin projection on STL10. Please see . We do remark that this was theonly attack in our main results that became successful without including smoothing. Regardless, withoutsmoothing, we conclude that malicious users may be able to get past a less stable cutoff.",
  "Extending Results To Imbalanced User Data Sets": "In federated learning, user data sets may not adhere to the assumption of being independent and identicallydistributed. This section explores TAGs effectiveness in the context of imbalanced user data sets, specificallyfocusing on the CIFAR10 data set. We investigate this scenario under the most potent attack setting, where40 percent of user submissions are malicious in each round. In Section B, we include our analysis on theweaker attack settings, revealing that TAG remains highly effective in defending against backdoor attacks,even without fine-tuning the scaling coefficient (). These results are consistent across imbalanced user datasets, irrespective of whether the trusted data set is imbalanced.",
  "While Trusted Aggregation (TAG) demonstrates substantial success as a defense mechanism against backdoorattacks in federated learning, several limitations must be considered:": "Extreme Non-IID DistributionOur experiments do include non-independent and non-identically dis-tributed (non-IID) data sets. However, more extreme cases of non-IID data sets remain unexplored. Forinstance, we do not test TAG in scenarios where each user possesses the entirety of only a few of the totalclasses. In such cases, the assumption of learning a single shared model for all users, which is the primaryobjective of this work, may not be reasonable. Limited Application ScopeWhile our framework is conceptually extendable to various machine learningmodels, such as regression and natural language processing (NLP), our experimental results are confined tostandard classification computer vision models and databases. Future work may be needed to adapt ourmethod and demonstrate its effectiveness in other applications. We do not claim that TAG can successfully defend against adaptive attacks. Adaptive attacks occur whenmalicious attackers are aware that our defense is in use, and they strive to return a model with outputdistributions similar to a benign model while still exhibiting backdoor behavior.This can be achievedby creating two copies of the global model: one trained on original, unmodified local data to produce abenign model (copy A) and the other (copy B) backdoor attacked with the poisoned data set. The attackerenforces similarity between the outputs of these two copies using techniques like L2 regularization as shownin Equation 2",
  "Regardless, TAG outperforms other considered defense methods even under adaptive attack. See .However, the effectiveness of TAG diminishes when faced with strong adaptive attacks": ": Comparison of TAG under adaptive attack with baseline defense methods not under adaptiveattack for various attack settings on CIFAR10.Adaptive attacks are successful against TAG and are alimitation of our proposed defense. However, even under adaptive attack, TAG exhibits lower attack successrates at termination than baseline defenses, not under adaptive attack. Adaptive AttacksIn summary, while TAG offers a valuable defense mechanism for backdoor attacks infederated learning, its efficacy may be compromised in certain scenarios and attack types. These limitationsshould guide further research in the field of federated learning security.",
  "Xiaoyu Cao, Minghong Fang, Jia Liu, and Neil Zhenqiang Gong. Fltrust: Byzantine-robust federated learningvia trust bootstrapping, 2020. URL": "Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised featurelearning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics,pp. 215223, 2011. Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Franoise Beaufays, Sean Augenstein,Hubert Eichner, Chlo Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction.arXiv preprint arXiv:1811.03604, 2018. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016. doi:10.1109/CVPR.2016.90. Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and openproblems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210, 2021.",
  "Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Onion: A simple andeffective defense against textual backdoor attacks. arXiv preprint arXiv:2011.10369, 2020": "Phillip Rieger, Thien Duc Nguyen, Markus Miettinen, and Ahmad-Reza Sadeghi. DeepSight: Mitigatingbackdoor attacks in federated learning through deep model inspection. In Proceedings 2022 Network andDistributed System Security Symposium. Internet Society, 2022.doi: 10.14722/ndss.2022.23156.URL Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert, andJonathan Passerat-Palmbach. A generic framework for privacy preserving deep learning. arXiv preprintarXiv:1811.04017, 2018. Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing board: Acritical evaluation of poisoning attacks on production federated learning. In 2022 IEEE Symposium onSecurity and Privacy (SP), pp. 13541371. IEEE, 2022.",
  "Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.Federated machine learning: Concept andapplications. ACM Transactions on Intelligent Systems and Technology (TIST), 10(2):119, 2019": "Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning:Towards optimal statistical rates. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35thInternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,pp. 56505659. PMLR, 1015 Jul 2018. URL Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang, Michael Mahoney, Prateek Mittal, Ram-chandran Kannan, and Joseph Gonzalez. Neurotoxin: Durable backdoors in federated learning. In Ka-malika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings ofMachine Learning Research, pp. 2642926446. PMLR, 1723 Jul 2022.",
  "AComparison of Defense Methods Against Backdoor Attacks (Continued)": ": Model performance under DBA without and with Neurotoxin (NT) with 10%, 20%, and 40%malicious updates on several data sets.Column names indicate attack settings, while rows correspondto data sets. All methods result in similar classification accuracies, indicating that TAG offers improvedbackdoor defense without cost to original task performance. : Global Min Mean Smoothing threshold visualized for the first 50 communication rounds with 10percent malicious updates on CIFAR10. Our threshold can easily differentiate between benign and maliciousupdates, and almost all benign updates contribute to the global model. Similar results hold under differentdata sets and attack settings.",
  "A.1TAG Classification Accuracy Without Attackers": "A successful backdoor defense can prevent attackers while best preserving the models performance on theoriginal task. The model ability for the original task must also be maintained in the absence of an attack. Inaddition to successfully preventing attacks when present, we observe in that our defense does nothinder the classification accuracy of the original task on STL10 compared to the FedAvg procedure. Thissupports the overall usefulness of our proposed method as the shared model will have improved security withour defense without cost if attacks do not threaten the system.",
  "A.2Model Generalizability": "In this section, we demonstrate that our results are not architecture dependent by repeating our mainexperiment results on CIFAR10 for another off-the-shelf image classification model. The following resultsare obtained using VGG16 with batch normalization, originally proposed in Simonyan & Zisserman (2014).In this experiment, Trim Mean is parameterized by = 0.1. However, other values for did not impactdefense. : VGG model performance under DBA without and with Neurotoxin (NT) with 10% maliciousupdates on CIFAR10. Column names indicate attack setting. Results are the same as when using ResNet18.TAG is the only method to prevent backdoor attacks, as shown by low attack success rates, while maintainingdesirable classification accuracy.",
  "A.3Size of Trusted Data Set": "Additionally, we want to determine whether our method depends on the size of the trusted data set. Hencewe revisit the most substantial attack setting for the CIFAR10 data set but only allow the validation user tohave a data set that is 20% of the size of the other local users. Note that for this experiment, all users havebalanced and representative data. This experiment is most applicable to the case where the centralized servermust collect data, especially for problems where the data is expensive. Here the validation set is now onlyallowed 100 images, yet TAG prevents the backdoor attack and can achieve improved accuracy compared tothe baseline robust aggregation methods. Hence, we additionally conclude we do not need validation dataof the same quantity as other local users to discriminate between benign and malicious returning models.",
  "BExtending Results To Imbalanced User Data Sets (Continued)": "In this subsection, we provide additional figures that complement .4, to assist in understanding theeffectiveness of backdoor attacks when local user data sets exhibit imbalanced distributions. We would expectwith imbalanced data, more variety in the change a user can make to the output distributions. Intuitively,our defense should be less effective as malicious behavior should become more difficulty to differentiate frombenign. This sections results help us understand how different backdoor defense is under other local userdata distributions, TAG scaling coeficients, and the validation data distribution and size. and show that even without tuning TAGs scaling coefficient , our proposed defenseis effective for imbalanced user data regardless of whether the trusted user has imbalanced data as well.Note that we are using = 2 as obtained from tuning our defense method on balanced local user data setsfor CIFAR10. Similar to other experimentation results, TAG is the only method to prevent our backdoorattacks without changing model performance for the original classification task. : Model performance under DBA without and with Neurotoxin (NT) backdoor attacks with 10%malicious updates under imbalanced local data sets without tuned . Column names indicate whether thetrusted user (Trusted) is imbalanced as well. The proposed method, TAG, performs well in defending againstbackdoor attacks, even when the local user data sets are imbalanced. Again, the other defense methods donot prevent any backdoor attack under imbalanced data. Consistently TAG scaling is robust to changes under weaker attacks but needs application-specific tuning tooffer its best backdoor defense. If is not modified from the previous experiments, all but one consideredbackdoor attacks are successful at 40% prevalence, see . However, in .4, we observe thatwith proper tuning of , even with imbalanced user data, TAG can prevent the powerful attack where 40%of returning user updates are malicious. TAG is a good choice for a defense method regardless of the datadistribution of its users.",
  "B.1Discussion on Fairness": "We use the imbalanced user experiments to better understand the fairness of our algorithm. We do not wishto systematically exclude benign users from updating the model while defending against backdoor attacks.Here we consider the CIFAR10 experiment with imbalanced users where 10% of model updates are malicious.When users have highly non-iid data, it becomes harder to understand whether that imbalance results frommalicious behavior or natural heterogeneity. Similar results hold under different attack settings. In , we show that three users were excluded from participating often in federated training. Theusers with a low proportion of accepted updates to the shared model are labeled by their user id for easyidentification.While excluding the malicious user of the three is desired behavior, as a consequence offiltering returning models, we systematically exclude two users. These such users have an extremely highproportion of their local training data consisting of a single class label. As a result, their locally updatedmodels have logits considered suspicious changes from the previous shared model which should work well onall class labels. We remark that when local datasets are highly heterogeneous, using a single shared model between usersmay not be appropriate. We leave a backdoor defense against highly heterogeneous users for future work.However, we expect the application of our method to have degraded performance as the degree of hetero-geneity grows. We recommend the use of TAG for when it is plausible that users share at least moderatesimilarities between their local datasets.",
  "CProof of Proposition 1": "Proof. Let vc denote the distribution of distances between logits produced for class c on a given data set bythe previous global model and the locally updated copy of a benign user. Assume that each element has theUniform distribution, vc Uniform(0, bc), from zero to some class-specific constant, bc, for all benign users."
}