{
  "Abstract": "The Vision Transformer (ViT) architecture has emerged as the backbone of choice for state-of-the-art deep models for computer vision applications. However, ViTs are ill-suited for privateinference using secure multi-party computation (MPC) protocols, due to the large number ofnon-polynomial operations (self-attention, feed-forward rectifiers, layer normalization). Wedevelop PriViT, a gradient-based algorithm to selectively Taylorize nonlinearities in ViTswhile maintaining their prediction accuracy. Our algorithm is conceptually very simple, easyto implement, and achieves improved performance over existing MPC-friendly transformerarchitectures in terms of the latency-accuracy Pareto frontier.",
  "Introduction": "Motivation: Deep machine learning models are increasingly being deployed by cloud-based providers,accessible only by API calls. In such cases, user data privacy becomes paramount, motivating the setting ofprivate inference (PI) using secure multiparty computation (MPC). In its simplest form, MPC-based privateinference is a two-party setup where a user (the first party) performs inference of their data on a model whoseweights are owned by the cloud service provider (the second party), with both sides encrypting their inputsusing cryptographic techniques prior to inference. The main technical barrier to widespread deployment of MPC-based PI approaches is the large number ofnonlinear operations present in a deep neural network model. Private execution of linear (or low-degreepolynomial) operations can be made fast using cryptographic protocols like homomorphic encryption and/orsecret sharing. However, private execution of nonlinear operations (such as ReLUs or softmax operations)",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Left: Step 1 - Fine-tuning of a pretrained ViT over target dataset to produce the teacher ViT.Middle: Step 2 - Duplicate teacher ViT, introduce parametric GELUs and attention mask to form studentViT. Train using cross-entropy loss, KL divergence, and L1 penalty to gradually find a sparse mask. Binarizethe mask post desired non-linearity budget. Right: Step 3 - With a frozen, binarized mask, further fine-tunethe student model using cross-entropy loss and KL divergence with the teacher.",
  "require Yaos Garbled Circuits, incurring high latency and storage overhead. Thus, unlocking fast, accurate,and efficient PI requires rethinking network design": "Consequently, an emerging line of work over the last five years has made several advances towards the designof MPC-friendly models; see more discussions below in . These methods approach PI from variousangles. Approaches such as Delphi (Mishra et al., 2020) or Circa (Ghodsi et al., 2021) propose to replaceReLUs with MPC-friendly approximations, while approaches such as CryptoNAS (Ghodsi et al., 2020) andSphynx (Cho et al., 2021) use neural architecture search (NAS) to search for network backbones with aminimal number of ReLUs. Peng et al. (2023) propose hardware-aware ReLU-reduced networks to achievebetter latencies. Some of the latest approaches in this direction (SNL by Cho et al. (2022), and SENetby Kundu et al. (2023)) derive inspiration from network pruning. However, the above body of work has gaps. The overwhelming majority of PI-aware model approaches havefocused on convolutional architectures, and have largely ignored transformer models. In particular, the properapplication of MPC to vision transformer (ViT) architectures remains less well-studied; see . Visiontransformers (Dosovitskiy et al., 2020) currently list among the best performing deep models in numerouscomputer vision tasks, spanning image classification, generation, and understanding. On the other hand,vision transformers are very bulky, possessing an enormous number of nonlinear operations of different types:GELUs, softmaxes, and layer norms. We are aware of three published approaches to addressing private inference for vision transformers.MPCViT (Zeng et al., 2022) proposes a MPC-aware NAS algorithm, along with a polynomial simplifi-cation of the attention mechanism coupled with knowledge distillation. SAL-ViT (Zhang et al., 2023)introduces a learnable approximation for the softmax and perform NAS to choose between two types ofattention layers. RNA-ViT (Chen et al., 2023) proposes an attention architecture with a compressed attentionmap and a Taylor approximation of the softmax. The last two works assume hybrid ViT models, a differentcryptographic security model, and have not made their models public, making direct apples-to-appliescomparisons difficult; in this paper, we will focus on MPCViT, which (to us) is the gold standard for privateinference over vision transformers. Our contributions and techniques.We introduce PriViT, a very simple algorithm for designingMPC-friendly vision transformers. PriViT either improves upon (or is competitive with) MPCViT both interms of latency and accuracy on TinyImagenet as well as CIFAR 10/100. Therefore, PriViT presents a",
  "siSoftmax(Xi) + (1 si)SquaredAttn(Xi),": "where SquaredAttn is just the unnormalized quadratic kernel, and ci, si are binary switching variables. Theseswitches decide whether to retain the nonlinear operation, or to replace it with its Taylor approximation(linear in the case of GELU, quadratic in the case of softmax1). Having defined this new network, we initializeall switch variables to 1, make both weights as well as switches trainable, and proceed with training usinggradient descent. Some care needs to be taken to make things work. We seek to eventually set most of the switching variables tozero since our goal is to replace most nonlinearities with linear units or low-degree polynomials; the survivingswitches should be set to one. We achieve this by augmenting the standard cross-entropy training loss with a1-penalty term that promotes sparsity in the vector of all switch variables, apply a homotopy-style approachthat gradually increases this penalty if sufficient sparsity is not reached, and finally binarize the variables viarounding. See . Implications. We note that our point-of-comparison method, MPCViT, also follows a similar strategy as(Cho et al., 2022): judiciously replace both GELUs and softmax operations in vision transformers with theirlinear (or polynomial) approximations. However, they achieve this via a fairly complex MPC-aware NASprocedure. A major technical contribution of their work is the identification of a (combinatorial) search space,along with a differentiable objective to optimize over this space. Our PriViT algorithm is conceptually much",
  "While our focus in this paper is sharply on private inference, our results also may hold two broader implicationsfor transformer architectures used in computer vision:": "1. First, most nonlinear operations in transformers appear to be superfluous. PriViT is able to remove nearly83% of GELUs and 97% softmax operations with less than 0.5% reduction in accuracy over CIFAR-100(Krizhevsky et al., 2009). 2. Second, given a target overall budget of softmaxes and GELUs, PriViT overwhelmingly chooses to retainmost of the nonlinearities in earlier layers, while discarding most of the later ones. These suggest thatthere is considerable room for designing better ViTs than merely stacking identical transformer blocks.",
  "Preliminaries": "Private inference. Prior work on private inference (PI) have proposed methods that leverage existingcryptographic primitives for evaluating the output of deep networks.Cryptographic protocols can becategorized by choice of ciphertext computation used for linear and non-linear operations. Operations arecomputed using some combination of: (1) secret-sharing (SS) (Shamir, 1979; Micali et al., 1987); (2) partialhomomorphic encryptions (PHE) (Gentry & Halevi, 2011), which allow limited ciphertext operations and (3)garbled circuits (GC) (Yao, 1982; 1986). In this paper, our focus is exclusively on the Delphi protocol (Mishra et al., 2020) for private inference. Wechoose Delphi as a matter of convenience; the general trends discovered in our work hold regardless of theencryption protocol, and to validate this we measure latency of our PriViT-derived models using multipleprotocols. Delphi assumes the threat model that both parties are honest-but-curious. Therefore, each partystrictly follows the protocol, but may try to learn information about the other partys input based on protocoltranscripts (Wang et al., 2022; Peng et al., 2023; Lu et al., 2021; Qin et al., 2022). Delphi combines cryptographic primitives such as secret sharing (SS) and homomorphic encryptions (HE)for all linear operations, and garbled circuits (GC) for ReLU operations. For convolutional architectures,the authors of Delphi shows empirical evidence that ReLU computation requires 90% of the overall privateinference time for typical deep networks. As a remedy, Delphi and SAFENET (Lou et al., 2021) propose neuralarchitecture search (NAS) to selectively replace ReLUs with polynomial operations. CryptoNAS (Ghodsiet al., 2020), Sphynx (Cho et al., 2021) and DeepReDuce (Jha et al., 2021) design new ReLU efficientarchitectures by using macro-search NAS, micro-search NAS and multi-step optimization respectively. Only recently has this line of literature turned its focus to transformer-style models. Approaches in thisvein include MPCViT (Zeng et al., 2022), SAL-ViT (Zhang et al., 2023), and RNA-ViT Chen et al. (2023).MPC-ViT uses NAS techniques to find cheaper approximations of the softmax attention mechanism, whileSAL-ViT and RNA-ViT propose new types of attention mechanisms. The latter two works claim state-of-theart over MPCViT; however, we found that a direct comparison is not possible for two reasons: (i) these",
  "dXWv.(1)": "To frame the computational challenges inherent to Vision Transformers (ViTs), we consider the ViT-base (12layer) model designed for 224 224 images. Overall, this architecture consists of(approximately) 726, 000GeLUs, 28, 000 softmax, and 4000 layer norms. All the non-linearities, when viewed through the lens of theDelphi protocol, become extremely resource-intensive operations.",
  "PriViT: MPC-Friendly ViTs": "We propose to learn an architecture that circumvents these computationally heavy operations. Our approachsurgically introduces appropriate Taylor approximations of the GeLU and softmax attention operationswherever possible (under the constraint that accuracy drops due to such approximations should be minimal.)The key challenge is to figure out where to perform these approximations.",
  "Switched Taylorization": "To begin, we focus on softmax and GeLUs and ignore layernorms; we found that these were far harder toTaylorize. For the former, we introduce auxiliary variables to act as switches. Given fW, let C and S be thetotal number of GeLUs and softmaxes. Further, let S = [s1, s2, ..., sS] and C = [c1, c2, . . . , cG] be collectionsof binary switch variables defined for all instances of GeLU and softmax activations. Our goal here is to learnW, S, and C to ensure high accuracy with as few nonlinearities as possible. We also use N to denote thenumber of tokens, H to denote the number of heads and m to denote the size of the token embedding (andconsequently the output size of the feedforward MLP).",
  "o = siSoftmax(Xi) + (1 si)SquaredAttn(Xi),(5)": "where Xi is the ith row of the attention matrix. As before, sis are initially real-valued, trainable and initializedto 1. The variables are binarized during inference allowing use of either Softmax or squared attention basedon the values of si. Further ablations of different candidate attention functions are presented in the resultssections.",
  "Training PriViT models": "To train PriViT models, we need to train three sets of variables: the weights of the transformer, W, theswitch variables for the GELU parameterization, C, and the switch variables for the attention parametrization,S. Our goal is to train a model that minimizes the number of nonlinearities to satisfy a given nonlinearitybudget, that is, C0 < C, and S0 < S, while increasing the overall performance.",
  "where L is the standard cross-entropy loss. We then optimize for each of the variables until the requiredsoftmax attention and GELU budgets": "Optionally, we can also make use of knowledge distillation during both training and fine-tuning. We introducea KL divergence loss on the soft labels generated by the teacher and student ViT model. This loss is addedto the Lprivit loss defined in eq. 6. Thus our final minimization objective looks as follows,",
  "where T denotes the weights of the teacher model, and Lkl is the KL divergence loss": "After every epoch, we count the number of GELUs and softmax attention operations by thresholding thesi and ci values. Once the model satisfies the required budgets, we freeze the chosen GELUs and softmaxattention operations by binarizing all si and ci values and fine-tune the model weights for the classificationtask. See for an overview of the method.",
  "GeLU 500k": ":Comparison of GELU distribution be-tween ViT-base (Base) and PriViT without softmaxlinearization. The x-axis represents the models layerindex, while the y-axis shows log-scaled GELU op-erations per layer.With an input tensor size of197 3072 for the GELU layer, each layer contains197 3072 = 605184 GELU operations. Top: 150Ktarget GELU. Bottom: 500K target GELU.",
  "Softmax": ": Comparison of softmax distribution inViT-base model (Base) versus PriViT without GeLUlinearization.The x-axis denotes the layer index,while the y-axis shows the softmax operations perlayer. With a 197 197 attention matrix across 12heads, the ViT-base model totals 2364 softmax oper-ations per layer. Notably, PriViT tends to substituteearlier layer softmaxes with linear operations. Top:1K target softmax; Bottom: 10K target softmax.",
  "Experimental setup": "Architecture and datasets. We apply PriViT algorithm to a pretrained checkpoint of ViT-Tiny (Steineret al., 2021) that is trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224 224, andfine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224 224. The pretrained ViTTiny checkpoints are made available by (WinKawaks, 2022). In this research work we focus on finetuning anexisting model checkpoint like ViT Tiny on a target standard image classification dataset (CIFAR-10/100(Krizhevsky et al., 2009) and Tiny-ImageNet). CIFAR-10/100 has images of size 32 32 while Tiny-ImageNethas 64 64. These images were resized to 224 224 before being given as an input. CIFAR-10 has 10 classeswith 5000 training images and 1000 test images per class. CIFAR-100 has 100 classes with 500 trainingimages and 100 test images per class. Tiny-ImageNet has 200 classes with 500 training images and 50 testimages per class. We also perform hyperparameter tuning and present more details in Appendix A ViT teacher pretraining. As the base model, we finetune a pretrained ViT-Tiny on CIFAR-10/100 for10 epochs. We use AdamW (Loshchilov & Hutter, 2017) as the optimizer with an initial learning rate andweight decay as 0.0001 and 0.0001 respectively, and decay the learning rate after every 30 epochs by factor0.1. We use the same hyperparameters for the TinyImagenet model. Joint optimization of student ViT and parametric non linearities. We use Adam (Kingma & Ba, 2014)optimizer with learning rate equal to 0.0001. We use knowledge distillation and use soft labels generated by theteacher model with a temperature of 4. The total loss is then, L = LPriViT +LKL, where LPriViT is Equation 6and LKL is the KL divergence loss between the logits of teacher and student model. The Lasso coefficient(Tibshirani, 1996) for parametric attention and GELU mask are set to g = 0.00003 and s = 0.00003respectively at the beginning of the search. We set warmup epochs to 5 during which we dont change anyhyperparameters of the model. Post warmup, we increment g by a multiplicative factor of 1.1 at the end ofeach epoch if the number of active GELUs of current epoch do not decrease by atleast 2 as compared toprevious epoch. Note that a GELU/softmax is considered active if its corresponding auxiliary variable isgreater than threshold hyperparameter = 0.001. We follow the same approach for s, with a multiplicativefactor of 1.1 and an active threshold of 200. Binarizing parametric nonlinearities, finetuning. When the GELUs and softmax budgets are satisfied,we binarize and freeze the the GELU and softmax auxiliary variables. We subsequently finetune the modelfor 50 epochs using AdamW with a learning rate 0.0001, weight decay 0.0001 and a cosine annealing learningrate scheduler (Loshchilov & Hutter, 2016). Our finetuning approach continues to use knowledge distillationas before. Non-linearity cost comparison. We conduct experiments to assess the computational cost of non-linearfunctions such as layernorm, softmax, and GeLU in comparison to ReLU within GC. The detailed resultsare reported in , brackets considers amortizing to a vector of inputs, e.g., a Layernorm(192) is anoperation over a vector length of 192 is equivalent to 6504 than the cost of a ReLU. It demonstrates that witha vector length of 197, all layernorm and softmax functions incur higher computational costs (i.e., number ofANDs) than ReLU. Specifically, they exhibit costs 6504, 18586 higher than that of ReLU respectively andfor pointwise GELU, we saw a cost 270 higher than that of ReLU. The cost of denominator of layernorm and",
  "TinyImagenet151.7569.8293381.4262.55600128.2366.98351331.3563.7600110.6064.46342307.4563.36600CIFAR 10088.2478.540372.2177.890075.9277.7444771.7776.990067.5475.4749871.4076.9900": "softmax can be amortized to the whole vector and thus incur less cost than GELU. We estimate the latencyof each model generated by PriViT using these conversion factors. To showcase an example, we estimate thenon-linearity cost of a hypothetical model with 1000 softmax operation, 1000 layernorm operations and 1000GELUs, by taking the weighted sum of each operations with their corresponding latency factor. GELU replacement post training. Mirroring the MPCViT+ approach, we also report the effect ofPriViT with all GELUs replaced with ReLUs, we call this model PriViT-R, and the original PriViT model asPriViT-G. Such an optimization is effective For low GELU budgets as it introduces very minimal errors. Forhigh GELU budgets, the error is quite significant that it affects the overall performance.",
  "Comparisons on standard benchmarks": "We benchmark PriViT against MPCViT, using the checkpoints publicly shared by the authors of Zeng et al.(2022). We calculate latencies with the Secretflow (Ma et al., 2023) framework using the SEMI2k (Crameret al., 2018) protocol. In our evaluation on various datasets, the performance of PriViT was benchmarked against both MPCViTand MPCViT+. We measure two metrics of importance the latency (measured in seconds), and accuracy.An ideal model would achieve high accuracy with low latency. 1. Tiny ImageNet: In terms of the Pareto frontier on the Tiny ImageNet dataset, PriViT showcases notableimprovement. On Tiny Imagenet, for an isoaccuracy of approximately 63% , PriViT achieved 5.77speedup compared to MPCViT as reported in . 2. CIFAR-10: Similarly, on the CIFAR-10 dataset, PriViT showcases noticeable improvements over MPCViTand MPCViT+. At an isoaccuracy of approximately 94%, PriViT achieves a 1.14 and 1.08 speedupover MPCViT and MPCViT+ respectively as reported in and . This represents non-trivialspeedup, despite that fact that PriViTs model architecture is significantly larger than MPCViTs modelarchitecture as noted in . A graph of these values can be found in . 3. CIFAR-100: Finally, on the CIFAR-100 dataset, PriViT also showcases noticeable improvements overMPCViT and MPCViT+. At an isoaccuracy of approximately 78%, PriViT achieves a 1.05 and 1.02speedup over MPCViT and MPCViT+ respectively as reported in and . We draw a similarconclusion to the analysis in CIFAR-10. PriViT achieves better accuracy with lower latency despite itslarger model size. A graph of these values can be found in .",
  "Tiny Imagenet65.1134.03": "We now analyze our new results, starting with latency comparisons. We first see that all of the modelspresent in MPC-Tiny are significantly slower than their counterparts in PriViT. Notably, the accuracy forthe MPC-Tiny models is worse than both PriViT models and MPCViT models on average. This is likely dueto the knowledge distillation procedure not transferring well to larger models. We also notice that for eachvalue of , the latency is similar regardless of the dataset, which is as expected and provides a good sanitycheck. In addition, we see that MPC-Tiny models have a relatively inflexible lower bound for latency. The parameter controls how much latency one can save by removing ReLU Softmaxes; even after we have reached = 0.1, the latency is still very significant due to the large model size. This is in contrast to PriViT wherewe control the nonlinearity Taylorization at a scalar level (instead of attention head level) which enables usto push the latency down to significantly lower levels. In conclusion, we show that a better comparison of PriViT can be made with MPC-Tiny as opposed to theoriginal published MPCViT work. Our results show that PriViT has significantly lower latency at similaraccuracies compared to MPC-Tiny.",
  "Ablation studies": "Knowledge Distillation. We incorporate knowledge distillation (KD) alongside supervised learning. Toassess the contribution of KD to the overall performance, we trained PriViT on the TinyImagenet datasetwith varying non-linearity budgets. We then compared its performance to a version of PriViT (as outlined in) that does not employ a teacher model for knowledge distillation. Our results in indicatethat, under identical latency conditions, incorporating KD enhances performance by approximately 5%. Contribution of pretraining. In PriViT, we utilize a pretrained checkpoint, which is subsequently fine-tuned. Post fine-tuning, we introduce a parametric GeLU and attention mechanisms to decrease non-linearitiesin the model. To gauge the impact of using a pretrained model on the overall performance, we contrastthe performance of PriViT with a variant of PriViT that is not built upon a pretrained model. Instead,this variant employs weights initialized from scratch and is trained with the same parametric non-linearitymask as used in PriViT to minimize non-linearities. The comparative outcomes of these approaches arepresented in . Our findings reveal that, for comparable latencies, PriViT with the pretrained checkpointoutperforms the alternative by about 14%. Choice of softmax approximation. We run PriViT over different softmax budget over CIFAR-100, andreport the accuracy of the resulting model versus the number of original softmax attention retained. Lowernumber of softmax operations implies higher the number of softmax attention replaced with our candidateattention operation. As per we see almost no performance drop for SquaredAttn, roughly 5%drop in performance for ScaleAttn and 10% drop in performance for UniformAttn in low budgets. ThusSquaredAttention outperformed the others across all softmax budgets, motivating its selection to replace thestandard softmax attention in PriViT. Fine-grained versus layer-wise Taylorization PriViT employs a unique approach where it selectivelyTaylorizes softmax and GELU operations. To probe the effectiveness of this method, we contrasted it withan alternative PriViT approach that Taylorizes a ViT model progressively, layer by layer. As illustrated in, our observations underscored the superiority of selective Taylorization. Visualization of non-linearity distribution. To understand which nonlinearities are preserved, weinvestigate the distribution of PriViT models under different softmax and GELU budgets.From ourobservations in we can conclude that GELUs in earlier encoder layers are preferred over the ones in",
  "Conclusion": "We introduce PriViT, a new algorithm for designing MPC-friendly vision transformers, and showed itscompetitive performance on several image classification benchmarks. A natural direction of future work is toextend similar techniques for designing other families of transformer architectures, such as Swin Transformers,as well as encoder-decoder transformer architectures. A key limitation of PriViT is the difficulty in properlyTaylorizing layer norms without introducing instability in the training; we leave an in-depth study of thisquestion to future work.",
  "Impact Statement": "This paper aims to improve the efficiency of private inference for vision transformer architectures. Facilitieswho wish to perform inference on vision models while keeping user data private (such as hospitals) would gainthe ability to use third party pretrained models without risking the leakage of confidential data. However,this advancement could also open the door to malicious users who would be able to use models withoutrevealing their intentions. Considering both perspectives, we believe that the pros of advancing this fieldvastly outweigh the cons and thus believe it will lead to a net-positive impact. This work was supported in part by the US Department of Educations GAANN (Graduate Assistance inAreas of National Need) Fellowship Program. The research was also supported in part by the NSF CAREERaward #2340137, NSF award #2154119 and by DARPA under the Data Protection in Virtual Environments(DPRIVE) program, contract HR0011-21-9-0003.",
  "Ronald Cramer, Ivan Damgrd, Daniel Escudero, Peter Scholl, and Chaoping Xing. Spd: efficient mpc modfor dishonest majority. In Annual International Cryptology Conference, pp. 769798. Springer, 2018": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated dataaugmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition workshops, pp. 702703, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. In International Conference on Learning Representations,2020.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technicalreport, U. Toronto, 2009": "Souvik Kundu, Shunlin Lu, Yuke Zhang, Jacqueline Tiffany Liu, and Peter Anthony Beerel. Learning tolinearize deep neural networks for secure and efficient private inference. In Proc. Int. Conf. LearningRepresentations, 2023. Dacheng Li, Hongyi Wang, Rulin Shao, Han Guo, Eric Xing, and Hao Zhang. MPCFormer: Fast, Perfor-mant Private Transformer Inference With MPC. In The Eleventh International Conference on LearningRepresentations, 2022.",
  "Qian Lou, Yilin Shen, Hongxia Jin, and Lei Jiang. Safenet: Asecure, accurate and fast neu-ral networkinference. In Proc. Int. Conf. Learning Representations, 2021": "Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, andLi Zhang. Soft: Softmax-free transformer with linear complexity. In Adv. Neural Inf. Proc. Sys. (NeurIPS),2021. Junming Ma, Yancheng Zheng, Jun Feng, Derun Zhao, Haoqi Wu, Wenjing Fang, Jin Tan, ChaofanYu, Benyu Zhang, and Lei Wang.SecretFlow-SPU: A performant and User-Friendly framework forPrivacy-Preserving machine learning. In 2023 USENIX Annual Technical Conference (USENIX ATC23), pp. 1733, Boston, MA, July 2023. USENIX Association. ISBN 978-1-939133-35-9. URL",
  "Nineteenth ACM Symp. on Theory of Computing, STOC, pp. 218229. ACM New York, NY, USA, 1987": "Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. Delphi:A cryptographic inference service for neural networks. In 29th USENIX Security Symposium (USENIXSecurity 20), pp. 25052522. USENIX Association, Aug. 2020. ISBN 978-1-939133-17-5. URL Jianqiao Mo, Jayanth Gopinath, and Brandon Reagen. Haac: A hardware-software co-design to accelerategarbled circuits. In Proceedings of the 50th Annual International Symposium on Computer Architecture,pp. 113, 2023. Hongwu Peng, Shanglin Zhou, Yukui Luo, Nuo Xu, Shijin Duan, Ran Ran, Jiahui Zhao, Shaoyi Huang, Xi Xie,Chenghong Wang, Tong Geng, Wujie Wen, Xiaolin Xu, and Caiwen Ding. Rrnet: Towards relu-reducedneural network for two-party computation based private inference. arxiv preprint: ArXiv:2302.02292v2,2023. Zhen Qin, Weixuan Sun, Huicai Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, andYiran Zhong. cosformer: Rethinking softmax in attention. In Proc. Int. Conf. Learning Representations,2022.",
  "of Computer Science (sfcs 1986), pp. 162167. IEEE, 1986": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 60236032, 2019. Samee Zahur, Mike Rosulek, and David Evans. Two halves make a whole: Reducing data transfer in garbledcircuits using half gates. In Advances in Cryptology-EUROCRYPT 2015: 34th Annual InternationalConference on the Theory and Applications of Cryptographic Techniques, Sofia, Bulgaria, April 26-30,2015, Proceedings, Part II 34, pp. 220250. Springer, 2015. Wenxuan Zeng, Meng Li, Wenjie Xiong, Wenjie Lu, Jin Tan, Runsheng Wang, and Ru Huang. Mpcvit: Search-ing for mpc-friendly vision transformer with heterogeneous attention. arXiv preprint arXiv:2211.13955,2022.",
  ": PriViT produces similar latency accuracy trade off for a variety of ViT models over CIFAR 10,highlighting its scalability to large model sizes": "PriViT on different ViT architectures. We study PriViT on the Data-Efficient Image Transformer(DeiT) (Touvron et al., 2021) to see how applicable the algorithm is to other ViT style architectures. Weperform latency and accuracy tests for DeiT-Tiny on CIFAR-10, CIFAR-100, and Tiny Imagenet. Resultsare found in . The accuracy-latency tradeoff is very similar in the DeiT-Tiny model to the ViT-Tinymodel. This suggests that PriVit is applicable to ViT style architectures as a whole, and not only to theoriginal ViT architecture. Analysis of performance degradation. In this analysis, we aim to compare the performance of trainedPriViT models with their finetuned versions. Our analysis is based on the class-level accuracy metric from theTiny ImageNet dataset, which consists of 200 classes. We focus on three specific parameters to understandthe performance degradation:",
  "Variance in Accuracy Difference: We analyze the consistency of the differences in accuracy across the 200classes by calculating the variance": "highlights that average accuracy degradation is anywhere between 1-13% for different non-linearitybudgets but certain classes seem to be more adversely affected even in low budgets as the max class leveldifference in accuracy is consistent around 30%. Hyperparameter Tuning. Following (Hassani et al., 2021) we use CutMix (Yun et al., 2019), Mixup(Zhang et al., 2017), Randaugment (Cubuk et al., 2020), and Random Erasing (Zhong et al., 2020) as dataaugmentation strategy. We probed multiple hyperparameter strategies for the joint optimization phase ofPriViT to ensure consistent good performance over multiple configurations of non-linearity budgets of softmaxand GELUs. Specifically we describe these strategies as follows: Late-Binarized Epoch (Strategy 1): This strategy involved 10 post-linearization training epochs. Thebinarization of auxiliary parameters, s and c, occurred late in the process, specifically after the linearizationwas complete. The penalty increment condition for this method was checked when the reduction in thesoftmax and GELU coefficients per epoch was less than 200 and 2, respectively. Both masks began withidentical penalties, signifying an equal starting penalty. Late-Binarized Incremental (Strategy 2): This strategy also encompassed 10 training epochs with latebinarization. Here, the penalty increment condition was activated with an increase in the softmax and GELUcoefficients per epoch. The starting penalty for both masks was equal. Late-Binarized Divergent Penalty (Strategy 3): Much like Strategy 2, this involved 10 epochs withlate binarization and an increment condition based on softmax and GELU coefficient rises. However, theinitial penalty was set to unequal, making the softmax penalty 20 times higher than the GELU penalty. Early-Binarized Incremental (Strategy 4): This strategy shared several similarities with Strategy2, including 10 training epochs and an increment condition based on coefficient increases. The difference,however, lay in its early binarization, occurring during the freezing of the auxiliary parameters. The startingpenalty was kept equal for both masks.",
  "-77.8278.835576.2775.9975.72-77.635176.7375.2176.24-77.0821076.0475.23-74.6576.352175.9274.8476.45-76.971576.1274.9976.32-76.96": "Prolonged Early-Binarized Epoch (Strategy 5): Spanning 50 post-linearization training epochs, thisstrategy adopted an early binarization approach. The penalty increment condition was activated when thereduction in softmax and GELU coefficients per epoch was under 200 and 2, respectively. The masks wereinitialized with equal penalties. Each of these strategies offered unique configurations in terms of epoch durations, binarization timings,increment conditions, and starting penalties, enabling a comprehensive assessment of the PriViT algorithmsperformance under various conditions. We test the different finetuning strategies described here by taylorizing PriViT for different softmax andGELU budgets and compare the test accuracy of the resulting model over CIFAR-100. highlightsthe comparative performance of all the strategies that we described. Strategy 5 seems to be performing bestover different configuration of nonlinearity budget which is important as we would want to find the bestmodel peformance for a particular non-linearity budget. Grid search of softmax and GELU configuration. In order to elucidate the nuanced trade-off betweensoftmax and GeLU operations, we executed a systematic grid search across an extensive parameter spaceencompassing varied softmax and GeLU configurations. Upon analysis of models exhibiting iso-latencies,as demarcated by the red lines in figure 9, it became evident that the trade-off dynamics are non-trivial.Specifically, configurations with augmented softmax values occasionally demonstrated enhanced performancemetrics, whereas in other scenarios, models optimized with increased GeLU counts exhibited superiorbenchmark results.",
  "CIFAR-100CIFAR-10": ": PriViTs ability to linearize GeLU operations visualized through performance on CIFAR datasets.As GELU operations decrease, CIFAR-100 and CIFAR-10 accuracies are affected, showcasing the trade-offbetween operation count and accuracy. Effect of using pre-trained checkpoints. To further investigate why using pretrained checkpoint isimproving performance, we report the non-linear distributions searched by PriViT and compare it with PriViTwithout pretrain for the nonlinearity budget of 315k and 320k respectively. We observe from our findings infigures 11,12 that the distribution found by the two methods differs across each layer. This supports ourtheory as to how PriViT operates under a strategic top-down paradigm. Starting with a fine-tuned model,it has the advantage of an architecture that has not just discerned overarching generalization patterns buthas also selectively pruned irrelevant information, streamlining its focus for a specific downstream task. Thisreduction of redundancy, undertaken from a vantage point of a pre-existing knowledge base, gives PriViT anedge.",
  "The following figure shows a graphical representation of the switching operation": "Search granularity.An important characteristic of PriViT is its flexibility to search over differentgranularity of non-linearities. GELU is a pointwise functions, thus PriViT can search either at embeddinglevel or at a token level. On the other hand, softmax is a token level operation, thus it cannot be broken intoa finer search space. Note that softmax operations can be extended to search over the head space or layerspace, and similarly GELU can be searched over the layer space. illustrates the search granularityover token and embedding space. Parametric mask. is an illustration of the working mechanism of the parametric mask introducedin PriViT. When the parameters are binarized it selects one of the two candidate function in the attentionmechanism, and the gelu activation. PriViT overview. provides an illustration of the complete PriViT algorithm, there are threedistinct phases namely Finetuning the teacher, joint optimization of network and parametric masks, and finalfinetuning.",
  "CRole of Delphi in our framework": "Delphi Mishra et al. (2020) encompasses two primary elements: a secure communication protocol and a NAStechnique, specifically an evolutionary method. The secure communication aspect focuses on cryptographicprotocols for neural networks within a multi-party computation setting. The NAS technique, known asDelphis planner, is designed primarily to eliminate ReLU operations from neural architectures. In our paper, we utilize Delphis secure communication protocol for private inference. Furthermore, wetook inspiration from Delphis planner to design an effective method to reduce non-linear operations forattention-based architectures, particularly in Vision Transformers (ViTs). This enhancement was necessarybecause the original Delphis planner is not optimally equipped for reducing non-linearities like softmax, gelu,and layernorms in ViT architectures. provides an estimate of Delphis planner lower bound on latency on ViT-Base with 80M parameters.To estimate this lower bound, we run PriViT on ViT-Base but only replace GELU operations with identityoperations, till no Gelus were left to linearize, i.e. the left most point in the cyan curve indicates a networkwith no Gelus. All softmax and layernorms are left intact, Delphis planner lower bound on latency is definedby the red vertical line which represents the latency of a model with no GELUs, and all softmax/layernormintact. We compare this lower bound with the latency accuracy curve produced by PriViT on ViT-Base thatTaylorize both softmax and GELUs."
}