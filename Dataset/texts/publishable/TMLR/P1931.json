{
  "Abstract": "Transformer models have consistently achieved remarkable results in various domains suchas natural language processing and computer vision. However, despite ongoing researchefforts to better understand these models, the field still lacks a comprehensive understand-ing. This is particularly true for deep time series forecasting methods, where analysis andunderstanding work is relatively limited. Time series data, unlike image and text informa-tion, can be more challenging to interpret and analyze. To address this, we approach theproblem from a manifold learning perspective, assuming that the latent representations oftime series forecasting models lie next to a low-dimensional manifold. In our study, we focuson analyzing the geometric features of these latent data manifolds, including intrinsic di-mension and principal curvatures. Our findings reveal that deep transformer models exhibitsimilar geometric behavior across layers, and these geometric features are correlated withmodel performance. Additionally, we observe that untrained models initially have differentstructures, but they rapidly converge during training. By leveraging our geometric analy-sis and differentiable tools, we can potentially design new and improved deep forecastingneural networks. This approach complements existing analysis studies and contributes to abetter understanding of transformer models in the context of time series forecasting. Codeis released at",
  "Introduction": "Over the past decade, modern deep learning has shown remarkable results on multiple challenging tasks incomputer vision (Krizhevsky et al., 2012), natural language processing (NLP) (Pennington et al., 2014), andspeech recognition (Graves et al., 2013), among other domains (Berman et al., 2023; Naiman et al., 2024;Goodfellow et al., 2016). Recently, the transformer (Vaswani et al., 2017) has revolutionized NLP by allowingneural networks to capture long-range dependencies and contextual information effectively.In addition,transformer-based architectures have been extended to non-NLP fields, and they are among the state-of-the-art (SOTA) models for vision (Dosovitskiy et al., 2020) as well as time series forecasting (TSF) (Wu et al.,2021; Zhou et al., 2022). Unfortunately, while previous works, e.g., Zeiler & Fergus (2014); Karpathy et al.(2015); Tsai et al. (2019) among others, attempted to explain the underlying mechanisms of neural networks,deep transformer models are still considered not well understood. The majority of approaches analyzing the inner workings of vision and NLP transformer models investigatetheir attention modules (Bahdanau et al., 2015) and salient inputs (Wallace et al., 2019). Unfortunately, timeseries forecasting methods have received significantly less attention. This may be in part due to their relativelyrecent appearance as strong contenders on TSF in comparison to non-deep and hybrid techniques (Oreshkinet al., 2020). Further, while vision and NLP modalities may be natural to interpret and analyze (Naiman& Azencot, 2023), time series requires analysis tools which may be challenging to develop for deep models.",
  "Published in Transactions on Machine Learning Research (10/2024)": "are constructed by finding a basis for the tangent space and normal space at a point p by applying PrincipalComponent Analysis, such that the first d coordinates (associated with the most significant modes, i.e.,largest singular values) represent the tangent space, and the rest represent the normal space. We define = [i1, , iK], where ij is given via",
  "Our research lies at the intersection of understanding deep transformer-based models, and manifold learningfor analysis and time series. We focus our discussion on these topics": "Analysis of transformers.Large transformer models have impacted the field of NLP and have led toworks such as Vig (2019) that analyze the multi-head attention patterns and found that specific attentionheads can be associated with various grammatical functions, such as co-reference and noun modifiers. Severalworks (Clark et al., 2019; Tenney et al., 2019; Rogers et al., 2021) study the BERT model (Devlin et al.,2019) and show that lower layers handle lexical and syntactic information such as part of speech, while theupper layers handle increasingly complex information such as semantic roles and co-reference. In Dosovitskiyet al. (2020), the authors inspect patch-based vision transformers (ViT) and find that the models globallyattend to image regions that are semantically relevant for classification.Caron et al. (2021) show thata self-supervised trained ViT produces explicit representations of the semantic location of objects withinnatural images. Chefer et al. (2021) compute a relevancy score for self-attention layers that is propagatedthroughout the network, yielding a visualization that highlights class-specific salient image regions. Nie et al.(2023) recently studied the effectiveness of transformer in TSF in terms of their ability to extract temporal",
  "Background and Method": "Time series forecasting.Given a dataset of multivariate time series sequences D := {xj1:T +h}Nj=1 wherex1:T +h = x1, . . . , xT +h RD, the goal in time series forecasting (TSF) is to accurately forecast the seriesxT +1:T +h, based on the sequence x1:T , where we omit j for brevity. The values T and h are typically referredto as lookback and horizon, respectively. The forecast accuracy can be measured in several ways of which themean squared error (MSE) is the most common. We denote by xT +1:T +h = f(x1:T ) the output of a certainforecast model, e.g., a neural network, then eMSE := 1 hT +ht=T +1 xt xt22 is the forecast error. In our study,we consider T = 96, and h = 96, 192, 336 and 720, and standard benchmark datasets including Electricity,Traffic, ETTm1, ETTm2, ETTh1, ETTh2, and weather (Wu et al., 2021). In App. A, we provide a detaileddescription of the datasets and their properties. Transformer-based TSF deep neural networks.State-of-the-art (SOTA) deep time series forecastingmodels appeared only recently (Oreshkin et al., 2020), enjoying a rapid development of transformer-basedarchitectures, e.g., Zhou et al. (2021); Wu et al. (2021); Liu et al. (2021); Zhou et al. (2022); Nie et al. (2023),",
  "Autoformer": "ETTm1ETTm2ETTh1ETTh2weatherelectricity MAPC 0.2 0.3 0.4 0.5 FEDformer : MAPC is correlated with model performance. Each color represents a different datasetwhile the size of the dot is determined by the forecast horizon (longer horizon results in a larger dot). Thetest mean squared error is proportional to the MAPC on multiple datasets.",
  "Data manifolds share similar geometric profiles": "In our first empirical result, we compute the intrinsic dimension (ID) and mean absolute principal curvature(MAPC) across the layers of Autoformer and FEDformer models on the traffic dataset. In , we plotthe ID (top row) and MAPC (bottom row) for Autoformer (left column) and FEDformer (right column)on multiple forecast horizons = 96, 192, 336, 720. The x-labels refer to the layers we sample, where labels1 to 4 refer to two sequence decomposition layers per encoder block (and thus four in total), labels 5 to 6denote the decoder decomposition layers, and label 7 is the linear output layer, see for the networkscheme. Our results indicate that during the encoding phase, the ID and the MAPC are relatively fixed forAutoformer and decrease for FEDformer, and during the decoder module, these values generally increasewith depth. Specifically, the ID values change from min(ID) = 1.2 to max(ID) = 8.1, showing a relativelysmall variation across layers. In comparison, the mean absolute principal curvature values present a largerdeviation as they range from min(MAPC) = 0.2 to max(MAPC) = 19.2.",
  "Final MAPC is correlated with performance": "We further investigate whether geometric properties of the learned manifold are related to inherent featuresof the model. For instance, previous works find a strong correlation between the ID (Ansuini et al., 2019) andMAPC (Kaufman & Azencot, 2023) with model performance and generalization. Specifically, the intrinsicdimension in the last hidden layer is correlated with the top-5 score on image classification, i.e., lower ID isassociated with lower error. Similarly, large normalized MAPC gap between the penultimate and final layersof CNNs is related to high classification accuracy. These correlations are important as they allow developersand practitioners to evaluate and compare deep neural networks based on statistics obtained directly fromthe train set. This is crucial in scenarios where, e.g., the test set is unavailable during model design. We show in plots of the test mean squared error (eMSE) vs. the MAPC in the final layer of Autoformerand FEDformer models trained on ETTm1, ETTm2, ETTh1, ETTh2, weather, and electricity. For eachdataset, we plot four colored circles corresponding to the four different horizons, where each circle is scaledproportionally to the horizon length. The dashed graphs are generated by plotting the eMSE with respect tothe MAPC, representing the best linear fit for each dataset. Due to different scales, an inlay of the electricitydataset is added to the bottom right corner of each architecture. We find a positive slope in all Autoformerand FEDformer models with an average correlation coefficient of 0.76 and 0.7, respectively (see full resultsin Tab. 1). In all cases, we observe a correlation between the test MSE and final MAPC, namely, the modelperforms better as curvature decreases, as shown in . MAPC 0.4 0.6 MSE",
  "FEDformer0.740.860.630.500.580.70": "As in Sec. 4.1, we identify different characteristics for TSF models with respect to classification neuralnetworks. While popular CNNs show better performance when the MAPC gap is high (Kaufman & Azencot,2023), we report an opposite trend, namely, better models are associated with a lower MAPC. We believe thisbehavior may be attributed to classification networks requiring final high curvatures to properly classify thestatistical distribution of the input information and its large variance. In addition, we note the relatively flatslope profiles presented across all datasets and architectures. Essentially, these results indicate that while themanifolds become more complex in terms of curvature, transformers yield a relatively fixed MSE, regardlessof the underlying MAPC. Thus, our results may hint that Autoformer and FEDformer are not expressiveenough. Indeed, the datasets we consider consist of many features (on the range of hundreds for electricityand traffic). Therefore, while TSF approaches may need highly expressive networks to model these datasetsdue to their complex statistics, it might be that current approaches can not achieve better representationsin these cases, and they tend to get stuck on a local minimum. We hypothesize that significantly deeperand more expressive TSF models as in Nie et al. (2023) and is common in classification (He et al., 2016)may yield double descent forecasting architectures (Belkin et al., 2019).",
  "Manifold dynamics during training": "Our analysis above focuses on fully trained deep neural networks and the geometric properties of the learnedmanifolds. In addition, we also investigate below the evolution of manifolds during training and their struc-ture at initialization. In prior works, Ansuini et al. (2019) observed that randomly initialized architecturesexhibit a constant ID profile, and further, there is an opposite trend in ID in intermediate layers vs. finallayers of convolutional neural networks during training. Kaufman & Azencot (2023) find that untrainedmodels have different MAPC profiles than trained networks, and they observe that the normalized MAPCgap consistently increases with model performance during training. Moreover, ID and MAPC profiles con-verge consistently to their final configuration as training proceeds. Motivated by their analysis, we studythe general convergence and trend of ID and MAPC of TSF models during training evolution. We show in the ID and MAPC profiles for Autoformer and FEDformer during training, where eachplot is colored by its sampling epoch using the hot colormap. First, the untrained ID and MAPC profiles(dashed black) are somewhat random in comparison to the other geometric profiles. Second, the overallconvergence to the final behavior is extremely fast, requiring approximately five epochs to converge in all ID",
  "Distribution of principal curvatures": "The CAML algorithm (Li, 2018) we employ for estimating the principal curvatures produces d(D d)values per point, yielding a massive amount of curvature information for analysis. Following Kaufman &Azencot (2023), we compute and plot in the distribution of principal curvatures for every layer,shown as a smooth histogram for Autoformer and FEDformer models on electricity and traffic datasets withhorizons = 192, 336. The histogram plots are colored by the network depth using the hot colormap. Thesedistributions strengthen our analysis in Sec. 4.1 where we observe a step-like pattern in MAPC, wherethe sharp jump in curvature occurs at the beginning of the decoder. Indeed, the curves in related tolayers 1 to 4 span a smaller range in comparison to the curves in layers 5 to 7. Further, the histograms showthat the distribution of curvature is relatively fixed across the encoder blocks, and similarly, a different butrather fixed profile appears in the decoder.",
  "Discussion": "Deep neural networks are composed of several computation layers. Each layer receives inputs from a precedinglayer, it applies a (nonlinear) transformation, and it feeds the outputs to a subsequent layer. The overarchingtheme in this work is the investigation of data representations arising during the computation of deep models.To study these latent representations, we adopt a ubiquitous ansatz, commonly known as the manifoldhypothesis (Coifman & Lafon, 2006): We assume that while such data may be given in a high-dimensionaland complex format, it lies on or next to a low-dimensional manifold. The implications of this inductive biasare paramount; manifolds are rich mathematical objects that are actively studied in theory (Lee, 2006) and",
  "practice (Chaudhry et al., 2018), allowing one to harness the abundant classical tools and recent developmentsto study deep representations": "Our study aligns with the line of works that aims at better understanding the inner mechanisms of deep neuralnetworks.Indeed, while modern machine learning has been dominating many scientific and engineeringdisciplines since the appearance of AlexNet (Krizhevsky et al., 2012), neural net architectures are stillconsidered not well understood by many. In this context, the manifold ansatz is instrumentalexistinganalysis works investigate geometric features of latent manifolds and their relation to the underlying taskand model performance. For instance, Ansuini et al. (2019) compute the intrinsic dimension of popularconvolutional neural networks. Following their work, Kaufman & Azencot (2023) estimate the mean absoluteprincipal curvatures. However, while CNNs are relatively studied from the manifold viewpoint, impactfulsequential transformer models (Vaswani et al., 2017), received less attention (Valeriani et al., 2023). The lackof analysis is even more noticeable for the recent state-of-the-art transformer-based time series forecastingworks, e.g., Wu et al. (2021). The main objective of our work is to help bridge this gap and study deepforecasting models trained on common challenging datasets from a manifold learning viewpoint. We compute the ID and MAPC of data representations from several different deep architectures, forecastingtasks and datasets. To this end, we employ differentiable tools (Facco et al., 2017; Li, 2018) that produce asingle scalar ID and many principle curvatures combined to a single scalar MAPC, per manifold. Our resultsraise several intriguing observations, many of them are in correspondence with existing work. First, the IDis much smaller than the extrinsic dimension, reflecting that learned manifolds are indeed low-dimensional.Second, the ID and MAPC profiles across layers are similar for many different architectures, tasks, anddatasets. In particular, we identify two phases, where in the encoder, ID and MAPC are decreasing or stayfixed, and in the decoder, both geometric features increase with depth. Third, the MAPC in the final layeris strongly correlated with model performance, presenting a correlation, i.e., error is lower when MAPC islower. Fourth, we observe that related but different datasets attain similar manifolds, whereas unrelateddatasets are associated to manifolds with different characteristics. Finally, untrained models present randomID and MAPC profiles that converge to their final configuration within a few epochs. Our analysis and observations lie at the heart of the differences between classification and regression tasks;a research avenue that only recently had started to be addressed more frequently (Muthukumar et al., 2021;Yao et al., 2022). Our results indicate a fundamental difference between image classification and time seriesforecasting regression models: while the former networks shrink the ID significantly to extract a meaningfulrepresentation that is amenable for linear separation, TSF models behave differently. Indeed, the ID generallyincreases with depth, perhaps to properly capture the large variance of the input domain where regressionnetworks predict. Moreover, while a high MAPC gap seems to be important for classification, we find anopposite trend in TSF regression problems. In conclusion, we believe that our work sets the stage for adeeper investigation of classification vs. regression from a manifold learning and other perspectives. Webelieve that fundamental advancements on this front will lead to powerful machine learning models, bettersuited for solving the task at hand.",
  "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning toalign and translate. In 3rd International Conference on Learning Representations, ICLR, 2015": "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practiceand the classical biasvariance trade-off. Proceedings of the National Academy of Sciences, 116(32):1584915854, 2019. Nimrod Berman, Ilan Naiman, and Omri Azencot. Multifactor sequential disentanglement via structuredKoopman autoencoders. In The Eleventh International Conference on Learning Representations, ICLR,2023. Nimrod Berman, Ilan Naiman, Idan Arbiv, Gal Fadlon, and Omri Azencot. Sequential disentanglement byextracting static information from A single sequence element. In Forty-first International Conference onMachine Learning, ICML, 2024.",
  "Pratik Prabhanjan Brahma, Dapeng Wu, and Yiyuan She. Why deep learning works: A manifold disen-tanglement perspective. IEEE transactions on neural networks and learning systems, 27(10):19972008,2015": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza, Max Mergenthaler, and Artur Dubrawski.N-HiTS: Neural hierarchical interpolation for time series forecasting. arXiv preprint arXiv:2201.12886,2022. Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. Riemannian walkfor incremental learning: Understanding forgetting and intransigence. In Proceedings of the Europeanconference on computer vision (ECCV), pp. 532547, 2018.",
  "Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016": "Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neuralnetworks. In 2013 IEEE international conference on acoustics, speech and signal processing, pp. 66456649.Ieee, 2013. Min Han, Shoubo Feng, CL Philip Chen, Meiling Xu, and Tie Qiu. Structured manifold broad learning sys-tem: A manifold perspective for large-scale chaotic time series analysis and prediction. IEEE Transactionson Knowledge and Data Engineering, 31(9):18091821, 2018.",
  "Ruei-Sung Lin, Che-Bin Liu, Ming-Hsuan Yang, Narendra Ahuja, and Stephen E. Levinson.Learningnonlinear manifolds from time series. In Computer Vision - ECCV, 2006": "Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer:Low-complexity pyramidal attention for long-range time series modeling and forecasting. In Internationalconference on learning representations, 2021. Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and Anant Sahai.Classification vs regression in overparameterized regimes: Does the loss function matter? The Journal ofMachine Learning Research, 22(1):1010410172, 2021.",
  "Ilan Naiman and Omri Azencot. An operator theoretic approach for analyzing sequence neural networks. InThirty-Seventh AAAI Conference on Artificial Intelligence, AAAI, pp. 92689276. AAAI Press, 2023": "Ilan Naiman, Nimrod Berman, and Omri Azencot. Sample and predict your latent: modality-free sequentialdisentanglement via contrastive estimation. In International Conference on Machine Learning, pp. 2569425717. PMLR, 2023. Ilan Naiman, N. Benjamin Erichson, Pu Ren, Michael W. Mahoney, and Omri Azencot. Generative modelingof regular and irregular time series data via Koopman VAEs. In The Twelfth International Conference onLearning Representations, ICLR, 2024. Xuan Son Nguyen, Luc Brun, Olivier Lzoray, and Sbastien Bougleux. A neural network based on spdmanifold learning for skeleton-based hand gesture recognition. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1203612045, 2019. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.A time series is worth 64words: Long-term forecasting with transformers. In The Eleventh International Conference on LearningRepresentations, ICLR, 2023.",
  "Tal Shnitzer, Ronen Talmon, and Jean-Jacques E. Slotine. Manifold learning with contracting observers fordata-driven time-series analysis. IEEE Trans. Signal Process., 65(4):904918, 2017": "Cory Stephenson, Suchismita Padhy, Abhinav Ganesh, Yue Hui, Hanlin Tang, and Sue Yeon Chung. On thegeometry of generalization and memorization in deep neural networks. In 9th International Conferenceon Learning Representations, ICLR 2021, 2021. Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings ofthe 57th Annual Meeting of the Association for Computational Linguistics, pp. 45934601, 2019. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.Transformer dissection: a unified understanding of transformers attention via the lens of kernel. arXivpreprint arXiv:1908.11775, 2019. Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and AlbertoCazzaniga.The geometry of hidden representations of large transformer models.arXiv preprintarXiv:2302.00294, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Jesse Vig. A multiscale visualization of attention in the transformer model. In Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics: System Demonstrations, pp. 3742, 2019. Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh. Allennlpinterpret: A framework for explaining predictions of NLP models. In Proceedings of the 2019 Conferenceon Empirical Methods in Natural Language Processing and the 9th International Joint Conference onNatural Language Processing, EMNLP-IJCNLP, pp. 712, 2019. Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers withauto-correlation for long-term series forecasting. Advances in Neural Information Processing Systems, 34:2241922430, 2021.",
  "Tao Yu, Huan Long, and John E Hopcroft. Curvature-based comparison of two neural networks. In 201824th International Conference on Pattern Recognition (ICPR), pp. 441447. IEEE, 2018": "Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,Part I 13, pp. 818833. Springer, 2014. Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting?In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 1112111128, 2023. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. In-former: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAIconference on artificial intelligence, volume 35, pp. 1110611115, 2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency en-hanced decomposed transformer for long-term series forecasting. In International Conference on MachineLearning, pp. 2726827286. PMLR, 2022.",
  "Blow we provide a detailed description of the datasets used in the paper. A summery of the datasets can befound in Tab. 2": "Electricity Transformer Temperature (ETT) Zhou et al. (2021):The ETT contains electricitypower load (six features) and oil temperature collected over a period of two years from two countries inChina. The dataset is versatile and exhibits short-term periodical patterns, long-term periodical patterns,long-term trends, and irregular patterns. The dataset is further divided to two granularity levels: ETTh1,ETTh2 for one hour level and ETTm1, ETTm2 for 15 minutes level.",
  "B.1ETT* datasets analysis": "To complement the results in the main article we add a comparison of the ID and MAPC of three differentETT datasets: ETTm1, ETTh1 and ETTh2. We notice that ETTm1, ETTh1 and ETTh2 show a hunchbacktrend for Autoformer while for FEDformer the hunchback trend appears for larger forecast horizons as shownin . Our MAPC results in show that MAPC are relatively fixed and start to rise at the decoder.",
  "Here we will provide a supplementary analysis of additional TSF models: vanilla Transformer (Vaswaniet al., 2017), Informer (Zhou et al., 2021) and PatchTST (Nie et al., 2023)": "Transformer and Infromer: These models have a similar architecture to the Autoformer and FEDformeras shown in . The models are composed of two encoder layers and one decoder layer, however, in contrastto Autoformer and FEDformer, Transformer and Infromer do not contain series decomposition layers. Theanalysis of Transformer and Infromer inspects the output of the encoder layers, decoder layer and the lastlinear layer. In we observe trends similar to ones shown for the Autoformer and FEDformer. Whencomparing different datasets, we see similar trends, a monotonic increase in ID for the Transformer anda saw-like behaviour for Informer (see ). The MAPC across datasets for the Transformer exhibits amonotonic increase trend while the Informer has a v-shape (see )",
  "Id": "TransformerInformer layer depth : ID profiles across layers of Transformer and Informer on electricity, traffic, weatherand ETTm1 datasets for multiple forecasting horizons. Each panel includes separate MAPC profilesper dataset, for several horizons (left to right) and architectures (top to bottom). sequence into patches and using them as inputs to the network. Each patch is then embedded, and fromthat point on, all sequential information becomes inaccessible, i.e., we cannot manually extract temporalinformation for each time stamp in the patch. Our analysis focuses on time series manifolds, where eachelement represents a single point in time, while the latent representation of the PatchTST model lie onproduct manifolds where each element is a sequence of points in time. Similar to the analysis performed onthe Transformer and Informer, we inspect the output of the encoder layers and the decoder layer which is thelast linear layer. We notice that the ID remains relatively stable during the encoding phase and increases inthe last layer, a behavior consistent across datasets and horizons. A similar trend is observed for the MAPCduring the encoding phase where on some datasets the MAPC increases on the last layer and on other itdecreases. See Figs. 13 and 14.",
  "D.1Intrinsic dimension": "To estimate the ID of data representations in TSF neural networks, we use the TwoNN (Facco et al., 2017)global id estimator. The ID-estimator utilizes the distances only to the first two nearest neighbors of eachpoint. This minimal selection helps reduce the impact of inconsistencies in the dataset during the estimationprocess. MAPC electricitytraffic weatherETTm1 MAPC TransformerInformer layer depth : MAPC profiles across layers of Transformer and Informer on electricity, traffic,weather and ETTm1 datasets for multiple forecasting horizons.Each panel includes separateMAPC profiles per dataset, for several horizons (left to right) and architectures (top to bottom).",
  "i=1(d+1)i(1)": "we follow the method proposed by Facco et al. (2017) based on the cumulative distribution F() = 1 d.The idea is to estimate d by a linear regression on the empirical estimate of F(). This is done by sortingthe values of in ascending order and defining F emp (i) .=iN . A straight line is then fitted on the pointsof the plane {(log i, log (1 F empi))}Ni=1. The slope of the line is the estimated ID. ID ETTm1ETTm2ETTh1 ETTh2Electricityweather MAPC layer depth : ID and MAPC profiles across layers of PatchTST on ETT*, electricity and weatherdatasets for multiple forecasting horizons. Each panel includes separate ID (top) and MAPC (bottom)profiles per dataset, for several horizons (left to right).",
  "D.2Comparison of intrinsic dimension estimators": "ID estimators operate based on a variety of principles. They are developed using specific features such as thenumber of data points within a fixed-radius sphere, linear separability, or the expected normalized distanceto the nearest neighbor. Consequently, different ID estimation methods yield varying ID values. In we proivde a comparison of several ID estimators from Scikit-dimension (Bac et al., 2021). The resultsindicate that the ID profile obtained by the TwoNN estimator used in this work is simillar to other IDestimation tools. We observe a different trend from the LPCA estimator which we attribute to the linearnature of the algorithm. LPCA is grounded in the observation that for data situated within a linear subspace,the dimensionality corresponds to the count of non-zero eigenvalues of the covariance matrix. We note thatthe inconsistency appears in the layers where the curvature increases and thus the manifold deviates from aflat (linear) surface.",
  "D.3Data density": "In comparison to the intrinsic dimension, which is a characteristic of the entire manifold, curvature in-formation is local.Moreover, curvatures are calculated using second-order derivatives of the manifold.Consequently, our study assumes that the data is dense enough to compute curvatures. However, the la-tent representations of data, are both high-dimensional and sparse, which presents significant difficulties incalculating local differentiable values on such as curvature. The typical characteristics of data used in machine learning require a large number of nearby points tocreate a stable neighborhood. One commonly used tool for this is k-Nearest-Neighbours (KNN). However,KNN can sometimes generate non-local and sparse neighborhoods, where the \"neighbors\" are effectively farapart in a Euclidean sense. Another approach is to use domain-specific augmentations, such as windowcropping, window warping or slicing. However, this approach only explores a specific aspect of the datamanifold and may overlook other important parts. A more effective approach, regardless of the domain, is tocompute the Singular Value Decomposition (SVD) for each time series. This generates a close neighborhoodby filtering out small amounts of noise in the data. This approach is well-motivated from a differentialgeometry standpoint, as it approximates the manifold at a point and samples the neighborhood. Neighborhood generation.To improve the local density of time sereis samples, we use a proce-dure similar to Yu et al. (2018) to generate artificial new samples by reducing the noise levels of theoriginal data.Specifically, given a d dimensional time series x1:T RT d, let x1:T = UV T be itsSVD, where U RT T , V Rdd and RT d a rectangular diagonal matrix with singular values{1, 2, , d} on the diagonal in descending order such that r is the rank of x1:T . Let m be the smallestindex such that the explained varince2m",
  "D.4Curvature estimation": "There are several methods available for estimating curvature quantities of data representations, as discussedin papers such as Brahma et al. (2015); Shao et al. (2018). For our purposes, we have chosen to use thealgorithm described in Li (2018), which is called Curvature Aware Manifold Learning (CAML). We optedfor this algorithm because it is supported by theoretical foundations and is relatively efficient. In order touse CAML, we need to provide the neighborhood information of a sample and an estimate of the unknownID. The ID is estimated using the TwoNN algorithm, as described in D.3, similarly to Ansuini et al. (2019);Kaufman & Azencot (2023). In order to estimate the curvature of data Y = {y1, y2, , yN} RD, we make the assumption that thedata lies on a d-dimensional manifold M embedded in RD, where d is much smaller than D. Consequently,M can be considered as a sub-manifold of RD. The main concept behind CAML is to compute a localapproximation of the embedding map using second-order information.",
  "f : Rd RD, yi = f(xi) + i ,i = 1, . . . , N ,(2)": "where X = {x1, x2, , xN} Rd are low-dimensional representations of Y , and {1, 2, N} are thenoises.In the context of this paper, the embedding map f is the transformation that maps the low-dimensional dynamics to the sampled features for each time stamp t that might hold redundant information. In order to estimate curvature information at a point yi Y , we follow the procedure described above todefine its neighborhood. This results in a set of nearby points {yi1, . . . , yiK}, where K represents the numberof neighbors. Using this set along with the point yi, we utilize SVD to construct a local natural orthonormalcoordinate frame",
  "yDd. This coordinate frame consists of a basis for the tangent": "space (first d elements) and a basis for the normal space. To be precise, we denote the projection of yi andyij for j = 1, . . . , K onto the tangent space spanned by /x1, . . . , /xd as xi and uij respectively. It isimportant to note that the neighborhood of yi must have a rank of r > d. If the rank is less than d, thenSVD cannot accurately encode the normal component at xi, leading to poor approximations of f at xi.Therefore, we verify that {yi1, . . . , yiK} has a rank of d + 1 or higher. The map f can be expressed in the alternative coordinate frame as f(x1, . . . , xd) = [x1, . . . , xd, f 1, . . . , f Dd].The second-order Taylor expansion of f at uij with respect to xi, with an error of O(|uij|22), is representedby",
  "xixjis its Hessian. We have a neighborhood {yi1, . . . , yiK} of yi,": "and their corresponding tangent representations {uij}. Using equation 3, we can form a system of linearequations, as explained in D.5. The principal curvatures are the eigenvalues of H, so estimating curvatureinformation involves solving a linear regression problem followed by an eigendecomposition. Each Hessian hasd eigenvalues, so each sample will have (D d) d principal curvatures. Additionally, one can compute theRiemannian curvature tensor using the principal curvatures, but this requires high computational resourcesdue to its large number of elements. Moreover, as the Riemannian curvature tensor is fully determined bythe principal curvatures, we focus our analysis on the eigenvalues of the Hessian. To evaluate the curvatureof manifolds, we estimate the mean absolute principal curvature (MAPC) by taking the mean of the absolutevalues of the eigenvalues of the estimated Hessian matrices.",
  "D.5Estimating the Hessian Matrix": "In order to estimate the Hessian of the embedding mapping f where = 1, . . . , D d, we build a set oflinear equations that solves Eq. 3. We approximate f by solving the system f = Xi, where Xi holdsthe unknown elements of the gradient f and the hessian H. We define f = [f (ui1) , , f (uiK)]T ,where uij are points in the neighborhood of xi, in the local natural orthogonal coordinates. The local naturalorthogonal coordinates are a set of coordinates that are defined at a specific point p of the manifold. They",
  "ij =u1ij, , udij,u1ij2, ,udij2,u1ij u2ij, ,ud1ij udij": "The set of linear equations f = Xi is solved by using the least square estimation resulting in Xi = f ,where Xi =f 1, , f d, H1,1, , Hd,d, H1,2, , Hd1,d. In practice, we estimate only theupper triangular part of H since it is a symmetric matrix. The gradient values f are ignored sincethey are not required for the CAML algorithm. We refer the reader for a more comprehensive and detailedanalysis in (Li, 2018)."
}