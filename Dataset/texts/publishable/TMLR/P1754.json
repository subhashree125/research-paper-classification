{
  "Abstract": "Gaussian Mixture Models (GMMs) have been recently proposed for approximating actorsin actor-critic reinforcement learning algorithms. Such GMM-based actors are commonlyoptimized using stochastic policy gradients along with an entropy maximization objective.In contrast to previous work, we define and study deterministic policy gradients for optimiz-ing GMM-based actors. Similar to stochastic gradient approaches, our proposed method,denoted Gaussian Mixture Deterministic Policy Gradient (Gamid-PG), encourages policyentropy maximization.To this end, we define the GMM entropy gradient using Varia-tional Approximation of the KL-divergence between the GMMs component Gaussians. Wecompare Gamid-PG with common stochastic policy gradient methods on benchmark dense-reward MuJoCo tasks and sparse-reward Fetch tasks. We observe that Gamid-PG outper-forms stochastic gradient-based methods in 3/6 MuJoCo tasks while performing similarly onthe remaining 3 tasks. In the Fetch tasks, Gamid-PG outperforms single-actor determinis-tic gradient-based methods while performing worse than stochastic policy gradient methods.Consequently, we conclude that GMMs optimized using deterministic policy gradients (1)should be favorably considered over stochastic gradients in dense-reward continuous controltasks, and (2) improve upon single-actor deterministic gradients.",
  "Introduction": "This study presents a comparison between deterministic and stochastic policy gradients for optimizing policiesrepresented as Gaussian Mixture Models (GMMs) in model-free deep reinforcement learning (RL). Model-free RL was successfully demonstrated in various control domains. Examples include video games (Mnihet al., 2015; Wurman et al., 2022), robotic control (Elguea-Aguinaco et al., 2023; Dey et al., 2021; 2024; Koberet al., 2013), traffic applications (Ault & Sharon, 2021; Sharon, 2021), and medical procedures (Zhou et al.,2021; Coronato et al., 2020). Specifically, actor-critic methods (Lillicrap et al., 2015; Fujimoto et al., 2018;Haarnoja et al., 2018; Schulman et al., 2017) have proven to be effective for RL in continuous control domains.These methods were shown to be most effective when coupled with high-capacity function approximatorssuch as neural networks for approximating both the actor and the critic. Prior work on actor approximation can be broadly divided into two categories; (1) deterministic (Silver et al.,2014), where the policy approximator maps states to actions, and (2) soft (Haarnoja et al., 2018), where thepolicy approximator maps states to a distribution defined over the action space. Each of these two classes hasknown benefits and limitations. In particular, deterministic actors are unable to represent complex policiesthat capture several modes of optimal behavior while soft actors commonly assume a specific parametricdistribution (often oversimplified) for which a closed-form gradient, with respect to the action distribution",
  "Published in Transactions on Machine Learning Research (12/2024)": "Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, JonasSchneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for research, 2018. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.Stable-baselines3: Reliable reinforcement learning implementations. The Journal of Machine LearningResearch, 22(1):1234812355, 2021.",
  "Reinforcement learning (RL)": "In RL, a policy (actuation function) is optimized over an underlying Markov decision process (MDP) whichis a tuple {S, A, P, R, , I0}. S is the state space; A is the action space; P(s, a, s) is the transitionprobability of the form P : S A S , representing the probability of transitioning from state s tostate s after taking action a; R(s, a) is the reward function of the form R : S A R, representing theimmediate utility gained from being in state s and taking action a; is the discount factor, representing thefactor of lost utility in future rewards; finally, I0 is a distribution over the initial state. An RL agent is assumed to follow an internal policy, , which maps states to actions. is commonlydefined either as deterministic, i.e., : S A, or as soft (stochastic), i.e., mapping states to a distributionover actions. The agents chosen action (at) at the current state (st) affects the environment such thata new state emerges (st+1) as well as some reward (rt) representing the immediate utility gained from",
  "t trt. In RL the observed return is used to tune a policy suchthat J() is maximized. The policy arg max[J()] is the optimal policy and is denoted by .1": "Common RL frameworks include value-based, policy-gradient, and actor-critic approaches.A value-based approach attempts to learn the expected future utility from states (state value) or from action-statepairs (action value or q-value).The policy returns actions that maximize the expected utility ((s) =arg maxa[Q(s, a)]).A prominent example of a value-based approach is the model-free deep Q-learningalgorithm (DQN) (Mnih et al., 2015). In the policy-gradient approach (Williams, 1992) a policy is definedthrough a parameterized differential equation, where the policy parameters are iteratively updated, followingthe policy gradient, towards favorable outcomes (as experienced through the reward function). Using state oraction value approximations for defining favorable outcomes for policy-gradient updates is usually referredto as an actor-critic approach. A prominent example of a state-of-the-art actor-critic approach is deep-deterministic-policy-gradient (DDPG) (Lillicrap et al., 2015; Fujimoto et al., 2018).",
  "Distributional RL": "In order to capture the intrinsic uncertainty of MDPs, a line of publications proposed to extend value-based approaches to estimate the distribution over returns. These include SPL-DQN (Luo et al., 2021),distributional-policy-gradient (Singh et al., 2022; Song & Zhao, 2020), and distributed-distributional-DDPG(D4PG) (Barth-Maron et al., 2018). These approaches still converge on a deterministic policy and, thus,might fail to capture diverse modes of optimal behavior, i.e., while the critic (Q-network) can capture multiplemodes of optimality in the Q-value space, the (unimodal) actor is limited in its ability to do the same.",
  "Soft policy density approximation": "Another line of work suggests training a policy as a distribution over actions, i.e., a soft policy. The fit-ted distribution is commonly parametric to allow closed-form gradient computation. For example, trainingmean and variance parameters of a Gaussian. Common parametric policy distributions used in the liter-ature include Gaussian (Schulman et al., 2015; 2017), Beta (Chou et al., 2017), and Delta (Silver et al.,2014; Lillicrap et al., 2015; Fujimoto et al., 2018). While such parametric distributions are easy to train(having closed-form gradients), they provide a limited representation power. Addressing this issue, Tessleret al. (2019) proposed Generative Actor-Critic (GAC). It applies Quantile-Regression (Koenker & Hallock,2001) over Autoregressive-Implicit-Quantile-Networks (Ostrovski et al., 2018) that can represent arbitrarilydensities. However, the reported results for GAC are not better (asymptotic performance and sample effi-ciency) than those reported for Soft Actor-Critic (SAC) (Haarnoja et al., 2018) which uses a single Gaussianas a soft actor policy. Another approach for capturing complex distributional properties, such as skewness,kurtosis, multimodality, and covariance structure is the Semi-Implicit Actor (SIA) (Yue et al., 2020). Thisapproach adopts a semi-implicit hierarchical construction (Yin & Zhou, 2018) for fitting highly expressive(yet not general) parametric distributions.",
  "Deep Deterministic Policy Gradient (DDPG)": "DDPG (Lillicrap et al., 2015) is a benchmark continuous control RL algorithm that trains both a critic, as adifferentiable Q-function approximator, Q : S A R, and a deterministic differentiable actor, : S A. DDPG Training.Given a randomly sampled batch of transitions, each of type < s, a, r, s >, (1) thecritic is trained to minimize the L2 TD-error (Doya, 1995), i.e., minimize:( Q(s, a) (r + Q(s, (s))))2;and (2) the actor, , is trained to maximize Q(s, (s)) while assuming the Q parameters constant. DDPG Exploration.An advantage of off-policy algorithms such as DDPG is that they can treat theproblem of exploration independently from the trained policy. As such, DDPG performs random explorationby sampling a noise value from an Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930) to generatetemporally correlated noise.The authors do not mention a particular reason for this choice (applyingcorrelated noise). Moreover, the authors mention that other random noise processes can be used. In ourexperiments with DDPG, we found that comparable results are achieved when the noise is sampled using a(simpler) non-correlated Gaussian with mean zero and variance, I.",
  "Gaussian Mixture Deterministic Policy Gradient": "We propose a variant of the DDPG approach termed Gaussian Mixture Deterministic Policy Gradient(GAMID-PG), or Gamid for short, where we (1) define the actor as a mixture of N Gaussians (insteadof a single Gaussian in DDPG), (2) define the actors policy through GMM sampling, and (3) include aGMM diversification objective as part of the actors gradient. The main motivation for the proposed al-gorithm is to enable deterministic policy gradients for general densities. This is justified by (1) the factthat specific types of density functions e.g., a single Gaussian, are sometimes incapable of converging to anoptimal policy (Tessler et al., 2019), and (2) benefits reported for training such general-density actors insoft-actor-critic optimization (with non-deterministic policy gradients) (Ren et al., 2021). Similar to DDPG,Gamid assumes MDPs with a continuous action space as the underlying environment. Our Gamid approachis detailed in Algorithm 1 available in the form of pseudocode. The hyper(meta)-parameters include (1) thenumber of Gaussians for the GMM policy, N, (2) a shared variance for all Gaussians as a scaled identitymatrix, , (3) a policy divergence temperature (possibly decaying), , and, (4) the target network updaterate, .",
  "end": "At this point, one might wonder are GMMs still considered universal density approximators when usinga single shared variance?.The answer is Yes.Calcaterra (2008) showed that linear combinations ofGaussians with a single variance are, indeed, a universal density approximator.However, considering asingle variance commonly requires combining more Gaussian in order to reach similar approximation accu-racy levels. Nonetheless, we observed (empirically) that optimal performance for Gamid is achieved whenthe number of Gaussians is fairly low (25). Moreover, setting a constant variance allows for a practicalapproximation of an entropy gradient term, as described later in .2. Given the shared variance, the GMM policy is defined solely by N means. These means are approximatedper Gaussian, i [0, . . . , N 1], with a differential function approximator, i : S [i] R, where [i]are the approximators tunable parameters. = i [i] is initialized randomly. Similarly, the Q-functionapproximators parameters, , are also initialized randomly.",
  "GMM policy sampling": "Sampling an action for a given state, s, from the N Gaussians GMM is performed in two stages. First,(Line 3), we sample one Gaussian, n, from a distribution defined by the GMM weighting coefficients, P.Next, (Line 4), we sample an action from n. P can be set in many ways, e.g., as a uniform distribution,i, pi = 1/N. We observed (empirically) that setting the coefficients using an -greedy approach (Mnihet al., 2015) is beneficial. That is, set pi = 1 + /N for arg maxi Q(s, (s; [i])) and j = i, pj = /N.This approach, however, introduces an extra hyperparameter, , which requires meta tuning.We alsoexperimented with existing approaches for tuning P to be proportional to the Q-values Ren et al. (2021).However, such an approach was not observed to perform better compared to the simpler -greedy approach.",
  "GMM training": "Known techniques for training GMM approximators from samples are mostly applicable for supervisedlearning, i.e., when the target GMM distribution is known (Figueiredo et al., 1999) or can be sampled (Arenzet al., 2020).Since, in our case, is not known a priori and is not available for sampling, we applya deterministic-policy-gradient approach for training the GMM (as the actor in Gamid).The proposedgradient is defined with respect to one Gaussian (i [0, . . . , N 1]) and one state, s. Similar to the originalDDPG algorithm, since we consider i to be constant, the gradient is defined only with respect to the mean,(s; [i]). It is based on two (possibly conflicting) optimization objectives.",
  ". Maximize the cross-entropy between Gaussian i and the GMM excluding Gaussian i, i.e.,max[i][N(x; (s; [i]), ) log GMMN\\i(x; (s; )dx]": "Optimization objective #1 follows the original DDPG actor training procedure. Optimization objective #2is inspired by MaxEnt frameworks, which complement the standard maximum reward objective (Objective#1) with an entropy maximization term (Aghasadeghi & Bretl, 2011; Toussaint, 2009; Rawlik et al., 2012;Haarnoja et al., 2018; Fox et al., 2016).These two optimization objectives are presented in Line 11 ofAlgorithm 1 where policy i is updated in the direction that increases the approximated Q value ANDincreases the Kullback-Leibler (KL) Divergence (Kullback, 1997) from a GMM distribution which includesall the other Gaussians (other than i). At this point, the reader might wonder Why maximize the KL-divergence and not the cross entropy as stated above?.There are two reasons for this choice: (1) inGamid, KL-divergence and cross entropy are equivalent with respect to the resulting gradients; (2) it allowsutilization of state-of-the-art KL-divergence approximation techniques for GMMs (Hershey & Olsen, 2007). From cross-entropy to KL-divergence.Cross-entropy (H) is similar to KL-Divergence (DKL) with theaddition of an entropy term. More specifically, the cross-entropy of a distribution f relative to a distributiong is defined as H(f, g) = DKL(fg) + H(f).",
  "Proposition 1. In Gamid, cross-entropy and KL-divergence result in the same gradients with respect to asingle Gaussian, i, and the GMM excluding i, denoted GMMN\\i": "Proof. The Shannon entropy of a single Gaussian, f = N(, ), is H(f) = 0.5 ln det(2e) (Huber et al.,2008) (note that refers to the mathematical constant and not a policy here). Since H(f) is not a functionof , we get H(f) = 0.As a result, for any other density g (e.g., a GMM), we have H(f, g) =DKL(f g). Note that H(f, g) is not needed for Gamid because it assumes a constant .",
  "bpb exp(||f b||2)(4)": "As we seek to diversify the GMM policy overall composing Gaussians, we consider uniform weights (b, pb =1/N) when setting Dvar in Line 11 of Gamid. As this is a constant scalar value, it is simply omitted andcan be viewed as a component of the temperature scalar (). Finally, it is important to note, that Gamid is compatible with various DDPG variants. As such, when seekingstate-of-the-art performance, one should implement it within the most effective DDPG variant. Indeed, inour experimental section, we report results for a Gamid implementation extending the Twin Delayed DDPG(TD3) variant (Fujimoto et al., 2018).",
  "Convergence of Gamid": "When setting the number of Gaussian to one (N = 1), Gamid is effectively the same as DDPG and sharessimilar convergence guarantees. However, when considering a mixture of N > 1 Gaussians, the objectivefunction per Gaussian differs from that of DDPG following the MaxEnt sub-objective. As such, DDPG andGamid might converge on a different local optimum. This issue can easily be addressed by decaying thepolicy divergence temperature (). Nonetheless, matching the convergence guarantees of DDPG providesminor value as no such guarantees are provided for the general case. Specifically, Lillicrap et al. (2015) state,As with Q learning, introducing non-linear function approximators means that convergence is no longerguaranteed. However, such approximators appear essential in order to learn and generalize on large statespaces.",
  "Experiments": "The goals of the reported experimental study are fourfold: (G1) to illustrate the benefits of a GMM-basedactor over a single Gaussian policy in the context of Gamid when attempting to capture multiple modes ofoptimality, (G2) to compare deterministic policy gradients versus stochastic policy gradients for optimizinga GMM-based actor, (G3) to compare Gamid against contemporary RL algorithms for continuous controltasks, and (G4) to demonstrate the performance sensitivity for Gamids hyperparameters. Full descriptionsfor all the domains (state and action space, reward function) are provided in Appendix A.",
  "(d) Step# 9000": ": Policy distribution at different training steps in the 1-D continuous bandit problem with thereward function shown in green. x-axis represents the action space and y-axis the scaled PDF (probabilitydensity function). Unlike single Gaussian policy-based methods such as SAC and TD3/DDPG, and GMMpolicy-based SACM (shaded in blue), Gamid (shaded in red) converges on the optimal action.",
  "(G1) Capturing Multiple Modes of Optimality": "To illustrate the benefits of Gamid over a single Gaussian policy, as commonly used in SAC and DDPG/TD3,in terms of escaping the local optima and finding the optimal solution, we consider a toy continuous banditproblem with a 1-D action space.We adopt this problem from Huang et al. (2023).It has a multi-modal (2 modes) deterministic reward function defined over the action space in a bandit setting. The exactexperimental settings follow those presented in Huang et al. (2023). For completeness, these settings areprovided in Appendix A. showcases the performance of Gamid against a single Gaussian policy with a parameterized mean(as in both SAC and DDPG) and standard deviation (as in SAC). The Gaussians for both SAC and DDPGare initialized with means around 0 (a). We observe that both the single Gaussian actor variantsmove toward the suboptimal action (Figures 1b and1c) and converge on it by the end of the trainingphase (d). Similarly, Gamid finds the suboptimal action during the initial learning stages but, incontrast to the single Gaussian variants, it gradually spreads out and converges on the optimal action by theend of the training phase. These results suggest that a GMM-based actor can be helpful in escaping local(sub)-optimum as opposed to a single Gaussian actor. However, when examining the performance of SACM(with 5 Gaussians), we notice that it also converges on the suboptimal action, despite training a GMM actor.While SACM uses a GMM-based actor, similar to Gamid, it trains it using stochastic gradients, in contrastto Gamid. These results suggest that when training a GMM actor, deterministic gradients can be more",
  "GMM Actor Divergence": "We analyze the diversity between the Gaussians in Gamid captured by the approximated KL-divergence.Specifically, we utilize Equation (4) for approximating the sum of KL-divergence over all Gaussians withrespect to the GMM. That is, we define Dvar = Ni=1 DKL(i(N \\ i)) (see definition from Line 11 inAlgorithm 1). contains plots of Dvar at each training step on three representative MuJoCotasks (Todorov et al., 2012), Hopper-v3, HalfCheetah-v3, and Walker2d-v3 with increasing order ofaction dimensionality. We provide results for two extreme values (0 and 10) to highlight their effect onDvar. We expect Dvar to be relatively higher for higher values of since a higher value of maximizesDvar (Line 11 in Algorithm 1). The Dvar curves for all the tasks in follow the expected trendas the curve corresponding to = 10 is consistently higher than the one corresponding to = 0.Forcomparison, we also include curves for the tuned values ( = opt.) as listed in which we observe tobe between the curves corresponding to = 0 and = 10. These curves are closer to = 0 since the tunedvalues are closer to zero at (0.01 0.3). We further visualize the evolution of the GMM Gaussian componentmeans in Gamid during different steps of the training stage. See Appendix D.1 for full details and results.",
  "(G2) Deterministic vs Stochastic Policy Gradients": "To analyze when deterministic policy gradients can be more effective than stochastic policy we consider 2domains. The first domain is a 2-D maze navigation grid-world environment, denoted MazeGrid, with asparse reward function adopted from Huang et al. (2023). The agent starts at the center of the grid andits objective is to reach the optimal goal.The other domain is the HalfCheetah-v3 environment fromMuJoCo (Todorov et al., 2012). Full details regarding the domains are provided in Appendix A. In , we observe that SACM outperforms Gamid in terms of sample efficiency by more than 1standard deviation in MazeGrid. Prior work has shown single Gaussian-based actors using MaxEnt-basedpolicy optimization to be more effective in challenging exploration (sparse reward) tasks as compared todeterministic policies (Dawood et al., 2023; Singh et al., 2019) which might explain these trends.",
  ": Training curves on MazeGrid. Solid curves present the average over five runs while the shadedregion represents two standard deviations": "On the other hand, in b we observe that Gamid outperforms SACM in terms of sample efficiencyin HalfCheetah-v3. Compared to stochastic policies, deterministic policies have been shown to be moreeffective in tasks requiring precise control (as is common in robotics domains) (Montenegro et al., 2024). Wespeculate that Gamid builds on these properties and uses the diversified actors to speed up exploration duringthe initial stages of learning. This suggests that a GMM optimized using deterministic policy gradients canbe effective in dense reward robotics tasks when compared to using stochastic gradients.",
  "(G3) Comparative Evaluation on Benchmark Tasks": "For the comparative evaluation, we compare Gamid against common RL algorithms for continuous controltasks using benchmark MuJoCo (Todorov et al., 2012) and Fetch (Plappert et al., 2018) domains. TheMuJoCo tasks utilize dense reward functions to learn locomotion tasks. The Fetch domains consist of a7-DoF robotic arm fitted with a gripper relying on sparse reward functions to learn to solve goal-reachingtasks.",
  ". Soft Actor-Critic (SAC) (Haarnoja et al., 2018) (Off-Policy) a MaxEnt actor-critic algorithmwhere the policy is trained to maximize a weighted combination of expected return and policyentropy": "2. Soft Actor-Critic Mixture (SACM) (Baram et al., 2021) (Off-Policy) a soft-actor optimizationapproach that uses a GMM as the policy approximator. This approach was reported to not improveperformance over a single Gaussian policy (SAC). 3. Probabilistic Mixture-of-Experts SAC (PMOE) (Ren et al., 2021) (Off-Policy) a soft-actoroptimization approach that uses a mixture of critics. This approach was shown to outperform otherMOE approaches, specifically: MOE with gating operation (Jacobs et al., 1991), Double OptionActor-Critic (DAC) option framework (Zhang & Whiteson, 2019), the Multiplicative CompositionalPolicies (MCP) (Peng et al., 2019), and PMOE with Gumbel-Softmax (Maddison et al., 2016). 4. Proximal-Policy-Optimization (PPO) (Schulman et al., 2017) (On-Policy) a trust regionpolicy optimization variant (Schulman et al., 2015) using clipped gradients to restrict the policychange between policy updates. 5. Twin Delayed DDPG (TD3) (Fujimoto et al., 2018) (Off-Policy) an extension of the originalDDPG algorithm which introduces three enhancements, namely, (1) Clipped Double-Q Learning,(2) Delayed Policy Updates, and (3) Target Policy Smoothing. For Baselines 1, 4, and 5 we used the implementations provided in Stable-baselines3 (Raffin et al., 2021).For Baseline 2, since there was no official implementation provided by the authors, we implemented itfollowing the pseudocode provided in the paper.For Baseline 3, we used the implementation provided",
  "(i) FetchPickAndPlace-v1": ": Training curves on continuous control benchmarks. Solid curves present the mean over five runswhile the shaded region represents the tolerance interval with = 0.05 and = 0.7 (Patterson et al., 2023).Curves have been smoothed (100 steps moving window) for visual clarity. Gamid (Blue curve) consistentlyperforms on par or better compared to existing baseline methods. by the authors. The hyperparameter values for each algorithm were set as the recommended values. Forcompleteness, these values are provided in Appendix B. The codebase for these experiments is available at None of the baselines results in meaningful learning in theFetch tasks due to sparse rewards. Consequently, we follow prior work (Ibarz et al., 2021; Raffin et al., 2021;Bajaj et al., 2023) and combine them with Hindsight Experience Replay (HER) (Andrychowicz et al., 2017).Note that PPO and PMOE are not straightforward to combine with HER and are thus omitted for thesetasks. 2 Post-training performance. presents the post-training performance of Gamid and the baselinealgorithms at the end of the training stage (2M training steps for Humanoid-v3 and 1M for the rest) onthe MuJoCo and Fetch tasks. Results reported in the table and trends in suggest that Gamidperforms on par with the baseline methods on tasks with lower degrees of freedom, as in Swimmer-v3(a), and Hopper-v3 (c). It outperforms them on tasks with higher degrees of freedom(Figures 4b, 4e, 4f). We hypothesize these trends stem from increased modes of optimality present in high-",
  "Push0.34 0.430.99 0.020.99 0.020.95 0.05Slide0.13 0.180.76 0.260.75 0.270.45 0.31PickAndPlace0.03 0.040.99 0.010.99 0.010.66 0.24": "dimensional control tasks and Gamids superior ability to capture such complex behavior patterns. Resultsin all the Fetch tasks (Figures 4g, 4h, and 4i) show that Gamid consistently outperforms TD3 in terms ofpost-training performance. However, SAC and SACM outperform both Gamid and TD3. These results alignwith where we observe that stochastic gradient-based approaches are relatively more effective insparse reward tasks than deterministic gradient-based approaches. Sample efficiency.We observe that Gamid has a better sample efficiency than the baselines inHalfCheetah-v3 (b) and Walker2d-v3 (e). In the rest of the domains, we do not seeany specific trend for Gamid as compared to the baselines. In all Fetch domains, Gamid has better sampleefficiency than TD3 but has worse sample efficiency than SAC and SACM. Performance consistency.In terms of the post-training performance, we observe that Gamid consistentlyperforms at least as well as TD3 on both dense-reward (MuJoCo) and sparse-reward (Fetch) tasks.Interms of the average performance, Gamid outperforms TD3 on 8/9 tasks (see ). We also reportan independent two-sample t-test (Cressie & Whitford, 1986) with the p-value significance level set to 0.05comparing Gamid and TD3. The results indicate that the advantage of Gamid over TD3 is statisticallysignificant in 4/9 tasks (HalfCheetah-v3, Walker2d-v3, FetchPush-v1, and FetchPickAndPlace-v1). Forthe rest of the domains, the performance difference is not statistically significant. These results suggestthat, while training with deterministic gradients, utilizing a GMM-based actor (as in Gamid) is consistentlyadvantageous with respect to returns when compared to utilizing a single-Gaussian actor (as in TD3).",
  "(G4) Sensitivity Analysis of Gamid": "We examine the sensitivity of Gamids performance with respect to its hyperparameters.The reportedresults exclude the hyperparameters that are shared with the original DDPG algorithm since an ablationstudy for those was presented in previous publications (Lillicrap et al., 2015; Fujimoto et al., 2018). Theresults are reported for a single domain (Walker2d) where Gamid performs significantly better than ex-isting approaches. Nonetheless, for completeness, ablation results for the other domains are reported inAppendix D. Number of Gaussians, N.We start by examining the impact of varying the number of Gaussians usedby Gamid (the N hyperparameter). a presents learning curves for five N values: 1 (original TD3),2, . . . , 5. The other hyperparameters were kept constant with values as reported in the comparative study.We observe that increasing the number of Gaussians up to N = 3 improves both the sample efficiency andpost-training performance when compared to a single Gaussian. N = 3 provides the best exploration balancewhile the marginal benefit from adding more Gaussians diminishes and stagnates at about four. This is a",
  "reasonable result as, with sufficient Gaussians, the GMM becomes expressive enough to represent any targetpolicy, so the addition of more Gaussians is not helpful": "Policy divergence temperature, .Next, we examine the impact of varying the policy divergencetemperature hyperparameter (). b presents learning curves for three static values (0.01, 0.1, 0.5)and a decaying version (linear decay from 0.1 to 0.01 in 30% of the total training steps). We observe thata decaying value performs best. This outcome (favoring entropy temperature decay) is in line with similarresults reported by Schulman et al. (2017) and Haarnoja et al. (2018). GMM weights, .Finally, we examine the impact of varying the GMM weights setting from Line 3(denoted P). c presents learning curves for four P -greedy assignments: = 0 (greedy), = 1/(10N)(low epsilon), = 1/(2N) (high epsilon), = 1/N (uniform). The results suggest that a low (yet not zero)epsilon performs best. This result is in line with -greedy trends reported in prior work (Mnih et al., 2015).",
  "Discussion": "While prior works have compared deterministic and stochastic policy gradients they did so for single Gaussianactors. These studies found that stochastic gradients outperform deterministic gradients on common bench-mark domains (Haarnoja et al., 2018). Our experimental study suggests that this trend (stochastic gradientsoutperform deterministic gradients) does not necessarily apply to GMM-based actors. We observe that in5 out of 6 dense-reward MuJoCo domains, optimizing such actors with deterministic gradients (Gamid)performed better compared to stochastic gradients (SACM). Gamid also leads to improvements over (singleGaussian) SAC in 3/6 MuJoCo tasks while consistently performing similar or better than TD3 in all theMuJoCo tasks. In the 3 sparse-reward Fetch tasks, we observe that stochastic gradient-based actors (SAC,SACM) outperform deterministic gradient-based actors (GMM). In stochastic gradient-based algorithms, theentropy of the policy, derived from a learned standard deviation as opposed to a fixed standard deviation indeterministic gradient-based algorithms, is generally high during the initial learning stages in sparse-rewardtasks. During this stage, the agent receives close to zero non-zero rewards that result in high policy stan-dard deviation and hence a close-to-random exploration which is key in such scenarios. We speculate thatsuch a property makes SAC and SACM more effective as compared to Gamid and TD3 in the Fetch tasks.Nonetheless, Gamid outperforms TD3 in all the Fetch tasks, suggesting that using stochastic gradients totrain a GMM actor can be more effective than doing the same over a single Gaussian actor. We observe thatGMM-based actors optimized using deterministic gradients as presented in Gamid do not adversely affectthe post-training performance of TD3 in the MuJoCo tasks. These results stand in contrast with findingsreported in Baram et al. (2021) that did not find any significant advantage for using GMM-based actors.",
  "Summary": "In this paper, we presented a comparison between stochastic and deterministic policy gradients to optimizeGaussian mixture model (GMM)-based policies.We introduced a novel approach, denoted Gamid, fortraining a GMM using deterministic gradients. Similar to the maximization of entropy in stochastic actors,Gamid incorporates a diversification objective that encourages the actors to spread out aiding the explorationcapabilities of the policy. Empirical studies on benchmark MuJuCo tasks show that, in terms of sampleefficiency and post-training performance, Gamid improves over the single Gaussian stochastic variant (SAC)in 3/6 domains. It consistently performs on par or better than the single Gaussian deterministic variant(TD3) in all the MuJoCo and Fetch tasks. These results suggest that deterministic gradient approaches canbe more effective for training GMM actors as compared to stochastic gradient approaches for dense-rewardcontrol tasks. Empirical results on sparse-reward Fetch tasks show that stochastic gradient approaches aremore effective than Gamid. Nevertheless, Gamid improves over the single Gaussian deterministic variantin 3/3 Fetch domains. Consequently, we hope this work will seed research on training soft actors usingdeterministic gradient approaches.",
  "Alejandro Agostini and Enric Celaya. Reinforcement learning with a gaussian mixture model. In The 2010International Joint Conference on Neural Networks (IJCNN), pp. 18. IEEE, 2010": "Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD interna-tional conference on knowledge discovery & data mining, pp. 26232631, 2019. Riad Akrour, Davide Tateo, and Jan Peters. Continuous action reinforcement learning from a mixture ofinterpretable experts. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):67956806, 2021.",
  "Craig Calcaterra. Linear combinations of gaussians with a single variance are dense in l2. In Proceedings ofthe World Congress on Engineering, volume 2, 2008": "Po-Wei Chou, Daniel Maturana, and Sebastian Scherer. Improving stochastic policy gradients in continuouscontrol with deep reinforcement learning using the beta distribution.In International conference onmachine learning, pp. 834843. PMLR, 2017. Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learningfor intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020.",
  "NAC Cressie and HJ Whitford. How to use the two sample t-test. Biometrical Journal, 28(2):131148, 1986": "Murad Dawood, Nils Dengler, Jorge de Heuvel, and Maren Bennewitz. Handling sparse rewards in reinforce-ment learning using model predictive control. In 2023 IEEE International Conference on Robotics andAutomation (ICRA), pp. 879885. IEEE, 2023. Sheelabhadra Dey, Sumedh Pendurkar, Guni Sharon, and Josiah P. Hanna. A joint imitation-reinforcementlearning framework for reduced baseline regret. In 2021 IEEE/RSJ International Conference on IntelligentRobots and Systems (IROS), pp. 34853491. IEEE Press, 2021. doi: 10.1109/IROS51168.2021.9636294.URL Sheelabhadra Dey, James Ault, and Guni Sharon. Continual optimistic initialization for value-based re-inforcement learning. In Proceedings of the 23rd International Conference on Autonomous Agents andMultiagent Systems, pp. 453462, 2024.",
  "Kenji Doya. Temporal difference learning in continuous time and space. Advances in neural informationprocessing systems, 8, 1995": "igo Elguea-Aguinaco, Antonio Serrano-Muoz, Dimitrios Chrysostomou, Ibai Inziarte-Hidalgo, SimonBgh, and Nestor Arana-Arexolaleiba. A review on reinforcement learning for contact-rich robotic manip-ulation tasks. Robotics and Computer-Integrated Manufacturing, 81:102517, 2023. Mlrio AT Figueiredo, Jose MN Leitao, and Anil K Jain. On fitting mixture models. In Energy MinimizationMethods in Computer Vision and Pattern Recognition: Second International Workshop, EMMCVPR99York, UK, July 2629, 1999 Proceedings 2, pp. 5469. Springer, 1999.",
  "Scott Fujimoto, Herke Hoof, and David Meger.Addressing function approximation error in actor-criticmethods. In International conference on machine learning, pp. 15871596. PMLR, 2018": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor.In International conference on machinelearning, pp. 18611870. PMLR, 2018. John R. Hershey and Peder A. Olsen.Approximating the kullback leibler divergence between gaussianmixture models. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP 07, volume 4, pp. IV317IV320, 2007. doi: 10.1109/ICASSP.2007.366913.",
  "Alessandro Montenegro, Marco Mussi, Alberto Maria Metelli, and Matteo Papini. Learning optimal deter-ministic policies with stochastic policy gradients. arXiv preprint arXiv:2405.02235, 2024": "Iman Nematollahi, Erick Rosete-Beas, Adrian Rfer, Tim Welschehold, Abhinav Valada, and Wolfram Bur-gard. Robot skill adaptation via soft actor-critic gaussian mixture models. In 2022 International Confer-ence on Robotics and Automation (ICRA), pp. 86518657. IEEE, 2022. Gerhard Neumann, Wolfgang Maass, and Jan Peters. Learning complex motions by sequencing simplermotion templates. In Proceedings of the 26th Annual International Conference on Machine Learning, pp.753760, 2009.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-tion algorithms. arXiv preprint arXiv:1707.06347, 2017": "Guni Sharon. Alleviating road traffic congestion with artificial intelligence. In Zhi-Hua Zhou (ed.), Proceed-ings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 49654969. In-ternational Joint Conferences on Artificial Intelligence Organization, 8 2021. doi: 10.24963/ijcai.2021/704.URL Early Career. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deter-ministic policy gradient algorithms. In International conference on machine learning, pp. 387395. Pmlr,2014.",
  "DomainAction dim.Obs. dim.Reward": "Swim.28forward_reward - ctrl_costHalfCh.617forward_reward - ctrl_costHopper.311healthy_reward + forward_reward - ctrl_costAnt.827healthy_reward + forward_reward - ctrl_cost - contact_costWalker.617healthy_reward + forward_reward - ctrl_costHuman.17376healthy_reward + forward_reward - ctrl_cost - contact_costPush.425sparse; block final target position reached = 0/not reached = -1Slide.425sparse; puck final target position reached = 0/not reached = -1Pick.425sparse; block final target position reached = 0/not reached = -1",
  "replay buffer).For the noise in DDPG, we used a Gaussian distribution with a mean of 0 and a fixedstandard deviation of 0.1, the recommended setting for MuJoCo tasks in RL Baselines3 Zoo library (": "Tables 89 present the default hyperparameters values used in our experiments on the MuJoCo and Fetchtasks. To allow for easy reproduction of the results, the presented hyperparam names follow the variablenames in the Stable-baselines3 (v1.6.2) (Raffin et al., 2021) open-source codebase.We used the tunedhyperparameter values for SAC, TD3, and PPO that have been provided in RL Baselines3 Zoo which isbuilt on top of Stable-baselines3. For SACM and PMOE we used hyperparameters recommended in (Baramet al., 2021) and (Ren et al., 2021) respectively for the MuJoCo tasks. For the Fetch tasks, we tuned thehyperparameters of SACM using Optuna (Akiba et al., 2019).",
  "CCompute": "All reported experiments were distributed between 2 machines; (1) a machine with 64 32-core AMD RyzenThreadripper PRO 5975WX CPUs, each clocked at 4.3 GHz with 250 GB RAM with 2 NVIDIA GeForceRTX 3090 24 GB GPUs (2) a machine with 16 8-core Intel(R) Core(TM) i7-9800X CPUs, each clocked at3.8 GHz with 16 GB RAM and an NVIDIA GeForce RTX 20280 Ti 12 GB GPU.",
  "Hyperparam.Swim.HalfCh.Hopper.Ant.Walker.Human.Push.Slide.Pick": "n_actors (N)352234488temperature_initial ()0.10.10.10.10.10.30.30.30.3temperature_final ()0.10.10.010.10.10.10.10.10.1temperature_fraction1.01.01.01.01.00.31.01.01.0epsilon_initial ()0.30.20.10.10.30.30.10.10.1epsilon_final ()0.030.020.10.10.030.10.010.010.01epsilon_fraction0.50.51.01.01.00.31.01.01.0 perform a principal component analysis (PCA) (Abdi & Williams, 2010) on the Gaussian means and plotthe top two PCA components. Comparing the top and middle rows of Figures 8 corresponding to = 0 and = 10 in Hopper-v3 respectively, we observe that the Gaussian means have a higher spread for = 10. Asimilar observation can also be made when comparing the top and middle rows of Figures 9 corresponding to = 0 and = 10 in HalfCheetah-v3 respectively. The trend in bottom rows Figures 8 and 9 correspondingto = opt. is less clear which we speculate could be attributed to noisy weight updates due to mini-batchgradient descent during training.",
  "Following Haarnoja et al. (2018), we include the sensitivity analysis for all the MuJoCo tasks (beyond therepresentative, single task, results presented in the main text)": "Figures 1015 present training curves similar to those presented in the ablation study from the main textalbeit on all the MuJoCo tasks. These results generally support the conclusions provided in the main text(1) adding more Gaussians to the GMM-based actor has a positive impact up to a certain threshold, beyondwhich, performance stagnates or even deteriorates, (2) setting a decaying policy divergence parameter isbeneficial, and (3) using an -greedy approach with a low (yet not zero) value is beneficial."
}