{
  "Abstract": "In severely over-parametrized regimes, neural network optimization can be analyzed by lin-earization techniques as the neural tangent kernel, which shows gradient descent convergenceto zero training error, and landscape analysis, which shows that all local minima are globalminima. Practical networks are often much less over-parametrized, and training behaviourbecomes more nuanced and nonlinear. This paper contains a fine grained analysis of thenonlinearity for a simple shallow network in one dimension. We show that the networkshave unfavourable critical points, which can be mitigated by sufficiently high local resolu-tion. Given this resolution, all critical points satisfy L2 loss bounds of optimal adaptiveapproximation in Sobolev and Besov spaces on convex and concave subdomains of the tar-get function. These bounds cannot be matched by linear approximation methods and shownonlinear and global behaviour of the critical points inner weights.",
  "Introduction": "In this paper, we analyze nonlinear aspects of neural network training for a simple model problem in su-pervised learning: For samples xi and data yi = f(xi) generated by some unknown target function f, finda neural network f with weights by minimizing the least squares loss. To motivate the results, we firstreview some common approaches in the literature. Landscape AnalysisGradient descent can easily get stuck in local minima. That this fact does notharm neural network training is the purview of landscape analysis.It aims to demonstrate that eitherthe loss has no local minima, in favour of saddle points, or all local minima have small loss value andtherefore provide good trained networks. Indeed, the papers Soudry & Carmon (2016); Kawaguchi (2016);Nguyen & Hein (2017); Ge et al. (2018); Du & Lee (2018); Soltanolkotabi et al. (2019); Venturi et al. (2019);Kawaguchi et al. (2019); Kawaguchi & Huang (2019) show that local minima are global minima, either understrong assumptions, or over-parametrization with more network width than number of samples. Absent suchassumptions, one needs to be more careful, e.g. the papers Swirszcz et al. (2017); Safran & Shamir (2018);He et al. (2020); Ding et al. (2022); Jentzen & Riekert (2024), find local minima that are not global. A morefine grained analysis of the landscape is included in He et al. (2020), which finds valleys of path connectedlocal minima.",
  "inf f f n()r,f K,(1)": "where n() is an indicator for the network size, like width, depth or total number of weights and r > 0 anasymptotic rate. Similar to the no-free-lunch theorem, such bounds cannot work for arbitrary f, which iswhy we restrict them to some compact set K. Typically, it bounds Sobolev, Besov or Barron norms or othersmoothness properties of the permissible targets f. Inequalities of type (1) are common in approximationtheory and have been studied extensively for neural networks. A literature overview is given later in theintroduction.",
  "f f n()r,f K, is a critical point of the training loss?(2)": "Such results are well established for partial differential equations (PDEs), where f is a nonlinear approxi-mation method like adaptive finite elements or wavelets and f the solution of a PDE Cohen et al. (2002);Morin et al. (2002); Binev et al. (2004). Similar results also exits for shallow neural networks, when trainedwith greedy algorithms Siegel & Xu (2022b); Siegel et al. (2023) instead of gradient descent. Linearization ArgumentsIn over-parametrized regimes, typically with more network width than train-ing samples, gradient descent training does not move the network weights far from their random initialization.As a result, one can obtain accurate descriptions of the training dynamics by linearising the network at theinitial value. Careful analysis then provides exponential gradient descent convergence to zero training loss.A common representative of this approach is the neural tangent kernel (NTK) introduced in Jacot et al.(2018); Li & Liang (2018); Allen-Zhu et al. (2019); Du et al. (2019b;a), and refined in Zou et al. (2020); Aroraet al. (2019a;b); Su & Yang (2019); Lee et al. (2019); Song & Yang (2019); Zou & Gu (2019); Kawaguchi& Huang (2019); Chizat et al. (2019); Oymak & Soltanolkotabi (2020); Ji & Telgarsky (2020); Nguyen &Mondelli (2020); Bai & Lee (2020); Cao & Gu (2020); Chen et al. (2021); Song et al. (2021); Lee et al. (2022);Gentile & Welper (2022); Welper (2024b;a); Keene & Welper (2024). Contrary to this analysis, much of the promise of neural networks relies on their severe non-linearity, leadingto e.g. high expressivity and excellent function approximation properties, even in high dimensions. Canthese be exploited by gradient descent training? If we consider less over-parametrization, or even slightlyunder-parametrized regimes, the weights can move farther from their initial and break the linear dominancein the training dynamics.Empirical studies Vyas et al. (2023) (see also Lee et al. (2020); Seleznova &Kutyniok (2022)) on image classification datasets show that in such regimes networks perform better thanextremely wide networks with dominantly linear behaviour. A theoretical understanding of these regimes isstill largely unknown. This leads to a second question:",
  "(Q2) Can training in under-parametrized or only slightly over-parametrized regimes exploit the nonlinearnature of neural networks?": "To this end, it is instructive to look at classical approximation methods, where f is replaced by e.g. splines,finite elements or wavelets. These depend nonlinearly on if adaptivity is used and linearly if not. The non-linear variations strictly include the linear ones so that inf f nonlinearf inf f linearf. Nonetheless,in the upper error bounds (1) this does neither change the number of degrees of freedom (weights) n() northe (maximal) rate r. It does change, however, the size of the compact sets Klinear Knonlinear for whichthe given rates can be achieved, with the latter being significantly larger. In summary, if we want to establish approximation results (2) for neural network critical points with nonlinearcompact sets Knonlinear, we have to carefully exploit the nonlinear nature of the networks and can no longerrely on vanilla NTK analysis.",
  "r=1wr(x br),(3)": "with ReLU activation, in a one dimensional interval x D R, trained on the L2 loss f fL2(D).This is probably the simplest choice with nonlinear weight dependence (of the br), non-convex loss and fullyunderstood approximation behaviour both in linear (br untrained) and non-linear (br trained) cases. Thecontinuous loss simplifies the analysis and places the problem in an under-parametrized regime, independentof the width m. Empirical losses, with large numbers of samples, are expected to show similar behaviour byclassical arguments in statistics and machine learning, different from the over-parametrized regime, wheretheir application is more complicated.",
  "limmx[br1,br]m(br br1) = constant|f (x)2/5|,(4)": "with possibly a different constant for each interval on which f is strictly convex or concave. The factorm 1/h is reciprocal to the uniform grid size h and used for normalization. The right hand side showsthat the breakpoints br are close wherever the second derivative f (x), and hence the local approximationdifficulty, is large. Generally, this requires global movement of breakpoints br dependent on f, from initiallocations independent of f. For finite m, analogous arguments show that at critical points of the loss thebreakpoints equidistribute the local smoothness",
  "f fL2(DI) |I|2f L2/5(DI),": "where |I| is the number of breakpoints in the respective intervals. To avoid bad local minima, these resultsrequire the critical points to have sufficient local resolution |br br1| so that f does not have highlyoscillatory features between breakpoints that are imperceptible to the gradient. The rigorous statements arein Theorems 3.3 and 3.4 and an example for the conditions is given in . The results demonstrate approximation errors (2) on subdomains where f is convex or concave with K :=K2/5 := {f L2(D) : f L2/5(D) 1}. A subtle, but crucial, observation is that f is measured in the veryweak L2/5 (quasi-) norm (or Besov spaces in Section B.3), which allows us to achieve high approximationorders for fairly rough functions f. These are not possible for purely linear approximation methods (byKolmogorov n-width lower bounds) and therefore demonstrate that finding local critical points of the losslandscape allows us to exploit some nonlinearity of the neural networks.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Arnulf Jentzen and Adrian Riekert. A proof of convergence for the gradient descent optimization methodwith random initializations in the training of neural networks with relu activation for piecewise lineartarget functions. Journal of Machine Learning Research, 23(260):150, 2022. Arnulf Jentzen and Adrian Riekert. Non-convergence to global minimizers for adam and stochastic gradientdescent optimization and constructions of local minimizers in the training of artificial neural networks,2024. Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve arbitrarily smalltest error with shallow ReLU networks. In International Conference on Learning Representations, 2020. Kenji Kawaguchi. Deep learning without poor local minima. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon,and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates,Inc., 2016. Kenji Kawaguchi and Jiaoyang Huang. Gradient descent finds global minima for generalizable deep neuralnetworks of practical sizes. In 2019 57th Annual Allerton Conference on Communication, Control, andComputing (Allerton), pp. 9299, 2019. Kenji Kawaguchi, Jiaoyang Huang, and Leslie Pack Kaelbling. Every Local Minimum Value Is the GlobalMinimum Value of Induced Model in Nonconvex Machine Learning. Neural Computation, 31(12):22932323, 12 2019. ISSN 0899-7667.",
  "These correspond exactly to the approximation bound in the introduction if we define Ks,p := {f L2(D) :|f|B2p(Lp) 1} with p = 2 and p = 2/5": "Up to minor differences, the Besov norms |f|Bsp(Lp) f (s)Lp are equivalent to Sobolev norms, whichbound the s-th derivative of f in Lp. The former are technical, but usually preferred in approximationtheory because they are well behaved for p < 1, as used in the nonlinear bound above. For orientation, thesespaces are often arranged as in . The sets Ks,p become larger with decreasing s and decreasing p.By Sobolev embedding theorems, one may trade some p for some s so that all spaces (s, p) above the dashedline in the figure are contained in L2 and thus Ks,p K0,2. See Section B.1 for definitions and DeVore &Lorentz (1993); DeVore (1998) for more details.",
  ": Diagram of Besov spaces. Each point (1/p, s) corresponds to one space Bsp(Lp) for s > 0 and Lpfor s = 0": "Let us compare the linear approximation um with the nonlinear approximation m. First observe that in(5) the rate m2 is identical for both methods. Generally, piecewise linear approximation does not achievehigher rates, even if f admits more smoothness. However, since L2/5(D) L2(D), the smoothnessconditions for nonlinear approximation are much weaker. For example f = sigmoid(x/) has norms",
  "Indeed, the second derivative is of size 2 in a region [c, c] and negligible outside. Thus f Lp(D)": "21Lp([c,c]) 21p . As goes to zero and f converges to a jump function, the L2 norm blows up,whereas the L2/5 norm remains bounded. This provides significantly better approximation bounds in (5)for nonlinear approximation. If we use Besov spaces instead of the second derivative, this extends to thejump function itself, which can be approximated by nonlinear methods up to order m2, whereas linearapproximation only achieves order < m1/2. While the linear approximation has a fixed number of breakpoints br near the jump or sharp gradients off, the adaptive approximation can allocate more resources where f is complicated. Indeed, algorithms andproofs for the approximation bounds (5), aim for breakpoints that equidistribute the local errors",
  "Furthermore, there is a network fw,b(x) = FW,V,B(x) of width m with the same breakpoints that is a criticalpoint of the loss minw,b fw,b f2L2(D)": "The proof is in Section B.4. Since our main theorems characterize critical points, it suffices to consider thesimplified variant of the networks. Finally, the term b0 + w0x can be generated by two ReLUs (x br), onewith breakpoint at the left boundary and one more left of the domain. Therefore, we drop b0 and w0 andarrive at the network",
  "2f f2L2(D)(10)": "on some finite domain D R and some target function f L2(D). This matches the infinite sample limit ofthe least squares loss and places us in an under-parametrized regime similar to classical statistics. Althoughthe ReLU activation has kinks, this loss is strongly differentiable for all weights . Indeed, it suffices toconsider the network as a map f() from parameters to L2(D) functions. This topology is sufficientlyweak to render the map differentiable, unlike the regular pointwise topology, which does not. See Gentile &Welper (2022) for details.",
  "() = 0,(11)": "which we examine more closely in the following.To ease the theoretical analysis, we start with somenotational cleanup, which does not alter the actual network. First, we drop inactive neurons with wr = 0.Second, we join neurons with identical bias br into one neuron and adjust the outer weights wr accordingly.Third, we drop neurons for biases br outside of the domain D, except for the largest br left of the domain,which influences the left boundary value of f. Finally, we add one artificial breakpoint b m at the right endof D, which does not change f inside D, but avoids technicalities. This yields m m neurons, which wereorder according tob0 < < b m(12)",
  "for some 1 < q, p and some sufficiently small constant C > 0 independent of f and hk. Then forl, k DI we have equidistributionf L2/5(Il) f L2/5(Ik)": "This is precisely the equidistribution (7) used in the proofs of CPwL approximation bounds (5). We discussthe result and the major assumption (13) after the approximation theorem below. In short, high oscillationsstrictly contained in one interval Ir are imperceptible to the gradient and therefore can lead to bad criticalpoints. The assumption ensures that we have enough breakpoints to fully resolve such oscillations.",
  "f fL2(DI) |I|2f L2/5(DI).(15)": "DiscussionApproximation Result: Note that |I| is the number of breakpoints in DI and therefore isanalogous to the number of breakpoints m on the full domain. Therefore, any critical point subject to thegiven conditions achieves asymptotically optimal CPwL approximation on subdomains DI D and canproperly utilize the nonlinearity of the network. If we have multiple subdomains DI1 and DI2, the totalallocation of breakpoints on each of these may be suboptimal. Resolution: The main purpose of assumption 13 is to prevent bad critical points: Oscillations of the targetf that are strictly contained inside one interval Ir do not change the gradient and can cause unfavorablecritical points. The assumption is satisfied if the networks local resolution hr is sufficiently high to captureall fine grained features of f. For comparison classical adaptive CPwL approximation methods contain dataoscillation terms in their error bounds. See for a more careful discussion.",
  "Proof Idea": "This section contains a short overview over the proof. The optimization of the outer weights wr is convexand therefore fairly simple. On the other hand, the optimization of the inner weights br is non-convex andthe main objective of the prove is to demonstrate their equidistribution in Theorem 3.3. Once this propertyis established, the approximation Theorem 3.4 follows by standard arguments DeVore (1998). The proofproceeds in several steps.",
  "fL1/2(Ir) hrf , rIr = hr+1f , `r+1Ir+1 fL1/2(Ir+1).(18)": "4. Refined Analysis: While the last two equations provide equidistribution on two neighbouring in-tervals, they are insufficient: (18) has the wrong norm, L1/2 instead of L2/5, and is too inaccuratewhen chaining over large numbers of intervals. (17) cannot be chained directly because the functions = ` are asymmetric. A more refined analysis of the asymmetry and passing to the limit m shows that the grid size limit h(x) = limm mhr for x Ir satisfies the differential equation",
  "5h2f ,": "where the left hand side originates from (17) and the right hand side from the asymmetry. Thisis a first order linear differential equation for h2.Solving it with an integrating factor leads to[h2(f )4/5] = 0 and the extra power 4/5 leads to proper grid size limit for L2/5 equidistribution.The main result Theorem 3.3 follows from a perturbation analysis of this ODE for finite hr.",
  "f (f + ), v = 0,v X + X": "It is easy to construct so that f is a bad approximation. To provide a simple example, let f = 0 so thatf = 0 must also be zero. Now choose two neighbouring breakpoints br and br+1 and define an oscillation supported inside Ir = [br1, br], with some margin to the boundary and orthogonal to all linear functionsP1. Then, we have X + X and f = 0 is a critical point for approximating 0 + c with arbitrarily largeapproximation error c. On the other hand, the network f may have an arbitrary number of breakpointsoutside of Ir. With optimal placement, they can all be used to approximate and make the approximation",
  "Avoiding Critical Points with High Oscillations": "This section provides an informal motivation how assumption (13) rules out bad local minima with sub-grid oscillations. To this end, first note that in case the target f is a simple function, like e.g. a seconddegree polynomial, it clearly cannot oscillate as in the counter example. Of course this is too strong anassumption, but what if f is close to a second order polynomial? Since second order approximation convergesfaster than first order approximation, for sufficiently small intervals Ir = [br1, br], we can expect",
  "(1 + C)1p2,r fL2(Ir) f fL2(Ir).(1 C)1p2,r fL2(Ir),(20)": "i.e. the error of approximating f and of approximating the simple function p2,r is about the same. Hence,if we can prove error equidistribution for locally simple functions p2,r, we also obtain error equidistributionfor ff fL2(Ir) p2,r fL2(Ir) p2,s fL2(Is) f fL2(Is) for any two intervals r and s.This is a slightly stronger alternative to the equidistribution of smooth-ness in Theorem 3.3. In summary, given (19), we can reduce equidistribution from arbitrary f to simplerequidistribution of piecewise second degree polynomials. Before we discuss (19) more carefully, let us apply our observations to the counter example from the lastsection. If the oscillation from the example is orthogonal to P1 and P2, the best approximations and thenetwork from the example satisfy p1 = p2 = f = 0 and we have",
  "(b) Fine interval Ir": ": Approximation of a oscillatory f = = L5 L3 constructed from Legendre basis polynomialsLi. On the coarse interval (left), the first and second degree approximation do not capture the functioncorrectly. On the zoom in to a fine interval (right), the second degree approximation captures the bulk ofthe function, whereas the first degree approximation does not.",
  "= cCh2rf (2)L2(I),": "for some constant C that is assumed to be sufficiently small. Direct approximation results tend to be sharpasymptotically, in which case we can directly compare the first and second order approximation to obtain(19):infp2P2 f p2L2(Ir) Ch2rf (2)L2(I) C infp1P1 f p1L2(Ir). A rigorous argument is e.g. in Lemma A.27, applied to f and constant polynomials p2. The lemma alsoincludes replacements f f, vIr f p2,r, vIr of f by local polynomials for the critical point conditions(16), which require a little more care because of missing absolute values and potential cancellation. A simple illustration is given in . We can see that the approximation on a coarse interval does notcapture the oscillatory f correctly. On the fine interval, the second degree approximation is fairly accurate,and captures the bulk of f, as required by (20).",
  "apply Theorems 3.3 and 3.4. However, once we can, the resulting equidistribution and approximation errorshave standard constants that are independent of C and S": "We have argued that the condition (13) is simple to verify for a given critical point. A more difficult questionis to what extent we can guarantee that these favourable critical points are the limit of gradient descent. Wecan choose hr arbitrarily small at the initial breakpoints. By stability of gradient descent, one may arguethat two breakpoints that are close at the initial may remain so during training. This would entail (13) alsofor the gradient descent limit. However, a rigorous argument requires a full understanding of the gradientdescent dynamics, which is not considered in this paper and left for future research.",
  "f(x) = 1 |x|0.1,x ,(21)": "which has a cusp at the origin. This entails that f is not contained in L2(), but is contained inL2/5() so that we expect a better performance of nonlinear approximation. Intuitively, this is achievedby moving more breakpoints to the vicinity of the cusp. shows the evolution of the networks andbreakpoints during gradient descent training with the following setup",
  "International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Re-search, pp. 242252, Long Beach, California, USA, 0915 Jun 2019. PMLR. Full version available at": "Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimizationand generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and RuslanSalakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pp. 322332, Long Beach, California, USA, 0915 Jun 2019a.PMLR. Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exactcomputation with an infinitely wide neural net. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlchBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32.Curran Associates, Inc., 2019b.",
  "Peter Binev, Wolfgang Dahmen, and Ron DeVore. Adaptive Finite Element Methods with convergence rates.Numerische Mathematik, 97(2):219268, April 2004": "Guy Bresler and Dheeraj Nagaraj. Sharp representation theorems for ReLU networks with precise dependenceon depth. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in NeuralInformation Processing Systems, volume 33, pp. 1069710706. Curran Associates, Inc., 2020. Yuan Cao and Quanquan Gu.Generalization Error Bounds of Gradient Descent for Learning Over-Parameterized Deep ReLU Networks.Proceedings of the AAAI Conference on Artificial Intelligence,34(04):33493356, April 2020. ISSN 2374-3468, 2159-5399.",
  "Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is sufficient tolearn deep re{lu} networks? In International Conference on Learning Representations, 2021": "Lnac Chizat and Francis Bach.On the global convergence of gradient descent for over-parameterizedmodels using optimal transport. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates,Inc., 2018. Lnac Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. InH. Wallach, H. Larochelle, A. Beygelzimer, F. dAlch Buc, E. Fox, and R. Garnett (eds.), Advances inNeural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.",
  "Selina Drews and Michael Kohler. On the universal consistency of an over-parametrized deep neural networkestimate learned by gradient descent, 2022": "Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic activation.In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on MachineLearning, volume 80 of Proceedings of Machine Learning Research, pp. 13291338. PMLR, 1015 Jul 2018. Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima ofdeep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36thInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,pp. 16751685, Long Beach, California, USA, 0915 Jun 2019a. PMLR.",
  "Kurt Hornik, Maxwell Stinchcombe, and Halbert White.Multilayer feedforward networks are universalapproximators. Neural Networks, 2(5):359366, 1989. ISSN 0893-6080": "Shokhrukh Ibragimov, Arnulf Jentzen, and Adrian Riekert. Convergence to good non-optimal critical pointsin the training of neural networks: Gradient descent optimization with one random initialization overcomesall bad non-global local minima with high probability, 2022. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalizationin neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.",
  "Michael Kohler and Adam Krzyzak. Analysis of the rate of convergence of an over-parametrized deep neuralnetwork estimate learned by gradient descent, 2022": "Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, andJeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent.In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances inNeural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and JaschaSohl-Dickstein. Finite versus infinite neural networks: an empirical study. In H. Larochelle, M. Ranzato,R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,pp. 1515615172. Curran Associates, Inc., 2020.",
  "Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-dimensional polyno-mials with sgd near the information-theoretic limit, 2024": "Jongmin Lee, Joo Young Choi, Ernest K Ryu, and Albert No. Neural tangent kernel analysis of deep narrowneural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, andSivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162of Proceedings of Machine Learning Research, pp. 1228212351. PMLR, 1723 Jul 2022. Bo Li, Shanshan Tang, and Haijun Yu. Better approximations of high dimensional smooth functions by deepneural networks with rectified power units. Communications in Computational Physics, 27(2):379411,2019. ISSN 1991-7120. Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descenton structured data. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett(eds.), Advances in Neural Information Processing Systems 31, pp. 81578166. Curran Associates, Inc.,2018.",
  "Jianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang.Deep network approximation for smoothfunctions. SIAM Journal on Mathematical Analysis, 53(5):54655506, 2021": "Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. The expressive power of neuralnetworks: A view from the width.In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.62316239. Curran Associates, Inc., 2017. Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layerneural networks. Proceedings of the National Academy of Sciences, 115(33):E7665E7671, 2018. ISSN0027-8424.",
  "Pedro Morin, Ricardo H. Nochetto, and Kunibert G. Siebert. Convergence of adaptive finite element methods.SIAM Review, 44(4):631658, 2002": "Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Doina Precup andYee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70of Proceedings of Machine Learning Research, pp. 26032612. PMLR, 0611 Aug 2017. Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed bypyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advancesin Neural Information Processing Systems, volume 33, pp. 1196111972. Curran Associates, Inc., 2020.",
  "Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta Numerica, 8:143195, 1999": "Grant M. Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptoticconvexity of the loss landscape and universal scaling of the approximation error. CoRR, abs/1805.00915,2018. Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer ReLU neural networks. InJennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on MachineLearning, volume 80 of Proceedings of Machine Learning Research, pp. 44334441, Stockholmsmssan,Stockholm Sweden, 1015 Jul 2018. PMLR. Mariia Seleznova and Gitta Kutyniok. Analyzing finite neural networks: Can we trust neural tangent kerneltheory? In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova (eds.), Proceedings of the 2nd Mathematicaland Scientific Machine Learning Conference, volume 145 of Proceedings of Machine Learning Research,pp. 868895. PMLR, 1619 Aug 2022.",
  "Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of largenumbers. SIAM Journal on Applied Mathematics, 80(2):725752, 2020": "Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization land-scape of over-parameterized shallow neural networks. IEEE Transactions on Information Theory, 65(2):742769, 2019. ChaeHwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan Cevher.Sub-quadratic overparameterization for shallow neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin,P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, vol-ume 34, pp. 1124711259. Curran Associates, Inc., 2021.",
  "Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees formultilayer neural networks, 2016": "Lili Su and Pengkun Yang. On learning over-parameterized neural networks: A functional approximationperspective. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.),Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Taiji Suzuki. Adaptivity of deep reLU network for learning in besov and mixed smooth besov spaces: optimalrate and curse of dimensionality. In International Conference on Learning Representations, 2019.",
  "Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks, 94:103114,2017. ISSN 0893-6080": "Dmitry Yarotsky. Optimal approximation of continuous functions by very deep ReLU networks. In SbastienBubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On LearningTheory, volume 75 of Proceedings of Machine Learning Research, pp. 639649. PMLR, 0609 Jul 2018. Dmitry Yarotsky and Anton Zhevnerchuk.The phase diagram of approximation rates for deep neuralnetworks. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in NeuralInformation Processing Systems, volume 33, pp. 1300513015. Curran Associates, Inc., 2020.",
  "A.1.1Critical Points": "We have already seen in the proof overview that the weights are a critical point if and only if , v = 0 forall v X + X, with residual := f f. For reference, this is stated again in the following lemma, togetherwith a characterization of the spaces X and X.Lemma A.1. Let be a critical point (11), with cleaned breakpoints in ascending order 12. Then",
  ", v = 0,v X = {v L2(D)|v|Ir P0, r = 1, . . . , m},": "Proof. In (16), we have seen that the critical points are given by the condition , v = 0 for all v in thespan X of the partial derivatives wr and in the span X of the partial derivatives br. Hence, it suffices toshow that the span of the derivatives matches the two sets in the lemma. Indeed, one readily computes",
  "wrf = , ( br) ,brf = , wr ( br)": "Clearly ReLU activations ( br) span piecewise linear functions and their derivatives wr ( br) spanpiecewise constants because in the cleanup before Theorem 3.4 we have already dropped all neurons withwr = 0. Finally, by the same cleanup, the leftmost breakpoint maybe inside or outside of D, but anyways,we must have v(b0) = 0 because no partial derivative has support left of this point.",
  "Although the second derivative might seem artificial at first, the function r will be more useful than rdown the road": "Proof. We abbreviate r := hr r hr+1 `r. Clearly `r and r are piecewise linear so that r X + X andit is sufficient to show r, Hs = 0 for a basis Hs, s = 0, . . . , m of X. We choose hat functions centered atbs, i.e.",
  "A.2.1Results": "We have seen in that we can achieve optimal approximation rates by equilibrating the smoothnessf L2/5(Ir) on all intervals Ir. In this section, we provide an argument that in the limit of small intervalshr 0, the equidistribution and the critical point conditions yield the same adapted grids. This section iskept informal, put provides intuition and guidelines for a rigorous analysis for finite hr in Section A.3.",
  "with possibly a different constant cI on each interval I for which f (x) is non-zero": "We observe that the grid size limit is identical to the density of the smoothness norm equidistribution inLemma 3.2, up to a global factor on each interval for which f is non-zero. Thus, in the limit, critical pointshave a proper grid distribution on every strictly convex or concave stretch of the target function f, but maybe imbalanced between these stretches.",
  "A.3Equilibrium for Finite h": "In this section, we prove the main equidistribution theorem, restated here for convenience:Theorem A.11 (Theorem 3.3, restated). Let be a critical point (11), with cleaned breakpoints in ascendingorder 12. For r, s {2, . . . , m}, let I = {r, r + 1, . . . , s} be a set of consecutive neurons with DI :=",
  "A.3.3Proof of the Main Result": "The assumptions for the main theorems confine the results to regions where f is convex or concave. For thetime being, we make this assumption explicit by assuming ar 0, which will be removed later.Lemma A.12. Let br, r [ m] be cleaned critical breakpoints (11), (12). Let ar, ar and r be defined by(28), (31), ar 0 and := 1/5. Then",
  "h2r[r+1 r][ar+1 ar] + h2rarRr= [h2r+1 h2r]ar+1r+1+ h2r[ar+1 + ak + [ar+1 ar]]r+1 h2r[r+1 r][ar+1 ar] + h2rarRr": "We will see later that the third but last and last lines contain small perturbation terms and the second butlast line is zero for f P1 (Lemma A.16) and close to zero for general f . For now, we eliminate thedifference r+1 r by Taylor expansion (34), (35) to obtain",
  "for all r, s I": "Proof. First note that by Lemma A.20 in the technical supplements we have ar ar+1 so that they cannotchange sign. Upon eventually replacing f with f, we may assume without loss of generality that ar 0.Then, the result follows from Lemma A.13 with the choice zk = h2kakk. To prove its assumptions, we haveto showh2r+1ar+1r+1 h2rarr cd(h2rarr)hr + cd(h2r+1ar+1r+1)hr+1",
  "B.3Main Results with Besov Norms": "In the main Theorem 3.4 we use the Sobolev type norm f Lq(DI), which is unusual for q := 2/5 < 1.This is permissible, because the assumptions (13) requires higher weak derivatives in regular Lp norms with1 p . In this section, we consider a similar result in Besov norms. These allow a larger range of q, p < 1in the assumptions. Up to an arbitrarily small discrepancy in smoothness, the approximation bounds usethe same norms than classical adaptive approximation in (5).",
  "B.4Networks in Inner Weights": "In this section we prove Lemma 3.1. Without loss of generality, we use the domain D = . We firstshow an abstract characterization of critical points, similar to the proof sketch in .Lemma B.2. For an arbitrary function f L2(D), the weights Rm are a critical point of the lossf f2L2(D) if and only if"
}