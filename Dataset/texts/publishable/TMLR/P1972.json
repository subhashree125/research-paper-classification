{
  "Abstract": "We introduce Pointer-Augmented Neural Memory (PANM), a versatile module designed toenhance neural networks ability to process symbols and extend their capabilities to longerdata sequences. PANM integrates an external neural memory utilizing novel physical ad-dresses and pointer manipulation techniques, emulating human and computer-like symbolprocessing abilities. PANM facilitates operations like pointer assignment, dereferencing, andarithmetic by explicitly employing physical pointers for memory access. This module can betrained end-to-end on sequence data, empowering various sequential models, from simple re-current networks to large language models (LLMs). Our experiments showcase PANMs ex-ceptional length extrapolation capabilities and its enhancement of recurrent neural networksin symbol processing tasks, including algorithmic reasoning and Dyck language recognition.PANM enables Transformers to achieve up to 100% generalization accuracy in compositionallearning tasks and significantly improves performance in mathematical reasoning, questionanswering, and machine translation. Notably, the generalization effectiveness scales withstronger backbone models, as evidenced by substantial performance gains when we testLLMs finetuned with PANM for tasks up to 10-100 times longer than the training data.",
  "Introduction": "Systematic generalization underpins intelligence, and it relies on the ability to recognize abstract rules,extrapolating them to novel contexts that are distinct yet semantically similar to the seen data. Currentneural networks or statistical machine learning fall short of handling novel data generated by symbolic ruleseven though they have achieved state-of-the-art results in various domains.Some approaches can showdecent generalization for single or set input data (Bahdanau et al., 2018; Gao et al., 2020; Webb et al.,2020).Yet, neural networks in general still fail in sequential symbol processing tasks, even with slight",
  "a": ": PANM architecture. (a) The data memory contains the encoded input sequence (b) The addressbank contains physical addresses associated with data memory slots. The base and end addresses (aB, aE)define the address range of the input sequence. (c) The Pointer Unit takes aB, aE, recurrently generates thecurrent pointer pat and gets its value pat via Mode-1 (red)/2 (green) Access. (d) The Controller takes pointerinformation, decoding input (zt = yt), and produce the t-th output token yt. We note that for illustrationpurposes, the figure depicts a special case where the pointer perfectly matches an address (e.g., pat = 0010),in practice, the pointer may not point exactly to a single address. novelty during inference (Lake & Baroni, 2018; Deltang et al., 2022). For instance, these models can easilylearn to duplicate sequences of 10 items, but they will fail to copy sequences of 20 items if they were notpart of the training data. These models overfit the training data and perform poorly on out-of-distributionsamples such as sequences of greater length or sequences with novel compositions. The issue also affects bigmodels like Large Language Models, making them struggle with symbolic manipulation tasks (Qian et al.,2023). This indicates that current methods lack a principled mechanism for systematic generalization. From a neuroscience perspective, it has been suggested that the brain can execute symbol processing throughvariable binding and neural pointers, wherein the sensory data are conceptualized into symbols that canbe assigned arbitrary values (Kriete et al., 2013). Like the brain, computer programs excel at symboliccomputations. Programmers use address pointers to dynamically access data or programs, and have flexiblecontrol over the variable. Their programs can work appropriately with unseen inputs. Building on these insights, we propose a pointer-based mechanism to enhance generalization to unseen lengthin sequence prediction, which is a crucial problem that unifies all computable problems (Solomonoff, 2010).Our mechanism is based on two principles: (I) explicitly modeling pointers as physical addresses, and (II)strictly isolating pointer manipulation from input data. As such, we need to design a memory that supportsphysical pointers, and create a model that manipulates the pointers to perform abstract rules and accessto the memory. Our memory, dubbed Pointer-Augmented Neural Memory (PANM), is slot-based RAM(Von Neumann, 1993) where each memory slot consists of two components: data and address. Unlike initialendeavors that implicitly model pointers as attention softmax (Vinyals et al., 2015; Kurach et al., 2015), ouraddresses are generated to explicitly simulate physical memory addresses, i.e., incremental binary numbers,which is critical for generalization to longer sequences. To manipulate a pointer, we create an address bank that contains physical addresses corresponding to theinput sequence, and use a neural network called Pointer Unit that is responsible for transforming pointersfrom an initial address in the address bank.Through attention to the address bank, a new pointer isgenerated as a mixture of the physical addresses, which can point to different memory slots to follow thelogic of the task. We aim to let the Pointer Unit learn the symbolic rules of the task in an end-to-endmanner. Finally, given a (manipulated) pointer, the model can access the data through 2 modes of pointer-based access: pointer dereference (Mode-1) and relational access (Mode-2). Our memory can be pluggedinto common encoder-decoder backbones such as LSTM or Transformer. Our contribution is a novel memory architecture, PANM, which incorporates explicit pointer and symbolprocessing, seamlessly enhancing sequential models for better generalization. In our experiments, we inte-",
  "grate PANM into a range of deep learning backbone models of varying scales to assess the improvement ingeneralization:": "First, we add PANM to recurrent neural networks (LSTM) and memory-augmented neural net-works (SRNN), demonstrating excellent generalization in symbol-processing tasks like algorithmsand context-free grammar. We also apply PANM to Transformer models, improving their performance on compositional learningwith SCAN and mathematics datasets. Additionally, PANM significantly enhances Transformer andBERT generalization in question answering and machine translation tasks.",
  "Finally, we demonstrate PANMs scalability by integrating it with LLMs (Llama2-7B), showingsignificant improvement in length extrapolation for NLP tasks requiring symbolic reasoning": "Our focus is not on striving for state-of-the-art results requiring specialized designs tailored to specifictasks. Instead, we aim to highlight that universal generalization improvement can be achieved by integratingour memory module into various sequential models, with minimal architectural changes, and showcase theimportance of using fundamental generalizing principles to address limitations of current deep learning.",
  "with a target sequence Yi =yitl(Yi)t=1 where l is a function returning the length of the sequence. A model": "takes the input Xi and generates an output sequence Yi =yitl( Yi)t=1 where the predicted sequence terminatesas the model outputs token yit=l( Yi) = EOS where EOS is a special end-of-sequence token. Each predictedtoken is sampled from a categorical distribution, parameterized by and conditioned on the input sequenceand optionally with previous output tokens: yit p(yt|Xi, yit) where yit can beyikt1k=1 (true outputs)",
  "tlog p(yit|Xi, yit)": "We are interested in the ability to handle inputs of arbitrary length, so we focus on settings in which the lengthof testing input sequences is larger than that of training ones: max l(Xi) < min l(Xj) with Xi Dtrain andXj Dtest. In the following sections, when there is no confusion, we will drop the sample index i or j forease of reading. We note that autoregression is a special case of the s2s formulation where the input andoutput are from the same domain, and the target sequence is one step ahead of the input sequence.",
  "Pointer Modeling": "Computers are powerful thanks to their ability to manipulate pointers. These pointers store the address ofdata in memory. Following C programming language notation, let p denote a pointer associated with a datad in memory M, then p is also the address of the memory slot containing d, i.e., p = &d where & operatorreturns the pointer associated with value d. We can access the data pointed by p as p = d where operatorreturns the memory value associated with pointer p, which is also known as pointer dereference. We can manipulate the pointer to execute various tasks. For example, given a list X storing elements inconsecutive memory slots, &X denotes the pointer of the first element of the list. If the task is to copythe list X to a new list Y , using pointers, this task can be executed by iterating over the elements of thelist and copying each element from X to Y regardless of the list length and the values in X. The copying",
  "Published in Transactions on Machine Learning Research (10/2024)": "Bigbench AbstractPrompt:Given the choice:People who live in glass houses shouldnt throwstones\\nGood things come in small packages\\nDont put new wine into old bottles\\nBeatswords into ploughshares\\nPractice what you preach\\nA man who is his own lawyerhas a fool for his client\\nA barking dog never bites\\nHe who pays the piper callsthe tune\\nApril showers bring forth May flowers\\nFish always stink from the headdown\\nAbsolute power corrupts absolutely\\nJack of all trades, master of none\\nThewages of sin is death\\nLove of money is the root of all evil\\nChristmas comes butonce a year\\nWhats sauce for the goose is sauce for the gander\\nIts the squeakywheel that gets the grease\\nPractice makes perfect\\nThe age of miracles is past\\nAnarmy marches on its stomach\\nIf the mountain wont come to Mohammed, then Mohammedmust go to the mountain\\nA soft answer turneth away wrath\\nHoney catches more fliesthan vinegar\\nDead men tell no tales\\nBuild a better mousetrap and the world willbeat a path to your doorLink to proverb\\nRevenge is a dish best served cold\\nAllpublicity is good publicity\\nDont meet troubles half-way\\nFirst impressions are themost lasting\\nIt takes two to tango\\nAn Englishmans home is his castle\\nLittle thingsplease little minds\\nCut your coat to suit your cloth\\nLook before you leap\\nCheatersnever win and winners never cheat\\nA golden key can open any door\\nA prophet isnot recognized in his own land\\nSilence is golden\\nSuccess has many fathers, whilefailure is an orphan\\nGod helps those who help themselves\\nLaughter is the bestmedicine\\nTheres no accounting for tastes\\nDo unto others as you would have them doto you\\nA stitch in time saves nine\\nNever judge a book by its cover\\nHard cases makebad law\\nA house divided against itself cannot stand\\nTime is money\\nThat which doesnot kill us makes us stronger\\nSeek and you shall find\\nFailing to plan is planningto fail\\nThe cobbler always wears the worst shoes\\nYou are never too old to learn\\nHewho laughs last laughs longest\\nDont shoot the messenger\\nGood things come to thosethat wait\\nSeeing is believing\\nHindsight is always twenty-twenty\\nOnly fools andhorses work\\nFlattery will get you nowhere\\nNothing new under the sun\\nWhat cant becured must be endured\\nA cat may look at a king\\nNo rest for the wicked\\nVirtue is itsown reward\\nOnce bitten, twice shy\\nHaste makes waste\\nA nods as good as a wink to ablind horse\\nThe best things in life are free\\nStrike while the iron is hot\\nFrom thesublime to the ridiculous is only one step\\nDistance lends enchantment to the view\\nToerr is human; to forgive divine\\nHe who hesitates is lost\\nYou cant hold with thehare and run with the hounds\\nA poor workman always blames his tools\\nFeed a cold andstarve a fever\\nFinders keepers, losers weepers\\nMake haste slowly\\nGreat oaks fromlittle acorns grow\\nLive for today for tomorrow never comes\\nIt takes a thief to catcha thief\\nIts better to light a candle than to curse the darkness\\nTheres honour amongthieves\\nMoney doesnt grow on trees\\nBetween two stools one falls to the ground\\nGivecredit where credit is due\\nThe apple never falls far from the tree\\nNothing is certainbut death and taxes\\nCleanliness is next to godliness\\nBad news travels fast\\nLifeis what you make it\\nThe customer is always right\\nOne hand washes the other\\nDontlet the grass grow under your feet\\nThe end justifies the means\\nFor want of a nailthe shoe was lost; for want of a shoe the horse was lost; and for want of a horse theman was lost\\nPossession is nine points of the law\\nAn apple a day keeps the doctoraway\\nMarriages are made in heaven\\n.In what follows, we provide short narratives, each of which illustrates a commonproverb.\\nNarrative:Today was Kims birthday, and she wanted to celebrate withall of her friends.Cindy told her she couldnt join them, because she wanted to getsome work done.Kim asked if she could please take some time away from work to havefun and celebrate, but Cindy wouldnt.She just talked about all of her future plansthat she had to prepare for.Later that night, Cindy saw all of the pictures fromKims birthday celebration, and she felt sad.None of her future plans were happeningany time soon, and in the meantime, she missed the party.\\nThis narrative is a goodillustration of the following proverb:Target:Live for today for tomorrow never comes",
  "Pointer-Augmented Neural Memory (PANM)": "PANM acts as an external memory module for any neural network to support it handling sequence data.In such a memory-augmented neural network, a neural Controller (Ctrl) interacts with the memory (M)to read/write data and make predictions on the output target. Unlike traditional neural memory, PANMis equipped with an address bank (A) and a Pointer Unit (PU) to support pointer operations. To simplifymemory writing operations, PANM transforms the whole input sequence X to the memory M in the encodingphase such that L = l(M) = l(X) using M = Encoder (X) where X RdxL, M RdmL and the Encoder,parameterized by , can be any neural encoder such as LSTM or Transformer.The address bank A {0, 1}bL is then created and bound to M as mentioned in the previous section.During decoding, theencoded information in M is not changed and the controller focuses only on reading from M to produce theright output sequence. An overview of PANM decoding process is given in .",
  "Pointer Unit": "At each timestep t of the decoding process, PANM makes use of pointer variables pat , which are initializedas a valid address in the address space and then updated by the Pointer Unit PU. In tasks like summarizinga lengthy article by extracting key points from specific paragraphs, the Pointer Unit functions like a sophis-ticated \"bookmark\" system, starting at the targeted paragraph and sequentially progressing through eachkey point, effectively managing documents of any length by using relative positions. In particular, the PU,implemented as an GRU (Chung et al., 2014), takes an address from A as its initial inputs, e.g., pa0 = aB,and recurrently produces a key hat that performs address attention to create succeeding pointer variables pat :",
  "pat = Awat(3)": "where ha0 is initialized as 0 in Eq. (1). In Eq. (2), 1 n l(X), denotes the parameters of the PU andga () is a feed-forward neural network to transform the address to the same space as hat . According to 1sprinciple I, pat is softly assigned a physical address value in the address bank. Our pointer, pat , offers severaladvantages over implicit pointers made up of the attention weights (wat ), which are commonly utilized inprevious works (Vinyals et al., 2015; Luong et al., 2015). First, pat is a combination of physical addressesrepresented by binary numbers, and therefore its dimension is generally independent of the sequence length.In contrast, the dimension of wat varies with the input length. Therefore, arithmetic transformations on patare easier than on wat . Second, longer testing length poses challenges for traditional attentions to accuratelyproduce wat pointing to unseen location. Using physical key A to compute wat mitigates this issue byemploying random physical address ranges (see 2.2). Following 1s principle II, the PU recurrently transforms the original pa0 to a series of pointers {pat }l( Y )t=1suitable for the current task without using input data. This prevents unseen testing inputs disturb PUstransformations. In the copy example, an ideal arithmetic transformation ensure pa0 = aB and pat+1 = pat +1,which performs perfectly for any sequence whose length 2b. We aim to learn PU to automatically discoverpointer manipulation rules from the task. As the rules are learned, generalization is achieved even when thetesting sequence is longer or contains novel items.",
  "Pointer-based Addressing Modes": "Mode 1 In this mode, PANMs pointer functions like an index, much like accessing a particular row in a datatable or a sentence in a passage, so if the PU points to a sentence, it directly retrieves that sentence, regardlessof the passage content.Particularly, the content from memory M is retrieved directly by dereferencingpointers.To dereference pointer pat , we utilize the A M binding and the address attention weight wat ,retrieving the pointer value associated with pat as pat = Mwat . Through this dereference operator, we canaccess to arbitrary data in the memory M without relying on the content of the memory. This propertyenables robustness in handling new sequence when the memory content is novel and the process stays thesame (e.g., copy a never-seen-before sequence). Accessing M indirectly via A allows more memory slots tobe added to M during inference without affecting the processing rule as long as the PU can transform thepointer variable correctly. During training, PU experiences pointer variables covering the whole address spacebecause the base address is sampled randomly. Hence, it is possible for PU to correctly transform pointersthat point to extended addresses of a growing memory as long as the pointer manipulation rule does notchange. The address attention can be used for multiple pointer variables. In this case, there would be",
  "h=1 where Ha is the numberof attention heads. These pointer values will be used by the Controller for other memory reading": "Mode 2 This mode uses a more complicated memory access mechanism to capture relations between pointersin complicated reasoning tasks. The accessed content is not the one associated with the current pointer, butthose whose contents are related to the current pointers value. As an example, selection sort algorithm mayrequire comparing items in a list with the Mode-1 pointers item to select the greater one. Another exampleis to summarize a document by identifying the most important sentences based on their relevance to a query.In this mode, PANMs pointer goes beyond direct retrieval by using the content of a sentence to generatea query that ranks sentences based on relevance, much like running a search query to identify and selectthe most important sentences in a document based on their contextual significance. We simulate that usingattention with the query as the current pointer value:",
  "The Controller": "The Controller Ctrl is responsible for decoding the memory to produce outputs. Unlike other methods,we have pointer-based memory access to provide the controller with symbol-processing information.Inparticular, at the t-th step of the decoding, Ctrl takes the pointer values (mode 1 and 2) as input togetherwith an optional decoding input (zt = yt), and uses a GRU to recurrently produce the hidden state hct:",
  "where the hidden state hc0 is initialized as": "i M[i] and is the parameters of Ctrl.The GRU handlescontent-based input, empowered with pointer information to construct rich hidden state representations.Furthermore, the pointer-based data gives the GRU access to correct items in the sequence even when thememory content becomes different from the training due to encountering novel sequence length. The Controller Ctrl uses the pointer values (mode 1), the related pointer values (mode 2) and the hiddenstate hct to make the final prediction. It simply concatenates them and forward to the go ()a MLP, togenerate the output token",
  ": Average performance summary with key metrics": "another decoder to process the decoding input zt = Decoder (yt). For example, we can use Transformeras the Decoder (see Appendix C and D.3). The Encoder and/or the Decoder will be commonly referredto as the backbone model in which PANM is built upon. A summary of PANMs operation is given in Algo.1 and in the Appendix.",
  "Experimental Results": "In our experiments, we use two pointer variables in Mode-1 access and one for Mode-2 to balance betweenperformance and computing cost (Ha = 2, Hc = 1, see more in Appendix C). The two Mode-1 pointervariables are initialized as the base and end addresses. All MLPs in PANM have 1 hidden layer of 128dimensions. We use 256-dimensional GRUs for PU and Ctrl. The memorys address space has b = 10 bits,corresponding to a maximum of 1024 unique addresses, which is greater than any sequence length in theexperiments. In 3.1-3.3, our chosen tasks are representative of various types of symbolic reasoning and well-known bench-marks to measure the symbol-processing capability of machine learning models. To showcase that these tasksare non-trivial, we report how Chat-GPT (Achiam et al., 2023) failed on our tasks in Appendix D.7. Tofurther validate this point, we finetune Llama2-7B (Touvron et al., 2023) on similar tasks and show thatthe LLM also fails to generalize even with finetuning on task data (see 3.5). In addition, we validate thecontribution of PANM in other practical tasks in 3.4 and 3.5. We summarize the tasks and backbonesused in our experiment in . We also summarize the final average performance of PNAM and the bestbaselines in . Baseline Choice Despite some being simple, our baselines are still very strong methods in our studiedtasks. For example, in our algorithmic reasoning, LSTM with attention or Pointer Networks are still dom-inant baselines, outperforming the more recent Transformers. There are also other sophisticated methodsfocusing generalization such as ESBN (Webb et al., 2020). In Dyck recognition, stack-based models arestill SOTA because their inductive bias is suitable for the task.Experiments in 3.3 adopt (Universal)Transformer+Relative Positional Encoding (RPE), which is a very strong Transformer variant focusing ongeneralization. For experiments with LLMs, Llama2-7B stands out as the most capable open-source LLMthat is suitable for our hardware. In our experiments, PANM-augmented models are ensured to have similarmodel size as the baselines and always share similar backbones for fair comparison.",
  "Algorithmic Reasoning": "In our first experiment, we study the class of symbol processing problems where an output sequence isgenerated by a predefined algorithm applied to any input sequence (e.g., copy and sort). The tokens in thesequences are symbols from 0 to 9. The input tokens can be coupled with meta information related to thetask such as the priority score in Priority Sort task. During training, the input sequences have length up toL tokens and can grow to L+1, 2(L+1), 4(L+1) or 8(L+1) during testing. Our setting is more challengingthan previous generalization tests on algorithmic reasoning because of four reasons: (1) the task is 10-classclassification, harder than binary prediction in Graves et al. (2014), (2) the testing data can be eight timelonger than the training and the training length is limited to L 10, which is harder than Grefenstetteet al. (2015), (3) there is no curriculum learning as in Kurach et al. (2015), and (4) the training label is theone-hot value of the token, which can be confusing in case one token appears multiple times in the inputsequence and tougher than using label as the index/location of the token as in Vinyals et al. (2015). Here, we design several tasks. Content-free tasks involve permuting tokens in input sequence using certainposition-based rules: First-In-First-Out (Copy), Last-In-First-Out (Reverse) and Mix. While the first tworules demand linear pointer manipulations (traverse the sequence from the head or tail, to output the targettoken), the last one uses a non-linear, length-dependent manipulation rule: if t is odd, yt = x L 2 ; if t is even,yt = x1. Content-based tasks need the inputs token value together with symbol processing to arrangethe output sequence. We introduce 3 tasks: Dynamic Recall, Priority Sort and ID Sort. Readers can findthe details of these tasks in Appendix D.1. Baselines are categorized into 4 groups: (1) Traditional RNNs such as LSTM (Hochreiter & Schmidhuber,1997), (2) Sequential attention models: Content Attention (Bahdanau et al., 2014), Location Attention(Luong et al., 2015), Hybrid Attention (our baseline concatenates the attention vectors from content andlocation attention), (3) MANNs such as NTM (Graves et al., 2014), DNC (Graves et al., 2016), Neural Stack(Grefenstette et al., 2015) and Transformer (Vaswani et al., 2017), and (4) pointer-aware models: NRAM(Kurach et al., 2015), PtrNet (Vinyals et al., 2015), ESBN (Webb et al., 2020) and our method PANM. Inthis synthetic experiment, we adopt LSTM as the encoder for PANM. All baselines are trained with fixednumber of steps (100K for ID Sort and 50K for the rest), which is enough for the training loss to converge.For each task, each baseline is trained 5 times with different random seeds and we use the best checkpointon L + 1 mode validation to evaluate the baselines. Results We report the average accuracy across different testing length for each task in . Over-all, PANM significantly outperforms the best competitors ranging from 10-20% per task. Compared withindividual baselines, the improvement is much higher (Appendix D.1). We illustrate how the pointer ma-",
  "(a)(b)": ": (a) Dyck: mean std. accuracy over 5 runs with different testing lengths. (b) Machine translationtask: Perplexity on Multi30K dataset (the lower the better). We sort the sequences in the data by lengthand create 2 settings using train/test split of 0.8 and 0.5, respectively. The baselines are Transformer andPANM. Left: The best test perplexity over 2 settings for different number of Transformers layers (1 to 3layers). Right: an example of testing perplexity curves over training epochs for the case of 0.5 train/testsplit (2 layers) where we run 3 times and report the meanstd. The y-axis is visualized using log scale. nipulation works for Copy and ID Sort in (c) and (d). In Copy, only Mode-1 access is needed. Asdecoding step t increases, Pointer Unit generates pat following the increment of the addresses as expected.In ID Sort, both Mode-1 and 2 are needed. The Pointer Unit generates pat incrementally to trace the inputtokens from left to right (Mode 1). Then, the Mode-2 pointer pct is computed via attention to discover tokenwith the same id, which will be the output at step t. Without Mode-2 access, PANM certainly fails thistask. Experiments with varied number of heads are in Appendix D.6.",
  "Dyck Language Recognition": "Truly understanding the hidden law of context-free grammars such as Dyck (Dn) is challenging for neuralnetworks, even those with memory and attention (Yu et al., 2019). The language consists of strings withbalanced pairs of brackets of different types (|1, |2,...,|n), generated by the following rules: S |iS|i withprobability p/n or SS with probability q or with probability 1 p q. Here, p, q are parameter of thelanguage and is equivalent to EOS token. We follow the sequence prediction task and datasets in Suzgunet al. (2019) where the input is an unfinished Dyck string, and the output is the set of possible brackets forthe next token, e.g., for D2, ([] ( or ) or [. We follow the authors to enable set prediction by representingoutput yt as a multi-hot vector. We adapt PANM to this autoregression task by masking M to ensure the decoder only see tokens up to thecurrent decoding step. Since the token set is simple, we do not need to use any encoder, i.e., raw inputtokens are stored in M. The SOTA baseline in this task is SRNN (Suzgun et al., 2019), an autoregressivemodel using stack as memory. We use this model as the decoder to make the setup of PANM close to SRNN.The only difference is that PANM has Pointer-Based Memory Access ( (b)). To make the task morechallenging, we limit the maximum training length L to 10 (D2) and 20 (D3) tokens, and the testing lengthsare L + 2, 2L, 4L, 8L. We choose L as minimum numbers such that the model can perform decently ontraining data. The standard training and testing sizes are 5000. We train the models for 5 epochs andevaluate them on the training data at the end of each epoch to save model checkpoints. We use the bestcheckpoints for generalization test. (a) reports the models accuracy for D2 and D3. Under ourextreme setting, SRNN generalization fades out quickly as test lengths increase, especially for D3 whereasPANM performance degradation happens at a much slower rate, outperforming SRNN by around 20% onaverage in both tasks at any test lengths.",
  "Compositional Learning": "SCAN In this task , one needs to map an input sentence into an output sequence of commands (Lake &Baroni, 2018). The sequences are compositional, consisting of reusable parts. For example, in one case,jump twice should be mapped to JUMP JUMP and in another, walk twice becomes WALK WALK. We",
  ": bAbI QA: Testing accuracy (mean std.) over 5 runs": "focus on the length split datasets where the training sequences are shorter than the test ones with 11length modes L = 22, 24, .., 40 (Newman et al., 2020). We adopt the benchmark, training procedure andbaselines prepared by Csords et al. (2021), which achieves strong results under standard s2s learning. Here, our aim is not to break SOTA, which can be achieve by hybrid-symbolic architectures (Chen et al.,2020; Shaw et al., 2021). Instead, we focus on improving Transformer generalization in this task, hencethe baselines are chosen as several variants of Transformers (TRM) targeted to sequence extrapolation,including those using Relative Positional Encoding (RPE (Dai et al., 2019)) and Universal Transformer(U. TRM (Dehghani et al., 2018)), which is an advanced Transformer variant that recurrently processeseach token, and can dynamically adjust the number of processing steps. Following Csords et al. (2021),each baseline is trained 5 times for 50K steps and the resulting model after training is used for evaluation(no validation). Here, we use Transformer as the Encoder, which is the same as the TRM, and stack theController to another Transform Decoder (see details in Appendix D.3). Hence, the only difference is thedecoding where PANM leverages pointer manipulation. shows that PANM outperforms other baselines in the hardest settings when the training length is up-to 22, 24, and 25. For 22 and 24 cases, general models like PANM cannot show perfect generalization becausesome testing compositions is entirely discarded from the train set. In easier settings, PANM shares the perfectmedian accuracy with the sophisticated U. TRM + RPE although it does not use RPE. Remarkably, despitesharing the same encoder, TRM performs much worse than PANM and even fails to learn in easy modes(33, 36, 40), indicating the importance of pointer handling in this testbed. One problem for other baselinesis the EOS decision (when to generate ending token), which requires length tracking (Newman et al., 2020).As they do not have content-free sequence iteration mechanisms, it is extremely hard to trace the lengthwithout overfitting to the training data. On the other hand, PANM can hypothetically generate pointerincrementally and capture the difference between the last and the first pointers, i.e. the input length, andinfer the output sequence length based on that information. Mathematical Problems We test our model on mathematics (Saxton et al., 2018) where the in-put/output are questions and answers about math and each token is a character.For exam-ple, What is 5 110911? 110916 (add_or_sub) and What is the hundreds digit of 31253? 2(place_value). The task requires not only math reasoning, but also natural language understanding. Wefollow the training from Csords et al. (2021) to conduct experiments on 2 subsets: add_or_sub (a.s) andplace_value (p.v), and compare our method with Transformer-based baselines. Here, we focus on theextrapolating test set involving larger numbers, more numbers, more compositions, and thus longer inputsequences than the training. We use TRM + RPE as the Encoder and the Controller is added to a normalTRM decoder. As shown in , on place_value, PANM does not suffer from performance crash asTRM + RPE (0% test accuracy, as admitted in the paper (Csords et al., 2021) even though it uses thesame encoder). PANM achieves similar results as U. TRM+ RPE on add_or_sub while outperforming itby 11% on place_value. We also report PANM +Transformer results in Appendix D.3.",
  "Other NLP Tasks": "Question Answering Our objective is to explore the PANMs generalization beyond obviously composi-tional data by applying it in a more practical setting of question answering. For this purpose, we utilizetwo datasets, namely bAbI (Weston et al., 2015) and SQUAD 1.1 (Rajpurkar et al., 2016) where the inputsequence is a context paragraph and a question, and the output is the answer. To add complexity to the task,we ensure the length of test sequence is greater than that of the training by sorting the context paragraph by",
  ": Synthetic Algorithmic Tasks: (zero-shot) LLMs mean std. BLEU accuracy (%) over 5 runs.Training length is 10 letters. Bold denotes best": "length and splitting the sorted data into 0.8/0.2 and 0.5/0.5 ratio. Details of the data/task are in AppendixD.4. In bAbI, we configure the PANM similarly to the one described in 3.3 using Transformer backbone,and test the models after 100-epoch training. The models predict the answer tokens given the context andquestion tokens. As shown in and Appendix (right), PANM helps Transformer generalizebetter, consistently improving around 6% and 5% using 0.8/0.2 and 0.5/0.5 splits, respectively. Notably,PANMs testing loss is not diverged quickly as Transformers, indicating PANMs capability of reducingoverfitting. In SQUAD, we use BERT as the backbone to predict the start and the end of the answer as inKenton & Toutanova (2019). PANM-assisted model outperforms the baselines by 1% and 2% exact matchaccuracy, respectively (). The improvement is significant as BERT is a big foundation model alreadypretrained with big data and robust against novel test data. Machine Translation Here, we want to verify the PANM in machine translation and show that PANM canwork with different number layers of Transformer. The results are presented in (b) where we reportthe model perplexity on Multi30K (en-de) dataset. The 30K-sample dataset is sorted by input length andsplit into training and testing s.t. testing sequences are longer, similar to QA task. The results demonstratePANM can consistently improve the generalization performance of Transformer across different split ratiosand the number of encoder/decoder layers.",
  "Scaled Generalization with LLMs": "In this section, we explore the compatibility of PANM with LLMs. Specifically, we use Llama-2 (Touvronet al., 2023) as the backbone model and integrate the PANM layer on top of the final attention layer. Thereare 3 baselines: the pretrained LLM (Llama2-7B), the finetuned LLM (Llama2-7B FT) and the finetunedLLM+PANM (PANM). The models are finetuned with the same training configuration such as LoRA (Huet al., 2021) and AdamW optimizer (Loshchilov & Hutter, 2018). The evaluation is executed using LanguageModel Evaluation Harness library (Gao et al., 2023). Synthetic Algorithmic Tasks In the first test, we use Copy and Dynamic Recall (D. Recall) similarto those described in 3.1. We finetune both the LLM and LLM+PANM on a training dataset and thenevaluate them on a separate testing dataset. In this test, each token is a letter sampled from the Englishalphabet (case sensitive), and the training data consists of 100,000 sequences, each of maximum 10 letterslong, combined with an instruction introducing the task (see Appendix D.5). After fine-tuning, we evaluatethe models on multiple testing sets, each of 1000 testing sequences with sequence lengths ranged from 10 to1000 letters. The results in show that normal LLM finetuning generally outperforms the pretrained LLM indownstream tasks, but performance degrades rapidly as the testing length increases. Specifically, withoutPANM, performance drops to 0% at test lengths of 100 and 20 for Copy and Dynamic Recall, respectively.In contrast, PANM achieves 86% and 80% at these testing lengths. Notably, PANM maintains reasonableperformance in the Copy task up to a test length of 1000. We also examine the evaluation setting when 5few-shot examples are added to the prompt to enable LLM in-context learning. The results in Appendix confirm that PANM significantly outperforms other baselines in this setting. Compared to theresults of the same tasks in 3.1, we observe that PANM reaps more benefits in maintaining generalizationwith a stronger backbone like Llama2-7B, highlighting the versatility and potential of PANM. BIG-bench Tasks Here, we focus on challenging benchmarks designed for LLMs. We pick the first twotasks from the BIG-bench benchmark (Srivastava et al., 2023), one is symbolic (Arithmetic) and one is not(Abstract). To ensure the experiment is both practical and challenging, we finetune the model using only asmall subset of the training set containing the shortest input sequences. We then evaluate the model on atesting set with the longest input sequences to assess its generalization to longer sequences. Details of thechosen tasks are given in Appendix D.5. The results in demonstrate that PANM significantly improves the generalization accuracy of finetunedLLMs, outperforming the original finetuned LLM by a large margin of 16% and 3% in Arithmetic andAbstract, respectively. Even when augmented with 5 in-context examples, PANM maintains a substantialgeneralization gain (19% and 7%), confirming its effectiveness as a plug-and-play memory module for LLMs.",
  "Related works": "There are many attempts to augment neural networks with external memory (MANN) to improve theirsymbol-processing ability.Pioneers such as NTM (Graves et al., 2014) and DNC (Graves et al., 2016)propose computer-like memory read/write operations with content-based attention mechanisms, and thus inprinciple, can execute any symbolic rules. However, learning the hidden law end-to-end from sequence datais extremely difficult. Therefore, MANNs including Transformers (Vaswani et al., 2017), may fail miserablyin out-of-distribution testbeds, especially length extrapolation (Deltang et al., 2022). Recent LLMs aregood at reasoning and generalization, but bad at symbolic processing (Qian et al., 2023; Tang et al., 2023).We use LLMs only to show our task difficulty (Appendix D.7), not as a baseline, because they are not onthe same scale as our method. Many recent works advocate the use of specialized memory architectures such as stacks (Grefenstette et al.,2015; Hao et al., 2018; Suzgun et al., 2019), key-value memory (Webb et al., 2020; Le et al., 2020a) andimproved attentions (Kurach et al., 2015; Russin et al., 2019; Dubois et al., 2020; Le et al., 2019). Thesemethods employ different inductive biases in designing the memory and attention, yet not following the twoprinciples advocated by our paper. Although they may work remarkably on certain synthetic tasks, they are",
  "Conclusion": "We introduce a neural memory model called PANM, designed to manipulate pointers and learn symbolprocessing rules for improved length extrapolation. PANM separates symbols from data and utilizes anaddress bank to enable data-isolated pointer manipulation through address attention. PANM consistentlyoutperforms strong baselines in tasks such as algorithm mining, compositional learning, mathematical rea-soning, context-free grammar recognition, and practical NLP tasks, even when test sequences are significantlylonger than training sequences. Remarkably, PANM achieves these results across various backbone models,including LSTMs, Transformers, and LLMs. LimitationsWhile PANM provides significant benefits in enhancing sequence processing, it does comewith certain limitations: (1) Computational Complexity: PANM introduces additional computational over-head, particularly when used with smaller models like LSTMs, where complexity can increase by 20-30%. Forlarger models, such as LLMs, this overhead is minimal, making PANMs operations relatively inexpensive.(2) Memory Requirements: The additional memory needed for the Address Bank and Pointer Unit is smallrelative to large backbone models. However, these components do contribute to overall memory usage, whichmay be a consideration in resource-constrained environments. (3) Implementation Challenges: IntegratingPANM into existing architectures might require engineering effort. Ensuring seamless interaction betweenthe backbone and PNAM and tuning PANM hyperparameters for optimal performance on specific tasks mayrequire additional experimentation. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023.",
  "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal trans-formers. In International Conference on Learning Representations, 2018": "Grgoire Deltang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, MarcusHutter, Shane Legg, and Pedro A Ortega. Neural networks and the chomsky hierarchy. arXiv preprintarXiv:2207.02098, 2022. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longersequences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,pp. 403413, 2020. Aaron Eisermann, Jae Hee Lee, Cornelius Weber, and Stefan Wermter. Generalization in multimodal lan-guage learning from simulation. In 2021 International Joint Conference on Neural Networks (IJCNN),pp. 18. IEEE, 2021. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Lau-rence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, ChrisOciepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang,Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation,12 2023. URL Tong Gao, Qi Huang, and Raymond Mooney. Systematic generalization on gscan with language conditionedembedding. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Com-putational Linguistics and the 10th International Joint Conference on Natural Language Processing, pp.491503, 2020.",
  "Hung Le, Truyen Tran, and Svetha Venkatesh. Neural stored-program memory. In International Conferenceon Learning Representations, 2020a. URL": "Hung Le, Truyen Tran, and Svetha Venkatesh. Self-attentive associative memory. In Hal Daum III andAarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 ofProceedings of Machine Learning Research, pp. 56825691, Virtual, 1318 Jul 2020b. PMLR. Yuxuan Li and James McClelland. Systematic generalization and emergent structures in transformers trainedon structured tasks. In NeurIPS 22 Workshop on All Things Attention: Bridging Different Perspectiveson Attention, 2022. URL Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, andDongmei Zhang. Compositional generalization by learning analytical expressions. Advances in NeuralInformation Processing Systems, 33:1141611427, 2020.",
  "Mirac Suzgun, Sebastian Gehrmann, Yonatan Belinkov, and Stuart M Shieber. Memory-augmented recurrentneural networks can learn generalized dyck languages. arXiv preprint arXiv:1911.03329, 2019": "Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang.Large language models are in-context semantic reasoners rather than symbolic reasoners. arXiv preprintarXiv:2305.14825, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tunedchat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.",
  "AMore Discussion on Related Works": "The proposed address attention in our paper is comparable to two known mechanisms: (1) location-basedattention (Luong et al., 2015; Dubois et al., 2020) and (2) memory shifting (Graves et al., 2014; Yang, 2016).The former uses neural networks to produce attention weights to the memory/sequence, which cannot helpwhen the memory grows during inference since the networks never learn to generate weights for the additionalslots. Inspired by Turing Machine, the latter aims to shift the current attention weight associated with amemory slot to the next or previous slot. Shifting-like operations can handle any sequence length. However,it cannot simulate complicated manipulation rules. Unlike our PU design which obeys 1s principle II, theattention weight and the network trained to shift it depend on the memory content M. That is detrimentalto generalization since new content can disturb both the attention weight and the shifting network as thememory grows. Another line of works tackles systematic generalization through meta-learning training (Lake, 2019), whileour method employs standard supervised training. These approaches are complementary, with our methodconcentrating on enhancing model architecture rather than training procedures, making it applicable indiverse settings beyond SCAN tasks. Additionally, the study by Hu et al. (2020) addresses syntactic gener-alization (Hu et al., 2020), a different problem compared to our paper, which emphasizes length extrapolationacross various benchmarks. Notably, our paper considers similar baselines, such as LSTM and Transformer,as those examined in the referenced papers. There are other lines of research targeting reasoning and gen-eralization using image input(Wu et al., 2020; Eisermann et al., 2021). They are outside the scope of ourpaper, which specifically addresses generalization for longer sequences of text or discrete inputs Our address bank and physical pointers can be viewed as some form of positional encoding. However, wedo not use simple projections or embeddings to force the attention to be position-only. Instead, we aim tolearn a series of transformations that simulate the position-based symbolic rules. At each time step, a newpointer (\"position\") is dynamically generated that reflects the manipulation rule required by the task (e.g.move to the next location), which is unlike the positional encoding approaches such as RPE (Dai et al., 2019)which aims to provide the model with information on the relative position or distance of the timesteps. Wesummarise the difference between our method and Transformer in .",
  "BMore Discussion on Base Address Sampling Mechanism": "We provide a simple example to illustrate how base address sampling help in generalization. Assume thetraining sequence length is 10, and the desired manipulation is p = p + 1 (copy task). Assume the possibleaddress range is 0, 1, ..., 19, which is bigger than any sequence length. If aB = 0 , the training address bankcontains addresses: 0, 1, ...8, 9. Without base address sampling, the model always sees the training addressbank of 0, 1, ...8, 9 and thus can only learn manipulating function for 0 p 9, thereby failing when testingaddress bank includes addresses larger than 9. Thanks to base address sampling, at some point of training, aB = 10 , the training address bank is10, 11, ...13, 19. The manipulating function sees p > 9 and can learn to transform p = p + 1 for p > 9,e.g., transform p = 10 p = 11. The learning happens because the pointers value (p) is used to predictthe output sequence. The task loss will reward p that follows the rule, and update the Pointer Unit suchthat it transforms the p following the rule to minimize the loss. During testing, the input length can be 12,we set aB = 0 and the address bank is 0, 1, ...., 10, 11. The learned manipulation can still be applied to newlocations 10th, and 11th. We can prove that the complexity of exposing all addresses to the model is practically small compared tothe normal training. Assume the training input sequence length is L, and the number of possible addressesis Lmax. Here, Lmax indicates the possible testing length that the model can handle. When Lmax ,the expected number of samples required for exposing all addresses is O (n log n) where n = Lmax/L (we canformulate this problem as Coupon collectors problem). For example, in even an extreme address range ofLmax = 106 (in practice we rarely need that big range) to train input sequences of length 10, we only need",
  "DifferenceTransformerPANM (Our)": "KeyKeys are computed based on inputThe keys in our approach areGenerationdata. Hence, when meeting novelgenerated as fixed numbers,data during testing, Transformerspecifically physical memorywill observe novel keys, and cannotaddresses. These keys are entirelywork properly.separate from the data. ExtendableThe dimension of attention weightsThe fixed nature of ourto Longervaries with input length, makingphysical addresses allowsSequencesarithmetic transformations onour pointers to be easilythese attention weights infeasiblemanipulated and extendable",
  ": PANM vs Transformer": "to sample 105 log 105 sequences, which is often smaller than the size of the training datasets. Empirically, inour experiments, we always train our method with the same number of batch size and training steps as otherbaselines to ensure a fair comparison, and we realize that it is always possible to expose all the addresses toour model during training.",
  "information from some initial addresses p pa0,hHa": "h=1 such that p = aE. During training, the learnedpointer arithmetic to perform Reverse at the first step of the decoding to produce value y1 can only be afunction of p: y1 = aE = f(p), that is, aE = f(p). During testing, aE can receive arbitrary value, so forwhatever learned f, we can always find a test sequence such that aE = f(p) f because aE = p. A similarargument can be used for aB and Copy task. In the main manuscript, we only experiment with 1 Mode-2 pointer (Hc = 1). If Hc = 0, obviously PANMwill fail in tasks such as ID Sort. Using more Hc and Ha can still be beneficial in exchange for slowercomputation (see Appendix D.6). In all experiments, we use 256-dimensional GRUs for the PU and Ctrl.The encoder and decoder (to stack the Controller on) can vary across tasks. The general plug-and-playframework is illustrated in . We also summarize operations of our model in Algo. 1.",
  "Model": ": PANM as a plug-and-play architecture. The encoder and decoder can be any model (LSTM,Transformer or BERT). PANM Controller can be used as the last layer of the Decoder to access the memoryduring decoding. To reduce the number of parameters of the augmented architecture, the decoders numberof layers can be decreased.",
  "DExperimental Details": "All datasets and public codebases used are licensed under Apache or MIT License.We trained all themodels on a single Tesla V100-SXM2 GPU. The running time of PANM depends on the Encoder and thespecific tasks. Overall, with 2 Mode-1 pointers and 1 Mode-2 pointer, PANM operates at 70-80% of thespeed of the backbone model. For instance, in the Copy task, PANM achieves 15 iterations per secondcompared to LSTMs 20 iterations per second.When using a Transformer Encoder, PANM runs at 77iterations per second, whereas the Transformer runs at 90 iterations per second. Notably, the larger thebackbone, the smaller the difference in running time. In LLM experiments, there is no significant differencein training/inference speed between settings with PANM and without PANM, with speeds of 15.2 iterationsper second and 15.6 iterations per second, respectively.",
  "We first give the details of the content-based tasks below": "In Dynamic Recall, an arbitrary input token is chosen as the query and is added to the end of the inputsequence. Depending on the length of the input, a.k.a, odd or even, the first target token will be on the leftor right of the query, following its succeeding tokens in the input. This task requires both content matching(find query token in the input sequence) and position-based access (shift left or right). In Priority Sort, each input token is associated with a priority score sampled from the standard normaldistribution. The target output will be tokens from the input sequence sorted ascending by their the score.This task can be solved in many ways and likely needs complicated symbol processing such as looping throughitems in the sequence and comparing the score of tokens. Finally, in ID Sort, each input token is augmented with an id feature vector sampled from standardmultivariate normal distribution such that every 2 tokens share one id. For example, with input x1, x2, x3, x4,x1 and x4 may share one id while x2 and x3 shares another id. The pairing is chosen randomly. The outputtoken at position i-th will be the input token that share id with the i-th input token. The correct output",
  "for the earlier example is x4, x3, x2, x1. This task is specifically designed to test the ability to learn Mode 2pointer-based memory access": "In this task, we implement the baselines such as LSTM, attention models and Transformer using Pytorchlibrary. The hidden state dimension for these models are set to 512, which results in around 1-3 millionsparameters. We tuned the number of layers of the encoder/decoder for these baselines in Copy task, andrealized that 1-layer gave the best performance.For NTM and DNC, we use public repositories1 withdefault controllers hidden state of 256 dimensions and 128-slot external memory, which results in around1.2 millions parameters. We use the ESBNs author codebase 2 with default parameter setting, resulting in1.2 million parameters. For PtrNet, since we do not use token index as the training label, we produce thepredicted token by performing weighted sum the input tokens using the PtrNets attention weights. PtrNetshyperparameters are the same as attention models. We could not find the authors code for Neural Stackand NRAM so we implemented them and tuned hyperparameters for the Copy task at length L such thatthe model sizes are about 1.1 million parameters. In this task PANM uses LSTM with hidden state of 256 asthe Encoder and does not stack the Controller on any decoder models, resulting in 1.1 million parameters. In this experiment, all the models are trained without teacher forcing as in Graves et al. (2014), i.e, theinput to the decoder is zero (zt = 0). The detailed average accuracy (mean std.) of each method togetherwith the actual length of each testing mode are reported in Tables 13-18. Overall, PANM observes significant improvement ranging from 10-20% on each task. We note that whencompared with individual baselines, the improvement is much higher. Consider Copy as an example (a), PANM outperforms the worst baseline Transformer by around 60% at 2(L + 1) and 30% at 4(L + 1),respectively. As stated earlier that our tasks are challenging, thus, originally strong baselines such as NTM,DNC, and Neural Stack do not generalize well at extreme lengths, especially in ID Sort. ID Sort is trickierthan content-free tasks, making some baselines fail at length L even though it is in the training data. Thebest other model in this case is Content Attention, which clearly underperforms our PANM from few % to50% (b). Without curriculum learning and under the 10-class prediction setting, methods that useimplicit pointers, such as PtrNet, NRAM, and ESBN, demonstrate mediocre performance on average whencompared to PANM. Furthermore, PANM also outperforms in length-dependent tasks (Mix, D. Recall),indicating that it can track the sequence length in extrapolation. We hypothesize that PANMs contentfreepointer generation mechanism to simulate list iteration makes it possible. In Copy, only Mode-1 access is needed. As decoding step t increases, Pointer Unit generates pat followingthe increment of the addresses as expected. That said, for several steps, the address attention is not sharp,showing other addresses pointed by the pointer, which is not surprising since we use soft attention and itis hard for a neural network to learn the exact rule: pat+1 = pat + 1. This problem gets worse as test lengthincreases as the error accumulates, especially when the same token can appear many times, which confusesthe model. This explains why PANMs performance drops clearly in the hardest case 8(L+1). Yet, it is stillsignificantly better than others whose results are near random prediction.",
  "D.2Dyck Language Recognition": "In this task, we adopt the SRNN code from Suzgun et al. (2019)3 using the default parameters. As explainedin the main text, this task is auto-regression, hence, zt = yt1. PANM adopts SRNN (an auto-regressivemodel) as the encoder and does not stack the Controller on any decoder models. The result is visualized in (left).",
  ": SCAN: PANMs exemplar learning curves": "its last layer by the Controller. Formally, zt in Eq. 7 becomes Decoder(yt) where the Decoder is a 2-layer Transformer. In Mathematics reasoning task, we use similar integration except that the Encoder isTransformer with relative positional encoding (TRM + RPE). By reducing the number of decoding layer, weensure PANMs hyperparameter number equivalent to that of the Transformer baseline (12M). All modelsare trained with teacher forcing as in Csords et al. (2021). SCAN The training size is 16990 and the test size is 3920. SCAN is a well-known and standard benchmarkfor testing compositional learning and generalization in sequential models. One property of this dataset isthat a new length often contains new rules that must be captured by the model to ensure generalization,and thus, if the model fails to learn a hidden rule, its performance may drop significantly from one lengthsplit to another. illustrates PANMs testing accuracy curves when L = 22, 24, 25, 26. Other learningcurves for L > 26 looks similar to L = 26 where PANM easily solves the task perfectly. Mathematical Problems reports the accuracy with mean and standard deviation. Here, weaugment TRM and TRM+RPE with PANM. Both shows improvement, especially for TRM+RPE, indicatingthat PANM is compatible with other methods designed to improve generalization in Transformer.",
  "D.4Other NLP Tasks": "The bAbI dataset consists of 20 synthetic tasks that evaluate various reasoning skills. To prepare the datafor each task, we combine train/valid/test into a single set and sort it by length and split it into trainingand testing sets, as described in the main text. We train the models jointly on all 20 tasks and measurethe accuracy of their answers, which are considered correct only if they match the ground truth answerperfectly. The training/evaluation follows exactly the standard protocol presented in (Le et al., 2020b). TheTransformer used here has 8 heads, 3 layers of encoding, 3 layers of decoding, and hidden dimensions of512. PANM uses the same Transformer backbone except that the decoder has 2 layers to make the modelsize equivalent. We run each model 5 times to report the mean and standard deviation as in (right). reports the detailed numbers.",
  "Target:F b P i P B m s G j": "Dynamic RecallPrompt:This is dynamic recall task.You are given a sequence of characters.Thelast character is the query and you need to retrieve the first character after orbefore the query in the sequence if the number of the characters in the sequence isodd or even, respectively.Given this:E V y D m I e V u f _ V \\n Your answer:",
  "D.6Additional Experiments": "Generalization to Shorter SequenceHere, we explore whether PANM can maintain its performancein a long-to-short generalization scenario. We selected a challenging synthetic task, Mix, and trained bothPANM and Location Attention (the best baseline for this task) on sequences of length 20-30, testing themon sequences of length 10. The training and testing curves in (left) reveal that while both modelsachieve near-perfect training results, only PANM shows smooth learning and signs of generalization to thetest length of 10, with an improvement margin of nearly 14% compared to Location Attention. Furthermore, in a more practical Machine Translation task, we trained both PANM and Transformer (thebest baseline for this task) on 50% longer sequences and tested on the remaining shorter sequences usingthe same 2-layer Transformer backbone. As shown in (right), the Transformer model overfits andfails to generalize to shorter sequences. In contrast, PANM achieves reasonable perplexity on the test data,outperforming the best Transformer result by approximately 25 points.",
  "Pointer HyperparametersIn this section, we confirm the logic presented in Appendix C by performingexperiments that involve varying the number and type of pointers": "Mode-1 Pointers We test the PANM version in 3.1 with Ha = 0, 1, 2, 3 on Copy, Reverse. We do not useMode-2 pointer here to avoid confusion (Hc = 0). plots the testing accuracy over training time. AsHa = 0, there is no pointer information for the Controller, PANM should be equivalent to an GRU and failto generalize. As Ha = 1, the only pointer is initialized either with the base or end address. As shown inFig., PANM cannot generalize in both Copy and Reverse tasks with single Mode-1 pointer, which is provedin Appendix C. In the caseHa = 3, we initialize them with the base, end and middle addresses. We observethat increasing Ha to 3 slightly reduces the performance in these tasks. We speculate that too many Mode-1pointers make the learning harder; in particular, learning to manipulate the third pointer may interfere withthat of the first or second pointer, which are more important in these tasks. Generally, most tasks onlyrequire list iterations from the head to the tail or vice versa. Hence, we keep Ha = 2 in all experiments tosave the computation cost. Mode-2 Pointers We fix Ha = 2, and vary Hc = 0, 1, 2 on Copy, Priority Sort, ID Sort. As shown in , without Mode-2 pointers (Hc = 0), generalization in Priority Sort and ID Sort is reduced significantlyby 50% and 30%, respectively because these tasks focus more on the content of the input sequence andoften demand comparing the content of different tokens. Interestingly, a content-free task like Copy alsosuffers from performance drop if there is no Mode-2 pointer. Specifically, we find out that for 2/5 runs,",
  ": Testing accuracy (mean std.) at 2(L+1) length over training steps. Different configurations ofMode-2 pointers are trained and evaluated 5 times": "the model converges to a suboptimal solution, leading to high variance and slightly lower mean accuracy.Perhaps, Mode-2 pointer allows the decoder to access the input instantly (like content-based attention),avoid forgetting, and thus, improve the prediction as the sequence is longer. Having more Mode-2 pointersgenerally improves the generalization in Copy and Priority Sort, yet the gain is small for Hc = 2, or evennegative in ID Sort. Therefore, we trade-off performance with computing efficiency by setting Hc = 1 in ourexperiments.",
  "D.7Failures of Chat-GPT in Our Tasks": "Large Language Models (LLMs), especially Chat-GPT, have shown remarkable results in reasoning and gen-eralization. Directly comparing Chat-GPT with other models used in our experiments would be unfair be-cause Chat-GPT was not directly trained with our datasets and it has much more parameters than our model.Therefore, in this section, we merely use Chat-GPT as a tool to verify that our chosen tasks, despite beingsimple, are non-trivial. The evaluated tasks are algorithmic reasoning and SCAN. We do not examine Dyckrecognition because the output encoding is complicated to represent in text. Other datasets are more commonand likely to be used for training Chat-GPT, thus, are not suitable for generalization test. For example, inMathematics task, if we ask Chat-GPT the question from the data What is the hundreds digit of 31253?,it provide the correct answer (2). However, slightly modifying the question to ensure it does not appear inthe training and testing set will successfully fool Chat-GPT:",
  "input x21, x22, ... output y21, y22, ......Question:input x1, x2, ... output": "Algorithmic ReasoningTo ensure that Chat-GPT does not memorize the output answer from its vasttraining data, we use non-digit symbols: ~!@#$%^&*( as 10 tokens of the datasets. For each task, we sample20 training examples of length L = 5 to build the in-context examples, and test on 1 longer sequence of length2L = 10. We conduct 20 trials and report the average test accuracy. summaries the evaluationresults. Overall, except for Copy task where Chat-GPT shows excellent generalization, other tasks are veryhard for Chat-GPT, indicating that the length extrapolation problem still poses a big challenge to today AItechniques. SCANIn this task, we sample 20 examples in the L-cutoff=40 split set (easiest) as in-context learningexamples and evaluate on 10 unseen sequences. Chat-GPT totally failed in this task. When testing on thesimilar length or longer length as the examples, Chat-GPT cannot produce any exact match results (exactmatch accuracy=0). Below are some failure examples:"
}