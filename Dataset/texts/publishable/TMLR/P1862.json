{
  "Abstract": "Despite recent progress in federated learning (FL), the challenge of training a global modelacross clients, having heterogeneous, class-imbalanced, and unlabeled data, is not fully re-solved. Self-supervised learning requires deep and wide networks, and federal training ofthose networks induces a huge communication/computation burden on the client side. Wepropose Modular Federated Contrastive Learning (MFCL) by changing the training frame-work from end-to-end to modular, meaning that instead of federally training the entirenetwork, only the first layers are trained federally through a server, and other layers aretrained at another server without any forward/backward passes between servers. We alsopropose Twin Normalization (TN) to tackle data heterogeneity. Results show that ResNet-18 trained with MFCL(TN) on CIFAR-10 achieves 84.1% accuracy when data is severelyheterogeneous while reducing the communication burden and memory footprint comparedto end-to-end training. The code will be released upon paper acceptance.",
  "Introduction": "Federated Learning (FL) has been introduced as a framework to learn from the data located at multipleclients without transferring the clients raw data to a central location, called the server McMahan et al.(2016; 2017); Konen`y et al. (2016a;b). In each FL round, each clients local model is trained on its dataset,and the local models parameters are sent to the server to be aggregated and broadcast back to the clients.The distribution of local datasets plays a vital role in orchestrating the real-world implementations of FL.Simply aggregating the model parameters trained on heterogeneous datasets does not improve the globalmodel accuracy unless additional tricks are adopted such as regularization techniques in local objectives ofthe clients Luo et al. (2021); Dieuleveut et al. (2021); Mu et al. (2021); Li et al. (2020; 2021); Durmus et al.(2021); Yu et al. (2020b), adaptive aggregation in the server Wang et al. (2020a), data sharing on the clientsor server Zhao et al. (2018); Hao et al. (2021), and data augmentation Shin et al. (2020) to reduce the gapbetween the local and global models. Meanwhile, addressing the challenges of data labelling in FL, someself-supervised methods have been proposed considering data heterogeneous clients Lubana et al. (2022); Luet al. (2022); Makhija et al. (2022); Huang et al. (2022); Miao & Koyuncu (2022); Yu et al. (2020a). Inparticular, contrastive Chen et al. (2020) and non-contrastive learning Grill et al. (2020); Chen & He (2021)methods have been applied to FL Zhang et al. (2020b); Zhuang et al. (2021; 2022). The existing techniquesare mostly based on training one or even two deep networks at each client, adopting various methods such asmoving average update at local models Zhuang et al. (2022), tuning the communication frequency betweenthe clients and server based on the divergence of local and global models Zhuang et al. (2021), or aggregatingand sharing the clients representations based on their distance Zhang et al. (2020b) to ensure that clientscan collaboratively learn from the unlabeled data Shi et al. (2021). For a detailed review, see Appendix A.1.",
  "(d) Modular resource com-patible FL (self-supervised)[ours]": ": Existing resource-compatible schemes and the proposed method. The red arrows indicate theforward and backward passes through all layers. The parameters of the bold connections are transmitted tothe server in each FL round, as indicated by the blue and green arrows. (a) Clients send all parameters tothe server in end-to-end (E2E) FedAvg. (b) Clients send parameters of some filters (a vertical cut) of themodel to the server in E2E HeteroFL. (c) Clients send parameters of some layers (a horizontal cut) of themodel to the server in E2E Split-Mix. (d) Clients send parameters of some layers (a horizontal cut) of themodel to the first server (gServer) in the proposed modular scheme. After the initial layers are trained, therest of the model is trained on the representations received in the rServer. The gray and pink arrows showthe forward and backward passes on the client side and rServer, respectively. The white arrows show thetransmission of the representation vectors, which is done only once. Typically, self-supervised learning relies on a large amount of data, deep and wide models, and/or large batchsizes to achieve good performance, which means high computing power and/or large memory footprints areneeded Zhuang et al. (2022). Such resources are often unavailable on clients devices, especially in cross-device FL scenarios. To address these issues, researchers have proposed different scaling methods for clientsto have different-sized local models depending on their available computation resources (). In Diaoet al. (2020); Horvath et al. (2021), the authors suggested dividing the network based on its width andassigning a different number of filters to different clients based on their computational resources (b).In more recent schemes, the network is divided based on its depth, and different numbers of modules (sub-models) are assigned to different clients Kim et al. (2022); Hong et al. (2022) (c). However, there aremainly three limitations in these resource-compatible architectures. First, in those schemes, only supervisedloss functions are considered. Shallow or narrow networks are typically insufficient for self-supervised tasksbecause they lack the capacity to learn rich, informative representations from complex or highly diverse data,particularly in scenarios involving heterogeneous or class-imbalanced data. In FL, where data heterogeneityis a major challenge, shallow networks are unlikely to extract representations that generalize well acrossdifferent clients, especially when each client has unique data distributions. Self-supervised learning methods,either contrastive Chen et al. (2020) or non-contrastive Grill et al. (2020), require networks with enoughdepth and capacity to align representations from diverse data sources effectively. Second, the performance of those schemes has not been comprehensively studied under severe heterogeneousand class-imbalanced (CIB) data scenarios. If some clients with unique data cannot afford to train a deepand wide sub-model, the global model cannot learn those unseen distributions. The performance degradationmight be even worsened when the local data is CIB or the number of clients is large (e.g., more than 10)Hong et al. (2022). Third, although those schemes are proposed for resource-limited clients, the computation(and communication) burden on the side of clients is still high Kim et al. (2022). Moreover, clients still needlarge memory to implement the entire graph to backpropagate the gradients from the last layer to the firstlayer of the whole network, which is needed for any type of end-to-end (E2E) learning. Although E2E learning performs well on numerous tasks, in some cases (e.g., ill-conditioned problems), E2Elearning might converge slowly, converge into possibly local optima, face vanishing gradients Shalev-Shwartz",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Accuracy comparison of MFCL and baselines on CIFAR-10 with M = 10 on ResNet-18, CIFAR-100 with M = 100 on ResNet-18, and Tiny-ImageNet with M = 100 on ResNet-50. The best performanceis highlighted in boldface. For SSFL methods, the top-1 accuracy with linear evaluation is reported. ForMFCL, BN is used at the rServer.",
  "We propose TN, for contrastive learning in FL, which performs better than BN and Group Normal-ization (GN) in severe data heterogeneous scenarios": "Through experiments, we demonstrate the effectiveness of the proposed MFCL, especially with TN,which achieves robust, stable, and state-of-the-art performance on severe heterogeneous and CIBdata while only a small-size client module is trained federally across clients. 1In FL literature, leveraging two (or more) servers has been demonstrated to be useful for addressing (i) data heterogeneityCaldarola et al. (2021); Ghosh et al. (2020), (ii) data privacy Thapa et al. (2022); Li et al. (2022), (iii) communication andcomputation efficiency Khan et al. (2021); Liu et al. (2020a); Wang et al. (2019), and (iv) personalization Mansour et al. (2020).In MFCL, adopting two servers ensures data privacy, which will be later discussed.",
  "and constructs contrastive client input mini-batches Bm =xm1,1, . . . , xmK,1, xm1,2, . . . , xmK,2. (b) Training": "phase 1: Client modules/decoders are trained federally across clients through the gServer. (c) Trainingphase 2: Clients generate representations zmi,j = f[T ]e(xmi,j), j = 1, 2 from the trained client module and sendthem to rServer. The rServer constructs Bs from the received representations and updates its parameters by minimizing contrastive loss.",
  "Proposed Modular Federated Contrastive Learning": "In the literature, there are a few works on contrastive learning-based FL, and in those works, the contrastiveloss is optimized at each client device. However, considering the memory and computing resource limitationsof the clients, it might be difficult or, at times, even impossible to perform contrastive learning at theclients Li et al. (2014), because the contrastive loss typically benefits from the larger model, larger batchsize, and/or longer training time Chen et al. (2020); Zhuang et al. (2022). Besides, directly optimizing thecontrastive loss across clients as in the traditional FL (i.e., averaging all parameters of clients local modelstrained with contrastive loss) may filter out the subtle yet useful information of the difference between theweights corresponding to the augmented versions of data, which is important in determining the similaritiesof examples. To address these issues, in our proposed MFCL, the contrastive loss is optimized at the rServer.Specifically, instead of federally training the entire deep network across clients on a contrastive loss, only apart of the model is trained by a contrastive loss at the rServer on data representations received from theclients unlabeled data extracted by the client module. The client module at the gServer and the rServermodule at the rServer are trained separately without any forward/backward passes between them.2",
  "Training Procedure for the Proposed MFCL": "Training Phase 1 for Client Modules: During the first stage of training, the client modules are federallytrained over M clients with coordination of the gServer. The client module, denoted by f[mt ]e(), consistsof the first several layers up to a cut-off layer of the entire model. Here, [mt ]e denotes the parameters ofthe client module at client m at FL round t. For unsupervised (federated) training of the client module, we",
  "minmtExDmLfedt1 (A (x) , g (f (A (x)))),(1)": "where Lfedt1 (, ) is the loss function to optimize mtand A () denotes data augmentation as follows. Fortraining the client modules/decoders, client m first creates two augmented versions xmi,j, j = 1, 2 of xmi Dm,where xmidenotes the i-th data sample of the local dataset Dm of client m. Combining the augmentedversions of K data samples, client m constructs the contrastive client input mini-batches of size 2K denotedby Bm =xm1,1, . . . , xmK,1, xm1,2, . . . , xmK,2. Training Phase 2 for the rServer Module: The second stage begins when federated training of client modulesand their decoders is finished, say at FL round T.Then, the decoder is discarded and, supplying twoaugmented versions xmi,j, j = 1, 2, of each data sample xmi Dm to the trained client module, client mproduces representation vectors, zmi,j = f[T ]e(xmi,j), j = 1, 2, called the contrastive client representation mini-batches, and transmits them to the rServer. Combining the representations received from at least Q Mclients, the rServer constructs the contrastive rServer mini-batches, Bs, such that each mini-batch Bs is largeenough for contrastive learning. In general, the rServer representations are much more CB, than the data ateach client (see in the Appendix). At the rServer, the rServer module, Ms, parameterized by s,is trained by minimizing the contrastive loss on contrastive rServer mini-batches, Bs (see Appendix A.2).Figures 2(b) and 2(c) show the two training phases of MFCL, respectively. Note that MFCL is designed to result in a single global model. To that end, when the training is done, theclient module and the rServer module form a single unified global model. To deploy this unified model, the(finally trained) rServer module should be broadcast to the clients. Algorithm 1 outlines the detailed processof the MFCL framework.",
  "Normalization for the Client Module/Decoder": "Without any normalization, CIB data in the mini-batch can increase the variance of the mini-batch gradients.Therefore, the output distribution of layers may diverge, which leads to lower accuracy Wu & He (2018). Inthe following, we first briefly discuss BN and GN. We then propose our normalization method, TN. 3For simplicity, the model parameters of the whole autoencoder (i.e., the client module and its decoder) are denoted by mt .[mt ]e and [mt ]d represent the model parameters of the client module and decoder parts of mt , respectively.4We used FedAvg McMahan et al. (2016) to aggregate the clients module parameters at the gServer. Other hyperparametersand augmentation techniques are detailed in Tables 12 and 13 in the Appendix.",
  "Batch Normalization (BN)": "The strong point of BN is that each channel is normalized over the entire mini-batch Ioffe & Szegedy (2015).BN works well in contrastive learning when each contrastive mini-batch is large and CB (this is the case forthe contrastive mini-batch at the rServer). However, when the mini-batch size is smaller and/or the data ofeach mini-batch is CIB, the batch statistics estimated by BN are biased and inaccurate. Therefore, using BNfor training the client modules/decoders on the heterogeneous and CIB data in FL leads to biased models,which are unable to accurately learn the features of those classes having only a small number of samples.",
  "Group Normalization (GN)": "The dependence of the batch-normalized outputs of each layer on the entire mini-batch makes BN powerful;however, it is also the source of BN drawbacks. GN, proposed for distributed training, normalizes a groupof channels of a sample together without utilizing the entire mini-batch Wu & He (2018). Specifically, GNrandomly chooses a group of C channels, which may have different scales, and normalizes them together.GN is applied to FL to address the issue of heterogeneous data across clients and works better than BNZhang et al. (2021); Yu et al. (2021; 2020b). The reason is that in FL, the model parameters (e.g., thefilters parameters in convolutional neural networks) are averaged over multiple clients. Averaging makes thescales of filters smoother. Applying smoother filters on channels leads to smoother features and normalizinga sample over a group of its channels, as in GN, becomes meaningful. One limitation of GN is that it relieson limited information to normalize each sample over a group of channels, compromising the precision inestimating the statistics of each sample to make the statistics independent of the mini-batch size.",
  "Twin Normalization (TN)": "We propose TN as a solution to address the lack of sufficient information in GN for accurately estimating thestatistics of a sample across a group of its features. Since in contrastive learning, there are two augmentedviews of each sample, it is possible to benefit from the rich and diverse information of those two examples andnormalize them together. TN normalizes positive examples together, which increases their similarity andfurther differentiates them from negative ones. For mathematical formulation, we let wc,li,j , i = 1, . . . , K; j =1, 2; c = 1, . . . , Cl denote the attributes of channel c at layer l for the j-th augmented version, xmi,j, j = 1, 2,of the i-th input data sample, xmi , in the clients mini-batch. We first determine the mean l,liand variance(l,li)2 of channels corresponding to augmented versions of each input in the l-th layer as follows:",
  "where wc,li,j is normalized wc,li,j for c l. Also, m,ltand m,ltdenote trainable scaling factor and shift,respectively, for layer l of the clients module and decoder at FL round t": "TN ensures that the attributes of examples are normalized independently of other examples in the clientscontrastive mini-batch. The pros of adopting TN are as follows: (1) Unlike BN, TN is not sensitive tomini-batch size, making it more suitable for FL where mini-batches can be small and vary across clients.(2) TNs ability to normalize augmented views together enhances the similarity between positive examplesand separates them from negative examples, which is crucial in contrastive learning. This improves thegeneralization of models trained in FL with heterogeneous CIB data. Although TN works well with heterogeneous CIB data, BN still works better than TN when the data ishomogeneous and balanced. Therefore, for training the rServer module at the rServer, we can still benefit",
  "The comparison between BN, GN, and TN": "With some inspiration from BN and GN, TN is uniquely adapted for federated contrastive learning. Specif-ically, BN relies on the whole mini-batch, and the size of the mini-batch plays an important role in thesuccess of BN. TN is designed to avoid some of the key drawbacks of BN in the context of FL, especially inheterogeneous and CIB scenarios. TN, unlike BN, is a sample-based normalization which normalizes two ex-amples of one sample and doesnt rely on mini-batches size, which makes it more efficient in class-imbalancedand heterogeneous data settings. GN normalizes across a group of channels, making it robust in scenarioswith small mini-batches. Compared to GN, TN has more diverse information per group of channels as TNnormalizes the group of channels of two augmented views of the same sample together, making the meanof those groups more accurate. TN effectively captures both inter-example and intra-example relationships,improving model performance. In neither BN nor GN, two examples of one sample are normalized togetheras in TN. shows the differences among BN, GN, and TN. In summary, while BN struggles with heterogeneous data due to its reliance on batch-wide statistics, andGN, though more robust, lacks inter-example normalization, TN combines the strengths of both. TN retainsGNs independence from mini-batch size while having inter-example (not inter-sample) relationships, in acontrolled manner by focusing on augmented pairs. This makes TN particularly well-suited for federatedcontrastive learning in heterogeneous environments, where it improves model performance without sufferingfrom the drawbacks of BN and GN.5",
  "Methodological Comparison": "The proposed MFCL might appear to be similar to Federated Split Learning (FSL) Thapa et al. (2022) inthat we also split the network at a cut-off layer into the client module and the rServer module. However,MFCL and FSL differ significantly in major aspects. First, in FSL, the smashed data (i.e., the output ofthe cut-off layer) must be communicated between the clients and server as many times as the number of FLrounds. In contrast, in MFCL, the data representations are transmitted from the clients to the rServer onlya single time, after the clients module and decoder are trained. Therefore, MFCL imposes a significantlylower data traffic burden compared to FSL. Second, unlike FSL which backpropagates the gradients of thesmashed data from the server to the clients during the training, MFCL trains two separated modules without 5Note that while TN is particularly beneficial in federated contrastive learning, it is not necessarily limited to this context.TN could be extended to any learning tasks that benefit from augmented data samples and the data is CIB.",
  "Data Privacy Comparison": "In MFCL, only the representations of the harshly augmented data are transmitted (disclosed) only to therServer. This means that the gServer has access only to the parameters of the client modules/decoders,not the representations produced by the client modules, whereas the rServer has access only to the datarepresentations, not the parameters of the clients modules/decoders. In FSL, the server has access onlyto a part of the model and this setup provides a certain level of data privacy Wang et al. (2023b); Turinaet al. (2020); Jeon & Kim (2020); Thapa et al. (2021); Li et al. (2022). Comparing MFCL with FSL, MFCLoffers more enhanced data privacy than FSL due to its distinct training phases on two distinct servers. Ontop of that, in MFCL, the data representations are sent to the rServer only a single time. In contrast, FSLentails the transmission of smashed data in every FL round throughout the training process, which makes itmore vulnerable to data leakage. Moreover, in MFCL, the client module/decoder and gServer do not haveany gradient transactions with the rServer, which has access to the data representations, ensuring that therServer does not have the gradients of representations. By contrast, in FSL, the server has smashed data aswell as the corresponding gradients, making the whole system even more vulnerable. MFCL bolsters dataprivacy by reducing the risks associated with data leakage.",
  "Setup and Performance Metric": "We perform our experiments on CIFAR-10, CIFAR-100 Krizhevsky (2009), and Tiny-ImageNet Le & Yang(2015) with ResNet-18 and ResNet-50. For ResNet-18, the first two layers, and for ResNet-50, the first 4layers are chosen as the client module (i.e., the encoder part) and we design the decoder part as a mirrorstructure of the encoder. presents the results of choosing the cut-off layer at different locations forthe two networks. In both cases, the client module and its decoder are composed of convolutional layers andTN layers (See Tables 9 and 10 in the Appendix for more details). The rServer module is composed of theremaining layers of the network, which is trained on the received representations with contrastive loss. We implemented MFCL with TensorFlow 2.14 following the standard structure of FL McMahan et al. (2017)and the contrastive learning Chen et al. (2020). To construct heterogeneous and CIB datasets over the clients,we used Dirichlet distribution, with concentration parameter ( > 0) Yurochkin et al. (2019); Wang et al.(2020b). If is set to a smaller value, the data becomes more heterogeneous and CIB, whereas when ,the data becomes homogeneous and CB (See in the Appendix). We used a single NVIDIA GeForceRTX 3090 GPU to simulate the clients and rServer modules. We evaluated the classification accuracy ofMFCL based on the linear evaluation method Chen et al. (2020). Specifically, we used non-linear projectionlayers as described in in the Appendix on top of the rServer module, and one linear Dense layer ontop of the projection layers for linear evaluation. In training phase 2, the rServer module, projection layers,and linear (classifier) layer can be jointly trained as long as the gradients are not backpropagated from thelinear (classifier) layers to the rServer module and the projection layers. Alternatively, the rServer moduleand the projection layers could be trained first, and then, the Dense layer on top of the frozen (trained)rServer module and projection layers can be fine-tuned using labelled data.",
  "Method0.010.12000.010.12000.010.1200": "FedAvg62.30.62 79.20.53 92.10.14 36.70.45 48.20.29 60.50.11 29.40.23 36.60.17 49.50.19FedProx70.80.34 83.00.19 93.20.10 36.90.18 48.70.21 67.00.17 30.10.19 36.90.23 50.200.10MOON65.40.25 77.70.31 94.10.18 37.20.28 49.10.25 67.60.16 30.70.26 36.70.14 50.60.18FSL54.10.20 66.70.31 77.00.16 30.50.15 29.40.12 37.60.08 19.80.33 25.40.29 31.00.21 FedSimCLR65.60.29 72.10.11 81.90.07 32.40.25 35.00.24 42.30.14 21.70.31 24.30.28 33.80.25Orchestra79.60.08 82.10.12 81.70.06 37.40.13 40.30.09 40.90.07 26.20.07 28.80.08 29.70.10FedEMA78.80.16 81.90.09 84.10.06 37.20.09 40.30.07 37.60.04 27.00.16 29.10.13 38.90.15FedU74.70.38 77.60.30 83.20.18 32.10.21 36.70.19 39.80.13 24.60.18 26.30.28 30.10.12FeatARC83.30.34 84.60.19 86.70.20 42.50.19 54.60.19 67.30.06 31.60.31 32.90.24 37.50.26MFCL(BN)15.30.41 32.50.33 86.40.12 14.10.37 28.30.38 51.30.31 12.90.28 22.10.26 29.50.19MFCL(GN)79.30.14 79.80.12 82.60.11 41.50.21 43.20.16 47.10.20 29.60.13 30.40.12 38.30.12MFCL(TN)84.10.22 84.60.21 85.30.19 44.40.17 45.50.21 47.80.13 33.10.30 33.90.14 42.20.18",
  "Baselines": "We consider the state-of-the-art works in self-supervised FL (SSFL) and supervised FL (SFL) that are pro-posed to address data heterogeneity. In the field of SSFL, we compare MFCL with FedU Zhuang et al. (2021)and FedEMA Zhuang et al. (2022).6 SimCLR Chen et al. (2020) with FedAvg as another baseline, denoted asFedSimCLR. We also consider FeatArc Wang et al. (2023a), a recent approach in decentralized learning thataims to address the challenge of learning from decentralized, heterogeneous data using contrastive learning.FedAvg McMahan et al. (2016), FedProx Li et al. (2020), and MOON Li et al. (2021) are fully supervisedFL. FSL Han et al. (2021) is our baseline for federated split learning. There are two main baselines for implementing SFL in resource-limited clients: HeteroFL Diao et al. (2020)and Split-MIX Hong et al. (2022). Those baselines and the majority of current resource-compatible schemesare in the SFL field, where the number of FL rounds and the computation burden of local updates arecompletely different from SSFL schemes.For a fair comparison, we report the communication burden,computation burden, and memory footprint from those papers.",
  "Accuracy Performance": "We train CIFAR-10 over M = 10 clients on ResNet-18, CIFAR-100 over M = 100 clients on ResNet-18, andTiny-ImageNet over M = 100 clients on ResNet-50. We consider three heterogeneous and CIB scenarios( = 0.1 and 0.01), and a homogeneous and CB scenario ( = 200). Each scenario is run 3 times, and themean and standard deviation of the top-1 accuracy on a uniform test set is reported.7 Improved accuracy in heterogeneous and CIB scenarios. From the numerical results in , onecan see that, in severe heterogeneous and CIB datasets, MFCL(TN) significantly outperforms other SFL andSSFL methods. Furthermore, MFCL(TN) outperforms MFCL(BN) and MFCL(GN) in those scenarios. Thefirst reason is that MFCL benefits from self-supervised learning, which is mathematically and experimentallyproved to be more robust to CIB data Liu et al. (2021). In supervised learning, the expected values of thesquares of the lengths of the gradient vectors corresponding to different classes are approximately related tothe square of the number of samples in those classes Anand et al. (1993); Yu et al. (2020c). This means thatin supervised learning when the dataset is CIB, the gradients of the classes with more samples are larger 6The code for these baselines is not publicly available. We implemented them with the parameters reported in their papers.7Unless otherwise specified, we assume that all clients participate in all rounds (i.e., Q = M).",
  "p, #, Rep. stand for phase, Number of, and Representations, respectively": "than those of the classes with fewer samples, which leads to the gradients being biased toward the class withmore samples. In contrast to this, self-supervised learning does not rely on the class labels to calculate theloss, and thus, the gradients are less biased, and the model can better learn the classes with fewer samples.Therefore, with self-supervised learning, we can mitigate the bias of labels. Second, MFCL benefits from TN. In severe heterogeneous and CIB scenarios, in addition to the bias oflabels, we face what we call the bias of features. The bias of labels can be mitigated by new designs of thenetwork (e.g., MFCL instead of E2E learning). However, the bias of features is the natural result of havingclients with heterogeneous CIB data. One effective way to moderate the bias of features is to use properlydesigned normalization techniques. TN mitigates the bias of features when data is heterogeneous and CIB.8 Accuracy in homogeneous and CB scenarios. MFCL works best when datasets are heterogeneous andCIB, which is a practical scenario. However, in the homogeneous and CB scenarios, SFL methods outperformSSFL and MFCL. The reasons are as follows. First, when clients have homogeneous CB data, the labelsare homogeneous, and the gradients are not biased. In those scenarios, SFL works better than SSFL Wu& He (2018). Second, when the mini-batch is CB and large enough, normalizing the data over the entiremini-batch (i.e., BN) works better than the sample-based normalization (e.g., GN or TN). The reason is thatthe batch statistics estimated from a large CB mini-batch are more accurate, yielding smaller variations ofstatistics from one mini-batch to another, and thus, it leads to better generalization performance.",
  "Client Side Resource Saving by MFCL": "Communication burden. In FL, the complexity of communication channels refers to the data traffic andthe frequency of communication. In the following, we compare the communication complexities of MFCLand the traditional E2E SSFL methods in both aspects. - Data traffic: Traditional E2E SSFL methods require sending a huge number of model parameters to theserver in each FL round. In MFCL, clients only train a shallow module federally to learn the low-levelfeatures, which requires only a few FL rounds. Therefore, the data traffic of training phase 1 in MFCL isfar lighter than that of the E2E SSFL methods. Meanwhile, MFCL has additional data traffic in trainingphase 2, which does not exist in the traditional E2E SSFL settings. Based on the simulations, considering arealistic scenario (e.g., using single precision format (32 bits) and adopting an error correction code with coderate 1 3, which is used in all real-world communication systems) to transmit the model parameters in trainingphase 1 and representation vectors in training phase 2, we found that the overall data traffic of MFCL islower than the corresponding E2E methods. compares the communication burden of MFCL withthat of FedSimCLR. In training phase 1, the communication burden, 1, of MFCL is computed by",
  "(bits) = (|zm| ) 32 R,(5)": "where, |zm| is the size of each representation vector from client m and is the number of representationvectors. The size of one representation vector depends on the kernel size and stride of the last layer of theclient module, and the number of data samples in the clients. For example, in ResNet-18, the size of onerepresentation vector is |zm| = 32 32 64 |Dm|, where the stride is 1 and the number of filters is 64. ForCIFAR-10 over M = 10 clients, |Dm| = 5000; and for Tiny-ImageNet over M = 100 clients, |Dm| = 1000.Each representation vector contains 2 different augmented versions of each data sample. In MFCL, eachclient sends = 8 representation vectors to the rServer (i.e., 16 different augmented samples for each datasample). The total communication burden for MFCL is = 1 + 2 which is reported in .Clearly,with more data in clients, the data traffic of MFCL would increase linearly. However, more data might needlarger models in E2E SSFL, which leads to higher data traffic for FedSimCLR.9 - Frequency of communications: Each communication event incurs overhead costs, including channel setup,data encoding/decoding, handoff management10 Kukliski et al. (2014), and session management Hasanet al. (2019). High-frequent communication increases latency and network load, potentially leading to delaysand congestion, especially in bandwidth-limited or high-latency networks like mobile networks. Frequentcommunication also drains the battery life of mobile devices and raises operational costs for servers andinfrastructure Mao et al. (2017). As reported in , each client utilizing MFCL uses the communicationchannels 16 times (i.e., 15 FL rounds plus one time to send the representation vectors), which is approximately6 times fewer than FedEMA and 50 times fewer than FedSimCLR. This demonstrates that MFCL significantlyoutperforms other E2E SSFL methods in terms of communication burden. Memory and computational efficiency. On the side of clients, the memory footprint to store modelparameters, feature maps, and gradients, is an important issue. The memory footprint of each client iscomputed by",
  "K 4,(6)": "where |m|, |gm|, 2K, and |wlm| are the number of model parameters, the number of gradients, the size ofBm, and the size of feature maps of different layers of the network, respectively. The feature map size ineach layer depends on the kernel size, the stride and the number of filters in the layer. As shown in ,the memory footprint of MFCL on the side of clients is much lower than the FedSimCLR. MFCL also haslower complexity in terms of FLOPs, which makes it suitable for resource-limited clients. MFCL and other resource-compatible baselines. In , the accuracy, communication burden ,computation burden , and memory footprint are reported for MFCL(TN) and other schemes designedfor resource-limited clients in SFL. FedPAC Xu et al. (2023) is another promising baseline to reduce thecommunication burden and address the data heterogeneity. Instead of updating all clients classifiers si-multaneously, FedPAC shares only classifiers that are relevant to a given client, minimizing communicationoverhead. However, FedPAC requires more local updates, which increases the computational burden andmemory footprint on the client side, making it less suitable for resource-limited clients.",
  "One can see that MFCL(TN) has better accuracy in severe heterogeneous data scenarios while reducing theclient memory footprint, with an increase in communication burden": "9Another important aspect for MFCL is that not all clients who participated in training phase 1 necessarily participate intraining phase 2. in the Appendix shows that MFCL is robustly working even in that situation.10Mobile devices may move between different wireless network coverages, called the cells, requiring handoffs that involvesignaling and maintaining session continuity. Frequent communication increases the number of handoffs.",
  "(e)": ": Different scenarios for the (raw) data at the clients and the contrastive rServer representationmini-batches (simply, representations) at the rServer. (a) clients have homogeneous and CB data; and therepresentations are CB at the rServer, (b) clients have homogeneous and CIB data; and the representationsare CIB at the rServer, (c) clients have heterogeneous and CIB data; and the representations are CB at therServer, (d) clients have heterogeneous and CIB data; and the representations are CIB at the rServer, and(e) clients have heterogeneous and CB data; and the representations are CB at the rServer. : The architecture of the client module and its decoder on ResNet-18. For the convolutional layer(Conv2D) and transposed convolution layer (Conv2DTranspose), we present the list of the parameters inthe sequence of input and output dimensions, kernel size, stride, and padding. For the max pooling layer(MaxPool2D), we show the list of the kernel and stride.",
  "More Simulation Results for TN": "Addressing data heterogeneity with TN. shows that TN has better performance over BN inlearning classes with a smaller number of samples. Let us consider 2 clients each with 5000 samples. Client1 has 4950 airplanes and 50 cars. Client 2 has 4950 cars and 50 airplanes. MFCL(BN) and MFCL(TN)are used to train ResNet-18 on those clients. When training is done, we use the synthesized samples tosee the gradients saliency map. a depicts a synthesized sample with half showing a car and halfshowing an airplane. Figures 4b and 4c show the gradients saliency map for MFCL(BN) on clients 1 and 2,respectively. For client 1, cars are rare and the network does not learn good features for them, while client2 struggles with rare airplanes. Figures 4d and 4e show the gradients saliency map for MFCL(TN) on thesame clients. Clearly, MFCL(TN) has learned better features for both classes. TN in other SSFL methods. To see if TN is still effective in other contrastive FL settings, we appliedTN (instead of BN) to FedSimCLR after the first two Conv2D layers and results are presented in . One can see that (i) TN can improve the performance of FedSimCLR and (ii) MFCL(TN) outperformsFedSimCLR (with BN, GN, or TN) when clients have heterogeneous and CIB data. One of the advantages of MFCL is that the batch size in the client module does not need to be large. Theeffects of other hyperparameters related to TN, such as batch size and number of channels per group, , aredetailed in Tables 16 and 17 in the Appendix.",
  "Extra privacy-preserving mechanisms in training phase 1 and 2": "Training phase 1. Training phase 1 of MFCL follows the traditional FL. Any privacy-preserving algo-rithms such as Differential Privacy Abadi et al. (2016), Secure Aggregation (SA) Bonawitz et al. (2017), orHomomorphic Encryption Zhang et al. (2020a) can be applied to the model parameters. We adopted SA in to avoid extra computation burden in clients. Training phase 2. Since rServer does not have access to the client modules model parameters, recon-structing data from harshly augmented samples using a random decoder is challenging. We also provideextra privacy by applying DataMix Liu et al. (2020b) to MFCL, and the results are listed in .11",
  "Possible Attack Scenarios": "We analyzed different scenarios, considering whether the gServer implements SA, whether the rServer appliesDM, and whether the two servers are colluding. SA is adopted to preserve the privacy of the clients data atthe gServer. Note that if two servers collude, SA cannot provide extra privacy at the rServer.12 The goal isto reconstruct the original sample x from the representations of augmented data with or without DataMix.The simulation results are shown in Table ??. Case 1: C1D0 (Servers collude and there is no DataMix) In this scenario, the gServer transmits theaveraged gradients to the rServer. The rServer, having access to both the gradients and the representationsof the augmented data, is capable of reconstructing the augmented data samples. However, a more complexchallenge lies in reconstructing the original data samples from the augmented data, without direct access tothe original data or labels necessary to train a network for such reconstruction.",
  "Case 2: C1D1 (Servers collude and DataMix is applied at clients side) This case is similar to case1, except that the clients use DataMix. One can see the reconstruction is even more challenging": "Case 3: C0D0 (Servers do not collude and there is no DataMix) In this case, the rServer does nothave the gradients. Therefore the decoding process to reconstruct the augmented data from the representa-tions of the augmented data should start from pure noisy gradients. Again, the more challenging step is toreconstruct the main data from the augmented data, without having access to data samples or the labels.",
  "From one can see MFCL(TN) works better than FedEMA Zhuang et al. (2022) which is the state-of-the-art baseline for SSFL. In what follows, we compare MFCL with FedEMA from various aspects:": "11It is also possible to add more layers to client modules, which creates a trade-off between privacy and accuracy ().12We implemented the basic form of SA, where gradients are masked on the client side so that the masks cancel out duringaggregation. This way, the gServer does not have access to the individual client gradients but can still access the aggregatedgradients. Alternative methods, such as Secure Multi-Party Computation, or combinations of SA with Homomorphic Encryptionor Differential Privacy, can ensure that even the aggregated gradients remain inaccessible to the gServer. Additionally, employinga Trusted Execution Environment (TEE) at the gServer could further enhance privacy without extra computation burden onthe side of clients.",
  "Reconstructed x": "CB dataset in each client. In FedEMA, the clients dataset is assumed to be CB (e.g., each client has2 classes of CIFAR-10). In , we utilized the Dirichlet distribution. However, for fair comparison in, we implemented MFCL(TN) using the data distribution from FedEMA with 5 clients (i.e., 2 classesper client for CIFAR-10 and 20 classes per client for CIFAR-100). One can see that with 5 clients and CBdatasets, the accuracy performance of FedEMA is close to that of MFCL(TN). Number of clients. As reported in of FedEMA Zhuang et al. (2022), the accuracy performance ofFedEMA declines with increasing the number of clients, rendering it unsuitable for cross-device FL. MFCLworks well with 100 clients () or even more, which is more realistic in cross-device scenarios. Number of samples per client. As reported in of Zhuang et al. (2022), the top 1 accuracy ofFedEMA Zhuang et al. (2022) improves with increasing the number of samples per client. The reason isthat in FedEMA a deep network, such as ResNet-50, is trained federally, and having more samples in eachclient reduces the likelihood of overfitting and enhances the training performance. In contrast, in MFCL ashallow autoencoder is trained federally, making it feasible to work effectively with a smaller number of datasamples, which is more realistic in cross-device FL. Communication burden and memory footprint. presents a comparison of the communicationburden, computation burden, and memory footprint between FedEMA and MFCL(TN). The communicationburden of FedEMA is marginally lower than that of MFCL(TN) and FedSimCLR. However, FedEMA incurs asubstantially higher computation burden and memory footprint. The reason is that FedEMA needs to trainand store two networks (i.e., online and target networks) at each client, leading to higher computationaldemands and memory usage. FedEMA works perfectly well in cross-silo applications where (1) the number of clients is not large and(2) each client has enough data samples and resources. MFCL(TN) is designed for cross-device applica-tions where (1) the number of clients can be large, and (2) clients do not have huge data, communication,computation, and memory resources.",
  "Conclusion": "To summarize. We have developed MFCL, as a novel approach to FL, transitioning from an E2E learningto a modular framework to leverage contrastive learning in resource-constrained clients. We also proposedTwin Normalization which is effective in addressing the challenges of heterogeneous CIB data at clients. Oursimulations demonstrate that MFCL(TN) achieves superior and more stable performance in highly hetero-geneous and CIB data scenarios. Furthermore, MFCL exhibits reduced communication burden, memoryfootprint, and computational complexity on the client side. It is noteworthy that MFCL works effectivelywith group normalization, indicating that it performs well even without twin normalization. Limitations. MFCL has great potential as a resource-effective approach for cross-device FL and can handlesevere heterogeneous and CIB datasets. However, there exist certain limitations for MFCL. When the numberof data samples per client is huge (e.g., in cross-silo FL), the client module should include additional layersto maintain performance. This requirement can compromise the resource efficiency advantages of MFCL.",
  "Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon. Greedy layerwise learning can scale to ima-genet. In International conference on machine learning, pp. 583593. PMLR, 2019": "Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel,Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machinelearning. In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,pp. 11751191, 2017. Debora Caldarola, Massimiliano Mancini, Fabio Galasso, Marco Ciccone, Emanuele Rodol, and BarbaraCaputo. Cluster-driven graph federated learning over multiple domains. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 27492758, 2021. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In Proceedings of International Conference on Machine Learning, pp.15971607, 2020.",
  "Aymeric Dieuleveut, Gersende Fort, Eric Moulines, and Genevive Robin. Federated expectation maximiza-tion with heterogeneity mitigation and variance reduction. arXiv preprint arXiv:2111.02083, 2021": "Moming Duan, Duo Liu, Xianzhang Chen, Renping Liu, Yujuan Tan, and Liang Liang.Self-balancingfederated learning with global imbalanced data in mobile systems. IEEE Transactions on Parallel andDistributed Systems, 32(1):5971, 2020. Alp Emre Durmus, Zhao Yue, Matas Ramon, Mattina Matthew, Whatmough Paul, and SaligramaVenkatesh. Federated learning based on dynamic regularization. In Proceedings of International Con-ference on Learning Representations, 2021.",
  "Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran. An efficient framework for clusteredfederated learning. Advances in Neural Information Processing Systems, 33:1958619597, 2020": "Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020. Dong-Jun Han, Hasnain Irshad Bhatti, Jungmoon Lee, and Jaekyun Moon. Accelerating federated learningwith split learning on locally generated losses. In ICML 2021 workshop on federated learning for userprivacy and data confidentiality. ICML Board, 2021.",
  "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distributionfor federated visual classification. arXiv preprint arXiv:1909.06335, 2019": "Wenke Huang, Mang Ye, and Bo Du. Learn from others and be yourself in heterogeneous federated learning.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1014310153, 2022. Bernd Illing, Jean Ventura, Guillaume Bellec, and Wulfram Gerstner. Local plasticity rules can learn deeprepresentations using self-supervised contrastive predictions. Advances in Neural Information ProcessingSystems, 34:3036530379, 2021. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In Proceedings of International Conference on Machine Learning, pp. 448456,2015.",
  "Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015": "Jingtao Li, Adnan Siraj Rakin, Xing Chen, Zhezhi He, Deliang Fan, and Chaitali Chakrabarti. Ressfl: A re-sistance transfer framework for defending model inversion attack in split federated learning. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1019410202, 2022. Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for stochasticoptimization. In Proceedings of the International Conference on Knowledge Discovery and Data Mining,pp. 661670, 2014.",
  "Hong Liu, Jeff Z. HaoChen, Adrien Gaidon, and Tengyu Ma. Self-supervised learning is more robust todataset imbalance. In Proceedings of NeurIPS Workshop on Distribution Shifts, 2021": "Lumin Liu, Jun Zhang, S.H. Song, and Khaled B. Letaief. Client-edge-cloud hierarchical federated learning.In ICC 2020 - 2020 IEEE International Conference on Communications (ICC), pp. 16, 2020a.doi:10.1109/ICC40277.2020.9148862. Zhijian Liu, Zhanghao Wu, Chuang Gan, Ligeng Zhu, and Song Han. Datamix: Efficient privacy-preservingedge-cloud inference. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part XI 16, pp. 578595. Springer, 2020b.",
  "Sindy Lwe, Peter OConnor, and Bastiaan Veeling. Putting an end to end-to-end: Gradient-isolated learningof representations. Advances in neural information processing systems, 32, 2019": "Nan Lu, Zhao Wang, Xiaoxiao Li, Gang Niu, Qi Dou, and Masashi Sugiyama. Federated learning fromonly unlabeled data with class-conditional-sharing clients. In Proceedings of International Conference onLearning Representations, 2022. Ekdeep Singh Lubana, Chi Ian Tang, Fahim Kawsar, Robert P Dick, and Akhil Mathur. Orchestra: Unsu-pervised federated learning via globally consistent clustering. arXiv preprint arXiv:2205.11506, 2022. Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity: Classifiercalibration for federated learning with non-iid data. Advances in Neural Information Processing Systems,2021.",
  "Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-task learning.Advances in neural information processing systems, 30, 2017": "Chandra Thapa, Mahawaga Arachchige Pathum Chamikara, and Seyit A Camtepe. Advancements of feder-ated learning towards privacy preservation: from federated learning to split learning. Federated LearningSystems: Towards Next-Generation AI, pp. 79109, 2021. Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun. Splitfed: Whenfederated learning meets split learning. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 36, pp. 84858493, 2022. Valeria Turina, Zongshun Zhang, Flavio Esposito, and Ibrahim Matta.Combining split and federatedarchitectures for efficiency and privacy in deep learning. In Proceedings of the 16th International Conferenceon emerging Networking EXperiments and Technologies, pp. 562563, 2020.",
  "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredientfor fast stylization. arXiv preprint arXiv:1607.08022, 2016": "Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federatedlearning with matched averaging. In Proceedings of International Conference on Learning Representations,2020a. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistencyproblem in heterogeneous federated optimization. Advances in neural information processing systems, 33:76117623, 2020b. Lirui Wang, Kaiqing Zhang, Yunzhu Li, Yonglong Tian, and Russ Tedrake. Does learning from decentralizednon-IID unlabeled data benefit from self supervision?In The Eleventh International Conference onLearning Representations, 2023a. URL",
  "Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang, Chenchen Liu, Zhi Tian, and Xiang Chen.Heterogeneous federated learning. arXiv preprint arXiv:2008.06767, 2020b": "Fuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang, Chenchen Liu, Zhi Tian, and Xiang Chen.Fed2: Feature-aligned federated learning. In Proceedings of the Conference on Knowledge Discovery andData Mining, pp. 20662074, 2021. Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradientsurgery for multi-task learning. Advances in Neural Information Processing Systems, 33:58245836, 2020c. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and YasamanKhazaeni. Bayesian nonparametric federated learning of neural networks. In International conference onmachine learning, pp. 72527261. PMLR, 2019. Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu. {BatchCrypt}: Efficienthomomorphic encryption for {Cross-Silo} federated learning. In Proceedings of USENIX Annual TechnicalConference (USENIX ATC 20), pp. 493506, 2020a.",
  "A.1Supervised FL with Heterogeneous and CIB Datasets": "There are plenty of works in SFL aiming to address the issue of data heterogeneity among the clients. Thefirst approach is to leverage a shared public or synthesized dataset on clients and/or the server to find asolution for the local models that under-represent the patterns at the clients Zhao et al. (2018); Hao et al.(2021). The second approach is based on regularization. FedProx Li et al. (2020) regularizes the Euclideandistance between the local and global models. MOON Li et al. (2021) uses contrastive loss to maximize theagreement of the representations learned by the local and global models. SCAFFOLD Karimireddy et al.(2020) reduces the bias in the gradients by introducing some control variates. FedDyn Acar et al. (2021)dynamically changes the local objectives at each FL round to ensure that the local optimum is consistent withthe global optimum. The third approach is based on reducing the bias of the aggregated parameters at theserver. The authors in Hsu et al. (2019), leveraging the momentum update on the server, tried to alleviatethe oscillations resulting from averaging the biased gradients. The authors in Wang et al. (2020a) proposedto match the local updates while aggregating to reduce the effect of heterogeneous data.In particular,to achieve a fair (if not perfect) performance of the model on the local datasets, researchers have recentlystudied personalized FL, through various approaches, e.g., by treating each client as a task in a multi-tasklearning framework Smith et al. (2017) or by dividing clients into different clusters based on their tasks andperforming cluster-based aggregation as in Sattler et al. (2020); Ghosh et al. (2020). In addition to the issue of data heterogeneity, the number of data samples for each class in a clients datasetis not necessarily the same. This means that each clients data might be often CIB rather than CB. Overthe past few years, some efforts have been devoted to addressing the issue of CIB data in the clients byregularization techniques as in Wang et al. (2021a), data level approaches such as data augmentation inDuan et al. (2020), or data distribution estimation in Yang et al. (2021). However, most of the existing FLmethods, including all the algorithms mentioned above, assume that clients have fully labelled data. shows different scenarios of data distribution of clients in FL.",
  "A.2Contrastive Learning and Self-Supervised FL": "Researchers have studied contrastive and non-contrastive approaches for SSFL Zhuang et al. (2022); Zhanget al. (2020b); Miao & Koyuncu (2022); Shi et al. (2021); Yu et al. (2020a). The contrastive loss, used inOord et al. (2018), is defined between two augmented views (i, j) of the same data sample in a mini-batchof the size of 2K, as follows:",
  ",(7)": "where hi and hj are the hidden representations of two positive examples13, 1[k=i] is an indicator function thatis equal to 1 if k = i, sim (hi, hj) is the cosine similarity between two vectors of the hidden representations, is a temperature scalar, and B is a randomly sampled mini-batch consisting of augmented pairs of images.For representation learning without a contrastive loss, the authors in Lu et al. (2022) proposed to useprior information about the clients dataset, instead of labels, to apply supervised learning methods tounlabeled data. Orchestra Lubana et al. (2022) is proposed as an SSFL scheme to partition the clients datainto distinguishable clusters. Recently, the authors in Zhuang et al. (2022) proposed a divergence-awareaggregation for FL, based on different self-supervised learning methods. Unfortunately, their scheme doesnot perform well when the number of clients is large (e.g., more than 10). Also, they did not consider thecase in which the clients datasets are CIB.",
  "LayerDetails": "1Conv2D(3, 64, 3, 1, 1)2TN, ReLU3Conv2D(64, 64, 3, 1, 1)4TN, ReLU5Conv2D(64, 128, 3, 1, 1)6TN, ReLU7Conv2D(128, 128, 3, 1, 1)8TN, ReLU, MaxPool2D(2, 2)9Conv2DTranspose(128, 128, 3, 1, 1)10TN, ReLU11Conv2DTranspose(128, 64, 3, 1, 1)12TN, ReLU13Conv2DTranspose(64, 64, 3, 1, 1)14TN, ReLU15Conv2DTranspose(64, 3, 3, 1, 1)16TN, ReLU17Conv2D(3, 3, 3, 1, 1) Therefore, we need to define the concept of CB or CIB representations at the rServer. For clarification, weprovide , which consists of data distribution in the clients and the distribution of the representationsat the rServer. In a, the clients have homogeneous and CB data in each client; In b, theclients have homogeneous and CIB data in each client; In Figures 6c and 6d, the clients have heterogeneousand CIB data in each client; In e, the clients have heterogeneous and CB data in each client. To sumit up, if the dataset is CB in all clients, the rServer mini-batches are always CB, even if some clients are notsuccessful in sending their representations vectors (Figures 6a and 6e). Otherwise, the rServer mini-batchescan be CB or CIB: the rServer mini-batches are CIB in Figures 6b and 6d, whereas they are CB in c.",
  "B.2.1Data Distribution": "Following baselines Lubana et al. (2022); Luo et al. (2021); Makhija et al. (2022), we chose M{10, 100} and{0.01, 0.1, 0.5}. For CIFAR-10, =0.01 implies each client has two very unbalanced classes and =0.1implies each client has 4 or 5 unbalanced classes, and so on. The smaller , the more unbalanced the classes(a), which is most realistic for FL.",
  "B.2.2Modules structures and hyperparameters": "In our experiments, the clients module includes the initial layers of a deep network, extending up to aspecified cut-off layer.The decoder for the client module is designed such that each client module, inconjunction with its decoder forms a convolutional autoencoder. Considering ResNet-18, the client module(i.e., the first two layers of the network) and its decoder are trained federally, with binary cross-entropy loss,across M = 10 clients for CIFAR-10 and M = 100 for CIFAR-100. For Tiny-ImageNet, we include the firstlayers of ResNet-50 as the clients module and train a deeper autoencoder federally across M = 100 clients.The details of the client module and decoder are provided in Tables 9 and 10 for ResNet-18 and ResNet-50,respectively. We use non-linear projection layers () on top of the rServer module and also one linear Dense layer isused on top of the projection layers for linear evaluation. A list of other hyperparameters and augmentationstrategies are listed in Tables 12 and 13, respectively.",
  "B.2.3MFCL performance on other backbones": "We provided more simulation results on more data scenarios in . Also, to thoroughly assess the per-formance and generalizability of MFCL, we conducted additional experiments using VGG-16-BN as anotherbackbone architecture, which is the VGG 16-layer model with batch normalization Simonyan & Zisserman(2014). This allowed us to evaluate how MFCL performs across different network structures, ensuring thatits effectiveness is not limited to the ResNet family. Similar to ResNet-18 on CIFAR-10, we select the firsttwo layers as the encoder module and design the decoder as the mirror structure of the encoder part. Wetrain the encoder module and its decoder federally across the clients and the next layers are trained at theServer. The simulation results are presented in . Since the client modules of the ResNet-18 and VGG-16-BN backbones are identical, the communication,computation, and memory footprints of MFCL(TN) are the same for both backbones on the client side.However, VGG-16-BN has 138 million parameters, which is why the communication, computation, andmemory footprint of E2E FedSimCLR with a VGG-16-BN backbone is higher than those with a ResNet-18backbone.",
  "B.2.4Effect of the batch size on the side of clients": "As outlined in Algorithm 1, the MFCL framework has two distinct phases: the first phase involves federatedtraining of the client module and its decoder using the gServer, while the second phase focuses on trainingthe representation modules at the rServer through contrastive learning. One of the benefits of MFCL is thatthe batch size in the client module, K, does not need to be large. The performance accuracy of MFCL withvarious K is provided in .",
  "B.2.5Effect of the client dropout in training phase 2": "When training phase 1 is finished, the clients send their data representations to the rServer.In real-world scenarios, however, not all the clients who participated in training phase 1 can join training phase2. Specifically, in training phase 2, the rServer may not successfully receive the representation vectors fromsome clients who participated in training phase 1. This could be due to the representation vectors beingcorrupted or lost during transmissions, or some clients being unable to send their data representations. Inthis case, our assumption that the contrastive rServer mini-batches are (closely) CB might not be validanymore. We investigated how the performance of our proposed MFCL with BN, GN, and TN is affected byclient dropout, considering the scenarios where (%) of clients cannot transmit their representation vectorsto the rServer during training phase 2. In a, we trained MCFL with BN, GN, and TN on the CIFAR-10 dataset with = 200 when M = 10clients participated in training phase 1, assuming the representation vectors from (%) clients are not receivedby the rServer in training phase 2. We repeated the same experiments in b with = 0.01. FromFigures 8a and 8b, one can see that both TN and GN exhibit similar behaviour when some clients aredropped during training phase 2. For = 200, the gap between TN and BN decreases as more clients aredropped in training phase 2. Meanwhile, for = 0.01, our proposed TN continues to be the best choice evenas more clients are dropped in training phase 2.",
  "(b)": ": The classification accuracy of the proposed MFCL with BN, GN, and TN, when the client dropoutrate is % in training phase 2. The number of clients participating in training phase 1 is M = 10. (a) = 200is considered, which is the scenario of homogeneous and CB data. (b) = 0.1 considered, which is a severecase of data heterogeneous clients with CIB data.",
  "B.3.1Effect of the number of channels per group in TN": "We evaluate the effect of the number of channels per group, , in TN in . Note that in the extremecase of 1 channel per group, GN is equivalent to Instance Normalization Ulyanov et al. (2016) but TN stillnormalizes the two examples together based on the statistics of both channels (i.e., 1 channel per groupfor each example). One can compare the accuracy performance of MFCL(TN) and MFCL(GN) when thenumber of channels per group is the same in both methods (i.e., = 4 channels per group in MFCL(GN) vs = 2 channels per group for MFCL(TN), which means 4 channels in total). Even using as few as 2 channelsper group for each example (i.e., 4 channels in total), MFCL(TN) has substantially better accuracy thanMFCL(GN).",
  "B.4Visualization of learned clusters by MFCL": "In this subsection, the features of the learned clusters are visualized to provide a deeper understanding ofthe advantages of the proposed normalization method, TN. Each client is assumed to have 2 classes of theCIFAR-10 dataset, with M = 10. Our objective is to evaluate the quality of the learned clusters with MFCL,at the output of the projection layers when BN, GN, or TN is utilized in the client modules and decoders. a presents the t-SNE visualization of raw input data at a client, while Figures 9b-9d display thet-SNE visualizations of the clusters learned from the data representations of all clients at the rServer. Figure",
  "return x * gamma + beta": "9b illustrates the learned clusters by MFCL when BN is used in the clients module/decoder. The proposedMFCL(BN) identifies 3 distinguishable clusters for truck, ship, and frog, with some misclassifications withinthe ship cluster. Specifically, MFCL(BN) fails to accurately separate airplanes from ships. Also, there is asmall cluster of automobiles, with the remainder of the automobile samples being very close to the truckcluster. c demonstrates the learned clusters by MFCL when GN is used in the client module. MFCL(GN)can learn 7 distinguishable clusters for truck, ship, frog, automobile, horse, dog, and airplane, with improvedseparation between the airplane and ship clusters. d shows the learned clusters by MFCL when TN is used in the clients module/decoder. MFCL(TN)successfully learns 8 distinguishable clusters for truck, ship, frog, automobile, airplane, deer, and horse.",
  "(d)": ": t-SNE visualization of data at a client and the clusters learned from the representation vectorsat the rServer in the proposed MFCL. CIFAR-10 is trained with M = 10, assuming each client has only 2classes. (a) t-SNE visualization of raw data in a single client. In the other three figures, t-SNE visualizationsof the clusters learned after the projection layer are shown when the normalization technique used in theclient module/decoder is (b) BN, (c) GN, and (d) TN."
}