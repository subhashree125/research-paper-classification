{
  "Abstract": "Deep Neural Networks (DNNs) are susceptible to adversarial inputs, such as imperceptiblenoise and naturally occurring challenging samples. This vulnerability likely arises from theirpassive, one-shot processing approach. In contrast, neuroscience suggests that human vi-sion robustly identifies salient object features by actively switching between multiple fixationpoints (saccades) and processing surroundings with non-uniform resolution (foveation).This information is processed via two pathways: the dorsal (where) and ventral (what)streams, which identify relevant input portions and discard irrelevant details. Building onthis perspective, we outline a deep learning-based active dorsal-ventral vision system andadapt two prior methods, FALcon and GFNet, within this framework to evaluate theirrobustness. We conduct a comprehensive robustness analysis across three categories: ad-versarially crafted inputs evaluated under transfer attack scenarios, natural adversarial im-ages, and foreground-distorted images. By learning from focused, downsampled glimpses atmultiple distinct fixation points, these active methods significantly enhance the robustnessof passive networks, achieving a 2-21% increase in accuracy. This improvement is demon-strated against state-of-the-art transferable black-box attack. On ImageNet-A, a benchmarkfor naturally occurring hard samples, we show how distinct predictions from multiple fixa-tion points yield performance gains of 1.5-2 times for both CNN and Transformer basednetworks. Lastly, we qualitatively demonstrate how an active vision system aligns moreclosely with human perception for structurally distorted images. This alignment leads tomore stable and resilient predictions, with lesser catastrophic mispredictions. In contrast,passive methods, which rely on single-shot learning and inference, often lack the necessarystructural understanding. 1",
  "Published in Transactions on Machine Learning Research (12/2024)": ": The figure highlights failure modes of a dual-stream active vision system. In both columns,the top row shows correct predictions on benign inputs, while the second row visualizes mispredictions.On the left, mispredictions occur when the ventral stream is impacted by adversarial noise, despite correctlocalization by the dorsal stream. On the right, adversarial noise affects the dorsal stream, shifting its focusto the background, leading the ventral stream to misclassify based on features from this misdirected region. Swin V2 - The Swin Transformer V2 is an updated version of the Swin Transformer that processesimages in shifted windows, enabling efficient hierarchical feature extraction. This design enhancesimage classification by effectively capturing both local and global features within the image. RVT - Robust Vision Transformer identified weaknesses in transformer models for adversarial ro-bustness and introduced novel techniques, such as position-aware attention scaling and patch-wiseaugmentation, to enhance robustness across various shifts. These innovations make RVT a moreresilient vision transformer, especially under adversarial and distributional shifts. Focal Mod - Focal Modulation Networks introduce a focal modulation mechanism that replaces theconventional self-attention approach, aligning more closely with human-like feature-based attentioninstead of spatial attention. This approach enhances the models ability to learn aligned features,resulting in a stronger learned representation for various vision-based tasks. For target ventral architectures in a transformer-based transfer attack, such as TGR, we selected an improvedstandard Vision Transformer in Swin V2, a robust vision transformer in RVT, and the Focal ModulationNetwork, which is inspired by neuroscience for enhanced feature representation. For each ventral model,we added FALcons learned VGG16-based dorsal stream (fD) to create active vision counterparts. FALcon-SwinV2-Tiny refers to a dual stream active vision system, with a SwinV2-Tiny ventral stream fV andFALcons VGG-16 based fD dorsal stream. (c) illustrates these improvements, showing consistenttrends similar to those observed with CNN-based transfer attacks and ventral backbones.",
  "We present a novel analysis of the inherent robustness of Active Vision systems across threedifferent categories of adversarial inputs": "Our experiments demonstrate that active vision improves the ventral only passive visions perfor-mance by 2-21% in accuracy against adversarial crafted inputs across various state-of-the-arttransfer attacks (4.2) on ImageNet (Deng et al., 2009) in a black-box transfer attack setup. Through both quantitative and qualitative analyses, we highlight the salient learning aspects con-tributing to the inherent robustness of these methods, including glimpse-based focused learningat downsampled resolutions (4.3) and inference from distinct saccadic points (4.4). We provide similar detailed analysis for the naturally adversarial ImageNet-A dataset, demon-strating 1.5-2 times improvement over the passive ventral method. We provide qualitative resultsfor diverse set of samples within this category. We present qualitative results that highlight the benefits of implicit structured learning in ac-tive systems for handling foreground object distortion, leading to more human-aligned andinterpretable predictions.",
  "Related Work": "Active Vision methods The methods discussed here explore the incorporation of active iterative strategiesfor input processing. RANet (Mnih et al., 2014) incorporates a recurrent attention network to selectivelyfocus on different parts of the input sequence over multiple time steps excelling in sequential tasks. Saccader(Elsayed et al., 2019) emulates saccadic eye movements to iteratively extract features from an image attending",
  "Active Vision systems": "In this section, we provide a focused overview of the inference process and highlight key insights into theinherent robustness of two active vision systems: FALcon (Ibrayev et al., 2024b) and Glance and FocusNetworks (GFNet) (Wang et al., 2020). These methods simulate foveation by cropping glimpses from theimage based on fixation (saccadic) points, without blurring the extracted glimpses. This approach can beinterpreted as foveation with an extreme cut-off. For detailed learning processes, readers are directed tosupplementary Sections 1.1 (FALcon) and 1.2 (GFNet). For the remainder of this manuscript, we will referto saccadic points as fixation points.",
  "FALcon": "Active Vision structure Both the dorsal fD and ventral fV streams are represneted by deep convolutionalneural networks. During training, only fD is trained to emulate the saccadic and foveated functions. ForfV , any pre-trained network can be selected. Inference During inference, the input image X is divided into grid cells, as illustrated in the first imageat the bottom part of . Each grid cell is considered as an initial fixation point (red dot). The",
  "Adversarial crafted images": "Adversarial transfer attack setup The section aims to illustrate that active vision networks GFNet(Wang et al., 2020) and FALcon (Ibrayev et al., 2024b) exhibit higher levels of robustness against transferredadversarial images than base passive classifiers (He et al., 2015). We follow the protocol for a black boxtransfer attack threat model as outlined in (Liu et al., 2017; Mahmood et al., 2021). Following this protocol,we define non-targeted transferability. Given a surrogate Classifier Si, we generate an adversarial samplefor an image/label pair (x, y) which is denoted as xadv. This is with respect to the surrogate Classifier Siand attack pair ASi. The adversarial sample, xadv, is said to transfer to another target Classifier Ti if theadversarial sample is mispredicted. This is formalized as the following:",
  "xadv = ASi(x, y) | Si(xadv) = y;Ti(xadv) = y(1)": "Metrics We measure the non-targeted transferability by computing the percentage of adversarial examplesgenerated using model Si, but still correctly classified by the model Ti (not transferred). We refer to thispercentage as accuracy. A higher accuracy means less susceptibility to transferred adversarial samples andhence higher robustness under this setup. For a test set with N samples, the accuracy is defined as:",
  "j=11{Ti(xadvj) = yj}(2)": "Remark In this study, we focus solely on empirically showcasing the inherent robustness of active visionmethods. We do not propose any adversarial defense for a black-box attack scenario or analyze the trans-ferability trends between surrogates and target samples. Therefore, we opt for standard accuracy (Accst)under transfer, where we denote s t as surrogate to target. Section (4.2) empirically demonstrates this via quantitative results. Sections (4.3) and (4.4) then focus onexplaining the salient features that provides this inherent robustness, by analysing the internal mechanics ofGFNet and FALcon, respectively, in the presence of transferred adversarial inputs.",
  "Implementation details": "We perform our extensive robustness analysis on Imagenet (Deng et al., 2009), a standard benchmark forimage classification. We utilize ImageNet pre-trained weights for GFNet and FALcon without any additionalfine-tuning. Following the active vision structures highlighted in , we employ GFNets with Res-Net50 as both global fG and local fL encoders.These encoders provide relevant image features to thedorsal and ventral streams which are recurrent neural networks, as illustrated in . Both encodersare trained on downsampled resolution images of (96, 96) pixels. For FALcon we employ VGG16 (Simonyan& Zisserman, 2015) as the dorsal fD stream, and ResNet50 as ventral fV . Please note that, unlike GFNet,the fV of FALcon is not involved during training. Instead, only the dorsal is trained on the downsampledimages, while a pre-trained ResNet50 is employed as fV during inference on image resolutions of (224,224)as indicated by H W in .This approach provides the flexibility to select various ventral streamsand demonstrate how the FALcon framework can enhance the performance of the underlying passive ventralnetworks. We utilize Torchattacks (Kim, 2020), an integrated library for generating adversarial attacks(Ravikumar et al., 2022) with PyTorch, to generate adversarial samples.",
  "Inherent robustness in the Black-box transfer attack setup": "In this section, we demonstrate the superior performance of active vision systems (e.g., FALcon and GFNet)over passive ventral ones (e.g., supervised ResNet) in a black-box transfer attack setup. Adversarial samplesgenerated from surrogate models are transferred to the unknown target models. For GFNet, we use theoutput from the final prediction step as described in .2. For FALcon, we evaluate two types ofpredictions: Top, where the most confident prediction is matched with the ground truth, and Any, whereany correct prediction from multiple outputs is considered.This is enabled by inference from multiple",
  "distinct fixation points. The Any prediction strategy highlights the full potential of active systems, detailedin .1": "Iterative attacks We generate adversarial samples from surrogate classifiers Si using iterative adversarialattacks such as PGD (Madry et al., 2018), MIM (Dong et al., 2018), VMI-FGSM (Wang & He, 2021),Patchwise-IFGSM (Gao et al., 2020) and TI-FGSM (Dong et al., 2019). In the first setup, ResNet34 isused for Si, and in the second setup, ResNet50 is used matching the ventral stream in the active visionnetworks.For GFNet, ResNet50-based samples attack both dorsal and ventral streams simultaneously.FALcon, however, offers more control over which stream is targeted, as detailed in Auto-PGD 2. We canalso substitute FALcons ventral stream; for instance, FALcon-CutMix-Any uses the default VGG16 fDstream but replaces the fV with a ResNet50 trained using CutMix loss (Yun et al., 2019). The performanceis then evaluated based on any correct prediction matched with the ground truth. We conduct L attackswith 10 iterative steps, = 2/255, and = 8/255 for all six iterative attacks including Auto-PGD (Croce& Hein, 2020). Adversarial samples are generated using the entire 50,000-sample ImageNet test set. Thecorresponding clean accuracy is presented at the top of . Quantitative analysis Table (1) demonstrate that active vision systems consistently improve upon theunderlying passive approach across all surrogate architectures and attacks.For instance, FALcon witha Supervised-ResNet50 and a CutMix-ResNet50 ventral stream shows steady performance improvementsof approximately 11%-23% and 7%-15% in accuracy, respectively, over the corresponding passive ven-tral backbones, proportional to the attack strengths. Specifically, for a supervised-ResNet50, FALcon-Topimproves performance by nearly 18% for PGD, while for CutMix, FALcon-Any achieves close to a 15%improvement for VMI. CutMix (Yun et al., 2019) has robustness properties stemming from its strong regu-larized feature representations as indicated by the higher baselines than supervised-ResNet50. Additionally,we consider an adversarially trained ResNet50 Madry et al. (2018), which serves as an Oracle method de-noted as Adv-T. Trained specifically for adversarial defense, this method offers the best-case performanceon transferred samples on average. In the second setup, we notice that FALcon and GFNet provide anadditional shield, even when the attack is generated using the ventral backbone and shared feature encoderrespectively. While GFNet employs a more complex framework with recurrent dorsal and ventral streamssharing a convolutional backbone, FALcon-Top offers a clear measure of quantitative improvement due to itsactive processing mechanism. As shown in , the non-zero results demonstrate robustness benefits evenwhen generating Auto-PGD-based adversarial samples using each active vision systems crucial networks.This table indicates that generating samples based on FALcons ventral stream is more effective than usingits dorsal stream. This is pictorially explained in Section A.1.3. Transfer attacks with Large Geometric Vicinity (LGV) The plot on presents results basedon a geometric space attack (Gubri et al., 2022). The intuition behind this attack is provided in in AppendixA.1.4. We follow a similar experimental setup as outlined in the paper (Gubri et al., 2022), combining LGVwith PGD and BIM (Kurakin et al., 2018) on 1000 randomly sampled images from the ImageNet validationset. We report accuracy (Accst), and the results indicate a consistent trend similar to the iterative attacksfor supervised-ResNet50 and CutMix-ResNet50.For instance, as depicted in the plot, FALcon-CutMix-Any improves upon Top by 3-4%, which in turn improves upon the baseline by 24-28% for BIM (orange)and PGD (blue), respectively. Conversely, when FALcon is paired with an adversarially trained ResNet50(Madry et al., 2018), we observe close to a 2% improvement on clean samples (notably low accuracy foradversarially trained models) but no significant improvement for adversarial samples.This is expected,as networks already trained on worst-case perturbed samples benefit less from active predictions based ondistinct fixation points. Transfer attacks with Token-Gradient Regularization (TGR) Setup We follow the experimentalsetup outlined in the original paper (Zhang et al., 2023) and present results on a test set of 1,000 randomlyselected images from the ImageNet validation set (Deng et al., 2009). The equation and the intuition behindthis attack are provided in Equation (3) in Appendix A.1.5. The TGR transferable attack plot in shows the accuracy (Accst) of different target networks on adversarial samples transferred from varioussurrogate architectures, including Vision Transformers (ViT-B/16) (Dosovitskiy et al., 2021), and theirvariants PiT-B, and CaiT-S/24 (Touvron et al., 2021; Heo et al., 2021). In addition to the baseline methodspreviously studied, we employ several notable vision transformer architectures for image classification, such",
  "Effects of glimpse-based downsampling (case study: GFNet)": "In this section, we use GFNet to explore how learning image representations based on glimpses at a down-sampled resolution contributes to the inherent robustness. Downsampling inherently causes reduction infeatures. Adversarial imperceptible noise is crafted based on the image in its original resolution (e.g. 224 224). Hence downsampling the image, distorts the noise along with it, thereby reducing its overall impacton predictions. As a result, it is probable to think that an inherent robustness offered by models processingan image via downsampled resolution stems from the distortions on the non-uniform adversarial noise. Toanalyse this factor we organize experiments in this section into 3 settings: Setting 1 Effect of processing downsampled clean images - Images from the test set are used forevaluation without any adversarial attack. The images are downsampled to (96, 96) and (128, 128)and inference is performed. Setting 2 Reduction of efficacy of adversarial noise post downsampling - Adversarial images are firstgenerated from full resolution images of (224, 224) and then downsampled to (96, 96) and (128, 128),separately, for inference. for an active vision method such as the GFNet, this is an inherent step oftheir learning and inference pipeline. However, for passive vision methods, we resize the adversarialinputs to match the resolutions separately. Setting 3 Generating adversarial attacks on downsampled images - The images are downsampledto lower resolutions first and then adversarial inputs are generated. These adversarial downsampledinputs are then passed for inference on both passive and active target models. Since downsamplingis performed first, the adversarial effect is not downgraded. For the passive target baseline, we use a ResNet50 pre-trained on ImageNet at resolutions of 224 224.For GFNets, we infer with two separate models trained on 96 96 and 128 128 resolutions. Notably, wemaintain consistency by evaluating GFNets on images of matching resolutions. To illustrate downsamplingeffects, passive baselines are tested on downsampled images of 96 and 128 resolutions (see ). Forsimplicity, we further refer to GFNets trained on 96 96 dimensions as \"GFNet-96\". Results presents quantitative results, focusing on Accst. The best performing models are high-lighted in bold. For Setting 1, a passive model trained on a higher resolution suffers a drop in performancewhen evaluated at downsampled input, unlike GFNets trained for downsampled resolutions. Setting 2 showsthat simply downsampling adversarial images to lower resolutions is beneficial. This indicates along withthe image resolution, the imperceptible adversarial noise also probably gets downsampled thereby reducingits effect on model predictions even when Ti is same as Si. Furthermore, under this setting, GFNet-96 ex-hibits greater inherent robustness than GFNet-128 when compared to their corresponding passive baselines.",
  "Effect of distinct fixation points (case study: FALcon)": "In this section, we use FALcon to demonstrate the effect of processing an image from distinct fixation pointson the robustness of active vision methods. The capability of FALcon to consider various fixation points isused to extract interpretable visualization results. Moreover, since the ventral model is not fine-tuned duringtraining, it allows for a fair comparison with passive baseline network.",
  "Initial Fixation Point Map": "In order to understand the impact of adversarial noise on regions of the image that influence model pre-dictions, we define an Initial Fixation Point Map (IFPM). IFPM displays the distribution of initial fixationpoints based on how each of them affects the decision-making of FALcon throughout the inference process. shows IFPMs generated for both clean and adversarial images. As described in .1, FAL-con processes every input from multiple initial fixation points. Red dots indicate all initial fixation points,equally distributed over the image dimensions. Each point is then presented to the dorsal, which retainsonly those, indicated by blue dots, that potentially resulted in the capture of an object through the series ofexpanding foveated glimpses. The ventral processes the final foveated glimpses that resulted from potential points to determine the classlabel of an object. As a result, various fixation points result in FALcon making correct or incorrect outputpredictions, indicated by green and magenta dots, respectively. By obtaining IFPM for clean and adversarialversions of the same image, we illustrate how the adversarial noise impacts FALcon in terms of its capacityto make correct predictions from various fixation points.",
  ": Precision of predictions: The number of po-tential and correct prediction points decreases as at-tack strength increases, reflecting the quantitative re-sults presented in": "Results IFPMs illustrated in show that de-spite the addition of adversarial noise, multiple ini-tial fixation points result in correct final predictions(d & f). IFPM clearly indicates the reduced num-ber of potential and correct points for an adversarialsample compared to the corresponding clean sample(c to e) and (d to f). Due to its non-uniformity andimperceptible criteria, the adversarial noise does notaffect each point equally. Hence, multiple fixationpoints lead to correct class predictions. This visuallyexplains the reason for the improved performance ofan active method over a passive one, supporting thequantitative results presented in the previous sec-tions. Although noise affects the method, its inher-ent processing from multiple fixations makes it lesssusceptible (f). In the second sample (2f), we cannotice of a magenta fixation point far away from theobject. This is not present for the clean sample andis a false positive due to the addition of the noise.Yet, around the object, we can see multiple greenpoints indicating correct prediction. This validates the hypothesis presented in the first column in Figure(2) (a).",
  "Explaining adversarial vulnerability of passive methods": "Setup As mentioned earlier, the probable cause of adversarial vulnerability of the passive vision methodsis that they process an input in one-shot with uniform resolution, where every input pixel is treated withthe same importance. This is visually demonstrated in this section via occlusion maps. illustratesocclusion maps for passive methods (b,c,e,f) and the final foveated glimpse for FALcon (d,g). An occlusionmap is a visual heatmap indicating key regions of an image when occluded, affect the model performance.The darker the region, the higher contribution it has on the final prediction. Occlusion maps are generatedbased on prediction labels. We first generate the adversarial sample and then generate the occlusion mapbased on the predicted adversarial label. We use ResNet34 as the surrogate model and PGD as the candidateadversarial attack. The occlusion maps under ResNet50 (f) are based on the transferred adversarial samplesfrom ResNet34. Similarly, for FALcon, we present the final foveated glimpse and the initial fixation pointbased on the transferred adversarial samples (g). Green solid boxes refer to correct predictions. Results For the clean samples, FALcon correctly predicts all three instances (d). The dark region (1c)aligns with the body of the correct class (tench), resulting in a correct classification for ResNet50. But asindicated in (1f), the dark region shifts and does not align with the body of the object after the injectionof adversarial noise leading to an incorrect prediction. For FALcon, although the final foveated glimpsecaptures the corresponding dark region, the initial fixation point is directed towards the head of the object(g). This indicates that FALcon was initially guided by more salient features of the object beforeencountering the probable adversarial patch later. Additionally, downsampling likely mitigates the impact",
  "Foreground distorted images": "Human aligned active vision for foreground distortions In this section, we qualitatively analyze theimpact of foreground object distortions on the predictions of both active and passive vision systems. Unlikeimperceptible adversarial noise that spreads non-uniformly across the image or naturally adversarial sampleswith varying degrees of adversity, these distortions visibly alter the structure of the foreground object. Humans rely heavily on the structural configuration of objects to understand and identify them from variousviewpoints.A visible deformation or distortion can significantly affect our confidence in recognizing anobject. For instance, referring to each image in , we may still be able to identify a golden retrieverdespite its parts being dispersed across the scene, though our confidence might waver. A clean image of agolden retriever might prompt a confident identification, whereas a distorted image might lead us to say, \"Ican see a golden retriever, but in parts dispersed across the scene.\" It is interesting to observe how DL-basedsystems predict these distorted adversarial images. This section examines the response of these systems to such distortions and compares the performance ofpassive versus active processing. Our objective is to assess how well these systems align with human visionin handling visible structural distortions and to demonstrate how the human-aligned perception of an activevision system leads to more stable and interpretable predictions. Setup The upper part of illustrates two methods for generating foreground-distorted images:(a) Image Shuffling and (b) Composite Images. For image shuffling, we divide an image into equal-sizedpatches and randomly shuffle them. If the foreground-to-background ratio is high, this process structurallydistorts the object. For composite images, we use the Grab-cut algorithm (Rother et al., 2004) to extractthe foreground, disassociate these parts, and paste them onto random backgrounds. For evaluation, we use a passive ResNet50 and select FALcon as our candidate active vision model. UnlikeGFNet, which predicts based on salient object parts, FALcon captures the entire object by gradually foveatingon salient features and implicitly learning the structure, stopping when no further improvement is possible.This makes FALcon ideal for analyzing whether its predictions remain human-aligned despite foregrounddistortions. To comprehensively assess this, we conduct two-fold paired experiments: image shuffling andcomposite images. By concurrently applying these distortions, we aim to observe and compare the predictiontrends of the two systems, thereby determining the robustness and human-alignment of their predictions. Qualitative analysis For the undistorted sample of a golden retriever, both FALcon and ResNet50 makecorrect predictions with confidences of 75.39% and 41.56%, respectively. After image shuffling, FALconmakes multiple distinct predictions based on each localized part, each with lower confidence than the wholeobject. The face, being the most distinctive part, has the highest confidence among the parts. Low-confidencepredictions (< 50%) can be thresholded, as shown in (c). In contrast, the passive classifiers con-fidence abnormally increases, a phenomenon supported by literature (Tao et al., 2024; Chowdhury et al.,2024). This likely occurs because standard classifiers, trained on various image crops during data augmenta-tion, rely on specific object parts for correct prediction rather than the overall structure. Thus, even whenthe image is shuffled and the structure is distorted, the discriminative parts still lead to strong predictions. A similar instance is shown with a white wolf sample. For the second permutation of the image, the activemethod makes a categorical misprediction based on the localized facial part, identifying it as a \"Russianwolfhound,\" which is still a \"wolf-looking\" dog breed. This misprediction is interpretable and less catastrophic,as evidenced by the provided sample of an actual Russian wolfhound. Image shuffling, the first of the twofold experiment indicates that passive methods rely mostly on parts for discrimination and lack structural",
  "understanding.In contrast, active methods, while affected by distortions, make lower-confident correctpredictions or interpretable, less catastrophic mispredictions": "In composite images, the same foreground is extracted, dissociated into parts, and then pasted onto randombackgrounds, which are likely unfamiliar for passive classifiers to associate with the foreground. As seenin both samples, passive classifiers fail miserably, making predictions that are not remotely correlated withthe actual foreground. On the other hand, active vision systems make lower-confidence correct predictionsor interpretable, less catastrophic mispredictions based on the part localized by the dorsal stream. Thesepredictions are also human-aligned; without seeing the original full image of a white wolf, humans mightsimilarly infer that the parts belong to a wolf or a similar-looking dog breed like a Siberian husky or anEskimo dog. For the corresponding composite image as well, the localized part resemble the body of a polarbear. Conversely, passive classifiers might make nonsensical predictions, such as identifying a polar bearsbody part as a schooner, a type of sailing vessel, which is clearly a catastrophic error.",
  "Conclusions": "In this work, we outline a deep learning-based active vision framework and advocate for its inherent ro-bustness. Specifically, we adapt two existing approachesFALcon Ibrayev et al. (2024b) and GFNet Wanget al. (2020)into the active dorsal-ventral framework, demonstrating their robustness against three cate-gories: adversarially crafted samples, naturally adversarial samples, and foreground-distorted images. In ablack-box transfer attack setup, we attribute the enhanced robustness to two key factors: (1) glimpse-basedprocessing at downsampled resolutions and (2) inference from multiple fixation points. Using GFNet, weshow how downsampling mitigates the impact of adversarial noise.With FALcon, we demonstrate howmultiple fixation points help avoid mispredictions due to the non-uniformity of adversarial noise. Employ-ing various state-of-the-art adversarial transfer attacks, we observe consistent performance improvementsof 2-21% over passive methods, except when using non-adversarially trained ventral networks. We extendthis understanding to natural adversarial samples, which model real-world challenges like object occlusions,unusual poses, and complex backgrounds that confuse passive classifiers. FALcons flexible framework allowsswapping different ventral streams, leading to a performance enhancement of 1.5x, even with an adversar-ially trained ventral stream. We further investigate the robustness of image classifiers against foregrounddistortions, using FALcon as our active vision model due to its capability to capture entire objects in undis-turbed images. Through a two-fold experiment, we visually demonstrate how the predictions of an activevision model are stable, resilient, and more human-aligned compared to the catastrophic mispredictions ofpassive classifiers. A potential future direction will be to explore optimized white-box attacks and defensemechanisms tailored to active vision systems. This exploration could lead to a deeper understanding of howto enhance these inherently robust systems, further strengthening their adversarial robustness. This work was supported in part by, the Center for the Co-Design of Cognitive Systems (CoCoSys), aDARPA-sponsored JUMP 2.0 center, the Semiconductor Research Corporation (SRC), and the NationalScience Foundation.",
  "Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li. Boosting adversarial attacks with momentum.In CVPR, 2018": "Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examplesby translation-invariant attacks. In Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.",
  "Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao. Focal modulation networks, 2022": "Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, and Gao Huang. Resolution adaptive networks forefficient inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,2020. Xinli Yue, Ningping Mou, Qian Wang, and Lingchen Zhao. Revisiting adversarial robustness distillationfrom the perspective of robust fairness. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. S. Yun, D. Han, S. Chun, S. Oh, Y. Yoo, and J. Choe. Cutmix: Regularization strategy to train strongclassifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision(ICCV), 2019.",
  "A.1.2Discussion with adversarially trained model": "In this section, we briefly examine the role of an adversarially trained model (Madry et al., 2018) as a baselinein our analysis, alongside the robustness properties observed in an active vision system. We present resultsfor both adversarially crafted samples () and naturally adversarial samples (), summarizingkey insights across various types of adversarial inputs. Accuracy on non-perturbed clean samples - The clean accuracy of an adversarially trained(Adv-T) model is lower than that of a standard classifier, a known trade-off resulting from adversarialtraining. This effect is evident in the clean accuracy results in . Black box transfer attack setup - In , we present Adv-T as an oracle method, trained todefend against worst-case perturbed samples within the L- norm ball. The model shows minimalperformance drop, even as the potency of adversarial attacks varies, with negligible decline comparedto its nominal accuracy.Therefore, a black-box transfer attack setup is not the most effectiveapproach for testing such a robust model. Comparison with active vision systems - In , we can see For attacks like PGD and MIM,GFNet outperforms Adv-T, while FALcon-CutMix shows stronger performance on PGD and PI-FGSM attacks. However, active vision systems with non-robust ventral methods, such as CutMixResNet50 and supervised ResNet50, as well as these passive ventral methods alone, experience anoticeable performance drop compared to clean accuracy, with results varying based on the attacksstrength. This approach effectively highlights key performance trends for analysis. Active vision system with Adv-T as ventral method - In (a), for transferred attacksfrom LGV, we observe no noticeable improvements when adversarial samples are transferred from asurrogate supervised ResNet50, as highlighted in . However, for clean samples, there is aslight improvement. Natural Adversarial samples - In , we show that for naturally hard samples, a passiveadversarially robust ResNet50 performs worse than a passive CutMix ResNet50. When integratedinto an active setup, the framework improves performance on these samples; however, the gain issmaller than that achieved with CutMix or supervised ResNet50 in the same setup. This reflectsthe limitations adversarially trained models face with naturally occurring samples, a drawback notshared by inherently robust active vision systems.",
  "A.1.3Transfer attacks illustration": "This section details the setup outlined in , under . In a black-box transfer attack setup,the specific configuration of the underlying model remains unknown. However, to evaluate the efficacy ofthe active vision system, we generate adversarial samples by transferring attacks using surrogate dorsal (fD)and ventral (fV ) streams. For FALcon, fD is represented by VGG16, and fV by ResNet50, as describedin .2.This approach allows us to assess which stream serves as a more effective surrogate foradversarial sample generation.The lower section of presents results when samples are craftedbased on the ventral stream, ResNet50. Similarly, for Large Geometric Vicinity (LGV) (Gubri et al., 2022),transferred samples are generated using the surrogate ventral stream (ResNet50), with results illustrated in (a).",
  "A.1.4Transferability from Large Geometric Vicinity (LGV)": "The paper introduces a technique called Transferability from Large Geometric Vicinity (LGV) (Gubri et al.,2022) to enhance the transferability of adversarial attacks in black box transfer setup. This is illustrated in of (Gubri et al., 2022). The method starts with an initial pretrained surrogate model and gathersmultiple weight sets for a few additional training epochs with a constant and high learning rate. This isdone to enhance the geometric diversity of the surrogate models within a wide weight optimum. A wideweight optimum refers to a region in the weight space of a neural network where many configurations ofweights result in similar, low loss values. In this region, the loss landscape is flatter or broader, meaning thatsmall changes in the weights do not drastically increase the loss. Wide optima are often preferred becausethey represent solutions that are more likely to capture general patterns in the data rather than overfitting",
  "A.1.5Token Gradient Regularization (TGR)": "The TGR method, introduced in (Zhang et al., 2023) provides a gradient based transfer attack algorithmfor Vision Transformers (ViT) (Dosovitskiy et al., 2021) and its variants such as Class-Attention in ImageTransformers (CaiT) (Touvron et al., 2021) and Pooling based Vision Transformer (PiT) (Heo et al., 2021).This algorithm, represented as TGR() removes tokens with extreme values and reduces variance in back-propagated gradients. It utilizes token gradient information from both the Attention and Query-Key-Valuecomponents within an attention block, as well as from the MLP component within the MLP block, togenerate adversarial samples Gradadv. This is illustrated in of (Zhang et al., 2023). The TGRfunction combines gradient information as follows:",
  "xadvt+1 = xadvt+ sgn(Gradadv)(3)": "Here, k denotes the top-k or bottom-k input gradients with highest and lowest values respectively whichdenote the extreme tokens. The scaling factor for gradients is s and is a hyper-parameter to control thestep size. This method is effective against CNN models as well. And hence forms a strong transfer attackfor both CNN and Transformer based backbones.",
  "A.1.6Surrogate models for Token Gradient Regularization (TGR)": "We follow the experimental setup of the original Token Gradient Regularization paper (Zhang et al., 2023)for selecting surrogate architectures. This paper demonstrated that transferable attacks, leveraging back-propagated gradients through attention blocks in specific surrogate vision transformers, are highly effectiveagainst other target vision transformer models. Additionally, it showed that transformer-based adversarialsamples can successfully transfer to CNNs, making this approach effective for attacking CNN models aswell. Following their setup, we chose a Vision Transformer (ViT) (Dosovitskiy et al., 2021) and its variants,including the Pooling-based Vision Transformer (PiT) (Heo et al., 2021) and Class-Attention in ImageTransformers (CaiT) (Touvron et al., 2021), as surrogate models. We provide some intuition regarding thesurrogate models. ViT - The Vision Transformer (ViT) architecture splits an image into fixed-size patches, treatseach patch as a token, and applies a standard transformer model to these tokens, enabling directapplication of transformer layers to image data without convolutional processing. In ViT-B/16, \"B\"stands for the Base model size, indicating a standard configuration with 12 transformer layers, and\"16\" refers to the patch size (16x16 pixels) into which the image is divided before processing. PiT - The Pooling-based Vision Transformer (PiT) modifies the Vision Transformer architecture byadding pooling layers between transformer blocks, which gradually reduce the spatial dimensions,similar to CNNs. This pooling improves efficiency and generalization. In PiT-B, the \"B\" stands for",
  "Base model size, indicating a standard configuration with a specific number of layers and attentionheads": "CaiT - The Class-Attention in Image Transformers (CaiT) enhances the standard Vision Trans-former by introducing class-attention layers at the end of the network, which focus on aggregatingglobal information for classification, and LayerScale mechanisms within each transformer block,which help stabilize deeper models by scaling the outputs of layers for better training. In CaiT-S/24, \"S\" stands for Small model size, indicating a smaller configuration with fewer parameters, and\"24\" specifies the number of transformer layers in the network. This setup enables CaiT to go deeperwhile remaining stable and efficient.",
  "A.1.7Target models for Token Gradient Regularization (TGR)": "In this subsection, we provide some intuition regarding the transformer based target models for the TGRsetup (c). For transformer based target ventral models, we use Swin Transformer V2 (Liu et al.,2022), Robust Vision Transformer (Mao et al., 2021), and Focal Modulation Networks (Yang et al., 2022).And as mentioned in , we use the tiny versions of these transformers for evaluation.",
  "A.1.8TGR visualizations": "We have included additional visualizations using TGR with CaiT-S/24 as the surrogate architecture. Thetarget model comprises a passive Swin Transformer V2 Liu et al. (2022) and an active vision model com-bining FALcons VGG16 dorsal stream with the same Swin V2 as the ventral stream. presentsocclusion map visualizations similar to those in , while illustrates failure modes criticalto understanding active system behavior. demonstrates a generalization of the visualizations fromCNN-based iterative attacks on CNN targets to a similar transformer-based scenario, showing consistenteffects in this setup."
}