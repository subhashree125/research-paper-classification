{
  "Abstract": "Tracking-by-detection has become the de facto standard approach to people tracking. Toincrease robustness, some approaches incorporate re-identication using appearance modelsand regressing motion oset, which requires costly identity annotations. In this paper, wepropose exploiting motion clues while providing supervision only for the detections, whichis much easier to do. Our algorithm predicts detection heatmaps at two dierent times, along with a 2D motionestimate between the two images. It then warps one heatmap using the motion estimateand enforces consistency with the other one. This provides the required supervisory signalon the motion without the need for any motion annotations. In this manner, we couple theinformation obtained from dierent images during training and increase accuracy, especiallyin crowded scenes and when using low frame-rate sequences.",
  "Introduction": "Multi-target tracking has been extensively studied (Ciaparrone et al., 2020; Yilmaz et al., 2006; Smeulderset al., 2014), and research has now pivoted towards challenges such as handling crowded scenes, low framerate sequences, and fast-moving objects. Most current approaches rely on the tracking-by-detection paradigm (Andriluka et al., 2008), which requiresstrong object detectors.Linking the detections into trajectories is entrusted to algorithms that do nottake the images into consideration anymore, such as Pirsiavash et al. (2011); Berclaz et al. (2011); Wanget al. (2019). Unfortunately, performance suers in crowded scenes, especially when the video frame rate islow (Bergmann et al., 2019). illustrates this. Motion prediction can be used to alleviate this problem.For example, the popular tracking-by-regression approach of Zhou et al. (2020); Xu et al. (2020) does thisby predicting peoples motion as a 2D motion oset. However, motion supervision is often derived from identity annotations, which can be costly. In this paper,we seek to exploit the motion information without having to provide identity or motion annotations, relyingsolely on detection annotations that are much easier to obtain. To this end, we introduce a dierentiableapproach to constructing a future detection heatmap by displacing a current heatmap using a 2D oset mapthat represents motion. Given a pair of images acquired at times t and t + 1, we predict detection heatmapsat both times, along with an oset map. We use the oset map to warp the heatmap estimated at time t to",
  "Published in Transactions on Machine Learning Research (11/2024)": "0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.50 0.55 0.60 0.65 0.70 0.75 MOTA (%) ByteTrackOursByteTrack Ground Figure A.4: Eect of Tracking on the Ground Plane: Working on the ground plane signicantlysimplify tracking. In the image plane, individuals appear to move faster when closer to the camera due toperspective eects. By negating this perspective eect, trajectories across the scene become more uniform.This uniformity is especially advantageous at low frame rates, where motion is more pronounced. However,at higher frame rates, this approach can be slightly counterproductive, likely due to calibration inaccuraciesor noise in the detection process.",
  "enforce consistency between the result and the one estimated at time t + 1. In this manner, we couple theinformation obtained from dierent images, which increases robustness": "The proposed motion estimation method can be used in conjunction with a wide variety of detectors andtrackers. The supervision for detection can originate from ground-truth detections when trained jointly witha detector, or from the output of a pre-trained detector, if trained separately. Crucially, in either case,motion osets are supervised without any additional annotations. We use the challenging single-view MOT17 dataset (Milan et al., 2016) and multi-view WILDTRACKdataset (Chavdarova et al., 2018) to demonstrate our models ability to predict accurate human motionwithout requiring any motion annotations. On MOT17 we outperform the state-of-the-art tracking methodBytetrack (Zhang et al., 2022) by a signicant margin in low frame-rate scenario. On WILDTRACK wesurpass recent multi-view detection and tracking techniques (Engilberge et al., 2023; Teepe et al., 2024;Cheng et al., 2023). Code can be found at",
  "Related Work": "Tracking is a core task of computer vision and has applications in a wide range of domains such as surveil-lance, autonomous driving, the medical eld or the autonomous sales space. Despite having been widelystudied (Ciaparrone et al., 2020; Yilmaz et al., 2006; Smeulders et al., 2014), multi-object tracking can stillbe challenging. In particular, self-occlusions are frequent in crowded environments and can result in misseddetections and identity switches. Furthermore, due to hardware constraints, some applications can onlyoperate at low frame rates, making association across frames dicult. In this section, we briey reviewexisting approaches and discuss multi-view tracking, which we use to validate our approach.",
  "Multi-View Tracking": "Using multiple viewpoints is an eective way to increase robustness in crowded scenes featuring heavy occlu-sions. Some methods rst extract monocular detections before projecting and matching them into a commonframe (Xu et al., 2016; Fleuret et al., 2008) while others combine view aggregation and prediction (Baquet al., 2017). Many recent ones (Hou et al., 2020; Song et al., 2021; Hou & Zheng, 2021; Engilberge et al.,2023;; Teepe et al., 2024) perform view alignment directly in the feature space. They assume the groundto be at and project features onto the common ground plane. There, they are spatially aligned and canbe easily aggregated to produce detection directly on the ground plane. The approach of (Engilberge et al.,2023) leverages properties of the ground plane by predicting weakly supervised human structure motionfor tracking. We also leverage the benet provided by such ground plane representations, but depart fromprevious work by predicting motion as an unconstrained 2D oset, allowing us to model and predict motionof arbitrary length.",
  "Approach": "Most recent single- and multi-view tracking approaches rely on simple association methods to link theoutput of strong single-frame object detectors. Typical constraints imposed on the linking process are limitson displacement length across frames (Engilberge et al., 2023; Bergmann et al., 2019), which are weak whenthe frame rate is high and inapplicable when it is low. To impose motion constraints while handling lowframe rates and the potentially large frame-to-frame displacements they entail, we want to avoid such strictlimits. To this end, we introduce the approach depicted by . For all sets of images taken at consecutive timesteps t and t + 1, we predict probabilities of detections at both time steps and a 2D displacement map fromtime t to t + 1. During training, we provide supervision for the detections and enforce consistency betweenthe detections and displacements, which provides an additional supervisory signal without any additionalannotations and increases performance, as will be shown .2",
  "Formalism": "We denote Itc an image drawn from camera c [1, C] at acquisition time t [1, T]. Here C 1 is the numberof cameras and T 1 is length of the video sequence. Let Ptc be pairs of consecutive images taken at timest and t + 1 by camera c. We dene as Pt, the set of all pairs of frames taken at time t and t + 1. For each camera c, let Kc be the intrinsic camera matrix and let Rc and tc be the camera rotation andtranslation parameters which can be obtained by calibrating the cameras (Tsai, 1987). As in many otherworks (Hou et al., 2020; Engilberge et al., 2023), we will reason in a ground plane that is common to allviews. We assume the ground to be at and set its z-coordinate to 0. We can now dene a homographyHc = Kc[Rc|tc] that relates points in the image plane c to corresponding points in the ground plane.The assumption of a at ground plane is valid in most crowd tracking scenarios but the approach can begeneralised to complex topographies. The generalisation is performed by replacing the homography with anon-linear mapping that can be precomputed. We denote the ground plane as a 2D grid of size w h. Wedene G = {(x, y) | x [0 . . . w] and y [0 . . . h]} the set of all locations in the ground plane. For every pair of frames Pt, our network predicts a heatmap Xt wh of the same dimension as theground plane. This heatmap represents the probability that someone is present at a given ground planelocation at time t. We also predict a corresponding displacement map t,t+1 R2wh that indicates wherea person located at a given place at time t is likely to be at time t + 1. Each value of t,t+1 is a 2D vectorthat represents the x and y components of the displacement of the person at that location. t,t+1 can beused to compute a heat map Xt+1 as discussed in .2. These estimated values can then be comparedto ground truth binary maps Xtgt and Xt+1gtthat denote the actual presence of someone at any given location.",
  "Motion-Based Supervision": "Given a frame Pt, most existing methods compute Xt and Xt+1 independently and are trained to makethem as similar as possible to Xtgt and Xt+1gt . By contrast, our approach estimates Xt and t,t+1 and usesthem to infer Xt+1. In eect, we force the estimated motion to be consistent with the observed probabilitiesof presence, which provides a supervisory signal on the predicted motion without having to provide motionannotations. Note that when we repeat the process on Pt+1, we also force the Xt+1 directly predicted bythe network to be similar to Xt+1gt . Thus, we also use the annotations as eectively as the earlier methods. To implement this, the inference of Xt+1 from Xt and t,t+1 must be dierentiable. To this end, we imple-mented the following algorithm that derives a value at any given location by spatially global interpolation.Let xtj and xt+1jbe probabilities of presence at location j of Xt and Xt+1.",
  "+ e4rl10 ,(2)": "where t,t+1iis the predicted displacement at location i of t,t+1 and d is the Euclidean distance betweencoordinates i and j. Note that i and j represent locations in the ground plane and consist of pairs (x, y) ofpixel coordinates. W(l) is a weight function that decays with distance l and r is a hyper-parameter that controls the speedof the decay. In we plot this function for dierent r. During training, lower values of r makeconvergence easier by allowing contribution from locations that are further apart. However, increasing rmakes the reconstruction more accurate. For this reason r is initialized with a small value during training,which is then increased over time as the precision of the predicted oset improves.",
  "Lfb : Forward/Backward Loss": "For any t, we can reverse the order of the images in the frame Pt and, instead of estimating t,t+1, estimatet+1,t. In other words, we can reverse the motion and, if the network is appropriately trained, we shouldobtain a motion that is approximately the opposite of the original one. To enforce this, we dene",
  "Lse : Motion Spatial Extent Loss": "In practice, the ground truth maps are obtained by pasting Gaussians kernels where people are and zeroselsewhere. As a result, individual detections appear as Gaussian peaks. Since we train our networks toapproximate these ground-truth maps, the same can be said about Xt. To reliably move such peaks fromtheir location at time t to that at time t + 1, the predicted motions should have roughly the same spatialextent. To encourage this, we dene",
  "Single-View Model": "We extend the state-of-the-art YOLOX architecture (Ge et al., 2021) to process a pair of frames, Pt, andto output an additional 2D displacement map, t,t+1. Since YOLOX returns bounding boxes, we convertthem to Gaussian heatmaps Xt and Xt+1.They are then concatenated and fed through a ResNet (He et al.,2016) parametrized by weight , which outputs a 2D displacement map t,t+1. Finally, the displacementmap and the heatmap Xt are fed to the prediction module of .2 to estimate Xt+1.",
  "Multi-View Model": "The network we use is similar in design to the ones of Hou et al. (2020); Engilberge et al. (2023). As shownin , it takes as input the images of Pt and uses a ResNet He et al. (2016) parametrized by weight 0to extract features from all images from all cameras at both time steps independently. The feature maps arethen projected onto a common ground plane using their respective image to ground plane homography Hc,as dened in .1. The results are concatenated and processed by a scene aggregator parametrizedby weight 1 that outputs a detection probability map Xt and a 2D displacement map t,t+1. Both are fedto the prediction module of .2 to estimate Xt+1.",
  "Training": "For both model Xt, Xt+1, and t,t+1 are used to compute the losses Lmot of Eq. 3 and Lse of Eq. 7. Tocompute t+1,t and the loss Lfb of Eq. 6, we simply reverse the temporal order of the images in Pt. Thetraining of the two models only diers with respect to Ldet . In the single view case, YOLOX is pretrainedand kept frozen, and therefore Ldet is ignored. These losses are summed into the full training loss L of Eq. 4that we minimize over our training set to set the network weights .",
  "Multi-view association": "For the multiview model we perform non-maximum suppression in each Xt and use k-means clustering onthe condence of the remaining maximum (with k = 2) to separate true detections from noise. To linkthese detections into trajectories, we use MuSSP (Wang et al., 2019) that does this using a min-cost owmethod that operates on a graph whose nodes are the detections connected by temporal edges that connectdetections at dierent times. These edges are weighted as follows. An edge connecting locations i and j attimes t1 and t2 respectively is taken to be",
  "Single-view association": "In our single-view pipeline, we retain the original post-processing step implemented by YOLOX. For trackassociation, we modify ByteTrack (Zhang et al., 2022). First, we modify the detection association step toutilize ground plane IoU instead of image plane IoU. Secondly, we replace the Kalman lter used in ByteTrackfor motion estimation with the learned predicted motion t,t+1 output by our extended YOLOX.",
  "Experiments": "In this section, we empirically validate our approach. After dening the experimental protocol, we rstevaluate the quality of the predicted motion. Subsequently, we assess the complete multi-view detectionand tracking pipeline on the WILDTRACK dataset (Chavdarova et al., 2018), and the single-view pipelineon MOT17 (Milan et al., 2016). Finally, we conduct multiple ablations to explain the contribution of eachcomponent to the overall performance.",
  "We evaluate our approach on WILDTRACK (Chavdarova et al., 2018) and MOT17 (Milan et al., 2016)": "WILDTRACKis a calibrated multiview multi-person detection and tracking dataset featuring 7 view-points focusing on an area of 1236m2 in the real world. It contains 400 synchronized frames per view witha resolution of 1080 1920 pixels. Each person is annotated with a bounding box. As in Engilberge et al.(2023) we discretize the ground plane such that one cell corresponds to 20cm in the real world yielding aground plane map of size 180 80 cells.",
  "ModelMOTAMOTPIDF1IDPIDR": "DeepOcclusion+KSP 69.6-73.283.865.0DeepOcclusion+KSP+ptrack 72.2-78.484.473.1ReST 81.6-85.7--MVDet + muSSP80.60.8079.479.279.6MVDeTr + muSSP89.40.5890.790.590.9EarlyBird 89.5-92.3--MVFlow 91.30.5793.592.794.2Ours meanstd91.70.20.560.0293.90.293.00.294.90.1Ours best91.90.5494.193.295.0 MOT17is a single-view, multi-person detection and tracking dataset comprising sequences captured byboth static and moving cameras. The sequence lengths vary from 450 to 1500 frames, with frame ratesranging from 14 to 30 fps, totaling 15,948 frames. Video resolutions are either 10801920 pixels or 640480pixels, and each sequence contains up to 222 tracks. Every individual is annotated with a bounding box.For our method, we utilize the groundplane homographies Hc determined in Dendorfer et al. (2022) to mapinto the ground-plane. Since the test set is private we follow the train/val split of Zhou et al. (2020).",
  "Our modes are implemented in Pytorch (Paszke et al., 2019) and trained on a single NVIDIA A100 GPU": "Single-View ModelThe detector is a modied YOLOX (Ge et al., 2021), as described in .4.We use the existing implementation of MMDetection (Chen et al., 2019) to develop our training pipeline.To augment the input images, we follow the strategy outlined in Ge et al. (2021). To accommodate pairs offrames as input, the detections are rst extracted for the two frames using a frozen YOLOX, the detectionare converted to gaussian heatmaps of 512 channels, rst channel is a gaussian mask, while the remaining512 channels are use to provide crop features of each detection. ResNet-50 (He et al., 2016) is used to extractfeature for each heatmaps independently. The number of channels in the rst layer of the ResNet is modiedfrom 3 to 513 accordingly. The resulting features are concatenated along their channel dimension beforebeing fed into two convolutional blocks consisting of ReLu(Nair & Hinton, 2010) - convolutional layer. Thelast convolutional layer output a 2 channels displacement map. The model is trained on MOT17 followingthe approach in Milan et al. (2016), with modications as described in .4. Training images aresupplied with random intervals between them to encourage the model to accurately learn motion behaviorat various frame rates. Multi-View ModelThe feature extractor is a ResNet-50 (He et al., 2016) whose last four layers havebeen removed such that it outputs feature maps containing 256 channels. The input images are augmentedusing a random crop strategy with a probability of 0.5. The scene aggregator is a multi-scale module, itcomprises four scales. Between each scale the feature resolution is halved. Each scale consists of four blocksof \"convolutional layer - batch normalization (Ioe & Szegedy, 2015) - ReLu (Nair & Hinton, 2010)\". Theoutputs of the four scales are bilinearly interpolated back to their original dimension. After concatenation,they go through a nal 11 convolutional layer with 3 output channels. One channel goes through a sigmoidfunction to produce the detection probability map. The other 2 are taken to be the motion osets. Themodel is trained on WILDTRACK using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of0.0001 and a batch size of one. The learning rate is halved after epochs 20, 40 and 60.",
  "CenterTrack 30val61.1--ByteTrack 30val77.084.177.5Ours30val76.684.176.1": "0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.50 0.55 0.60 0.65 0.70 0.75 MOTA (%) MOTA vs. Low Framerate on MOT17 ByteTrackOurs : Tracking results in low FPS scenario. In lowframe rate scenarios our model outperforms the ByteTrackbaseline in term of MOTA. The lower the frame rate thehigher the performance gap, showing the benet of the pro-posed approach. when the input/output resolutions increase. Therefore the reconstruction step is implemented with a slidingwindow, where the window size can be set to any value, making memory consumption xed and not dependenton heatmap resolution.For all experiments we use a window size of 59 pixels, meaning that, at mostthe reconstruction step can handle motion oset of length 59/2 = 29.5 pixels. Which is larger than anygroundtruth motion. HyperparametersThe reconstruction hyperparameter r is initialized to 0.8, during training it is in-creased at the end of every epoch by 0.08 until it caps out at 5. Additionally when computing the detectionloss on reconstruction, the ground truth heatmap is also passed through the reconstruction module with azero oset to have the same imprecision (scaled up Gaussian peak linked to the value of r ) on both theground truth and prediction. The hyperparameters in the loss L are set to fb = 0.05 and se = 1. Asummary of the hyperparameters can be found in Table A.6.",
  "Metrics": "To gauge the quality of the nal trajectories, we report the same CLEAR MOT metrics (Kasturi et al.,2009) as in previous works Chavdarova et al. (2018); Engilberge et al. (2023). They include Multiple ObjectDetection Accuracy (MODA), Multiple Object Tracking Accuracy (MOTA), and Multiple Object TrackingPrecision (MOTP). We also report identity preservation metric IDF1 (Ristani et al., 2016), computed fromIdentity Precision (IDP) and Identity Recall (IDR). Since we are concerned with motion and tracking people,MOTA, MOTP, and IDF1 are the most signicant because they quantify the quality of trajectories, insteadof that of detections in individual frames like MODA. We use the py-motmetrics and MMDetection (Chenet al., 2019) libraries to compute these metrics. Since a specicity of our approach is to predict 2D motion osets, we also seek to quantify how good theyare. To this end, we compute the L1 distance between predicted and the ground truth osets. For non-zero ground truth osets, we also compute the average angular error and norm between ground truth andprediction. We will refer to these measures as L1, Angle. and Norm.",
  "Motion evaluation": "To evaluate, the quality of our motion osets, we compare them to those produced by two additional baselines.First, since our motion oset representation shares similarities with optical ow, we used RAFT (Teed &Deng, 2020), a network pre-trained to estimate optical ow, to estimate the ow in each one of the viewpoints.The resulting ows are projected onto the ground plane and averaged. Second, we trained a version of ourmodel using full supervision by skipping the reconstruction and instead directly applying an L2 loss betweenpredicted and the ground-truth oset. In other words, unlike in our normal approach, we provide full motionsupervision. The groundtruth motions are derived from identity annotations present in WILDTRACK. Wereport the results in . Our weakly supervised outperforms the RAFT baselines by signicant marginsand comes close to the fully supervised method on both datasets. In , we provide a qualitative resulton WILDTRACK to show this visually. The supplementary material contains video results illustrating therobustness of our motion prediction method even in a low frame rate scenario. : Motion oset evaluation We evalu-ate 2D motion oset according to three metrics,L1 error, Angular error (in degree) and Normerror. Our model outperforms the optical owbaseline by a large margin. It even performs com-petitively against the fully supervised methods,especially in terms of norm error.",
  "Ablation Study": "To understand which components of the model contribute to the overall performance, we conduct an ablationstudy on the WILDTRACK dataset. We test three components of our model, the smoothing term Lse, thetemporal consistency term Lfb of our loss both dened in .3 and the reconstruction from motiondened in .2. To disable the reconstruction from motion, smoothing and temporal consistency wesimply omit the losses. As it can be seen in , adding Lmot greatly improves the motion metrics at the cost of slightly degradingthe MODA one. This was to be expected since the same model has to predict motion on top of the originaldetection. Further adding Lse and Lfb only has benecial eect both in terms of MODA, L1, and angleerror. The best score is obtained when the three elements are combined. We also conduct an ablation study on the benet of using motion oset in the muSSP (Wang et al., 2019)association algorithm. We modify Eq. (8) by removing the last term Wm and hence ignore the predictedmotion oset. When muSSP is executed without motion oset MOTA drops by 0.5 points and IDF1 0.9 bypoints compared to the last row of . This conrms the usefulness of integrating motion oset insideassociation algorithms.",
  "with Lse and Lfb we observe that the model converges to predicting the real motion. Additional results areprovided in the appendix to showcase this": "A second limitation of the proposed dierentiable reconstruction from motion is its memory cost.Thememory consumption grows quadratically with respect to the dimension of the space it is reconstructing. Tomitigate this diculty. the implementation relies on a sliding window mechanism, as described in .1.This xes the required amount of memory but also limits the maximum range of motion to half the size ofthe window. In practice it hasnt been a problem, but it could be one for very high resolution scenarios thatfeature long-range motions. In such a scenario memory would be a bottleneck anyway, and parallelizing theworkload across multiple GPUs would be the way to overcome this limitation.",
  "Broader Impact": "Our research contributes to the eld of multi-object tracking by proposing a motion prediction method thatreduces the need for costly identity annotations, relying only on detection supervision. This developmenthas signicant implications for real-world applications such as surveillance, autonomous driving, and publicsafety, where robust tracking is crucial but manually labeling motion data is prohibitively expensive. Byimproving tracking performance in challenging low-frame-rate scenarios and crowded environments, this workhas the potential to enhance the eciency and accuracy of systems that monitor human activity, makingthem more accessible and scalable. However, the deployment of such technologies must be done with care, as they could raise privacy concernsif used for mass surveillance or other invasive purposes. It is important that future applications of this workconsider ethical implications and adhere to strict guidelines to ensure the protection of individual privacyand mitigate risks of misuse.",
  "Conclusion": "We have proposed a network to predict both detection heatmaps and motion osets given only detectionsupervision. To this end, we use the consistency between the predicted heatmaps and osets as a supervisorysignal. We have demonstrated that this increases performance in challenging single- and multi-view people-tracking scenarios. The eect of this approach is particularly noticeable in low frame-rate scenarios, whereconventional tracking-by-detection approaches are most likely to fail.",
  "Pattern Recognition, 2018": "Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, WansenFeng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao,Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,Chen Change Loy, and Dahua Lin. MMDetection: Open MMLab detection toolbox and benchmark.arXiv preprint arXiv:1906.07155, 2019.",
  "International Conference on Machine Learning, 2010": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An Imperative Style, High-Performance DeepLearning Library. In Advances in Neural Information Processing Systems, 2019. H. Pirsiavash, D. Ramanan, and C. Fowlkes.Globally-Optimal Greedy Algorithms for Tracking aVariable Number of Objects. In Conference on Computer Vision and Pattern Recognition, pp. 12011208, June 2011.",
  "A.1Comparison with Kalman Filter": "The original ByteTrack algorithm uses a Kalman Filter to estimate the motion of objects between frames.We aim to assess the contribution of the Kalman Filter to ByteTracks performance and compare it to ourproposed motion estimation method. 0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.50 0.55 0.60 0.65 0.70 0.75 MOTA (%) ByteTrackOursByteTrack w/o Kalman Filter Figure A.1: Comparison with Kalman Filter: In our adaptation of ByteTrack, we substitute the KalmanFilter with the motion oset predicted by our method. This comparison demonstrates that the Kalman Filterexerts a negligible inuence on ByteTrack, oering slight enhancements at high frame rates while providingno benet or even impairing performance at lower frame rates. In contrast, our approach markedly enhancesperformance at low frame rates. To this end, we present results at various frame rates for a modied version of ByteTrack where the KalmanFilter has been removed.In other words, new detections are directly matched to the head of existingtrajectories. The results of this modied model can be found in Figure Fig. A.1. Surprisingly, the motion estimatedthrough the Kalman Filter only has a marginal eect on the tracking performance. At high frame rates (>7.5 FPS), MOTA slightly increases, while at lower frame rates, it has no eect or is detrimental. Our weaklysupervised method for motion estimation performs signicantly better than the Kalman Filters.",
  "A.2Additional Motion Baselines": "As discussed in the limitations section of the main article, the proposed method for training a motionpredictor with detection supervision can have multiple global minima. Any displacement map that mapsall motion at timestep t to a unique detection at timestep t + 1 would result in Lmot = 0. In practice,when combined with the smoothing term Lse and the time consistency term Lfb, we observe that the model",
  "converges to predicting the actual motion.We argue that, considering time consistency and given theprovided data, it is easier for the model to learn the true motion than any other solution": "While the results in of the main paper already demonstrate the quality of the predicted displacements,we propose to compare the motion estimated by our model to two other baselines that could minimize theloss Lmot = 0. 0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.50 0.55 0.60 0.65 0.70 0.75 MOTA (%) ByteTrackOursMotion to Nearest Figure A.2: A Simple Motion Baseline: We compare our motion prediction to a basic baseline wherethe motion from one timestep to the next is estimated by identifying the nearest detection. Utilizing thismotion in place of the Kalman lter in ByteTrack results in the green curve depicted above. Surprisingly,this straightforward baseline outperforms ByteTrack at low FPS; however, it is disadvantageous at high FPS.Our methods outperforms this baseline in both scenarios.",
  "First, we will test a baseline that estimates motion such that it maps a detection at time t to the nearestdetection at time t + 1. The results for this baseline can be found in Fig. A.2": "Surprisingly, such a simple baseline outperforms ByteTrack at low frame rate regimes (<3 FPS) and un-derperforms at higher FPS. When comparing the proposed motion estimation method to this baseline, theweakly supervised motion greatly outperforms the baseline. This would tend to conrm that the proposedmodel is not merely learning to map detections to their nearest neighbors.",
  "A.2.2Motion from Bipartite Matching": "One could argue that the previous baseline does not guarantee Lmot to be zero, since multiple detectionscan be mapped towards the same detection, which is indeed a valid concern.Therefore, we propose asecond baseline that guarantees reaching a global minimum of Lmot. We suggest estimating motion usinga bipartite matching method to match detections from time t with those from time t + 1, utilizing theHungarian algorithm. The results can be found in Fig. A.3. Contrary to expectations, the bipartite matching baseline performsworse than the nearest neighbor approach. This is likely due to the one-to-one matching constraint, which,when applied to noisy detections, can lead to detections being wrongly matched with distant ones, resulting inlarge motion errors. Even adding a cuto distance does not signicantly improve performance. Nonetheless,the bipartite matching baseline is slightly benecial in very low frame rate scenarios (<2 FPS) but detrimentalotherwise. When compared to the proposed approach, its performance is signicantly lower, conrming thatthe proposed model is not learning a form of bipartite matching but is more likely capturing the true motion.",
  "A.3Tracking on the Groundplane": "Our method utilizes the ground plane to predict motion and associate detections at low frame rates. Toassess its contribution to the overall performance of our model, we propose comparing it with a groundplane-adapted version of ByteTrack. The association step in ByteTrack is modied as follows: Detections are projected onto the ground plane:The bottom center of the bounding box is projected onto the ground plane using the frame homography H.This ground point serves as the center of a square ground bounding box, with a side length of 25. Theseground bounding boxes are used to compute similarity scores, which replace ByteTracks original similarityscores. Results for this baseline can be found in Fig. A.4. Below 5 FPS, ByteTrack Ground signicantly outperforms ByteTrack, which is expected since operating onthe ground plane mitigates the eects of perspective projection, thereby making motion and speed uniformacross the scene and simplifying association. Above 5 FPS, it performs worse than ByteTrack, which maybe attributed to noise in the detections and inaccuracies in ground plane projection.",
  "A.4Detection Features": "In the single-view scenario, our motion prediction model is trained solely using the detection map. To enrichthe scene representation, the input map can be augmented by appending convolutional features correspondingto each detection onto their channel dimension. To achieve this, we utilize a pretrained OSNet (53) to extractfeatures with a dimensionality of 512 for each detection bounding box. Its important to note that thesefeatures are solely provided as input to the model and are not utilized to measure the similarity betweendetections.",
  "A.5Time Interval": "Given that our model processes pairs of frames and aims to predict accurate motion across a broad rangeof frame rates, we explore dierent strategies for sampling training pairs. In Table A.1, we present resultsfrom training pairs sampled with intervals ranging from [1 2[ to [8 13[. These ndings indicate thatintervals more closely aligned with the target frame rate lead to superior outcomes. Consequently, for allour experiments, we select the training sampling interval based on the desired target frame rate. Table A.1: Ablation Results on MOT17: We evaluate two additional elements of our training process:the interval between sampled frame pairs during training and the inclusion of detection features in the inputmap. Incorporating detection features appears advantageous across all scenarios. Moreover, the trainingframe interval achieves optimal results when it closely mirrors the inference regime. A frame interval of 1corresponds to 30 FPS, whereas an interval of 10 equates to 3 FPS.",
  "ByteTrack 30val77.084.177.5Ours30val76.684.176.1ByteTrack + FRCNN30val66.280.471.0Ours + FRCNN30val65.780.467.8": "0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.45 0.50 0.55 0.60 0.65 MOTA (%) MOTA vs. Low Framerate on MOT17 - Faster R-CNN Detector ByteTrackOurs Figure A.5:Tracking results in low FPS scenario.When using a Faster-RCNN object detector the proposedapproach outperform the Bytetack approach.The lowerthe frame rate the higher the performance gap, showing thebenet of the proposed approach. Quantitative resultsThe results are shown in Table A.2 and Fig. A.5. As expected, both ByteTrack +Faster R-CNN and Our + Faster R-CNN perform worse than their counterparts using YOLOx. Nonetheless,our approach, even when combined with Faster R-CNN, surpasses ByteTrack + Faster R-CNN at most framerates. These results are consistent with those obtained using the YOLOx detector and demonstrate that theproposed approach performs reliably across dierent object detectors.",
  "In addition to tracking performance, we provide an analysis of the memory usage of our approach for bothtraining and inference on the MOT17 dataset. Results can be found in Table A.3": "During training, the memory consumption of the dierentiable reconstruction step grows quadratically withrespect to the dimension of the space being reconstructed. In our implementation, this is managed usinga sliding window mechanism. During inference, the reconstruction step is not needed, and the memoryoverhead of our approach is minimal. Table A.3: Memory Consumption Training and inference GPU memory consumption for ByteTrack(YOLOx) and our method (motion prediction). Tests were run on an NVIDIA V100 with a batch size of 1during inference and 2 during training. When training our method, YOLOx is kept frozen, so its memoryfootprint is not included. During inference, the memory consumption of ByteTrack (YOLOx) is added tothe memory footprint of our method.",
  "BAdditional MOT17 results": "Due to the private nature of the test set and the need to conduct experiments at various frame rates, resultsin the main paper are conducted exclusively on a validation set, obtained by splitting the original trainingdata in half. The rst half is used for training and the second one for validation. For more informationabout the data split, see (52). For completeness, we provide results at 30FPS on the private test set. Results can be found in Table A.4.The results on the test set are consistent with those on the validation set and are on par with the originalByteTrack (52). Note that at 30FPS, the overlap of consecutive objects in a trajectory is large and themotion is small. In such scenarios, there is little benet in using motion; nonetheless, our method is able topreserve the original performance. Table A.4: Tracking performance on MOT17 test set with private detectors. At high FPS, predict-ing motion is of limited interest, nonetheless our method obtains results on par with the original ByteTrack.",
  "ByteTrack 30val70.778.670.5Ours30val70.678.669.8": "0.4 0.50.71.02.03.05.06.0 7.5 10.015.030.0 Framerate (FPS) 0.2 0.3 0.4 0.5 0.6 0.7 MOTA (%) MOTA vs. Low Framerate on MOT20 ByteTrackOurs Figure A.6: Tracking results in low FPS scenario. Inlow frame rate scenarios our model outperforms the Byte-Track baseline in term of MOTA. The lower the frame ratethe higher the performance gap, showing the benet of theproposed approach. MOT20is a single-view, multi-person detection and tracking dataset comprising sequences captured byboth static and moving cameras. The sequence lengths vary from 429 to 3315 frames, with frame rates of25 fps, totaling 13410 frames. The sequence represent busy scene and can contains up to 1211 tracks. Everyindividual is annotated with a bounding box. For our method, we utilize the groundplane homographies Hc",
  "determined in Dendorfer et al. (14) to map into the ground-plane. Since the test set is private we follow thetrain/val split of MMDetection": "Quantitative resultsResults can be found in Table A.5 and Fig. A.6. Our approach surpasses the originalYOLOX + ByteTrack at most frame rates, thus demonstrating the advantage of learning to represent motion.In low frame rate scenarios, it signicantly outperforms the baselines: at 2FPS, MOTA is improved by morethan 10%.Those results are in line with the results on MOT17 and show that the proposed approachperforms consistently across datasets with various crowd densities.",
  "F.2Tracking": "Previous methods have utilized the MultiviewX dataset solely for evaluating multiview detection tasks. Weextend its use to include tracking evaluation. Employing the same evaluation protocol as in the WILD-TRACK dataset, we report our ndings in Table A.8.Utilizing publicly available code, we establish abaseline for the MVFlow method (16). Our model demonstrates a signicant improvement over MVFlow onthe MultiviewX dataset."
}