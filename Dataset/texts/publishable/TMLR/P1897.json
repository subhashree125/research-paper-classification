{
  "Abstract": "Adjustable hyperparameters of machine learning models typically impact various keytrade-offs such as accuracy, fairness, robustness, or inference cost. Our goal in this paperis to find a configuration that adheres to user-specified limits on certain risks while beinguseful with respect to other conflicting metrics.We solve this by combining BayesianOptimization (BO) with rigorous risk-controlling procedures, where our core idea is to steerBO towards an efficient testing strategy. Our BO method identifies a set of Pareto optimalconfigurations residing in a designated region of interest.The resulting candidates arestatistically verified, and the best-performing configuration is selected with guaranteed risklevels. We demonstrate the effectiveness of our approach on a range of tasks with multipledesiderata, including low error rates, equitable predictions, handling spurious correlations,managing rate and distortion in generative models, and reducing computational costs.1",
  "Introduction": "Deploying machine learning models in the real-world requires balancing different performance aspects suchas low error rate, equality in predictive decisions (Hardt et al., 2016; Pessach & Shmueli, 2022), robustnessto spurious correlations (Sagawa et al., 2019; Yang et al., 2023), and model efficiency (Laskaridis et al.,2021; Menghani, 2023). In many cases, we can influence the models behavior favorably via hyperparametersthat determine the model configuration. However, selecting a configuration that accurately aligns with user-specified requirements on test data can be particularly challenging. This complexity is amplified when theprocess involves numerous possible configurations that demand significant resources to evaluate, such asthose necessitating the retraining of large neural networks for novel settings. Bayesian Optimization (BO) is widely used for efficiently selecting configurations of functions that requireexpensive evaluation, such as hyperparameters that govern the model architecture or influence the trainingprocedure (Shahriari et al., 2015; Wang et al., 2022; Bischl et al., 2023). The basic concept behind BO isto substitute the costly function of interest with a cheap, easily optimized probabilistic surrogate model.This surrogate is then used to select promising candidate configurations while balancing exploration and",
  "Published in Transactions on Machine Learning Research (10/2024)": "Additional Results.We first show that the constraints are satisfied in all cases by both GuideBO andthe baselines in Fig. E.1. In addition, we show the objective values obtained by the different methods asa function of the budget (see Fig. E.2) and present the budget required to achieve specific objective levels(see Fig. E.3). The results show that GuideBO consistently outperforms the baselines. Furthermore, weexplore the influence of varying the budget N in comparison to a dense uniform grid (with 1000 points) inFig. E.4. We show that N = 100 is sufficient to match (and sometimes outperform) the performance of thedense grid, highlighting the computational advantage of the proposed method. Furthermore, we conductedfive additional experiments for the early-time classification task using various structured time-series datasetswith large hyperparameter spaces ranging from 8 to 12 dimensions, demonstrating again that GuideBO ispreferable over the baselines (see Table D.1 and Fig. E.5). We also examine the influence of the parameter in Fig. E.6, showing that the method is generally insensitive to . Moreover, Fig. E.7 shows that using theproposed region is preferable over a single-sided upper bound at , implying that it is important to excludeinefficient configurations. Finally, we present examples of GuideBOs outcomes in Fig. E.8, highlightingits effectiveness in identifying relevant configurations within the defined region of interest, as opposed torecovering the entire front. Limitations and future work. While our proposed method establishes an efficient mechanism for selectingrisk-controlling model configurations and improves upon previous work, it has some limitations.Multi-fidelity optimization is a popular method for hyperparameter optimization, where resources are allocatedefficiently (Li et al., 2018; 2020). In this approach, additional resources (e.g., more epochs) are allocatedto promising configurations that performed well with fewer resources, while configurations that showedpoorer performance are discarded. Our current method cannot be directly applied in this setting. Moreover,the calibrated selection procedure requires splitting the data between the validation set used for selectingthe subset of promising configurations, and the calibration set used for their verification. Although this iscommonly done in other conformal prediction and risk control methods (Angelopoulos et al., 2021; Bai et al.,2022; Ringel et al., 2024), in many practical settings there is only limited available data. Another issue isthat there might be a distribution shift between the validation/calibration data and test data, and thus theselected configuration might not be risk-controlling with respect to shifted test data distributions (Gibbs &Cands, 2024; Zollo et al., 2024). These challenges should be explored in future work.",
  "Contribution. Our main ideas and results can be summarized as follows:": "1. We introduce the region of interest in the objective space, which significantly reduces the search space forcandidate configurations, thereby leading to more efficient statistical testing with fewer computations. 2. We define a new BO procedure to identify configurations that are Pareto optimal and lie in the definedregion of interest. These configurations are subsequently validated through statistical testing. 3. Our approach facilitates risk-controlled model selection in complex and costly settings that necessitatemodel retraining or involve extensive configuration spaces. We present a broad range of problems, whereour approach can be valuable for valid control and effective optimization of diverse performance aspects,including classification fairness, predictive robustness, generation capabilities, model compression andruntime reduction.",
  "Related work": "Conformal prediction and risk control.Conformal prediction is a popular model-agnostic anddistribution-free uncertainty estimation framework that returns prediction sets or intervals containing thetrue value with high probability (Vovk, 2002; Vovk et al., 2015; 2017; Lei et al., 2013; 2018; Gupta et al.,2020; Barber et al., 2021). Coverage validity, provided by standard conformal prediction, has recently beenextended to controlling general statistical losses, allowing guarantees in expectation (Angelopoulos et al.,2022) or with user-defined probability (Bates et al., 2021).Our contribution extends the foundationalwork of Angelopoulos et al. (2021), which addresses the broader scenario of controlling multiple riskfunctions. This is achieved by selecting an appropriate low-dimensional hyperparameter configuration usingmultiple hypothesis testing (MHT). Additionally, we draw upon the recently introduced Pareto Testingmethod (Laufer-Goldshtein et al., 2023) that further improves computational and statistical efficiencyby solving a multi-objective optimization (MOO) problem and focusing the testing procedure over theapproximated Pareto optimal set. In this paper, we point out that recovering the entire Pareto front isredundant and costly and suggest instead to recover a focused part of the front that is aligned with thepurpose of efficient testing. This enables highly-expensive hyperparameter tuning that involves retrainingof large models with a limited compute budget. Black-box model selection under multiple objectives. The demand for automating the creation of ma-chine learning pipelines is rapidly increasing. Hyperparameter optimization aims to streamline this process byfine-tuning model configurations with minimal manual intervention. However, this is a challenging black-boxoptimization problem that often involves trade-offs among factors such as predictive performance (Schmuckeret al., 2021), computational time (Wang et al., 2019; Lu et al., 2019), and fairness (Pfisterer et al., 2019;Martinez et al., 2020). In this context, we focus on model selection with multiple objectives, incorporatinguser-specified statistical constraints on certain losses. Bayesian Optimization (BO).BO is a commonly used sequential model-based optimization techniqueto efficiently find an optimal configuration for a given black-box objective function (Shahriari et al., 2015;Frazier, 2018; Wang et al., 2022). It can be applied to constrained optimization problems (Gardner et al.,2014) or multi-objective scenarios involving several conflicting objectives (Karl et al., 2022).However,when used in model hyperparamaeter tuning, the objective functions can only be approximated throughvalidation data, resulting in no guarantees on test time performance.To account for that we resort tostatistical testing, and utilize the effectiveness of BO to efficiently explore the configuration space andidentify promising candidates for testing.",
  "Problem formulation": "Consider an input X X and an associated label Y Y drawn from a joint distribution pXY PXY 2.We learn a model f : X Y, where Rn is an n-dimensional hyperparameter that determines themodel configuration. The model weights are optimized over a training set Dtrain by minimizing a given lossfunction, while the hyperparameter determines different aspects of the training procedure or the finalsetting of the model. For example, can weigh the different components of the training loss function, affectthe data on which the model is trained, or specify the final mode of operation in a post-processing procedure. We wish to select a model configuration according to different, often conflicting performance aspects, suchas low error rate, fairness across different subpopulations and low computational costs. In many practicalscenarios, we would like to constrain several of these aspects with pre-specified limits to guarantee a desirableperformance in test time. Specifically, we consider a set of objective functions of the form : PXY R.We assume that there are c constrained objective functions 1, ...., c, where i() = EpXY [Li(f(X), Y ; )]and Li : Y Y R is a loss function. In addition, there is a free objective function free defining a singledegree of freedom for minimization. The selection of is carried out based on two disjoint data subsets: (i)a validation set Dval = {Xi, Yi}ki=1 and (ii) a calibration set Dcal = {Xi, Yi}k+mi=k+1. We will use the validationdata to identify a set of candidate configurations, and the calibration data to calibrate the identified set. Theconstraints are formulated by an (, )-risk control criterion as was defined by Angelopoulos et al. (2021):",
  "PDcal (i() i) 1 , i {1, . . . , c},(1)": "where i is the upper bound of the i-th objective function, and is the desired confidence level, both selctedby the user. Note that the probability in (1) is defined over the randomness of the calibration data Dcal,namely if = 0.1, then the selected configuration will satisfy the constraints at least 90% of the time acrossdifferent calibration datasets. We provide here a brief example of our setup in the context of algorithmic fairness and derive additionalapplications in 6.In many cases, we wish to increase the fairness of the model without significantlysacrificing performance. For example, we would like to encourage similar true positive rates across differentsubpopulations, while constraining the expected error. One approach to enhancing fairness involves addingfairness-promoting terms to the standard cross-entropy loss function (Lohaus et al., 2020; Padh et al., 2021;Chuang & Mroueh, 2020). In this case, contains the weights assigned to each loss term, which are thencombined to form the overall training loss.Adjusting these weights leads to different accuracy-fairnesstrade-offs for the resulting model. Our goal is to select a configuration that optimizes fairness, whileguaranteeing that the overall error would not exceed a certain limit with high probability.",
  "In our method, two critical components play a central role:optimization of multiple objectives andstatistical testing for configuration selection. We hereby provide a short overview on these topics": "Multi-Objective Optimization (MOO). Consider an optimization problem over a vector-valued func-tion () = (1(), . . . , d()) consisting of d objectives. When dealing with conflicting objectives, there is nosingle optimal solution that simultaneously minimizes all objectives. Instead, there is a set of optimal configu-rations representing different trade-offs among the given objectives. This is the Pareto optimal set, defined by:",
  "p = { : { : , = } = },(2)": "where denotes that dominates if for every i {1, . . . d}, i() i(), and for somei {1, . . . d}, i() < i().Accordingly, the Pareto optimal set consists of all points that are notdominated by any point within . Given an approximated Pareto front P, a common quality measure isthe hypervolume indicator (Zitzler & Thiele, 1998) defined with respect to a reference point r Rd:",
  "where H( P; r) = {z Rd : p P : p z r} and 1H( P,r) is the Dirac delta function that equals 1": "if z H( P; r) and 0 otherwise. An illustration is provided in Fig. B.1. The reference point defines theboundaries for the hypervolume computation. It is usually set to the nadir point that is defined by the worstobjective values, so that all Pareto optimal solutions have positive hypervolume contributions (Ishibuchiet al., 2018). For example, in model compression with error and cost as objectives, the reference point canbe set to (1.0, 1.0), since the maximum error and the maximum normalized cost equal 1.0. The hypervolumeindicator measures both the individual contribution of each solution to the overall volume, and the globaldiversity, reflecting how well the solutions are distributed. It can be used to evaluate the contribution of anew point to the current Pareto front approximation, defined as the Hypervolume Improvement (HVI):",
  "HV I((), P; r) = HV (() P; r) HV ( P; r).(4)": "where () P denotes the extended set consisting of the current approximated Pareto front and a newpoint (). The expression in (4) represents the increase in hypervolume achieved by adding a new point(). The hypervolume indicator serves both as a performance measure for comparing different algorithmsand as a score for maximization in various MOO methods (Emmerich et al., 2005; 2006; Bader & Zitzler,2011; Daulton et al., 2021). BO.BO is an effective method for optimizing black-box objective functions that are costly to evaluate(e.g. selecting the hyperparameters of large neural networks). These methods employ a surrogate model toapproximate the expensive objective function and iteratively select new configurations using an acquisitionfunction that balances exploration and exploitation. Formally, we assume a total budget of N iterations,representing the maximum number of allowed function evaluations. We start with an initial set of randomconfigurations C0 = {0, . . . , N0} and their associated objective values L0 = {(1), . . . , (N0)}, whereN0 < N.We then perform N N0 iterations, each time adding one configuration to the current setCn, 1 n N N0, where |Cn| denotes the set size. Commonly, a Gaussian Process (GP) (Williams& Rasmussen, 2006) serves as a surrogate model, providing an estimate with uncertainty given by theGaussian posterior.We assume a zero-mean GP prior g() N (0, k(, )), characterized by a kernelfunction : R.Let R|Cn| denote the vector of covariance values i = (, i) betweena new point and a given point j Cn, and K a |Cn| |Cn| matrix with elements Kij = (i, j),i, j {1, . . . , |Cn|}. In addition, we define the label vector q R|Cn| consisting of the given objective valuesqi = (i), i {1, . . . , |Cn|}. Based on these definitions, the posterior distribution of the GP is given byp(g|, Cn, Ln) = N ((), (, )), with () = (K +2I)1q and (, ) = (, )T K + 2I1 .Here 2 is the observation noise variance, i.e. (i) N(g(i), 2). The next configuration for evaluation,is selected by optimizing an acquisition function defined on top of the surrogate model. Common acquisitionfunctions include: probability of improvement (PI) (Kushner, 1964), expected improvement (EI) (Mokus,",
  "Method": "Our approach involves two main steps: (i) performing BO to generate a small set of potential configurations,and (ii) applying MHT over the candidate set to identify valid configurations. Considering the shortcomingsof Pareto Testing, we argue that the two disjoint stages of optimization followed by testing are suboptimal,especially for resource-intensive MOO. As an alternative, we propose adjusting the optimization procedurefor better testing outcomes by focusing only on the most relevant parts in the objective space. To accomplishthis, we need to (i) specify a region of interest guided by our testing goal, and (ii) establish a BO procedurecapable of effectively identifying configurations within the defined region. In the following we describe thesesteps in details.",
  "Defining the Region of Interest": "We aim to define a region of interest within the objective space Rc+1, where we seek to identify candidateconfigurations that are likely to be valid and efficient during the process of MHT. We start with the caseof a single constraint (c = 1). Recall that in the testing stage we define the null hypothesis H : () > for a candidate configuration , and compute a p-value for a given empirical loss over the calibration datacal() = 1 mk+mj=k+1 (Xj, Yj; ). A valid p-value p has to be super-uniform under the null hypothesis, i.e.P (p u) u, for all u . As demonstrated in (Angelopoulos et al., 2021), a valid p-value can becomputed based on concentration inequalities that quantify the proximity of the sample loss to the expectedpopulation loss. If the loss is bounded by 1, we can apply Hoeffdings inequality to derive the followingp-value (see Appendix B.1):",
  "2m.(6)": "As an example, consider the error rate as a loss function, and assume that we would like to bound the errorrate by 5% ( = 0.05), with a significance level of = 0.1. By (6), if the empirical loss of a calibration setof size m = 5000 is up to max = 4%, then we have enough evidence to declare that this configuration issafe and its error will not exceed 5% on new unseen data drawn from the same distribution. In the BO procedure, we are interested in identifying configurations that are likely to be both valid andefficient.On the one hand, in order to be valid the loss must not exceed max.On the other hand,from efficiency considerations, we would like to minimize the free objective as much as possible.Thismeans that the constrained loss should be close to max (from bellow) due to the inverse relation betweenthe free objective and the constrained objective.An illustration demonstrating this idea is providedin , where the irrelevant regions are: (i) the brown part on the right where the configurations arenot satisfying the constraint, and (ii) the green part on the left where the configurations are not effectivelyminimizing 2. Ideally, we would like to find configurations with expected loss equal to the limiting testingthreshold max. However, during optimization we can only evaluate the loss over a finite-size validationdata with |Dval| = k samples.To account for that, we construct an interval [low, high] around max based on the size of the validation data. In this region, we wish to include empirical loss values that arelikely to correspond to an expected value of max based on the evidence provided by the validation data.Let opt1() =1kkj=1 1(Xj, Yj; ) denote the empirical loss computed over Dval. We define the regionR(, k, m, , ) containing opt1() values that are likely to be obtained under 1() = max with at least1 2 probability. Specifically, there exists a region R(, k, m, , ), with (0, 0.5], such that:",
  ".(8)": "Note that setting is an empirical decision that is independent of both the MHT procedure and the chosensignificance level . For small the region expands, accommodating more optional configurations but witha lower density. Conversely, a larger produces a smaller region, leading to denser sampling around thelimiting value. Also note that, whenever k increases, the width of the region decreases, reflecting a growingconfidence that the observed losses are representative of actual expected loss. In practice, we use the tighterHoeffding-Bentkus inequality for the p-value computation in Eq. (5) and for defining the region of interestby Eqs. (6) and (8) (see Appendix B.1). In the case of multiple constraints, the null hypothesis is defined as H : i where i() > i, i.e. that atleast one of the constraints is not satisfied. A valid p-value is given by p = maxi{1,...,c} p,i, where p,i isthe p-value corresponding to the i-th constraint (see Appendix. B.2). Consequently, we define the region ofinterest in the multi-constraint case as the intersection of the individual regions (as illustrated in Fig. B.2):",
  "Local Hypervolume Improvement": "Given our definition of the region of interest, we derive a BO procedure that recovers Pareto optimal pointsin the intersection of R(, k, m, , ) and P. Our key idea is to use the HVI in Eq. (4) as an acquisitionfunction, while modifying it to capture only the region of interest. To this end, we properly define thereference point r Rc+1 to enclose the desired region.",
  "n = arg maxHV I(g(), P; r),(13)": "which leads to recovering only the relevant section, rather than the entire front. We evaluate the objectivefunctions on the new selected configuration, and update our candidate set accordingly. The BO processcontinues iterating until the maximum budget N is reached. The resulting set of candidate configurationsis denoted as CBO. Our proposed BO method, GuideBO, is summarized in Algorithm 1 and illustrated in for c = 1.",
  ":return CBO": "Note that in MOBO it is common to use an HVI-based acquisition function that also takes into accountthe predictive uncertainty as in EHVI (Emmerich et al., 2005) and SMS-EGO (Ponweiser et al., 2008).However, our preliminary runs showed that these approaches do not work well in the examined scenarioswith small budget (N ), as they often generated points outside the region of interest. Similarly,for these scenarios the random scalarization approach, proposed in (Paria et al., 2020), was less effective forgenerating well-distributed points inside the desired region.",
  "Testing the Final Selection": "We follow (Angelopoulos et al., 2021; Laufer-Goldshtein et al., 2023) for testing the selected candidateset.Prior to testing we filter and order the configurations in the set CBO.Specifically, we retain onlyPareto optimal configurations from CBO, and then sort the remaining configurations by increasing p-values,approximated by Dval. Next, we recompute the p-values based on Dcal and perform FST, where we starttesting from the first configuration and continue until the first time the p-value exceeds . As a result, weobtain the validated set Cvalid, and choose a configuration minimizing the free objective:",
  "The method is summarized in Algorithm C.1.As a consequence of (Angelopoulos et al., 2021; Laufer-Goldshtein et al., 2023) we achieve a valid risk-controlling configuration, as we now formally state": "Theorem 5.1. Let Dval = {Xi, Yi}ki=1 and Dcal = {Xi, Yi}k+mi=k+1 be two disjoint datasets.Suppose thep-value p, derived from Dcal, is super-uniform under H for all . Then the output of Algorithm C.1satisfies Eq. (1). The proof is provided in Appendix B.3.Note that in situations where we are unable to identify anystatistically valid configuration (i.e., Cvalid = ), we set = null.To avoid this situation, the usershould select limits 1, . . . , c that are likely to be feasible.In practice, this can be achieved usingthe initial pool of configurations C0, which is generated at the start of the BO procedure.This repre-sentative set provides an indication of the possible achievable limits.Specifically, the user may selecti [minC0 i(), maxC0 i()], i {1, . . . , c}, and can further refine this choice during the BO iter-ations as more function evaluations are accumulated.",
  "We demonstrate the effectiveness of GuideBO across various tasks with diverse objectives. In each setting,the definition of varies, affecting the model differently either during or after training": "Classification Fairness. In many classification tasks, it is important to take into account the behavior ofthe predictor with respect to different subpopulations. Assuming a binary classification task and a binarysensitive attribute a = {1, 1}, we consider the Difference of Demographic Parity (DDP) as a fairnessscore (Wu et al., 2019):DDP(f) = E1f(x)>0|a = 1 E1f(x)>0|a = 1.(15)",
  "We define the following loss, parameterized by = , which consists of two regularization terms thatprompt fairness:R(f; ) = BCE(f) + 1 DDP(f) + 2 MixUP(f),(16)": "where BCE(f) is the standard binary cross-entropy loss, and DDP(f) and MixUP(f) reguralize the modelsfairness.DDP(f) is the hyperbolic tangent relaxation of (15) (Padh et al., 2021), andMixUP(f) is amixup regularization interpolating samples between groups (Chuang & Mroueh, 2020) (see Appendix Dfor further details). Changing the values of leads to different models that trade-off accuracy for fairness.In this setup, we have a 2-dimensional hyperparamter and two objectives: (i) the error of the modelerr() = E1f(X)=Y, and (ii) the DDP defined in Eq. (15) ddp() = DDP(f). Classification Robustness.Predictors often rely on spurious correlations found in the data (suchas background features), which leads to significant performance variations among different subgroups.Recently, Izmailov et al. (2022) demonstrated that models trained using expected risk minimizationsurprisingly learn core features in addition to spurious ones. Accordingly, they proposed to enhance modelrobustness by retraining the final layer on a balanced dataset.We modify their approach to generatedifferent configurations, balancing the trade-off between robustness to differences in subpopulations andoverall performance across the entire population. Given a dataset D (either the training set or a part of the validation set), we define a parameterized datasetD as follows. Suppose the data consists of samples (X, Y, G), where G G is the group label and G is theset of all groups present in the data. We denote by a |G|-dimensional hyperparameter combination thatlies in the |G|-1 probability simplex. The vector consists of the probabilities of each group appearing inD. To create D, we first sample the group membership label according to , and then uniformly samplean example from the chosen group, allowing for repetition in sampling. Consequently, is a |G|-dimensionalhyperparameter that controls the proportion of each group in D. The dataset is considered evenly balancedacross groups when all probabilities are equal to 1/|G|. It is equivalent to the original dataset when matchesthe prior probability over G. We define two objective functions: (i) the average error err() = E1f(X)=Y,and (ii) the worst error over all subgroups worst-err() = maxgG E1f(X)=Y |G = g. Robust and Selective Classification. We also examine the case of selective classification and robustness.The selective classifier can abstain from making a prediction when it is unsure (Geifman & El-Yaniv, 2017).Specifically, the model abstains when its confidence is lower than a threshold , i.e.f(x) < , wheref(x) denotes the probability of the predicted class.This approach can potentially enhance predictionperformance by trading-off coverage, defined as the proportion of the population for which the classifierprovides a prediction.In this case, we have a |G| + 1-dimensional hyperparameter = (, ) and anadditional objective function of the mis-coverage rate mis-cover() = E1f(x)<. VAE.Variational Autoencoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) are generativemodels that leverage a variational approach to learn the latent variables underlying the data, and cangenerate new samples by sampling from the latent prior distribution. We focus on a -VAE (Higgins et al.,2016), which balances the reconstruction error (distortion) and the Kullback Leibler (KL) divergence (rate):",
  "Early Time Classification.We adapt the early time classification scheme proposed by Ringel et al": "(2024) for predicting the label of a given input data stream as quickly as possible. Specifically3, we focuson employing LLMs for the task of reading comprehension, where the goal is to analyze a long document(given as context) and select an answer to a provided question. Let t(X) denote a heuristic confidencemeasure of the prediction made based on the input X received until time t (e.g. the maximum predictedprobability). We define the stopping-time for input X as (X) = {mint : t(X) t or t = tmax}. Inthis setting, the hyperparameter = (1, 2, . . . , tmax) consists of the thresholds for all possible stoppingtimes t = 1, . . . , tmax. We have two objectives: (i) the accuracy difference between the full-time predictionand the early-time prediction diff-acc() = E(1f(X)=Y 1f(X)=Y )+and (ii) the normalized halt timetime() = E [(X)/tmax].",
  "Experiments": "We describe the experimental setup and present our main results. Further experimental details, as well asadditional results are provided in Appendixes D and E, respectively. In the experiments we demonstratethe efficiency of the proposed method compared to baselines in two main ways: (i) For a fixed budget, weshow that GuideBO achieves the lowest values for the free objective function compared to baselines; (ii) Fora fixed level of the objective function, we show that GuideBO requires the smallest budget to achieve thatlevel. Additional ablation studies are conducted to highlight the robustness of GuideBO.",
  "Baselines": "We define several baselines. We emphasize that in the second testing stage, both GuideBO and the baselinesfollow the same testing procedure that guarantees risk control. The baselines differ only in their optimizationmechanisms during the first stage; thus, they can all be considered variants of Pareto Testing (Laufer-Goldshtein et al., 2023).We define two simple baselines and three multi-objective optimizers aimed atrecovering the full Pareto front:",
  "Adult32,5593,6184,5224,523CelebA162,77019,8679,9819,981MNIST50,00010,0005,0005,000AG News120,0002,5002,5002,600Quality-1,5371,5361,536": "Fairness.We use the Adult (Dua et al., 2017) dataset,which consists of samples of individuals with 14 featuresas an input. The goal is to predict whether their annualincome is above 50k$. Gender is considered as a sensitiveattribute.Robustness + Robust and selective classifica-tion. We use CelebA (Lin et al., 2019) and consider a bi-nary prediction task of whether a person has a blond hair.The spurious correlation is associated with the gender at-tribute, resulting in |G| = 4 groups: (blond, female), (blond, male), (non-blond, female), (non-blond, male).VAE. We use the MNIST dataset (LeCun, 1998), which consists of grayscale images of handwritten digits.Pruning.We use AG News (Zhang et al., 2015) dataset where the task is to predict the category (out offour options) of news articles based on their content.Early-Time Classification.We use the QuALITY dataset (Pang et al., 2022), which consists of tripletswith a question, multiple choice answers, and a long context, along with the corresponding correct choice.The long context is partitioned into tmax = 10 segments.",
  "Evaluation": "We emphasize again the purpose of each data split. The training data is used to learn the model parameters.The validation data is used for selecting candidate hyperparameter configurations, using either GuideBO(Algorithm 1) or the baseline procedures. The same data is also used for ordering the chosen configurationsbefore testing. The calibration data is used for the FST procedure over the ordered set of chosen configura-tions. The final selected (14) determines the model setup. Lastly, the performance of the selected modelis assessed on the test dataset. Since the same testing procedure is applied across all methods, the chosen configuration is guaranteed tosatisfy the specified constraints, as we verify empirically (see Fig. E.1). Therefore, our primary metrics forevaluating the efficiency of each method are: (i) its ability to minimize the free objective function within agiven budget, and (ii) its capacity to reach certain levels of the free objective with the least budget. We repeat the experiments with 5 random seeds over the optimization procedure. For each seed, we furthergenerate 20 random splits of calibration and test subsets. Accordingly, we obtain 5 20 = 100 randomtrials, and report the mean and standard deviation across all trials. For each task, we choose the valuesof according to the objective values obtained for the initially generated configurations. lists therange values for each objective. We select values that lie within the range defined by these extreme points,ensuring they are not too close to either boundary.This is because values that are too small may notbe statistically achievable, while excessively large values can be trivially satisfied, with tighter control notsignificantly improving the free objective. We set = 0.1 and = 0.01.",
  "(f) Early Classification": "Figure E.1: The values of the constrained objective functions across tasks and different limits (marked bydashed red lines). The objectives are evaluated over Dtest for the configuration that was chosen by eachmethod. All method satisfy the limits due to the testing procedure. Satisfying Constraints and Tighter Control. We show the values of the constrained objective functionsin Fig. E.1, where the red dashed lines depict the limit. We see that the constraints are satisfied by allmethods as expected, since in any case the configurations are validated through the testing procedure.Notably, GuideBO obtains tighter control compared to baselines in nearly all cases. This is consistent withour earlier finding that GuideBO better minimizes the free objective function. Varying Optimization Budget.In this experiment we fixed the constraint ( limit) for each task andvaried the optimization budget (number of iterations/function evaluations). We show the improvement inthe free objective function as a function of the optimization budget across tasks on Fig. E.2. It can be seenthat GuideBO delivers consistently strong performance in all scenarios, while the other baselines have mixedresults, excelling in some cases and underperforming in others. In addition, for each task we define threelevels of the free objective: between 90% of the maximum value and 110% of the minimum value obtainedby the different baselines, and order these levels from high to low value. We compare the budget that isrequired for each baseline to reach a certain level in all tasks and for all defined levels. Results are shown in",
  ": Rank count across settings and the averagerank.GuideBO is ranked first in almost all cases,while the baselines have an inconsistent performance": "Results are presented in showing the valuesof the free objective function, evaluated over testdata, across all tasks and levels. To summarizethe results, we rank the methods in each scenario,and report the counts of all rankings, as well as theaverage rank in . We observe that GuideBOconsistently outperforms all baselines in nearly allcases.The multi-objective baselines outperformthe simple baselines that distribute configurationsacross the entire space.Moreover, the baselinesexhibit inconsistent performance, delivering satis-factory results for certain tasks or specific values,but falling short in others. This inconsistency canbe attributed to the arbitrary distribution of theconfigurations for the baselines.As a result, wesometimes randomly obtain configurations that areclose to the testing limit (thus efficient), while atother times, the closest configuration is relativelyfar (thus inefficient). In contrast, GuideBO achieves a dense sampling of the relevant part of the Paretofront, leading to more precise and stable control across various conditions.",
  "Conclusion": "We introduce a versatile framework designed for reliable model selection.This framework is capable ofmeeting statistical risk limitations while simultaneously optimizing other conflicting metrics. We establisha confined region within the objective space that is a promising target for statistical testing. Our proposedmethod, referred to as GuideBO, is employed to pinpoint configurations that are Pareto optimal and lie inthe specified region. We statistically validate the set of candidate configurations using multiple hypothesistesting to achieve verified control guarantees. The broad applicability and effectiveness of our approach aredemonstrated for tuning different types of hyperparameters across various tasks and objectives, includinghigh-accuracy, fairness, robustness, generation and reconstruction quality and cost and time considerations.",
  "Syrine Belakaria, Aryan Deshwal, and Janardhan Rao Doppa. Max-value entropy search for multi-objectivebayesian optimization. In Advances in Neural Information Processing Systems, volume 32, 2019": "Syrine Belakaria, Aryan Deshwal, Nitthilan Kannappan Jayakodi, and Janardhan Rao Doppa. Uncertainty-aware search framework for multi-objective bayesian optimization. Proceedings of the AAAI Conferenceon Artificial Intelligence, 34(06):1004410052, 2020. Bernd Bischl, Martin Binder, Michel Lang, Tobias Pielok, Jakob Richter, Stefan Coors, Janek Thomas,Theresa Ullmann, Marc Becker, Anne-Laure Boulesteix, et al. Hyperparameter optimization: Founda-tions, algorithms, best practices, and open challenges. Wiley Interdisciplinary Reviews: Data Mining andKnowledge Discovery, 13(2):e1484, 2023.",
  "Dheeru Dua, Casey Graff, et al. Uci machine learning repository, 2017": "Michael Emmerich, Nicola Beume, and Boris Naujoks. An emo algorithm using the hypervolume measure asselection criterion. In International Conference on Evolutionary Multi-Criterion Optimization, pp. 6276.Springer, 2005. Michael TM Emmerich, Kyriakos C Giannakoglou, and Boris Naujoks. Single-and multiobjective evolu-tionary optimization assisted by gaussian random field metamodels. IEEE Transactions on EvolutionaryComputation, 10(4):421439, 2006. Matthias Feurer, Katharina Eggensperger, Edward Bergman, Florian Pfisterer, Bernd Bischl, and FrankHutter. Mind the gap: Measuring generalization performance across multiple objectives. In InternationalSymposium on Intelligent Data Analysis, pp. 130142. Springer, 2023. Carlos M Fonseca and Peter J Fleming. Multiobjective optimization and multiple constraint handling withevolutionary algorithms. i. a unified formulation. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 28(1):2637, 1998.",
  "Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. Advances inneural information processing systems, 29, 2016": "Daniel Hernndez-Lobato, Jose Hernandez-Lobato, Amar Shah, and Ryan Adams. Predictive entropy searchfor multi-objective bayesian optimization. In International conference on machine learning, pp. 14921501.PMLR, 2016. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, ShakirMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variationalframework. In International conference on learning representations, 2016.",
  "D Ienco and R Gaetano.Tiselac:time series land cover classification challenge. 2007": "Hisao Ishibuchi, Ryo Imada, Yu Setoguchi, and Yusuke Nojima. How to specify a reference point in hyper-volume calculation for fair performance comparison. Evolutionary computation, 26(3):411440, 2018. Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew G Wilson. On feature learning in the presenceof spurious correlations. Advances in Neural Information Processing Systems, 35:3851638532, 2022. Florian Karl, Tobias Pielok, Julia Moosbauer, Florian Pfisterer, Stefan Coors, Martin Binder, Lennart Schnei-der, Janek Thomas, Jakob Richter, Michel Lang, et al. Multi-objective hyperparameter optimizationanoverview. arXiv preprint arXiv:2206.07438, 2022.",
  "Benjamin Letham, Brian Karrer, Guilherme Ottoni, and Eytan Bakshy. Constrained bayesian optimizationwith noisy experiments. Bayesian Analysis, 14(2), 2019": "Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-Tzur, Moritz Hardt, Ben-jamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. Proceedingsof Machine Learning and Systems, 2:230246, 2020. Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Anovel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):152, 2018.",
  "Michael Lohaus, Michael Perrot, and Ulrike Von Luxburg. Too relaxed to be fair. In International Conferenceon Machine Learning, pp. 63606369. PMLR, 2020": "Zhichao Lu, Ian Whalen, Vishnu Boddeti, Yashesh Dhebar, Kalyanmoy Deb, Erik Goodman, and WolfgangBanzhaf. Nsga-net: neural architecture search using multi-objective genetic algorithm. In Proceedings ofthe genetic and evolutionary computation conference, pp. 419427, 2019. Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descentwith controlled ascent in pareto optimization.In International Conference on Machine Learning, pp.65976607. PMLR, 2020.",
  "Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks.arXiv preprint arXiv:2010.04104, 2020": "Kirtan Padh, Diego Antognini, Emma Lejal-Glaude, Boi Faltings, and Claudiu Musat. Addressing fairnessin classification with a model-agnostic multi-objective algorithm. In Uncertainty in Artificial Intelligence,pp. 600609. PMLR, 2021. Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, VishakhPadmakumar, Johnny Ma, Jana Thompson, He He, et al. Quality: Question answering with long inputtexts, yes!In 2022 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pp. 53365358, 2022. Biswajit Paria, Kirthevasan Kandasamy, and Barnabs Pczos. A flexible framework for multi-objectivebayesian optimization using random scalarizations. In Uncertainty in Artificial Intelligence, pp. 766776.PMLR, 2020.",
  "Vladimir Vovk, Ivan Petej, and Valentina Fedorova. Large-scale probabilistic predictors with and withoutguarantees of validity. In Advances in Neural Information Processing Systems (NeurIPS), 2015": "Vladimir Vovk, Jieli Shen, Valery Manokhin, and Min-ge Xie. Nonparametric predictive distributions basedon conformal prediction. In Proceedings of the Sixth Workshop on Conformal and Probabilistic Predictionand Applications, 2017. Tobias Wagner and Heike Trautmann. Integration of preferences in hypervolume-based multiobjective evolu-tionary algorithms by means of desirability functions. IEEE Transactions on Evolutionary Computation,14(5):688701, 2010. Bin Wang, Yanan Sun, Bing Xue, and Mengjie Zhang. Evolving deep neural networks by multi-objectiveparticle swarm optimization for image classification. In Proceedings of the genetic and evolutionary com-putation conference, pp. 490498, 2019. Handing Wang, Markus Olhofer, and Yaochu Jin. A mini-review on preference modeling and articulation inmulti-objective optimization: current status and challenges. Complex & Intelligent Systems, 3:233245,2017.",
  "Yunchuan Zhang, Sangwoo Park, and Osvaldo Simeone. Bayesian optimization with formal safety guaranteesvia online conformal prediction. arXiv preprint arXiv:2306.17815, 2023": "Eckart Zitzler and Lothar Thiele. Multiobjective optimization using evolutionary algorithmsa comparativecase study. In International conference on parallel problem solving from nature, pp. 292301. Springer,1998. Thomas P Zollo, Todd Morrill, Zhun Deng, Jake Snell, Toniann Pitassi, and Richard Zemel. Prompt riskcontrol: A rigorous framework for responsible deployment of large language models.In The TwelfthInternational Conference on Learning Representations, 2024.",
  "AAdditional related work": "Gradient-Based MOO.When dealing with differentiable objective functions, gradient-based MOOalgorithms can be utilized. The cornerstone of these methods is Multiple-Gradient Descent (MGD) (Sener& Koltun, 2018; Dsidri, 2012), which ensures that all objectives are decreased simultaneously, leading toconvergence at a Pareto optimal point. Several extensions were proposed to enable convergence to a specificpoint on the front defined by a preference vector (Lin et al., 2019; Mahapatra & Rajan, 2020), or learningthe entire Pareto front, using a preference-conditioned model (Navon et al., 2020; Lin et al., 2020; Chen &Kwok, 2022; Ruchte & Grabocka, 2021). However, this line of research focuses on differentiable objectives,optimizing the loss space used during training (e.g. cross entropy loss), which is typically different from theultimate non-differentiable metrics used for evaluation (e.g. error rates). Furthermore, it focuses on recov-ering a single or multiple (possibly infinitely many) Pareto optimal points, without addressing the actualselection of model configuration under specific constraints, which is the problem we tackle in this paper. Incorporating user preferences in MOO.Another area of research has explored integrating decisionmaker preferences into multi-objective optimization (Branke, 2016; Wang et al., 2017; Xin et al., 2018).These methods are typically classified based on the stage in which they are applied in the optimizationprocess: before, during, or after optimization (Branke & Deb, 2005). Preferences can be specified in variousways, such as using reference points representing an ideal solution (Deb & Sundar, 2006), imposing con-straints (Fonseca & Fleming, 1998), setting maximal or minimal trade-offs (Branke et al., 2001), and usingdesirability functions that non-linearly scale the objectives to (Wagner & Trautmann, 2010). Addition-ally, preferences can be learned by asking the decision maker to rank solutions or select the most preferredsolution from a set (Frnkranz & Hllermeier, 2010).Another interactive preference learning approachinvolves incorporating explanations that help the decision maker in understanding the trade-offs betweenobjectives (Misitano et al., 2022). In this paper, we focus on preferences provided by the user as hard limitson certain objectives, which require validation through statistical testing. Our BO procedure aims to extracta set of promising configurations that are designed to achieve tight control guarantees.",
  "B.2A valid p-value for multiple constraints": "We prove that taking the maximum p-value across constraints is a valid p-value for the combined hypothesis.Lemma B.1. Let p,i be a p-value for H,i : i() > i, for each i {1, . . . , c}. Define p := max1ic p,i.Then, for all such that H : i where i() > i holds, we have:",
  "Proof. Recall that Dval and Dcal are two disjoint, i.i.d. datasets. Therefore, Dcal is i.i.d. w.r.t the returnedconfiguration set optimized in Algorithm 1 over Dval": "We now prove that the testing procedure returns a set of valid configurations with FWER bounded by .Let H be the first true null hypothesis in the sequence. Given that p is a super uniform p-value underH, the probability of making a false discovery at is bounded by . This means that the event that His rejected (false discovery) occurs with probability lower than . However, if H fails to be rejected (nofalse discovery), then all other H that follow in the sequence also fail to be rejected (regardless of if H istrue or not). Therefore, the probability of making any false discovery is bounded by , which satisfies theFWER control requirement.",
  "Algorithm C.1 Configuration Selection": "Definitions: f is a configurable model set by an hyperparameter . Dval = {Xi, Yi}ki=1 and Dcal = {Xi, Yi}k+mi=k+1 aretwo disjoint subsets of validation and calibration data, respectively. {1, . . . , c} are constrained objective functions,and free is a free objective. {1, . . . , c} are user-specified bounds for the constrained objectives. is the configurationspace. is the tolerance.N is the optimization budget, and N0 is the size of the intial pool of configurations.ParetoOptimalSet() returns Pareto optimal points.",
  "We provide here further details on the datasets, application specifications, model architectures, trainingprocedures, and examined scenarios": "Initialization. For GuideBO, HVI and EHVI we randomly sample an initial set of size N0, and performN N0 iterations of BO. We use a uniform grid for n = 1 and LHS for n > 1. The values of N and N0 foreach task are provided in . For ParEGO we use the default initialization defined by the SMAC3implementation. Fairness. For computing DDP(f), the indicator 1f(x)>0 in Eq. (15) is relaxed using tanh(c max(0, f(x)))with c = 3 (Padh et al., 2021).In addition, we define a linear interpolation in the input space: xt =t x1 + (1 t) x1, for t where x1 and x1 represent samples with attributes a = 1 and a = 1,",
  "MixUP(f) = Et [|EX [xf(xt), x1 x1]|] ,(34)": "regularizing the expected inner product between the Jacobian on mixup samples and the difference x1 x1 (Chuang & Mroueh, 2020). Our model is a 3-layer feed-forward neural network with hidden dimensions. We train all models using Adam optimizer with learning rate 1e 3 for 50 epochs and batch size256. Robustness. We use a ResNet-50 model pretrained on ImageNet. Following (Izmailov et al., 2022), wetrain the models for 50 epochs with SGD with a constant learning rate of 1e 3, momentum decay of 0.9,batch size 32 and weight decay of 1e 4. We use random crops and horizontal flips as data augmentation.We use half of the CelebA validation data to train the last layer, and the other half for BO. VAE. We use the implementation provided by (Chadebec et al., 2022) of a ResNet-based encoder anddecoder, trained using AdamW optimizer with 1 = 0.91, 2 = 0.99, and weight decay 0.05. We set thelearning to 1e4 and the batch size to 64. The training process consisted of 10 epochs. We use binary-crossentropy reconstruction loss for training the model, and the mean squared error normalized by the totalnumber of pixels (728) as the reconstruction objective function for hyperparameter tuning. Pruning. We use a BERT-base model (Devlin et al., 2018) with 12 layers and 12 heads per layer. Wefollow the recipe in (Laufer-Goldshtein et al., 2023) and attach a prediction head and a token importancepredictor per layer. The core model is first finetuned on the task. We compute the attention head importancescores based on 5K held-out samples out of the training data. We freeze the backbone model and train theearly-exit classifiers and the token importance predictors on the training data (115K samples). Each prediction head is a 2-layer feed-forward neural network with 32 dimensional hidden states, and ReLUactivation. The input is the hidden representation of the [CLS] token concatenated with the hidden repre-sentation of all previous layers, following (Woczyk et al., 2021). Similarly, each token importance predictor is a 2-layer feed-forward neural network with 32 dimensionalhidden states, and ReLU activation. The input is the hidden representation of each token in the currentlayer and all previous layers (Woczyk et al., 2021). Early-Time Classification. We adapt the setup described in (Ringel et al., 2024), and use the processedmodel outcomes that appear in their implementation4. The context of each question is divided into sentences,which are grouped into tmax = 10 sets. The input sequence until time t is provided as a prompt that includesthe context sentences up to timestep t, along with the question and its four options, labeled A, B, C,and D. The prompt concludes with The answer is:\\n\\n. The prompt is processed by the Vicuna-13Bmodel. We provide additional results for structured time series data, following (Ringel et al., 2024). The detailsof all datasets are summarized in Table D.1. Each dataset is partitioned into four distinct parts: 80% fortraining and the remaining 20% are equally divided to validation, calibration and test subsets. For eachdataset we define an exist point every fixed number of timesteps (hop-size) and obtain a hyperparameterdimension n ranging from 8 to 12. A standard LSTM is used for feature extraction with one recurrent layerwith a hidden size of 32, except for WalkingSittingStanding where the model consists of 2 recurrent layers,each with a hidden size of 256. The output of the last recurrent layer is followed by two fully connectedclassification heads, one for classifying the label and the other for estimating the classification confidence.The loss consists of two terms: cross-entropy loss for the label, and binary cross entropy loss (weighted by0.2) for whether the classifier is correct or not. The models are trained with Adam optimizer, with a learningrate of 0.001, and a batch size of 64.",
  "(f) Early Classification ( = 0.05)": "Figure E.3: The budget that is required to achieve several different levels of the free objective functionacross different tasks. In nearly all cases (15 out of 18 scenarios), we find that GuideBO requires the smallestbudget compared to the baselines. Moreover, it often achieves levels that the other baselines cannot reach. Figure E.4:Results of the proposed method over AG News (pruning task) for different number ofevaluations, and with a grid of uniform thresholds. Accuracy reduction is controlled and cost is minimized. of all 3 hyperparmeters with a total of N = 1000 configurations. We see on Fig. E.4 that the relative costgradually improves with the increase in N. It reaches (and in some cases outperform) the dense grid baselinewith N = 100 (that is 10% decrease in budget). This indicates that using our proposed method we cansignificantly decrease the required budget without scarifying performance.",
  "Figure E.5: The values of the free objective functions across datasets and different limits for the task ofearly time classification. GuideBO is ranked first in 20 out of 24 cases": "Additional experiments for early-time classification task with different datasets. We conductedadditional experiments for the task of early time classification with different datasets. Following (Ringel et al.,2024), we show results on five additional structured time series datasets that are publicly available via theaeon toolkit5: Tiselac (Ienco & Gaetano, 2007), ElectricDevices (Chen et al., 2015), PenDigits (Alpaydin& Kaynak, 1998), Crop (Tan et al., 2017), and WalkingSittingStanding (Anguita et al., 2013). An LSTMmodel serves as the base sequential classifier for all datasets.Results of all experiments on early timeclassification (including Quality dataset) are presented on Fig. E.5. The average rank for each methodacross datasets and levels is: GuideBO: 1.2, HVI: 2.8, Random: 3.2, EHVI: 3.4, ParEGO: 4.1, Uniform:5.2. We conclude that, similar to our previous results, GuideBO outperforms the baselines in nearly allcases, ranking first in 20 out of 24 scenarios. Influence of .We examine the influence of , which determines the boundaries of the region of interest.Figure E.6 shows the scores obtained for different values of . We observe that in most cases there is nonoticeable difference in the performance with respect to . However, it appears that moderate values, neithertoo large nor too small, are preferable. Ablation study - one-sided upper bound.We compare the proposed method to the case that the BOsearch is constrained by a one-sided bound at the upper limit defined by . This means that the referencepoint is set to ri = i for i 1, . . . , c, while rc+1 is set according to the maximum value of free, as in thestandard full Pareto front approach. Figure E.7 shows the values of the free objective across tasks. Wesee that in most cases performing the search in the defined region of interest is preferable to a single-sidedbound. This shows the benefit of removing low risk, inefficient configurations from the search space (thegreen section in ). Demonstration of BO Selection.We show the outcomes of the BO procedure across several tasks inFig. E.8. We compare the proposed method to HVI that recovers the entire Pareto front. The referencepoint defined in (12) is marked by a green square, and the boundaries of the region of interest are depicted",
  "Figure E.6: Influence of . Showing the scores of the free objective for different values of , which controlsthe width of the region of interest, defined in Eq. (8)": "by dashed lines.The blue points correspond to the configurations in the initial pool C0, while the redpoints correspond to the configurations selected by the BO procedure. We see that the specified region issignificantly smaller compared to the entire front. Moreover, we observe that by GuideBO we obtain a denseset of configurations in the region of interest as desired. In contrast, for HVI we obtain samples all overthe front. In addition, the distribution of points is not always evenly spread along the front, so that certainpart of the front are denser than other. This explains why HVI (and similarly the other baselines) is inferiorcompared to GuideBO since for certain values the distribution of the selected configurations is sparse nearthe limiting value.",
  "(h) Early-Time Class. (Full)": "Figure E.8: Demonstration of the selection outcomes of the BO procedure, comparing the proposed method(left) to full recovery of the Pareto front by HVI (right): the green square is the defined reference point,the blue points correspond to the initial set of configurations, and the red points correspond to selectedconfigurations. Dashed lines enclose the region of interest."
}