{
  "Abstract": "Contrastive Learning (CL) aims to create effective embedding for input data by minimizingthe distance between positive pairs, i.e., different augmentations or views of the same sample.To avoid degeneracy, CL also employs auxiliary loss to maximize the discrepancy betweennegative pairs formed with views of distinct samples. As a self-supervised learning strategy,CL inherently attempts to cluster input data into natural groups. However, the often impropertrade-off between the attractive and repulsive forces, respectively induced by positive andnegative pairs, can lead to deformed clustering, particularly when the number of clusters kis unknown. To address this, we propose NRCC, a CL-based deep clustering framework thatgenerates cluster-friendly embeddings. NRCC repurposes Stochastic Gradient HamiltonianMonte Carlo sampling as an approximately invariant data augmentation, to curate hardnegative pairs that judiciously enhance and balance the two adversarial forces througha regularizer. By preserving the cluster structure in the CL embedding, NRCC retainslocal density landscapes in lower dimensions through neighborhood-conserving projections.This enables the application of mode-seeking clustering algorithms, typically hindered byhigh-dimensional CL feature spaces, to achieve exceptional accuracy without needing apredetermined k. NRCCs superiority is demonstrated across various datasets with differentscales and cluster structures, outperforming 20 state-of-the-art methods.",
  "Introduction": "Contrastive learning is a self-supervised learning technique that trains a model by encouraging it to distinguishbetween similar and dissimilar pairs of data points, enhancing its ability to learn meaningful representations(Chen et al., 2020b). A representation can be considered accurate if two semantically similar samples lie closeto each other in the feature space while the dissimilar ones reside at a distance. In other words, the learnedembedding space should be geometry-aware, preserving the local neighborhood as well as distinguishing thenatural groups present in the data. Hence, CL methods attempt to learn feature mapping by minimizing thedistance between a positive pair. Such a pair is formed by a couple of augmented variants of the same sampleand thus resides in close proximity, crowding the local neighborhood. Further, to maintain a considerable",
  "Published in Transactions on Machine Learning Research (11/2024)": "batch size ( = 0.05 n/256). Notably, we have multiplied the learning rate for the feature extractor by10 for the predictor networks of BYOL and NRCC, a crucial step discussed in Chen & He (2021); Grill et al.(2020) for achieving satisfactory performance. For the other hyperparameters of NRCC, we have conducted a grid search over the possible choices of eachto find the combination that worked best on average. Finally, we set the temperature (searched between{0.1, 1, 10} and the regularization weight (varied between {0.1, 0.5, 1} to 0.1 each. In the case of SGHMC,we set 1, 2, 3, and as 0.1, 0.05, 0.99, and 1 (increasing the number of updated did not provide anyconsiderable improvement), respectively. Following conventional guidelines, the mini-batch size was 512 forMoCo and 256 for the remaining models, including NRCC. We have constructed the long-tailed versions of CIFAR-10 and CIFAR-20, namely CIFAR-10-LT and CIFAR-20-LT, by employing the codes from Tang et al. (2020). We have followed the guidelines of Zhou et al. (2020)and Cao et al. (2019) while constructing the LT variants of CIFAR, that resulted in two datasets both with acluster imbalance ratio (the number of samples in the most crowded cluster divided by the number of thesample in the smallest one) of 10. Specifically, the long-tailed datasets predominantly contained samplesfrom the majority clusters (head), while only a few samples were present in the other clusters (tail). highlights the key properties of the datasets used in this study.",
  "InfoNCEInfoNCE+NRCC (Ours)": ": UMAP projections of ImageNet-10 embeddings for InfoNCE, BYOL, InfoNCE+NRCC, andBYOL+NRCC. The cluster collapse and fragmentation are respectively shown using solid red and dottedblack borders. The proposed NRCC brings visible improvement in retaining cluster structure in the UMAPspace compared to the corresponding non-regularized baseline CL methods. Given that UMAP preserveslocal neighborhoods, we conclude that NRCC also obtains a cluster-friendly embedding space. on average while the views reside in the local neighborhood. The second challenge is solved using additionalattraction and repulsion through the existing positive pairs, and SGHMC-curated negative pairs that aretraded off against each other. Such a strategy can be added directly with InfoNCE as the paradigm alreadyallows negative pairs. However, minimal architectural and algorithmic modifications are required for positivepair-only BOYL to cater to the effective usage of negative pairs. The novelty and usefulness lie in the proficiency of NRCC at aiding CL in finding cluster-friendly embeddings.Further, such embeddings can be safely projected to lower dimensions by neighborhood preserving techniques(Van der Maaten & Hinton, 2008; McInnes et al., 2018) to allow mode-seeking clustering algorithms withoutrequiring k. As a motivating example in , we present the UMAP (McInnes et al., 2018) projectionsof embeddings learned by InfoNCE, BYOL, and their NRCC regularized counterparts on the ImageNet-10(Russakovsky et al., 2015) dataset. From , it is evident that InfoNCE suffers from both clustercollapse and fragmentation. BYOL, though improved, is still susceptible to collapse. On the other hand,NRCC, in both the cases, exhibits considerable improvement in preserving cluster structures.",
  "Highlights from our contributions are as follows:": "We propose a new regularization framework, NRCC, that uses hard negatives and can be seamlesslycoupled with the existing CL paradigms, such as InfoNCE and BYOL, to learn a clustering-friendlyembedding space. We provide a theoretical characterization of negative pair construction that facilitates cluster-friendlyrepresentation learning. Further, we show that SGHMC sampling can be repurposed as an augmentationstrategy, and the generated negative pairs satisfy approximate invariance (Chen et al., 2020a) i.e., theyare hard pairs offering intensified repulsion with improved local crowding. NRCC relieves the user from knowing the number of clusters beforehand. Given that NRCC-enabled CLembedding preserves cluster structure, it allows for non-parametric mode-seeking clustering algorithms tobe applied effectively, following a local neighborhood-preserving dimensionality reduction. In , we review the existing works on DC and CL. explains the NRCC regularizer, justifiesboth theoretically and empirically the choice of repurposing SGHMC sampling as the preferred augmentationmethod in NRCC, and demonstrates the promise NRCC offers when coupled with InfoNCE and BYOL.Empirical evaluation of the efficacy of NRCC with GridShift (Kumar et al., 2022) against the state-of-the-artsin , shows a 4.7% improvement in clustering accuracy by BYOL+NRCC+UMAP+GridShift on anaverage over eight datasets. Finally, we make concluding remarks in 5.",
  "Deep Clustering": "Clustering performance is highly dependent on the representation of the data, i.e., how well the naturalclusters are retained and expressed in the feature space. The rich embedding learned by deep neural networksfound its natural usage even in the very early days (Yang et al., 2016; 2017). A primary direction for DCmethods employs simultaneous optimization of multiple objectives for finding an embedding space that canbe easily clustered by a specific clustering method (Guo et al., 2017; Caron et al., 2018; Asano et al., 2019;Li et al., 2020b; 2022). For example, Yang et al. (2016) proposed a deep representation learning suitablefor hierarchical clustering while Caron et al. (2018) employed classification to iteratively refine weak clusterassignments. With the advent of CL, the DC research community was quick to identify the similaritiesbetween the two objectives and came up with several approaches that fuse the two for commendable clusteringperformances. For example, CL was used by Van Gansbeke et al. (2020) to generate reliable pseudo labelswhile many others (Li et al., 2021; Shen et al., 2021; Li et al., 2022; Sadeghi et al., 2022) proposed end-to-endframeworks where the CL objective was repurposed for clustering. In an allied direction, (Zhong et al., 2021;Zheng et al., 2021) graphs were employed to identify neighboring pseudo-positive samples. However, theseefforts were plagued by the limitations of CL, i.e., irregularities induced by improper negative pairs, classcollision (Huang et al., 2022), etc. A more recent work by Huang et al. (2022) managed to overcome the classcollision problem but instead imposed a reliance on the knowledge of the number of clusters k to be known inadvance, limiting its practical applicability. Instead, with NRCC, we return to the initial DC strategy ofemploying CL to find a cluster-friendly representation that can be conserved after neighborhood-preservingdimensionality reduction. Consequently, NRCC offers flexibility for the choice of clustering methods, alleviatesthe impact of the curse of dimensionality, and imposes no compulsion on the prior knowledge of k.",
  "Mode Seeking Clustering Methods": "Center-based clustering methods, like k-means, require the knowledge of the number of clusters (k) in advance.Moreover, such methods demand structural and distributional constraints on the clusters. These limitationscan be avoided by density-based mode-seeking clustering strategies such as MeanShift and its variants (Cheng,1995; Jang & Jiang, 2021; Kumar et al., 2022). The MeanShift family of methods is non-parametric that doesnot demand prior knowledge of k. Depending solely on density estimates, these methods can automaticallyreveal natural clusters by identifying the modes of arbitrary distributions. However, MeanShift (Cheng, 1995)is not scalable due to its quadratic computational complexity with the number of samples (O(N 2)). Thiscrucial shortcoming of MeanShift was addressed by MeanShift++ (Jang & Jiang, 2021) which employs a",
  "Preliminaries": "We begin with a set of N N+ samples {xi}iI that reside in a high-dimensional space X Rd, where|I| = N. The high ambient dimension of X prevents distance-based clustering algorithms like k-meansfrom performing at their true potential. Moreover, the curse of dimensionality makes the application ofkernel density estimators infeasible, thereby restricting the usage of non-parametric mode-seeking clusteringalgorithms on such data. Hence, it becomes reasonable to project the data onto a low dimensional embeddingspace H R d, where the natural clusters are appropriately conserved or enhanced even further, d d. Aspreviously elaborated in , in the case of DC, such cluster-friendly representation space is foundthrough a deep neural network. For example, an encoder f : X H can be used to map a sample xi fromits native space to the corresponding latent representation hi = f(xi; ) H. Here, represents the set oftunable parameters that characterize f. With this setup, we move on to the CL-specific case. We start with a set of possible augmentations Tfrom which we randomly select two data transformations, namely T a and T b. To facilitate a mini-batchlearning, from {xi}iI we randomly sample a set N containing n data points without replacement. Now,given this mini-batch N, a set of n positive pairs is created as P+ = {(xai , xbi)}ni=1, where xai = T a(xi) andxbi = T b(xi) are two different views of the same data instance. As such, they share a strong semantic similarity.This aids the network in identifying the natural groups present in the data through the induced attractionwhile enhancing the similarity between a positive pair in the embedding space. However, naively optimizingonly the positive pair-driven objective leads to a degenerate distribution supported on a single point in theembedding space. Traditionally, this is handled using an auxiliary contrastive loss that offers repulsive forceby maximizing the dissimilarity between negative pairs. Formally we denote the set of negative pairs fora batch as P = Pa Pb, where Pa = {(xai , xkj )}ni,j=1,i=j,k{a,b} and Pb = {(xbi, xkj )}ni,j=1,i=j,k{a,b}. Inother words, for the mini-batch N, Pa denotes the collection of both the views xaj and xbj for each of thesamples xj in the set N \\ {xi} = {x1, , xi1, xi+1, , xn} paired with xai , for all i = 1, 2, , n whilePb is constructed analogously. Consequently, the cardinality of P turns out to be 4n(n 1). In the case of InfoNCE (Chen et al., 2020b), the CL objective is not directly optimized in the latent spacelearned by f but in f g, where g : H Z. Specifically, g is realized through a dense neural network thatyields zi = g(hi). Assuming that zi Z is normalized, and is a temperature hyperparameter regulatingthe degree of affinity between samples, the InfoNCE loss lI can be defined as follows:",
  "i=1lBi,a + lBi,b, where lBi,a = ||q(zai ) zbi||22 and lBi,b = ||q(zbi) zai ||22.(4)": "Note that the outputs and mappings of the target network are distinguished from their online counterpartsusing a bar (). While BYOL directly improves over InfoNCE in general, its clustering potential may nothave been fully exploited, as in the absence of counteracting repulsive forces, the embedding space may stillcoalesce multiple clusters (Chen & He, 2021).",
  "Regularizing CL Loss for Clustering": "Following the discussion in .1, it is evident that we need to focus on three primary fronts toachieve good clustering. First, maintain a trade-off between repulsive and attractive forces such that theclusters neither collapse nor fragment but remain uniformly distributed and well separated in the embeddingspace. Second, if similar samples form a negative pair, the repulsion force should be adjusted adaptively toprotect the learning from being misguided. Third, negative pairs are the direct source of repulsion in a CLframework. Hence, ignoring them completely, as in BYOL, may not be helpful for clustering as that maylead to undesirable cluster collapse. We now proceed to detail the proposed NRCC regularizer that can beeffortlessly coupled with InfoNCE or BYOL.",
  ". Comparing (8) with (10), it becomes evident that the proposed NRCC further strengthens theattraction from its non-regularized counterpart": "2. If we consider (9) and (11), then notice that there are two terms of opposite signs that are newlyintroduced by the regularizer. The first term on the left acts as a countermeasure against negativepairs made of semantically similar samples. In such cases, this term is likely to be larger, therebydiminishing the effective repulsion to limit the impact of an improper negative pair on the learning.",
  "Upon closer inspection, we can see that (10) resembles the same for (13) in the sense that both enhanceattraction similarly. However, the lesser additive constant, i.e., 1": "2 in BYOL, may slightly reduce the attractiveforce compared to InfoNCE. This is desirable given that BYOL, by design, is driven by a strong negativegradient; thus, to gain the benefit, the NRCC-induced attraction should be added in a regulated manner. Ifwe move our focus to the positive gradient-driven repulsion, there also BYOL shares similarity with InfoNCE,as evident from (14) and (11). Thus, our previous discussion in the context of regularized InfoNCE still holdsin the case of BYOL. In other words, the NRCC not only introduces repulsion in BYOL but also adaptivelyrestricts the ill effects of improper negative pairs. Till this point, we have observed the impact of NRCC in improving the clustering potential of CL methods.However, NRCC can only work at its true potential if zc can produce a hard negative pair that furtherimproves the repulsion. To this end, we take a deeper look at the standard augmentation techniques anddiscuss the applicability of repurposing SGHMC in the context.",
  "Characterizing Data Augmentation in CL": "The data samples can be deemed as instances of i.i.d. observations X1, X2, , XN P, on the sample spaceX. Some augmentation methods like magnification and rotation are simply affine transformations. Whereasothers, such as cropping or color jitter, may not be so. Either way, semantic features from the original imagesmostly stay intact post-transformation. We can represent such augmentations as a group of transformsT , acting on the sample space. As such, there exists a map : T X X, such that it is associative(TT , x) = (T, (T , x)) = (T, T x), for T, T T and given the identity e T , (e, x) = ex = x(Chen et al., 2020a). In our study, we do not assume that exact invariance holds, i.e. given X P, it mayso happen that TX=d X. Under such a setup, the positive pair (xai , xbi) obtained by applying T a and T b independently on xi may not be identically distributed. This stems from the fact that the augmentationtechniques vary vastly between themselves. We emphasize that the goal of the augmentation is not achievingdistributional identity. Rather, we want to simulate perturbed offshoots of the given observation that sharesemantic information, i.e., to attain approximate invariance (Chen et al., 2020a). Let us now focus on the crucial task of learning to embed data from X onto a lower-dimensional space H.Specifically, we employ a ResNet-type architecture (He et al., 2016) that tends to preserve the local geometry",
  ": Rd Rd, where :=FCIdW,b (ConvwM,bM + Id) (Convw1,b1 + Id) P,(15)": "where Convwi,bi = ConvwLi ,bLi Convw1i ,b1i . Also, = ({wji }M,Li,j=1, {bji}M,Li,j=1, W, b) denotes the parameterspace and Id are the additive identities in subsequent spaces. With this setup, let us now consider thosenetworks that satisfy maxi,j ||wji || ||bji|| Bc and W b Bf, given constants Bc, Bf > 0.These are our preferred candidates that induce the map f. By definition, hki = (xki ), k {a, b}. Observethat, hki s also hail from non-identical distributions. We first show that given a desirable augmentationtechnique, the realized discrepancy between the embedded observation and its augmented counterpart remainsbounded in expectation. This points towards a statistical characterization of augmentations that regulate thefluctuations in estimates. It also testifies that simulated positive pairs indeed crowd the local neighborhood. Firstly, let us assume that the augmentation T follows a probability distribution Q such that the output of theembedding (X, T) (TX) belongs to L2(P Q), i.e. it has finite second moment. Let PN be the plug-inestimator based on the realizations of X1, X2, , XN given by PN := 1",
  "N": "iI Xi. Also let Y1, Y2, , YNbe a sample drawn from the Q-augmented distribution, for example, i.i.d. copies of xa. Based on the same,construct PT,N, the plug-in estimator. Given two non-negative real sequences {an}nN and {bn}nN, wedenote the suppression of the universal constant D > 0, such that lim supnanbn D, by an bn.",
  "Proof. Please find the proof in Appendix B.1": "Proposition 3.1 indicates that, on average, observations obtained through a suitable augmentation techniquewould not deviate significantly from the original inputs in the embedding space. The worst-case discrepancy isbounded by the solution to a linear program (Sriperumbudur et al., 2012), making it finite, easily determined,and controllable. As a result, the likelihood of simulated negative pairs lying drastically farther away andcausing irregular repulsion, leading to cluster disintegration, is reduced. We now focus on SGHMC, intendingto repurpose it to fit the desirability standards for CL-based clustering.",
  "Desirability of SGHMC as an Augmentation Technique in CL-based Clustering": "In an effort to generate xci, a better-augmented view of a sample xi N, that is more useful for constructingclustering-friendly negative pairs tailor-made for NRCC, we start by listing down two desirable criteria. First,compared to xai and xbi, the new augmented sample xci should lie significantly further from xi in the featurespace. Second, xci should resemble xi in terms of their generating law supported on the embedding space.We initialize with a probability distribution over pairwise distances between images. Our methodology fordefining the same focuses on producing a replicate that exhibits an embedding located within the localneighborhood of embedded xi. It is given as follows:",
  "Update s : sj+1 = sj + 2pj,(18)": "where 1, 2, 3 are control parameters of SGHMC. At completion of iterations, we obtain s = s and returnthe same as the augmented view xci. Note that our expression for the SGHMC algorithm is a reparametrizeddiscretization of the underdamped Langevin dynamics. The SGHMC method is further detailed as thefollowing Algorithm 1 for ease of implementation.",
  "Return s = s as the augmented sample xci": "The second criterion can be satisfied by showing that SGHMC acts as an approximately invariant augmentationtechnique and hence is desirable for CL-based clustering methods. Let us start by defining Pn = 1 nni=1 P(xi),based on observations {x1, x2, , xn} in a batch N. To establish a statistical characterization, we firstimpose mild regularity conditions, standard in related literature (Raginsky et al., 2017), on the underlyingdistribution.Assumption 3.2 (Regularity of P). P is bounded and continuously differentiable, having boundedgradients such that for any given , there exists > 0 satisfying",
  "Proof. Please find the proof in Appendix B.2": "From Theorem 3.6, we can conclude that after a certain point in time, the augmented views iteratively obtainedusing SGHMC possess a distribution that lies arbitrarily close to the stationary benchmark, dependent onthe target. The approximate global minimizer thus obtained, i.e. s, also holds the same property. Since the1-Wasserstein distance W1 is upper bounded by W2, the result obtained in Theorem 3.6 can also be extendedto the W1 metric. We have previously observed the efficacy of general augmentation procedures under W1(see Appendix B). This result gives us an immediate guarantee that SGHMC achieves approximate invariancein the augmented samples.",
  "Revisiting Negative Pairs in NRCC": "Based on the discussion in .2, it is evident that the amount of repulsion induced by NRCC can befurther amplified directly by placing the negative pairs farther away in the embedding space. However, duringimplementation, zs are always normalized as z/||z||2, in turn following a projected (or offset) distributionsupported on a hyper-sphere (Mardia & Jupp (2009), Chapter 9). As such, the points being restricted onthe surface, pushing two clusters wide apart brings one or both of them closer to some other cluster. Thissituation can only be avoided if the augmented views are aware of the density landscape and crowd the localneighborhood rather than spreading out uniformly. As per our findings in Sections 3.3-3.4, we can argue thatSGHMC satisfies this owing to its approximate invariance. To further empirically validate this claim, weundertake a comparative study covering both the qualitative and the quantitative aspects. The qualitativestudy in demonstrates that the outputs generated by a single step of SGHMC are indeed visuallydistinct from their original counterparts, especially in contrast to the four popular data augmentations.This is quantitatively supported by , where, given 1000 ImageNet-10 samples, the mean Euclideandistance between a negative pair in the proposed BYOL+NRCC embedding space is higher for a singleupdate of SGHMC. Thus, we can conclude that SGHMC, when used in NRCC, can produce negative pairsthat simultaneously generate higher repulsion and preserve the cluster structure in the embedding space. : Mean and Standard Deviation of Euclidean distance in the proposed BYOL+NRCC embedding spacebetween 1000 ImageNet-10 samples and their corresponding transformed views obtained by five augmentationsnamely, Random color jitters, Random gray-scale, Random horizontal rotation, Random crop with resizing,and one step of SGHMC (ours).",
  "(c)": ": Plots of k-Nearest Neighbor (kNN) classification accuracy, Global Standard Deviation, andClustering Accuracy against training iterations on ImageNet-10, for BYOL variants in terms of differentcouplings of negative pairs Specifically, the SGHMC augmented view xcj is passed through (a) an onlinenetwork with prediction, (b) a target network with prediction, and (c) a typical target network withoutprediction. The third case (c) provides the best and most stable performance among the three alternatives. this schematically in the following . However, BYOL+NRCC brings additional challenges, given thatno negative pair is involved in the original non-regularized formulation. If we consider the case of calculatinglBi,a then given xj = xi to pass its SGHMC augmented view xcj one needs to consider two possible scenarios.First, xcj can be passed through either the online or the target network path. Second, if xcj follows the targetbranch, then it can either employ or discard an additional predictor at the end. Thus, to find a suitablenetwork structure, we employ an ablation study following Chen & He (2021). We mainly consider threeconfigurations. First, pass xcj to the online network. Second, feed xcj to the target network while using anadditional prediction block. Third, pass xcj to the target network without a prediction block. For each of thethree cases, over the training iteration on the ImageNet-10 dataset, we track the k-Nearest Neighbor (kNN)accuracy, global standard deviation, and clustering accuracy using GridShift (Kumar et al., 2022) followinga UMAP dimensionality reduction, setting the number of neighbors k as 5. We illustrate the findings in from which we can make three observations. First, if xcj passes through the online network, thenthe training is unstable as the momentum-based running average in the target network of BYOL offers thedesired stability. Second, if xcj is passed through the target with a prediction block, then the global standarddeviation slowly drops, indicating a collapse. Ideally, for a 256-variate projected Normal distribution, theglobal standard deviation should plateau at 1/ 256 0.06 (Chen & He, 2021). This may be due to the factthat additional prediction in the target brings discrepancy in the learning, such that it is forcing clusterseparability in the space spanned by {q(z); z Z} but not in the intended embedding space H. Third, if xcjpasses through a typical target network armed with stop gradient without additional prediction block, thenall three indicators maintain stability and demonstrate improvement over training. Now that we have the necessary clarity of effectively coupling BYOL and NRCC, let us follow up on thediscussion in .2. If we consider the stop-gradient and predictor block in BYOL, then the actualgradients modify from their simplified versions in (12) to the following:",
  "Discard g and return f as the feature encoder": "NRCC achieves its true potential when it is applied on the Z-space of the target network in the BYOLarchitecture. In other words, while regularizing, we only need to account for the stop gradient and ignore theprediction block. This only impacts the negative gradient-induced attraction while the positive gradient-basedrepulsion remains the same as in (14). After revising (13), the regularized negative gradient takes the form:",
  ".(20)": "The simplified formulations (12) and (13), when compared with their respective counterparts in (19) and (20)leads to two key observations. First, the canonical and regularized positive gradients remain exactly the sameafter considering the BYOL-specific heuristics. Second, as already mentioned in .2, the negativegradient of the canonical BYOL now has a multiplicative factor zai zai that does not impact the additional forceexerted by the regularizer. Thus, the motivation in favor of NRCC that we had mentioned earlier in .2 remains unaltered even if we account for the implementation-specific heuristics. We present the schematicworkflow of BYOL+NRCC in . We also describe InfoNCE+NRCC and BYOL+NRCC, respectively, inthe following Algorithms 2 and 3. The code base is available at",
  "Clustering in the NRCC Regularized CL Embedding Space": "After optimizing the InfoNCE+NRCC or BYOL+NRCC loss, we can safely discard everything except the f,i.e., the original encoder. In the case of BYOL+NRCC, we only retain the f from the target branch. Now, atthe end of training, we get the cluster-friendly embedding. If we know the number of clusters beforehand,then we can apply a parametric technique like k-means to the learned f space. Otherwise, we can project theembeddings to a lower-dimensional space through methods like UMAP or t-SNE (Van der Maaten & Hinton,2008) that retain the local neighborhoods. Now, NRCC, along with SGHMC-based augmentation, managesto properly identify and separate the clusters in the embedding space. Thus, the revealed cluster structuresare not distorted after additional neighborhood-preserving dimension reduction. This further enables the useof mode-seeking clustering methods like GridShift that do not require knowledge of the number of clusters.",
  "targetonline": ": For BYOL+NRCC, the network architecture remains the same while the algorithm (originaldenoted in black) adds an extra pass (shown by the blue line) of the negative pair through the target network(the branch in green). A negative pair is created by taking an augmentation of xi as xai and the SGHMCtransformation of another sample xj as xcj. To calculate the NRCC regularizer, the xai is passed through theonline network (the branch in green) to get q(zai ) while xcj goes through target to be mapped to zcj. The losscan then be calculated by (6) for updating online while ensuring stop gradient on target. The target is thenupdated using the momentum-driven running average of the online parameters as in Grill et al. (2020).",
  "Experimental Protocol": "To ensure a fair comparison, we refrain from using pre-trained models in all our experiments. We useResNet-34 as our backbone network following the conventional recommendation. Moreover, as per standardprotocol (Kim & Ye, 2022; Chen et al., 2020b), we train the backbone for 1000 epochs with a batch size256. The number of iterations of SGHMC is set to 1 as increasing it did not provide any considerable",
  "UMAP+GridShift (Ours)BYOL88.493.887.9": "performance boost. The rest of the hyper-parameters for the proposed method are tuned by grid search,and we detail the same in Appendix C. All the contending algorithms follow their originally recommendedsettings. For evaluating the clustering performance of the proposed methods, we consider four types ofdatasets, namely large-scale moderate resolution CIFAR-10, CIFAR-100 (Krizhevsky & Hinton, 2009), andSTL-10 (Coates et al., 2011), moderate scale higher resolution subsets of ImageNet such as ImageNet-10 andImageNet-Dogs (Russakovsky et al., 2015), large-scale higher resolution TinyImageNet (Le & Yang, 2015),and large scale moderate resolution long-tailed CIFAR-10-LT (Tang et al., 2020) and CIFAR-20-LT (Tanget al., 2020). Further details on datasets are provided in Appendix C. For CIFAR variants, we follow a similararchitectural modification to ResNet-34 as suggested by SimCLR (Chen et al., 2020b). All contenders mapthe input to 256-dimensional embedding space while data reduction, if used, further decreases it to threesuch that mode-seeking algorithms can be applied. The code base for the proposed techniques can be foundat",
  "Ablation Study": "We begin with an ablation study on the CIFAR-10 dataset to understand how the different components of theproposed framework impact its clustering performance. We break down the ablation study into four stages.First, setting the baseline performances of InfoNCE and BYOL when a k-means clustering is used. Second,evaluating the improvement of baseline clustering performances if t-SNE (Van der Maaten & Hinton, 2008)or UMAP is used for dimension reduction, followed by GridShift. Third, validate the significance of NRCCfor obtaining a cluster-friendly embedding space by applying the regularizer with standard augmentationtechniques. Fourth, gathering evidence in support of SGHMC as a fitting augmentation technique in theNRCC framework. In , we detail our findings in terms of three clustering performance evaluationmetrics, namely Normalized Mutual Information (NMI) (Strehl & Ghosh, 2002), Clustering Accuracy (ACC)(Xie et al., 2016), and Adjusted Rand Index (ARI) (Hubert & Arabie, 1985). From , we can observethat the baseline performances in terms of all three indices improve when GridShift is applied on the lowerdimensional space instead of k-means on the CL-embedding space. Moreover, the improvements by GridShiftare more pronounced when UMAP is used for dimension reduction. We can make two critical conclusions fromthese findings. First, the mode-seeking algorithm on a lower-dimensional space not only removes the demand",
  "Performance Comparison on Benchmark Datasets": "For the comparative study of clustering performance, we take six benchmark datasets, namely, CIFAR-10,CIFAR-20, STL-10, and the three ImageNet subsets. As competing algorithms, we consider 20 state-of-the-art(SOTA) methods spread across six different DC strategies. First, the non-CL-techniques, such as, IIC (Jiet al., 2019), DCCM (Wu et al., 2019) and PICA (Van Gansbeke et al., 2020). Second, the methods thatdirectly produce cluster assignments, like CC (Li et al., 2021), MiCE (Tsai et al., 2021), GCC (Zhong et al.,2021), TCL (Li et al., 2022), TCC (Shen et al., 2021), and C3 (Sadeghi et al., 2022). Third, CL-methodsthat are not tailored for clustering but produce a generalized embedding that can be efficiently clustered,for example, SimClr (Chen et al., 2020b), EBCLR (Kim & Ye, 2022), MoCo (He et al., 2020), SimSiam(Chen & He, 2021), and BYOL (Grill et al., 2020). Fourth, CL methods tailored for clustering tasks, suchas IDFD (Tao et al., 2021) and PCL (Li et al., 2020a). Fifth, multi-stage methods where a pre-training isfollowed by a clustering task-specific fine-tuning, like SCAN (Van Gansbeke et al., 2020) and NMM (Danget al., 2021). Sixth, a couple of recent contrastive clustering methods explore the importance of betteraugmentation strategies in the context, namely, CCES (Yin et al., 2023) and SACC (Deng et al., 2023).",
  "Margin+6.5+12.9+0.8+0.1+4.9+9.6": ": Comparison of clustering performance of the proposed NRCC+GridShift, coupled with InfoNCE andBYOL, against 20 other State-Of-The-Art (SOTA) DC methods in terms of ARI on six benchmark imagedatasets of varying scale and resolution. The best is boldfaced, the second best underlined, and the Margin isdefined as the difference between the best proposed and its nearest existing SOTA.",
  "Margin+5.0+2.9+4.5+3.4+4.0+5.0": "NMI and ARI measures. Closer inspection of reveals that the proposed BYOL+NRCC+GridShiftobtains the best performance on all the six datasets, improving accuracy by 5.10% on average from thenearest contenders among the 20 SOTA methods. The proposed InfoNCE+NRCC+GridShift also attains thesecond position on four datasets. This is an expected outcome given that BYOL is known to be a superiorrepresentation learner (Grill et al., 2020). Similar behavior is reflected in Tables 4 and 5 as well. Specifically,BYOL+NRCC+GridShift maintains its best position in terms of both NMI and ARI over all six datasets,leading ahead from its nearest contender, respectively, by 5.80% and 5.43% on average. The other NRCCvariant, InfoNCE+NRCC+GridShift, also performs consistently, standing second over five datasets in termsof NMI and four datasets when ARI is considered. These findings unanimously testify that NRCC indeedprovides a cluster-friendly embedding space that, when preserved by data dimensionality reduction throughlocal neighborhood preserving UMAP, can significantly improve performance by GridShift without requiringthe knowledge of the number of clusters. The generally improved representations learned by BYOL overInfoNCE are also further validated through their clustering performance.",
  "Performance Comparison on Long-tailed Datasets": "Up to this point, we have only validated the performance of the proposed BYOL+NRCC+GridShift ondatasets with uniform-sized clusters. However, in reality, it is common to find datasets with imbalancedclusters or long-tailed cluster distributions. Thus, we consider two imbalanced datasets, namely, CIFAR-10-LTand CIFAR-20-LT, to validate NRCCs effectiveness. In , we compare the clustering performance ofthe BYOL+NRCC+GridShift against MoCo and baseline BYOL in terms of NMI, ACC, and ARI. From, we can observe that BYOL+NRCC+GridShift retains its best performance, followed by BYOL,while MoCo performs the worst among the three. Specifically, applying NRCC+GridShift improves NMI,ACC, and ARI, respectively, by 4.2%, 3.45%, and 4.75%, on average, over the baseline algorithm. Thus, asper the empirical evidence, we can safely conclude that even in the case of imbalanced clusters, NRCC is stillcapable of finding an embedding space where independent cluster structures are accurately preserved.",
  "Coupling NRCC with Other State-of-the-arts CL Methods": "Up to this point, we restrict our discussion to InfoNCE and BYOL, two distinct foundational CL methods.Over the past few years, some other CL methods such as SimSiam Chen & He (2021) and Barlow TwinsZbontar et al. (2021) also gained popularity due to their superiority over the predecessors in some downstreamtasks like classification and object detection. SimSiam can be thought of as a variant of BYOL that discardsmomentum-based updates. However, Chen & He (2021) agree that such a change may sacrifice accuracy.This may be caused by a slightly irregular feature embedding that does not preserve the cluster structuresto the expected extent. On the other hand, Barlow Twins may compromise discriminative features in anattempt to reduce redundancy. This may negatively impact clustering performance which can worsen underincompatible choices of augmentation. Hence, coupling NRCC may not be proven beneficial for such baselinesthat are not canonically supportive of clustering performance. To validate this intuition, we consider anablation study on the CIFAR-10 dataset in the following . We can see from that SimSiam andBarlow Twins both perform better than SimCLR but worse than BYOL. Moreover, after coupling NRCC,both SimSiam and Barlow Twins perform worse than the two proposed techniques. Thus, NRCC aids bothSimSiam and Barlow Twins in improving their clustering performance but fails to totally compensate for",
  "The Scalability of NRCC": "To evaluate the scalability of the proposed NRCC we conduct a couple of experiments. The first experimentis focused on large-scale datasets with a high number of clusters such as the full ImageNet-1k. The secondexperiment investigates the applicability of NRCC with larger backbone networks such as the deepervariants of ResNet and Visual Transformers (ViT). In both cases, we consider the BYOL+NRCC+GridShiftvariant as our proposed method of choice given it maintains a consistent performance improvement overInfoNCE+NRCC+GridShift. The following documents the findings of these two experiments. In the first experiment with ImageNet-1k, we follow the experimental protocol described in Li et al. (2020a)and train a ResNet-50 for 200 epochs. To ensure a fair comparison, we only consider the state-of-the-artmethods that have a reported result (either in the original paper or in a later reference such as Li et al.(2020a)) on ImageNet-1k in terms of Adjusted Mutual Information (AMI) under the common protocol (Liet al., 2020a). From the top half of we can observe that even when the number of clusters is increasedin ImageNet-1k the proposed BYOL+NRCC+GridShift sustains a commendable lead in Adjusted MutualInformation (AMI) over the state-of-the-art contenders. Specifically, the proposed BYOL+NRCC+GridShiftmethod achieves a 5.9% improvement in AMI from its closest and most recent contender PIPCDR (Kumar& Lee, 2025). The experiment also shows that if we replace the traditional ResNet-50 backbone with abetter performing ViT-Small network then a further 1.2% performance gain can be achieved for our method.Moreover, in comparison to the BYOL baseline, using ViT-Small the proposed NRCC regularizer can providea 11.9% boost in AMI. Thus we can safely establish NRCC as scalable to a large number of clusters. In the second experiment, we consider the ImageNet-Dogs dataset and vary the backbone network ofBYOL+NRCC+GridShift between two ResNet and three ViT variants. Specifically, we take ResNet-34 andResNet-50, the two widely popular residual backbones in contrastive learning literature, which are also usedthroughout this paper. We further consider the more recent ViT-Tiny, Vit-Small, and ViT-Base networksas backbones that gradually increase the number of parameters from 5.8M to 86M. For fairness, all fivenetworks are trained under the same experimental protocol detailed in .1 and Appendix C. If wesummarize the bottom part of that presents the results of this second experiment then we canobserve four interesting factors. First, changing the backbone from ResNet-34 to the larger ResNet-50 doesnot improve the performance but slightly decreases all three metrics. Second, Vit-Tiny suffers a somewhatlimited performance deterioration of 2.7% (compared to the best performer Vit-Small) on average with only26% of parameters. Third, Vit-Small and ResNet-34 though share an almost equal number of parameters theViT has an improved performance in terms of all three indices, making it the best among the five choices.Fourth, similar to ResNet increasing the number of parameters from Vit-Small to ViT-Base also acts againstthe performance. We can make two conclusions from these observations. First, ViT owing to its generallybetter performance than ResNet has the potential to become a primary backbone for contrastive clustering.Second, at least for most clustering tasks, blindly increasing model capacity without accounting for the dataavailability and problem complexity may not be a straightforward strategy for finding success. This is evident",
  "Conclusion and Future Works": "We recognize the uncontrolled interplay between positive pair-induced attraction and, if present, negativepair-influenced repulsion as critical factors leading to collapsed or fragmented clusters in CL-based DCmethods. In response, we introduce the NRCC regularizer, which carefully balances these two counteractingforces while enhancing them to ensure that the inherent cluster structure in the dataset remains intact in theembedding space. To achieve this, we re-envision SGHMC, a sampling method initially designed to improvethe computational efficiency of Hamiltonian Monte Carlo for online inference, as an approximately invariantaugmentation strategy in CL. As per our knowledge, this is the first work that aims to repurpose a samplingmethod such as SGHMC, as a data augmentation strategy. To further justify the aptitude of SGHMC asan augmentation in the CL framework we theoretically establish its approximately invariant property andempirically demonstrate its ability of curating hard negative pairs following our criteria. Utilizing SGHMC-generated negative pairs in NRCC introduces a density-aware repulsive force, facilitating the learning of acluster-friendly embedding. Consequently, the representation acquired by NRCC can be effectively projectedto a lower-dimensional space using a local neighborhood-preserving method. This opens up opportunities toapply mode-seeking algorithms that do not necessitate prior knowledge of the number of clusters. There are two limitations of the current work that open potential future avenues of research. First, unlike end-to-end methods (Huang et al., 2022), we take a modular approach, where three subsequent steps for embeddinglearning, dimensionality reduction, and mode-seeking clustering are used. On one hand, such an approachoffers flexibility allowing the user to employ state-of-the-art neighborhood-preserving dimensionality reductiontechniques and clustering algorithms of choice to further improve application-specific performance. Moreover,unlike existing approaches, this modularity enables our framework to allow mode-seeking clustering algorithmsthat remove the dependency on the number of clusters. On the other hand, if a perfect harmony amongthe different choices for the components is not met, then that may restrict the framework from performingat its true potential. To alleviate this issue, a balance can be achieved by simultaneously performing thedimensionality reduction task in the CL embedding learning framework. Specifically, instead of directly usingan external UMAP-type technique, one may emulate the same through an additional auxiliary loss in the CLobjective itself. Such an approach may further benefit from shifting the problem to the graph contrastivelearning domain, where representing the neighborhood and the modes can become easier. However, this",
  "Xiaozhi Deng, Dong Huang, Ding-Hua Chen, Chang-Dong Wang, and Jian-Huang Lai. Strongly augmentedcontrastive clustering. Pattern Recognition, 139:109470, 2023": "Xuefeng Gao, Mert Grbzbalaban, and Lingjiong Zhu. Global convergence of stochastic gradient hamiltonianmonte carlo for nonconvex stochastic optimization: Nonasymptotic performance bounds and momentum-based acceleration. Operations Research, 70(5):29312947, 2022. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020. Xifeng Guo, Long Gao, Xinwang Liu, and Jianping Yin. Improved deep embedded clustering with localstructure preservation. In Proceedings of the Twenty-Sixth International Joint Conference on ArtificialIntelligence, pp. 17531759, 2017.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 97299738, 2020. Qianjiang Hu, Xiao Wang, Wei Hu, and Guo-Jun Qi. Adco: Adversarial contrast for efficient learning ofunsupervised representations from self-trained negative adversaries. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 10741083, 2021. Jiabo Huang, Shaogang Gong, and Xiatian Zhu. Deep semantic clustering by partition confidence maximisation.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 88498858,2020. Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for clusteringvia prototype scattering and positive sampling. IEEE Transactions on Pattern Analysis and MachineIntelligence, 2022.",
  "Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of classification, 2:193218, 1985": "Jennifer Jang and Heinrich Jiang. Meanshift++: Extremely fast mode-seeking with applications to segmenta-tion and object tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 41024113, 2021. Xu Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised imageclassification and segmentation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 98659874, 2019. Yannis Kalantidis, Mert Bulent Sariyildiz, Noe Pion, Philippe Weinzaepfel, and Diane Larlus. Hard negativemixing for contrastive learning. Advances in neural information processing systems, 33:2179821809, 2020.",
  "Kenta Oono and Taiji Suzuki. Approximation and non-parametric estimation of resnet-type convolutionalneural networks. In International conference on machine learning, pp. 49224931. PMLR, 2019": "Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradientlangevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pp. 16741703. PMLR,2017. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115:211252, 2015.",
  "Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learningresearch, 9(11), 2008": "Wouter Van Gansbeke, Simon Vandenhende, Stamatios Georgoulis, Marc Proesmans, and Luc Van Gool. Scan:Learning to classify images without labels. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part X, pp. 268285. Springer, 2020. Tao Wang, Guangpin Tao, Wanglong Lu, Kaihao Zhang, Wenhan Luo, Xiaoqin Zhang, and Tong Lu. Restoringvision in hazy weather with hierarchical contrastive learning. Pattern Recognition, 145:109956, 2024.",
  "Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empiricalmeasures in Wasserstein distance. Bernoulli, 25(4A):2620 2648, 2019": "Jianlong Wu, Keyu Long, Fei Wang, Chen Qian, Cheng Li, Zhouchen Lin, and Hongbin Zha.Deepcomprehensive correlation mining for image clustering. In Proceedings of the IEEE/CVF internationalconference on computer vision, pp. 81508159, 2019. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametricinstance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 37333742, 2018.",
  "Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis. InInternational conference on machine learning, pp. 478487. PMLR, 2016": "Bo Yang, Xiao Fu, Nicholas D Sidiropoulos, and Mingyi Hong. Towards k-means-friendly spaces: Simultaneousdeep learning and clustering. In international conference on machine learning, pp. 38613870. PMLR,2017. Jianwei Yang, Devi Parikh, and Dhruv Batra. Joint unsupervised learning of deep representations and imageclusters. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 51475156,2016.",
  "ANotations": "The following describes the different mathematical notations used throughout the paper. Apartfrom these, for the proofs of Proposition 3.1 and Theorem 3.6 (in Appendix B.1 and B.2 respectively) anyadditionally required notations and definitions are clearly described topically and are not carried forwardelsewhere. Thus, for simplicity we discard them from .",
  "NotationDescription": "N, XN data samples residing in the native space X.N+, RThe set of all positive integers and the set of real numbers.d, dThe dimensionality of the native space X and the embedding space.HThe embedding space of dimensionality d.fAn encoder mapping from X to H.x, hA sample in native space from the set X and the embedding space H.T , T a, T bThe set of all possible augmentations and two augmentations in Txa(xb)The sample x transformed by T a(T b).N, nA mini-batch N of size n.P+(P)The set of all positive (negative) pairs in a mini-batch.Pa(Pb)The set of negative pairs w.r.t. T a(T b).gAdditional dense layer to map the embedding space to another space (Chen et al., 2020b).z, ZThe mapped sample g(h) and the space spanned by the mapping g., sim(, )Temperature parameter controlling the extent of similarity sim(, ), e.g. inner product , .lI(lIi,a, lIi,b)InfoNCE loss function (w.r.t. i-th sample and augmentation T a(T b)).qAdditional prediction block in target branch of BYOL (Grill et al., 2020).lB(lBi,a, lBi,b)BYOL loss function (w.r.t. i-th sample and augmentation T a(T b)).()Mappings and outputs for target branch of BYOL (Grill et al., 2020).l()NRCC regularized loss functions.Hyper-parameter for the relative weight of the NRCC regularizer.zc, zcArbitrary augmented view of x N.P, X, PNThe probability distribution on space X, a random variable following P,and empirical distribution based on N i.i.d. observations of Xs.A map T X X.eAn identity augmentation e T .=dNon-identically distributed.Convw,b, C, CConvolution operation with weight w, bias b, and activation mapping from C channels to C.FCW,bFully connected layer with activation , weight W , and bias b.PA typical padding operation.M, LThere are M residual block each of depth L in the deep network under concern., The embedding function realized through a deep network with the parameter space .IdIdentity mapping.Bc, Bf , D, , Constants greater than zero, used for various bounds.Note that D is controlled by two non-negative real sequences {an}nN and {bn}nN.Q, Y, PT,NThe probability distribution of T , a random variable following Q-augmented distribution,and empirical distribution based on N i.i.d. copies of Y .PProbability distribution over pairwise distance characterized by ., 1, 2, 3Control parameters of SGHMC.p, sIntermediate vectors associated with SGHMC.PnAn empirical expectation of P over the batch.W1, W21 and 2-Wasserstein distances.",
  "d": "If exact invariance holds, i.e. TX=d X then, PT,N acts as a consistent estimator of P under the Wassersteindistance. In other words, the error will plunge to zero as N increases. However, the departure from sucha scenario would hinder the shrinkage of the discrepancy even in a large sample regime. To demonstratethe same, we write, W1(PT,N, P) W1(PT,N, PN) + W1(PN, P), where the term W1(PN, P)a.s. 0 as N (Weed & Bach, 2019). Further, in a non-asymptotic sense, it is upper bounded by O(N 1/ max (d,2)), with highprobability. Therefore, essentially the error committed during estimation is the disagreement between the twoplug-in estimators. This will give us the extent of deviation due to augmentation. Following Sriperumbuduret al. (2012), we can also write,",
  "where L1X is the class of real-valued 1-Lipschitz maps on X. Also, Wi =1N , Zi = Yi for i = 1, , N andWN+i = 1": "N , ZN+i = Xi for i = 1, , N. The solution to (21) can be achieved by solving a linear program(Sriperumbudur et al., 2012). As such, we suffer at the most a deterministic error due to a particular choiceof augmentation. Now, combining the results obtained so far and taking expectation, we get",
  "CImplementation Details": "We have rigorously trained all models for 1,000 epochs, following the conventional recommendations (Taoet al., 2021; Tsai et al., 2021). We have utilized the Stochastic Gradient Descent (SGD) optimizer with acosine learning rate scheduler, that includes a warm up for the initial 50 updates. For MoCo (He et al., 2020),BYOL (Grill et al., 2020), and NRCC, we set the base learning rate to 0.05, dynamically scaling it with the",
  "DThe Number of Iterations in SGHMC": "Throughout the article, we have theoretically and empirically established how SGHMC is immensely usefulfor NRCC as an approximately invariant augmentation strategy. However, to effectively balance the trade-offbetween computational overhead and performance gain (see ) we only iterate SGHMC once. Thisfollow-up study focuses on empirically understanding the behavior of SGHMC-based augmentation when thealgorithm is iterated multiple times. In we take a path similar to that in by consideringthe mean and standard deviation of Euclidean distances between 1000 negative pairs from the ImageNet-10dataset that are generated by SGHMC over gradually increasing iterations up to 13. A closer scrutinyof reveals that the SGHMC-augmented samples progressively converge over iterations towards astationary benchmark as indicated theoretically (see Theorem 3.6). This is visually supported by that demonstrates the augmented instances for two samples (the same ones previously used in Fig 2)over progressive SGHMC iterations. In both cases, the visible dissimilarity between the augmentationscorresponding to distinct samples diminishes over iterations and almost becomes identical at the end ofthe 11th step, signifying near convergence. From and 1, we also observe that the negative pairsgenerated by SGHMC even at iteration 13 still lie at a distance 34% greater compared to the maximumattainable by traditional transformations."
}