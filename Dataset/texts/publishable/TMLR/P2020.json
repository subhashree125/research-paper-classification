{
  "Abstract": "In some causal inference scenarios, the treatment variable is measured inaccurately, for in-stance in epidemiology or econometrics. Failure to correct for the effect of this measurementerror can lead to biased causal effect estimates. Previous research has not studied methodsthat address this issue from a causal viewpoint while allowing for complex nonlinear depen-dencies and without assuming access to side information. For such a scenario, this studyproposes a model that assumes a continuous treatment variable that is inaccurately mea-sured. Building on existing results for measurement error models, we prove that our modelscausal effect estimates are identifiable, even without side information and knowledge of themeasurement error variance. Our method relies on a deep latent variable model in whichGaussian conditionals are parameterized by neural networks, and we develop an amortizedimportance-weighted variational objective for training the model. Empirical results demon-strate the methods good performance with unknown measurement error. More broadly, ourwork extends the range of applications in which reliable causal inference can be conducted.",
  "Introduction": "Causal inference deals with how a treatment variable X causally affects an outcome Y . This is differentfrom just estimating statistical dependencies from observational data, because even when we know that Xcauses Y and not the other way around, the statistical relationship could be affected by confounders Z,i.e., common causes of X and Y . Knowing the causal relationships is crucial in fields that seek to makeinterventions, e.g., medicine or economics (Pearl, 2009; Peters et al., 2017; Imbens & Rubin, 2015). Causal inference may be complicated by variables being subject to noise, e.g., inaccurate measurement,clerical error, or self-reporting (often called misclassification for categorical variables). If not accounted for,it is well-known that this error can bias statistical and causal estimates in a fairly arbitrary manner, andconsequently, measurement error models have been widely studied to address this bias (Carroll et al., 2006;Buonaccorsi, 2010; Schennach, 2016; 2020). In this paper, we assume the treatment X and outcome Y to be continuous, while the confounders Z can becategorical, discrete or continuous. (The case of a binary or categorical X is briefly discussed in AppendixA.) While measurement error may occur in any of the variables, we explicitly model it only in X.Weimplicitly also include measurement error in Y , but in our model it is indistinguishable from the inherentnoise in the true value of Y . The practical impact of this is limited, since the measurement error for Y doesnot bias regression due to its zero-mean additive nature that we assume. On the other hand, measurementerror in Z could be relevant (Miles et al., 2018), but it is restricted outside the scope of this work.",
  ": Causal graph for our proposed model.The observed variables X (noisy treatment), Y (ef-fect/outcome), and Z (confounders) are shaded to distinguish them from the hidden variable X (truetreatment)": "The causal graph expressing our assumptions is depicted in . An example reflecting this scenarioin the context of healthcare is the effect of blood pressure on some continuous health outcome Y , such asarterial stiffness as measured by pulse wave velocity, or kidney function as measured by glomerular filtrationrate (GFR) or by the concentration of albumin in urine. We assume X is a single measurement of bloodpressure, and thus it is highly subject to instantaneous variation, which plays the role of the measurementerror. Some additional error probably also results from measurement apparatus inaccuracy. Here, the bloodpressure that actually matters in terms of health outcomes (and whose effects we are interested in) is a longerterm sliding window average, which is unobserved and which we denote by the latent variable X. To enable statistical identifiability, we assume the measurement error to be additive, zero-mean and indepen-dent from the other variables. These are a standard set of assumptions in the measurement error literature,called strongly classical measurement error. Non-classical measurement error is also widely studied and welldocumented to occur in practice (see Schennach (2016) for a review), but it is restricted outside the scope ofthis work. However, we believe our modeling and inference methodology is general enough that extensionsto these cases are fairly straightforward. presents a synthetic example of the skewing effect ofmeasurement error in the treatment. While it is often believed that this type of measurement error onlyintroduces attenuating bias to a naive estimate not accounting for the error (Yi et al., 2021), this exampledemonstrates that on the contrary, amplification is also possible. Here attenuation happens only aroundX = 0, but for small and large X the naive estimate amplifies the strength of the dependency. The measurement error problem is often addressed by making additional measurements to enable modelidentification. The information obtained in this way could be a known measurement error variance, or so-called side information, such as repeated measurements, instrumental variables, or a gold-standard sampleof accurate measurements (Yi et al., 2021). However, we study the scenario where none of this is available,and we must rely on assumptions that could reasonably be made a priori, such as the error being additiveand having a zero mean. Further, we study the scenario where the dependencies between the variables canbe complex and nonlinear, and an observed confounder is present. There is a gap in the literature for causalinference when all of these challenging aspects are present. To address this gap, our paper provides a solution by inferring a structural causal model (SCM) from ob-servational data using deep latent variable modeling and importance weighted variational inference (Zhanget al., 2019). The inferred SCM enables the computation of any interventional and counterfactual distri-butions on the variables in the model, but to evaluate the fit, we consider the accuracy of the estimationof the following quantities: 1) the function Y (z, x) = E[Y |z, do(x)], 2) the noise variances 2 and 2,and 3) the function p(y|do(x)), which maps x to the density of Y |do(x). The combination of 1) and 2)also evaluates the estimation accuracy of p(y|z, do(x)), since with Z satisfying the backdoor criterion andby assuming a conditionally Gaussian Y , we have p(y|z, do(x)) = p(y|z, x) = N(y|Y (z, x), 2). Theseparameters/distributions were chosen because they are either model parameters, reflect probable use casesof the model, or both. To facilitate practical computation, we assume conditionally Gaussian variables.However, we emphasize that the purpose of this is to simplify computations, and the identification of themodel does not rely on this assumption, see .3.1 for the details. We leave up to future work to relaxit by using flexible probability density estimators such as normalizing flows. We evaluate our algorithm ona wide variety of synthetic datasets, as well as semi-synthetic data.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Case 2 considers the case when Y (z, x) as a function of x is of the form a + b ln(ecx + d) with d > 0.(Having d < 0 is impossible as we assume the function g is defined on R).In this case, our model isidentifiable, as X is Gaussian and thus its density is not of the form",
  "Models for causal estimation": "For causal inference, this study uses the structural causal model (SCM) framework (Pearl, 2009). A SCMconsists of the following components: 1) A list of endogenous random variables V = (V1, V2, . . . , VN).2) A directed acyclic graph G = (V, E) called the causal graph, whose vertices are the variables Vand whose edges E express causal effects between variables.3) A list of exogenous random variablesU = (U1, U2, . . . , UN) that represent external noise and are mutually independent.4) A list of deter-ministic functions F = (f1, f2, . . . , fN), called the causal mechanisms, such that for i = 1, . . . , N we haveVi = fi(PAi, Ui), where PAi are the random variables that are the parents of Vi in G. This construct allowsnot only the modeling of observational data but also the effects of interventions. An intervention on variableVi in an SCM is defined as a replacement of the mechanism fi with another mechanism fi. The simplestcase is a hard intervention, denoted by do(Vi = vi), where fi simply assigns a constant value vi to Vi. Whenestimating any causal quantities from data, we assume the data to be i.i.d. according to the SCM. Anintervention applied on one individual or unit (each corresponding to a data point) does not affect others.The properties of consistency and not having multiple versions of an intervention (Hernn & Robins, 2021)follow trivially from the definition of an SCM. To model measurement error, we use an SCM with the causal graph G depicted in . Note that thecausal graph is also a Bayesian network that implies conditional independencies according to d-separation(Bishop, 2006). The variables in the graph are the confounders/covariates Z, the accurate treatment valueX which is unobserved, the noisy measurement of the treatment, denoted by X, and the noisy value of theoutcome, denoted by Y . Except for X, the other variables are observed. The SCM with independent noiseterms and no hidden confounders implies the common assumption of causal sufficiency. Therefore, whenthe SCM is identified, any interventional or counterfactual distributions can be computed. These includee.g. Yx, which denotes the distribution of Y after intervening with do(X = x) (alternatively denoted byY |do(X = x)), or the distribution of Yx1 Yx2, which compares the effects of two treatment values x1 andx2, or the distribution of Yx|Y = y, Z = z, which denotes the outcome that would have been obtained fromintervention do(X = x) when in reality no intervention was made and the values Y = y and Z = z wereobserved.",
  "This paper combines themes of statistical and causal estimation, which use related but different definitionsof identifiability. However, we use everywhere the same unified definition:": "Definition 1 (Identifiability of an estimand f). Let {P B,L} be a family of joint probability distributionsof the observed variables B and latent variables L, parameterized by . Denote by P B the correspondingmarginal distribution for B. Then an estimand f() (where f is a deterministic function) is identifiable iffor every , we have",
  "P B = P B f() = f()": "Here f represents anything one is interested in estimating from data (including but not limited to causalparameters), and as a special case when f() = , we get the definition of model identifiability. Intuitively,these mean that knowing the distribution of the observed variables, for example by having estimated it withan infinite amount of i.i.d. samples, the estimand f() (or simply ) can be uniquely determined. In causalinference literature based on SCMs, identifiability often means that an interventional distribution can becomputed from infinite observational data and the causal graph structure (Peters et al., 2017). In contrast,with our definition this computation can additionally use any assumptions made about the underlying SCM. Because X is hidden, the SCM or any causal queries that consider the effects of an intervention do(X = x)are not identifiable without further assumptions, as the relationship between X and the other variables cannot be estimated. Therefore, for identification, we rely on additional assumptions on the measurement errormechanism X = fX(X, UX) such that the SCM becomes statistically identifiable. In the literature, muchattention has been devoted to making the model identifiable using a known error variance or side information.However, we instead rely on the assumptions of independence and zero-mean additive errors (Schennach &Hu, 2013): First, we assume that the measurement error is strongly classical, i.e., the observed treatment X",
  "Y = Y (Z, X) + Y,(2)": "where the noise Y is the exogenous variable UY independent of all other variables except Y and has a zeromean. The conditional mean of Y , Y (z, x) = E[Y |Z = z, X = x], is a deterministic, possibly nonlinearfunction that is continuously differentiable everywhere with respect to x. (Note that by uppercase letters wedenote random variables and by lowercase letters their specific realized values. Thus Y (z, x) refers to thevalue of Y with argument values z and x, while Y (Z, X) denotes the random variable that is obtainedby applying Y to the random variables Z and X.) Although not necessary for identifiability (see .3.1 for details), we further assume Gaussian distributions to facilitate practical computations, yielding",
  "Y N(0, 2),(5)": "We assume that X, Y , and X are real-valued scalars, although multivariate extensions are relatively straight-forward. The covariate Z can be a combination of categorical, discrete, or continuous variables, but the othervariables are assumed to be continuous. Consequently, the learnable parameters are the variance parameters 2 and 2 as well as the (deterministic) functions X, X and Y , which are all parameterized by neuralnetworks to allow flexible dependencies between the variables. The zero-mean assumptions in Equations (4)and (5) are made to achieve statistical identifiability. Otherwise, for any non-zero mean Y , we could obtaina new observationally equivalent model by using Y (z, x) = Y (z, x) Y . Similarly, for any non-zeromean X, we could obtain a new observationally equivalent model by using X(z) = X(z) X andY (z, x) = Y (z, x + X).",
  "CEME+: Causal Effect estimation with Measurement Error with known noise. This is otherwise the sameas CEME, except that the variance of the measurement error 2 is assumed to be known": "Oracle: This method assumes the model in a, where the true treatment X is observed and is useddirectly for fitting the model. Hence, this model provides a loose upper bound on the performance thatcan be achieved by the models CEME and CEME+ which only observe the noisy treatment X. The modelconsists of only one neural network, parametrizing Y (Z, X). Naive: This is the same as Oracle, except that it naively uses the observed noisy treatment X instead ofthe true treatment X when estimating the causal effect (model depicted in b). Hence, it providesa baseline that corresponds to the usual approach of neglecting the measurement error in causal estimation.The model consists of only one neural network, parametrizing Y (Z, X). To summarize the roles of the methods, CEME is our proposed method, CEME+ enables comparison withthe case of a known measurement error variance 2, Oracle is a loose upper bound for performance, andNaive is a baseline. For inference, CEME and CEME+ use amortized variational inference to estimate thelatent variable (described in the next section). In contrast, Oracle and Naive, which do not have latentvariables, are trained using gradient descent with mean squared error (MSE) loss for predicting Y .",
  "(b) Causal graph (incorrectly) assumed by themethod Naive": "model), observed variables whose distributions are modeled (X and Y in our case), and optionally observedvariables whose distributions are not modeled but on which the model is conditioned (the covariates Z inthis paper). We use this method to infer the SCM, after which causal effect estimates of interest can becalculated from the model parameters by Monte Carlo sampling over observed covariates Z. For the distribution p(X, X, Y |Z), the method requires defining a generative model (also called decoder)p(x, x, y|z), where denotes the parameters. In this paper, we use the CEME model defined in Equations(1)(5) with consisting of the standard deviations and as well as the functions X(z), X(z) andY (z, x), which are modeled as fully connected neural networks. (Thus in terms of the practical computa-tions, the parameters are not the functions itself but their respective neural network weights.) AVI also requires modeling the posterior p(X|Z, X, Y ) with a so-called encoder, for which we useq(x|z, x, y) = N (x|q(z, x, y; ), q(z, x, y; )) , where denotes the encoder parameters, which are thefunctions q(z, x, y; ) and q(z, x, y; ), or alternatively, the weights of the two fully connected neural net-works that model these functions. The parameters and are optimized with stochastic gradient descent using the negative evidence lowerbound (ELBO, defined in the next section) as the loss function. This approach can be used for LVMs thatgeneralize the variational autoencoder (VAE) to more than just one hidden and one observed node (Kingma& Welling, 2019) (CEME/CEME+ uses the Bayesian network in ).",
  ":= LK,(9)": "where the expectations are with respect to q(xi,j|zi, xi, yi).For each data point i, there are K i.i.d.realizations of the latent variable x, indexed by j. The inequality in (8) is obtained by applying Jensensinequality.The standard ELBO corresponds to K = 1.For practical optimization, the expectation inEquation (8) is estimated by sampling the K realizations of the latent variable xi,j once per data point i. We use the importance weighted ELBO instead of the standard ELBO because the latter places a heavypenalty on posterior samples not explained by the decoder, which forces a bias on the decoder to com-pensate for a misspecified encoder (Kingma & Welling, 2019). Increasing the number of importance sam-ples alleviates this effect, and in the limit of infinite samples, decouples the optimization of the decoder",
  "Identifiability of causal estimation with a noisy treatment": "The identifiability proof of the CEME model builds closely on an earlier result by Schennach & Hu (2013),of which we provide an overview here. It is an identifiability result on a slightly different measurement errormodel, which differs from ours in that it does not include the covariate Z, but on the other hand it is moregeneral in that it does not assume that X, X and Y are (conditionally) Gaussian. The exact form ofthe result is included in Appendix C. The model includes scalar real-valued random variables Y, X, X, Xand Y related viaY = g(X) + YandX = X + X,(10) where only X and Y are observed.We assume that 1) X, X and Y are mutually independent, 2)E[X] = E[Y ] = 0, and 3) some fairly mild regularity conditions. It is shown by Schennach & Hu (2013)that this model is identifiable if the function g in Equation (10) is not of the form",
  "g(x) = a + b ln(ecx + d),(11)": "and even if it is, nonidentifiability requires x to have a specific distribution, e.g. a Gaussian when g is linear.From this result, we see that the CEME model assumptions of conditionally Gaussian variables (made tosimplify computation) do not help with identification, as it brings the model towards the non-identifiablespecial cases. With these preliminaries, we are now ready to prove the following proposition that establishesthe identifiability of the CEME model: Proposition 1. The measurement error model defined in Equations (1)(5) is model identifiable if 1) forevery z, Y (z, x) is continuously differentiable everywhere as a function of x, 2) for every z, the set = {x :",
  "x Y (z, x) = 0} has at most a finite number of elements, and 3) there exists z for whichY (z, x) is not linear in x (i.e. of the form Y (x) = ax + b)": "Proof. The full proof is provided in Appendix D, but its outline is the following: First, we show that arestricted version of our model where z can only take one value is identifiable as long as Y (z, x) is notlinear in x. This follows from the identifiability theorem by Schennach & Hu (2013) because its assumptionsare satisfied by the restricted version of our model, together with the assumptions of Proposition 1. Second, we show the identifiability of our full model by looking separately at the values of z for whichY (z, x) is or is not linear in x. For the model conditioned on the latter type of z (nonlinear cases),identifiability was shown in the first part of the proof. For the remaining linear cases, we use the fact thatwe already identified and with the nonlinear cases, which results in a linear-Gaussian model with knownerrors, which is known to be identifiable (see e.g. Equations (4.9-4.17) in Gustafson (2003)). Thus the entiremodel is identified. We note that assumption 1) in Proposition 1 is satisfied by the neural networks used in this work, asthey are continuously differentiable due to them using the extended linear unit (ELU) activation function.Assumption 2) is true in general and does not hold only in the special case where the derivative of the neuralnetwork Y (z, x) is zero with respect to x in infinitely many points, which is possible only if the functionis constant on an interval. Breaking assumption 3) would require the network Y (z, x) to be linear in x for every z. Even when this is the case, we still hypothesize that the model is identifiable as long as there aremultiple values of the linear slope for different z. This is suggested by solving a system of equations similarto Gustafson (2003), which appears to have one or at most two distinct solutions. On another hand, theproof of Proposition 1 shows that if there was no z for which Y (z, x) was linear in x, we would not need",
  "Experiments and results": "We conducted two experiments: One with datasets drawn from Gaussian processes, where a large number ofdatasets is used to establish the robustness of our results. Second, we use an augmented real-world dataseton the relationship between years of education and wages, to study the effect of complex real-world patternsfor which the assumptions of our model do not hold exactly.",
  "Synthetic datasets from Gaussian processes": "For the synthetic experiment, we generate datasets whose underlying distribution follows the CEME modelwith all variables being scalars. Gaussian processes (GPs) are used, see Rasmussen & Williams (2006) foran introduction. In brief, a random function f(t) for t Rn is a Gaussian process if f(t1), ..., f(tk) is jointlyGaussian for any k N and any t1, ..., tk Rn. They are denoted by GP(, K) and characterized by a meanfunction (ti) and kernel function K(ti, tj), by which one obtains the mean vector and covariance matrix forthe jointly Gaussian f(t1), ..., f(tk) (for any k N and any t1, ..., tk Rn). In this study, the mean functions(t) are constant. The data generation proceeds according to Algorithm 1. For the GPs, we use the squared",
  "only approximations of true samples from the GPs (steps 2 and 4 in Algorithm 1), as described in AppendixB along with other experiment details. Three datasets generated in this manner are shown in": "The constant L is varied to obtain datasets with different levels of noise, which are either L = 0.1 (smallnoise), L = 0.2 (medium noise), or L = 0.4 (large noise). The different training dataset sizes used are 1000,4000, and 16000 data points. The test data (used for evaluating the models) consist of 20000 data points. Foreach combination of noise level and training dataset size, we generate 200 different datasets (with differentunderlying distributions) using Algorithm 1. For all of these, we fit each model (CEME, CEME+, Naive andOracle) 6 times, and for the results pick the run with the best score on a separate validation dataset (distinctfrom the test dataset). All models use the same neural network architecture to facilitate a fair comparison,i.e. three fully connected hidden layers with 20 nodes each and the ELU activation function. : Error in E[y|z, do(x)] estimation in the synthetic experiment. In addition to data for Naive,missing from the figure are some outliers for CEME and CEME+ for noise level 40% and training datasetsize 1000. Key observations are that 1) the proposed CEME/CEME+ methods offer clear benefit over Naivethat does not account for measurement error, 2) CEME and CEME+ seem to converge with increasingtraining set size and compare relatively well with the loose upper bound Oracle, and 3) CEME handlesunknown measurement error variance well, since CEME+ that knows it, performs only slightly better.",
  "i=1(Y,(zi, xi ) Y (zi, xi ))2,": "where denotes the estimate and the sum is over the N = 20000 data points in the test set. The accuracyof the estimation of is reported in , which uses the metric relative error ( )/, where isthe estimate and the true value. The relative error in the estimation of the measurement error noise isreported in in the same way. The accuracy in the estimation of p(y|do(x)) is presented in and assessed using Average Interventional Distance (AID) Rissanen & Marttinen (2021):",
  ": Error in estimation of X standard devia-tion by CEME in the synthetic experiment. The es-timates seem to converge with increasing training setsize": "To evaluate our methods, we need to know theground truth values of the parameters that we es-timate. Having access to real observed data alonedoes not permit this, which is why we augment thereal data with synthetic variables that mimic thereal ones. The known data generating processes ofthese synthetic variables enable us to compute theground truth. To this end, we first train a neuralnetwork to predict the outcome, and then modifythe dataset by replacing the outcome values with theneural network predictions to which Gaussian noiseis added.The noisy treatment X is obtained byadding Gaussian noise to X. Six separate datasetsare created, each corresponding to a different levelof SD of X. The levels are proportional to the SDof X, and are 0%, 20%, 40%, 60%, 80% and 100%.The choice of education years as treatment was inpart motivated by the need to know the true treat-ment accurately to obtain the ground truth causaleffect.The CEME/CEME+ models are misspeci-fied for this dataset because the true treatment, thenumber of education years, is ordinal, instead of con-ditionally Gaussian, as the models assume.Thispresents an opportunity to evaluate the sensitivity of the CEME/CEME+ algorithms to model misspecifi-cation.",
  "(b) Error in p(y|do(x)) estimation": ": Error in causal effect estimation in the education-wage data experiment. The proposedCEME/CEME+ methods offer improvement compared to the baseline Naive, but not as much as in thesynthetic experiment. A likely reason for this is that CEME/CEME+ incorrectly assume a continuous X,unlike Oracle and Naive. Moreover, the difference between CEME and CEME+ is larger than in thesynthetic case. We run this experiment for the same four algorithms as in the synthetic experiment, defined in Sec-tion 2.1, with three hidden layers of width 26 and the ELU activation function.The training proce-dure and model structures are the same as in .1 except that now Z is multivariate and hy-perparameters are different.Details on the experiment and how the semisynthetic datasets were cre-ated are available in Appendix B. The full data of 2990 points is split into 72% of training data, 8%of validation data (used for learning rate annealing and early stopping) and 20% of test data (usedfor evaluating the models), all amounts rounded to the nearest integer.The code used for data pre-processing and running the experiment is available at",
  "Results": "The main results of the experiment with education-wage data are presented in Figures 9a, 9b, 10a and10b. They use the same metrics as the corresponding results for the synthetic data experiment. The pointsrepresent median values and the bands represent interquantile ranges. Each data point corresponds to onetraining run of the model, and the x-axis in each figure indicates which dataset was used (they differ onlyin the SD of the augmented noise X). There is no entry for CEME+ for the 0% X SD dataset, becauseusing variational inference for a model with no hidden variables is not useful and is problematic in practicebecause terms in the ELBO become infinite. We notice that while CEME and CEME+ still offer a clear improvement over not accounting for measurementat all (Naive), they seem to be further below in performance from Oracle (which acts as a benchmark foroptimal, or even beyond-optimal performance for the CEME/CEME+ algorithms). A potential reason forthis is that the CEME models, which model the true number of education years as a continuous latentvariable, are misspecified unlike Oracle and Naive, which only condition on the number of education yearsbut do not model it (though Naive assumes that there is no measurement error). Moreover, CEME andCEME+ have more parameters than Oracle and Naive; therefore they could be hurt more by the limiteddataset size and high-dimensional covariate. The larger difference between CEME and CEME+ than in thesynthetic experiment might be because CEME+ avoids some of the detriment from model misspecificationby having access to the true standard deviation of X.",
  "(b) Estimation of Y standard deviation": ": Error in the estimation of treatment and outcome noises in the education-wage dataexperiment. On the left, only CEME is depicted because it is the only method that estimates X standarddeviation. These figures help demonstrate that the proposed CEME/CEME+ offer clear benefit over thebaseline Naive even when they are misspecified.",
  "Conclusion": "In this study, we provided a model for causal effect estimation in the presence of treatment noise, withcomplex nonlinear dependencies, and with no side information or knowledge of the measurement errorvariance. We confirmed the models identifiability theoretically and evaluated it experimentally, comparingit to a baseline that does not account for measurement error at all (Naive) and to an upper bound inperformance in the form of Oracle.A notable advantage of the model is its flexibility: It offered goodperformance on a diverse set of synthetic datasets and was useful for correcting for measurement error evenon a real-world dataset for which it was clearly misspecified.Our approach is also flexible in a secondway: after using amortized variational inference to infer the SCM and the posterior of hidden variables, anyinterventional or counterfactual distributions may be computed. A limitation of the CEME/CEME+ methods is their assumption of independent additive noise for bothtreatment and outcome, which might not always hold Schennach (2016). On the other hand, these assump-tions were critical for attaining identifiability without side information; therefore relaxing them would likelymean having to introduce side information to the model Schennach (2020). Another limitation of our study isthe lack of comparison with other state-of-the-art methods. However, the authors are not aware of any thatexist which estimate the same quantities as our method and are designed for the setting where the treatmentis noisy, the regression function does not have any strict parametric form, the model is conditioned on acovariate, and no side information is available. In addition, our study does not consider the robustness ofestimation when there is no access to a perfect set of covariates satisfying the backdoor criterion. Finally, the model is limited by assuming Gaussian distributions in all conditionals. An interesting directionfor future research could be to relax this assumption by using flexible distributions, such as normalizingflows. The identifiability result in Schennach & Hu (2013) suggests a model generalized in this way wouldretain its identifiability.In addition, since it is straightforward to generalize the amortized variationalapproach for other related latent variable models, an interesting future direction would be to apply thealgorithm to models that relax the classical measurement error assumptions Schennach (2016; 2020), forwhich efficient inference methods are not yet available.",
  "Broader Impact Statement": "Our method could be used for estimating treatment effects in applications where incorrect inferences mightin the worst case lead to severe adverse outcomes, e.g. in healthcare. For this reason it is important toconsider the validity of the assumptions of our method in any particular use case as well as to test anyimplementations carefully before practical deployment.",
  "Author Contributions": "All authors have accepted responsibility for the entire content of this manuscript and approved to its sub-mission. AP had the main responsibility on all aspects of the work. PM supervised the work and providedthe initial research idea. The authors wrote the paper together. This work was supported by the Academy of Finland (Flagship programme: Finnish Center for ArtificialIntelligence FCAI, and grants 336033, 352986, 358246) and EU (H2020 grant 101016775 and NextGenera-tionEU).",
  "John P Buonaccorsi. Measurement error: models, methods, and applications. Chapman and Hall/CRC,2010": "Yuri Burda, Roger B. Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. In Yoshua Ben-gio and Yann LeCun (eds.), Proceedings of the 4th International Conference on Learning Representations(conference track), 2016. David Card. Using geographic variation in college proximity to estimate the return to schooling. In Louis NChristofides, E Kenneth Grant, and Robert Swidinsky (eds.), Aspects of Labour Market Behaviour: Essaysin Honour of John Vanderkamp, pp. 201222. University of Toronto Press, 1995.",
  "AReview of literature on the identifiability of related models": "CEME with categorical variables. Identifiability is proven by Xia et al. (2020) for a model with thesame Bayesian networks as ours () where the treatment X and covariate Z are categorical. Forthis result, misclassification probabilities need to have a certain upper bound to avoid the label switchingproblem. It is also assumed that the conditional distribution Y |Z, X is either a Poisson, normal, gamma, orbinomial distribution. This scheme also allows dealing with missing treatment values by defining a missingvalue as an additional category. Measurement error models from a statistical perspective.There is a wide variety of researchon measurement error solely from a statistical (non-causal) perspective, studying assumptions under whichmeasurement error model parameters can be identified, and exploring different practical methods for inferringthem. These include the general method of moments (GMM) as well as kernel deconvolution and sieveestimators (Yi et al., 2021; Schennach, 2020). Many of our assumptions are relaxed, such as having Gaussiannoise terms, the independence of the measurement error X or outcome noise Y of the other variables,and the outcome Y being non-differential, i.e. independent of X. See Schennach (2016; 2020) for a reviewon this kind of results. A wide variety of literature also studies measurement error models with additional assumptions compared toours. These consist mainly of assuming the availability of side information such as repeated measurementsor instrumental variables (IVs), or assuming a strict parametric form (e.g. linear or polynomial) for theregression function E[Y |Z, X] (and E[X|Z]). See Yi et al. (2021) for a review. The IVs differ from ourcovariate Z in that they are conditionally independent of Y given X. On the other hand, Ben-Moshe et al.(2017) consider a covariate Z that directly affects Y . However, they assume that the effects of Z and X onY are either decoupled or E[Y |Z, X] has a polynomial parametric form. Related latent variable models. There are also latent variable models studied that resemble the mea-surement error model. These include the causal effect variational autoencoder (CEVAE), where only a proxyof the confounder is observed (Louizos et al., 2017; Rissanen & Marttinen, 2021) and nonlinear independentcomponent analysis (Khemakhem et al., 2020). Also, Schennach (2016; 2020) consider the identification oflatent variable models more generally.",
  "BExperiment details": "Amortized variational inference is prone to getting stuck at local optima, for example, in the case of theso-called posterior collapse (Kingma & Welling, 2019). To counter this, we start training in both experimentswith an increased but gradually annealed weight for the ELBO terms log q(xi,j|si) and log p(xi|xi,j), whichcauses the model to initially predict posterior x close to x, to ensure that the posterior does not get stuckat the prior. A related approach was taken by Hu et al. (2022), where the model was pre-trained assumingno measurement error.",
  "B.1Synthetic experiment": "In the synthetic experiment data generation, the functions X, X are only approximately sampled froma GP to save in computation cost, as with exact values the size of the GP kernel scales proportionally to thesquare of the number of data points, and the matrix operations performed with it scale even worse. Thus,we sample only K = 1000 points from the actual Gaussian processes corresponding to each of the functionsX, X and Y . These functions are then defined as the posterior mean of the corresponding GP, giventhat the K points have been observed. The points are evenly spaced such that the minimum is the smallestvalue in the actual generated data minus one quarter of the distance between the smallest and largest valuesin the actual data. Similarly, the maximum is the largest value in the actual data plus the distance betweenthe smallest and largest values in the actual data. This is achieved by first generating the actual valueszii=1..N, then sampling the K points for X, and X, then generating xi=1..N and then finally sampling",
  "the K points for Y .) For Y , as it has two arguments, we used K = 312 = 961 points arranged in atwo-dimensional grid": "When training the model, learning rate annealing is used, which means that if there have been a set numberof epochs without improvement in the validation score, the learning rate will be multiplied by a set learningrate reduction factor. The training is stopped when a set number of epochs has passed without the validationscore improving. The hyperparameter values used are listed in . They were optimized using a random parametersearch. We use 8000 data points as the validation set used for annealing learning rate and for early stopping.Unless otherwise stated, the same hyperparameter value was used for all algorithms and datasets. From allthe 43200 runs in the synthetic experiment, only 33 runs crashed, yielding no result.",
  "B.2Experiment with education-wage data": "For the education-wage dataset by Card (1995), the covariates used were personal identifier, whether theperson lived near a 2 year college in 1966, whether the person lived near a 4 year college in 1966, age, whetherthe person lived with both parents or only mother or with step parents at the age of 14, several variables onwhich region of USA the person lived in, whether the person lived in a metropolitan area (SMSA) in 1966and/or 1967, whether the person was enrolled in a school in 1976, whether the person was married in 1976,and whether the person had access to a library card at the age of 14. The semi-synthetic dataset used in our experiments is obtained from the original dataset by Card (1995) asfollows: We exclude multiple covariates present in the original dataset that correlate heavily with the numberof education years and would thus make measurement error correction much less useful. These include thenumber of schooling years of the mother and father, their intelligence quotients, Knowledge of the World ofWork (KWW) score, work experience in years, its square, work experience years divided into bins, and wage.We also drop NLS sampling weight. Missing values are handled by dropping all data items that containthem. This reduces the size of the dataset from 3010 to 2990. Then, both X (number of education years) and all covariates in Z are scaled to have a zero mean andunit variance. The noisy treatment X is obtained by adding a normally distributed additive noise X toX. Six separate datasets are created, each corresponding to a different level of SD of X. The levels areproportional to the SD of X, and are 0%, 20%, 40%, 60%, 80% and 100%.",
  "DProof of Proposition 1": "Proof. First, consider the measurement error model defined in Equations (1)(5) conditioned on a specificvalue Z = z, effectively removing Z as a variable. We show that this model, called the restricted model, asopposed to the original full model, satisfies the assumptions in Theorem 1, which thus determines when therestricted model is identifiable. First, the restricted model satisfies Equations (13) and (14). Assumption 1follows directly from the definition of the model, as |X| and |Y | follow the half-normal distribution, whichhas the known finite expectation",
  "/. Assumption 2 is satisfied because X and Y are Gaussian sotheir characteristic functions have the known form exp(it 2t2/2) and thus do not vanish for any t R(t is denoted by or in Assumption 2)": "Assumption 3 of Theorem 1 is not needed because it is in its proof by Schennach & Hu (2013) only used tofind the distributions of the errors X and Y given that the density fx(x) and regression function g(x)are known. However, in our case we already know by assumption that the error distributions are Gaussianwith a zero mean, and moreover, we can find their standard deviation from",
  "andVar[X] = Var[X] + Var[X],": "which hold because of the independence of Y (z, X) and Y as well as X and X, respectively. Assump-tion 4 is satisfied because X|Z = z is normally distributed and thus admits a uniformly bounded densityw.r.t. the Lebesgue measure that is supported everywhere. Assumption 5 is the same as assumption 1 ofour Proposition 1. Assumption 6 follows from assumption 2 of Proposition 1 since the density of X|Z = zis continuous and nonvanishing everywhere. With the assumptions of Theorem 1 satisfied, we check what the cases 1-3 therein imply for our restrictedmodel. From case 1 we obtain that it is identified except when Y (z, x) as a function of x is of the forma + b ln(ecx + d).",
  "fx(x) = A exp(BeCx + CDx)(eCx + E)F(17)": "with C R and A, B, D, E, F [0, ). We see this by taking a logarithm of both the Gaussian density andthe density in Equation (17) and noting that the first is a second-degree polynomial, but the latter is a sumwithout a second-degree term, so they are not equal. Thus, our model is identifiable in this case. The remaining case is for when Y (z, x) is linear in x. In this case, the restricted model is not identifiedas x is normally distributed and both x and y have normal factors as they are normal themselves. Next, we prove the identifiability of the full model, i.e. where z may take any value in its range. We start byassuming in accordance with Definition 1 that two conditional observed distributions from the model matchfor every z:z, x, y :p(x, y|z) = p(x, y|z).(18)",
  "by using Definition 1 on the restricted model, which is identified according to the first part of the proof": "Similarly, we obtain the equalities in (19) for every z for which Y (z, x) is not linear in x. Thus, forthe full equality of the models (i.e. for = ), it remains to be shown that we obtain X(z) = X(z),X(z) = X(z) and Y (z, x) = Y (z, x) also for those z for which Y (z, x) is linear in x. Noting thatwe already know that = and = , we obtain for such z a linear-Gaussian measurement error modelwith known measurement error, which is known to be identifiable (see e.g. Gustafson (2003)). Thus ="
}