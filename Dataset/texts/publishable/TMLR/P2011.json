{
  "Abstract": "In this paper, we focus on the important yet understudied problem of Continual FederatedLearning (CFL), where a server communicates with a set of clients to incrementally learnnew concepts over time without sharing or storing any data. The complexity of this problemis compounded by challenges from both the Continual and Federated Learning perspectives.Specifically, models trained in a CFL setup suffer from catastrophic forgetting which isexacerbated by data heterogeneity across clients. Existing attempts at this problem tendto impose large overheads on clients and communication channels or require access tostored data which renders them unsuitable for real-world use due to privacy. In this paper,we attempt to tackle forgetting and heterogeneity while minimizing overhead costs andwithout requiring access to any stored data. We study this problem in the context of VisionTransformers and explore parameter-efficient approaches to adapt to dynamic distributionswhile minimizing forgetting. We achieve this by leveraging a prompting based approach(such that only prompts and classifier heads have to be communicated) and proposing anovel and lightweight generation and distillation scheme to consolidate client models at theserver. We formulate this problem for image classification and establish strong baselines forcomparison, conduct experiments on CIFAR-100 as well as challenging, large-scale datasetslike ImageNet-R and DomainNet. Our approach outperforms both existing methods and ourown baselines by as much as 7% while significantly reducing communication and client-levelcomputation costs. Code available at",
  "Introduction": "Federated Learning (FL) is a privacy-preserving learning paradigm that enables learning a global modelthrough communication with a distributed set of clients. These clients have exclusive access to private data,and collaborate with a central server to learn a shared task by communicating parameters such as modelweights, gradients, or learning statistics. For example, the popular FedAvg McMahan et al. (2023) methodworks by iteratively aggregating client models by averaging their model weights. Classical FL methods suchas FedAvg have garnered significant attention due to the increasing demand for user privacy and the growthof edge computing. However, currently most federated learning methods focus on learning statically, that is across a fixed set ofcategories determined a-priori. In non-federated works, on the other hand, there has been a great deal ofprogress on learning an increasing number of categories incrementally, referred to as continual learning (andmore specifically class-incremental learning) Hsu et al. (2018); van de Ven & Tolias (2019). In addition to theproblem of catastrophic forgetting, incremental learning introduces challenges to typical federated learning(FL) scenarios by inherently involving non-Independent and Identically Distributed (non-IID) data, whichhas been shown to cause issues of model divergence Zhao et al. (2018); Li et al. (2020b). While heterogeneousfederated learning Li et al. (2020b) approaches have been developed, they do not support the dynamic datadistributions that occur in continual learning and the real-world. For example, such a setting has immense",
  "practical impact to applications such as healthcare, autonomous vehicles, and chat-bots Rieke et al. (2020);Nguyen et al. (2022)": "Therefore, in this paper we look at the understudied problem of Continual Federated Learning (CFL) Yoon et al.(2021); Ma et al. (2022); Qi et al. (2023). While a few CFL methods exist, they address the issue of forgettingusing approaches such as: reducing inter-client interference, knowledge distillation, and image-level generativereplay Yoon et al. (2021); Dong et al. (2022); Zhang et al. (2023). Specifically, they often communicate fullmodel weights, real/synthesized image-level data, or gradients. Additionally, some methods store old data inmemory buffers or train a generative model to mimic local data; at the very least, all methods share completemodels parameters with the server which can lead to privacy leaks with advancements in model inversionand other extraction techniques Carlini et al. (2023). As a result, many of these methods fail to effectivelyuphold the principles of CFL, such as communication efficiency, computational efficiency, and privacy. To mitigate forgetting while adhering to the core principles of CFL, we propose HePCo: HeterogeneousPrompt Consolidation (). Our method is driven by the goals of (i) minimizing communication costs, (ii)improving client privacy, and (iii) client-level computation efficiency. We first propose to leverage prompting-based methods, which have shown successful results in the rehearsal-free continual learning setting Wanget al. (2022c;b). This also has the benefit of utilizing frozen Vision Transformer backbones, meaning thatonly prompts and classifiers have to be transmitted, reducing communication. The key contribution of ourapproach is then to answer the question of how to merge prompts from different clients in a scalable manner.Towards this end, we propose a lightweight method for generating pseudo-data in the latent space and distillingclient model information. Importantly, we distill data from both the past task label distribution as well asthe current task label distribution, preventing both catastrophic forgetting and performance degradation dueto client heterogeneity. In summary, we make the following key contributions:",
  ". We modify and extend popular continual prompting based approaches to the setting of ContinualFederated Learning (CFL), implementing a range of methods that we will open-source": "2. We introduce a light-weight pseudo-latent knowledge distillation mechanism to aggregate client mod-els trained on non-IID data partitions. Importantly, our approach incurs no additional client-sidecomputation and does not store or generate training data. 3. We outperform both existing methods and contributed baselines by as much as 7% while drasticallyreducing communication costs by only sharing a small set of model parameters, which constitutes only~9.5% of the total parameters.",
  "Related Work": "Prompting for Continual Learning. Continual learning algorithms fall into several primary groups.Some methods involve expanding the models architecture as new tasks arise Ebrahimi et al. (2020); Leeet al. (2020); Lomonaco & Maltoni (2017); Maltoni & Lomonaco (2019); Rusu et al. (2016), while othersregularize the model with prior task knowledge either in the weight space or the prediction space Ahn et al.(2021); Aljundi et al. (2018); Hou et al. (2018); Kirkpatrick et al. (2017); Li & Hoiem (2017); Zenke et al.",
  "Published in Transactions on Machine Learning Research (09/2024)": "James Smith, Yen-Chang Hsu, Jonathan Balloch, Yilin Shen, Hongxia Jin, and Zsolt Kira. Always bedreaming: A new approach for data-free class-incremental learning. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pp. 93749384, October 2021. James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle,Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-basedprompting for rehearsal-free continual learning. arXiv preprint arXiv:2211.13218, 2022. Accepted forpublication at CVPR 2023.",
  "In this section, we describe the formulation by introducing the class-incremental learning and heterogeneousfederated learning aspects in our CFL setting": "Class-Incremental Federated Learning. We focus on the class-incremental learning scenario, where amodel is tasked with learning new classes over time. Under this setting, a global model is learned through asequence of N global tasks T = {T 1, T 2, ..., T N}. Following the standard assumption in continual learning, thesets of categories1seen across distinct global tasks are mutually exclusive. As this is done in a federated setup,each task is learned through R independent rounds by randomly sampling a set of stateless clients C = {c1,c2, c3, ..., cS} in each round. In a stateless setting, the total number of clients is kept very large to simulatereal-world FL applications (like mobile devices). The server does not keep track of clients as new clients arevisited in each round.Further, previously seen data cannot be accessed at any time during the training. Heterogeneous Federated Learning. To simulate a real-world heterogeneous federated learning scenario,we use three configuration parameters to control the level of heterogeneity with increasing granularity: splitratio, category ratio, and imbalance ratio. At the top level, the most common heterogeneity is varying localdataset sizes across the clients. A specific client ci can be exposed to a subset of the current task datasetDt as their local dataset Dti, and the size |Dti| varies from each other. We denote this as the split ratio = |Dti|/|Dt|. At a lower level, the local dataset Dti consists of a subset of the categories from those in thecurrent global task. Specifically, a global task T t consists of categories Kt, where |Kt| denotes the number ofunique categories. In a given round r, each client ci sees data containing Kti Kt categories. We denote = |Kti|/|Kt| as the category ratio which is a value between 0 and 1. At the lowest level, each category canhave a different amount of data. We follow Cao et al. (2019) to also create a long-tail distribution for localdata which is different for each client. This distribution is governed by an imbalance ratio . If = 1, eachclient ci is allocated samples uniformly from Kti categories. In summary, a smaller split ratio , a smallercategory ratio , or a smaller imbalance ratio increases heterogeneity thereby increasing the complexity ofthe task. Formalizing the setting in this manner enables us to methodically vary the parameters, therebysimulating a heterogeneous setting. This formalization unifies the setups previously employed separatelyacross federated and continual learning landscapes Li et al. (2020a); Rebuffi et al. (2017a) combining theirdistinct characteristics. As discussed before, combining client models in a heterogeneous setting causes the obtained model toforget global knowledge from the previous rounds as shown by Lee et al. (2022); Hsu et al. (2019). Also,training clients on locally-available datasets induces forgetting of global knowledge outside the localdistributions. Intra-task forgetting Ma et al. (2022) measures such performance drops induced by the dataheterogeneity (non-IIDness) across clients. Inter-task forgetting measures the drop in performance on oldtasks {T 1, T 2, ..., T t1} after learning a new task T t.",
  "Background: L2P": "L2P (Learning to Prompt) Wang et al. (2022d) is a continual learning method that learns a set of modelembeddings (prompts) that can be dynamically inserted into a pretrained vision transformer. (ViT) Dosovitskiyet al. (2020). Prompts hold task-specific information that instructs the model to solve the corresponding tasks.L2P maintains a prompt pool P = {P1, P2, , PM} of size M, where Pi RLpD are prompt parameterswith Lp as the prompt length (chosen as a hyperparameter) and D the embedding dimension. Each prompt",
  ": Latent generation and distillation with underlying decomposed prompting scheme": "Pi has an associated key ki RD. An input image x is converted into a visual query q(x) RD by passingit through the frozen vision transformer encoder pt. Prompts are selected from the pool by measuringthe cosine similarity between associated keys and the visual query. Top N prompts that have the highestkey-query similarity are chosen to be inserted into the transformer. Here, the ViT encoder is frozen and theprompts and keys are learned using two separate optimization losses.",
  "Method": "In this section, we describe our novel approach called HePCo (Heterogenous Prompt Consolidation) whichtackles forgetting and heterogeneity using a data-free distillation strategy applied in the models latent space.Knowing the limitations of FedAvg under non-IID data partitioning, we propose to fine-tune the global modelat server side by distilling knowledge from client models. However, unlike prior CFL works, we first proposeto leverage the current state of art prompting methods in continual learning. Such methods optimize learnableparameters that augment the input to a transformer model (prompt tuning) or its underlying attentionmechanism (prefix tuning). These methods have been shown to obtain strong performance in traditionalrehearsal-free continual learning settings. These prompting-based approaches Wang et al. (2022d); Smithet al. (2022) can be thought of implicit mechanisms to isolate model parameters across tasks. Implementinga prompting scheme at the client level can thus prevent forgetting of past tasks while efficiently adapting tonew tasks. Despite these advantages, there is a key challenge in applying prompting to a federated setting: It is not obvioushow the server should combine prompts learned by the individual clients on heterogeneous sources of data.Naively averaging the prompt weights is suboptimal (as we show in Sec. 6) and simply maintaining a growingprompt pool scales poorly with the number of clients. Our key novelty is therefore to propose a lightweightdistillation method, applied to the latent-space of the model, which greatly mitigates intra-task and inter-taskforgetting. Crucially, rather than averaging different client prompts, we perform distillation of these promptsin a data-free manner by generating pseudo-data. Importantly, the generation and distillation operationsare computationally cheap as they are carried out in the latent space of the model. This design prioritizesprivacy and efficiency, which are crucial for federated learning. In summary, with our method, communicationcosts between clients and the server are low, heterogeneous information is effectively aggregated, and themethod achieves state-of-art performance. Below we detail our method and depict it in .",
  "Client Side: Decomposed Prompting": "While L2P is quite successful in protecting against forgetting, its performance is limited by certain designchoices, as noted by Smith et al. (2022). We therefore adapt L2P to side-step these issues, including forour baselines, resulting in better accuracy across the board. Specifically, rather than using discrete promptsobtained via a hard maximum function which restricts capacity and introduces an additional hyperparameter(N, corresponding to top-N prompts), we form our final prompt p by taking sum of the Pi weighted bytheir cosine scores. Such a prompt is inserted into a subset of the self attention layers of the ViT encoder.This subset of layers is determined by hand as in Smith et al. (2022). In the federated learning setting, each client hosts the aforementioned prompting scheme and learns the keyand prompt matrices along with the classifier in an end-to-end fashion while keeping the ViT backbone frozen.After learning the local task, in our method, the clients transfer the key, prompt and classifier weights to theserver. By sending only a limited number of parameters to the server, the overall communication load issignificantly reduced compared to sharing the entire model. Our approach requires clients to share only theprompt and classifier weights. The server lacks knowledge of the exact locations for inserting the prompts, asthis information is known only to the respective clients. To reconstruct the exact client model, the serverwould need to conduct an exhaustive search over the number of layers in the transformer and try variouscombinations to determine the precise insertion positions. This method represents a positive step towardsenhancing client privacy compared to approaches that share complete model weights.",
  "cC wc. We call these as theprovisional server weights": "Due to data heterogeneity, the local model weights diverge which degrades the performance of this aggregatedmodel. We therefore propose to fine-tune the server model using data-free distillation in the latent space toprevent this degradation. We generate pseudo data in the latent space of the visual query q(x) RD which isessentially the output space of the vision encoder. The advantage of generating in this space is that it allows usto fine-tune both the classifier and the key-prompt weights without needing a forward-pass through the encoder.We use a lightweight feedforward neural network as our conditional generator with a D dimensional output.This generator takes as input a class label (from categories in current task t) and a noise vector of dimensionDnoise sampled from the standard normal distribution N(0, 1). We encode the class label using an embeddinglayer and concatenate the obtained class embedding with the noise vector to form the input of the generator.From the generator, we obtain a pseudo latent of dimension D conditioned on the class label as follows:",
  "z = G(, y; gen),(1)": "where z RD is the generated pseudo latent and RDnoise N(0, 1) is the noise vector. For effectiveknowledge distillation, pseudo data should conform to the latent space of the client models. We optimizefor a classification loss which is a weighted sum of classification losses for each individual client, similar toZhang et al. (2022b). The total classification loss can be given as:",
  "cCLCE((z; wc), y)(3)": "where Lccls is the cross-entropy loss between the prediction of local model c given latent z and sampled classlabel y. Here, denotes the classifer (last layer). However, optimizing for just the classification loss encouragesthe generator to produce pseudo latents which are easy to be classified and hence less effective for distillation.Our goal is to generate latents that create a discrepancy between the provisional server model and clients,thereby providing a learning opportunity for the server. To promote the generation of such hard samples, we",
  "mingen EN(0,1) [LclsKLLKLMSELMSE](5)": "The trained generator is then used to perform data-free knowledge distillation which helps combat intra-taskforgetting and allows the server model to achieve a high accuracy on the global data distribution of thecurrent task. However, as the generator is trained to generate pseudo-data corresponding to the current taskdistribution only, a model fine-tuned with this pseudo-data suffers from inter-task forgetting as shown in ourablation experiments in .2.1. To prevent this, we train a separate copy of the generator (gen) togenerate latents corresponding to the previously seen tasks. At the server side, we assume access to the key,prompt and classifier weights corresponding to the previous tasks global model which is the fine-tuned globalmodel after the Rth round of the task t1. Similar to above, we optimize for the classification and disagreementlosses jointly. Here the classification loss Lcls is computed for the previous task server model and LKL andLMSE are computed between this and the provisional server model. We empirically show that using pseudo-data corresponding to past tasks for knowledge distillation helps mitigate inter-task forgetting to a great extent.",
  "Server Side: Latent Space Knowledge Distillation": "Once the generator is trained to generate pseudo-latents corresponding to the current and previous tasks,we use it to efficiently fine-tune the provisional server model w. We use the pseudo-latents obtained from thegenerator to fine-tune both the classifier head and key-prompt weights (K and P) without requiring a forwardpass through the full model. We use the key, prompt and classifier weights corresponding to the currentround client models and the last-task server model to fine-tune the server model. As it operates in a lowdimensional latent space and updates a small subset of parameters, this distillation process is much cheaperin terms of computation compared to training the entire model. Also, this design does not require clientsto share entire models with the server which reduces the client-server communication costs to a great extentand improves privacy of the client model. While we introduce additional server overhead, it is importantto note that, in the context of CFL, clients are typically edge devices with limited computing power, whileservers have ample computational resources. We prioritize client-level efficiency while making efficient useof the servers resources. In .2.2, we compute this overhead and show that it is competitive to thatincurred by existing state-of-the-art (SOTA) methods. To perform knowledge distillation, we first generate a batch of pseudo-data from the generators correspondingto the current round and previous task. We mix the current and previous task batches to form a singlecomposite batch according to a hyperparameter named replay ratio which determines the size of the previoustask batch relative to the current round batch. We use this composite batch of pseudo-latents to fine-tunethe key-prompt weights and the classifier weights separately. To fine-tune the key-prompt weights, we first obtain distillation targets in the form of final prompts p bypassing the pseudo latents through the prompting mechanism of the teacher models (clients and previous-taskserver model). Notably, we do not require full models to generate these targets, as having only the key-promptand classifier weights is sufficient. Now, to fine-tune the key-prompt weights of server model, we optimize",
  "cCLcMSE + yt1Lt1MSE,(6)": "where LcMSE denotes the MSE loss between client c and the provisional server model and Lt1MSE loss between the provisional model and the previous task server model. Further, y MSE denotes thet1 is an indicatorvariable which is set to 1 if y was seen in previous tasks and 0 if present in current task.Next, we fine-tune the classifier layer of the provisional server model.As discussed in .2,pseudo-latents were obtained from the generator by conditioning on randomly sampled class labels. Thecomposite batch of pseudo-latents used in the previous step is employed here again as input to the classifier,and the cross-entropy loss is computed between the predictions of the provisional server and the class labelsupon which the pseudo-latents were conditioned. This approach allows us to fine-tune the classifier weightsusing the same batch of pseudo-latents used to fine-tune the key-prompt weights. Operating in the latentspace allows us to efficiently fine-tune the key-prompt and classifier modules without requiring forward passesthrough the entire model. Our ablation studies in .2.1 highlight the efficacy of this approach.",
  "Experiments": "Model Architecture. We use the ViT-B/16 backbone Dosovitskiy et al. (2020) pretrained on Imagenet-1KRussakovsky et al. (2015) as the encoder for our method and all baselines. We use a prompt pool size (M) of100 and a prompt length (Lp) of 20 with dimension (D) being 768 and insert prompts into 1-5 Multi-headSelf Attention (MSA) layers of the ViT encoder following the standard practice Wang et al. (2022b) andperform prefix-tuning as done in Smith et al. (2022), by prepending prompts to the keys and values of theMSA layers. The classifier () is a fully-connected layer with input dimension D and output dimension equalto the number of classes. We implement the generator gen using a three layer fully-connected network andtrain it for 100 epochs. We encode the class label using an embedding matrix and concatenate the classembedding to the sampled noise vector before feeding into the generator. Datasets. We conduct our experiments on three image classification datasets. First, we adapt CIFAR-100Krizhevsky et al. (2009) to our formulation as it is a commonly used benchmark in CFL. Additionally,we evaluate our methods on the larger-scale ImageNet-R Hendrycks et al. (2021) and DomainNet Penget al. (2019) which have been used in recent continual learning works Smith et al. (2022) but havent beenexplored in a continual federated learning setting. These datasets capture real-world distribution shiftsthat can be challenging for models pre-trained on ImageNet to generalize to. The total number of classesfor CIFAR-100, ImageNet-R and DomainNet are 100, 200 and 345 respectively. We divide these datasetsinto 10-task (CIFAR-100, ImageNet-R) and 5-task (DomainNet) benchmarks. The 10-task setups containa longer task sequence with small number of classes per task whereas the 5-task setup has a shorter tasksequence with more classes per task. CIFAR and ImageNet have 10 and 20 classes, while DomainNet has69 classes per task. Following Wang et al. (2022a), we use 20% of the training set as our validation datasetto determine hyperparameters for our approach and all competing baselines. Configuration. We learn each task through R = 10 communication rounds by selecting C = 5 stateless clientsper round. Thus, we have 100 total rounds for a 10-task setup and 50 for a 5-task setup. For all experimentsreported in Tables 1-3, we use a category ratio = 0.6 which means that if a task contains 10 categories,each active client is randomly assigned 6 of these categories. We analyze the affect of different category ratiosin Sec. 6.2.1. Overall, HePCo outperforms existing methods across all category ratios, particularly excellingin scenarios with smaller category ratios, which signify higher heterogeneity and, consequently, a higher levelof task complexity. Further, we use a split ratio = 0.1 which allows a client to be assigned 10% of theimages corresponding to the subset of categories. We train local models for 10 epochs per round.",
  "Imbalance ratio () = 0.05 = 0.01 = 0.05 = 0.01 = 0.05 = 0.01": "FedAvg-FT8.81 1.539.18 1.269.26 1.028.88 1.2413.02 1.2911.65 1.84FedLwF.MC Rebuffi et al. (2017a)50.40 0.8840.39 1.0619.94 0.7813.34 1.4157.34 0.8452.46 0.72FedAvg-Prompt62.72 1.7954.43 1.5736.51 0.8628.16 1.1247.73 1.2543.23 1.03Fed-CPrompt Bagwe et al. (2023)65.42 1.1256.85 1.7939.03 1.0430.14 1.1654.44 0.8049.96 0.74CFed Ma et al. (2022)70.26 1.2062.04 1.6234.62 1.4125.74 1.0859.89 0.6855.22 0.80TARGET Zhang et al. (2023)66.47 1.2258.13 1.5430.20 1.3519.84 1.4156.44 0.4551.82 0.58HePCo (Ours)70.34 1.0861.70 1.4845.45 0.9841.68 1.4461.10 0.7658.82 0.84 Chaudhry et al. (2019a); Lopes et al. (2017) FN which measures the drop in performance on previous tasksafter learning a new task averaged over all N tasks. As noted by Smith et al. (2022), AN is the moreinformative metric as it encompasses both forgetting and plasticity (new task performance). Our approachprovides parameter efficiency which is reflected by reduced communication and local computation costs; albeitat the expense of a small overhead at the server. We quantify this communication efficiency by specifying thenumber of parameters shared by our approach relative to the original model size in 6.2.2. Baselines. We compare our method against existing state-of-the-art approaches: TARGET Zhang et al.(2023), CFed Ma et al. (2022) and Fed-CPrompt as well as two strong baselines that we introduce. TARGETand CFeD mitigate forgetting by replaying old task data through generative methods or by assuming accessto surrogate datasets. Fed-CPrompt uses a prompting-based approach similar to ours but introduces acontrastive loss at the client side. This additional loss aims to mitigate forgetting by alleviating heterogeneityacross clients and tasks. At the server side, Fed-CPrompt aggregates clients using the standard FedAvgalgorithm. In contrast, our approach does not introduce any additional computation at the clients end.Instead, we focus on altering the aggregation procedure at the server to combat forgetting. For fair comparison,we adapt Fed-CPrompt to our experimental setup and use the same Vision Transformer (ViT) architecture asin our method. Similarly for all other methods, we adapt their implementations to use the same ViT backbonewith proper modifications. We tune hyperparameters for our proposed method and all compared baselines.We observe that the FedLwF method Ma et al. (2022) used in prior CFL work performs poorly owing to thecomplexity of our setting. As the heterogeneity across clients increases, coupled with an increase in the lengthof the global task sequence, the performance of FedLwF deteriorates catastrophically. We instead adaptLwF.MC with sigmoid binary cross-entropy loss as described in Smith et al. (2023) which is a strong continuallearning baseline to our setting. We call this method FedLwF.MC which achieves a much better performancethan its vanilla counterpart. Additionally, we introduce a simple yet strong prompting-based methods whichwe call FedAvg-Prompt. FedAvg-Prompt differs from our method in the model aggregation part at the serverside where the clients are simply averaged to obtain the server model. Additionally for completeness, wereport the performance of FedAvg-FT where the entire client models are sequentially finetuned on new taskdata (as opposed to learning only prompts and classifier) and aggregated using FedAvg. Finally, we reportthe performance of our decomposed prompting scheme in a centralized, traditional continual learning setting.This can be thought of as an upper bound performance for all prompt-based methods included here.",
  "Main Results": "The results presented in Tables 1 and 2 demonstrate the dominant performance of our method in terms ofaverage accuracy and forgetting across across all datasets and setups. The gains achieved by our methodare more pronounced in the ImageNet-R setup which has longer task sequences and offer a significant shiftfrom the pretrained distribution. Our approach achieves absolute improvements of up to 7% in averageaccuracy compared to the best baseline. All baselines that fine-tune the entire model are seen to struggle withlonger sequences (CIFAR, Imagenet-R), showing significant forgetting. Under the class-balanced setting of, our approach achieves absolute improvements of more than 7% on ImageNet-R in average accuracycompared to TARGET Zhang et al. (2023), which is the current SOTA. For the class-imbalanced settings in, our approach outperforms the competition by even wider margins. The notable performance dropsobserved across all methods highlight the complexity of this setting. Most importantly, HePCo achieves thesesolid results while enjoying low communication costs and without introducing any additional costs at theclient-side. Furthermore, our approach faithfully aligns with the principles of Federated Learning (FL) by notassuming access to any storage, be it surrogate datasets or generated images, in contrast to methods likeCFed Ma et al. (2022), TARGET Zhang et al. (2023) and GLFC Dong et al. (2022).",
  "We perform ablations experiments on CIFAR-100 in the No Imbalance setting from": "Ablating distillation of previous server model. By removing the previous task server model fromthe distillation and generation steps, we highlight its importance in alleviating forgetting. By ablating thiscomponent, we observe a significant drop in performance indicated by a rise in forgetting (FN) and a drop inaverage accuracy (AN). The underlying intuition is that without the replay of past task data, the methodstrongly prioritizes learning of the current task leading to a loss of knowledge from previously seen tasks. Inother words, using past task latents for replay mitigates inter-task forgetting. Ablating disagreement losses in generation. To demonstrate the effectiveness of disagreement losses ingeneration, we set the lambda coefficients KL and MSE to zero and observe a 6% drop in accuracy. Asdiscussed before, the intuition here is that in absence of the disagreement losses, the generator is prone togenerate easily discriminable examples that lead to low classification loss but are less effective in distillation.To further highlight the importance of the individual losses, i.e LMSE and LKL, we individually ablate themand observe performance drops. Ablating distillation sites. Our approach uses pseudo-latents to fine-tune the key-prompt and classifierweights of the server model. In this experiment, we ablate the decision of fine-tuning the prompt components",
  ": Overhead costs incurred by our method against top performing baselines": "and the classifier separately and observe a decline in accuracy in both cases. The drop in performanceis more pronounced when we do not perform distillation for the classifier. This experiment highlights ourdecision to fine-tune both prompt components and classifiers by operating in the latent space. Varying the category ratio. shows the performance of all methods for different values ofcategory ratio. We observe that HePCo consistently outperforms competing methods without requiringany hyperparameter or design changes. The performance gap between HePCo and the competing methodswidens with decreasing category ratio, indicating its effectiveness in high heterogeneity settings.",
  "Overhead Cost Analysis": "Memory & Communication Overhead. Our method introduces additional parameters forming theprompting mechanism. The additional parameters amount to ~9.4% of the original size of the ViT encoder.Our method only needs to communicate the learnable parameters in the model which are the classifier andkey-prompt components amounting to ~9.5% of the original model size. Methods that finetune the entiremodel need to learn and communicate all parameters in the encoder and classifier. Hence, our approachrequires only 9.5% of the communication costs compared to all other methods that share complete models.Furthermore, the current state-of-the-art methods like CFed and TARGET require communicating a datasetof images (obtained from the surrogate dataset or a generative mechanism) after every round or task whichsignificantly increases the communication overhead in addition to sharing complete models! For a ViT-B/16architecture, sharing a complete model amounts to 330.3 MB of information. In addition to complete modelweights, TARGET sends a buffer of 8k synthesized image-level data from server to clients to performdistillation leading to a communication volume of 714.7 MB. In contrast, our approach, only sends 31.37 MBof data in each client-server exchange. We report this information shared (in MB) during each client-servercommunication in . Computation Overhead. Our method does not require any extra computation at the client side butintroduces an overhead at the server side. This overhead includes the time required to train the generatorsand perform knowledge distillation. To quantify this overhead, we conducted benchmarking using 2 NVIDIATITAN RTX GPUs in a 5 client setup, as described in the experiments section. We report results in in terms of overhead time in seconds (s) per round. Our method adds an extra 220 seconds of computationaltime at the server side per round, in contrast to the 98 seconds introduced by CFed and the 128 secondsincurred by TARGET. It is crucial to emphasize that our method does not impose any additional overhead onthe client side, unlike CFed and TARGET which incur 68 seconds and 62 seconds per client respectively. Inthose methods, the client is responsible for learning the current task as well as distilling knowledge from pasttasks. By transferring the computation load from the client to the server, we prioritize client-level efficiency.In most practical federated learning scenarios, edge devices have limited computational capacity compared tothe server. Our approach prioritizes client-level efficiency, even if it entails a slight trade-off in server-levelefficiency. Storage Overhead. As our method operates in a stateless FL setup, we do not require clients to maintainany state information or additional storage. Our approach requires the server model to store the classifier andprompt components corresponding to the last task model which is used in distillation resulting into a storagecost equal to ~9.5% of the base encoder model size. Other baselines Rebuffi et al. (2017a) incur extra storagecosts at the client side equal to the size of entire encoder and classifier i.e ~86M parameters. Additionally,CFed and TARGET incur costs equivalent to storing an entire image dataset at both server and individualclient levels. We report the storage costs incurred by these methods (in MB) at both server and client side in. The storage cost does not include the space needed to store the current round model itself. For",
  "Conclusion": "In conclusion, we propose HePCo (Heterogeneous Prompt Consolidation) for continual federated learning.We formalize the setting and provide a methodical approach to simulate real-world conditions combiningperspectives from continual and federated landscapes. Our method harnesses the prompt learning capabilitiesof foundation models to facilitate an efficient distillation framework for consolidating heterogeneous clients.By generating pseudo-data in a low-dimensional latent space, our approach enables parameter-efficient anddata-free distillation of information from clients to server. We demonstrate the superior performance ofour method compared to existing state-of-the-art methods through a series of experiments that emulatechallenging real-world scenarios. By requiring clients to share parts of their models, we significantly reducecommunication costs and enhance privacy. Importantly, our approach does not impose any additionaloverheads on the client side, making it highly valuable for real-world deployment.",
  "Discussion": "Limitations. It is worth noting that prompting-based methods are still relatively new and not extensivelystudied, making the explainability of these prompts challenging. Therefore, future work should focus ontesting the robustness of these methods in diverse setups to ensure their effectiveness in different scenarios.Considering the typical asymmetry in the availability of computational resources across servers and clients,our approach prioritizes client-level efficiency. Yet, the computation overhead introduced at the server, maybe an issue for some use-cases. Although the generation and distillation procedures are relatively lightweight,they still rely on availability of server-side compute resources, which may not be universally accessible inall scenarios. Additionally, our approach necessitates clients to use pretrained vision transformers, leavingopen the question of how this framework can be extended to accommodate other architectures. These areinteresting avenues for future research. Broader Impact. The machine learning community is increasingly leaning towards the adoption of large-scale models for various applications. However, updating these models with new data poses a significantchallenge. Retraining models from scratch each time new data arrives is computationally expensive andcan have substantial financial Justus et al. (2018) and environmental Patterson et al. (2021); Lacoste et al.(2019) implications. Our approach offers a solution by enabling incremental learning on new data withoutthe need for complete model retraining. Additionally, our use of prompting techniques allows for significantreductions in communication and local computation costs while enhancing privacy, which is especially criticalfor on-device edge computing applications.",
  "This material is based upon work supported by the National Science Foundation under Grant No. 2239292": "Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, andVenkatesh Saligrama. Federated learning based on dynamic regularization. arXiv preprint arXiv:2111.04263,2021. Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. Ss-il: Separatedsoftmax for incremental learning. In Proceedings of the IEEE/CVF International Conference on ComputerVision (ICCV), pp. 844853, October 2021.",
  "Arslan Chaudhry, MarcAurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efficient lifelonglearning with a-GEM. In International Conference on Learning Representations, 2019a": "Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet K Dokania,Philip HS Torr, and MarcAurelio Ranzato. Continual learning with tiny episodic memories. arXiv preprintarXiv:1902.10486, 2019b. Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Dual-teacher class-incremental learning with data-free generative replay. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 35433552, 2021.",
  "Tyler L Hayes and Christopher Kanan. Lifelong machine learning with deep streaming linear discriminantanalysis. arXiv preprint arXiv:1909.01520, 2019": "Tyler L Hayes, Nathan D Cahill, and Christopher Kanan. Memory efficient experience replay for streaminglearning. In 2019 International Conference on Robotics and Automation (ICRA), pp. 97699776. IEEE,2019. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many facesof robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021. Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Lifelong learning via progressivedistillation and retrospection. In Proceedings of the European Conference on Computer Vision (ECCV),pp. 437452, 2018. Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin.Learning a unified classifierincrementally via rebalancing. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 831839, 2019.",
  "Alessio Mora, Irene Tenison, Paolo Bellavista, and Irina Rish. Knowledge distillation for federated learning:a practical guide. arXiv preprint arXiv:2211.04742, 2022": "Anh Nguyen, Tuong Do, Minh Tran, Binh X. Nguyen, Chien Duong, Tu Phan, Erman Tjiputra, and Quang D.Tran. Deep federated learning for autonomous driving. In 2022 IEEE Intelligent Vehicles Symposium (IV),pp. 18241830, 2022. doi: 10.1109/IV51971.2022.9827020. Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learning to remember:A synaptic plasticity driven framework for continual learning. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 1132111329, 2019.",
  "Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residualadapters. Advances in neural information processing systems, 30, 2017a": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incremen-tal classifier and representation learning. In 2017 IEEE Conference on Computer Vision and PatternRecognition, CVPR17, pp. 55335542, 2017b. Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger Roth, Shadi Albarqouni, Spyridon Bakas,Mathieu Galtier, Bennett Landman, Klaus Maier-Hein, Sbastien Ourselin, Micah Sheller, Ronald Summers,Andrew Trask, Daguang Xu, Maximilian Baust, and Manuel Jorge Cardoso. The future of digital healthwith federated learning. npj Digital Medicine, 3, 12 2020. doi: 10.1038/s41746-020-00323-1. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115(3):211252, 2015. Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, KorayKavukcuoglu, Razvan Pascanu, and Raia Hadsell.Progressive neural networks.arXiv preprintarXiv:1606.04671, 2016.",
  "Lixu Wang, Chenxi Liu, Junfeng Guo, Jiahua Dong, Xiao Wang, Heng Huang, and Qi Zhu. Federatedcontinual novel class learning, 2023. URL": "Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su,Vincent Perot, Jennifer Dy, and Tomas Pfister. Dualprompt: Complementary prompting for rehearsal-freecontinual learning, 2022a. Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su,Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continuallearning. arXiv preprint arXiv:2204.04799, 2022b. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot,Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 139149, 2022c.",
  "Ross Wightman. Pytorch image models. 2019": "Guile Wu, Shaogang Gong, and Pan Li. Striking a balance between stability and plasticity for class-incrementallearning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11241133,2021. Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong Li, Arun Mallya, Derek Hoiem, Niraj K Jha, andJan Kautz. Dreaming to distill: Data-free knowledge transfer via deepinversion. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 87158724, 2020.",
  "BExperimental Details": "Implementation Details. For fair comparison, we use the ViT-B/16 backbone pretrained on Imagenet-1Kas the encoder for all methods. We implement our methods in PyTorch and use the PyTorch Image Modelslibrary Wightman (2019) to obtain pretrained checkpoints. We use 2 NVIDIA A40 GPUs for all experiments.For each result reported in this paper, we calculate the mean and standard deviation over separate runs.",
  "Training Details. For all methods, we use the Adam Kingma & Ba (2017) optimizer with 1 = 0.9 and2 = 0.999. We resize images to 224 224 and normalize to": "Hyperparameter Search. Following DualPrompt Wang et al. (2022b), we use 20% of the training datasetas our validation data and conduct a hyperparameter search. We tune hyperparameters for both our approachand all competing baselines. We use a batch size of 64 for both local and server-side training, determinedafter searching over values of 16, 32, 64, 128. For our method and the prompting-based baselines, we use alearning rate of 1e-3, while for baselines that tune the entire model (FedAvg, FedLwF.MC), we use 5e-5. Wesearch for learning rates among {1e6, 5e5, 1e5, 5e4, 1e4, 5e3, 1e3, 5e2, 1e2}. Our method employsa three-layer fully-connected network as the generator. We encode class labels using an embedding matrix oflength 64 and concatenate it with a 64-dimensional noise vector. This dimension was chosen after searchingover 32, 64, 128, 256. While our approach is robust to various values, 64 performs best. The generatorarchitecture has input sizes of per layer, with an output size of 768 which is the dimensionof the visual query. We opt for a three-layer architecture to maintain lightweight generation and training.We train the generator for 100 epochs using a batch size of 64 and a learning rate of 1e4 using the Adamoptimizer. After a logarithmic scale search, we find a learning rate in the range [5e-5, 1e-4] to provide optimalresults. We fine-tune the server model using a learning rate of 1e4 for 200 epochs. Through grid search, we findthat the model is robust to various learning rate and epoch combinations, with these values providing thebest average accuracy on the validation set. We use a replay ratio of 0.5 for our method, which means wemix 50 pseudo-latents corresponding to previous tasks for every 100 pseudo-latents corresponding to thecurrent task. We conduct a search over values like [0, 0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1] and find0.5 to result into the best average accuracy AN. We observe a stability-plasticity trade-off controlled bythis hyperparameter with larger values leading to lower forgetting (FN) but lower current task accuracies(plasticity) and smaller values yielding the opposite effect. Through a hyperparameter search within at 0.1 increments, we choose KL and MSE values to be 1 and 0.1 respectively. We find these values towork best across all datasets reported in the paper. Overall, KL values between [0.6, 3] yield similarly goodresults, with too low (close to 0) and too high (close to 5) values leading to poor performance."
}