{
  "Abstract": "Machine learning systems have been widely used to make decisions about individuals whomay behave strategically to receive favorable outcomes, e.g., they may genuinely improvethe true labels or manipulate observable features directly to game the system without chang-ing labels. Although both behaviors have been studied (often as two separate problems)in the literature, most works assume individuals can (i) perfectly foresee the outcomes oftheir behaviors when they best respond; (ii) change their features arbitrarily as long as itis affordable, and the costs they need to pay are deterministic functions of feature changes.In this paper, we consider a different setting and focus on imitative strategic behaviors withunforeseeable outcomes, i.e., individuals manipulate/improve by imitating the features ofthose with positive labels, but the induced feature changes are unforeseeable. We first pro-pose a Stackelberg game to model the interplay between individuals and the decision-maker,under which we examine how the decision-makers ability to anticipate individual behavioraffects its objective function and the individuals best response. We show that the objectivedifference between the two can be decomposed into three interpretable terms, with eachrepresenting the decision-makers preference for a certain behavior. By exploring the rolesof each term, we theoretically illustrate how a decision-maker with adjusted preferences maysimultaneously disincentivize manipulation, incentivize improvement, and promote fairness.Such theoretical results provide a guideline for decision-makers to inform better and sociallyresponsible decisions in practice.",
  "Introduction": "Individuals subject to algorithmic decisions often adapt their behaviors strategically to the decision rule toreceive a desirable outcome. As machine learning is increasingly used to make decisions about humans, therehas been a growing interest to develop learning methods that explicitly consider the strategic behavior ofhuman agents. A line of research known as strategic classification studies this problem, in which individualscan modify their features at costs to receive favorable predictions.Depending on whether such featurechanges are to improve the actual labels genuinely (i.e., improvement) or to game the algorithms maliciously(i.e., manipulation), existing works have largely focused on learning classifiers robust against manipulation",
  "Published in Transactions on Machine Learning Research (10/2024)": "3. Estimate P I: Apply an arbitrary decision threshold to the population, the resulting population prob-ability density distribution will be a mixture of (1 )(1 q)P I + [(1 )q + ]PX|1. Similarly, withminor assumptions on the distribution family of P I, we can estimate P I. 4. Estimate : With q, P I known, the decision-maker can first apply another arbitrary to new samplesfrom the population and observe the resulting new population. This gives the new qualification rate p.Because p = + (1 )(1 PM()q) where PM() is the probability of manipulation under , we canthen compute the value of PM(). Note that the decision-maker also knows how many individuals (amongall individuals) are discovered to manipulate (cheat), and let this proportion be c, then we can estimatethe manipulation detection probability asc",
  "learning (Bandura, 1962).App. A provides more related work and differences with existing models arediscussed in App. B.1": "Under this model, we first study the impacts of the decision makers ability to anticipate individual behavior.Similar to Zhang et al. (2022), we consider two types of decision-makers: non-strategic and strategic. Wesay a decision-maker (and its policy) is strategic if it has the ability to anticipate strategic behavior andaccounts for this in determining the decision policies, while a non-strategic decision-maker ignores strategicbehavior in determining its policies. Importantly, we find that the difference between the decision-makerslearning objectives under two settings can be decomposed into three interpretable terms, with each termrepresenting the decision-makers preference for certain behavior. By exploring the roles of each term on thedecision policy and the resulting individuals best response, we further show that a strategic decision-makerwith adjusted preferences (i.e., changing the weight of each term in the learning objective) can disincentivizemanipulation while incentivizing improvement behavior. We also consider settings where the strategic individuals come from different social groups and explore theimpacts of adjusting preferences on algorithmic fairness. We show that the optimal policy under adjustedpreferences may result in fairer outcomes than non-strategic policy and original strategic policy without ad-justment. Moreover, such fairness promotion can be attained simultaneously with the goal of disincentivizingmanipulation. Our contributions are summarized as follows: 1. We propose a probabilistic model to capture both improvement and manipulation; and establish a novelStackelberg game to model the interplay between individuals and decision-maker. The individuals bestresponse and decision-makers (non-)strategic policies are characterized (Sec. 2).",
  ". We show the objective difference between non-strategic and strategic policies can be decomposed intothree terms, each representing the decision-makers preference for certain behavior (Sec. 3)": "3. We study how adjusting the decision-makers preferences can affect the optimal policy and its fairnessproperty, as well as the resulting individuals best response (Sec. 4). We also illustrate how the decision-maker can adjust preferences to disincentivize manipulation, incentivize improvement and promote fair-ness in practice (Sec. 4 and App. B.5).",
  "Problem Formulation": "Consider a group of individuals subject to some ML decisions. Each individual has an observable featureX R and a hidden label Y {0, 1} indicating its qualification state (0\" being unqualified and 1\" beingqualified).2 Let := Pr(Y = 1) be the populations qualification rate, and PX|Y (x|1), PX|Y (x|0) be thefeature distributions of qualified and unqualified individuals, respectively. A decision-maker makes binarydecisions D {0, 1} (0\" being reject and 1\" being accept) about individuals based on a threshold policywith acceptance threshold R: (x) = PD|X(1|x) = 1(x ). To receive positive decisions, individualswith information of policy may behave strategically by either manipulating their features or improving theactual qualifications.3 Formally, let M {0, 1} denote individuals action, with M = 1 being manipulationand M = 0 being improvement. Outcomes of strategic behavior. Both manipulation and improvement result in the shifts of featuredistribution. Specifically, for individuals who choose to manipulate, we assume they manipulate by steal-ing\" the features of those qualified (Zhang et al., 2022), e.g., students cheat on exams by hiring qualifiedimposters. Moreover, we assume the decision-maker can identify the manipulation behavior with probability (Estornell et al., 2021). Individuals, once getting caught manipulating, will be rejected directly. Forthose who decide to improve, they work hard to imitate the features of those qualified (Bandura, 1962; Raab 2Similar to prior work (Zhang et al., 2022; Liu et al., 2018), we present our model in one-dimensional feature space. Notethat our model and results are applicable to high dimensional space, in which individuals imitate and change all features as awhole based on the joint conditional distribution PX|Y regardless of the dimension of X. The costs can be regarded as the sumof an individuals effort to change features in all dimensions.3We assume all unqualified individuals who stay in the decision-making system either manipulate or improve.In otherwords, they already have a sufficiently large initial utility by staying in the system and trying to be qualified. We discuss thedetails in App. B.3.",
  "P I(x)areincreasing in x R": "Assumption 2.1 is relatively mild and has been widely used (e.g., (Tsirtsis et al., 2019; Zhang et al., 2020b)).It can be satisfied by a wide range of distributions (e.g., exponential, Gaussian) and the real data (e.g., FICOdata used in Sec. 5). It implies that an individual is more likely to be qualified as feature value increases.Meanwhile, compared to the unqualified individuals, the individuals who improve but fail also tend to havehigher feature values. Individuals have a good knowledge of their true qualifications by observing their peersor previous individuals who received positive decisions Raab & Liu (2021), and only unqualified individualshave incentives to take action Dong et al. (2018) since PX|Y (x|1) is always the best attainable outcome (asmanipulation and improvement only bring additional cost but no benefit to qualified individuals).",
  "Individuals best response": "An individual incurs a random cost CM 0 when manipulating the features (Zhang et al., 2022), whileincurring a random cost CI 0 when improving the qualifications (Liu et al., 2020). The realizations ofthese random costs are known to individuals when determining their action M; while the decision-maker onlyknows the cost distributions. Thus, the best response that the decision-maker expects from individuals is theprobability of manipulation/improvement. illustrates the strategic interaction between them.",
  ": Illustration of the strategic interaction": "Formally, given a policy (x) = 1(x ) with threshold , an individual chooses to manipulate only ifthe expected utility attained under manipulation UM() outweighs the utility under improvement UI().Suppose an individual benefits w = 1 from the acceptance, and 0 from the rejection.Given that eachindividual only knows his/her label y {0, 1} and the conditional feature distributions PX|Y but not theexact values of the feature x, the expected utilities UM() and UI() can be computed as the expected benefitminus the cost of action, as given below.",
  "(1)": "The above formulation captures the imitative strategic behavior with unforeseeable outcomes (e.g., collegeadmission example in Sec. 1): individuals best respond based on feature distributions but not the realizations,and the imitation costs (e.g., hiring an imposter) for individuals from the same group follow the samedistribution (Liu et al., 2020), as opposed to being a function of feature changes.equation 1 above canfurther be written based on CDF of CM CI, i.e., the difference between manipulation and improvementcosts. We make the following assumption on its PDF.Assumption 2.2. The PDF PCMCI(x) is continuous with PCMCI(x) > 0 for x (, 1 q). Assumption 2.2 is mild only to ensure the manipulation is possible under all thresholds .Under theAssumption, we can study the impact of acceptance threshold on manipulation probability PM().Theorem 2.3 (Manipulation Probability). Under Assumption 2.2, PM() is continuous and satisfies thefollowing: (i) If q + 1, then PM() strictly increases. (ii) If q + < 1, then PM() first increases andthen decreases, thereby existing a unique maximizer max. Moreover, maximizer max increases in q and . Thm. 2.3 shows that an individuals best response highly depends on the success rate of improvement qand the identification rate of manipulation .When q + 1 (i.e., improvement can succeed or/andmanipulation is detected with high probability), individuals are more likely to manipulate as increases.Note that although individuals are generally more likely to benefit from improvement than manipulation,as increases to the maximum (i.e., when the decision-maker barely admits anyone), the \"net benefit\" ofimprovement compared to manipulation will finally diminish to 0 because both actions are useless. Thus,more individuals tend to manipulate under larger , making PM() strictly increasing and reaching themaximum. When q + < 1, more individuals are incentivized to improve as the threshold gets farther awayfrom max. This is because the manipulation in this case incurs a higher benefit than improvement at max.As the threshold increases/decreases from max to the minimum/maximum (i.e., the decision-maker eitheradmits almost everyone or no one), the \"net benefit\" of manipulation compared to improvement decreasesto 0 or . Thus, PM() decreases as increases/decreases from max.",
  "Decision-makers optimal policy": "Suppose the decision-maker receives benefit u (resp. penalty u) when accepting a qualified (resp. unqual-ified) individual, then the decision-maker aims to find an optimal policy that maximizes its expected utilityE[R(D, Y )], where utility is R(1, 1) = u, R(1, 0) = u, R(0, 1) = R(0, 0) = 0. As mentioned in Sec. 1, we consider strategic and non-strategic decision makers. Because the former cananticipate individuals strategic behavior while the latter cannot, their learning objectives E[R(D, Y )] aredifferent. As a result, their respective optimal policies are also different.",
  "+ (1 PM()) (1 q)(1 F I()(3)": "The policy that maximizes the above objective function U() is the strategic optimal policy. We denote thecorresponding optimal threshold as . Compared to non-strategic policy, U() also depends on q, , PM()and is rather complicated. Nonetheless, we will show in Sec. 3 that U() can be justified and decomposedinto several interpretable terms.",
  "() = PM()(1 )1 FX|Y (|1)1 FX|Y (|0)": "As shown in equation 4, the objective difference can be decomposed into three terms 1, 2, 3. It turnsout that each term is interpretable and indicates the impact of a certain type of individual behavior on thedecision-makers utility. We discuss these in detail as follows. 1. Benefit from the successful improvement 1: additional benefit the decision-maker gains due to thesuccessful improvement of individuals (as the successful improvement causes label change). In the collegeadmission example, 1 stands for the utility gain caused by students studying hard to be qualified. 2. Loss from the failed improvement 2: additional loss the decision-maker suffers due to the individualsfailure to improve; this occurs because individuals who fail to improve only experience feature distributionshifts from PX|Y (x|0) to P I(x) but labels remain. In the college admission example, 2 stands for theutility loss caused by students who seem to try but fail to be qualified. 3. Loss from the manipulation 3: additional loss the decision-maker suffers due to the successfulmanipulation of individuals; this occurs because individuals who manipulate successfully only changePX|Y (x|0) to PX|Y (x|1) but the labels remain unqualified. In the college admission example, 3 standsfor the utility loss caused by students who cheat/hire imposters. Note that in Zhang et al. (2022), the objective difference () has only one term corresponding to theadditional loss caused by strategic manipulation. Because our model further considers improvement behavior,the impact of an individuals strategic behavior on the decision-makers utility gets more complicated. Wehave illustrated above that in addition to the loss from manipulation 3, the improvement behavior alsoaffects decision-makers utility. Importantly, such an effect can be either positive (if the improvement issuccessful) or negative (if the improvement fails). The decomposition of the objective difference () highlights the connections between three types of policies:1) non-strategic policy without considering individuals behavior; 2) strategic policy studied in Zhang et al.(2022) that only considers manipulation, 3) strategic policy studied in this paper that considers both ma-nipulation and improvement. Specifically, by removing 1, 2, 3 (resp. 1, 2) from the objective functionU(), the strategic policy studied in this paper would reduce to the non-strategic policy (resp. strategicpolicy studied in Zhang et al. (2022)). Based on this observation, we regard 1, 2, 3 each as the decision-makers preference to a certain type of individual behavior, and define a general strategic decision-makerwith adjusted preferences.",
  ". Improvement-proof decision-maker: the one with k2 > 0 and k1 = k3 = 0; it only considersimprovement but the goal is to avoid loss caused by the failed improvement": "The above examples show that a decision-maker, by changing the weights k1, k2, k3 could find a policythat encourages certain types of individual behavior (as compared to the original policy ). Although thedecision-maker can impact an individuals behavior by adjusting its preferences via k1, k2, k3, we emphasizethat the actual utility it receives from the strategic individuals is always determined by U() given inequation 3. Indeed, we can regard the framework with adjusted weights (equation 5) as a regularizationmethod. We discuss this in more detail in App. B.4.",
  "Preferences shift the optimal threshold": "We will start with the original strategic decision-maker (with k1 = k2 = k3 = u(1 )) whose objectivefunction follows equation 3, and then investigate how adjusting preferences could affect the decision-makersoptimal policy. Complex nature of original strategic decision-maker. Unlike the non-strategic optimal policy, theanalytical solution of strategic optimal policy that maximizes equation 3 is not easy to find. In fact, theutility function U() of the original strategic decision-maker is highly complex, and the optimal strategicthreshold may change significantly as , FX|Y , F I, CM, CI, , q vary. In App. C.2, we demonstrate thecomplexity of U(), which may change drastically as , , q vary. Although we cannot find the strategicoptimal threshold precisely, we may still explore the impacts of the decision-makers anticipation of strategicbehavior on its policy (by comparing the strategic threshold with the non-strategic ), as stated inThm. 4.1 below.Theorem 4.1 (Comparison of strategic and non-strategic policy). If min PM() 0.5, then there existsq (0, 1) such that q q, the strategic optimal is always lower than the non-strategic . Thm. 4.1 identifies a condition under which the strategic policy over-accepts individuals compared to thenon-strategic one. Specifically, min PM() 0.5 ensures that there exist policies under which the ma-jority of individuals prefer improvement over manipulation. Intuitively, under this condition, a strategicdecision-maker by lowering the threshold (from ) may encourage more individuals to improve. Because qis sufficiently large, more improvement brings more benefit to the decision-maker.",
  ": Illustration of scenario 1 (left) and scenario 2 (right) in Thm. 4.5: adjusting preferences decreasesmanipulation probability PM()": "Optimal threshold under adjusted preferences. Despite the intricate nature of U(), the optimalstrategic threshold may be shifted by adjusting the decision-makers preferences, i.e. changing the weightsk1, k2, k3 assigned to 1, 2, 3 in equation 5. Next, we examine how the optimal threshold can be affectedcompared to the original strategic threshold by adjusting the decision-makers preferences. Denote (ki)as the strategic optimal threshold attained by adjusting weight ki, i {1, 2, 3} of the original objectivefunction U(). The results are summarized in . Specifically, the threshold gets lower as k1 increases(Prop. 4.2). Adjusting k2 or k3 may result in the optimal threshold moving toward both directions, but wecan identify sufficient conditions when adjusting k2 or k3 pushes the optimal threshold to move toward onedirection (Prop. 4.3 and 4.4).",
  "Increase k3Discourage manipulation(k3)": "Proposition 4.2. Increasing k1 results in a lower optimal threshold (k1) < . Moreover, when k1 issufficiently large, (k1) < .Proposition 4.3. When 0.5 (the majority of the population is unqualified), increasing k2 results in ahigher optimal threshold (k2) > . Moreover, when k2 is sufficiently large, (k2) > .Proposition 4.4. For any feature distribution PX|Y , there exists an (0, 1) such that whenever ,increasing k3 results in a lower optimal threshold (k3) < . Prop.4.2 to 4.4 reveal that adjusting preferences may lead to predictable changes of optimal strategicthresholds under certain conditions. So far we have shown how the optimal threshold can be shifted asthe decision makers preferences change. Next, we explore the impacts of threshold shifts on individualsbehaviors and show how a decision-maker with adjusted preferences can (dis)incentivize manipulation andinfluence fairness.",
  "Preferences as (dis)incentives for manipulation": "In Thm. 2.3, we explored the impacts of threshold on individuals best responses PM().Combinedwith our knowledge of the relationship between adjusted preferences and policy (Sec. 4.1), we can furtheranalyze how adjusting preferences affect individuals responses. Next, we illustrate how a decision-maker maydisincentivize manipulation (or equivalently, incentivize improvement) by adjusting its preferences.Theorem 4.5 (Preferences serve as (dis)incentives). Compared to the original strategic policy , decision-makers can adjust preferences to disincentivize manipulation (i.e., PM() decreases) under certain scenarios.Specifically,",
  "P I()1q": "1q implies that is lower/higher thanmax in Thm. 2.3. In , we illustrate Thm. 4.5 where the left (resp. right) plot corresponds to scenario1 (resp. scenario 2). Because increasing k1 (resp. k2) results in a lower (resp. higher) threshold than ,the resulting manipulation probability PM is lower for both scenarios. The detailed experimental setup andmore illustrations are in App. C.",
  "Preferences shape algorithmic fairness": "The threshold shifts under adjusted preferences further allow us to compare these policies against a certainfairness measure. In this section, we consider strategic individuals from two social groups Ga, Gb distinguishedby some protected attribute S {a, b} (e.g., race, gender). Similar to Zhang et al. (2019; 2020b; 2022), weassume the protected attributes are observable and the decision-maker uses group-dependent threshold policys(x) = 1(x s) to make decisions about Gs, s {a, b}. The optimal threshold for each group can befound by maximizing the utility associated with that group: maxs E[R(D, Y )|S = s].",
  "For threshold policy with thresholds (a, b), we measure the unfairness asEXP Ca [1(x a)]EXP Cb [1(x": "b)]. Define the advantaged group as the group with larger EXP Cs [1(X s)] under non-strategic optimalpolicy s, i.e., the group with the larger true positive rate (resp. positive rate) under EqOpt (resp. DP)fairness, and the other group as disadvantaged group. Mitigate unfairness with adjusted preferences. Next, we compare the unfairness of different policiesand illustrate that decision-makers with adjusted preferences may result in fairer outcomes, as compared toboth the original strategic and the non-strategic policy.Theorem 4.6 (Promote fairness while disincentivizing manipulation). Without loss of generality, let Ga bethe advantaged group and Gb disadvantaged. A strategic decision-maker can always simultaneously disincen-tivize manipulation and promote fairness in any of the following scenarios:",
  "Corollary 4.7. If none of the three scenarios in Thm. 4.6 holds, adjusting preferences is not guaranteed topromote fairness and disincentivize manipulation simultaneously": "Thm. 4.6 identifies all scenarios under which a decision-maker can simultaneously promote fairness anddisincentivize manipulation by simply adjusting k1, k2. Otherwise, it is not guaranteed that both objectivescan be achieved at the same time, as stated in Corollary 4.7. A practical guideline for socially responsible decisions. The theoretical results in Thm. 4.5 give con-ditions under which the decision-maker can adjust preferences to disincentivize manipulation and encourageimprovement, while the ones in Thm. 4.6 further sheds light on how the decision-maker can make explainableand socially responsible decisions under the unforeseeable strategic individual behavior: instead of addingseparate regularizers to prevent manipulation or promote fairness, we show that the decision-maker mayadjust their preferences in an interpretable way to disincentivize manipulation, incentivize improvement andpromote fairness at the same time. However, applying our theoretical results in practice needs access to several model parameters such asq, , CM, CI, F I which can be estimated empirically. In App. B.5, we assume the decision-maker needs toestimate all parameters except the feature distribution for the qualified individuals PX|Y (x|1) and the quali-fication rate . Then we provide an estimation procedure for the parameters using controlled experimentson an experimental population4. With the model parameters, the decision-maker can derive accurate ex-pressions for the decompositions 1, 2, 3 and verify whether any condition in Thm. 4.5 and 4.6 holds. Thisenables the decision-maker to design a policy by adjusting preferences. For example, if estimated probabili-ties q + 1, then the decision-maker may train a policy with increased k1 to disincentivize manipulationbehavior (by Thm. 4.5, case 1).",
  "We conduct experiments on both synthetic Gaussian data and FICO score data (Hardt et al., 2016b)5": "FICO data (Hardt et al., 2016b).FICO scores are widely used in the US to predict peoples creditworthiness. We use the preprocessed dataset containing the CDF of scores FX|S(x|s), qualification likelihoodsPY |XS(1|x, s), and qualification rates s for four racial groups (Caucasian, African American, Hispanic,Asian). All scores are normalized to . Similar to Zhang et al. (2022), we use these to estimate theconditional feature distributions PX|Y S(x|y, s) using beta distribution Beta(ays, bys). The results are shownin . We assume the improved feature distribution P I(x) Beta a1s+a0s",
  ", b1s+b0s": "2and CM CI N(0, 0.25) for all groups, under which Assumption 2.2 and 2.1 are satisfied (see ). We also consideredother feature/cost distributions and observed similar results. Note that for each group s, the decision-makerfinds its own optimal thresholds or s(ki) or sby maximizing the utility associated with that group,i.e., maxs E[R(D, Y )|S = s]. We first examine the impact of the decision-makers anticipation of strategic behavior on policies. In (App. C.1), the strategic s and non-strategic optimal threshold s are compared for each group underdifferent q and . The results are consistent with Thm. 4.1, i.e., under certain conditions, s is lower thans when q is sufficiently large. We also examine the individual best responses. shows the manipulation probability PM() as a functionof threshold for Caucasians and Asians (blue) versus African Americans and Hispanic (orange) when q =0.3, = 0.5. For all groups, there exists a unique max that maximizes the manipulation probability. Theseare consistent with Thm. 2.3. We also indicate the manipulation probabilities under original strategic optimalthresholds s; The results indicate that original strategic optimal thresholds may cause the disadvantagedgroups to have higher manipulation probabilities. Note that the scenario considered in satisfies the condition 1.(ii) in Thm. 4.5, because the originalstrategic s < max for both groups. We further conduct experiments in this setting to evaluate the impactsof adjusted preferences. We first adopt EqOpt as the fairness metric, under which EXP Cs [1(X )] = 4Miller et al. (2020) already argued that designing policy for any strategic classification problem is a non-trivial causalmodeling problem, thereby making the controlled experiments necessary for applications in practice.5code available at",
  ": PM() of Caucasian and African American (left plot) and of Asian and Hispanic (right plot)": "FX|Y S(|1, s) and the unfairness measure of group Ga, Gb can be reduced toFX|Y S(|1, a) FX|Y S(|1, b).Experiments for other fairness metrics are in App. C.1. The results are shown in , where dashed red anddashed blue curves are manipulation probabilities under non-strategic and strategic (k1), respectively.Solid red and solid blue curves are the actual utilities U() and U((k1)) received by the decision-maker.The difference between the two dotted green curves measures the unfairness between Caucasians/AfricanAmericans or Asians/Hispanics. All weights are normalized such that k1 = 1 corresponds to the originalstrategic policy, and k1 > 1 indicates the policies with adjusted preferences.Results show that whencondition 1(ii) in Thm. 4.5 is satisfied, increasing k1 can simultaneously disincentivize manipulation (PMdecreases with k1) and improve fairness. These validate Thm. 4.5 and 4.6.",
  ": Impact of adjusted preferences (FICO data) when there is a Gaussian noise on . The noises have0 mean, and 0.05, 0.1 standard deviation from the left two plots to the right two plots": "Tab.2 compares the non-strategic , original strategic , and adjusted strategic (k1) when k1,c =k1,aa = 1.5. It shows that decision-makers by adjusting preferences can significantly mitigate unfairness anddisincentivize manipulation, with only slight decreases in utilities. Robustness of results when q, are noisy.We also present experiments to relax the assumption thatthe decision-maker knows q, exactly on the Caucasian/African groups. Instead, they only know q + or + where is a Gaussian noise. We do 10 rounds of simulations and produce plots with expectation anderror bars similar to ( shows the results with noisy q, while shows the results with noisy). The results show adjusting k still works under noisy q and . Gaussian Data.We also validate our theorems on synthetic data with Gaussian distributed PX|Y S inApp. C.2. Specifically, we examined the impacts of adjusting preferences on decision policies, individualsbest response, and algorithmic fairness. As shown in , 20 and Tab. 5, 6, 7, these results are consistentwith theorems, i.e., adjusting preferences can effectively disincentivize manipulation and improve fairness.Notably, we considered all three scenarios in Thm. 4.5 when condition 1.(i) or 1.(ii) or 2 is satisfied. Foreach scenario, we illustrate the individuals best response PM in and show that manipulation can bedisincentivized by adjusting preferences, i.e., increasing k1 under condition 1.(i) or 1.(ii), or increasing k2under condition 2.",
  "Conclusions & Limitations": "This paper proposes a novel probabilistic framework and formulates a Stackelberg game to tackle imitativestrategic behavior with unforeseeable outcomes. Moreover, the paper provides an interpretable decompositionfor the decision-maker to incentivize improvement and promote fairness simultaneously. The theoreticalresults depend on some (mild) assumptions and are subject to change when , q, CM, CI change. Althoughwe provide a practical estimation procedure to estimate the model parameters, it still remains a challengeto estimate model parameters accurately due to the expensive nature of doing controlled experiments. Thismay bring uncertainties in applying our framework accurately in real applications.",
  "Broader Impact Statement": "We believe our proposed framework can promote socially responsible machine learning under strategic clas-sification settings since the outcomes of strategic behaviors in many real-world settings are imitative andunforeseeable, thereby being more appropriately captured by our model. However, as mentioned in Sec. 6,we need certain assumptions and an estimation procedure to apply the model in practice, which may bringunexpected social outcomes.",
  "Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning: Limitations andOpportunities. 2019": "Flavia Barsotti, Ruya Gokhan Kocer, and Fernando P. Santos. Transparency, detection and imitation instrategic classification.In Proceedings of the Thirty-First International Joint Conference on ArtificialIntelligence, IJCAI-22, pp. 6773, 2022. Yahav Bechavod, Katrina Ligett, Steven Wu, and Juba Ziani.Gaming helps!learning from strategicinteractions in natural dynamics. In International Conference on Artificial Intelligence and Statistics, pp.12341242, 2021.",
  "Itay Eilat, Ben Finkelshtein, Chaim Baskin, and Nir Rosenfeld. Strategic classification with graph neuralnetworks, 2022": "Andrew Estornell, Sanmay Das, and Yevgeniy Vorobeychik. Incentivizing truthfulness through audits instrategic classification. In Proceedings of the AAAI Conference on Artificial Intelligence, number 6, pp.53475354, 2021. Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian.Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD international con-ference on knowledge discovery and data mining, pp. 259268, 2015.",
  "Moritz Hardt, Meena Jagadeesan, and Celestine Mendler-Dnner.Performative power.In Advances inNeural Information Processing Systems, 2022": "Keegan Harris, Valerie Chen, Joon Kim, Ameet Talwalkar, Hoda Heidari, and Steven Z Wu.Bayesianpersuasion for algorithmic recourse. Advances in Neural Information Processing Systems, 35:1113111144,2022a. Keegan Harris, Dung Daniel T Ngo, Logan Stapleton, Hoda Heidari, and Steven Wu. Strategic instrumentalvariable regression: Recovering causal relationships from strategic responses. In International Conferenceon Machine Learning, pp. 85028522, 2022b. Hoda Heidari, Vedant Nanda, and Krishna Gummadi. On the long-term impact of algorithmic decisionpolicies: Effort unfairness and feature segregation through social learning. In 36th International Conferenceon Machine Learning, pp. 26922701, 2019.",
  "Xueru Zhang, Mohammad Mahdi Khalili, and Mingyan Liu. Long-term impacts of fair machine learning.ergonomics in design, 28(3):711, 2020a": "Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. Howdo fair decisions fare in long-term qualification? In Advances in Neural Information Processing Systems,pp. 1845718469, 2020b. Xueru Zhang, Mohammad Mahdi Khalili, Kun Jin, Parinaz Naghizadeh, and Mingyan Liu. Fairness inter-ventions as (Dis)Incentives for strategic manipulation. In Proceedings of the 39th International Conferenceon Machine Learning, pp. 2623926264, 2022.",
  "A.1Strategic classification": "Generally, strategic behaviors can cause feature and label distribution of individuals to shift, which havelong been closely related to concept drift (Lu et al., 2018), preference shift (Carroll et al., 2022), andalgorithm recourse (Karimi et al., 2022). Strategic classification has been extensively studied since (Hardtet al., 2016a) formally modeled the interaction between individuals and a decision maker as a StackelbergGame, and proposed a framework for strategic classification. While taking the individuals best response intoaccount, the decision maker can make the optimal decision by anticipating strategic manipulation. Duringrecent years, more complex models on strategic classification have been proposed (Ben-Porat & Tennenholtz,2017; Dong et al., 2018; Braverman & Garg, 2020; Jagadeesan et al., 2021; Izzo et al., 2021; Ahmadi et al.,2021; Tang et al., 2021; Zhang et al., 2020b; 2022; Eilat et al., 2022; Liu et al., 2022; Lechner & Urner, 2022;Chen et al., 2020b; Xie & Zhang, 2024a). Ben-Porat & Tennenholtz (2017) developed a best response linearregression predictor where two players compete and each gets a payoff depending on the proportion of thepoints he/she predicts more accurately than the other player. Dong et al. (2018) focused on the online versionof the strategic classification algorithm. Chen et al. (2020b) developed a strategic-aware linear classifier tominimize the Stacelberg regret. Tang et al. (2021) considered the setting where the decision maker only knewa subset of individuals actions. Levanon & Rosenfeld (2022) generalized strategic classification to situationswhere individuals and the decision maker have aligned interests. Lechner & Urner (2022) proposed a novel lossfunction considering both the accuracy of the prediction rule and its vulnerability to strategic manipulation.Eilat et al. (2022) relaxed the assumption that individual best responses are independent of each other andproposed a robust learning framework based on a Graph Neural Network. Horowitz et al. (2024) consideredthe self-selection problem in strategic classification where agents decide whether to participate in the system.Xie & Zhang (2024b) proposed a welfare-aware optimization framework in strategic learning settings. Regarding randomness, Braverman & Garg (2020) proposed that randomized linear classifiers can be morerobust to strategic behaviors. Moreover, Jagadeesan et al. (2021) added noise to standard strategic classi-fication and modified the standard microfoundations into alternative microfoundations to let a portion ofindividuals be irrational and not have perfect knowledge about the decision makers policy. However, theyjust consider directly adding noise to the best response without modeling the unforeseeable and imitativenature of human agents behavior.",
  "A.2Improvement with a label change": "Another line of research takes improvement into account(Liu et al., 2018; Zhang et al., 2020b; Liu et al.,2020; Rosenfeld et al., 2020; Chen et al., 2020a; Haghtalab et al., 2020; Kleinberg & Raghavan, 2020; Alonet al., 2020; Miller et al., 2020; Shavit et al., 2020; Bechavod et al., 2021; Jin et al., 2022; Barsotti et al.,2022; Ahmadi et al., 2022a; Raab & Liu, 2021; Heidari et al., 2019; Somerstep et al., 2024). Liu et al. (2018;2020); Zhang et al. (2020b); Rosenfeld et al. (2020); Ahmadi et al. (2022b) studied the conditions underwhich individuals will choose to improve their qualifications. Specifically, Liu et al. (2018) investigated howdifferent decision rules (e.g. maxutil, fair) influence population qualification. Liu et al. (2020) modeled theimprovement cost as a random variable and further pointed out that a subsidizing mechanism for individualcosts can be beneficial for improving behaviors. Zhang et al. (2020b) studied the dynamic of populationqualification under a partially observed Markov decision problem setting, where improvement probability isgiven as a parameter. Rosenfeld et al. (2020) proposed a Look-ahead regularization to directly penalize thedrop of population qualification. Ahmadi et al. (2022b) proposed a common improvement capacity modeland a individualized improvement capacity model to optimize social welfare and fairness while consideringindividual improvement. Jin et al. (2022); Chen et al. (2023) focused on designing subsidy mechanisms toincentivize improvement.",
  "A.3Studies considering both behaviors": "There are other studies considering both strategic manipulation and improvement, but most of them modeledmanipulation and improvement in a similar way(Chen et al., 2020a; Haghtalab et al., 2020; Kleinberg &Raghavan, 2020; Alon et al., 2020; Miller et al., 2020; Shavit et al., 2020; Bechavod et al., 2021; Jin et al.,2022; Barsotti et al., 2022; Ahmadi et al., 2022a; Harris et al., 2022b; Horowitz & Rosenfeld, 2023; Yanet al., 2023; Chen et al., 2023). Specifically, as illustrated in the abstract, the above-listed works all assume",
  "A.4Machine learning fairness": "While machine learning algorithms are able to achieve high accuracy in different tasks, they are likely tobe unfair to individuals from different ethnic groups. To measure the fairness of algorithms, various metricshave been proposed including demographic parity (Feldman et al., 2015), equal opportunity (Hardt et al.,2016b), equalized odds (Hardt et al., 2016b) and equal resource (Gupta et al., 2019). More importantly, several works have studied how strategic behaviors impact fairness (Liu et al., 2018; Zhanget al., 2020b; Liu et al., 2020; Zhang et al., 2022). Specifically, Liu et al. (2018) considered one-step feedbackwhere static fairness does not promote dynamic fairness. Zhang et al. (2020b) analyzed the long-term impactof static fairness metrics based on dynamics of population qualification. Liu et al. (2020) studied how hetero-geneity across groups and the lack of realizability can destroy long-term fairness in strategic classification.Zhang et al. (2022) has proposed a probabilistic model to demonstrate strategic manipulation as well asthe fairness impacts of strategic behaviors, where the individuals shift their feature distribution instead ofdirectly changing their features. The work also assumed randomness in manipulation cost. Meanwhile, itexplored influences on different fairness metrics when strategic manipulation is present(Barocas et al., 2019;Hardt et al., 2016b).",
  "B.1The comparison between our model and causal strategic learning": "Previous works in causal strategic learning model every strategic classification problem as a structural causalmodel (SCM). SCM is a graphic model depicting the causal relationships between different features and thelabel, where features can be classified as causal or non-causal after a causal discovery process (Miller et al.,2020). Strategic manipulation means intervening in the non-causal nodes and improvement corresponds tointervening in the causal nodes. Though the model takes both behaviors into account and can accommodatecomplex causal structures, it has the following weaknesses: (i) The individuals can intervene in any featurenode arbitrarily with a deterministic outcome to any value once their budgets permit, which is not practicalas illustrated in 1; (ii) In most real-world cases, individuals are not able to intervene the observable featuresdirectly. Instead, they intervene in other unobserved features (causal or non-causal) to change the observablefeatures. So it is sometimes meaningless to distinguish whether an observable feature is causal or non-causal,because the root causes of its value change may be diverse. We illustrate (ii) more clearly in , a causal graph where U, V are unobserved. However, U is non-causaland V is causal. It is easy to see only X is observable and correlated to Y , but its change can be eithercausal\" or non-causal\" with respect to Y .",
  ". Loan application:": "(a) Manipulation: an unqualified applicant may steal\" the features from qualified ones by purchasing asocial security card (SSN) from the hackers. The stolen\" features are still random when the applicantdecides to purchase an SSN because the card is often randomly drawn from many stolen cards ofqualified individuals. (b) Improvement: an unqualified applicant may observe the qualified individuals profiles and strive toimitate their behaviors. However, the applicant never knows the realization of his/her features beforetrying to improve. The applicant can only try their best to mimic qualified individuals and expectsthe successful imitation will cause his/her feature distribution to shift.",
  ". Job application:": "(a) Manipulation: an unqualified applicant may steal\" the features from qualified ones by hiring animposter to take the interview instead of him/her (especially when remote interviews are prevalenttoday). Similar to previous examples, the applicant does not know the exact feature realization whenmaking the decision to manipulate. (b) Improvement: an unqualified applicant may still observe the features of qualified ones by readingtheir interview preparation tips or looking at their technical portfolios. Then they may try hard toimitate the qualified individuals. Similar to previous examples, the applicant still has no idea of theexact outcome when he/she decides to improve.",
  "B.3The option of \"doing nothing\"": "Our model implicitly includes the action of \"doing nothing\". Specifically, our model assumes improvementonly succeeds with probability q; and the improvement cost differs among agents which is modeled as arandom variable CI. For agents who do nothing, we may consider them as those who take improvementaction at zero costs but fail to improve. Because no cost is incurred and the improvement fails, theresulting features remain the same and the outcome is equivalent to \"doing nothing\". Although it may be intuitive to model \"doing nothing\" as a separate action, we argue that it is more realistic inpractice for individuals to always take an action. As pointed out by Horowitz et al. (2024), agents who decide",
  "B.4Discussion on adjusted preferences": "Utility loss from the adjusted preferences. Although adjusting preferences is a simple yet effectiveway to promote fairness and disincentivize manipulation, the actual utility received by the decision-makerinevitably diminishes as k1 or k2 changes (as the actual utility the decision-maker receives is always de-termined by the original function U() in equation 3). Nonetheless, such diminished utilities may still behigher than the utility under non-strategic policy . This is illustrated empirically in Sec. 5 and AppendixC. Adjusted preference as a regularizer to promote fairness. We have shown that adjusting weightsk1, k2, k3 in learning objective (equation 5) can control the individual behavior and algorithmic fairness.Indeed, we can view this adjustment mechanism as a regularization method: by adjusting weights, we areessentially changing the objective U() by adding a regularizer, i.e.,",
  "(, k1, k2, k3) (, u(1 ), u(1 ), u(1 ))": "Weights k1, k2, k3 are the regularization parameters.The analysis in Sec. 4.2 and 4.3 suggests that tolearn optimal policies that satisfy certain constraints such as bounded fairness violation and/or boundedindividuals manipulation, we may transform this constrained optimization into a regularized unconstrainedoptimization. This view, by incorporating fairness and strategic classification in a simple unified framework,may provide insights for researchers from both communities.",
  "B.5Estimate Model Parameters": "A complete estimation procedure. With only the knowledge of conditional distribution of qualified in-dividuals PX|Y (x|1) and the populations qualification rate , we introduce a complete procedure to estimatePX|Y (x|0), q, P I, , PCMCI(x) sequentially. Specifically, we need to do controlled intervention experimentson an experimental population as follows. 1. Estimate PX|Y (x|0): Set the lowest decision threshold = 0 to estimate PX|Y (x|0).Since all un-qualified individuals will be accepted, the resulting distribution is the original mixture distribution(1 ) PX|Y (x|0) + PX|Y (x|1). Thus, with minor assumptions on the feature distribution fami-lies, we can estimate PX|Y (x|0). 2. Estimate q: Apply the strictest auditing procedures (e.g., audit everyone in ) to the populationto disable manipulation.With manipulation disabled and arbitrary decision threshold applied, allunqualified people choose to improve, and the resulting qualification rate is (1 )q + .Thus, byexamining the qualification rate after the intervention we can get the estimation of q.",
  "PM()": "5. Finally, with all previous parameters known, we can apply different to the population several times toobtain data points of PM. Then since PM corresponds to points of FCMCI, with minor assumptions onthe distribution family of PM, we can directly fit the distribution and get PCMCI. It is worth noting that all the above steps can be more robust by doing multiple intervention experiments,and controlled experiments are necessary Miller et al. (2020). We will add the above discussion to thepaper to improve its significance. Finally, with all the parameters, the decision-maker can first applyThm. 4.6 to see how to adjust its preferences, and then perform a grid search to find the best k. Robustness of results when q, are noisy. We also present an experiment to relax the assumption thatthe decision-maker knows q, exactly on FICO data. Instead, they only know q + or + where is aGaussian noise. show the results with noisy q, while show the results with noisy ). Theresults show adjusting k still works under noisy q and although inconsistency exists. : Comparison between three types of optimal thresholds (FICO data) when there is a Gaussian noiseon q with standard deviation 0.1 and k1,c = k1,aa = 1.25. For utility and PM, the left value in parenthesisis for Group a, while the right is for Group b. The fairness metric is eqopt.",
  "(Non)-strategic optimal threshold and utility": "To illustrate the complex nature under different permutations of parameters, with the pre-determined pa-rameters in equation 6 and = 0.6, we vary q and plot both non-strategic optimal thresholds and regularstrategic ones with respect to different as shown in the bottom plot of , where the lower graphsillustrate Thm. 4.1, i.e. the red line is always under the blue line. We also demonstrate the strategic utility under different combinations of q, with pre-determined parametersin equation 6 and = 0.3 or = 0.6. and 14 suggest the complicated nature of regular strategicutility under different parameter combinations. It is possible to have 0,1 or 2 extreme points.",
  "Illustration of threshold shifts while adjusting k": "To illustrate 4.5, we demonstrate the effects of adjusting each of k1, k2, k3. According to and 16,we can see when k1 is large enough, the optimal strategic threshold is definitely lower than the optimalnon-strategic ones. However, when is small, we need larger k1 to pull downward. According to and 18, we can see when the population is majority qualified, adjusting k2 is not guaranteed to shift",
  "Illustration of condition 1.(i), Thm. 4.5": "We first show a parameter setting satisfying condition 1.(i) in Thm. 4.5. With pre-determined parametersin equation 6 , we set q = = 0.5 and a = 0.2, b = 0.25. This matches the notation tradition in Sec. 4.3where group a is the disadvantaged group with a lower qualified percentage. Also, because q + 1, thesetting satisfies condition 1.(i) in Thm. 4.5. We first illustrate the manipulation probability under optimaloriginal strategic threshold s as in . From , we can set k1a = k2a = 1.25 to let the strategicutility still be larger than the one under non-strategic optimal threshold(i.e. the solid blue line is above thesolid red line), while lower the cumulative density dramatically (i.e. the dotted green line) to admit more",
  "Illustration of condition 1.(ii), Thm. 4.5": "With pre-determined parameters in equation 6 , we set q = = 0.25 and a = 0.4, b = 0.6.Thismatches the notation tradition in Sec. 4.3 where group a is the disadvantaged group with a lower qualifiedpercentage. We first illustrate the manipulation probability under optimal original strategic threshold sas in . reveals that 1.(ii) in 4.5 is satisfied because the orange and green points are bothlocated before the extreme large point of PM(). Thus, we could increase k1s to disincentivize manipulationwhile improving fairness as shown in .From , we can set k1a = k2a = 1.25 to let thestrategic utility still be larger than the one under non-strategic optimal threshold(i.e. the solid blue lineis above the solid red line), while lower the cumulative density dramatically (i.e. the dotted green line) toadmit more qualified individuals and disincentivize manipulation (i.e. the dashed blue line). In ,We summarize the comparison between non-strategic , original strategic , and adjusted strategic (k1)(when k1,c = k1,aa = 1.25). It shows that decision-makers by adjusting preferences can significantly mitigateunfairness and disincentivize manipulation, with only slight decreases in utilities.",
  "Illustration of condition 2, Thm. 4.5": "Besides, we also show one more parameter setting satisfying condition 2 in Thm. 4.5. With pre-determinedparameters in equation 6 , we also set q = = 0.2 and a = 0.3, b = 0.35. This matches the notationtradition in Sec. 4.3 where group a is the disadvantaged group with a lower qualified percentage. Also, basedon , q + < 1 and a, b < 0.5, the setting satisfies condition 2 in Thm. 4.5. We first illustratesthe manipulation probability under optimal original strategic threshold s and non-strategic threshold s asin . As shown in , for both groups, we demonstrate the manipulation probability for , and (k1) when k1 varies, (non)-strategic utility and cumulative density conditioned on Y = 1 (i.e. thismeasures the unfairness based on equal opportunity). This plot suggests we can find suitable k2a and k2b todisincentivize manipulation and promote fairness, while also making the utility higher than the one undernon-strategic optimal threshold. From , we can set both k2a and k2b at 1.25 to let the strategic utility",
  ": Strategic optimal threshold (k1) after increasing k1 while keeping k2, k3 fixed. Left figure hasq = 0.01 and right figure has q = 0.99, while both figures have = 0.6": "still be larger than the utility under non-strategic optimal threshold (i.e. the solid blue line is above thesolid red line), while keeping the cumulative density function closer (i.e. the green dotted line) to mitigateunfairness, and also disincentivize manipulation (i.e. the blue dashed line). The details of comparisons areshown in .",
  "D.1Derivations of equation 1": "UM() is the expected utility gain of an unqualified agent if choosing to manipulate: i. If the manipulationis not exposed, the probability of admission is 1 FX|Y (|1) because the manipulation leads the agentsto get his/her new feature from PX|Y =1, which happens at a probability 1 ; ii.If the manipulationis exposed, the probability of admission is 0, which happens at a probability ; iii. If the agent does notmanipulate, the probability of admission is 1FX|Y (|0) because now his/her feature is from the unqualifiedpopulation, and keep in mind that the agents will never know the exact values of his/her feature whenhe/she makes decisions; Then according to the total probability theorem, the expectation of utility gainUM() = (1 ) (1 FX|Y (|1)) + 0 (1 FX|Y (|0)) CM. UI() is the expected utility gain of an unqualified agent if choosing to improve: i. If the improvementsucceeds, the probability of admission is 1 FX|Y (|1) because the improvement leads the agents to gethis/her new feature from PX|Y =1, which happens at a probability q; ii. If the manipulation is exposed,the probability of admission is 1 F I(), which happens at a probability 1 q; iii. If the agent does notmanipulate, the probability of admission is 1 FX|Y (|0).",
  "D.3Proof of Thm. 4.1": "Assume (a, b). When q 1, improvement will always succeed. Also, Thm. 2.3 reveals PM() reachesits minimum when a, so PM(a) < 0.5. Thus, improvement will always bring a benefit that is largerthan manipulation to the strategic decision-maker (since improvement always succeeds). Thus, the decisionmaker may set a threshold as low as possible ( a) to maximize its utility, which will always be lower thanthe non-strategic optimal threshold.",
  "D.4Proof of Prop. 4.2": "Assume (a, b). Consider the situation when k2, k3 both stay fixed and k1 , U = + U is dominatedby k11. Noticing 1 reaches its maximum when a, we will also have the new optimal (k1) a.Since a is the minimum possible value of the threshold, the optimal threshold when ka is large enoughwill definitely be smaller than the optimal non-strategic threshold as well as the original optimal strategicthreshold.",
  "D.5Proof of Prop. 4.3": "Assume (a, b). Consider the situation when k1, k3 both stay fixed and k2 , U = + U is dominatedby k22. 2 0 both when b or a (i.e. 2 reaches its minimum). However, the non-strategic utilityshould be 0 when b but smaller than 0 when a if not majority of people are qualified. This willmake the new optimal (k2) b. Since b is the maximum possible value of the threshold, the optimalthreshold when k2 is large enough will definitely be larger than the optimal non-strategic threshold as wellas the original optimal strategic threshold.",
  "get 1(1)PX|Y (X|1)PX|Y (X|0). This suggests the term will first increase and then decrease. Thus, the maximizer of": "k33 = k3PM (1(1)PX|Y (X|1)PX|Y (X|0)) will locate before the root of (1)(1FX|Y (|1))(1FX|Y (|0)).Then noticing that increasing will lower the value of the root, we can confirm the existence of to make theroot small enough, thereby making the maximizer of k3 3 smaller enough. Then because U is dominatedby k33, (k3) will also be small enough.",
  "Define Fcs as some cumulative density function (CDF) associated with fairness metric C. The unfairnessEXP Ca [1(x a)] EXP Cb [1(x b)] can also be written as Fca(a) Fca(b)": "1.Under situation 1, Thm.4.5 already reveals increasing k1 can disincentivize strategic manipulation.Meanwhile, Fcs(s(k1)) will decrease for both groups because s(k1) decreases for both group. Thus, theremust exist k1a, k1b to mitigate the difference between Fca(a(k1)) and Fcb(b(k1)), which is promoting thefairness at the same time of disincentivizing manipulation. 2.Under situation 2, Thm.4.5 already reveals increasing k2 can disincentivize strategic manipulation.Meanwhile, Fcs(s(k2)) will increase for both groups because s(k2) increases for both group. Thus, theremust exist k2a, k2b to mitigate the difference between Fca(a(k2)) and Fcb(b(k2)), which is promoting thefairness at the same time of disincentivizing manipulation. 3.Under situation 3, Thm. 4.5 already reveals increasing k1 for group a and increasing k2 for group b candisincentivize strategic manipulation. Meanwhile, Fcs(a(k1)) will decrease for a and Fcs(b(k2)) increase forb. Thus, because a is already the disadvantaged group, the difference between Fcs(a(k1)) and Fcs(b(k2))will be mitigated, which is promoting the fairness at the same time of disincentivizing manipulation.",
  "D.9Proof of Corollary 4.7": "Corollary 4.7 can be derived directly from Thm. 4.5 and Thm. 4.6. To recap, Thm. 4.5 identifies allscenarios under which manipulation is guaranteed to be disincentivized via adjusting preferences; Theoremreftheorem:fairness finds all scenarios when promoting fairness and disincentivizing manipulation can beattained simultaneously; Corollary 4.7 emphasizes all scenarios where disincentivizing manipulation doesnot guarantee fairness improvement. In Corollary 4.7, to ensure the manipulation to always be disincentivized, both groups a, b should satisfyeither scenario identified in Thm. 4.5. This results in four possible combinations, and three out of thesefour are the scenarios found in Thm.4.6.The left one situation is the case in Corollary 4.7 (group asatisfies condition 2 and group b satisfies condition 1). In this case, group a can be disincentivized only by"
}