{
  "Abstract": "We study stochastic Cubic Newton methods for solving general, possibly non-convex mini-mization problems. We propose a new framework, the helper framework, that provides aunified view of the stochastic and variance-reduced second-order algorithms equipped withglobal complexity guarantees; it can also be applied to learning with auxiliary information.Our helper framework offers the algorithm designer high flexibility for constructing andanalyzing stochastic Cubic Newton methods, allowing arbitrary size batches and using noisyand possibly biased estimates of the gradients and Hessians, incorporating both the variancereduction and the lazy Hessian updates. We recover the best-known complexities for thestochastic and variance-reduced Cubic Newton under weak assumptions on the noise. Adirect consequence of our theory is the new lazy stochastic second-order method, whichsignificantly improves the arithmetic complexity for large dimension problems. We alsoestablish complexity bounds for the classes of gradient-dominated objectives, includingconvex and strongly convex problems. For Auxiliary Learning, we show that using a helper(auxiliary function) can outperform training alone if a given similarity measure is small.",
  "i=1fi(x),(1)": "or, more generally, as an expectation over some given probability distribution: f(x) = Ef(x, ). When fis non-convex, this problem is especially difficult since finding a global minimum is NP-hard in general (Hillar& Lim, 2013). Hence, the reasonable goal is to look for approximate solutions. The most prominent family ofalgorithms for solving large-scale problems of the form (1) are the first-order methods, such as the StochasticGradient Descent (SGD) (Robbins & Monro, 1951; Kiefer & Wolfowitz, 1952). They employ only stochasticgradient information about the objective f(x) and guarantee the convergence to a stationary point, which isa point with a small gradient norm. Nevertheless, when the objective function is non-convex, a stationary point may be a saddle point or even alocal maximum, which might not be desirable. Another common issue is that first-order methods typicallyhave a slow convergence rate, especially when the problem is ill-conditioned. Therefore, they may not besuitable when high precision for the solution is required.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Our new methods posses the best-known global complexity bounds for stochastic second-order optimizationin the non-convex case, and significantly benefit in terms of total arithmetic computations, by reducing thenumber of computed Hessians and the required number of matrix factorizations. For auxiliary learning, we demonstrate a provable benefit of using auxiliary data compared to trainingalone. Besides investigating general non-convex functions for which we proved global convergence rates to asecond-order stationary point, we also studied the classes of gradient-dominated functions with improvedrates of convergence to the global minima.",
  "Contributions": "We introduce the helper framework, which we argue encompasses multiple methods in a unifiedway. Such methods include stochastic methods, variance reduction, Lazy methods, core sets, andsemi-supervised learning. This framework covers previous versions of the variance-reduced stochastic Cubic Newton methodswith known rates. Moreover, it provides us with new algorithms that employ Lazy Hessian updatesand significantly improves the arithmetic complexity (for high dimensions), by using the same Hessiansnapshot for several steps of the method. In the case of Auxiliary learning, we provably show a benefit from using auxiliary tasks as helpers inour framework. In particular, we can replace the smoothness constant with a similarity constant,which might be smaller. Moreover, our analysis works both for the general class of non-convex functions, as well as forthe classes of gradient-dominated problems, which include convex and uniformly convex functions.Hence, in particular, we justify new improved complexity bounds for the deterministic Cubic Newtonmethod with Lazy Hessian updates (Doikov et al. (2022)); as well as for the stochastic Cubic Newtonalgorithms with variance reduction that take into account the total arithmetic cost of the operations(see ). In the following table, we provide a comparison of the total complexities (in terms of the number of stoch.gradient calls) for finding a point with small gradient norm Ef(x) (non-convex case), or a globalsolution in terms of the functional residual Ef(x) f (convex case, gradient dominated functions ofdegree = 1), by different first-order and second-order optimization algorithms. We take into account thatthe cost of one stochastic Hessian is proportional to d times the cost of the stochastic gradient, where d is theproblem dimension, which holds for general dense problems.",
  "d / 1/2(new)This work": ": The total number of stochastic gradient computations for solving the problem with accuracy. n is thenumber of functions (the data size), and d is the dimension of the problem. We use x y to denote min(x, y). We see that for d n2/3 (large dimension setting) it is better to use the new VRCN-Lazy method thanthe VRCN algorithm. Moreover, note that both in the SCN and VRCN algorithms, we need to solve acubic subproblem with a new approximate Hessian matrix at each iteration. This means that, in case of theexact steps, an expensive matrix factorization needs to be computed every iteration for these algorithms. Atthe same time, our new VRCN-Lazy method benefits from utilizing a matrix factorization for many steps,significantly improving the total arithmetical cost of the method.",
  "Notation and Assumptions": "We consider the finite-sum optimization problem (1) and we assume that our objective f is bounded frombelow, denoting f := infx f(x), and use the following notation: F0 := f(x0) f , for some initial x0 Rd. We denote by x := x, x1/2, x Rd, the standard Euclidean norm for vectors, and the spectral norm forsymmetric matrices by H := max{max(H), min(H)}, where H = H Rdd. We will also use x yto denote min(x, y).",
  "y x3 .(2)": "Here, g and H are estimates of the gradient f(x) and the Hessian 2f(x), respectively. Note that solving(2) can be done efficiently even for non-convex problems (see Conn et al. (2000); Nesterov & Polyak (2006);Cartis et al. (2011a)). Generally, the cost of computing x+ is O(d3) arithmetic operations, which are neededfor evaluating an appropriate factorization of H. Hence, it is of a similar order as the cost of the classicalNewtons step. Inexact first-order solvers for the cubic subproblem were studied in Carmon & Duchi (2020);Nesterov (2022).",
  "Note that this definition implies that if c(x) 3/2 then x is an (, c)-approximate local minimum": "Computing gradients and Hessians.It is clear that computing the Hessian matrix can be much moreexpensive than computing the gradient vector. We denote the corresponding arithmetic complexities ofcomputing one Hessian and gradient of fi(x), 1 i n by HessCost and GradCost. We will make and followthe convention that HessCost = d GradCost, where d is the dimension of the problem. For example, thisis known to hold for neural networks using the backpropagation algorithm (Kelley, 1960). However, if theHessian has a sparse structure, the cost of computing the Hessian can be cheaper Nocedal & Wright (2006).Then, we can replace d with the effective dimension deff := HessCost",
  "fx(y) + Mrx(y),": "where fx() is an approximation of f around current point x, and rx(y) is a regularizer that encodes howaccurate the approximation is, and M > 0 is a regularization parameter. In this work, we are interested incubically regularized second-order models of the form (2) and we use rx(y) := 1",
  "+ f(y) h(y)expensive": "We discuss the actual practical choices of the helper function h below. We assume now that we can afford thesecond-order approximation for the cheap part h around the current point x. However, approximating thepart f h can be expensive (as for example when the number of elements n in finite sum (1) is huge), or evenimpossible (due to lack of data). Thus, we would prefer to approximate the expensive part less frequently.For this reason, let us introduce an extra snapshot point x that is updated less often than x. Then, we useit to approximate f h. Another question we still need to ask is what order should we use to approximatef h? We will see that order 0 (i.e., a constant) leads us to the basic stochastic methods, while for orders 1and 2, our methods are akin to classical variance reduction techniques.",
  "H(h, x, x)(y x), y x,(3)": "where C(x, x) is a constant, G(h, x, x) is a linear term, and H(h, x, x) is a matrix. Note that if x x, thenthe best second-order model of the form (3) is the Taylor polynomial of degree two for f around x, and thatwould yield the exact Newton-type method. However, when the points x and x are different, we obtain muchmore freedom in constructing our models. For using this model in our cubically regularized method (2), we only need to define the gradient g = G(h, x, x)and the Hessian estimates H = H(h, x, x), and we can also treat them differently (using two different helpers,h1 and h2, correspondingly). Thus, we come to the following general second-order (meta)algorithm. Weperform S rounds, the length of each round is m 1, which is our key parameter:",
  "orxt=arg mini{tm+1,...,t}f(xi)(use the best iterate).(5)": "Clearly, option (5) is available only in case we can efficiently estimate the function values. However, wewill see that it serves us with better global convergence guarantees for the gradient-dominated functions. Itremains to specify how we choose the helpers h1 and h2. We need to assume that they are somehow similarto f. Let us present several efficient choices that lead to implementable second-order schemes.",
  "Basic Stochastic Methods": "If the objective function f is very expensive (for example of the form (1) with n ), one option is toignore the part f h i.e. to approximate it by a zeroth-order approximation: f(y) h(y) f(x) h(x).Since it is a constant, we do not need to update x. In this case, we have:",
  "However, as we will show next, this seemingly pessimistic dependence leads to the same rate of classicalsubsampled Cubic Newton methods discovered in Kohler & Lucchi (2017); Xu et al. (2017; 2016)": "At this point, let us discuss the specific case of stochastic optimization, where f has the form of (1), with npotentially being very large. In this case, it is customary to sample batches at random and assume the noiseto be bounded in expectation. Precisely speaking, if we assume the standard assumption that for one indexsampled uniformly at random, we have Eif(x) fi(x)2 2g and Ei2f(x) 2fi(x)3 3h , x,then it is possible to show that for",
  "Let the Objective Guide Us": "If the objective f is such that we can afford to access its gradients and Hessians from time to time (functionsof the form (1) with n < being reasonable), then we can do better than the previous chapter. In thiscase, we can use a better approximation of the term f(y) h(y). From a theoretical point of view, we cantreat the case when f is only differentiable once, and thus, we can only use a first-order approximation off h; in this case, we will only be using the Hessian of the helper h but only gradients of f. However, in ourcase, if we assume we have access to gradients, then we can also have access to the Hessians of f as well(from time to time); for this reason, we consider a second-order approximation of the term f h. If we followthe procedure that we described above, we find:",
  "H(h2, x, x) :=2h2(x) 2h2(x) + 2f(x).(12)": "We see that there is an explicit dependence on the snapshot x, and thus, we need to address the questionof how we should update this snapshot point in Algorithm 1. In general, we can update it with a certainprobability p 1m, or we can use more advanced combinations of past iterates (e.g., a moving average).However, for simplicity, we study option 4 (i.e., using the last iterate for updating the snapshot x); thus, it isupdated only once every m iterations. We also need to address the question of measuring the similarity in this case. Since we employ a second-orderapproximation of f h, it seems natural to compare the function f and its helpers h1 and h2 by using thedifference between their third derivatives or, equivalently, the Hessian Lipschitz constant of their difference.Precisely, we make the following similarity assumption:",
  "Based on the choices of the helpers h1 and h2, we can have many algorithms.We discuss particularimplementations in the following sections": "We start by presenting the variance reduction combined with the Lazy Hessian updates, which rely onsampling batches randomly. Our new method will significantly improve complexity bound (10) obtainedfor the basic helpers, and achieve the best overall performance among all known variants of stochasticsecond-order methods (see ). Then we discuss other applications: the core sets, which try to intelligently find a weighted representativebatch describing the whole dataset; semi-supervised learning, engineering the helpers using unlabeled data;and, more generally, auxiliary learning, which tries to leverage auxiliary tasks in training a given main task.We show that the auxiliary tasks can naturally be treated as helpers.",
  "Variance Reduction and Lazy Hessians": "First, note that choosing h1 = h2 = f gives the classical Cubic Newton method (Nesterov & Polyak, 2006),whereas choosing h1 = f and h2 = 0 gives the Lazy Cubic Newton (Doikov et al., 2022). In both cases, werecuperate the known rates of convergence. However, these choices requires to compute the full gradients,that can be expensive for solving problem (1) with large number n of components.",
  "d and gV R(n, d) (nd)4/5(n2/3d+n)": "In particular, for d n2/3 we have gLazy(n, d) gV R(n, d) and thus for d n2/3 it is better to use LazyHessians along with the variance reduction, obtaining complexity (19) than becomes strictly better than (17).We also note that for the Lazy approach, we can keep a factorization of the Hessian (this factorization inducesmost of the cost of solving the cubic subproblem); thus, it is as if we only need to solve the subproblem onceevery m iterations, so the Lazy approach has a big advantage compared to the general approach, and theadvantage becomes even bigger for the case of large dimensions.",
  "The result in (14) is general enough to include many other applications that are limited only by our imagination;there are, to cite a few such applications :": "Core sets. (Bachem et al., 2017) The idea of core sets is simple: can we summarize a potentially large dataset using only a few (potentially weighted) important examples? Many reasons, such as redundancy, make theanswer yes. Devising approaches to find such core sets is outside of the scope of this work. Formally, for theobjective function f(x) of the finite-sum structure (1), the core set helper can be given by the following form:",
  "Example 3.8. Let core representatives f1, . . . , fm be fixed, and assume that each function from our targetproblem (1) be given byfi(x):=fji(x) + i(x),1 i n,": "where ji {1, . . . , m} is the index of the corresponding core representative and i(x) is some noise. Then,function of from (20) with weights wj := ni=1[ji = j] will be a helper function according to our framework.Assuming that each i has Lipschitz Hessian with constant L will imply Assumption 3.5 for h1 = h2 = hwith 1 = 2 = L.",
  "all the while guaranteeing an improved rate. So then, if we can design such smallbatches with small 1 and 2, we can keep reusing them and enjoy the improved rate without needing largebatches": "Auxiliary learning. (Baifeng et al.; Aviv et al.; Xingyu et al.) study how a given task f can be trained inthe presence of auxiliary (related) tasks; our approach can indeed be used for auxiliary learning by treatingthe auxiliaries as helpers; if we compare (14) to the rate that we obtained without the use of the helpers:O(",
  "L 1": "Semi-supervised learning. (Yang et al., 2021) Semi-supervised learning is a machine learning approachthat uses both labeled and unlabeled data during training. In general, we can use the unlabeled data toconstruct the helpers; we can start, for example, by using random labels for the helpers and improvingthe labels with training. There are at least two special cases where our theory implies improvement byonly assigning random labels to the unlabeled data. In fact, for both regularized least squares and logisticregression, we notice that the Hessian is independent of the labels (only depends on inputs). Indeed, let us consider the classical logistic regression model (e.g., Murphy (2012)) for the set of labeleddata {(ai, bi)}ni=1, where bi = 1. Then fitting the model can be formulated as the following optimizationproblem:",
  "i=1(ai x)aiai": "Thus, we see that the last expression does not depend on the input labels bi. Therefore, if the unlabeled datacomes from the same distribution as the labeled data, then we can use it to construct helpers which, at leasttheoretically, have 1 = 2 = 0. Because the Hessian is independent of the labels, we can technically endowthe unlabeled data with random labels. Theorem 3.6 will imply in this case E[L(xout)] = O(",
  "Ef(xt) f Ef(xt),(24)": "where the expectation is taken with respect to the iterates (xt) of our algorithms; this is a stronger assumptionthan (23). To avoid using (24), we can assume that the iterates belong to some compact set Q Rd andthat the gradient norm is uniformly bounded: x Q : f(x) G. Then, a (, )-gradient dominated onset Q function is also a (G3/2, 3/2)-gradient dominated on this set for any > 3/2.",
  "M + 1.(27)": "Theorem 4.2 shows (up to the noise level) the global sublinear rate for 1 < 3/2, the global linear rate for = 3/2 (which can also be seen by taking the limit 3/2 in (25)) and the superlinear rate for > 3/2,after the initial phase of t0 iterations. We would like to point that instead of Assumption 3.1 we only need to assume Eh1[h1(x)f(x)] 1and Eh2[2h2(x) 2f(x)2] 22which might be weaker depending on the value of . We also provethe following theorem, which extends the results in Theorem 3.6 to the gradient-dominated functions, forthe advanced helpers. In this case, we set the snapshot line 3 in Algorithm 1) as in (5) i.e., the snapshotcorresponds to the state with the smallest value of f during the last m iterations.",
  "To Be Lazy or Not to Be": "To verify our findings from Subsection 3.3, we consider a logistic regression problem (21) with 2-regularizationon the a9a data set Chang & Lin (2011). We consider the variance-reduced cubic Newton method from(Zhou et al., 2019) (referred to as full VR), its lazy version where we do not update the snapshot Hessian(Lazy VR), the stochastic Cubic Newton method (SCN), the Cubic Newton algorithm (CN), GradientDescent with line search (GD\") and Stochastic Gradient Descent (SGD). We report the results in terms oftime and gradient arithmetic computations needed to arrive at a given level of convergence. shows that the lazy version saves both time and arithmetic computations without sacrificing theconvergence precision. In these graphs, Gradcost is computed using the convention that computing onehessian is d times as expensive as computing one gradient. Arith. comput. , (Gradcost) ff Time, s ff GDLazy VRfull VRCNSCNSGDAdam Logistic regression: a9a, d = 123, n = 32561, L2-regularization",
  "Auxiliary Learning": "Our goal is to show that the helper framework is very general and that it goes beyond the variance reductionand lazy Hessian computations. For the previously considered problem of training the logistic regression(using the same a9a data set), we suppose that we also have access to unlabeled data (in this sense, thisbecomes semi-supervised learning). Specifically, we have a labeled dataset Dl = {(ai, bi)}Nli=1 and an unlabeleddata set Du = {ai}Nl+Nui=Nl+1, we suppose that both data sets are sampled from the same distribution P(a,b).Our goal is to minimizef(x) = E(a,b)P(a,b)[log(1 + exp(bxa))]. A simple computation (see .4) shows that the Hessian of f only depends on Pa, and, for this reason,we can use unlabeled data to construct a good approximation of the true Hessian (if we can sample from Pa,",
  "Hessian Cost vs Gradient Cost": "We consider again a diagonal neural network and estimate the time costs needed for computing its gradient,Hessian, decomposing the Hessian, and solving the cubic subproblem. shows that the average costof computing the Hessian is significantly higher than the cost of computing one gradient, and the quotientgrows approximately linearly with d (the dimension of the problem). also shows that the cost ofdecomposing the Hessian dominates that of solving the cubic subproblem which explains how our lazy VRmethod saves time compared to other methods.",
  "Dimension": "Hessian / Gradient Average Hessian cost / Gradient cost Hessian / Gradientd / 10 : (left) times needed to compute the gradient, the Hessian, decompose the Hessian and solve the cubicsubproblem for a diagonal neural network with n = 10000 and different values of the dimension d. (right) average timefor computing the Hessian divided by the average time needed to compute the gradient. we notice that the Hessiancomputation cost is almost proportional to d the gradient cost. We also see that the time needed to decompose theHessian dominates the time needed to solve the cubic subproblem. Time, s ff Mushrooms, Non-convex Lazy VRfull VRSGD Time, s Covtype, convex Time, s w8a, Non-convex Labeled data access 100Auxiliary Learning, w8a CNAux-CN",
  "Limitations and possible extensions": "Estimating similarity between the helpers and the main function.While we show in this workthat we can have an improvement over training alone, this supposes that we know the similarity constants1, 2; hence, it will be interesting to have approaches that can adapt to such constants. Engineering helper functions.Building helper task with small similarities is also an interesting idea.Besides the examples in supervised learning and core-sets that we provide, it is not evident how to do it in ageneralized way. Using the helper to regularize the cubic subproblem.We note that while we proposed to approximatethe cheap\" part as well in , one other theoretically viable approach is to keep it intact andapproximately solve a proximal type\" problem involving h; this will lead to replacing L by , but thesubproblem is even more difficult to solve. However, our theory suggests that we dont need to solve thissubproblem exactly; we only need m L",
  "BAdditional Experiments": "Arith. comput. , (Gradcost) ff GDLazy VRfull VRCNSCNSGDAdam Time, s ff Logistic regression on Mnist (n=60K, d=780) : Logistic regression on the MNIST dataset for different algorithms. We notice that second-order methodsclearly outperform first-order methods. We also note how cubic Newton (CN) starts as the fastest but our Lazy VRmethod catches up to it. Arith. comput. , (Gradcost) ff GDLazy VRfull VRCNSCNSGDAdam Time, s ff Logistic regression on Cifar10 (n=50K, d=3072) : Logistic regression on the CIFAR10 dataset for different algorithms. We notice that second-order methodsclearly outperform first-order methods. We also note that our Lazy VR has a big advantage in terms of time comparedto all other methods. Arith. comput. , (Gradcost) ff GDLazy VRfull VRCNSGDAdam Time, s ff A9A, 2-layer MLP : A two-layer NN trained on the A9A dataset. We notice that GD (with line search) outperforms all thealgorithms but gets stuck at an error of 106. Lazy VR is the second fastest and even ends up beating GD to a smallererror, suggesting that it escaped the local minimum GD was stuck at. Again Lazy VR saves more time compared tofull VR.",
  ". We have the following special cases based on the choice of the helper functions": "Basic Cubic Newton (Nesterov & Polyak, 2006) corresponds to 1 = 2 = 0. Thus, we getits known rate in the convex case. We reach an -global minimum using the following number ofarithmetic operations:Ond GradCost.(41) Cubic Newton with Lazy Hessian updates (Doikov et al., 2022) corresponds to 1 = 0, 2 = L.It gives f(xout) f = OLD3S2m. By choosing m = d, we reach an -global minimum using thefollowing number of arithmetic operations:",
  "Omin{(nd)4/5, n2/3d+n} GradCost.(43)": "In Masiha et al. (2022), the global complexity estimate for the stochastic Cubic Newton with VarianceReduction was established for the gradient dominated functions of degree = 1. However, theirresult is different from ours, assuming only stochastic samples of the gradients and Hessians, while inour work, we allow recomputing the full gradient and Hessian once per m iterations, similar in spiritto (Zhou et al., 2019; Wang et al., 2019). In our analysis, we take into account the total arithmeticcost of the operations. Hence, to the best of our knowledge, our complexity estimate (43) is new."
}