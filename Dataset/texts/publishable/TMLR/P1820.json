{
  "Abstract": "We study the adaptation of Soft Actor-Critic (SAC), which is considered as a state-of-the-art reinforcement learning algorithm, from continuous action space to discrete actionspace. We revisit vanilla discrete SAC, i.e., SAC for discrete action space, and provide anin-depth understanding of its Q value underestimation and performance instability issueswhen applied to discrete settings.We thereby propose Stable Discrete SAC (SD-SAC),an algorithm that leverages entropy-penalty and double average Q-learning with Q-clip toaddress these issues.Extensive experiments on typical benchmarks with discrete actionspace, including Atari games and a large-scale MOBA game, show the efficacy of SD-SAC.Our code is at:",
  "Introduction": "In the conventional model-free reinforcement learning (RL) paradigm, an agent can be trained by learningan approximator of action-value (Q) function (Mnih et al., 2015; Bellemare et al., 2017). The class of actor-critic algorithms (Mnih et al., 2016; Fujimoto et al., 2018) evaluates the policy function by approximating thevalue function. Motivated by maximum-entropy RL (Ziebart et al., 2008; Rawlik et al., 2012; Abdolmalekiet al., 2018), soft actor-critic (SAC) (Haarnoja et al., 2018a) introduces action entropy in the framework of",
  "Published in Transactions on Machine Learning Research (11/2024)": "As for MsPacman, the player controls a Pacman, who scores points by eating dots in a maze while avoidingfloating ghosts. When Pacman eats an energy pill, she can attack the ghosts to gain higher scores. In all three environments, the agent can quickly gain deceptive rewards through short-term payoffs. ForAssault and Jamesbond, all points come from shooting actions that hit specific targets, while avoidingobstacles can prevent the loss of life but does not bring clear rewards. Thus, agents often excel at shootingbut struggle with dodging. In MsPacman, the numerous dots in the maze provide many rewards for theagents movement. As a result, the agent finds it difficult to learn advances strategies such as avoiding ghostsand picking up energy pills to attack ghosts. The presence of deceptive rewards leads to the training processstuck in local optima, making it challenging to explore better, long-term strategies.",
  "Related Work": "Adaptation of Action Space.The most relevant works to this paper are vanilla discrete SAC(Christodoulou, 2019), TES-SAC (Xu et al., 2021) and Soft-DQN (Vieillard et al., 2020). Discrete SACreplaces the Gaussian policy with a categorical one and discretizes the Q-value output to adapt SAC fromcontinuous to discrete action space. However, as we will point out, a direct discretization of SAC will havespecific failure modes with poor performance. TES-SAC proposes a new scheduling method for the targetentropy parameters in discrete SAC. Soft-DQN has discretized SAC by adopting the maximum-entropy RLto DQN, utilizing only a Q value parametrization and directly applies a softmax operation to the Q-valuesto take action. Q Estimation. Previous works (Fujimoto et al., 2018; Ciosek et al., 2019; Pan et al., 2020; Duan et al.,2021) have already expressed concerns about the estimation bias of Q value for SAC. SD3 (Pan et al.,2020) proposes to reduce the Kurtosis distribution of Q approximately by using the softmax operator on theoriginal Q value output to reduce the overestimation bias. OAC (Ciosek et al., 2019) constrains the Q valueapproximation objective by calculating the upper and lower boundaries of two Q-networks. DistributionalSAC (Duan et al., 2021) replaces the Q learning target with the expected reward sum obtained from thecurrent state to the end of the episode and uses a multi-frame estimates target to reduce overestimation.Maxmin Q-learning (Lan et al., 2020) controls estimation bias by minimizing the complete ensemble in thetarget. MME (Han & Sung, 2021) extends max-min operation to the entropy framework to adapt to SAC.REM (Agarwal et al., 2020) ensembles Q-value estimations with a random convex combination to enhancegeneralization in the offline setting. REDQ (Chen et al., 2021b) reduces the estimation bias by minimizinga random subset of Q-functions. AEQ (Gong et al., 2023) adjusts the estimation bias by using the mean ofQ-functions minus their standard deviation. However, little research is on discrete settings. Our approachfocuses on reducing the underestimation bias for the double Q-estimators to enhance exploration. Performance Stability. Flow-SAC (Ward et al., 2019) applies a technique called normalizing flows policyon continuous SAC leading to the finer transformation that improves training stability when exploringcomplex states. However, applying normalizing flows to discrete domains will cause a degeneracy problem(Horvat & Pfister, 2021), making it difficult to transfer to discrete actions. SAC-AWMP (Hou et al., 2020)improves the stability of the final policy by using a weighted mixture to combine multiple policies. Based onthis method, the cost of network parameters and inference speed is significantly increased. ISAC (Banerjeeet al., 2022) increases SAC stability by mixing prioritized and on-policy samples, enabling the actor to repeatlearns states with drastic changes. Repeatedly learning priority samples, however, runs the risk of settlinginto a local optimum. By comparison, our method improves policy stability in case of drastic state changeswith an entropy constraint.",
  "where Qi represents i-th target-critic network": "Policy Update Iteration The policy, parameterized by , distills the softmax policy induced by the softQ-function. The discrete SAC policy directly maximizes the probability of discrete actions, in contrast tothe continuous SAC policy, which optimizes the two parameters of the Gaussian distribution. Then, thediscrete SAC policy is updated by minimizing KL divergence between the policy distribution and the softQ-function.",
  "Unstable Coupling Training": "The first failure mode comes from the instability caused by fluctuations in Q function distribution andpolicy entropy during training. The maximum entropy mechanism in SAC effectively balances explorationand exploitation. However, due to the existence of entropy term in the soft Bellman error, and the mechanismin discrete SAC that aligns the policy with the Q function, the policy update iteration (Eq. 8) is stronglycoupled with Q-learning iteration (Eq. 5). In environments with deceptive rewards (Hong et al., 2018), the agent can gain substantial returns in theearly stages of training through short-term rewards, causing the Q value of specific actions to rise rapidlyand the Q function distribution to become sharper. The coupling learning paradigm of discrete SAC leadsto a sharper policy distribution and, thus, a decline in entropy. Consequently, the Q learning target becomesunstable, which can, in turn, deteriorate the policy learning. As a result, the agent falls into local optimaand struggles to discover alternative strategies with larger long-term payoffs. To illustrate this issue moreconcretely, we take the training process of discrete SAC in the Atari game Asterix as an example. As shown in (a), the player controls Asterix, which can move horizontally and vertically. In eachround, horizontally moving objects appear. Asterix scores points by collecting objects and loses a life whencollecting a lyre. In the early stage of the game, rounds often appear where there are only scoring objectsand no life-losing lyres ((b)), allowing the agent to score quickly by collecting objects, resulting indeceptive rewards. These rewards make the Q function sharper, thereby reducing the entropy of the policy.In (a), we sample a fixed set of states, and measure the variance of Q function across different actionsfor these states. We find that the Q function variance increases rapidly, indicating that the Q functionbecomes sharp quickly. Policy entropy also decreased during this period. As the learning process continues, the policy entropy drops rapidly, and the action probabilities becomedeterministic quickly ((c)). The agent can collect objects but struggles to avoid obstacles effectively.After the policy entropy reaches its lowest point at round 2 million steps, neither the episode length ((d))nor the number of steps with rewards ((e)) increases significantly. At the same time, the drastic changeof policy entropy misleads the learning process, and thus, both Q-value and policy fall into local optimum((c) and (b)). Since both policy and Q-value converge to the local optimum, it becomes hard forthe policy to explore efficiently in the later training stage. Even the policy entropy re-rises in the later stage((c))), the performance of policy does not improve anymore ((f)). Similar situations also occurin other Atari environments, and we provide more examples in Appendix A.6.",
  ": Measuring Q variance, estimation of Q-value, policy entropy, episode length, steps with rewards,and score on Atari Game Asterix with discrete SAC over 10 million timesteps": "As shown in Eq. 11, the improvement of Q(st, at) relies on the Q-estimation of the following states and policyentropy. However, a sharper Q function causes the drastically shifting entropy, increasing the uncertainty ofgradient updates and misleading the learning of the Q-network. Since the soft Q-network induces the policy,the policy can also become misleading and hurt performance. To mitigate this phenomenon, the key is toensure the smoothness of policy entropy change to maintain stable training. In the next section, we willintroduce how to constrain the policys randomness to ensure smooth policy changes.",
  "Pessimistic Exploration": "The second failure mode comes from pessimistic exploration due to the double Q-learning mechanism. Toaddress the issue of overestimation in DQN, double Q-learning was proposed. This approach mitigates theproblem by employing two independent Q-networks, and using the minimum value between them as the finalQ-value. The concept was initially introduced by Double DQN (Van Hasselt et al., 2016) in the discretedomain. In the continuous domain, TD3 (Fujimoto et al., 2018) and SAC (Haarnoja et al., 2018a) alsoadopt clipped double Q-learning to mitigate overestimation, making it a favored technique across variousreinforcement learning algorithms. Empirical results demonstrate that the clipped double Q-learning trick can boost SAC performance incontinuous domains, but its impact remains unclear in discrete domains.Therefore, we need to revisitclipped double Q-learning for discrete SAC. In our experiments, in discrete domains, we find that discrete SAC tends to suffer from underestimationbias instead of overestimation bias. This underestimation bias can cause pessimistic exploration, especiallyin the case of sparse reward. Here, we illustrate how the popularly used clipped double Q-learning trickcauses underestimation bias and how the policy used with this trick tends to converge to suboptimal actionsfor discrete action spaces. Our work complements previous work with a more in-depth analysis of clippeddouble Q-learning. We demonstrate the existence of underestimation bias and then illustrate its impact onAtari games.",
  "Vappox(st) = Eat mini=1,2[Qi(st, at) log((at | st))],(13)": "where Qi represents estimation of target-critic networks parameterized by i. The estimated bias for Qican be calculated as i = Qi(s, a) Q(s, a). On the one hand, when 1 > 2 > 0, the clipped doubleQ-learning trick can help mitigate overestimation error due to the min operation. On the other hand, when1 < 2 < 0 or 1 < 0 < 2, the clipped double Q-learning trick will lead to underestimation (i.e., Vappox < V )and consequently result in pessimistic exploration (Pan et al., 2020; Ciosek et al., 2019). Does this theoretical underestimate occur in practice for discrete SAC and hurt the performance?Weanswer this question by showing the influence of the clipped double Q-learning trick for discrete SAC inAtari games, as shown in . Here, we compare the true value to the estimated value. The resultsare averaged over three independent experiments with different random seeds. We find that, in (a),the approximate values are lower than the true value over time, demonstrating the underestimation biasissue. At the same time, we also run experiments for discrete SAC with a single Q (DSAC-S), which usesa single Q-value for bootstrapping instead of clipped double Q-values. As shown in (b), without theclipped double Q-learning trick, the estimated value of DSAC-S is higher than the true value and thus has anoverestimation bias. However, in (c), we discover that even though DSAC-S suffers from overestimationbias, it performs much better than discrete SAC which adopts the clipped double Q-learning mechanism.This indicates that the clipped double Q-learning trick can lead to pessimistic exploration issues and hurtthe agents performance.",
  "Entropy-Penalty": "The drastic change of Q function distribution and entropy affects the optimization of the Q-value. Dueto the mutual coupling of the Q function and policy training in discrete SAC, we optimize policy entropyto alleviate the unstable effect on training caused by a sharp Q function distribution and a rapid dropin entropy. Simply removing the entropy term will injure the exploration ability under the framework ofmaximum entropy RL. An intuitive solution is to introduce an entropy penalty in the objective of policy toavoid entropy chattering. We will introduce how to incorporate the entropy penalty in the learning processfor the discrete SAC algorithm. Recall the objective of policy in discrete SAC as in Eq. 8. For a mini-batch transition data pair (st, at, rr, st+1)sampled from the replay buffer, we add an extra entropy term Hold to the transition tuple which reflectsthe randomness of policy (i.e., (st, at, r, st+1, Hold)), where old denotes the policy used for data sampling.We calculate the entropy penalty by measuring the distance between Hold and H. Formally, the objectiveof the policy is as follows:",
  "EstD([Eatold[ log(old)] Eat[ log()])2,(14)": "where Eatold[ log(old)] represents policy entropy of old, Eat[ log()] represents policy entropyof , and denotes a coefficient for the penalty term and is set to 0.5 in this paper. By constraining thepolicy objective with this penalty term, we increase the stability of the policy learning process. shows the training curves to demonstrate how the entropy penalty mitigates the failure mode of policydrastic change. In (b), the entropy of discrete SAC (the blue curve) drops quickly, and the policy fallsinto a local optimum at the early training stage. Later, the policy stops improving and even suffers fromperformance deterioration, as shown in the blue curves in (c) and (d). On the contrary, our proposed method (i.e., discrete SAC with entropy-penalty) demonstrates better stabilitythan discrete SAC. As shown in (a), entropy penalty effectively constrains the sharpness of Q function,as a result, the policy changes smoothly during training ((b)). Consequently, compared with discreteSAC, the policy in our approach can keep improving during the whole training stage and does not sufferfrom a performance drop at the later training stage (the red curves in (c) and (d)). It is worth noting that, since the instability in training mainly manifests as the existence of policy entropyterm in optimization, imposing constraints in the entropy space is more effective than constraints in thepolicy space. Other common methods, such as the KL penalty, limit the magnitude of policy updates andimpose additional restrictions on policy updates. This is proved in experiments: KL penalty (the yellowcurve) cannot effectively constrain the rise in Q variance ((a)) and the decrease in entropy ((b)).Consequently, the final Q-value and score of the KL penalty are lower than those with the entropy penalty,with a difference of 12% and 23%, respectively.",
  "The entropy-penalty term 1": "2EstD([Eatold[ log(old)] Eat[ log()])2, in conjunction with thetemperature , jointly regulates the exploration of policy. Different from other trust region methods such asKL constraint (Schulman et al., 2015) or clipping surrogate objective (Schulman et al., 2017), our methodpenalizes the change of action entropy between old and new policies to address policy instability duringtraining. By adding regularization in entropy space instead of policy space, our method can mitigate thedrastic changes of policy entropy while maintaining the inherent exploratory ability of discrete SAC (asshown in (b), the policy entropy changes smoothly. It keeps at a relatively high value to encourageexploration).",
  "Double Average Q-learning with Q-clip": "While several approaches(Ciosek et al., 2019; Pan et al., 2020) have been proposed to reduce underestimationbias, they are not straightforward to be applied to discrete SAC due to the use of Gaussian distribution.In this section, we introduce a novel variant of double Q-learning to mitigate the underestimation bias fordiscrete SAC.",
  "y = r + mini=1,2 Qi(s, (s)).(15)": "When neural networks approximate the Q-function, there exists an unavoidable bias in the critics. Sincepolicy is optimized concerning the low bound of double critics, for some states, we will have Q2(s, (s)) >Qtrue > Q1(s, (s)). This is problematic because Q1(s, (s)) will generally underestimate the true value,and this underestimated bias will be further exaggerated during the whole training phase, which results inpessimistic exploration. To address this problem, we propose to mitigate the underestimation bias by replacing the min operatorwith avg operator. This results in taking the average between the two estimates, which we refer to as doubleaverage Q-learning:y = r + avg(Q1(s, (s)), Q2(s, (s))).(16)",
  "L(i) = max(Qi y)2, (Qi + clip(Qi Qi, c, c)) y)2,(17)": "where Qi represents the critic networks estimate, Qi represents estimation of target-critic networks,and c is the hyperparameter denoting the clip range. This clipping operator prevents the Q-network fromperforming an incentive update beyond the clip range. In this way, the Q-learning process is more robustto the abrupt change in data distribution. Combining the clipping mechanism (Eq. 17) with double averageQ-learning (Eq. 16), we refer to our proposed approach as double average Q-learning with Q-clip. demonstrates the effectiveness of our approach. We compare the discrete SAC and various ensemble Q-estimation methods, including Randomized Ensembled Double Q-learning (REDQ) Chen et al. (2021b) andRandom Ensemble Mixture (REM) Agarwal et al. (2020), with our proposed method, SD-SAC. In (a),the Q-value estimate of discrete SAC is underestimated than the true value. Therefore, the policy of discreteSAC suffers from pessimistic exploration and results in poor performance (purple curve in (e)). Onthe contrary, in (d), with double average Q-learning and Q-clip, the Q-value estimate eliminatesunderestimation bias and improves quickly at the early training stage. The improvement of Q-value carriesover to the performance of policy.Consequently, our approach outperforms baseline discrete SAC by alarge margin ((e)). The result also demonstrates that even though REDQ has less estimation bias in(b), it still suffers from underestimation bias, leading to suboptimal performance due to pessimisticexploration. Although REM addresses the underestimation issue in (c), the overestimation bias of REMsignificantly exceeds that of our proposed method, resulting in a rapid decline in performance at 8 millionsteps. In (d), we also notice that the Q-value overestimates the true value during the early trainingstage but finally converges to the true value after the training process. This encourages early exploration,which is consistent with the principle of optimism in the face of uncertainty (Kearns & Singh, 2002).",
  "Experimental Setup": "To evaluate our algorithm, we compare our SD-SAC with most related baselines, i.e., discrete SAC(Christodoulou, 2019), TES-SAC (Xu et al., 2021), Soft-DQN (Vieillard et al., 2020) and Rainbow(Hesselet al., 2018) which is widely accepted algorithm in the discrete domain. We measure their performancein 20 Atari games chosen as the same as (Christodoulou, 2019) for a fair comparison. We evaluate for 10episodes for every 50000 steps during training, and execute 3 random seeds for each algorithm for 10 millionenvironment steps (or 40 million frames). For the baseline implementation of discrete-SAC, we use Tian-shou 1. We find that Tianshous implementation performs better than the original paper by Christodoulou(Christodoulou, 2019), thus we use the default hyperparameters in Tianshou on all 20 games. We start the game with up to 30 no-op actions, similar to (Mnih et al., 2013), to provide the agent with arandom starting position. To obtain summary statistics across games, following Hasselt (Van Hasselt et al.,2016), we normalize the score for each game as follows: Score normalized =Score agent Score randomScore human Score random .",
  "Seaquest": "algo discrete SAC + penalty + avg-q (SD-SAC)discrete SACdiscrete SAC + avg-qdiscrete SAC + penalty : Scores of variant discrete SAC, which includes discrete SAC, discrete SAC with entropy-penalty,discrete SAC with double average Q learning with Q-clip,for Atari games Assault, Asterix, Enduro, Freeway,Kangaroo and Seaquest. reported results, we adopt the normalized scores of discrete SAC and TES-SAC reported in the correspondingpublication (Xu et al., 2021).When comparing our method to the discrete SAC and TES-SAC, meannormalized scores increase by 38% and 35.5%, respectively. And our method improves the median normalizedscores by 10.7% and 9.0% while compared with discrete SAC and TES-SAC. To verify the effect of a longer training process, table 1 also compares discrete SAC, Rainbow, Soft-DQN,and our method performance on 10 million steps. Compared with discrete SAC, our method has improvedthe normalized scores by 68.6% and 23.3% on mean and median, respectively. Additionally, our proposedmethod outperformed Rainbow by 32.6% on the mean and by 34.9% on the median. Better Q-estimationand steady policy updates are responsible for the performance increase in average scores. The experimentalresults demonstrate that benefiting from the deterministic greedy policy and entropy regularization in theevaluation step, Soft-DQNs performance improves rapidly in the early stages and achieves the best resultsat 1 million steps. However, due to the early convergence of the deterministic greedy policy, Soft-DQNsperformance stagnates after 4 million steps, as seen in . Our method outperforms Soft-DQN in thefinal 10 million steps by 20.8% on average and 6.4% on median, due to the training stability brought byentropy penalty and the optimistic exploration altered by the double avg-Q with Q-clip.",
  "Ablation Study": "shows the learning curves for 6 environments. Entropy-penalty (purple curve) increases performancecompared to the discrete SAC in each of the six environments and even increases 2x scores in Assault. Thisshows that discrete SAC can perform excellently after removing unstable training. Except for Asterix, thealternative choice of clipped double Q-learning, which is double average Q learning with Q-clip (yellow curve),also shows some improvement compared to the discrete SAC in 5 environments. Additional improvementscan be derived when the combination of both alternative design choices is used simultaneously. To evaluate the influence of hyperparameter tuning, we also conducted a comprehensive hyperparameteranalysis. By experimenting with different and learning rate in the discrete SAC algorithm, we identifythe performance upper bound of discrete SAC. The results show that, under various values, SD-SACconsistently outperforms this upper bound, demonstrating that entropy penalty serves as a better and more",
  ": The loss surfaces of discrete SAC and our method on Atari game Seaquest with trained weightsat 3 million, 5 million and 10 million steps": "shows loss surfaces of the discrete SAC and our method by using the visualization method proposed in(Li et al., 2018; Ota et al., 2021) with the loss of TD error of Q functions. According to the sharpness/flatnessin these two sub-figures, our method has a nearly convex surface, while discrete SAC has a more complexloss surface. The surface of our method has fewer saddle points than the discrete SAC, which further showsthat it can be more smoothly optimized during the training process.",
  "Conclusions and Future Work": "Many algorithmic design choices in reinforcement learning are limited to the regime of the chosen benchmarktasks. We highlight that soft actor-critic (SAC), that widely accepted design choices in continuous actionspace do not necessarily generalize to new discrete environments. We conduct failure mode analysis andobtain two main insights: 1) due to the deceptive reward, the unstable coupling update of policy and Qfunction will further disturb training; 2) the underestimation bias caused by double Q-learning results inthe agents pessimistic exploration and inefficient sample usage. We thereby propose two alternative designchoices for SAC: entropy-penalty and double-average Q-learning with Q-clip, resulting in a new algorithm,called SD-SAC. Experiments show that our alternative design choices increase the training stability andQ-value estimation accuracy, which ultimately improves overall performance. In addition, we also apply ourmethod to the large-scale MOBA game Honor of Kings 1v1 to show the scalability of our optimizations. Finally, the success obscures certain flaws, one of which is that our improved discrete SAC still performspoorly in instances involving long-term decision-making. One possible reason is that SAC can not accuratelyestimate the future only by rewarding the current frame. In order to accomplish long-term choices with SAC,our next study will concentrate on improving the usage of the incentive signal across the whole episode.",
  "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Ried-miller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018": "Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforce-ment learning. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 104114.PMLR, 2020. URL Chayan Banerjee, Zhiyong Chen, and Nasimul Noman. Improved soft actor-critic: Mixing prioritized off-policy samples with on-policy experiences. IEEE Transactions on Neural Networks and Learning Systems,2022.",
  ", virtual, pp. 2573225745, 2021.URL": "Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcementlearning. In Thirty-second AAAI conference on artificial intelligence, 2018. Zhang-Wei Hong, Tzu-Yun Shann, Shih-Yang Su, Yi-Hsiang Chang, Tsu-Jui Fu, and Chun-Yi Lee. Diversity-driven exploration strategy for deep reinforcement learning. Advances in neural information processingsystems, 31, 2018.",
  "Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape ofneural nets. Advances in neural information processing systems, 31, 2018": "Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, DavidSilver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio andYann LeCun (eds.), 4th International Conference on Learning Representations, ICLR 2016, San Juan,Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, andMartin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, abs/1312.5602, 2013. URL Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. nature, 518(7540):529533, 2015.",
  "workshop, 2019": "Hua Wei, Jingxiao Chen, Xiyang Ji, Hongyang Qin, Minwen Deng, Siqin Li, Liang Wang, Weinan Zhang,Yong Yu, Liu Linc, Lanxiao Huang, Deheng Ye, QIANG FU, and Yang Wei. Honor of kings arena: anenvironment for generalization in competitive reinforcement learning. In Thirty-sixth Conference on NeuralInformation Processing Systems Datasets and Benchmarks Track, 2022. URL Yaosheng Xu, Dailin Hu, Litian Liang, Stephen McAleer, Pieter Abbeel, and Roy Fox.Target entropyannealing for discrete soft actor-critic. Advances in Neural Information Processing Systems workshop,2021. Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia Chen, Zhao Liu, Fuhao Qiu,Hongsheng Yu, et al. Towards playing full moba games with deep reinforcement learning. Advances inNeural Information Processing Systems, 33:621632, 2020a. Deheng Ye, Guibin Chen, Peilin Zhao, Fuhao Qiu, Bo Yuan, Wen Zhang, Sheng Chen, Mingfei Sun, XiaoqianLi, Siqin Li, et al. Supervised learning achieves human-level performance in moba games: A case study ofhonor of kings. IEEE Transactions on Neural Networks and Learning Systems, 2020b. Deheng Ye, Zhao Liu, Mingfei Sun, Bei Shi, Peilin Zhao, Hao Wu, Hongsheng Yu, Shaojie Yang, XipengWu, Qingwei Guo, et al. Mastering complex control in moba games with deep reinforcement learning. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 66726679, 2020c.",
  "A.2Further Comparison of SD-SAC and DSAC": "Beyond the comparison of , we further measure SD-SAC and DSAC in terms of episode length andnumber of steps with rewards in . After 2e6 steps, the algorithm with entropy penalty demonstratessignificant longer episode lengths and more reward steps compared to discrete SAC. This indicates that theentropy penalty helps the agent learn both scoring and avoidance skills, leading to continued performanceimprovement.",
  "We take the Atari games Assault, Jamesbond and MsPacman as examples to further illustrate the manifes-tation and impact of deceptive rewards in game environments. ()": "In the game Assault, a mothership releases different kinds of aliens. They move along the screen, with thebottom-most alien firing various types of weapons. The player controls a cannon that can shoot bulletshorizontally or vertically to attack the aliens and fireballs they shot. Hitting an alien scores points, whilebeing hit or cannon overheating results in a loss of life. In Jamesbond, the player controls a craft that needs to complete various mission to achieve final victory. Inthe first mission, the player must navigate through a desert with craters, acoid overhead satellite scans andhelicopter bombings, and score points by hitting diamonds through fixed-angle shooting.",
  "A.6.2Plots of Training Process": "We present the training process in the three aforementioned environments with deceptive rewards in -16. It can be observed that in each case, deceptive rewards cause a rapid increase in Q variance and adecrease in policy entropy, leading the training process to fall into local optima. 0.00.20.40.60.81.0 Timesteps1e7 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 Averaged value Q Function Variance",
  "A.6.3Comparison of Different Algorithms on Additional Atari Environments": "Here in we provide comparative performance curves of DSAC, DSAC with entropy penalty, andDSAC with KL penalty in three additional Atari game environments: Assault, Jamesbond, and MsPacman.As shown in the results, the entropy penalty consistently offers the best early-stage regulation of entropychanges across all three environments. This regulation helps prevent the agent from falling into local optimumduring the learning process, thereby improving the final score performance.",
  "B.1SAC Training Pattern on MuJoCo": "We only observe the failure modes in discrete SAC. The reason SAC does not exhibit these failure modes incontinuous environments is twofold. First, SAC employs the reparameterization trick, fitting actions with aGaussian distribution, allowing it to adapt to deceptive rewards without sacrificing policy diversity. Second,in continuous environments, actions that deviate slightly from the best response may have minimal impacton the outcome, whereas in discrete settings, different actions can have entirely distinct meanings. Therefore,our analysis primarily focuses on the challenges SAC faces in discrete environments. To validate this point, we test SAC on three tasks of the MuJoCo environment. Results in indicatethat in MuJoCo, the SAC algorithm does not encounter local optimum issues; policy entropy changes areminimal and gradual, while the scores stadily increase. This suggests that SAC does not face the problemsdescribed in the paper when applied to continuous tasks.",
  ": Scores on Seaquest: a) variants entropy-penalty coefficient with 0.1, 0.2, 0.5 and 1. b) variantsQ-clip c with 0.5, 1, 2 and 5": "of policy change is determined by the entropy-penalty coefficient . Intuitively, an excessive penalty term willlead to policy under-optimization. We experiment with different in {0.1, 0.2, 0.5, 1}. We find that = 0.5can effectively limit entropy randomness while improving performance. The Q-clip constrains different rangesof Q value range c, and experiments with different ranges c in {0.5, 1, 2, 5} show that 0.5 is a reasonableconstraint value.",
  "B.2.3Various Learning Rates for Discrete SAC": "We introduce various learning rates for experiments on Asterix using vanilla discrete SAC in . Anexcessively high learning rate leads to early convergence of entropy, while an excessively low learning rateresults in insufficient optimization. The experiments show that the entropy instability issue of discrete SACis not caused by inappropriate learning rate settings. 0.00.20.40.60.81.0 Timesteps1e7 0.5 1.0 1.5 2.0",
  "B.2.5Comparison Across SD-SAC and DSAC": "We first determine discrete SACs best combination of ((a)) and learning rate ((a)). Then,we compare the scores between SD-SAC with different , and DSAC with this combination ( = 0.5; lr= 1e-5) in (b). The result shows that across various values, SD-SAC consistently outperformsDSAC. This indicates that the entropy penalty is a better and more balanced constraint than merely limitingthe extent of policy updates.",
  "B.3Computation Overhead": "We test the computational speed on a machine equipped with an Intel(R) Xeon(R) Platinum 8255C CPU @2.50GHz with 24 cores and a single Tesla T4 GPU. The unit \"it/s\" represents the number of steps interactingwith the environment per second. Detailed data are shown in the below. The results demonstratethat our method has a 10.86% reduction(265.41->236.58) in speed compared to the vanilla discrete SAC,while maintaining the same parameter size.",
  "B.4Cosine Similarity Comparison": "We visualize the changes in cosine similarity between adjacent states before and after incorporating theentropy penalty in the DSAC algorithm in . The results indicate that, following the addition ofthe entropy penalty, state transitions exhibit smaller and more stable changes. This observation furthersubstantiates that the entropy penalty contributes to more stable policy updates, thereby enhancing theoverall performance of the algorithm."
}