{
  "Abstract": "Generative adversarial networks constitute a powerful approach to generative modeling.While generated samples often are indistinguishable from real data, there is no guaranteethat they will follow the true data distribution. For scientific applications in particular,it is essential that the true distribution is well captured by the generated distribution. Inthis work, we propose a method to ensure that the distributions of certain generated datastatistics coincide with the respective distributions of the real data. In order to achieve this,we add a new loss term to the generator loss function, which quantifies the difference betweenthese distributions via suitable f-divergences. Kernel density estimation is employed toobtain representations of the true distributions, and to estimate the corresponding generateddistributions from minibatch values at each iteration. When compared to other methods,our approach has the advantage that the complete shapes of the distributions are takeninto account. We evaluate the method on a synthetic dataset and a real-world dataset anddemonstrate improved performance of our approach.",
  "Introduction": "Generative adversarial networks (GANs) (Goodfellow et al., 2014) comprise a generator and a discriminatornetwork trained adversarially until the generator manages to produce samples realistic enough to fool thediscriminator. Since their conception, GANs have become a popular tool for generative modeling (Hong et al.,2019; Gui et al., 2021). The GAN framework is generally applicable and it is probably best known for itssuccesses in image generation (Reed et al., 2016; Mathieu et al., 2016; Isola et al., 2017; Ledig et al., 2017). Although GANs have proven powerful, challenges such as mode collapse and non-convergence remain (Saxenaand Cao, 2021). It is often the case that the generated samples, while realistic, stem from only a subspaceof the true data distribution, or do not reflect the relative frequencies with which they occur accurately.For scientific applications in particular, such as in cosmology (Rodriguez et al., 2018; Villaescusa-Navarroet al., 2021) or high-energy physics (Paganini et al., 2018; Alanazi et al., 2021), where GANs may serve asdifferentiable surrogate models for expensive but highly accurate numerical simulations, having a good matchbetween the distributions is essential (Kansal et al., 2023). It is this latter aspect that we tackle in this work, by matching properties of the generated distribution withthose of the real data distribution. In particular, we consider statistics of the dataset such as the powerspectrum components, and match their distributions. We incorporate these requirements in the form ofprobabilistic constraints since it is not properties of individual samples that are enforced, but collectivecharacteristics of the dataset. The approach is chiefly aimed at applications in science, where suitable statistics",
  "to be matched can be chosen through domain knowledge. The only requirement on the statistics is that theyneed to be differentiable": "The main ingredients of our approach are the following: we approximate both the distributions of the realdata and the generated data statistics efficiently via kernel density estimation (KDE) (Silverman, 1986). Ineach iteration, the mismatch between true and generated distributions is then calculated through suitablef-divergences and added as an additional term to the generator loss. That way, we end up with a constrainedgenerated distribution. Using f-divergences, as opposed to e.g. low-order moments of the distributions, hasthe advantage that the full shapes of the distributions are taken into account. In the following, we refer toour method as probabilistically constrained GAN (pcGAN).",
  "Related Work": "The field of physics-informed machine learning, where prior knowledge is introduced into the ML model, hasbeen an active area of research in recent years (Karniadakis et al., 2021; Cuomo et al., 2022). In the contextof GANs, two main approaches for including prior knowledge in the model exist. In the first approach, the constrained values can be fed as additional inputs into the discriminator, such thatit can explicitly use constraint fulfillment as a means to distinguish between real and generated data. InStinis et al. (2019), GANs are employed for interpolation and extrapolation of trajectories following knowngoverning equations. The generated trajectories are constrained to fulfill these equations by passing theconstraint residuals as additional inputs to the discriminator; in order to prevent the discriminator frombecoming too strong, some noise is added to the residuals of the real data, which might otherwise be veryclose to zero. When extrapolating, the GAN is applied iteratively from some initial condition; in order totrain stably, it learns to predict the correct trajectory from slightly incorrect positions of the previous step. In Yang et al. (2019), a physically-informed GAN (PI-GAN) is developed to model groundwater flow. Theymake use of the same basic idea as physics-informed neural networks (Raissi et al., 2019) and employ automaticdifferentiation in order to obtain a partial differential equation (PDE) residual on the GAN output, which isin turn fed into the discriminator. By evaluating the GAN prediction at many different points and comparingto an equivalent ensemble of true values of the corresponding physical field, the GAN is constrained to adhereto a stochastic PDE. In the second approach, prior knowledge may be taken into account via additional loss terms in eitherdiscriminator or generator loss: in Khattak et al. (2018; 2019), GANs are employed to simulate detectorsignals for high-energy physics particle showers. Here, physical constraints such as the particle energy aretaken into account via additional generator loss terms. In Yang et al. (2021), the incorporation of imprecise deterministic constraints into the GAN is investigated;e.g. the case where the GAN output is supposed to follow a PDE, but where the PDE parameters are notknown accurately could be formulated as an imprecise constraint. In a first step, deterministic constraintscan be included by adding the constraint residuals as an additional loss term to the generator loss; theyargue that it is better to add such terms to the generator since this strengthens the weaker party in theadversarial game, instead of giving an even larger advantage to the discriminator. In order to make theconstraint imprecise, they do not require that the residuals go to zero, but instead only include residualsabove a certain threshold value 2 in the loss. The work closest in aim to ours is probably that by Wu et al. (2020), where a statistical constrained GAN isintroduced. They add an additional term to the generator loss function in order to constrain the covariancestructure of the generated data to that of the true data. This additional term is a measure of similaritybetween the covariances, and they concluded that the Frobenius norm was the best choice for this purpose.They use their method to obtain better solutions for PDE-governed systems. Similar to Wu et al. (2020), our method also imposes probabilistic constraints via an additional term to thegenerator loss. However, there are significant differences: firstly, our method does not consider the covariancestructure of the dataset in particular, but instead allows to constrain on arbitrary statistics of the data.Secondly, our method uses f-divergences to match the distributions of true and generated data statistics",
  "Background": "The basic idea of generative adversarial networks (GANs) (Goodfellow et al., 2014) is to train a generatorto generate samples of a given distribution and a discriminator (or critic) to distinguish between real andgenerated data. During the training, both networks are pitted against each other in a minimax game withvalue function",
  "minGmaxDV (D, G) = Expdata(x) [log D(x)] + Ezpz(z) [log(1 D(G(z)))] .(1)": "Here, D denotes the discriminator, G the generator, x samples drawn from the real data and z randomlygenerated latent space vectors serving as input to the generator; pdata and pz denote the real data distributionand the latent vector distribution, respectively. Discriminator and generator are then trained alternatingly(with m 1 discriminator updates between each generator update); in (Goodfellow et al., 2014), it is shownthat a stable equilibrium to the minimax problem (1) exists and that the optimal solution lies in the generatorproducing samples from the true data distribution. The standard GAN can be very difficult to train and often suffers from mode collapse. In Arjovsky et al.(2017), the Wasserstein GAN (WGAN) was introduced, where they suggest the earth-mover (EM) distanceas a new loss for the GAN. They show that the discriminator and generator losses can then be expressed as",
  "LG = D(xgen),(2b)": "under the condition that the discriminator is Lipschitz continuous. Rather crudely, this is enforced byclipping the weights of the discriminator. In the end, the terms in (2) are approximated as expectations overminibatches. With this loss function, the discriminator can be interpreted as a critic that assigns scores to both true andgenerated samples. These scores are not constrained to any specific range and can therefore give meaningfulfeedback to the generator also when the discriminator is outperforming. Advantages of the WGAN includeimproved learning stability as well as meaningful loss curves (Gui et al., 2021). In this work, we also consider two other common variants of the GAN: firstly, the WGAN with gradientpenalty (WGAN-GP) (Gulrajani et al., 2017), where the aforementioned weight clipping is avoided byinstead imposing a penalty on the discriminator that is supposed to enforce Lipschitz continuity. Secondly,the spectrally normalized GAN (SNGAN) (Miyato et al., 2018), where Lipschitz continuity is ensured byconstraining the spectral norm of each layer of the discriminator explicitely.",
  "Published in Transactions on Machine Learning Research (09/2024)": "0.5 0.0 0.5 1.0 real 0.5 0.0 0.5 1.0 WGAN + pc (pcGAN) 1.0 0.5 0.0 0.5 Wu et al. + pc 0.5 0.0 0.5 1.0 WGAN-GP + pc 0.5 0.0 0.5 1.0 SNGAN + pc 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5",
  "Quantifying the Mismatch": "Let p, q be arbitrary probability density functions (PDFs). For f-divergences h, it holds that h(p, q) 0,with equality if and only if p = q. These properties justify the use of f-divergences for the function h in (3).A major advantage of using f-divergences, as opposed to e.g. the Wasserstein distance, is that they areefficient to calculate.",
  "q(x)dx.(4)": "The KL divergence is asymmetric and we consider the forward KL, also known as zero-avoiding, in order toensure a complete overlap of areas with non-zero probability of the distributions; in case of the reverse, orzero-forcing, KL, the loss term would typically tend to match q to one of the peaks of p and hence fail tomatch the distributions in a way suitable for our purposes.",
  "Obtaining Representations": "In order to evaluate the loss terms in (3), means of extracting representations for both the true and generatedPDFs are required. We denote these representations as ptrue and pgen. Note that ptrue will need to bedetermined only once, in advance of the GAN training, since it remains constant. In contrast to the true",
  "where N denotes the number of datapoints, K thekernel function and s the bandwidth of the ker-nel. The choice s =1": "200(maxj(zsj)minj(zsj)) hasproven to give accurate representations for the truedistributions (compare e.g. the leftmost plots in Figs.1, 2, and 5), as we typically have N 1000 andcan afford to choose such a small value. Throughoutthe paper, we use Gaussian kernels with K(x) =(2)1/2ex2/2. We evaluate different kernel choicesin Appendix B.1. We also approximate the generated distributions ateach iteration using KDE, using the constraint val-ues as obtained from the current minibatch samples.That is, we obtain the approximate generated PDFsas",
  "where nbatch denotes the batch size": "For pgen(zs), choosing s adequately is crucial andrequires more thought than in the case of ptrue. Thisis due to the fact that there are much fewer samplesavailable in the minibatches. The bandwidths sare chosen separately for each constraint zs, underthe criterion that pgen as obtained from minibatchesdrawn from the true data should have a mismatchas small as possible with ptrue. Since the optimal values of s would be expected to depend both on the rangeof value zs in the true dataset and the batch size, we parameterized them as",
  "A detailed description of how to determine the optimal values for f s is given in Appendix A": "In order to improve the accuracy of pgen (assuming that pgen does not change drastically between subsequentiterations), we can include information from the preceding minibatches via an exponentially decaying historicalaverage:pigen(zs) = (1 )pgen(zs) + pi1gen (zs),(10) where the parameter defines the strength of the decay and i denotes the current iteration. In this way,the potentially strong fluctuations between minibatches are smoothened out, allowing for a more accuraterepresentation of pgen. With representation (10) for the generated distribution and (7) for the true distribution, the one-dimensionalintegrals required for evaluating h(ptrue, pgen) in (3) can be carried out numerically. In , the variousrepresentations are illustrated.",
  "s s . The first term in the": "definition of quantifies the constraint fulfillment relative to the best-fulfilled one and the second termprevents already well-matched constraints from ceasing to be included. The global weighting factor needsto be tuned separately. A high-level overview of the method is given in Algorithm 1 and the pcGAN training is detailed in Algorithm 2.Note that, while our training algorithm is based on the WGAN, the modified generator loss (3) is moregeneral and can be used for other types of GANs as well.",
  "Results 1": "In this section, we present the results obtained with our model. We introduce a set of evaluation metrics andconsider a synthetic example and a real-world dataset from physics. The evaluation metrics are chosen tocover different aspects of the generated distribution and evaluate the GAN performance both in the samplespace and in a lower-dimensional space of high-level features, as is common practice (Kansal et al., 2023). Wecompare the pcGAN to the unconstrained WGAN, WGAN-GP, SNGAN, and the statistical constrained GANfrom Wu et al. (2020). We investigate the impact that training parameters have on the model performanceand we combine the probabilistic constraint with the different GAN variants to evaluate its potential forimproving their performance. More results are given in the appendices. In Appendix B.1, we investigate the impact that the choice of kernelfor the KDE has on the model performance. In Appendix B.2, we evaluate how well the MMD loss wouldperform instead of the f-divergences for matching the constraints. A discussion on the training time requiredfor the different models is given in B.4. Additional information on the datasets, the training parameters, andthe high-level features is given in Appendix C.",
  "P + R.(12)": "In the context of generative modeling, the precision is the fraction of generated samples that lie in the realdata manifold and the recall gives the fraction of real data samples that lie in the generated data manifoldSajjadi et al. (2018). They are calculated as suggested in Kynknniemi et al. (2019), with choice k = 10 forthe k-nearest neighbor.",
  "SNGAN": "0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0 0.5 0.0 0.5 1.0",
  "f=1V (phisttrue(xf) phistgen (xf)).(14)": "To get an idea of the complexity of each metric, it helps to consider them in the following way: the F1 scoretakes the full shape of the data distribution into account, dF the first two moments, and Vc and Vf themarginal distributions of the constrained statistics and the chosen set of high-level features, respectively.",
  "For our first experiment, we consider a superposition of sine waves. Each wave consists of two sine waves,x = 1": "22i=1 sin(it), with angular frequencies sampled randomly from i |N(1, 1)|, and we generate 200equally-spaced measurements in the interval t . In total, we create 100 000 samples of size 200 toserve as training data. We perform the Fourier transform for real-valued inputs for each time series in thedataset and we use the square roots of the power spectrum components (i.e. the absolute values of the Fouriercoefficients) as the statistics to constrain when training the GAN; that is, we have 101 separate constraints(compare Appendix C.1). In , results for the different GAN variants are depicted. The data generated by the pcGAN matchesthe true distributions very well. The method of Wu et al. (2020) comes in second, managing to cover thecorrect range of constraint values, but failing to adhere to the precise shapes of the PDFs. The unconstrained",
  "d2F": "100 ()21.523.9718.423.0318.593.4723.262.9319.522.1217.274.1318.165.6317.633.47F1 ()0.140.050.110.040.110.030.100.070.130.040.170.060.180.040.130.04Vc ()0.560.050.710.090.760.100.960.060.500.030.900.121.050.150.770.05Vf ()1.200.111.130.061.070.071.010.061.080.061.100.081.100.131.070.05 : (Synthetic example) The pcGAN with MMD loss is evaluated for different weighting factors and kernel widths 0 via different performance metrics, defined in .1: the Frchet distance dF ,the F1 score, the agreement of the constraint distributions Vc, and the agreement of the distributions of aselection of high-level features Vf. The arrows indicate whether high or low values are better. Ten runs havebeen conducted per model, and the mean values plus-or-minus one standard deviation are given. Bold fonthighlights best performance. this appendix, we do not consider MMD networks but explore instead the effectiveness of using MMD as theloss function for matching the high-level statistics. That is, we use the MMD loss instead of f-divergences(compare .1) in (3).",
  "WGAN, WGAN-GP and SNGAN are distinctly worse and tend to assign too much weight to the highestpeak of the distribution": "In , we evaluate the performance of the different models in terms of the evaluation metrics definedin .1. The results for the constraint distributions are well reflected here and the unconstrainedversions of the GAN tend to perform worse than both the pcGAN and the method of Wu et al. (2020) onmetrics other than the F1 score. When considering the F1 score, pcGAN is ahead of the unconstrainedmodels. Between the pcGAN and Wu et al. (2020), pcGAN outperforms Wu et al. (2020) in all metrics apartfrom dF , where the method of Wu et al. (2020) is slightly better. This makes sense since dF only evaluatesagreement of the first and second-order moments of the distribution; the latter are precisely what the methodof Wu et al. (2020) constrains. In addition to the pcGAN, results for combinations of the other GAN variants with the probabilistic constraintare also given in the table. It is apparent that adding the constraint also leads to improved performance ofthe other models. In Appendix B.3, a plot visualizing the constraint fulfillment equivalent to is givenfor the different constrained GANs. In , the impact of the global weighting factor is investigated. A clear improvement with increasing is visible for most of the metrics, up to 100. Overall, the value = 500 appears to be a good choice. Thefact that dF and F1 also improve indicates that the constraints help to produce a better diversity of samples. In , we consider the impact that the batch size and the historical averaging have on the results. BothdF and constraint fulfillment improve with increasing batch size, although we observe diminishing returns forbatch sizes larger than 256. The inclusion of historical averaging improves dF , with higher values of yieldinglarger improvements, whereas constraint fulfillment Vc is only weakly affected by the choice of and the",
  "IceCube-Gen2 Radio Signals": "The IceCube neutrino observatory (Aartsen et al., 2017) and its planned successor IceCube-Gen2 (Aartsenet al., 2021) are located at the South Pole and make use of the huge ice masses present there in order todetect astrophysical high-energy neutrinos. Deep learning methodology has already been employed to extractinformation such as shower energy or neutrino direction from radio-detector signals (Glaser et al., 2023;Holmberg, 2022). Holmberg (2022) also investigated the use of GANs to simulate detector signals. We aregoing to consider the filtered Askaryan radio signals from Holmberg (2022), which were generated using",
  "WGANWGAN + pc(pcGAN)Wu et al.Wu et al.+ pcWGAN-GPWGAN-GP+ pcSNGANSNGAN+ pc": "d2F ()25.3412.8516.719.5215.828.1514.216.205.366.096.884.809.027.077.093.17F1 ()0.350.170.380.120.360.080.360.060.480.080.450.060.460.040.460.06Vc ()0.970.110.300.060.990.120.300.050.710.120.160.041.010.060.170.04Vf ()0.760.090.700.080.750.050.680.070.660.060.650.070.650.050.630.04 : (IceCube-Gen2) The different GAN variants and their combinations with the probabilisticconstraint are evaluated via different performance metrics, defined in .1: the Frchet distance dF ,the F1 score, the agreement of the constraint distributions Vc, and the agreement of the distributions of aselection of high-level features Vf. The arrows indicate whether high or low values are better. Ten runs havebeen conducted per model, and the mean values plus-or-minus one standard deviation are given. Bold fonthighlights best performance. Parameters: bs = 256, = 2, = 0.9, h = KL. the NuRadioMC code (Glaser et al., 2020) according to the ARZ algorithm (Alvarez-Muiz et al., 2010).These signals take the form of 1D waveforms and in our experiments we want to focus solely on the shapeof these waves, not their absolute amplitudes; this is achieved by normalizing each signal to its maximumabsolute value. We use the pcGAN to constrain the generated data on the distributions of the minimum andmaximum values of the signals. The results are depicted in . The pcGAN matches the characteristics of both minimum and maximumdistribution well. In particular, it manages to match the spikes at -1 and 1 more accurately than any of theother models. The distributions as obtained via the other models also exhibit the two peaks in each of thedistributions but do not reproduce their precise shapes correctly. Out of the remaining models, WGAN-GPmatches the distributions best, with only slightly less pronounced spikes at -1 and 1 than the pcGAN. A plotshowing the constraint fulfillment for the different constrained GANs is given in Appendix B.3. In , the evaluation metrics are given for the different GAN variants together with their constrainedversions. The constraints are matched well for all of the constrained models. In terms of the remainingmetrics, adding the constraint yields improvements for WGAN, Wu et al. and SNGAN. For WGAN-GP, onthe other hand, a slight decrease in performance can be observed. While WGAN-GP performs best on d2Fand F1, WGAN-GP + pc is the best choice for overall performance when taking constraint fulfillment intoaccount.",
  "Conclusions and Future Work": "We have presented the probabilistically constrained GAN (pcGAN), a method to incorporate probabilisticconstraints into GANs. The method is expected to be particularly useful for scientific applications, whereit is especially important for generated samples to represent the true distribution accurately and wheresuitable statistics to be matched can be identified through domain knowledge. For a given statistic z, this isachieved by adding the mismatch between the corresponding true and generated distribution as quantified viaa suitable f-divergence to the generator loss. Kernel density estimation is employed to obtain representationsfor the distributions of the statistic. By adequately weighting the different loss terms, a large number ofstatistics can be matched simultaneously. We have evaluated our method using two different datasets. Our experiments clearly demonstrate that theprobabilistic constraint is effective at matching the chosen dataset statistics. In terms of the evaluationmetrics, the pcGAN constitutes a significant improvement over the standard WGAN. Depending on thedataset under consideration, it can also outperform WGAN-GP, SNGAN, and the method of (Wu et al.,2020). Combining the probabilistic constraint with GAN variants other than the WGAN also improves therespective models in most cases. For future work, it would be interesting to extend the method to consider the joint distribution of differentstatistics, in order to also include correlations between them in the constraint. Furthermore, it would beimportant to find a way to make the method compatible with conditional GANs in order to widen the rangeof possible applications. Finding automated ways for obtaining suitable statistics to match, e.g. by usingfeatures of classifier networks, could improve the approach and would allow for its application to situationswhere insufficient domain knowledge is available. In principle, the probabilistic constraint could also be addedto other types of generative models. The main requirements would be that new samples are generated duringeach iteration of the training procedure and that a suitable spot for adding the loss term can be identified.Investigating the applicability of the approach to other generative models, such as autoencoders (Kingmaand Welling, 2014) or denoising diffusion probabilistic models (Ho et al., 2020), therefore constitutes anotherpromising avenue for future research.",
  "Acknowledgements": "The work is financially supported by the Swedish Research Council (VR) via the project Physics-informedmachine learning (registration number: 2021-04321). We thank the anonymous reviewers from TMLR fortheir constructive feedback, and Jens Sjlund for comments on an early manuscript. Our special thanks alsogo to Christian Glaser, for interesting discussions and for providing access to the IceCube data generator. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27.Curran Associates, Inc., 2014.",
  "Yongjun Hong, Uiwon Hwang, Jaeyoon Yoo, and Sungroh Yoon. How generative adversarial networks andtheir variants work: An overview. ACM Computing Surveys (CSUR), 52(1):143, 2019": "Jie Gui, Zhenan Sun, Yonggang Wen, Dacheng Tao, and Jieping Ye. A review on generative adversarialnetworks: Algorithms, theory, and applications. IEEE transactions on knowledge and data engineering,2021. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generativeadversarial text to image synthesis. In International conference on machine learning, pages 10601069.PMLR, 2016.",
  "Divya Saxena and Jiannong Cao. Generative adversarial networks (GANs) challenges, solutions, and futuredirections. ACM Computing Surveys (CSUR), 54(3):142, 2021": "Andres C Rodriguez, Tomasz Kacprzak, Aurelien Lucchi, Adam Amara, Raphal Sgier, Janis Fluri, ThomasHofmann, and Alexandre Rfrgier. Fast cosmic web simulations with generative adversarial networks.Computational Astrophysics and Cosmology, 5(1):111, 2018. Francisco Villaescusa-Navarro, Daniel Angls-Alczar, Shy Genel, David N Spergel, Rachel S Somerville,Romeel Dave, Annalisa Pillepich, Lars Hernquist, Dylan Nelson, Paul Torrey, et al. The CAMELS project:Cosmology and astrophysics with machine-learning simulations. The Astrophysical Journal, 915(1):71,2021. Michela Paganini, Luke de Oliveira, and Benjamin Nachman. Accelerating science with generative adversarialnetworks: an application to 3d particle showers in multilayer calorimeters. Physical review letters, 120(4):042003, 2018. Yasir Alanazi, Nobuo Sato, Tianbo Liu, Wally Melnitchouk, Pawel Ambrozewicz, Florian Hauenstein,Michelle P. Kuchera, Evan Pritchard, Michael Robertson, Ryan Strauss, Luisa Velasco, and YaohangLi. Simulation of electron-proton scattering events by a feature-augmented and transformed generativeadversarial network (FAT-GAN). In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International JointConference on Artificial Intelligence, IJCAI-21, pages 21262132, 2021. Raghav Kansal, Anni Li, Javier Duarte, Nadezda Chernyavskaya, Maurizio Pierini, Breno Orzari, and ThiagoTomei. Evaluating generative models in high energy physics. Physical Review D, 107(7):076017, 2023.",
  "George Em Karniadakis, Ioannis G Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang. Physics-informed machine learning. Nature Reviews Physics, 3(6):422440, 2021": "Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo, Gianluigi Rozza, Maziar Raissi, and FrancescoPiccialli. Scientific machine learning through physicsinformed neural networks: where we are and whatsnext. Journal of Scientific Computing, 92(3):88, 2022. Panos Stinis, Tobias Hagge, Alexandre M Tartakovsky, and Enoch Yeung. Enforcing constraints for interpo-lation and extrapolation in generative adversarial networks. Journal of Computational Physics, 397:108844,2019. Liu Yang, Sean Treichler, Thorsten Kurth, Keno Fischer, David Barajas-Solano, Josh Romero, ValentinChuravy, Alexandre Tartakovsky, Michael Houston, Mr Prabhat, et al. Highly-scalable, physics-informedGANs for learning solutions of stochastic PDEs. In 2019 IEEE/ACM Third Workshop on Deep Learningon Supercomputers (DLS), pages 111. IEEE, 2019. Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deeplearning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational physics, 378:686707, 2019. Gul Rukh Khattak, Sofia Vallecorsa, and Federico Carminati. Three dimensional energy parametrizedgenerative adversarial networks for electromagnetic shower simulation. In 2018 25th IEEE InternationalConference on Image Processing (ICIP), pages 39133917, 2018.",
  "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings ofInternational Conference on Learning Representations (ICLR), 2015": "Mark G Aartsen, M Ackermann, J Adams, JA Aguilar, M Ahlers, M Ahrens, D Altmann, K Andeen,T Anderson, I Ansseau, et al. The IceCube neutrino observatory: instrumentation and online systems.Journal of Instrumentation, 12(03):P03012, 2017. Mark G Aartsen, R Abbasi, M Ackermann, J Adams, JA Aguilar, M Ahlers, M Ahrens, C Alispach, P Allison,NM Amin, et al. IceCube-Gen2: the window to the extreme universe. Journal of Physics G: Nuclear andParticle Physics, 48(6):060501, 2021. Christian Glaser, S McAleer, Sigfrid Stjrnholm, P Baldi, and SW Barwick. Deep-learning-based reconstructionof the neutrino direction and energy for in-ice radio detectors. Astroparticle Physics, 145:102781, 2023.",
  "end forif = min (mean (H, dim = 1))f s = af[if]": "We propose a method to determine optimal valuesfor f s that is based on the assumption that datasampled from the true distribution should on aver-age have the best possible match between the truedistribution and the KDE approximation obtainedvia the minibatches. The procedure is summarized in Algorithm 3. Inorder to determine the optimal value of f s for agiven constraint zs in (9), we perform a grid searchover possible values f. Introducing the standarddeviation of zs into the definition of via c helpsto narrow the range in which the optimal valuesf s lie. The grid is defined in the array af. Foreach value of f, we evaluate the mismatch betweenthe true distribution ptrue(zs) (7) and the generateddistribution pgen(zs) (8) via the f-divergence h. The minibatches are sampled from the true data since the aim is to obtain a mismatch as small as possiblefor true data. The obtained values for the mismatch are then averaged over Navg minibatches. Subsequently,the value f corresponding to the minimum mean value is determined; this value is the desired optimal valuef s. Figures 8 and 9 illustrate the grid search.",
  ": Different choices of kernel": "Here, we investigate the impact that the specific choice ofkernel for approximating the distributions in (3) has onthe performance of the pcGAN. We consider the followingkernels (compare ): Gaussian (G), uniform (u),Epanechnikov (epa), triweight (tri), and cosine (cos). Thekernel bandwidths are obtained via (9) and Algorithm 3.The results are depicted in . The choice of kernel does not have a big impact on themodel performance. Overall, the Gaussian kernel seems tobe the best choice as it consistently performs well for all ofthe metrics. A potential reason why the Gaussian kernelis superior can be found in its unbounded support. Thismeans that there will always be some overlap betweenreal and generated distributions, as obtained via KDE,enabling more informative gradients.",
  "B.2Using Maximum Mean Discrepancy to Match Statistics": "The maximum mean discrepancy (MMD) is a kernel-based statistical test that can be employed to determinewhether two distributions are the same (Gretton et al., 2012). It has been used as a loss function to establishgenerative moment matching networks, a distinct class of generative models (Li et al., 2015; Dziugaite et al.,2015). While an adversarial approach has been suggested to improve MMD networks by learning moresuitable kernels (Li et al., 2017), they constitute their own model class and not an extension of the GAN. In",
  "()13.393.0819.733.5715.334.6015.155.7011.691.83F1 ()0.170.060.130.040.200.050.140.030.180.04Vc ()0.200.020.210.020.310.080.250.060.230.03Vf ()0.800.070.890.060.840.040.840.050.820.04": ": (Synthetic example) The pcGAN with different choices of kernel K is evaluated via differentperformance metrics, defined in .1: the Frchet distance dF , the F1 score, the agreement of theconstraint distributions Vc, and the agreement of the distributions of a selection of high-level features Vf.The arrows indicate whether high or low values are better. Ten runs have been conducted per model, andthe mean values plus-or-minus one standard deviation are given. Bold font highlights best performance.Parameters: bs = 256, = 500, = 0.9, h = KL.",
  "where M is the number of generated samples and N the number of real samples in the current minibatches": "One drawback of this approach is the mixed loss term in equation (16); it would be too computationallycostly to take into account the entire dataset at each iteration wherefore we also need to batch the real data.When using f-divergences in loss (3) of our approach, similar problems can be circumvented by evaluatingptrue once on a fixed grid in advance of the training. Here, the same trick does not work, since the statisticsas extracted from the generated data determine the points at which the kernel K needs to be evaluated. The results for the MMD loss are given in . We consider different weighting factors for the lossterm, as well as Gaussian kernels with different bandwidth. The bandwidths of the Gaussian kernels for thedifferent constraints are given by the values s in (9) times the factors 0 given in the figure. When multiplefactors are given, then the sum of the corresponding Gaussian kernels is used. Both in terms of matching theconstraints and in terms of the performance metrics, the method performs better than the standard WGAN,but worse than the pcGAN (compare Table. 1).",
  "B.3Additional plots": "In , plots of the constraint fulfillment for the different GAN variants combined with the probabilisticconstraint are given. It is apparent that all the constrained models reproduce the constraint distributionswell, with WGAN-GP + pc giving the smoothest fit. The distributions obtained with SNGAN + pc are a bitmore jagged than the other ones.",
  "Synthetic (5.2)1.201.951.382.061.912.681.402.07IceCube-Gen2 (5.3)0.370.460.450.460.510.560.600.67": "The runtime required for one run of 100 000 iterations for the various models and for the different datasets isgiven in . The time required to extract the representations ptrue for all of the constrained statisticscombined is negligible in comparison: for the synthetic dataset, it takes 66 seconds, and for the IceCube-Gen2dataset 0.05 seconds. These numbers have been obtained on a system with NVIDIA RTX 3060 Ti 8GB GPU,Intel Core i7 7700-K @ 4.2GHz CPU, and 16GB RAM. Overall, adding the probabilistic constraint increasesthe runtimes of the corresponding base GANs by around 40% 60% in case of the synthetic dataset, where101 statistics are matched. For the IceCube-Gen2 dataset, where only 2 statistics are matched, the increasesin runtime are significantly lower at less than 30%.",
  "C.2Synthetic Example (.2)": "The synthetic dataset consists of 100 000 samples of size 200, generated as described in .2. Forthis example, we employed convolutional networks for both the discriminator and generator; details on thecorresponding network architectures are given in Tables 9 and 10, respectively. In layers where both batchnormalization and an activation function are listed in the column Activation, batch normalization is appliedto the input of the layer whereas the activation function is applied to the output. Padding is employed ineach layer such that the given output sizes are obtained; reflection padding is utilized.",
  "Synthetic (5.2)50100 0002e-40.57000000.9[0, 0.005]IceCube-Gen2 (5.3)50100 0005e-40.54000000.9[0, 0.1]": "In , the search for the best values f s in (8) is illustrated for h = KL. It is apparent, that there isa clear, batch size-dependent minimum of the KL-divergence for each constraint, with larger batch sizestending towards larger values of f s; this is due to the fact that more samples in the minibatch allow for amore fine-grained approximation of the generated distribution. In the top right plot, optimal values of f sare depicted for all components ps[i] of the power spectrum. The spike around i 10 is the result of someoutliers in the values of the power spectrum components; they lead to a high standard deviation of the truedistribution, which in turn requires a large f s in order to obtain small enough standard deviations for theKDE to resolve the narrow peak well. In the bottom row, approximations of the generated distributions as obtained via the minibatches are depicted.It is apparent that the mixtures of Gaussians approximate them reasonably well, with larger batch sizestypically giving better results. In , samples from the true distribution as well as generated samples from the different GANs aredepicted. All of the GANs produce reasonable-looking results, although upon closer inspection it becomesapparent that they do not necessarily constitute a superposition of two sine waves. Only the WGAN seemsto have a tendency to produce rugged curves.",
  "C.3IceCube-Gen2 (.3)": "For this example, we considered 50 000 IceCube-Gen2 radio-detector signals of size 200 (generated usingthe NuRadioMC code (Glaser et al., 2020) according to the ARZ algorithm (Alvarez-Muiz et al., 2010)),normalized to their respective maximum absolute values. The networks employed are a mixture of convolutionaland fully connected networks, which have been based on the architectures used in Holmberg (2022); details ondiscriminator and generator architectures are given in Tables 11 and 12, respectively. For the discriminator,the input to the network is first fed through four convolutional layers in parallel, the outputs of which are",
  "C.4Training Parameters": "Here, we summarize the training parameters used for the different experiments. Nit gives the number oftraining iterations, lr the learning rate, and the weighting factor for the constraints in (3). In the columnclamping, the range is given to which network parameters of the discriminator D were clamped in order toenforce the Lipschitz constraint in WGANs (Arjovsky et al., 2017). The ADAM optimizer (Kingma and Ba,2015) was used for training the networks, with hyperparameter values 1 and 2; a scheduler was employedthat reduced the learning rate by a factor of fsched after itsched iterations. The weighting factor for thestatistical constraint from Wu et al. (2020) was chosen as Wu = 1. The weighting factor for the gradientpenalty in WGAN-GP was chosen as GP = 10. The parameter m, which gives the number of discriminatorupdates per generator update, was chosen as 1."
}