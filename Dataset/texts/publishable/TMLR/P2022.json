{
  "Abstract": "The recursive node fetching and aggregation in message-passing cause inference latency when de-ploying Graph Neural Networks (GNNs) to large-scale graphs. One promising inference accelera-tion direction is to distill GNNs into message-passing-free student Multi-Layer Perceptrons (MLPs).However, the MLP student without graph dependency cannot fully learn the structure knowledgefrom GNNs, which causes inferior performance in heterophilic and online scenarios. To address thisproblem, we first design a simple yet effective Structure-Aware MLP (SA-MLP) as a student model.It utilizes linear layers as encoders and decoders to capture features and graph structures withoutmessage-passing among nodes. Furthermore, we introduce a novel structure-mixing knowledge dis-tillation technique. It generates virtual samples imbued with a hybrid of structure knowledge fromteacher GNNs, thereby enhancing the learning ability of MLPs for structure information. Exten-sive experiments on eight benchmark datasets under both transductive and online settings show thatour SA-MLP can consistently achieve similar or even better results than teacher GNNs while main-taining as fast inference speed as MLPs. Our findings reveal that SA-MLP efficiently assimilatesgraph knowledge through distillation from GNNs in an end-to-end manner, eliminating the needfor complex model architectures and preprocessing of features/structures. Our code is available at",
  "Introduction": "Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Hamilton et al., 2017) have rapidly gained prominence foranalyzing graph datasets in diverse domains such as social networks (Sankar et al., 2021), traffic networks (Wang et al.,2020) and recommendation systems (He et al., 2020; Liu et al., 2021). The primary success of GNNs stems fromthe message-passing mechanism (Gilmer et al., 2017) that extracts graph knowledge by aggregating neighborhoodfeatures via graph structures. This iterative aggregation process help nodes capture long-range neighbor and structureinformation, ultimately generating a more expressive node representation for downstream tasks. However, the number",
  ": An overview of our distillation framework.A structure-aware MLP student learns from GNNs via astructure-mixing KD strategy to achieve substantially faster inference without sacrificing performance": "of neighbors for each node would exponentially increase as the number of layers increases (Zhang et al., 2022; Yanet al., 2020). Therefore, the recursive neighbor fetching induced by message-passing leads to inference latency, makingGNNs hard to deploy for latency-constrained applications, especially for large-scale graphs (Hu et al., 2020). Common inference acceleration methods, such as pruning (Zhou et al., 2021) and quantization (Tailor et al., 2021;Zhao et al., 2020), can speed up GNNs to some extent by reducing the Multiplication-and-ACcumulation (MAC)operations. However, they are limited by the recursive aggregation of GNNs. Knowledge distillation (KD) is an-other generic neural network learning paradigm for deployment that transfers knowledge from high-performance butresource-intensive teacher models to resource-efficient students (Hinton et al., 2015). Motivated by the impressiveresults of MLP-like models in computer vision (Melas-Kyriazi, 2021; Liu et al., 2022), one promising direction isto utilize KD to learn an efficient student MLP with the knowledge from pre-trained teacher GNNs (Zhang et al.,2022; Tian et al., 2023). After mimicking the output (soft labels) of GNNs, student MLPs can be deployed for fasterinference while maintaining performance. However, limited by the model structure and distillation strategy, MLPs are still unable to fully learn graph knowledgefrom GNNs. Specifically, 1) The standard MLPs only process node features (Zhang et al., 2022) or pre-computedDeepWalk embedding (Tian et al., 2023). The performance gain when distilling GNNs into MLPs can be primarilyattributed to the strong memory capabilities (Szegedy et al., 2013) of MLPs, i.e., large MLPs can memorize outputsof GNNs on all observed nodes under transductive scenarios (Zhang et al., 2022). Therefore, the distilled MLP mayfail in heterophilic scenarios, where node labels are highly correlated with structural information (Zhu et al., 2020).Similarly, in online scenarios (Si et al., 2023) where test nodes are not available to be pre-computed and distilledduring training, the MLP may also struggle. 2) Compared to other complex relational or hidden distillation methods,logit-based KD benefits from marginal computational and storage costs (Zhao et al., 2022). Although logit-basedKD is easy to deploy in practical applications, its performance is inferior when distilling GNNs into MLPs (Zhanget al., 2022). The reason lies in the logits limited capacity to convey comprehensive graph knowledge. Hence, thesupervision signal of logit-based KD is too sparse to transfer the graph knowledge from GNNs to MLPs effectively. To address the above problems, as shown in , we first design a simple yet effective Structure-Aware MLP(SA-MLP) student model, which can process structure inputs in an end-to-end and low-latency manner. It utilizes twolinear encoders to explicitly encode node features and local structure information separately. Then, it concatenatesthem for another linear decoder to generate the final prediction. The model consists of only two layers, naturallyincorporating structure information while offering faster inference. Second, we propose a novel structure-mixingknowledge distillation via the mixup (Zhang et al., 2018) technique to enhance the distillation density and improve the",
  "Published in Transactions on Machine Learning Research (08/2024)": "structure-awareness of student MLP. It generates the virtual samples, which mix structures and soft labels of two nodesas additional supervision signals to guide the student MLP learning graph knowledge. This distillation strategy stillonly requires logits of GNNs, bypassing internal details of GNNs, making it lightweight and easy to use. Remarkably,we demonstrate that even without a sophisticated design, the linear layers of the SA-MLP and the logits from GNNsare sufficient for transferring graph knowledge to the student MLPs. We conduct extensive experiments on eight public benchmark datasets under transductive and online scenarios (Siet al., 2023). The results demonstrate that SA-MLP surpasses other MLP-like students and attains comparable oreven superior performance to teacher GNNs across all scenarios while offering significantly faster inference. Ourcontributions are summarized as follows:",
  "Notation and Problem Setting": "Consider a graph G = (V, E), with N nodes and E edges. Let A RNN be the adjacency matrix, with Ai,j = 1 ifedge(i, j) E, and 0 otherwise. Let D RNN be the diagonal degree matrix. Each node vi is given a d-dimensionalfeature representation xi and a c-dimensional one-hot class label yi. The feature inputs are then formed by X RNd,and the labels are represented by Y RNc. The labeled and unlabeled node sets are denoted as VL and VU, and wehave V = VL VU. The task of node classification is to predict the labels Y by exploiting the nodes features X andthe graph structure A.",
  "h(k)i= fh(k1)i,h(k1)j: j N(vi),(1)": "where representation hi is updated iteratively in each layer by collecting messages from its neighbors denoted asN(vi). The graph convolution operator f is usually implemented as a weighted sum of node representations accordingto the adjacent matrix A as in GCN Kipf & Welling (2017) and GraphSAGE Hamilton et al. (2017) or the attentionmechanism in GAT Velickovic et al. (2018). However, this recursive expansion and aggregation of neighbors causeinference latency, because the number of neighbors fetching will exponentially increase with the increasing number oflayers Zhang et al. (2022); Yan et al. (2020).",
  "vVKL(ysv, ytv),(3)": "where is a hyper-parameter controlling the strength of KD. The goal of our paper is to utilize soft labels of GNNsto learn MLP-like students that can incorporate both features and structure information. Such that the learned MLPcan deploy to both transductive and online scenarios, with a much lower computational cost while achieving similaror even better performance compared with GNNs.",
  "GNNs and Inference Acceleration": "Most GNNs follow the message-passing mechanism Gilmer et al. (2017). For example, GCN Kipf & Welling (2017)aggregates local neighbor information according to the Laplacian matrix, GAT Velickovic et al. (2018) employs atten-tion in the aggregation, GraphSAGE Hamilton et al. (2017) introduces learnable aggregator functions to incorporatethe local neighborhood, and GCNII Chen et al. (2020) introduces residual and initial connections. However, all ofthese methods suffer from inference latency induced by recursive aggregation. Some existing work focuses on speed-ing up GNN inference from the model compression perspective by pruning GNN parameters Zhou et al. (2021) andquantizing with low-precision integer arithmetic Zhao et al. (2020), such as Binarized DGCNN Bahri et al. (2021) andDegree-quant Tailor et al. (2021). Although these approaches can reduce model parameters and MAC operations, theyare still limited by the neighbor-fetching latency. By using contrastive learning to train an MLP, Graph-MLP also at-tempts to avoid neighbor fetching Hu et al. (2021), but it only considers transductive rather than more practical onlinesettings. There are also some works for neighbor sampling Zou et al. (2019); Chen et al. (2018), normalization Kose& Shen (2023), and sparsification Kose & Shen (2023) to speed up GNN training, which are complementary to ourgoal of inference acceleration.",
  "Knowledge Distillation for GNNs": "Existing GNN KD models try to distill large GNNs into smaller GNNs Zheng et al. (2022); Joshi et al. (2021; 2022).For instance, LSP Yang et al. (2020) and TinyGNN Yan et al. (2020) conduct KD while preserving local information,whereas GFKD Deng & Zhang (2021) and DFAD Zhuang et al. (2022) achieve graph-level KD via graph generationand adversarial training. Moreover, CPF Yang et al. (2021) utilizes KD to learn a label propagation student and enjoyprior knowledge. However, these message-passing-based methods still require information-fetching, resulting in infer-ence latency Zhang et al. (2022). To eliminate message-passing, GLNN Zhang et al. (2022) is proposed that a teacherGNN teaches pure MLP student graph knowledge via KD. However, the MLP student may fail when structure infor-mation is essential to classification. It implies that the MLP does not fully learn structure information. NOSMOG Tianet al. (2023) incorporates positional features from DeepWalk Perozzi et al. (2014) into an MLP and utilizes adversariallearning with relational distillation to transfer graph knowledge from the hidden embeddings of GNNs. An alterna-tive method involves using VQ-VAE and GNNs hidden information for pretraining a structure codebook Yang et al.(2024); Luo et al. (2024). However, these approaches require knowledge of the GNNs architecture and hidden infor-mation for distillation. Moreover, the pre-computation of DeepWalk embeddings or pretraining codebooks on largegraphs can be costly. In contrast, our SA-MLP model doesnt require any pre-computation. It can learn structuralknowledge end-to-end using a structure-mixing distillation strategy, which only requires soft labels of GNNs.",
  "Sample": ": Illustration of SA-MLP, which encodes the feature and structure information with a decouple encoder andutilizes a linear decoder to generate final predictions. Moreover, to improve the learning ability of structure knowledgefrom teacher GNNs, we utilize structure-mixup to generate a virtual sample v1 that contains hybrid structure knowl-edge to enhance the distillation density. structure inputs for faster inference failed to thoroughly learn structure knowledge from teacher GNNs. To effectivelyexploit features and structure information to generate the final prediction, as shown in , we propose the simpleyet effective Structure-Aware MLP (SA-MLP) model with the corresponding decouple (feature/structure) encoder anda decoder. For inference efficiency, all these modules are implemented with one linear layer.",
  "Decouple Encoder": "For the feature encoder, same as GNNs, we simply utilize a linear layer to transform input features X into featureembedding that contains self-information. However, unlike GNNs, to design an efficient structure encoder, the keyquestion is how to capture the structure information in A without interaction among nodes. Inspired by a naturallanguage processing model FastText Joulin et al. (2016) which efficiently represents the sentence (line-structured ofsequential words) as the summation of a bag of word embeddings, we treat each nodes local structure, i.e., Ai,:, asa bag of context nodes for vi to extract structure information. This idea can naturally be implemented by consideringcolumns of A as features and feeding them into a linear layer to capture structure information. Therefore, we encodefeature and structure information individually to get feature embedding HX Rnd1 and structure embedding HA Rnd2 by",
  "HA = f(A) = AWA + DA,(5)": "where mapping WX Rdd1, bias bX R1d1, and mapping WA RNd2 are learnable parameters. Note that, incontrast to single-layer GNNs, the mapping WA for A serves as a learnable positional embedding table for each node,akin to a word embedding table in natural language processing. This approach offers better generalization capabilitiescompared to the pre-computed DeepWalk positional embeddings utilized in NOSMOG Perozzi et al. (2014), Tianet al. (2023). Moreover, we inject node centrality in the bias DA RNd2, which measures how important a nodeis in the graph and is usually a strong signal for graph understanding Ying et al. (2021). Specifically, the degree biasDAi,: of vi is learnable embedding vector specified by its degree.",
  "Ys = g([HX||HA]),(6)": "where indicates a ReLU activation function. Here, we also tested other fusion mechanisms and increased the numberof layers for g but did not observe noticeable improvement, see . The reason a linear decoder suffices mightbe that the decoupled encoder has already learned rich graph knowledge through distillation, hence a simple decoderexhibits good generalization capabilities.",
  "Scalability and Complexity": "Compared to GNNs, SA-MLP discards the recursive aggregation and simplifies the handling of structural informationby treating the columns of A as input features for f. This approach allows SA-MLP to maintain the speed ofstandard MLPs, benefiting from the sparsity of real-world adjacency matrices A and the efficient sparse-dense matrixmultiplication with WA. Moreover, although N may be large, such WA is similar to the widely used user/itemembedding in large-scale industrial recommendation systems He et al. (2017); Barkan & Koenigstein (2016), whichis scalable due to the embedding infrastructure. When the system is dynamically increasing in the online scenarios, itpossesses scalability by collecting new nodes/edges and appends node ID embedding into WA for periodic retraining. The time complexity of SA-MLP in each full-batch forward pass is O(dE + Nd2K), in which d is the hidden dimen-sion, N is the number of nodes, E is the number of edges, and K is the number of layers. The cost is O(dE) for thefirst linear mapping of A and O(d2) for each MLP. In fact, the operations of mapping A can be easily implementedin the sparse matrix form, which results in high time efficiency. While most GNNs have to propagate features usingthe adjacency in each layer, their complexity is usually O(dKE + Nd2K), and the O(dKE) term makes it difficultto deploy to large-scale graphs Yan et al. (2020). In terms of memory complexity, SA-MLP requires additional storage for the structural embeddings WA, leading to amemory footprint of O(Nd). However, during the stages of model training and inference, WA is conveniently storedas an embedding table within parameter servers Li et al. (2013), thus only the pertinent parameters required for eachindividual batch are retrieved. According to the structure of each batchs lines within the adjacency matrix A, themodel extracts the structural embeddings of D neighbors for B nodes from WA, giving rise to a memory complexityof O(BDd). This efficient design ensures manageable memory usage even under vast scales.",
  "Structure-Mixing Knowledge Distillation": "Although SA-MLP can efficiently encode features and structure information, discarding the message-passing amongnodes still causes suboptimal performance. To allow SA-MLP to enjoy both the efficiency of MLP and the accuracyof GNNs, we conduct cross-model KD from GNNs to SA-MLP. Since the outputs of GNNs are considered to includestructure information Yang et al. (2021); Yan et al. (2020), the logit-based KD Hinton et al. (2015) has been utilizedto extract graph knowledge from GNNs. Inspired by the mixup data augmentation strategy in computer vision Zhang et al. (2018), and without relying onsophisticated KD designs and additional information about GNNs, we propose a structure-mixup variant to enhancethe density of graph knowledge distillation. Mixup generates virtual samples via a linear combination of paired inputsX and labels Y. It can enhance the generalization of models by increasing the density of the vicinal data distributionfor training Zhang et al. (2018). Our proposed structure-mixup simultaneously mixes features X, structure A, andteachers output Yt to generate virtual distillation samples as follows.",
  "(7)": "where the hyper-parameter controls the strength of interpolation. The subscript means the index of the corre-sponding batch sample pair after random shuffling for linear combination, e.g., the row index of nodes from [1,2,...,n]to [5,n-1,...,2] after shuffling. The mixed pair of structure A, features X, and the teachers output Yt contains amore comprehensive set of relations between structure inputs and labels, enhancing the density of distillation from theteacher GNN to the SA-MLP. Note that the mixup on graphs is challenging due to the irregularity and connectivity of the input structure A forGNNs. Existing mixup methods for GNNs attempt to blend hidden embeddings through complex structure alignmenttechniques Wang et al. (2021); Han et al. (2022). However, our approach circumvents the challenge about how to adaptthe mixed structure A to GNNs. Instead, we focus on the mixed outputs Yt from existing teacher GNNs, which serveas additional supervision signals to transfer knowledge to student MLPs. Then, the mixed structure A and features X",
  "Ys = SA-MLP( X, A).(8)": "To the best of our knowledge, we are the first to introduce the mixup in the KD of GNNs to enhance the distillationdensity. To transfer the graph knowledge from GNNs to MLP, we minimize the KullbackLeibler (KL) divergencedistance between the hybrid output Ys of students and Yt of teachers. Compared with relational or hidden distillation,we bypasses the detail of GNNs and still only requires the soft outputs of teachers, therefore benefits from marginalcomputational and storage costs. The overall objective on this structure-mixing knowledge distillation (SKD) is:",
  "Datasets": "To evaluate the performance of the proposed SA-MLP, we consider eight public benchmark datasets, including threecitation datasets Sen et al. (2008) (Cora, Citeseer, Pubmed), two larger OGB datasets Hu et al. (2020) (Arxiv, Products),and three heterophily datasets (Chameleon, Squirrel, Arxiv-year) Pei et al. (2020); Lim et al. (2021) whose structureinformation is important. We used the standard public splits of OGB datasets, and ten frequently used fully supervisedsplits (48%/32%/20% of nodes per class for train/validation/test) provided by Pei et al. (2020); Zhu et al. (2020) ofother datasets for a fair comparison and reproduction. Note that for three small citation datasets, these splits reducerandomness and the possibility of overfitting Zhu et al. (2020); Pei et al. (2020), which are stricter than the randomsplits used in GLNN Zhang et al. (2022). The statistics are shown in and more details can be found in theAppendix.",
  "Transductive and Online Setting": "For the transductive setting, we use all node features and structures for training and distillation. For the online setting,we hold out all test and validation nodes (VU) with their connections when training. For every split, we extract graphG to the subgraph GL that only contains nodes VL with corresponding edges, and the subgraph Gonline including GLplus such edges from VL to VU.",
  "Baselines and Training Details": "In the following experiments, as in Zhang et al. (2022), we also use GraphSAGE Hamilton et al. (2017) as our basicteacher model to investigate the learning ability of the proposed SA-MLP from GNNs. Moreover, for the heterophilydatasets, we apply residual connections to improve the performance of GraphSAGE. Following the standard setting Huet al. (2020); Bo et al. (2021), we fix the hidden dimension of SA-MLP as 128 for all datasets except 64 for Products.The number of layer in SA-MLP is 2 for all datasets for inference efficiency. We use Adam Kingma & Ba (2014)for optimization, LayerNorm Ba et al. (2016), and tune other hyper-parameters, including dropout rate from [0, 0.2,0.5], learning rate from [0.01, 0.005, 0.05], weight decay from [0, 5e-4, 5e-5], from [0, 0.2, 0.5], and from [0.5,0.8, 1] for distillation via validation sets of each dataset. We report results of GLNN and NOSMOG with the sameexperimental setup if available. If the results were not previously reported, we conducted a hyper-parameter searchbased on the official codes. More comparisons with NOSMOG and GLNN under random splits can be found in theAppendix.",
  "Transductive": "As shown in , SA-MLP outperforms all baselines, including teacher GNN models across all datasets. Thereason is that SA-MLP encodes the structure information in an alternative way, which may be complementary toGNNs, especially in the heterophily datasets (Chameleon, Squirrel and Arxiv-year). Comparing SA-MLP to NOS-MOG, our SA-MLP achieves an improvement of up to 3.88%. Note that SA-MLP is end-to-end learning without thepre-computed deep-walk positional embedding, the relational distillation and adversarial learning used in NOSMOG,demonstrating the simplicity but effectiveness of SA-MLP.",
  "Online": "As shown in , we have the following observations: 1) GLNN only improves slightly on MLP and retains alarge performance gap compared with teacher GNNs. GLNN without graph dependency failed to mimic the output ofGNNs in test nodes. Because distillation only occurs on the training nodes. 2) NOSMOG performs better than GLNNby incorporating the pre-computed deep-walk positional embedding and the relational distillation with adversariallearning. However, in most cases, it is still inferior to GNNs due to the limited generalization of deep-walk in the testnodes in online settings. 3) SA-MLP still consistently outperforms the others across all datasets since the SA-MLPcan utilize the connection from training nodes to the newest test nodes. Combined with structure-mixing distillation,such structure information improve the generalization ability of SA-MLP in online settings. Hence the performanceof SA-MLP is even beyond teacher GNN. : Experiment results in node classification for the transductive setting: We report the mean test accuracy (%)and standard deviation over ten runs. represents the improvement of SA-MLP, i.e., GNN 0 indicates that itoutperforms GNN.",
  "DatasetSAGEMLPGLNNNOSMOGSA-MLPGNNMLPNOSMOG": "Cora80.782.44 74.752.22 74.981.84 77.522.86 83.60 1.78 2.82(3.64%) 8.85(11.80%)6.08(7.53%)Citeseer73.241.73 72.412.18 72.551.79 73.421.59 75.15 1.91 1.91(2.60%) 2.74(3.78%)1.73(2.36%)Pubmed87.980.66 86.650.35 88.250.43 88.860.00 89.21 0.31 1.23(1.38%) 2.56(2.90%)0.35(0.40%)Arxiv67.690.24 56.050.46 56.350.21 63.660.25 68.01 0.24 0.32(0.50%) 11.96(21.22%) 4.35(6.43%)Product65.550.88 62.470.10 62.450.34 65.320.42 67.46 0.36 1.91(2.92%) 4.99(7.99%)2.14(3.26%)Chameleon 63.731.58 46.362.52 46.912.09 59.052.31 65.78 2.33 2.05(3.47%) 19.42(41.40%) 6.73(10.56%)Squirrel55.651.47 33.181.94 33.271.78 44.162.24 56.96 1.80 1.31(2.97%) 23.78(71.48%) 12.80(23.00%)Arxiv-year 48.420.46 36.710.21 36.920.10 40.850.36 49.75 0.25 1.33(3.26%) 13.04(35.32%) 8.90(18.38%)",
  "Variants of Student MLPs and KDs": "We study the five variants of SA-MLP under homophily (Cora, Citeseer and Pubmed) and heterophily (Chameleonand Squirrel) in both transductive and online settings. Due to space limits, we report the average test accuracy of thesevariants, as shown in and . Our findings reveal: 1) Role of structure information in student MLP: from the comparison among variants a, b, and c in , thestructure information A is crucial for the performance of SA-MLP, especially in the online and heterophily scenarios.Moreover, the degree information D with centrality encoding in Equation equation 5 can further improve performance. 2) Role of KD strategy: By comparing variants b with d and c with e, we can find that the structure-mixing distilla-tion strategy consistently outperforms the standard logit-based distillation for both MLP(X, A) and MLP(X, A, D)student variants. And the improvement is more significant under the heterophilic graphs. The reason is that the nodeclassification of these graphs is more structure-oriented, and the structure-mixing distillation can reduce the sparsityof structure knowledge to improve the structure awareness of student MLPs.",
  "Effects of Teacher GNN Architecture": "We compare different teacher architectures, including SAGE Hamilton et al. (2017), GCNII Chen et al. (2020), andGAT Velickovic et al. (2018). We choose three citation datasets in the transductive and online settings and report themean accuracy. As shown in , in the online settings, GLNN and NOSMOG suffer from weak generalizationand cannot achieve similar results as teacher GNNs after being distilled. However, SA-MLP can achieve better resultsthan different architecture teacher GNNs in all settings. The results show that structure awareness is the key tothe model generalization ability, and our SA-MLP and distillation strategy are general and robust to accelerate thedeployment of various GNNs.",
  "Hyper-parameter Sensity of Structure-Mixing Knowledge Distillation": "We also investigate the sensitivity of the parameter , which controls the strength of mixup in structure-mixing knowl-edge distillation. As shown in , the was effective in a wide range under both homophily and heterophilydatasets in transductive and online scenarios. Moreover, it significantly increases the performance under the het-erophilic dataset Chameleon in the Online scenario. The reason is that the structure information in heterophilic graphsand online scenarios is the key clue for generalization. Increasing the strength of structure-mixing can enhance thedensity of distillation to improve the structure knowledge of SA-MLP.",
  "Robustness to Noisy Features": "To illustrate the significance of structure information, following Zhang et al. (2022), we introduce varying levels ofGaussian noise to node features, denoted as X = (1)X+, where represents the noise level, and is theGaussian noise independent of X. We then visualize the mean results of the PubMed dataset in transductive and onlinesettings at different noise levels in . We observe that the performance of MLP and GLNN drop significantlyas increases. However, SA-MLP remains comparable to teacher GNN and NOSMOG, as they both explicitly utilizestructured inputs instead of relying solely on node features and NOSMOG employs adversarial training. Note thatGNN conducts recursive aggregation from neighbors to extract graph knowledge and overcome noise, while SA-MLPdirectly processes structure inputs.",
  "Efficiency Comparison": "To further analyze the efficiency of SA-MLP, in , we visualize the trade-off between prediction accuracyand inference time on large-scale Products and Arxiv-year datasets. Following GLNN Zhang et al. (2022), the logscale inference times (ms) are computed based on 10 randomly chosen nodes. We observe consistent trends in bothhomophily and heterophily, with SA-MLP providing an optimal balance of high accuracy and fast inference time.Notably, SA-MLP demonstrates performance improvement over MLP, GLNN and NOSMOG with similar inferencespeeds, especially for MLP and GLNN. While increasing the hidden size of GLNN to GLNNw4 (4 times wider thanGLNN) and GLNNw8 (8 times wider than GLNN) improves their performance, they still lag behind SA-MLP andrequire longer inference times. In comparison to teacher GNNs that display similar performance to SA-MLP, thesemodels demand considerably longer inference times, making them unsuitable for many large-scale data real-worldapplications. For instance, the 3-layer GraphSAGE (SAGE-L3) requires 1957ms on the Products dataset, whereasSA-MLP needs only 2ms, achieving a 978x inference speed up without sacrificing performance. Although NoSMOGalso performs well, it requires preprocessing of the structure with DeepWalk, which is actually very slow on largegraphs. However, this preprocessing time is not included in the inference time. In contrast, SAMLP can directlyprocess the raw input without the need for additional preprocessing.",
  "Conclusion": "We have presented a message-passing free SA-MLP, a practical solution to address the deployment of GNNs viaknowledge distillation. This is achieved by designing a simple yet effective structure-aware student MLP model andcombining it with a novel structure-mixing knowledge distillation strategy. Experiments on eight benchmark datasetsshow that SA-MLP possesses both the accuracy of GNNs and the inference speed of MLPs, whether in homophilic orheterophilic graphs, as well as in transductive and online scenarios. One future work is to investigate the applicationof SA-MLP to other downstream tasks, such as graph classification and link prediction on social networks.",
  "Oren Barkan and Noam Koenigstein. Item2vec: neural item embedding for collaborative filtering. In IEEE 26thInternational Workshop on Machine Learning for Signal Processing (MLSP), pp. 16, 2016": "Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutionalnetworks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 39503957, 2021. Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importancesampling. In 6th International Conference on Learning Representations, International Conference on LearningRepresentations, 2018.",
  "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.Intriguing properties of neural networks. preprint arXiv:1312.6199, 2013": "Shyam Anil Tailor, Javier Fernandez-Marques, and Nicholas Donald Lane. Degree-quant: Quantization-aware trainingfor graph neural networks. In International Conference on Learning Representations, 2021. Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla. Learning MLPs on graphs: A unifiedview of effectiveness, robustness, and efficiency. In International Conference on Learning Representations, 2023.",
  "Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. ACMWeb Conference, 2021": "Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu. A comprehensive survey on graph neural networks. IEEETransactions on Neural Networks and Learning Systems, 32(1):424, 2021. doi: 10.1109/TNNLS.2020.2978386. Bencheng Yan, Chaokun Wang, Gaoyang Guo, and Yunkai Lou. Tinygnn: Learning efficient graph neural networks.In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.18481856, 2020. Cheng Yang, Jiawei Liu, and Chuan Shi. Extract the knowledge of graph neural networks and go beyond it: Aneffective knowledge distillation framework. In Proceedings of the Web Conference, pp. 12271237, 2021. Ling Yang, Ye Tian, Minkai Xu, Zhongyi Liu, Shenda Hong, Wei Qu, Wentao Zhang, CUI Bin, Muhan Zhang,and Jure Leskovec. Vqgraph: Rethinking graph representation space for bridging gnns and mlps. In The TwelfthInternational Conference on Learning Representations, 2024. Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling knowledge from graph convolu-tional networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 70747083, 2020. Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Dotransformers really perform badly for graph representation? Advances in Neural Information Processing Systems,34:2887728888, 2021.",
  "Hongkuan Zhou, Ajitesh Srivastava, Hanqing Zeng, Rajgopal Kannan, and Viktor Prasanna. Accelerating large scalereal-time gnn inference using channel pruning. VLDB, 14(9):15971605, 2021": "Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily ingraph neural networks: Current limitations and effective designs. In Advances in Neural Information ProcessingSystems, volume 33, 2020. Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen K Ahmed, and Danai Koutra. Graph neuralnetworks with heterophily. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 1116811176, 2021. Yu-Lin Zhuang, Lingjuan Lyu, Chuan Shi, Carl Yang, and Lichao Sun. Data-free adversarial knowledge distillationfor graph neural networks. In International Joint Conferences on Artificial Intelligence Organization, 2022. Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent importance sam-pling for training deep and large graph convolutional networks. Advances in Neural Information Processing Systems,32, 2019.",
  "Homophilic Datasets": "Citeseer, Pubmed, Cora Kipf & Welling (2017): For the basic citation datasets, nodes correspond topapers, edges correspond to citation links, the sparse bag-of-words are the feature representation of eachnode, and the label of each node represents the topic of the paper. Note that we use the public ten datasplit(48%/32%/20% for Train/Val/Test) in Pei et al. (2020); Zhu et al. (2020) to reduce randomness andenhance reproducibility. Compared with the GLNN that uses only 20 nodes of each class for training,the results of our splits are more stable and reduce the possibility of overfitting Zhu et al. (2020). Arxiv Hu et al. (2020): The Arxiv dataset is a large-scale citation network collected from all ComputerScience arXiv papers. Each node is an arXiv paper, and edges are citation relations between papers. Thefeatures are 128-dimensional averaged word embeddings of each paper, and labels are subject areas ofpapers. Products Hu et al. (2020): The Products is a large-scale Amazon product co-purchasing network. Nodesrepresent products sold in Amazon, edges indicate the products purchased together, and features are100-dimensional bag-of-words features.",
  "Heterophilic Datasets": "Squirrel, Chameleon Pei et al. (2020): Chameleon and Squirrel are web pages extracted from differenttopics in Wikipedia. Similar to WebKB, nodes and edges denote the web pages and hyperlinks amongthem, respectively, and informative nouns in the web pages are employed to construct the node featuresin the bag-of-word form. Webpages are labeled in terms of the average monthly traffic level. Arxiv-year Lim et al. (2021): Modifying node labels of the Arxiv dataset to the year of paper, andthe goal is to predict the year of paper publication that allows for evaluation of GNNs in large-scalenon-homophilous settings.",
  "A.2Additional Comparison of Citation Datasets": "We provide additional experiments for the comparison under the random splits (20 labeled nodes of each class duringtraining) of citation datasets used in GLNN and report the mean test accuracy in . We can observe that SA-MLP also consistently outperforms GLNN, NOSMOG and teacher GNN. Moreover, we conduct the production (prod)scenario that involves both inductive (ind) and transductive (trans) settings used in GLNN (see ). We find thatSA-MLP can also achieve the best performance across all scenarios. However, the performance of these splits that onlycontained 20 labeled nodes is susceptible to hyper-parameters since the few training data cause the risk of overfitting.",
  "A.3Additional Comparison of GLNN+": "GLNN also provides a larger GLNN+, which scales hidden dimension from 256 to 1024 for Arxiv (GLNNw4) and2048 for Product (GLNNw8), with a larger capacity but a slower speed. We provide additional experiments for theGLNN+ of the trans and online for large-scale OGB datasets. We omit other datasets since the performance of GLNN+is similar to that of GLNN. From , we find that GLNN+ can improve the performance of large-scale datasetsunder the trans setting. However, it achieves similar results to GLNN under the online setting, which implies thatthe improvement of trans for OGB datasets is due to the memory capacity, i.e., the larger parameters of GLNN+ canmemorize all the teacher outputs. It still does not fully understand the structure information and generalizes limitedlyon unseen test nodes under the online setting. However, the improvement over both trans and online of our SA-MLPis due to explicit structure awareness.",
  "A.4Additional Inference Time Comparison": "Following the settings GLNN Zhang et al. (2022), we also compare SA-MLP with other inference acceleration tech-niques with GNNs, including vanilla SAGE, quantized SAGE from FP32 to INT8 (QSAGE), SAGE with 50% weightspruned (PSAGE), and inference with neighbor sampling with fan-out 15 (NSSAGE). All GNNs have three layers and256 hidden units, while GLNN+ has 1024 hidden units for Arxiv and 2048 for Products to achieve optimal perfor-mance Zhang et al. (2022). As shown in , all MLP-like students achieve substantially faster inference than GNNvariants. Moreover, SA-MLP is the only method that processes structured inputs explicitly but offers the fastest infer-ence speed without sacrificing performance. Following the original paper of NOSMOG, we set the hidden size to 256and achieved the best performance. Although it processed the pre-compputed deepwalk embedding, the Setting thehidden Without considering the time cost of preprocessing by DeepWalk, NoSMOGs inference time is significantlyless than that of GLNN. The reason is that it uses 256 as the hidden size to achieve optimal performance. Compared toGLNN+ and NOSMOG, the speedup can be attributed to Pytorchs sparse tensor multiplication with structure inputsand the smaller hidden unit size (128 for Arxiv and 64 for Products) employed in SA-MLP."
}