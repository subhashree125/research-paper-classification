{
  "Abstract": "Many graphs or networks are heterogeneous by nature, involving various vertex types andrelation types. Most graph learning models for heterogeneous graphs employ meta-pathsto guide neighbor selections and extract composite relations. However, the use of meta-paths to generate relations between the same vertex types may result in directed edgesand failure to fully utilize the other vertex or edge types in the data. To address sucha limitation, we propose Heterogeneous graph adaptive flow network (HetaFlow), whichremoves the need for meta-paths. HetaFlow decomposes the heterogeneous graph into flowsand performs convolution across heterogeneous vertex and edge types, using an adaptationto change the vertex features based on the corresponding vertex and edge types duringaggregation. Experiments on real-world datasets for vertex clustering and vertex classificationdemonstrate that HetaFlow outperforms other benchmark models and achieves state-of-the-art performance on commonly used benchmark datasets. The codes are available at",
  "Introduction": "A significant number of real-world graphs come with a diversity of vertex types and relation types. Forexample, the ACM dataset (Wang et al., 2019) (example data shown in ) consists of four types ofvertices: paper, author, conference, and subject. Many heterogeneous graph neural network (HGNN) modelsare based on the idea of meta-paths (Huang et al., 2016; Zhang et al., 2018a; Hu et al., 2018). A meta-path isan ordered sequence of edge types, which defines a new composite relation between the source vertex typeand the destination vertex type. For example, in the ACM dataset, a relation between two papers can bedescribed by the meta-path Paper-Author-Paper (P-A-P), which represents an authors co-authorship ofthese two papers; or Paper-Subject-Paper (P-S-P), which implies that the two papers investigate the samesubject. Depending on the meta-path, the relation between vertices can have different semantics. Most meta-path-based heterogeneous graph models for semi-supervised vertex classification and clustering(Zhang et al., 2019a; Shi et al., 2018; Fan et al., 2019) follow two similar steps to create homogeneous graphson which traditional GNN models can then be applied: 1) Meta-paths are chosen so that the source anddestination vertex types are the same. 2) Then, one employs a set of predefined meta-paths to decompose theoriginal heterogeneous graph into one or several subgraphs with homogeneous vertex types. An appropriateGNN model on each subgraph can then be applied and the results fused to obtain the final inference.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou.Token dropping for efficient bert pretraining. In Proc. 60th ACL, volume 32, pp. 37743784, Dublin, Ireland,2022. Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. Leveraging meta-path based context for top-nrecommendation with a neural co-attention model. In Proc. 24th ACM Int. Conf. Knowl. Discovery DataMining, pp. 15311540, 2018.",
  ": An example of a heterogeneous graph based on the ACM dataset": "We propose a novel Heterogeneous graph adaptive flow network (HetaFlow). HetaFlow first decomposes thegraph into several groups of paths, which we call parallel flows. Then, for each edge, the model encodes itaccording to its edge type and its two end vertex types. HetaFlow applies type-specific linear transformationsto project heterogeneous vertex attributes for each vertex to a common vector space. Each edge thus gets itstype-based edge feature. Next, HetaFlow performs intra-path aggregation along each path in every parallelflow, and inter-flow aggregations across multiple parallel flows with type-based feature adjustments. The computational complexity of HetaFlow is lowered significantly compared to these other models andits model size depends only on the number of parallel flows used. See the discussion about model size andcomplexity in Appendix B and Appendix C. In this work, we introduce the concepts of paths and parallel flows to heterogeneous graphs and designtype-based adjustments. We generalize the model to heterogeneous graphs with additional mechanisms tohandle the heterogeneity of the graphs. Extensive numerical experiments are provided to verify our modulechoices.",
  "Graph neural networks": "GNNs extend convolution neural networks (CNNs) to general graph-structured data. There are two mainclasses of GNNs, namely spectral-based and spatial-based. Spectral-based GNNs operate in the Fourierdomain. An example is ChebNet (Defferrard et al., 2016), which processes graph signals (vertex features)using Chebyshev polynomials of the graph Laplacians. By simplifying the parameters of ChebNet, GCN(Kipf & Welling, 2016) overcomes the overfitting problem of ChebNet. However, spectral-based GNNs requirethe whole graph as an input in the worst case when the polynomial degrees are high, and are sensitive tograph structures. They thus suffer from scalability issues. Spatial-based GNNs imitate the convolution operation in CNNs. GNNs of this kind operate directly ona vertexs neighbors. An example is GraphSAGE (Hamilton et al., 2017), which learns embeddings byaggregating features of the target vertex and its neighbor vertices. Other spatial-based GNN variants havebeen proposed by improving the aggregator function. GAT (Velickovic et al., 2018) employs the attentionmechanism in the aggregator to assign different importance weights based on the neighboring vertices features.GFCN (Ji et al., 2020) decomposes a graph into non-intersecting paths and performs convolutions along thesepaths before aggregating them. It can be shown that GFCN recovers the GCN model. Inspired by the idea ofGGNN (Li et al., 2016) that adds gated recurrent unit (GRU) (Cho et al., 2014) into the aggregator function,GaAN (Zhang et al., 2018b) combines GAT and GGNN for spatio-temporal graphs. To improve the accuracy,STAR-GCN (Zhang et al., 2019c) employs various GCN encoder-decoders.",
  "Heterogeneous graph embedding": "Various heterogeneous graph embedding methods have been proposed to embed vertex features into a low-dimensional space. For example, metapath2vec (Dong et al., 2017) employs meta-path-based random walksand skip-gram (Mikolov et al., 2013a) to generate vertex embeddings. With the guidance of manually-definedmeta-paths, ESim (Shang et al., 2016) learns vertex features in a user-preferred embedding space. HIN2vec(Fu et al., 2017) learns vertex embeddings and meta-path representations simultaneously by carrying outmultiple prediction training tasks. Given a meta-path, HERec (Shi et al., 2018) proposes an algorithm thatconverts a heterogeneous graph into meta-path-based neighbors and then employs the DeepWalk (Perozziet al., 2014) model to learn the target vertex embedding. Similar to HERec, HAN (Wang et al., 2019) alsodecomposes a heterogeneous graph into multiple meta-path-based homogeneous graphs. HAN uses a graphattention mechanism to combine results from different meta-paths. These heterogeneous graph embedding models suffer from the limitations discussed in due to thereliance on meta-paths to construct homogeneous subgraphs. The use of meta-paths inevitably introducessome limitations like discarding vertex features or loss of vertices and connections. For example, metapath2vec,ESim, HIN2vec, and HERec omit vertex features, which lowers their performance when performing taskson graphs with rich vertex features. Furthermore, HERec and HAN only consider the two end vertices ofeach meta-path while ignoring the vertices along the path, which results in information loss. The methodmetapath2vec achieves suboptimal performance as it takes only one meta-path as the input. SeHGNN(Yanget al., 2023) improves the performance by introducing feature projection. However, it needs a complicatedextra data pre-processing step. Several HGNNs (Zhang et al., 2019b; Yun et al., 2019; Lv et al., 2021; Ahn et al., 2022) have been proposedto solve the meta-path selection dilemma by generating new graph structures. For instance, HetGNN (Zhanget al., 2019b) divides neighbors into subsets based on their types and employs an aggregator function foreach type of neighbor. However, as pointed out by Lv et al. (2021), HetGNN has information missing inhomogeneous baselines and GAT with correct inputs gets clearly better performance. Simple-HGN (Lvet al., 2021) is based on GAT. It leverages type information by introducing learnable type embeddings andenhanced modeling power with residual connections and l2 normalization on the output embeddings. Many attention-based models failed to learn node-level and relation-level attention simultaneously. BA-GNNIyer et al. (2021) employs a novel bi-level graph attention mechanism to overcome this issue. Instead of fromthe global graph context, BA-GNN attends to both types of information from local neighborhood contexts.It achieved good performance. However, it assigns the same weights to the neighbor vertices of the samedistance, which means the directions of edges are not exploited during aggregation. ie-HGCN Yang et al. (2021) proposed a hierarchical architecture as object-level and type-level aggregation.ie-HGCN can learn the representations of objects and automatically discover and exploit the most usefulmeta-paths. Its two-level aggregation structure shows good performance and interpretability in capturingsemantic relations. Our HetaFlow has a similar design. HetaFlows intra-path aggregation and inter-pathaggregation empower it to catch semantic relations efficiently. As mentioned in , ie-HGCN fails toconsider the directions of edges. It assigns the same weights to neighbor vertices in the same distance. By",
  "decomposing the graph into 1-D flows, HetaFlow is capable of learning different weights to neighbors withthe same distance": "HGT Hu et al. (2020) and HetSANN Hong et al. (2020) are also non-meta-path-based methods. Theyavoid the meta-path selection dilemma by projecting the representations to the same space and employingattention mechanisms to help aggregations. They make use of the type information by assigning differentaggregation weights to various edge types. However, this may not be enough since the same-type edges mayconnect vertices of different vertex types, which usually happens when there exists only one edge type in theheterogeneous graph. HetaFlow mitigates this problem by introducing an adjustment component. HetaFlowemploys learnable parameters to generate adjustment weights based on the embeddings of types of verticesand edges. When aggregating on the same edge type, the features of neighbor vertices are adjusted differentlybased on their vertex types.",
  "Preliminaries": "In this section, we define important terminologies related to heterogeneous graphs. Graphical illustrations areprovided in . We summarize the commonly used notations in this paper in . Throughout thispaper, we consider a graph G = (V, E), where V is the set of vertices and E is the set of edges of the graph.",
  "Motivation and differences": "Before delving into the details of the model, we first discuss why we choose to decompose the heterogeneousgraph into parallel flows. Without relying on any prior meta-path knowledge, we want to decompose aheterogeneous graph into subgraphs that traditional GNNs can handle. An intuitive idea is to form subgraphsby selecting the edges that follow similar patterns, i.e., build subgraphs with homogeneous edge type. Forexample, consider the heterogeneous graph shown in (a). It can be decomposed into two subgraphs: onesubgraph is shown in (b) and the other subgraph consists of the three separate blue lines in (a).However, even though a subgraph has homogeneous edge types, it may still not be appropriate to apply ahomogeneous GNN on the subgraph due to the following two considerations. Firstly, there may exist multiple vertex types in each subgraph, and thus we need to make adjustmentsbased on the types of the vertices when performing convolutions. We resolve this challenge by introducingvertex feature transformation, which is discussed in detail in .3. As discussed in the comparisons in.2, many non-meta-path-based methods, like HGT Hu et al. (2020) and HetSANN Hong et al. (2020)employ different weights for the aggregations along each edge type. However, edges of one edge type mayconnect various types of vertices. For instance, paper type vertices can be linked to both conference typeand journal type vertices by be submitted to type edge. When predicting the research area of a papervertex, it is better to assign different weights to conference type and journal type vertices because journalsare more general in some research area. However, HGT and HetSANN shall use the same filter since the edgetype is the same. We introduce the adaptation adjustments to improve the performance against this problem. Secondly, some types of edges may be directed, like the teacheri-teaches-studentj relations in a socialnetwork. Consider a path with 3 vertices and two edges, vi-teaches-vj-teaches-vk, where each vertex standsfor a person. When trying to predict the academic interest of vj, we may find features of vi and vk are ofdifferent importance. Thus, it may be appropriate to assign different weights to neighboring vertices accordingto their positions instead of distances. Allocating the same weight to vertices at the same distance may resultin suboptimal results as the weights no longer capture the importance of these vertices to the target vertex atwhich the convolution is applied. We introduce parallel flow decomposition that decomposes the graph into 1D paths and allows us to assigndifferent weights to vertices based on their positions in a neighborhood of the target vertex on the 1D path.Moreover, though the edges of each subgraph are likely to follow similar patterns, they may contain differentmeanings and need to be processed differently. For example, consider a computer vision task. In an image,the horizontal and vertical edges follow the same pattern (both can be detected by similar 1D convolution",
  "Implementation": "Let O be an operation that takes a heterogeneous graph G0 = (V, E0) as the input and outputs a subgraphG1 = (V1, E1) with homogeneous edge type. One intuitive example of O is to select all edges of a certaintype in G0. Starting with a given heterogeneous graph G0, we build a subgraph G1 = O(G0). Then, we repeatthis operation on the remaining part, i.e., the complement of graph G1 with respect to G0, to obtain anotherhomogeneous edge type subgraph G2 = O(Gc1), where Gc1 = (V, E0\\E1). This procedure is repeated until noedges remain. The original heterogeneous graph is thus decomposed into several subgraphs with homogeneous edge types.Then, we can follow Ji et al. (2020) to decompose each subgraph into parallel flows and take the union of thedecompositions. This union is a parallel flow decomposition of the original heterogeneous graph. For completeness, we now provide a brief description of the parallel flow decomposition implementation. Notethat this approach is only one of many possible approaches. For details, we refer the reader to Ji et al. (2020).For every subgraph Gi = (Vi, Ei) (can be possibly disconnected), a forest Gi1 = (Vi1, Ei1) consisting of treesthat span the components of Gi is found by performing a depth-first search of the graph (in this paper, weselect a vertex with the highest degree in subgraph Gi to be the root vertex for each component). Denotethe complement of the forest Gi1 with respect to Gi as Gci1, whose edge set is Ei\\Ei1 and isolated verticesafter removing Ei1 are removed. Repeating the process, we again find a spanning forest Gi2 for Gci1. Thisprocedure is repeated until the remaining complement is empty. The procedure terminates in a finite numberof steps n because the number of edges remaining in the complement at each step decreases. We then haveGi = nj=1 Gij. Next, we consider the parallel flow decomposition of a tree in each forest Gij obtained in the above procedure.Let this tree be T and degmax be its maximal degree. We can label each edge in T with a number in theset {1, 2, . . . , (degmax +1)/2} such that every label is used at most twice for edges in T connected to eachvertex v in T. Finally, to form a parallel flow, we take edges that have the same label. The tree T is nowdecomposed into multiple parallel flows.",
  "Feature embedding": "We first record the types of vertices and edges so that we can adjust the vertex features according to theirtypes. For most datasets, the vertex types and edge types are categorical. We use one-hot encoding for them. For an edge (u, v) connecting vertices u and v, let Tu R|A| denote the one-hot encoding of the type of vertexu, and Tuv R|R| be the one-hot encoding of the type of edge (u, v). Then, its edge embedding is given by:",
  "Zuv = Tu Tuv Tv.(1)": "Different vertex types may have feature vectors with unequal dimensions and belong to different featurespaces. So we apply a type-based linear transformation for each type of vertices before feeding vertex featurevectors into the intra-path aggregation module. Let xv RdA denote the original feature vector of vertex v,and WA RddA denote the parametric weight matrix for type A vertices. Then, for a vertex v of type",
  "Intra-path aggregation": "Since the intra-path aggregation involves only vertices on this path, it is natural to consider 1D convolution.For each parallel flow P, we employ the same 1D convolution filter of odd length h for all paths P P. LetN Pv = P Nv((h 1)/2) to denote neighbors of vertex v on path P. The 1D convolution at each vertex valong the path P is given by:",
  "where the weights wPu are the 1D convolution filter weights for the parallel flow P": "To fuse features learned from different edge types and parallel flows, we perform vertex-level and flow-basedadjustments. The weighting coefficients employed in this process are called adjustment weights. We design an adjustment weight that modifies the edge weights based on the types of edges and their endvertices. Let Wa and ba be learnable parameters. Then, the vertex-level adjustment weight Auv is defined as:",
  "uN PvwPu (hu Auv).(5)": "Next, we use attention mechanisms to learn the importance ePvu of a neighboring vertex u to vertex v on thepath P, which is then normalized across all choices of u N Pv using the softmax function. Then, we obtain anormalized importance weight Pvu for each neighboring vertex on this path. Furthermore, to reduce the variance introduced by the parallel flow decomposition, we extend the attentionmechanism to multiple heads by concatenating K independent attention mechanisms. Let hPi Rd denotethe features of vertex i after intra-path aggregation,ePvu",
  "(6)": "To fuse the features hPv in (6) of the same vertex v on different paths P, we incorporate an attentionmechanism based on paths. Each path from a parallel flow consists of only one edge type. Let TP be the edgetype encoding of path P, and Wf, bf be learnable parameters. The path-based adjustment weight FP forpath P is defined as:",
  "Inter-flow aggregation": "Suppose there are M parallel flows in the parallel flow decomposition of .2. Then for each vertex v,since it belongs to at most one path in each parallel flow, we have at most M latent vectorshPv : P Pv,where Pv is the collection of all paths that contain vertex v and |Pv| M. Since paths in Pv are not equallyimportant to the vertex v, it is reasonable to use the attention mechanism to assign different weights tofeatures obtained from different paths. We use Pv to denote the relative importance of path P to vertexv. We perform a weighted sum of all the path-based feature vectors of v using the parameterized attentionvector q Rd as follows:",
  "Training": "To process the vertex representations, we use the following loss functions. Here we denote the set of verticesthat have labels as VL, the number of classes as C, the one-hot label vector of vertex v as yv, and thepredicted probability vector of vertex v as hv. For semi-supervised learning, we use the cross entropy loss asfollows:",
  "HetaFlow is capable of representing a more general polynomial, where each monomial consists of severalvariates. This observation is expressed in the following result": "Proposition 1. Suppose no edge is contained in different paths inside a given set of parallel flows. If M isthe number of monomials and N denotes the largest number of variates contained in one monomial, then thereis a HetaFlow model with M + 2 Mi=1Nj=1 kij hidden layers producing the same output as the convolutionfilter",
  "Proof. See Appendix A": "Proposition 1 shows that there is no loss in generality in adopting HetaFlow with parallel flow decompositionwhen compared to traditional heterogeneous GNNs like HAN Wang et al. (2019), MAGNNFu et al. (2020),and Simple-HGN(Lv et al., 2021). It is clear from (14) that the representations we can learn using HetaFloware strictly richer than that achievable by the heterogeneous GNNs in (13). This motivates us to consider theuse of parallel flow decompositions in HetaFlow. In contrast, (14) is a multi-variate polynomial and involves interactions between different edge types with thesame weights assigned to the same kind of interaction. It is also clear that HetaFlow is not restricted to thatparticular form. By utilizing 1D convolutions along paths, HetaFlow can assign different weights to verticesat the same distance in the path (i.e., the 1D filter is non-symmetric). And under this case, it is obvious thatwe can not use any polynomial to represent it.",
  "Time complexity": "We provide a detailed general analysis of the model size and computational complexity of HetaFlow inAppendix B and Appendix C. Here we follow the assumptions of SeHGNN(Yang et al., 2023) to make clearercomparisons with other models. The theoretical results are summarized as We assume a one-layer structure with M meta-paths for SeHGNN and HAN. Let l be the maximum hopof meta-paths. To ensure the same receptive field size, we assume a l-layer structure for Simple-HGN andHetaFlow. The dimension of input and hidden vectors is d. Instead of considering all vertices, here we discussthe case when doing tasks on vertices of one vertex type. The number of target-type vertices is n. For HAN,let e1 be the average number of neighbors in meta-path neighbor graphs. Let e2 be involved neighbors during",
  "O(|R||P|nd2 + nld2) = O((|R||P| + l)nd2).(15)": "For complicated graphs whose number of types of vertices and edges are large, |R| is normally much smallerthan the total number of edge types |R|. There are |A|2 possible edge types and edges of at most 2|A|1 edgetypes connecting target-type vertices (for the generality of discussion, we consider the directed graph here. Ifit is undirected, then |R| is at most |A|). So, HetaFlow only needs to decompose at most 2|A| 1 subgraphs.Moreover, HetaFlow can discard parallel flows that do not contain target-type vertices. For instance, considerthe example claimed in , HetaFlow can discard parallel flow 2 in (d) if the target vertex type isthe blue vertex type. So |P| is normally smaller than the total number of parallel flows |P|. And the numberof possible meta-paths is large for complicated graphs. Take length 5 meta-paths as an example (we assumethe endpoints of meta-paths are of the target node type), each intermediate position on the meta-path has|A| possibilities. There are |A|3 possible meta-paths that contain target vertices as the endpoints. Thus,M |R||P| when trying to consider all possible meta-paths for complicated graphs. For the four datasets we tested in experiments, if want to consider all possible meta-path, then we haveM |R||P|. so the theoretical complexity of HetaFlow is much lower than that of SeHGNN, and muchlower than that of HAN and Simple-HGN according to (Yang et al., 2023).",
  "Experiments": "In this section, we conduct experiments on the clustering and classification tasks using benchmark datasets.We compare the performance of HetaFlow to other state-of-the-art baseline models. The implementationdetails are claimed in Appendix D. After testing with benchmarks, we further conduct experiments on severalvariants of HetaFlow to validate the effectiveness of each component of our model. We test the validity ofvertex feature adjustment, the importance of introducing parallel flow decomposition, and the influence ofdifferent decomposition methods separately. These ablation studies are illustrated in Appendix E.",
  "We test the performance of HetaFlow on the Heterogeneous Graph Benchmark (HGB) (Lv et al., 2021), usingthe following vertex classification datasets:": "ACM. We choose the papers published in SIGMOD, KDD, MobiCOMM, SIGCOMM, and VLDB.The papers belong to three categories: Data Mining, Wireless Communication, and Database. Thenwe construct a subset that comprises 5835 authors (A), 3025 papers (P), and 56 subjects (S). Paperfeatures are the bag-of-words embedding of keywords. The papers are labeled based on their classes.",
  "ACM10,9424547,872831830DBLP26,1284239,56664334Freebase180,09881,057,6883671IMDB21,420486,642651232": "we place the authors into four categories: information retrieval, data mining, database, and machinelearning. The research area of each author is labeled based on the conferences they participate in.We take the bag-of-words representation of keywords as the author features. IMDB. We construct a heterogeneous graph that consists of 4278 movies, 5257 actors, and 2081directors after data preprocessing. We label the movies (Action, Comedy, and Drama) according totheir genre information. Movie features are the bag-of-words representation of its plot keywords. Freebase. Freebase is a huge knowledge graph of the worlds information. It involves many aspectslike music, movies, people, and so on. Following the procedure of a previous survey Yang et al. (2020),we sample a subgraph of 8 categories of vertex types with about 1,000,000 edges. Simple statistics of the datasets are summarized in , and network schema is illustrated in Appendix 8.For vertices with no attributes, we use the average of the features of their neighbors as their input features.",
  "Baselines": "To evaluate the performance of HetaFlow, we compare it with widely-used baselines: RGCN (Schlichtkrullet al., 2018), HAN (Wang et al., 2019), GTN (Yun et al., 2019), RSHN (Zhu et al., 2019), HetGNN (Zhanget al., 2019b), MAGNN (Fu et al., 2020), HetSANN (Hong et al., 2020), HGT (Hu et al., 2020), GCN (Kipf& Welling, 2016), GAT (Velickovic et al., 2018), MECCH (Fu & King, 2024), SeHGNN Yang et al. (2023)and Simple-HGN (Lv et al., 2021). For comparison purposes, when testing on the homogeneous and the random-walk-based models, we firstbuild a homogeneous subgraph, which is homogeneous in both edge and vertex types, for each meta-path. Wethen test the model on each subgraph. After obtaining the performance of the model for every meta-path, wetake the best one as the performance of the model. For those models based on random walks, the windowsize is set to be 5, walk length to be 100, walks per vertex to be 40, and the number of negative samples tobe 5. For a fair comparison, the embedding dimension is decided according to the best results reported in thepaper Shi et al. (2018) for all models.",
  "Classification results": "From , we see that HetaFlow has the best performance for most cases, although it does not utilizemeta-paths. In general, HetaFlow has comparable performance on the IMDB dataset and outperforms theother baselines by 0.5 0.8% on DBLP and ACM datasets. This verifies our conjecture that parallel flowdecomposition suffers less information loss than meta-path-based reconstruction methods and can make useof edge directions. Compared to the models that simply average over vertex neighbors, GAT and Simple-HGN perform wellsince they train weights to weigh the information properly. Compared to HAN and MAGNN, HetaFlow,which requires no manually defined meta-paths, captures even richer semantics successfully.",
  "Macro-F1Micro-F1Macro-F1Micro-F1Macro-F1Micro-F1": "RGCN91.55 0.74 91.41 0.75 91.52 0.50 92.07 0.50 58.85 0.26 62.05 0.15HAN90.89 0.43 90.79 0.43 91.67 0.49 92.05 0.62 57.74 0.96 64.63 0.58GTN91.31 0.70 91.20 0.71 93.52 0.55 93.97 0.54 60.47 0.98 65.14 0.45RSHN90.50 1.51 90.32 1.54 93.34 0.58 93.81 0.55 59.85 3.21 64.22 1.03HetGNN85.91 0.25 86.05 0.25 91.76 0.43 92.33 0.41 48.25 0.67 51.16 0.65MAGNN90.88 0.64 90.77 0.65 93.28 0.51 93.76 0.45 56.49 3.20 64.67 1.67HetSANN90.02 0.35 89.91 0.37 78.55 2.42 80.56 1.50 49.47 1.21 57.68 0.44HGT91.12 0.76 91.00 0.76 93.01 0.23 93.49 0.25 63.00 1.19 67.20 0.57 GCN92.17 0.24 92.12 0.23 90.84 0.32 91.47 0.34 57.88 1.18 64.82 0.64GAT92.26 0.94 92.19 0.93 93.83 0.27 93.39 0.30 58.94 1.35 64.86 0.43Simple-HGN 93.42 0.44 93.35 0.45 94.01 0.24 94.46 0.22 63.53 1.36 67.36 0.57MECCH92.74 0.40 92.67 0.36 94.34 0.29 95.08 0.25 62.59 1.96 64.62 2.38SeHGNN94.05 0.35 93.98 0.36 95.06 0.17 95.42 0.17 67.11 0.25 69.17 0.38",
  "Training time0.856.29--0.770.310.48": "All models suffer poor performance on IMDB. This may be due to the labels of vertices: though each movievertex may be assigned multiple labels, we merely select the most related one as its label. HetaFlow achievesa smaller performance gain on DBLP than on the other two datasets. This is mainly because it turns outthat one of the manually defined meta-paths (A-P-C-P-A) contains most of the information necessary forcorrect classification. Thus, the performance gain is not as much as on the other two datasets ACM andIMDB. Another reason is that some vertices are discarded since they are not on the meta-paths. Therefore,GCN test only a part of the dataset and may avoid vertices that are hard to classify. Comparing the results in and , we observe that although SeHGNN is the best performer onIMDB, it has poor performance on Freebase. This may be due to Freebase having insufficient vertex features.We see that the performance of HetaFlow is more stable across different datasets, which suggests that theadaptive adjustments in HetaFlow are useful. The training time of GAT and GCN is not recorded since theperformance is of one meta-path. HetaFlow has a fast training speed with the help of data pre-processing(the parallel flow decomposition) and lower complexity.",
  "Clustering": "We conduct the clustering task to evaluate the embeddings learned from the above algorithms. We utilize theK-Means algorithm to perform vertex clustering and the number of clusters K is set to the number of classesfor every dataset, i.e. 4 for DBLP and 3 for both IMDB and ACM. We use the same ground truth as in vertexclassification. Moreover, we use the Normalized Mutual Information (NMI)(Strehl & Ghosh, 2002) and theAdjusted Rand Index (ARI)(Yeung & Ruzzo, 2001) to measure the quality of the clustering results. Since theperformance of K-Means is highly affected by the initial centroids chosen, we repeat K-Means ten times foreach run of the model. Furthermore, we run each model 10 times and report the averaged results in . As seen in , on DBLP and ACM, GCN performs better than metapath2vec for the vertex classificationtask while metapath2vec is better for the clustering task. Random-walk-based methods (i.e., metapath2vec)",
  "HetaFlowlimit4.98M Macro-F1 89.430.19 89.970.14 90.320.11 91.260.11Micro-F1 89.200.24 89.610.18 90.540.14 90.920.08": "have advantages in vertex clustering because the use of random walks makes vertices that have small distancesin the graph to be close in the embedding space (You et al., 2019). Thus, the positional information is concerned,which facilitates the K-Means algorithm as it clusters vertices based on the Euclidean distances betweenembeddings. Despite this, HetaFlow performs consistently better than all baselines, which demonstrates thatthrough the use of parallel flow decompositions, HetaFlow can learn a powerful vertex embedding withoutany meta-paths.",
  "Parallel flow decomposition methods": "We study two aspects of the parallel flow decomposition: the influence of introducing such decomposition andthe efficiency of various decomposition methods. We test different approaches on the ACM dataset. The main motivations for introducing parallel flow decomposition are discussed in .2. HetaFlowallows us to assign different weights to the neighboring vertices, i.e., weights are assigned based on positionsinstead of distances. With the same number of layers, HetaFlow models can represent a richer class ofpolynomials than traditional GNN approaches. Firstly, we test the models without parallel flow decomposition nor limits on the model sizes. Secondly, wetest models that employ parallel flow decomposition but with a limit on the model size. Then, we test modelswithout parallel flow decomposition but with similar constraints on model sizes. The results are shown in In , HetaFlowpf denotes our base HetaFlow model (model size is measured by the number of parameters,which is limited to 5M in this experiment), i.e., the one we used to compare with other baselines .HetaFlowwo is the equivalent model without utilizing parallel flow decomposition and has no limits on itsmodel size. It decomposes the heterogeneous graph into subgraphs with homogeneous edge types instead.During convolutions, HetaFlowwo shall assign weights according to the distance as normal GNNs andHetaFlowwo also keeps the adjustment layers. HetaFlowlimit denotes the model that does not employ parallelflow decomposition and whose model size is no larger than 5M. From , we observe that HetaFlowpfgenerally has on-par performance as HetaFlowwo even though the model size is much smaller. HetaFlowlimithas the worst performance.",
  ": Examples of different parallel flow decomposition methods": "Next, we test different methods to select paths in the parallel flow decomposition using the ACM dataset,and the results are reported in . For the decomposition method mentioned in .2.1, there existmultiple ways to allocate labels to the edges of the spanning subtree. We consider the following methods to allocate edge labels during the parallel flow decomposition procedure(all methods start from the root vertex and then target at the neighbor vertices following the descendingorder of their degrees in the tree): (i) For each vertex under consideration, label adjacent edges according to the descending order of theirend vertices degrees in the original heterogeneous graph G. For instance, consider the spanning treewhose root vertex is v8 as shown in (a). The vertex v8 has four neighbors in the tree: v2 withdegree 4 in the original graph, v3 with degree 3, v4 with degree 2, and v7 with degree 3. We assignlabel 1 to the two edges that connect vertices with the first two highest degrees, i.e., the edge (v8, v2)and the edge (v8, v7). Then we remove v2 and v7 from the neighbor set Nv(8). Repeat the procedure.After all edges that connect v8 are labeled, we then move to v2, the neighbor vertex of v8. Since oneor several edges that connect our target vertex may have already been assigned with labels, we skipthe corresponding labels so that no label is used more than twice. Here, we obtain the two parallelflows of (b) as shown in (a). We call this variant HetaFloworder. (ii) For each vertex v under consideration, select the neighboring vertex with the highest degree in G asthe left neighbor and the neighboring vertex with the lowest degree in G as the right neighbor toconstruct a path. Remove these from the neighbor set Nv(1) and repeat the procedure to obtainanother path and so on. Consider the example of v8 in (b) again. We obtain the parallel flowsshown in (b). This variant is denoted as HetaFlowposition. (iii) For each vertex under consideration, assign random labels to the edges connecting it (from 1 to(degmax +1)/2) under the constraints that no label is used more than twice. This is our base modelHetaFlowpf.",
  ": Visualization of other embedding methods on DBLP. Each point indicates one author and its colorindicates the research area": "From , we see that the homogeneous GNN, namely GCN, give more separate islands and the boundariesbetween different colors are not clear. Furthermore, we observe an area where all four types of vertices aremixed. One possible reason is that these two models only take information from one meta-path. For somevertices, their labels may be hard to tell apart under certain meta-paths. This phenomenon also appears in the results of other models, but HAN and HetaFlow perform better as theytake information from more meta-paths or the whole heterogeneous graph, respectively. For HAN, we observethat there are at most three kinds of vertices that are mixed together. Meanwhile, there exist places whereall four types of vertices are mixed in the results of GCN and metapath2vec. When it comes to the result ofHetaFlow, there are fewer places where more than two types of vertices are mixed. Moreover, fewer vertices,which are of different colors, are mixed when compared with the visualization of HAN.",
  "Limitations and Future Work": "In graph classification tasks that involve a multitude of graphs in both the training and test sets, HetaFlowmay not perform well. This is because the parallel flow decomposition is not unique. We may get differentsubgraphs when performing parallel flow decomposition using different methods on the same graph. As aresult, the filters and attention weights learned on one training graph may not perform well on another graphunless more restrictions are imposed to make the parallel flow decomposition unique. In HetaFlow, the graph is decomposed into multiple parallel flows, with each vertex now appearing (deg +1)/2times. Moreover, unlike meta-path-based methods, HetaFlow retains all vertices in the graph. The memoryusage of HetaFlow is thus higher during runtime. A future research direction is to investigate the use ofsparse matrix representations and ideas like drop token (Hou et al., 2022) to reduce memory storage. Anoptimization example is illustrated in Appendix F. Possible future research directions include the investigationof techniques like sparse matrix representations and dropping of tokens and graph edges to reduce the model",
  "Conclusion": "In this paper, we have proposed a new HGNN framework called HetaFlow to perform semi-supervised vertexclassification and clustering tasks on heterogeneous graphs. HetaFlow addresses three limitations of existingHGNNs: loss of vertex content features due to vertex selection, the dependence on meta-path, and allocationof the same weights to neighboring vertices at the same distances. HetaFlow introduces the notion of paralleldecomposition for heterogeneous graphs, which is used to enable the model to assign different weights toneighboring vertices at the same distance. This property benefits detailed vertex-level adaptive adjustments.HetaFlow applies four building block components: vertex content transformation, intra-path aggregation,adaptive adjustments, and inter-flow aggregation. In experiments, HetaFlow achieves state-of-the-art resultson benchmark datasets in the vertex classification and clustering tasks. Hongjoon Ahn, Yongyi Yang, Quan Gan, Taesup Moon, and David Wipf. Descent steps of a relation-awareenergy produce heterogeneous graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,and Kyunghyun Cho (eds.), Advances in Neural Inf. Process. Syst., 2022. URL",
  "Michal Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphswith fast localized spectral filtering. CoRR, abs/1606.09375, 2016": "Yuxiao Dong, Nitesh V. Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning forheterogeneous networks. In Proc. Int. Conf. Knowl. Discovery Data Mining, pp. 135144, Halifax, NS,Canada, August 2017. ACM. doi: 10.1145/3097983.3098036. Shaohua Fan, Junxiong Zhu, Xiaotian Han, Chuan Shi, Linmei Hu, Biyu Ma, and Yongliang Li. Metapath-guided heterogeneous graph neural network for intent recommendation. In Proc. Int. Conf. Knowl. DiscoveryData Mining, pp. 24782486, Anchorage, AK, USA, August 2019. ACM. Tao-Yang Fu, Wang-Chien Lee, and Zhen Lei. Hin2vec: Explore meta-paths in heterogeneous informationnetworks for representation learning. In Proc. Conf. Inf. Knowl. Manage., pp. 17971806, Singapore, SG,Singapore, November 2017. ACM. doi: 10.1145/3132847.3132953.",
  "Xinyu Fu and Irwin King. Mecch: metapath context convolution-based heterogeneous graph neural networks.Neural Networks, 170:266275, 2024": "Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. MAGNN: metapath aggregated graph neural networkfor heterogeneous graph embedding. In The Web Conf. 2020, pp. 23312341, Taipei, Taiwan, April 2020.ACM / IW3C2. doi: 10.1145/3366423.3380297. William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. InProc. Conf. Neural Inf. Process. Syst., pp. 10241034, Long Beach, CA, USA, December 2017. NIPS. Huiting Hong, Hantao Guo, Yucheng Lin, Xiaoqing Yang, Zang Li, and Jieping Ye. An attention-based graphneural network for heterogeneous structural learning. In Proc. AAAI, volume 34, pp. 41324139, 2020.",
  "Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard S. Zemel. Gated graph sequence neural networks.In Proc. Int. Conf. Learn. Representations, San Juan, Puerto Rico, May 2016. Conf. Track Proc": "Minnan Luo, Feiping Nie, Xiaojun Chang, Yi Yang, Alexander G Hauptmann, and Qinghua Zheng. Adaptiveunsupervised feature selection with structure regularization. IEEE Trans. Neural Networks Learn. Syst.,29(4):944956, 2017. Qingsong Lv, Ming Ding, Qiang Liu, Yuxiang Chen, Wenzheng Feng, Siming He, Chang Zhou, Jianguo Jiang,Yuxiao Dong, and Jie Tang. Are we really making much progress? revisiting, benchmarking and refiningheterogeneous graph neural networks. In Proc. 27th ACM SIGKDD, pp. 11501160, 2021. Toms Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations invector space. In Proc. Int. Conf. Learn. Representations, Scottsdale, Arizona, USA, May 2013a. WorkshopTrack Proc. Toms Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed representationsof words and phrases and their compositionality. In Proc. Conf. Neural Inf. Process. Syst., pp. 31113119,Lake Tahoe, Nevada, USA, December 2013b. NIPS.",
  "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. InProc. Int. Conf. Knowl. Discovery Data Mining, pp. 701710, New York, NY, USA, August 2014. ACM": "Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling.Modeling relational data with graph convolutional networks. In Proc. 15th ESWC, pp. 593607, Heraklion,Crete, Greece, 2018. Springer. Jingbo Shang, Meng Qu, Jialu Liu, Lance M. Kaplan, Jiawei Han, and Jian Peng. Meta-path guidedembedding for similarity search in large-scale heterogeneous information networks. CoRR, abs/1610.09769,2016.",
  "Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. J. Mach. Learn. Res., 9(11):25792605, 2008": "Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio.Graph attention networks. In Proc. Int. Conf. Learn. Representations, Vancouver, BC, Canada, 2018.OpenReview.net. Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S. Yu. Heterogeneous graphattention network. In Proc. World Wide Web Conf., pp. 20222032, San Francisco, CA, USA, May 2019.ACM. doi: 10.1145/3308558.3313562. Carl Yang, Yuxin Xiao, Yu Zhang, Yizhou Sun, and Jiawei Han. Heterogeneous network representationlearning: A unified framework with survey and benchmark. IEEE Transactions on Knowledge and DataEngineering, 34(10):48544873, 2020.",
  "Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. Simple and efficient heterogeneousgraph neural network. In Proc. AAAI, volume 37, pp. 1081610824, 2023": "Yaming Yang, Ziyu Guan, Jianxin Li, Wei Zhao, Jiangtao Cui, and Quan Wang. Interpretable and efficientheterogeneous graph convolutional network. IEEE Transactions on Knowledge and Data Engineering, 35(2):16371650, 2021. Ka Yee Yeung and Walter L Ruzzo. Details of the adjusted rand index and clustering algorithms, supplementto the paper an empirical study on principal component analysis for clustering gene expression data.Bioinformatics, 17:763774, 2001.",
  "Chuxu Zhang, Dongjin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. Heterogeneous graphneural network. In Proc. 25th ACM SIGKDD, pp. 793803, 2019b": "Daokun Zhang, Jie Yin, Xingquan Zhu, and Chengqi Zhang. Metagraph2vec: Complex semantic pathaugmented heterogeneous network embedding. In Proc. Pacific-Asia Conf. Knowl. Discovery Data Mining,pp. 196208. Springer, 2018a. Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan: Gated attentionnetworks for learning on large and spatiotemporal graphs. In Proc. Conf. Uncertainty Artif. Intell., pp.339349, Monterey, California, USA, August 2018b. AUAI Press. Jiani Zhang, Xingjian Shi, Shenglin Zhao, and Irwin King. STAR-GCN: stacked and reconstructed graphconvolutional networks for recommender systems. In Proc. Int. Joint Conf. Artif. Intell., pp. 42644270,Macao, China, August 2019c. IJCAI. doi: 10.24963/ijcai.2019/592. Runwu Zhou, Xiaojun Chang, Lei Shi, Yi-Dong Shen, Yi Yang, and Feiping Nie. Person reidentificationvia multi-feature fusion with adaptive graph learning. IEEE Trans. Neural Networks Learn. Syst., 31(5):15921601, 2019.",
  ": An example HetaFlow network to model a given monomial 3S43S1S23": "Given a target polynomial Mi=1 biNj=1 Skijaij , we can construct the corresponding HetaFlow model bygenerating the equivalent output of each item in this polynomial and finally fuse them. The output ofone monomial can be achieved by concatenating several convolution layers and fusion layers, where everyconvolution layer uses a 31 filter and fusion layers take different fusion functions with respect to the graphshift operator to be used. Here we assume that S = A, the adjacency matrix, so fusion layers take sum as thefusion function (same fusion function if S is the Laplacian matrix; if use normalized adjacency matrix ornormalized Laplacian matrix, then fusion functions should be changed to average). Recall that Sn denotes the graph shift operator of the subgraph Gn, where Gn is the subgraph consisting ofall parallel flows with the edge type Tn R|R|. The proposition hypothesis assumes that there is no commonedge between the parallel flows of the same subgraph. After convolutions with the filter (1, 0, 1) alone eachparallel flow of the subgraph Gn, we can achieve the monomial Sn with a fusion layer to sum the outputof each parallel flow. For example, we can achieve the monomial 3S43S1S23 with the model shown in ,where assume the graph shift to be the adjacency matrix. For the general expression biNj=1 Skijaij , the i-th monomial of our target polynomial, we can choose the filter(1, 0, 1) and perform convolutions on the subgraph Sai1 by ki1 times, followed by convolutions on the subgraphSai2 by ki2 times and so on. HetaFlow repeats similar operations until finally obtaining Nj=1 Skijaij . Then bymultiplication with an appropriate coefficient, HetaFlow generates this monomial. The target polynomial isachieved by summing all the monomials. An explicit example of the whole model is shown in .",
  "Lemma 1. Suppose G = G1 G2. Then (G) (G1) + (G2)": "Proof. Assume that covers of G1 and G2 are given by two sets P = {P1, . . . , Pm} and Q = {Q1, . . . , Qn}respectively, where (G1) = |P| and (G2) = |Q|. Remove all common edges of the two sets from P toobtain R, where R = P if there does not exist any common edge. Then, the union R Q is a parallel flowdecomposition of G. Thus, (G) |R| + |Q| |P| + |Q| = (G1) + (G2) and the proof is complete.",
  "Since G = |R|i=1 Gi, the result follows from Lemma 1": "For a heterogeneous graph G = (V, E) with |A| vertex types and |R| edge types, the transformation layer t has|A|dtindtout learnable parameters, where dtin and dtout are the dimensions of the input and output embeddings,respectively. Suppose the graph G is decomposed into (G) parallel flows. Suppose that there are N convolution layers. Forconvolution layer n {1, 2, . . . , N}, assume that the length of the filter used is kn, and the dimensions of theinput and output embeddings are dnin and dnout, respectively. Since each convolution layer employs one filterfor each flow, a convolution layer n contains (G)kndnindnout learnable parameters. For each convolution layer,there exists one corresponding adaptive layer and two attention parts. According to our encoding method, thedimension of edge embeddings is 2|A| + |R|. To calculate the corresponding adaptive weights, the adaptivelayer employs four weight matrices whose shapes are dnin (2|A| + |R|), dnin 1, dnout |A| and dnout 1. Theattention mechanism introduces 2(dnin + dnout) parameters for each convolution layer n.",
  "CComputational complexity": "We adopt the same notations as in the previous subsection. Consider the convolution process in a parallel flowP in the convolution layer n. HetaFlow applies the convolution filter at most |VP| times, once at each vertex ofthe flow, where |VP| is the number of vertices contained in the parallel flow P. For each vertex, kn dot productsare computed. Thus, the computational complexity for flow P in the convolution layer n is O(|VP|kndnindnout).Let |Vmax| = maxP |VP|. The complexity for all parallel flows is bounded by O((G)|Vmax|kmaxdmaxindmaxout ). Compared with the convolution process, the vertex-level adjustment process requires one extra step tocalculate the weights. However, this can be omitted since the dimensions of edge-type embeddings are muchsmaller than the dimensions of vertex features. Following similar steps in the above discussion, the finalcomplexity of vertex-level adjustments is O((G)|Vmax|kmaxdmaxindmaxout ). As for other tasks, the complexities are O(|V|dtindtout) and O(|V|dpindpout) for transformation and predictionpart respectively, where layer t is the transformation layer and layer p is the prediction layer. Similarly, forpath-based adjustments, the complexity is O(|V|dooutdoout) after ignoring the extra step of weight calculation,where layer o is the layer that outputs features with the highest dimension.",
  "DImplementation details": "We employ the Adam optimizer (Kingma & Ba, 2015) to optimize the model. To find a suitable initialparameter setting, we test multiple initial hyperparameter settings, including: different dimensions of thesemantic-level features h from 26 to 21; several numbers of the attention head K including {8, 4, 3, 2, 1};different numbers of convolution layers from 1 to 4; and various dropout rates from 0.4 to 0.85 with a stepsize of 0.15; two learning rates including {0.01, 0.05}; two regularization parameters {0.001, 0.00005}. Wecompare with baselines from Lv et al. (2021) and Ahn et al. (2022). For ablation studies, we follow thesettings in Wang et al. (2019). During the robust test, only part of the training set is available, namely20%, 40%, 60%, 80%, while the random split of training, validation, and testing sets is fixed. Furthermore, weapply early stopping with a patience of 100.",
  "E.1Robustness test": "We test the robustness of HetaFlow by limiting the size of the training set and following the settings in Wanget al. (2019). The results are shown in Appendix 9. In general, HetaFlow outperforms the other baselines by13% and has comparable performance when it does not. We believe this is due to parallel flow decompositionsuffers less information loss than meta-path-based reconstruction methods.",
  "uN Pv(wPu + Auv)hu.(20)": "Furthermore, variants of multiplicative adjustment are possible. In our base model, we adjust features basedon the vertex types of the target vertex and its neighboring vertex during convolution, and the edge types oftheir connection. See (1), (4) and (5). This method provides detailed adjustments but has high computationalcosts. One vertex feature may be assigned with different adjustment weights in two different convolutions onthe same subgraph.",
  "uN PvwPu huP.(22)": "This method results in notable computational cost reduction. For instance, consider the example shown in. Vertex v11 needs four adjustment weights for the base model but requires only one adjustment weightfor this second approach. However, the second approach provides less detailed adjustments than our basemodel. We report the results obtained from the three datasets on vertex classification in . Each score (i.e.,either Macro-F1 or Micro-F1) is an average of the scores in different training proportions. Here HetaFlowbase isour base model, i.e., the one used to compete with other baselines in Appendix 9 and . Let HetaFlowwobe the variant model without utilizing vertex feature adjustments; HetaFlowflow be the variant that follows(21) and (22), which performs flow-based multiplicative adjustments; and HetaFlowadd be the model thatuses the additive adjustment in (20). All other settings are the same for these variants of HetaFlow.",
  "FDrop Token Examples": "Though the number of filters in HetaFlow does not scale with the size of the graph, the heterogeneity of agraph affects the number of parallel flows required. Since each parallel flow requires a separate filter, we mayneed to discard some edges to decrease the parallel flow number. We may need to make a trade-off betweenthe number of parallel flows and the number of omitted edges.",
  ": An example molecule graph. Different colors for edges indicate different types": "For example, consider the heterogeneous molecule graph shown in , which has one type of verticesand two types of edges. Assume that twelve black edges in this graph belong to edge type 1 while the rededge connecting v6 and v7 belong to edge type 2. To contain all edges in the parallel flow decomposition, wewill need two parallel flows. One parallel flow contains the path P1 = (v6, v7) while the other consists of twopaths P2 = (v1, v2, v6, v10, v9, v5, v1) and P3 = (v3, v4, v8, v12, v11, v7, v3). However, if we discard the type 2edge, then we only need one parallel flow. This means that by discarding the edge (v6, v7), we reduce the sizeof our model almost by half while only losing 7.7% of the edges. Research on methods that choose the edgesto be discarded is part of our future work. HetaFlows convolution filter assigns various weights to the neighbor vertices based on the position relationshipsbetween them and the central vertex. On the other hand, if one chooses the graph adjacency or Laplacianmatrix as the graph shift operator for a GCN, then a common weight is shared by the neighbors with thesame distance to the central vertex. In this regard, HetaFlow is much closer to CNN, which means it maybenefit from techniques developed for CNNs. Our future work includes investigations into this observation."
}