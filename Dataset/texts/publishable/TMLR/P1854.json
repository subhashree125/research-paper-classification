{
  "Abstract": "We present SOLO, a single transformer for Scalable visiOn-Language mOdeling. Current largevision-language models (LVLMs) such as LLaVA mostly employ heterogeneous architecturesthat connect pre-trained visual encoders with large language models (LLMs) to facilitatevisual recognition and complex reasoning. Although achieving remarkable performancewith relatively lightweight training, we identify four primary scalability limitations: (1)The visual capacity is constrained by pre-trained visual encoders, which are typically anorder of magnitude smaller than LLMs. (2) The heterogeneous architecture complicatesthe use of established hardware and software infrastructure. (3) Study of scaling laws onsuch architecture must consider three separate components visual encoder, connector,and LLMs, which complicates the analysis. (4) The use of existing visual encoders typicallyrequires following a pre-defined specification of image inputs pre-processing, for example, byreshaping inputs to fixed-resolution square images. This inflexibility can create bottlenecksand impede scalability. A unified single Transformer architecture, like SOLO, effectivelyaddresses these scalability concerns in LVLMs; however, its limited adoption in the moderncontext likely stems from the absence of reliable training recipes that balance both modalitiesand ensure stable training for billion-scale models. In this paper, we introduce the firstopen-source training recipe for developing SOLO, an open-source 7B LVLM with the singleTransformer architecture using moderate academic resources (8 x A100 80GB GPUs). Thetraining recipe involves initializing from LLMs, sequential pre-training on ImageNet and web-scale data, and instruction fine-tuning on our curated high-quality datasets. On extensiveevaluation, SOLO demonstrates performance comparable to LLaVA-v1.5-7B, particularlyexcelling in visual mathematical reasoning1.",
  "Introduction": "Large vision-language models (LVLMs) demonstrate remarkable performance on downstream tasks (Li et al.,2023c; Zhu et al., 2023; Liu et al., 2023c; Chen et al., 2023c; Kim & Ji, 2024). They can effectively extractvisual information (Wang et al., 2023b) and follow human instructions to generate insightful responses (Liet al., 2023b; Chen et al., 2024d). Two established approaches for vision-language modeling include: (1)Connecting pre-trained visual encoders (Dosovitskiy et al., 2021b; Radford et al., 2021) and large languagemodels (LLMs) (Touvron et al., 2023; Jiang et al., 2023) via a learned projection module that maps thevisual embeddings to the embedding space of LLMs (Dai et al., 2023; Gao et al., 2023; Liu et al., 2023c),or an intermediate symbolic layer Wang et al. (2024c). (2) Leveraging a pre-trained visual encoder to extractfeatures and aligning feature embeddings with a pre-defined codebook (Esser et al., 2021) to convert eachimage into a sequence of discrete visual tokens, thus enabling LVLMs to process both images and languagetokens (Wang et al., 2022b; Peng et al., 2022; Anil et al., 2023; Team, 2024; Diao et al., 2023).",
  "Published in Transactions on Machine Learning Research (11/2024)": "Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, JingrenZhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In International Conference on Machine Learning, ICML 2022, 17-23 July2022, Baltimore, Maryland, USA. Pmlr, 2022a. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais KhanMohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretrainingfor all vision and vision-language tasks. CoRR, abs/2208.10442, 2022b. doi: 10.48550/ARXIV.2208.10442.URL",
  "-billion parameter count, is initialized from Mistral LLM v0.1 (Jiang et al., 2023) and leverages its extensivepre-trained knowledge": "This modeling strategy is inspired by the foundational modeling framework of VisualBERT (Li et al.,2019) and industry efforts to scale unified LVLMs to the billion-scale (Bavishi et al., 2023). Despite thesimplicity and scalability, its limited contemporary adoption can be attributed to the lack of reliabletraining recipes, as balancing vision and language modalities in unified LVLMs often leads to trainingdivergence. This paper details the first open-source recipe for developing scalable unified LVLMs, usingmodest academic computational resources, specifically 8 NVIDIA A100 80GB GPUs (3). Our training recipeinvolves initializing with pre-trained LLMs, sequential pre-training on ImageNet and web-scale datasets,and instruction fine-tuning on our curated high-quality data mixture. While still lags behind recent state-of-the-art LVLMs on evaluation benchmarks, SOLO exhibits performanceon par with LLaVA-v1.5-7B (4) and the variant LLaVA-7B (6), which is created following our trainingrecipe in the controlled setting. In particular, SOLO distinguishes itself in the domain of visual mathematicalreasoning. Further scalability analysis reveals SOLOs better scaling behaviors, inference speed advantages,easier scaling laws analysis, and the scalability and benefits of our flexible image preprocessing pipeline(6.2). In addition, through comprehensive ablation studies, we validate the design choices of our trainingrecipe. Our empirical results confirm that the sequential pre-training on ImageNet and web-scale datasetsand instruction fine-tuning on our carefully curated data mixture are both essential for the training of suchsingle Transformer LVLMs (5). Interestingly, we find that after removing the first stage of pre-trainingon ImageNet, the LVLM will produce outputs of drastically different quality while exhibiting similarimage-conditioned language modeling loss (5.1, ).",
  "Scalability Limitations in Existing LVLMs": "The scalability constraints of existing LVLMs are currently articulated from four critical perspectives thatlimit their efficiency in utilizing expanded computational resources and larger datasets due to the bottlenecksin the system design: Fixed and Constrained Visual CapabilitiesThe fixed nature of visual encoders severely limits theadaptability of LVLMs to novel visual data distribution and more complex vision-language tasks since theseencoders are trained on specific distributions and training objectives. Current approaches address this issueby continuing the training of visual encoders (Bai et al., 2023) or by integrating features derived fromvarious visual encoders (Lin et al., 2023). Nonetheless, the scope of data used for continued pre-training issubstantially less than that used initially, which only marginally enhances encoder adaptability, and employingmultiple encoders complicates the process of image feature extraction, thereby impeding the scalability ofLVLMs. Moreover, the smaller scale of visual encoders compared to LLMs frequently results in the visualunderstanding component becoming a bottleneck. Consequently, visual representation learning is limited tothe smaller visual encoders, hindering the full utilization of LLM capabilities in existing LVLMs. Challenges in Efficient Training and DeploymentThe heterogeneous architecture with multiplecomponents complicates the implementation of machine learning systems for efficient training and deploymentsin terms of speed. (1) Training challenge: At large training scale (i.e., multi-node clusters), it is necessaryto distribute not only the Transformer-based LLMs but also the vision model and the MLP connector acrossmultiple devices, employing techniques such as tensor and pipeline parallelism. Thus, prevalent LVLMs cannotdirectly use existing industry-grade training frameworks optimized for the Transformer architecture (Shoeybiet al., 2019; Cano et al., 2023), thus necessitating the development of new tensor-sharding mechanisms. Inaddition, AI alignment typically employs algorithms such as Proximal Policy Optimization (PPO) (Schulmanet al., 2017), which necessitate simultaneously maintaining multiple models (e.g., reward and critic models)in GPU memory and cause difficulty in the algorithm implementations for heterogeneous architectures. (2)Deployment: The heterogeneous architecture complicates the deployment process due to similar model andtensor sharding challenges described above. Consequently, this hampers the large-scale services of existing",
  "Unified Vision-Language Modeling with Integrated Architectures": "We revisit the foundational modeling framework of VisualBERT (Li et al., 2019), initially proposed in theearly stages of research on pre-trained vision-language models. The key idea is to use one single Transformer,initialized from BERT (Devlin et al., 2018) in VisualBERT, to uniformly process the image patches andlanguage tokens. Fuyu-8B exemplifies the industrys effort to scale this modeling approach (Li et al., 2019)to billion-scale models (Bavishi et al., 2023). However, the limited widespread implementation of this unifiedarchitecture may be due to the lack of an established training recipe, as only the pre-trained model is releasedby Bavishi et al. (2023) without training details. Training such unified LVLMs presents significant challenges inbalancing the two modalities and maintaining stable training, for which clear solutions are currently lacking. Inthis paper, we present SOLO with full details of its unified and integrated architecture design and training recipe.",
  "SOLO: Scalable Vision-Language Modeling": "SOLO consolidates image and language capabilities into a single model, enables data-driven determination ofvisual representations and parameter allocation across visual and language modalities, simplifies the scalinglaws analysis, and allows it to handle high-resolution images and those with uncommon aspect ratios flexibly.For large-scale training (3.2), SOLO also seamlessly integrates with established software frameworks forlarge-scale Transformer pre-training (Shoeybi et al., 2019).",
  "Model Architecture": "The architecture of SOLO is shown in , which diverges from earlier models primarily in the extraction ofvisual features. Instead of resizing the image into a fixed resolution adapted to the pre-trained image encoders,SOLO keeps their original resolutions and aspect ratios. The feature extraction involves splitting the image intopatches with a pre-defined size. Through a trainable linear projection, these raw image patches (in pixels) aretransformed to obtain continuous embeddings that represent the visual features of the images. Thus, we canintegrate image and language processing within a single model. We maintain a list of special tokens designedexplicitly for visual modality encoding: <vision> and </vision> tokens mark the beginning and end ofa span of image patches respectively; <vrow_sep> acts as a row separator within the image patches and helpsthe model distinguish between different rows of image patches, aiding in structured visual understanding.",
  "Stage-2": "Capfusion (subset) (Yu et al., 2024)204, 97823, 681, 8646, 664, 351, 8631, 172, 726, 5051, 172, 726, 505Websight (Laurenon et al., 2024)71, 5791, 922, 6712, 300, 945, 2151, 087, 060, 5111, 213, 884, 704CC3M (Sharma et al., 2018b)32, 7602, 331, 4391, 064, 477, 31476, 092, 147988, 385, 167Detailed Captions (lz)6, 225368, 767202, 016, 77044, 788, 200157, 228, 570LLaVAR (Zhang et al., 2023b)3, 602422, 315117, 448, 78431, 390, 55686, 058, 228DVQA (Kafle et al., 2018)2, 917200, 00094, 853, 79655, 653, 79639, 200, 000OCR-VQA (Mishra et al., 2019)1, 593165, 74651, 920, 70521, 161, 01830, 759, 687FigureQA (Kahou et al., 2017)1, 526100, 00049, 586, 30524, 803, 25624, 783, 049SlimPajama, a different subset (Soboleva et al., 2023)120, 38504, 300, 998, 1614, 300, 998, 1610",
  "Stage-3": "ALLaVA-LAION (Chen et al., 2024a)13, 725438, 992442, 509, 490176, 660, 898265, 848, 592ALLaVA-VLFLAN (Xu et al., 2024b)4, 469207, 549144, 577, 37777, 835, 91966, 741, 458LLaVAR (Zhang et al., 2023b)3, 602422, 315117, 448, 78431, 390, 55686, 058, 228DVQA (Kafle et al., 2018)2, 917200, 00094, 853, 79655, 653, 79639, 200, 000FigureQA (Kahou et al., 2017)1, 526100, 00049, 586, 30524, 803, 25624, 783, 049SlimPajama, a different subset (Soboleva et al., 2023)12, 0850430, 688, 442430, 688, 4420",
  ": The input image resize algorithmto maintain the aspect ratio": "Formally, we define the patch size P and the maximal resolu-tion M. For an image of dimension size (W, H), it is resizedto (W , H) to ensure divisibility by P. details theresizing process, which adjusts the W and H to the nearestmultiples of P while preserving the original aspect ratio to theextent possible and complying with the constraints imposed byM. Subsequently, the image is divided into N patches, whereN = (W /P) (H/P), each with dimensions P P 3. Atrainable linear projector then maps each patch from a flattenedP P 3 vector to an output dimension compatible with theembedding space of LLMs, extracting N embeddings as theimages feature representation. These visual embeddings, alongwith special visual modality tokens and embeddings of the texttokens, are concatenated and processed through a single Trans-former, facilitating unified vision-language modeling. Notably,compared to prevalent LVLMs, this modeling strategy facilitatesa much earlier fusion of visual and language modalities, allowing LVLMs to extract relevant informationconditioned on the given instructions. In our implementation, we initialize SOLO from the Mistral-7B-v0.1base LLM. The max resolution M of processed images is set as 1024. The patch size P is set as 32.",
  "Pre-Training": "We introduce a three-stage pre-training curriculum that progressively enhances the visual capabilities ofLVLMs while preserving their fundamental language capabilities. We present datasets and their statistics foreach stage in Tab. 1. Stage-1 ImageNet Pre-Training for InitializationWe leverage ImageNet21K (Ridnik et al., 2021a),encompassing a broad spectrum of fine-grained visual categories, for the initial pre-training stage. In thisprocess, we train SOLO to predict only fine-grained labels in natural language tokens (class name of images,",
  "Instruction Fine-Tuning": "Dataset CurationWe meticulously select a diverse range of supervised datasets to perform instructionfine-tuning, aiming to enhance their performance across various domains of vision-language tasks. Ourdataset selection strategy is based on the empirical analysis derived in Laurenon et al. (2024); Lin et al.(2024); Lu et al. (2024), and is mainly driven by the objective to cover a comprehensive range of data types,including language-only data, detailed image captions, scientific documents, tables, documents, charts, OCRand text-rich images, and general visual question-answering (VQA) tasks. In A, we present datasets andtheir statistics in Tab. 7 and more details regarding data curation. Implementation DetailsWe utilize DeepSpeed (Rasley et al., 2020), as implemented in Accelerate (Guggeret al., 2022), for instruction fine-tuning. The choice to use Accelerate over Megatron for fine-tuning is",
  "Model": "We select various open-source LVLMs for comparison to better understand the capabilities of SOLO. Basedon the release time and capabilities of LVLMs, we select 3 groups of LVLMs to better understand thecurrent development phase of SOLO. Level-1 LVLMs represent the pioneering generation, which initiatethe integration of visual encoders with pre-trained LLMs, with releases prior to October 2023. Level-2LVLMs, released before early 2024, typically feature a more refined selection of instruction fine-tuning data toenhance performance. Level-3 marks the state-of-the-art (SoTA) LVLMs, released within the last five months,incorporating advanced training recipes, superior LLM backbones, and support for high-resolution images.",
  "Level-3: (10) Monkey (Li et al., 2024b). (11) LLaVA-NEXT (Liu et al., 2024b), (12) MiniCPM-v2 (Huet al., 2024b), (13) DeepSeek-VL (Lu et al., 2024)": "Each LVLM may have multiple variants based on different LLM sizes and architectures. If possible, we optfor the variant equipped with a 7B Mistral LLM. For the remaining LVLMs, we select the variant whoseconfiguration most closely aligns with our specifications (Mistral-7B-LLM). We directly present the evaluationresults of existing LVLMs from the leaderboard (OpenCompass) when available, to ensure a fair comparison.",
  "Benchmarks": "We select a wide range of benchmarks, encompassing both general vision-language tasks and specific task-oriented datasets, for evaluation and analysis. For general vision-language capability evaluation, we chooseMMStar (Chen et al., 2024b), MME (Fu et al., 2024), and SEED-Bench (Li et al., 2024a). Specifically, MMStarmeasures elite vision-indispensable capabilities, MME measures both the perception and cognition capabilities,and SEED-Bench covers 12 evaluation dimensions covering various aspects of LVLMs capabilities. For scientificdocument understanding, we choose AI2D (Kembhavi et al., 2016) and ScienceQA (Lu et al., 2022a). Forvisual mathematical reasoning, we choose MathVista (Lu et al., 2023). We adopt VLMEvalKit (Contributors,2023; Duan et al., 2024) to perform the unified evaluation.",
  "Results": "The experimental results are shown in Tab. 2. We find that SOLO significantly outperforms Level-1 LVLMs andalso performs comparably to Level-2 LVLMs, despite slightly underperforming Level-3 LVLMs. Furthermore,SOLO excels in task-oriented benchmarks, especially in areas requiring scientific knowledge and mathematicalreasoning, due to its successful integration of image representation and complex reasoning within a singleunified model. Overall, while SOLO does not yet meet the SoTA performance of the leading LVLMs (Level-3)within the prevalent multi-component LVLM framework, it marks a substantial progression in unifiedvision-language modeling. It is important to consider that SOLO is trained using limited academic resources,specifically 8 A100 GPUs. This is in sharp contrast with the resources used to produce SoTA LVLMs. Forexample, the technical report of DeepSeek-VL (Lu et al., 2024) mentions that DeepSeek-VL-7B is trained ona cluster of 64 nodes, each comprising 8 Nvidia A100 GPUs, totaling 512 GPUsthis represents a resource",
  "OursSOLOMistral-7B35.51260.064.473.334.461.4": "scale 64 times greater than that used for SOLO. SOLO serves as a pivotal model, showcasing the scientificvalue of training LVLMs with unified architectures. This establishes SOLO as a viable candidate for futuredevelopments aimed at closing the performance gap with SoTA LVLMs, with more flexibility and scalabilityby avoiding issues in prior architectures (2.1).",
  "LVLMs Generate Meaningless Captions without Stage-1 Pre-training": "We assess the necessity of Stage-1 pre-training by comparing the Stage-2 LVLM checkpoints with and withoutundergoing Stage-1 ImageNet pre-training. In , we observe that these two variants overall achievesimilar pre-training loss curves on vision-language modeling and (text) language modeling. Select Checkpoints for ComparisonWe select two checkpoints for comparison: one using caption-onlypre-training (Stage-2 only) and the other utilizing SOLOs two-stage pre-training, both of which achieve anequivalent vision-language modeling loss of 2.1. Qualitative ComparisonWe randomly select one example for qualitative analysis (in ). Despite theequivalent loss of the selected checkpoints in , we find that without ImageNet pre-training (Stage-2only), the model generates irrelevant and meaningless image captions, indicates a training divergence. Quantitative ComparisonWe perform a quantitative comparison on the same two checkpoints by trainingthem on the instruction fine-tuning data mixture for 800 steps (see a). Compared to the two-stagepre-trained SOLO, we observe a performance degradation across multiple benchmarks on the checkpointwithout Stage-1 pre-training, further validating the importance of the first stage. DiscussionWe hypothesize that discrepancies between a models vision and language capabilities can leadto the observed behaviors. Specifically, when there is a significant imbalancesuch as with the Mistral 7Bmodel, which possesses advanced language abilities but lacks vision understandingthe model may reduceloss by replicating caption patterns, including redundant text tokens irrelevant to the visual content. Forinstance, in a caption like This is a dog, the essential element is dog. Focusing solely on minimizinglanguage modeling loss without a robust initialized vision representation may lead the model to favor genericphrases like This is a\" over the more discriminative dog. This is because the former includes more tokens,disproportionately influencing the overall language modeling loss. Pre-training on ImageNet at Stage 1,which emphasizes predicting only the dog token, helps the model develop a solid visual representation,effectively narrowing the gap between vision and language capabilities and mitigating this issue. In addition,the results indicate that pre-training loss on vision-language data does not reliably indicate the performance of",
  "Step": "9 101 1.1 1001.2 1001.3 1001.4 1001.5 1001.6 1001.7 100Text LM Loss 1x Text Data2x Text Data3x Text Data : Stage 2 language modeling loss when trained on a mixture with different quantities of text data. 1xreflects the data mixture in Tab. 1, 2x and 3x represent mixtures with 2 or 3 times more text data comparedto 1x while keeping the amount of vision data unchanged. language modeling losses when conditioned on visual inputs, LVLMs exhibit markedly different behaviorsand performance across various downstream tasks. This contrasts with findings from pure language modeling,where pre-training loss strongly correlates with various downstream task performance (Du et al., 2024). We also demonstrate that the loss associated with the instruction fine-tuning data mixture does not reliablyindicate task performance. We train a variant of SOLO with a learning rate of 1e-4, deviating from theprescribed rate of 1e-5 in our recipe. We show the training curves (a) and the downstream performanceevaluation (b) of these two variants. The two variants exhibit similar training behaviors and losses onthe instruction fine-tuning data mixture, yet they display significant performance disparities in downstreamevaluation benchmarks. Overall, our analysis highlights the need to identify a dependable metric for evaluating LVLMs with theunified architecture in pre-training, particularly for establishing scaling laws in future research.",
  "Stage-2 Pre-Training on Web-Scale Data": "Stage-2 Pre-Training Improves Performance on Top of Stage-1We verify the effectiveness ofStage-2 pre-training on web-scale data by comparing the performance of two LVLMs. Each model is fine-tunedfor 800 steps using the same instruction fine-tuning data mixture but initialized differentlyone from theend of Stage 1 and the other from Stage 2. In b, we observe significant improvement on all evaluationdatasets after pre-training on web-scale data, showing the substantial advantages of Stage 2 pre-trainingcompared to solely using ImageNet data (Stage 1 only). Combining ImageNet and Captioning at Stage-2 Hurts PerformanceIn addition, it is pertinentto ask whether ImageNet21K data can be combined with web-scale data for Stage 2. We include an ablationwith SOLO trained on ImageNet21K and the web-scale data included in the second stage. illustrates thetraining curves for comparison. The results suggest that while ImageNet pre-training effectively establishes aninitial visual representation, it may not be optimal for subsequent Stage-2 pre-training on web-scale data, asit potentially impedes the optimization of vision-language modeling on image captions (i.e., vision languagemodeling loss stop improving). This discrepancy may arise from the divergence in image classification andcaptioning capabilities; the former is emphasized in the first stage. This two-stage approach aligns with theprinciples of continual curriculum learning, where the model must maintain proficiency in familiar taskswhile integrating new ones. This conclusion is also supported by our evaluation of downstream tasks (see",
  ": The evaluation performance of various ablations to validate key ingredients of our recipe. TheMME scores are normalized for better illustration": "c). We train two different checkpoints with and without ImageNet21K data in the second stage onthe instruction fine-tuning data mixture (3.2.2) for 800 steps. Note that we select two checkpoints withcomparable vision-language modeling losses for analysis. The results indicate that incorporating ImageNet21Kdata in the second stage may detrimentally impact overall performance by inhibiting adaptation to andlearning from web-scale data.",
  "Performance Boost via Instruction Fine-Tuning": "We evaluate the performance of SOLO on different training steps throughout the instruction fine-tuning stage(see d). The results indicate a consistent improvement in SOLOs performance with prolonged trainingon the fine-tuning dataset, although the MME scores exhibit some fluctuations. This outcome contrasts withthe findings of Liu et al. (2024a), where the performance quickly plateaus upon training with a limited subsetof the fine-tuning dataset. This illustrates the increased scalability of SOLO during the instruction fine-tuningstage, suggesting that acquiring additional high-quality supervised datasets for fine-tuning could consistentlyenhance performance.",
  ": The controlled analysis of Fuyu-8B andLLaVA-7B": "We conduct a controlled analysis to compare SOLOwith LLaVA and Fuyu (see ).SOLO withour training recipe consistently outperforms Fuyu-8B, which adopts the same unified modeling strat-egy, across all evaluation benchmarks. To facilitatea controlled comparison with LLaVA, we developLLaVA-7B, which integrates CLIP-ViT-336 andMistral-7B-base-v0.1, utilizing our specific trainingprocedure and data. The results reveal that LLaVA-7B achieves performance similar with LLaVA-v1.5-7B (Liu et al., 2024a), indicating that our trainingrecipe, which utilizes large-scale datasets and exten-sive training, may not significantly impact LLaVA-style LVLMs. Notably, while LLaVA-7B excels ingeneral visual-language tasks, SOLO demonstrates su-perior capabilities in visual mathematical reasoning,with overall performance being similar.",
  "Scalability Analysis": "Superior Scaling Properties of SOLOWe demonstrate that existing LVLMs with heterogeneous ar-chitectures exhibit diminishing returns despite increases in high-quality instruction fine-tuning data whileSOLO shows better scaling behaviors. We perform instruction fine-tuning on pre-trained LLaVA obtained in6.1 and compare its scaling behaviors with SOLO by measuring the performance improvement per trainingtoken. For evaluation, we fine-tune both models for 50 steps as their initial checkpoints, as their pre-trainedversions are not effective at following instructions. Given that both models are fine-tuned on the same dataset,we can directly compare their average improvement in benchmark performance per training token duringtraining. In the implementation, we measure performance improvement every 500 steps, normalized by thenumber of training tokens encountered during those steps, and average this to calculate the final metric. shows that SOLO outperforms mLLaVA on the performance improvement per token metric across allevaluation benchmarks. This suggests SOLO benefits more from high-quality instruction fine-tuning data anddemonstrates better scalability, indicating that its performance could further be improved more compared tomLLaVA with more data.",
  "LLaVA-Next0.0641LLaVA-Interleave0.0691Qwen2-VL0.0588SOLO0.0235": "SOLO Exhibits Training and Inference Speed AdvantageWe measure the training speed of SOLO compared to LVLMswith heterogeneous architectures by measuring throughput. Us-ing the same 8xA100 server, we compare the number of tokensprocessed per second during SOLO training and the officialLLaVA implementation (Liu et al., 2023a). Our SOLO imple-mentation achieves a throughput of 20K tokens per second,while LLaVA reaches 10.5K tokens per second, highlightingSOLOs significant advantage in training speed. We also measure the inference speed of SOLO compared toseveral LVLMs with heterogeneous architectures, including LLaVA-Next, LLaVA-Interleave, and Qwen-VL.The evaluation is conducted on a single A100 GPU using 10,000 COCO images and a consistent prompt:Generate the Caption for the <image>. All inference code is implemented using HuggingFace, and theinference latency is measured from the input being provided to the model and the output of the first token forfair comparison among all models since they may generate free-form responses in different lengths. The results,shown in Tab. 3, indicate that SOLO consistently outperforms other LVLMs with heterogeneous architecturesby a significant margin, demonstrating its clear advantage in inference speed and large-scale development.",
  "SOLO88.7592.3698.5997.7584.7096.55": "SOLO Facilitates Easier Scaling Laws Anal-ysisWe conduct a simplified scaling laws ex-periment to show that the performance gainsfrom scaling up data for SOLO is more pre-dictable compared to LLaVA-Style LVLMs.Specifically, we follow Kaplan et al. (2020) tofit the analytical function that LVLMs trainedwith a limited dataset (instruction fine-tuning in our case):",
  "D,": "where Dc and D are constants to be estimated, D is the number of training tokens, L(D) is the benchmarkperformance in our case. We use data points from the instruction fine-tuning of SOLO and mLLaVA tofit this function and report the coefficient of determination R2, which reflects the quality of the fit and isequivalent to the predictability of performance. The results below demonstrate that SOLO s performanceis more predictable compared to mLLaVA, indicating that SOLO facilitates easier scaling laws analysis. Wefurther provide more discussion about the better scalability of SOLO in 7.3.",
  "(b) The downstream performance of two variants": ": The training curves and downstream performance evaluation of two variants of SOLO. We find thatthey show significant performance differences although achieving a similar loss on the instruction fine-tuningdata mixture. SOLO Demonstrates Improved Performance when Scaling up Image ResolutionWe train SOLOon different image resolutions during the instruction fine-tuning stage for 1,000 steps due to the computelimits (see ). The image resolution during inference matches that used in the instruction fine-tuningstage. We find that SOLO continues to improve the performance with increasing image resolution, especiallyfor the visual mathematical reasoning task. In addition, there is no significant difference in performancebetween the 1024-square resolution and the adapted resolution with 1024 as the maximum used in SOLO. Thisdemonstrates the efficiency and scalability of our flexible image pre-processing pipeline.",
  "SOLO71.1932.02SOLO50.4228.13": "SOLOBenefitsfromitsDynamicHigh-Resolution CapabilityWe eval-uate LVLMs on high-resolution imagesand images with extreme aspect ratiosthat diverge from those found in natu-ral images.For a controlled analysis,we select two benchmark datasets (Sci-enceQA, MathVista) where SOLO andLLaVA-Next achieve comparable performance (within 1 absolute point). In addition, we implement mLLaVA,which follows LLaVAs modeling framework but is trained using SOLO s recipe for further comparison. Weselect subsets from the original benchmarks that include images with either a width or height exceeding 800pixels. Also, we select those subsets with a ratio (width/height) greater than 3 or less than 1/3 for extremeaspect ratio analysis. We evaluate LVLMs performance on these subsets. Our results, as shown in Tab. 5,verify SOLO s advantages regarding the performance on high-resolution images and images with extremeaspect ratios. The poorer performance of mLLaVA and LLaVA-Next is likely due to their heuristic resizingrules, which constrain images to predefined resolution settings. In contrast, SOLO retains the original aspectratios, which appears to be a more optimal approach.",
  ": The evaluation of language capability": "We find that on a 7B scale, balancing vision andtext capabilities can be challenging. Specifically, weobserve that during Stage-2 pre-training, despite theinclusion of text-only pre-training data (3.2.1) tomaintain the language capability of the original LLM,the language modeling loss on the language-only pre-training subset still steadily increases as trainingcontinues. In , we introduce a setting wherewe gradually increase the proportion of text-onlydata per batch (Tab. 1) and monitor the languagemodeling loss for text. The results suggest that aug-menting text data proportions does not alleviate therise in language modeling loss, indicating challengesin achieving balanced vision and text capabilities ina 7B-scale model. To further understand the degradation in language ability of SOLO, we evaluate SOLO on standard LLM evalu-ation benchmarks, including MMLU (Hendrycks et al., 2020), GSM8k (Cobbe et al., 2021), HellaSwag (Zellerset al., 2019), and RACE (Lai et al., 2017). Our analysis includes comparisons with the backbone LLMof SOLO, specifically Mistral-7B-v0.1-base, as well as Mistral-7B-v0.1-Instruct (see ). We observe adecline in language capabilities, particularly in knowledge-intensive benchmarks such as MMLU. There aretwo potential reasons: (1) Integrating vision capabilities may compromise language performance. (2) Thequality of Mistrals pre-training corpus is better than the open-source Slimpajama we employ. Overall, the",
  "Risk of error propagationLowHigher due to additional complexities": "current results indicate a limitation in the current version of SOLO, as effective performance in real-worldvision-language tasks often necessitates strong foundational language capabilities, including knowledge andreasoning. Thus, we plan to maintain the language capabilities of SOLO in the upcoming version by enrichingthe pre-training dataset with a higher-quality text corpus and increasing the proportion of text data.",
  "Scaling Laws Analysis with SOLO": "We demonstrate how SOLO s unified architecture facilitates easier analysis by comparing its experimentalplan for deriving scaling laws to those of heterogeneous-architecture LVLMs. To simplify the analysis process,we follow the standard experimental setup in Hoffmann et al. (2022) to proportionally scale up the modelsize and training tokens by a factor of 20.For SOLO, we can follow the standard scaling laws approach by selecting sampling models of varying sizes andassigning the appropriate number of training tokens based on the predefined scaling factor (e.g., 20 times).After training each model on its designated tokens, the pre-training loss can thus be measured to obtain the(expended FLOPs, performance) data point. The obtained data points can be used to fit the power law curvethat predicts the performance of the target model (e.g., a model with 70B parameters) given the FLOPsexpected to expend.In contrast, for LVLMs with heterogeneous architectures, such as LLaVA, scaling laws analysis introducesadditional complexity. Two key issues arise: (1) Parameter allocation: How should the models parametersbe distributed among the three components (visual encoder, connector, LLM)? (2) Consistency duringscaling: Will this parameter allocation (e.g., a 0.05:0.01:0.94 ratio) remain fixed as model size, data size, andcompute scale up? To address these, an additional parameter allocation law must be derived to predict theoptimal distribution of parameters based on model size and training tokens. In implementation, for each sizeof the sampling model, multiple configurations regarding parameter allocation must be tested to determinethe optimal pre-training loss and the corresponding best allocation, significantly increasing computationaldemands. Moreover, its challenging to figure out the best analytical form to map the model size and trainingtokens to the parameter allocation ratios. Consequently, the introduction of the parameter allocation lawcomplicates analysis and increases the risk of error propagation.",
  "Related Work": "Model ArchitectureExisting research advances the development of LVLMs capable of addressing di-verse tasks via a unified interface that can directly generate natural language, thus avoiding task-specificmodifications (Wang et al., 2021; 2022a; Li et al., 2023c). Utilizing pre-trained LLMs (Brown et al., 2020b;Bubeck et al., 2023) as the language component paired with pre-trained visual encoders (Radford et al.,2021; Dosovitskiy et al., 2021a), recent approaches further enhance the instruction-following, user-friendlyresponses generation, and complex reasoning ability of LVLMs (Liu et al., 2023c; Zhu et al., 2023; Daiet al., 2023; Alayrac et al., 2022; Li et al., 2023a; Ye et al., 2024). Concurrently, Wang et al. (2022b); Penget al. (2022); Anil et al. (2023); Team (2024); Ge et al. (2023) propose to further learn a codebook in theinitial stage to discretize the continuous embeddings extracted by visual encoders into a sequence of imagetokens. These approaches enable a uniform vision-language modeling strategy for image and language tokens.However, the dependence on pre-trained visual encoders restricts the scalability of LVLMs. In this study, weaddress this challenge by readopting the conventional vision-language modeling approach that utilizes a singleTransformer for both image and text processing (Li et al., 2019). Furthermore, while Bavishi et al. (2023)",
  "Training DataTypically, LVLMs leverage extensive image-caption pair datasets (Lin et al., 2014a;": "Schuhmann et al., 2021; 2022; Yu et al., 2024; Chen et al., 2023b) to train a projector or a codebook thatmap continuous image features into the embedding space of LLMs, thereby aligning the two modalities (Liet al., 2023c; Gong et al., 2023; Zeng et al., 2023; Sun et al., 2023a). Furthermore, large-scale vision-languageinstruction tuning datasets (Su et al., 2023; Wei et al., 2023; Liu et al., 2023b; Gong et al., 2023; Gaoet al., 2023; Li et al., 2023a) and feedback datasets (Li et al., 2023d; Sun et al., 2023b; Chen et al., 2024c;Zhang et al., 2024b) are utilized to further boost the fundamental capabilities of LVLMs and align LVLMswith human preferences, ensuring their ability to comprehend instructions and generate responses that areuser-friendly. In this work, we propose a recipe that encompasses the selection of pre-training and instructionfine-tuning datasets, along with corresponding multi-stage paradigms, to facilitate the training of billion-scaleLVLMs of a single Transformer architecture. Evaluation BenchmarksThe progress of LVLMs is guided and measured by the continuous developmentof evaluation benchmarks (Ferraro et al., 2015; Kafle et al., 2019; Gan et al., 2022; Chen et al., 2024d). Initially,evaluation primarily concentrates on fundamental visual-language skills, such as image captioning (Lin et al.,2014b; Plummer et al., 2015), basic visual information recognition (Antol et al., 2015; Goyal et al., 2017),compositional visual understanding (Hudson & Manning, 2019), and knowledge reasoning based on visualinformation (Marino et al., 2019; Schwenk et al., 2022). Current benchmarks are advancing to encompass moreintricate capabilities, requiring LVLMs to perform detailed visual analysis and complex reasoning (Uppal et al.,2022; Zhang et al., 2024a). These benchmarks range from general assessments across various domains andskills (Li et al., 2024a; Fu et al., 2024; Chen et al., 2024b; Yu et al., 2023) to specific tests targeting particularabilities, such as scientific document understanding (Kembhavi et al., 2016; Lu et al., 2022a), mathematicalreasoning (Lu et al., 2023; Wang et al., 2024a), multi-discipline understanding and reasoning (Yue et al.,2023; Wu et al., 2024), hallucination (Li et al., 2023e), and OCR ability (Liu et al., 2023d). In this work, weselect the advanced general and skill-specific benchmarks for evaluation.",
  "Conclusion": "This work revisits the simple vision-language modeling framework with a single Transformer. We argue thatthis approach effectively mitigates the scalability limitations inherent in prevailing models. With academicresources, we build SOLO, a 7B LVLM initialized from the Mistral LLM. We detail the training recipe andconduct extensive analysis and evaluation to validate the ingredients in our recipe. Experimental results showthat SOLO demonstrates performance comparable to LLaVA-v1.5, supporting the continued investigation intothis unified vision-language modeling approach for improved scalability.",
  "Limitations and Broader Impact Statement": "The investigation into large-scale vision-language modeling using a unified transformer architecture remainsnascent, with our model not yet reaching optimal performance across diverse benchmarks.Continuedadvancements in the direction of unified LVLMs for scalable vision-language modeling are anticipated.However, although developing LVLMs with strong capabilities brings significant advancements in AI, it alsoposes potential negative impacts. One concern is the risk of misuse, where the model could be employed formalicious purposes, such as generating misleading content that could manipulate public opinion or deceiveindividuals. Additionally, the model may inadvertently exacerbate biases present in the training data, leadingto unfair or discriminatory outcomes in decision-making processes.",
  "Acknowledgement": "We thank the reviewers and the action editor for their valuable suggestions and comments. This research isbased upon work supported by U.S. DARPA ECOLE Program No. HR00112390060. The views and conclusionscontained herein are those of the authors and should not be interpreted as necessarily representing the officialpolicies, either expressed or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized toreproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.",
  "Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 80768084, 2019": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shotlearning. arXiv preprint arXiv:2204.14198, 2022. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, JohanSchalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson,Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap,Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan,Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer,Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, MaximKrikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anas White, Anders Andreassen,Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski,and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. doi:10.48550/ARXIV.2312.11805. URL Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, andDevi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference oncomputer vision, pp. 24252433, 2015. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for traininglarge autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.",
  "Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and SanakTarlar. Introducing our multimodal models, 2023. URL": "Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Maral Rusinol, Ernest Valveny, CV Jawahar,and Dimosthenis Karatzas. Scene text visual question answering. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 42914301, 2019. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020a. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 2020b.",
  "virtual, June 19-25, 2021, pp. 1287312883. Computer Vision Foundation / IEEE, 2021.doi:10.1109/CVPR46437.2021.01268. URL": "Francis Ferraro, Nasrin Mostafazadeh, Lucy Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell,et al. A survey of current datasets for vision and language research. arXiv preprint arXiv:1506.06833, 2015. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation benchmark formultimodal large language models, 2024. Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, Jianfeng Gao, et al. Vision-language pre-training:Basics, recent advances, and future trends. Foundations and Trends in Computer Graphics and Vision,14(34):163352, 2022. Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, ConghuiHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprintarXiv:2304.15010, 2023.",
  "Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large languagemodel. arXiv preprint arXiv:2307.08041, 2023": "Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang,Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. arXivpreprint arXiv:2305.04790, 2023. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqamatter: Elevating the role of image understanding in visual question answering. In Proceedings of theIEEE conference on computer vision and pattern recognition, pp. 69046913, 2017. Sylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp Schmid, Zachary Mueller, Sourab Mangrulkar, MarcSun, and Benjamin Bossan. Accelerate: Training and inference at scale made simple, efficient and adaptable. 2022.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimallarge language models. arXiv preprint arXiv:2203.15556, 2022. Jinyi Hu, Yuan Yao, Chongyi Wang, Shan Wang, Yinxu Pan, Qianyu Chen, Tianyu Yu, Hanghao Wu, YueZhao, Haoye Zhang, et al. Large multilingual models pivot zero-shot multimodal learning across languages.arXiv preprint arXiv:2308.12038, 2023. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, YuxiangHuang, Weilin Zhao, Xinrong Zhang, Zhen Leng Thai, Kai Zhang, Chongyi Wang, Yuan Yao, ChenyangZhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, andMaosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies.CoRR, abs/2404.06395, 2024a. doi: 10.48550/ARXIV.2404.06395. URL Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, YuxiangHuang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalabletraining strategies. arXiv preprint arXiv:2404.06395, 2024b. Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning andcompositional question answering. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pp. 67006709, 2019. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023. Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations viaquestion answering. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 56485656, 2018.",
  "Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016": "Aniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi.Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension.In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pp. 49995007, 2017. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, andDavide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. Advancesin neural information processing systems, 33:26112624, 2020.",
  "Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modalmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a": "Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench-2:Benchmarking multimodal large language models. CoRR, abs/2311.17092, 2023b. doi: 10.48550/ARXIV.2311.17092. URL Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1329913308, 2024a.",
  "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating objecthallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023e": "Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and XiangBai. Monkey: Image resolution and text label are important things for large multi-modal models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2676326773,2024b. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-trainingfor visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 2668926699, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computervision, 2014a. Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In Computer Vision - ECCV 2014- 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V. Springer,2014b.",
  "Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next:Improved reasoning, ocr, and world knowledge, January 2024b. URL": "Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen,Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv preprintarXiv:2305.07895, 2023d. Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, ZhuoshuLi, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprintarXiv:2403.05525, 2024. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and Song-ChunZhu. Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. arXivpreprint arXiv:2110.13214, 2021. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science questionanswering. In NeurIPS, 2022a. Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, andAshwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.arXiv preprint arXiv:2209.14610, 2022b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-WeiChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundationmodels in visual contexts. arXiv preprint arXiv:2310.02255, 2023.",
  "lz. Detailed caption. URL": "Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual questionanswering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computervision and pattern recognition, pp. 31953204, 2019. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A benchmark forquestion answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Minesh Mathew, Viraj Bagal, Rubn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. In-fographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,pp. 16971706, 2022. Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual questionanswering by reading text in images. In 2019 international conference on document analysis and recognition(ICDAR), pp. 947952. IEEE, 2019.",
  "Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. Beit v2: Masked image modeling withvector-quantized visual tokenizers. CoRR, abs/2208.06366, 2022. doi: 10.48550/ARXIV.2208.06366. URL": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and SvetlanaLazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentencemodels. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting visionand language with localized narratives. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part V 16, pp. 647664. Springer, 2020.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from naturallanguage supervision. In International Conference on Machine Learning, 2021. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizationsenable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACMSIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020.",
  "Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor. Imagenet-21k pretraining for the masses,2021b": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, AarushKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimizationalgorithms. CoRR, abs/1707.06347, 2017. URL": "Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa:A benchmark for visual question answering using world knowledge. In European Conference on ComputerVision, pp. 146162. Springer, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.Conceptual captions: A cleaned,hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association forComputational Linguistics, 2018a.",
  "Techcrunch. Etched is building an AI chip that only runs one type of model. Accessed: 2024-07-06": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumder, Soujanya Poria, Roger Zimmer-mann, and Amir Zadeh. Multimodal research in vision and language: A review of current and emergingtrends. Information Fusion, 77:149171, 2022.",
  "Shujin Wu, Yi R Fung, Sha Li, Yixin Wan, Kai-Wei Chang, and Heng Ji. Macaroon: Training vision-languagemodels to be your engaged partners. arXiv preprint arXiv:2406.14137, 2024": "Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu,Maosong Sun, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images.arXiv preprint arXiv:2403.11703, 2024a. Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby, Ying Shen, Di Jin, Yu Cheng, Qifan Wang, and Lifu Huang.Vision-flan: Scaling human-labeled tasks in visual instruction tuning. arXiv preprint arXiv:2402.11690,2024b. Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang.mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1304013051, 2024. Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and JingjingLiu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1402214032, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, andLijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprintarXiv:2308.02490, 2023. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen,Ruobing Xie, Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprintarXiv:2404.02078, 2024. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal understanding andreasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023.",
  "Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu, and Dong Yu. Mm-llms: Recentadvances in multimodal large language models. arXiv preprint arXiv:2401.13601, 2024a": "Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, ShuangruiDing, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li,Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer:A vision-language large model for advanced text-image comprehension and composition. arXiv preprintarXiv:2309.15112, 2023a. Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar:Enhanced visual instruction tuning for text-rich image understanding. arXiv preprint arXiv:2306.17107,2023b. Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin,Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, and Jing Shao. Spa-vl: A comprehensive safety preferencealignment dataset for vision language model, 2024b.",
  "ADetails of Instruction-tuning Data Curation": "The curated instruction fine-tuning data mixture is shown in Tab. 7. Each category is chosen to addressspecific challenges and capabilities of SOLO. For instance, datasets like UltraInteract-SFT (Yuan et al., 2024)and CodeAct-General (Wang et al., 2024b) enable the refinement of language processing and reasoning abilities,while visually rich datasets such as LVIS-Instruct4V (Wang et al., 2023a) and Localized Narratives (Pont-Tusetet al., 2020) enhance the models basic image understanding and recognition abilities. Scientific documentdatasets like TQA (Kembhavi et al., 2017) are included to bolster the models ability to parse and reason withacademic visual information. Furthermore, OCR and text-heavy image datasets such as TextCaps (Sidorovet al., 2020) and OCR-VQA (Mishra et al., 2019) provide a crucial source for the models ability to interprettext within complex images. By selecting datasets with a broad range of complexities, sizes, and focuses, weensure a robust fine-tuning process that prepares SOLO to handle real-world applications effectively, reflectinga deep and detailed understanding of both vision and language data. Additionally, we conduct a thoroughmanual inspection and comparisons of the fine-tuning datasets, employing random sampling techniques onsome datasets such as DVQA (Kafle et al., 2018) and FigureQA (Kahou et al., 2017) to guarantee diversityand prevent data imbalance.",
  "CEffectiveness of Curated Data Mixture for Instruction Fine-Tuning": "We conduct an ablation study to validate the curated data mixture for instruction fine-tuning. The ablationsincluded are: (1) Without GPT-4V Data: All data generated by GPT-4V, including detailed captions andinstructional fine-tuning samples, is excluded from the fine-tuning mixture. (2) With Additional OCR Data:Additional OCR data from LLaVAR is incorporated into the fine-tuning mixture to enhance OCR capabilities,which are crucial for tasks like require extract text information from charts. (3) With More GPT-4V Data:Data from GPT-4V used in the third stage of pre-training is added to the fine-tuning mixture. (4) ExtendedTraining Duration: SOLO is trained for an additional epoch to investigate the effects of prolonged training. The results are presented in b. Our analysis indicates that incorporating additional OCR data doesnot significantly enhance performance in scientific document comprehension or visual mathematical reasoningtasks, which rely extensively on visual text understanding. This lack of improvement can be attributed tothe discrepancy between general OCR data and the specific demands of scientific charts. Identifying effectivemethods for collecting OCR data pertinent to scientific chart comprehension remains a critical area forfuture research. Furthermore, incorporating this OCR data seems to adversely affect overall visual-languagecapabilities, as demonstrated by general benchmarks. Regarding the use of GPT-4V data, our findings suggestthat a measured inclusion during the pre-training annealing stage enhances performance (see B), whereasexcessive incorporation during fine-tuning can hurt the overall performance. We also find that SOLO exhibitsminimal performance decline when trained solely on existing supervised datasets, excluding all data generatedby GPT-4V. This demonstrates that GPT-4V data is not essential for enhancing the core capabilities ofSOLO. The results overall justify the choice of datasets included in the supervised fine-tuning data mixture.However, although we observe a continual performance improvement across training steps within one epoch(see d), prolonged training on repetitive samples could lead to overfitting and decreased performance.This suggests that while extended exposure to diverse training data generally enhances model performance,overfitting remains a critical challenge when models are exposed repeatedly to a limited data subset. Overall,the ablation study proves the effectiveness of our curated data mixture."
}