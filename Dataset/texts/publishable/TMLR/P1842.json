{
  "Abstract": "In the dynamic field of digital content creation using generative models, state-of-the-artvideo editing models still do not offer the level of quality and control that users desire.Previous works on video editing either extended from image-based generative models in azero-shot manner or necessitated extensive fine-tuning, which can hinder the productionof fluid video edits. Furthermore, these methods frequently rely on textual input as theediting guidance, leading to ambiguities and limiting the types of edits they can perform.Recognizing these challenges, we introduce AnyV2V, a novel tuning-free paradigm designedto simplify video editing into two primary steps: (1) employing an off-the-shelf image editingmodel to modify the first frame, (2) utilizing an existing image-to-video generation modelto generate the edited video through temporal feature injection. AnyV2V can leverage anyexisting image editing tools to support an extensive array of video editing tasks, includingprompt-based editing, reference-based style transfer, subject-driven editing, and identitymanipulation, which were unattainable by previous methods. AnyV2V can also supportany video length. Our evaluation shows that AnyV2V achieved CLIP-scores comparable toother baseline methods. Furthermore, AnyV2V significantly outperformed these baselinesin human evaluations, demonstrating notable improvements in visual consistency with thesource video while producing high-quality edits across all editing tasks. The code is availableat",
  "Introduction": "The development of deep generative models (Ho et al., 2020) has led to significant advancements in contentcreation and manipulation, especially in digital images (Rombach et al., 2022; Nichol et al., 2022; Brookset al., 2023; Ku et al., 2024; Chen et al., 2023c; Li et al., 2023). However, video generation and editinghave not reached the same level of advancement as images (Wang et al., 2023; Chen et al., 2023a; 2024; Hoet al., 2022). In the context of video editing, training a large-scale video editing model presents considerablechallenges due to the scarcity of paired data and the substantial computational resources required. To overcome these challenges, researchers proposed various approaches (Geyer et al., 2023; Cong et al., 2023;Wu et al., 2023a;b; Liu et al., 2023a; Liang et al., 2023; Gu et al., 2023b; Cong et al., 2023; Zhang et al.,2023d; Qi et al., 2023; Ceylan et al., 2023; Yang et al., 2023; Jeong & Ye, 2023; Guo et al., 2023), whichcan be categorized into two types: (1) zero-shot adaptation from pre-trained text-to-image (T2I) modelsor (2) fine-tuned motion module from a pre-trained T2I or text-to-video (T2V) models. The zero-shotmethods (Geyer et al., 2023; Cong et al., 2023; Jeong & Ye, 2023; Zhang et al., 2023d) usually suffer fromflickering issues due to a lack of temporal understanding. On the other hand, fine-tuning methods (Wuet al., 2023a; Chen et al., 2023b; Wu et al., 2023b; Gu et al., 2023b; Guo et al., 2023) require more timeand computational overhead to edit videos. Moreover, all the methods can only adhere to certain typesof edits. For example, a user might want to perform edits on visuals that are out-of-distribution from thelearned text encoder (e.g. change a person to a character from their artwork). Thus, a highly customizable",
  "Subject Image": ": AnyV2V is an evolving framework to handle all types of video-to-video editing tasks without anyparameter tuning. AnyV2V disentangles video editing into two simpler problems: (1) Single image editingand (2) Image-to-video generation with video reference. With the above motivations, we aim to develop a video editing framework that requires no fine-tuning andcaters to user demand. In this work, we present AnyV2V, designed to enhance the controllability of zero-shotvideo editing by decomposing the task into two pivotal stages:",
  ". Leverage the innate knowledge of the image-to-video (I2V) model to generate the edited video withthe edited first frame, source video latent, and the intermediate temporal features": "Our objective is to propagate the edited first frame across the entire video while ensuring alignment with thesource video. To achieve this, we employ I2V models (Zhang et al., 2023c; Chen et al., 2023d; Ren et al.,2024) for DDIM inversion to enable first-frame conditioning. With the inverted latents as initial noise and themodified first frame as the conditional signal, the I2V model can generate videos that are not only faithful tothe edited first frame but also follow the appearance and motion of the source video. To further enforce theconsistency of the appearance and motion with the source video, we perform feature injection in the I2Vmodel. The two-stage editing process effectively offloads the editing operation to existing image editing tools.This design (detail in ) helps AnyV2V excel in:",
  "Compatibility: It provides a highly customized interface for a user to perform video edits on anymodality. It can seamlessly integrate any image editing methods to perform diverse editing": "Simplicity: It does not require any fine-tuning nor additional video features like previous works (Guet al., 2023b; Wu et al., 2023b; Ouyang et al., 2023) to achieve high appearance and temporalconsistency for video editing tasks. From our findings, without any fine-tuning, AnyV2V can perform video editing tasks beyond the scope ofcurrent publicly available methods, such as reference-based style transfer, subject-driven editing, and identitymanipulation. AnyV2V also can perform prompt-based editing and achieve superior results on commonvideo editing evaluation metrics compared to the baseline models (Geyer et al., 2023; Cong et al., 2023; Wuet al., 2023b). We show both quantitatively and qualitatively that our method outperforms existing SOTA",
  "Published in Transactions on Machine Learning Research (11/2024)": "motion-consistent videos when performing subject-driven object swapping. In the first example, AnyV2Vsuccessfully replaces the cat with a dog according to the reference image and maintains highly aligned motionand background as reflected in the source video. In the second example, the car is replaced by our desiredcar while maintaining the rotation angle in the edited video. Identity ManipulationBy integrating the identity-preserved image personalization model Instan-tID (Wang et al., 2024b) with ControlNet (Zhang et al., 2023b), this approach enables the replacementof an individuals identity to create an initial frame. Our AnyV2V framework then processes this initialframe to produce an edited video, swapping the persons identity as showcased in . To the best ofour knowledge, our work is the first to provide such flexibility in the video editing models. Note that theInstantID with ControlNet method will alter the background due to its model property. It is possible toleverage other identity-preserved image personalization models and apply them to AnyV2V to preserve thebackground.",
  "Related Works": "Video generation has attracted considerable attention within the field (Chen et al., 2023a; 2024; OpenAI; Wanget al., 2023; Hong et al., 2022; Zhang et al., 2023a; Henschel et al., 2024; Wang et al., 2024c; Xing et al., 2023;Chen et al., 2023d; Bar-Tal et al., 2024; Ren et al., 2024; Zhang et al., 2023c). However, video manipulationalso represents a significant and popular area of interest. Initial attempts, such as Tune-A-Video (Wu et al.,2023b) and VideoP2P (Liu et al., 2023b) involved fine-tuning a text-to-image model to achieve video editingby learning the continuous motion. The concurrent works at that time such as Pix2Video (Ceylan et al.,2023) and Fate-Zero (Qi et al., 2023) go for zero-shot approach, which leverages the inverted latent from atext-to-image model to retain both structural and motion information. The progressively propagates to otherframes edits. Subsequent developments have enhanced the results but generally follow the two paradigms(Geyer et al., 2023; Wu et al., 2023a; Cong et al., 2023; Yang et al., 2023; Ceylan et al., 2023; Ouyang et al.,2023; Guo et al., 2023; Gu et al., 2023b; Esser et al., 2023; Chen et al., 2023b; Jeong & Ye, 2023; Zhanget al., 2023d; Cheng et al., 2023). Control-A-Video (Chen et al., 2023b) and ControlVideo (Zhang et al.,2023d) leveraged ControlNet (Zhang et al., 2023b) for extra spatial guidance. TokenFlow (Geyer et al., 2023)leveraged the nearest neighbor field and inverted latent to achieve temporally consistent edit. Fairy (Wuet al., 2023a) followed both paradigms which they fine-tuned a text-to-image model and also cached theattention maps to propagate the frame edits. VideoSwap (Gu et al., 2023b) requires additional parametertuning and video feature extraction (e.g. tracking, point correspondence, etc) to ensure appearance andtemporal consistency. CoDeF (Ouyang et al., 2023) allows the first image edit to propagate the other frameswith one-shot tuning. UniEdit (Bai et al., 2024) leverages the inverted latent and feature maps injection toachieve a wide range of video editing with a pre-trained text-to-video model (Wang et al., 2023). However, none of the methods can offer precise control to users, as the edits may not align with the usersexact intentions or desired level of detail, often due to the ambiguity of natural language and the constraintsof the models capabilities. For example, VideoP2P (Liu et al., 2023a) is restricted to only word-swappingprompts due to the reliance on cross-attention. There is a clear need for a more precise and comprehensivesolution for video editing tasks. Our work AnyV2V is the first work to empower a diverse array of videoediting tasks. We compare AnyV2V with the existing methods in . As can be seen, our method excelsin its applicability and compatibility.",
  "Image-to-Video (I2V) Generation Models": "In this work, we focus on leveraging latent diffusion-based (Rombach et al., 2022) I2V generation modelsfor video editing. Given an input first frame I1, a text prompt s and a noisy video latent zt at time step t,I2V generation models recover a less noisy latent zt1 using a denoising model (zt, I1, s, t) conditioned onboth I1 and s. The denoising model contains a set of spatial and temporal self-attention layers, where theself-attention operation can be formulated as:",
  "d)V,(2)": "where z is the input hidden state to the self-attention layer and W Q, W K and W V are learnable projectionmatrices that map z onto query, key and value vectors, respectively. For spatial self-attention, z represents asequence of spatial tokens from each frame. For temporal self-attention, z is composed of tokens located atthe same spatial position across all frames.",
  "d ) in a text-to-image (T2I)denoising U-Net capture the semantic regions (e.g. legs or torso of a human body) during the image generationprocess": "Given an input source image IS and a target prompt P, PnP first performs DDIM inversion to obtain theimages corresponding noise {zSt }Tt=1 at each time step t. It then collects the convolution features {f lt} andattention scores {Alt} from some predefined layers l at each time step t of the backward diffusion processzSt1 = (zSt , , t), where denotes the null text prompt during denoising. To generate the edited image I, PnP starts from the initial noise of the source image (i.e. zT = zST ) andperforms feature injection during denoising: zt1 = (zt , P, t, {f lt, Alt}), where (, , , {f lt, Alt}) represents",
  "Style Image": ": AnyV2V takes a source video V S as input. In the first stage, we apply a block-box image editingmethod on the first frame I1 according to the editing task. In the second stage, the source video is invertedto initial noise zST , which is then denoised using DDIM sampling. During the sampling process, we extractspatial convolution, spatial attention, and temporal attention features from the I2V models decoder layers.To generate the edited video, we perform a DDIM sampling by fixing zT as zTT and use the edited first frameas the conditional signal. During sampling, we inject the features and attention into corresponding layers ofthe model. the operation of replacing the intermediate feature and attention scores {f lt , Alt } with {f lt, Alt}. This featureinjection mechanism ensures I to preserve the layout and structure from IS while reflecting the descriptionin P. To control the feature injection strength, PnP also employs two thresholds f and A such that thefeature and attention scores are only injected in the first f and A denoising steps. Our method extends thisfeature injection mechanism to I2V generation models, where we inject features in convolution, spatial, andtemporal attention layers. We show the detailed design of AnyV2V in .",
  "AnyV2V": "Our method presents a two-stage approach to video editing. Given a source video V S = {I1, I2, I3, ..., In},where Ii is the frame at time i and n denotes the video length, we first extract the initial frame I1 andpass it into an image editing model img to obtain an edited first frame I1 = img(I1, C). C is the auxiliaryconditions for image editing models such as text prompt, mask, style, etc. In the second stage, we feed theedited first frame I1 and a target prompt s into an I2V generation model and employ the inverted latentfrom the source video V S to guide the generation process such that the edited video V follows the motion ofthe source video V S, the semantic information represented in the edited first frame I1, and the target prompts. An overall illustration of our video editing pipeline is shown in . In this section, we explain eachcore component of our method.",
  "Flexible First Frame Editing": "In visual manipulation, controllability is a key element in performing precise editing. AnyV2V enables morecontrollable video editing by utilizing image editing models to modify the videos first frame. This strategicapproach enables highly accurate modifications in the video and is compatible with a broad spectrum of imageediting models, including other deep learning models that can perform image style transfer (Gatys et al.,2015; Ghiasi et al., 2017; Ltzsch et al., 2022; Wang et al., 2024a), mask-based image editing (Nichol et al.,",
  "Structural Guidance using DDIM Inverison": "To ensure the generated videos from the I2V generation model follow the general structure as presented inthe source video, we employ DDIM inversion to obtain the latent noise of the source video at each time stept. Specifically, we perform the inversion without text prompt condition but with the first frame condition.Formally, given a source video V S = {I1, I2, I3, ..., In}, we obtain the inverted latent noise for time step t as:",
  "zSt = DDIM_Inv((zt+1, I1, , t)),(3)": "where DDIM_Inv() denotes the DDIM inversion operation as described in Appendix 3. In ideal cases, thelatent noise zST at the final time step T (initial noise of the source video) should be used as the initial noisefor sampling the edited videos. In practice, we find that due to the limited capability of certain I2V models,the edited videos denoised from the last time step are sometimes distorted. Following Li et al. (2023), weobserve that starting the sampling from a previous time step T < T can be used as a simple workaround tofix this issue.",
  "Appearance Guidance via Spatial Feature Injection": "Our empirical observation (.4) suggests that I2V generation models already have some editingcapabilities by only using the edited first frame and DDIM inverted noise as the model input. However, wefind that this simple approach is often unable to correctly preserve the background in the edited first frameand the motion in the source video, as the conditional signal from the source video encoded in the invertednoise is limited. To enforce consistency with the source video, we perform feature injection in both convolution layers andspatial attention layers in the denoising U-Net. During the video sampling process, we simultaneously denoisethe source video using the previously collected DDIM inverted latents zSt at each time step t such thatzSt1 = (zSt , I1, , t). We preserve two types of features during source video denoising: convolution featuresf l1 before skip connection from the lth1 residual block in the U-Net decoder, and the spatial self-attentionscores {Al2s } from l2 = {llow, llow+1, ..., lhigh} layers. We collect the queries {Ql2s } and keys {Kl2s } insteadof directly collecting Al2s as the attention score matrices are parameterized by the query and key vectors.We then replace the corresponding features during denoising the edited video in both the normal denoisingbranch and the negative prompt branch for classifier-free guidance (Ho & Salimans, 2022). We use twothresholds conv and sa to control the convolution and spatial attention injection to only happen in the firstconv and sa steps during video sampling.",
  "Motion Guidance through Temporal Feature Injection": "The spatial feature injection mechanism described in .3 significantly enhances the background andoverall structure consistency of the edited video. While it also helps maintain the source video motion tosome degree, we observe that the edited videos will still have a high chance of containing incorrect motioncompared to the source video. On the other hand, we notice that I2V generation models, or video diffusionmodels in general, are often initialized from pre-trained T2I models and continue to be trained on video data.During the training process, parameters in the spatial layers are often frozen or set to a lower learning ratesuch that the pre-trained weights from the T2I model are less affected, and the parameters in the temporallayers are more extensively updated during training. Therefore, it is likely that a large portion of the motioninformation is encoded in the temporal layers of the I2V generation models. Concurrent work (Bai et al.,2024) also observes that features in the temporal layers show similar characteristics with optical flow (Horn& Schunck, 1981), a pattern that is often used to describe the motion of the video. To better reconstruct the source video motion in the edited video, we propose to also inject the temporalattention features in the video generation process. Similar to spatial attention injection, we collect the source",
  "zt1 = (zt , I, s, t ; {f l1, Ql2s , Kl2s , Ql3t , Kl3t }),(4)": "where ( ; {f l1, Ql2s , Kl2s , Ql3t , Kl3t }) denotes the feature replacement operation across different layers l1, l2, l3.Our proposed spatial and temporal feature injection scheme enables tuning-free adaptation of I2V generationmodels for video editing. Our experimental results demonstrate that each component in our design is crucialto the accurate editing of source videos. We showcase more qualitative results for the effectiveness of ourmodel components in .",
  "Implementation Details": "We employ AnyV2V on three off-the-shelf I2V generation models: I2VGen-XL1 (Zhang et al., 2023c),ConsistI2V (Ren et al., 2024) and SEINE (Chen et al., 2023d). For all I2V models, we use conv = 0.2T,sa = 0.2T and ta = 0.5T, where T is the total number of sampling steps. We use the DDIM (Song et al.,",
  ": With different image editing models, AnyV2V can achieve a wide range of editing tasks, includingreference-based style transfer, subject-driven editing, and identity manipulation": "2020) sampler and set T to the default values of the selected I2V models. Following PnP (Tumanyan et al.,2023b), we set l1 = 4 for convolution feature injection and l2 = l3 = {4, 5, 6, ..., 11} for spatial and temporalattention injections. During sampling, we apply text classifier-free guidance (CFG) (Ho & Salimans, 2022) forall models with the same negative prompt Distorted, discontinuous, Ugly, blurry, low resolution, motionless,static, disfigured, disconnected limbs, Ugly faces, incomplete arms across all edits. To obtain the initial editedframes in our implementation, we use a set of image editing model candidates including prompt-based imageediting model InstructPix2Pix (Brooks et al., 2023), style transfer model Neural Style Transfer (NST) (Gatys",
  "AnyV2V (SEINE)28.9%8.3%0.29100.9631AnyV2V (ConsistI2V)33.8%11.7%0.28960.9556AnyV2V (I2VGen-XL)69.7%46.2%0.29320.9652": "et al., 2015), subject-driven image editing model AnyDoor (Chen et al., 2023c), and identity-driven imageediting model InstantID (Wang et al., 2024b). We experiment with only the successfully edited frames, whichis crucial for our method. We conducted all the experiments on a single Nvidia A6000 GPU. To edit a16-frame video, it requires around 15G GPU memory and around 100 seconds for the whole inference process.We refer readers to Appendix A for more discussions on our implementation details and hyperparametersettings.",
  "Tasks Definition": "1. Prompt-based Editing: allows users to manipulate video content using only natural language.This can include descriptive prompts or instructions. With the prompt, Users can perform a widerange of edits, such as incorporating accessories, spawning or swapping objects, adding effects, oraltering the background. 2. Reference-based Style Transfer: In the realm of style transfer tasks, the artistic styles of Monetand Van Gogh are frequently explored, but in real-life examples, users might want to use a distinctstyle based on one particular artwork. In reference-based style transfer, we focus on using a styleimage as a reference to perform video editing. The edited video should capture the distinct style ofthe referenced artwork. 3. Subject-driven Editing: In subject-driven video editing, we aim at replacing an object in thevideo with a target subject based on a given subject image while maintaining the video motion andpersevering the background. 4. Identity Manipulation: Identity manipulation allows the user to manipulate video content byreplacing a person with another persons identity in the video based on an input image of the targetperson.",
  "Evaluation Results": "As shown in and 4, AnyV2V can perform the following video editing tasks: (1) prompt-based editing,(2) reference-based style transfer, (3) subject-driven editing, and (4) identity manipulation. we compareAnyV2V against three baseline models Tune-A-Video (Wu et al., 2023b), TokenFlow (Geyer et al., 2023) andFLATTEN (Cong et al., 2023) for (1) prompt-based editing. Since there exists no publicly available baselinemethod for task (2) (3) (4), we evaluate the performance of three I2V generation models under AnyV2V. Weincluded a more comprehensive evaluation in Appendix B. Prompt-based EditingUnlike the baseline methods (Wu et al., 2023b; Geyer et al., 2023; Cong et al.,2023) that often introduce unwarranted changes not specified in the text commands, AnyV2V utilizes theprecision of image editing models to ensure only the targeted areas of the scene are altered, leaving the restunchanged. Combining AnyV2V with the instruction-guided image editing model InstructPix2Pix (Brookset al., 2023), AnyV2V accurately places a party hat on an elderly mans head and correctly paints the plane",
  "Frame 1Frame 9Frame 17Frame 25Frame 97Frame 105Frame 113Frame 121": ": AnyV2V can edit video length beyond the training frame while maintaining motion consistency.The first row is the source video frames while the second rows are the edited. The editing prompt of theimage was turn woman into a robot using image model InstructPix2Pix (Brooks et al., 2023). in blue. Additionally, it maintains the original videos background and fidelity, whereas, in comparison,baseline methods often alter the color tone and shape of objects, as illustrated in . Also, for motiontasks such as adding snowing weather, I2V models from AnyV2V provide inherent support for animating thesnowing while the baseline methods would result in flickering. For quantitative evaluations, we conduct ahuman evaluation to examine the degree of prompt alignment and overall preference of the edited videosbased on user voting, and also compute the metrics CLIP-Text for text alignment and CLIP-Image fortemporal consistency. In detail, CLIP-Text is computed by the average cosine similarity between textembeddings from CLIP model (Radford et al., 2021) and CLIP-Image is computed in the same way butbetween image embeddings for every pair of consecutive frames. shows that AnyV2V generallyachieves high text-alignment and temporal consistency, while AnyV2V with I2VGen-XL backbone is themost preferred method because it does not edit the video precisely. Style Transfer, Subject-Driven Editing and Identity ManipulationFor these novel tasks, we stressthe alignment with the reference image instead of the text prompt. As shown in , we can observethat for task (2), AnyV2V can capture one particular style that is tailor-made, and even if the style is notlearned by the text encoder. In the examples, AnyV2V captures the style of Vassily Kandinskys artworkComposition VII and Vincent Van Goghs artwork Chateau in Auvers at Sunset accurately. For task (3),AnyV2V can replace the subject in the video with other subjects even if the new subject differs from theoriginal subject. In the examples, a cat is replaced with a dog according to the reference image and maintainshighly aligned motion and background as reflected in the source video. A car is replaced by our desired carwhile the wheel is still spinning in the edited video. For task (4), AnyV2V can swap a persons identity toanyone. We report both human evaluation results and find that AnyV2V with I2VGen-XL backbone is themost preferred method in terms of reference alignment and overall performance, plotted in , which isin Appendix B. I2V BackbonesWe find that AnyV2V (I2VGen-XL) tends to be the most robust both qualitatively andquantitatively. It has a good generalization ability to produce consistent motions in the video with highvisual quality. AnyV2V (ConsistI2V) can generate consistent motion, but sometimes the watermark wouldappear due to its training data, thus harming the visual quality. AnyV2V (SEINE)s generalization ability isrelatively weaker but still produces consistent and high-quality video if the motion is simple enough, such asa person walking.",
  ": Visual comparisons of AnyV2Vs editing results after disabling temporal feature injection (T.I.),spatial feature injection (S.I.) and DDIM inverted initial noise (D.I.)": "Editing Video beyond Training Frames of I2V model Current state-of-the-art I2V models (Chen et al.,2023d; Ren et al., 2024; Zhang et al., 2023c) are mostly trained on video data that contains only 16 frames.To edit videos that have length beyond the training frames of the I2V model, an intuitive approach would begenerating videos in an auto-regressive manner as used in ConsistI2V (Ren et al., 2024) and SEINE (Chenet al., 2023d). However, we find that such an experiment setup cannot maintain semantic consistency in ourcase. As many works in extending video generation exploit the initial latent to generate longer video (Qiuet al., 2023; Wu et al., 2023c), we leverage the longer inverted latent as the initial latent and force anI2V model to generate longer frames of output. Our experiments found that the inverted latent containsenough temporal and semantic information to allow the generated video to maintain temporal and semanticconsistency, as shown in .",
  "Ablation Study": "To verify the effectiveness of our design choices, we conduct an ablation study by iteratively disabling thethree core components in our model: temporal feature injection, spatial feature injection, and DDIM invertedlatent as initial noise. We use AnyV2V (I2VGen-XL) and a subset of 20 samples in this ablation study andreport both the frame-wise consistency results using CLIP-Image score in and qualitative comparisonsin . We provide more ablation analysis of other design considerations of our model in the Appendix. Effectiveness of Temporal Feature Injection According to the results, after disabling temporal featureinjection in AnyV2V (I2VGen-XL), while we observe a slight increase in the CLIP-Image score value, theedited videos often demonstrate less adherence to the motion presented in the source video. For example,in the second frame of the couple sitting case (3rd row, 2nd column in the right panel in ), themotion of the woman raising her leg in the source video is not reflected in the edited video without applying",
  "AnyV2V (I2VGen-XL)0.9648AnyV2V (I2VGen-XL) w/o T. Injection0.9652AnyV2V (I2VGen-XL) w/o T. Injection & S. Injection0.9637AnyV2V (I2VGen-XL) w/o T. Injection & S. Injection & DDIM Inversion0.9607": "temporal injection. On the other hand, even when the style of the video is completely changed, AnyV2V(I2VGen-XL) with temporal injection is still able to capture this nuance motion in the edited video. Effectiveness of Spatial Feature Injection As shown in , we observe a drop in the CLIP-Imagescore after removing the spatial feature injection mechanisms from our model, indicating that the editedvideos are not smoothly progressed across consecutive frames and contain more appearance and motioninconsistencies. Further illustrated in the third row of , removing spatial feature injection will oftenresult in incorrect subject appearance and pose (as shown in the ballet dancing case) and degeneratedbackground appearance (evident in the couple sitting case). These observations demonstrate that directlygenerating edited videos from the DDIM inverted noise is often not enough to fully preserve the source videostructures, and the spatial feature injection mechanisms are crucial for achieving better editing results. DDIM Inverted Noise as Structural Guidance Finally, we observe a further decrease in CLIP-Imagescores and a significantly degraded visual appearance in both examples in after replacing the initialDDIM inverted noise with random noise during sampling. This indicates that the I2V generation modelsbecome less capable of animating the input image when the editing prompt is completely out-of-domain andhighlights the importance of the DDIM inverted noise as the structural guidance of the edited videos.",
  "Conclusion": "In this paper, we presented AnyV2V, a novel unified framework for video editing. Our framework is training-free, highly cost-effective, and can be applied to any image editing model and I2V generation model. Toperform video editing with high precision, we propose a two-stage approach to first edit the initial frame ofthe source video and then condition an I2V model with the edited first frame and the source video featuresand inverted latents to produce the edited video at any length. Comprehensive experiments have shownthat our method achieves outstanding outcomes across a broad spectrum of applications that are beyondthe scope of existing SOTA methods while achieving superior results on both common video metrics andhuman evaluation. For future work, we aim to find a tuning-free method to bridge the I2V properties intoT2V models so that we can leverage existing strong T2V models for video editing.",
  "Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J Mitra. Pix2video: Video editing using image diffusion.2023": "Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, YaofangLiu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality videogeneration. arXiv preprint arXiv:2310.19512, 2023a. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan.Videocrafter2: Overcoming data limitations for high-quality video diffusion models.arXiv preprintarXiv:2401.09047, 2024.",
  "Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang, JianmingZhang, HyunJoon Jung, and Xin Eric Wang. Photoswap: Personalized subject swapping in images, 2023a": "Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David JunhaoZhang, Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactivesemantic point correspondence. arXiv preprint, 2023b. Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animateyour personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725,2023. Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, ZhangyangWang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable longvideo generation from text. arXiv preprint arXiv:2403.14773, 2024.",
  "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprintarXiv:2010.02502, 2020": "Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, AlekseiSilvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust largemask inpainting with fourier convolutions. arXiv preprint arXiv:2109.07161, 2021. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-drivenimage-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 19211930, June 2023a. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-drivenimage-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 19211930, 2023b.",
  "Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preservinggeneration in seconds. arXiv preprint arXiv:2401.07519, 2024b": "Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advances inNeural Information Processing Systems, 36, 2024c. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models.arXiv preprint arXiv:2309.15103, 2023.",
  "ADiscussion on Model Implementation Details": "When adapting our AnyV2V to various I2V generation models, we identify two sets of hyperparameters thatare crucial to the final video editing results. They are (1) selection of U-Net decoder layers (l1, l2 and l3) toperform convolution, spatial attention and temporal attention injection and (2) Injection thresholds conv,sa and ta that control feature injections to happen in specific diffusion steps. In this section, we providemore discussions and analysis on the selection of these hyperparameters.",
  "A.1U-Net Layers for Feature Injection": "To better understand how different layers in the I2V denoising U-Net produce features during video sampling,we perform a visualization of the convolution, spatial and temporal attention features for the three candidateI2V models I2VGen-XL (Zhang et al., 2023c), ConsistI2V (Ren et al., 2024) and SEINE (Chen et al., 2023d).Specifically, we visualize the average activation values across all channels in the output feature map from theconvolution layers, and the average attention scores across all attention heads and all tokens (i.e. averageattention weights for all other tokens attending to the current token). The results are shown in . According to the figure, we observe that the intermediate convolution features from different I2V modelsshow similar characteristics during video generation: earlier layers in the U-Net decoder produce featuresthat represent the overall layout of the video frames and deeper layers capture the high-frequency detailssuch as edges and textures. We choose to set l1 = 4 for convolution feature injection to inject backgroundand layout guidance to the edited video without introducing too many high-frequency details. For spatialand temporal attention scores, we observe that the spatial attention maps tend to represent the semanticregions in the video frames while the temporal attention maps highlight the foreground moving subjects (e.g.the running woman in ). One interesting observation for I2VGen-XL is that its spatial attentionoperations in deeper layers almost become hard attention, as the spatial tokens only attend to a single orvery few tokens in each frame. We propose to inject features in decoder layers 4 to 11 (l2 = l3 = {4, 5, ..., 11})to preserve the semantic and motion information from the source video.",
  "We perform additional ablation analysis using different feature injection thresholds to study how thesehyperparameters affect the edited video": "Effect of Spatial Injection Thresholds conv, saWe study the effect of disabling spatial feature injectionor using different conv and sa values during video editing and show the qualitative results in . Wefind that when spatial feature injection is disabled, the edited videos fail to fully adhere to the layout andmotion from the source video. When spatial feature injection thresholds are too high, the edited videos arecorrupted by the high-frequency details from the source video (e.g. textures from the womans down jacketin ). Setting conv = sa = 0.2T achieves a desired editing outcome for our experiments. Effect of Temporal Injection Threshold taWe study the hyperparameter of temporal feature injectionthreshold ta in different settings and show the results in . We observe that in circumstances whereta < 0.5T (T is the total denoising steps), the motion guidance is too weak that it leads to only partlyaligned motion with the source video, even though the motion itself is logical and smooth. At ta > 0.5T, thegenerated video shows a stronger adherence to the motion but distortion occurs. We employ ta = 0.5T inour experiments and find that this value strikes the perfect balance on motion alignment, motion consistency,and video fidelity.",
  "Video": "Convolution FeaturesSpatial Attention ScoresTemporal Attention Scores Layer 1Layer 4Layer 7Layer 11Layer 4Layer 7Layer 11Layer 4Layer 7Layer 11 HighLow : Visualizations of the convolution, spatial attention and temporal attention features during videosampling for I2V generation models decoder layers. We feed in the DDIM inverted noise to the I2V modelssuch that the generated videos (first row) are reconstructions of the source video.",
  "B.2Qualitative Results": "Prompt-based EditingBy leveraging the strength of image editing models, our AnyV2V frameworkprovides precise control of the edits such that the irrelevant parts in the scene are untouched after editing. Inour experiment, we used InstructPix2Pix (Brooks et al., 2023) for the first frame edit. Shown in ,our method correctly places a party hat on an old mans head and successfully turns the color of an airplaneto blue, while preserving the background and keeping the fidelity to the source video. Comparing ourwork with the three baseline models TokenFlow (Geyer et al., 2023), FLATTEN (Cong et al., 2023), andTune-A-Video (Wu et al., 2023b), the baseline methods display either excessive or insufficient changes inthe edited video to align with the editing text prompt. The color tone and object shapes are also tilted.It is also worth mentioning that our approach is far more consistent on some motion tasks such as addingsnowing weather, due to the I2V models inherent support for animating still scenes. The baseline methods,on the other hand, can add snow to individual frames but cannot generate the effect of snow falling, as theper-frame or one-shot editing methods lack the ability of temporal modeling. Reference-based Style TransferOur approach diverges from relying solely on textual descriptors forconducting style edits, using the style transfer model NST (Gatys et al., 2015) to obtain the edited frame.This level of controllability offers artists the unprecedented opportunity to use their art as a reference for videoediting, opening new avenues for creative expression. As demonstrated in , our method captures thedistinctive style of Vassily Kandinskys artwork Composition VII and Vincent Van Goghs artwork Chateauin Auvers at Sunset accurately, while such an edit is often hard to perform using existing text-guided videoediting methods. Subject-driven EditingIn our experiment, we employed a subject-driven image editing model Any-Door (Chen et al., 2023c) for the first frame editing. AnyDoor allows replacing any object in the target imagewith the subject from only one reference image. We observe from that AnyV2V produces highly",
  "B.3.1Dataset": "Our human evaluation dataset contains a total of 89 samples that have been collected from For prompt-based editing, we employed InstructPix2Pix (Brooks et al., 2023) to compose theexamples. Topics include swapping objects, adding objects, and removing objects. For subject-driven editing,we employed AnyDoor (Chen et al., 2023c) to replace objects with reference subjects. For Neural StyleTransfer, we employed NST (Gatys et al., 2015) to compose the examples. For identity manipulation, weemployed InstantID (Wang et al., 2024b) to compose the examples. See for the statistic.",
  "B.3.2Interface": "In the evaluation setup, we provide the evaluator with generated images from both the baseline models andthe AnyV2V models. Evaluators are tasked with selecting videos that best align with the provided prompt orthe reference image. Additionally, they are asked to express their overall preference for the edited videos. Fora detailed view of the interface used in this process, please see .",
  "C.1Limitations": "Inaccurate Edit from Image Editing Models. As our method relies on an initial frame edit, the imageediting models are used. However, the current state-of-the-art models are not mature enough to performaccurate edits consistently (Ku et al., 2024). For example, in the subject-driven video editing task, we foundthat AnyDoor (Chen et al., 2023c) requires several tries to get a good editing result. Efforts are requiredin manually picking a good edited frame. We expect that in the future better image editing models willminimize such effort. Limited ability of I2V models. We found that the results from our method cannot follow the sourcevideo motion if the motion is fast (e.g. billiard balls hitting each other at full speed) or complex (e.g. a",
  "C.2License of Assets": "For image editing models, InstructPix2Pix (Brooks et al., 2023) inherits Creative ML OpenRAIL-M Licenseas it is built upon Stable Diffusion. Neural Style Transfer (Gatys et al., 2015) is under Creative CommonsAttribution 4.0 License, and code samples are licensed under the Apache 2.0 License. InstantID (Wang et al.,2024b) is under Apache License 2.0. AnyDoor (Chen et al., 2023c) is under the MIT License.",
  "C.3Societal Impacts": "Postive Social Impact. AnyV2V has the potential to significantly enhance the capabilities of video editingsystems, making it easier for a wider range of users to manipulate images. This could have numerous positivesocial impacts, as users would be able to achieve their editing goals without needing professional editingknowledge, such as using Photoshop or painting. Misinformation spread and Privacy violations. As our technique allows for object manipulation, itcan produce highly realistic yet completely fabricated videos of one individual or subject. There is a riskthat harmful actors could exploit our system to generate counterfeit videos to disseminate false information.Moreover, the ability to create convincing counterfeit content featuring individuals without their permissionundermines privacy protections, possibly leading to the illicit use of a persons likeness for harmful purposesand damaging their reputation. These issues are similarly present in DeepFake technologies. To mitigate therisk of misuse, one proposed solution is the adoption of unseen watermarking, a method commonly used totackle such concerns in image generation.",
  "C.5Ethical Concerns for Human Evaluation": "We believe our proposed human evaluation does not incur ethical concerns due to the following reasons: (1)the study does not involve any form of intervention or interaction that could affect the participants well-being.(2) Rating video content does not involve any physical or psychological risk, nor does it expose participantsto sensitive or distressing material. (3) The data collected from participants will be entirely anonymous andwill not contain any identifiable private information. (4) Participation in the study is entirely voluntary, andparticipants can withdraw at any time without any consequences.",
  "C.6New Assets": "Our paper introduces several new assets including a human evaluation dataset and demo videos generated byAnyV2V. Each asset is thoroughly documented, detailing its creation, usage, and any relevant methodologies.The human evaluation dataset documentation includes details on how participant consent was obtained. Thedemo videos are provided as an anonymized zip file to comply with submission guidelines, with detailedinstructions for use. All assets are shared under an open license to facilitate reuse and further research."
}