{
  "Abstract": "Many approaches for optimizing decision making models rely on gradient based methodsrequiring informative feedback from the environment. However, in the case where suchfeedback is sparse or uninformative, such approaches may result in poor performance.Derivative-free approaches such as Bayesian Optimization mitigate the dependency on thequality of gradient feedback, but are known to scale poorly in the high-dimension settingof complex decision making models. This problem is exacerbated if the model requiresinteractions between several agents cooperating to accomplish a shared goal. To addressthe dimensionality challenge, we propose a compact multi-layered architecture modelingthe dynamics of agent interactions through the concept of role. We introduce DependencyStructure Search Bayesian Optimization to efficiently optimize the multi-layered architectureparameterized by a large number of parameters, and show an improved regret bound. Ourapproach shows strong empirical results under malformed or sparse reward.",
  "Introduction": "Decision Making Models choose sequences of actions to accomplish a goal. Multi-Agent Decision MakingModels choose actions for multiple agents working together towards a shared goal. Multi-Agent ReinforcementLearning (marl) has emerged as a competitive approach for optimizing Decision Making Models in themulti-agent setting.1 marl optimizes a policy under the partially observable Markov Decision Process(pomdp) framework, where decision making happens in an environment determined by a set of possiblestates and actions, and the reward for an action is conditioned upon the partially observable state of theenvironment. A policy forms a set of decision-making rules capturing the most rewarding actions in a givenstate. marl utilizes gradient-based methods requiring informative gradients to make progress. This approachbenefits from dense reward, which allows reinforcement learning methods to infer a causal relationship betweenindividual actions and their corresponding reward. This feedback may not be present in the scenario of sparse",
  "reward(Pathak et al., 2017; Qian & Yu, 2021). In addition, gradient-based methods are susceptible to fallinginto local maxima": "In contrast to optimization by marl, Bayesian Optimization (bo) offers an alternative approach to policyoptimization. Since bo is a gradient-free optimizer capable of searching globally, applying bo to multi-agentpolicy search (maps) both ensures global searching of the policy, and overcomes poor gradient behavior inthe reward function (Qian & Yu, 2021). The chief challenge in bo for maps is the high dimensionality ofcomplex multi-agent interactions. A significant degree of high-dimensional multi-agent interactions exist in maps. For example, consideringan autonomous drone delivery system, several agents (i.e., drones) must work together to maximize thethroughput of deliveries. In doing so, these agents may separate themselves into different roles, for example,long-distance or short-distance deliveries. The optimal policy for each role may be significantly different dueto distances to recharging base stations (e.g., drones must conserve battery). In forming the optimal policy,the interaction between agents must be considered to both optimally divide the task between the drones, aswell as coordinate actions between drones (e.g., collision avoidance). These interactions may change overtime. For example, a drone must avoid collision with nearby drones, which changes as it moves through theenvironment. With many agents, these interactions become more complex. However, we propose the usage of bo for maps on memory-constrained devices which necessitates verycompact policies which enables the possibility of overcoming the above limitation. In the context of memory-constrained devices such as Internet of Things (IoT) devices (Merenda et al., 2020), small policies mustbe used. Secondly, in environments with sparse reward feedback, training these networks with rl presentssignificant challenges due to unhelpful policy gradients. Finally, the possibility of globally optimizing acompact policy for memory-constrained systems is appealing due to its strong performance guarantees. To allow for the construction of compact policies, we utilize specific multi-agent abstractions of role androle interaction. In role-based multi-agent interactions, an agents policy depends on its current role andsparse interactions with other agents. By simplifying the policy space with these abstractions, we increase itstractability for global optimization by bo and inherit the strong empirical performance demonstrated bythese approaches. We realize this simplification of the policy space by expressing the role abstraction and roleinteraction abstractions as immutable portions of the policy space, which are not searched over during policyoptimization. To achieve this, we use a higher-order model (hom) which generates a policy model. The homis divided into immutable instructions (i.e., algorithms) corresponding to the abstractions of the role and roleinteraction and mutable parameters that are used to generate (gen) a policy model during evaluation. To optimize our proposed hom, we specialize bo by exploiting task-specific structures.A promisingavenue of High-dimensional Bayesian Optimization (hdbo) is through additive decomposition. Additivedecomposition separates a high-dimensional optimization problem into several independent low-dimensionalsub-problems (Duvenaud et al., 2011; Kandasamy et al., 2015). These sub-problems are independently solvedthus reducing the complexity of high dimensional optimization. However, a significant challenge in additivedecomposition is learning the independence structure which is unknown a-priori. Learning the additivedecomposition is accomplished using stochastic sampling such as Gibbs sampling (Kandasamy et al., 2015;Rolland et al., 2018; Han et al., 2020) which is known to have poor performance in high dimensions (Johnsonet al., 2013; Barbos et al., 2017). In our work, we overcome this shortcoming by observing the gen process of the hom. In particular, we canmeasure a surrogate Hessian during the gen process which significantly simplifies the task of learning theadditive structure. This surrogate Hessian informs the dependency structure of the optimization problem dueto the equivalence between a zero Hessian value, and independence between dimensions due to the linearityof addition. We term this approach Dependency Structure Search GP-UCB (dss-gp-ucb) and visualize ourapproach in . Our proposed bo approach is also applicable to policy-search in the single-agent setting,showing its general-purpose applicability in Decision Making Models. In this work, we make the followingcontributions:",
  "Background": "Bayesian Optimization:Bayesian optimization (bo) involves sequentially maximizing an unknownobjective function v : R. In each iteration t = 1, . . . , T, an input query t is evaluated to yield a noisyobservation yt v(t) + with i. i. d. Gaussian noise N(0, 2). bo selects input queries to approach theglobal maximizer arg max v() as rapidly as possible. This is achieved by minimizing cumulativeregret RT Tt=1 r(t), where r(t) v() v(t). Cumulative regret is a key performance metric of bomethods. The probability distribution of v is modeled by a Gaussian process (GP), denoted GP ((), k(, )), that is,every finite subset of {v()} follows a multivariate Gaussian distribution (Rasmussen & Williams, 2006).A GP is fully specified by its prior mean () and covariance k(, ) for all , , which are, respectively,assumed w.l.o.g. to be () = 0 and k(, ) 1. Given a vector yT [yt]t=1,...,T of noisy observations fromevaluating v at input queries 1, . . . , T after T iterations, the GP posterior probability distribution of vat some input is a Gaussian with the following posterior mean kT () and variance [kT ]2():",
  "Related work": "Decision Making Models:Decision Making Models (Rizk et al., 2018; Roijers et al., 2013) determineactions taken by an agent or agents in order to achieve a goal. We focus on the pomdp setting and optimizing apolicy to accumulate maximum reward while interacting with a partially observable environment (Shani et al.,2013). Many approaches exist which can be broadly categorized into direct policy search and reinforcementlearning methods. Direct policy search (Heidrich-Meisner & Igel, 2008; Lizotte et al., 2007; Martinez-Cantin,2017; Papavasileiou et al., 2021; Wierstra et al., 2008) searches the policy space in some efficient manner.Reinforcement learning (Arulkumaran et al., 2017; Fujimoto et al., 2018; Haarnoja et al., 2018; Lillicrapet al., 2015; Lowe et al., 2017; Mnih et al., 2015; Schulman et al., 2017) starts with a randomly initializedpolicy and reinforces rewarding behavior patterns to improve the policy. Bayesian Optimization for Decision Making Models:bo has been utilized for direct policy search inthe low dimensional setting (Lizotte et al., 2007; Wilson et al., 2014; Marco et al., 2016; Martinez-Cantin, 2017;von Rohr et al., 2018). However, these approaches have not scaled to the high dimensional setting. In morerecent works, bo has been utilized to aid in local search methods similar to reinforcement learning (Akrouret al., 2017; Eriksson et al., 2019a; Wang et al., 2020a; Frhlich et al., 2021; Mller et al., 2021). However,these approaches require evaluation of an inordinate number of policies typical of local search methods anddo not provide regret guarantees. Recently, combinations of local and global search methods have beenproposed (McLeod et al., 2018; Shekhar & Javidi, 2021). However, these approaches rely on informative anduseful gradient information and have not been shown to scale to the high dimensional setting. MARL for multi-agent decision making:A well-known approach for cooperative marl is a combinationof centralized training and decentralized execution (CTDE) (Oliehoek et al., 2008).The multi-agentinteractions of CTDE methods can be implicitly captured by learning approximate models of other agents(Lowe et al., 2017; Foerster et al., 2018) or decomposing global rewards (Sunehag et al., 2017; Rashid et al.,",
  "Published in Transactions on Machine Learning Research (10/2024)": "We repeat these experimental scenarios in the marl setting with similar results in where marlapproaches are compared against dss-gp-ucb in the CTDE setting. Thus our validation shows that in bothrl and marl strong performance requires dense, informative feedback which may not be present outside ofsimulator settings. In these settings, our approach of optimizing small compact policies using dss-gp-ucboutperforms related work in both rl and marl.",
  "with the global state represented as s [si]i=1,...,n. Each agent i cooperatively chooses an action ai Ai": "with the global action represented by a [ai]i=1,...,n. Each state, action pair is associated with a rewardfunction: (s, a). In order to achieve the common task, a policy parameterized by : S A governsthe action taken by the agents, after observing state s S. The goal of rl is to learn the optimal policyparameters that maximizes the accumulation of rewards during a predefined number of interactions withthe environment,2 v(). In contrast to rl, which receives feedback on the reward of an action with everyinteraction, we treat v() as an opaque function measuring the value of a policy. We utilize bo to optimize using solely the accumulated reward, v(), as feedback from the environment.",
  "Architectural design": "To achieve a compact and tractable policy space, we consider policies under the useful abstractions of roleand role interaction. These abstractions have consistently shown strong performance in multi-agent tasks.Therefore, we can simplify the policy space by limiting it to only policies using these abstractions, but stillhave powerful and expressive policies suitable for multi-agent systems. As role and role interaction are immutable abstractions within our policy space, we express them as staticalgorithms which are not searched over during policy optimization. These algorithms take as input parameterswhich are mutable and searched over during policy optimization. This combination of immutable instructions,and mutable parameters reduces the size of the search space,3 yet is still able to express policies whichconform to the role and role interaction abstractions. We term this approach a higher-order model (hom) which generates (gen) the model using instructionsand parameters into a policy model during evaluation. This hom is separated into role assignment, androle interaction stages. We visualize an overview of this approach in , left. The hom parameters areinterpreted in context of the current state by the instructions (Alg. 1, Alg. 2, Alg. 3) of the hom to form thepolicy model which dictates the resultant action. In our work, each hom component of role assignment androle interaction is implemented as a neural network.",
  "Role assignment": "Following the success of role based collaboration in multi-agent systems, we assume the interaction anddecision making of each agent is governed by its assigned role. For example, in drone delivery, roles could beshort-distance deliveries, and long-distance deliveries. In filling these roles, the state of each of the agents areconsidered. E.g., a drone with low battery may be limited to only performing short-distance deliveries. Astraightforward approach to implement role based interaction is to permute agents into an equivalent numberof roles.4 We assume that an optimal policy can be decomposed as follows: 2Further rl overview can be found in Arulkumaran et al. (2017).3This approach to efficiency is similar in spirit to the work of Lee et al. (1986).4This is a common assumption in multi-agent systems, see, e.g., Le et al. (2017).",
  "where is a permutation function dependent on the state, s1, . . . , sn. Our approach to role assignment issimple and general purpose, which is also well studied and theoretically principled": "To capture this behavior, we utilize a per role affinity function: r,i() which is the affinity to take on role iand is parameterized by r,i. This function evaluates the affinity of agent taking on role i using the state ofagent: s. The optimal permutation maximizes the total affinity of an assignment: ni=1 r,i(s(i)) where represents a permutation. This problem can be efficiently solved using the Hungarian algorithm (Kuhn,1955). We integrate the Hungarian algorithm in our hom approach during the gen process. We formalizethis in Algorithm 1 which forms the instructions in the role assignment hom. Given Algorithm 1, during gen process, the agents state, s1, . . . , sn is contextually interpreted to yielda permutation model:.Going forward, we consider the problem of determining the joint policyr(a(1), . . . , a(n) | s(1), . . . s(n)) which enables collaborative interactions.",
  "Role interaction": "Capturing multiple roles working together is an important part of an effective multi-agent policy. For examplein drone delivery, drones must both divide the available task among themselves, as well as use collisionavoidance while executing deliveries. Modeling role interactions must accomplish two goals. Firstly, agentinteractions may change over time. For example collision avoidance strategies involve the closest drones whichchange as the drone moves within the environment. Secondly, efficient parameterization is needed as thenumber of interactions can scale exponentially due to considering interactions between many agents.",
  ": Left: hom architecture. gen uses r and gduring evaluation to yield a model which represents thepolicy. r and g are optimized by bo. Right: Inferringa given s": "To overcome these challenges, we propose a homwhich generates (gen) a graphical model. The us-age of a graphical model decomposes the exponen-tially scaling interaction problem into a pairwiseinteraction model, along with a message passing ap-proach to facilitate complex interactions betweenmany agents.5 The gen process is conditioned onthe agents state, thus enabling dynamic role in-teractions; in addition the gen process allows for amore compact policy space with far fewer parameters.The resultant generated graphical model capturesthe state-dependent interaction between roles andyields the resultant actions for each role. After gen,the interaction between roles are captured by theresultant conditional random field. This is presentedin , right. The MRF (Markov Random Field)represents arbitrary undirected connectivity between nodes a(1), . . . , a(n), which is denoted by G. Thisconnectivity allows different roles to collaborate together to determine the joint action. To generate graphicalmodels of the above form, our hom uses edge affinity functions, g,v(), which enables dynamic arbitraryconnectivity between roles. For all pairs of roles with state, s(i), s() an edge is generated if the affinitybetween these two states is sufficiently high (i.e., > 0). This dynamic edge generation approach overcomesthe quadratic parameter scaling if all pairs of agents were separately modelled. The graphical model genprocess is presented in Algorithm 2 which yields a graphical model. To cooperatively determine a set of actions for roles given the graphical model, we perform inference over thegraphical model presented in using Message Passing Neural Networks (Gilmer et al., 2017) (MPNN).We present iterative message passing rules to map from s to a:",
  "i=1,...,n(3)": "where M is the message function parameterized by g, which enables interaction between connected nodes, Uis the action update function parameterized by g,e which updates the nodes internal hidden state conditionedon the messages received, and N (i) denotes the neighbors of (i). The message passing procedure allows forcooperative determination of all roles actions using pairwise message passing. Roles which are not immediateneighbors of each other influence each others behavior through intermediary connecting nodes. The messagepassing procedure concludes after iterations of message passing with the policy actions indicated by thehidden states,h(i)",
  "Additive decomposition": "Although our hom policy representation is compact, it is still of significant dimensionality which makesoptimization with bo difficult. hdbo is challenging due to the curse of dimensionality with common kernelssuch as Matern or RBF.6 This curse of dimensionality stems directly from the difficulty of finding the globaloptima of a high-dimensional function (e.g., a value function v() determining the value of a policy in someunknown environment). A common technique to overcome this is through assuming additive structuraldecomposition on v: v() Mi=1 v(i)((i)) where v(i) are independent functions, and (i) (i) (Duvenaudet al., 2011). The additive decomposition simplifies a high-dimensional optimization problem since the optimaof a function constructed through addition of subfunctions can be found by independently optimizing eachsubfunction as visualized in . In the context of bo, additive decomposition significantly simplifies theoptimization problem due to the properties of Multivariate Gaussian variables. : Left, above, plot of f(x, y) = xy; below, plotof f(x, y) = x + y. The curvature of additively con-structed functions is zero; non-zero curvature indicatesdependency among input variables. Right, examiningthe Hessian learns the dependency structure whichdecomposes complex problems into simpler problemssolved by GP-UCB.",
  "In additive decomposition we denote the domainof the optimization problem, 1 . . . D": "for some dimensionality D, that is the domain isconstructed through the Cartesian product of eachof its dimensions.Each subfunction to optimize,v(i), corresponds to a subdomain restricted to somesubset of these dimensions, (i) {1, . . . , D}.Typically, it is assumed that each (i) is of lowdimensionality (i.e., v(i) is defined on only a few di-mensions for each i). This structural assumptionis combined with the assumption that each v(i) issampled from a GP. Due to the properties of Mul-tivariate Gaussians, if v(i) GP0, k(i)((i), (i))",
  "then v GP0,": "i k(i)((i), (i))(Rasmussen &Williams, 2006), which follows from the addition oftwo Gaussian random variables is also a Gaussianrandom variable. This assumption decomposes ahigh dimensional GP surrogate model of v into aset of many low dimensional GPs, which is easier tojointly learn and optimize. To contextualize an additive decomposition, we represent the decomposition by a dependency graph betweenthe dimensions: Gd (Vd, Ed) where Vd {1, . . . , D} and Ed {(a, b) | a, b (i) for some i}. Asimple decomposition of an additive function and its associated dependency graph is visualized in . 6A parallel area in hdbo is of computational efficiency of acquisition which is outside the scope of this work. We refer readersto the works of Mutny & Krause (2018), Wilson et al. (2020), and Ament & Gomes (2022).",
  "ab = 0which implies their connectivity within Ed": "In practice, observing the Hessian of the value function, Hv, is not possible due to v being an opaque function.However, during the gen process we can observe the Hessian of the policy, H. This surrogate Hessian isclosely related to the Hv as v() is determined through interaction of the policy with an unknown environment.Because the value of a policy is a function of the policy; it follows by the chain rule that H is an importantsub-component of Hv. We utilize the surrogate Hessian in our work and demonstrate its strong empiricalperformance in validation. Following this reasoning, we consider algorithms with noisy query access to theHessian, Hv. Note that we assume that the surrogate Hessian, H, can well serve as a noisy surrogate forthe true Hessian, Hv.7",
  "ab + (a,b)h]a,b=1,...,D where (a,b)h N(0, 2n) i.i.d. Each query to Hhas corresponding regret of r()": "Under this assumption, we show that its possible to learn the underlying dependency structure of Gd = (Vd, Ed)with a polynomial number of queries to the noisy Hessian. We present dss-gp-ucb in Algorithm 4 and provetheoretical results regarding its performance. In the first stage of dss-gp-ucb, we perform C1 queries to the",
  "i k(i), the sum of the aforementioned kernels and inference and acquisition proceeds same asGP-UCB (lines 6-9)": "To bound the cumulative regret, Rt T0t=1 C1r(t,h) + Tt=T0 r(t), we follow the following process. First,we bound the number and size of cliques of graphs sampled from the Erds-Rnyi model with high probability.Second, we bound the mutual information of an additive decomposition given the mutual information ofits constituent kernels using Weyls inequality. Third, we use similar analysis as Srinivas et al. (2010) tocomplete the regret bound. Theorem 2. Let k be the kernel as in Assumption 1, and Theorem 1. Let kT (d) : N R be a monotonicallyincreasing upper bound function on the mutual information of kernel k taking d arguments. The cumulativeregret of dss-gp-ucb is bounded with high probability as follows:",
  "Validation": "We compare our work against recent algorithms in marl on several multi-agent coordination tasks and rlalgorithms for policy search in novel settings. We also perform ablation and investigation of our proposedhom at learning roles and multi-agent interactions. We defer experimental details to Appendix A. All presented figures are average of 5 runs with shading representing Standard Error, the y-axis representscumulative reward, the x-axis displayed above represents interactions with the environment in rl, x-axisdisplayed below represents iterations of bo. Commensurate with our focus on memory-constrained devices, allpolicy models consist of < 500 parameters.",
  "Ablation": "We investigate the impact of Role Assignment (RA) and Role Interaction (RI) as well as model capacityon training progress. We conduct ablation experiments on Multiagent Ant with 6 agents, PredPrey with 3agents, and Heterogenous PredPrey with 3 agents (Peng et al., 2021). Multiagent Ant is a MuJoCo (Todorovet al., 2012) locomotion task where each agent controls an individual appendage. PredPrey is a task where",
  ": Left two plots: Sparse reward drone delivery task. Rightmost: Comparison with hdbo approaches.The left two plots validate the same approaches on different environments": "predators must work together to catch faster, more agile prey. Het. PredPrey is similar, except the predatorshave different capabilities of speed and acceleration. In ablation experiments, our default configuration is Med- RA - RI which employs components of RA and RI parameterized by neural networks with three layers andfour neurons on each layer (medium sized neural network). The Sm, small, model is instead parameterizedwith neural networks of 1 layer with 2 neurons each. When RA is ablated, the agents interact directly withouttaking on any role based specialization. When RI is ablated, the agents action is determined without anycoordination between agents. We present our ablation in .",
  "DDPGPPOSACTD3IntrinsicDDPGPPOSACTD3IntrinsicDDPGPPOSACTD3 IntrinsicDDPGPPOSACTD3Intrinsic": "Baseline90.771105.692045.242606.172144.00604.20 1760.65 2775.66 1895.76 1734.0044.45121.38 58.73 48.78 1950.002203.80892.814297.03 1664.46 2210.00Sparse 232.881007.802563.971407.401964.00877.93 1567.14 3380.60 1570.84 2074.0035.5999.5046.75 47.23 1758.801470.62 1471.33 1673.46 2297.43 1952.00Sparse 5 2687.97961.31711.56762.611916.00814.59 1616.79 3239.20 2290.67 1972.0026.6668.6943.84 40.12 1856.00961.30697.931697.25 2932.27 1924.00Sparse 20 2809.89624.07694.30379.121838.00783.95 1629.28 2535.17 1436.33 1537.2019.1254.6337.78 37.03 2108.00663.04365.391010.63276.561810.00Sparse 50 3067.3767.43663.28253.661091.20816.25 1010.73 1238.03551.43642.0023.7351.5238.78 30.01812.00572.12428.29349.47298.28834.75Sparse 100 3323.43 4021.56679.30115.43450.40988.36324.51260.52342.48406.809.6421.0927.98 30.10376.60523.89205.93200.16147.22480.60Sparse 200 3098.37 8167.98 107.14 147.86258.60765.05222.76300.36281.68350.809.9721.6933.35 30.48342.80182.84193.43187.16148.06353.20",
  "dss-gp-ucb1147.211009.3175.731008.90": "For a simpler coordination task such as Multiagent Ant, we observe limited improvement through RA orRI. In contrast, RI shows strong improvement in PredPrey and Het. PredPrey. It is because, in PredPrey,predators must work together to catch the faster prey. Since the agents in PredPrey are homogeneous, ablatingRA makes the optimization simpler and more compact without losing expressiveness. Thus, ablating RAleads to a performance increase. In Het. PredPrey, the predator agents have heterogeneous capabilitiesin speed and acceleration. Thus, RA plays a critical role in delivering strong performance. We also showthat overly shrinking the model size (Sm - RA - RI) can hurt performance as the policy model is no longersufficiently expressive. This is evidenced in the Multiagent Ant task. We observed that using neural networksof three layers with four neurons each to be sufficiently balanced across a wide variety of tasks. In , we present the detected Hessian structure by dss-gp-ucb in the respective tasks. The de-tected Hessian structures generally show strong block-diagonal associativity in the hom parameters, i.e.,[r,i, g,v, g,, g,e]. This shows that our approach can detect the interdependence within the sub-parameters,but relative independence between the sub-parameters. We observe more off-diagonal connectivity in thecomplex coordination tasks of PredPrey and Het. PredPrey. The visualization of Hessian structure onPredPrey shows that our approach can detect the importance of jointly optimizing role assignment andinteraction to deliver a strong policy in this complex coordination task. We investigate the learning behaviorof the hom further in Appendix B.",
  "Comparison with MARL": "We compare our method with competing marl algorithms on several multi-agent tasks where the number ofagents is increased. We validate both the hom with dss-gp-ucb (dss-gp-ucb (MM)) and neural networkpolicies trained in the CTDE paradigm (dss-gp-ucb (CTDE)). In the CTDE paradigm, both RI and RA areablated reducing the policy model to a neural network which is identical across all agents. We observe thaton complex coordination tasks such as PredPrey and Het. PredPrey our approach delivers more performantpolicies when coordination is required between a large number of agents. This is presented9 in .Although SOG (Shao et al., 2022), a Comm-marl approach shows compelling performance with a smallnumber of agents, with 15 agents, both dss-gp-ucb (CTDE) and dss-gp-ucb (MM) outperform this strategy.We highlight that dss-gp-ucb (CTDE) outperforms Comm-marl approaches without communication duringexecution. We also note that dss-gp-ucb (MM) outperforms dss-gp-ucb (CTDE) showing the value ofour hom approach in complex coordination tasks. We defer further experimental results in this setting toAppendix B.",
  "Policy optimization under malformed reward": "We compare against several competing rl and marl algorithms under malformed reward scenarios. We trainneural network policies with dss-gp-ucb and competing algorithms. We consider a sparse reward scenariowhere reward feedback is given every S environment interactions for varying S. shows that theperformance of competing algorithms is severely degraded with sparse reward and dss-gp-ucb outperformscompeting approaches on most tasks with moderate or higher sparsity. Although intrinsic motivation (Singhet al., 2004; Zheng et al., 2018) has shown evidence in overcoming this limitation, we find that our approachoutperforms competing approaches supported by intrinsic motivations at higher sparsity. This improvement 9We plot with respect to total environment interactions for l, and total policy evaluations for bo. See Appendix J, AppendixK, and Appendix L for alternate presentations of data more favorable to rl and marl under which our conclusions still hold.",
  "Het. PredPrey 15": "0.00.51.01.52.0 1e6 0.00.51.01.52.0 1e6 DSS-GPUCB (HOM)DSS-GPUCB (CTDE)CDS-QPLEXG2ANETCOMIXFACMACMADDPGSOGRODE : Scaling analysis. Training curves of dss-gp-ucb and competitors with increasing number of agents.The left column shows PredPrey with 6, 9, and 15 agents. The right column shows Het, PredPrey with 6, 9,and 15 agents. is important as sparse and malformed reward structure scenarios can occur in real-world tasks (Aubret et al.,2019). We repeat this validation in Appendix B with marl algorithms in multi-agent settings and consider adelayed feedback setting with similar results.",
  "Comparison with HDBO algorithms": "We compare with several related work in hdbo. This is presented in , rightmost plots. We compareagainst these algorithms at optimizing our hom policy. For more complex tasks that require role basedinteraction and coordination, our approach outperforms related work. TreeBO (Han et al., 2021) is alsoan additive decomposition approach to hdbo, but uses Gibbs sampling to learn the dependency structure.However, our approach of learning the structure through Hessian-Awareness outperforms this approach.Additional experimental results are deferred to Appendix B.",
  "Drone delivery task": "We design a drone delivery task that is well aligned with our motivation of considering policy search inmemory-constrained devices on tasks with unhelpful or noisy gradient information. In this task, drones mustmaximize the throughput of deliveries while avoiding collisions and conserving fuel. This task is challenging asa positive reward through completing deliveries is rarely encountered (i.e., sparse rewards). However, agentsoften receive negative rewards due to collisions or running out of fuel. Thus, gradient-based approaches caneasily fall into local minima and fail to find policies that complete deliveries.10 We compare dss-gp-ucbagainst competing approaches in , leftmost two plots. We observe that marl based approaches failto find a meaningfully rewarding policy in this setting, whereas our approach shows strong and compelling",
  "Conclusion": "We have proposed a hom policy along with an effective optimization algorithm, dss-gp-ucb. Our hom anddss-gp-ucb are designed to offer strong performance in high coordination multi-agent tasks under sparse ormalformed reward on memory-constrained devices. dss-gp-ucb is a theoretically grounded approach to booffering good regret bounds under reasonable assumptions. Our validation shows dss-gp-ucb outperformsrl and marl at optimizing neural network policies in malformed reward scenarios. Our hom optimizedwith dss-gp-ucb outperforms marl approaches in high coordination multi-agent scenarios by leveraging theconcepts of role and role interaction. Furthermore, we show through our drone delivery task, our approachoutperforms marl approaches in multi-agent coordination tasks with sparse reward. We make significantprogress on high coordination multi-agent policy search by overcoming challenges posed by malformed rewardand memory-constrained settings.",
  "We thank Jonathan Scarlett for pointing out a small mistake in our proof": "Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, GeoffreyIrving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,Dandelion Man, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, JonathonShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and XiaoqiangZheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL Software available from tensorflow.org.",
  "Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell.Curiosity-driven exploration byself-supervised prediction. In International conference on machine learning, pp. 27782787. PMLR, 2017": "Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, WendelinBhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. Advances inNeural Information Processing Systems, 34:1220812221, 2021. Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagentbidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraftcombat games. arXiv preprint arXiv:1703.10069, 2017.",
  "AExperimental Details": "We used Trieste (Berkeley et al., 2022), Tensorflow (Abadi et al., 2015), and GPFLow (Matthews et al., 2017)to build our work and perform comparisons using MushroomRL (DEramo et al., 2021), MultiagentMuJoCo(de Witt et al., 2020), OpenAI Gym (Brockman et al., 2016), and Multi-agent Particle environment (Loweet al., 2017). When comparing with related work, we used neural network policies of equivalent size. Allof our tested policies are < 500 parameters, however the XL models are constructed using 3 layers of 400neurons each. To estimate the Hessian, we used the Hessian-Vector product approximation. We relaxed the discrete portionsof our hom policy into differentiable continuous approximation for this phase using the Sinkhorn-Knoppalgorithm for the Role Assignment phase. For role interaction network connectivity, we used a sigmoid tocreate differentiable soft edges between each role. We pragmatically kept all detected edges in the Hessianwhile maintaining computational feasibility. We observed that our approach could support up to 1500 edgesin the dependency graph prior to experiencing computational intractability. We used the Matern-5",
  "A.1Ablation and Investigation": "In the ablation, we perform experiments on MultiagentMuJoCo with environments Multiagent Ant with6 segments, Multiagent Swimmer with 6 segments, Predator Prey with 3 predators, and HeterogeneousPredator Prey with 3 predators. In the Predator Prey environment, multiple predators must work together tocapture faster and more agile prey. In Heterogeneous Predator Prey, each Predator has differing capabilitiesof speed and acceleration. This modification is challenging as a policy must not only coordinate betweenthe Predators, but roles based specialization must be considered given the heterogeneous nature of eachpredators capabilities. To generate , we examined policy for Multiagent Ant with 6 agents for the role based policy specialization.The policy modulation plots were generated by examining the PredPrey and Het. PredPrey environmentsrespectively.",
  "A.2Comparison with MARL": "For the marl setting, we compare against MADDPG (Lowe et al., 2017), FACMAC (Peng et al., 2021),COMIX (Peng et al., 2021), RODE (Wang et al., 2021b) and CDS (Li et al., 2021) using QPLEX (Wanget al., 2021a) as a base algorithm. We also compare against Comm-marl approaches SOG (Shao et al., 2022),and G2ANet (Liu et al., 2020). RODE and QPLEX are limited to discrete environments, thus we are unableto provide comparisons on continuous action space tasks such as Multiagent Ant or Multiagent Swimmer.All marl environments were trained for 2, 000, 000 timesteps. The neural network policies were 3-layerseach with 15 neurons per layer, and were greater than or equal to the size of the compared hom policy. ForActor-Critic approaches, we did not reduce the size or expressivity of the critic. All used hyperparametersand Algorithmic configurations were as advised by the authors of the work. In the marl setting we use Multiagent Ant, Multiagent Swimmer, Predator-Prey, Heterogeneous Predator-Prey. Multiagent Ant, and Multiagent Swimmer are MuJoCo locomotion tasks where each agent controlsa segment of an Ant or Swimmer. Predator-Prey (PredPrey N) environment is a cooperative environmentwhere N of agents work together to chase and capture prey agents. In Heterogeneous Predator Prey, eachPredator has differing capabilities of speed and acceleration. This modification is challenging as a policymust not only coordinate between the Predators, but roles based specialization must be considered given theheterogeneous nature of each predators capabilities. We also validated related work on the drone deliverytask under which a drone swarm of N agents (Drone Delivery-N) must complete deliveries of varying distanceswhile avoiding collisions and conserving fuel. The code of which is available in supplementary materials andwill be open sourced. We used batching (Picheny et al., 2022) in our comparisons with marl to allow for a large number of iterationsof bo. We used a batch size of 15 in our comparison experiments. In this setting, all MuJoCo environments",
  "A.3RL and MARL under Malformed Reward": "For single agent rl we compared against SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017),TD3 (Fujimoto et al., 2018), and DDPG (Lillicrap et al., 2015) as well as an algorithm using intrinsicmotivation (Zheng et al., 2018). In single agent setting, we trained related work for 200, 000 timesteps. Inthe marl setting, we trained for 2, 000, 000 timesteps. In both single-agent setting and multi-agent settingall policy networks for both dss-gp-ucb and related work was 3 layers of 10 neurons each. The testedenvironments were standard OpenAI Gym benchmarks of Ant, Hopper, Swimmer, and Walker2D. In the marl setting we compared against COVDN (Peng et al., 2021), COMIX, FACMAC, and MAD-DPG. Comparisons were not possible against other approaches as these do not support continuous actionenvironments and are restricted to discrete action spaces.",
  "A.8Hyperparameter for Higher-Order Model": "For our hom we utilized simple grid search in order to pick the hyperparameter settings. Overly large neuralnetworks suffered from difficulty of optimization by bo, whereas, overly small neural networks suffered fromperformance difficulty on several environments. We found that neural networks of 3 layers, and 4 neuronseach performed well across a wide number of tested environments.",
  "B.2Comparison with MARL": "We present an expanded version of in including the results for Multiagent-Ant and Multiagent-Swimmer. We observe that in this relatively uncomplicated task not well-suited for our approach withdense reward, our hom approach shows comparable performance to marl approaches and far outperformsdss-gp-ucb (CTDE). This shows the overall value of our hom approach.",
  "B.3RL and MARL under Malformed Reward": "We present additional experiments under malformed reward for both rl and marl. We formally definethe Sparse reward scenario. Let v() =1 r where the value of the policy is determined through interactions with some unknown environment and each interaction is associated with the reward, r. Typically,rl algorithms observe the reward, r after every interaction with the environment. We consider a sparsereward scenario where reward feedback is given every S steps: rS S r if 0 mod S and 0 o.w. Inaddition to the sparse reward setting described earlier, we also consider the setting of delayed reward. Thedelayed reward scenario is defined:rD rD if > D and 0 o.w. Thus in the delayed reward scenario,feedback on an action taken is delayed. This scenario is important as it arises in long term planning taskswhere the value of an action is not immediately clear, but rather is ascertained after significant delays. Wepresent the complete table comparing related works in rl with dss-gp-ucb in . As can be seen,similar to the Sparse reward scenarios, significant degradation can be observed across all tested rl algorithmswith dss-gp-ucb outperforming rl algorithms with moderate to severe amount of sparsity or delay. Thisdegradation cannot be overcome by increasing the size of the policy, as we verify with the XL models whichare orders of magnitude larger with 3 layers of 400 neurons.",
  "DDPGPPOSACTD3IntrinsicDDPGPPOSACTD3IntrinsicDDPGPPOSACTD3IntrinsicDDPGPPOSACTD3Intrinsic": "Baseline 735.19(12.94)1224.60(31.01)2758.40(165.17) 2789.28(148.96)2560.00(91.35)1126.75(54.88)2315.99(215.04)3461.47(55.22)2281.42(502.82) 2302.00(115.32)67.28(11.09)123.90(4.21)72.93(11.24)50.64(0.26)2524.00(107.80)2920.52(233.23) 1041.51(250.84)4496.50(62.78)1946.67(468.29)2680.00(81.63)Sparse 2 636.88(37.47)1181.21(31.71)2656.32(465.10) 1479.48(240.63)2474.00(68.80)1580.92(221.75) 2004.17(277.09)3443.45(51.74)1621.54(652.16) 2512.00(166.09)69.55(14.27)101.49(11.75)51.34(3.58)50.07(1.46)2137.00(312.22)1966.29(325.89) 1742.26(528.65) 1783.93(702.32) 2805.47(487.28) 2792.00(146.95)Sparse 5 515.01(34.27)1128.62(45.65)876.94(4.66)922.45(7.91)2434.00(133.67)1118.18(37.46)2106.16(421.55)3372.21(67.27)2661.46(595.17)2236.00(58.83)113.11(30.07)70.30(15.13)48.65(1.66)54.82(6.45)2232.00(180.56)1577.68(338.12)884.58(430.36)2055.79(590.11) 2990.79(376.18) 2516.00(131.84)Sparse 20 515.01(34.27)657.98(29.03)823.76(10.79)865.98(3.39)2260.00(134.28)1204.52(131.32) 2378.65(197.54) 2862.10(354.24) 1858.63(645.13) 1941.60(395.32)77.75(14.17)56.77(9.94)45.54(0.97)52.02(4.69)2430.00(51.77)1300.61(83.97)405.58(81.94)1354.90(558.24)962.23(195.65)2496.00(193.11)Sparse 50 515.01(34.27)15.60(5.48)802.86(14.53)855.26(11.19)1487.20(273.48)1068.08(13.54)1120.64(209.65) 1343.22(490.56)569.26(111.17)1208.40(438.47)62.89(14.23)53.39(7.82)44.60(2.39)52.49(5.12)1168.60(347.72)1150.83(58.12)494.80(79.40)561.26(99.60)723.43(179.75)1244.00(336.40)Sparse 100 515.01(34.27) 320.51(147.49)791.90(17.81)858.21(8.65)615.00(69.76)1068.30(4.95)349.76(58.74)275.72(25.53)610.10(149.87)425.20(50.65)43.90(3.86)33.41(4.94)39.77(1.18)41.13(2.61)457.20(74.06)965.61(49.66)236.56(19.79)341.90(113.31)276.49(49.34)499.20(79.82)Sparse 200 515.01(34.27) 605.62(371.26)734.80(37.97)855.26(11.19)275.60(44.18)1070.79(9.05)231.51(28.29)331.94(30.02)446.48(127.60)378.40(63.95)57.84(11.71)29.18(1.40)41.58(5.45)50.91(6.31)360.60(10.33)931.29(64.05)213.46(19.25)553.02(117.77)232.43(36.01)391.00(51.74)Sparse XL 100 636.74(17.15)149.05(22.88)767.14(16.06)950.98(3.26)612.00(152.71)1274.85(145.43)522.46(73.13)745.89(143.22)485.85(191.12)840.20(92.72)55.00(3.96)48.31(4.70)67.36(8.05)61.08(12.17)600.80(123.76)1064.29(15.42)242.14(27.16)543.12(87.70)340.95(126.72)664.80(65.77)Sparse XL 200 636.74(17.15)207.31(20.43)735.49(21.74)950.93(3.07)426.60(38.49)1232.07(88.70)521.58(15.14)494.12(121.20)243.78(73.73)381.20(16.22)45.12(2.27)38.37(2.50)56.41(9.45)57.03(4.22)475.60(13.04)1082.17(19.44)211.45(31.80)916.00(41.96)314.68(30.21)549.60(116.24)Lag 1 678.29(20.14)1185.23(50.81)2472.69(394.47) 2791.95(183.88)2756.00(42.58)1363.76(270.35) 2332.61(277.97) 3070.40(384.04) 2968.75(348.30)2482.00(86.46)92.38(18.71)112.32(12.11) 71.84(18.38)50.45(0.81)2466.00(75.66)2759.46(297.35) 1281.11(354.59) 3763.69(710.79) 3973.95(121.84) 2488.00(102.11)Lag 5 515.01(34.27)925.87(29.70)907.16(8.80)912.37(7.18)2352.60(333.51)1062.45(2.56)2215.38(320.02)3390.11(62.47)3404.84(53.10)2482.00(109.96)91.55(17.14)89.87(14.44)59.44(4.63)51.26(4.11)2414.00(183.10)1836.65(285.47) 1150.46(409.99) 4027.20(470.29) 3429.69(353.47) 2310.00(102.33)Lag 20 515.01(34.27)246.04(68.10)858.72(11.61)895.80(4.28)2220.00(148.22)896.93(149.20)2278.98(287.09)3512.11(18.23)2251.41(572.46) 2302.00(100.18)58.04(3.18)61.09(5.22)48.65(4.03)50.73(4.15)2528.00(60.49)1168.09(83.23)1058.63(237.01) 2139.30(644.12) 2422.67(647.97)2220.00(99.32)Lag 50 515.01(34.27)47.10(13.04)857.88(11.48)886.99(11.81)1285.60(308.68)1062.32(5.64)968.13(262.18)619.88(118.08)795.28(104.06)1394.60(260.90)104.00(16.80)40.15(3.55)80.56(22.86)33.18(3.89)1689.20(290.96)1302.88(182.77)412.56(78.02)658.04(166.07)696.85(165.26)1243.60(217.52)Lag 100 515.01(34.27) 536.23(312.30)818.49(7.29)865.77(7.49)273.20(40.32)1182.87(112.11)228.23(34.10)278.11(35.84)371.62(44.65)302.40(38.33)63.01(6.61)36.83(4.75)42.58(1.82)44.88(2.28)388.60(56.65)989.92(66.76)222.44(17.09)390.56(95.75)224.57(35.10)396.40(50.45)Lag 200 515.01(34.27) 510.41(279.94)743.10(16.65)855.32(11.23)421.60(29.18)1079.57(21.63)282.80(49.28)289.69(24.11)578.05(158.82)319.20(33.32)61.89(13.75)27.51(1.52)34.54(5.75)55.55(8.01)359.60(44.53)902.89(112.65)199.60(7.11)416.00(137.76)171.80(21.82)324.60(31.72)Lag XL 100 636.74(17.15)70.24(30.12)707.49(11.16)953.04(3.31)507.00(54.07)779.16(269.88)640.55(12.56)327.03(6.84)241.02(66.35)489.80(40.90)54.91(6.36)42.82(2.41)76.54(10.58) 50.88(13.32)536.80(34.29)1082.21(26.81)235.37(34.48)669.73(88.18)315.20(73.78)540.40(36.24)Lag XL 200 636.74(17.15)207.36(23.19)685.66(20.04)953.04(3.31)503.80(21.51)711.02(246.17)517.26(15.69)360.07(11.12)359.04(160.28)404.00(20.62)49.14(2.60)36.33(3.32)48.74(3.04)30.97(5.69)385.80(56.44)1098.09(29.28)229.31(25.57)507.49(110.36)360.45(160.82)390.60(51.72)",
  "COVDNCOMIXMADDPGFACMACCOVDNCOMIXMADDPGFACMACCOVDNCOMIXMADDPGFACMACCOVDNCOMIXMADDPGFACMAC": "Baseline993.37(7.67)987.34(5.56)1158.81(103.49) 1178.14(13.13)707.18(254.76)47.81(2.53)726.56(240.11) 690.00(265.66)22.72(0.80)27.23(1.36)23.30(1.48) 21.56(0.96)471.99(137.26) 534.00(160.71)439.98(9.61)730.93(166.76)Sparse 5910.63(28.97)913.24(3.53)966.24(3.45)985.03(5.57)909.67(75.78)751.98(215.42) 710.89(253.17)39.28(0.00)28.22(3.82)23.41(0.67)22.05(0.53) 22.41(0.55)371.49(56.22)737.90(90.51)307.31(10.45)664.39(274.71)Sparse 50 357.51(214.37) 170.64(188.91)910.93(19.72)901.54(25.12)999.65(3.65)413.24(217.82) 365.07(265.90) 358.56(260.68)21.15(0.65)23.22(0.26)22.18(0.75) 26.03(2.57)286.90(12.19)312.98(21.99)409.52(49.06)977.40(24.51)Sparse 100 488.37(179.55)61.31(5.82)896.47(39.15)899.15(22.66)1010.32(4.35)413.31(243.48) 219.10(131.83)102.83(36.52)20.28(1.79)21.58(0.46)27.40(4.19) 22.11(0.21)508.62(191.08)309.26(10.43)263.82(15.56)731.75(176.06)Sparse 200 291.94(196.13)12.34(24.74)905.87(17.07)870.28(52.66)1019.50(1.30)755.29(215.79) 366.28(267.00) 406.36(251.71)27.66(3.93) 41.14(17.35) 23.24(0.37) 24.80(2.03)293.90(25.71)327.03(13.54)389.88(119.13)401.46(246.66)Sparse XL 100 862.70(179.55)547.97(5.82)961.81(39.15)987.17(22.66)1013.17(4.35)712.36(243.48)69.06(131.83)785.60(36.52)23.12(1.79)23.46(0.46)22.96(4.19) 22.57(0.21)340.11(191.08)265.49(10.43)260.02(15.56)962.91(176.06)Sparse XL 200 518.74(196.13)652.31(24.74)974.26(17.07)994.67(52.66)1019.21(1.30)763.63(215.79)77.31(267.00)217.94(251.71)23.94(3.93) 39.78(17.35) 23.54(0.37) 28.39(2.03)249.29(25.71)315.90(13.54)271.30(119.13)299.13(246.66)Lag 1769.84(7.67)515.87(5.56)1002.32(103.49) 1077.82(13.13)250.85(254.76)266.31(2.53)404.23(240.11)63.95(265.66)25.47(0.80)28.36(1.36)25.96(1.48) 26.09(0.96)657.45(137.26) 533.82(160.71)434.24(9.61)1076.71(166.76)Lag 5444.94(28.97)364.64(3.53)962.40(3.45)971.26(5.57)999.21(75.78)266.06(215.42) 414.70(253.17)1023.31(0.00)24.54(3.82)31.22(0.67)22.78(0.53) 27.26(0.55)277.99(56.22)377.74(90.51)391.49(10.45)1013.72(274.71)Lag 50 260.69(214.37) 50.01(188.91)909.06(19.72)937.04(25.12)1014.64(3.65)85.18(217.82)694.96(265.90) 383.68(260.68)20.16(0.65)22.67(0.26)26.55(0.75) 25.34(2.57)258.22(12.19)337.57(21.99)499.60(49.06)929.80(24.51)Lag 100 414.25(179.55)226.27(5.82)904.20(39.15)929.69(22.66)1008.45(4.35)478.59(243.48) 377.17(131.83)694.69(36.52)22.38(1.79)23.64(0.46)22.38(4.19) 24.72(0.21)276.62(191.08)287.94(10.43)204.96(15.56)501.91(176.06)Lag 200 510.90(196.13)111.36(24.74)927.35(17.07)896.37(52.66)1007.96(1.30)763.23(215.79)61.20(267.00)95.84(251.71)23.88(3.93) 23.95(17.35) 33.38(0.37) 33.18(2.03)303.89(25.71)310.16(13.54)481.03(119.13) 1021.91(246.66)Lag XL 100 919.13(179.55)603.48(5.82)980.67(39.15)982.36(22.66)1006.33(4.35)800.86(243.48) 694.84(131.83)401.55(36.52)26.16(1.79)22.59(0.46)22.03(4.19) 30.44(0.21)290.60(191.08)282.74(10.43)463.22(15.56)757.86(176.06)Lag XL 200 787.95(196.13)585.44(24.74)993.04(17.07)977.40(52.66)979.42(1.30)440.34(215.79) 115.25(267.00) 361.25(251.71)42.00(3.93) 47.40(17.35) 31.36(0.37) 24.24(2.03)175.67(25.71)313.19(13.54)364.66(119.13) 1006.25(246.66)",
  "B.4Comparison with HDBO Algorithms": "We compare with several related work in High-dimensional bo including TurBO (Eriksson et al., 2019b),AleBO (Letham et al., 2020), LineBO (Kirschner et al., 2019), TreeBO (Han et al., 2021), and GIBO (Mlleret al., 2021). This is presented in . We experienced out-of-memory issues with AleBO after approximately100 iterations, hence the AleBO plots are truncated. We compare against these algorithms at optimizingour hom policy for solving various multi-agent policy search tasks. We validated on Multiagent Ant with 6agents, PredPrey with 3 agents, Het. PredPrey with 3 agents, Drone Delivery with 3 agents, and also Het.PredPrey with 6 agents. We observe that these competing works offer competitive performance for simplertasks such as Multiagent Ant and PredPrey with 3 agents. However for more complex tasks that requirerole based interaction and coordination, our approach outperforms related work. This is evidenced in Het.PredPrey 3, Het. PredPrey 6 as well as the Drone Delivery task with 3 agents. Thus our validation shows that for simpler task, competing related works are able to optimize for simplepolicies of low underlying dimensionality. However, for more complex tasks which require sophisticatedinteraction using both Role and Role Interaction, related work is less capable of optimizing for strong policiesdue to the complexity of the high-dimensional bo task. In contrast, our work offers the capability of findingstronger policies for these complex tasks and scenarios.",
  "), and , [0, r]d, then kij(, ) c exp(d2) for some constant cdependent on r": "Proof. The above is straightforward given the above Lemma. We note that although the RBF kernel maytake on negative values in the domain = [0, r]d, this values experience strong tail decay showing thequasi-satisfaction of our assumptions. The above Lemma and Corollary shows that our assumptions are satisfied by the RBF Kernel when = D,and quasi satisfied when = [0, r]D after choosing a suitable ph and 2h. We show how these assumptionsare quasi-satisfied by the Matern- 5",
  "FProof of Theorem 1": "Our proof of Theorem 1 relies in being able to determine whether an edge does or does not exist in thedependency graph. To be able to do this, we examine the Hessian. As we have shown in Proposition 1,examining the Hessian answers this question. The challenge of Theorem 1 is detecting this dependency undernoisy observations of the Hessian, as well as in domains where the variance of the second partial derivative isoften zero, i.e., kij(, ) = 0 with high probability. To overcome this challenge, we sample the Hessianmultiple times to both find portions of the domain where kij(, ) 2h, and also reduce the effect of thenoise on learning the dependency structure. To proceed with the analysis, we first prove a helper lemmashowing that if we can construct two Normal variables of sufficiently different variances, then its possible toaccurately determine which Normal variable has low, and high variance by taking a singular sample fromeach. This helper lemma will be used later to help determine edges in the dependency graph. As we shallsoon show, If an edge exists, we are able to construct a Normal variable with high variance. Correspondingly,if an edge does not exist, we are able to construct a Normal variable with low variance.",
  "Our proof of Theorem 2 is presented under the same setting and assumptions as the work of Srinivas et al.(2010)": "To prove Theorem 2, we rely on several helper lemmas. The high-level sketch of the proof is to use theproperties of Erds-Rnyi graph to bound both the size of the maximal clique as well as the number ofmaximal cliques with high probability. Once these two quantities are bounded, we are able to analyze themutual information of the kernel constructed by summing the kernels corresponding to the maximal cliquesof the sampled Erds-Rnyi graph as indicated in Assumption 1. Finally, once this mutual information isbounded, we use similar analysis as Srinivas et al. (2010) to complete the regret bound.",
  "with probability 1": "Now that we have bounded both the number of cliques, as well as the sizes of the maximal cliques withhigh probability, we now consider the mutual information of the kernel constructed by summing the kernelscorresponding to the maximal cliques of the dependency graph. Lemma 6. Define I(yA; v) H(yA) H(yA | v) as the mutual information between yA and vwith H(N(, )) 12 log|2e| as the entropy function.Define kT maxA:|A|=T I(yA; v) whenv GP(0, k (, )).Let [ki]i=1,...,M be arbitrary kernels defined on the domain with upper boundson mutual information [kiT ]i=1,...,M, then the following holds true:",
  "To prove the above, we first state Weyls inequality for convenience:": "Lemma 7. Let H, P Rnn be two Hermitian matrices and consider the matrix M = H + P.Leti, i, i, i = 1, . . . , n be the eigenvalues of M, H, and P respectively in decreasing order. Then, for alli r + s 1 we havei r + s.",
  "> L ae(L/b)2, i = 1, . . . , D": "Let kT (d) : N R be a monotonically increasing upper bound function on the mutual information of kernel ktaking d arguments. Let k (, ) be four times differentiable on the continuous domain [0, r]d for somebounded r (i.e., compact and convex). For any 1, 2, 3, 4, 5, 6 (0, 1). Let, t t T0C1 and let",
  "log1/pg D/3 + 1where cb is some constant dependent on 5": "Proof. The proof is a consequence of the helper lemmas and theorems we have proved. First we considerPhase 1 of dss-gp-ucb where t T0. By Theorem 1, at most T0C1 = C21 queries will be made during Phase1, and Lemma 8 indicates the maximum regret for any query. Consulting the respective Theorem and Lemma,we are able to bound the cumulative regret during Phase 1 by:",
  "HOn the Surrogate Hessian, H": "In .5 we remarked that although we cannot observe Hv, we can observe a surrogate Hessian, Hwhich is related to Hv by the chain rule. We justify our choice here with showing how H is an importantsub-component of Hv (Skorski, 2019). Although the reasoning we give is in one dimension, an analogousargument can be made in arbitrary dimensions using the chain rule for vector-valued functions yielding theHessian tensor (Magalhes, 2020). We have v : R is a function of the policy and can be expressed as acomposition of functions:",
  "Hv = r + H g": "where r, g, and H arise from the corresponding highlighted terms in Eq. 6 with r representing some unknownremainder term and representing the Hadamard product. Given the above, it is straightforward to seehow H serves as a surrogate Hessian for Hv. Indeed if r = H g and g has no zero entries thenH = 0 = Hv = 0. In our use case, we are most concerned with non-zero entries in the Hessian, Hv, andthe surrogate Hessian, H is well served for determining Hv = 0 due to the above.",
  "Our drone delivery task was inspired by recent research work in studying unique problems in drone deliveryvehicle routing problems (Dorling et al., 2017)": "Drones fly from delivery point to delivery point where completing a delivery gives a large amount of reward,but running out of fuel and collisions give a small amount of negative reward. After completing a delivery,the delivery point is randomly removed within the environment. A collision gives a small amount of negativereward and momentarily stops the drone. Completing a delivery refills the drone fuel and allows it to continueto make more deliveries. The amount of reward given increases quadratically with the distance of the deliveryto highly reward long distance deliveries which require long term planning. To compound this requirementfor long term planning, fuel consumption also dramatically increases at high velocities to encourage long-termfuel efficiency planning. In this complex scenario requiring long term planning, rl approaches can easily fallinto local minima of completing short distance, low reward deliveries and fail to sufficiently explore (undersparse reward) policies which complete long distance deliveries with careful planning.",
  "JReplot With Timesteps": "We replot the relevant figures in and while maintaining total environment interactions as thesingular independent variable. We note that there is no significant change to our conclusions as a consequenceof this replotting. We also highlight that although total environment interactions is considered the importantindependent variable in rl and marl, in bo typically the total evaluated policies is considered the moreimportant independent variable as each evaluation is assumed to be costly.",
  "LTables With best found policy so far in RL": "We generate new tables investigating rl and marl under sparse or malformed reward. In and 8 weshow the value of the best found policy during the training process for rl and marl. Our observations andconclusions remain the same where rl and marl performance severely degrades under sparse and malformedreward and is often outperformed by our dss-gp-ucb approach."
}