{
  "Abstract": "Expanding reinforcement learning (RL) to offline domains generates promising prospects,particularly in sectors where data collection poses substantial challenges or risks. Pivotalto the success of transferring RL offline is mitigating overestimation bias in value estimatesfor state-action pairs absent from data. Whilst numerous approaches have been proposed inrecent years, these tend to focus primarily on continuous or small-scale discrete action spaces.Factorised discrete action spaces, on the other hand, have received relatively little attention,despite many real-world problems naturally having factorisable actions. In this work, weundertake a formative investigation into offline reinforcement learning in factorisable actionspaces. Using value-decomposition as formulated in DecQN as a foundation, we present thecase for a factorised approach and conduct an extensive empirical evaluation of several offlinetechniques adapted to the factorised setting. In the absence of established benchmarks, weintroduce a suite of our own comprising datasets of varying quality and task complexity.Advocating for reproducible research and innovation, we make all datasets available forpublic use alongside our code base.",
  "Introduction": "The idea of transferring the successes of reinforcement learning (RL) to the offline setting is an enticing one.The opportunity for agents to learn optimal behaviour from sub-optimal data prior to environment interactionextends RLs applicability to domains where data collection is costly, time-consuming or dangerous (Langeet al., 2012). This includes not only those domains where RL has traditionally found favour, such as gamesand robotics (Mnih et al., 2013; Hessel et al., 2018; Kalashnikov et al., 2018; Mahmood et al., 2018), but alsoareas in which online learning presents significant practical and/or ethical challenges, such as autonomousdriving (Kiran et al., 2022) and healthcare (Yu et al., 2021a). Unfortunately, taking RL offline is not as simple as naively applying standard off-policy algorithms to pre-existing datasets. A substantial challenge arises from the compounding and propagation of overestimationbias in value estimates for actions absent from data (Fujimoto et al., 2019b). This bias stems from theunderlying bootstrapping procedure used to derive such estimates and subsequent maximisation to obtainpolicies, whether implicit such as in Q-learning or explicit as per actor-critic methods (Levine et al., 2020).Fundamentally, agents find it difficult to accurately determine the value of actions not previously encountered,and thus any attempt to determine optimal behaviour based on these values is destined to fail. Online, such",
  "inaccuracies can be compensated for through continual assimilation of environmental feedback, but offlinesuch a corrective mechanism is no longer available": "In response to these challenges, there has been a plethora of approaches put forward that aim to both curbthe detrimental effects of overestimation bias as well as let agents discover policies that improve over thosethat collected the data to begin with (Levine et al., 2020). The last few years in particular have seen awide variety of approaches proposed, making use of policy constraints (Fujimoto et al., 2019b; Zhou et al.,2021; Wu et al., 2019; Kumar et al., 2019; Kostrikov et al., 2021b; Fujimoto & Gu, 2021), conservativevalue estimation (Kostrikov et al., 2021a; Kumar et al., 2020), uncertainty estimation (An et al., 2021;Ghasemipour et al., 2022; Bai et al., 2022; Yang et al., 2022; Nikulin et al., 2023; Beeson & Montana, 2024)and environment modelling (Kidambi et al., 2020; Yu et al., 2021b; Argenson & Dulac-Arnold, 2020; Janneret al., 2022; Yu et al., 2020; Swazinna et al., 2021), to name just a few. Each approach comes with its ownstrengths and weaknesses in terms of performance, computational efficiency, ease of implementation andhyperparameter tuning. To date, most published research in offline RL has focused on either continuous or small-scale discrete actionspaces. However, many complex real-world problems can be naturally expressed in terms of factorised actionspaces, where global actions consist of multiple distinct sub-actions, each representing a key aspect of thedecision process. Examples include ride-sharing (Lin et al., 2018), recommender systems (Zhao et al., 2018),robotic assembly (Driess et al., 2020) and healthcare (Liu et al., 2020). Formally, the factorised action space is considered the Cartesian product of a finite number of independentdiscrete action sets, i.e. A = A1 ... AN, where Ai contains ni (sub-)actions and N corresponds to thedimensionality of the action space. The total number of actions, often referred to as atomic actions, is thusNi=1 ni, which can undergo combinatorial explosion if N and/or ni grow large. For example, in robotics aset of actions could correspond to moving a joint up, down or not at all, i.e. ni = 3. For machines withmultiple joints the global action is the set of individual actions for each joint. The total number of possibleglobal actions is thus 3N which grows exponentially as the number of joints increases. In recognition of this possibility, various strategies in the online setting have been devised to preserve theeffectiveness of commonly used discrete RL algorithms (Tavakoli et al., 2018; Tang & Agrawal, 2020). Theconcept of value-decomposition (Seyde et al., 2022) is particularly prominent, wherein value estimates for eachsub-action space are computed independently, yet are trained to ensure that their aggregate mean convergestowards a universal value. The overall effect is to reduce the total number of actions for which a value needsto be learnt from a product to a sum, making problems with factorisable actions spaces much more tractablefor approaches such as Q-learning. Referring back to the robotics example, by using a decomposed approachthe number of actions requiring value estimates is reduced from 3N to 3N. In this work, we undertake an initial investigation into offline RL in factorisable action spaces.Usingvalue-decomposition, we show how a factorised approach provides several benefits over a standard atomicaction representation.We conduct an extensive empirical evaluation of a number of offline approachesadapted to the factorised setting using a newly created benchmark designed to test an agents ability to learncomplex behaviours from data of varying quality. In the spirit of advancing research in this area, we provideopen access to these datasets as well as our full code base: To the best of our knowledge, this investigation represents the first formative analysis of offline RL infactorisable action spaces.We believe our work helps pave the way for developments in this importantdomain, whilst also contributing to the growing field of offline RL more generally.",
  "(s,a,r,s)B(Q(s, a) y(r, s))2 ,(1)": "where y(r, s) = r + maxa Q(s, a) is referred to as the target value, and B denotes a batch of transitionssampled uniformly at random from a replay buffer B. To promote stability during training, when calculatingQ-values in the target it is common to use a separate target network Q(s, a) (Mnih et al., 2013; Hesselet al., 2018) with parameters updated towards either via a hard reset every specified number of steps,or gradually using Polyak-averaging. In the offline setting, an agent is prohibited from interacting with the environment and must instead learnsolely from a pre-existing dataset of interactions B = (sb, ab, rb, sb), collected from some unknown behaviourpolicy (or policies) (Lange et al., 2012). This lack of interaction allows errors in Q-value estimates tocompound and propagate during training, often resulting in a complete collapse of the learning process (Fuji-moto et al., 2019b). Specifically, Q-values for out-of-distribution actions (i.e. those absent from the dataset)suffer from overestimation bias as a result of the maximisation carried out when determining target values(Thrun & Schwartz, 1993; Gaskett, 2002). The outcome is specious Q-values estimates, and policies derivedfrom those estimates consequently being highly sub-optimal. In order to compensate for this overestimationbias, Q-values must be regularised by staying close to the source data (Levine et al., 2020).",
  "Decoupled Q-Network": "By default, standard Q-learning approaches are based on atomic representations of action spaces (Sutton &Barto, 2018). This means that, in a factorisable action space, Q-values must be determined for every possiblecombination of sub-actions. This potentially renders such approaches highly ineffective due to combinatorialexplosion in the number of atomic actions. Recalling that the action space can be thought of as a Cartesianproduct, then for each Ai we have that |Ai| = ni, and so the total number of atomic actions is Ni=1 ni.This quickly grows unwieldly as the number of sub-action spaces N and/or number of actions within eachsub-action space ni increase. To address this issue, the Branching Dueling Q-Network (BDQ) proposed by Tavakoli et al. (2018) learnsvalue estimates for each sub-action space independently and can be viewed as a single-agent analogue toindependent Q-learning from multi-agent reinforcement learning (MARL) (Claus & Boutilier, 1998). Seydeet al. (2022) expand on this work with the introduction of the Decoupled Q-Network (DecQN), whichcomputes value estimates in each sub-action space independently, but learns said estimates such that theirmean estimates the Q-value for the combined (or global) action. Such an approach is highly reminiscent ofthe notion of value-decomposition used in cooperative MARL (Sunehag et al., 2017; Rashid et al., 2020b;a;Du et al., 2022), with sub-action spaces resembling individual agents.",
  "i=1maxaiU ii(s, ai)": "As each utility function only needs to learn about actions within its own sub-action space, this reduces thetotal number of actions for which a value must be learnt to Ni=1 ni, thus preserving the functionality ofestablished Q-learning algorithms. Whilst there are other valid decomposition methods, in this work we focusprimarily on the decomposition proposed in DecQN. In Appendix H we provide a small ablation justifyingour choice.",
  "Offline RL": "Numerous approaches have been proposed to help mitigate Q-value overestimation bias in offline RL. In BCQ(Fujimoto et al., 2019b), this is achieved by cloning a behaviour policy and using generated actions to formthe basis of a policy which is then optimally perturbed by a separate network. BEAR (Kumar et al., 2019),BRAC (Wu et al., 2019) and Fisher-BRC (Kostrikov et al., 2021a) also make use of cloned behaviour policies,but instead use them to minimise divergence metrics between the learned and cloned policy. One-step RL(Brandfonbrener et al., 2021; Gulcehre et al., 2020a) explores the idea of combining fitted Q-evaluation andvarious policy improvement methods to learn policies without having to query actions outside the data.This is expanded upon in Implicit Q-learning (IQL) (Kostrikov et al., 2021b), which substitutes fitted Q-evaluation with expectile regression. TD3-BC (Fujimoto & Gu, 2021) adapts TD3 (Fujimoto et al., 2018) tothe offline setting by directly incorporating behavioural cloning into policy updates, with TD3-BC-N/SAC-BC-N (Beeson & Montana, 2024) employing ensembles of Q-functions for uncertainty estimation to alleviateissues relating to overly restrictive constraints as well as computational burden present in other ensemblesbased approaches such as SAC-N & EDAC (An et al., 2021), MSG (Ghasemipour et al., 2022), PBRL (Baiet al., 2022) and RORL (Yang et al., 2022). In the majority of cases the focus is on continuous action spaces, and whilst there have been adaptationsand implementations in discrete action spaces (Fujimoto et al., 2019a; Gu et al., 2022), these tend to onlyconsider a small number of (atomic) actions. This is also reflected in benchmark datasets such as D4RL (Fuet al., 2020) and RL Unplugged (Gulcehre et al., 2020b). Our focus is on the relatively unexplored area offactorisable discrete action spaces.",
  "Action decomposition": "Reinforcement learning algorithms have been extensively studied in scenarios involving large, discrete actionspaces. In order to overcome the challenges inherent in such scenarios, numerous approaches have been putforward based on action sub-sampling (Van de Wiele et al., 2020; Hubert et al., 2021), action embedding(Dulac-Arnold et al., 2015; Gu et al., 2022) and curriculum learning (Farquhar et al., 2020). However, suchapproaches are tailored to handle action spaces comprising numerous atomic actions, and do not inherentlytackle the complexities nor utilise the structure posed by factorisable actions. For factorisable action spaces various methods have been proposed, such as learning about sub-actionsindependently via value-based (Sharma et al., 2017; Tavakoli et al., 2018) or policy gradient methods (Tang& Agrawal, 2020; Seyde et al., 2021). Others have also framed the problem of action selection in factorisableaction spaces as a sequence prediction problem, where the sequence consists of the individual sub-actions(Metz et al., 2017; Pierrot et al., 2021; Chebotar et al., 2023).",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Individual performance comparison for dog-trot n = {3, 10, 30, 50, 75, 100}.Figures are meannormalised scores one standard error, with 0 and 100 representing random and expert policies, respectively.At the bottom of the table we also provide totals, split by data quality and the entire set. We see across alldatasets that DecQN-IQL/OneStep perform best, followed by DecQN-CQL and DecQN-BCQ.",
  "A case for factorisation and decomposition in offline RL": "As mentioned in .2, value-decomposition provides a mechanism for overcoming challenges in stan-dard Q-learning arising from exponential growth in the number of atomic actions. This can be seen mostclearly from a practical standpoint, in which the number of actions that require value estimation is sig-nificantly reduced when moving from atomic to factorised action representation (see for example).However, even in cases where atomic representation remains feasible, there are still several benefits to afactorised and decomposed approach which are particularly salient in the offline case. To see this, we begin by making the assumption that Q-value estimates from function approximation Q(s, a)carry some noise (s, a) (Thrun & Schwartz, 1993). This noise can be modelled as a random variable, suchthat Q(s, a) = Q(s, a) + (s, a), where Q(s, a) is the true but unknown Q-value for policy . For clarity,we emphasise this noise stems from the fact the function approximator cannot represent the true Q-functionprecisely, as opposed to other factors such as sampling variation. As noted by Thrun & Schwartz (1993), theexpected overestimation will be maximal if all actions for a particular state share the same target Q-value.Incorporating all these ideas into the Q-learning framework we can define the target difference as",
  "= maxa (s, a).(4)": "To incorporate decomposition as defined in Equation 2, the above can be extended to utility value estimates(Ireland & Montana, 2023).Now we assume that utility value estimates from function approximationU ii(s, ai) carry some noise i(s, ai), such that U ii(s, ai) = U ii (s, a) + i(s, ai), where U ii (s, ai) is the truebut unknown utility value for policy i. Under the additional assumption that true Q-values decompose inthe same way as Equation 2 (Ireland & Montana, 2023), we can define the target difference under DecQN as",
  "(5)": "We can expand on each of these by considering the composition of global actions in terms of in-distributionand out-of-distribution sub-actions.When actions are represented atomically, a particular global actiona is either in-distribution or out-of-distribution, depending on its presence (or absence) in the dataset.However, under a factorised representation a global action a is composed of sub-actions (a1, a2, ..., aN), and",
  ";(7)": "where we have dropped the prime symbol to avoid overloading notation. Now the maximum is taken over apooled sample of errors from two different distributions as opposed to a single distribution in Equations 4and 5. The important point to note here is that in both cases the target difference is now dependent on the rela-tive number of in-distribution and out-of-distribution actions/sub-actions. Since errors from in-distributionactions/sub-actions are smaller than for out-of-distribution, the larger the number of in-distributionactions/sub-actions the smaller the expected overestimation. In the offline setting, we cannot change thestatus of an action/sub-action from out-of-distribution to in-distribution through environment interaction.However, as noted above, we can improve the coverage of sub-actions through factorisation. Hence, for thesame dataset we can potentially reduce overestimation bias moving from an atomic action representation toa factorised one. This is particularly beneficial in the offline setting where issues relating to overestimationbias primarily stem from Q-value estimates for out-of-distribution actions.",
  "Appropriateness of decomposition in the offline setting": "The previous analysis assumes both the approximate and true Q-function decompose as per Equation 2. Ingeneral, such a decomposition is unlikely to perfectly capture intricate aspects of an environment. Nonethe-less, such a decomposition has been shown to be effective in the online setting (Ireland & Montana, 2023),and we note that under the following conditions it can act as reasonable approximation, allowing us to retainthe aforementioned benefits of improved action/sub-action coverage.",
  "Weak inter-action dependence": "If the effect of each sub-action on the reward or the next state is relatively independent of other sub-actions,the decomposition is more likely to hold. This condition aligns with the notion of action independence infactored MDPs, where the transition dynamics can be decomposed across sub-actions with minimal cross-interaction.In such cases, the global Q-function can be closely approximated by a sum of local utilityfunctions. This can also be tied to work on factored MDPs (Kearns & Koller, 1999), where state and actionspace factorisations allow efficient policy computation under certain independence assumptions. Of course,true independence of sub-actions in an MDP might be rare in practice, especially when complex dependenciesexist between sub-actions (e.g., coordination between multiple control signals). Still, if dependencies areweak, factorisation can be a good approximation. Its also important to emphasise this only holds undercertain independence assumptions that often arise in factored MDPs but may not be present in more generalMDPs.",
  "Approximately factorisable reward structures": "The decomposition is also reasonable when the reward function can be roughly expressed as a sum overindividual sub-actions, which is again related to the literature on reward decomposition in factored MDPs(Guestrin et al., 2003). In scenarios where the global reward is a combination of local rewards for each sub-action, factorisation provides a natural approximation for value function decomposition. The caveat is that,in many real-world scenarios, rewards are not perfectly factorisable, and there could be interactions betweensub-actions that affect the total reward in a non-linear way. Thus, such an approximation may break downin complex environments with highly entangled reward structures. However for certain structured problems,especially those with weak interactions, this approximation can still yield practical benefits.",
  "Limitations of decomposition in the offline setting": "It is important to highlight that while the benefits of a decomposed approach in the offline setting can offsetshortcomings in modelling inter-action dependence, there are limitations. For settings where sub-actionsare strongly dependent, the value of a global action may differ significantly to the average value of itsconstituent utilities, introducing errors that are no longer outweighed by the reduction in overestimationbias from factorisation/decomposition. As an example, consider the phenomena of pharmacodynamic and pharmacokinetic drug interactions. Ina healthcare setting, it is common for patients to receive multiple drugs as part of a treatment regime,with dosages and routes adjusted based on how the patient responds. If our goal is to train an offline RLto optimise treatment regimes, the level of interaction (positive or negative) between drugs/dosages/routewould dictate whether a factorised/decomposed approach would be appropriate.If there is little to nointeraction then a factorised/decomposed approach could be effective. If there are strong interactions afactorised/decomposed approach would likely fail to capture these important characteristics and hence bemuch less effective.",
  "DecQN-BCQ": "Batch Constrained Q-learning (BCQ) (Fujimoto et al., 2019b;a) is a policy constraint approach to offlineRL. To compensate for overestimation bias for out-of-distribution actions, a cloned behaviour policy isused to restrict the actions available for target Q-values estimates, such that their probability under thebehaviour policy meets a relative threshold . This can be adapted and integrated into DecQN by cloningseparate behaviour policies ii for each sub-action dimension and restricting respective sub-actions availablefor corresponding target utility value-estimates. The target value from Equation (3) becomes:",
  "DecQN-CQL": "Conservative Q-learning (CQL) (Kumar et al., 2020) attempts to combat overestimation bias by targeting Q-values directly. The loss in Equation (1) is augmented with a term that pushes-up on Q-value estimates foractions present in the dataset and pushes-down for all others. Under one particular variant this additionalloss takes the formLCQL() =",
  "where aj denotes the jth sub-action within the ith sub-action space, and is a hyperparameter that controlsthe overall level of conservatism. The full procedure can be found in Algorithm 2": "Note this particular implementation does not directly substitute the decomposition as per Equation 2 intothe original CQL loss. Instead, we apply CQL directly to the utility values and then take the mean acrosssub-action dimensions. This way we avoid having to estimate Q-values for all atomic actions (which a directsubstitution would require) and we retain the equivalence to behavioural cloning at the sub-action level as",
  "s,aB[L2(Q(s, a) V(s))] ;": "where if we denote u = Q(s, a) V(s) then L2(u) = | 1(u < 0)|u2 is the asymmetric least squares forthe (0, 1) expectile. The state-action value function Q(s, a) is trained using the same loss as Equation(1), with the target value now y(r, s) = r + V(s).",
  "We can derive an alternative approach to IQL which removes the requirement for a separate state valuefunction altogether. Noting that V (s) =": "a (a | s)Q(s, a), we can instead use the cloned behaviour policy(a | s) and state-action value function Q(s, a) to calculate the state value function V (s) instead. Thiscan be adapted and incorporated into DecQN by replacing Q(s, a) with its decomposed form as per Equation(2) and adjusting the policy to reflect a sub-action structure. We denote this approach DecQN-OneStep asit mirrors one-step RL approaches that train state value functions using fitted Q-evaluation (Brandfonbreneret al., 2021). The full procedure can be found in Algorithm 4.",
  "Environments and datasets": "At present, there are relatively few established environments/tasks specifically designed for factorised actionspaces. As such, there is an absence of benchmark datasets akin to those available for continuous or small-scale discrete action spaces such as D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020b). Inlight of this, we introduce our own benchmarking suite constituting the following environments and tasks. Maze: A simple maze-based environment first introduced by Chandak et al. (2019) in which an agent istasked with reaching a target goal location. The state space is continuous and comprises the agents currentlocation in the maze. The agents movement is controlled by a series of N actuators which can be turnedon or off, with each actuator corresponding to a unit of movement in a single direction, with actuator iapplying force at an angle of 2i/N when activated, as illustrated in . The net outcome of actuatorselection is the vectoral summation of the movements associated with each actuator. The agents actionspace comprises the unique combination of the set of binary decision with respect to each actuator, leadingto to an action set that is exponential in the number of actuators such that |A| = 2N. DeepMind control suite: A discretised variant of the DeepMind control suite (Tunyasuvunakool et al.,2020), as previously adopted by Seyde et al. (2022); Ireland & Montana (2023). This suite contains a varietyof environments and tasks, which although originally designed for continuous control can easily be repurposedfor a discrete factorisable setting by discretising individual sub-action spaces, thus creating scenarios thatcan vary in size and complexity. This discretisation process involves selecting a subset of actions from theoriginal continuous space which then become the discrete actions available to the agent. For example, a",
  "Experimental evaluation": "We train agents using DecQN, DecQN-BCQ, DecQN-CQL, DecQN-IQL and DecQN-OneStep on our bench-mark datasets and evaluate their performance in the simulated environment. We also train and evaluateagents using a factorised equivalent of behavioural cloning to provide a supervised learning baseline. Per-formance is measured in terms of normalised score, where scorenorm = 100 scorescorerandom scoreexpertscorerandom with 0representing a random policy and 100 the expert policy from the fully trained agent. We repeat exper-iments across five random seeds, reporting results as mean normalised scores one standard error acrossseeds. For each set of experiments we provide visual summaries with tabulated results available in Ap-pendix G for completeness. Full implementation details, including network architectures, hyperparametersand training procedures are provided in Appendix D",
  "Case study: DQN-CQL vs DecQN-CQL": "Before considering the full benchmark, we conduct a case study directly comparing DQN-CQL (i.e. CQLunder atomic action representation) and DecQN-CQL. Following the procedure outlined in weconstruct a medium-expert dataset for the cheetah-run task for number of bins ni {3, 4, 5, 6} andcompare the performance of resulting policies and overall computation time. In addition we construct arandom-medium-expert dataset for the Maze task with N = 15 actuators for varying dataset sizes andcompare performance and computational resources. In we see that as the number of sub-actions ni increases for the cheetah-run task, DQN-CQLexhibits a notable decline in performance, whereas DecQN-CQL performance declines only marginally. Wealso see a dramatic increase in computation time for DQN-CQL whereas DecQN-CQL remains roughlyconstant. For DQN-CQL, these outcomes are symptomatic of the combinatorial explosion in the numberof actions requiring value-estimation and the associated number of out-of-distribution global actions. Theseissues are less prevalent in DecQN-CQL due to its factorised and decomposed formulation.To providefurther insights, in Appendix F we also examine the evolution of Q-values errors during training, findingthese errors are consistently lower for DecQN-CQL than DQN-CQL, aligning with each algorithms respectiveperformance. : Comparisons of performance (left) and computation time (right) for DQN-CQL and DecQN-CQLon the cheetah-run-medium-expert dataset for varying numbers of bins. As the number of bins increases,DQN-CQL suffers notable drops in performance and increases in computation time, whereas DecQN-CQLis relatively resilient in both areas. In we see that as the size of the dataset for the Maze task decreases, DQN-CQL exhibits a morenotable decline in performance than DecQN-CQL. This is particularly the case when the size of the datasetis very small, i.e. 250 transitions. These outcomes support our notion of better action coverage undera factorised representation than atomic, where value estimates for observed sub-actions can be combinedto provide better estimates of global actions that havent been observed.Furthermore, this is achievedthrough much more computationally efficient means, with training time for DecQN-CQL being over 8 timesfaster than DQN-CQL (4mins vs 34mins, respectively) and GPU usage 7 times less (246MBs vs 1728MBs,respectively).",
  "DeepMind Control Suite": "In we summarise each algorithms performance across the full range of tasks, setting the numberof sub-actions ni = 3. This necessitates the use of value-decomposition for all but the most simple tasks,as highlighted in . In general we see that all offline RL methods outperform behavioural cloningacross all environments/tasks and datasets, with the exception of DecQN-BCQ for random-medium-expertdatasets which performs quite poorly. In terms of offline methods specifically, in general DecQN-CQL hasa slight edge over others for lower dimensional tasks such as finger-spin, fish-swim and cheetah-run,whilst DecQN-IQL/OneStep have the edge for higher-dimensional tasks such as humanoid-stand and dog-trot. For medium-expert datasets we see in most cases all offline methods are able to learn expert ornear-expert level policies.Extracting optimal behaviour from random-medium-expert datasets provessignificantly more challenging than in the Maze environment, likely a result of these environments/tasksbeing much more complex coupled with datasets being both highly variable and constituting relatively fewexpert trajectories in relation to trajectory length.",
  "Increasing the number of sub-actions": "To help provide insights into the ability of our chosen offline methods to scale to larger and larger actionspaces, we increase the number of sub-actions within each sub-action space and repeat our experiments. We 1For medium-expert datasets we note that BC is able to achieve expert-level performance on par with our offline methods.We attribute this to the relative simplicity of the environment coupled with a reasonable level of expert-level trajectories withinthe datasets. Such outcomes are not replicated in DMC tasks which are more complex and have much longer trajectories",
  "Discussion and conclusion": "In this work, we have conducted a formative investigation of offline reinforcement in factorisable actionspaces. Through empirical evaluation, we have shown how a factorised and decomposed approach offersnumerous benefits over standard/atomic approaches. Using a bespoke benchmark we have undertaken anextensive empirical evaluation of several offline RL approaches adapted to this factorised and decomposedframework, providing insights into each approachs ability to learn tasks of varying complexity from datasetsof differing size and quality. In general, our empirical evaluation demonstrates our chosen offline methods adapt well to the factorisedsetting when combined with value-decomposition in the form of DecQN. With one exception, all approachesare consistently able to outperform behavioural cloning regardless of data quality, and where datasets contain",
  "David Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline RL without off-policyevaluation. Advances in neural information processing systems, 34:49334946, 2021. 4, 9": "Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip Thomas. Learning actionrepresentations for reinforcement learning. In International conference on machine learning, pp. 941950.PMLR, 2019. 9, 10, 23 Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu,Alexander Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autore-gressive Q-functions. In Conference on Robot Learning, pp. 39093928. PMLR, 2023. 4",
  "Caroline Claus and Craig Boutilier.The dynamics of reinforcement learning in cooperative multiagentsystems. AAAI/IAAI, 1998(746-752):2, 1998. 3": "Driess, Ha, and Toussaint. Deep visual reasoning - learning to predict action sequences for assembly tasks.In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 46454651.IEEE, 2020. 2 Wei Du, Shifei Ding, Lili Guo, Jian Zhang, Chenglong Zhang, and Ling Ding. Value function factorizationwith dynamic weighting for deep multi-agent reinforcement learning. Information Sciences, 615:191208,2022. 3, 5 Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt,Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep reinforcement learning in largediscrete action spaces. arXiv preprint arXiv:1512.07679, 2015. 4 Gregory Farquhar, Laura Gustafson, Zeming Lin, Shimon Whiteson, Nicolas Usunier, and Gabriel Synnaeve.Growing action spaces. In International Conference on Machine Learning, pp. 30403051. PMLR, 2020.4",
  "Chris Gaskett. Q-learning for robot control. PhD thesis, Australian National University, 2002. 3": "Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating uncertaintiesfor offline RL through ensembles, and why their independence matters. Advances in Neural InformationProcessing Systems, 35:1826718281, 2022. 2, 4 Omer Gottesman, Fredrik Johansson, Joshua Meier, Jack Dent, Donghun Lee, Srivatsan Srinivasan, LinyingZhang, Yi Ding, David Wihl, Xuefeng Peng, et al.Evaluating reinforcement learning algorithms inobservational health settings. arXiv preprint arXiv:1805.12298, 2018. 5 Pengjie Gu, Mengchen Zhao, Chen Chen, Dong Li, Jianye Hao, and Bo An. Learning pseudometric-basedaction representations for offline reinforcement learning. In International Conference on Machine Learning,pp. 79027918. PMLR, 2022. 4",
  "Carlos Guestrin, Daphne Koller, Ronald Parr, and Shobha Venkataraman. Efficient solution algorithms forfactored MDPs. Journal of Artificial Intelligence Research, 19:399468, 2003. 7": "Caglar Gulcehre, Sergio Gmez Colmenarejo, Jakub Sygnowski, Thomas Paine, Konrad Zolna, Yutian Chen,Matthew Hoffman, Razvan Pascanu, Nando de Freitas, et al.Addressing extrapolation error in deepoffline reinforcement learning. Offline Reinforcement Learning Workshop at Neural Information ProcessingSystems, 2020, 2020a. 4 Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Thomas Paine, Sergio Gmez, Konrad Zolna, RishabhAgarwal, Josh S Merel, Daniel J Mankowitz, Cosmin Paduraru, et al. Rl unplugged: A suite of benchmarksfor offline reinforcement learning.Advances in Neural Information Processing Systems, 33:72487259,2020b. 4, 9 Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan,Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in deep reinforcementlearning. In Thirty-second AAAI conference on artificial intelligence, 2018. 1, 3 Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado van Hasselt, and DavidSilver. Distributed prioritized experience replay. In International Conference on Learning Representations,2018. 24 Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, andDavid Silver. Learning and planning in complex action spaces. In International Conference on MachineLearning, pp. 44764486. PMLR, 2021. 4 David Ireland and Giovanni Montana. Revalued: Regularised ensemble value-decomposition for factorisableMarkov decision processes. In The Twelfth International Conference on Learning Representations, 2023.5, 7, 9, 21, 24, 25, 28, 29",
  "Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 1, 2, 3": "Kaixiang Lin, Renyu Zhao, Zhe Xu, and Jiayu Zhou.Efficient large-scale fleet management via multi-agent deep reinforcement learning. In Proceedings of the 24th ACM SIGKDD international conference onknowledge discovery & data mining, pp. 17741783, 2018. 2 Siqi Liu, Kay Choong See, Kee Yuan Ngiam, Leo Anthony Celi, Xingzhi Sun, and Mengling Feng. Rein-forcement learning for clinical decision support in critical care: comprehensive review. Journal of medicalInternet research, 22(7):e18477, 2020. 2 Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang Geng, and Sergey Levine. Action-quantizedoffline reinforcement learning for robotic skill learning. In 7th Annual Conference on Robot Learning, 2023.8, 9 A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra. Benchmark-ing reinforcement learning algorithms on real-world robots. In Conference on robot learning, pp. 561591.PMLR, 2018. 1",
  "Yunhao Tang and Shipra Agrawal.Discretizing continuous action space for on-policy optimization.InProceedings of the aaai conference on artificial intelligence, volume 34, pp. 59815988, 2020. 2, 4": "Arash Tavakoli, Fabio Pardo, and Petar Kormushev. Action branching architectures for deep reinforcementlearning. In Proceedings of the aaai conference on artificial intelligence, volume 32, 2018. 2, 3, 4, 28 Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. InProceedings of the Fourth Connectionist Models Summer School, volume 255, pp. 263. Hillsdale, NJ, 1993.3, 5, 21 Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez,Timothy Lillicrap, Nicolas Heess, and Yuval Tassa.dm_control: Software and tasks for continuouscontrol. Software Impacts, 6:100022, 2020. 9",
  "(|A| + 1)2(|A| + 2).(10)": "For |A| > 1, i.e. more than one action, E[Zs] is positive and increasing w.r.t. |A| whereas V ar(Zs) is positiveand decreasing. Hence there is a bias/variance trade-off for overestimation in target Q-values. This trade-off becomes apparent when comparing overestimation for DQN and DecQN, where |A| = Ni=1 nifor the former and |A| = Ni=1 ni for the latter. As shown by Ireland & Montana (2023), if i(s, ai) are alsomodelled as i.i.d. uniform random variables U(b, b), then the expected target difference under DecQN islower than DQN (E[Zdecs ] E[Zdqns]), but the variance is higher (V [Zdecs ] V [Zdqns]). We now consider this bias/variance trade-off in the context of in-distribution and out-of-distributionactions/sub-actions. This time, let U(b, b) and U(kb, kb) be the distribution of errors for in-distributionand out-of-distribution actions/sub-actions, respectively, where k > 1. When all actions/sub-actions areeither in-distribution or out-of-distribution the above conclusions hold, with the additional observation thatboth bias and variance are lower when all actions are in-distribution compared to all out-of-distribution (asb < kb). The picture becomes more complex when there is a mixture of in-distribution and out-of-distributionactions/sub-actions. This stems from the fact there is no longer a closed form for the expectation/varianceof the target difference since errors arise from two different distributions. This is exacerbated by the relativecoverage of sub-actions and atomic actions. In terms of expectation, since this is an increasing function ofboth b and |A| we can deduce the property E[Zdecs ] E[Zdqns] holds regardless of the value of k or the numberof in-distribution actions/sub-actions. The variance is more nuanced since this is an increasing function forb but decreasing for |A|. The value of k and number of in-distribution actions/sub-actions will determinewhether the property V [Zdecs ] V [Zdqns] still holds. To illustrate this, we conduct a series of simulations based on different action/sub-action configurations andcompare how the expectation and variance of the target difference under DQN and DecQN change based onthe coverage of atomic actions. The details are as follows. DQN simulations: Let |Ain| and |Aout| be the number of in-distribution and out-of-distribution atomicactions for a given state, respectively, such that |A| = |Ain| + |Aout|.For |Ain| {0, 1, ..., |A|} sample|Ain| values from a U(b, b) distribution and |A| |Ain| values from a U(kb, kb) distribution and storethe maximum value for the pooled sample. Repeat this 10000 times and calculate the mean and variance ofthese stored maximums. These are the estimates of E[Zdqns] and V [Zdqns]. DecQN simulations Let |Aini | and |Aouti| be the number of in-distribution and out-of-distribution sub-actions in dimension i for a given state, respectively, such that |Ai| = |Aini |+|Aouti|. For |Ain| {0, 1, ..., |A|}sample |Ain| atomic action from all available actions without replacement and calculate the number offactorised actions |Aini | in each sub-action dimension i. In each sub-action dimension i sample |Aini | valuesfrom a U(b, b) distribution and |Ai| |Aini | values from a U(kb, kb) distribution and store the maximumvalue for the pooled sample for each dimension i. Calculate and record the average maximum value acrossall dimensions N (as per the DecQN decomposition). Repeat this 10000 times and calculate the mean and",
  "end for": "that take in a state and output sub-action utility values. We maintained a fixed exploration parameter = 0.1 and updated target network parameters using Polyak-averaging with = 0.005. We used the Adamoptimiser (Kingma & Ba, 2014) with learning rate 3e 4 and a batch size of 256. Once we have trained the DecQNs we create the benchmark datasets by collecting data using a greedypolicy derived from the learned utility values. Each dataset contains 10k transitions. For the expert policywe trained the DecQNs until their test performance approached the maximum return possible (approximately100). For the medium policy we aimed for a test performance of approximately 1/3rd of the expert.",
  "C.2DeepMind Control Suite": "To collect the DMC suite datasets we followed the training procedures laid out by (Seyde et al., 2022;Ireland & Montana, 2023) to train the Decoupled Q-networks.To expedite the data collection processduring training we used a distributed setup using multiple workers to collect data in parallel (Horgan et al.,2018). We parameterised the utility functions using a (shared) single ResNet layer followed by layer norm,followed by a linear head for each of the sub-action spaces which predicts sub-action utility values. Fulldetails regarding network architecture and hyperparameters can be found in . Once we have trained the DecQNs we create the benchmark datasets by collecting data using a greedypolicy derived from the learned utility values. For the case study in .1 each dataset contains 100ktransitions and for the main experiments in .3 each dataset contains 1M transitions. As each episode",
  "DFull implementation details": "For both Maze and DMC environments/tasks, utility functions are parameterised by neural networks, com-prising a 2-layer MLP with ReLU activation functions and 512 nodes, taking in a normalised state as inputand outputting utility values for each sub-action space. We use the same architecture for policies, withthe output layer a softmax across actions within each sub-actions-space. State value functions mirror thisarchitecture except in the final layer which outputs a single value. We train networks via stochastic gradientdescent using the Adam optimiser (Kingma & Ba, 2014) with learning rate 3e4 and a batch size of 256. Forstate and state-action value functions we use the Huber loss as opposed to MSE loss. We set the discountfactor = 0.99 and the target network update rate = 0.005. We utilise a dual-critic approach, takingthe mean across two utility estimates for target Q-values. For the maze task, agents are trained for 100kgradient updates. For the DMC tasks, agents are trained for 1M gradient updates. The only hyperparameters we tune are the threshold in BCQ, conservative coefficent in CQL, expectile and balance coefficient in IQL and balance coefficient in OneStep. We allow these to vary acrossenvironment/task, but to better reflect real-world scenarios where the quality of data may be unknown, weforbid variation within environments/tasks.",
  "D.1Allowing hyperparameter variation within environment/task": "As per , in we provide examples of performance improvement after permitting hyperpa-rameter variation within the same environment/task. In general, we see lower quality datasets benefit fromsmaller hyperparameters (i.e. those that weight more towards RL and less towards BC) and higher qualitydatasets benefit from larger hyperparameters (i.e. those the weight more towards BC and less towards RL).This mirrors findings from previous papers outlined in .",
  "FCase study Q-value errors": "To provide further insights into Q-value errors for the case study in .1, we examine the evolutionof Q-value errors during training. Every 5k gradient updates we obtain a MC estimate of true Q-valuesusing discounted rewards from environmental rollouts. We then compare these MC estimates with Q-valuespredicted by both DQN-CQL and DecQN-CQL networks for the respective actions taken. To make betteruse of rollouts (which can be expensive), we calculate MC estimates and DQN-/DecQN-CQL Q-values forthe first 500 states in the trajectory, as using a discount factor of = 0.99 discounts rewards by over 99%for time-steps beyond 500 (and all tasks considered have trajectory length 1000). In total we perform 10rollouts, giving 5000 estimates of the error between true and DQN-CQL/DecQN-CQL Q-values. In we plot the mean absolute error over the course of training, with the solid line representing the mean acrossfive random seeds and shaded area the standard error. For all values of ni we observe the mean absolute erroris less for DecQN-CQL than DQN-CQL, particularly for ni > 3, aligning with each algorithms respectiveperformance in .",
  "HDecomposition comparisons": "In this Section we compare the DecQN decomposition to two alternative methods that can be used forfactorisable discrete action spaces. The first is based on the Branching Dueling Q-Network (BDQ) proposedby Tavakoli et al. (2018). Using our notation, each utility function is considered its own independent Q-function, i.e.Qii(s, ai) = U ii(s, ai) .(11) Each Q-function is trained by bootstrapping from its own target, and no decomposition is used. That is, thetarget for Qii(s, ai) is given by y = r + maxaiAi Qii(s, ai). The findings of Ireland & Montana (2023)demonstrate that, in the online setting, BDQ is unable to match the performance of DecQN. This is likelycaused by the fact that, as each sub-action space is now learnt independently, the effects of other sub-actions",
  "i=1U ii(s, ai) .(12)": "Whilst this may seem a subtle change, Ireland & Montana (2023) proved that the mean and variance ofthe learning target under this decomposition are both higher than DecQN. Empirical experiments by Seydeet al. (2022); Ireland & Montana (2023) also confirm the inferior performance of the sum decompositioncompared to the mean. In we can see that the sum decomposition is less performant in each of the tasks and datasetsthan the mean.For BDQ, we see that whilst in some cases performance is better than using the sumdecomposition, it is generally still less performant than using the mean decomposition.Owing to theseresults, we focus on the mean decomposition in our main work.",
  "Environment -datasetBCDecQNDecQN-BCQDecQN-CQLDecQN-IQLDecQN-OneStep": "DogTrot (n = 3)-expert98 0.70.1 0.194.4 0.599.5 0.798.9 2.2101.2 1.6-medium-expert62 3.70 0.374.9 3.584.8 3.789.3 1.493.9 1.4-medium43.8 0.50.1 0.149.1 0.646.5 0.552 0.350.2 0.2-random-medium-expert37.2 3.60.1 0.20.1 0.143.4 0.544.1 1.244.9 0.7 DogTrot (n = 10)-expert97.3 1.7-0.3 096.8 1.899.2 1.3106.5 1.1113.4 0.9-medium-expert58.1 5.50.5 0.182.3 4.683.4 2.1105.1 2.1109.8 1.2-medium34.7 0.40.5 0.236.9 0.339.1 0.641 0.847.2 0.4-random-medium-expert33.6 4.40.1 0.16.1 2.833.4 1.552.1 2.345.4 4.1 DogTrot (n = 30)-expert99.7 0.4-0.3 0100.3 2.399.8 0.8102.8 0.4106.4 0.8-medium-expert70.3 4.90.6 0.274.1 890.7 3.798.3 1.6100.7 3-medium33.1 0.20.7 0.233.6 0.433.4 0.439.9 0.140.5 0.5-random-medium-expert22.4 0.90.2 0.15.2 2.222.9 2.531.6 1.330.4 1.8 DogTrot (n = 50)-expert98.2 0.50.5 0.198.6 0.697.7 0.699.2 0.9100.4 1.4-medium-expert67.7 5-0.3 068.7 4.288.9 2.195 2.197.1 1.4-medium34.9 0.4-0.3 036.5 0.736.7 0.245.1 0.643.1 0.7-random-medium-expert21 20.7 0.19.3 1.527.6 1.327.9 2.436.4 1.8 DogTrot (n = 75)-expert98.4 0.80.6 0.299.6 1.198 1101.4 1.4102.9 0.9-medium-expert75.3 2.60 074.9 16.785 289.9 0.797.9 2.5-medium41.4 0.20.5 0.141.3 0.842.5 0.148.5 0.447.8 0.5-random-medium-expert27.7 0.70.7 0.20 0.133.4 245.3 2.943.6 2.6 DogTrot (n = 100)-expert96.4 1.10.1 095 1.8100.4 0.7103.2 0.4105.5 0.8-medium-expert70.9 6.50.4 0.174.6 5.874.4 5.193.9 1101.5 2.5-medium34.7 0.80.1 0.134.5 0.534.8 0.740.9 0.241.5 0.5-random-medium-expert18.9 2.70.3 0.10 012.7 2.424 2.826.2 1 Sum-expert588 5.20.7 0.4584.7 14594.6 5.1612 6.4629.8 6.4-medium-expert404.3 28.21.2 0.7449.5 64.9507.2 18.7571.5 8.9600.9 12-medium222.6 2.51.6 0.7231.9 5.8233 2.5267.4 2.4270.3 2.8-random-medium-expert160.8 14.32.1 0.830.2 17.6173.4 10.2225 12.9226.9 12-all1375.7 50.25.6 2.61296.3 102.31508.2 36.51675.9 30.61727.9 33.2"
}