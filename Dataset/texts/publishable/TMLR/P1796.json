{
  "Abstract": "Generative models have recently achieved remarkable success and widespread adoption insociety, yet they often struggle to generate realistic and accurate outputs. This challengeextends beyond language and vision into fields like engineering design, where safety-criticalengineering standards and non-negotiable physical laws tightly constrain what outputs areconsidered acceptable.In this work, we introduce a novel training method to guide agenerative model toward constraint-satisfying outputs using negative data examples ofwhat to avoid. Our negative-data generative model (NDGM) formulation easily outperformsclassic models, generating 1/6 as many constraint-violating samples using 1/8 as much datain certain problems. It also consistently outperforms other baselines, achieving a balancebetween constraint satisfaction and distributional similarity that is unsurpassed by any othermodel in 12 of the 14 problems tested. This widespread superiority is rigorously demonstratedacross numerous synthetic tests and real engineering problems, such as ship hull synthesiswith hydrodynamic constraints and vehicle design with impact safety constraints. Ourbenchmarks showcase both the best-in-class performance of our new NDGM formulationand the overall dominance of NDGMs versus classic generative models. We publicly releasethe code and benchmarks at",
  "Introduction": "Generative models have demonstrated impressive results in vision, language, and speech. However, even withmassive datasets, they struggle with precision, often generating physically impossible images or factuallyincorrect text responses. These mistakes are examples of constraint violation; ideally, generative modelswould be constrained to only generate valid correct samples. While constraint violation is a nuisancein image or text synthesis, it is a paramount concern in domains like engineering design with high-stakes(including safety-critical) constraints. Generative models synthesizing designs for car or airplane components,for example, may be subject to geometric restrictions (such as colliding components), functional requirements(such as load-bearing capacity), industry standards, and manufacturing limitations.",
  "(d) Negative data allowsgenerative models to bettersatisfy constraints throughtighter density estimates": ": Negative data helps generative models learn real-world data distributions, which often have gapsin their support caused by constraints. For example, by examining bike frames with disconnected components,a model can better learn to generate geometrically valid frames. As generative models are increasingly applied to engineering problems, their blatant violation of objective,ubiquitous, and non-negotiable constraints becomes increasingly problematic. We assert that this constraint-violation issue is largely attributable to the fact that generative models are classically shown only positive(constraint-satisfying) datapoints during training, and are never exposed to negative (constraint-violating)datapoints to avoid. Completely satisfying constraints using this training approach is equivalent to learning abinary classification problem with only one class present in the data, a challenging task. Instead, by studyingnegative data in addition to positive data, generative models can better avoid constraint-violating samplesduring generation (). This aligns with their distribution-matching objective since negative datapointsshould have near-zero density in the original real-world distribution that the model is trying to mimic. Wewill refer to models that train using negative data as negative-data generative models, or NDGMs. Although many existing generative modeling formulations, such as binary class-conditional models, can besimply adapted into NDGMs, specialized NDGMs have also been proposed. The prior state-of-the-art (SOTA)method suffers from two major shortcomings: It never specifically learns the density ratio between the positiveand negative data and it often suffers from mode collapse. We conceptualize and test a new NDGM thatovercomes these issues through the use of a multi-class discriminative model that learns individual densityratios and a Determinantal-Point-Process (DPP)-based loss that encourages diverse sample sets. Throughextensive benchmarking, we demonstrate that our new formulation outperforms both simple baselines andthe current state-of-the-art NDGMs on highly non-convex test problems, a variety of real-world engineeringtasks, and high-dimensional image-based tasks. Our key contributions are summarized below. (i) We introduce a new NDGM which significantly outperforms the constraint satisfaction of vanillagenerative models while addressing mode collapse issues in current NDGMs. These advancementsare enabled by estimating individual density ratios and introducing a diversity-based training loss. (ii) We evaluate our model on an expansive set of benchmarks including specially-constructed testproblems, authentic engineering tasks featuring real-world constraints from engineering standards,and a final high-dimensional topology optimization study. We compare our model to 10 baselinetraining formulations spanning adversarial models, variational autoencoders, and diffusion models. (iii) We demonstrate that our model frequently achieves an unmatched tradeoff between constraintsatisfaction and distribution learning, in some cases attaining 95-98% lower constraint violation thanclassic generative models, while achieving top-three distributional similarity scores. (iv) We show that our NDGM model can significantly outperform vanilla models, generating 1/6 as manyconstraint-violating samples using only 1/8 as much data. This makes our model an excellent choicein data-constrained problems involving constraints.",
  "Constraints in Engineering and Design": "Constraints are ubiquitous in design. A designer creating ship hulls, for example, must adhere to a medley ofgeometric constraints, performance requirements, and safety regulations from authoritative bodies. Generatingconstraint-satisfying designs can be exceedingly difficult. As many practitioners turn to data-driven generativemodels to tackle engineering problems (Regenwetter et al., 2022a), this difficulty remains (Woldseth et al.,2022; Regenwetter et al., 2023). For example, even a generative model that sees 30K examples of valid shiphulls can only generate valid hulls with a 6% success rate in our experiments. The overwhelming majority of deep generative models in design do not actively consider constraints (Woldsethet al., 2022; Regenwetter et al., 2022a), despite constraint satisfaction being an explicit goal in many ofthe design problems they address (Oh et al., 2019; Nie et al., 2021; Bilodeau et al., 2022; Chen et al., 2022;Chen & Fuge, 2019; Cheng et al., 2021). Several engineering design datasets feature constraint-violatingdesigns (Regenwetter et al., 2022b; Bagazinski & Ahmed, 2023; Giannone & Ahmed, 2023; Maz & Ahmed,2023), and many others have checks for validity (Whalen et al., 2021; Wollstadt et al., 2022), allowing datasetsof constraint-violating (negative) designs to be curated. In some cases, datasets of positive examples are evencreated through search by rejecting and discarding negative samples (Bagazinski & Ahmed, 2023; Regenwetteret al., 2022b), making negative data essentially free. In any problem where negative data is available or canbe generated, NDGMs can be applied.",
  "Divergence Minimization in Generative Models": "Before discussing divergence minimization in NDGMs, we first discuss divergence minimization in conventionalgenerative models. Let pp(x) be the (positive) data distribution and p(x) the distribution sampled by thegenerative model. Given N samples from pp(x), the objective of generative modeling is to find a setting of , such that, for an appropriate choice of discrepancy measure, p pp. A common choice for thisdiscrepancy measure is the KullbackLeibler or KL divergence:",
  "= arg minKL[ppp].(2)": "In practice, direct optimization of Eq. (2) is often intractable. As such, it is common in deep generativemodeling to learn by using either a tractable lower bound to a slightly different variant of Eq. (2) (Kingma& Welling, 2013; Burda et al., 2015; Ho et al., 2020; Snderby et al., 2016) or by using plug-in or directestimators of the divergence measure (Casella & Berger, 2002; Sugiyama et al., 2012a; Gutmann & Hyvrinen,2010; Srivastava et al., 2017; Goodfellow et al., 2014; Srivastava et al., 2023; Poole et al., 2019). In bothof these cases, under certain conditions, as N , theoretically, it holds that, . However, sinceN is limited, there remains a finite discrepancy between the model and data distributions. This mismatchoften manifests in p allocating high probability mass in regions where pp may not have significant empiricalsupport. In domains such as engineering design, where invalid (negative) designs tend to be very close to thevalid (positive) designs, this leads to the generation of invalid designs with high probability. This lack ofprecision underpins the relatively limited success of deep generative models in the engineering design domain(Regenwetter et al., 2023).",
  "Divergence minimization in GANs.Generative Adversarial Networks (GANs) (Goodfellow et al., 2014;": "Arjovsky et al., 2017; Mohamed & Lakshminarayanan, 2016; Srivastava et al., 2017; Nowozin et al., 2016) area powerful framework for generating realistic and diverse data samples. GANs have two main components:a generator f, which generates samples according to the density p, and a discriminator f, which is a binary classifier. The generator learns to generate synthetic data samples by transforming random noise intomeaningful outputs, while the discriminator aims to distinguish between real and generated samples. Thestandard GAN loss can be written as:",
  "L(, ) = Epp(x)[log f(x)] + Ep(x)[1 log(f(x))].(3)": "Training a GAN involves iterating over min max L(, ).GANs can also be interpreted in terms ofestimating the density ratio (Gutmann & Hyvrinen, 2010; Srivastava et al., 2017) between the data andthe generated distribution r(x) = pp(x)/p(x). This ratio can be estimated by a discriminative model asr = f(x)/(1 f(x)) and r = 1 gives us p = pp. The optimal discriminator prediction and generatordistribution are:",
  "(p(x) + pp(x)), p(x) = pp(x).(4)": "Divergence minimization in other generative models.Many other types of generative models similarlyminimize divergence between p and pp. These models include popular likelihood-based models like VariationalAutoencoders (VAEs) (Kingma & Welling, 2013) and Denoising Diffusion Probabilistic Models (DDPMs) (Hoet al., 2020). We will not discuss the mathematics behind divergence minimization for these likelihood-basedmodels, but we do benchmark several variants in our results. In general, we refer to unaugmented GANs,VAEs, and DDPMs as vanilla models throughout the paper.",
  "Negative-Data Generative Models (NDGMs)": "In this section, we discuss the NDGM framework. We explain how generative models can be adjusted toexploit negative data to improve constraint satisfaction. Let pn denote the negative distribution i.e., thedistribution of constraint-violating datapoints. A key assumption in the negative data formulation is that ppand pn have nearly mutually exclusive support. Instead of training using only the positive distribution pp,we now seek to train a generative model using both pp and pn. In this section, we discuss several existingmethods to do so. These methods range from simple baselines like auxiliary classifiers to state-of-the-artformulations like discriminator overloading. 2.3.1Class ConditioningClass conditional modeling is a simple approach to incorporate constraints into generative models, whichis popular in many design generation problems (Nie et al., 2021; Behzadi & Ilie, 2021; Maz & Ahmed,2023; Shin et al., 2023; Heyrani Nobari et al., 2021). In conditional modeling, a generative model typicallyconditions on the constraints denoted as c and learns a conditional distribution, p(x|c), where x representsthe generated output. Off-the-shelf class-conditional models can be simple NDGMs, where the positiveand negative data each constitute one class. During inference, the model attempts to satisfy constraints bygenerating conditionally positive samples. Broadly speaking, the negative data formulation for generativemodels can be seen as a specific case of class-conditional generation. However, as we demonstrated inAppendix A, generic class-conditional training formulations for generative models are not as effective asspecialized NDGMs. 2.3.2Classifier-Augmented Generative ModelsAnother common approach to actively satisfy constraints using generative models involves training a supervisedmodel to predict constraint satisfaction. After training, this supervised model is subsequently queried duringgenerative model training or inference. Often, this model predicts constraint violation likelihood, though itcan also predict intermediates that are combined in a more complex constraint check (Wang et al., 2022).Typically, this classifier f learns:",
  "pn(x) + pp(x).(5)": "This frozen classifier can be incorporated into the training of a generative model by adding an auxiliary loss,LP C to the generative models loss, LGM to calculate a total loss, LT ot = LGM + LP C, as in (Regenwetter& Ahmed, 2022). Here, is some weighting parameter and LP C is expressed as:",
  "LP C = Ep(x)[log f(x)].(6)": "Pre-trained classifiers for constraints can also be applied during inference in certain models, such as indiffusion model guidance (Maz & Ahmed, 2023; Giannone & Ahmed, 2023). They can also work alongside atrained generative model as a rejection-sampling postprocessing step, which is discussed in Appendix D. 2.3.3Discriminator Overloading (DO)Discriminator overloading is a technique to directly incorporate negative data into GAN model training.This formulation was proposed in two of the first papers to train a generative model using both positive andnegative data (though we have made slight modifications for generality): Rumi-GAN (Asokan & Seelamantula,2020) and Negative Data Augmentation GAN (NDA-GAN) (Sinha et al., 2021). We refer to these formulationsas discriminator overloading since the discriminator is overloaded by learning to discriminate between (1)positives and (2) fakes or negatives. As such, the discriminator estimates:",
  "Methodology": "NDGMs suffer from a widespread tendency toward mode collapse seen across a variety of baselines. Critically,the state-of-the-art GAN-DO training formulation is particularly prone to this issue due to its conflationof fakes and negatives during training. To address these shortcomings, we propose a new NDGM trainingformulation that introduces two new innovations: First, and central to our approach, we propose to learn theratios between the positive, negative, and fake distributions individually, rather than conflating the negativesand fakes. Second, we add an auxiliary diversity-based training objective to NDGM training which directlymitigates mode collapse. We describe these innovations in detail below.",
  "Learning Individual Density Ratios Using a Multi-Class Discriminator": "GAN-DO learns a density ratio between pp and an amalgamation of pn and p. Rather than conflatingthe negative data and fake samples, we instead advocate to learn pairwise density ratios between the threedistributions. Noting that multi-class classifiers are strong density ratio estimators (Srivastava et al., 2023),we propose to learn these ratios using a multi-class discriminator. This multi-class discriminator model learnsto discriminate three classes: positive, negative, and fake, thereby learning their pairwise density ratios:",
  "pp(x) + p(x) + pn(x) c p, n, .(9)": "Note that f,p is a reweighted version of Eq. 7. Though this multi-class formulation is similar to discriminatoroverloading, instead of showing the discriminator a weighted amalgamation of fakes and negatives (as in DO),the multi-class discriminator instead treats fakes and negatives as separate classes, and can potentially refineits knowledge by distinguishing them. Complemented by a generator model which tries to maximize f,p(x),this classifier fulfills the role of the discriminator in an adversarial training formulation. Notably, f,p/f,nestimates pp/pn, which is never directly learned in the discriminator overloading formulation. We also note that there are numerous other solutions to learn density ratios between positive and negativedata distributions. For example, a direct estimator of pp/pn can operate alongside the classic discriminator ina two-discriminator NDGM variant. We discuss the general motivation for density ratio learning in NDGMsin Appendix C and the mathematical formulation behind the double discriminator variant in Appendix E.",
  "Addressing Mode Collapse Using a Diversity-Based Loss": "When augmented with validity-based training objectives, NDGMs tend to collapse in valid regions of thesample space. This effectively allows them to excel in validity and precision, but struggle with recall anddiversity. This tendency arises because a conservative NDGM will avoid regions of the distribution nearthe constraint boundary, resulting in incomplete coverage. One approach to improve recall is to explicitlyencourage diversity of generated samples.Diversity is often a desired goal in generative modeling forengineering design applications (Regenwetter et al., 2023). As Chen & Ahmed (2021a) note, incorporatingdiversity can also help models generalize and avoid mode collapse. Determinantal Point Process (DPP)-baseddiversity measures (Kulesza et al., 2012) have been used in a variety of generative applications in design (Chen& Ahmed, 2021b; Nobari et al., 2021; Regenwetter & Ahmed, 2022) and elsewhere (Elfeki et al., 2019; Mothilalet al., 2020). The DPP loss is calculated using a positive semi-definite DPP kernel S. Entries of this matrix are calculatedusing some modality- and problem-dependent similarity kernel, such as the Euclidean distance kernel.The (i, j)th element of S can be expressed in terms of the similarity kernel k and samples xi and xj asSi,j = k(xi, xj), and the loss as:",
  "Experiments and Results": "We now present experiments on (i) 2D densities, where we benchmark 11 different models including ours,the SOTA, baseline NDGMs, and vanilla models; (ii) A dozen diverse engineering tasks featuring real-worldconstraints from regulatory authorities and engineering standards, where we demonstrate the potency of ourapproach (iii) a high-dimensional free-form topology optimization problem where we explore the impact ofnegative data quality. We also perform ablation studies and data-efficiency studies. We include significantadditional visualization and statistical testing for experimental results in Appendix A. Details on datasetsand model training are included in Appendix F.",
  "Extensive Benchmarking on 2D Densities with Constraints": "We first construct a pair of 2D densities as easy-to-visualize tests for NDGM models to visually showcase theircharacteristics. Despite being low-dimensional and relatively structured, these problems are very challengingfor vanilla models and NDGMs alike. Notably, constraint-violating regions appear in close proximity tohigh-density regions of the valid data distribution. For a model to succeed, precise estimation of constraintboundaries is essential.",
  "(h) GAN-MDD (Ours)": ": Generated distributions from select generative models on Problem 2, a uniform distribution withmany circular invalid regions. Positive data points and samples are shown in blue and negative ones in black.Our proposed NDGM model, GAN-MDD learns the distribution most faithfully. : Comparison of F1 scores () and invalidity rates () for benchmarked models on Problem 1 (left)and Problem 2 (right). Mean scores over six instantiations are plotted. Scores closer to the bottom left aremore optimal. Triangular markers indicate that the score lies off the plot in the indicated direction. Classconditioning, classifier loss, and guidance are denoted with (CC), (CLF), and (Guided), respectively. Models. We test 11 variants of GAN, VAE, and DDPM models. Among these are: three vanilla models(GAN, VAE, DDPM), trained only on positive data; three class conditional models (GAN, VAE, DDPM),trained on both positive and negative data in a binary class conditional setting as in .3.1; threemodels augmented with a frozen pre-trained validity classifier to steer models during training (GAN, VAE)or during inference using guidance (DDPM), as in .3.2; GAN with discriminator overloading as inNDA-GANs (Sinha et al., 2021) and Rumi-GANs (Asokan & Seelamantula, 2020) (GAN-DO) (.3.3);Our multi-class-discriminator GAN with diversity (GAN-MDD). Each model is tested six times. Metrics. We seek to measure each models reliability in constraint-satisfaction, distribution learning ability,and ability to avoid mode collapse. We therefore score each model on three metrics: 1) Invalidity thefraction of generated samples that violate the constraints (negative samples). 2) F1 score for generativemodels, a common distributional similarity metric proposed in Sajjadi et al. (2018). 3) DPP Diversity score, ametric which highlights mode collapse and is measured as described in .2. The ideal model maximizesF1 and minimizes invalidity and DPP diversity. Results. Figures 2 and 3 plot the datasets and the generated distributions of a subset of the tested models.All models distributions are plotted in Figures 13 and 14 in Appendix A. Compared to baseline models, ourGAN-MDD model learns the best estimate of the data distribution while avoiding constraint violation. plots mean F1 scores and invalidity rates on both problems for all models. The scores confirmthat our GAN-MDD achieves an optimal tradeoff between statistical similarity and constraint satisfaction. contains the numerical scores, with mean and standard deviations over the six training runs, as wellas statistical significance testing using 2-sample t-tests. Our GAN-MDD model significantly outperformsthe state-of-the-art GAN-DO in distributional similarity and diversity while achieving similar constraint-satisfaction performance.",
  "Is Negative Data More Valuable than Positive Data?": "Given an infinite amount of data, model capacity, and computational resources, generative models cantheoretically approach an exact recovery of the underlying data distribution, pp. In practical scenarios,however, data and computational throughput are limited, particularly in fields like engineering design andscientific research. Thus, simply increasing the volume of data is not a viable strategy to improve constraintsatisfaction. Fortunately, we find that NDGMs can be significantly more data-efficient than vanilla generativemodels, giving them a significant advantage in data-constrained domains. : Study of invalidity rates for GAN-MDDtrained with different numbers of positive datapoints(Np) and negative datapoints (Nn). Note that GAN-MDD without negative data (Nn = 0) trains as a vanillaGAN. Diversity loss is turned off. Scores are averagedover four instantiations. Lower is better. NDGMs cangenerate significantly fewer constraint-violating samples,even when trained on orders of magnitude less data.",
  "Nn = 1K2.3%1.6%1.9%Nn = 4K0.6%0.7%0.7%Nn = 16K0.5%0.2%0.2%": ": Comparison of invalidity rate () andnumber of datapoints () under various mixtures ofpositive and negative data on Problem 2. Pointsare labeled using their proportion of negative data.Any amount of negative data tested improves per-formance. Interestingly, a small proportion of nega-tive data (20%) yields better constraint satisfactionthan higher proportions in this problem. In and , we explore how the number of positive and negative training datapoints (Np andNn, respectively) affect invalidity rates on the 2D test problems. Full scores with standard deviations andstatistical tests are included in . To remove confounding factors, we benchmark a fixed-size GAN-MDDmodel without diversity loss (note: GAN-MDD is equivalent to a vanilla GAN when trained with no negativedata). We test four model instantiations per dataset mixture. We find that for pure positive datasets, adding more data yields diminishing returns, while mixing in negativedata drastically improves constraint satisfaction. For example, in Problem 1, with Np = 1K, Nn = 1K,GAN-MDD generates 1/6 as many invalid samples with 1/8 as much data compared to Np = 16K, Nn = 0.On Problem 2 with Np = 4K, Nn = 1K, GAN-MDD generates 1/5 as many invalid samples with 1/3 asmuch data compared to Np = 16K, Nn = 0. Since NDGMs can be significantly more data-efficient thanvanilla models, practitioners seeking to improve their generative models may attain much more value bycollecting even a small amount of negative data, rather than additional positive data. This study also promptsan interesting research question: What is the optimal ratio of positive to negative data? A rigorous answer,though certainly problem dependent, could lead to more efficient dataset generation and curation.",
  "Ablation Study: Examining the Effect of Diversity Loss": "In many negative data settings, a diversity-based training objective can serve to modulate the tradeoffbetween constraint satisfaction and distributional similarity. To showcase this, we examine the performanceof GAN-MDD over a sweep of diversity loss weights ( from Eq. 11). Simultaneously, we examine theperformance of GAN-DO augmented with our DPP-based diversity loss. These experiments illustrate theeffect of the diversity loss in modulating the tradeoff between constraint satisfaction and distributionalsimilarity. They also serve as an experimental ablation study to confirm the effect of our two NDGMinnovations: the multi-class discriminator (versus discriminator overloading) and the diversity-based loss. We visually showcase distributions generated by GAN-MDD and GAN-DO under different weights of inFigures 6 and 7. As expected, higher diversity yields better distributional similarity but poorer constraint satisfaction in both models across both problems. GAN-MDD generates very neat distributions with higherdiversity, creating the most visually similar sampled distribution compared to the ground-truth distribution.In contrast, GAN-DO never truly captures the ground truth distribution, despite overcoming its moreegregious distribution collapse issues.",
  "(f) = 0.2": ": Generated distributions from the baseline GAN-DO for Problem 1 (a-c) and Problem 2 (d-f),demonstrating effect of diversity loss weight, . Although adding diversity loss eases mode collapse issues,GAN-DO with large diversity weight struggles with high invalidity. : Comparison of GAN-MDD and GAN-DO invalidity rates () and F1 scores () for a varietyof diversity weights on Problem 1 (left) and Problem 2 (right). Gray markers indicate scores for otherbenchmarked models. Mean scores over six instantiations are plotted. Scores closer to the bottom left aremore optimal. Triangular markers indicate that the score lies off the plot in the indicated direction. Exactscores are included in . We present a summary of numerical scores in , with full scores in . These scores illustrateGAN-MDDs widespread dominance across a variety of diversity weights. Across all weights tested, GAN-MDD achieves significantly better distributional similarity scores than GAN-DO. Furthermore, for everynonzero diversity weight, GAN-MDD achieves significantly better constraint satisfaction.",
  "Negative-Data Generative Models Excel in Engineering Tasks": "Generative models are commonly used to tackle engineering problems with constraints (Oh et al., 2019; Nieet al., 2021), but are often criticized for their inability to satisfy them (Woldseth et al., 2022; Regenwetteret al., 2023). To assess how NDGMs fare in real engineering problems, we have curated a benchmarkof a dozen diverse engineering tasks, which are discussed in detail in Appendix F. These problems spannumerous engineering disciplines including assorted industrial design tasks (compression spring, gearbox, heatexchanger, pressure vessel), structural and material design tasks (Ashby chart, cantilever beam, reinforcedconcrete, truss, welded beam), and several complex high-level design problems: Ship hulls with hydrodynamicconstraints; bike frames with loading requirements; automobile chassis with performance requirements inimpact testing. A variety of constraints are applied, including engineering standards from authoritative bodieslike the American Concrete Institute (ACI), the American Society of Mechanical Engineers (ASME), and theEuropean Enhanced Vehicle-Safety Committee (EEVC). As a select example, we visualize several positiveand negative datapoints from the FRAMED bike frame dataset (Regenwetter et al., 2022b) in .",
  "(b) Constraint-violating (negative) bike frames": ": Example visualization for the bike frame engineering problem. The FRAMED bike framedataset describes a 37-dimensional 3D parametric CAD problem that defines dozens of geometric constraints(disconnected components, negative tube thickness, etc.). Vanilla generative models generate a variety ofconstraint-violating bikes, while NDGMs like GAN-MDD reliably generate valid bikes. We evaluate a vanilla GAN, the state-of-the-art GAN-DO, and our GAN-MDD. We measure invalidity rate,F1 score, and DPP diversity scores over seven training runs for each problem. For each problem and metric,we evaluate whether models are significantly (p < 0.05) superior to one another using one-sided 2-samplet-tests. A summary of benchmarking results is presented in , with detailed results in Tables 3, 4,and 5. We summarize key results as follows: Invalidity Score: GAN-DO and GAN-MDD are the highest performers in invalidity score, sig-nificantly outperforming a vanilla GAN in almost all problems. GAN-DO is more often the topperformer, but sometimes falls short of GAN-MDD.",
  "F1 Score: GAN-MDD is the winner in distributional similarity by a small margin over the GAN.However, the state-of-the-art GAN-DO falls significantly short of both the GAN and GAN-MDD inmany problems": "Diversity Score: GAN-MDD is the overwhelming winner in diversity score, significantly outper-forming both the GAN and GAN-DO in the majority of problems, and achieving the highest meanscore in all but one problem. In summary, our GAN-MDD is able to achieve significantly better sample validity and diversity than vanillamodels while maintaining generally comparable distributional similarity scores. In contrast, the state-of-the-art GAN-DO is able to achieve marginally higher sample validity, but does so at the expense of distributionalsimilarity and sample diversity. Unless constraint satisfaction is favored above all else, we foresee thatGAN-MDD will offer a more optimal tradeoff of performance considerations for most users in most problems. : Invalidity rates, F1 scores, and diversity scores for 12 engineering datasets. We benchmark a vanillaGAN, a GAN with discriminator overloading (DO), and a GAN with a multi-class discriminator and diversityloss (MDD). Mean scores over seven training instances are presented, with best scores for each metric oneach problem underlined. Lower invalidity rates and diversity scores are better, while higher F1 scores arebetter. For each dataset and metric, we compare models in a pairwise fashion and identify when a modeloutperforms a competitor to a statistically significant degree (details in Tables 3-5). These pairwise winsare tallied across all problems and presented in the last row.",
  "Examining Negative Data Quality in High-Dimensional Constrained Engineering Problems": "Having tested a variety of tabular engineering problems, we next consider whether our proposed methodscan translate to higher-dimensional domains such as images. We examine a common engineering designproblem known as topology optimization (TO), which seeks to optimally distribute material in a spatialdomain to achieve a certain objective (often minimizing mechanical compliance) (Sigmund & Maute, 2013).Simply put, TO is often used to create structures with high rigidity and low weight. The use of generativemodels for TO is very popular (Shin et al., 2023), but existing methods have been criticized for significantshortcomings (Woldseth et al., 2022) related to constraint satisfaction, such as generated topologies not beingfully connected. Disconnected topologies tend to be highly sub-optimal and are impractical to fabricate.",
  "(c) Rejection-sampled negatives": ": Topology optimization is a challenging structural engineering problem that searches for optimalplacements of material. Valid (positive) structural topologies are completely continuous (left). Invalid(negative) topologies have floating material and can be procedurally-generated by artificially adding smallfloating patches of material (middle). They can also be collected through rejection-sampling of topologiescreated by a generative model (right). Constraint violation is annotated with red circles. We train NDGMs using disconnected topologies as negative data, using the classification guidance datasetfrom Maz & Ahmed (2023). The positive data is comprised of optimized, spatially continuous structures,while the negative data is largely comprised of procedurally-generated negatives with artificially-added floatingcomponents. For comparison, we create an alternative negative dataset by replacing procedurally-generatednegatives in the dataset with rejection-sampled topologies generated by a vanilla GAN trained on thepositive data. A simple continuity check flags any discontinuous topologies to add to the rejection-samplednegative dataset. A few topologies from each dataset are visualized in . We hypothesize that theserejection-sampled negatives are harder negatives (closer to the positive distribution) and are hence moreinformative than the procedurally-generated negatives. : Means and standard deviations of performance metrics for the topology optimization problemover six tests with pairwise statistical significance comparisons. Lower scores are better. A models symbol(//) is shown if statistically significant in a pairwise comparison. Best mean scores are bolded.",
  "(e) GAN-MDD usingrejected negatives": ":Visualization of topologies generated by a GAN trained only on positive topologies andGAN-DO/GAN-MDD models additionally trained on procedurally-generated negatives or rejection-samplednegatives. Valid samples are marked with a blue check mark while invalid samples are marked with a red X.Additional samples are visualized in Figures 15 through 19. We run five types of experiments: GAN models trained on only positive data, GAN-DO and GAN-MDDtrained on procedurally-generated data, and GAN-DO and GAN-MDD trained on rejection-sampled data.Some generated samples are visualized in . We evaluate six instantiations of each experiment andevaluate pairwise comparisons using 2-sample t-tests with p < 0.05 significance. In evaluating models, wemeasure the proportion of generated topologies with disconnected components (invalidity rate), as well asthe average fraction of image pixels disconnected from the largest continuous structure in each generatedtopology (violation magnitude). To identify mode collapse, we also measure DPP in the pixel space (diversityscore). Numerical scores are presented in . These results suggest several noteworthy conclusions: 1. For both types of negative data, GAN-MDD outperforms GAN-DO in mean scores in every metric.This difference is statistically significant for invalidity rate score on procedurally-generated negativesand for violation magnitude score and diversity score on rejection-sampled negatives. 2. GAN-DO significantly outperforms the vanilla GAN in invalidity rate only for rejection-samplednegatives, suggesting that it is more sensitive to negative data quality than our GAN-MDD. As seenin , GAN-DO can suffer from severe mode collapse. GAN-DO scores are also the mostvariable, indicating unpredictable training, stability, and convergence. 3. All NDGMs trained on rejection-sampled negatives achieve better mean scores in every metric thanany NDGM trained on procedurally-generated negatives. This indicates that negative data qualitycan be even more impactful than the choice of NDGM type. It also highlights the potency of rejectionsampling as a negative data sampling strategy when a black-box constraint check is available. In summary, our benchmarking on the topology optimization problem illustrates the potency of GAN-MDDon a high-dimensional image-based problem but also illustrates the importance of negative data qualityin NDGM performance. Best practices to generate high-quality negative data remain an open researcharea. Regardless, GAN-MDD is the highest overall performer on either type of negative data, particularly inconstraint satisfaction rate. : A batch of 32 random topologies generated by a select GAN-DO instantiation trained onrejection-sampled negatives. Samples fall under just a handful of data modes, indicating egregious modecollapse. A similar grouping is shown for a GAN-DO model trained on procedurally-generated negatives in.",
  "Discussion & Conclusion": "Adding pairwise density ratio estimation and diversity to NDGMs. We presented a new NDGMformulation, GAN-MDD, which estimates individual density ratios, rather than conflating fakes and negativesas done in the current SOTA. Our model also incorporates a Determinantal Point Process-based diversity loss.These innovations empower GAN-MDD to achieve an optimal tradeoff between constraint satisfaction anddistributional similarity, allowing it to outperform baseline models and existing NDGMs alike, across a varietyof problems. In specific benchmarks, GAN-MDD manages to generate 1/6 as many constraint-violatingsamples as a vanilla model using only 1/8 as much data. GANs versus diffusion models using negative data. Despite the growing popularity of diffusion models,GANs remain state of the art in many engineering design problems. On the challenging 2D problems, we findthat: 1) vanilla DDPMs struggle to learn constraints and 2) DDPMs using guidance based on negative datacan only achieve good constraint satisfaction at the expense of distributional similarity. This preliminarystudy indicates that DDPMs struggle to achieve as optimal of a tradeoff between constraint satisfaction anddistributional similarity as our specialized adversarial NDGM model, at least in some problems. We lookforward to future research which advances the capabilities of negative data diffusion models. NDGMs are underutilized. We believe NDGMs are underutilized in engineering design. This assertion issubstantiated by several observations: 1) The widespread use of vanilla models in engineering design despitetheir limitations (Regenwetter et al., 2022a). 2) The relatively low cost of collecting negative data versuspositive data in many engineering contexts. 3) The overwhelming dominance of NDGMs over vanilla modelsin our engineering benchmarks. 4) The data-efficiency improvements we demonstrated using negative data. Generating high-quality negative data. Selecting strategies to generate negative data is an importantresearch question. In the final case study on topology optimization, rejection-sampling resulted in strongernegative data than the procedural generation method. It also required access to an oracle (constraintevaluator), which may be unavailable or prohibitively expensive in some applications. However, there arenot always cheap, viable procedural generation approaches for negative data either. Effective negative datageneration remains largely problem-dependent and the relative quality of negative data generation approachesis not necessarily straightforward. We anticipate that domain-agnostic methods to generate high-qualitynegative data could pair well with NDGMs and expand their impact. Leveraging negative data for classifier-based rejection sampling. Negative data can be used totrain supervised binary classification models to predict constraint satisfaction. These classifiers can then beapplied during generative model inference to iteratively reject and regenerate any samples projected to violateconstraints until a sufficiently large sample set is attained. If the generative model has a very low validityrate, this can increase the inference cost by orders of magnitude and add significant stochasticity to thegeneration process. To generate a single ship hull, for example, we estimate that a vanilla generative modelwould take a projected 33 times longer, or 222 times longer in a reasonable bad-case scenario, comparedto an NDGM. Although expensive, this rejection sampling approach yields extremely strong validity rates,surpassing even the most accurate NDGMs. Though in many senses an apples-to-oranges juxtaposition,comparing NDGMs with rejection sampling offers hints about the theoretical limits of NDGM performance.Classifier-based rejection sampling using negative data is discussed in more detail in Appendix D. Limitations.As we demonstrate, NDGMs are sensitive to the quality of their negative training data.Although negative data is often cheaper than positive data in engineering design problems, generatinghigh-quality negative data may be challenging in some domains. In other domains, sourcing any kind ofnegative data may be impossible. In domains where high-quality negative data is unavailable, NDGMs willnaturally be impractical.",
  "Conclusion": "In this paper, we presented a new negative-data generative model (NDGM), GAN-MDD, that innovatesover previous methods by learning individual pairwise density ratios and avoiding mode-collapse using adiversity-based loss. In extensive benchmarks across constructed tests and a dozen real engineering problems,we demonstrated that GAN-MDD outperforms 10 other formulations. Moreover, it is data-efficient, generating1/6 as many invalid samples as a vanilla model using 1/8 as much data. Finally, we showed that GAN-MDDcan excel in challenging high-dimensional problems. GAN-MDDs data efficiency and optimal balancebetween distributional similarity and constraint satisfaction make it significantly more practical than existinggenerative models for engineering and design tasks. Thus, we anticipate and advocate for the more widespreaduse of NDGMs in data-driven engineering design and other constrained generative modeling domains.",
  "arXiv preprint arXiv:2305.08279, 2023": "Mohammad Mahdi Behzadi and Horea T. Ilie.GANTL: Toward Practical and Real-Time TopologyOptimization With Conditional Generative Adversarial Networks and Transfer Learning. Journal ofMechanical Design, 144(2), 12 2021. ISSN 1050-0472. doi: 10.1115/1.4052757. URL 021711. Martin Philip Bendse and Noboru Kikuchi. Generating optimal topologies in structural design using ahomogenization method. Computer Methods in Applied Mechanics and Engineering, 71(2):197224, 111988. ISSN 00457825. doi: 10.1016/0045-7825(88)90086-2. URL Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. Generative modelsfor molecular discovery: Recent advances and challenges. Wiley Interdisciplinary Reviews: ComputationalMolecular Science, 12(5):e1608, 2022. Ramin Bostanabad, Yichi Zhang, Xiaolin Li, Tucker Kearney, L Catherine Brinson, Daniel W Apley,Wing Kam Liu, and Wei Chen. Computational microstructure characterization and reconstruction: Reviewof the state-of-the-art techniques. Progress in Materials Science, 95:141, 2018. Andrew Brock, Theodore Lim, James Millar Ritchie, and Nick Weston. Context-aware content generationfor virtual environments. In International Design Engineering Technical Conferences and Computers andInformation in Engineering Conference, volume 50084, pp. V01BT02A045. American Society of MechanicalEngineers, 2016.",
  "George Casella and Roger L. Berger. Statistical Inference. Duxbury, Pacific Grove, CA, 2002": "Michael Chang, Alyssa L Dayan, Franziska Meier, Thomas L Griffiths, Sergey Levine, and Amy Zhang. Neuralconstraint satisfaction: Hierarchical abstraction for combinatorial generalization in object rearrangement.arXiv preprint arXiv:2303.11373, 2023. Hongrui Chen and Xingchen Liu. Geometry enhanced generative adversarial networks for random heteroge-neous material representation. In International Design Engineering Technical Conferences and Computersand Information in Engineering Conference, IDETC-21, Virtual, Online, Aug 2021. ASME. Qiuyi Chen, Jun Wang, Phillip Pope, Wei Chen, and Mark Fuge. Inverse design of two-dimensional airfoilsusing conditional generative models and surrogate log-likelihoods. Journal of Mechanical Design, 144(2):021712, 2022.",
  "Wei Chen, Kevin Chiu, and Mark Fuge. Aerodynamic design optimization and shape exploration usinggenerative adversarial networks. In AIAA Scitech 2019 Forum, pp. 2351, 2019": "Yu Cheng, Yongshun Gong, Yuansheng Liu, Bosheng Song, and Quan Zou. Molecular design in drug discovery:a comprehensive review of deep generative models. Briefings in bioinformatics, 22(6):bbab344, 2021. Kristy Choi, Chenlin Meng, Yang Song, and Stefano Ermon. Density ratio estimation via infinitesimalclassification. In International Conference on Artificial Intelligence and Statistics, pp. 25522573. PMLR,2022. Matthew Dering, James Cunningham, Raj Desai, Michael A Yukish, Timothy W Simpson, and Conrad STucker. A physics-based virtual environment for enhancing the quality of deep generative designs. InInternational Design Engineering Technical Conferences and Computers and Information in EngineeringConference, volume 51753, pp. V02AT03A015. American Society of Mechanical Engineers, 2018.",
  "Shrinath Deshpande and Anurag Purwar. Computational creativity via assisted variational synthesis ofmechanisms using deep generative models. Journal of Mechanical Design, 141(12), 2019": "Shrinath Deshpande and Anurag Purwar. An Image-Based Approach to Variational Path Synthesis ofLinkages. Journal of Computing and Information Science in Engineering, 21(2), 10 2020. ISSN 1530-9827.doi: 10.1115/1.4048422. URL 021005. Mohamed Elfeki, Camille Couprie, Morgane Riviere, and Mohamed Elhoseiny. Gdpp: Learning diversegenerations using determinantal point processes. In International conference on machine learning, pp.17741783. PMLR, 2019.",
  "International Design Engineering Technical Conferences and Computers and Information in EngineeringConference, volume 87301, pp. V03AT03A012. American Society of Mechanical Engineers, 2023": "Giorgio Giannone, Akash Srivastava, Ole Winther, and Faez Ahmed. Aligning optimization trajectories withdiffusion models for constrained design generation. Advances in Neural Information Processing Systems,36, 2024. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processingsystems, NIPS14, pp. 26722680, Cambridge, MA, USA, 2014. MIT Press. Michael Gutmann and Aapo Hyvrinen. Noise-contrastive estimation: A new estimation principle forunnormalized statistical models. In Proceedings of the thirteenth international conference on artificialintelligence and statistics, pp. 297304. JMLR Workshop and Conference Proceedings, 2010. Jessica B Hamrick, Kelsey R Allen, Victor Bapst, Tina Zhu, Kevin R McKee, Joshua B Tenenbaum, andPeter W Battaglia. Relational inductive bias for physical construction in humans and machines. arXivpreprint arXiv:1806.01203, 2018. Amin Heyrani Nobari, Wei (Wayne) Chen, and Faez Ahmed. RANGE-GAN: Design Synthesis UnderConstraints Using Conditional Generative Adversarial Networks. Journal of Mechanical Design, pp. 116,09 2021. ISSN 1050-0472. doi: 10.1115/1.4052442. URL Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information ProcessingSystems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL",
  "Runze Li, Yufei Zhang, and Haixin Chen. Learning the aerodynamic design of supercritical airfoils throughdeep reinforcement learning. AIAA Journal, pp. 114, 2021": "Xiang Li, Shaowu Ning, Zhanli Liu, Ziming Yan, Chengcheng Luo, and Zhuo Zhuang. Designing phononiccrystal with anticipated band gap through a deep learning based data-driven method. Computer Methodsin Applied Mechanics and Engineering, 361:112737, 2020. Siyan Liu, Zhi Zhong, Ali Takbiri-Borujeni, Mohammad Kazemi, Qinwen Fu, and Yuhao Yang. A case studyon homogeneous and heterogeneous reservoir porous media reconstruction by using generative adversarialnetworks. Energy Procedia, 158:61646169, 2019. Zhaocheng Liu, Lakshmi Raju, Dayu Zhu, and Wenshan Cai. A hybrid strategy for the discovery and designof photonic structures. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 10(1):126135, 2020.",
  "Lukas Mosser, Olivier Dubrule, and Martin J Blunt. Reconstruction of three-dimensional porous media usinggenerative adversarial neural networks. Physical Review E, 96(4):043309, 2017": "Ramaravind K Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers throughdiverse counterfactual explanations. In Proceedings of the 2020 conference on fairness, accountability, andtransparency, pp. 607617, 2020. Zhenguo Nie, Tong Lin, Haoliang Jiang, and Levent Burak Kara. Topologygan: Topology optimization usinggenerative adversarial networks based on physical fields over the initial domain. Journal of MechanicalDesign, 143(3):031715, 2021.",
  "Sharad Rawat and MH Herman Shen.Application of adversarial networks for 3d structural topologyoptimization. Technical report, SAE Technical Paper, 2019": "Lyle Regenwetter and Faez Ahmed. Design target achievement index: A differentiable metric to enhancedeep generative models in multi-objective inverse design. In International Design Engineering TechnicalConferences and Computers and Information in Engineering Conference, volume 86236, pp. V03BT03A046.American Society of Mechanical Engineers, 2022. Lyle Regenwetter, Brent Curry, and Faez Ahmed. BIKED: A dataset and machine learning benchmarksfor data-driven bicycle design. In International Design Engineering Technical Conferences and Computersand Information in Engineering Conference, IDETC-21, Virtual, Online, Aug 2021. ASME.",
  "Ari Seff, Wenda Zhou, Nick Richardson, and Ryan P Adams. Vitruvion: A generative model of parametriccad sketches. arXiv preprint arXiv:2109.14124, 2021": "Shashank Sharma and Anurag Purwar. Path synthesis of defect-free spatial 5-ss mechanisms using machinelearning. In International Design Engineering Technical Conferences and Computers and Information inEngineering Conference, volume 83990, pp. V010T10A034. American Society of Mechanical Engineers,2020. Conner Sharpe and Carolyn Conner Seepersad. Topology design with conditional generative adversarialnetworks. In International Design Engineering Technical Conferences and Computers and Information inEngineering Conference, volume 59186, pp. V02AT03A062. American Society of Mechanical Engineers,2019.",
  "Casper Kaae Snderby, Tapani Raiko, Lars Maale, Sren Kaae Snderby, and Ole Winther. Ladder variationalautoencoders. In Advances in neural information processing systems, pp. 37383746, 2016": "Akash Srivastava, Lazar Valkov, Chris Russell, Michael U. Gutmann, and Charles Sutton. Veegan: Reducingmode collapse in gans using implicit variational learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,volume 30, pp. 33083318. Curran Associates, Inc., 2017. URL Akash Srivastava, Seungwook Han, Kai Xu, Benjamin Rhodes, and Michael U. Gutmann. Estimatingthe density ratio between distributions with high discrepancy using multinomial logistic regression.Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL",
  "Ren Kai Tan, Nevin L Zhang, and Wenjing Ye. A deep learningbased method for the design of microstructuralmaterials. Structural and Multidisciplinary Optimization, 61(4):14171438, 2020": "Yingheng Tang, Keisuke Kojima, Toshiaki Koike-Akino, Ye Wang, Pengxiang Wu, Mohammad Tahersima,Devesh Jha, Kieran Parsons, and Minghao Qi. Generative deep learning model for a multi-level nano-opticbroadband power splitter. In 2020 Optical Fiber Communications Conference and Exhibition (OFC), pp.13. IEEE, 2020. Sofia Valdez, Carolyn Seepersad, and Sandilya Kambampati. A framework for interactive structural designexploration. In International Design Engineering Technical Conferences and Computers and Informationin Engineering Conference, IDETC-21, Virtual, Online, Aug 2021. ASME. Jun Wang, Wei Wayne Chen, Daicong Da, Mark Fuge, and Rahul Rai. Ih-gan: A conditional generative modelfor implicit surface-based inverse design of cellular structures. Computer Methods in Applied Mechanicsand Engineering, 396:115060, 2022. Liwei Wang, Yu-Chin Chan, Faez Ahmed, Zhao Liu, Ping Zhu, and Wei Chen. Deep generative modeling formechanistic-based learning and design of metamaterial systems. Computer Methods in Applied Mechanicsand Engineering, 372:113377, 2020.",
  "Rebekka V Woldseth, Niels Aage, J Andreas Brentzen, and Ole Sigmund. On the use of artificial neuralnetworks in topology optimisation. Structural and Multidisciplinary Optimization, 65(10):294, 2022": "Patricia Wollstadt, Mariusz Bujny, Satchit Ramnath, Jami J Shah, Duane Detwiler, and Stefan Menzel.Carhoods10k: An industry-grade data set for representation learning and design optimization in engineeringapplications. IEEE Transactions on Evolutionary Computation, 26(6):12211235, 2022. Tianju Xue, Thomas J Wallin, Yigit Menguc, Sigrid Adriaenssens, and Maurizio Chiaramonte. Machinelearning generative models for automatic design of multi-material 3d printed composite solids. ExtremeMechanics Letters, 41:100992, 2020.",
  "Rosen Yu, Cyril Picard, and Faez Ahmed.Fast and accurate bayesian optimization with pre-trainedtransformers for constrained engineering problems. arXiv preprint arXiv:2404.04495, 2024": "Yonggyun Yu, Taeil Hur, Jaeho Jung, and In Gwun Jang. Deep learning for determining a near-optimaltopological design without any iteration. Structural and Multidisciplinary Optimization, 59(3):787799,2019. Hui Zhang, Lei Yang, Changjian Li, Bojian Wu, and Wenping Wang. Scaffoldgan: Synthesis of scaffoldmaterials based on generative adversarial networks. Computer-Aided Design, 138:103041, 2021. ISSN0010-4485. doi: URL Wentai Zhang, Zhangsihao Yang, Haoliang Jiang, Suyash Nigam, Soji Yamakawa, Tomotake Furuhata,Kenji Shimada, and Levent Burak Kara. 3d shape synthesis for conceptual design and optimization usingvariational autoencoders. In International Design Engineering Technical Conferences and Computers andInformation in Engineering Conference, volume 59186, pp. V02AT03A017. American Society of MechanicalEngineers, 2019.",
  "Full scores with standard deviations and significnace testing are included in": ": Full scores from the dataset size study presented in the main paper, showing both mean scores andstandard deviations over the four runs. Scores significantly (p < 0.05 using a 2-sample t-test) better thanthe corresponding GAN trained with the same amount of positive data are bolded. Lower is better. Therelatively large deviation seen in vanilla models may explain the slight counterintuitive increase in invalidityrate with more data in Problem 1.",
  "(iii) Constraint Check: A black-box oracle is available to determine whether a design satisfies constraints.This check may be computationally expensive, limiting its use": "(iv) Closed-form Constraints: An inexpensive closed-form constraint is available. In such scenarios,direct optimization is often favored over generative models in design problems. In other cases,constraint-enforcing rules can be built into the model structure, an approach used in some generativemodels for molecular design (Cheng et al., 2021; Imrie et al., 2020). We note that each level of constraint information is strictly more informative than the previous. In this paper,we focus on the scenario in which a limited dataset of negative samples is available (ii) or can be generatedusing an oracle (iii), but closed-form constraints are not available. This scenario is common in applicationssuch as structural design, mobility design (e.g., cars, bikes, ships, airplanes), and material synthesis. Density Ratio Estimation.Density Ratio Estimation (DRE) (Sugiyama et al., 2012b) is a criticaltechnique in machine learning, particularly when evaluating distributions is infeasible or is computationallyexpensive (Mohamed & Lakshminarayanan, 2016). DRE techniques are heavily employed for generativemodeling and score matching estimation (Goodfellow et al., 2014; Gutmann & Hyvrinen, 2010; Srivastavaet al., 2023; Choi et al., 2022). In the context of GANs (Goodfellow et al., 2014; Arjovsky et al., 2017),the DRE methodology forms the underlying basis for their operation. A well-known technique for DREis probabilistic classification Sugiyama et al. (2012b), where a binary classifier is used to learn the ratio.However, accurately performing DRE with finite samples is particularly challenging in high-dimensionalspaces. To overcome this challenge, prior works have employed a divide-and-conquer approach. An example ofthis is the Telescoping Density Ratio Estimation (TRE) method (Gutmann & Hyvrinen, 2010; Rhodes et al.,2020), which divides the problem into a sequence of easier DRE sub-problems. Despite its success, there arelimitations to this approach, especially when the number of intermediate bridge distributions is increased. Noise contrastive estimator (NCE (Gutmann & Hyvrinen, 2010)) and hybrid generative models (Srivastavaet al., 2023; 2017; Rhodes et al., 2020) are also based on the density ratio as underlying methodology, providinga flexible paradigm for large scale generative modeling. Density Ratio Estimation in the Negative Data Context.Let pn denote the negative distributioni.e., the distribution of constraint-violating datapoints. Instead of training using only the positive distributionpp, NDGM formulations seek to train a generative model p using both pp and pn. Assuming mutual absolutecontinuity of pp, p and pn, and starting from first principles, we can now re-write Eq. 2 as:",
  "dx.(13)": "While the solution for Eq. 13 is the same as the solution for Eq. 2 i.e. p = pp, the model is now directlyincentivized to allocate the same amount of probability mass to the samples from pn as does the datadistribution pp. This ensures that when trained using finite N, the model avoids allocating high probabilitymass to invalid samples. In other words, training under Eq. 13 encourages the model to minimize itsdiscrepancy with respect to pp such that its discrepancy with respect to pn matches exactly that of pp and pn. Generative Models for Engineering Design.Generative models have recently seen extensive use indesign generation tasks (Regenwetter et al., 2022a). Generative Adversarial Nets, for example, have seenextensive use in many applications. In topology optimization, GANs (Li et al., 2019; Rawat & Shen, 2019;Oh et al., 2018; 2019; Sharpe & Seepersad, 2019; Nie et al., 2021; Yu et al., 2019; Valdez et al., 2021; Maz &Ahmed, 2023) are often used to create optimal topologies, potentially bypassing time-consuming iterativesolvers. In computational materials design, GANs (Tan et al., 2020; Yang et al., 2018; Zhang et al., 2021;Mosser et al., 2017; Lee et al., 2021; Liu et al., 2019), VAEs (Cang et al., 2018; Li et al., 2020; Liu et al.,2020; Wang et al., 2020; Xue et al., 2020; Tang et al., 2020; Chen & Liu, 2021), and other models are usedto generate synthetic data to better learn process-structure-property relations (Bostanabad et al., 2018). Avariety of generative models have been applied to 2D shape synthesis problems (Yilmaz & German, 2020;Chen & Fuge, 2018; Chen et al., 2019; Chen & Fuge, 2019; Nobari et al., 2022; Li et al., 2021; Dering et al.,2018), such as airfoil design, and 3D shape synthesis problems (Shu et al., 2020; Nobari et al., 2022; Brocket al., 2016; Zhang et al., 2019) such as mechanical component synthesis in engineering design. Finally,generative models have been proposed as a method to tackle various miscellaneous product and machinedesign tasks (Deshpande & Purwar, 2019; Sharma & Purwar, 2020; Regenwetter et al., 2021; Deshpande &Purwar, 2020). Constraint Satisfaction in Machine Learning.From a general point of view, Constraint SatisfactionProblems (CSPs) have been long studied in computer science and optimization about optimal allocation,graph search, games, and path planning (Russell, 2010). However, such constraints are mostly related toalgorithmic complexity and memory allocation. In generative design, the goal of constraint satisfaction differsas it aims to achieve both high-performance designs and diverse distribution coverage through probabilisticmodeling (Regenwetter et al., 2022a). Recently, Neural Constraint Satisfaction (Chang et al., 2023) has beenproposed to deal with objects in a scene to solve intuitive physics problems (Smith et al., 2019; Hamricket al., 2018).In the CAD domain, structured models to handle constraints have been proposed (Seffet al., 2021; Para et al., 2021). Conditional generative models have been proposed for structural topologyoptimization (Nie et al., 2021), leveraging physical fields (Nie et al., 2021; Maz & Ahmed, 2023), denseapproximations (Giannone & Ahmed, 2023), and trajectory alignment (Giannone et al., 2024) for high-qualitycandidate generation. These approaches rely on explicit constraint satisfaction. Instead, we focus on implicitconstraint satisfaction, leveraging a dataset of invalid configurations to enhance the model capacity to generatevalid designs.",
  "DComparison to Rejection Sampling": "In constrained generation problems, rejection sampling is a simple, yet powerful strategy to ensure constraintsatisfaction. Unfortunately, a black-box constraint check is not always available and may be prohibitivelycostly. In the negative data domain, negative data provides an opportunity to train a supervised constraintevaluation surrogate which can be used during rejection sampling, generally at lower cost than a query to aground-truth constraint oracle. To generate a batch of samples, the generative model and predictive modelwill be queried in a loop, discarding any predicted invalid samples and collecting predicted valid samplesuntil a sufficient number of predicted valid samples is accumulated.",
  "D.1Benchmarking": "We benchmark a vanilla GAN, VAE, and DDPM in this manner, training a supervised classifier using thenegative data, then applying the classifier during inference. and present the scores, whileFigures 22 and 23 show the distribution plots. Rejection sampling outperforms GAN-MDD and GAN-DO(and most baselines) in terms of invalidity rate. Noting that a supervised classifier has a much simpler taskthan a constrained generative model, we would not expect NDGMs to learn constraints boundaries moreprecisely than classifiers. Thus, it is unrealistic to expect NDGMs to beat this rejection sampling baselinein constraint satisfaction scores. As expected, vanilla models augmented by rejection sampling still oftenunderperform NDGMs in F1 score. : Comparison of F1 scores () and invalidity rates () for GAN-MDD and GAN-DO againstrejection sampling baselines (+RS) on Problem 1 (left) and Problem 2 (right). Gray markers indicatescores for other models benchmarked in the main paper. Mean scores over six instantiations are plotted.Scores closer to the bottom left are more optimal. Triangular markers indicate that the score lies off the plotin the indicated direction.",
  "D.2Rejection Sampling Carries Significant Inference-Time Costs": "Rejection sampling using a predictive model carries significant downsides when deployed in practice. Mostnotably, the number of model calls is expected to at least double, and it may increase by orders of magnitudeif the generative models validity rate is very low. Furthermore, the inference time of the model will bestochastic. For a model with validity rate v, the expected mean and variance of the number of function calls(nc) is:",
  "v2(14)": "To illustrate the huge inference cost scaling and unpredictability for models with low validity rate, compares the rejection sampling cost of a GAN trained on Problem 1 and a GAN trained on the ship hullengineering problem. The validity rate of these two models is hugely different 91.7% and 6.1%, respectively.Rejection sampling increases the expected number of model calls from 1 to 2.18 for Problem 1, and to 33.17for the ship hull problem. We also estimate the expected worst case scenario out of 1000 inference runs. Asshown, in one of a thousand inferences, we expect to require at least 222 calls to the model to generate avalid ship hull. Inference memory costs also increase, further complicating real-time deployment of generative models. Thiscombination of factors can make rejection sampling less practical for real-time deployment of generativemodels. These significant downsides motivate the further exploration and development of NDGMs. Naturally, NDGMs can also be used with rejection sampling to accelerate inference due to their highervalidity rates and sometimes superior distribution learning. Nonetheless, rejection sampling serves as a usefulreference point for projecting the frontier of NDGM modeling capabilities, assuming perfect generation basedon the quality of constraint boundary estimates learned by supervised models.",
  "(p(x) + pn(x)).(16)": "The rationale behind the double discriminator algorithm is intuitive when viewed as an expansion of a vanillaGAN. The generator aims for its samples to be classified as positive by the original discriminator and not asnegative by the extra discriminator. We benchmark this double discriminator variant below, titled GAN-DDD (double discriminator + diversity).In practice, we also find that an alternate formulation that combines this simple two-discriminator conceptwith discriminator overloading (DO) also works well in many cases. The alternative formulation consists ofthe classic discriminator, f estimating pp/p and an overloaded discriminator, f estimating (pp + p)/pn.The total loss function is then expressed as:",
  "E.2Testing": "We benchmark GAN-DDD on the 2D test problems using a negative data weight of = 0.4. Scores areplotted against other models in and tabulated in , while generated distributions are plottedin . Although it performs very well in Problem 1, GAN-DDD generates many invalid samplesin Problem 2. In general, we find that the careful tuning of the negative data weighting parameter anddiversity loss weighting parameter ( and , respectively) have significant impacts on the tradeoff betweendistributional similarity and validity. Therefore, we generally recommend GAN-MDD as a simpler off-the-shelfmethod, even though GAN-DDD may perform better in specific scenarios.",
  ": Scores for GAN-DDD on Problem 1 and Problem 2. Mean scores and standard deviations oversix instantiations are shown": ": Comparison of GAN-DDDs F1 scores () and invalidity rates () to baselines on Problem 1(left) and Problem 2 (right). Gray markers indicate scores for other models benchmarked in this paper, witha few selectively annotated. Mean scores over six instantiations are plotted. Scores closer to the bottom leftare more optimal. Triangular markers indicate that the score lies off the plot in the indicated direction.",
  "F.12D Experiments": "F.1.1Dataset DetailsProblem 1: Datapoints are randomly sampled from one of six modes, each of which is a 2D Gaussian.Distribution centers are spaced at an equal radius. Points in close proximity to the center of any distributionare labeled as negatives, while others are labeled as positives. Sampling is performed until 5K positive samplesand 5K negative samples are acquired, and any excess in the oversampled class is discarded. Problem 2: Datapoints are uniformly sampled. A square grid of centerpoints is overlaid over the distribution.Datapoints within a specified proximity to a centerpoint are considered negative, while all others are labeledas positive. Sampling is performed until 5K positive samples and 5K negative samples are acquired, and anyexcess in the oversampled class is discarded. F.1.2Training DetailsAll tested networks (encoder, decoder, generator, discriminator, DDPM noise model, auxiliary discriminator)are deep networks with one hidden layer of 400 neurons and ReLU activations. A batch size of 256 is usedthroughout. Models are trained using the Adam optimizer (Kingma & Ba, 2014) with a learning rate 3 104.Models are trained for 10000 epochs. The noise dimension for the GAN and latent dimension for the VAEare set at 8. Diversity weights are set at 0.1.",
  "F.2.1Dataset DetailsSeveral of the engineering datasets were compiled and described in Yu et al. (2024). For all datasets in thissection, optimization objectives are not utilized": "Ashby Chart: Taken from (Jetton et al., 2023), this problem explores physically feasible combinations ofmaterial properties, according to known physical materials from an Ashby chart. The constraint functioncombines an analytical constraint and a lookup from an Ashby chart. Material properties considered aredensity, yield strength, and Youngs modulus. Material classes included are foams, natural materials, polymers,composites, ceramics, and metals. 1K positive samples and 1K negative samples are selected using uniformrandom sampling. Bike Frame: The FRAMED dataset (Regenwetter et al., 2022b) is comprised of 4292 in-distribution (positive)human-designed bicycle frame models. FRAMED also contains 3242 constraint-violating (negative) designs,some of which were human-designed and some of which were synthesized by generative models. FRAMEDalso contains 10095 generative model-synthesized valid designs that are not assumed to be in-distributionand are thus unused in this benchmark. Constraints consist of a set of empirical geometric checks and ablack-box 3D reconstruction check. Constraints are unified using an all-or-nothing approach. Validity scoreson this dataset are only evaluated using empirical checks. Cantilever Beam: This problem considers the design of a five-component stepped cantilever beam. Thethickness and height of each of the five components are the design variables, while the lengths of eachcomponent are given (fixed). Taken from (Gandomi & Yang, 2011), this problem has numerous geometricconstraints and an overall constraint limiting the total deflection allowed by the design under a simpleconcentrated load at the tip of the beam. 1K positive samples and 1K negative samples are selected usinguniform random sampling. Car Impact: This problem quantifies the performance of a car design under a side impact scenario based onEuropean Enhanced Vehicle-Safety Committee (EEVC) procedures (Gandomi et al., 2011). The car chassisis represented by 11 design parameters. Critical deflection, load, and velocity thresholds are specified forvarious components of a crash dummy, constituting 10 constraints. 1K positive samples and 1K negativesamples are selected using uniform random sampling. Compression Spring: This problem, taken from (Gandomi & Yang, 2011), centers around the design ofa helical compression spring parameterized over coil diameter, wire diameter, and number of spring coils.",
  "A constraint on free length and a constraint on displacement under a compressive load are specified. 1Kpositive samples and 1K negative samples are selected using uniform random sampling": "Concrete Beam: Taken from (Gandomi & Yang, 2011), this problem centers around the design of a simplysupported concrete beam under a distributed load case. The beam is parameterized using a cross sectionalarea, base length, and height and is subject to a safety requirement indicated in the American ConcreteInstitute (ACI) 319-77 code. 1K positive samples and 1K negative samples are selected using uniform randomsampling. Gearbox: This gearbox (speed-reducer) design problem, taken from (Gandomi & Yang, 2011) features7 parameters describing key geometric components like shaft diameters, number of teeth on gears, andface width of gears. Nine constraints are given, spanning considerations like bending stress on gear teeth,transverse stress and deflection on shafts, and surface stresses. 1K positive samples and 1K negative samplesare selected using uniform random sampling. Heat Exchanger: This problem, sourced from (Yang & Gandomi, 2012) considers the design of a heatexchanger involving eight design parameters and six constraints focused on geometric validity. 1K positivesamples and 1K negative samples are selected using uniform random sampling. Pressure Vessel: This cylindrical pressure vessel design problem is taken from (Gandomi & Yang, 2011).The pressure vessel is parametrized according to four parameters, namely the cylinder thickness, sphericalhead thickness, inner radius, and cylinder length. Four geometric and structural constraints are specified inaccordance with American Society of Mechanical Engineers (ASME) design codes. 1K positive samples and1K negative samples are selected using uniform random sampling. Ship Hull: The SHIPD Dataset (Bagazinski & Ahmed, 2023) is comprised of 30k valid (positive) ship hulldesigns and 20k invalid (negative) ship hull designs. The SHIPD dataset includes numerous constraintsspanning geometric rules and functional performance targets, focusing on various types of hydrodynamicperformance. Three-Bar Truss: Taken from (Yang & Gandomi, 2012), this truss design problem considers the design ofa three-beam truss parameterized by the length of two of the beams (symmetry specifies the length of thethird). The system is subject to one geometric constraint and two maximum stress constraints. 1K positivesamples and 1K negative samples are selected using uniform random sampling. Welded Beam: Taken from (Gandomi & Yang, 2011), this problem concerns a cantilever beam welded toa flat surface under a simple concentrated load at the tip of the beam. The beam is parametrized using aweld thickness, welded joint length, beam thickness, and beam width. Five structural constraints are given,specifying a maximum shear stress, bending stress, buckling load, and deflection, as well as a geometricconstraint on the beam. 1K positive samples and 1K negative samples are selected using uniform randomsampling. F.2.2Training DetailsThe generator and discriminator are deep networks with one hidden layer of 400 neurons and ReLU activations.A batch size of 256 is used throughout. Models are trained using the Adam optimizer with a learning rate3 104. Models are trained for 10000 epochs. The noise dimension for the GAN and latent dimension forthe VAE are set at 8. Diversity weights are tuned by selecting the highest score from 10 increments tested.",
  "F.3Topology Optimization Experiments": "F.3.1Dataset DetailsThe GAN was trained exclusively on 32436 valid (connected) topologies generated through iterative optimiza-tion (SIMP) (Bendse & Kikuchi, 1988). The GAN-MDD and GAN-DO models are trained on a medleyof disconnected topologies generated by iterative optimization (2564), and either procedurally-generatedsynthetic topologies (35000) or GAN-generated disconnected topologies (92307). Synthetic topologies weresourced directly from the classification dataset of (Maz & Ahmed, 2023). The GAN used to generatedisconnected topologies for rejection was the exact GAN benchmarked in the paper (the first instantiation ofsix). Topologies were checked for continuity and rejected samples were added to the negative dataset. All",
  "positive and negative data were augmented sevenfold using horizontal and vertical flips, as well as quarter-turnrotations, prior to training": "F.3.2Training DetailsThe model architectures of the GAN, GAN-DO and GAN-MDD are identical except for the final outputdimension of the discriminator. Both the generator and discriminator are simple 5-layer convolutional neuralnetworks. The generator has 3.6M parameters, while the discriminator has 2.8M parameters. For morearchitectural details, we refer the reader to the codebase. The latent dimension is 100, batch size is 128, andlearning rate for both models is 3 104, using the Adam optimizer."
}