{
  "Abstract": "We study the piecewise-stationary dueling bandits problem with K arms, where the timehorizon T consists of M stationary segments, each of which is associated with its ownpreference matrix.The learner repeatedly selects a pair of arms and observes a binarypreference between them as feedback.To minimize the accumulated regret, the learnerneeds to pick the Condorcet winner of each stationary segment as often as possible, despitepreference matrices and segment lengths being unknown. We propose the Beat the WinnerReset algorithm and prove a bound on its expected binary weak regret in the stationary case,which tightens the bound of current state-of-art algorithms. We also show a regret boundfor the non-stationary case, without requiring knowledge of M or T. We further propose andanalyze two meta-algorithms, DETECT for weak regret and Monitored Dueling Bandits forstrong regret, both based on a detection-window approach that can incorporate any duelingbandit algorithm as a black-box algorithm. Finally, we prove a worst-case lower bound forexpected weak regret in the non-stationary case.",
  "Introduction": "The stochastic multi-armed armed bandit (MAB) problem (Thompson, 1933; Robbins, 1952) is an onlinelearning framework in which an agent (learner) chooses repeatedly from a set of K options called arms in the course of a sequential decision process with (discrete) time horizon T. Each arm ai is associatedwith an unknown reward distribution having finite mean and being stationary over the learning process.Choosing arm ai results in a random reward sampled from that distribution. The learners goal is to choosethe optimal arm, i.e., the one having highest mean reward, as often as possible, because a suboptimal armwith lower (expected) reward implies a positive regret (reward difference). Due to the absence of knowledgeabout the reward distributions, this task of cumulative regret minimization comes with the challenge oftackling the exploration-exploitation dilemma: As the learner requires reasonably certain estimates of thearms mean rewards, it must explore by choosing each arm sufficiently often. Otherwise, because rewardsare random, it may believe that a suboptimal arm is optimal. On the other side, too much exploration is notgood either, because exploration comes at the cost of exploitation (choosing the presumably optimal arm),thereby increasing cumulative regret. In many practical applications, the assumption of stationary reward distributions is likely to be violated. Inlight of this, the non-stationary MAB problem has received increasing interest in the recent past (Hartland",
  "Published in Transactions on Machine Learning Research (MM/YYYY)": ":Cumulative binary strong regret averaged over 500 random problem instances with shaded areasshowing standard deviation across 10 groups, WSS with exploitation parameter = 1.05: T = 106, = 0.1, = 0.6. :Cumulative binary strong regret averaged over 500 random problem instances with shaded areasshowing standard deviation across 10 groups, WSS with exploitation parameter = 1.05: T = 107, = 0.1, = 0.6.",
  ") in the stationary setting and O( KM": "2 log(K + T)) in the non-stationary setting() under the assumption that the suboptimality gap is known. BtWR enjoys a tighter regretbound than other state-of-the-art algorithms for the stationary case, and does not need to know the timehorizon T or the number of segments M for the non-stationary case.Next, we propose the MonitoredDueling Bandits (MDB) meta-algorithm for strong regret based on a detection-window approach, which isparameterized with a black-box dueling bandits algorithm for the stationary setting (). We bound",
  "MKplus the sum of regret that the black-box algorithm would have": "incurred when being run on its own for each (stationary) segment. In , we additionally present withDETECT an adaptation of MDB towards weak regret and prove its expected regret to be in O(KM log T)plus the sum of regret that the black-box algorithm would have incurred when being run on its own for each",
  "The algorithm itself does not require , but the underlying black-box dueling bandits algorithm might do.2 With BtW as the underlying black-box dueling bandits algorithm": "Related Work.There is a wealth of work on the non-stationary bandit problem with numerical rewards(Hartland et al., 2006; Kocsis & Szepesvri, 2006; Garivier & Moulines, 2011; Liu et al., 2018; Auer et al.,2019). See Lu et al. (2021) for a good overview of this branch of literature. From a methodological pointof view, the work by Cao et al. (2019) is closest to our approaches MDB and DETECT. For the duelingbandits problem (Yue & Joachims, 2009), a variety of algorithms for strong regret based on the Condorcetwinner has been proposed: Beat the Mean (Yue & Joachims, 2011), Interleaved Filter (Yue et al., 2012),SAVAGE (Urvoy et al., 2013), RUCB (Zoghi et al., 2014a), RCS (Zoghi et al., 2014b), RMED (Komiyamaet al., 2015), MergeRUCB (Zoghi et al., 2015), MergeDTS (Li et al., 2020). Although weak regret has beenproposed by Yue et al. (2012), Winner Stays (Chen & Frazier, 2017) and Beat the Winner (Pekz et al.,2020) are the only algorithms specifically tailored to this regret that we are aware of. Remarkably, theirexpected regret bounds are constant w.r.t. T. Bengs et al. (2021) give a survey of dueling bandits. Non-stationary dueling bandits have been considered by Saha & Gupta (2022); Buening & Saha (2023); Suk& Agarwal (2023). All of them consider dynamic strong regret, which is a stricter notion of regret as thelearners selection are compared against a dynamic benchmark arm every time step. Saha & Gupta (2022)propose with Dex3.S an algorithm inspired by EXP3 (Auer et al., 2002b) from the adversarial setting thatachieves a high probability strong regret bound of O( KMT log KT/) with at least 1 2 probability if thenumber of stationary segments M is known. Buening & Saha (2023) rely on this previous work in order topropose with ANACONDA an algorithm that adapts to the environment without requiring the number ofsegments. It achieves an expected strong regret of O(K",
  "KMT) strong regret butunder the relatively strong assumption of stochastic transivity and the stochastic triangle inequality": "Our main focus is on static weak regret, where the benchmark arm may only change on certain time segments,and the learner does not suffer regret as long as the segment-wise benchmark arm occurs in the duel. Weask ourselves the question to what extent the regret bounds change for weak regret if non-stationarity is nowpresent. In particular, what the dependence on the time horizon T will be. In contrast to strong regret, thenotion of weak regret in dynamic environments is so far left unexplored.",
  "Problem Statement": "The piecewise-stationary dueling bandits problem involves a finite set of K arms A = {a1, . . . , aK}, a timehorizon T N, and M 1 changepoints 1, . . . , M1 N unknown to the learner with 1 < 1 < . . . <M1 T dividing the entire learning time into M stationary segments. We additionally define dummychangepoints 0 = 1 and M = T + 1, allowing us to express the m-th stationary segment as the setSm := {m1, . . . , m 1} spanning between the (m 1)-th and the m-th changepoint exclusively. For eachstationary segment Sm the environment is characterized by a preference matrix P (m) KK unknownto the learner, with each entry P (m)i,jdenoting the probability that the arm ai wins against aj in a duel. From here on we write p(m)i,j:= P (m)i,j . To be well-defined, we assume that p(m)i,j + p(m)j,i= 1 for all ai, aj, i.e.,the probability of a draw is zero. For each segment Sm, we assume the existence of the Condorcet winneram that beats all other arms with probability greater than half, i.e., p(m)m,i > 1",
  "tSmrvt": "Although we give upper bounds on expected cumulative binary strong and weak regret, they are also validfor the non-binary versions, because rS rS and rW r W, and vice versa for lower bounds. A list ofsymbols used frequently throughout the paper is given by . For the remainder of the paper, we usethe terms piecewise-stationary and non-stationary interchangeably.",
  "Beat the Winner Reset": "The first piecewise-stationary dueling bandits algorithm that we propose is the Beat the Winner Resetalgorithm (BtWR) (see Algorithm 1). It is a variant of the Beat the Winner (BtW) algorithm (Pekz et al.,2020) and to be applied for weak regret. It proceeds in explicit rounds playing the same pair (aI, aJ) untila certain termination condition is fulfilled. BtWR starts by drawing an incumbent arm aI uniformly atrandom, puts the other K 1 arms in random order into a FIFO queue Q, and initializes the round counterc to be 1. It proceeds by iterating through rounds until the time horizon is reached. In each round, thechallenger arm aJ is dequeued from Q and the variables wI and wJ counting wins of the incumbent andchallenger, respectively, are initialized. The pair (aI, aJ) is played until one of them has reached c manywins, called the winner of that round. The winner of a round becomes the incumbent aI for the next roundand the loser is enqueued in Q, being played again K 1 rounds later. At last, c (counting the numberof successive rounds with the same incumbent aI) is incremented by one if the incumbent aI has won theround. Otherwise, if the challenger aJ has won, it is reset to 1. An illustration of BtWR is given in . :Scheme of BtWR: a1 is chosen as the first incumbent and a2 as the challenger. After a1 has won1 many duels against a2 in the first round, a2 is put back to the end of the queue and a3 becomes the newchallenger for the second round. The pair (a1, a3) is then played until one arm wins 2 many times. Sincethe challenger arm a3 has won the second round, it becomes the new incumbent and the counter c is setback to 1 for the third round. BtWR differs to its predecessor BtW in the way it updates the round length.More specifically, BtWincrements it by one in each round, independent of whether the current incumbent has lost the round.Setting it back to one, allows BtWR to react to changepoints by resetting its internal state back to the timeof initialization (aside from the order of arms in Q), whenever the new optimal arm has won a round for thefirst time against the current incumbent. We show in the upcoming analyses how to set c in order to boundBtWRs cumulative expected weak regret.",
  "(1)": "for some (0, 1) that we specify later. Note that this requires to be known. We denote by n the firsttime step of the algorithms n-th round and define it as 1 := 1 and n := inf{t > n1 | Jt1 = Jt} for n 2.Further, let cn be the value of c during the n-th round. First, we bound the probability of the optimal arm(w.l.o.g. a1) to lose one of the remaining rounds with probability as soon as it becomes the incumbent.",
  "(K + cn 1)": "Proof. Let n1 = inf{n n | Jn = 1} be the first round from n onward in which a1 is the current challengerand ni = inf{n > ni1 | Jn = 1} be the i-th round for i 2. Let U be a random variable denoting thenumber of times a1 has to become challenger before winning a round, i.e., nU = n 1. Since the queuecontains K 2 many arms, it takes at most K 2 rounds for a1, potentially sitting in last place of thequeue, to become the challenger and one more round to get to the end of the queue or to get promoted tothe incumbent. Consequently, we have n1 n K 2, and ni ni1 K 1 for all i {2, . . . , U}. Hence,we obtain:",
  "Lemma 3. Consider an arbitrary segment Sm. Given that am is the incumbent of the n-th round, theprobability of am losing at least one of the remaining rounds is at most": "Using Lemma 3 we can adapt Theorem 1 for the case where BtWR enters a new segment with c being somenumber c 1, providing us with a bound on the incurred expected regret in that particular segment whichcan be viewed as an instance of the stationary setting. Again, we set = 1/e.",
  "2log(K + T)": "As in the stationary setting, BtWR does not require the time horizon T. Additionally, the regret bounds holdwith BtWR being oblivious about the number of stationary segments M. It is worth mentioning that, unlikethe stationary case, the regret bound for the non-stationary case depends now on the time horizon T. This isattributable to the regret caused by the delay to have the current segments optimal arm as the incumbent.",
  "Monitored Dueling Bandits": "Next, we present Monitored Dueling Bandits (MDB) (see Algorithm 2), our first algorithm using a detection-window approach to tackle dueling bandits with strong regret. Its core idea is to use the same changepointdetection mechanism as MUCB (Cao et al., 2019) by scanning for changepoints within a window of fixedlength w for each pair of distinct arms, thus using K(K1)/2 windows. Each detection window for a distinctpair, say (ai, aj), monitors the absolute difference of the absolute win frequencies of ai over aj of the firstand the second half of the window. Once this absolute difference exceeds some threshold b, the changepointdetection is triggered leading to a reinitialization of MDB. In contrast to MUCB, MDB does not incorporatea specific dueling bandits algorithm with the task to minimize regret in stationary segments. Instead, itrequires a black-box algorithm Alg for the (stationary) dueling bandits problem as an input and periodicallyalternates between choosing pairs selected by Alg and exploring the preference matrix in a round-robinmanner in order to fill all detection windows with the dueling outcomes. The exploration rate by which thepreference matrix is palpated for changepoints is given by a parameter . An illustration is shown in . To be more specific, MDB requires for its parameterization the time horizon T, an even window length w, athreshold b > 0, an exploration rate (0, 1], and a dueling bandits algorithm Alg. In particular, we assumethat MDB can interact with the black-box dueling algorithm Alg by (i) running it for one time step leadingto a concrete choice of a pair of arms and (ii) forwarding the corresponding dueling outcome for that pair toAlg leading to an update of its internal state. This interaction is represented by the RunAndFeed procedurein Algorithm 2. Moreover, we assume that MDB can reset Alg to its initial state. MDB starts by setting to zero, which represents the last time step in which a changepoint was detected, and initializes ni,j foreach distinct pair (ai, aj), counting the number of times the pair has been played since the last detectedchangepoint. An arbitrary ordering of all distinct pairs (ai, aj) is fixed, starting to count at the index zero.At the beginning of each time step, MDB uses the value of r to decide whether to choose a pair of arms",
  "T, K1": "Assumption (1) is similar to (1) in .2.1 for MUCB (Cao et al., 2019) and requires a minimal lengthfor all stationary segments depending on L. Thus, it can be viewed as an implicit upper bound on w andlower bound for . It is essential and guarantees MDB enough time steps to fill all windows before the nextchangepoint, even after a delay of L/2. Assumption (2) is required to guarantee short delay with certainprobability and the variable c will play a role in optimizing the parameters. The difficulty of detecting achangepoint with the detection-window approach increases with shrinking . The bound is necessary to allow",
  "K 1 + ERS(T)": "Next, we bound the probability that MDB wrongly triggers the changepoint detection in the stationarycase and additionally the probability of MDB missing the changepoint by more than L/2 time steps in thenon-stationary case with exactly one changepoint.Lemma 6. Let 1 be the first detection time. The probability of MDB raising a false alarm in the stationarysetting, i.e., M = 1, is bounded by",
  "+ RSM(Alg)": "MDB requires M and T to be known and is sensible to , as smaller changes of entries in the preferencematrix between segments impede its ability to detect a changepoint. Nevertheless, if a suitable duelingbandits algorithm is used for Alg such as RUCB or RMED it has a nearly optimal strong regret bound w.r.t.T and M in light of the lower bound shown by Saha & Gupta (2022).",
  "DETECT": ":Scheme of DETECT: in the first running phase of Alg that spans the first T time steps only pairsselected by Alg are played. It is followed by a detection phase filling the K 1 many detection windows.The arms aj and aj are the first two of K 1 many arms in the ordering . It ends with the raise of analarm in time step and is followed by a new running phase of Alg. All windows are filled after w(K 1)time steps in the detection phase. The second algorithm utilizing detection-windows that we propose is the Dueling Explore-Then-ExploitChangepoint Test (DETECT) algorithm (see Algorithm 3). It is a specialization of MDB (see )for weak regret. DETECT is given a black-box dueling bandits algorithm Alg for the stationary settingand alternates between two phases: one in which it is running Alg for T many time steps, and a detectionphase, in which it scans for the next changepoint and resets Alg upon detection. We take advantage of theopportunity to choose all pairs that contain the best arm without incurring a regret penalty, thus leavingone arm free to be chosen for exploration in the detection phases. DETECT starts by running Alg for a fixednumber of time steps T, where we assume that interaction with Alg can be done by means of a RunAndFeedprocedure as in MDB. In addition, we assume that Alg can provide at any time step its suspected Condorcetwinner (CW) aI (to which most dueling bandit algorithms can be extended effortlessly). After running Algfor T time steps and obtaining Algs suspected CW aI, DETECT initializes a detection phase in which pairs",
  "L := w(K 1)": "Another new parameter we have to take into consideration is the adaptation of to entries in the preferencematrices that relate to winning probabilities of the Condorcet winner in each segment. The definition of in is not appropriate anymore because DETECT cannot detect changes at any other entries thanthose being related to the by Alg suspected Condorcet winner. Hence, we define",
  "m=1ERW(m1, m1 + T 1)": "be Algs expected cumulative weak regret incurred over all T-capped stationary segments {m1, . . . , m1+T 1}.Theorem 4. Let m be the time step at which the changepoint detection is triggered for the first time withDETECT being initialized at m1. Let p, q be such that P (m < m | aIm = am) p for all m Mand P (m m + L/2 | m m, aIm = am) q for all m M 1. Then,",
  "+ (1 p T )MT + RWM(Alg)": "Note that the presented bound is not necessarily linear in T, since we still have the freedom to set T, effectingthe probability p T with which the used black-box algorithm Alg returns the Condorcet Winner. We analyzep T for BtW and Winner-Stays in Appendix F, and derive a suitable choice of T in Corollary 6, which leadsto a bound ofERW(T)= O (log T/2MK + M log T) + RWM(Alg). Similar to our remark in about , we would like to point out that the knowledge of (and forMDB) is necessary in order to derive parameterizations for which we can prove theoretical results. Again,this is common practice in the multi-armed bandit literature.",
  "Empirical Results": "In the following we evaluate BtWR, MDB, and DETECT empirically on synthetic problem instances bycomparing them to dueling bandits algorithms for the stationary setting w.r.t. their cumulative regret.Besides the regret in dependence of T and M, we are also interested in the shape of the regret curves for aspecific scenario in order to give a glimpse of how the algorithms cope with the challenge of non-stationarity.We have generated for each chosen scenario 500 random problem instances, each consisting of a sequenceof M randomly generated preference matrices. For each matrix P (m) a Condorcet winner am exists withone of its winning probabilities being exactly 1/2 + and the others being drawn uniformly at random from[1/2 + , 1]. All winning probabilities between pairs that do not include am are drawn uniformly at randomfrom . The next matrix P (m+1) is manipulated such that at least one of am winning probabilitieschanges by . We have used = 0.1 and = 0.6 for most scenarios, but also investigate the algorithmsregret curves depending on these quantities within a range of reasonable values. The Condorcet winnerchanges with each new preference matrix. The changepoints are distributed evenly across the time horizon,implying equal segment lengths in each scenario. This is done with the intention to obtain meaningful regretaverages for a single parameter specification. For further empirical results see -13.",
  "Weak Regret": "We compare BtWR and DETECT (parameterized as given in Corollary 2) with Winner-Stays (WS) (Chen& Frazier, 2017) as the incorporated black-box algorithm against BtW in . Both, BtWR and DETECTimprove on its stationary competitor BtW: not only is the cumulative regret lower in single scenarios, butthey also show a desirable dependency on T. In addition with the linear growth w.r.t. M, this confirms ortheoretical results in Theorem 2 and 4. BtW experiences difficulties adapting to its changing environment,visible by its increasing regret jumps at each new changepoint, caused by the time it takes to adapt to thenew preference matrix. In comparison, BtWR and DETECT surmount the difficulty of non-stationaritywithout an increasing regret penalty. Further of interest are the regret curves depending on and . Thecumulative regret of DETECT and especially of BtWR shrinks with increasing gap . The latter observationconfirms our result in Theorem 2. In contrast to that, BtW seems to have, at least in the context of non-stationarity, constant performance across a broad range of values. Inspecting the dependency on , itis striking how DETECTs regret curve drops with increasing , which is in accordance with our result inCorollary 2, while BtW and BtWR show constant performance.",
  "Strong Regret": "We compare MDB (parameterized as given in Corollary 1) with Winner Stays Strong (WSS) (Chen &Frazier, 2017) as the incorporated black-box algorithm against WSS itself and DEX3.S (Saha & Gupta,2022) in . Both instances of WSS have exploitation parameter = 1.05. MDB accumulates less regretthan WSS and DEX3.S across single scenarios with similar sublinear dependencies on M and T. UnlikeMDB and DEX3.S, the considered stationary algorithm WSS shows increasing regret jumps, indicating itsmaladjustment for the non-stationary setting. Further, we observe how MDBs cumulative regret decreasesfor larger , whereas the drop in regret depending on is rather negligible.",
  "Sensitivity Analysis for BtWR": "Our satisfying regret bound in Theorem 2 and empirical findings in .1, which demonstrate BtWRssuperiority in the non-stationary setting over its competitors, rely on the assumption that the gap of theparticular problem instance is known. But since this is not guaranteed in practice, one might ask by howmuch BtWRs performance detoriates if the unknown is only estimated. In order to answer this questionempirically, we conducted an evaluation of BtWR with the same setup as described above. With the onlydifference being that we have run multiple instantiations of BtWR, parameterized with different estimates of, on multiple problem classes that differ in their true gap . We consider for both the true gap and itsestimate values ranging from 0.025 to 0.25, which we find reasonable, as values outside our chosen intervalare unlikely to bet met in practice.",
  "Conclusion": "We considered the piecewise-stationary dueling bandits problem with M segments, for which we proposedand analyzed three actively adaptive algorithms.For BtWR, we have shown satisfactory regret boundsfor weak regret in the non-stationary and stationary setting. It stands out from previous non-stationaryalgorithms by not requiring the time horizon T or M to be given, which allows it to be applied when thesequantities are not known upfront. With MDB and DETECT, we have presented two meta-algorithms basedon detection-windows and incorporating black-box dueling bandit algorithms. Their regret bounds can beinterpreted as the sum of the black-box algorithms regret if it would know the changepoints plus a penaltyof non-stationarity. In addition, we have proven worst-case lower bounds on the expected weak regret. Ourempirical results clearly suggest that all three algorithms outperform standard dueling bandits algorithmsin the non-stationary setting.",
  "Viktor Bengs, Rbert Busa-Fekete, Adil El Mesaoudi-Paul, and Eyke Hllermeier. Preference-based onlinelearning with dueling bandits: A survey. Journal of Machine Learning Research, 22:7:17:108, 2021": "Thomas Kleine Buening and Aadirupa Saha. ANACONDA: an improved dynamic regret algorithm for adap-tive non-stationary dueling bandits. In Proceedings of International Conference on Artificial Intelligenceand Statistics (AISTATS), pp. 38543878, 2023. Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with changedetection for piecewise-stationary bandit. In Proceedings of International Conference on Artificial Intelli-gence and Statistics (AISTATS), pp. 418427, 2019.",
  "Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits problem.Journal of Computer and System Sciences, 78(5):15381556, 2012": "Masrour Zoghi, Shimon Whiteson, Rmi Munos, and Maarten de Rijke. Relative upper confidence boundfor the k-armed dueling bandit problem. In Proceedings of International Conference on Machine Learning(ICML), pp. 1018, 2014a. Masrour Zoghi, Shimon A. Whiteson, Maarten de Rijke, and Remi Munos. Relative Confidence Samplingfor Efficient On-line Ranker Evaluation. In Proceedings of ACM International Conference on Web Searchand Data Mining (WSDM), pp. 7382, 2014b. Masrour Zoghi, Shimon Whiteson, and Maarten de Rijke. Mergerucb: A method for large-scale online rankerevaluation. In Proceedings of ACM International Conference on Web Search and Data Mining (WSDM),pp. 1726, 2015. Masrour Zoghi, Toms Tunys, Lihong Li, Damien Jose, Junyan Chen, Chun Ming Chin, and Maartende Rijke. Click-based hot fixes for underperforming torso queries. In Proceedings of International ACMSIGIR Conference on Research and Development in Information Retrieval (SIGIR), pp. 195204, 2016.",
  "Problem setting of piecewise-stationary dueling bandits": "Aset of armsKnumber of armsaii-th armTtime horizonMnumber of stationary segmentsmm-th changepointSmm-th stationary segmentP (m)preference matrix of the m-th stationary segmentp(m)i,jprobability that arm ai is preferred over aj in the m-th segmentX(t)It,Jtbinary dueling outcome between aIt and aJtamCondorcet winner of the m-th segment(m)i,jcalibrated pereference probability of ai over aj in the m-th segment(m)isuboptimality gap of ai in the m-th segmentminimal suboptimality gap(m)i,jchange between ai and aj at the m-th changepoint(m)m-th segmental changeminimal segmental change(m)segmental change w.r.t. CWminimal segmental change w.r.t. CW",
  "Regret measures": "rStinstantaneous strong regretrWtinstantaneous weak regretr Stinstantaneous binary strong regretr Wtinstantaneous binary weak regretRv(T)cumulative regret for regret type vRv(t1, t2)cumulative regret of type v of Alg run on its own from t1 to t2Rv(T)cumulative regret of type v of Alg run on its ownRvM(Alg)expected cumulative regret of type v of Alg over all stationary segments",
  "Algorithm-specific symbols": "cnumber of required wins to end a round of BtWR with round counter cprobability of CW to lose a remaining round of BtWR as current incumbentnfirst time step of BtWRs n-th roundAlgutilized black-box algorithm by MDB and DETECTexploration rate of MDBwwindow length of MDB and DETECTbthreshold of MDB and DETECTLnumber of time steps to fill all of MDBs detection windowsLnumber of time steps to fill all of DETECTs detection windowsTphase length in which DETECT runs Algp(m)Tprobability that DETECTs suspected CW of the m-th segment is correct",
  "K E[M + 1]": "Note that c1 = 1 and cmi = 1 holds for all i {1, . . . , M } because the incumbent Imi1 = 1 has lostthe previous round. Finally, we can bound the expectation of M by E[M ] E[X] where X is a discreterandom variable with P(X = x) = (1 ) x for all x N0. This is justified by Lemma 1, guaranteeingthat the probability of a1 losing a round once it is incumbent is at most . Let Y be a random variablewith Y = X + 1, then it holds Y Geo(1 ). Since E[Y ] =1",
  "m=2ERW(m1, m 1)": "For each m 2 let Am = {{n N | n Sm} = } be the event that there exists a round starting in Sm andm1 = inf{n N | n Sm} be the first round starting in Sm given Am. Let m0 = sup{n N | n < m1}be the last round that started before Sm, on the event of Am we have m0 = m1 1, but not necessarilym0 Sm1. The term in the sum can be further decomposed for all m 2:",
  "In the following we will utilize the McDiarmids inequality which can be seen as a concentration inequalityon a function g of n independent random variables": "Lemma 11. (McDiarmids inequality)Let X1, . . . , Xn be independent random variables all taking values in the set X and c1, . . . , cn R. Further,let g : X n R be a function that satisfies |g(x1, . . . , xi, . . . , xn) g(x1, . . . , xi, . . . , xn)| ci for all i andx1, . . . , xn, xi X. Then for all > 0 holds",
  "K 1 T + ERS(T)": "Proof. Let (i, j) be the position of the pair (ai, aj) in the ordering for i < j, starting to count at0.In order to be well-defined for all i, j, let (i, j) = 1 for all i j and define the event Ai,j,t :={(t 1) mod K(K1)/2 = (i, j)} for all i, j, t. Given 1 > T, we have = 0 guaranteed for the wholeruntime of the algorithm and the number of times a pair (ai, aj) with i < j is played can be bounded by:",
  "The required distance between 1 and 2 of at least L/2 time steps is implied by Assumption (1), whereasthe resulting condition on (1)i,j in order to apply Lemma 13 is given by Assumption (2)": "Before stating our main result in Theorem 3, we introduce notation specifically tailored to the upcomingproof. Let EmRS(t1, t2)be the expected cumulative binary strong regret from time steps t1 to t2 inclusively of MDB started at m1. Note that E1RS(0, T)= ERS(T). In order to capture false alarms and delay,we further define m to be the time step of the first triggered changepoint-detection of MDB started atm1. Thus, we can express a false alarm in the m-th segment raised by MDB started at m1 as the event{m < m}. A large delay of L/2 or more time steps for m in the absence of a false alarm is then given by{m m + L/2}.",
  "m=1ERS(m1, m 1)": "In conjunction with the previous explanation on how to formalize false alarms and large delays, p representsan upper bound for the probability of MDB raising a false alarm in the m-th segment given MDB is startedat m1 for each m. Similarly, q bounds the probability of MDB having a delay of at least L/2 for the",
  "L + Em+1RS(m, T)": "In the case of m m + L the detection windows contain no samples from the previous preference matrixP (m) due to the fact that after L time steps all detection windows are filled with new samples from P (m+1).Thus, the detection of m+1 applies as if MDB would have been started at m, but Alg is still running withinvalid observations from the previous segment Sm. Hence, it potentially suffers maximal regret for Sm+1:",
  "since p maxm pm and q maxm qm": "Theorem 3 bounds MDBs expected regret depending on p and q, but these are not explicitly given by theproblem statement nor the choice of our parameters. A bound solely based on the given parameters is moredesirable. We catch up on this by plugging in the bounds obtained in Lemma 6 and 7 directly.",
  "2wMTK": "For certain values of M, T, and K (and also w which is the only parameter of our choice) the derivedformula for violates the condition 1. MDB could still be executed, but it would have maximal possiblestrong regret, same as for = 1, since detection steps are conducted perpetually without a break and thusonly pairs containing two different arms are played. At last, we derive the optimal choices for the missingparameters w and b.",
  "= ERW( T)+ (T T) (1 P (1 > T, aI = a1)) ,": "where we rely on T T for the first equation and utilized for the first inequality the fact that in the caseof 1 > T and aI = a1 no regret is incurred in the detection phase due to properties of the binary weakregret. The condition of T T is implied by Assumption (1) since we have |S1| = T due to the absence ofchangepoints. We can bound the probability of a false alarm given that Alg has identified the optimal armsuccessfully by using Lemma 12. Lemma 9 Consider a scenario with M = 1. Let 1 be the first detection time and aI be the first suspectedCondorcet winner returned by Alg. The probability of DETECT raising a false alarm given that aI = a1 isbounded by",
  "The required distance between 1 and 2 of at least L/2 time steps is implied by Assumption (1), whereasthe condition on (1)is given by Assumption (2)": "Before stating our main result in Theorem 4, we adopt again the notation used in Appendix D for usingEm [R(t1, t2)] with f being the binary weak regret aggregation function. Furthermore, let aIm be the firstsuspected Condorcet winner returned by Alg when DETECT is started at m1. Since DETECTs successof detecting changepoints is highly depending on Algs ability to find the true optimal arm after T time stepsand our analysis capitalizes on that, we define p(m)Tas the probability that the suspected Condorcet winner",
  "a T returned by Alg after T time steps is the optimal arm, i.e., p(m)T:= P (a T = am), given it is run in a": "stationary setting with preference matrix P (m). Based on that, let p T minm p(m)T, stating a lower boundvalid for all M preference matrices. Since Assumption (1) ensures that the first running phase ends beforem given that DETECT is started at m1, we can later relate p T to the results in Lemma 21 and 23 if weare given WS or BtW as Alg, respectively.",
  "In the case of an false alarm, DETECT could recover completely if the next changepoint is at least T + L": "time steps away, such that the running phase is finished and all detection windows are filled and preparedto encounter the next changepoint. Since we cannot simply assume this to happen, we have to deal withthe opposite event in which either the detection windows are not filled to a sufficient extent, or even worse,the running phase of Alg is not finished yet, leaving it in a corrupted state to enter the next segment, thusinvalidating the lower bound p T .",
  "= EmRW(m, T) | m m, aIm = am": "This is justified by the fact that in every time step of the first detection phase the true Condorcet winnerreturned by Alg is played given that m m and aIm = am. Thus, there is no weak regret incurred fromm1 + T to m 1, which we used for the second equation. We split the remaining term into:",
  "+ Em+1RW(m, T),": "where we utilized Assumption (1) ensuring us that after DETECT being resetted in m, the next detectionphase has at least L time steps left before m+1 such that m+1 can be detected as if DETECT was startedat m. Summarizing, on the event of m m and aIm = am we obtain:",
  "which proves the hypothesis": "Theorem 4 bounds DETECTs expected regret depending on p and q, but these are not explicitly given bythe problem statement nor the choice of our parameters. A bound solely based on the given parameters ismore desirable. We catch up on this by plugging in the bounds obtained in Lemma 9 and 10 directly.",
  "MT": "for some a . The intention is to choose C as high as possible without changing the asymptotic behaviorof the bound given in Theorem 4. With greater C there is more space for the probabilities p and q to increasesuch that the window length w can be chosen smaller. We fulfill the demands towards p and q posed inTheorem 4 by ensuring that the above mentioned bounds are greater than the ones given in Lemma 9 and10 to obtain2Tp T exp2b2",
  "MK": "Finally, we want to close the analysis by setting T and consequently p T specifically for the usage of WS andBtW. To this end, we derive T in Corollary 6 and 8 such that the summand (1 p T )MT in Corollary 2 isbounded by M log T and we set C = (1 p T )MT in order to keep the same asymptotic bounds. We tacklethe trade-off between T and p T in Corollary 7 and 9 from the other side by assuming T = T to be givenand calculate the expected regret bounds on the basis of the resulting p T . Since we need a valid lower boundp T for all stationary segments, we define pmin := minm p(m)min and plug it into the respective lower bounds inLemma 21 and 23. Note that the value of T can exceed segment lengths for small enough pmin.",
  "The arms scores depending on the round and iteration are generalized in Lemma 15 that we leave withoutproof": "Lemma 15. Fix any arbitrary round r N and iteration i {1, . . . , K 1} of the Winner Stays algorithm.Then in the beginning of iteration i in round r the incumbent has score (r 1)(K 1) + i 1 and thechallenger 1 r. Next, we will derive a bound for the probability that the winner of the last round is the Condorcet winnergiven a number of time steps T based on the connection to Gamblers ruin game. We use parts of the prooffor the regret bound of WS provided in Pekz et al. (2020), but correct errors that the authors have made.In Gamblers ruin two players participate, one having a dollars at disposal, the other b. The game consistsof consecutive rounds (not to confuse with rounds of WS) in which the first player wins with probabilityp. The winner of a round receives one dollar from the losing player. As soon as one player has run out ofmoney, the game is over. Each iteration of WS can be viewed as such a game with the dueling arms beingthe players of the game and the difference between each arms score to the score at which it loses the roundbeing its money at hand. Throughout the analysis we utilize the following result: Lemma 16. (Gamblers ruin)In a game of Gamblers ruin with the first player having a dollars and the second player having b dollars,given the winning probability p of the first player over the second player, the first players probability to win",
  "pa+b": "Next, we prove similar to Pekz et al. (2020) lower bounds for the probability of a winning a round giventhat it won the previous round or lost in the other case. For that purpose let Wr,i and Lr,i be the events thata wins iteration i in round r and loses iteration i in round r, respectively. Further define pmin := mini= p,iand x := 1pmin",
  "xrK": "In the other case of a having lost the previous round r 1 we can upper bound its winning probability inround r similarly. It is chosen randomly as the challenger for some iteration j in which it has to beat theincumbent before winning in all remaining K 1 j iterations. Let Cr,i be the event that a is chosen asthe challenger in iteration i of round r 2 given that Lr1,K1, obviously we have P(Cr,i) 1",
  "x": "We can now say with which probability a loses any round r, but we are not finished yet because we needto know which is the last round completed by WS after T many time steps. A key quantity for this is thelength of a Gamblers ruin game, which can be bounded in expectation.",
  "T": "Finally, what is left to show is with which probability a wins the last completed round by combiningLemma 18 and Corollary 10. Let a T be the winner of that round, meaning that it is the incumbent of thefirst iteration in the succeeding round. We call a T also the suspected optimal arm being the one that WSwould return if we were to ask it for the optimal arm.",
  "F.2Beat the Winner": "Same as for WS we will derive a lower bound for the probability that after T time steps the suspectedoptimal arm by BtW is indeed optimal. Therefore, we define its suspected optimal arm to be the currentincumbent. As before, we use pmin := mini= p,i. The authors in Pekz et al. (2020) have already given auseful property about the last round lost by the optimal arm:",
  "e(2pmin1)2": "Proof. Let U be the last round lost by a. Then a is played again in round U + K 1 and wins that round.Thus, BtW will always suggest the true optimal arm as the suspected Condorcet winner after the end ofround U + K 1. Since the r-th round consists of at most 2r 1 time steps, we get the following conditionfor T to guarantee that round U + K 1 is finished:",
  "for k = 1": "Let Ht = {(aIt, aJt, X(t)It,Jt)}tt be the history of observations up to time step t. Let Qmk denote theprobability distribution over histories induced by assuming that the preference matrix Pk is used for the m-th segment, i.e., P (m) = Pk, and thus ak is the Condorcet winner in the m-th segment. The correspondingexpectation is denoted by Emk []. In the case of M = 1 we omit the superscripts in order to simplify notation. Lemma 24. Saha & Gupta (2022)Let HT be the set of all possible histories HT for M = 1 and let f : HT [0, B] be a measurable functionthat maps a history HT to number in the interval [0, B]. Then for every k = 1 holds"
}