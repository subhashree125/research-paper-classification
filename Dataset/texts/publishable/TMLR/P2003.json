{
  "Abstract": "A common way of partitioning graphs is through minimum cuts. One drawback of classi-cal minimum cut methods is that they tend to produce small groups, which is why morebalanced variants such as normalized and ratio cuts have seen more success. However, webelieve that with these variants, the balance constraints can be too restrictive for some ap-plications like for clustering of imbalanced datasets, while not being restrictive enough forwhen searching for perfectly balanced partitions. Here, we propose a new graph cut algo-rithm for partitioning graphs under arbitrary size constraints. We formulate the graph cutproblem as a Gromov-Wasserstein with a concave regularizer problem. We then propose tosolve it using an accelerated proximal GD algorithm which guarantees global convergence toa critical point, results in sparse solutions and only incurs an additional ratio of O(log(n))compared to the classical spectral clustering algorithm but was seen to be more efficient.",
  "Introduction": "Clustering is an important task in the field of unsupervised machine learning. For example, in the contextof computer vision, image clustering consists in grouping images into clusters such that the images withinthe same clusters are similar to each other, while those in different clusters are dissimilar. Applicationsare diverse and wide ranging, including, for example, content-based image retrieval (Bhunia et al., 2020),image annotation (Cheng et al., 2018; Cai et al., 2013), and image indexing (Cao et al., 2013). A popularway of clustering an image dataset is through creating a graph from input images and partitioning it usingtechniques such as spectral clustering which solves the minimum cut (min-cut) problem. This is notablythe case in subspace clustering where a self-representation matrix is learned according to the subspaces inwhich images lie and a graph is built from this matrix (Lu et al., 2012; Elhamifar & Vidal, 2013; Cai et al.,2022; Ji et al., 2017; Zhou et al., 2018). However, in practice, algorithms associated with the min-cut problem suffer from the formation of somesmall groups which leads to bad performance. As a result, other versions of min-cut were proposed that takeinto account the size of the resulting groups, in order to make resulting partitions more balanced. This notionof size is variable, for example, in the Normalized Cut (ncut) (Shi & Malik, 2000), size refers to the totalvolume of a cluster, while in the Ratio Cut (rcut) problem (Hagen & Kahng, 1992), it refers to the cardinalityof a cluster. A common method for solving the ncut and rcut problems is the spectral clustering approach(Von Luxburg, 2007; Ng et al., 2001) which is popular due to often showing good empirical performance andbeing somewhat efficient.",
  "Published in Transactions on Machine Learning Research (09/2024)": "of the Gromov-Wasserstein problem with an 2 loss which is always feasible. Specifically, the first, secondand third constraints are equivalent to defining X to be an element of the transportation polytope with auniform target distribution and a source distribution consisting of the degrees of the nodes. These degreescan be represented as proportions instead of absolute quantities by dividing them over their sum, yieldingthe following problem:min",
  "Balanced Clustering": "A common class of constrained clustering problems is balanced clustering where we wish to obtain a partitionwith clusters of the same size.For example, DeSieno (1988) introduced a conscience mechanism whichpenalizes clusters relative to their size, Ahalt et al. (1990), then employed it to develop the FrequencySensitive Competitive Learning (FSCL) algorithm. In Li et al. (2018), authors proposed to leverage theexclusive lasso on the k-means and min-cut problems to regulate the balance degree of the clusteringresults. In Chen et al. (2017), authors proposed a self-balanced min-cut algorithm for image clusteringimplicitly using exclusive lasso as a balance regularizer in order to produce balanced partitions. Lin et al.(2019) proposed a simplex algorithm to solve a minimum cost flow problem similar to k-means. Pei et al.(2020) proposes a clustering algorithm based on a unified framework of k-means and ratio-cut and balancedpartitions. The time and space complexity of our method are both linear with respect to the number .Wuet al. (2021) explores a balanced graph-based clustering model, named exponential-cut, via redesigning theintercluster compactness based on an exponential transformation. Liu et al. (2022) proposes to introducea novel balanced constraint to regularize the clustering results and constrain the size of clusters in spectral",
  "Constrained Clustering": "Some clustering approaches with generic size constraints, which can be seen as an extension of balancedclustering, also exist. In Zhu et al. (2010), a heuristic algorithm to transform size constrained clusteringproblems into integer linear programming problems was proposed.Authors in Ganganath et al. (2014)introduced a modified k-means algorithm which can be used to obtain clusters of preferred sizes. Clusteringparadigms based on OT generally offer the possibility to set a target distribution for resulting partitions. InNie et al. (2024), a parameter-insensitive min cut clustering with flexible size constraints is proposed. Genevayet al. (2019) proposed a deep clustering algorithm through optimal transport with entropic regularization.In Laclau et al. (2017); Titouan et al. (2020); Fettal et al. (2022), authors proposed to tackle co-clusteringand biclustering problems using OT demonstrating good empirical performance.",
  "Gomov-Wasserstein Graph Clustering": "The Gromov-Wasserstein (GW) partitioning paradigm S-GWL (Xu et al., 2019) supposes that the Gromov-Wasserstein discrepancy can uncover the clustering structure of the observed source graph G when the targetgraph Gdc only contains weighted self-connected isolated nodes, this means that the adjacency matrix of Gdcis diagonal. The weights of this diagonal matrix as well as the source and target distribution are special func-tions of the node degrees. Their approach uses a regularized proximal gradient method as well as a recursivepartitioning scheme and can be used in a multi-view clustering setting. The problem with this approachis its sensitivity to the hyperparameter setting which is problematic since it is an unsupervised method.Abrishami et al. (2020) proposes an OT metric with a component based view of partitioning by assigningcost proportional to transport distance over graph edges.Another approach, SpecGWL (Chowdhury &Needham, 2021) generalizes spectral clustering using Gromov-Wasserstein discrepancy and heat kernels butsuffers from high computational complexity. Given a graph with n node, its optimization procedure involvesthe computation of a gradient which is in O(n3 log(n)) and an eigendecompostion O(n3) and therefore isnot usable for large scale graphs. Liu & Wang (2022) leverages the OT probability to seek the edges of thegraph that characterizes the local nonlinear structure of the original feature. A recent approach (Yan et al.,2024) uses a spectral optimal transport barycenter model, which learns spectral embeddings by solving abarycenter problem equipped with an optimal transport discrepancy and guidance of data.",
  "Preliminaries": "In what follows, n = {p Rn+| ni=1 pi = 1} denotes the n-dimensional standard simplex. (w, v) = {Z Rnk+|Z1 = w, Z1 = v} denotes the transportation polytope, where w n and v k are the marginalsof the joint distribution Z and 1 is a vector of ones, its size can be inferred from the context. Matrices aredenoted with uppercase boldface letters, and vectors with lowercase boldface letters. For a matrix M, itsi-th row is mi and mij is the j-th entry of row i. Tr refers to the trace of a square matrix. ||.|| refers to theFrobenius norm.",
  "j wij": "Normalized k-Cut Problem.In practice, solutions to the minimum k-cut problem do not yield satisfac-tory partitions due to the formation of small groups of vertices. Consequently, versions of the problem thattake into account some notion of \"size\" for these groups have been proposed. The most commonly used oneis normalized cut (Shi & Malik, 2000):",
  "TrHD1/2LD1/2H.(8)": "A solution H for the ncut problem is formed by stacking the first k-eigenvectors of the symmetricallynormalized Laplacian Ls = D1/2LD1/2 as its columns, and then applying a clustering algorithm such ask-means on its rows and assign the original data points accordingly (Ng et al., 2001). The principle is thesame for solving rcut but instead using the unnormalized Laplacian.",
  "Optimal Transport": "Discrete optimal transport.The goal of the optimal transport problem is to find a minimal cost trans-port plan X between a source probability distribution of w and a target probability distribution v. Herewe are interested in the discrete Kantorovich formulation of OT (Kantorovich, 1942). When dealing withdiscrete probability distributions, said formulation is",
  "OT(M, w, v) =minX(w,v) M, X ,(9)": "where ., . is the Frobenius product, M Rnk is the cost matrix, and mij quantifies the effort neededto transport a probability mass from wi to vj.Regularization can be introduced to further speed upcomputation of OT. Examples include entropic regularization (Cuturi, 2013; Altschuler et al., 2017) andlow-rank regularization (Scetbon & marco cuturi, 2022), as well as, other types of approximations (Quanrud,2019; Jambulapati et al., 2019). Discrete Gromov-Wasserstein Discrepancy.The discrete Gromov-Wasserstein (GW) discrepancy(Peyr et al., 2016) is an extension of optimal transport to the case where the source and target distri-butions are defined on different metric spaces:",
  "i,j,k,lL(mik, mjl)xijxkl(10)": "where M Rnn and M Rkk are similarity matrices defined on the source space and target spacerespectively, and L : RR R is a divergence measure between scalars, L(M, M) symbolizes the nnkktensor of all pairwise divergences between the elements of M and M. denotes tensor-matrix product.Different approximation schemes have been explored for this problem Altschuler et al. (2018).",
  "Normalized Cuts via Optimal Transport": "As already mentioned, the good performance of the normalized cut algorithm comes from the normalizationby the volume of each group in the cut. However, the size constraint is not a hard one, meaning that obtainedgroups are not of exactly the same volume. This leads us to propose to replace the volume normalization bya strict balancing constraint as follows:",
  "k 1Tr(XLX)(13)": "This formulation is a special case of the Gromov-Wasserstein problem for a source space whose similaritymatrix in the initial space is M = L and whose similarity matrix in the destination space is M = I. Notethat a ratio cut version can be obtained by replacing the volume constraint with",
  "Graph Cuts with Arbitrary Size Constraints": "From the previous problem, it is easy to see that target distribution does not need to be uniform, and assuch, any distribution can be considered, leading to further applications like imbalanced dataset clustering.Another observation is that any notion of size can be considered and not only the volume or cardinality ofthe formed node groups. We formulate an initial version of the generic optimal transport graph cut problemas:minX(s,t)Tr(XLX) minX(s,t)LX, X ,(16) where si is the relative size of the element i and tj is the desired relative size of the group j. Through theform that uses the Frobenius product, it is easy to see how our problem is related to the Gromov-Wassersteinproblem.",
  "Regularization for Sparse Solutions": "We wish to obtain sparse solutions in order to easily interpret them as partition matrices of the input graph.We do so by aiming to find solutions over the extreme points of the transportation polytope which arematrices that have at most n + k 1 non-zero entries (Peyr et al., 2019). We do so by introducing aregularization term to problem 16. Consequently, we consider the following problem which we coin OT-cut:",
  "Optimization, Convergence and Complexity": "We wish to solve problem 17 which is nonconvex, but algorithms with convergence guarantees exist forproblems of this form. Specifically, we will be using a nonconvex proximal gradient descent based on Li &Lin (2015). The pseudocode is given in algorithm 1. Proposition 1. For step size =12, the iterates X(t) generated by the nonconvex PGD algorithm for ourproblem are all extreme points of the transporation polytope, and as such, have at most n + k 1 nonzeroentries.",
  "Proposition 3. For a graph with n nodes, the complexity of an iteration of the proposed algorithm isOkn2 log n": "Proof. We note that in practice n >> k and that the complexity of the network simplex algorithm for somegraph GEMD = (VEMD, VEMD) is in O(|VEMD||EEMD| log |EEMD|) (Orlin, 1997). In our case, this graphhas |VEMD| = n + k (since n >> k, we can drop the k) and |EEMD| = nk. The other operation that isperformed during each iteration is the matrix multiplication whose complexity is in O(k|E|) where |E| isthe number of edges in the original graph. In the worst case when matrix L is fully dense, we have that|E| = n2.",
  "Datasets": "We perform experiments on graphs constructed from image datasets, namely, MNIST (Deng, 2012), Fashion-MNIST (Xiao et al., 2017) and KMNIST (Clanuwat et al., 2018). We generate these graphs using threesubspace clustering approaches: low-rank subspace clustering (LRSC) (Vidal & Favaro, 2014), least-squareregression subspace clustering (LSR) (Lu et al., 2012) and elastic net subspace clustering (ENSC) (You",
  "EU-Email1,00532,77096.8%42109.0": "et al., 2016). We also consider four graph datasets: DBLP, a co-term citation network; and ACM, a co-author citation networks (Fan et al., 2020). EU-Email an email network from a large European researchinstitution (Leskovec & Krevl, 2014). Indian-Village describes interactions among villagers in Indian villages(Banerjee et al., 2013). The statistical summaries of these datasets are available in .",
  "Performance Metrics": "We adopt Adjusted Rand Index (ARI) (Hubert & Arabie, 1985) to evaluate clustering performance.Ittakes values between 1 and -0.5; larger values signify better performance. To evaluate the concordance ofthe desired and the obtained cluster distributions, we use the Kullback-Leibler (KL) divergence (Kullback& Leibler, 1951). The KL divergence between two perfectly matching distributions will be equal to zero.Otherwise, it would be greater than zero. Smaller KL values signify better concordance.",
  "Experimental settings": "Our two variants, OT-ncut and OT-rcut are implemented via the Python optimal transport package (POT)(Flamary et al., 2021). We use random initialization and use uniform target distributions unless explicitlystated otherwise. We also set = 1/2 and the number of iterations to 30 for the image graphs and 20for the other graphs. We also use normalized laplacian matrices. For the baselines, we use the Scikit-Learn(Pedregosa et al., 2011) implementation of spectral clustering. We use the official implementations of S-GWL(Xu et al., 2019) and SpecGWL (Chowdhury & Needham, 2021). Furthermore, we considered the baselines",
  ". We reported results on the naturally occuring graphs only due to excessive run times over the imagegraphs": "2. Louvain and Infomap do not allow to specify the number of clusters. Comparison between partitionswith a different number of clusters using ARI is not meaningful. As such, we only reported resultson datasets for which those algorithms manage to recover the ground truth-number of clusters (forall runs). Louvain was dropped since it never managed to find the ground-truth number of clusters.",
  "Results": "Toy Datasets.Our algorithm deals with a graph cut-like criterion which means that it should partition adataset according to its connectivity. This means that it should work on datasets on which metric clusteringapproaches such as k-means fail. Two toy examples are given in a and b. Clustering Performance. presents the clustering performance on the graph datasets.In allcases, one of our two variants has the best results in terms of ARI except on EU-Email where Infomap hasthe best performance. describes results obtained on image graph datasets. One of our two variantsgives the best results on all three datasets with the graphs generated by LRSC and LSR. On the graphsgenerated by ENSC, the best result is obtained only on Fashion-MNIST while SpecGWL has the best resultson MNIST. Spectral clustering gives the best performance on KMNIST. Note that better results can alsobe obtained with our variants by trading-off some computational efficiency. Specifically, this can be done byusing several different initializations and taking the one that leads to minimizing the objective the most. Imbalanced Datasets.Results on long-tailed versions of CIFAR-10 are reported in table 5. We notice thatusing ground truth cluster distribution constraints leads to better results when comparing to the traditionalspectral clustering algorithm. Statistical Significance Testing shows the performance ranks of the different methods aver-aged over all the runs on the datasets we considered in terms of ARI. The Nemnyi post-hoc rank test(Nemenyi, 1963) shows that OT-rcut and OT-ncut perform similarly and outperform the other approachesfor a confidence level of 95%. Other approaches perform similarly.",
  "Spectral0.05660.06220.05840.0682OT-ncut0.08310.07300.07940.0693OT-rcut0.07310.07790.07520.0574": "Concordance of the Desired & Resulting Cluster Sizes.To evaluate our algorithms ability toproduce a partition with the desired group size distribution, we use the KL divergence metric. Specifically,we compare the distribution obtained by our OT-rcut and OT-ncut variants against the target distributionspecified as a hyperparameter (t). presents the KL divergences for both variants on various datasets.Predictably, our approaches achieve near-perfect performance on most datasets. Notably, OT-rcut is alwaysable to perfectly recover the desired group sizes. This has to do with the fact that, up to a constant, all theentries in the solutions to the rcut problem are integers. This is not necessarily the case for ncut but theKL divergence is still very small due to the sparsity of the solutions. Running Times.As shown in and , OT-ncut and OT-rcut are the fastest in termsof execution times compared to other approaches on all datasets.As the graphs got larger, SpecGWLconsistently had the largest runtimes.We also report the running times of spectral clustering and ourOT-ncut approach on subsets of increasing size of MNIST as well as for increasing numbers of clusters infig 2. The efficiency of our approach becomes increasingly significant compared to spectral clustering asthe number of nodes grows. However, spectral clustering matches our approachs efficiency as the numberof clusters increases. Our approach can be made more efficient by adopting sparse representations of theoptimal transport plans when doing matrix multiplication.",
  "Conclusion": "In this paper we proposed a new graph cut algorithm for partitioning with arbitrary size constraints throughoptimal transport. This approach generalizes the concept of the normalized and ratio cut to arbitrary sizedistributions to any notion of size. We derived an algorithm that results in sparse solutions and guaranteesglobal convergence to a critical point. Experiments on balanced and imbalanced datasets showed the supe-riority of our approach both in terms of clustering performance and empirical execution times compared tospectral clustering and other OT-based graph clustering approaches. They also demonstrated our approachsability to recover partitions that match the desired ones which is valuable for practical problems where wewish to obtain balanced or constrained partitions.",
  "Limitations": "The node and cluster size distribution parameters can either be set using prior domain knowledge or throughtuning them by trying different possible values and then selecting the best one via internal clustering qualitymetrics such as Davies-Bouldin index (Davies & Bouldin, 1979). In cases where no domain knowledge exitsand parameter tuning is impossible, we can weigh each node by its degree and give clusters uniform sizes.This option is similar to what is done normalized cuts where no prior knowledge on size distributions isexplicitly available Shi & Malik (2000). This issue will be studied in future works.",
  "Abhijit Banerjee, Arun G Chandrasekhar, Esther Duflo, and Matthew O Jackson. The diffusion of microfi-nance. Science, 341(6144):1236498, 2013": "Ayan Kumar Bhunia, Yongxin Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Sketch less formore: On-the-fly fine-grained sketch-based image retrieval. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 97799788, 2020. Vincent D Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre.Fast unfolding ofcommunities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008,2008. Nicolas Bonneel, Michiel Van De Panne, Sylvain Paris, and Wolfgang Heidrich. Displacement interpolationusing lagrangian mass transport. In Proceedings of the 2011 SIGGRAPH Asia conference, pp. 112, 2011. Jinyu Cai, Jicong Fan, Wenzhong Guo, Shiping Wang, Yunhe Zhang, and Zhao Zhang.Efficient deepembedded subspace clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 110, 2022. Xiao Cai, Feiping Nie, Weidong Cai, and Heng Huang. New graph structured sparsity model for multi-label image annotations. In Proceedings of the IEEE International Conference on Computer Vision, pp.801808, 2013.",
  "Jie Cao, Zhiang Wu, Junjie Wu, and Wenjie Liu. Towards information-theoretic k-means clustering for imageindexing. Signal Processing, 93(7):20262037, 2013": "Xiaojun Chen, Joshua Zhexue Haung, Feiping Nie, Renjie Chen, and Qingyao Wu. A self-balanced min-cutalgorithm for image clustering. In Proceedings of the IEEE International Conference on Computer Vision,pp. 20612069, 2017. Xiaojun Chen, Renjie Chen, Qingyao Wu, Yixiang Fang, Feiping Nie, and Joshua Zhexue Huang. Labin:balanced min cut for large-scale data. IEEE transactions on neural networks and learning systems, 31(3):725736, 2019.",
  "Chakib Fettal, Lazhar Labiod, and Mohamed Nadif. Efficient and effective optimal transport-based biclus-tering. Advances in Neural Information Processing Systems, 35:3298933000, 2022": "Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurlie Boisbunon, Stanislas Cham-bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Lo Gautheron, Nathalie T.H.Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, VivienSeguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimaltransport. The Journal of Machine Learning Research, 22(1), jan 2021. ISSN 1532-4435. Nuwan Ganganath, Chi-Tsun Cheng, and K Tse Chi. Data clustering with cluster size constraints using amodified k-means algorithm. In 2014 International Conference on Cyber-Enabled Distributed Computingand Knowledge Discovery, pp. 158161. IEEE, 2014.",
  "James B Orlin. A polynomial time primal network simplex algorithm for minimum cost flows. MathematicalProgramming, 78(2):109129, 1997": "Ferran Pars, Dario Garcia Gasulla, Armand Vilalta, Jonatan Moreno, Eduard Ayguad, Jess Labarta,Ulises Corts, and Toyotaro Suzumura. Fluid communities: A competitive, scalable and diverse communitydetection algorithm. In Complex Networks & Their Applications VI: Proceedings of Complex Networks2017 (The Sixth International Conference on Complex Networks and Their Applications), pp. 229240.Springer, 2018. Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learningin python. the Journal of machine Learning research, 12:28252830, 2011.",
  "Yue Xu, Yong-Lu Li, Jiefeng Li, and Cewu Lu. Constructing balance from imbalance for long-tailed imagerecognition. In European Conference on Computer Vision, pp. 3856. Springer, 2022": "Yuguang Yan, Zhihao Xu, Canlin Yang, Jie Zhang, Ruichu Cai, and Michael Kwok-Po Ng. An optimaltransport view for subspace clustering and spectral clustering. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 38, pp. 1628116289, 2024. Chong You, Chun-Guang Li, Daniel P Robinson, and Ren Vidal. Oracle based active set algorithm forscalable elastic net subspace clustering. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 39283937, 2016.",
  "SymbolDescription": "G = (V, E)A graph with a set of vertices V and edges E.A1, . . . , AkA partition of the nodes of graph GAThe complementary set of AAAdjacency matrix of GDDiagonal matrix of degrees of GLLaplacian matrix of GXA partition matrix or a transport plan depending on contextvol(.)Volume of a set of nodes|.|Cardinality of a setTrTrace operator< ., . >Frobenius product.Frobenius norm.0Zero normTensor-matrix productA transportation polytopeM, MSimilarity matricesL(M, M)The tensor of all pairwise divergences between the elements of M and MGA partition matrixY, Z, YTransport planss, tProbability distributionsICThe characteristic function of C1Vector of onesct, s, , Scalars"
}