{
  "Abstract": "For scalable machine learning on large data sets, subsampling a representative subset is acommon approach for efficient model training. This is often achieved through importancesampling, whereby informative data points are sampled more frequently. In this paper, weexamine the privacy properties of importance sampling, focusing on an individualized privacyanalysis. We find that, in importance sampling, privacy is well aligned with utility but atodds with sample size. Based on this insight, we propose two approaches for constructingsampling distributions: one that optimizes the privacy-efficiency trade-off; and one basedon a utility guarantee in the form of coresets. We evaluate both approaches empirically interms of privacy, efficiency, and accuracy on the differentially private k-means problem. Weobserve that both approaches yield similar outcomes and consistently outperform uniformsampling across a wide range of data sets. Our code is available on GitHub.1",
  "Introduction": "When deploying machine learning models in practice, two central challenges are scalability, i.e., thecomputationally efficient handling of large data sets, and the protection of user privacy. A common approachto the former challenge is subsampling, i.e., performing demanding computations only on a subset of thedata, see, e.g., Alain et al. (2016); Katharopoulos & Fleuret (2018). Here, importance sampling is a powerfultool that can reduce the variance of the subsampled estimator. It assigns higher sampling probabilities todata points that are more informative for the task at hand while keeping the estimate unbiased. For instance,importance sampling is commonly used to construct coresets, that is, subsets whose loss function valueis provably close to the loss for the full data set, see, e.g., Bachem et al. (2018). For the latter challenge,differential privacy (Dwork et al., 2006b) offers a framework for publishing trained models in a way thatrespects the individual privacy of every user. Differential privacy and subsampling are related via the concept of privacy amplification by subsampling(Kasiviswanathan et al., 2008; Balle et al., 2018), which states, loosely speaking, that subsampling withprobability p improves the privacy parameter of a subsequently run differentially private algorithm by afactor of approximately p. A typical application of this result involves re-scaling the query by a factor of1/p to eliminate the sampling bias, thereby approximately canceling out the privacy gains, but keepingthe efficiency gain. It forms the foundation for many practical applications of differential privacy, such asdifferentially private stochastic gradient descent (Bassily et al., 2014; Abadi et al., 2016). So far, privacy amplification has been predominantly used with uniform sampling (Steinke, 2022). Althoughthe potential of data-dependent sampling for reducing sampling variance is well understood (e.g., Robert",
  "Uniform": "Individual privacy loss : Illustration of three subsampling strategies for the learning task of k-means clustering on the Songdata set. We show a scatter plot of the first two principal components of the sampled points. The marker sizeis proportional to the importance weight, while the color represents the individual privacy loss before sampling.Left: Our privacy-constrained sampling selects data points with higher individual privacy loss more frequentlyand with lower weight. Middle: Coreset-based sampling selects data points based on their potential impacton the objective function. Right: Uniform sampling selects data points with equal probability. & Casella (2005)), it has largely remained untapped in differential privacy because its privacy benefits havenot been as clear (Bun et al., 2022; Drechsler & Bailie, 2024). A longstanding objection to data-dependentsampling is that the privacy amplification factor scales with the maximum sampling probability when appliedto heterogeneous probabilities, leading to worse privacy than uniform sampling when controlling for samplesize. Recently, Bun et al. (2022) confirmed that this also holds for probability-proportional-to-size sampling a sampling strategy closely related to importance sampling and further noted that additional privacyleakage may arise from data points influencing other data points sampling probabilities. We find that these challenges can be addressed with an appropriate sampling scheme and an individualizedprivacy analysis incorporating more information than previous work.We propose Poisson importancesampling, defined as sampling each data point independently with a probability that depends only on thedata point itself and weighting the point by the reciprocal of the probability. In this setting, we conducta privacy analysis that explicitly considers the individual privacy loss of each data point when sampled witha specific weight and probability. Our analysis reveals that, in importance sampling, privacy is typically notat odds with utility but with sample size. This is for two reasons: (i) the most informative points typicallyhave the highest privacy loss, and (ii) when importance weights are accounted for, decreasing the samplingprobability typically increases the privacy loss. Perhaps counterintuitively, we conclude that we should assignhigh sampling probabilities to data points with high privacy loss to have good privacy and good utility. At first, statement (ii) might seem to suggest that we should reject weighted sampling altogether becausethere is no privacy gain. However, this is misleading because the statement applies even more to the widelyused uniform sampling when weights are accounted for. Indeed, we find that importance sampling is superiorto uniform sampling in terms of mitigating the impact on privacy and utility. By including sampling weightsinto the framework of privacy amplification, we make this effect explicit and highlight that the primarypurpose of subsampling (whether uniform or data-dependent) is to improve the efficiency of the mechanism,not its privacy-utility trade-off.",
  "Based on this insight, we derive two specific approaches for constructing importance sampling distributionsfor a given mechanism": "Our first approach navigates the aforementioned adverse relationship between privacy and samplesize by minimizing the expected sample size subject to a worst-case privacy constraint. This approacheffectively equalizes the individual privacy losses of the mechanism. We call this approach privacy-constrained sampling. It applies to any differentially private mechanism whose individual privacy",
  "loss profile is known. We provide an efficient algorithm that optimizes the sampling probabilitiesnumerically": "Our second approach is based on the concept of coresets, which are small, representative subsetsthat come with strong utility guarantees in the form of a confidence interval around the loss functionvalue of the full data set. We derive the privacy properties of a coreset-based sampling distributionwhen used in conjunction with differentially private k-means clustering. The two approaches are visualized in alongside uniform sampling. The figure shows that theprivacy-constrained weights are highly correlated to the utility-based coreset weights, supporting the intuitionthat privacy and utility are well aligned in importance sampling. We empirically evaluate the proposed approaches on the task of differentially private k-means clustering. Wecompare them to uniform sampling in terms of efficiency, privacy, and accuracy on eight different data sets ofvarying sizes (cf. ). We find that both of our approaches consistently provide better utility for a givensample size and privacy budget than uniform sampling on all data sets. We find meaningful privacy-utilityimprovements even in the medium-to-low privacy regime where uniform sampling typically fails to do so.One practical implication is that importance sampling can be used effectively to subsample the data set onceat the beginning of the computation, while uniform subsampling typically requires repeated subsampling ateach iteration.",
  "Preliminaries": "We begin by stating the necessary concepts that also introduce the notation used in this paper. We denoteby X Rd the set of all possible data points.A data set D X is a finite subset of X.We useBd,p(r) = {x Rd | xp r} to refer to the p-norm ball of radius r in d dimensions. When the dimensionis clear from context, we omit the subscript d and write Bp(r). Differential Privacy. Differential privacy (DP) is a formal notion of privacy stating that the output ofa data processing method should be robust, in a probabilistic sense, to changes in the data set that affectindividual data points. It was introduced by Dwork et al. (2006b) and has since become a standard tool forprivacy-preserving data analysis (Ji et al., 2014). The notion of robustness is qualified by a parameter 0that relates to the error rate of any adversary that tries to infer whether a particular data point is present inthe data set (Kairouz et al., 2015; Dong et al., 2019). Differential privacy is based on the formal notion ofindistinguishability.Definition 1 (-indistinguishability). Let 0. Two distributions P, Q on a space Y are -indistinguishableif P(Y ) eQ(Y ) and Q(Y ) eP(Y ) for all measurable Y Y.Definition 2 (-DP). A randomized mechanism M: X Y is said to be -differentially private (-DP) if,for all neighboring data sets D, D X , the distributions of M(D) and M(D) are -indistinguishable. Twodata sets D, D are neighboring if D = D {x} or D = D {x} for some x X. Formally, we consider a randomized mechanism as a mapping from a data set to a random variable. Differentialprivacy satisfies several convenient analytical properties, including closedness under post-processing (Dworket al., 2006b), composition (Dwork et al., 2006b;a; 2010; Kairouz et al., 2015), and sampling (Kasiviswanathanet al., 2008; Balle et al., 2018). The latter is called privacy amplification by subsampling.Proposition 3 (Privacy Amplification by Subsampling). Let D be a data set of size n, S be a subset of Dwhere every x has a constant probability p of being independently sampled, i.e., q(x) = p (0, 1], and M bean -DP mechanism. Then, M(S) satisfies -DP for = log(1 + (exp() 1)p). In the case of heterogeneous sampling probabilities p1, . . . , pn, the privacy amplification scales with maxi piinstead of p. It is important to note that this only provides meaningful privacy amplification if is sufficientlysmall. For > 1, we only have log(1/p). Personalized Differential Privacy. In many applications, the privacy loss of a mechanism is not uniformacross the data set. This heterogeneity can be captured by the notion of personalized differential privacy(PDP) (Jorgensen et al., 2015; Ebadi et al., 2015; Alaggan et al., 2016).",
  "Published in Transactions on Machine Learning Research (12/2024)": "Kareem Amin, Alex Kulesza, Andres Muoz, and Sergei Vassilvtiskii. Bounding user contributions: A bias-variance trade-off in differential privacy. In International Conference on Machine Learning, pp. 263271.PMLR, 2019. Olivier Bachem, Mario Lucic, and Andreas Krause. Scalable k-means clustering via lightweight coresets. InProceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,pp. 11191127. ACM, 2018. Maria-Florina Balcan, Travis Dick, Yingyu Liang, Wenlong Mou, and Hongyang Zhang. Differentially privateclustering in high-dimensional euclidean spaces. In International Conference on Machine Learning, pp.322331. PMLR, 2017. Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses viacouplings and divergences. In Advances in Neural Information Processing Systems, volume 31, 2018. Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient algorithmsand tight error bounds. In Proceedings of the 55th Annual Symposium on Foundations of Computer Science,pp. 464473. IEEE, 2014.",
  "Importance Sampling. In learning problems, the (weighted) objective function that we optimize is oftena sum over per-point losses, i.e., D =": "xD w(x)(x). Here, we typically have w(x) = 1 x D for the fulldata D. Uniformly subsampling a data set yields an unbiased estimation of the mean. A common way toimprove upon uniform sampling is to skew the sampling towards important data points while retaining anunbiased estimate of the objective function. Let q be an importance sampling distribution on D which we useto sample a subset S of m n points and weight them with w(x) = (mq(x))1. Then, the estimation of theobjective function is unbiased, i.e., ES[S] = D.",
  "Privacy Amplification via Importance Sampling": "This section presents our framework for importance sampling with differential privacy. In .1, weintroduce the notion of Poisson importance sampling and state and discuss its general privacy properties. Sec-tion 3.2 presents our first approach for deriving importance sampling distributions, namely privacy-constrainedsampling. Finally, in .3, we give a numerical example that illustrates the aforementioned resultsand compares them to uniform subsampling for the Laplace mechanism. We defer all proofs to Appendix D.",
  "We begin by introducing Poisson importance sampling, which is the sampling strategy we use throughout thepaper. It is a weighted version of the Poisson sampling strategy described in Proposition 3": "Definition 5 (Poisson Importance Sampling). Let q: X be a function, and D = {x1, . . . , xn} X bea data set. A Poisson importance sampler for q is a randomized mechanism Sq(D) = {(wi, xi) | i = 1}, wherewi = 1/q(xi) are weights and 1, . . . , n are independent Bernoulli variables with parameters pi = q(xi). By having each probability q(xi) depend only on the data point xi itself and keeping the selection eventsindependent, we ensure that the influence of any single data point on the sample is small. It is important to notethat the function definition of q must be data-independent and considered public information, i.e., fixed beforeobserving the data set, while the specific probabilities {q(xi)}ni=1 evaluated on the data set are not published.Note also that Poisson importance sampling outputs a weighted data set. We intend for the subsequentmechanism to use these weights to offset the sampling bias, e.g., by using a weighted sum when the goal is toestimate a population sum. However, note that our formal privacy results also apply to biased mechanisms. In order to characterize the privacy properties of Poisson importance sampling, we analyze the impact of thesampling probability jointly with the data points individual privacy loss in the base mechanism. For thisreason, we express our results within PDP, which is in contrast to previous characterizations of data-dependentsampling (e.g., Bun et al. (2022)). Our first result describes the general case of an arbitrary PDP mechanismsubsampled with an arbitrary importance sampling distribution. Theorem 6 (Amplification by Importance Sampling). Let M : [1, ) X Y be an -PDP mechanismthat operates on weighted data sets, q: X be a function, and Sq() be a Poisson importance samplerfor q. The mechanism M = M Sq satisfies -PDP with",
  "q(x).(1)": "Proof. The proof is analogous to the proof of the regular privacy amplification theorem with = 0, see, e.g.,Steinke (2022, Theorem 29). Let A be any measurable subset from the probability space of M(D). We beginby defining the functions P(Z) = PrM(D) A | Sq(D) = Zand P (Z) = PrM(D) A | Sq(D) = Z.",
  "Sampling with optimal privacy-efficiency trade-off": "We now describe how to construct a sampling distribution that achieves a given -DP guarantee with minimalexpected sample size. The motivation for this is twofold. First, as we have seen in the previous subsection,sample size is the primary limiting factor to privacy in importance sampling and additionally serves as theprimary indicator of efficiency. Secondly, by imposing a constant PDP bound as a constraint we can ensurethat the subsampled mechanism satisfies -DP by design. This is not obvious from Theorem 6, as the resultingPDP profile may be unbounded.",
  "wi 1,for all i.(2c)": "The constraint in Equation (2b) captures the requirement that should be bounded by for all x X,and the constraint in Equation (2c) ensures that 1/wi is a probability. When the PDP profile is linear in w,the problem can be solved with standard convex optimization techniques. Below, we provide a more generalresult that guarantees a unique solution and an efficient algorithm for a broad class of (possibly) nonconvexPDP profiles. In order to guarantee a unique solution, we require the following mild regularity conditions.",
  "Assumption 9. For all x X, there is a constant vx 1 such that (w, x) > log(1 + w(e 1)) for allw vx": "Assumption 8 ensures that the feasible region is non-empty, because w1 = w2 = = wn = 1 always satisfiesthe constraints, while Assumption 9 essentially states that, asymptotically, should grow at least logarith-mically fast with w, ensuring that the feasible region is bounded. Formally, our existence result is as follows. Theorem 10. Let Assumptions 8 and 9 be satisfied. There is a data set-independent function w: X [1, ),such that, for all data sets D X , Problem 7 has a unique solution w(D) = (w1(D), . . . , wn(D)) of theform wi (D) = w(xi). Furthermore, let M be a mechanism that admits the PDP profile and Sq be a Poissonimportance sampler for q(x) = 1/w(x). Then, M Sq satisfies -DP. The fact that the weights are of the form wi (D) = w(xi) is important for privacy. It ensures that theprobability map q() is only a function of xi and not of the remainder of the data set, which is a requirementfor Poisson importance sampling.",
  "Importance sampling for the Laplace mechanism": "We conclude this section with a simple numerical example for Theorems 6 and 10 to illustrate that (i) privacyand utility are well-aligned goals in importance sampling and (ii) uniform sampling is highly suboptimaleven for very fundamental mechanisms. For this purpose, we generate synthetic data on which we run aLaplacian sum mechanism and compare privacy-constrained sampling to uniform sampling as well as toan idealized benchmark in terms of privacy and variance at a fixed expected sample size.",
  "i=1q(xi) = m": "The variance-optimal distribution serves as a lower bound on the variance achievable by any DP samplingdistribution. It does not satisfy -DP itself, as it requires oracle access to the data set. As we show below, theprivacy-constrained distribution achieves performance close to variance-optimal while also satisfying -DP. We generate n = 1000 points from an isotropic multivariate normal distribution in d = 10 dimensionswith variance 2 = 1/d in each dimension. First, we visualize the importance weights w(xi) for eachsampling strategy. For this, we fix a target sample size at m = 19 and compute the weights w(xi) for eachsampling strategy that achieve the target sample size in expectation. The results are shown in (left).Remarkably, the privacy-constrained weights and the variance-optimal weights are almost identical. Next, we compare mean-squared error (MSE) and privacy loss of the sampling strategies at different privacylevels. For this, we vary the noise scale b of the Laplace mechanism over a coarse grid in . Foreach b, we fix the expected sample size mb by computing the privacy-constrained weights. Then, we computethe corresponding weights for the other two sampling strategies such that they achieve mb in expectation.We then compute the individual PDP losses (xi) for each sampling strategy and obtain their respectivemaximum over the data set. Finally, we compute the MSE between the quantities xiD xi and MLWS( D)of each sampling strategy by averaging over 1000 independent runs. The resulting PDP losses and MSEsin (right) show substantial improvements of privacy-constrained sampling over uniform sampling.",
  "Importance Sampling for DP k-means": "In this section, we apply our results on privacy amplification for importance sampling to the differentiallyprivate k-means problem. We define a weighted version of the DP-Lloyd algorithm, and analyze its PDPprofile. Then, we derive the required constants for the privacy-constrained distribution and establish a privacyguarantee for a coreset-based sampling distribution. All proofs are deferred to Appendix D. Differentially Private k-means Clustering. Given a data set D as introduced before, the goal of k-meansclustering is to find a set of k N cluster centers C = {c1, . . . , ck} that minimize the distance of the datapoints to their closest cluster center. The objective function is given as D(C) = ni=1 d(xi, C), where d(, )is the distance of a point x to its closest cluster center cj, which is d(x, C) = minj x cj22. The standardapproach of solving the k-means problem is Lloyds algorithm (Lloyd, 1982). It starts by initializing the cluster",
  "xCj x. Those two steps are iterated until convergence": "The standard way to make Lloyds algorithm differentially private is to add appropriately scaled noise to bothsteps of the algorithm. Gaussian noise has been suggested (Blum et al., 2005), resulting in (, )-DP and Laplacenoise has been suggested (Su et al., 2016; 2017) for 1 geometry, resulting in -DP. Here, we give a generalizedversion based on the exponential mechanism (McSherry & Talwar, 2007) that guarantees -DP for any pgeometry. We refer to it as DP-Lloyd. Let Rk be a random vector whose entries independently follow azero mean Laplace distribution with scale count > 0. Furthermore, let 1, . . . , k Rd be independent randomvectors, each drawn from the density p() exp(p/sum). The cluster centers are then updated as follows:",
  "Assuming the points have bounded p-norm, i.e., X = Bp(r) for some r > 0, then DP-Lloyd preserves -DPwith = (r/sum + 1/count)T after T iterations": "Weighted DP Lloyds Algorithm. In order to apply importance subsampling to k-means, we first definea weighted version of DP-Lloyd. Each iteration of DP-Lloyd consists of a counting query and a sum query,which generalize naturally to the weighted scenario. Specifically, for a weighted data set S = {(wi, xi)}ni=1,we define the update step to be",
  "Besides the privacy-constrained distribution, we also consider a coreset-based sampling distribution. Beforedoing so, we first introduce the idea of a coreset": "Coresets for k-means.A coreset is a weighted subset S D of the full data set D with cardinalitym n, on which a model performs provably competitive when compared to the performance of the modelon D. Since we are now dealing with weighted data sets, we define the weighted objective of k-means to beD(C) = xD w(x)d(x, C), where w(x) 0 are the non-negative weights. In this paper, we use a samplingdistribution inspired by a lightweight coreset construction as introduced by Bachem et al. (2018).Definition 14 (Lightweight coreset). Let > 0, k N, and D X be a set of points with mean x. Aweighted set S is a (, k)-lightweight coreset of the data D if for any C Rd of cardinality at most k we have|D(C) S(C)|",
  "nx, where m n is the expected subsamplesize and x = 1": "nni=1 xi22 is the average squared 2-norm. To ensure proper probabilities, it is necessary toconstrain the subsampling size to m nxr2. Compared to Bachem et al. (2018), there are three changes:(i) the change to a Poisson sampling setting, (ii) the assumption that the data set X is centered, and (iii) theintroduction of which yields a uniform sampler for the choice of = 1. We compute the -DP guarantee for DP Lloyds algorithm with coreset-based sampling as follows. We applyTheorem 10 to the PDP profile derived in Proposition 13 and the coreset-based sampling distribution. Forpositive constants a, b, s, t > 0, this yields a -PDP guarantee of the form",
  "1b + sx22.(5)": "In order to derive an -DP guarantee, we need to bound (x) over the domain of x B2(r). We observe that(x) depends on x only through x2. Therefore, we can bound (x) numerically by maximizing it over thedomain x2 [0, r] via grid search. The maximum always exists because b and s are strictly positive.",
  "Experiments": "We now evaluate our proposed sampling approaches (coreset-based and privacy-constrained) on the task ofk-means clustering where we are interested in three objectives: privacy, efficiency, and accuracy. The point ofthe experiments is to investigate whether our proposed sampling strategies lead to improvements in terms ofthe three objectives when compared to uniform sampling. Our measure for efficiency is the subsample size mproduced by the sampling strategy. This is because Lloyds algorithm scales linearly in m (for fixed k and T)and the computing time of the sampling itself is negligible in comparison. We measure accuracy via thek-means objective evaluated on the full data set and privacy as the -DP guarantee of the sampled mechanism. Data. We use the following eight real-world data sets: Covertype (Blackard & Dean, 1999) (n = 581,012,d = 54), FMA2 (Defferrard et al., 2017) (n = 106,574, d = 518), Ijcnn1 3 (Chang & Lin, 2001) (n = 49,990,d = 22), KDD-Protein4 (n = 145,751, d = 74), MiniBooNE (Dua & Graff, 2017) (n = 130,064, d = 50), Pose5",
  "(Catalin Ionescu, 2011; Ionescu et al., 2014) (n = 35,832, d = 48), RNA (Uzilov et al., 2006) (n = 488,565,d = 8), and Song (Bertin-Mahieux et al., 2011) (n = 515,345, d = 90)": "We pre-process the data sets to ensure each data point has bounded 2-norm. Following the common approachin differential privacy to bound contributions at a quantile (Abadi et al., 2016; Geyer et al., 2017; Amin et al.,2019), we set the 2 cut-off point r to the 97.5 percentile and discard the points whose norm exceeds r. Moreover,we center each data set since this is a prerequisite for the coreset-based sampling distribution, see .",
  "d2 and count =3": "4d2sum, where = 0.225 as suggested by Su et al.(2016). Here, B is a constant controlling the noise scales that we select to achieve a specific target epsilon {0.5, 1, 3, 10, 30, 100, 300, 1000.0} for a given (expected) subsample size m and vice versa. We evaluate the following three different importance samplers and use various sample sizes, i.e., m , depending on the data set. For the coreset-based (core) sampling, the sampling distribution isqcore(x) = m",
  "Covertype": "unifcoreopt Relative subset size [%] m/ndata unifcoreopt Total relative computation time [%] Covertype m=20000 Weight computationSubset samplingDP-Lloyd Relative subset size [%] m/ndata : Left: Total relative computation times (left y-axis) and relative subset sizes (right y-axis) asfunctions of subsample sizes m for five data sets. Right: Relative computation times decomposed in weightcomputation, sampling, and DP-Lloyd for the Covertype data and a subsample size of m=20,000. privacy parameter but on all data, i.e., without any subsampling, as a black line (full). Unsurprisingly, thetotal cost of the subsampling methods approaches the total cost of full as m approaches n. Note that thishappens faster for our subsampling methods than for unif. Lastly, we measure the computation times of the different sampling strategies to confirm that they areshort relative to the computation time of DP-Lloyd. This supplements the iteration complexity analysisfrom Proposition 12. (left) shows the total relative computation times (left y-axis), i.e., the time to(i) compute the weights, (ii) subsample the data set, and (iii) compute DP-Lloyd for various subset sizes m,relative to computing DP-Lloyd on all data using = 3.0 for five data sets. Each line in the figure refers to acombination of data set and sampling strategy. In addition, we show the fraction m/ndata (right y-axis) perdata set as a black dotted line. To improve readability, we omit MiniBooNE, Song and RNA from the plotbecause they overlap with the lines of other data sets. For completeness, we show the numbers in tabular formin Appendix C for all data sets. A sampling strategy can be considered efficient if its relative vertical offsetfrom the black dotted line is small. This is the case for all data sets and sampling strategies. Additionally, (right) depicts the decomposed computation times for the Covertype data set and a subsamplesize of m=20,000. We can see that the time needed to subsample the data set (violet) is negligible and thatthe time DP-Lloyd takes (green) is almost static. For this data set, the weight computation (blue) for theopt weight computation takes significantly more time than for unif and core. However, the difference issmall relative to the runtime of DP-Lloyd on all data. Specifically, a subset size of m=20,000 amounts toapproximately 3.5% of the Covertype data set. Using the opt subsampling strategy takes slightly more than4% of the time DP-Lloyd takes on the full data set.",
  "Related Work": "The notion of personalized differential privacy is introduced by Jorgensen et al. (2015) and Ebadi et al.(2015), as well as by Alaggan et al. (2016) under the name of heterogeneous differential privacy. In Jorgensenet al. (2015), the privacy parameter is associated with a user, while it is associated with the value of adata point in the work of Ebadi et al. (2015) and ours. Jorgensen et al. (2015) achieve personalized DP bysubsampling the data with heterogeneous probabilities, but without accounting for the bias introduced bythe heterogeneity. Moreover, this privacy analysis is loose as it does not exploit the inherent heterogeneity ofthe original mechanisms privacy guarantee. Recently, there has been renewed interest in PDP due to its connection to individual privacy accountingand fully adaptive composition (Feldman & Zrnic, 2021; Koskela et al., 2023; Yu et al., 2023). In thiscontext, privacy filters have been proposed as a means to answer more queries about a data set by reducing a",
  "Conclusion": "We introduced and analyzed Poisson importance sampling for subsampling differentially private mechanisms.We observed that, for typical mechanisms, privacy is well aligned with utility and at odds with sample size.Based on this insight, we proposed two importance sampling distributions: one that navigates the trade-offbetween privacy and sample size and another based on coresets which have strong utility guarantees. Theempirical results suggest that both strategies have stronger privacy and utility than uniform sampling at anygiven sample size. Promising directions for future work include extensions to (, )-DP or Rnyi-DP as well as establishing formalutility guarantees via coresets. For the latter, recent work on confidence intervals for stratified sampling (Linet al., 2024) might serve as a starting point. Moreover, Poisson importance sampling is directly applicableto a streaming setting, because it considers each data point separately. This provides an opportunity toimprove efficiency further. In federated learning, Poisson importance sampling might be used to improve clientselection (Zhang et al., 2024). Additionally, its connection to fairness could be explored where importancesampling can be used to mitigate bias (Wang et al., 2023).",
  "Acknowledgements": "This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program(WASP) funded by the Knut and Alice Wallenberg Foundation. This research has been carried out as part ofthe Vinnova Competence Center for Trustworthy Edge Computing Systems and Applications at KTH RoyalInstitute of Technology. Martn Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computerand Communications Security, pp. 308318. ACM, 2016.",
  "Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The million song dataset. InProceedings of the 12th International Conference on Music Information Retrieval, 2011": "Jock A Blackard and Denis J Dean. Comparative accuracies of artificial neural networks and discriminantanalysis in predicting forest cover types from cartographic variables.Computers and Electronics inAgriculture, 24(3):131151, 1999. Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: the sulq framework. InProceedings of the Twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of DatabaseSystems, pp. 128138, 2005. Mark Bun, Jrg Drechsler, Marco Gaboardi, Audra McMillan, and Jayshree Sarathy. Controlling privacy lossin sampling schemes: An analysis of stratified and cluster sampling. In 3rd Symposium on Foundations ofResponsible Computing, FORC 2022, June 6-8, 2022, Cambridge, MA, USA, volume 218 of LIPIcs, pp.1:11:24. Schloss Dagstuhl - Leibniz-Zentrum fr Informatik, 2022.",
  "Cristian Sminchisescu Catalin Ionescu, Fuxin Li. Latent structured models for human pose estimation. InInternational Conference on Computer Vision, pp. 22202227, 2011": "Chih-chung Chang and Chih-Jen Lin. IJCNN 2001 challenge: Generalization ability and text decoding. InProceedings of International Joint Conference on Neural Networks, volume 2, pp. 10311036. IEEE, 2001. Vincent Cohen-Addad, Alessandro Epasto, Vahab Mirrokni, Shyam Narayanan, and Peilin Zhong. Near-optimal private and scalable k-clustering. In Advances in Neural Information Processing Systems, volume 35,pp. 1046210475, 2022.",
  "Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. Differentially private clustering: Tight approximationratios. In Advances in Neural Information Processing Systems, volume 33, pp. 40404054, 2020": "Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers, Pauli Virtanen, DavidCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus,Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernndez del Ro, MarkWiebe, Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, HameerAbbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, 2020. Zhiyi Huang and Jinyan Liu. Optimal differentially private algorithms for k-means clustering. In Proceedingsof the 37th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, pp. 395408,2018. Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6M: Large Scale Datasetsand Predictive Methods for 3D Human Sensing in Natural Environments. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2014.",
  "Uri Stemmer and Haim Kaplan. Differentially private k-means with constant multiplicative error. In Advancesin Neural Information Processing Systems, volume 31, 2018": "Dong Su, Jianneng Cao, Ninghui Li, Elisa Bertino, and Hongxia Jin. Differentially private k-means clustering.In Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, pp. 2637,2016. Dong Su, Jianneng Cao, Ninghui Li, Elisa Bertino, Min Lyu, and Hongxia Jin. Differentially private k-meansclustering and a hybrid approach to private optimization. ACM Transactions on Privacy and Security, 20(4):133, 2017.",
  "Andrew V Uzilov, Joshua M Keegan, and David H Mathews. Detection of non-coding rnas on the basis ofpredicted secondary structure formation free energy change. BMC Bioinformatics, 7(1):130, 2006": "Rui Wang, Pengyu Cheng, and Ricardo Henao. Toward fairness in text generation via mutual informationminimization based on importance sampling. In International Conference on Artificial Intelligence andStatistics, 25-27 April 2023, Palau de Congressos, Valencia, Spain, pp. 44734485. PMLR, 2023. Jianxin Wei, Ergute Bao, Xiaokui Xiao, and Yin Yang. Dpis: An enhanced mechanism for differentiallyprivate sgd with importance sampling. In Proceedings of the 2022 ACM SIGSAC Conference on Computerand Communications Security, pp. 28852899, 2022. Da Yu, Gautam Kamath, Janardhan Kulkarni, Tie-Yan Liu, Jian Yin, and Huishuai Zhang. Individualprivacy accounting for differentially private stochastic gradient descent. Transactions on Machine LearningResearch, 2023. Jiaojiao Zhang, Dominik Fay, and Mikael Johansson. Dynamic privacy allocation for locally differentiallyprivate federated learning with composite objectives. In ICASSP 2024 - 2024 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pp. 94619465, 2024.",
  "ACan sampling improve privacy?": "We have pointed out in that, in importance sampling, the privacy loss is not necessarily reducedby decreasing the sampling probability. In this section, we make this statement more precise and provideformal conditions on the PDP profile under which weighted sampling does or does not improve privacy whenreweighting is accounted for. First, note that any mechanism M that simply ignores the weights {wi}ni=1 has a weighted PDP profile(w, x) that is constant in w. In this case, Theorem 6 reduces to the established amplification by subsamplingresult (Proposition 3), which indeed implies that M satisfies a stronger privacy guarantee than M. This ispossible because such a mechanism is not invariant under splitting a weighted point (w, x) into w unweightedpoints {(1, x)}wi=1. The following proposition provides a more general sufficient condition under which Msatisfies a stronger privacy guarantee than M. Proposition 16. Let M : X [1, ) Y be an -PDP mechanism, M be its unweighted counterpartdefined as M({xi}ni=1) = M({(1, xi)}ni=1), and (x) = (1, x) be the PDP profile of M. Furthermore, let be differentiable in w and (w, x) = (w, x)/w. Then, there is a Poisson importance sampler S() forwhich the following holds. Let be the PDP profile of M S implied by Theorem 6. For any x X, if(1, x) < 1 e(1,x), then (x) < (x). Proof. Let x X be any data point for which (1, x) < 1 e(1,x) holds. We treat (x) as a function ofthe selection probability q(x) and show that it is increasing at q(x) = 1. We write q(x)(x) to make thedependence on q(x) explicit and define (w) = exp(1/w(x)) 1. We have",
  ",": "where we used the assumption (w, x) w(w, x) in the first inequality and the fact that ez z + 1 for allz 0 in the second inequality. As a result, is minimized at w = 1 and, hence, q is minimized at q(x) = 1.To complete the proof, observe that q(x) = (x) for q(x) = 1. For instance, the condition (w, x) w(w, x) is satisfied everywhere by any profile of the form (w, x) =f(x)wp, where p 1 and f is any non-negative function that does not depend on w. This includes anymechanism that is invariant under splitting a weighted point (w, x) into w unweighted points {(1, x)}wi=1since group privacy implies a linear PDP profile in this case. All mechanisms considered in this paper satisfythis invariance. It is important to note that, even in a case where we cannot hope to improve upon the original mechanism,it is still possible to obtain a stronger privacy amplification than with uniform subsampling at the samesampling rate. Indeed, the uniform distribution is never optimal unless the PDP profile of the originalmechanism is constant.",
  "Ijcnn148,740221.511.26Pose34,936482250.641249739.50MiniBooNE126,812502213.05295947.13FMA103,9095186020.235838871.88": "depicts the results for the remaining data sets Ijcnn1, Pose, MiniBooNE, and FMA. Note thatthose data sets are smaller in terms of the number of data points as the data sets shown in , see. As seen in , we can observe the smallest difference among the subsampling strategies acrossall data sets occurs for Ijcnn1, especially for the m=15,000 case. All subsampling strategies behave alike,except for the m=10,000 case where unif is worse than core and opt. On Pose, MiniBooNE, and FMA, oursubsampling strategies consistently outperform unif. In addition, depicts the cost-sample-size-mratios for the remaining data sets Ijcnn1, Pose, MiniBooNE, and FMA for fixed privacy budgets of = 3 and = 100. Once again, the only data set in which no clear improvement is visible is Ijcnn1.",
  ": end for": "Theorem 10. Let Assumptions 8 and 9 be satisfied. There is a data set-independent function w: X [1, ),such that, for all data sets D X , Problem 7 has a unique solution w(D) = (w1(D), . . . , wn(D)) of theform wi (D) = w(xi). Furthermore, let M be a mechanism that admits the PDP profile and Sq be a Poissonimportance sampler for q(x) = 1/w(x). Then, M Sq satisfies -DP.",
  "v u22for all u, v Rd.(9)": "Proof of Proposition 12. First, we show that Assumption 11 implies Assumption 9. That is, we want tofind a value vi for each i such that all wi vi are infeasible. For some fixed i, define g(w) = (w, xi) andg(w) =ddwg(w). We apply the strong convexity condition from Equation (9) to exp g at u = 1 and v = w:",
  "Since the left hand-side of Equation (10) is strongly convex and the right hand-side is linear, there can be atmost two solutions to the equality. We distinguish two cases": "Case 1: g(1) = and g(1) < 1 eg(1) In this case, there is a neighborhood around w = 1 in which wehave eg(w) < we 1+ 1. This implies that there are two solutions to Equation (10), one at w = 1 andone in w (1, vi]. The latter is the desired solution. Case 2: g(1) < or g(1) 1 eg(1) In this case, either condition guarantees that there is exactly onesolution to Equation (10) in w [1, vi]. With the first condition, it follows from the convexity of exp g. Ifthe first condition is not met but the second one is, then we have eg(w) we 1+ 1 for all w 1. Then,the uniqueness follows from strong convexity.",
  "Tw.(3)": "Proof. The weighted DP-Lloyd algorithm consists of T weighted sum mechanisms and T weighted countmechanisms. The weighted sum mechanism is an exponential mechanism while the weighted count mechanismuses Laplace noise. We first derive the PDP profile for a general exponential mechanism and then reduce thetwo special cases to the general case. Let D = {(wi, xi)}ni=1 {(w0, x0)} and D = {(wi, xi)}ni=1 denote neighboring data sets.Let M(D)be a mechanism taking values in Rd, distributed according to the probability density function f(y) exp(k d(y, g(D))) where k > 0, d(, ) is a metric on Rd and g : [1, ) X Y is a function where Y Rd.In order to bound the privacy loss of M, we require that the influence on g of any single weighted point(w0, x0) be bounded. Specifically, we require that there exist a function : X R0 such that"
}