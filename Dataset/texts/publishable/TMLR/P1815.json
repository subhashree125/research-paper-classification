{
  "Abstract": "Deep learning models have significantly improved prediction accuracy in various fields, gain-ing recognition across numerous disciplines. Yet, an aspect of deep learning that remainsinsufficiently addressed is the assessment of prediction uncertainty. Producing reliable un-certainty estimators could be crucial in practical terms. For instance, predictions associatedwith a high degree of uncertainty could be sent for further evaluation. Recent works in un-certainty quantification of deep learning predictions, including Bayesian posterior credibleintervals and a frequentist confidence-interval estimation, have proven to yield either invalidor overly conservative intervals. Furthermore, there is currently no method for quantifyinguncertainty that can accommodate deep neural networks for survival (time-to-event) datathat involves right-censored outcomes.In this work, we provide a non-parametric bootstrapmethod that disentangles data uncertainty from the noise inherent in the adopted optimiza-tion algorithm. The validity of the proposed approach is demonstrated through an extensivesimulation study, which shows that the method is accurate (i.e., valid and not overly con-servative) as long as the network is sufficiently deep to ensure that the estimators providedby the deep neural network exhibit minimal bias. Otherwise, undercoverage of up to 8%is observed.The proposed ad-hoc method can be easily integrated into any deep neuralnetwork without interfering with the training process. The utility of the proposed approachis demonstrated through two applications: constructing simultaneous confidence bands forsurvival curves generated by deep neural networks dealing with right-censored survival data,and constructing a confidence interval for classification probabilities in the context of binaryclassification regression. Code for the data analysis and reported simulation is available atGithubsite:",
  "Introduction": "Deep neural networks (DNNs) have gained popularity due to several key factors: (i) High performance - DNNshave demonstrated superior performance in various tasks, such as image recognition, speech processing, andnatural language understanding, often surpassing traditional statistical or machine learning methods. (ii)Ability to learn complex patterns - with multiple layers of neurons, DNNs can learn intricate and hierarchicalpatterns in data, making them effective for tasks that involve complex data structures. (iii) Scalability -DNNs can handle large-scale data and benefit from the availability of big data and powerful computationalresources. (iv) Flexibility and adaptability - DNNs can be adapted to a wide range of applications anddomains. Ongoing research and development in neural network architectures, optimization techniques, andhardware accelerators, such as GPUs and TPUs, continually enhance the capabilities and efficiency of DNNs.",
  "These factors, among others, have led to the widespread use and popularity of deep neural networks in bothacademia and industry (LeCun et al., 2015)": "However, a recent survey on uncertainty in DNN (Gawlikowski et al., 2023) suggests that the real-worldapplications of DNNs are still restricted, primarily because of their failure to offer reliable estimates ofuncertainty.Accurate uncertainty estimates are of high practical importance.For instance, predictionsmade with a high degree of uncertainty can be either disregarded or referred to human specialists for furtherevaluation (Gal & Ghahramani, 2016; Gawlikowski et al., 2023, and references therein). In this work, wepresent a resampling-based estimator of DNN prediction uncertainty that can be integrated with any DNNalgorithm without interfering with its operation or compromising its accuracy.",
  "Quantifying Uncertainty in Deep Learning - Related Works": "Recently, there has been increasing attention to assessing uncertainty within DNNs. Gawlikowski et al. (2023)provided a comprehensive overview of uncertainty estimation in neural networks, reviewing recent advancesin the field and categorizing existing methods into four groups based on the number (single or multiple) andthe nature (deterministic or stochastic) of the neural networks used. Alaa & Van Der Schaar (2020) notedthat existing methods for uncertainty quantification predominantly rely on Bayesian neural networks (BNNs),while Bayesian credible intervals do not guarantee frequentist coverage, and approximate posterior inferenceundermines discriminative accuracy. They also mentioned that BNNs often require major modifications tothe training procedure, and exact Bayesian inference is computationally prohibitive in practice. Moreover,Kuleshov et al. (2018) and Alaa & Van Der Schaar (2020) demonstrated that Bayesian uncertainty estimatesoften fail to capture the true data distribution. For instance, a 95% posterior credible interval generallycontains the true value much less than 95% of the time. Kuleshov et al. (2018) suggested an additionalcalibration step, showing that it can guarantee the desired coverage rate for a variety of BNN regressionalgorithms. Alaa & Van Der Schaar (2020) introduced a discriminative jackknife (DJ) procedure for point-wise predictiveconfidence intervals, a frequentist method based on Barber et al. (2021). Their approach is applicable tovarious DNN models, easy to implement, and can be applied in an ad-hoc fashion without interfering withmodel training or compromising its accuracy. The experiments in their study show that DJ is often overlyconservative, with coverage rates exceeding the desired nominal level. For example, achieving a 100% coveragerate instead of the intended 90%. This conservatism, while valid, suggests that narrower intervals could beused while still maintaining the desired nominal coverage level.Obviously, using a narrower confidenceinterval at the same confidence level is more informative and thus advantageous. The resampling method presented in the current work will be demonstrated to provide an accurate coveragerate, as long as the estimators provided by the DNN have only a small bias. In such cases, our proposedapproach effectively achieves the desired target coverage rate without being overly conservative. While thepoint-wise predictive confidence interval by Alaa & Van Der Schaar (2020) is suitable for various deep learningmodels, it is not applicable to deep learning for survival analysis for two reasons. First, their approach relieson residuals-the difference between the actual and predicted outcomes-but in censored survival data,the actual outcome is often unobservable. Second, they focus on confidence intervals for the outcome of anew observation, whereas in some deep learning applications, it is more relevant to estimate a function of anew observation, such as the probability of a new patient surviving the next ten years. This limitation willbe elaborated further in the following sections. Our proposed approach is suitable for various deep learningmethods, including those used in survival analysis.",
  "Survival Prediction with Deep Learning": "Survival analysis involves modeling the time until a predefined event occurs. This type of analysis is commonin various fields such as medicine, public health, epidemiology, engineering, and finance. Survival predictionmethods are often used at some baseline time, such as the time of diagnosis or treatment initiation, toaddress questions such as, What is the probability that this individual will be alive in ten years, given theirbaseline characteristics? Clinicians often use these predicted probabilities to make critical decisions aboutpatient care and the implementation of specific therapies. In the context of credit risk, a credit scoring model",
  "involves predicting the probability that an account will default over a future time period based on a numberof observed features that characterize account holders or applicants": "Training datsets of survival data typically include censored or truncated observations. Censored data arisewhen the exact time of the event is unknown but falls within a certain period. There are several typesof censoring.Right censoring: The individual is known to be free of the event at a given time.Leftcensoring: The individual experienced the event before the study began. Interval censoring: The eventoccurs within a specified interval. Truncation involves different schemes. Left truncation: Only individualswith event time beyond a certain time are included in the sample. Right truncation: Only individuals whohave experienced the event by a specific time are included. Each type of censoring and truncation requires aspecialized estimation procedure to obtain reliable risk predictions (Klein & Moeschberger, 2006; Kalbfleisch& Prentice, 2011). In this work, we mainly focus on right-censored data since this is the most commonsetting. However, our approach is general and can be adopted to any type of censoring and truncation. The popularity of survival DNNs has been on the rise in recent years. Nearly every new development inDNN is swiftly accompanied by an adaptation for survival analysis. Recent reviews on deep learning forsurvival analysis can be found in Hao et al. (2021); Zhong et al. (2022); Deepa & Gunavathi (2022); Wiegrebeet al. (2024). To summarize, Faraggi & Simon (1995) replaced the linear component of the well-known Coxproportional hazards model Cox (1972) with a one-hidden-layer neural network. However, their approachdid not produce significant improvements over the traditional Cox model in terms of the concordance indexHarrell et al. (1982). Later, Katzman et al. (2016) expanded on Faraggi & Simon (1995)s approach byusing a multilayer neural network, named DeepSurv, which achieved remarkable results in a study on breastcancer. Various adaptations of the Cox model using more complex architectures have been developed forspecific applications, such as genomics data, clinical research, and medical imaging (Yousefi et al., 2017;Ching et al., 2018; Matsuo et al., 2019; Haarburger et al., 2019; Li et al., 2019). Additional DNNs methodsfor survival data include Ranganath et al. (2016); Chapfuwa et al. (2018); Giunchiglia et al. (2018); Leeet al. (2018); Ren et al. (2019), among others. In the comprehensive systematic review by Wiegrebe et al.(2024), it is observed that of the 61 reviewed methods, 26 are based on extended versions of the Cox model.This includes DeepSurv by Katzman et al. (2016) and CoxTime by Kvamme et al. (2019), an efficient andflexible extension of DeepSurv that incorporates time-dependent effects. Another popular approach is theDeepHit of Lee et al. (2018), a discrete-time deep learning survival method that avoids assumptions about theunderlying stochastic process. However, all current DNN approaches for survival analysis overlook estimatingthe uncertainty of predictions.",
  "Contributions": "In this work, we introduce a novel resampling approach for estimating prediction uncertainty, applicable toa wide range of deep neural networks and machine learning methods. The approach is presented in a generalcontext, with its utility demonstrated through two applications: survival analysis and binary classificationregression. Point-wise confidence intervals and simultaneous confidence bands are used to express the uncertainty aroundestimated parameters or functions, but they differ in their scope and interpretation. In survival analysis,for example, a point-wise confidence interval for the survival probability at a certain time point is onlyvalid for that single time point. A simultaneous confidence band provides an estimate for the value of afunction over a range of points. Confidence bands are generally more complex to construct due to the needto account for the correlation between points over the range. A common incorrect practice is to plot point-wise confidence intervals for multiple time points and to interpret the resulting curves as a confidence band.This misinterpretation suggests, for example, 95% confidence that the area between the curves covers thetrue survival curve. However, it is well known that the bands obtained this way are too narrow for such aninference to be correct. To the best of our knowledge, no published works exist on confidence bands basedon neural network or DNN analysis. Moreover, the currently existing methods for prediction uncertaintybased on DNNs are not applicable for survival data with censoring. In this work, we present a bootstrap approach that can be applied ad hoc to any deep neural network (DNN)for estimating uncertainty. Unlike existing methods, our approach provides: (1) Valid point-wise confidence",
  "Setup": "Optimization of any DNN usually involves multiple random steps, such as initial values, batch partitions,and training-validation data splitting (the training set is used for computing the loss function, and thevalidation set is used for stopping criteria in the optimization procedure). Running the same DNN on thesame dataset multiple times provides highly similar results but not necessarily identical ones. Some of therandom steps, such as the step of training-validation data splitting, can be easily controlled by the userswithout compromising the optimization quality, but others, such as batch partitions, cannot. A naive application of a non-parametric bootstrap for prediction uncertainty estimation involves trainingmultiple DNNs on different bootstrap training samples, which are random samples with replacement from thetraining dataset. One of the fundamental requirements for successfully applying the bootstrap procedure isto apply the exact same procedure on each bootstrap sample as done on the original training data. However,in DNN procedures, bootstrap samples will often be based on nonidentical random steps that are intrinsicparts of the DNN and cannot easily be controlled by the user or are not recommended to be fixed. Hence,the results of the bootstrap samples include not only variability due to the data but also arbitrary variabilitydue to the use of different randomness in various steps within the optimization procedure. Therefore, sucha naive approach is expected to overestimate uncertainty, as will be shown below. Let n be the DNN estimator based on n observations (training and validation), to be used for prediction orestimation at a new test point. It is desirable for a prediction or estimation to be accompanied by a measureof error or uncertainty. In particular, we consider an interval or band around the prediction or estimation,as defined in the following examples. Example 1: Consider a standard supervised learning setting where x denotes the features, x X Rd,and y Y denotes the outcome. A prediction model f(x; ) is trained to predict y using the data Dn ={(xi, yi) , i = 1, . . . , n} and we get f(x; n). For a new test point with features x X the outcome y ispredicted by f(x; n). A point-wise confidence interval (also known as a prediction interval) [L(x), U(x)] oflevel (1 )100%, (0, 1), is defined by",
  "Pr{y [L(x), U(x)]} = 1 (1)": "where L(x) and U(x) are functions of f(x; n) and the probability is taken with respect to a new testpoint (x, y) and Dn. An estimator of uncertainty in the prediction of y is expressed through the pointwiseconfidence interval [L(x), U(x)]. For any given , it is desired to find the pair L(x) and U(x) that will beclose together in some sense. For example, if U(x) L(x), which represents the length of the confidenceinterval, is random, the goal is to select the pair L(x) and U(x) that makes the average length of the intervalsmallest. Alaa & Van Der Schaar (2020) considered this example but with a conservative definition of the point-wiseconfidence interval, such that the equality sign in Eq. (1) is replaced with . Therefore, their proposedapproach is often provides a confidence level much higher than 1 at the cost of a wider interval. Clearly,wider intervals imply less confidence, and vice versa. Example 2: In time-to-event data with right censoring, the true event times are typically unknown for allindividuals. The training and validation data consists of yi = (ti, di) and xi for each observation i, whereti = min(ti , ci), ti and ci are the event and right-censoring times, respectively, and di = I(ti = ti ) is theevent indicator. Hence, the dataset (training and validation) consists of Dn = {(xi, ti, di) , i = 1, . . . , n}. Fora new test point x, the goal in survival analysis is usually not to predict the event time t based on x, butrather to provide an estimator for the conditional survival function, given x, within a pre-specified interval,",
  "S(s|x) = Pr(t > s|x; ) , x X , s [0, )": "Hence, in this setting, Sn(s|x, n) is an estimated survival curve produced by the DNN (e.g., DeepHit andCoxTime) for a vector of features x and s . Usually, is defined not only by the desired uppervalue but also by the data. For example, the survival function cannot be reliably estimated beyond the lastobserved failure time (i.e., the maximum value of ti such that di = 1).",
  "Pr{S(s0|x) [L(s0|x), U(s0|x)]} = 1 ,(2)": "where L(s0|x) and U(s0|x) are functions of Sn(s0|x, n) and the probability is taken with respect to a newtest point x and Dn. In many applications (Sachs et al., 2022, and references therein), it is of interest tofind upper and lower curves which guarantee, with a given confidence level, that the conditional survivalfunction of a new test x is covered within the curves for all s in a desired interval B, B . Namely, wewish to find two random functions L(s|x) and U(s|x), which are functions of Sn(s|x, n), such that",
  "Main Idea - Ensemble-Based Bootstrap Procedure": "In the following, we develop a bootstrap approach as a method of estimating the distribution of a function ofa DNN statistic and a new data point, denoted by n(x, n). In examples 1 and 2 above, n(x, n) = f(x; n)and n(x, n) = Sn(s0|x, n), respectively. To generate valid confidence intervals or simultaneous confidencebands based on DNNs, it is required to assume that the DNNs under consideration provide consistentestimators (Carney et al., 1999; Alaa & Van Der Schaar, 2020). Although these assumptions do not holdin general, it is often accepted that the bias of various DNNs estimators is small and much smaller thanthe variance (Geman et al., 1992; Carney et al., 1999). The simulation study in Sections 3 and 4 supportsthe assumptions regarding CoxTime (Kvamme et al., 2019) and classification regression with deep networks,under certain smoothness constraints, given a sufficiently large training dataset and a sufficiently deepnetwork. Let o denote the true value of the desired quantity and assume that the distribution of the difference,n o, contains all the information needed for assessing the precision of n. We start by decomposing ninto a sum of two random variables Z1, Z2, such that",
  "where F1 and F2 are unknown distributions with means 0 and variances v1 and v2, respectively, so the desiredvariance isvar(n) = v1 + v2 + 2v12": "where v12 = cov(Z1, Z2). Z1 is constant given the dataset Dn, while Z2 captures the added variability dueto the inherited randomness of the DNN optimization procedure. Thus, Z2 is not constant given Dn, and aslong as v2 > 0, running the same DNN on the same dataset multiple times provides highly similar results butnot necessarily identical ones. A naive bootstrap procedure based on B bootstrap samples of the trainingdataset provides (b)nand then (b)n= (b)n (x, (b)n ), b = 1, . . . , B. Similarly, each (b)nis decomposed intoZ(b)1+ Z(b)2where the conditional distributions, given Dn, are",
  "Therefore,varZ(b)1+ Z(b)2 Z1 Z2|Dn= v1 + 2v2 + 2v12(6)": "since cov(Z(b)j , Z2|Dn) = 0, j = 1, 2, b = 1, . . . , B. The extra variance, v2, in Eq.(6), is the reason thata naive bootstrap variance estimator, overestimates the variance.However, even if the naive bootstrapprocedure provides conservative confidence intervals or simultaneous confidence bands, it should not generallybe considered a valid approach, as it includes arbitrary noise that is not part of the true uncertainty we wishto estimate. This will be demonstrated in the simulation study in Sections 3 and 4.",
  "The above decomposition motivates the following ensemble-based bootstrap procedure for eliminating theextra variance v2:": "1. Generate an ensemble estimator Mn= M 1 Mm=1 n,m, where n,m=n,m(x, n,m) andn,1, . . . , n,M are M repetitions of the DNN applied on the original dataset without interferingwith the model training. M should be large enough such that v2/M 0. In our simulation anddata analysis we found that M = 100 is sufficient.",
  "This approach requires B + M runs of the DNN": "A DNN training process is based on an iterative process with a convergence rule. To avoid over fitting,during training, the model is evaluated on a holdout validation dataset after each epoch, while the gradientdescent procedure is applied only on the training dataset. If the performance of the model, evaluated onthe validation dataset, is not improving for a pre-specified number of consecutive runs (known as patience),that is, the validation loss function increases, the training process is stopped. The final model is the bestperforming model in terms of validation loss. The patience is often set somewhere between 10 and 100 (10or 20 is more common, we used 15), but it depends on the dataset and network. To avoid overlap betweenthe training and holdout validation dataset within each bootstrap sample, we first do training-validationsplitting and then bootstrap the training dataset. Consider, for example, a binary classification regression where y {0, 1}. The objective is to estimate theprobability p(x) = Pr(y = 1|x) using a DNN estimator, n = pn(x), along with the corresponding confidenceinterval. Specifically, we aim to satisfy",
  "whereWn(s|x; n) = [Sn(s|x; n){1 Sn(s|x; n)}]12": "and Sn(s|x; n) is defined similarly to Sn(s|x; n) but with values truncated to the range of 0.01 to 0.99.Hence, the width of the confidence band of S(s|x) based on the above, is proportional to the approximatedstandard error of its estimator, {Sn(s|x; n)(1 Sn(s|x; n)}12 .The use of truncated survival estimatorSn(s|x; n) in the denominator is in order to avoid instabilities in cases where Sn(s|x; n) is too close toeither 1 or 0. The following is the modified algorithm:",
  "andU prop(s|x) = minSn(s|x; n) + Wn(s|x; n)dbootsp,1(x) , 1": "Through a comprehensive simulation study and benchmark datasets, we show in the next sections thatconfidence bands based on the naive bootstrap approach are overly conservative, resulting in wide confidencebands. In contrast, with deep networks and a sufficiently large training datasets, the proposed approachprovides the desired confidence level with narrower confidence widths compared to the naive bootstrapapproach. Moreover, the weight-based bands, Lprop and U prop, are narrower than those of LKS and U KS.",
  "Simulation Study - Survival Analysis": "We illustrate the performance of the proposed ensemble-based approach by constructing confidence bandsfor survival curves, specifically focusing on the CoxTime DNN (Kvamme et al., 2019). When comparingCoxTime with DeepSurv (Katzman et al., 2016), the two leading DNNs methods for survival analysis,we observe that DeepSurv often shows substantial bias in estimating S(|x) for new observations, whereasCoxTime shows substantially less bias. CoxTime is highly efficient in terms of runtime.One of its effective shortcuts cleverly adopts conceptsfrom nested case-control designs, utilizing a small random subsample of the risk set, referred to as controls,for each loss function evaluation at each observed failure time, instead of the entire risk set. Moreover, thecontrols are resampled at each epoch. Users can specify the number of controls. While the authors suggestedthat even a very small number of controls, as few as one, is sufficient, it will be demonstrated here that thenumber of controls can substantially influence the performance of confidence bands.",
  "Data Generation and Measures of Performances": "We evaluated five different settings, each with multiple repetitions. The extensive number of repetitions foreach setting generated a significant amount of data. To save space, the results were stored and summarizedusing a grid of points T defined for each setting. The hazard functions considered in the simulation studyare of a general formh(t|x) = h0(t) exp{g(t, x)} ,",
  "with the functions h0 and g based on Kvamme et al. (2019) (Settings 13) and Zhong et al. (2022) (Settings45):": "Setting 1 Linear proportional hazards (PH) model:h0(t) = 0.1, g(t, x) = g(x) = T x with =(0.44, 0.66, 0.88).Each covariate xj, j = 1, 2, 3, was uniformly sampled from U.Censor-ing times were generated from an exponential distribution with parameter 1/30, and individualsstill at risk at time t = 30 were censored at that time, resulting in approximately a 30% censoringrate. T = [0.1, 27] with jumps of 0.1.",
  "and the rest follows Setting 1. This resulted in approximately 20% censoring rate": "Setting 3 Non linear and non PH model: h0(t) = 0.02, g(t, x) = a(x) + b(x)t, a(x) = T x + 2/3(x21 + x23 +x1x2 + x1x3 + x2x3) + x3, b(x) = {0.2(x0 + x1) + 0.5x0x1}2 and the rest follows Setting 1. Thisresulted in approximately 30% censoring rate.",
  "g(x) = x21x32 + log(x3 + 1) + x4x5 + 1 + exp(x5/2) 8.2": "were the covariates generated from a Gaussian copula on and correlation parameter 0.5. Right-censoring times were generated from an exponential distribution with parameter 1/28, and individ-uals still at risk at time t = 34 were censored at that time. This resulted in approximately a 50%censoring rate. T = with jumps of 0.1.",
  "Published in Transactions on Machine Learning Research (11/2024)": "method improve and their confidence bands become narrower. While larger control sizes might yield betterresults, they would also be more computationally expensive.Setting 3 demonstrates high sensitivity tothe number of layers, with 6 layers and 8 controls providing good performance in terms of coverage rates.Coverage rates in Settings 4 and 5 also improve with an increased number of controls. Our findings suggestthat more layers might be required to achieve the small bias claim of DNNs survival estimators based onCoxTime of Kvamme et al. (2019). Even though the naive bootstrap procedure can yield over-coverage incases with highly biased DNN estimates (e.g., Setting 3 with only 4 layers), it should not be regarded as avalid approach. The over-coverage simply indicates that the additional arbitrary noise in the naive bootstrapwas large enough to offset the bias. Figures 3 and 4 present the empirical coverage rates and widths for different sizes of controls with n =1000, 1500 and 2000. reveals that even under small sample sizes, the naive bootstrap approachremains overly conservative, whereas the proposed ensemble-based bootstrap methods perform well in termsof coverage rates. shows that as n increases, the width decreases as expected, and the conservativenature of the naive bootstrap leads to substantially wider confidence bands. Notably, the proportional-KS method consistently produces narrower simultaneous confidence bands while maintaining coverage ratessimilar to those of the KS method. Clearly, the ensemble estimator SMnexhibits reduced bias and variance compared to that of Sn (resultsnot shown). However, the computational impracticality of constructing a confidence band for the ensemblesurvival estimator SMn persists using the proposed ensemble-based method.",
  "Additional Computational Aspects": "The analysis was conducted using the coxtime package in Python with Kaiming initialization. We employeda dropout rate of 0.1 and a batch size of 1000. The learning rate was dynamically determined using thelrfinder method, and the Adam optimizer was used for optimization. The networks were standard multi-layer DNNs with ReLU activation and batch normalization between layers. In coxtime, transformers isutilized, and observed failure times were standardized to have a zero mean and a variance of one, whichis necessary due to the inclusion of t in function g. We implemented batch normalization with respect tox.The training was conducted for 1500 epochs.To ensure the effectiveness of the proposed ensemble-bootstrap method, it is essential to minimize estimator bias (a standard requirement). Accordingly, forcomplex settings, we explored various layer configurations, demonstrating that deeper networks with widerlayers are necessary in these cases. For the proposed ensemble-based bootstrap method, each dataset with n training and validation observationshad g(t, xi), i = 1, . . . , n, estimated M times, resulting in gn,m(t, xi), m = 1, . . . , M. We then computedgMn (t, xi) =1MMm=1 gn,m(t, xi) to be used in the Breslow estimator of H0(t) = t0 h0(s)ds. In particular,define the cumulative hazard H(t|x) = t0 h0(s) exp{g(s, x)}ds, and its estimator is given by",
  "Results": "The simulation results are summarized in Figures 14 and include empirical coverage rates and widths ofthe simultaneous confidence bands of the naive bootstrap method, as well as for the two ensemble-basedapproaches, one based on Kolmogorov-Smirnov (KS) statistics, LKS, U KS, and its modification Lprop, U prop. Figures 1 and 2 present the empirical coverage rates and widths for different sizes of controls with n = 10, 000.The results indicate that the naive bootstrap approach consistently exhibits significant over-coverage acrossall settings. In contrast, our ensemble approach achieves coverage rates that are reasonably close to thenominal level. Additionally, as the number of controls increases, the coverage rates of the ensemble-based",
  "j=1(1)j{2.5 0.25(j 1)}xj+5": "The variables (x1, x2) were sampled from a bivariate normal distribution with expectation vector (0, 0), unitvariances, and a correlation of 0.65.x3 was uniform sampled from U, x4 and x5 were uniformlysampled from U, and x6, . . . , x15 were sampled independently from the standard normal distribution.The objective is to estimate the probability p(x) = Pr(y = 1|x) using a DNN estimator along with thecorresponding confidence interval based on Eq. (7) and the subsequent algorithm. We conducted the simulations with sample sizes of n = 1000, 2000 and 5000, splitting the data into 80% fortraining and 20% for validation. Each configuration, with M = 100 and B = 500, is repeated R = 100 times.The test set comprised 1000 observations. The results, shown in , were obtained using a DNN with5, 7 and 9 fully connected hidden layers, each consisting of 128 nodes and utilizing ReLU activation. Theoutput layer consisted of a single neuron with sigmoid activation, and the model was trained using logisticloss. The implementation was done in Keras. It is apparent that the naive bootstrap approach yields overly conservative coverage rates with unnecessarilywide intervals and the coverage rates are not improved as the number of layers increases. In contrast, thecoverage rates of the proposed method are close to the nominal rates and the reduction in confidence intervalwidth achieved by our approach is substantial. Although the proposed method with n = 1000 and five layersshows slight undercoverage (e.g., 0.867 instead of 0.90), the coverage rate improves as the number of layersincreases (e.g., 0.892 with nine layers). These results align with those observed in Setting 3 of .1,where the complex function g appears sensitive to the number of layers, and the bias of the DNN estimatestend to decrease substantially as the number of layers increases. Once again, the over-coverage observed in",
  "FLCHAIN: Assay of Serum Free Light Chain.This dataset includes 6,524 observations with 8covariates and a censoring rate of 0.70": "in Kvamme et al. (2019) shows that CoxTime and DeepSurv are the top two methods based onthe concordance measure when analyzing these datasets. In the current analysis, pre-encoded categoricalvariables were used as-is, while numerical variables were standardized following Kvamme et al. (2019)srecommendations. Hyperparameters tuning was conducted using cross-validation. A grid search over thehyperparameters search space, as detailed in , was performed by splitting the data into 10 folds foreach configuration and scoring the C-index on the held-out set. The set of hyperparameters with the highestaverage C-index was selected. The following are the selected hyperparameters for each dataset: SUPPORT- 4 hidden layer, layer width of 256, dropout 0.3 and batch size of 256; METABRIC - 2 hidden layer, layerwidth of 256, dropout 0.3 and batch size of 1024; Rot.&GBSG - 1 hidden layer, layer width of 128, dropout0.1 and batch size of 256; and FLCHAIN - 1 hidden layer, layer width of 256, dropout 0.1 and batch size of256. The results of the confidence bands analysis, summarized in , are based on 10-fold cross-validation.The held-out fold serves as the test set, while the remaining data were split 80%-20% into training andvalidation sets. The total computation time required for each dataset, including the selection of hyperpa-rameters, is as follows: SUPPORT - 210 minutes, METABRIC - 66 minutes, Rot.& GBSG - 52 minutes,and FLCHAIN - 98 minutes. Examples of survival curves along with their confidence bands can be foundin . As anticipated, the widths of the ensemble-based methods are shorter than those of the naivebootstrap, with the proportional-KS method generally having the narrowest widths.",
  "the decomposition of n and (b)n , as defined in (4) and (5), hold true. (ii) A discussion on existing deepneural networks that produce consistent estimators": "Assume that M is sufficiently large such that the uncertainty of the ensemble estimator due to the inherentrandomness of the DNN optimization process, is zero; that is, varM= v1 and varM Dn) = 0. Forsimplicity, we focus on the asymptotic variance, and similar arguments can be used for other statisticalfunctionals. We write,",
  "=v1 + v2 + 2v12": "The third equality follows from similar reasoning as before, specifically by decomposing (b)ninto Z(b)1andZ(b)2 . For M(b)n(which does not need to be computed in practice), Z(b)2 0. The last equality assumesthat var(Z1) = var(Z(b)1 ) = v1, var(Z2) = var(Z(b)2 ) = v2, and cov(Z1, Z2) = cov(Z(b)1 , Z(b)2 ) = v12. Theseassumptions are based on a common principle (or proven result) in bootstrap methods, often referred toas bootstrap distribution consistency.In our setting, this principle asserts that the (asymptotic) distri-",
  "a(n)(b)n Mninstead of a(n)(b)n n": "Under mild regularity conditions, and using a DNN whose complexity grows with the data, as often used inpractical applications, Schmidt-Hieber (2020) established consistency and convergence rate of the nonpara-metric function estimator in the regression model Y = g(x) + where g is an unknown function and is arandom noise. Recently, Zhong et al. (2022) extended these results to the partially linear Cox mode of theform h0(t) exp{T z + g(x)}. The work of Zhong et al. (2022) covers Settings 1,2,4 and 5 of our numericalstudy, but do not cover Setting 3 with g(t, x), although our numerical results are excellent when the numberof layers is large enough.",
  "Concluding Remarks": "In this paper, we introduced a bootstrap methodology to estimate uncertainty in predictions or estimationsbased on DNN. The core concept involves an ensemble approach to separate data uncertainty from the noiseinherent in the optimization algorithm. Our method is general and can be applied to any DNN analysiswithout interfering with or compromising the DNN predictions themselves. We demonstrated the utility of our general approach in the context of survival analysis, where the primaryfocus is on estimating the survival function based on a new set of features and in the context of binaryclassification. Our comprehensive simulation study indicates that the proposed method is valid and not overlyconservative, as long as the estimators provided by the DNN have only a small bias. Otherwise, undercoverageof up to 8% is observed. Often, bias reduction in DNN can be achieved by increasing the number of layers andnodes. Future research should aim to refine the proposed approach to reduce its computational complexityand perform a comprehensive theoretical analysis to rigorously establish the (asymptotic) accuracy of thecoverage probability across diverse conditions. Code for the data analysis and reported simulation is avalableat Githubsite: We thank the anonymous reviewers for helpful discussions and feedback. The work was supported by theIsrael Science Foundation (ISF) grant number 767/21 and by a grant from the Tel-Aviv University Centerfor AI and Data Science (TAD). Ahmed Alaa and Mihaela Van Der Schaar. Discriminative jackknife: Quantifying uncertainty in deep learningvia higher-order influence functions. In International Conference on Machine Learning, pp. 165174, 2020.",
  "Rina Foygel Barber, Emmanuel J Cands, Aaditya Ramdas, and Ryan J Tibshirani. Predictive inferencewith the jackknife+. The Annals of Statistics, 49(1), 2021": "John G Carney, Pdraig Cunningham, and Umesh Bhagwan. Confidence and prediction intervals for neuralnetwork ensembles. In IJCNN99. International Joint Conference on Neural Networks. Proceedings (Cat.No. 99CH36339), volume 2, pp. 12151218. IEEE, 1999. Paidamoyo Chapfuwa, Chenyang Tao, Chunyuan Li, Courtney Page, Benjamin Goldstein, Lawrence CarinDuke, and Ricardo Henao. Adversarial time-to-event modeling. In International Conference on MachineLearning, pp. 735744. PMLR, 2018.",
  "Stuart Geman, Elie Bienenstock, and Ren Doursat. Neural networks and the bias/variance dilemma. Neuralcomputation, 4(1):158, 1992": "Eleonora Giunchiglia, Anton Nemchenko, and Mihaela van der Schaar. Rnn-surv: A deep recurrent model forsurvival analysis. In Artificial Neural Networks and Machine LearningICANN 2018: 27th InternationalConference on Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part III 27,pp. 2332. Springer, 2018. Christoph Haarburger, Philippe Weitz, Oliver Rippel, and Dorit Merhof. Image-based survival predictionfor lung cancer patients using cnns. In 2019 IEEE 16th International Symposium on Biomedical Imaging(ISBI 2019), pp. 11971201. IEEE, 2019.",
  "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436444, 2015": "Changhee Lee, William Zame, Jinsung Yoon, and Mihaela Van Der Schaar.Deephit: A deep learningapproach to survival analysis with competing risks. In Proceedings of the AAAI conference on artificialintelligence, volume 32, 2018. Hongming Li, Pamela Boimel, James Janopaul-Naylor, Haoyu Zhong, Ying Xiao, Edgar Ben-Josef, and YongFan. Deep convolutional neural networks for imaging data based survival analysis of rectal cancer. In 2019IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019), pp. 846849. IEEE, 2019. Koji Matsuo, Sanjay Purushotham, Bo Jiang, Rachel S Mandelbaum, Tsuyoshi Takiuchi, Yan Liu, andLynda D Roman. Survival outcome prediction in cervical cancer: Cox models vs deep-learning model.American journal of obstetrics and gynecology, 220(4):381e1, 2019."
}