{
  "Abstract": "Multiparty computation approaches to secure neural network inference commonly rely ongarbled circuits for securely executing nonlinear activation functions. However, garbledcircuits require excessive communication between server and client, impose significant storageoverheads, and incur large runtime penalties; for example, securely evaluating ResNet-32using standard approaches requires more than 300MB of communication, over 10s of runtime,and around 5 GB of preprocessing storage. To reduce these costs, we propose an alternativeto garbled circuits: Tabula, an algorithm based on secure lookup tables. Our approachprecomputes lookup tables during an offline phase that contains the result of all possiblenonlinear function calls. Because these tables incur exponential storage costs in the numberof operands and the precision of the input values, we use quantization to reduce thesestorage costs to make this approach practical. This enables an online phase where securelycomputing the result of a nonlinear function requires just a single round of communication,with communication cost equal to twice the number of bits of the input to the nonlinearfunction. In practice our approach costs 2 bytes of communication per nonlinear functioncall in the online phase. Compared to garbled circuits with 8-bit quantized inputs, whencomputing individual nonlinear functions during the online phase, experiments show Tabulawith 8-bit activations uses between 280-560 less communication, is over 100 faster, anduses a comparable (within a factor of 2) amount of storage; compared against other state-of-the-art protocols Tabula achieves greater than 40 communication reduction. Thisleads to significant performance gains over garbled circuits with quantized inputs during theonline phase of secure inference of neural networks: Tabula reduces end-to-end inferencecommunication by up to 9 and achieves an end-to-end inference speedup of up to 50,while imposing comparable storage and offline preprocessing costs.",
  "Functions": ": The Tabula approach to computing nonlinear functions for secure neural network inference.Tabula precomputes lookup tables [T]0, [T]1 stored on client and server respectively, and also initializesshares of the secret s so that the client holds [s]0 and the server holds [s]1. The lookup tables [T]i containthe result of all possible nonlinear function calls to an activation function and uses quantization to makestoring all possible function calls in the table feasible. These lookup tables map secret shares of the quantizedinputs to the nonlinear function to secret shares of the output of the activation function. During the onlinephase, these lookup tables enable extremely efficient nonlinear activation function execution and proceeds by1) securely truncating the inputs, 2) reconstructing a blinded index and 3) looking up the blinded index inthe lookup tables [T]i. Our code is released at network model M while the client holds an input x. The objective of a secure inference protocol is for theclient to compute M(x) without revealing any additional information about the clients input x to the server,and without revealing any information about the servers model M to the client. A protocol for secure neuralnetwork inference brings significant value to both the server and the clients. The clients sensitive input datais kept secret from the server, shielding the user from malicious data collection. Additionally, the client doesnot learn anything about the servers model, which prevents the model from being stolen by competitors. Current state-of-the-art multiparty computation approaches to secure neural network inference requiresignificant communication between client and server, lead to excessive runtime slowdowns, and incur largestorage penalties (Mishra et al., 2020a; Ghodsi et al., 2021; Jha et al., 2021; Rathee et al., 2020; Juvekaret al., 2018; Cho et al., 2021). The source of these expenses is computing nonlinear activation functions withgarbled circuits (Yao, 1986). Garbled circuits are costly in terms of computation, communication, and storage.Concretely, executing ReLU activation functions using garbled circuits requires over 2 KB of communicationper scalar element of the input (Mishra et al., 2020a) and imposes over 17 KB of preprocessing storage perscalar element of the input (Mishra et al., 2020a; Ghodsi et al., 2021). These costs make state-of-the-art neuralnetwork models prohibitively expensive to deploy: on ResNet-32, state-of-the-art multiparty computationapproaches for a single secure inference require more than 300 MB of data communication (Mishra et al.,2020a), take more than 10 seconds for an individual inference (Mishra et al., 2020a), and impose over 5 GB ofpreprocessing storage per inference (Ghodsi et al., 2021). These communication, runtime, and storage costspose a significant barrier to deployment, as they degrade user experience, drain clients batteries, induce highnetwork expenses, and eliminate applications that require sustained real time inference. To replace garbled circuits and other methods (Rathee et al., 2020; Huang et al., 2022) for privately computingnonlinear functions, we propose Tabula, a two-party secure protocol to efficiently evaluate neural networknonlinear activation functions. During an offline preprocessing phase, Tabula generates tables that containthe encrypted result of evaluating a nonlinear activation function over a range of all possible quantized inputs.New tables are precomputed for each nonlinear function performed during inference, and these tables aresplit across client and server. Then, at inference time, Tabula performs two steps to evaluate a nonlinearactivation function: 1) securely quantize neural network activation inputs down to the precision of the range",
  "Published in Transactions on Machine Learning Research (06/2024)": "use with precisions below 8. Specifically, with 8 bits of precision for activation Tabula achieves an 8.25,4.1 and 2 savings vs 32-bit, 16-bit and 8-bit garbled circuits; with ultra low precision Tabula achieveseven more gains (4 bits yields around 136 storage reduction vs 32-bit garbled circuits and 17 reduction vs8-bit garbled circuits). These results imply that standard techniques to quantize activations down below 8bits and advanced techniques to quantize below 4 bits (Ni et al., 2020; de Bruin et al., 2020; Zhao et al., 2020)can be applied with Tabula to achieve significant storage savings. Notably, Tabula achieves storage savingsat ultra low precision activations as a 1-bit reduction in activation precision yields a 2 storage reduction,unlike for garbled circuits where storage is reduced linearly. 45678910 11 12 Precision for Activation Storage/Mem. (KB) TabulaGarbled Circuits",
  "PRG(i.e: AES-128)": ": Comparison of our work against other approaches for securely computing nonlinear activationfunctions across selected axes. Unless specified costs refer to the cost of the online phase. Compared togarbled circuits, the most widely used state-of-the-art protocol for securely computing nonlinear functions,our approach sees significant improvements in communication and runtime at comparable storage costs.We also compare our approach against less generic protocols for non-linear function computation (tree-based comparator, limited to only ReLU) on the basis of communication where we again see considerableimprovements. Finally, compared to function secret sharing (FSS) schemes, our approach is comparable inattaining low communication cost while being computationally more efficient.",
  "Multiparty Computation Approaches to Secure Neural Network Inference": "Multiparty computation approaches to secure neural network inference have been limited by the costs ofcomputing both the linear and nonlinear portions of the network (Mohassel & Zhang, 2017; Rouhani et al.,2017; Rathee et al., 2020; Keller, 2020). Recent works like Minionn, Gazelle and Delphi (Liu et al., 2017;Juvekar et al., 2018; Mishra et al., 2020a; Lehmkuhl et al., 2021; Rathee et al., 2020; Jha et al., 2021; Choet al., 2021; Ghodsi et al., 2021) have optimized the linear operations of secure neural network inference viatechniques like preprocessing to the point they are no longer a major system bottleneck (Mishra et al., 2020a).Hence, current state-of-the-art approaches to secure inference like Minionn, Gazelle, Delphi, and CrypTFlow2are primarily bottlenecked by nonlinear operations. Specifically, these approaches rely on garbled circuits, ora circuit-based protocol, to compute nonlinear activation functions (e.g: ReLU) (Liu et al., 2017; Juvekaret al., 2018; Mishra et al., 2020a; Keller & Sun, 2021; Dalskov et al., 2020), resulting in notable drawbacksincluding high communication, runtime and storage costs. Our approach addresses the problems posed by garbled circuits by eliminating them altogether. Our methodis centered around precomputing lookup tables containing the encrypted results of nonlinear activationfunctions, and using quantization to reduce the size of these tables to make them practical.",
  "Lookup Tables for Secure Computation": "Lookup tables have been used to speed up computation for applications in both secure multiparty computation(Launchbury et al., 2012; Damgrd et al., 2017; Keller et al., 2017; Rass et al., 2015; Dessouky et al., 2017)and homomorphic encryption (Li et al., 2019; Crawford et al., 2018). These works have demonstrated thatlookup tables may be used for garbled circuits computation Heath et al. (2024) or as an efficient alternative togarbled circuits, provided that the input space is small. Prior works have primarily focused on using lookuptables to speed up traditional applications like computing AES (Keller et al., 2017; Damgrd et al., 2017;Launchbury et al., 2012; Dessouky et al., 2017) and data aggregation (Rass et al., 2015). Notable exceptionsinclude (Crawford et al., 2018) which focuses on linear regression, and (Rathee et al., 2021) which appliesvariants of a lookup table as part of the protocol to secure machine learning inference. Lookup tables are",
  "also widely used as cryptographic primitives in SNARKS (Setty et al., 2023; Arun et al., 2023), notably asefficient primitives for non-arithmetic operations inside circuits (Arun et al., 2023)": "To the best of our knowledge, there exists little prior work which applies secure lookup tables to the privateexecution of large neural networks. Most current state-of-the-art secure inference systems like (Mishraet al., 2020a; Mohassel & Zhang, 2017; Ghodsi et al., 2021; Jha et al., 2021) use garbled circuits. Twoexceptions to this include (Rathee et al., 2020; Huang et al., 2022), which use a tree-based secure comparator.However, the tree-based secure comparator used in (Rathee et al., 2020; Huang et al., 2022) is significantlylimited to only the ReLU activation function, and still requires significant computation and communicationoverhead. Another work, (Rathee et al., 2021), does indeed use lookup tables as part of their protocolfor evaluating activation functions, but crucially focuses on ensuring numerical precision, leading to lowersystem performance. We highlight that a key distinction in our use of lookup tables is that we store allpossible results of the nonlinear function in these tables, which uses exponential storage. This storage ismade manageable by heavily quantizing the neural network. This strategy of securely evaluating a functionthrough lookup tables, although known theoretically (Ishai et al.; Damgrd & Zakarias, 2016), to the best ofour knowledge has not been applied practically until now, due to the exponential storage costs. The securelookup table approach of \"storing everything in a table\" is remarkably well suited to securely and efficientlycomputing neural network nonlinear activation functions for two reasons: 1) neural network activations maybe quantized to extremely low precision with little degradation to accuracy and 2) neural network activationfunctions are single operand. These two factors allow us to limit the size of the lookup table to be sufficientlysmall to be practical, and consequently we can achieve the significant performance benefits of secure lookuptables at runtime (i.e two orders of magnitude less communication). While work on quantization applied tosecure inference exists (Dalskov et al., 2020; Keller & Sun, 2021), these works do not combine this propertywith lookup tables for evaluating nonlinear functions. In summary, we emphasize that prior works that usesecure lookup tables have either applied them towards non-ML applications (i.e: MPC for AES-128), or havenot leveraged exponential-sized secure lookup tables in combination with neural network quantization tomake them practical; although the exponential-sized secure lookup table approach for securely computingfunctions is known, the unique combination of this technique with neural network quantization has not beenpreviously explored. In this work, we demonstrate that this unique combination of techniques can be appliedto dramatically reduce the costs of secure neural network inference.",
  "Function Secret Sharing": "The secure lookup table approach (Ishai et al.) employed by Tabula is related to function secret sharing(FSS) approaches used in various private neural network inference approaches like Wagh (2022); Gupta et al.(2022); Agarwal et al. (2022). The secure lookup table approach of \"storing all function inputs/outputs ina secure lookup table\" can be theoretically categorized as a FSS approach. But there are several concretedifferences between the secure table lookup approach Tabula uses compared to traditional FSS approaches.These distinctions lead to significant runtime differences. Concretely, our secure lookup table approach incursexponential storage costs which necessitates aggressive activation quantization to make storage costs practical.However, this approach also enables a highly efficient online phase which requires just one 8-bit memoryaccess (in addition to the 2B communication between parties). FSS approaches, on the other hand, rely onusing distributed point functions (DPFs) or distributed comparison functions (DCFs), (Boyle et al., 2019;2020) which in turn require evaluating PRGs (i.e: AES-128). Specifically, a table lookup using FSS requiresat least log(N) PRG or AES-128 evaluations, where N is the number of entries in the table, leading to 8-16AES-128 evaluations per activation function call. This cost increases for more complex nonlinear functions(Boyle et al., 2019; 2020). Evaluating PRGs like AES-128 is comparatively more expensive than Tabulawhich requires just 1 8-bit memory access, as a modern processor even with hardware acceleration computesonly around 100M AES-128 operations (aes), whereas a modern processor has a memory bandwidth in the100GB/s range. As such, Tabula is much more computationally efficient compared to FSS schemes, thoughas a drawback requires aggressive quantization to make practical. Another benefit to Tabula is that itscommunication cost is always 2B regardless of the nonlinear function being securely computed. This is notthe case for function secret sharing where more complicated nonlinear functions may cost more than 2B Boyleet al. (2019; 2020). Furthermore, a third advantage is that Tabula exhibits information theoretic security inthe online phase, while function secret sharing schemes are only computationally secure up to a factor of the",
  "Background": "Secure Inference Objectives, Threat ModelSecure neural network inference seeks to compute a sequence of linear and nonlinear operations parameterizedby the servers model over a clients input while revealing as little information to either party beyond themodels final prediction. Formally, given the servers models weights Wi FMiLipand the clients privateinput x, the goal of secure neural network inference is to compute ai = A(Wiai1) where a0 = x and A is anonlinear activation function, typically ReLU. Wi FMiLipare the weights of the neural network representedas a fixed point number in the finite field of modulus p. The dimensions Mi and Li correspond to the outputand input dimensions to the linear layer at i. Convolutions may be cast as a matrix multiply and fit withinthis framework. State-of-the-art secure neural network inference protocols like Delphi operate under a two-party semi-honestsetting (Mishra et al., 2020a; Lehmkuhl et al., 2021), where only one of the parties is corrupted and thecorrupted party follows the protocol. Importantly, these secure inference protocols do not protect thearchitecture of the neural network being executed, only its weights, and furthermore do not secure anyinformation leaked by the predictions themselves (Mishra et al., 2020a). As we follow Delphis protocol forthe overall secure execution of the neural network these security assumptions are implicitly assumed. Cryptographic Primitives, Notations, DefinitionsTabula utilizes standard tools in secure multiparty computation. Our protocols operate over additivelyshared secrets in finite fields. We denote Fp as a finite field over n-bit prime p. We use [x] to denote a twoparty additive secret sharing of the scalar x Fp such that x = [x]0 + [x]1, where party i holds additive share[x]i but knows no information about the other share. Delphi Secure Inference ProtocolThe Delphi framework is a set of protocols consisting of an offline preprocessing phase and an online secureinference phase for securely evaluating neural networks over private client data without revealing to the clientthe weights of the neural network. The Delphi framework is concerned with the overall evaluation of theneural network (both linear and nonlinear layers). Tabula fits into the Delphi framework by replacing theiruse of garbled-circuits protocols for secure nonlinear function evaluation, which is the most computationallyexpensive part of their protocol Mishra et al. (2020a). To understand how Tabula fits into the Delphiframework (Mishra et al., 2020b), we outline how this protocol operates. Per-Input Preprocessing PhaseThis phase prepares for the secure execution of a single input. The purpose of the preprocessing phaseis to initialize the parties with correlated randomness that enables efficient online inference. Thisphase, as specified in the Delphi paper, requires the use of linearly homomorphic encryption (Mishraet al., 2020a) to ensure that the parties do not reveal to each other their secret blinding factors whichwould compromise the privacy of the entire protocol. For each linear layer Wi FMiLip, the clientgenerates a random vector Rc FLipwhere Li is the length of the inputs to the current linear layer.The client encrypts Rc with their linearly homomorphic public encryption key k to Enck(Rc) andsends this value to the server. The server, upon receiving Enck(Rc), generates their own secret vectorRs FMipwhere Mi is the length of the outputs of the current linear layer. The server then encrypts",
  "[F(x)]1": ": Tabula online protocol. Initially, the client and server hold secret shares of the input [x]. Bothparties begin by executing the secure truncation protocol to obtain shares of [xtrunc]. Then, the client andserver perform the secure table lookup protocol, where they exchange blinded secrets [xtrunc]i +si to computextrunc + s. Finally, they use this value as an index into local lookup tables to compute Ti[xtrunc + s] whichare secret shares of the result of the nonlinear function evaluation. Rs with the clients public key k to obtain Enck(Rs). The server then computes and returns to theclient Enck(WiRc + Rs). The client decrypts this value to obtain WiRc + Rs which is then stored inpreparation for the online inference phase. Online Inference PhaseThis phase performs inference on the clients input. For linear layers, the client and server beginwith additive secret shares of the linear layers input x. That is the client and server hold [x]0and [x]1 respectively, such that [x]0 + [x]1 = x. As the initial step, the client adds [x]0 with thatlayers Rc to obtain [x]0 + Rc. Then the client sends this vector to the server who adds their ownshare of the layer input [x]1 to obtain x + Rc. The server, upon calculating x + Rc, then computesWi(x + Rc) + Rs = Wix + WiRc + Rs (recall that Rs was the secret vector that the server generatedfor this particular layer). At this point, the client holds WiRc + Rs from the preprocessing phaseand the server has computed Wix + WiRc + Rs. The difference between these two values is Wix.Thus the two parties have obtained a secret sharing of Wix. Then, the two parties must computea nonlinear activation function over these shares to obtain shares of the activations; in the Delphiframework, garbled circuits are employed to securely perform this operation Mishra et al. (2020a),and it is by replacing this part of the protocol that Tabula obtains considerable computationalgains. After calculating shares of the activations, the secure inference phase repeats starting fromthe linear part of the protocol for the rest of the layers of the network. As stated, after performing the protocol for the linear phase, the client and server hold secret shares of theinput to the nonlinear activation function F. Hence we need to construct a secure protocol for performingnonlinear activation functions. This protocol must operate such that the client and server, each holding asecret share of x, can calculate secret shares of F(x) without leaking any information about x itself. F, thenonlinear function for neural network activations, is typically ReLU, but may also be include other nonlinearfunctions like sigmoid or tanh. As a note, we emphasize that details on the homomorphic preprocessing phasecan be found in the Delphi paper (Mishra et al., 2020a).",
  "Tabula for Securely and Efficiently Evaluating Neural Network Nonlinear Activation Functions": "Tabula is divided into a preprocessing phase that initializes a lookup table for each individual nonlinearfunction call used in the neural network, and an online phase which securely quantizes the activation inputsand looks up the result of the function in the previously initialized tables. An overall figure for our protocolis shown in . We emphasize that our paper primarily focuses on the online phase of execution, whichdetermines the systems real time response speed after knowing a clients input, rather than the preprocessingphase, which may be done offline without knowing the clients input data. We also develop a secure andreasonably efficient algorithm for the preprocessing phase of Tabula and conduct thorough experiments in theresults section to demonstrate its viability and effectiveness. We leave further innovations to the preprocessingalgorithm to future research.",
  "In practice, we use a 64-bit finite field modulus to reduce the chance that a secure truncation operationcatastrophically fails to less than 1000": "264 . Hence, by configuring the modulus appropriately, with high probability,the secure truncation protocol computes the correctly truncated value with a small off by one error whichmay be tolerated by neural networks (Ni et al., 2020; Reagan et al., 2018). Requiring 64 bits for the field increases the communication cost required by the linear portions of the protocolover other approaches that commonly use 32 bits, however, the reduction in communication cost by usingTabula tables more than makes up for this communication penalty, and this is shown in the results (notewithout Tabula we use 32-bit precision for the linear layers). In our experiments, using a 64-bit field size was essential to maintaining accuracy; using a 32-bit field sizeTabula saw considerably worse accuracy due to catastrophic failures from the secure truncation protocol, asa single catastrophic failure anywhere in the computation propagates throughout the network and ruins theentire inference. We refer to (Mohassel & Zhang, 2017) for more details. Developing more effective securetruncation techniques is an important topic for further research. We emphasize that there is a distinction between the field size used for the linear layers and the number ofbits for the activations. Concretely, the 64-bit field sizes are used to represent the fixed-point values that areoperands to the linear layers of the network, allowing for near full precision multiply-accumulates duringlinear operations. This is opposed to quantized n-bit (generally n=8) inputs which are used as inputs to thelookup table, which determines the amount of memory used for the tables, as each table has 2n entries. Ourmethod is functionally equivalent to performing the linear operation over 64-bit fixed point values, quantizingthe result down to n bits, computing the activation function over the truncated result, then scaling back upto 64-bit fixed point values for the next linear operation. Tabula Online PhaseGiven these fundamental building blocks, we describe the Tabula protocol. In the preprocessing phase,Tabula generates multiple shared tables [T] as described above for each nonlinear function call that isperformed when executing the neural network. How much to truncate/quantize the network activations ischosen offline to maximize network accuracy. In the online execution phase, Tabula quantizes the inputs tothe activation function and uses this input to securely lookup up the result of the function. The full protocolis shown in . The security of Tabula is ensured by the security of the secure truncation protocol(Mohassel & Zhang, 2017) and the secure table lookup protocol (Keller et al., 2017; Damgrd & Zakarias,2016; Ishai et al.). Tabula Online Phase Communication and Storage CostTabula achieves significant communication benefits during the online phase at comparable storage costs. Asshown in , Tabula requires just one round of communication to compute any arbitrary function,unlike garbled circuits, which may require multiple rounds for more complex functions. As an example,ReLU implemented using garbled circuits takes two rounds, whereas Tabula requires just one. Additionally,communication complexity is independent of the complexity of the nonlinear function being computed.Specifically, revealing sx requires both parties to send their local shares, each nonlinear activation callincurs communication cost corresponding to the number of bits in Fp. Since we use 64-bit Fp, this resultsin 16 bytes of communication per activation function, the cost of transferring 8-byte field values backand forth. However, we can apply an optimization to reduce this down to twice the cost of transferringthe size of the input to the table, rather than the field size. If the size of the table is 2b entries, andif finite field size p is also power of two, then we can have party i first mod their secret shares by 2b before exchanging them. Hence, the two parties hold [xtrunc]i mod 2b before adding their secret sharesof the table secret [s]i to the value and exchanging it; this brings down the total cost of the protocol to2 b bits. Modding by 2b yields the correct answer as xtrunc mod p = [xtrunc]0 + [xtrunc]1 + pl for some l,and then (xtrunc mod p) mod 2b = ([xtrunc]0 + [xtrunc]1 + pl) mod 2b = ([xtrunc]0 + [xtrunc]1) mod 2b =([xtrunc]0 mod 2b) + ([xtrunc]1 mod 2b). This equivalence shows that the two parties can first perform amodulus of their shares with 2b, and that their shares would still sum up to the original sum with the correctmodulus. With this optimization, communication is now 2 b bits per nonlinear function call; if we use 8-bitactivations, then b = 8, and we use 2 bytes of communication total per call during the online phase, an 8improvement over the 16 bytes as previously stated.",
  "Vector & ShareBeaver Triple Outer Product": ": Tabula preprocessing protocol. Client and server generate secrets s0, s1 and encode them in anindicator vector (i.e: construct a vector of length equal to the field size, then setting a one to the positionof the partys secret index). The parties then secret share this indicator vector with the other party. Toobtain the entry for Ti[x], the parties compute an outer product between the shared indicator vectors anda 2-dimensional table containing F(m + n + x) (where m, n span the two dimensions of the table), whichobtains [F(x + s)] where s = s0 + s1. This works as the 2-D coordinates formed by where the indicatorvectors are set privately select m, n through the dot-product; since this is done via private MPC operations,no information is leaked to either party about their corresponding secrets. Storage and memory, as mentioned previously, grow exponentially with the precision that is used for activationsand linearly with the number of activations in the neural network. Storage costs are thus n 2k Na bitswhere n is the number of bits to use for the the output of the activation function, k is the number of bitsto the input of the activation function, and Na is the total number of activations that are performed bythe neural network. The majority of the storage cost comes from the 2k factor, the size of the individualtables, which grows exponentially with input space / precision of the activations. However, as neural networkactivations may be heavily quantized down without significantly affecting model quality (Ni et al., 2020;de Bruin et al., 2020; Zhao et al., 2020), we can reduce this factor enough to be practical; we also highlightmore advanced techniques like using a variable number of bits per layer of the network can be employed forbetter performance (Dong et al., 2019). We verify that quantization has negligible impact on model qualityin our results. We highlight that every bit of precision that is trimmed from the activation yields a factor oftwo reduction in storage and memory costs, and hence more advanced quantization techniques (Ni et al.,2020; de Bruin et al., 2020; Zhao et al., 2020) to reduce precision yields significant benefits. As storage andmemory varies with the precision of activations that is used, there is a natural tradeoff between the accuracyof the model and the achieved memory/storage requirement. We examine these tradeoffs in the results.",
  "Tabula Preprocessing Phase": "Similar to garbled cicuits, Tabula tables require a preprocessing phase that initializes the client and serverwith a single-use table that is used once per activation function call during the inference phase. We develop asecure and efficient protocol for initializing Tabula tables, detailed below. Preprocessing Phase Problem StatementGiven a nonlinear function F : Fp Fp, we wish to securely initialize the client and server with tablesthat map any possible input in Fp to secret shares of the result of the nonlinear function F. Specifically,we wish to initialize on the client a table [T]0 Fpp, and on the server a table [T]1 Fpp, such that[T]0[s + x] + [T]1[s + x] = F(x), where s Fp is a secret unknown to both client and server. Additionally, atthe end of the protocol, we want the client to hold s1 Fp and server to hold s2 Fp such that s1 + s2 = s. Tabula Secure Preprocessing ProtocolTo achieve this preprocessing step securely, the client and server first randomly generate s0 and s1 respectively,and s is implicitly defined as s0+s1 (though, as the parties do not know each others secrets, they hence do not",
  "= [F(s + x)]": "Preprocessing Phase SecuritySecurity and privacy is preserved as each step of the protocol consists entirely of secure steps: secret sharingP and Q leaks no information about the vectors (hence leaks no information about either s0, s1, and s), andBeaver triple multiplication is likewise secure (Beaver, 1991). Concretely, we see that the only communicationbetween client and server occurs when they exchange blinded secrets (i.e: [P], [Q]) and when they performBeaver triple multiplication. As these steps leak no information to either party about the underlying secrets,the client and server compute [T] without leaking any information about s0, s1 and hence leak no informationabout s. Preprocessing Phase Communication and Computation CostThe bulk of the preprocessing phase lies in computing an outer product between P, Q. We perform this outerproduct just once and reuse it across i, x in Ti[x]. Hence, the protocol requires performing just a single outerproduct between vectors Fpp. This incurs O(p2) Beaver triple multiplication operations, and assuming thata sufficient number of Beaver triples were generated before the preprocessing phase, communication cost isnaively O(p2 log(p)) bits assuming that the values of the vectors are each log(p) bits.",
  "n=0F(m + n + x) [PQT ][m, n]": "Observe that [Ti(x)]1 [Ti(x)]2 is either F(s + x) or F(s + x) (in the case that the first party has the 1and the second party has the 0 in the selected index, and the reverse). We perform an extra Beaver triplemultiplication by the correction factor to eliminate this potential negation (by multiplying it by the parity ofthe sum of [PQT ]), which costs an extra O(log(p)) bits of communication per inner-product. Since thereare only p inner products, these correction factors cost a negligible O(p log(p)) communication. With thisoptimization, preprocessing communication cost is now O(p2) bits. As p, the quantized field size of theactivation domain, is set to be extremely small (i.e.: less than or equal to 256 for 8-bit quantized activations),preprocessing communication costs 2(256)2 = 216 = 131072 bits = 16 KB per table (the factor of two at thefront is because Beaver triple multiplication requires both parties exchange secret shared values, and we have2562 Beaver triple multiplication operations). This is comparable to the 17.5 KB cost that garbled circuitswith full precision requires (Mishra et al., 2020a). We emphasize that the prior analysis assumed that Beaver triples were obtained beforehand in a pre-preprocessing phase; we think this is reasonable that in a practical scenario parties would obtain sufficientamounts of Beaver triples for any protocol due to their importance. However, accounting for Beaver triplepreprocessing, communication cost is still an asymptotic O(p2) bits assuming that Beaver triples weregenerated using oblivious transfer e.g: Nielsen et al. (2012), which requires just 2 OT calls to generate 1-bitBeaver triples. Using the OT procedure proposed in Huang et al. (2022) the concrete cost of a single OT is 3bits for 1-bit values. Hence, pre-preprocessing costs for the Beaver triples would still be O(p2) bits, with ahigher constant factor burden. On a concrete example of 8-bit activations, the cost for preprocessing theBeaver triples would amount to 6 2562 bits = 48 KB of communication. While this exceeds the 17.5 KBcost of garbled circuits, we believe that the online benefits of Tabula more than make up for this detriment. In terms of computation, we see that for precomputing a single table, we require summing across p2 values(each entry of the outer product) for every entry of the table. Since there are p entries in the table, computationcosts scale as O(p3). However, these operations may be efficiently vectorized and parallelized as they arestandard matrix operations. For 8-bit tables, this is around 16 million field operations.",
  "Note on Tabula Security": "Although the Tabula protocol assumes a semi-honest threat model as inherited from the Delphi (Mishraet al., 2020a) framework, more generally Tabulas online phase which consists of utilizing a secure lookupprocedure is information-theoretically secure (Ishai et al.). This is intuitive as all communication betweenparties are randomly blinded by an additive factor. This is another advantage that Tabula holds overgarbled circuits implementations many of which are only computationally secure up to a security parameterin the semi-honest setting (mpc).",
  "Results": "We present results showing the benefits of Tabula over garbled circuits for secure neural network inference.We evaluate our method on neural networks including a large variant of LeNet for MNIST, ResNet-32 forCifar10, and ResNet-34 / VGG-16 for Cifar-100, which are relatively large image recognition neural networksthat prior secure inference works benchmark (Ghodsi et al., 2021; Mishra et al., 2020a; Jha et al., 2021; van derHagen & Lucia, 2021). Unless otherwise stated, we compare against an implementation of the Delphi protocol(Mishra et al., 2020a) using garbled circuits for nonlinear activation functions, without neural architecturechanges, during the online inference phase. As before, we use 64-bit fields for Tabula to reduce the impactof the secure truncation protocol, however, for the baseline implementation that uses garbled circuits we use",
  "Communication Reduction": "ReLU Communication ReductionWe benchmark the amount of communication required to perform a single ReLU with Tabula vs garbledcircuits. shows the amount of communication required by both protocols during online inference.Tabula achieves significant (> 280) communication reduction compared to garbled circuits. Note ourimplementation of garbled circuits on 32-bit inputs achieves the same communication cost as reported by(Mishra et al., 2020a) (2KB communication for 32-bit integers).",
  ": Tabula with 8-bit activations vs garbled circuits communication cost for one ReLU": "We additionally compare ReLU communication of our protocol against recent works like CrypTflow2 (Ratheeet al., 2020) and Cheetah (Huang et al., 2022). CrypTflow2 and Cheetah similarly utilize a tree-based securecomparison protocol dependent on oblivious transfer (Rathee et al., 2020; Huang et al., 2022). Howeverunlike CrypTflow2, Cheetah swaps out the underlying oblivious transfer implementation for a more efficientversion Huang et al. (2022). Our following analysis assumes that CrypTFlow2 uses a more efficient OTprotocol based on preprocessing which reduces the online communication costs beyond what they present intheir paper; broadly, the tree-based comparison method that CrypTflow2 utilizes requires at least 6 callsto 1-out-of-128 oblivious transfer for optimal communication complexity (Rathee et al., 2020), which, withpreprocessing, takes at least 6 128 = 768 bits or 96 bytes, as oblivious transfer with preprocessing requiressending all n bits to the original sender at the end (Beaver, 1995; Naor & Pinkas, 1999). Tabula requires just16 bits of communication regardless of the nonlinear function being computed, obtaining a 48 improvementin communication over the tree based comparison method of CrypTflow2 / Cheetah assuming the use ofthis preprocessing-based OT method. Cheetahs approach on the other hand uses the same tree-basedcomparison approach (Huang et al., 2022) but swaps out the underlying OT method for a more efficientversion; specifically, Cheetahs communication cost is 11 L where L is the bitlength of the field element,which results in 88 bits of communication for 8-bit values and 352 bits for 32-bit values. Tabula requires just16 bits of communication, which represents a 5.5 and 22 reduction respectively. SiRNN (Rathee et al.,2021), another paper which utilizes an OT based protocol, uses more communication than that of ReLU ofCrypTflow (Rathee et al., 2020), hence our method would see > 48 communication improvement whencompared to their approach. We also compare communication cost against FSS approaches (Boyle et al., 2019;Gupta et al., 2022; Agarwal et al., 2022; Ryffel et al., 2021). Generally, for the ReLU op Tabula obtains the",
  "B96B44B11B2B48 / 22 / 5.5 / 1": ": Tabula (8-bit activations) vs CrypTFlow2 (Rathee et al., 2020) and Cheetah (Huang et al., 2022)and FSS (Boyle et al., 2019; 2020) online communication cost for performing a single ReLU operation duringthe online phase. For CryptFlow2 the communication is based on an OT method that uses preprocessingwhich achieves better online communication cost than what is described in Rathee et al. (2020). Note FSS,Cheetah, CrypTFlow2 costs are specific to the ReLU op, while Tabula communication cost is the same forany function provided they are quantized down to a sufficiently small table size. Total Online Communication ReductionWe benchmark the total amount of online communication required during the online phase of a single privateinference for various network architectures including LeNet, Resnet-32, ResNet-34 and VGG (batch size 1). shows the number of ReLUs per network, as well as the communication costs of using garbled circuits(for 32/16/8 bit inputs) vs Tabula. Tabula reduces communication significantly (> 20, > 10, > 5 vs32,16,8 bit garbled circuits) across various network architectures.",
  ": Tabula vs garbled circuits total online communication cost during secure inference for differentnetwork architectures": "We additionally compare end-to-end communication costs against Rathee et al. (2020), the current state-of-the-art for neural network inference, on various networks Minionn and ResNet34(Liu et al., 2017), shown in. Tabulas compact tables enable much lower communication costs during the online phase of secureneural network inference, leading to an order of magnitude reduction in communication costs. Finally, shows the communication reduction Tabula achieves compared to garbled circuits withAn-bit quantized inputs at a fixed accuracy threshold, and shows Tabula achieves over 89 communicationreduction across networks to maintain close to full precision accuracy. These values reflect total online",
  "ResNet-341.47M59.5 MB590MB9.9": ": Tabula vs CrypTFlow2 (Rathee et al., 2020) end-to-end communication cost for performing secureneural network inference on selected networks (Minionn (Liu et al., 2017) CIFAR10 architecture, and ResNet34CIFAR100). communication costs, not just ReLU communication costs, and hence we find we are primarily bottlenecked bythe communication for the linear layers rather than nonlinear layers. Also, we do not make any architecturalchanges to the neural network (e.g: replace any ReLU operations with quadratic operations, retrain, etc).",
  "(d) CIFAR100 ResNet34": ": Runtime breakdown across linear and nonlinear (ReLU) layers comparing Tabula with 8-bit inputsand garbled circuits with 32,16,and 8-bit inputs. Tabula achieves significant performance gains on nonlinearlayers, leading to major runtime speedups. We compare runtime and communication costs for initializing a single ReLU operation. shows thecost of preprocessing a single ReLU operation for Tabula with 8-bit inputs, and garbled circuits. In termsof communication costs, Tabula is comparable to GC with 32-bit inputs; however, Tabula requires morecommunication than GC with 16/8 bit inputs. In terms of runtime, Tabula generally requires significantlymore computation than garbled circuits, leading to higher runtime. The majority of Tabula preprocessingruntime is spent towards computing field operations for performing the multiply-add-accumulate operationbetween the outer product and the nonlinear function (recall that computation costs for an 8-bit input scalesas O(2563)). These computation costs can be significantly decreased through further parallelization andvectorization.",
  "Storage Costs": "We compare the storage savings Tabula achieves against garbled circuits. Recall that Tabula requiresstoring a single lookup table for each nonlinear activation call. This storage cost grows exponentially withthe size of its tables, which dictates the precision of the activations. Using less storage means reducing theprecision for the activations of the neural network and introduces some amount of error into the nonlinearfunction. This creates a tradeoff between storage and network accuracy. Similar to garbled circuits, Tabulatables must be stored on both client and server, and likewise, storage costs are equivalent for both client andserver; hence, in our results we show the storage cost for a single party. Below we show both the storagesavings for a single ReLU operation disregarding the accuracy impact from the quantization, and additionallythe storage vs accuracy tradeoffs for various networks (LeNet, ResNet32/34, VGG). Storage costs directlytranslate to memory usage costs at inference time since the lookup tables or garbled circuits must be loadedinto memory to be used to evaluate the nonlinear functions. ReLU Storage Savings vs PrecisionWe compare the storage use between Tabula and garbled circuits for a single ReLU operation. Tabulasstorage use is the the size of its table multiplied by the number of bits of elements in the original field, whichwe default to 64-bit numbers. Garbled circuits, on the other hand, uses 17KB, 8.5KB, and 4.25KB for each32-bit, 16-bit, and 8-bit ReLU operation respectively (Mishra et al., 2020a). presents the storage usage of both Tabula and garbled circuits for a single ReLU operation, andshows that Tabula achieves comparable storage use to garbled circuits at precisions 8-10, and lower storage",
  ": Tabula and garbled circuits storageuse for a single ReLU operation": "Storage Savings and Accuracy TradeoffWe present Tabulas total storage usage versus accu-racy tradeoff in . In this experiment, we directlyquantize the networks activations during execution timeuniformly across layers, recording the achieved accuracyand memory/storage requirements for a single inference.As shown in , across various tasks and networkarchitectures, activations may be quantized to 9 bits or be-low while maintaining within 1-3% accuracy. This allowsTabula to achieve comparable or even less storage usethan garbled circuits at a fixed accuracy threshold. We em-phasize that future work may apply more advanced quan-tization techniques (Ni et al., 2020; de Bruin et al., 2020)to reduce activation precision below 8-bits and achieveeven better storage savings. Our results here show thateven with very basic quantization techniques, Tabula achieves comparable storage usage versus garbledcircuits, and indicate that Tabula is more storage efficient as fewer bits of precision for the activations areused. Storage (KB) 0.976 0.978 0.980 0.982 0.984 0.986 0.988 0.990",
  "Runtime Speedup": "We compare the runtime speedup Tabula achieves over garbled circuits. As noted in various secure neuralnetwork inference works (Mishra et al., 2020a; Ghodsi et al., 2021; Cho et al., 2021), executing nonlinearactivation functions via garbled circuits takes up the majority of secure neural network execution time, hence,replacing garbled circuits with an efficient alternative has a major impact on runtime. Below we present theTabulas runtime benefits when executing individual ReLU operations and when executing relatively largestate-of-the-art neural networks.",
  ": Tabula runtime speedup vs garbled circuits on a single ReLU operation. Tabula is orders ofmagnitude faster than garbled circuits": "operation. Tabula achieves over 100 runtime speedup due to its simplicity: the cost of transferring 16 bytesof data and a single access to RAM is orders of magnitude faster than garbled circuits. Our implementation ofgarbled circuits on 32-bit inputs is slower than reported in Delphi (Mishra et al., 2020a). Our implementationof garbled circuits takes around 184 us per ReLU, whereas the reported is 84 us (Mishra et al., 2020a).However, even if the implementation in Delphi achieves an optimal 4 speedup with 8-bit quantization,Tabula is still 38 faster. Neural Network Runtime SpeedupWe present Tabulas overall speedup gains over garbled circuits across various neural networks includingLeNet, ResNet32/34 and VGG16. and shows that Tabula reduces runtime by up to 50across different neural networks, bringing execution time below 1 second per inference for the majority of thenetworks. Bigger networks are increasingly bottlenecked by ReLU operations, and hence Tabulas runtimereduction increases in magnitude with the size of the neural network under consideration. showsa breakdown of where execution time is being spent, for both Tabula and garbled circuits. As shown,Tabula reduces the runtime spent on computing activation functions by up to orders of magnitudes. Withbigger networks, the impact of executing nonlinear activation functions is larger. Hence, Tabula sees greaterruntime improvement on larger networks. Additionally, in the runtime breakdown chart, the linear layersfor Tabula were considerably slower than the linear layers when using garbled circuits we believe thatcache effects caused this difference in performance, as Tabula keeps all tables in memory, which may haveoverflowed to swap memory. Despite the slowdown in the linear layers, this has negligible impact on runtimedue to non-linear layers being the dominant cost, and Tabula sees considerably performance gains by beingfaster on the nonlinear layers. Runtime (s) 0.9898 0.9899 0.9900 0.9901 0.9902 0.9903 0.9904 0.9905",
  "Communication (b)163841792089604480": ": Tabula vs Garbled Circuits runtime and communication preprocessing costs for a single ReLUoperation. Note: Tabula preprocessing costs, like runtime costs, stay constant regardless of activation function,unlike garbled circuits. We further show the effect of number of bits used for the activation function on preprocessing communicationcosts. As each bit that is eliminated reduces the size of the table by a factor of 2, saving a single bitexponentially decreases runtime and communication costs. As seen in , at around 5 bits Tabulapreprocessing communication costs become lower than communication cost for garbled circuit at the samebitwidth.",
  ": Tabula preprocessing com-munication cost vs Garbled Circuitsfor different number of bits": "We additionally compare end-to-end preprocessing communicationcosts across various models (LetNet, ResNet, VGG) between Tabulaand Garbled Circuits. shows a comparison of the communi-cation costs between different models. Again, Tabula requires morecommunication than GC with 8/16-bit inputs but less than GC with32-bit inputs, due to the need for computing outer products usingBeaver triples that scale with the cardinality of the field. Althoughthis is costly, results show that preprocessing can be feasibly per-formed at similar cost to GC with 32-bit inputs. Further researchand algorithmic developments may drive down the preprocessing costof initializing Tabula tables.",
  "Conclusion": "Tabula is a secure and efficient protocol for computing nonlinearactivation functions in secure neural network inference. Our approachobtains considerable computational benefits over garbled circuits and other approaches to securely computingnonlinear functions. To conclude, we point out the following observation: quantization, as applied to improvestandard neural network performance, typically obtains sublinear runtime improvements (as low bitwidthops typically do not scale linearly in perf. with bits due to hardware inefficiencies), and linear memoryimprovements. Through our method, quantization as applied to secure neural network inference, obtainssuper-linear runtime/communication improvements that scale with the complexity of the underlying nonlinearoperation, and exponential memory improvements. We believe that, quantization, an already importantperformance improvement technique for neural networks, will be even more crucial for secure neural networkinference, and that our method Tabula is a key approach towards realizing this fact. Additionally, Tabulawill see improvement to both the online and offline phases with further advancements to neural networkquantization. Tabula is a step towards sustained, low latency, low energy, low bandwidth real time secureinference applications.",
  "David Heath, Vladimir Kolesnikov, and Lucien K. L. Ng. Garbled circuit lookup tables with logarithmicnumber of ciphertexts. Cryptology ePrint Archive, Paper 2024/369, 2024. URL": "Tuan Hoang, Thanh-Toan Do, Tam V. Nguyen, and Ngai-Man Cheung. Direct quantization for traininghighly accurate low bit-width deep neural networks. In Proceedings of the Twenty-Ninth International JointConference on Artificial Intelligence, IJCAI-20. International Joint Conferences on Artificial IntelligenceOrganization, 2020. URL Zhicong Huang, Wen jie Lu, Cheng Hong, and Jiansheng Ding.Cheetah: Lean and fast secure Two-Party deep neural network inference.In 31st USENIX Security Symposium (USENIX Security 22),pp. 809826, Boston, MA, August 2022. USENIX Association. ISBN 978-1-939133-31-1. URL",
  "Hoi-Kwong Lo. Insecurity of quantum secure computations. Phys. Rev. A, 1997": "Jeffrey L. McKinstry, Steven K. Esser, Rathinakumar Appuswamy, Deepika Bablani, John V. Arthur, Izzet B.Yildiz, and Dharmendra S. Modha. Discovering low-precision networks close to full-precision networks forefficient embedded inference, 2019. Pratyush Mishra, Ryan Lehmkuhl, Akshayaram Srinivasan, Wenting Zheng, and Raluca Ada Popa. Delphi:A cryptographic inference service for neural networks. In 29th USENIX Security Symposium (USENIXSecurity 20), 2020a.",
  "AAppendix": "Polynomial approximation vs quantizationAmple research has been dedicated towards exploring how to make polynomial approximations more amenableto neural networks (as enabling polynomial activations eliminates system bottlenecks imposed by nonlinearfunctions). However a significant body of evidence demonstrates that polynomial activations face remarkablebarriers to achieving high accuracy, especially for deep networks; on the other hand, research indicates thatlarge and deep neural networks (including networks like VGG, ResNet, LSTMs and transformers on taskslike Cifar100 and ImageNet) may be quantized to 8 bits and below (oftentimes to 4 or even 2 or 1 bit) withlittle loss of accuracy. Hence, rather than use polynomial activations, our work suggests that quantizationis the more preferred approach. This is a significant observation driving our approach. Below, we show acomparison of the accuracy performance of quantization vs polynomial activation.",
  ": Comparison of polynomial activations vs quantization on ResNet18 Cifar100": "Tables 11 and 12 compares the final test accuracy achieved by polynomial activations vs quantization forResNet32 Cifar10 and ResNet18 Cifar100 respectively. These results were obtained from Garimella et al.(2021); Choi et al. (2019); Hoang et al. (2020). As seen, polynomial approximations significantly harmaccuracy, while activation quantization, even at very low precision (2-bit / 3-bit) results in near losslessaccuracy. This phenomenon extends to large datasets such as ImageNet where 4-bit ResNet50 achieves losslessperformance Abdolrashidi et al. (2021), whereas polynomial activations incur significant accuracy loss on tinyimagenet Garimella et al. (2021). In many of these quantization works, accuracy performance includes thequantization of the weights as well as the activations; as our approach requires only activation quantization,it may be inferred that even better accuracies may be attained than what these numbers indicate. In the experiments shown in the main text, we do not do quantized retraining as in the results immediatelyabove (and instead directly apply quantization to pretrained weights; this is known as post-training quantiza-tion), hence, there is room for accuracy improvement over what was demonstrated in the main text. Thisreaffirms the potential of our approach in achieving efficient and accurate secure neural network inference. Note on truncation errorsAll accuracies reported in our paper are obtained by running the protocols in full and account for truncationerror resulting from the secure truncation protocol. Notably, in our implementation, we maintain a singleseparate static scale parameter per layer that is known to both client and server (leaking negligible modelinformation), ensuring that the underlying integer values of secretly shared activations are maintained between (this is a common technique when performing quantized inference with limited bitwidth datatypes foracceleration on hardware). As P(catastrophic truncation error) is proportional to the chance that the secretblinding factor is less than the secret value, the probability of catastrophic error for one truncation is 21464.This means, there is a 99.98% chance that all 1,000,000 calls to a network with 300,000 ReLUs succeeds.With just 80 bits, P(catastrophic error) is 21480, leading to a 99.99959% prob. that 1,000,000,000 calls to a300,000 ReLU network all succeed. The common off by one errors, like quantization error, has negligibleimpact on model quality (a similar finding by Huang et al. (2022)). This is a key detail and difference fromprior works (which do not maintain a separate scale, significantly inflating catastrophic truncation errorprobability). Detecting truncation error and retrying them is a topic for future work."
}