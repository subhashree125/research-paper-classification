{
  "Abstract": "Machine learning tasks are generally formulated as optimization problems, where onesearches for an optimal function within a certain functional space. In practice, parame-terized functional spaces are considered, in order to be able to perform gradient descent.Typically, a neural network architecture is chosen and fixed, and its parameters (connectionweights) are optimized, yielding an architecture-dependent result. This way of proceedinghowever forces the evolution of the function during training to lie within the realm of whatis expressible with the chosen architecture, and prevents any optimization across architec-tures. Costly architectural hyper-parameter optimization is often performed to compensatefor this. Instead, we propose to adapt the architecture on the fly during training. We showthat the information about desirable architectural changes, due to expressivity bottleneckswhen attempting to follow the functional gradient, can be extracted from backpropagation.To do this, we propose a mathematical definition of expressivity bottlenecks, which enablesus to detect, quantify and solve them while training, by adding suitable neurons. Thus,while the standard approach requires large networks, in terms of number of neurons perlayer, for expressivity and optimization reasons, we provide tools and properties to developan architecture starting with a very small number of neurons. As a proof of concept, we showresults on the CIFAR dataset, matching large neural network accuracy, with competitivetraining time, while removing the need for standard architectural hyper-parameter search.",
  "Introduction": "Issues with the fixed-architecture paradigm.Universal approximation theorems such as (Horniket al., 1989; Cybenko, 1989) are historically among the first theoretical results obtained on neural networks,stating the family of neural networks with arbitrary width as a good candidate for a parameterized space offunctions to be used in machine learning. However the current common practice in neural network trainingconsists in choosing a fixed architecture, and training it, without any possible architecture modificationmeanwhile. This inconveniently prevents the direct application of these universal approximation theorems,as expressivity bottlenecks that might arise in a given layer during training will not be able to be fixed. Thereare two approaches to circumvent this in daily practice. Either one chooses a (very) large width, to be sure toavoid expressivity and optimization issues (Hanin & Rolnick, 2019b; Raghu et al., 2017), to the cost of extracomputational power consumption for training and applying such big models; to mitigate this cost, modelreduction techniques are often used afterwards, using pruning, tensor factorization, quantization (Louizoset al., 2017) or distillation (Hinton et al., 2015). Or one tries different architectures and keeps the mostsuitable one (in terms of performance-size compromise for instance), which multiplies the computationalburden by the number of trials. This latter approach relates to the Auto-DeepLearning field (Liu et al.,2020), where different exploration strategies over the space of architecture hyper-parameters (among otherones) have been tested, including reinforcement learning (Baker et al., 2017; Zoph & Le, 2016), Bayesianoptimization techniques (Mendoza et al., 2016), and evolutionary approaches (Miller et al., 1989; Stanleyet al., 2009; Miikkulainen et al., 2017; Bennet et al., 2021), that all rely on random tries and consequently",
  "Published in Transactions on Machine Learning Research (June/2024)": "V. N. Vapnik and A. Ya. Chervonenkis. On the uniform convergence of relative frequencies of events to theirprobabilities. Theory of Probability and its Applications, 16(2):264280, 1971. doi: 10.1137/1116025. URL P. Virtanen, R. Gommers, T.E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Pe-terson, W. Weckesser, J. Bright, S.J. van der Walt, M. Brett, J. Wilson, J.K. Millman, N. Mayorov,A.R.J. Nelson, E. Jones, R. Kern, E. Larson, C.J. Carey, I. Polat, Y. Feng, E.W. Moore, J. VanderPlas,D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E.A. Quintero, C.R. Harris, A.M. Archibald, A.H.Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithmsfor Scientific Computing in Python. Nature Methods, 17:261272, 2020.",
  "is the sample distribution, from which the dataset {(x1, y1), ..., (xN, yN)} is sampled, with xi Rp andyi Rd": "For the sake of simplicity we consider a feedforward neural network f : Rp Rd with L hidden layers,each of which consisting of an affine layer with weights Wl followed by a differentiable activation functionl which satisfies l(0) = 0. The network parameters are then := (Wl)l=1...L. The network iterativelycomputes:",
  "Approach": "Functional gradient descent.We take a functional perspective on the use of neural networks. Ideallyin a machine learning task, one would search for a function f : Rp Rd that minimizes the loss L bygradient descent: f t = fL(f) for some metric on the functional space F (typically, L2(Rp Rd)), wheref denotes the functional gradient and t denotes the evolution time of the gradient descent. The descentdirection vgoal := fL(f) is a function of the same type as f and whose value at x is easily computableas vgoal(x) = (fL(f)) (x) = u(u, y(x))u=f(x) (see Appendix A.1 for more details). This direction",
  "s.t. A": "This linear functional1 space is a first-order approxima-tion of the neighborhood of f within FA. The directionvGD obtained above by gradient descent is actually notthe best one to consider within TA. Indeed, the best movev would be the orthogonal projection of the desired di-rection vgoal := fL(f) onto TA. This projection iswhat a (generalization of the notion of) natural gradientwould compute (Ollivier, 2017).",
  "=: TA L(f)": "Lack of expressivity.When vgoal does not belong to the reachable subspace TA, there is a lack ofexpressivity, that is, the parametric space A is not rich enough to follow the ideal functional gradientdescent.This happens frequently with small neural networks (see Appendix A.4 for an example).Theexpressivity bottleneck is then quantified as the distance v vgoal between the functional gradient vgoaland the optimal functional move v given the architecture A (in the sense of Eq. 3).",
  "Generalizing to all layers": "Ideal updates.The same reasoning can be applied to the pre-activations al at each layer l, seen asfunctions al : x Rp al(x) Rdl defined over the input space of the neural network. The optimalparameter update for a given layer l then follows the projection of the desired update alL(f) of thepre-activation functions al onto the linear subspace T alAof pre-activation variations that are possible withthe architecture, as we will detail now. Given a sample (x, y)D,standard backpropagation already iteratively computes vlgoal(x):= (alL(f)) (x) = u (L(WL L1(WL1 ... l(u))), y)|u=al(x), which is the derivative of the loss(f(x), y) with respect to the pre-activations u = al(x) of each layer. This is usually performed in orderto compute the gradients w.r.t. model parameters Wl, as Wl(f(x), y) =al(x)",
  "Wlal(f(x), y)": "vlgoal(x) := (alL(f)) (x) indicates the direction in which one would like to change the layer pre-activations al(x) in order to decrease the loss at point x.However, given a minibatch of points (xi),most of the time no parameter move is able to induce this progression for each xi simultaneously, becausethe -parameterized family of functions al is not expressive enough. Activity update resulting from a parameter change.Given a subset of parameters (such as the onesspecific to a layer: = Wl), and an incremental direction to update these parameters (e.g. the one resultingfrom a gradient descent: =",
  "Tr(4)": "where ||.|| stands for the L2 norm, ||.||Tr for the Frobenius norm, and V l(X, ) is the activity updateresulting from parameter change as defined in previous section. In the two following parts we fix theminibatch X and simplify notations accordingly by removing the dependency on X. Proofs of this sectionare deferred to Appendix C.",
  "Best move without modifying the architecture of the network": "Let W lbe the solution ofEquation (4) when the parameter variation is restricted to involve onlylayer l parameters, i.e. Wl.This move is sub-optimal in that it does not result from an update of allarchitecture parameters but only of the current layer ones. In this case the activity update simplifies asV l(Wl) = Wl Bl1 and the problem becomes:",
  "of layer l 1, that is to say onto the span of all possible post-activation directions, through the projector1nBTl1 1": "nBl1BTl1+ Bl1. To increase expressivity if needed, we will aim at increasing this span with themost useful directions to close the gap between this best update and the desired one. Note that the updateW l consists of a standard gradient (V lgoalBTl1) followed by a change of metric in the pre-activation space",
  "Reducing expressivity bottleneck by modifying the architecture": "To get as close as possible to V lgoal and to increase the expressive power of the current neural network,we modify each layer of its structure.At layer l 1, we add K neurons n1, ..., nK with input weights1, ..., k and output weights 1, ..., K (cf. ). We have the following expansions by concatenation:W Tl1 W Tl11...Kand Wl Wl1...K. We note this architecture modification K where is the concatenation sign and K := (k, k)Kk=1 are the K added neurons. The added neurons could be chosen randomly, as in usual neural network initialization, but this would notyield any guarantee regarding the impact on the system loss. Another possibility would be to set either inputweights (k)Kk=1 or output weights (k)Kk=1 to 0, so that the function f(.) would not be modified, while itsgradient w.r.t. would be enriched from the new parameters. Another option is to solve an optimizationproblem as in the previous section with the modified structure K and jointly search for both theoptimal new parameters K and the optimal variation W of the old ones:",
  "where l is the expressivity bottleneck (defined in Eq. (4)). For convolutional layers instead of fully-connectedones, Equation (9) becomes an inequality ()": "In practice before adding new neurons (, ), we multiply them by an amplitude factor found by a simpleline search (see Appendix E.3), i.e. we add (, ). The addition of each neuron k has an impact onthe bottleneck of the order of 2k provided is small. We observe the same phenomenon with the loss asstated in the next proposition :Proposition 3.3. For > 0, solving (8) using Vgoalproj = VgoalV (W ) is equivalent to minimizing theloss L at order one in V l. Furthermore, performing an architecture update with W (5) and a neuronaddition with K,(3.2) has an impact on the loss at first order in as :",
  "Tr 0 .(12)": "The k could be used in a selection criterion realizing a trade-off with computational complexity. A selec-tion based on statistical significance of singular values can also be performed. The full algorithm and itscomplexity are detailed in Appendices E.4 and E.5. We finish this section by some additional propositionsand remarks.Proposition 3.4. If S is positive definite, then solving (8) is equivalent to taking k = Nk and findingthe K first eigenvectors k associated to the K largest eigenvalues of the generalized eigenvalue problem:NN T k = Sk . Corollary 1. For all integers m, m such that m + m R, at order one in V , adding m + m neuronssimultaneously according to the previous method is equivalent to adding m neurons then m neurons byapplying successively the previous method twice. One should also note that the family {V l+1((k, k))}Kk=1 of pre-activity variations induced by adding theneurons K,is orthogonal for the trace scalar product. We could say that the added neurons are orthogonalto each other (and to the already-present ones) in that sense. Interestingly, the GradMax method (Evciet al., 2022) also aims at minimizing the loss 10, but without avoiding redundancy (see Appendix B.1 formore details).",
  "About greedy growth sufficiency and TINY convergence": "One might wonder whether a greedy approach on layer growth might get stuck in a non-optimal state. Bygreedy we mean that every neuron added has to decrease the loss. Since in this work we add neurons layerper layer independently, we study here the case of a single hidden layer network, to spot potential layergrowth issues. For the sake of simplicity, we consider the task of least square regression towards an explicitcontinuous target f , defined on a compact set. That is, we aim at minimizing the loss:",
  "First, if one allows only adding neurons but no modification of already existing ones:": "Proposition 4.1 (Exponential convergence to 0 training error by ReLU neuron additions). It is possible todecrease the loss exponentially fast with the number t of added neurons, i.e. as tL(f), towards 0 trainingloss, and this in a greedy way, that is, such that each added neuron decreases the loss. The factor is",
  "In particular, there exists no situation where one would need to add many neurons simultaneously to decreasethe loss: it is always feasible with a single neuron": "TINY might get stuck when no correlation between inputs xi and desired output variations f (xi) f(xi)can be found anymore. To prevent this, one can choose an auxiliary method to add neurons in such cases,for instance random neurons (with a line search over their amplitude, cf. Appendix D.3), or locally-optimalneurons found by gradient descent, or solutions of higher-order expressivity bottleneck formulations usingfurther developments of the activation function. We will name completed-TINY the completion of TINY byany such auxiliary method.",
  "Now, if we also update already existing weights when adding new neurons, we get a stronger result:": "Proposition 4.2 (Completed-TINY reaches 0 training error in at most n neuron additions). Under certainassumptions (full batch optimization, updating already existing parameters, and, more technically: polynomialactivation function of order n2), completed-TINY reaches 0 training error in at most n neuron additionsalmost surely. Hence we see the importance of updating existing parameters on the convergence speed. This optimizationprotocol is actually the one we follow in practice when training neural networks with TINY (except whencomparing with other methods using their protocol). Note that our approach shares similarity with gradient boosting Friedman (2001) somehow, as we grow thearchitecture based on the gradient of the loss. Note also that finding the optimal neuron to add is actuallyNP-hard (Bach, 2017), but that we do not need new neuron optimality to converge to 0 training error.",
  "Comparison with GradMax on CIFAR-100": "The closest growing method to TINY is GradMax (Evci et al. (2022)), as it solves a quadratic problem similarto (8). By construction, the objective of GradMax is to decrease the loss as fast as possible considering aninfinitesimal increment of new neurons. The main difference is that GradMax does not take into accountthe expressivity of the current architecture as TINY does in (8) by projecting vgoal. In-depth details aboutthe difference between the GradMax and TINY are provided in Appendix B.1. In this section, we show on the CIFAR-100 dataset that solving (8) instead of (23) (defined by GradMax)to grow a network using a naive strategy allows better final performance and almost full expressive power.",
  "Accuracy": "TINY traintest epochs 0.6 0.7 0.8 0.9 1.0 GradMax Starting architecture : 1/64 t = 1 : Evolution of accuracy as a function of epochs for the setting t = 1, s = 1/64 during extra trainingfor TINY and GradMax. Mean and standard deviations are estimated over four runs. Results with othersettings can be found in Figures 14 and 15 in the appendix.",
  "/6469.5 0.2 568.7 0.6 557.0 0.4 1058.4 0.2 10": ": Final accuracy on test set of ResNet18 after architecture growth (grey) and after convergence (blue).The number of stars indicates the multiple of 50 epochs needed to achieve convergence. With the startingarchitecture ResNet1/64 and t = 0.25, the method TINY achieves 65.80.1 on test set after its growth andit reaches 69.50.2 5after 5 := 550 additional epochs (examples of training curves for the extra trainingcan be found in ). Means and standard deviations are performed on 4 runs for each setting. tecture far from full expressivity, i.e. ResNet1/64, while TINY is able to handle it. As for the setting s = 1/4,both methods seem equivalent in terms of final performance and achieve full expressivity. The curves on , which are extracted from in the appendix, show that TINY models haveconverged at the end of the growing process, while GradMax ones have not. This latter effect contrastswith GradMax formulation which is to accelerate the gradient descent as fast as possible by adding neurons.Furthermore GradMax needs extra training to achieve full expressivity: for the particular setting s =1/64, t = 1, the extra training time required by GradMax is twice as high as TINYs, as shown in .This need for extra training also appears for all settings in . In particular for s = 1/64, t = 0.25,the difference in performance after and before extra training goes up to 20 % above the initial performancewhile it is only of 6% for TINY.",
  "Comparison with Random on CIFAR-100 : initialisation impact": "In this section, we focus on the impact of the new neurons initialization. To do so, we consider as a baselinethe Random method, which initializes the new neurons according to a Gaussian distribution: (k, k)Kk=1 N(0, Id) or a uniform distribution U. Also, when adding new neurons, we now search for the bestscaling using a line-search on the loss. Thus, we perform the operation K K, with the amplitudefactor R defined as :",
  "i(fK(xi), yi)with K = (k, k)Kk=1(14)": "with L a positive constant. More details can be found in Appendix E.3.2 and in Algorithm 1. With suchan amplitude factor, one can measure the quality of the directions generated by TINY and Random byquantifying the maximal decrease of loss in these directions. To better measure the impact of the initialisation method, and to distinguish it from the optimization process,we do not perform any gradient descent. This contrasts with the previous section where long training timeafter architecture growth was modifying the direction of the added neurons, dampening initialization impactwith training time, especially as they were added with a small amplitude factor (cf Section E.3.1). With these two modifications to the protocol of previous section, we obtain . We see the crucialimpact of TINY initialization compared to the Random one. Indeed, TINY reaches more than 17% accuracyjust by adding neurons (without any further update), which accounts for about one quarter of the totalaccuracy with the full method (69% in using in plus gradient descent).On the opposite, theRandom initialization does not contribute to the accuracy through the growing process (just about 1%); thiscan be explained and quantified as follows. To study the random setting, we can model v(X) and vgoal(X)as independent variables where vgoal N0d, 1",
  "for the first layer and1": "512 for the last layer when compared to the true gradient,and consequently when compared to TINY. Furthermore, one can take into account the effect of a line searchover the random direction: in that case the expected relative loss gain is quadratic in the angle between thedirections and therefore of the order of magnitude of164 or1",
  "respectively (see Appendix D.3)": "Note that the search interval of Equation (14) can be shrunk to [0, L] with TINY initialization, as the firstorder development of the loss in Equation (10) is positive. This property is the direct consequence of thedefinition of V as the minimizer of the expressivity bottleneck (Eq. (8)). One can also note that we donot include GradMax in , because its protocol initializes the on-going weights to zero (k 0) andimposes a small norm on its out-going weights (||k|| = ). Those two aspects make the amplitude factor",
  "Conclusion": "We provided the theoretical principles of TINY, a method to grow the architecture of a neural net whiletraining it; starting from a very thin architecture, TINY adds neurons where needed and yields a fullytrained architecture at the end. Our method relies on the functional gradient to find new directions thattackle the expressivity bottleneck, even for small networks, by expanding their space of parameters. Thisway, we combine in the same framework gradient descent and neural architecture search, that is, optimizingthe network parameters and its architecture at the same time, and this, in a way that guarantees convergenceto 0 training error, thus escaping expressivity bottlenecks indeed. While transfer learning works well on ordinary tasks, for it to succeed, it needs to fine-tune and use largearchitectures at deployment in order to extract and manipulate common knowledge. Our method has theadvantage of being generic and could also produce smaller models as it adapts the architecture to a singletask.",
  "The authors address their deepest thanks to Stella Douka, Andrei Pantea, Stphane Rivaud & FranoisLandes for the exchanges and discussions on this project": "This work was supported by grants ANR-22-CE33-0015-01 and ANR-17-CONV-0003 operated by LISN toSylvain Chevallier and by ANR-20-CE23-0025 operated by Inria to Guillaume Charpiat. This work was alsofunded by the European Union under GA no. 101135782 (MANOLO project). Numerical computation was enabled by the scientific Python ecosystem:Matplotlib Hunter (2007),Numpy Harris et al. (2020), Scipy Virtanen et al. (2020), pandas pandas development team (2020), Py-Torch Paszke et al. (2019).",
  "Peter Grnwald and Teemu Roos. Minimum description length revisited. International Journal of Math-ematics for Industry, 11(01):1930001, 2019. doi: 10.1142/S2661335219300018. URL": "Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In Kamalika Chaudhuriand Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,volume 97 of Proceedings of Machine Learning Research, pp. 25962604. PMLR, 0915 Jun 2019a. URL Boris Hanin and David Rolnick.Deep relu networks have surprisingly few activation patterns.InH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advancesin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019b.URL C.R. Harris, K.J. Millman, S.J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor,S. Berg, N.J. Smith, R. Kern, M. Picus, S. Hoyer, M.H. van Kerkwijk, M. Brett, A. Haldane, J. Fernndezdel Ro, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi,C. Gohlke, and T.E. Oliphant. Array programming with NumPy. Nature, 585(7825):357362, 2020.",
  "Kaitlin Maile, Emmanuel Rachelson, Herv Luga, and Dennis George Wilson. When, where, and how to addnew neurons to ANNs. In First Conference on Automated Machine Learning (Main Track), 2022. URL": "Hector Mendoza, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter.To-wards automatically-tuned neural networks. In Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren(eds.), Proceedings of the Workshop on Automatic Machine Learning, volume 64 of Proceedings of Ma-chine Learning Research, pp. 5865, New York, New York, USA, 24 Jun 2016. PMLR.URL Risto Miikkulainen, Jason Zhi Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink, Olivier Francon, BalaRaju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy, and Babak Hodjat.Evolving deep neuralnetworks. CoRR, abs/1703.00548, 2017. URL",
  "The pandas development team. pandas-dev/pandas: Pandas, February 2020. URL": "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,J. Bai, and S. Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. InAdvances in Neural Information Processing Systems (NeurIPS), pp. 80248035. Curran Associates, Inc.,2019.",
  "P. Wolinski, G. Charpiat, and O. Ollivier. Asymmetrical scaling layers for stable network pruning. OpenRe-view Archive, 2020": "Lemeng Wu, Dilin Wang, and Qiang Liu. Splitting steepest descent for growing neural architectures. InH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advancesin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. URL Lemeng Wu, Bo Liu, Peter Stone, and Qiang Liu.Firefly neural architecture descent:a gen-eral approach for growing neural networks.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Bal-can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2237322383. Curran Associates, Inc., 2020.URL Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and HartwigAdam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings ofthe European Conference on Computer Vision (ECCV), pp. 285300, 2018. Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.Understanding deeplearning requires rethinking generalization.In International Conference on Learning Representations,2017. URL"
}