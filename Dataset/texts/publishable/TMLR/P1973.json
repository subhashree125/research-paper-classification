{
  "Abstract": "Hallucination is a common problem for Large Vision-Language Models (LVLMs) with longgenerations which is difficult to eradicate. The generation with hallucinations is partiallyinconsistent with the image content. To mitigate hallucination, current studies either focus onthe process of model inference or the results of model generation, but the solutions they designsometimes do not deal appropriately with various types of queries and the hallucinationsof the generations about these queries. To accurately deal with various hallucinations, wepresent a unified framework, Dentist, for hallucination mitigation. The core step is tofirst classify the queries, then perform different processes of hallucination mitigation basedon the classification result, just like a dentist first observes the teeth and then makes aplan. In a simple deployment, Dentist can classify queries as perception or reasoning andeasily mitigate potential hallucinations in answers which has been demonstrated in ourexperiments. On MMbench, we achieve a 13.44%/10.2%/15.8% improvement in accuracy onImage Quality, a Coarse Perception visual question answering (VQA) task, over the baselineInstructBLIP/LLaVA/VisualGLM.",
  "Introduction": "Hallucination in Large Vision-Language Models (LVLMs) is a critical issue, which manifests as the modelsgenerated content partially deviating from the actual content of the image (Jing et al., 2023a). For example,when provided with a image and two questions as input, LVLMs inaccurately identify characters actionsand misinterpret relationships between characters, as illustrated in . Such inaccuracies can lead tomisinformation, potentially degrading the user experience and misleading individuals. This issue underscoresthe necessity for ongoing improvements to enhance the reliability and accuracy of LVLMs, mitigating the riskof hallucinations and their consequent misinformation. To tackle the above challenge, existing work either focuses on optimizing the training data and the parametersof the existing model (Liu et al., 2023b; Lu et al., 2023), or correcting the hallucinations during the generationstage without model update (Yin et al., 2023). The former collects high-quality training data, such as addingnegative instances to the training data to avoid overconfidence in the model (Liu et al., 2023a). The lattermainly utilize the generated object information from the vision foundation model (such as blip2 (Li et al.,2023a)) to detect hallucinations and eliminate them. For example, Woodpecker (Yin et al., 2023) extractsmain objects from the response generated by LVLMs and then verified these objects with object segmentationtool (Liu et al., 2023d) and VQA models (Li et al., 2023a). Similarly, HalluciDoctor (Yu et al., 2023a) makesthe description-oriented answer chunks extraction and formulates corresponding questions, uses answers forthese questions which are gathered from various LVLMs to do the consistency cross-checking and removehallucinations.",
  ": An example image of hallucination. The generation of the model is partially inconsistent with theimage, which we call perception hallucination and reasoning hallucination respectively": "There are two main hallucinations in the model response (Ji et al., 2023): perception hallucination andreasoning hallucination, as shown in . The former is manifested by incorrectly describing image contentin the model generation, such as errors when describing object attributes, while the latter refers to themodel producing fallacies in logical reasoning answers. Although the previous methods for hallucinationmitigation have achieved success, they still have a common problem, that is, when faced with these two typesof hallucinations, the fixed verification method may sometimes be inappropriate and ineffective. For example,for reasoning queries, it is not effective to use object detection on pictures to verify whether the object inthe answer exists, as shown in In addition, sometimes when the corrected answer obtained by theexisting method is used as input to perform the same correction once, the answer after the second correctionis inconsistent with the first time, which means that one correction did not remove all the hallucinations. In order to solve this problem, we propose a unified framework for two main kinds of hallucination mitigation.Whether it is a descriptive answer or a logical reasoning answer, our framework Dentist will try to correctthe parts of the answer that do not match the content of the picture. Specifically, the framework weproposed is a verification loop, and each loop is divided into two core steps: (1) Potential hallucinationclassification divides the query into two categories: perception and reasoning, which also classifies thepotential hallucinations in answers when these queries are used as input to LVLMs. (2) Divide-and-conquertreatment makes the mitigation based on the classification. The generation for the perception query willbe verified by the sub-questions, while the generation for the reasoning query will be verified with the helpof Chain-of-Thought (CoT). To ensure that hallucinations are mitigated as much as possible, the aboveverification loop will continue until the revised generation no longer changes semantically significantly or theloop limit is reached. We complete quantitative experiments on MMbench (Liu et al., 2023e), LLaVA-QA90 (Liu et al., 2023c),CHAIR (Rohrbach et al., 2018) and POPE (Li et al., 2023c) using three models: InstructBLIP (Dai et al.,2023), VisualGLM (Ding et al., 2021; Du et al., 2022) and LLaVA (Liu et al., 2023c), respectively, to test theeffectiveness of our proposed method. In addition, we also conduct a comparative experiment with a currenteffective LVLM hallucination mitigation method, Woodpecker (Yin et al., 2023). Our method demonstratesthe effectiveness and superiority in many visual language tasks, and promotes the performance of the baselinemodels. In particular, in the experiment, we achieve a 13.44%/10.2%/15.8% improvement in accuracy in thevisual language task of Image Quality, compared with the baseline models InstructBLIP/LLaVA/VisualGLM.",
  "Large Vision-Language Model": "Inspired by the success of Large Language Models (LLMs) (Wang et al., 2022; Zhao et al., 2023; Jing et al.,2024a; 2023b), the multimodal learning community shifted research attention to LVLMs. LVLMs mainlyuse the cross-modality aligner to connect the visual encoder (such as CLIP (Radford et al., 2021)) andLLMs (such as LLaMA (Touvron et al., 2023)) to tackle vision-language tasks. For example, LLaVA (Liuet al., 2023c) connects a vision encoder and a LLM for general-purpose visual and language understanding,suggesting practical tips for building a general-purpose visual agent. Meanwhile, InstructBLIP (Dai et al.,2023) introduce an instruction-aware Query Transformer which extracts visual features from the outputembeddings of the frozen image encoder, and feeds the visual features as soft prompt input to the frozenLLM. In addition, VisualGLM (Ding et al., 2021; Du et al., 2022) use Qformer (Li et al., 2023b) which buildsa bridge between the visual model and the language model. Though these LVLMs have powerful visuallanguage understanding ability on the generation task, sometimes their outputs still contain hallucinationsthat need to be corrected. Some studies (Wang et al., 2023b; Awal et al., 2023) use LLMs to improve theperformance of LVLMs, which is worth learning from.",
  "Hallucination": "With the progress of research on LVLMs, the problem of hallucination has gradually been exposed (Baiet al., 2024), and it has attracted more and more attention. Research around hallucination focuses on threeaspects: detecting (Gunjal et al., 2023; Luo et al., 2024), mitigating (Kang et al., 2023; Lu et al., 2023; Wanget al., 2023a; Yin et al., 2023; Yu et al., 2023a;b; Leng et al., 2023; Huang et al., 2023; Jing & Du, 2024),and evaluating hallucinations (Jing et al., 2023a; Wu et al., 2024; Jing et al., 2024b). In this paper, wemainly focus on hallucination mitigation. Previous works on hallucination mitigation can be divided into twocategories: model inference optimization and model generation optimization. The first category focuses onthe process of the training and inference of the LVLMs. RLHF-V (Yu et al., 2023b) collects human feedbackat the data level and learns the correctional human feedback at the training level to reduce hallucinations inmodel generations. Ever (Kang et al., 2023) points out that mitigating hallucinations in real time duringmodel inference is more appropriate than generating corrections from the model outputs, as the latter issubject to snowballing effects. VIGC (Wang et al., 2023a) uses an iterative method to concatenate the shortsentences generated each time, and ensures accuracy by controlling the length of the generation. On theother hand, the second category focuses on the aspect of the generation of LVLMs, designing methods toobtain hallucination information from the output of the model and do the mitigation. Leng et al. (2023)introduces Visual Contrastive Decoding (VCD) to counteract the statistical biases and mitigate hallucinationsby contrasting model outputs generated from original and distorted visual inputs. For example, Woodpecker(Yin et al., 2023) makes the question formulation and visual knowledge validation base on the keywords whichare extracted from the output of the model and uses an LLM to modify the hallucinations in the generatedresponses. Despite the success of the existing method, they overlook the diversity of hallucinations which results in afixed hallucination elimination method that cannot be applied to all hallucination situations well. To solvethis problem, we propose a unified framework for mitigating hallucinations, the core step of which is toclassify potential hallucinations caused by different queries.",
  "OriginalAnswer": ": An overview of the proposed method. The components using GPT are indicated in orange. Theicons of open and closed eyes indicate whether the component is a pure text task or is related to an image. Theblack line represents the original part of LVLM. The blue line represents the forward path of the verificationprocess, and the orange line represents the feedback path in the verification loop. The core point is tocustomize different methods of mitigating hallucinations by classifying the query. The reasoning sectionis used to mitigate the hallucinations caused by reasoning queries, while the perception section is used tomitigate the hallucinations caused by perception queries.",
  "Task Formulation": "Suppose we have a dataset D = {(Qi, Ii, Yi)}Ni=1, where Ii is the image, Qi refers to the query of the imageand Yi represents the truth answer to the corresponding query. Note i represents the index of samples in thedataset D. We omit the index of Qi, Ii and Yi in the following discussion for the sake of brevity. Thereafter,we feed the image and query sample to the LVLM as follows,",
  "Potential Hallucination Classification": "As we mentioned before, there are two main types of hallucination: perception hallucination and reasoninghallucination. These two types of hallucinations correspond to two types of queries: perception query andreasoning query (Liu et al., 2023e). The perception query feature mainly requires the model to have the abilityto perceive visual features, such as attribute recognition, scene description, etc. The hallucinations caused bythis type of queries can be effectively alleviated through visual level approaches, like object detection. The",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Results of InstructBLIP(Baseline and ours) across the 20 ability dimensions defined in MMBench.The blue area is the result of the baseline, and the red area is ours. See the legend. From this figure, we canintuitively see that our method can enhance the performance of baseline in terms of Image Impression, ImageQuality and Future Prediction, etc. For more comprehensive evaluation results on LLaVA and VisualGLM,please refer to and",
  "Divide-and-conquer Treatment": "After query classification, the LVLM responses of perception and reasoning queries need to be processeddifferently. This is because, as we mentioned before, different types of queries examine different capabilitiesof the LVLM, and the mitigation methods required for hallucinations in answers are also different. To deal with the perception query, we need to generate verification sub-questions based on the original queryand the original answer with hallucinations generated by the LVLM. The LVLM answers these sub-questionsto obtain sub-answers and finally we aggregate these answers to form the output with fewer hallucinations.For reasoning queries, a common phenomenon is that the LVLM only generates the results of logical reasoningand the logical reasoning process we want to see is omitted in the generated content. In response to thissituation, the method we propose is to use the CoT.",
  "Visual Verification for Perceptron": "LVLMs are prone to hallucinations when generating long descriptive texts (Liu et al., 2023b). This exactlycorresponds to the situation of the responses of perception queries. We are inspired that when the long answerto perception queries contains hallucinations, we can split the long answer into short sentences and designverification sub-questions based on the key points in the sentences. We formulate this process as follows:",
  "where Ps() is a prompt which can instruct ChatGPT to generate sub-questions qi according to the query Qand the original answer Y , and corresponding details can be found in Appendix A.4": "After generating verification sub-questions, we feed them to the LVLM along with the original image.Thereafter, we can get the verification sub-answers from LVLMs. It is worth mentioning that the LVLM weused for generating the sub-answers is just the original model which has hallucinations that need to be revised.It can probably be replaced with any visual question answering (VQA) model, but would be accompaniedby the suspicion of using a better model for better work. To demonstrate the ability of our approach tomitigate hallucinations rather than the ability of the rectified models, we chose to use the original LVLM. Weformulate this process as follows:yi = M(I, qi),(5)",
  "Chain of Thought for Reasoning": "The answers generated by the LVLM from reasoning queries are not as \"clear\" as the answers to perceptionquestions, which means the LVLMs answers tend to only contain the results of logical reasoning, but notthe process of logical reasoning and the basis for that (perception about visual details before reasoning).Therefore, the method of the perception section is no longer applicable because of the missing part aboutperception in the reasoning answer. To solve this problem, we use the CoT prompt method to obtain answersthat contain more details which are beneficial to our following process. At the same time, the LVLM will alsoimprove the accuracy of reasoning when performing CoT. Add \"Lets think step by step\" to the start of theoriginal query to do the CoT and use ChatGPT by prompt to obtain the revised answer as follows:",
  "Validation Loop": "After the above steps, we obtain the preliminary verified answer Y which may still contain hallucinationsthat have not been eliminated because of the imperfections of the verification sub-questions generation. Inorder to solve this problem, we propose to regard the entire verification framework as a repeated block in theverification loop chain. We show the overall procedure in Algorithm 1. The verified answer is treated as theoriginal answer and re-verified. The difficulty of loop verification is how to judge when the hallucinationsin the answer has been completely removed so the loop can be stopped. We believe that if and only if theverified answer does not change significantly semantically after a new round of verification, it means thatall the hallucinations that can be eliminated have been eliminated. On the other hand, if the answer stillchanges significantly after a specific number of rounds of verification, we believe that there is a snowballerror phenomenon in the verification cycle. We will stop the loop and use only the answer from the firstverification as the final revised answer. We use ChatGPT to determine whether the answer has convergedand is no longer changing semantically. This corresponds to the Similar function in Algorithm 1 and theprompt template is given in Appendix A.7.",
  "Experiment Settings": "Benchmarks. MMBench is a novel multi-modality benchmark, which develops a fine-grained abilityassessment for LVLMs. The MMBench evaluation standard is divided into three levels. The L-1 ability levelincorporates Perception and Reasoning, L-2 ability level consists of Coarse Perception, Fine-grained, etc. andL-3 ability level covers Image Style, Image Scene, Image Emotion, etc. Relying on such hierarchical andfine-grained capability assessment, the performance of LVLM can be comprehensively evaluated. The datasetwe use is MMBench-Test(EN). LLaVA-QA90 is also a dataset used to evaluate LVLMs. LLaVA-QA90 contains 90 questions and 30 imagestaken from COCO Val 2014 (Lin et al., 2014). To evaluate the generated response, we feed the query, image,and model response to GPT-4V (OpenAI, 2023) to get a score of a scale of 1 to 10. The prompt templateis available in Appendix A.9. We respectively pair baseline LVLMs with Dentist, baseline LVLMs withWoodpecker, and provided their responses to GPT-4V for scoring, which ensures that the scores are mutuallyreferenced, thereby making them more reliable. Caption Hallucination Assessment with Image Relevance (CHAIR) (Rohrbach et al., 2018) is awidely-used metric for evaluating object hallucination in image captioning tasks. By comparing the imagecaptions generated by the model with the ground truth objects in the corresponding image, CHAIR evaluatesthe degree of hallucination of the model and explains the performance of the model. CHAIR has twovariants: CHAIRs (Cs) and CHAIRi (Ci), both of which reflect the degree of hallucination of the model,the difference being that CHAIRs is at the sentence level and CHAIRi is at the object instance level. Thecalculation is as follows:",
  "|{all mentioned objects}|": "POPE (Li et al., 2023c) is also an evaluation method for object hallucination. Three kinds of samplingsettings of random, popular, adversarial, are constructed on the dataset according to human annotationor automatic visual segmentation tools. The difference between them lies in the negative sample samplingmethod. Where, the random setting randomly samples objects that do not exist in the image; the popularsetting samples objects that are not present in the current image, but are most common throughout thedataset; the adversarial setting samples objects in the dataset that co-appear most frequently with objectsin the current image, but are not present in the current image. POPE has a high degree of fairness androbustness, which helps us to better demonstrate the hallucination mitigation effect of Dentist. In terms of sampling settings, we sample 100 images and construct 6 questions for each type of samplingsetting for each image. Each question is a \"Yes or No\" question that transforms the model task into a simplerbinary classification task. In terms of evaluation metrics, we adopt Accuracy, Precision, Recall and F1 scoreas the evaluation metrics. Baselines. We first select 3 currently mainstream LVLMs as our baseline models, including InstructBLIP,LLaVA, and VisualGLM. In addition, we also compare against the baseline Woodpecker (Yin et al., 2023)which is a training-free hallucination correction method for LVLMs.",
  "Overall33.936.932.751.054.851.232.036.428.7": "Implementation Details. We utilize GPT-3.5-turbo-06132 to assist in keyword extraction, sub-questiongeneration, verification loop, and verification answer integration. Experiments have proven that GPT-3.5-turbo can tackle these tasks. On MMBench, we set the experiment rounds to 2: (1) In the first round ofevaluation, we have the model generate raw predictions according to MMBenchs evaluation rules and submitthem to MMBenchs official platform to obtain various accuracy rates; (2) In the second round of evaluation,based on the original prediction of the model, query classification, different verification processes and answerintegration are carried out using GPT-3.5-turbo (specific details can be found in ). Similarly, weupload the results of the second round of evaluation to the official MMBench platform to obtain variousaccuracy rates; (3) Finally, we jointly analyze the results of two rounds of evaluation to demonstrate theeffectiveness and superiority of Dentist. Previous studies (such as POPE (Li et al., 2023c)) have found a strong correlation between LVLMs hallucinationand the length of the generated text, and in our experiments we find this to be true. In the CHAIR evaluation,since the LVLMs we select all have remarkable instruction following ability, we notice that when the LVLMare prompted to \"generate as detailed a description as possible\", the CHAIR score of the model is much higherthan when they are prompted to \"generate a short description\" (a higher CHAIR score indicates a higherdegree of hallucination). This is not desirable in a common usage scenario. But we can take advantage ofthis feature to better demonstrate the ability of Dentist to mitigate hallucinations. Therefore, in the CHAIRexperiment, we prompt the model to generate a detailed description.",
  "Results (RQ1)": "Results on MMBench. The results on MMBench are summarized in Tab. 1. From this table, wehave several observations. (1) The largest accuracy improvement among the three LVLMs exceeds 15.6%,showing that Dentist have excellent correction effects, making obvious improvements in various metrics forthe baselines. (2) Dentist performs outstandingly in Image Emotion, Image Quality, Future Prediction,Attribute Recognition, etc., which indicates that Dentist is capable of mitigating hallucination in coarseperception, fine-grained perception and logic reasoning. (3) Among all metrics, Image Quality shows thehighest improvement, which indicates that Dentist is particularly effective for hallucinations in such problems.",
  "VisualGLMBaseline5.65.04.036.5Woodpecker2.01.61.338.3Dentist6.2 (+0.6)5.8 (+0.8)4.7 (+0.7)39.8": "Results on LLaVA-QA90. If manual verification is required, the evaluation on LLaVA-QA90 is labor-intensive and somewhat subjective. Therefore, it is necessary to use a powerful evaluation tool to ensureconsistency in evaluation standards while also possessing strong visual language task answering and instructionfollowing abilities. Therefore, we consider utilizing the powerful LVLM, GPT-4V. Specifically, we involve GPT-4V in scoring and evaluating model responses by setting appropriate prompt words. We have designed thefollowing three metrics: Accuracy: how accurate is the model response about the image content; Detailedness:level of details of the responses; Logicality: whether the reasoning content of response is reasonable. Inaddition, we also use GPT-3.5-turbo-0613 to calculate the proportion of logical reasoning sentences includedin the passage without hallucination, and record it as Precision to better check the rationality of the reasoningcontent. The prompt template is available in Appendix A.9. We conduct the same evaluation on the currenteffective hallucination correction method, Woodpecker, and compare the baseline LVLMs, Dentist, andWoodpecker scores. Tab. 2 shows the results. Obviously, equipped with our verification method, the models performance hasbeen comprehensively improved across the three metrics. On average, there is an improvement of over 0.5points (relative improvement exceeding 13.6%), and Dentist scores better than Woodpecker on all baselines.This indicates that Dentist not only improves the accuracy and detailedness of LVLMs in describing imagecontent, but also enhances the rationality of reasoning content. At the same time, the Precision of Dentisthas also improved to some extent, indicating that Dentist can increase the proportion of effective reasoningcontent in LVLMs answers. It is worth noting that Woodpeckers score in VisualGLM decreases significantly. We find that the reason forthe decrease in score is the failure of Woodpeckers object detector. On the one hand, this proves that theWoodpecker over-relies on its object detector, and when its detector fails, the correction effect of Woodpeckerwill become worse; on the other hand, it proves that Dentist has stronger robustness. Results on CHAIR. We compare the effects of Dentist and Woodpecker on mitigating hallucinations. Thespecific method is as follows: we select 2000 examples in COCO Val 2014, let the baseline model generatecorresponding captions, and then apply Dentist and Woodpecker respectively to process the hallucinationcorrection of the captions, and calculate the two metrics of the CHAIR to analyze the difference between thetwo correction methods. Tab. 3 shows the results. It is easy to draw the following conclusions from the results: (1) The model withremarkable performance may also produce more hallucinations. For example, LLaVA scored very high on",
  "Dentist68.1063.4286.9068.8780.90": "Results on POPE. We evaluate InstructBLIP, LLaVA and VisualGLM for hallucination on POPErespectively, and compare the results of baseline LVLMs, Woodpecker and Dentist. Tab. 4 summarizesthe results of POPE under the random, popular, and adversarial sampling settings. It can be seen that,in all sampling settings, VisualGLM is relatively weak, noting that its Recall and Yes Rate are both veryhigh (close to 1), which indicates that VisualGLM produces relatively severe hallucinations of objects notin the image (i.e. negative samples). The reason for this phenomenon is probably that VisualGLM fails todeal with the imbalanced distribution of the dataset during the training process. The other baseline modelsare above 70% on these metrics. Dentist brings significant improvements to these baseline LVLMs, whichvalidates Dentists excellent performance in mitigating hallucination. Specifically, under the relatively simple",
  "Ablation Studies (RQ2)": "To explore the effect of the query classification and verification loop, we conduct ablation studies in this section.Query Classification. We study three different variants and evaluate their performance on MMBench. (1)w/o-Classification: we disable the query classification section of Dentist ; (2) w/o-Reasoning: we classifyall queries into perception for verification; (3) w/o-Perception: we classify all queries into reasoning forverification. For the sake of brevity, we will refer to these three variants as w/o-Cla, w/o-Rea and w/o-Per inthe following discussion.",
  "Baseline33.74%31.15%Dentist37.62%35.92%w/o-Classification34.86%32.73%w/o-Reasoning38.94%25.48%w/o-Perception28.34%38.44%": "Tab. 5 shows the results of InstructBLIP. We can see that: (1) If query classification is not performed andverification is performed directly (w/o-Cla), the accuracy is not much higher than the baseline, and in somecases there is even a problem of reduced accuracy. Because at this point, the way Dentist corrects the modelsanswers is completely random, which largely depends on the performance and habits of GPT-3.5-turbo: itcan be seen that the perception accuracy may not differ much from the baseline, or slightly higher than thebaseline, while the reasoning accuracy may decrease. This is because the query classification section tends totreat the problem as perception for processing. (2) If all queries are classified into perception (w/o-Rea) (thisis what most current LVLM hallucination mitigation methods do), it can be seen that the perception accuracyis greatly improved, while the reasoning accuracy is greatly attenuated. This is because Dentist also verifiesthe reasoning problem as perception, so the verification method is not appropriate, resulting in a decrease inaccuracy; (3) In the same way, if all problems are classified as reasoning (w/o-Per), the reasoning accuracy isgreatly improved, and correspondingly, the perception accuracy is reduced; (4) It can also be found that theperception accuracy of w/o-Rea may even be slightly higher than that of Dentist. We speculate that this isdue to the misjudgment by GPT-3.5-turbo when classifying queries, such as mistakenly categorizing a verysmall number of perception queries as reasoning, while w/o-Rea precisely corrects this part of the misjudged",
  ": Results of verification loop": "Verification Loop. Verification loop is also a com-ponent that we need to study. We conduct additionalexperiments by varying the number of verificationloops in our framework and evaluating it on MM-Bench to demonstrate its effectiveness. shows the results. We can see that when thenumber of verification loop is small, there is a slightimprovement in accuracy as the number of loopsincreases. However, when the number of cycles islarge, the accuracy actually decrease as the numberof cycles increases. We separately take out one ofthe cases for observation and found that when thenumber of cycles is large, the output of the LVLM andGPT gradually become chaotic and uncontrollable,which may lead to an avalanche of decrease in the accuracy of the model when the number of cycles is largeenough. Therefore, we conclude that verification loop is effective, but special attention needs to be paid tolimiting its frequency. When the model answer matches the validation answer, it is important to exit theloop validation in a timely manner.",
  "Case Study (RQ3)": "We provide two testing examples in to conduct qualitative analysis. It is obvious from the aboveexample that: In the first example, Dentist classifies the query as \"Perception\". The original response ofLVLM is that \"There is a person holding two red apples in the picture\", which is obviously wrong. AfterDentist extracts the keywords \"apple\", \"two\", \"red\", etc., three corresponding sub-questions are generated.Dentist then answers the sub-questions one by one. Since the sub-questions are more targeted and usually",
  "Limitations and Future Work": "This study acknowledges limitation in the Dentist framework. When performing verification, we take theanswers of the verification questions as the ground truth, which may still contain hallucinations. In termsof reasoning hallucination mitigation, the CoT for reasoning we use is relatively simple. In addition, loopverification also increases time cost. In future work, we may refine the CoT for reasoning and add validationof the details obtained from the CoT. In order to reduce time and money costs, simplifying prompts withoutcompromising effectiveness is a feasible research direction.",
  "Conclusion": "In this work, we propose a unified framework for hallucination classification and mitigation. We are thefirst to distinguish treatment based on the classification of hallucinations and use a validation cycle for theremoval of hallucinations. Our framework has a clear design which is easily integrated into various LVLMs,and provides convenience for new classifications and treatments to integrate into the framework. To evaluatethe effectiveness of our framework, we conduct experiments on the three baseline LVLMs on MMbench,LLaVA-QA90, CHAIR and POPE, which shows that Dentist can significantly improve the baseline LVLMson these benchmarks. At the same time, we compare the results of LLaVA-QA90 and CHAIR with those ofWoodpecker, and the results shows that Dentist not only has excellent hallucination correction ability, butalso has strong robustness. This work was supported by the OpenAI Research Access Program (Award ID: 0000006384), which providedaccess to advanced GPT models. The authors thank OpenAI for their support and for fostering innovativeAI and machine learning research.",
  "Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision languagemodels. arXiv preprint arXiv:2308.06394, 2023. URL": "Qidong Huang, Xiao wen Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang,and Neng H. Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trustpenalty and retrospection-allocation. ArXiv, abs/2311.17911, 2023. URL Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, AndreaMadotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM ComputingSurveys, 55(12):138, 2023.",
  "Liqiang Jing, Ruosen Li, Yunmo Chen, Mengzhao Jia, and Xinya Du. FAITHSCORE: evaluating hallucinationsin large vision-language models. CoRR, abs/2311.01477, 2023a. doi: 10.48550/ARXIV.2311.01477. URL": "Liqiang Jing, Xuemeng Song, Kun Ouyang, Mengzhao Jia, and Liqiang Nie. Multi-source semantic graph-basedmultimodal sarcasm explanation generation. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp.1134911361. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.ACL-LONG.635.URL Liqiang Jing, Xuemeng Song, Xuming Lin, Zhongzhou Zhao, Wei Zhou, and Liqiang Nie. Stylized data-to-textgeneration: A case study in the e-commerce domain. ACM Trans. Inf. Syst., 42(1):25:125:24, 2024a. doi:10.1145/3603374. URL",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485,2023c. URL": "Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, HangSu, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection.arXiv preprint arXiv:2303.05499, 2023d. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprintarXiv:2307.06281, 2023e. URL Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Baochen Sun, Carl Yang, andJie Yang. Evaluation and mitigation of agnosia in multimodal large language models. arXiv preprintarXiv:2309.04041, 2023. URL",
  "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucinationin image captioning. arXiv preprint arXiv:1809.02156, 2018": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv,abs/2302.13971, 2023. URL Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong, Pan Zhang, Xiao wen Dong, Weijia Li, Wei Li,Jiaqi Wang, and Conghui He. Vigc: Visual instruction generation and correction. ArXiv, abs/2308.12714,2023a. URL Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Annual Meeting ofthe Association for Computational Linguistics, 2022. URL Ziyue Wang, Chi Chen, Peng Li, and Yang Liu. Filling the image information gap for vqa: Prompting largelanguage models to proactively ask questions. In Conference on Empirical Methods in Natural LanguageProcessing, 2023b. URL",
  "Jinge Wu, Yunsoo Kim, and Honghan Wu. Hallucination benchmark in medical visual question answering.arXiv preprint arXiv:2401.05827, 2024. URL": "Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. arXivpreprint arXiv:2310.16045, 2023. URL Qifan Yu, Juncheng Li, Longhui Wei, Liang Pang, Wentao Ye, Bosheng Qin, Siliang Tang, Qi Tian, andYueting Zhuang. Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data. arXiv preprintarXiv:2311.13614, 2023a. URL Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu,Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua.Rlhf-v: Towards trustworthy mllms via behav-ior alignment from fine-grained correctional human feedback.ArXiv, abs/2312.00849, 2023b.URL Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, RuiyangRen, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large languagemodels. ArXiv, abs/2303.18223, 2023. URL",
  "A.1Multiple Responses Study": "In order to comprehensively study the robustness of Dentist and the consistency of experimental results,we re-conduct POPE evaluation under the random sampling setting, and the other settings are the sameas those introduced in .1. The difference is that we let the LVLM test each example 10 times andlet our framework process each of these responses. We provide two new baselines: (1) Direct RejectionBaseline (DR Baseline): If all the responses of one sample have errors, deem the sample incorrect, else werandomly choose one of the responses which has no errors to be the answer. (2) Repeated CorrectionBaseline (RC Baseline): For each example, when our framework detects hallucinations in all ten responsesof the model, the hallucinations are corrected normally. We speculate that the performance of DR Baselinewill significantly decrease because only the classification function of our framework are enabled to detecterrors, while the answer correction function of our framework is disabled, and the performance of RC Baselineshould be consistent with Tab. 4.",
  "A.2Discussion on Reproducibility": "We provide detailed discussion on the effect of our framework on correcting hallucinations and the repro-ducibility of the above experimental results. It should be emphasized that since our method is training-free,all parameters of the model are fixed. We focus on the following questions: (1) When LVLM repeatedlygenerates captions for the same image, will it produce the same hallucinations? (2) When using our frameworkto process a series of model responses in (1), can we obtain consistent results? Can we guarantee that thehallucinations can be corrected every time? We continue to discuss the results of Appendix A.1.From the DR Baseline, we can see whether the sameLVLM will repeatedly hallucinate the same image, and from the RC Baseline, we can see whether ourframeworks correction of hallucinations is repeatable. We analyze the responses of LVLMs and the results inTab. 6, and arrive at the following conclusions: (1) When the model parameters are fixed, it is very easy tohallucinate the same image repeatedly, as long as the same image and the prompt with the similar semanticare provided. This is also the reason for the poor performance of DR Baseline, as the same hallucinationsrepeatedly appears in multiple responses of LVLM. (2) Our framework is still able to find and correct thehallucinations generated by LVLM in the face of repeated tests, and the performance is almost the sameas the previous experiment, that is, the performance of RC Baseline is not significantly deviated from Tab.4 which shows that our framework is very effective in detecting hallucinations generated by models andcorrecting hallucinations. Therefore, we believe that our experimental results are reproducible and that thehallucination correction capability of our framework is reproducible.",
  "A.15GPT-4V-aided evaluation alignment method": "When evaluating on LLaVA-QA90, we respectively pair baseline LVLMs with Dentist, baseline LVLMs withWoodpecker, and provided their responses to GPT-4V for scoring. Thus the scores need to be aligned. Thealignment of the scores is as followed: Suppose that when the responses of LVLMs and Dentist are providedto GPT-4V for scoring, their scores are SBaseline1 and SDentist respectively, and when LVLMs is paired withWoodpecker, their scores are SBaseline2 and SW oodpecker respectively. The final aligned scores are shown in.",
  "Role:You are my language assistant for generating sub-questions.Please generate sub-questions to verify the caption of the picture based on QA-examples below": "Rules:1.The number of sub-questions cannot exceed three.2.Extract keywords such as objects, quantities, and locations to generate sub-questions.3.Each sub-question should have a different focus.4.Don't ask repeated questions in different sub-questions.5.If my input contains multiple choice questions, please generate sub-questions based on thequestion, options and answers. Examples:1.my input:\"Question: Write a detailed description for this picture.Answer: The picture shows a man standing on the back of the yellow taxi, with a yellow shirt and blackpants, and a blue backpack on his back. The taxi is driving on a city street with cars and taxis in thebackground.\"sub-questions you generated:\"1.Is there a man standing on the back of a taxi in this picture?2.What color are the T-shirt and pants that man wear?3.What's in the background? \"",
  "Role:You are my language assistant for correcting or remaining my passage.Below is a passage and some Q&A pairs. You need to modify the passage or just keep it unchangedbased on the Q&A pairs": "Rules:1.The information provided by the Q&A pairs is the ground truth, and the information in the passagemay contain errors.2.If the passage conflict with the Q&A pairs, find them and correct the passage based on the Q&Apairs. Try to make minimal changes to retain the original sentence. Then give me the passage whichhave been corrected by you.3.If the passage has no confliction with the Q&A pairs, just keep the original passage and give me that.4.At any time your output should only be a passage. Examples:1.Passage:\"There are two apples in the picture, they look stale.\"Q&A pairs:Q:How many apples are there in the picture? A:There are three apples in the picture.Q:Do these apples look fresh in the picture? A:No, they look stale.Your output:\"There are three apples in the picture, they look stale.\"",
  "Prompt": "You are my language assistant. You need to calculate the proportion of logical reasoning sentencesincluded in the following two passages respectively.Specifically, for each passage, you need to follow these steps to calculate the proportion:1. Count the number of all sentences in the passage.2. Count the number of logical reasoning sentences in the passage.3. Divide the number of logical reasoning sentences by the number of all sentences to get theproportion. Please output a single line, containing only two values indicating the proportion for Passage 1 and 2,respectively.This means that you can only output two values and not any other text analysis. The two values areseparated by a space."
}