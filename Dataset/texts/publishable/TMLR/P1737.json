{
  "Abstract": "As machine learning (ML) algorithms are used in applications that involve humans, concernshave arisen that these algorithms may be biased against certain social groups. Counterfac-tual fairness (CF) is a fairness notion proposed in Kusner et al. (2017) that measures theunfairness of ML predictions; it requires that the prediction perceived by an individual inthe real world has the same marginal distribution as it would be in a counterfactual world, inwhich the individual belongs to a different group. Although CF ensures fair ML predictions,it fails to consider the downstream effects of ML predictions on individuals. Since humansare strategic and often adapt their behaviors in response to the ML system, predictionsthat satisfy CF may not lead to a fair future outcome for the individuals. In this paper,we introduce lookahead counterfactual fairness (LCF), a fairness notion accounting for thedownstream effects of ML models which requires the individual future status to be counter-factually fair. We theoretically identify conditions under which LCF can be satisfied andpropose an algorithm based on the theorems. We also extend the concept to path-dependentfairness. Experiments on both synthetic and real data validate the proposed method1.",
  "Introduction": "The integration of machine learning (ML) into high-stakes domains (e.g., lending, hiring, college admissions,healthcare) has the potential to enhance traditional human-driven processes. However, it may introducethe risk of perpetuating biases and unfair treatment of protected groups. For instance, the violence riskassessment tool SAVRY has been shown to discriminate against males and foreigners (Tolan et al., 2019);Amazons previous hiring system exhibited gender bias (Dastin, 2018); the accuracy of a computer-aidedclinical diagnostic system varies significantly across patients from different racial groups (Daneshjou et al.,",
  "Published in Transactions on Machine Learning Research (12/2024)": "Theorems 5.1 and 5.2 provide insights on designing algorithms to train a predictor with perfect or RelaxedLCF. Specifically, given training data D = {(x(i), y(i), a(i))}ni=1, we first estimate the structural equations.Then, we choose a parameterized predictor g that satisfies the conditions in Theorem 5.1 or 5.2. An exampleis shown in Algorithm 1, which finds an optimal predictor in the form of g(y, u) = p1y2 + p2y + p3 + h(u)under LCF, where p1 =1 2(||w||22+2), is the training parameter for function h, and p2, p3 are two othertraining parameters. Under Algorithm 1, we can find the optimal values for p2, p3, using training data D.If we only want to satisfy Relaxed LCF (Definition 5.2), p1 can be a training parameter with 0 < p1 < T.",
  "Related Work": "Causal fairness has been explored in many aspects in recent years research. Kilbertus et al. (2017) point outthat no observational criterion can distinguish scenarios determined by different causal mechanisms but havethe same observational distribution. They propose the definition of unresolved discrimination and proxydiscrimination based on the intuition that some of the paths from the sensitive attribute to the predictioncan be acceptable. Nabi & Shpitser (2018) argue that a fair causal inference on the outcome can be obtainedby solving a constrained optimization problem.These notions are based on defining constraints on theinterventional distributions. Counterfactual fairness (Kusner et al., 2017) requires the prediction on the target variable to have the samedistribution in the factual world and counterfactual world. Many extensions of traditional statistical fairnessnotions such as Fair on Average Causal Effect (FACE) (can be regarded as counterfactual demographicparity) (Khademi et al., 2019), Fair on Average Causal Effect on the Treated (FACT) (can be regarded ascounterfactual equalized odds) (Khademi et al., 2019), and CAPI fairness (counterfactual individual fairness)(Ehyaei et al., 2024) have been proposed. Path-specific counterfactual fairness (Chiappa, 2019) consideredthe path-specific causal effect. However, the notions are focused on fairness in static settings and do notconsider the future effect. Most recent work about counterfactual fairness is about achieving counterfactualfairness in different applications, such as graph data (Wang et al., 2024a;b), medical LLMs (Poulain et al.,2024), or software debuging (Xiao et al., 2024).Some literature try to use counterfactual fairness forexplanations (Goethals et al., 2024). Connecting counterfactual fairness with group fairness notions (Anthis& Veitch, 2024) or exploring counterfactual fairness with partial knowledge about the causal model (Shaoet al., 2024; Pinto et al., 2024; Duong et al., 2024; Zhou et al., 2024) are also receiving much attention.Machado et al. (2024) propose an idea of interpretable counterfactual fairness by deriving counterfactualswith optimal transports (De Lara et al., 2024). While extending the definition of counterfactual fairness toinclude the downstream effects has been less focused on. Several studies in the literature consider the downstream effect on fairness of ML predictions. There aretwo kinds of objectives in the study of downstream effects: ensuring fair predictions in the future or fairtrue status in the future.The two most related works to our paper are Hu & Zhang (2022) and Tanget al. (2023). Hu & Zhang (2022) consider the problem of ensuring ML predictions satisfy path-specificcounterfactual fairness over time after interactions between individuals and an ML system.Tang et al.(2023) study the impact on the future true status of the ML predictions. Even though they consideredthe impact of a counterfactually fair predictor, their goal is to ensure parity-based fairness. Therefore, thecurrent works lack the consideration of ensuring counterfactual fairness on the true label after the individualresponds to the current ML prediction. Our paper is aimed at solving this problem.",
  "Background: counterfactuals": "If the probability density functions of unobserved variables are known, we can leverage the structural equa-tions in SCM to find the marginal distribution of any observable variable Vi V and even study howintervening certain observable variables impacts other variables.Specifically, the intervention on vari-able Vi is equivalent to replacing structural equation Vi = fi(pai, Upai) with equation Vi = v for some v.Given new structural equation Vi = v and other unchanged structural equations, we can find out how thedistribution of other observable variables changes as we change value v. In addition to understanding the impact of an intervention, SCM can further facilitate counterfactualinference, which aims to answer the question what would be the value of Y if Z had taken value z in thepresence of evidence O = o (both Y and Z are two observable variables)? The answer to this question isdenoted by YZz(U) with U following conditional distribution of Pr{U = u|O = o}. Given U = u andstructural equations F, the counterfactual value of Y can be computed by replacing the structural equationof Z with Z = z and replacing U with u in the rest of the structural equations. Such counterfactual istypically denoted by YZz(u). Given evidence O = o, the distribution of counterfactual value YZz(U) canbe calculated as follows,2",
  "uPr{YZz(u) = y} Pr{U = u|O = o}.(2)": "Example 3.1 (Law school success). Consider two groups of college students distinguished by genderA {0, 1} whose first-year average (FYA) in college is denoted by Y . The FYA of each student is causallyrelated to (observable) grade-point average (GPA) before entering college XG, entrance exam score (LSAT)XL, and gender A.Suppose there are two unobservable variables U = (UA, UXY ), e.g., UXY may beinterpreted as the students knowledge. Consider the following structural equations:",
  "XL = bL + wALA + UXY ,Y = bF + wAF A + UXY ,": "where (bG, wAG, bL, wAL, bF , wAF ) are know parameters of the causal model. Given observation XG = 1, A = 0,the counterfactual value can be calculated with an abduction-action-prediction procedure Glymour et al.(2016): (i) abduction that finds posterior distribution Pr{U = u|XG = 1, A = 0}. Here, we have UXY = 1bGand UA = 0 with probability 1; (ii) action that performs intervention A = 1 by replacing structural equations 2Given structural equations equation 1 and the marginal distribution of U, Pr{U = u, O = o} can be calculated using theChange-of-Variables Technique and the Jacobian factor. As a result, Pr{U = u|O = o} = Pr{U=u, O=o}",
  "Counterfactual Fairness": "Counterfactual Fairness (CF) was first proposed by Kusner et al. (2017); it requires that for an individualwith (X = x, A = a), the prediction Y in the factual world should be the same as that in the counterfactualworld in which the individual belongs to a different group. Mathematically, CF is defined as follows: a, a A, X X, y Y,",
  "PrYAa(U) = y|X = x, A = a= PrYAa(U) = y|X = x, A = a,": "While the CF notion has been widely used in the literature, it does not account for the downstream impactsof ML prediction Y on individuals in factual and counterfactual worlds. To illustrate the importance ofconsidering such impacts, we provide an example below. Example 3.2. Consider automatic lending where an ML model is used to decide whether to issue a loanto an applicant based on credit score X and sensitive attribute A. As highlighted in Liu et al. (2018),issuing loans to unqualified people who cannot repay the loan may hurt them by worsening their futurecredit scores. Assume an applicant in the factual world is qualified for the loan and does not default. Butin a counterfactual world where the applicant belongs to another group, he/she is not qualified. Undercounterfactually fair predictions, both individuals in the factual and counterfactual worlds should receivethe loan with the same probability. Suppose both are issued a loan, then the one in the counterfactual worldwould have a worse credit score in the future. Thus, it is crucial to consider the downstream effects whenlearning a fair ML model.",
  "Characterize downstream effects": "Motivated by Example 3.2, this work studies CF in a dynamic setting where the deployed ML decisions mayaffect individual behavior and change their future features and statuses. Formally, let X and Y denote anindividuals future feature vector and status, respectively. We use an individual response r to capture theimpact of ML prediction Y on individuals, as defined below. Definition 3.1 (Individual response). An individual response r : U V Y U V is a map fromthe current exogenous variables U U, endogenous variables V V, and prediction Y Y to the futureexogenous variables U and endogenous variables V . One way to tackle the issue in Example 3.2 is to explicitly consider the individual response and imposea fairness constraint on future status Y instead of the prediction Y . We call such a fairness notion theLookahead Counterfactual Fairness (LCF) and present it in .",
  "Based on Kusner et al. (2017), a predictor that only uses U1 and U2 as input is counterfactually fair.3": "Therefore, Y = h(U1, U2) satisfies CF. Let U1 and U2 be uniformly distributed over .Note thatthe response r(U1, Y ) and r(U2, Y ) imply that individuals make efforts to change feature vectors throughchanging the unobservable variables, which results in higher Y in the future. It is easy to see that a CFpredictor h(U1, U2) = U1 + U2 minimizes the MSE loss E{(Y Y )2} if A {1, 1} and Pr{A = 1} = 0.5.However, since U1 Y = U2 Y = 1, we have:",
  "Y = fY (X1, ..., Xd, U Y ).(6)": "The above scenario implies that individuals respond to ML model by strategically moving features towardthe direction that increases their chances of receiving favorable decisions, step size > 0 controlsthe magnitude of data change and can be interpreted as the effort budget individuals have on changing theirdata. Note that this type of response has been widely studied in strategic classification literature (Rosenfeldet al., 2020; Hardt et al., 2016a). The above process with d = 2 is visualized in . : A causal graph and individualresponses with two features X1, X2.Theblack arrows represent the connections de-scribed in structural functions. The red ar-rows represent the response process. Thegreen dash arrows are the potential connec-tion to prediction Y .",
  "Our goal is to train an ML model under LCF constraint. Beforepresenting our method, we first define the notion of counter-factual random variables": "Definition 5.1 (Counterfactual random variable). Let x anda be realizations of random variables X and A, and a = a. Wesay X := XAa(U) and Y := YAa(U) are the counterfactualrandom variables associated with (x, a) if U follows the condi-tional distribution Pr{U|X = x, A = a} as given by the causalmodel M. The realizations of X, Y are denoted by x and y.",
  ", we can achieve perfect LCF": "It is worth noting that Definition 4.1 can be a very strong constraint and imposing Y Aa(U) and Y Aa(U)to have the same distribution may degrade the performance of the predictor significantly. To tackle this, wemay consider a weaker version of LCF.Definition 5.2 (Relaxed LCF). We say Relaxed LCF holds if (a, a) A2, a = a, X X, y Y, we have:",
  "Definition 5.2 implies that after individuals respond to ML model, the difference between the future status Y": "in factual and counterfactual worlds should be smaller than the difference between original status Y in factualand counterfactual worlds. In other words, it means that the disparity between factual and counterfactualworlds must decrease over time. In , we empirically show that constraint in equation 9 is weakerthan the constraint in equation 3 and can lead to a better prediction performance.Corollary 5.1 (Relaxed LCF with predictor in equation 8). Consider the same causal model defined inTheorem 5.1 and the predictor defined in equation 8. Relaxed LCF holds if p1 (0, T).",
  "Therefore |y y| < |y y|. With law of total probability, the Relaxed LCF is satisfied": "Apart from relaxing p1 in predictor as shown in equation 8, we can also relax the form of the predictor tosatisfy Relaxed LCF, as shown in Theorem 5.2.Theorem 5.2 (Predictor under Relaxed LCF). Consider the same causal model defined in Theorem 5.1. Apredictor g( Y , U) satisfies Relaxed LCF if g has the following three properties:",
  "is a parameter for function h, and l is a loss function.Output: p2, p3,": "It is worth noting that the results in Theorems 5.1 and 5.2 are for linear causal models. When the causalmodel is non-linear, it is hard to construct a model satisfying perfect LCF in Definition 4.1. Nonetheless, wecan still show that it is possible to satisfy Relaxed LCF (Definition 5.2) for certain non-linear causal models.Theorem 5.3 below focuses on a special case when X is not linearly dependent on A and UX and it identifiesthe condition under which Relaxed LCF can be guaranteed. Theorem 5.3. Consider a bijective causal model M = (U, V, F), where U is a scalar exogenous variable,V = {A, X, Y }, and the structural equations X = fX(A, U) Rd, Y = fY (X, U) R can be written in theform of",
  "Therefore, |y y| < |y y|. With law of total probability, we have the Relaxed LCF satisfied": "Theorems 5.2 and 5.3 show that designing a predictor under Relaxed LCF highly depends on the form ofcausal structure and structural equations. To wrap up this section, we would like to identify conditionsunder which Relaxed LCF holds in a causal graph that X is determined by the product of UX and A. Theorem 5.4. Consider a non-linear causal model M = (U, V, F), where U= {UX, UY }, UX=[U1, U2, ..., Ud]T, V = {A, X, Y }, X = [X1, X2, ..., Xd]T, A {a1, a2} is a binary sensitive attribute. As-sume that the structural functions are given by,",
  "Property (iii) ensures that || < 2|y y|. Therefore, |y y| < |y y|": "Although the structural equation associated with Y is still linear in X and UY , we emphasize that such alinear assumption has been very common in the literature due to the complex nature of strategic classificationZhang et al. (2022); Liu et al. (2020); Bechavod et al. (2022). For instance, Bechavod et al. (2022) assumedthe actual status of individuals is Y = X, a linear function of features X. Zhang et al. (2022) assumedthat X itself may be non-linear in some underlying traits of the individuals, but the relationship between Xand P(Y = 1|X) is still linear. Indeed, due to the individuals strategic response, conducting the theoreticalanalysis accounting for such responses can be highly challenging. Nonetheless, it is worthwhile extendingLCF to non-linear settings and we leave this for future works.",
  "Path-dependent LCF": "An extension of counterfactual fairness called path-dependent fairness has been introduced in Kusner et al.(2017). In this section, we also want to introduce an extension of LCF called path-dependent LCF. We willalso modify Algorithm 1 to satisfy path-dependent LCF. We start by introducing the notion of path-dependent counterfactuals. In a causal model associated with acausal graph G, we denote PGA as a set of unfair paths from sensitive attribute A to Y . We define XPcGAas the set of features that are not present in any of the unfair paths. Under observation X = x, A = a, wecall YAa,XPcGAxPcGA (U) path-dependent counterfactual random variable for Y , and its distribution can be",
  "Synthetic Data": "We generate the synthetic data based on the causal model described in Theorem 5.1, where we set d = 10and generated 1000 data points. We assume UX and UY follow the uniform distribution over and thesensitive attribute A {0, 1} is a Bernoulli random variable with Pr{A = 0} = 0.5. Then, we generate Xand Y using the structural functions described in Theorem 5.1.5 Based on the causal model, the conditionaldistribution of UX and UY given X = x, A = a are as follows,",
  ",UY |X = x, A = a Uniform(0, 1).(11)": "Baselines.We used two baselines for comparison: (i) Unfair predictor (UF) is a linear model withoutfairness constraint imposed. It takes feature X as input and predicts Y . (ii) Counterfactual fair predictor(CF) only takes the unobservable variables U as the input and was proposed by Kusner et al. (2017). Implementation Details.To find a predictor satisfying Definition 4.1, we train a predictor in the formof Eq. 8. In our experiment, h(u) is a linear function. To train g(y, u), we follows Algorithm 1 with m = 100.We split the dataset into the training/validation/test set at 60%/20%/20% ratio randomly and repeat theexperiment 5 times. We use the validation set to find the optimal number of training epochs and the learningrate. Based on our observation, Adam optimization with a learning rate equal to 103 and 2000 epochs givesus the best performance. Metrics.We use three metrics to evaluate the methods. To evaluate the performance, we use the meansquared error (MSE). Given a dataset {x(i), a(i), y(i)}ni=1, for each x(i) and a(i), we generate m = 100 valuesof u(i)[j] from the posterior distribution. MSE can be estimated as follows,6",
  "Results. illustrates the results when we set = 10 and p1 =T2 .The results show that ourmethod can achieve perfect LCF with p1 = T": "2 . Note that in our experiment, the range of Y is [0, 3.73],and our method and UF can achieve similar MSE. Moreover, our method achieves better performance thanthe CF method because Y includes useful predictive information and using it in our predictor can improveperformance and decrease the disparity simultaneously. Because both CF and UF do not take into accountfuture outcome Y , |Y Y | is similar to |Y Y |, leading to UIR = 0. Based on Corollary 5.1, the value ofp1 can impact the strength of fairness. We examine the tradeoff between accuracy and fairness by changing",
  "the value of p1 fromT512 to T": "2 under different . a shows the MSE as a function of AFCE. The resultsshow that when = 1 we can easily control the accuracy-fairness trade-off in our algorithm by adjusting p1.When becomes large, we can get a high LCF improvement while maintaining a low MSE. To show howour method impacts a specific individual, we choose the first data point in our test dataset and plot thedistribution of factual future status Y and counterfactual future status Y for this specific data point underdifferent methods. illustrates such distributions. It can be seen in the most left plot that there isan obvious gap between factual Y and counterfactual Y . Both UF and CF can not decrease this gap forfuture outcome Y . However, with our method, we can observe that the distributions of Y and Y becomecloser to each other. When p1 = T",
  ": Causal model forthe Law School Dataset": "We further measure the performance of our proposed method using the LawSchool Admission Dataset Wightman (1998). In this experiment, the objectiveis to forecast the first-year average grades (FYA) of students in law school usingtheir undergraduate GPA and LSAT scores. Dataset.The dataset consists of 21,791 records. Each record is character-ized by 4 attributes: Sex (S), Race (R), UGPA (G), LSAT (L), and FYA(F). Both Sex and Race are categorical in nature. The Sex attribute can beeither male or female, while Race can be Amerindian, Asian, Black, Hispanic,Mexican, Puerto Rican, White, or other. The UGPA is a continuous variableranging from 0 to 4. LSAT is an integer-based attribute with a range of .FYA, which is the target variable for prediction, is a real number ranging from4 to 4 (it has been normalized). In this study, we consider S as the sensitiveattribute, while R, G, and L are treated as features.",
  ": Density plot for Y and Y in synthetic data. For a chosen data point, we sampled a batch of Uunder the conditional distribution of it and plot the distribution of Y and Y": "Causal Model.We adopt the causal model as presented in Kusner et al. (2017), which can be visualized in. In this causal graph, K represents an unobserved variable, which can be interpreted as knowledge.Thus, the model suggests that students grades (UGPA, LSAT, FYA) are influenced by their sex, race, andunderlying knowledge. We assume that the prior distribution for K follows a normal distribution, denotedas N(0, 1). We adopt the same structural equations as Kusner et al. (2017):",
  "F=N(wKF K + wRF R + wSF S, 1)": "Implementation Details.Note that race is an immutable characteristic. Therefore, we assume that theindividuals only adjust their knowledge K in response to the prediction model Y . That is K = K + K Y .In contrast to synthetic data, the parameters of structural equations are unknown, and we have to use thetraining dataset to estimate them. Following the approach of Kusner et al. (2017), we assume that G and Fadhere to Gaussian distributions centered at wKG K +wRGR+wSGS +bG and wKF K +wRF R+wSF S, respectively.Note that L is an integer, and it follows a Poisson distribution with the parameter exp{wKL K +wRLR+wSLS +bL}. Using the Markov chain Monte Carlo (MCMC) method Geyer (1992), we can estimate the parametersand the conditional distribution of K given (R, S, G, L). For each given data, we sampled m = 500 differentks from this conditional distribution. We partitioned the data into training, validation, and test sets with60%/20%/20% ratio. : Results on Law School Data: comparison with two baselines, unfair predictor (UF) and counterfac-tual fair predictor (CF), in terms of accuracy (MSE) and lookahead counterfactual fairness (AFCE, UIR).",
  "and p1 = T": "2 where T = 1/(wFK)2. Theresults show that our method achieves a similar MSE as the CF predictor. However, it can improve AFCEsignificantly compared to the baselines. shows the distribution of Y and Y for the first data pointin the test set in the factual and counterfactual worlds. Under the UF and CF predictor, the disparitybetween factual and factual Y remains similar to the disparity between factual and counterfactual Y . Onthe other hand, the disparity between factual and counterfactual Y under our algorithms gets better for bothp1 = T/2 and p1 = T/4. b demonstrates that for the law school dataset, the trade-off between MSEand AFCE can be adjusted by changing hyperparameter p1. show the factual and counterfactualdistributions in real data experiment. It can be seen that our method is the only way that can decrease thegap between Y and Y in an obvious way.",
  "Conclusion": "This work studied the impact of ML decisions on individuals future status using a counterfactual inferenceframework. We observed that imposing the CF predictor may not decrease the group disparity in individualsfuture status. We thus introduced the lookahead counterfactual fairness (LCF) notion, which takes intoaccount the downstream effects of ML models and requires the individual future status to be counterfactuallyfair. We proposed a method to train an ML model under LCF and evaluated the method through empiricalstudies on synthetic and real data.",
  "Tri Dung Duong, Qian Li, and Guandong Xu. Achieving counterfactual fairness with imperfect structuralcausal model. Expert Systems with Applications, 240:122411, 2024": "Ahmad-Reza Ehyaei, Kiarash Mohammadi, Amir-Hossein Karimi, Samira Samadi, and Golnoosh Farnadi.Causal adversarial perturbations for individual fairness and robustness in heterogeneous data spaces. InProceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1184711855, 2024. Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu Zhao, Changhua Pei, Fei Sun,Junfeng Ge, Wenwu Ou, et al. Towards long-term fairness in recommendation. In Proceedings of the 14thACM international conference on web search and data mining, pp. 445453, 2021.",
  "Lydia T Liu, Sarah Dean, Esther Rolf, Max Simchowitz, and Moritz Hardt. Delayed impact of fair machinelearning. In International Conference on Machine Learning, pp. 31503158. PMLR, 2018": "Lydia T Liu, Ashia Wilson, Nika Haghtalab, Adam Tauman Kalai, Christian Borgs, and Jennifer Chayes.The disparate equilibria of algorithmic decision making when individuals invest rationally. In Proceedingsof the 2020 Conference on Fairness, Accountability, and Transparency, pp. 381391, 2020. Jing Ma, Ruocheng Guo, Aidong Zhang, and Jundong Li. Learning for counterfactual fairness from obser-vational data. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and DataMining, pp. 16201630, 2023. Agathe Fernandes Machado, Arthur Charpentier, and Ewen Gallic. Sequential conditional transport onprobabilistic graphs for interpretable counterfactual fairness. arXiv preprint arXiv:2408.03425, 2024.",
  "Raphael Poulain, Hamed Fayyaz, and Rahmatollah Beheshti. Aligning (medical) llms for (counterfactual)fairness. arXiv preprint arXiv:2408.12055, 2024": "Nir Rosenfeld, Anna Hilgard, Sai Srivatsa Ravindranath, and David C Parkes. From predictions to decisions:Using lookahead regularization. Advances in Neural Information Processing Systems, 33:41154126, 2020. Pengyang Shao, Le Wu, Kun Zhang, Defu Lian, Richang Hong, Yong Li, and Meng Wang. Average user-sidecounterfactual fairness for collaborative filtering. ACM Transactions on Information Systems, 42(5):126,2024.",
  "Zeyu Tang, Yatong Chen, Yang Liu, and Kun Zhang.Tier balancing: Towards dynamic fairness overunderlying causal factors. arXiv preprint arXiv:2301.08987, 2023": "Songl Tolan, Marius Miron, Emilia Gmez, and Carlos Castillo.Why machine learning may lead tounfairness: Evidence from risk assessment for juvenile justice in catalonia. In Proceedings of the SeventeenthInternational Conference on Artificial Intelligence and Law, pp. 8392, 2019. Zichong Wang, Zhibo Chu, Ronald Blanco, Zhong Chen, Shu-Ching Chen, and Wenbin Zhang. Advanc-ing graph counterfactual fairness through fair representation learning. In Joint European Conference onMachine Learning and Knowledge Discovery in Databases, pp. 4058. Springer, 2024a.",
  "Linda F Wightman. Lsac national longitudinal bar passage study. lsac research report series. 1998": "Yongkai Wu, Lu Zhang, and Xintao Wu. Counterfactual fairness: Unidentification, bound and algorithm.In Proceedings of the twenty-eighth international joint conference on Artificial Intelligence, 2019. Ying Xiao, Jie M Zhang, Yepang Liu, Mohammad Reza Mousavi, Sicen Liu, and Dingyuan Xue. Mirrorfair:Fixing fairness bugs in machine learning software via counterfactual predictions. Proceedings of the ACMon Software Engineering, 1(FSE):21212143, 2024.",
  "Depeng Xu, Shuhan Yuan, and Xintao Wu. Achieving differential privacy and fairness in logistic regression.In Companion Proceedings of The 2019 World Wide Web Conference, pp. 594599, 2019": "Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness be-yond disparate treatment & disparate impact: Learning classification without disparate mistreatment. InProceedings of the 26th international conference on world wide web, pp. 11711180, 2017. Xueru Zhang, Mohammadmahdi Khaliligarekani, Cem Tekin, et al. Group retention when using machinelearning in sequential decision making: the interplay between user dynamics and fairness. Advances inNeural Information Processing Systems, 32:1526915278, 2019. Xueru Zhang, Ruibo Tu, Yang Liu, Mingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng Zhang. Howdo fair decisions fare in long-term qualification? Advances in Neural Information Processing Systems, 33:1845718469, 2020. Xueru Zhang, Mohammad Mahdi Khalili, Kun Jin, Parinaz Naghizadeh, and Mingyan Liu. Fairness inter-ventions as (Dis)Incentives for strategic manipulation. In Proceedings of the 39th International Conferenceon Machine Learning, pp. 2623926264, 2022. Zeyu Zhou, Ruqi Bai, and David I Inouye. Improving practical counterfactual fairness with limited causalknowledge. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models,2024. Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong. Counterfactual fairnesswith partially known causal graph. Advances in Neural Information Processing Systems, 35:12381252,2022."
}