{
  "Abstract": "Finding efficient routes for data packets is an essential task in computer networking. Theoptimal routes depend greatly on the current network topology, state and traffic demand,and they can change within milliseconds. Reinforcement Learning can help to learn networkrepresentations that provide routing decisions for possibly novel situations. So far, thishas commonly been done using fluid network models. We investigate their suitability formillisecond-scale adaptations with a range of traffic mixes and find that packet-level networkmodels are necessary to capture true dynamics, in particular in the presence of TCP traffic. Tothis end, we present PackeRL, the first packet-level Reinforcement Learning environment forrouting in generic network topologies. Our experiments confirm that learning-based strategiesthat have been trained in fluid environments do not generalize well to this more realistic, butmore challenging setup. Hence, we also introduce two new algorithms for learning sub-secondRouting Optimization. We present M-Slim, a dynamic shortest-path algorithm that excelsat high traffic volumes but is computationally hard to scale to large network topologies,and FieldLines, a novel next-hop policy design that re-optimizes routing for any networktopology within milliseconds without requiring any re-training. Both algorithms outperformcurrent learning-based approaches as well as commonly used static baseline protocols inscenarios with high-traffic volumes. All findings are backed by extensive experiments inrealistic network conditions in our fast and versatile training and evaluation framework.1",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Illustration of network monitoring graphs (left column) and corresponding routing actions (rightcolumn) in two consecutive timesteps, taken from the evaluation on the nxXS topology preset of .1.The dark red color on the monitoring graphs edge illustrations denote the maximum packet buffer fill ofthe incident network device in the past timestep, the light red color denotes the packet buffer fill at the endof the past timestep (e.g. 50% red = 50% filled). In the action visualization, routing nodes hold distinctcolors, and the small colored arrows placed on the edges show where packets destined for the correspondinglycolored destination node are sent next. The two upper rows of the figure show network states seen and actionstaken by M-Slim, the two lower rows show network states seen and actions taken by FieldLines). M-Slimadjusts a few routing selections as relevant edges of the monitoring graph become less congested (e.g. edges7 5, 3 0 and 0 2) from the first to the second timestep. On the other hand, FieldLines is much moreconservative and only changes the next-hop selection for nodes 0 and 8 at routing node 5, even though e.g.the edges between nodes 0 and 7 have considerably changed in state.",
  "Related Work": "Researchers working on conventional TE approaches have recognized the need for standardized evaluationenvironments: REPETITA (Gay et al., 2017b) aims at fostering reproducibility in TE optimization research,and the goal of YATES (Kumar et al., 2018) is to facilitate rapid prototyping of TE systems. While they arenot deliberately designed with RL in mind, it is possible to extend them to support training and evaluatingRL-based routing policies. These frameworks are limited by their abstract network model which does notmodel packet-level interactions. Our experiments demonstrate that RL routing policies trained in suchenvironments perform significantly worse when tasked to route in more realistic packet-level environments. The simulation frameworks RL4Net (Xiao et al., 2022), its successor RL4NET++ (Chen et al., 2023) andPRISMA (Alliche et al., 2022) are perhaps the closest relatives to PackeRL. They provide the same kind ofclosed interaction loop between RL models and algorithms in Python and packet-level network simulation inns-3. However, in contrast to PackeRL, out-of-the-box support for arbitrary network topologies including linkdatarates and delays is left to the user, and simulation is limited to constant bit-rate traffic between all pairs ofnodes. The latter does not apply to PRISMA, which however only simulates UDP traffic. Moreover, shows that PackeRL runs simulation steps several times faster than the numbers reported in Chen et al.(2023). This may be due to PackeRL leveraging the shared-memory interface of ns3-ai for communicationbetween learning and simulation components, instead of inter-process communication via ZeroMQ (Hintjens,2013). As the authors of ns3-ai noted in Yin et al. (2020), this drastically cuts communication times betweenlearning and simulation components. A common approach to learn RO with deep RL is to infer link weights that are used to compute routingpaths (Stampa et al., 2017; Pham et al., 2019; Sun et al., 2021; Bernrdez et al., 2023). Out of the existingapproaches, only MAGNNETO (Bernrdez et al., 2023) can generalize to unseen topologies by using GNNs intheir policy designs. The caveat of MAGNNETO is the iterative process of pEq steps required to optimize",
  "Preliminaries and Problem Formulation": "We consider wired Internet Protocol (IP) networks using the connection-less User Datagram Protocol(UDP) (Postel, 1980) and the connection-based Transmission Control Protocol (TCP) (Eddy, 2022). TCP inparticular is responsible for the majority of todays internet traffic (Schumann et al., 2022). Routing selects paths in a network along which data packets are forwarded. To determine the best paths, theRouting Protocol (RP)s algorithm uses the network topology as input. For example, Open Shortest-PathFirst (OSPF) (Moy, 1997) propagates each links data rate such that every router knows the entire networkgraph, annotated by data rate. Then, every router uses Dijkstra to compute shortest paths, where by defaultthe path cost is the sum of the inverse of link data rates (Section B.5 explains the path cost calculationprocess). Several network performance indicators exist, such as goodput (i.e. the bitrate of traffic receivedat the destination), latency/delay (the time it takes for data to travel from the source to the destination),packet loss (the percentage of data that is lost during transmission), or packet jitter (the variability inpacket arrival times, which can affect the quality of real-time applications or lead to out-of-order data arrival).Achieving a favorable trade-off between these performance metrics is non-trivial, not least because theirimportance may vary depending on the type/use case of network and the traffic characteristics.",
  "Routing Optimization as an RL Problem": "We formalize RO as a Markov Decision Process with the tuple xS, A, T , ry, splitting the continuous-timenetwork operation into time slices of length sim. The space of network states S consists of attributed graphswith global, node- and edge-level performance and load values, as well as topology characteristics like linkdatarate and packet buffer size. We model network states as directed graphs St pVt, Et, XVt,t, XEt,t, xu,tqwith nodes Vt and edges Et at step t. Node and edge features are given by XVt,t txv,t P RdVt | v P Vtu andXEt,t txe,t P RdEt | e P Etu respectively, and xu,t P RdU denotes global features that are shared betweenall nodes. Sections A.4 and B.4 contain details on how network states are obtained in our framework. Theaction at P A consists of a next-hop neighbor selection v P Nu per routing node u P Vt for each possibledestination node z P Vt, or formally: at tpu, zq v | u, v, z P Vt, v P Nuu. These destination-based routingactions are valid for all packets processed in the upcoming timestep, i.e. the actions are taken in the controlplane (Mestres et al., 2017). The transition function T : S A S evolves the current network state usingthe induced routing actions to obtain a new network state. Its transition probabilities are unknown becauseit depends on the upcoming traffic demands, which are often unpredictable in practice (Wendell & Freedman,2011). Finally, r : S A R is a global reward function which assesses the fit of a routing action in thegiven network topology and state. Here, we use the global goodput, measured as MB received per step, asour reward function. Our goal is to find a policy : S A r0, 1s that maximizes the return, i.e., theexpected discounted cumulative future reward Jt : Epa|sq8k0 krpst`k, at`kq. Thus, in our defaultsetting, an optimal policy maximizes the long-term global goodput. Despite its simplicity, this optimizationobjective provides competitive results. Section C.3 shows that optimizing for different objectives, includingmulti-component objectives, does not improve results consistently.",
  "PackeRL: An Overview": "PackeRL is a framework for training and evaluating deep RL approaches that route packets in IP computernetworks. It interfaces the discrete-event network simulator ns-3 (Henderson et al., 2008) for repeatableand highly configurable RO experiments with realistic network models. It provides a Gymnasium-likeinterface (Towers et al., 2023) to the learning algorithm and provides near-instantaneous communicationbetween learning algorithm and simulation backend by using the shared memory extension ns3-ai (Yin et al.,2020). PackeRL advances RL research for RO in the following ways: It supports optimization for several common objectives that can be combined with adjustableweightings. Nonetheless, both RL and non-RL approaches alike can be evaluated with respect toa range of performance metrics. While we optimize our approach for goodput by default, we showresults for alternative optimization objectives in Section C.3.",
  "It provides access to an extensive range of network scenarios. .1 provides further information": "It implements a closed interaction loop between RL policy and network simulation, using In-BandNetwork Telemetry (Kim et al., 2015) to monitor the network state and provide state snapshots St.The time period simulated per environment step can be arbitrarily large or small, and thus, unlikemost existing frameworks, it supports online RO experiments with or without RL. It can be used to train RL routing policies within a few hours and evaluate them within a fewminutes. Thus we dispel the concerns raised by related work that packet-based environments are tooslow (Ferriol-Galms et al., 2023) or too complex to implement (Kumar et al., 2018). We provide a detailed explanation of PackeRL in Section A. Section A.1 explains the structure of computernetworks in ns-3, Section A.2 expands on the interaction loop between RL policy and environment, Section A.3clarifies how the actions at are installed in the simulated network, and Section A.4 provides details on hownetwork states St are obtained from the network simulation. Finally, general simulation parameters areexplained in Section B.1.",
  "Network Scenario Generation with synnet": "PackeRL offers versatile simulation conditions via synnet, a standalone module for network scenario generation.In synnet, network scenarios consist of the network topology and a set of events. The network topologyconsists of the graph of routing nodes, links between them, as well as parameters like link data rate or delayand packet buffer size. For generating the topologies, we use random graph models commonly found in theliterature (Barabsi, 2009; Erds et al., 1960; Watts & Strogatz, 1998). Events can be of two types: Trafficdemand events contain an arrival time, demand size and type (UDP vs. TCP). We use random modelsto generate demand arrival times and volumes, such that the generated network traffic resembles observedreal-world traffic patterns (Benson et al., 2010). Link failure events consist of a failure time and the edge thatis going to fail. Here, too, we use random models for link failure times that resemble the patterns found inoperative networks (Bogle et al., 2019). Sections A.5 and A.6 provide more details on the scenario generationprocess as well as examples, while Sections B.2 and B.3 contain the parameters used for the random models.",
  "Policy Designs for Routing in Packet-Level Environments": "This section introduces two RL policy designs for RO trainable in PackeRL, namely our next-hop selectionpolicy FieldLines and our adaptation of MAGNNETO which we call M-Slim (short for MAGNNETO-Slim).Our results in show that both M-Slim and FieldLines clearly outperform MAGNNETO, underliningthe benefit of learning to route in PackeRL. Nevertheless, they use different approaches to obtain routingactions at from the given network state St. As illustrated in , M-Slim adopts MAGNNETOsapproach of computing routing paths from inferred link weights, while FieldLines directly selects next-hop",
  "neighbors per destination node v P V and routing node u P V . .2 explains how this circumvents theneed for computing shortest paths on every re-optimization": "The information available in the RO problem can be represented as graphs with node, edge and global features.Thus, GNNs are highly suitable models because their permutation equivariance enables generalization toarbitrary network topologies. While MAGNNETO uses Message Passing Neural Networks (MPNNs) (Gilmeret al., 2017) for its actor module, we use a variant valled Message Passing Networks (MPNs) (Sanchez-Gonzalez et al., 2020; Freymuth et al., 2024) that supports node, edge, and global features. Using MultilayerPerceptrons (MLPs) f l, initial features x0v and x0e with e pv, uq P E, the l-th step is given as",
  "M-Slim: Learning Link-Weight Optimization in Packet-Level Simulation": "states that MAGNNETOs iterative process for obtaining routing paths is too slow to warrantsub-second RO. In fact, the inference times reported in are too high even to follow our trainingprotocol in PackeRL because it involves a larger number of temporally fine-grained interaction steps. Reducinginference time is required to enable training in PackeRL, and to this end we present our MAGNNETOadaptation called M-Slim. While the output of MAGNNETOs actor architecture specifies a set of linkswhose weights shall be incremented, in M-Slim we interpret this output as link weights directly. This reducesthe amount of model inference steps and APSP computations per routing update from pEq to 1 and enablessub-second routing re-optimization in larger network topologies. The change in interpretation of the actors output requires further design adjustments. To account forexploration during training rollouts, we treat the actor output as the mean t for a diagonal GaussianNpt, M-Slimq from which we then sample the actual link weights. M-Slim is a learnable parameter. Duringevaluation, the values of t are used as link weights directly. Finally, we apply the Softplus function (Zhenget al., 2015) to the output of the actor module to ensure the values are positive and thus usable as link weights.Concerning the architecture of the actor module, we replace the MPNN design used by MAGNNETO by anMPN-based design with L 2 steps and parameters as detailed in Appendix B.8 to reduce model complexity.As of Appendix C.2 shows, compared to when using MAGNNETOs MPNN architecture for M-Slim,the MPN design further decreases inference time without compromising on performance. Otherwise, M-Slimuses the same input and output representations as MAGNNETO: They receive the latest Link Utilization",
  "FieldLines: Fast Next-Hop Routing in any Network Topology": "M-Slim greatly reduces computation time per re-optimization step, but it still requires one APSP passper routing update as illustrated in . A single such computation overshadows the computationalcomplexity term OpV Dq of GNN inference (Alkin et al., 2024) for a maximum node degree D as the networkgrows in scale, making the shortest-paths computation the computational bottleneck. We thus turn to ournext-hop selection RO approach FieldLines to further reduce inference times: It utilizes the results of oneinitial APSP computation and only requires re-computing APSP when the network topology changes. FieldLines consists of an actor module and a selection module : The actor module : S R|V ||E| firstcreates an embedding of the network state that it then uses to provide numerical values z,e for each possiblecombination of network edge e P E and destination node z P V . We implement as an MPN with L 2 stepsand parameters as described in Appendix B.8. Intuitively, the output values z,e describe how well e pv, uqis suited as a next-hop edge from node v for packets destined for z. Producing such an output requiresthe network features to contain some form of positional embedding. However, by default, a GNN cannotspatially identify nodes and edges of the input topology in relation to each other due to its permutationequivariance property (Bronstein et al., 2021). Therefore, we provide auxiliary positional information as nodeinput features by calculating APSP once, and then supplying the resulting path distances between nodes.For the path computation, we use the link weights calculated by EIGRP as described in Section B.5. Forhigh-frequency RO, removing the need for APSP on every re-optimization reduces the overall computationalcomplexity. Importantly, the actor module is still topology-agnostic because the topology and positionalinformation is supplied as input, allowing FieldLines to generalize to novel topologies during inference. The selection module : S R|V ||E| A treats the next-hop edge rating supplied by as logits overoutgoing edges per node v P V and destination z. During training rollouts, uses Boltzmann explorationby sampling a next-hop edge from these distributions, using a learnable temparature parameter . Duringevaluation, it simply chooses the maximum probability edges for each pair of routing node v and destinationnode z. In both cases, the resulting next-hop choices correspond to rules in destination-based routing.Importantly, this policy design is able to form non-coalescent routing paths (i.e routing loops) because we donot force coalescence. Our results show that constraining our routing to coalescent paths is not necessary,and there is evidence that, in certain network situations, routing loops can even be beneficial to routingperformance (Brundiers et al., 2021)3.",
  "Experiment Setup": "In our experiments, we consider an episodic RL setting in which every training and evaluation episode comeswith its own network scenario. This includes the network topology as well as traffic demands and link failureevents for the entire length of simulation T. To account for packet-level dynamics in PackeRL, we simulateH 100 steps per episode and set sim, the time simulated per environment step, to 5 ms, which yieldsT 500 ms. This implies that, as opposed to existing fluid-based environments, training and evaluation inPackeRL involves a larger number of temporally fine-grained inference steps. Furthermore, our training andevaluation protocols cover varying network topologies, traffic situations, and the presence of link failures.Since such a setting requires routing algorithms to provide quick updates for arbitrary topologies without theneed for re-training, we do not evaluate approaches that are hand-crafted or fine-tuned to a specific networktopology. Instead, as Bernrdez et al. (2023) have shown that learning useful general-purpose representationsfor RO is possible, we design our evaluation to shed light on the importance of packet-level dynamics forlearning more suitable representations.",
  "Evaluated Routing Approaches": "We compare MAGNNETO, which is trained in a fluid-based environment, to M-Slim and FieldLines, whichare trained in PackeRL. To evaluate MAGNNETO in PackeRL, at each timestep, we feed it the latest TMof sent bytes. Beginning with zero LU and randomly initialized integer link weights, it uses current LUand link weights as edge features and increments one or more link weights. Its fluid-based environmentthen distributes the TMs traffic volumes across the flow network using the paths obtained from an APSPcomputation and obtains new LU values for the next iteration. This way, MAGNNETO uses the initialTM for pEq optimization steps in which it iteratively adjusts the link weights for path computation. Thefinal link weights are used to obtain the actual routing paths communicated to PackeRL. On the contrary,M-Slim and FieldLines are evaluated by doing one deterministic inference pass on the respective policyarchitecture, which directly yields shortest path weights in the case of M-Slim, and node-centric next-hopselections per packet destination in the case of FieldLines. In addition to the learned approaches, we considertwo shortest-path baselines: OSPF is introduced in .1, and EIGRP (Savage et al., 2016) alsoinvolves link delays in its link weight calculation. These two routing protocols are widely adopted becausetheir topology discovery mechanisms allow for routing in any network topology, and the heuristics used forcomputing shortest paths work well in most practical situations. Yet, by default, these protocols are obliviousto varying traffic conditions and network utilization. Therefore, for evaluating the two baselines it is sufficientto compute shortest paths once at the start of the episode and every time the network topology changes. Weuse standard reference values for the path computation process, which is explained in Section B.5. Lastly, weinclude results of two random policies for reference: Random (NH) chooses random next-hop edges for eachpossible destination z P V at each routing node v P V , and Random (LW) uses shortest-path routing withrandomly generated link weights. While Random (LW) corresponds to an untrained MAGNNETO/M-Slimpolicy, Random (NH) corresponds to an untrained FieldLines policy.",
  "Network Scenarios": "We consider five groups of randomly generated topologies of varying scale, generated as described inAppendix A.5. We call these groups the nx family and work with nxXS (610 nodes), nxS (1125 nodes),nxM (2650 nodes), nxL (51100 nodes) and nxXL (101250 nodes). Figures 10 and 11 visualize exampletopologies. We train and evaluate on a grid of combinations of traffic scaling values mtraffic and TCP fractions",
  "Training and Evaluation Details": "We train M-Slim and FieldLines on 16 random seeds for 100 iterations of 16 episodes each. We train FieldLineson the nxXS topologies while restricting the training of M-Slim to the two well-known network topologiesNSFNet and GEANT2, with link datarate values taken from Bernrdez et al. (2023), for better comparabilityto MAGNNETO. For both approaches, we use PPO (Schulman et al., 2017) and refer to Section B.7 forhyperparameter details. Given the episode length H 100, this results in a total of 160 000 training steps pertraining run which, depending on TCP ratio and amount of traffic, take between 3 and 14 hours of training on4 cores of an Intel Xeon Gold 6230 CPU. For FieldLines, the first five iterations are warm-start iterations. Inthese iterations, we replace the actions sampled during rollout by the ones suggested by the EIGRP baseline.We train MAGNNETO on 8 random link weight initialization seeds as per their protocol (Bernrdez et al.,2023), using their fluid-based environment and PPO implementation. The performance values presented inthis work are obtained by taking the mean over 100 evaluation episodes, except for nxXL for which weuse 30 evaluation episodes. For the learned approaches, we exclude non-convergent runs by showing theperformance of the better-performing half of the used random seeds.",
  "Learning to Route in PackeRL": "In , we report results of all evaluated approaches on the nxXS topology preset. EIGRP slightlyoutperforms OSPF in all evaluated scenarios, which is the reason why we do not include OSPF in theremaining results. Furthermore, the two random policies perform significantly worse than all other approaches.Concerning the learned approaches, for MAGNNETO, unlike the results reported in Bernrdez et al. (2023),even the best run does not beat static shortest-paths routing. On the other hand, our policies M-Slim andFieldLines provide routing performance that rivals EIGRP, with consistently lower average packet delays.Also, as further illustrated by , both our policy designs improve goodput and packet drop ratio fornetwork scenarios with high-intensity and TCP-dominated traffic. Unlike , each matrix of displays the values of a single metric and a single approach, but over a grid of traffic setups of varyingintensity and TCP ratio. This way, it becomes evident that the performance FieldLines and M-Slim relativeto EIGRP improves for scenarios with intense and TCP-heavy traffic, and that learned high-frequency RO isparticularly beneficial for such kinds of traffic. On the other hand, for scenarios with low traffic intensity,EIGRP is able to maintain a slightly higher goodput.",
  "Generalizing to Unseen Network Topologies": "shows the results of evaluating above sections policies on the larger topology presets nxS, nxM,and nxXL. Relative to EIGRP, MAGNNETOs performance stays inferior yet stable, but due to theexcessive inference times depicted in .3, we did not evaluate it on topology presets larger than nxM.Interestingly, M-Slim is able to increase its goodput advantage over EIGRP with increasing network scalewhile maintaining solid values for other metrics. On the other hand, FieldLiness performance becomes moreand more similar to that of the EIGRP baseline as the network scale increases. New network topologies can also arise due to failure events. shows results on the nxS topologypreset with randomly generated link failures as described in Section A.6. All learned approaches are ableto adapt in the face of one or more link failures. However, FieldLines and M-Slim seem to behave slightlydifferently upon registering a link failure: While FieldLines favorizes a lower average packet delay, M-Slimmaintains goodput and instead gives away its delay advantage. In any case, both our policy designs drop lessdata after link failures in traffic-intense situations.",
  "Simulation and Inference Speed": "The left side of reports the mean inference time of the evaluated policies on the various networkscales of the nx topology preset. MAGNNETOs optimization loop quickly leads to inference times of severalseconds. The single-pass inference of M-Slim and FieldLines brings down inference time considerably, but forlarge networks the inference time of M-Slim exceeds one second because it requires an APSP pass every step.FieldLines, which only requires that when the topology changes, keeps its inference times in the millisecond",
  "The results shown in earlier parts of this section suggest several findings, which we will discuss in the following": "Firstly, the two random policies shown in perform notably worse than all other approaches. Theperformance of Random (NH) in particular is far off, since it does not check for routing path coalescenceand thus frequently includes routing loops and detours in its actions. This shows that obtaining high-qualityrouting actions is not a trivial task, and that using RL to learn to route is beneficial. Next, we note a contrastbetween the performance of MAGNNETO and M-Slim. While MAGNNETO performs worse than the staticshortest-path baselines (EIGRP), M-Slim outperforms them in all scenarios except those with low-volumetraffic. This shows that placing a routing policy trained in a fluid-based environment into a packet-basedone hurts performance, which is avoidable by directly training in a packet-based environment like PackeRL.In general, both our RL-powered approaches outperform EIGRP in scenarios with more intense traffic. Weassume that congestion events happen more frequently in these scenarios and that the dynamic routingprovided by M-Slim and FieldLines can better deal with this challenge. We also note that the advantageover EIGRP is slightly more pronounced for TCP-heavy traffic. This may be explained by TCPs sendingrate adjustment. TCP actively probes for the maximum sending rate maintainable without loss of data. Thislikely leads to more congestion events, which in turn may be handled better by an adaptive routing policy. .2 suggests that all learned approaches can deal with link failures effortlessly and that theygeneralize to large topologies even though they have been trained on much smaller and/or random topologies.Interestingly, while M-Slim seems to maintain its relative advantage over EIGRP, FieldLines seems toprioritize maintaining low latency over maximum global goodput when faced with link failures. Also, we notethat the advantage of FieldLines over EIGRP disappears with growing network topology size. A possibleexplanation for the generalization behavior of FieldLines is indicated by the Action Fluctuation row of. It denotes the average percentage of next-hop decisions over all routing and destination nodes ratethat change between a timestep and the next. Evidently, the action fluctuation is much lower for FieldLinesthan for M-Slim, which indicates that FieldLines changes its routing actions much less frequently. of the appendix provides an illustrating example for this difference. Moreover, as the network scale increases,the routing of FieldLines becomes even more static. We suspect that this is the reason why, with growingnetwork size, FieldLines performance becomes more and more similar to that of EIGRP. The performancereported for M-Slim in Figures 5 and 6 suggests that re-optimizing routing less conservatively may furtherincrease the performance of FieldLines. We hypothesize that, in order to increase the dynamicity of FieldLines",
  "Conclusion": "This work highlights the importance of packet-level simulation environments for RL-enabled routing optimiza-tion, together with policy designs that are trainable in such environments. Firstly, we show that a recentlyproposed approach trained in a fluid-based network environment fails to reproduce its training performancein packet-based simulation. Adapting the approach to the more realistic packet-based simulation settingnegates the decline in performance and shows why packet-based training and evaluation environments areeven needed. Our adaptation called M-Slim also cuts the time needed to re-optimize routing by one to twoorders of magnitude, but still requires more than a second for large network topologies. Our novel next-hopselection policy design FieldLines is the first of its kind that can optimize routing within milliseconds for anynetwork topology without the need for re-training. Finally, our evaluation setup demonstrates the versatilityof our new packet-level training and evaluation framework PackeRL. It supports training and evaluatingpolicies on a wide range of realistic network topologies and traffic setups, provides closed-loop simulation viaits ns-3 network simulator backend, and facilitates a detailed analysis of RL-based approaches with respectto a handful of well-known performance metrics. With the findings of this work, we hope to inspire futureresearch on RL-enabled routing optimization, and provide a few pointers in the following final subsection.",
  "Future Work": "We have demonstrated the usefulness of PackeRL only for single-path routing but not for multi-path routing.This is due to the absence of standard Multipath TCP (MPTCP) implementations for ns-3 or any otherpopular packet-level network simulator4. As existing implementations of MPTCP for ns-3 are incompletewith respect to the official specification (Nadeem & Jadoon, 2019; Ford et al., 2020), we leave the extensionof PackeRL to multipath routing for future work. In this work, for simplicity, we have evaluated the routing performance when optimizing for a single objectiveor a weighted sum of multiple components with fixed weights. But in different network scenarios, therelative importance of these metrics may vary. Further efforts may investigate the relative importance of theoptimization components in different network scenarios and turn to Multi-Objective RL (Hayes et al., 2022)to train a family of policies that provides better control over performance. Despite the strong results of FieldLines, .4 has shown that its routing becomes increasingly static asnetwork sizes grow, making it gradually lose its advantage. suggests that adding more messagepassing steps to the MPN module does not solve the problem on its own, but revisiting the action-selectionmechanism used during the training of FieldLines may alleviate this issue. Furthermore, despite greatlyreduced inference times, FieldLines still requires several ms to re-optimize routing. This does not influencerouting in our experiments because we pause the environment during action selection as is commonly done inRL research. Long inference times may however degrade routing quality in the even more realistic settingwhere network operation would continue during action inference. Learning sub-second routing optimizationwith RL in such a soft real-time setting (Marchand et al., 2004) is an interesting and challenging avenue forfurther research, not least because hardware specifications will start to influence the routing results. Finally, we note that the RL approaches considered in our experiments were trained and evaluated in a fullycentralized manner. This raises the question: How do we account for the delays incurred by communicatingstate information across the computer network? High-frequency RO policies may need to respect Age ofInformation (AoI) (Yates et al., 2021) when capturing and processing the network state. Here, redistributingrouting control to the nodes could reduce the influence of AoI, as nodes may restrict the topological coverageof input and output to their neighborhoods. It is an open question whether such systems of networked agentscan achieve efficient routing, and if so, how they should be designed and deployed. We believe that buildingon FieldLines next-hop routing design can help to answer this question in future work.",
  "Broader Impact Statement": "Automating computer networks promises to greatly increase operational efficiency and save costs throughover-provisioning or manual configuration. Here, RL-powered RO may become a cornerstone in autonomouscomputer networks. Other infrastructures like road networks or power grids may also benefit from thisprogress, given their structural similarity. Our work opens the door for future research on automation for suchkinds of systems. Anyhow, as for most RL application domains, misusing RL approaches in computer networksfor malicious intents is conceivable. Specifically, the black-box nature of RL-powered RO approaches can beabused to infiltrate the networks decision making, causing disturbances or loss of data if appropriate securitymeasures are not taken. Furthermore, learned routing approaches may put certain kinds of traffic at anunnatural disadvantage in order to optimize overall routing performance. In addition to mindful deploymentby the network engineers, RL-powered networking components should therefore include safeguards againstthe most common attack vectors. Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, GeoffreyIrving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,Dandelion Man, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, JonathonShlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan,Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and XiaoqiangZheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL Software available from tensorflow.org. Mohammad Alizadeh, Tom Edsall, Sarang Dharmapurikar, Ramanan Vaidyanathan, Kevin Chu, AndyFingerhut, Vinh The Lam, Francis Matus, Rong Pan, Navindra Yadav, and George Varghese. CONGA:Distributed congestion-aware load balancing for datacenters. In ACM SIGCOMM Conference 2014, pp.503514, New York, NY, USA, August 2014. Association for Computing Machinery. ISBN 978-1-4503-2836-4.doi: 10.1145/2619239.2626316. URL",
  "Benedikt Alkin, Andreas Frst, Simon Schmid, Lukas Gruber, Markus Holzleitner, and Johannes Brandstetter.Universal physics transformers. arXiv preprint arXiv:2402.12365, 2024": "Redha A Alliche, Tiago Da Silva Barros, Ramon Aparicio-Pardo, and Lucile Sassatelli. PRISMA: A packetrouting simulator for multi-agent reinforcement learning. In 2022 IFIP Networking Conference (IFIPNetworking), pp. 16. IEEE, 2022. Marcin Andrychowicz, Anton Raichuk, Piotr Staczyk, Manu Orsini, Sertan Girgin, Raphal Marinier,Leonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem.What matters for on-policy deep actor-critic methods? a large-scale study. In 8th International Conferenceon Learning Representations (ICLR 2020), October 2020. URL",
  "Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velikovi. Geometric deep learning: Grids,groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021": "Alexander Brundiers, Timmy Schller, and Nils Aschenbruck. On the benefits of loops for segment routingtraffic engineering. In 2021 IEEE 46th Conference on Local Computer Networks (LCN), pp. 3240. IEEE,2021. Gustavo Carneiro, Pedro Fortuna, and Manuel Ricardo. Flowmonitor: A network monitoring frameworkfor the network simulator 3 (ns-3). In 4th International ICST Conference on Performance EvaluationMethodologies and Tools (VALUETOOLS09), pp. 110, 2009. Bo Chen, Di Zhu, Yuwei Wang, and Peng Zhang. An approach to combine the power of deep reinforcementlearning with a graph neural network for routing optimization. Electronics, 11(3):368, January 2022. ISSN2079-9292. doi: 10.3390/electronics11030368. URL Jiawei Chen, Yang Xiao, and Guocheng Lin. RL4NET++: A packet-level network simulation framework fordrl-based routing algorithms. In 8th IEEE International Conference on Network Intelligence and DigitalContent (IC-NIDC 2023), pp. 248253. IEEE, 2023.",
  "Alan Ford, Costin Raiciu, Mark J. Handley, Olivier Bonaventure, and Christoph Paasch. TCP extensionsfor multipath operation with multiple addresses. RFC 8684, 2020. URL": "Niklas Freymuth, Philipp Dahlinger, Tobias Wrth, Simon Reisch, Luise Krger, and Gerhard Neumann.Swarm reinforcement learning for adaptive mesh refinement. Advances in Neural Information ProcessingSystems 2024 (NeurIPS 24), 36, 2024. Thomas M. J. Fruchterman and Edward M. Reingold. Graph drawing by force-directed placement. Software:Practice and Experience, 21(11):11291164, November 1991. ISSN 00380644, 1097024X. doi: 10.1002/spe.4380211102.",
  "Phillipa Gill, Navendu Jain, and Nachiappan Nagappan. Understanding network failures in data centers:Measurement, analysis, and implications. In ACM SIGCOMM Conference 2011, pp. 350361, 2011": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural messagepassing for quantum chemistry. In 34th International Conference on Machine Learning (ICML 2017), pp.12631272. PMLR, 2017. Yingya Guo, Yulong Ma, Huan Luo, and Jianping Wu. Traffic engineering in a shared Inter-DC WAN viadeep reinforcement learning. IEEE Transactions on Network Science and Engineering, 9(4):28702881,July 2022. ISSN 2327-4697. doi: 10.1109/TNSE.2022.3172283.",
  "Pieter Hintjens. ZeroMQ: Messaging for Many Applications. OReilly Media, 2013": "Wanwei Huang, Bo Yuan, Sunan Wang, Jianwei Zhang, Junfei Li, and Xiaohui Zhang. A generic intelligentrouting method using deep reinforcement learning with graph neural networks. IET Communications, 16(19):23432351, 2022. ISSN 1751-8636. doi: 10.1049/cmu2.12487. URL Changhoon Kim, Anirudh Sivaraman, Naga Katta, Antonin Bas, Advait Dixit, Lawrence J. Wobker, et al.In-band network telemetry via programmable dataplanes. In ACM SIGCOMM Conference 2015, volume 15,pp. 12, 2015.",
  "Praveen Kumar, Chris Yu, Yang Yuan, Nate Foster, Robert Kleinberg, and Robert Soul. YATES: Rapidprototyping for traffic engineering systems. In ACM Symposium on SDN Research (SOSR), pp. 17, 2018": "Xuan Mai, Quanzhi Fu, and Yi Chen. Packet routing with graph attention multi-agent reinforcementlearning. In 2021 IEEE Global Communications Conference (GLOBECOM), pp. 16, December 2021. doi:10.1109/GLOBECOM46510.2021.9685941. Audrey Marchand, Maryline Silly-Chetto, and Rue Christian Pauc. Dynamic scheduling of soft aperiodictasks and periodic tasks with skips. In 25th IEEE Real-Time Systems Symposium Work-In-Progress Session,2004. Athina Markopoulou, Gianluca Iannaccone, Supratik Bhattacharyya, Chen-Nee Chuah, Yashar Ganjali,and Christophe Diot. Characterization of failures in an operational IP backbone network. IEEE/ACMTransactions on Networking, 16(4):749762, 2008.",
  "Matt Mathis, Jamshid Mahdavi, Sally Floyd, and Allyn Romanow. RFC2018: TCP selective acknowledgementoptions, 1996": "Alaitz Mendiola, Jasone Astorga, Eduardo Jacob, and Marivi Higuero. A survey on the contributions ofsoftware-defined networking to traffic engineering. IEEE Communications Surveys & Tutorials, 19(2):918953, 2016. Albert Mestres, Alberto Rodriguez-Natal, Josep Carner, Pere Barlet-Ros, Eduard Alarcn, Marc Sol, VictorMunts-Mulero, David Meyer, Sharon Barkai, Mike J Hibbett, et al. Knowledge-defined networking. ACMSIGCOMM Computer Communication Review, 47(3):210, 2017.",
  "Sebastian Orlowski, Roland Wessly, Michal Piro, and Artur Tomaszewski.SNDlib 1.0SurvivableNetwork Design Library. Networks, 55(3):276286, 2010. ISSN 1097-0037. doi: 10.1002/net.20371. URL": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in Neural Information Processing Systems (NeurIPS 19), 32, 2019. Tran Anh Quang Pham, Yassine Hadjadj-Aoul, and Abdelkader Outtagarts. Deep reinforcement learningbased QoS-aware routing in knowledge-defined networking. In Trung Q. Duong, Nguyen-Son Vo, and Van CaPhan (eds.), Quality, Reliability, Security and Robustness in Heterogeneous Systems, Lecture Notes of theInstitute for Computer Sciences, Social Informatics and Telecommunications Engineering, pp. 1426, Cham,2019. Springer International Publishing. ISBN 978-3-030-14413-5. doi: 10.1007/978-3-030-14413-5_2. Pinyarash Pinyoanuntapong, Minwoo Lee, and Pu Wang. Distributed multi-hop traffic engineering viastochastic policy gradient reinforcement learning. In 2019 IEEE Global Communications Conference(GLOBECOM), pp. 16, December 2019. doi: 10.1109/GLOBECOM38437.2019.9013134.",
  "Neil Spring, Ratul Mahajan, and David Wetherall. Measuring ISP topologies with rocketfuel. SIGCOMMComput. Commun. Rev., 32(4):133145, August 2002. ISSN 0146-4833. doi: 10.1145/964725.633039. URL": "Giorgio Stampa, Marta Arias, David Sanchez-Charles, Victor Muntes-Mulero, and Albert Cabellos. Adeep-reinforcement learning approach for software-defined networking routing optimization, September2017. URL Penghao Sun, Zehua Guo, Julong Lan, Junfei Li, Yuxiang Hu, and Thar Baker. ScaleDRL: A scalable deepreinforcement learning approach for traffic engineering in SDN with pinning control. Computer Networks,190:107891, May 2021. ISSN 1389-1286. doi: 10.1016/j.comnet.2021.107891. Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, ManuelGoulo, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierr, SanderSchulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL",
  "Daniel Turner, Kirill Levchenko, Alex C Snoeren, and Stefan Savage. California fault lines: Understandingthe causes and impact of network failures. In ACM SIGCOMM Conference 2010, pp. 315326, 2010": "Asaf Valadarsky, Michael Schapira, Dafna Shahaf, and Aviv Tamar. Learning to route. In 16th ACMWorkshop on Hot Topics in Networks (HotNets 2017), pp. 185191, New York, NY, USA, 2017. Associationfor Computing Machinery. ISBN 978-1-4503-5569-8. doi: 10.1145/3152434.3152441. URL Mowei Wang, Linbo Hui, Yong Cui, Ru Liang, and Zhenhua Liu. xNet: Improving expressiveness and granu-larity for network modeling with graph neural networks. In IEEE Conference on Computer Communications(INFOCOM 2022), pp. 20282037. IEEE, 2022. Ning Wang, Kin Hon Ho, George Pavlou, and Michael Howarth. An overview of routing optimizationfor internet traffic engineering. IEEE Communications Surveys & Tutorials, 10(1):3656, 2008. ISSN1553-877X. doi: 10.1109/COMST.2008.4483669.",
  "Yang Xiao, Jianxue Li, Jiawei Wu, and Jun Liu. On design and implementation of reinforcement learningbased cognitive routing for autonomous networks. IEEE Communications Letters, 27(1):205209, 2022": "Dahai Xu, Mung Chiang, and Jennifer Rexford. Link-state routing with hop-by-hop forwarding can achieveoptimal traffic engineering. IEEE/ACM Transactions on Networking, 19(6):17171730, December 2011.ISSN 1558-2566. doi: 10.1109/TNET.2011.2134866. Zhiying Xu, Francis Y. Yan, Rachee Singh, Justin T. Chiu, Alexander M. Rush, and Minlan Yu. Teal:Learning-accelerated optimization of WAN traffic engineering. In ACM SIGCOMM Conference 2023, pp.378393, New York, NY, USA, September 2023. Association for Computing Machinery. ISBN 9798400702365.doi: 10.1145/3603269.3604857. URL Qingqing Yang, Xi Peng, Li Chen, Libin Liu, Jingze Zhang, Hong Xu, Baochun Li, and Gong Zhang.Deepqueuenet: Towards scalable and generalized network performance estimation with packet-level visibility.In ACM SIGCOMM Conference 2022, pp. 441457, 2022. Roy D. Yates, Yin Sun, D. Richard Brown, Sanjit K. Kaul, Eytan Modiano, and Sennur Ulukus. Age ofInformation: An introduction and survey. IEEE Journal on Selected Areas in Communications, 39(5):11831210, 2021. Hao Yin, Pengyu Liu, Keshu Liu, Liu Cao, Lytianyang Zhang, Yayu Gao, and Xiaojun Hei. ns3-ai: Fosteringartificial intelligence algorithms for networking research. In Proceedings of the 2020 Workshop on ns-3(WNS3), pp. 5764, 2020. Xinyu You, Xuanjie Li, Yuedong Xu, Hui Feng, Jin Zhao, and Huaicheng Yan. Toward packet routingwith fully distributed multiagent deep reinforcement learning. IEEE Transactions on Systems, Man, andCybernetics: Systems, 52(2):855868, February 2022. ISSN 2168-2232. doi: 10.1109/TSMC.2020.3012832.",
  "Chiyuan Zhang, Oriol Vinyals, Remi Munos, and Samy Bengio. A study on overfitting in deep reinforcementlearning. arXiv preprint arXiv:1804.06893, 2018": "Qizhen Zhang, Kelvin K. W. Ng, Charles Kazer, Shen Yan, Joo Sedoc, and Vincent Liu. MimicNet: Fastperformance estimates for data center networks with machine learning. In ACM SIGCOMM Conference2021, pp. 287304, 2021. Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural networks usingsoftplus units. In 2015 International Joint Conference on Neural Networks (IJCNN), pp. 14. IEEE, 2015.",
  "A.1Network Structure and Simulation in ns-3": "Networks in ns-3, by default, consist of nodes and links/connections between nodes, as illustrated in .For modeling simplicity, we limit ourselves to connected network topologies that hold full-duplex Point-to-Point (P2P) connections transmitting data error-free and at a constant pre-specified datarate. Nodesthemselves do not generate or consume data; Instead, applications are installed on nodes that generate datadestined for other applications, or consume the data that is destined for them (red boxes in example networknodes in ). To transport data between nodes we install an Internet Stack on top of each node,adding IP and TCP/UDP components in a way that mimics the OSI reference model (Zimmermann, 1980).Also, nodes do not put data on the P2P link themselves, or read data from it. This is done by the networkdevices (rectangles attached to the nodes in ) that belong to a P2P connection, which are installedas interfaces on the two nodes that are being connected. Upon installation of the Internet Stack, the P2Pconnection between two nodes is assigned an IPv4 address space, with concrete IPv4 addresses given to theincident network devices.",
  "A.2PackeRLs Online Interaction Loop": "PackeRL uses a two-component Python/C++ environment, with inter-component communication realizedby shared memory module of ns3-ai. The Python component provides a Gymnasium-like interface to thelearning loop, while the C++ component is a wrapper and entry point for simulations in ns-3. Initially, thePython environment starts an instance of its C++ counterpart in a subprocess, providing general simulationparameters (c.f. Section B.1) and a network scenario generated with synnet. The C++ environment enters itssimulation loop and first installs the network topology in ns-3: It configures nodes, links and network devicesaccordingly, as well as a TCP and a UDP sink application per node. Then, it starts the ns-3 Simulator thatruns the actual simulation steps. The initial network state S0 is communicated from the C++ component tothe Python environment. Within the episode loop, the Python part provides the current network state St tolearned or baseline policies, and communicates the routing action at to the C++ component alongside theupcoming traffic and link failure events. Source applications are then created according to the upcomingtraffic demands, with sending start times set to the respective demand arrival times. For TCP traffic demands,the source application attempts to send its data as quickly as possible, and we use ns-3s default TCPCUBIC (Ha et al., 2008) to modulate the actual sending rate. Note that we currently do not supportTCP Selective Acknowledgement (SACK) (Mathis et al., 1996) due to a presumed bug in ns-3 that causessimulation crashes. For UDP traffic demands, we use the sending rate provided by the scenario as explainedin Section A.6. The provided routing actions are installed as described in Section A.3. Next, using theSimulator, ns-3 simulates the installed network for a duration of sim: The installed source applications",
  "A.3Installing and Using Routing Actions in ns-3": "Routing in computer network involves two primary tasks: determining the best paths and forwarding thepackets along these paths. For the latter, routers keep a set of routing rules in their memory. Each rulespecifies a next-hop neighbor to which packets with a certain destination are to be forwarded. This rule set -also known as a routing table - gets populated by the installed RP, and we adopt this mechanism for ourwork. Most contemporary IP networks employ destination-based routing, i.e. routers forward packets byfinding the routing table entry that matches the packets destination address, and sending the packet to thenext-hop neighbor specified in that entry. As mentioned in , we adopt the commonly used forwardingmechanism using routing tables stored in the routers memory that may get updated by the RP. Therefore,regardless of the routing actions representation, we need to convert it into a set of routing table rules foreach concerned routing node, where each rule contains a next-hop neighbor preference for a given destinationnode. We do the conversion prior to placing the actions into the shared memory module on the Python side.On the C++ side, the OddRouting module serves as a drop-in replacement for other routing protocols likeOSPF that allows the installation of the provided routing next-hop preferences onto the network nodes. Sincenot all node pairs in a network necessarily communicate with each other, the OddRouting module stores thereceived routing preferences in a separate location on the routing node, and only fills the nodes routingtables on-demand once a packet arrives at a node for which no suitable routing rule exists in its routing table.All subsequent packets destined for the same target node will have access to the newly installed table entryuntil the start of a new timestep, when new routing preferences will be stored in the node and its routingtable will be flushed. Otherwise, OddRouting resembles the other IPv4 RPs implemented in ns-3, leveragingthe line-speed capability of the forwarding plane.",
  "A.4Network Monitoring in ns-3": "In order to efficiently obtain the state of the computer network, we utilize the In-Band Network Telemetrycapabilities provided by ns-3s FlowMonitor module (Carneiro et al., 2009). It utilizes probes installed onpackets to track per-flow statistics such as traffic volume, average and maximum packet delay, and node-levelrouting events down to the IP level. We use this information to obtain TMs of packets/bytes sent and received(for visualization and the MAGNNETO baseline), as well as global average/maximum packet delay andnode-level traffic statistics that form part of the monitored network state S. Moreover, our network topologyis modeled as a graph with edges that consist of physical connections between network devices installed onnodes. FlowMonitor does not capture queueing and drop events happening on the network device level, andwe therefore also report events happening in network devices and channels to obtain edge-level informationon link utilization, packet buffer fill, and bytes/packets sent/received/dropped. Since the P2P connectionsare full-duplex, we model the network monitoring as a directed graph where an edge of the original networktopology is replaced with one edge in each direction. These directed edges contain the state of the respectivesender device, i.e. edge pu, vq contains packet buffer load, link utilization and traffic statistics for trafficbuffered in u flowing to v. At the end of a timestep t, St holds global, node and edge features that reflect theoverall network performance and utilization during timestep t, as well as its load state at the end of timestept. For the edges, we add their datarate, delay and packet buffer capacities to the list of features. For theinitial state S0, all utilization and traffic values are set to zero.",
  "A.5synnet: Generating Network Topologies": "Network topologies vary greatly depending on the scope and use case of the network. For this work, weorientate our scenario generation towards the topologies spanned by the edge routers that connect datacentersin typical Inter-Datacenter Wide Area Networks (Inter-DC WANs). These are usually characterized byloosely meshed powerful edge routers and high-datarate medium-latency links that connect two such routerseach. While we also employ link delay values in the low ms range, we scale down typical datarate valuesfor Inter-DC WANs to lie in the high Mbps range, to speed up simulation times under stress situationswithout loss of generality of the simulation results. For simplicity, we set the packet buffer sizes of networkdevices incident to P2P connections to the product of link datarate and round-trip delay, which is commonthroughout the networking literature (Spang et al., 2022). To generate random network topology graphs, we use the ER model (Erds et al., 1960) and the WSmodel (Watts & Strogatz, 1998), and, up to 50 nodes, the BA (Barabsi, 2009) model. All models areavailable via NetworkX graph analysis package (Hagberg et al., 2008). Figures 10 and 11 show examples forsuch random topology graphs. In any case, nodes and edges are assigned unique integer IDs for identificationpurposes. To add the missing datarate and delay values to the links, we follow the following steps: 1. We first embed the random graph into a two-dimensional plane using the Fruchterman-Reingoldforce-directed algorithm (Fruchterman & Reingold, 1991) to create synthetic positional informationfor the random graphs nodes, similar to the position information provided for nodes in relatednetwork datasets (Orlowski et al., 2010; Knight et al., 2011; Spring et al., 2002). 2. The resulting positional layout is centered around the two-dimensional point of origin, which we useto obtain location weights per node that are inversely proportional to its distance to the origin. Wescale the location weights wp to lie in r1, wpmaxs.",
  "Our process to generate flow-level traffic is inspired by reported traffic characteristics of real-world datacenters (Benson et al., 2010):": "1. Inspired by gravity TMs (Roughan, 2005), we generate a \"traffic potential matrix\" B ccT andrandomly perturb its values by rand. Its values bij describe the expected (not actually measured)relative traffic intensity between each source-destination node pair i, j and will be used for theupcoming demand generation. Diagonal entries are set to 0 to exclude self-traffic, meaning thattraffic demand generation is skipped. 2. We use a flow size tradeoff parameter flow P r0, 1s to balance the frequency and size of arriving flows.Smaller values lead to more but smaller traffic demands, larger values lead to fewer but larger trafficdemands.",
  ". In order to sample inter-arrival times between demands for each pair of nodes i, j, we use a log-logisticdistribution with a fixed shape parameter t 0 and a scale t flow` 1": "mtraffic that uses a traffic scalingparameter mtraffic depending on the evaluated task and class of graph topology. Starting from 0,we sample inter-arrival times for each node pair until we have reached the total simulated time scaledby traffic intensity bijHsim (with H being the episode length, sim being the simulated time perepisode step, and arrival times obtained via cumulative summation of inter-arrival times). Finally,we obtain the actual demand arrival times Hsim per node pair i, j by dividing the generatedarrival times by bi,j, capping at 50 ms. 4. For each generated traffic demand, we sample a demand size using a Pareto distribution. It usesa fixed scale parameter s that also specifies the minimum demand size in bytes, and a shapes sbase ` logp 1",
  "37flow q depending on a shape base parameter sbase that determines the tail weightof the demand size distribution. We cap the demand sizes at 1 TB": "5. A fraction pTCP P r0, 1s of the generated traffic demands is marked as TCP traffic demands, with therest being marked as UDP demands. While the simulation will try to finish TCP demands as quicklyas possible and under the sending rate moderation of TCP, we assign a constant sending rate of 1Gbps for UDP demands of less than 100 KB, and a constant sending rate drawn uniformly fromr1, 5s Mbps for all other UDP demands. For creating link failure events, for each step of the episode, we obtain link failure probabilities from a Weibulldistribution as explained in Section B.3 and draw a boolean sample for each edge that determines whether itwill fail in the upcoming step. In order to maintain the connected-ness of the network topology, we thenrestrict the creation of link failure events to edges that are non-cut at the time of event.",
  "B.1Simulation in ns-3": "We set up the applications to send data packets of up to 1472 bytes, which accounts for the commonly usedIP packet maximum transmission unit of 1500 bytes and the sizes for the IP (20 bytes) and ICMP (8 bytes)packet header. UDP packets thus are 1500 bytes large, whereas TCP may split up data units received fromthe upper layer as required. We set the simulation step duration sim to 5 ms and make each episode lastH 100 steps, simulating a total of T 500 ms per episode.",
  "B.2synnet: Topology Generation": "For the BA model we use an attachment count of 2, and stop using the BA model altogether for networksabove 50 nodes as kurtosis of the node degree distribution becomes too high at that point. For the ER modelwe set the average node degree to 3, and for the WS we choose a rewiring probability of 30% and an attachmentcount of 4. For a deterministic positional embedding of the graphs nodes via the Fruchterman-Reingoldforce-directed algorithm, we set its random seed to 9001. We set the maximum node location weight wpmaxand the maximum node degree weight wdmax to 10, and use a weight tradeoff parameter of c 0.6. We usea base average edge delay value mdelay of 5 ms, minimum and maximum edge datarate values of vmin 50e6and vmax 200e6, and a random perturbation of rand 0.1 (i.e. random perturbation by up to 10%).",
  "B.3synnet: Traffic and Link Failure Event Generation": "For random perturbation, we again use rand 0.1 (i.e. random perturbation by up to 10%). The flow sizetradeoff parameter flow is set to 0.5, the demand interarrival time distribution shape parameter t is set to1.5, the demand size distribution scale parameter s is set to 10 (i.e. demands are at least 10 bytes), and thedistribution shape base parameter is set to sbase 0.4. As per Bogle et al. (2019), we model link failureprobabilities per simulated step as a Weibull distribution Wp, kq, using a shape parameter 0.8 and ascale parameter k 0.001. illustrates the resulting probability distributions for traffic demandsand link failures. : Cumulative probabilities for demand interarrival times (left, log-logistic distribution) and demandsizes in bytes (right, Pareto distribution). The red points at the end of the curves denote the cumulativeprobability at 50 ms and 1 GB.",
  "The following features are monitored in our ns-3 simulation:": "global:maximum link utilization (maxLU P r0, 1s), average datarate utilization avgTDU P r0, 1s,average packet delay avgPacketDelay P R`, maximum packet delay maxPacketDelay P R`, averagepacket jitter avgPacketJitter P R`, globally sent/received/dropped/retransmitted bytes in N0. edge:link utilization LU P r0, 1s, maximum relative packet buffer fill txQueueMaxLoad P r0, 1s,relative packet buffer fill at end of simulation step txQueueLastLoad P r0, 1s, packet buffer capacityin N`, channel datarate and delay in N`, sent/received/dropped bytes in N0.",
  "node:sent/received/retransmitted bytes in N0": "By default, we do not use the node features in our experiments. This is because our feature ablation inSection C.5 shows that including this information globally and on an edge level is enough and leads to thebest performance. Consequently, for our experiments we have dU 9, dE 10 and dV 0 for the monitorednetwork state Gt. Also, we normalize all input features akin to Schulman et al. (2017). Finally, we stack thefour latest observations pSt3, St2, St1, Stq, using zero-value padding.",
  "B.6Line Digraphs": "For a directed graph G pV, Eq, its Line Digraph (Harary & Norman, 1960) G1 pE, Pq is obtained bytaking the original edge set E as node set, and connecting all those new nodes that, as edges in G, form adirected path of length two: P tppu, vq, pw, xqq|pu, vq, pw, xq P E, v wu. Both MAGNNETO and our adaptation M-Slim operate on the Line Digraph of the network state St. Thismeans that edge input features like LU and past link weight are provided as node features in S1t and thatM-Slim outputs link weight values as node features.",
  "B.7PPO": "Given the episode length H 100, each training iteration of 16 episodes by default uses 1600 sampledenvironment transitions to do 10 update epochs with a minibatch size of 400. We multiply the value lossfunction with a factor of 0.5, clip the gradient norm to 0.5 and use policy and value clip ratios of 0.2 asper Schulman et al. (2018). We use a discount factor of 0.99 and use GAE 0.95 for GeneralizedAdvantage Estimation (Andrychowicz et al., 2020). We model the value function baseline that ProximalPolicy Optimization (PPO) uses for variance reduction as separate network that is defined analogous to therespective policy, but uses a mean over all outputs to provide a single value estimate of the global observation.While MAGNNETO is implemented and trained in Tensorflow (Abadi et al., 2015), its PPO algorithm usesthe same scaling and clipping parameters except for an additional entropy loss mixin of 0.001.",
  "B.8Policy Architectures and Implementation": "The original implementation of MAGNNETO uses an MPNN design (Gilmer et al., 2017) implemented inTensorflow that consists of L 8 message passing layers as described in Bernrdez et al. (2023). Each layeruses a 2-layer MLP with a hidden size 128 and an output size of 16 for the message processing function mpq,a concatenation of min and max for the message aggregation operation , and a final node feature update hpqthat is realized by a 3-layer MLP with hidden dimensions 128 and 64. The base dimensionality of the featurevectors hi is 16, meaning that every input and output of the MLP is of that size. At the start of inference,MAGNNETO uses zero-value padding to reach 16 dimensions since the input consists of only two features(LU and past link weights). After L message passing steps, a final readout function yields the actions peredge. They specify which link weights should be incremented by one. The readout function is implementedas a 3-layer MLP with latent dimensions 128 and 64 and dropout layers with a rate of 50% after the firsttwo MLP layers. Except for the final readout layer, the weights for all MLP layers are initialized with anorthogonal matrix with a gain of?",
  ": Illustration of the MPN architecture used in our routing approaches M-Slim and FieldLines": "For M-Slim and FieldLines, we use a more compact policy architecture leveraging the MPN structurevisualized in . We use 2 message passing steps, the mean and min aggregation function in parallel for, and LeakyReLU for all activation functions. Moreover, we apply layer normalization (Ba et al., 2016) andresidual connections (He et al., 2016) to node and edge features independently after each message passing.For all feature update blocks, we use MLPs with 2 layers and a hidden layer size of 12. In our experiments,the auxiliary distance measure provided to the readout of the FieldLines actor module is the sum of EIGRPlink weights for the shortest path from i to j. Finally, for the learnable softmax temperature used duringexploration by FieldLiness selector module , we use an initial value of 4. For the learnable standarddeviation M-Slim, we use an initial value of 1. We implement our policy modules in PyTorch (Paszke et al.,2019) and use the Adam optimizer with a learning rate of 5e-5 (Kingma & Ba, 2014) for FieldLines, and3e-3 for M-Slim.",
  "CAblation Studies": "In this section we report results for additional experiments that represent ablation studies on our policydesign FieldLines. Except for Section C.4, all experiments are run on the nxXS topology preset. Defaulthyperparameter values are mentioned and explained in Section B. We do the ablations on 8 random seedseach, and report results on the better half of them.",
  "C.1Learning Settings": "shows results for learning hyperparameter ablations. The first two columns per matrix show resultsfor different starting values for the learnable temperature parameter . While a higher starting temperaturedoes not significantly change performance, a lower starting temperature leads to inconsistent improvementsand deteriorations across the metrics. The third and fourth columns per matrix show that a notably higherlearning rate leads to a collapse in performance, but also that an even lower learning rate is not neededbecause it does not improve performance. The last two columns show that a lower discount factor does notimprove performance, while a higher discount factor incurs minimal performance losses in intense UDP traffic.",
  "C.2Architecture Ablations": "Figures 15 and16 shows results for architectural ablations on the FieldLines policy design. The first twocolumns of show variations on the latent dimension used by the MLPs within the MPN, which inour main experiments is set to 12. The numbers do not show a clear benefit of either a smaller or a largerlatent dimension, but given the worse average delay for UDP traffic of lesser intensity, we keep the latentdimension of 12 even though for the evaluated scenarios a latent dimension of 6 may be enough. The lasttwo columns show results when using either one of the minimum or mean aggregation functions for thepermutation-invariant aggregation of the MPNs node feature update. The overall performances are verysimilar, such that using e.g. only the mean function for to reduce complexity is conceivable. Concerning thedepth of the used MPN architecture, shows that using L 2 message passing layers provides thebest overall results, but the best performing random seeds are very close across all MPN depths. Interestingly,for small graphs and high-volume UDP traffic as well as for larger graphs and medium-volume TCP traffic,average performance across multiple random seeds decreases for MPNs with more layers, because a growingnumber of seeds fails to converge properly. Also, as the action fluctuation numbers show, adding moremessage passing layers does not solve FieldLiness problem of increasingly static routing for larger graphtopologies. The M-Slim architecture used in this work is considerably smaller than the original MPNN architectureused for MAGNNETO (which is detailed in Section B.8), since this further improves performance whilefurther decreasing inference time. shows results for architectural ablations on M-Slims policyarchitecture. The figures M-Like column displays results of an M-Slim policy that uses the original MPNN-based policy architecture of MAGNNETO. While being worse than the default architecture for M-Slim, itstill outperforms MAGNNETO clearly, particularly for TCP traffic. This demonstrates that most of theimprovement is achieved by leveraging PackeRLs packet-level dynamics during training. also showsthat reducing the number of message passing layers or the latent dimensionality further reduces inferencetimes but compromises routing performance in some traffic settings. On the other hand, increasing the latentdimensionality or layer count increases inference time but does not improve routing performance.",
  "C.3Optimizing for Different Objectives": "Recent deep RL-powered RO approaches optimize for varying objectives. For example, they have maximizedthroughput (Fu et al., 2020), or minimized maximum LU (Bernrdez et al., 2023; Chen et al., 2022), packetdelay/latency Guo et al. (2022); Sun et al. (2021) or drop counts (Fu et al., 2020). Therefore, to assess thevalidity of our chosen reward function, presents results for FieldLines on the nxXS preset with",
  "C.4Training on Different Topologies": "Furthermore, we investigate how the choice of training topologies affects FieldLines routing performance.For this, we evaluate three models on the nxS topology preset that have been trained on different topologies.In addition to the default model which is trained on nxXS, we train a model on nxS and another onjust the two topologies NSFNet and GEANT2, configured in the same way as in Bernrdez et al. (2023). shows that training on just NSFNet and GEANT2 yields minimally worse results, implying thatcovering a wide range of different network topologies in the training procedure may be beneficial. On theother hand, training on larger topologies does not improve performance and is therefore not preferred due tolonger training times (up to 37 hours to simulate the same amount of training steps).",
  "C.5Feature Importance": "Finally, shows results for FieldLines when restricting or adjusting the policys access to somefeatures. Overall, while the individual interactions between available features and policy performance arecomplex, the results generally show that all monitored global and edge features are relevant to some extent,and that removing global or edge features generally decreases performance in some of the evaluated settings.Adding node-level sent/received/retransmitted features (first two columns of the plot) does not improveperformance, and in conjunction with a larger latent dimension leads to notable deterioration. Removingedge features, in general, decreases performance in most settings. Interestingly, some edge feature removalscan come with improvements in the average delay, and discarding edge features altogether is the next bestapproach. Concerning global features, all features are relevant to some extent, but the combined removal of"
}