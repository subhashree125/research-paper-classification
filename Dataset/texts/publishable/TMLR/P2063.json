{
  "Abstract": "Detecting out-of-distribution (OOD) samples is a critical task for reliable machine learning.However, it becomes particularly challenging when the models are trained on long-taileddatasets, as the models often struggle to distinguish tail-class in-distribution samples fromOOD samples. We examine the main challenges in this problem by identifying the trade-offsbetween OOD detection and in-distribution (ID) classification, faced by existing methods.We then introduce our method, called Representation Norm Amplification (RNA), whichsolves this challenge by decoupling the two problems. The main idea is to use the norm ofthe representation as a new dimension for OOD detection, and to develop a training methodthat generates a noticeable discrepancy in the representation norm between ID and OODdata, while not perturbing the feature learning for ID classification. Our experiments showthat RNA achieves superior performance in both OOD detection and classification comparedto the state-of-the-art methods, by 1.70% and 9.46% in FPR95 and 2.43% and 6.87% inclassification accuracy on CIFAR10-LT and ImageNet-LT, respectively. The code for thiswork is available at",
  "Introduction": "The issue of overconfidence in machine learning has received significant attention due to its potential to produceunreliable or even harmful decisions. One common case when the overconfidence can be harmful is when themodel is presented with inputs that are outside its training distribution, also known as out-of-distribution(OOD) samples. To address this problem, OOD detection, i.e., the task of identifying inputs outside thetraining distribution, has become an important area of research (Nguyen et al., 2015; Bendale & Boult,2016; Hendrycks & Gimpel, 2017). Among the notable approaches is Outlier Exposure (OE) (Hendryckset al., 2019a), which uses an auxiliary dataset as an OOD training set and regulates the models confidenceon that dataset by regularizing cross-entropy loss (Hendrycks et al., 2019a), free energy (Liu et al., 2020),or total variance loss (Papadopoulos et al., 2021). These methods have proven effective and become thestate-of-the-art in OOD detection. However, recent findings have highlighted the new challenges in OODdetection posed by long-tailed datasets, characterized by class imbalance, which are common in practice. EvenOE and other existing techniques struggle to distinguish tail-class in-distribution samples from OOD samplesin this scenario (Wang et al., 2022b). Thus, improved methods are needed to tackle this new challenge. To address OOD detection in long-tail learning, we need to solve two challenging problems: (i) achieving highclassification accuracy on balanced test sets, and (ii) distinguishing OOD samples from both general andtail-class in-distribution (ID) data, which is underrepresented in the training set. While previous works haveexplored each problem separately, simply combining the long-tailed recognition (LTR) methods (Kang et al.,",
  "Published in Transactions on Machine Learning Research (08/2024)": "Jaewoo Park, Jacky Chen Long Chai, Jaeho Yoon, and Andrew Beng Jin Teoh. Understanding the featurenorm for out-of-distribution detection. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pp. 15571567, 2023. Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S Kaseb, Kent Gauen, Ryan Dailey,Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, et al. Dynamic sampling in convolutional neuralnetworks for imbalanced data classification. In 2018 IEEE conference on multimedia information processingand retrieval (MIPR), pp. 112117. IEEE, 2018.",
  "Related works": "OOD detection in long-tail learningWhile substantial research has been conducted in OOD detectionand long-tail learning independently, the investigation of their combined task, referred to as LT-OOD, hasemerged only recently. The LT-OOD task encompasses three main lines of research: training methodologies(Wang et al., 2022b; Choi et al., 2023), post-hoc scoring techniques (Jiang et al., 2023), and abstention classlearning (Miao et al., 2023; Wei et al., 2023) for LT-OOD tasks. PASCL (Wang et al., 2022b) first addressed OOD detection in the context of long-tail learning, introducinga contrastive learning idea to improve the separability between ID tail classes and OOD data. BEL (Choiet al., 2023) tackled the LT-OOD task, highlighting class imbalances in the distribution of auxiliary OOD set.They proposed a method to regularize auxiliary OOD samples from majority classes more heavily duringenergy-based regularization. However, recent methods like PASCL and BEL employ the two-branch technique,where each branch is tailored for OOD detection and classification, respectively. In contrast, our method doesnot use such two-branch technique. Instead, we effectively address both challenges through a single model,leveraging the idea to disentangle ID classification and OOD detection, as will be observed in .2. Jiang et al. (2023) tackled the LT-OOD task using a post-hoc method, adapting traditional OOD scoringto the long-tailed setting. Despite its effectiveness, this approach relies on model bias induced by classimbalances in the training dataset. Thus, it is hard to be combined with training methodologies aimed atresolving class imbalances to improve the classification accuracy. Recently, abstention class-based methods tailored for LT-OOD tasks have emerged, including EAT (Wei et al.,2023) and COCL (Miao et al., 2023). EAT integrats four techniques: multiple abstention classes, CutMix(Yun et al., 2019) augmentation for tail-class ID data, the auxiliary branch fine-tuning, and ensemble models.COCL combines two techniques: debiased large margin learning and outlier-class-aware logit calibration.Debiased large margin learning entails training networks with two contrastive loss terms: one for tail-classID prototypes and OOD samples, and the other for head-class ID samples and OOD samples. While thesemethods offer substantial improvements, they require a comprehensive approach with multiple techniques,and face challenges with OOD representations collapsing into the same class despite semantic differences. Norm-based OOD detectionPrevious studies have investigated the utilization of norms for OODdetection scores. Dhamija et al. (2018) pioneered the use of representation norms for OOD detection, with atraining method, Objectosphere, directly attenuating representation norms of auxiliary OOD data duringtraining to increase separation between ID and OOD data in the feature space. However, training with RNAloss proves more effective than directly attenuating OOD representation norms to reduce activation ratiosand representation norms (see .3.1). CSI (Tack et al., 2020) incorporated norms into their scoringmethod, observing that the gap in representation norms between ID and OOD data merges implicitly incontrastive learning. NAN (Park et al., 2023) and Block Selection (Yu et al., 2023) also utilized feature normsfor OOD detection, considering deactivated feature coordinates and selecting the most suitable hidden layerfor norm utilization, respectively. While these approaches leverage representation norms for OOD scoring,they lack a training method aimed at widening the gap between the norms of ID and OOD data. In contrast,our method not only advocates for using representation norms for OOD detection in long-tail context butalso propose a corresponding traning method. There also exist techniques to address overconfidence issue for ID data by controlling the norm of logits.ODIN (Liang et al., 2018) uses temperature scaling for softmax scores at test time and LogitNorm (Weiet al., 2022) normalizes logits of all samples during training. However, these methods do not consider theunderconfidence issue of tail classes. In long-tail learning, networks often produce underconfident outputsfor tail predictions. Thus, solving both the overconfidence problem of OOD data and the underconfidenceproblem of tail ID data is crucial. Our method tackles this challenge by amplifying the norms of only IDrepresentations, while indirectly reducing the representation norms of OOD data by BN statistics updatesusing ID and OOD data. More reviews of related works are available in Appendix A.",
  "OOD detection": "In this section, we formally define the task of OOD detection. Consider a classification model that takesimages as input and outputs predicted labels, consisting of a representation extractor f with learnableparameters and a linear classifier W (see a). The model is trained on a labeled dataset {(xi, yi)}i tooptimize such that f(xi) = yi for all i. In-distribution data is sampled from the training data distribution,while out-of-distribution data comes from a different distribution. The main challenge of OOD detection taskis that overparameterized deep neural networks tend to be overconfident in their predictions for OOD data.This overconfidence hinders the models ability to distinguish between ID and OOD samples. OOD samples are detected using a decision function G, which employs an OOD scoring function S. Thefunction S assigns a scalar score to each sample, indicating how the sample is likely to be an OOD data. Thedecision function G is defined as:",
  "where is a threshold": "Outlier Exposure (OE) (Hendrycks et al., 2019a) is an effective training methodology for OOD detection, usingan auxiliary OOD set. The OOD data in this set are from a distribution disjoint with the distribution of bothID and the test OOD data. OE trains the model as regulating the output softmax vectors of auxiliary OODsamples to have a uniform distribution over classes. The detailed loss function is explained in .1.",
  "Long-tail learning": "In real-world scenarios, datasets often exhibit imbalanced class distributions, leading to models biased towardsclasses with a larger number of samples. This issue becomes more pronounced as the imbalance increases.Long-tailed datasets is defined imbalanced datasets where a few classes (head) have many samples and manyclasses (tail) have few samples. Long-tailed datasets typically have extremely imbalanced label distributions,often following an exponential distribution. Addressing this problem has led to extensive research in long-taillearning, as detailed in Appendix A. Logit Adjustment (LA) (Menon et al., 2021) is a prominent method for tackling the long-tail learning task.The rationale behind LA is to adjust the model to be Bayes-optimal for balanced test sets by subtracting thelogarithm of the label probability from the logits. This is formulated as follows:",
  "= arg maxy[C]wy f(x)y log y,": "where S(W f(x))y is the softmax output for label y given the weight matrix W = [w1, w2, . . . , wC], C is thenumber of classes, [C] := {1, . . . , C}, and C is the estimate of the label distribution over C classes.In the formulation above, S(W f(x))y represents the output of the biased model trained on an imbalanceddataset. To achieve a Bayes-optimal classification model for a balanced distribution during training, thelogits are adjusted by adding the logarithm of the label probability. The LA loss is defined as:",
  "Motivation: trade-offs between OOD detection and long-tailed recognition": "We first examine the OOD detection in long-tailed recognition to understand the main challenges. Specifically,we observe trade-offs between the two problems, when the state-of-the-art OOD detection method, OutlierExposure (OE) (Hendrycks et al., 2019a), is simply combined with the widely used long-tail learning method,Logit Adjustment (LA) (Menon et al., 2021). OE exposes auxiliary OOD samples during training to discourageconfident predictions on rare OOD samples by enforcing a uniform logit for OOD samples, while LA encouragesconfident predictions on rare tail-class ID samples by applying a label frequency-dependent offset to eachlogit during training. In , we compare the performance of models trained with the cross-entropy (CE) loss, CE+OE loss(OE), LA, LA+OE, and PASCL (Wang et al., 2022b), which is a recently developed method for OODdetection in long-tailed recognition by using the contrastive learning (CL) idea, combined with LA+OE. Wereport the results of ResNet18 trained on CIFAR10-LT (Cui et al., 2019), when the performance is evaluatedfor an OOD test set, SVHN (Netzer et al., 2011). FPR95 is an OOD detection score, indicating the falsepositive rate (FPR) for ID data at a threshold achieving the true positive rate (TPR) of 95%. FPR95 can becomputed for each class separately, and we report the FPR95 scores averaged over all classes (Avg.) and thebottom 33% of classes (Few) (in terms of the number of samples in the training set), and similarly for theclassification accuracies. Comparing the results for CE vs. CE+OE and for LA vs. LA+OE, we can observe that OE increases the gapbetween the softmax confidence of ID and OOD data, which makes it easier to distinguish ID data from OODdata by the confidence score, resulting in the lower FPR95. However, compared to the model trained withLA, the model trained with LA+OE achieves a lower classification accuracy, reduced by 1.98% on averageand 6.25% for the Few class group. One can view LA+OE as a method that trades off the OOD detection performance and ID classificationaccuracy, especially for tail classes, since it achieves worse FPR95 but better classification accuracy comparedto the OOD-tailored method, CE+OE, while it achieves better FPR95 but worse accuracy than the LTR-tailored method, LA. PASCL, which uses contrastive learning to push the representations of the tail-classID samples and the OOD auxiliary samples, improves both the FPR95 and the classification accuracy forthe Few class group, each compared to those of CE+OE and LA, respectively. However, it still achieves aslightly lower average classification accuracy (-0.82%) than the model trained only for LTR by LA loss. The main question is then whether such trade-offs between OOD detection and long-tailed recognitionare fundamental, or whether we can achieve the two different goals simultaneously, without particularlycompromising each other. All the existing methods reviewed in aim to achieve better OOD detection,long-tailed recognition, or both by enforcing the desired logit distributions for ID vs. OOD data or headvs. tail classes, respectively. However, we observe the limitations of such approaches in achieving the twopossibly conflicting goals simultaneously through the logit space, even with the help of contrastive learning.",
  "Previous ways of exposing auxiliary OOD data during training": "Let DID and DOOD denote an ID training dataset and an auxiliary OOD dataset, respectively. The state-of-the-art OOD detection methods expose the auxiliary OOD data during training to minimize the combinationof a classification loss LID for ID data and an extra loss LOOD for OOD data, i.e., L = LID + LOOD forsome tunable parameter > 0. As an example, one can choose the regular softmax cross-entropy loss as theID loss, LID = E(x,y)DIDlog",
  "c[C] exp(wc f(x))wy f(x), where f(x) RD is the representation": "vector of the input data x and wc RD is the classifier weight (i.e., the last-layer weight) for the label c [C].For long-tailed recognition, the LA method (Menon et al., 2021) uses the logit-adjusted loss to improve theclassification accuracy of ID data, i.e., LID = LLA in Equation 2. On the other hand, the loss for OOD trainingdata is often designed to control the logit distribution or the softmax confidence of the OOD samples. Forexample, the OE method minimizes the cross entropy between the uniform distribution over C classes and thesoftmax of the OOD sample, i.e., LOOD = ExDOODlog",
  "c[C]1Cwc f(x)": "OE perturbs tail classification: gradient analysisWe explain why training with OOD data to minimizethe loss of the form L = LID +LOOD can lead to a degradation in classification accuracy, particularly for tailclasses. Let T = {(xi, yi), xi}Bi=1 be a training batch sampled from the sets of ID training samples {(xi, yi)}and unlabeled OOD samples {xi}, respectively. For simpleness of analysis, we assume a fixed feature extractorf() and examine how ID/OOD samples in the batch affect the updates of the classifier weights {wc}Cc=1.Note that the gradient of the loss L = LID + LOOD on the batch T with respect to the classifier weight wc is",
  "where yci = 1 if yi = c and 0 otherwise. For LID = LLA, S(W f(xi))c in the first term is replaced by thesoftmax output for the logits adjusted by the label-dependent offsets as in Equation 2": "From Equation 3, the gradient is a weighted sum of the representations of ID data {f(xi)}Bi=1 and OODdata {f(xi)}Bi=1, where the weights depend on the gap between the softmax output S(W f(xi))c and thedesired label, yci for ID data and 1/C for OOD data, respectively. Thus, the gradient balances the effects ofID samples, tuned to minimize the classification error, and those of OOD samples, tuned to control theirconfidence levels. For a tail class t, however, the proportion of ID samples with the label yi = t is relativelysmall. As a result, during training, the gradient for the classifier weight wt of the tail class is dominatedby the representations of OOD samples rather than those of the tail-class ID samples, particularly at theearly stage of training when the label distribution for OOD samples deviates significantly from a uniformdistribution. In a, we plot the log-ratios (log(wcLID1/wcLOOD1)) of the gradient normsbetween ID and OOD samples with respect to the classifier weights of bottom 3 (tail) classes vs. top 3 (head)classes. Classifier weights for tail classes are more heavily influenced by OOD samples than those for headclasses, especially in the early stages of training, resulting in a greater degradation of classification accuracy,",
  "(b) Activation ratio at the last ReLU layer": ": (a) The gradient ratio of ID classification loss to OOD detection loss with respect to the classifierweight of LA+OE model trained on CIFAR10-LT, i.e. log(wcLID1/wcLOOD1). In particular, LLAand LOE are used as LID and LOOD, respectively. Note that the log-ratio for tail classes is less than zero atthe early stage of training, indicating that the gradient update is dominated by OOD data rather than IDdata. (b) The activation ratio of ID and OOD representations at the last ReLU layer in the models trainedby RNA, LA, and OE. CIFAR10 and SVHN are used as ID and OOD sets, respectively.",
  "Proposed OOD scoring and training method with auxiliary OOD data": "Representation Norm (RN) scoreWe present a simple yet effective approach to expose OOD dataduring training to create a discrepancy in the representation between ID and OOD data. The main strategy isto utilize the norm of representations (i.e., feature vectors). Note that the representation vector f(x) can bedecomposed into its norm f(x)2 and direction f(x) = f(x)/f(x)2. Simply scaling the norm f(x)2of the representation without changing its direction does not change the class prediction, since for any scalingfactor s > 1, we have arg maxc[C] S(W sf(x))c = arg maxc[C] S(W f(x))c. We thus leverage the normof representations to generate a discrepancy between the representations of ID vs. OOD data, while notenforcing the desired logit distribution for OOD data. This approach, termed as Representation Norm (RN)score, is formulated asSRN(x) = f(x)2,(4)",
  "for a test sample x. Following the convention for OOD detection, samples with larger scores are detected asOOD data, while those with smaller scores are considered as ID data": "Representation Norm Amplification (RNA)We propose a training method to obtain desired repre-sentations for ID vs. OOD data. Our goal in the representation learning is twofold: (i) to make the norms ofthe representations for ID data relatively larger than those of OOD data, and (ii) to learn the representationsthat achieve high classification accuracy even when trained on long-tailed datasets. To achieve the first goal,we propose Representation Norm Amplification (RNA) loss LRNA:",
  "(b) Values before the last BN layer": ": (a) During training, RNA uses both ID and auxiliary OOD data. The network parametersare updated to minimize the classification loss LLA (Equation 2) of the ID samples, regularized by theirrepresentation norms through the RNA loss LRNA (Equation 5). The OOD data only indirectly contributeto the updating of the model parameters, as being used to update the running statistics of the BN layers.(b) This illustration represents the latent values of ID (red) and OOD (blue) samples before the last BNlayer of the RNA-trained model. The error bars denote the maximum and minimum values among the latentvectors averaged over ID and OOD data, respectively. Through the training, the running mean of the BNlayer (black) converges between the ID and OOD values. After passing through the last BN and ReLUlayers (before the classifier), the shaded region beneath the BN running mean is deactivated. RNA effectivelygenerates a noticeable gap in the activation ratio at the last ReLU layer and the representation norm betweenID vs. OOD data, which serves as the basis for our OOD score.",
  "(1 + hi2).(7)": "The gradient norms are inversely proportional to representation norms. Consequently, during the trainingprocess, the magnitude of updates to model parameters decreases as ID representation norms increase. Thisbehavior promotes convergence of the training procedure. Further details on training dynamics can be foundin Appendix E.9. We then combine this loss with the logit adjustment (LA) loss LLA (Menon et al., 2021),tailored for long-tail learning, and define our final loss function as below:",
  "where > 0 is a hyperparameter balancing the two objectives. We set = 0.5 for all the experiments in": "It is important to note that we only use ID data to optimize the loss function (Equation 8) by gradientdescent. OOD training data do not provide gradients for model parameter updates, but instead are simplyfed into the model as shown in a. We use OOD data to regularize the model by updating the runningstatistics of the BN layers from both ID and OOD data. OOD samples contribute to updating only therunning mean and variance of BN layers, causing changes to the latent vectors of ID data. However, theOOD representations do not directly participate in the loss or gradient for the classifier weights. Thus, OODdata only indirectly influences parameter updates. In the rest of this section, we will explain why forwardingOOD data is sufficient to regularize the model to produce relatively smaller representation norms of OODdata compared to those of ID data, when the total loss is regularized by LRNA. Effect of auxiliary OOD data: regularizing the activation ratioIn common deep neural networkarchitectures, BN layers are often followed by ReLU layers (Nair & Hinton, 2010). BN layers normalize inputsby estimating the mean and variance of all the given data. Consequently, the coordinates of input vectorswith values below the running mean of the BN layer are deactivated as they pass through the next ReLU",
  "(b) RN histogram of RNA": ": (a) The histogram of representation norms of ID/OOD test data on LA-trained models on CIFAR10-LT. The red bars represent the density of representation norms of ID data and the blue bars represent thatof OOD data (SVHN). (b) The histogram of representation norms of ID/OOD test data on RNA-trainedmodels on CIFAR10-LT. The evident gap in the distributions of representation norms of ID and OOD dataenables effective OOD detection using representation norms. layer (b). In b, we compare the activation ratio (the fraction of activated coordinates) atthe last ReLU layer (prior to the last linear layer) for ID data (CIFAR-10), and OOD data (SVHN), usingmodels trained by RNA, LA, and OE. RNA exhibits distinct activation ratio patterns, clearly separatingID and OOD data. RNA activates the majority of the coordinates of representation vectors of ID data(66.08% on CIFAR10) while deactivating those of OOD data (4.17% on SVHN). This indicates that BNlayer regularization using both ID and OOD data, along with enlarging the ID representation norms throughLRNA, results in a substantial discrepancy in the activation ratio at the last ReLU layer. Moreover, since theactivation ratio corresponds to the fraction of non-zero entries in the representation vector, a notable gapin the activation ratio signifies a difference in representation norms between ID and OOD data, facilitatingreliable OOD detection by the RN score (Equation 4). a and 4b present histograms of representationnorms of ID vs. OOD data when trained by LA and RNA, respectively. The evident gap between ID and OODtest sets is shown only for the RNA-trained models, enabling effective OOD detection with representationnorms. Additional RN histograms for RNA-trained models on other pairs of ID and OOD test sets areavailable in Appendix E.10. The training dynamics of RNA, including loss dynamics of LA loss (Equation 2)and RNA loss (Equation 5), as well as representation norm dynamics of head and tail data in ID training/testdatasets, are provided in Appendix E.9.",
  "Experimental setup": "Datasets and training setupFor ID datasets, we use CIFAR10/100 (Krizhevsky, 2009) and ImageNet-1k(Deng et al., 2009). The long-tailed training sets, CIFAR10/100-LT, are built by downsampling CIFAR10/100(Cui et al., 2019), making the imbalance ratio, maxc N(c)/ minc N(c), equal to 100, where N(c) is the numberof samples in class c. The results for various imbalance ratios, including balanced scenario, are reportedin . We use 300K random images (Hendrycks et al., 2019a) as an auxiliary OOD training set forCIFAR10/100. For ImageNet, we use ImageNet-LT (Liu et al., 2019) as the long-tailed training set andImageNet-Extra (Wang et al., 2022b) as the auxiliary OOD training set. For experiments on CIFAR10/100, we use the semantically coherent out-of-distribution (SC-OOD) benchmarkdatasets (Yang et al., 2021) as our OOD test sets. For CIFAR10 (respectively, CIFAR100), we use Textures(Cimpoi et al., 2014), SVHN (Netzer et al., 2011), CIFAR100 (respectively, CIFAR10), Tiny ImageNet (Le &Yang, 2015), LSUN (Yu et al., 2015), and Places365 (Zhou et al., 2018) from the SC-OOD benchmark. ForImageNet, we use ImageNet-1k-OOD (Wang et al., 2022b), as our OOD test set. All the OOD training setsand OOD test sets are disjoint. Further details on datasets can be found in Appendix D.3. We train ResNet18(He et al., 2015) on CIFAR10/100-LT datasets for 200 epochs with a batch size of 128, and ResNet50 (He",
  "et al., 2015) on ImageNet-LT for 100 epochs with a batch size of 256. We set = 0.5 in (8). More details onthe implementation are available in Appendix D.1": "Evaluation metricsTo evaluate OOD detection, we use three metrics the area under the receiveroperating characteristic curve (AUC), the area under the precision-recall curve (AUPR), and the false positiverate of ID samples at the threshold of true positive rate of 95% (FPR95). For ID classification, we measurethe total accuracy (ACC) and also the accuracy averaged for a different subset of classes categorized asMany, Medium, and Few in terms of the number of training samples. All the reported numbers are theaverage of six runs from different random seeds.",
  "Results": "Results on CIFAR10/100The results on CIFAR10/100-LT are summarized in , where the OODdetection performances are averaged over six different OOD test sets. We compare against several baselines,including MSP (Hendrycks & Gimpel, 2017), OECC (Papadopoulos et al., 2021), EngeryOE (Liu et al., 2020),OE (Hendrycks et al., 2019a), and two recently published methods tailored for LT-OOD tasks, PASCL (Wang",
  "Additional experiments": "We perform a series of ablation study to investigate our method from multiple perspectives. Our evaluationsinvolve variants of RNA training loss, diverse combinations of OOD training and scoring methods, differentimbalance ratios, RNA-trained models with different configurations of auxiliary OOD sets, and calibrationperformance. Further ablation studies are available in Appendix E, where we include the results about therobustness to in Equation 8 (E.2), trade-offs in models trained by LA+OE (E.3), the size of auxiliaryOOD sets (E.4), the training batch ratios of ID and OOD samples (E.5), alternative auxiliary OOD sets(E.6), the structure of the projection function h (E.7), and OOD fine-tuning task (E.8).",
  "Variants of RNA training loss": "We conduct an ablation study on variants of the RNA loss. Our proposed RNA loss (LID-amp.) amplifiesID representation norms during training. We examine two alternative variants: a loss to attenuate OODrepresentation norms, denoted to as LOOD-att., and a combination of both losses. The OOD-attenuationloss is defined as LOOD-att. = ExDOOD [log(1 + h(f(x)))], which is the negative counterpart of the RNA",
  "RNA (ours)91.7391.9692.9073.9374.0075.0661.2761.5075.55": "loss in Equation 5. As shown in , these variants are not as effective as the original RNA loss onCIFAR10-LT. As shown in , the combination of attenuation loss and amplification loss widens the gapin representation norm and activation ratio compared to the case of using only the attenuation loss. However,the amplification loss alone creates a more substantial gap than the combination loss. In particular, thecombination loss succeeds in enlarging the ID norms, but whenever attenuation loss is used (either alone orwith amplification loss), the resulting OOD norm is not as low as the case of using only the amplification loss. The rationale behind this result lies in the optimization process. When solely employing the amplificationloss (RNA loss), the optimizer amplifies the norm h(f(x)) for ID data, necessarily by promoting highactivation ratios for ID data while reducing the activation ratio for OOD data (as low as 2% in our experiment),due to the BN regularization using both ID and OOD data. However, applying the attenuation loss (eitheralone or in combination form) does not necessarily suppress activations for OOD samples in the featurecoordinates to minimize the loss. Instead, the loss can be minimized by mapping the OOD features into itsnull space. Consequently, the resulting activation ratios and the representation norms for OOD samples arenot as low as the case of using only the amplification loss.",
  "Scoring methods": "We evaluate the effects of our training method (Equation 8) and scoring method (Equation 4) separately. presents the mean AUC over six OOD test sets for models trained with different objectives: CE,OE, PASCL, BEL and RNA, and evaluated with various OOD scoring methods: MSP, Energy, and RN.On CIFAR10-LT, the RNA-trained model achieves the highest AUC for MSP and RN scoring, and thesecond-best for Energy scoring. On CIFAR100-LT, RNA ranks the first with MSP and second with Energyand RN. This can be attributed to the fact that RNA training indirectly increases the confidence of ID databy amplifying representation norms. Consequently, RNA maintains competitive AUC performance even withMSP or Energy scoring, which rely on the confidence level. Additionally, our RN scoring method performscomparably to MSP or Energy scoring for the models trained by OE, PASCL, and BEL. While these methodsregularize the confidence of auxiliary OOD samples, they lead to a significant gap in representation normsbetween ID vs. OOD data (as shown in b), which allows our RN score to detect OOD samples. The experiments with ImageNet-LT show different trends. The RNA-trained model exhibits a lower AUCwith confidence-based scores (MSP and Energy) compared to the models trained using OE or PASCL.However, when RN scoring is used, all the training methods (OE, PASCL, BEL and RNA) achieve significantimprovements of 10.22%, 9.28%, 10.47%, and 14.05% over Energy scoring, respectively. This highlights thesuperiority of the representation-based OOD scoring method (RN), especially for a large scale dataset.",
  "Imbalance ratio": "In , we present the AUC and ACC metrics for the models trained on CIFAR10/100-LT with differentimbalance ratios. Higher ratios indicate greater imbalance, posing a more challenging training scenario. Aratio of 1 represents a balanced dataset, not a long-tailed one. Our proposed method consistently outperformsPASCL in both AUC and ACC across the imbalance ratios of 1, 10, 50, 100, and 1000, including a balanced",
  "RNA (ours)12.560.4917.670.4112.650.42": "training set. On the other hand, when trained on CIFAR100-LT, RNA improves ACC across all the imbalanceratios, but it exhibits relatively lower AUC when the imbalance ratio is 1 or 10. These results shows thesuperiority of our model for various imbalance ratios of the training set, especially for high imbalance ratios.",
  "Use of auxiliary OOD data": "In our ablation study, we investigate the impact of incorporating auxiliary OOD data in our method. Theresults in reveal that employing the RNA loss without any auxiliary data leads to a modest AUCimprovement of merely 2.89% over the baseline MSP approach. In contrast, the combination of RNA losswith BN regularization, including auxiliary OOD samples, yields a substantial gain of 20.22%. This clearlyhighlights the significant role of BN regularization using auxiliary OOD data, coupled with the RNA loss. We also explore the potential for eliminating the need for auxiliary samples. We conduct an experimentwherein the auxiliary OOD set is replaced with augmented images derived from the original training samples.We employ a random cropping augmentation strategy spanning 5% to 25% of the original image size. Thisstrategy yields a notable AUC gain of 10.89% over the MSP baseline and an 8.00% gain compared to notutilizing any auxiliary samples but applying RNA loss. While not matching the performance gains of ouroriginal method with the auxiliary OOD set, this suggests the potential for replacing auxiliary OOD sampleswith augmented training samples in applying our method.",
  "We assess the calibration performance of our method on CIFAR10/100-LT and ImageNet-LT. To measure cali-bration, we utilize Expected Calibration Error (ECE), defined as ECE := Mm=1|Bm|": "n |ACC(Bm) Conf(Bm)|,where M and n are the number of bins and test data, respectively, Bm := {x : (m1)/M < Conf(x) m/M}represents the confidence bin, and ACC(Bm) and Conf(Bm) denote average accuracy and confidence withinthe bin Bm, respectively. In , we report the measured ECE values for models trained with CE, OE,PASCL, and RNA. The models are trained on the long-tailed datasets, and the ECE values are evaluatedon the balanced test sets. RNA achieves the highest ECE values for CIFAR10 and ImageNet. This resultdemonstrates that our method exhibits strong calibration performance for ID data.",
  "Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minorityover-sampling technique. Journal of artificial intelligence research, 16:321357, 2002": "Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distributiondetection using outlier mining. In Machine Learning and Knowledge Discovery in Databases. ResearchTrack: European Conference, ECML PKDD 2021, Bilbao, Spain, September 1317, 2021, Proceedings, PartIII 21, pp. 430445. Springer, 2021. Hyunjun Choi, Hawook Jeong, and Jin Young Choi. Balanced energy regularization loss for out-of-distributiondetection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pp. 1569115700, June 2023.",
  "Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure.Proceedings of the International Conference on Learning Representations, 2019a": "Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning canimprove model robustness and uncertainty. Advances in neural information processing systems, 32, 2019b. Y. C. Hsu, Y. Shen, H. Jin, and Z. Kira. Generalized odin: Detecting out-of-distribution image withoutlearning from out-of-distribution data. In 2020 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 1094810957, 2020. doi: 10.1109/CVPR42600.2020.01096. Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shiftsin the wild. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in NeuralInformation Processing Systems, 2021.",
  "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift, 2015": "Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Detecting out-of-distribution data through in-distribution class prior. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th InternationalConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1506715088.PMLR, 2329 Jul 2023. Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalantidis.Decoupling representation and classifier for long-tailed recognition. In Eighth International Conference onLearning Representations (ICLR), 2020.",
  "Bingyi Kang, Yu Li, Sa Xie, Zehuan Yuan, and Jiashi Feng. Exploring balanced feature spaces for representationlearning. In International Conference on Learning Representations, 2021": "Julian Katz-Samuels, Julia B Nakhleh, Robert Nowak, and Yixuan Li. Training OOD detectors in theirnatural habitats. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, andSivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pp. 1084810865. PMLR, 1723 Jul 2022. Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020.",
  "Yifei Ming, Ying Fan, and Yixuan Li. Poem: Out-of-distribution detection with posterior sampling. InInternational Conference on Machine Learning. PMLR, 2022": "Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings forout-of-distribution detection? In The Eleventh International Conference on Learning Representations,2023. Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for generalizableout-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,pp. 52165223, 2020.",
  "Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. InInternational Conference on Machine Learning, 2010": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits innatural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and UnsupervisedFeature Learning 2011, 2011. Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 427436, 2015. Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang. Outlierexposure with confidence control for out-of-distribution detection. Neurocomputing, 441:138150, jun 2021.doi: 10.1016/j.neucom.2021.02.007.",
  "Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors.ICML, 2022": "Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive learningon distributionally shifted instances. In Advances in Neural Information Processing Systems, 2020. Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang, Changqing Yin, and Junjie Yan.Equalization loss for long-tailed object recognition. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 1166211671, 2020. Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set fornonparametric object and scene recognition.IEEE Transactions on Pattern Analysis and MachineIntelligence, 30(11):19581970, 2008. doi: 10.1109/TPAMI.2008.128.",
  "Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logitmatching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,2022a": "Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know whatthey dont know? In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances inNeural Information Processing Systems, 2021. Haotao Wang, Aston Zhang, Yi Zhu, Shuai Zheng, Mu Li, Alex J Smola, and Zhangyang Wang. Partial andasymmetric contrastive learning for out-of-distribution detection in long-tailed recognition. In InternationalConference on Machine Learning (ICML), pp. 2344623458, 2022b.",
  "Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a large-scaleimage dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015": "Yeonguk Yu, Sungho Shin, Seongju Lee, Changhyun Jun, and Kyoobin Lee. Block selection method for usingfeature norm in out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1570115711, June 2023. Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 60236032, 2019. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million imagedatabase for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(6):14521464, 2018. doi: 10.1109/TPAMI.2017.2723009. Jianggang Zhu, Zheng Wang, Jingjing Chen, Yi-Ping Phoebe Chen, and Yu-Gang Jiang. Balanced contrastivelearning for long-tailed visual recognition. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 69086917, 2022a. Yao Zhu, YueFeng Chen, Chuanlong Xie, Xiaodan Li, Rong Zhang, Hui Xue', Xiang Tian, bolun zheng, andYaowu Chen. Boosting out-of-distribution detection with typical features. In S. Koyejo, S. Mohamed,A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems,volume 35, pp. 2075820769. Curran Associates, Inc., 2022b.",
  "In this section, we provide more detailed review on some of the related works, discussed in Sec. 2": "OOD detectionOOD scores are typically evaluated based on output confidence (Bendale & Boult, 2016;Hendrycks & Gimpel, 2017; Liang et al., 2018; Liu et al., 2020), feature space properties (Lee et al., 2018b;Dong et al., 2021; Sun et al., 2021; Song et al., 2022; Sun & Li, 2022; Sun et al., 2022; Vaze et al., 2022; Wanget al., 2022a; Ahn et al., 2023; Djurisic et al., 2023; Yu et al., 2023; Zhu et al., 2022b), or gradients (Huanget al., 2021). Many methods have been developed to encourage the separability of ID and OOD data by thesescores, for example by exposing auxiliary OOD data during training and regularizing their confidence levels(Malinin & Gales, 2018; Hein et al., 2019; Liu et al., 2020; Mohseni et al., 2020; Papadopoulos et al., 2021;Yang et al., 2021; Katz-Samuels et al., 2022; Ming et al., 2022), or by promoting the ID-OOD separability inthe embedding space (Dhamija et al., 2018; Lee et al., 2018a; Hendrycks et al., 2019b; Choi & Chung, 2020;Hsu et al., 2020; Tack et al., 2020; Sehwag et al., 2021; Wang et al., 2021; Ming et al., 2023). However, thesemethods often suffer from performance degradation in a long-tail learning setup (Wang et al., 2022b). Long-tail learningLong-tail learning addresses the classification problem when trained on imbalanced orlong-tailed class distributions. When trained on imbalanced datasets, the model is often biased towards themajority classes, generating underconfident predictions on the minority or tail classes. There have been manymethods to fix this problem by over- or under-sampling (Kubat et al., 1997; Chawla et al., 2002; Pouyanfaret al., 2018; Kang et al., 2021), adjusting margins or decision thresholds (Cao et al., 2019; Tan et al., 2020;Menon et al., 2021), or modifying the loss functions (Menon et al., 2021; Cui et al., 2021; Li et al., 2022; Zhuet al., 2022a). In particular, some methods attempt to increase the value of the logit for the tail classes by thepost-hoc normalization of the weight of the classifier (the last linear layer) (Kang et al., 2020), or by applyinga label frequency-dependent offset to each logit during training (Menon et al., 2021). While these approacheshave shown empirical effectiveness and statistical robustness in the context of long-tail learning, they oftenconflict with the principle of out-of-distribution (OOD) detection methods, which regulate the confidence ofmodels on rare samples to prevent overconfident predictions (Hendrycks et al., 2019a; Papadopoulos et al.,2021). OOD detection and controlling overconfidenceThere exist OOD detection methods that do notuse auxiliary outlier data during training, but effectively mitigate the overconfidence issue of the models.For example, LogitNorm (Wei et al., 2022) characterizes that training with cross-entropy loss causes logitnorms to keep increasing as the training progresses, and this often leads to overconfident predictions of themodels on both in-distribution and OOD test data. Therefore, the LogitNorm method normalizes the logitof every input data during training to prevent the logit norm from increasing too much. Some recentlypublished works, Rectified Activations (ReAct) (Sun et al., 2021) and Directed Sparisification (DICE) (Sun& Li, 2022), on the other hand, focus on the fact that on a model trained only on in-distribution data, OODdata still activate a non-negligible fraction of units in the penultimate layer, where the mean activation isbiased towards having sharp positive values and there are noisy units with high variances of contributionto the class output. Thus, these methods use post-hoc activation truncation (Sun et al., 2021) or weightsparsification (Sun & Li, 2022), to avoid overconfident predictions on OOD data. OOD detection with abstention classOOD detection methods can be categorized into score-basedmethods and methods incorporating abstention classes. Score-based methods employ specific OOD scoringmetrics to measure the OOD-ness of test samples such as MSP (Hendrycks & Gimpel, 2017), Energy (Liuet al., 2020), and our proposed method, RN. RNA and all the baseline methods including MSP (Hendrycks& Gimpel, 2017), OECC (Papadopoulos et al., 2021), EnergyOE (Liu et al., 2020), OE (Hendrycks et al.,2019a), PASCL (Wang et al., 2022b), and BEL (Choi et al., 2023) belong to the score-based OOD detectioncategory. On the other hand, OOD detection methods with an abstention class use networks with additionalclass for OOD data, referred to as abstention classes. Consequently, the network outputs (C + 1)-dimensionalvectors, where C is the number of classes. During training, these methods often employ classification lossto categorize auxiliary OOD samples into the abstention class. These methods faced challenges that OODrepresentations tended to collapse, being treated as the same class, despite dissimilarities in image semantics",
  "BBroader impacts and limitations": "Mitigating OOD uncertainty is crucial, particularly in safety-critical applications such as medical diagnosisand autonomous driving. In these domains, the ability to effectively identify and handle OOD samples isessential to ensure the reliability and safety of the systems. By addressing the challenges associated withOOD detection, we can significantly enhance the trustworthiness and applicability of deep learning models inthese critical domains. Our proposed method specifically focuses on tackling the challenging scenarios where reliable OOD detectionneeds to be performed on models trained with long-tailed distributions, which is a common occurrence inpractical settings but has received limited attention in previous research. By introducing our method, weenhance the safety and reliability of deploying deep models, even when trained on imbalanced datasets thathave not been extensively preprocessed for class balance. This enables the application of deep learningmodels in real-world scenarios, where long-tailed distributions are prevalent, without compromising theirOOD detection capabilities. Our proposed method does have some limitations. Firstly, it relies on the availability of an auxiliary OODdataset for effective performance. We address this issue by investigating the possibility of replacing theauxiliary OOD dataset by augmented training dataset as reported in . Additionally, as shown in , our method demonstrates suboptimal performance on the near OOD testset, where the OOD data is semantically similar to the training data. This limitation arises because therepresentation distributions of the near OOD test data are closer to the in-distribution training data ratherthan the auxiliary OOD data, resulting in some test OOD samples having large representation norms. Thisvulnerability to near OOD data can be alleviated by employing confidence-based OOD scoring methods.However, it entails a trade-off in OOD detection performance for far OOD data, as discussed in Sec. E.1. Toaddress this limitation, future research should focus on developing training methods that can enhance theOOD detection performance specifically for near OOD test sets in long-tail learning scenarios. By overcomingthis challenge, we can further strengthen the robustness and practical applicability of OOD detection methodsin more broad real-world settings.",
  "D.1Implementation details": "We train ResNet18 (He et al., 2015) on CIFAR10/100-LT datasets for 200 epochs with a batch size of 128.We optimize the model parameters with Adam optimizer (Kingma & Ba, 2014) with an initial learning rateof 0.001 and we decay the learning rate with the cosine learning scheduler (Loshchilov & Hutter, 2017). Theweight decay parameter is set to 0.0005. We train ResNet50 (He et al., 2015) on ImageNet-LT for 100 epochs with a batch size of 256. We optimize themodel parameters with SGD optimizer with a momentum value of 0.9 and an initial learning rate of 0.1, andwe decay the learning rate with the cosine learning scheduler. The weight decay parameter is set to 0.0005. We set the balancing hyperparameter = 0.5 as the default value. We do not use the learnable affineparameters and of all BN layers in the model, since they are trained to amplify both ID and OOD databy converging to large values. During training, the batch consists of an equal number of ID and OOD datasamples. For example, a batch size of 256 means that it contains 256 ID samples and 256 OOD samples.",
  "D.3Datasets": "For the CIFAR benchmark, we employ 300K random images as an auxiliary OOD training set followingHendrycks et al. (2019a). 300K random images is a subset of 80M Tiny Images dataset (Torralba et al.,2008). The selection process for the 300K random images ensures that the image classes are disjoint withthose in the CIFAR10 and CIFAR100 datasets. For the experiments on ImageNet-1k, we use ImageNet-Extra (Wang et al., 2022b) as an auxiliary OODtraining set, and ImageNet-1k-OOD, published in Wang et al. (2022b), as our OOD test set. ImageNet-Extrais created by sampling 517,711 data points from 500 randomly chosen classes in ImageNet-22k, where theclasses are disjoint from the classes in ImageNet-1k. ImageNet-1k-OOD contains 50,000 OOD test images",
  "In this section, we summarize the main baseline methods that we compare to our method in the experiments": "Outlier Exposure (OE) (Hendrycks et al., 2019a): OE is a training method specificallydesigned for out-of-distribution (OOD) detection, with the objective of amplifying the discrepancyin softmax confidence between in-distribution (ID) and OOD data. It leverages an auxiliary OODdataset effectively during the training process to enhance the models ability to distinguish betweenID and OOD samples. The OE method involves constructing training batches that consist of bothID training samples and auxiliary OOD samples. By combining these samples, the method aimsto minimize a composite loss function that includes both the classification loss for ID samples andan additional loss term for OOD samples. This additional loss term is defined as the cross-entropybetween the uniform distribution and the softmax probabilities of the OOD samples. Energy OE (Liu et al., 2020): EnergyOE (Liu et al., 2020) is a variant of the OE methodthat offers an alternative approach to controlling the confidence levels of OOD samples. Instead ofregularizing the cross-entropy loss of auxiliary OOD samples, EnergyOE maximizes the free energyof OOD samples. By maximizing the free energy, EnergyOE aims to increase the uncertainty andreduce the confidence associated with OOD samples, facilitating their detection and differentiationfrom in-distribution samples. Outlier Exposure with Confidence Control (OECC) (Papadopoulos et al., 2021):OECCis another variant of the OE method, which controls the total variation distance between the networkoutput for OOD training samples and the uniform distribution, regularized by Euclidean distancebetween the training accuracy and the average confidence on the models prediction on the trainingset. Partial and Asymmetric Supervised Contrastive Learning (PASCL) (Wang et al., 2022b):PASCL (Wang et al., 2022b), which is a recently developed method for OOD detection in long-tailedrecognition, uses the ideas from supervised contrastive learning (Khosla et al., 2020), combined withLogit Adjustment (LA) (Menon et al., 2021) and outlier exposure (OE) (Hendrycks et al., 2019a), toachieve both high classification accuracy and reliable OOD detection performance. In particular, thismethod applies the partial and asymmetric contrastive learning that pushes the representations oftail-class in-distribution samples and those of OOD training samples, while pulling only tail-classin-distribution samples of the same class. In addition, in the second stage, the Batch Normalization(BN) (Ioffe & Szegedy, 2015) layers and the last linear layer are fine-tuned using only ID trainingdata in order for the running mean and standard deviation of BN layers to be re-fit to the IDdata distribution only, since the BN layers are fitted to the union of ID and auxiliary OOD datadistribution before the second stage. Balanced Energy Learning (BEL) (Choi et al., 2023): BEL (Choi et al., 2023) is an enhancedalgorithm derived from Energy OE (Liu et al., 2020). Energy OE (Liu et al., 2020) trains the modelto output smaller energy for ID samples and larger energy for auxiliary OOD samples. While bothmethods share the scheme of energy regularization for OOD detection, BEL (Choi et al., 2023)weighs auxiliary OOD samples individually based on the pseudo-label of each sample. Initially, theprior label distribution is estimated on the auxiliary OOD set by the model undergoing training.Subsequently, in the loss term and energy margin, auxiliary OOD samples estimated as head classare more emphasized. This approach effectively achieves superior OOD detection performance, butit still requires auxiliary fine-tuning stage for achieving high classification accuracy in long-tailedscenarios, akin to PASCL (Wang et al., 2022b).",
  "RNA (ours)67.561.2491.972.0153.930.5553.680.4168.150.5088.970.2770.710.56": ", 9 and 10 present the AUC, AUPR and FPR95 metrics, respectively, for the models trained usingRNA and other baseline approaches across the six OOD test sets. These results collectively demonstrate theOOD detection performance. Specifically, in , RNA achieves the best results for all the six OOD testsets when the model is trained on CIFAR10-LT. For CIFAR100-LT, RNA achieves the best results on fourOOD test sets as well as on average, except for CIFAR10 and Tiny ImageNet, which are semantically moresimilar to the training set (CIFAR100-LT) than the other OOD test sets. In , our proposed methodexhibits the highest AUPR values across all the OOD test sets when trained on CIFAR10-LT. Similarly, for",
  "RNARN62.6380.1974.33": "all but the CIFAR10 test set, RNA achieves the highest AUPR values when trained on CIFAR100-LT. In our proposed method shows the best FPR95 values across all but the CIFAR100 test sets whentrained on CIFAR10-LT. On the other hand, while RNA achieves the lowest FPR95 values for SVHN, LSUN,and Places365 when trained on CIFAR100-LT, it is outperformed by PASCL in terms of the average FPR95value across the OOD test sets. Near OOD detection poses a significant challenge in the field of OOD detection. In these scenarios, the OODtest sets closely resemble the ID datasets. RNA method exhibits suboptimal performance for near OODdetection as shown in , 9 and 10. This is particularly noticeable when using CIFAR100-LT as the IDtraining set and CIFAR10 as the OOD test set. In contrast, other methods like OE with MSP score exhibitrelatively robust performance in such settings. We attribute this observed trend to the inherent characteristics of the CIFAR10 and CIFAR100 datasets.While these datasets consist of disjoint classes, certain classes share overlapping features. For instance,presenting data from the dog class of CIFAR10 OOD test set to a CIFAR100-trained model may activatefeatures associated with CIFAR100 classes like fox, raccoon, skunk, otter, and beaver. Consequently,the logits corresponding to these multiple classes may simultaneously have high values, potentially leading toa low MSP value. However, the representation norm might be large due to these related features. For suchan example, the near OOD sample may be detected by MSP but incorrectly categorized as an ID sample bythe RN score. To support this hypothesis, illustrates the OOD detection performance (AUC) fortwo OOD scoring methods, MSP and RN, when applied to the RNA-trained model on CIFAR100-LT. Asanticipated, using MSP leads to an improvement in near OOD detection performance, but at the cost ofworsening far OOD detection.",
  "(b) CIFAR100-LT": ": The ID classification performance (ACC) and OOD detection performance (FPR95) of modelstrained by LA+OE. FPR95 is averaged over six OOD test sets as in .1. The models are trainedwith the training loss, LLA + LOE, where denotes the balancing hyperparameter. The results illustratethe trade-offs inherent in combining long-tail learning methods (LA) and OOD detection methods (OE).",
  "E.2Hyperparameter robustness": "Our proposed training method uses the combination of the two loss terms, the classification loss and the RNAloss regularizing the representation norm of input data, as shown in Equation 8. The relative importance ofthe two loss terms can be tuned with the hyperparameter . Although all the results in the main paper areobtained with the fixed value of = 0.5, we extensively explore the performance results when models aretrained with varying the hyperparameter for CIFAR10-LT, CIFAR100-LT, and ImageNet-LT. In ,we plot the AUC and ACC performances for 10 values of = 0.1, 0.2, . . . , 1.0. For CIFAR10/100-LT andImageNet-LT, the AUC values (blue lines) are robust to the change in when is greater than 0.2, althoughthey are much lower when is small. On the other hand, the classification accuracy (red lines) decreasesas increases when trained on CIFAR10-LT and CIFAR100-LT, and this tendency is more dramatic whentrained on CIFAR100-LT. For ImageNet-LT, the accuracy does not change significantly as values change.",
  "E.3Trade-offs in LA+OE-trained models": "We explore the influence of LA and OE in LA+OE models by adjusting the balancing parameter in theloss function, LLA + LOE. presents the results of LA+OE models trained on CIFAR10-LT andCIFAR100-LT with various values of 0.01, 0.05, 0.1, 0.5, and 1.0. Across both datasets, LA+OE-trainedmodels exhibit superior ID classification performance but inferior OOD detection performance with small ,while showing superior OOD detection performance but inferior ID classification performance with large .These findings highlight the trade-offs between ID classification and OOD detection performance.",
  "300k92.9029.1878.7366.90": "We present the performance results of RNA-trained models varying the size of auxiliary OOD training setson CIFAR10-LT in . The OOD detection and ID classification performance remain comparable whenthe number of OOD samples is 3k, 30k, or 300k. However, when the number of OOD samples is reduced to300, which is approximately 2.42% of the number of ID training samples, the AUC and ACC drop rapidlyby 23.35% and 34.36%, respectively. In conclusion, RNA-trained models achieve comparable performancein both OOD detection and ID classification when the number of OOD samples is at least a quarter of theID samples. However, the OOD samples are too few, the performance of the RNA-trained model degradessignificantly.",
  "In , we present experiments varying the OOD sample size. Our method demonstrates worse FPR95performances with smaller OOD sample sizes": "Our approach detects OOD data based on representation norms, which are proportional to the gap betweenlatent vectors before the last BN layer and the running mean of the last BN layer. So, we will check thegaps with varying the OOD sample size. Let ID and OOD denote the mean of the latent vectors before thelast BN layer for ID and OOD samples, respectively, and BN denote the running mean of the last BN layer.After training with RNA, we observe that ID > BN > OOD, as shown in b, which implies thatmany ID representations are activated, while many OOD representations are deactivated after RNA training.",
  "For superior OOD detection based on representation norms, ID representation norms should be relativelylarger than OOD representation norms. Thus,IDBN": "BNOOD is proportional to the OOD detection performance.At different ratio of ID to OOD, different number of ID or OOD samples accounts for calculating BN, sinceBN is the mean of all the ID and OOD samples in the batch. Therefore, as the OOD sample size decreases,BN gets closer to ID than OOD, so the ratioIDBN",
  "E.6Alternative auxiliary OOD training set": "We additionally conduct experiments employing ImageNet-RC as an auxiliary OOD dataset for CIFAR10-LTID dataset. The results are presented in . Our results demonstrate the effectiveness of the RNA, whichconsistently outperforms both the OE and PASCL, in terms of OOD detection and ID classification. Whilethe AUC and AUPR metrics exhibited comparable performance across the three methods, the FPR95 metricindicated RNAs superiority, surpassing OE and PASCL by 4.55% and 6.11%, respectively. Furthermore, RNAdemonstrats a 4.00% enhancement in ID classification accuracy compared to OE, and a 0.89% improvementover PASCL. These results validate RNAs effectiveness not only when trained with the 300K random imagesbut also when employed with other OOD auxiliary sets.",
  "h(f(x))65.142.5345.900.93": "We conduct an ablation study to evaluate the impact of architectural variations, specifically focusing on thestructure of the projection function h. In the proposed approach, we employ a 2-layer MLP configuration forh. As alternative configurations, we train the model using RNA loss on CIFAR10-LT, where in Equation 5,h(f(x)) is replaced either by the feature norm f(x) or the linearly projected norm W h f(x). Theresults presented in indicate that the structure of the projection function h is not a critical elementin our method, as the OOD detection and ID classification performances remain comparable across the modelvariants. The reason we use the 2-layer MLP projection function in our proposed approach is to regulatethe situations where the feature norm of ID samples increases excessively during training. Nevertheless, ourfindings indicate that employing the feature norm itself within the logarithmic term adequately prevents thedivergence of f(x), as shown in and .2.",
  "E.8Fine-tuning task with auxiliary OOD data": "In the fine-tuning task for OOD detection (Hendrycks et al., 2019a; Liu et al., 2020; Papadopoulos et al.,2021), a classification model is initially trained without the OOD set and then later fine-tuned with the OODset to improve its OOD detection ability. We evaluate the effectiveness of our training method when applied to the fine-tuning task. The pre-trainedmodels are obtained by training a model on CIFAR10/100-LT and ImageNet-LT, respectively, using onlythe classification loss, either CE or LA. We then train this model for 10 (respectively 3) more epochson CIFAR10/100-LT and 300K random images (respectively ImageNet-LT and ImageNet-Extra) with anobjective of LID + LOOD. Here, we use LCE or LLA as LID, and LOE, LEnergyOE, or LRNA as LOOD. summarizes the results. PT and FT stand for pre-training and fine-tuning methods, respectively. Wecan oberve that RNA attains the most optimal performances in both OOD detection and ID classification.Notably, when the classification loss during pre-training or fine-tuning is LA, the classification accuracy ofmodels fine-tuned using OE or EnergyOE is significantly lower than that of RNA. This discrepancy could beattributed to the inherent trade-offs between OOD detection and ID classification, arising from the applicationof OE as discussed in .1.",
  "(d) CIFAR100-LT": ": The training dynamics of the models trained with RNA on CIFAR10-LT and CIFAR100-LT. Thetraining loss dynamics of LA loss (red) and RNA loss (blue) are in (a) and (b), and the representation normdynamics of head and tail data in ID training and test dataset, OOD training set (300k random images), andOOD test set(SVHN) are presented in (c) and (d).",
  "E.9Training dynamics of RNA": "a and 7b depict the changes of training losses (LA loss LLA and RNA loss LRNA), respectively,over the training. c and 7d illustrate the dynamics of representation norms of training ID, test ID,training OOD, and test OOD data, over the training. These figures demonstrate that the training effectivelyworks to widen the gap between ID and OOD representation norms.",
  "E.10Distributions of representation norms": "We present additional figures similar to b with different ID vs. OOD datasets in . We observea large gap between ID and OOD representation norms when CIFAR10 is used as the ID set, and a smallerbut still distinct gap when CIFAR100 is the ID set, and a very small gap when CIFAR100 is the ID set andCIFAR10 is the OOD set, which correspond to the OOD detection performance results for each OOD test setin , , and ."
}