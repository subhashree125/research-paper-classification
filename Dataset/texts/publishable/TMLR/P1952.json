{
  "Abstract": "We study EgalMAB, an egalitarian assignment problem in the context of stochastic multi-armed bandits. In EgalMAB, an agent is tasked with assigning a set of users to arms. Ateach time step, the agent must assign exactly one arm to each user such that no two usersare assigned to the same arm. Subsequently, each user obtains a reward drawn from theunknown reward distribution associated with its assigned arm. The agents objective is tomaximize the minimum expected cumulative reward among all users over a fixed horizon.This problem has applications in areas such as fairness in job and resource allocations, amongothers. We design and analyze a UCB-based policy EgalUCB and establish upper bounds onthe cumulative regret. In complement, we establish an almost-matching policy-independentimpossibility result.",
  "Introduction": "The multi-armed bandit (MAB) problem serves as a model for online decision-making under uncertainty,finding applications in diverse domains (Shen et al., 2015; Durand et al., 2018; Ding et al., 2019; Muelleret al., 2019; Forouzandeh et al., 2021). In the classical stochastic MAB problem, an agent is provided witha set of K arms, each associated with an unknown distribution. At each round, the agent plays an armand receive a reward drawn from its distribution. The agents goal is to maximize the expected cumulativereward obtained over a fixed number of time steps T. In our work, we study the problem of egalitarian assignment in the context of stochastic MABs, which werefer to as EgalMAB. In this scenario, the agent is provided with a set of U < K users. At each time step,the agent must assign exactly one arm to each user such that no two users are assigned to the same arm.Subsequently, each user obtain a reward drawn from the reward distribution associated with its assignedarm. The agents objective is to maximize the minimum expected cumulative reward among all users. EgalMAB finds applications in various domains, including ensuring fairness in job and resource allocations.Consider the job assignment problem depicted in . In this scenario, there are U users with recurringjobs of equal load (e.g., hourly database updates) and K shared cloud computing resources with fluctuatingcomputational power (e.g., due to unrelated loads that are running on neighbouring cores of the same physicalnode (Kousiouris et al., 2011)). The agents objective is to distribute these jobs fairly among the availableresources such that over T trials, the user who has to wait the longest across all their jobs (i.e., receives theleast cumulative reward) is not significantly worse off compared to other users.",
  "E[Su,T ]": ": An illustration with K = 8 arms and U = 7 users. After time step T, each user u [U] has someexpected cumulative reward E[Su,T ]. The agents objective is to maximize minu[U] E[Su,T ], which is theminimum expected cumulative reward across all users. Similarly, ride-hailing services present another scenario where fair assignment is desirable. In this scenario,there are U passengers and K drivers who offer varying degrees of experience to passengers (e.g., due tovehicle condition and driver behavior). The agents objective is to allocate users fairly to vehicles such thatover T trips, no passenger faces a significantly worse overall experience than others. As illustrated in both examples, EgalMAB embodies the principle of egalitarianism, which is also known asthe Rawlsian maximin principle (Rawls, 1971), in its notion of fairness. Egalitarianism is a fundamentalnotion of justice based on the difference principle, which seeks to maximize the welfare of those in societywho are the worst-off. Likewise, our agents objective is to maximize the cumulative reward of the userreceiving the lowest cumulative reward among all U users. Given our examples above, solving the EgalMABproblem yields a policy that minimizes the overall waiting time of the user who has to wait the longest andmaximizes the experience of the passenger who has the most negative encounters. This is in contrast to theclassic MAB setting where only overall utility is considered. This prompts the fundamental question thatour work seeks to answer: How can we design an agents assignment policy that optimizes overall utility for individualusers while also ensuring that no user within a group consistently encounters substantiallysub-optimal outcomes? Our contributions are summarized as follows. We formally define the EgalMAB problem in andpropose EgalUCB, a UCB-based solution to EgalMAB, in . In and 6, we establish thatEgalUCB achieves an expected regret of at most",
  "Related Works": "MAB with Multiple Plays.The multiple-play multi-armed bandit (MP-MAB) problem (Anantharamet al., 1987; Gai et al., 2012; Chen et al., 2016; Komiyama et al., 2015) expands upon the classical MABframework by allowing the agent to play U 2 distinct arms. The MP-MAB problem has been extendedin various ways, including combinatorial bandits (Cesa-Bianchi & Lugosi, 2012; Kveton et al., 2015a; Chenet al., 2016), cascading bandits (Kveton et al., 2015b; Wen et al., 2017), and MP-MAB with shareablearms (Wang et al., 2022). An adjacent problem to ours is identifying the top U arms while minimizingregret, which can be framed as a matroid bandit problem (Kveton et al., 2014) with a uniform matroid ofrank U. Although similar to EgalMAB in that the agent has to select multiple arms, unlike EgalMAB, these",
  "b = 1b = 2b = 3b = T/3": ": A trace of EgalUCB with K = 5 arms and U = 3 users. When b = 2, the three arms with thehighest UCBa,b values are a {2, 4, 5}, which are then assigned in a round-robin fashion across time stepst {4, 5, 6}. As b increases, the estimates for a for all arms a improves. By b = T/3 for large T, the threearms with the highest UCBa,b values are most likely a {1, 2, 3}, which are then assigned over time stepst {T 2, T 1, T}.",
  "formulations of the MP-MAB problem do not involve multiple users; that is, the reward is with respect tothe agent instead of users": "MAB with Multiple Users.The extension of multiple users into the classic MAB problem has beenextensively explored in the context of cognitive radio networks (Jouini et al., 2009; Liu & Zhao, 2010; Avner& Mannor, 2014). Unlike EgalMAB where a centralized agent assigns the arms to the users, these works havefocused on scenarios where multiple decentralized users interact with a single MAB instance. As a resultof this decentralization, it is possible for multiple users to play the same arm simultaneously, resulting in acollision that can negatively affect the received reward. Fairness in MAB.The introduction of fairness considerations into the classical MAB problem has gar-nered significant interest.Much attention has been directed towards addressing fairness concerns thattypically involve ensuring that each arm is played a minimum number of times, known as fairness in ex-posure (Claure et al., 2020; Chen et al., 2020; Li et al., 2020; Wang et al., 2021), or with a probabilityproportional to the arms merit, known as meritocratic fairness (Joseph et al., 2016; 2018). While EgalMABemphasizes fairness among users, the aforementioned works deal with fairness among arms. However, thereare alternative approaches that maintain a focus on fairness among users. One such approach involves maxi-mizing the Nash social welfare (NSW) function. This function is defined as the product of rewards obtainedby all users, which intuitively encodes the notion of fairness as it increases only when most users achievehigh rewards. Hossain et al. (2021) investigated a scenario involving U users and K arms, in which eacharms reward varies for each user due to different perceived utilities. In each time step, only one arm isplayed, and all users obtain rewards according to their perceived utilities. Their primary objective is tomaximize the NSW. Sawarni et al. (2023) examined an alternative fairness framework within the contextof linear bandits. In their scenario, a new user is introduced at each time step, and fairness is ensured byconsidering the product of rewards across all time steps. Both of these works assume that only a single armis played at each time step, while the agent in EgalMAB simultaneously selects multiple arms. Additionally,instead of aiming to maximize the NSW function, EgalMAB focuses on maximizing the cumulative reward ofthe worst-off user.",
  "Published in Transactions on Machine Learning Research (10/2024)": "This research/project is supported by the National Research Foundation, Singapore under its AI SingaporeProgramme (AISG Award No: AISG2-PhD/2021-08-011).This research/project is also supported by aMinistry of Education Tier 2 grant under grant number A-9000423-00-00. lvaro Gonzlez, Fernando Ortega, Diego Prez-Lpez, and Santiago Alonso. Bias and unfairness of collab-orative filtering based recommender systems in movielens dataset. IEEE Access, 10:6842968439, 2022.doi: 10.1109/ACCESS.2022.3186719. V. Anantharam, P. Varaiya, and J. Walrand. Asymptotically efficient allocation rules for the multiarmedbandit problem with multiple plays-part i: I.i.d. rewards. IEEE Transactions on Automatic Control, 32(11):968976, 1987. doi: 10.1109/TAC.1987.1104491. Jean-Yves Audibert and Sbastien Bubeck. Minimax policies for adversarial and stochastic bandits. InProceedings of the 22nd Annual Conference on Learning Theory, Berlin, Germany, 2009. Springer-Verlag.",
  "B pa d, where is the Lebesgue measure andpa = dPa/d is the RadonNikodym derivative for Pa": "Agent Policy.A solution to the EgalMAB problem is characterized by an agent policy := (t)t. Duringeach time step t [T], the map t considers the actions and rewards history (which we will define shortly, afterintroducing relevant notations) and assigns each user u [U] to an arm Au,t [K] such that no two usersare assigned the same arm. Subsequently, each user u receives a reward Xu,t drawn independently from thedensity pAu,t. Using these notations, we will denote the history that t considers as (A1, X1, . . . , At1, Xt1)where At := (Au,t)u and Xt := (Xu,t)u.",
  "U minu[U] E[Su,T ]": "where := 1 + + U is the sum of the expected reward of the top U arms. The choice to compare withT/U is natural because the maximum expected cumulative reward obtained by the user with the leastreward is at most T/U, which is obtained by pulling the best arms in a round robin. To see this, observethat the sum of cumulative rewards for all U users is",
  "EgalUCB Policy": "In this section, we describe our policy EgalUCB that achieves near-optimal regret for the EgalMAB problem.The EgalUCB policy is based on the UCB1 policy (Auer et al., 2002) for solving classic MAB problems; however,it differs from the UCB1 in several key aspects. We present the pseudocode for EgalUCB in Algorithm 1,complemented by a visual guide in . We also provide an alternate version of the pseudocode withmore implementation details in Appendix B.",
  "end": "EgalUCB partitions the horizon into B := T/U blocks, each with U steps.We assume, without loss ofgenerality, that T is divisible by U. In cases where this is not true, the difference in expected regret betweenthe best and worst user is at most (1 + + U/2) (KU/2 + + U), which is independent of T. Let A U S denote that set A is a subset of set S with size U. EgalUCB begins by initializing some statistics(Line 1). At the start of each block b [B], it selects any set Ab U [K] consisting of the highest-ranked Udistinct arms as determined by their upper confidence bounds UCBa,b (Line 3). Over a block with U steps,these arms are then assigned to the U users in a round-robin fashion (Line 4). After observing the rewards,EgalUCB updates its statistics (Lines 56). Since each user is assigned to every arm in Ab exactly once, we have E[Su,T ] = E[Su,T ] for all u, u [U].From a technical standpoint, this simplifies the regret by eliminating the min operator in the regret RT :",
  "Main Results": "In this section, we present our main theoretical results.We first state some necessary definitions andnotations. Then, we present the regret upper bounds for the EgalUCB policy. Following that, we discuss thepolicy-independent regret lower bound for EgalMAB. Let Xa,t be the reward obtained from playing arm a for the t-th time across all users. Note that if user uplays arm a at time step t, then the reward obtained is Xu,t = Xa,Ta,t where Ta,t is the number of timesarm a is played up till time step t.",
  "is obtained by selecting the worst set of U arms, and the minimum non-zero sub-optimality gap min :=U U+1 is obtained by replacing arm U in A with arm U + 1, assuming that U = U+1": "Let = (p1, . . . , pK) be an instance of EgalMAB. We say that is a 1-subgaussian EgalMAB if for all armsa [K], X pa is a 1-subgaussian random variable. Theorems 1 and 2 respectively provide problem-dependent and problem-independent upper bounds for the expected cumulative regret of running EgalUCBon a 1-subgaussian EgalMAB.",
  "When the number of arms K and users U are fixed, the problem-independent upper bound increases withthe number of time steps T at a rate of O(": "T ln(T)). Notably, when U = 1, the EgalMAB instance and theEgalUCB policy reduce to the classic MAB instance and the UCB1 policy (Auer et al., 2002). Consequently,both the problem-dependent and problem-independent upper bounds can be reduced to the bound for classicUCB1 with minimal effort1. Next, we examine how the number of users U affects performance. Consider some fixed time horizon T andnumber of arms K. Since T K, the first terms in both the problem-independent and problem-dependentupper bounds dominates the regrets. Furthermore, when K = U, every user would have played every armexactly once after each block, yielding an expected cumulative reward of b after block b. By definition,this implies that the expected cumulative regret RT = 0. This behavior is reflected in both upper bounds,since K U = 0 and max = 0 when K = U.",
  "Additionally, the problem-independent upper bound decreases as U approaches K, and this reduction scaleswith O(1/": "U). There are two reasons for this. Firstly, if we fix some EgalMAB instance and vary U,then increasing U results in decreasing T/U. Secondly, since EgalUCB assigns the arms in a round-robinfashion during each block, as long as the UCB values of the top arms are consistently among the highestregardless of their order, EgalUCB will also consistently select a good set of arms. This implies that as Uincreases, this problem becomes more statistically robust to the variability inherent in the estimates of thearms. Loosely speaking, the more users we have, the easier it is to match the performance of a policy alwaysplays round-robin the set of arms A. We further consider the scope for algorithmic improvement by deriving a policy-independent lower bound.Let = (p1, . . . , pK) be an EgalMAB instance and be any policy. We denote R as the expected cumulativeregret of running on for T time steps. Theorem 3 provides a policy-independent lower bound for theregret R. This bound applies to the class V of all Gaussian EgalMAB instances = (p1, . . . , pK) where, forall a [K], the reward density pa = N(a, 1) and a .",
  "Problem-Dependent Upper Bound": "The regret upper bound in Theorem 1 consists of a sum of two terms: the first term is dependent on Tand the second is independent of T. The first (resp. second) term arises from the regret accumulated overblocks where some good event Eb occurs (resp. did not occur). This good event Eb is the event that a andits estimate a are at a distance of at most b1,Ba,b1 at the beginning of block b for all a [K]. Formally,we define",
  "a,Ba,b1 a b1,Ba,b1": "Note that we will often abuse the set notation when defining events. In particular, when we have a propositionP, we use E = {P} to signify that E is the set of all outcomes in the underlying probability space where Pholds. Lemma 1 shows that Eb occurs with high probability.",
  "|Lb,j| < jU|Lb,i| iUAb > 0": "It is clear that at most one of {Gb,i}i can occur. However, to show that it is a partition for Fb, we also needto show that at least one of {Gb,i}i must happen. This is shown in Lemma 4.Lemma 4. Assume that b > K/U. On the event Fb, exactly one of the events in {Gb,i}i occurs. Using the newly-defined events {Gb,i}b,i, we bound the contributions of the the high-probability term inLemma 5. The intuition behind the events {Gb,i}b,i, which is a common construction used in the proof ofcombinatorial semi-bandits Kveton et al. (2015a), is that it serves to upper bound the high probability termin the regret by the number of times the set of arms Ab is played. This allows us to introduce the reciprocalof the gap term for individual arms a,Na which then serves to derive a meaningful problem-dependentbound on the regret.Lemma 5. Let = (p1, . . . , pK). Suppose that pa is the density for a 1-subgaussian distribution for alla [K]. Then, after T time steps,",
  "Problem-Independent Upper Bound": "Much like the expression in Theorem 1, the first term of the regret bound in Theorem 2 arises from thecumulative regret accumulated in blocks where the high-probability event Eb occurs. Conversely, the secondterm arises from the regret accumulated in the initial blocks and the blocks where Eb did not occur. Theproof of Theorem 2 can be found in Appendix C.",
  "8expDKL(PP)": "Part of the novelty in our analysis involves reducing the computation of this KL-divergence to a combinatorialproblem. Lemma 7 decompose the KL-divergence from Lemma 6 into an expression that involves countingthe number of times each arm a A is played, and Lemma 8 shows, using a counting argument, that thenumber of times an arm a A is played is at most TU 2/(K U).",
  "Synthetic Experiments": "To empirically verify the problem-independent upper bound in Theorem 2, we conducted experiments onthree synthetic datasets: Gaussian bandits with variance 2 = 1.0, Gaussian bandits with variance 2 = 0.1and Bernoulli bandits. In each environment, we fixed K = 10 and varied U from 1 to 5. For each choice of U, we ran the experiment30 times. In each run, we randomly generated the ground truth expected reward a for each arm a usinga uniform distribution with support [0.01, 0.99]. shows the expected regret incurred by EgalUCBover T = 150,000 time steps. As predicted by Theorem 2, the expected regret RT is sub-linear in T anddiminishes with U.",
  ": Expected regret incurred by EgalUCB overT = 150,000 time steps on the MovieLens 25Mdataset with K = 500. Each line is associated witha different number of users U": "We conducted another experiment to verify the rate at which RT diminishes with U when K U. We fixedK = 210 and T = 218 while varying U {21, . . . , 28}. We ran EgalUCB on instances of Bernoulli EgalMABwhere a = 0.8 if a [U] and 0.5 otherwise. This choice of a ensures that is kept constant for allchoices of U. shows the log-log plot of RT against U. We observe that RT diminishes with U ata rate of O(U c) for some constant c 1.0. This corroborates with our policy-independent lower bound inTheorem 3 and suggests that our problem-independent upper bound in Theorem 2 may be loose. To assess the rate at which RT diminishes with U when U K, we ran a similar experiment with T =126,000, K = 20, and U {2, 4, . . . , 18, 20} on Bernoulli EgalMAB instances.For each U, we ran theexperiment 30 times. shows the plot of RT against U. We observe that as U approaches K, theregret decreases to 0, and specifically when U = K, the regret is exactly 0. This observation aligns with theproblem-independent upper bound in Theorem 2.",
  "Google Cluster Usage Trace Dataset": "The Google Cluster Usage Traces dataset comprises 2.4 TiB of compressed traces that record the workloadsexecuted on Google compute cells (Wilkes, 2020).These traces are organized into tables that containinformation about the machines and the instances running on them. In our experiment, we focus on the InstanceUsage table from the clusterdata_2019 trace.This tablecontains traces of both processor and memory usage during instance execution. To adapt to the EgalMABsetting, we designate each arm a as a machine, uniquely identifiable using the machine_id field in thetable. We implicitly construct its reward distribution Pa by drawing an entry uniformly from the trace thatcorresponds to the machine and return the negative of the cycles_per_instruction field for its reward.",
  "MovieLens 25M Dataset": "The MovieLens 25M dataset is widely used in recommender systems (Kuelewska, 2014; Forouzandeh et al.,2021) and collaborative filtering (He et al., 2017; lvaro Gonzlez et al., 2022) research.This datasetencompasses a substantial collection of 25,000,000 user ratings contributed by 162,000 users for a repositoryof 62,000 movies. To adapt to the EgalMAB setting, we randomly select K = 500 movies and treat them as arms. For each moviea [K], we implicitly construct its reward distribution Pa using the user ratings provided by existing users inthe dataset. This empirical distribution is categorical and has support residing within {0.5, 1.0, . . . , 4.5, 5.0}.Then, we formulate a scenario involving U {10, 20, 30, 40, 50} unseen users. At each time step t [T]where T = 150, 000, we employ the EgalUCB policy to assign movies to these users.",
  "T ln(T) (K U) U 1.We also derived a lower bound that matches the upper bound up to a multiplicative gap of 1/": "U and aterm logarithmic in T. Our experiments on simulated and real-world data validated the theoretical analysis.Our empirical results lead us to conjecture that EgalUCB is indeed tight with respect to U. This gap couldpotentially be reconciled with a more refined analysis of the upper bound in future work. Other future worksinclude: Adversarial semi-bandits.It is natural to consider the adversarial variant of our setup. After choosinga randomized assignment at each time step, an adaptive online adversary chooses the reward for each arm.Since that the set of all randomized assignments B (a.k.a. the Birkhoff polytope) is a convex set and theexpected reward given a randomized assignment is a convex function, we conjecture that a modificationof Component Hedge (Koolen et al., 2010) and PermELearn (Helmbold & Warmuth, 2009) can achieve anasymptotically near-optimal solution under the egalitarian consideration.",
  "Thompson Sampling.Another possible direction is to develop a Thompson sampling approach for theEgalMAB problem, potentially by adapting Combinatorial Thompson Sampling (Wang & Chen, 2018)": "Arms with capacity.Suppose that at each time step, we can assign at most C users to each arm.When multiple users are assigned to the same arm at a given time step, we assume that they receive thesame reward. This scenario particularizes to our setting when C = 1. Since the optimal assignment is toround-robin the top U/C arms, we can redefine the regret with = 1 + + U/C. We can show that amodified version of EgalUCB, in which we split the horizon into B = TC/U blocks and play the U/C armswith the highest UCB value, achieves a regret of R(T) 2136(K U/C) ln(T)1min + KCU 1max wheremin and max are redefined by replacing U by U/C. We conjecture that this bound may be improved bydividing the horizon into C phases in which we eliminate all except K/c arms in phase c [C] and play theU/c arms with the highest UCB values in a round-robin fashion. If one can prove that the top U/c armssurvive the elimination after each phase with high probability, it is possible to achieve a factor of C1 in thefirst term in the upper bound on R(T).",
  "Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang. Combinatorial multi-armed bandit and its extensionto probabilistically triggered arms. J. Mach. Learn. Res., 17(1):17461778, Jan 2016": "Yifang Chen, Alex Cuellar, Haipeng Luo, Jignesh Modi, Heramb Nemlekar, and Stefanos Nikolaidis. Thefair contextual multi-armed bandit. In Proceedings of the 19th International Conference on AutonomousAgents and MultiAgent Systems, AAMAS 20, pp. 18101812, California, United States, 2020. InternationalFoundation for Autonomous Agents and Multiagent Systems. Houston Claure, Yifang Chen, Jignesh Modi, Malte Jung, and Stefanos Nikolaidis. Multi-armed banditswith fairness constraints for distributing resources to human teammates.In Proceedings of the 2020ACM/IEEE International Conference on Human-Robot Interaction, HRI 20, pp. 299308, Cambridge,United Kingdom, 2020. Association for Computing Machinery.doi: 10.1145/3319502.3374806.URL Kaize Ding, Jundong Li, and Huan Liu. Interactive anomaly detection on attributed networks. In Proceedingsof the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 19, pp. 357365,New York, United States, 2019. Association for Computing Machinery. doi: 10.1145/3289600.3290964. Audrey Durand, Charis Achilleos, Demetris Iacovides, Katerina Strati, Georgios D. Mitsis, and Joelle Pineau.Contextual bandits for adapting treatment in a mouse model of de novo carcinogenesis. In Finale Doshi-Velez, Jim Fackler, Ken Jung, David Kale, Rajesh Ranganath, Byron Wallace, and Jenna Wiens (eds.),Proceedings of the 3rd Machine Learning for Healthcare Conference, volume 85 of Proceedings of MachineLearning Research, pp. 6782, California, United States, 2018. PMLR. URL Saman Forouzandeh, Kamal Berahmand, and Mehrdad Rostami. Presentation of a recommender systemwith ensemble learning and graph embedding: A case on movielens. Multimedia Tools and Applications,80(5):78057832, Feb 2021. doi: 10.1007/s11042-020-09949-5. Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.Combinatorial network optimization with unknownvariables: Multi-armed bandits with linear rewards and individual observations. IEEE/ACM Transactionson Networking, 20(5):14661478, 2012. doi: 10.1109/TNET.2011.2181864.",
  "David P. Helmbold and Manfred K. Warmuth. Learning permutations with exponential weights. Journal ofMachine Learning Research, 10(58):17051736, 2009. URL": "Safwan Hossain, Evi Micha, and Nisarg Shah. Fair algorithms for multi-agent multi-armed bandits. InM. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advancesin Neural Information Processing Systems, volume 34, pp. 2400524017, New York, United States,2021. Curran Associates, Inc.URL Matthew Joseph, Michael Kearns, Jamie Morgenstern, and Aaron Roth.Fairness in learning:Clas-sic and contextual bandits.In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.),Advances in Neural Information Processing Systems, volume 29, New York, United States, 2016.Curran Associates, Inc.URL Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. Meritocratic fairness forinfinite and contextual bandits. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, andSociety, AIES 18, pp. 158163, New York, United States, 2018. Association for Computing Machinery.doi: 10.1145/3278721.3278764. Wassim Jouini, Damien Ernst, Christophe Moy, and Jacques Palicot. Multi-armed bandit based policies forcognitive radios decision making issues. In 2009 3rd International Conference on Signals, Circuits andSystems (SCS), Medenine, Tunisia, 2009. Institute of Electrical and Electronics Engineers (IEEE). doi:10.1109/ICSCS.2009.5412697. Junpei Komiyama, Junya Honda, and Hiroshi Nakagawa. Optimal regret analysis of thompson samplingin stochastic multi-armed bandit problem with multiple plays. In Proceedings of the 32nd InternationalConference on International Conference on Machine Learning, volume 37 of ICML15, pp. 11521161,Lille, France, 2015. JMLR.org.",
  "Urszula Kuelewska. Clustering algorithms in hybrid recommender system on movielens data. Studies inLogic, Grammar and Rhetoric, 37:125139, Jan 2014. doi: 10.2478/slgr-2014-0021": "Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson. Matroid bandits: fastcombinatorial optimization with learning. In Proceedings of the Thirtieth Conference on Uncertainty inArtificial Intelligence, UAI14, pp. 420429, 2014. Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvari.Tight regret bounds for stochas-tic combinatorial semi-bandits.In Guy Lebanon and S. V. N. Vishwanathan (eds.), Proceedings ofthe Eighteenth International Conference on Artificial Intelligence and Statistics, volume 38 of Pro-ceedings of Machine Learning Research, pp. 535543, California, United States, 2015a. PMLR.URL",
  "Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. Fairness of exposure in stochastic bandits, 2021": "Siwei Wang and Wei Chen.Thompson sampling for combinatorial semi-bandits.In Jennifer Dy andAndreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-ume 80 of Proceedings of Machine Learning Research, pp. 51145122. PMLR, Jul 2018. URL Xuchuang Wang, Hong Xie, and John C. S. Lui. Multiple-play stochastic bandits with shareable finite-capacity arms. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, andSivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162of Proceedings of Machine Learning Research, pp. 2318123212, Maryland, United States, 2022. PMLR.URL Zheng Wen, Branislav Kveton, Michal Valko, and Sharan Vaswani. Online influence maximization underindependent cascade model with semi-bandit feedback. In Proceedings of the 31st International Confer-ence on Neural Information Processing Systems, NIPS17, pp. 30263036, New York, USA, 2017. CurranAssociates Inc."
}