{
  "Abstract": "In this paper we investigate transformer architectures designed for partially observable on-line reinforcement learning. The self-attention mechanism in the transformer architecture iscapable of capturing long-range dependencies and it is the main reason behind its effective-ness in processing sequential data. Nevertheless, despite their success, transformers havetwo significant drawbacks that still limit their applicability in online reinforcement learning:(1) in order to remember all past information, the self-attention mechanism requires accessto the whole history to be provided as context. (2) The inference cost in transformers is ex-pensive. In this paper, we introduce recurrent alternatives to the transformer self-attentionmechanism that offer context-independent inference cost, leverage long-range dependencieseffectively, and performs well in online reinforcement learning task. We quantify the impactof the different components of our architecture in a diagnostic environment and assess per-formance gains in 2D and 3D pixel-based partially-observable environments (e.g. T-Maze,Mystery Path, Craftax, and Memory Maze). Compared with a state-of-the-art architecture,GTrXL, inference in our approach is at least 40% cheaper while reducing memory use morethan 50%. Our approach either performs similarly or better than GTrXL, improving morethan 37% upon GTrXL performance in harder tasks.",
  "Introduction": "In many real-world settings agents often have limited observability of the environment making decisionmaking extra challenging. For example, an agent designed to drive cars must remember the road signs it sawa few minutes ago to adjust its velocity if there are any major changes to the road. A naive approach wouldbe to store the entire history of camera observations. However, such an approach is not scalable as the historyof observations can be longer than the memory available to the agent (McCallum, 1996). Alternatively, theagent can learn a compressed representation of the history of observations, and use it to make decisions. Thisapproach, however, is not feasible in continuing problems where agents face an unending stream of experienceand information critical for decision making occurred in the distant past. Therefore, we need agents thatcan incrementally update their internal representation of the environment state with computation that doesnot growth as a function of total experience. In this paper, we investigate incremental state construction inthe context of partially observable online reinforcement learning (RL), where agents learn while interactingwith the world and thus computation and memory require special consideration. In RL, incremental state construction is a long-studied problem with many possible solution methods. Re-current neural network (RNN) architectures provide a framework for learning such representations due to",
  "Published in Transactions on Machine Learning Research (10/2024)": "the Memory Maze environment is too difficult for the agents to be able to utilize their long-term memorycapabilities. The reward signal is sparse, which might make it difficult for the agent to learn long-termdependencies.It is also possible that learning long-term dependencies in navigation tasks is harder, ingeneral, and longer training is necessary for the benefits of long-term memory to show.",
  "Preliminaries": "In this section, we provide a brief background on Transformers. We first discuss the canonical transformerarchitecture and then we discuss the linear transformer approach, which is the basis of our approach. The transformer architecture was introduced for supervised next token prediction tasks (Vaswani et al.,2017).Our main contribution is a new self-attention mechanism; this section provides the backgroundrequired to understand the self-attention mechanism in transformers. Self-attention is mechanically simple. For a given query token i (embedded in xi .= X(i, )), we outputan embedded context vector that weights each input tokens importance (attention weighted) to the querytoken. The input to the self-attention layer is a matrix X RNd, an embedding of each input token (1 toN) into a vector, Rd. The output is a matrix A RNdh, where dh is the head dimension. Algorithm 1shows a single self-attention layer with learnable parameters WQ, WK, WV Rddh.",
  "Output: A RNdh": "We can think of the process in two steps. In step one we cal-culate the attention weights. We compare each token in thecontext to all other tokens in the context (QKT ). The weightsare then scaled the size of the embedding dimension and nor-malized with an element-wise softmax. In step two, we computeand return the attention-weighted context vectors, one for eachinput in X. The self-attention mechanism in Algorithm 1 is computation-ally expensive. We define the inference cost of self-attentionas the cost for processing a single element in a sequence. Togenerate representations for a single element, a query vector iscalculated instead of query matrix using a single input (Algorithm 1, step 1). In canonical self-attentionmechanism, processing a single element requires having the full sequence as input for generating the valueand key matrix (Algorithm 1, step 2 and 3). Thus, the inference cost depends on the input sequence lengthN. For a naive implementation, the inference cost has O(Nd2) time and O(Nd) space complexity; increasingthe sequence length linearly increases the computational complexity. A simple mitigation is to limit the sizeof the input sequence by maintaining a window of the history of input activations in memory (Dai et al.,2019), but doing so limits the past information the self-attention mechanism can recall.",
  ": vt WV xt4: Ct Ct1 + vt kt5: st st1 + kt6: at (Ctqt)/(st qt)Output: at Rdh,Ct Rdhdk, st Rdk": "The linear transformer architecture (Katharopou-los et al., 2020) introduces a general way of for-mulating self-attention as a recurrent neural net-work by replacing the softmax with a kernel func-tion, leveraging its equivalence to applying kernelsmoothing over inputs (see work by Tsai et al.,2019). A single time-step of inference of the linear trans-former self-attention is described in Algorithm 2.Note that we present the algorithm for processinga single input vector in the case of the linear trans-former. This is in contrast to Algorithm 1, whichpresents the algorithm for processing a sequence.Let k(a, b) = (a)(b), where : Rdh Rdkis a non-linear feature map, dk is the output dimension of the feature map , and k : Rdh Rdh R+.Additionally, let be defined as the outer product vector operation.At a given timestep t, the lineartransformer self-attention maintains a matrix, Ct1 Rdhdk, and a vector, st Rdk, as a recurrent state,which is updated iteratively using the current input vector, xt. Different from Algorithm 1, Algorithm 2applies the feature map, , to generate the query and key for a given time-step (lines 1 and 2). The lineartransformer self-attention stores the outer product of value and key vectors as a recurrent state matrix, Ct(line 4). Additionally, the sum of the key vectors is stored as a recurrent normalization vector st (line 5).The attention output vector, at, is calculated by multiplying the recurrent state with the query vector, andnormalizing it using the product of the normalization vector, st, and the query vector, qt (line 6). The linear transformers self-attention has a context-independent inference cost, unlike the canonicalself-attention mechanism.In Algorithm 2, processing a single input vector (xt) has a space and timecomplexity of O(ddk), assuming d, the embedding dimension (of the input), is greater than dh, which is thesize of the attention-weighted context vector, at. Unlike vanilla self-attention, the computational complexitydoes not depend on the context length, making it more efficient for longer sequences.",
  "Gated Linear Transformers (GaLiTe)": "In this section, we introduce GaLiTe to address two of the limitations of linear transformers. Specifically,(1) the recurrent equations in Algorithm 2 (lines 5 and 6) add positive values to the recurrent state, withoutany mechanism to delete past information. (2) Performance critically depends on the choice of the kernelfeature map (lines 1 and 2); element-wise functions such as the Exponential Linear Unit (ELU) typicallyperform worse than softmax (Katharopoulos et al., 2020). GaLiTe mitigates these two issues by introducing a gating mechanism and a parameterized feature map. Thegating mechanism controls the flow of information at each index of C (the location of the recurrent states ofthe self-attention mechanism), allowing arbitrary context memory (inducing a trade-off with precision). Theparameterized feature map is used to calculate the key and query vectors in the self-attention mechanism,eliminating the choice of the kernel feature map .",
  "Ct .= Ct1 + vt kt,(1)st .= st1 + kt.(2)": "As kt is a function of and under the assumption of a positive feature map , Equation 2 adds arbitrarypositive values to st1.Similarly, Equation 1 adds arbitrary positive values to Ct1 if the elements invalue vector vt are positive. Both Equation 1 and 2 have no way to control the flow of past informationand the values in the recurrent state could grow. Instead, we use a normalized exponential averagewithelement-wise learned decay parameterswhich smoothly reduces the impact of past information. Gating mechanisms can be used to control the flow of information in recurrent updates.We pro-pose a learned outer-product-based gating mechanism that decays every element of Ct1 and st1allowing the network to learn the decay for each element (also known as the memory location).We introduce learnable parameters WRdhd,WRdkd,and gating vectors t,andt.Let g be a sigmoid function defined as g(x).=1/1+ex, we define t and t as follows:",
  "kt .= (WKxt),(7)qt .= (WQxt).(8)": "We consider a deterministic approach to learn the key and value vectors in the linear transfomer self-attentionmechanism. We introduce modifications to kt, qt, and gating vectors calculation described in Equations 7, 8,3, and 4 respectively. We start by introducing a hyperparameter that controls the dimension of the featuremaps used to construct kt and qt. Let Wp1, Wp2, Wp3 Rd be learnable parameters. We modify thedimensions of W as W Rdhd, getting rid of dk, the kernel feature map dimension. Let f() be a function",
  "t .= f(g(Wp3xt) g(Wxt)).(11)": "Using the modified key, query, and gating vectors, the recurrent states Ct Rdhdh and st Rdhare calculated according to Equations 5 and 6. It is important to note that the feature map dimension,dk = dh, is now controlled by the hyperparameter . Equations 9 and 10 use outer products to learnmultiplicative interactions in the key and query vectors. Learning multiplicative interactions in the featurevectors allows learning complex non-linear relationships through training instead of relying on an explicitnon-linear element-wise function or on random feature maps. Using outer products to generate an expansivefeature map allows us to have a large feature map output dimension. Having a large feature map outputdimension is essential as it correlates with the memory capacity (see the work by Schlag et al., 2021). Finally, we use the relu activation function to ensure the output of the feature map is positive. A positivefeature map is a common assumption in the linear transformer literature as it is simple way to ensure thatsimilarity scores produced by the underlying kernel function are positive. The Gated Linear Transformer (GaLiTe) self-attention incorporates the changes discussed above into thelinear transfomer self-attention.The pseudo-code for GaLiTe is available in Appendix A. GaLiTe hassimilar space and time complexity as the linear transfomer. For processing a single element in a sequence,GaLiTe has a space and time complexity of Od2and Od2, respectively. In comparison, the lineartransfomer requires O (dkd) and O (dkd). Notice dk is defined to be the output dimension of the kernelfeature map, which is dh in GaLiTe. Similar to the linear transfomer, the space and time complexity ofGaLiTe is independent of N and only depend on static hyperparameters d and .",
  "Approximate Gated Linear Transformer (AGaLiTe)": "Operating on large matrices is expensive. Recall that GaLiTe stores a matrix of dimension d2h as a recurrenthidden state. This becomes more problematic with the use of multiple heads and layers; which are typicallyrequired to improve stability during the training (see the work by Michel et al., 2019). For example, previousapplications of transformers in RL by Parisotto et al. (2020) use 8 heads and 12 layers; 96 heads in total.Second, the update to Ct makes use of expensive and memory heavy operations: an outer product, element-wise matrix sum, and multiplication. Our goal is to approximate the recurrent state update in Equation 5 with an approximation that uses lessspace than O(d2). Recall that Equation 5 replaces Ct with Ct1 plus a new outer product. To derive anapproximation, we want to replace Ct1 with a matrix that has a lower rank. Also, we want to derive anupdate rule that is an approximation of Equation 5, but instead of updating the full-rank matrix, Ct1, itupdates the low-rank approximation. Our second approach, called Approximate Gated Linear Transformer (AGaLiTe), uses a low-rank approx-imation to reduce the space complexity of GaLiTe. We replace the previous recurrent state matrix, Ct1,with a set of vectors, reducing the space complexity of GaLiTe by d. We introduce an approximation of theKronecker delta function using a sum of cosine functions and we use this to approximate Ct1. We introduce an approximation that uses a sum of cosine functions to approximate a sum of outer products.This approximation is deterministic, it does not introduce variance in the approximation, and it keeps incre-mental updates to the state end-to-end differentiable. Our approach is inspired by the rank-1 approximationintroduced by Ollivier et al. (2015), but instead of using random numbers to approximate a Kronecker deltafunction, we use a trigonometric identity that relates a Kronecker delta function to an integral over cosines.Recall that the Kronecker delta function is defined for integers m and n such that mn = 1 if m = n, and",
  "r n.(12)": "It can further be shown that limr mn = mn. The derivation for this result is presented in AppendixB.1. We use the approximation of the Kronecker delta function in Equation 12 to approximate the recurrentstate update in Equation 5. Briefly, the approximation introduces the approximate Kronecker delta functionto approximate Ct as a sum of r outer-products, where each of the vectors in the outer-product is definedrecursively and updated using the value and key at the current timestep. For a given r, we maintain recurrentstates vkt1 and kkt1 for k = 0, 1, . . . , r. For k .= 2k",
  "AGaLiTeO(rd)Od2 + rd": "The pseudocode for AGaLiTe can be found in Ap-pendix C. Unlike Equation 5, Equations 13 and 14define a recurrence over vectors instead of matri-ces.If r d, then the recurrence is more effi-cient in space than the recurrence in Equation 5.In Appendix E, we provide an empirical evaluationof the impact of different values of r in the qual-ity of the approximation, showing that, in practice,it seems small values of r do not compromise thequality of the approximation or the overall perfor-mance. The computational complexity of AGaLiTeis O(rd) and Od2 + rdin space and time. WithAGaLiTe, we have significantly improved the com-plexity of the self-attention mechanism and thesedifferences manifest in experiments as we show next.We compare the computational complexities of ourproposed approaches and GTrXL (Parisotto et al., 2020) in . We provide empirical latency measure-ments of forward pass using the AGaLiTe architecture and compare it to GTrXL in Appendix K. We alsodiscuss the parallelization of GaLiTe and AGaLiTe over a sequence of data in Appendix F.",
  "Empirical Evaluation": "This section investigates our proposed approaches in several partially observable reinforcement learning (RL)control problems. The memory requirements vary across the environments we consider. In T-Maze (Bakker,2001), the agent must remember a single cue signal. In CartPole, the agent must estimate the hidden stateby integrating information over time. In Mystery Path (Pleines et al., 2023), the agent must remembermultiple locations in a grid environment. In Craftax (Matthews et al., 2024), a 2D survival game, the agentfaces with partial observability as it can only observe a limited portion of a large 2D map. In Memory Mazeenvironment (Paukonis et al., 2023), the agent must retain the layout of a 3D maze in addition to severallocations across the maze. Additionally, we also provide results in Long Range Arena (Tay et al., 2021) in Appendix D, a classicalbenchmark used to evaluate the ability to learn long-range dependencies in a supervised learning scenario. We",
  ": Success rate in the last 100K timesteps av-eraged over 50 runs in T-Maze (shown inset).Theshaded regions represents the standard error": "We trained seven agents for five million steps inthe T-Maze environment, for corridor lengths 120200. The network architecture for each agent has ashared representation learning layer, either an RNNor a transformer, which is then followed by sepa-rate actor and critic heads.Two of these agentswere trained using an RNN as the shared represen-tation layer, namely LSTM (Hochreiter & Schmid-huber, 1997) and GRU (Cho et al., 2014).Theother two agents used a transformer, particularlythe GTrXL architecture (Parisotto et al., 2020), andthe linear transformer architecture (Katharopouloset al., 2020).In GTrXL, the memory size hyper-parameter, defined as the amount of stored history,controls the context length. We train two GTrXLagents, GTrXL-128 and GTrXL-256, correspondingto memory sizes 128 and 256.Note that for thecorridor lengths considered, GTrXL-256 has the en-tire episode provided as input.We also evaluateGaLiTe ( = 4) and AGaLiTe ( = 4, r = 1); we doso by replacing the XL-attention (Dai et al., 2019)of GTrXL with one of the two approaches, while preserving the order of the layers and the gating of GTrXL.This allows us to evaluate exactly the impact of the newly introduced self-attention mechanisms withoutother confounders. The base RL algorithm for all agents use Advantage Actor-Critic (A2C) (Wu et al.,2017). Architecture-specific hyperparameters and tuning strategies are described in Appendix G.1. summarizes the main results.We report the success rate, the percentage of correct decisions,averaged over the last 100K timesteps of the experiment. An agent that chooses randomly at the intersectionwould achieve a success rate of 0.5. In this experiment, GTrXL is sensitive to the amount of history providedas input; GTrXL-128 (brown) fails for corridor lengths greater than 120, whereas GTrXL-256 (orange) workswell across all corridor lengths. GaLiTe (purple) and AGaLiTe (red) match the performance of GTrXL-256despite not having access to the entire episode as input. Note that AGaLiTe performs close to GaLiTe evenwith r = 1 (the approximation parameter). GRU (green) and linear transformer (grey) outperform LSTM(blue), but their performance drop in the longest corridor lengths. AGaLiTe is more computationally efficientthan GTrXL-256 in T-Maze. For a single attention head, AGaLiTe uses roughly 125.1 times fewer operationsthan GTrXL-256, and 36.57 times less space. Also, AGaLiTe uses roughly 62.67 times fewer operations and18.28 times less space than GTrXL-128. Partially Observable Classic Control.Inspired by previous work (Morad et al., 2022; Duan et al.,2016), we explored two variants of partially observable CartPole (Barto et al., 1983). In the first variant, wemask out the velocity information from the observation vector and only allow positional information. Thismodification makes the problem difficult as the agent now needs to estimate these velocities itself. The secondmodification introduced an additional challenge by adding noise to the positional information communicated",
  ": The total reward is binned over10 timesteps and averaged over 30 differentseeds standard error": "We used both GRU and GTrXL as baselines for this prob-lem. GRU-based approaches perform best on these partiallyobservable classical control tasks, even compared to transform-ers (Morad et al., 2022). We used PPO (Schulman et al., 2017)and trained all agents for 5M steps on the two variants of Cart-pole. We also performed an extensive sweep of the hyperparam-eters of PPO and GRU, which is described in Appendix G.2. summarizes the results of our experiment in theNoisy stateless CartPole, the second variant from above. TheAGaLiTe agent learns faster and finds a better-balancing pol-icy than the GRU and GTrXL agents.The results for theother variant of partially observable CartPole (without noise)is qualitatively similar and can be found in Appendix G.2. Mystery Path.In Mystery Path (Pleines et al., 2023), theagent is required to remember multiple cue signals for long periods of time in a 2D pixel-based environment.In this environment, the agents goal is to reach a target position by traversing through a random invisiblepath. Episodes have fixed length and the agent is reset back to the start location (along with a feedbackobservation) upon deviating from the path. We consider two configurations of this environment: MPGridand MP; MP is more difficult. In MP, there are six actions and smoother motion dynamics compared toMPGrid, with grid-like movements and four actions. MPGrid has a maximum episode length of 128, whileMPs is 512. Appendix G.3 describes the environment and the configurations considered.",
  "MPGridMP": ": Left: Learning curves in MPGrid (averaged over 15 seeds 95% bootstrapped CI) along with aninset figure showing a possible ground truth maze layout. Right: Learning curves in MP (averaged over 5seeds 95% bootstrapped CI) along with inset figure depicting the agents observation. The agent does notobserve the path to the goal (left); a red cross is shown as feedback if the agent deviates off from the path,with the agent being reset to the start tile (right).",
  "PPO+RNN": ": Learning curves of GTrXL-128and AGaLiTe in the Craftax symbolic en-vironment (averaged over 15 seeds 95%bootstrapped CI). The inset plot shows theresult of rendering a sample observation(left) and the full map (right). Craftax.Crafter (Hafner, 2021) is a 2D open-world survivalgame where an agent needs to forage for food and water, findshelter to sleep, defend against monsters, collect materials, andbuild tools. This environment is designed to evaluate an agentsability to perform a wide range of tasks purely from a scalarreward signal. The environment is partially observable as theagent can only observe a portion of the large randomly gener-ated map that it navigates. Our hypothesis is that an agentwith better memory capabilities and longer context shouldachieve higher performance through navigating and utilizingthe cues in the environment effectively. We consider the sym-bolic variant of the environment, Craftax symbolic, detailed inMatthews et al. (2024). The symbolic variant simplifies theobservations of original pixel-based crafter by encoding eachpixel as a one-hot vector and an intensity value. We trained a GTrXL agent with memory size 128 and anAGaLiTe agent with feature map dimension = 8, and r = 1for 1B steps. The architecture sizes for GTrXL and AGaLiTewere chosen similar to the ones used in the T-Maze and MysteryPath experiments. PPO was the base RL agent used. Detailson hyperparameters sweeps can be found in Appendix G.5. shows the total episodic reward achieved by the AGaLiTe-based agent compared with a GTrXL-based agent. We observe that the AGaLiTe achieves a higher reward than both GTrXL and previouslyreported PPO+RNN baseline (Matthews et al., 2024). In Appendix J we compare the performance acrossthe various game-related achievements present in Crafter. We find that AGaLiTe achieves higher scores inseveral of these achievements. We also considered a 3D navigation environment called Memory Maze (Paukonis et al., 2023) that has afixed horizon and that requires the agent to remember multiple cue signals for long periods of time. Atthe beginning of each episode, a new maze is generated randomly and several objects of different colors aredistributed across the maze. The agent perceives a 64 64 RGB image with a colored border indicatingthe color of the current object of interest. Once the agent touches the object, it gets a +1 reward and theborders colors change. The agents goal is to maximize rewards within the fixed time budget. Thus, theagent must remember the objects locations to travel through the maze as quickly as possible. (inset) provides an illustration of the Memory Maze environment. In the main paper, we report results onthe largest maze size, 15 15, with an episode duration of 4,000 steps. Results for other maze sizes can befound in Appendix H.",
  ": Learning curves in MemoryMaze .The bold lines report total episodic rewardaveraged over three seeds; the other linesare individual seeds. The inset plot showsa sample observation": "MemoryMaze.Wetraineda GTrXLagentandanAGaLiTe agent, each with 22M learnable parameters, for 100Msteps using the Async-PPO algorithm (Petrenko et al., 2020).The GTrXL agent had a memory size of 256, and the AGaLiTeagent had a feature map = 4 and an approximation hyper-parameter r = 7.We based our architectures for both thepolicy and the critic on the work by Petrenko et al. (2020).In this work, a ResNet (He et al., 2016) is used to extract thefeatures from the input image, then a sequence of features arefed into an RNN or a transformer. We detail the hyperparam-eters used, the architecture sizes, and the tuning strategy inAppendix G.4. shows the total episodic reward achieved by ourAGaLiTe-based agent compared with a GTrXL-based agent.The asymptotic performance of all the three agents is similar,but the GTrXL-based agent exhibits faster learning early on.Additionally, we explore the impact of smaller GTrXL memorysizes in Appendix I. We find that reducing the memory sizeof GTrXL did not affect the performance in this environment,suggesting that the agents might not be utilizing their memorycapabilities effectively. Finally, we looked at the agents utilization of the computational resources. For a single attention head,AGaLiTe uses roughly 125 times fewer operations than GTrXL-256 and it uses 46 times less space. Addi-tionally, we measured the frames per second (FPS) and the memory usage from 12 AGaLiTe and GTrXLagents. Overall, AGaLiTe achieves 535.63 0.52 FPS while GTrXL achieves 373.63 0.49 FPS, correspond-ing to a 43.36% improvement. Further, AGaLiTe uses 52.37% less memory than the GTrXL agent. Thenumber of operations and space used are asymptotic information that highlight the benefits one can expectwhen using even bigger neural network architectures, such as those now common in industry. The FPS ratedemonstrates the performance gain when AGaLiTe is instantiated in a particular network architecture.",
  "Ablation Study": "In this section, we present ablations for each of the three modifications proposed to the linear transformerarchitecture in AGaLiTe, highlighting the importance of each modification. We present the results for theseablations in . Our first ablation evaluates the impact of proposed gating mechanisms in AGaLiTe. We conduct this ablationin the MPGrid environment. This environment requires an agent to selectively filter and remember multiplepieces of information throughout an episode. Our proposed gating mechanism significantly outperforms anAGaLiTe without the gating mechanism in a. The other two ablations compare the proposed feature map and approximation approach to other approachesin the literature. We conducted these ablations in the T-Maze environment with the corridor length set to200. We use the same hyperparameter tuning strategy as described in Section G.1, but focusing on the besthyperparameter on corridor length 200. We report the success rate while training an agent for 5M steps over50 seeds. To evaluate the impact of different feature maps in AGaLiTe, we consider two alternatives, proposed inthe existing literature. The first uses an element-wise feature map ELU + 1 (Clevert et al., 2016), whichwas used originally in the linear transformer architecture. The second is the deterministic parameter freeprojection (DPFP) introduced by Schlag et al. (2021), which was shown to outperform exisiting featuremap approaches in language modelling tasks. We present these results in b. We observed that ourproposed feature map outperform both of these methods.",
  ": Ablation of various components of the AGaLiTe architecture": "Finally, we compare AGaLiTes approximation to an alternative incremental low-rank approximation method.We consider the rank-1 trick introduced by Ollivier et al. (2015). The rank-1 trick approximates a Kroneckerdelta function using random signs drawn from a uniform distribution. Similar to our proposed approximationapproach, the rank-1 trick could be applied to derive incremental updates to a low-rank decomposition of amatrix. We derived an approximation using the rank-1 trick and compared it to our proposed approximation(with r = 1) in c. We observe that our proposed approximation approach outperforms the rank-1trick.",
  "Related Work": "Similar to our approach, several previous works have explored extensions to the linear transformer architec-ture, addressing its limitations. The DeltaNet architecture (Schlag et al., 2021) implements a scalar gatingmechanism to accumulate the value vectors over time, and then uses an error-correcting delta rule to updatethe recurrent states. In contrast, our proposed approach utilizes an element-wise gating mechanism applieddirectly over the recurrent states. DeltaNet also introduces a deterministic feature map called DPFP wehave found to perform worse compared to our proposed feature map in . RecurrentDeltaNet ar-chitecture (Irie et al., 2021) proposed improvements to the DeltaNet architecture by introducing additionalrecurrence and non-linearity to the updates of the key, value, and query vectors. However, the non-linearupdate rule in RecurrentDeltaNet limits its parallelizability over an input sequence. In contrast, our pro-posed approach uses linear update rules, and could be parallelized over an input sequence (Appendix F).In a concurrent and independent work, Aksenov et al. (2024) explored a learnable kernel function inspiredfrom Taylor expansion of exponential functions, demonstrating impressive in-context learning performance.Notably, none of these work improve upon the computational complexity or introduce approximations torecurrence mechanism of the linear transformer. Gating mechanisms such as the one we used in GaLiTe and AGaLiTe are commonly used in RNNs to controlthe flow of information and mitigate the impact of vanishing gradients (Hochreiter & Schmidhuber, 1997).Often, scalar gating mechanisms have been applied, such as in the linear transformer (Peng et al., 2021).However, using a single learned coefficient could be sub-optimal as it does not allow for a more fine-grainedcontrol of the flow of past information from each index location in a recurrent state. The choice of the feature map can have a significant impact on the overall performance (Schlag et al.,2021).For example, a non-expansive map based on ELU+1 can be used (Katharopoulos et al., 2020),however, element-wise activation functions are limited in their ability to learn complex non-linear rela-tionships and using them as a feature map limits the memory capacity of the architecture (Schlag et al.,2021). Alternatively, random feature maps can be used to approximate a softmax function(Peng et al.,2021; Choromanski et al., 2021). Although randomized feature maps are equivalent to softmax function inexpectation, they introduce additional variance. Our model is deterministic.",
  "Conclusion and Future Work": "Transformers have revolutionized many branches of AI research, but their computational requirements makeextension to other domains such as online RL difficult. In this paper, we have introduced two recurrentalternatives of the self-attention mechanism in transformers, called Gated Linear Transformer (GaLiTe) andApproximate Gated Linear Transformer (AGaLiTe). We demonstrate the efficacy of both approaches ina several partially observable reinforcement learning tasks (e.g., T-Maze, Mystery Path, Craftax, MemoryMaze). When compared to a state-of-the-art architecture GTrXL, the inference cost of our approach is morethan 40% cheaper while reducing memory use more than 50%. Future work could explore algorithmic improvements to AGaLiTe such as using updates based on efficientreal-time recurrent learning (Zucchet et al., 2023; Williams & Zipser, 1989). Furthermore, the applicationof AGaLiTe to model-based RL algorithms such as the Dreamer V3 (Hafner et al., 2023) could be exciting.Finally, Morad et al. (2024) leverages the properties of linear transformer approaches to propose a batching",
  "Acknowledgements": "We would like to thank Martha White, Dale Schuurmans, and Michael Bowling for providing valuablefeedback and for their helpful discussions. We would like to thank Martha White for also providing accessto additional computational resources.We would like to thank Vincent Liu for providing feedback onthe derivations presented in this paper. The research is supported in part by the Natural Sciences andEngineering Research Council of Canada (NSERC), the Canada CIFAR AI Chair Program, the Universityof Alberta, Google Cloud Incubator, TPU Research Cloud Program, and the Digital Research Alliance ofCanada. Yaroslav Aksenov, Nikita Balagansky, Sofia Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, andDaniil Gavrilov. Linear transformers with learnable kernel functions are better in-context models. InAssociation for Computational Linguistics, 2024.",
  "Matthew Brand. Incremental singular value decomposition of uncertain data with missing values. In EuropeanConference on Computer Vision, 2002": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. In Neural Information Processing Systems, 2020.",
  "Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv:2001.04451,2020": "Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni,Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distillation. InInternational Conference on Learning Representations, 2023. Kuang-Huei Lee, Ofir Nachum, Mengjiao Sherry Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, IanFischer, Winnie Xu, Eric Jang, Henryk Michalewski, et al. Multi-game decision transformers. In NeuralInformation Processing Systems, 2022.",
  "Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcementlearning via supervised pretraining. In International Conference on Learning Representations, 2024": "Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and FeryalBehbahani.Structured state space models for in-context reinforcement learning.Advances in NeuralInformation Processing Systems, 2024. Michael Matthews, Michael Beukman, Benjamin Ellis, Mikayel Samvelyan, Matthew Jackson, Samuel Cow-ard, and Jakob Foerster. Craftax: A lightning-fast benchmark for open-ended reinforcement learning. InInternational Conference on Machine Learning, 2024.",
  "Paul Michel, Omer Levy, and Graham Neubig.Are sixteen heads really better than one?In NeuralInformation Processing Systems, 2019": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-national Conference on Machine Learning, 2016. Steven Morad, Ryan Kortvelesy, Matteo Bettini, Stephan Liwicki, and Amanda Prorok. POPGym: Bench-marking partially observable reinforcement learning. In International Conference on Learning Represen-tations, 2022.",
  "Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. Randomfeature attention. In International Conference on Learning Representations, 2021": "Olivier Petit, Nicolas Thome, Clement Rambour, Loic Themyr, Toby Collins, and Luc Soler. U-Net trans-former: Self and cross attention for medical image segmentation. In International Workshop on MachineLearning in Medical Imaging, 2021. Aleksei Petrenko, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, and Vladlen Koltun. Sample factory:Egocentric 3D control from pixels at 100000 FPS with asynchronous reinforcement learning. In Interna-tional Conference on Machine Learning, 2020.",
  "Marco Pleines, Matthias Pallasch, Frank Zimmer, and Mike Preuss. Memory Gym: Partially observablechallenges to memory-based agents. In International Conference on Learning Representations, 2023": "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gimnez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, AliRazavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, andNando de Freitas. A generalist agent. Transactions on Machine Learning Research, 2022.",
  "Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequencemodeling. In International Conference on Learning Representations, 2023": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. InInternational Conference on Learning Representations, 2021. Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.Transformer dissection: A unified understanding of transformers attention via the lens of kernel.InConference on Empirical Methods in Natural Language Processing and the International Joint Conferenceon Natural Language Processing, 2019.",
  "Ronald J Williams and David Zipser. A learning algorithm for continually running fully recurrent neuralnetworks. Neural Computation, 1989": "Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region method fordeep reinforcement learning using Kronecker-factored approximation. In Neural Information ProcessingSystems, 2017. Huasong Zhong, Jingyuan Chen, Chen Shen, Hanwang Zhang, Jianqiang Huang, and Xian-Sheng Hua. Self-adaptive neural module transformer for visual question answering. IEEE Transactions on Multimedia,2020.",
  "Ct =(1 t) (1 t) Ct1 +t vtt kt.(23)": "We use the approximation of the Kronecker delta function in Equation 20 to approximate the update inEquation 23. We will start by representing the recurrent state Ct as a sum of outer products. We will do soby recursively expanding using the definition of Ct. Following the recursive definition we have:",
  "+(1 t) (1 t1) t1 vt2(1 t) (1 t1) t2 kt2+ Ct3": "Next, we rewrite the recursive expansion as a single summation term of outer product. This is possiblebecause upon following the recursive definition, we can see above that each term of the expansion could bewritten as an outer product of vectors. The vectors used to calculate the outer product at each timestepconsists of element-wise vector product involving either the value or key, and their corresponding gatingvectors. We show the expansion for only until t 3, but the same steps could be followed until t = 0.We define product operation such that: ji(ai) .= 1 if j > i, and ji(ai) .= ai ai+1 . . . aj. Weintroduce variables li and mi to represent the left and right terms of the outer-product at a given timestep,",
  "i=0(cos (ki) li) (cos (kj) mj)": "Next, we will use the distributive property: (mi=0 ai)(nj=0 bj) = mi=0nj=0(ai bj), where a0, . . . , amand b0, . . . , bn are arbitrary vectors. We will use it to rewrite the above equation as a single summation ofouter products, where the summation is defined only over r. This is a key operation that allows us to writethe approximation to be written temporally iterative manner. Applying the distributive property from right",
  "k=0vkt kkt ,(30)": "We regroup the terms and derive a recursive relationship of vkt and kkt with respect to vkt1 and kkt1. First,we separate the term in the summation when i = t. Next, we factorize (1 t) from the summation term.Finally, following the definition in Equation 28, we replace the second term with vit1.",
  "vkt.= cos(kt)t vt + (1 t) vkt1(34)kkt.= cos(kt)t kt + (1 t) kkt1(35)": "Since limr mn = mn, it follows that limr Ct = Ct. Unlike Equation 23, Equation 34 and 35 definea recurrence over vectors instead of matrices, and if r d, the recurrence is much more efficient in spacethan the recurrence in Equation 23. We leave it to future work to formally derive the approximation error.In Section E we show the approximation error with a synthetic error under different values of r. Lastly, since the current state Ct could be represented as a sum of outer products in a non-recurrent manner,we can avoid explicitly calculating Ct and instead calculate the attention output at as follows:",
  "CApproximate Gated Linear Transformer (AGaLiTe)": "Algorithm 4 shows the Approximate Gated Linear Transformer (AGaLiTe).We highlight changes fromAlgorithm 3 in blue. The algorithm maintains a set of vectors k0t1, ..., krt1 Rdh, v0t1, ..., vrt1 Rdh,and st1 Rdh as the recurrent state at a given time-step t.The number of vectors stored could becontrolled by modifying the hyperparameter r, which should ideally be set to a small value. The key, query,and value vectors are calculated similarly to GaLiTe. The recurrent state update is modified to use theapproximation in Equation 33. At each time step, the recurrent vectors are updated using element-wisevector multiplication and addition operations (lines 10-14). The operation on each recurrent vector could beexecuted in parallel. The attention output is calculated without ever explicitly calculating Ct (lines 16-18).",
  "DResults in Long Range Arena": "We additionally evaluate on the ListOps and Text (IMDB) from the Long Range Arena (Tay et al., 2021) asto evaluate the architectures ability to learn long-range dependencies in a supervised learning scenario. Theperformance of AGaLiTe ( = 8, r = 1) is compared with the transformers and linear transformers in . The exact hyperparameters for AGaLiTe are listed in . We found that AGaLiTe outperforms thepreviously reported results of the transformer and linear transformer architecture (Tay et al., 2021) in bothof these tasks.",
  "EEffect of r on the Quality of Approximation in AGaLiTe": "We empirically evaluate the effect of r on the quality of the approximation of the current state matrix Ct.Ideally, we want to set r to a small value as the space complexity of AGaLiTe is directly proportional tor. We consider a synthetic example where the value vt and key kt at each time step are sampled randomlyfrom a normal distribution. We set the embedding dimension d to 128 and randomly sample values and keysfor 100 timesteps. Instead of using vectors t and t for gating at every timestep, we use a constant valuec. We then compare the difference between the current state matrix Ct computed using the exact methodin Equation 5, with the current state matrix Ct computed using the approximate method in Equation 33at the 100th time-step. We use the Frobenius norm to measure the difference between the two matrices.We repeat the experiment for different values of r and c. For each configuration, we report the mean erroracross 50 independent runs. shows the results of this experiment. We observe that the error inapproximation decreases with increasing value of r. For most values of r and c, the approximation error islow. This is useful since it allows us to set r to a small value, thereby reducing the space complexity of themodel. In fact, in the largest experiments described in this thesis, we set r to 7. Interestingly, we observeperiodic bands in the error plot. It is possible that this is due to the periodicity of the cosine functionsused in the attention mechanism. We leave further exploration around the theoretical nature of the error inapproximation for future work.",
  "FParallelization over an Input Sequence": "Transformers are naturally designed for parallelism over a sequence of input data, as the self-attention oper-ation does not have dependencies between different parts of the input sequence. It is essential to consider theparallelizability of transformer architectures, when the input sequence is presented in a batched fashion. Sucha scenario is common in practice, as most existing actor-critic approaches such as PPO and A2C (Schulmanet al., 2017; Mnih et al., 2016) estimate gradient updates to the actor and critic using batches of trajectoriescollected through agent-environment interactions. Furthermore, most modern hardware accelerators, suchas GPUs and TPUs, excel in handling parallelizable algorithms, and parallelization is vital for effectivelytraining large models. Extension of Algorithm 3 and 4 to accommodate parallelization over a sequence of inputs is straightforward,depending on whether the computation has dependencies on the previous state or not. The majority of thecomputations in both algorithms, which involve calculating keys, queries, values, gating vectors, and theattention vector, do not depend on the previous state and can be parallelized over the sequence. The onlypart of the algorithm that depends on the previous state is the update of the current state. In Algorithm 3,this is done from lines 13-14, and in Algorithm 4, from lines 10-15. The update of the current state in bothalgorithms is implemented as a first order recurrence. This operation is parallelizable as such recurrencescould be expressed as an associtiave binary operations (see Blelloch, 1990). In our implementation, we usedthe associative_scan operation in Jax to parallelize GaLiTe and AGaLiTe over an input sequence.",
  "G.1T-Maze": "Environment Description: The T-Maze environment considered in this paper is similar to the one pro-posed by Bakker (2001). shows two possible episodes in the T-Maze environment. At each timestep,the agent receives a 16-bit binary observation. The first two bits correspond to the cue signal which is either01 or 10 at the first timestep of an episode, depending on whether the reward is located at the left or rightturn at the intersection, respectively. The cue bits are zero in all other timesteps. We consider the largestpossible corridor length as 200. To encode the corridor information, the agent additionally receives 8-bitgray code encoding of its current location. The gray code encoding is zero at the beginning of an episodeand is updated at each timestep. To make the problem more challenging, we added 6 noisy distractor bitsto the observation. The distractor bits are sampled uniformly at random at each timestep. The agent cantake one of the four possible discrete actions at each timestep: up, down, left, or right. The agent receives areward of -0.1 at each non-terminal timestep. At termination, the agent receives a reward of +4 for takingthe correct turn and a reward of -1 for taking an incorrect turn. The reward of +4 is chosen to encouragethe agent to take the correct turn at the intersection. The difficulty of this environment can be increased byincreasing the corridor length. Increasing the corridor length requires the agent to remember the signal fora longer number of timesteps. Since the agents observations include distractor bits, the agent also needs tolearn to ignore the distractor bits and focus on the cue signal.",
  "Episode AEpisode B": ": The T-Maze environment. The agent has to remember a binary cue (denoted by green text),shown only at the beginning of the episode, in order to take the correct turn at the intersection and receivea positive reward. The figure shows two possible episodes and the optimal path an agent must take. Theagents current location is provided as gray code encoding in the observation, along with distractor signals.The corridor length could be varied to increase the difficulty of the problem. Hyperparameters and Tuning Strategy: We include the architecture configuration for each of the 5architectures in . Our hyperparameter tuning strategy is as follows: We train 5 seeds per architecturefor each corridor length in 120-200 and hyperparameter configuration for 5M steps. We identify the besthyperparameter configuration according to the best mean success rate in the last 100K steps across allcorridor lengths. A few additional details are worth reporting for the purposes of reproducibility. We conducted all experimentsusing Python and implemented the agents using the Jax library (Bradbury et al. (2018)). We used the GTrXLimplementation from the DIEngine library (engine Contributors, 2021). Each agent is trained using 16-coremachine with 12GB RAM. The network weights are initialized using orthogonal initialization (Saxe et al.(2014)). A single run using the slowest architecture takes around 20 hours to complete.",
  "G.3Mystery Path": "Environment Description:Pleines et al. ( 2023) introduced the Mystery Path environment as partof the Memory Gym benchmark, which aimed to test agents abilities to memorize many events over anepisode. The Mystery Path is a 7 7 grid environment with pixel-based observations. At the beginningof each episode, the start position of the agent, the origin, is sampled from the grids borders. Then, thetarget position is sampled from the grids borders on the opposite side of the origin. A randomly generatedpath then connects both the origin and the goal. a shows an example of a generated origin, goal,and path. The agents observation, shown in b, is a 64 64 RGB image containing the origin,",
  ": Non-noisy Partially Observable CartPole": "the target, and the agent. The agent gets a +1 reward when it reaches the goal and a 0.1 reward whenvisiting a new tile on the path to the goal. If the agent falls off the path, as in c, a red crossappears as visual feedback, and the agent returns to the origin. The reward is zero in all other timesteps.We consider two variants of this environment, MPGrid and MP. MPGrid has maximum episode length of128, uses grid-like movements and 4 possible actions (left, right, up and down). On the other hand, MP hasa maximum episode length of 512, has smoother movements, and a larger action space that allows diagonalmovements.",
  ": A visualization of the Mystery Path environment": "Hyperparameters and Tuning Strategy: The architecture sizes used for Mystery Path experiments arekept same as in , however, we used actor and critic layer dimension of 256. We detail the hyperparam-eters used for the PPO algorithm that used for training the agents in the Mystery Path environment in . We tune learning rate and entropy coefficient for the sweeps mentioned in . Our hyperparametertuning strategy is as follows: we train 3 seeds per architecture for each the hyperparameter configuration for60M steps in the Mystery Path Grid environment. Finally, we identify the best hyperparameter configurationaccording to the best episodic reward in the last 1M training steps.",
  "13x13": ": The Memory Maze environment. On the left, we show a possible maze layout for all four MemoryMaze configurations. The maze layout is randomized at each episode. On the right, we show two sampleobservations that the agent receives. The agents observation at each time-step is 6464 RGB pixels and theaction space is discrete. The border color of the observation image indicates the target object color whichthe agent needs to find to receive a reward. After collecting the object, the border color changes, indicatingthe next target object. The episode lengths are fixed depending on the Memory Maze configuration, withlarger configurations having longer episodes. Environment Description: The Memory Maze environment evaluates an agents long-term memory capa-bilities in a partially observable RL setting. illustrates this environment. The agents observationat each time-step is an image with 64 64 RGB pixels, and the action space is discrete. In each episode,the agent starts in a randomly generated maze containing several objects of different colors. The agentsobjective is to find the target object of a specific color, indicated by the border color in the observationimage. Upon successfully touching the correct object, the agent receives a +1 reward, and the next randomobject is chosen as the new target. If the agent touches an object of the wrong color, there is no effect on theenvironment. The maze layout and object locations remain constant throughout the episode. Each episodelasts for a fixed amount of time. Since the maze layout is randomized at each episode, the agent must learnto quickly remember the maze layout, the target object locations, and the paths leading to them.",
  "G.5Craftax": "Hyperparameters and Tuning Strategy: The architecture sizes used for Craftax experiments are keptsame as in . We detail the hyperparameters used for the PPO algorithm that used for training theagents in the Craftax environment in . We tune learning rate and entropy coefficient for the sweepsmentioned in . Our hyperparameter tuning strategy is as follows: we train 5 seeds per architecturefor each the hyperparameter configuration for 100M steps in the Craftax symbolic environment. Finally, weidentify the best hyperparameter configuration according to the best episodic reward in the last 1M trainingsteps.",
  "Oracle": ": Learning curves of GTrXL and AGaLiTe agents in the Memory Maze environment. The x-axisrepresents the number of environment steps, and the y-axis represents the total reward in an episode. Eachagent is trained with 3 different random seeds. The bold lines represent the mean return across the 3 seeds,and the blurred lines represent the individual seeds. Each point is the average episodic reward over 1Menvironment steps. The dotted grey line represents the performance of an oracle agent that has access tothe entire maze layout, target object locations and paths leading to them.",
  "IEvaluating Impact of GTrXLs Context in Memory Maze": "This experiment evaluates the impact of GTrXLs context length in the Memory Maze environment. Weshowed earlier that GTrXLs performance is bottlenecked by the memory size in T-Maze. Our hypothesisis that a similar conclusion should hold in the Memory Maze environment. We expect that GTrXL with alarger memory size would outperform GTrXL with a smaller memory size. We should also be able to showthat an AGaLiTe would outperform a GTrXL with a small memory size. To investigate this, we train twoadditional GTrXL agents with memory sizes of 64 and 128 in the Memory Maze 13 13 environment. The learning curves of training the three memory sizes of GTrXL and AGaLiTe in the Memory Maze13 13 environment is shown in . Asymptotically, all four agents achieve similar performance.The individual learning curves, however, indicate that the GTrXL-64 agent is slower to converge than theGTrXL-128 and GTrXL-256 agents. The results failed to provide sufficient evidence to support our hypothesis. The performance obtained bythe three agents does not appear to be different. This observation leads us to the following speculation:",
  "GTrXL-64AGaLiTe": ": Learning curves of GTrXL agents with different memory sizes in the Memory Maze 13 13environment. The x-axis represents the number of environment steps, and the y-axis represents the totalreward in an episode. Each agent is trained with 3 different random seeds. The bold lines represent the meanreturn across the 3 seeds, and the blurred lines represent the individual seeds. Each point is the averageepisodic reward over 1M environment steps.",
  "KLatency Measurements": "In this section we provide additional empirical evidence of the computational efficiency of our proposedapproach, by comparing the latency of forward pass using GTrXL and AGaLiTe. We measure the timerequired in milliseconds (ms) to do a forward pass in two scenarios: (1) processing single element in streamingsequence, (2) processing an entire sequence in parallel. We configure the architecture sizes of GTrXL andAGaLiTe according to the values used by Parisotto et al. (2020): 12 layers, 8 heads, dh = 64, d = 256. Wecollected all data in a single Google Cloud instance with NVIDIA A100 GPU, 12 CPUs and 80GB RAM. First, we compare the time required in milliseconds (ms) to do a forward pass using a single element instreaming sequence. We present the results of these comparisons in a. According to Dai et al.(2019), XL attention used in the GTrXL architecture has a limited context.The context length of XLattention, how far back in time the transformer architecture can remember, is O(ML), where L is thenumber of layers and M is the memory size.We measure the impact of increasing the context length(varying M) of GTrXL (x-axis) on the latency to do a single forward pass (y-axis). AGaLiTe does notexplicit hyper-parameter that allows controlling the context length, and the use of a recurrent hidden stateallows for a potentially unlimited context. Therefore, we consider three AGaLiTe architectures with featuremap hyper-parameter , and plot it as a straight line. We observe that the gap between GTrXLand AGaLiTe increases dramatically with increasing context length. Next, we measure the time required to do a forward pass over a batch, that is process an entire inputsequence in parallel. We present the results of these comparisons in b. We vary the length ofthe input sequence (x-axis) and measure the time required to do a forward pass over the entire sequence(y-axis).We consider two GTrXL architectures with memory size M .We consider threeAGaLiTe architectures with . We observe that the gap between GTrXL and AGaLiTe increasesdramatically with increasing sequence length."
}