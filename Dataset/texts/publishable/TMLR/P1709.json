{
  "Abstract": "Factorization machine (FM) variants are widely used for large scale real-time contentrecommendation systems, since they offer an excellent balance between model accuracy andlow computational costs for training and inference. These systems are trained on tabulardata with both numerical and categorical columns. Incorporating numerical columns posesa challenge, and they are typically incorporated using a scalar transformation or binning,which can be either learned or arbitrarily chosen. In this work, we provide a systematic andtheoretically-justified way to incorporate numerical features into FM variants by encodingthem into a vector of function values for a set of functions of ones choice. We view FMs as approximators of segmentized functions, namely, functions from a fieldsvalue to the real numbers, assuming the remaining fields are assigned some given constants,which we refer to as the segment. From this perspective, we show that our technique yieldsa model that learns segmentized functions of the numerical feature spanned by the set offunctions of ones choice, namely, the spanning coefficients vary between segments. Hence,to improve model accuracy we advocate the use of functions known to havepowerfulapproximation capabilities, and offer the B-Spline basis due to its well-known approximationpower, widespread availability in software libraries and its efficiency in terms of computationalresources and memory usage. Our technique preserves fast training and inference, andrequires only a small modification of the computational graph of an FM model. Therefore,incorporating it into an existing system to improve its performance is easy. Finally, weback our claims with a set of experiments that include a synthetic experiment, performanceevaluation on several data-sets, and an A/B test on a real online advertising system whichshows improved performance. We have made the code to reproduce the experiments availableat",
  "Introduction": "Traditionally, online content recommendation systems rely on predictive models to choose a set of items todisplay by predicting the affinity of the user towards a set of candidate items. These models are usuallytrained on feedback gathered from a log of interactions between users and items from the recent past. Forsystems such as online ad recommenders with billions of daily interactions, speed is crucial. The training",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Comparison of the test cross-entropy loss obtained with Splines and bins. Both methods sufferfrom sparsity issues as the number of intervals grows, but Splines are able to utilize their approximationpower with a small number of intervals, before sparsity takes effect. The bands are 90% bootstrap confidenceintervals based on multiple experiment repetitions: for each number of boundaries and numerical field typewe ran 15 experiments to neutralize the effect of random initialization.",
  "Related work": "Putting aside the FM variants, there is a large body of work dealing with neural networks training overtabular data (Arik & Pfister, 2021; Badirli et al., 2020; Gorishniy et al., 2021; Huang et al., 2020; Popovet al., 2020; Somepalli et al., 2022; Song et al., 2019; Hollmann et al., 2022). Neural networks have thepotential to achieve high accuracy and can be incrementally trained on newly arriving data using transferlearning techniques. Additionally, due to the universal approximation theorem (Hornik et al., 1989), neuralnetworks are capable of representing a segmentized function from any numerical feature to a real number.However, the time required to train and make inferences using neural networks is significantly greater thanthat required for factorization machines. Even though some work has been done to alleviate this gap forneural networks by using various embedding techniques (Gorishniy et al., 2022), they have not been able tooutperform other model types. As a result, in various practical applications, FMs are preferred over NNs. Forthis reason, in this work we focus on FMs, and specifically on decreasing the gap between the representationpower of FMs and NNs without introducing a significant computational and conceptual complexity. A very simple but related approach to ours was presented in Covington et al. (2016). The work uses neuralnetworks and represents a numerical value z as the triplet (z, z2, z) in the input layer, which can be seen",
  "Background and problem statement": "Factorization machines are formally described as models whose input is a feature vector x representing rowsin tabular data-sets. In the context of recommendation systems, the data-set contains past interactionsbetween users and items, whose columns, often named fields, and whose values are features. The columnsdescribe the context, such as the users gender and age, the time of visit, or the article the user is currentlyreading, whereas others describe the item, such as product category, or item popularity. In this section we formally describe how we assume that the data is modeled as the feature vector x, describethe FM family we shall focus on in this paper, and formally state the main problem we aim to solve.",
  "Feature vector structure": "Consider a tabular data-sets with m fields {1, . . . , m}, each can be either numerical or categorical field. Wedenote a row in the dataset with (z1, . . . , zm), each zi is a feature (value) associated with its correspondingfield f(i). Each feature zi is mapped to a vector of values by applying its corresponding encoding functionenci():",
  ",": "For example, consider three fields: field 1 is the users country, field 2 is the users device type, and field 3 isthe product category. All three are categorical. Then enc1() one-hot encodes the country, enc2() one-hotencodes the device, and enc3() one-hot encodes the product category. As a result, in this concrete example,x is the concatenation of three one-hot encodings.",
  "Segmentized view of models": "In this work we aim to study models learned on tabular data as a function of one or two tabular columns,while the remaining are kept constant. Therefore, we need some compact notation for the concept of viewinga multivariate function as a function of one or two variables, while keeping the rest constant. For a given vector y we shall denote by yk the vector obtained from all its components except for yk. For afunction (y), we shall denote by seg[; k](yk) the function obtained by assigning the values yk to all itsarguments, except for yk, producing a function of yk. Formally, we have",
  "seg[; k](yk)(yk) = (y)": "This partial application concept is not new. The mathematical term is currying, coined by Frege (1893).However, we shall use the term segmenized view of for a reason that will become apparent immediately.Similarly to the exclusion of a single coordinate k, for a pair of coordinates k, j we define y(k,j) and",
  "enc = enc": "Given a field f, the function seg[enc, f](zf) is a function of zf, given a concrete assignment zf to theremaining fields. Recalling our three-field example of country, device type, and product category, the functionseg[enc, 2](US, Furniture) characterizes the model as a function of the device type for users from the USthat interact with furniture products. From the recommender systems perspective, we are characterizing themodel for a given segment of users and items, hence the name segmentized view, and the notation seg.",
  "The factorization machine family": "In this work we consider several model variants, which we refer to as the factorization machine family. Thefamily includes the celebrated factorization machine (FM) (Rendle, 2010), the field-aware factorizationmachine (FFM) (Juan et al., 2016), the field-weighted factorization machine (FwFM) (Pan et al., 2018), andthe field-matrixed factorization machine (FmFM) (Sun et al., 2021; Pande, 2021), that generalized the formervariants. FMs are typically employed for supervised learning tasks on a dataset {(xt, yt)}Nt=1 with eitherbinary or real-valued labels yt.",
  "j=i+1Ai,jxixj,": "where the coefficient matrix A is represented in factorized form. The vectors v1, . . . , vn are the featureembedding vectors. Classical matrix factorization is recovered when we have only user id and item id fields,whose values are one-hot encoded. The model FM does not represent the varying behavior of a feature belonging to some field when interactingwith features from different fields. For example, genders may interact with ages differently than they interactwith product categories.Initially, to explicitly encode this information into a model, the Field-AwareFactorization Machine (FFM) was proposed by Juan et al. (2016). Each embedding vector vi is modeled as aconcatenation of field-specific embedding vectors for each of the m fields:",
  "j=i+1xivi, xjvjMf(i),f(j),(1)": "where w0 R, w Rn, and vi Rki are learned parameters. The field-interaction matrices Mf(i),f(j) Rkf(i)kf(j) can be either learned or predefined, and may have a special structure. Note that this model familyallows a different embedding dimension for each field, since the matrices Mf(i),f(j) do not have to be square. Let us explain why all the models described here are special cases of FmFMs in equation 1 in terms of theirrepresentation power: To recover the classical FMs, we take equation 1 with Mf(i),f(j) = I. To recoverFFMs, let Pf be the matrix that extracts vi,f from vi, namely, Pfvi = vi,f. Then FFMs are also a specialcase of the FmFM model in equation 1 with Me,f = P Tf Pe. Finally, to recover FwFMs we just need totake equation 1 with Me,f = re,fI. Given this property, we use FmFM in equation 1 to describe and proveproperties which should hold for the entire family.",
  "= + xk": "Consequently, an inherent limitation of the FmFM family is that it can represent only affine relationshipsbetween any one given input and the prediction. Thus, if the encoding function of a numerical field f islinear scaling, such as min-max scaling or standardization, then encFmFM FmFM enc will only be able torepresent affine functions of that field. In most cases in practice, such an affine relationship is not expressiveenough. To overcome this limitation, numerical columns typically undergo binning or quantization: the numericalrange is partitioned into a finite set of disjoint intervals, and encf() one-hot encodes the interval its argumentbelongs to. Binning a numerical field f, this results in seg[encFmFM, f](zf) being a step function of zf whosejumps are at the binning boundaries. This is because the encoding of zf anywhere inside a given intervalbetween two boundaries is constant, and thus the models output is constant for any zk in that interval.Similarly, for two numerical fields e < f the segmentized views seg[encFmFM, e, f](z(e,f)) are piecewise-constantfunctions of (ze, zf), where the constant-value regions are grid cells formed by the bin boundaries of fields eand f.",
  "Problem statement": "When learning a model, our aim is for encFmFM = FmFM enc to approximate some optimal predictor as afunction of the original tabular fields. Thus, we may think of segmentized views of encFmFM as approximationsof the segmentized views of . Binning, therefore, results in piecewise-constant approximations. Binningwith a very coarse grid may result in a poor approximation. Conversely, a grid that is too fine mightsignificantly reduce the amount of training data falling into each interval, resulting in over-fitting. Thiskind of over-fitting is also known as the data sparsity issue. However, it is nothing more than the classicalapproximation-estimation balance. With respect to this problem, our work has two aims - theoretical and practical. Theoretically, we aimto show that the interplay between a slightly modified FmFM model family and the classical well-knownencoding schemes for numerical features, such as polynomials and splines, yields a powerful family of models:the segmentized views for each field are spanned by the chosen basis, whereas the views for each pair offields are spanned by the basis tensor products, with the span coefficients depending on the remainingfield. Consequently, the model learns a potentially different segmentized view for each segment, while thesegmentized views inherit the approximation power of the chosen basis. Second, we aim to utilize this theoretical insight to show how to improve an existing recommender systemdriven by models from the FmFM family trained with binned numerical features. Concretely, we show howthis insight allows improving the approximation-estimation balance by avoiding sparsity for various kinds ofnumerical fields encountered in practice, and propose a deployment scheme for our modified models thatrequires only minor changes to an existing serving2 part of a recommender system with only minor changes.",
  "j=i+1Ui,:, Uj,:Mf(i),f(j)": "Here, Ui,: is a Python notation for denoting the i-th row of U. In a sense, the first two lines extract therelevant embedding vectors and linear coefficients, and the last line combines them to produce a score. Our modification is applied to the first two extraction lines. Before combining the vectors and the linearcoefficients, we first apply a field-wise reduction to the embedding vectors and linear coefficients of eachfield. We consider two kinds of reductions: (a) the identity reduction, which just returns its input; and (b) asum reduction, which sums up the embedding vectors belonging to the field:",
  "iI(f) xiwi, where I(f) are the indices in x containing the encoding of field f": "The identity reduction is equivalent to multiplying by the identity matrix on the left, whereas the summationreduction is equivalent to multiplying by a single-row matrix of ones 1T of the appropriate size . Thus, eachfield f has an associated reduction matrix Rf, and the resulting model is obtained by pre-multiplying theoriginal U and y by a block-diagonal reduction matrix that applies the appropriate reduction the embedding",
  "(2)": "For every field f, the matrix Rf is either the identity matrix, or a row of ones. Note, that after the reductions,the matrix U and the vector y may have a different number of rows, denoted by n in the formula above. As a slight abuse of notation, we shall now refer to this slightly modified model in equation 2 as the FmFM.This is because an FmFM is obtained by choosing all reductions to be the identity. Next, we shall describehow we incorporate basis encoding into this model.",
  "Suppose we would like to encode a continuous numerical field f. We choose a set of functions Bf1 , . . . , Bff ,": "and encode the field as encf(z) = (Bf1 (z), . . . , Bff (z))T . For that field, we choose the summing reduction,namely, Rf is a row of ones. When the field f under consideration if clear from the context, we omit itfor clarity, and write B1, . . . , B. The result is that for this field, there is one row in U that is equal to",
  "iI(f) Bi(z)vi and one row in y that is equal to": "iI(f) Bi(z)wi. Note that each field can have its own setof basis functions, and in particular, the size of the bases may differ. The computational graph, includingbasis encoding, is depicted in . We note that the block-diagonal matrix is only a mathematicalformalism, and a reasonable implementation to compute the score is similar to what is illustrated in the figure:extract vectors corresponding only to the non-zero components of x associated with each field, perform thereductions for each field, and then compute the pairwise inner products. As a final note, observe that binning is obtained as a special case of our basis encoding method. Indeed,suppose we have intervals that partition our numerical range, choosing the basis B1, . . . , B to be just theindicator functions of these intervals yields a model encFmFM that is equivalent to binning.",
  "i=1iBi(z) +": "An elementary formal proof can be found in Appendix A.1, but it can be explained intuitively. Looking atequation 2, both the matrix U and the vector y have one row that is linear in the basis B1, . . . , B, whereasthe other rows are constant. Since FmFM is affine in any one input feature, it must be an affine function ofthe basis.",
  "j=0i,jBi(ze)Cj(zf) +": "The proof can be found in Appendix A.2. The intuition is similar in nature to the one we have for thespanning property for one field, but now two instead of one rows of y and U are affine in the bases. Its important to note that the segmentized view seg[encFmFM, e, f](z(e,f)) has O( ), but the model doesnot actually learn O( ) parameters. Instead, it learns O( + ) parameters in the form of + embeddingvectors. Consequently, the spanning coefficient matrix = (i,j),i,j=0 for each segment is not learnedexplicitly, but rather its low-rank factorization is learned in the form of embedding vectors. This extends theobservation made in Rgamer (2022) for classical FMs to the broader FmFM family.",
  "Splines and the B-Spline basis": "Spline functions (Schoenberg, 1946) are piece-wise polynomial functions of degree d with up to d1 continuousderivatives defined on some interval [a, b]. The interval is divided into disjoint sub-intervals at a set ofbreak-points a = t0 < t1 < < td = b, where each polynomial piece is defined on [tj, tj+1]. It iswell-known that spline functions of degree d defined on d + 1 break-points can be written as weightedsums of the celebrated B-Spline basis (de Boor, 2001, pp 87) comprising of exactly functions. For brevity,we will not elaborate their explicit formula in this paper, and point out that its available in a variety ofstandard scientific computing packages, e.g., the scipy.interpolate.BSpline class of the SciPy package(Virtanen et al., 2020). In this paper we concentrate on the cases of d = 2 and d = 3, which are quadratic and cubic splines,respectively, with uniformly spaced break-points. At this stage we assume that the values of our numericalfield lie in a compact interval [a, b], which we assume w.l.o.g is . We discuss the more generic cases in the",
  "following sub-section. A visual depiction of the cubic B-Spline basis with five break-points is illustrated in": "The B-Spline basis is our proposed candidate for recommender systems for two reasons: computationalefficiency, and approximation power. For efficiency, an important property of the B-Spline basis of degree d isthat at any point only 1 + d basis functions are non-zero. Thus, regardless of the number of basis functions we use, computing the models output remains efficient, since the reduction described in requires aweighted sum of only 1 + d vectors, regardless of the size of the basis. For the approximation power, it is known (de Boor, 2001, pp 149), that splines of degree d can approximatean arbitrary function g with k 1 + d continuous derivatives up to an error bounded by O(g(k)/k)4,where g(k) is the kth derivative of g. The spanning property (Lemma 1) ensures that the models segmentizedoutputs are splines spanned by the same basis, and therefore are more powerful in approximating the optimalsegmentized outputs than step functions. Assuming that the functions we aim to approximate are smoothenough and vary slowly, in the sense that their high-order kth derivatives are small, the approximationerror goes down at the rate of O( 1",
  ")": "A direct consequence is that we can obtain a theoretically good approximation which is also achievable inpractice, since we can be accurate with a small number of basis functions, and this significantly decreases thechances of sparsity and over-fitting issues. This is in contrast to binning, where high-resolution binning isrequired to for a good theoretical approximation accuracy, but it may not be achievable in practice.",
  "Continuous numerical fields with arbitrary domain and distribution": "Splines approximate functions on a compact interval. Thus, numerical fields with unbounded domains pose achallenge. Moreover, the support of each B-Spline function is only a sub-interval of the domain defined by1 + d consecutive knots. Thus, even if a numerical field f is bounded in [a, b], a highly skewed distributionmay cause starvation of the support of some basis functions: if P(zf support(Bi)) is extremely small,there will be little training data to effectively learn a useful representation of Bi. Here we do not invent any wheels, and suggest using common machine-learning practice: a scalar transformTf : R to the features of field f, such as min-max scaling, or quantile transform5. We note thatstandardization is less appropriate, since standardized values do not necessarily lie in a bounded interval, asrequired by the B-Spline basis. The quantile transform idea is useful for conducting benchmarks, but in a real-time recommender system itmay be prohibitive due to its computational complexity. A quantile transform is obtained by learning theempirical CDF of the training data, and using it as the transform Tf. But the empirical CDF is just anapproximation of the data generating distribution, and we may use others. Simple parametric distributionshave closed-form and fast to compute CDF formulae, and we found that in practice it works well: just fit afew parametric distributions to the training data of a given field f, and use the best-fit as Tf. We found thisto work well in practice, as shown in .4.",
  "Integration into an existing system by simulating binning": "Suppose we would like to obtain a model which employs binning of the field f into a large number N ofintervals, e.g. N = 1000. As we discussed, in most cases we cannot directly learn such a model because of theapproximation-estimation balance. However, we can generate such a model from another model trained usingour scheme to make initial integration easier. The idea is best explained by referring, again, to and equation 2. For a given numerical field f, thereduction stage produces only one row of of the post-reduction matrix U, which we shall denote by u. Infact, this row is a function of zf, namely:",
  "Evaluation": "We divide this section into three parts. First, we use a synthetically generated data-set to show that ourtheory holds - the model learns segmentized output functions that resemble the ground truth. Then, wecompare the accuracy obtained with binning versus splines on several data-sets. The code to reproduce theseexperiments is available in the supplemental material. Finally, we report the results of a web-scale A/B testconducted on a major online advertising platform serving real traffic.",
  "Learning artificially chosen functions": "We used a synthetic toy click-through rate prediction data-set with four fields, and zero-one labels (click /non-click). Naturally, the cross-entropy loss is used to train models on such tasks. We have three categoricalfields each having two values each, and one numerical field in the range . For each of the eight segmentconfigurations defined by the categorical fields, we defined functions p0, . . . , p7 (see ) describing theCTR as a function of the numerical field. Then, we generated a data-set of 25,000 rows, such that for eachrow i we chose a segment configuration si {0, ..., 7} of the categorical fields uniformly at random, the valueof the numerical field zi Beta-Binomial(40, 0.9, 1.2), and a label yi Bernoulli(psi(zi)). We trained an FFM (Juan et al., 2016) provided by Yahoo-Inc (2023) using the binary cross-entropy loss onthe above data, both with binning of several resolutions and with splines defined on 6 sub-intervals . Thenumerical field was navely transformed to by simple normalization. We plotted the learned curve forevery configuration in . Indeed, low-resolution binning approximates poorly, a higher resolutionapproximates better, and a too-high resolution cannot be learned because of sparsity. However, Splinesdefined on only six sub-intervals approximate the synthetic functions {pi}7i=0 quite well.",
  "(d) 6 break-point segmentized cubic spline approximation(0.3432)": ": Results of segmentized approximations of four FFM models trained on synthetic data. In each plot,a family of segmentized functions on the interval , plotted in blue, are approximated by the model,plotted in orange. 12 bins are more accurate than 5, but 120 bins are even less accurate than 5 due to sparsity.With splines we achieve best accuracy. Next, we compared the test cross-entropy loss on 75,000 samples generated in the same manner with forseveral numbers of intervals used for binning and cubic Splines. For each number of intervals we performed15 experiments to neutralize the effect of random model initialization. As is apparent in , Splinesconsistently outperform in this theoretical setting. The test loss obtained by both strategies increases if thenumber of intervals becomes too large, but the effect is much more significant in the binning solution.",
  "Public tabular data-sets": "Since our approach works on any tabular dataset, and isnt specific to recommender systems, we mainly testour approach versus binning on several tabular data-sets with abundant numerical features that have a strongpredictive power: the California housing (Pace & Barry, 1997) , adult income (Kohavi, 1996), Higgs (Baldiet al., 2014) (we use the 98K version from OpenML (Vanschoren et al., 2014)), and song year prediction(Bertin-Mahieux et al., 2011). For the first two data-sets we used an FFM, whereas for the last two we usedan FM, both provided by Yahoo (Yahoo-Inc, 2023), since FFMs are significantly more expensive to trainwhen there are many columns, and even more so with hyper-parameter tuning. The above data-sets werechosen since they were used in a previous line of work by Gorishniy et al. (2021; 2022) on numerical featuresin tabular data-sets. We chose the subset of these data-sets whose labels are either real-valued or binary,since multi-class or multi-label classification problems require a model that produce a scalar for each class.Factorization machines are limited to only one scalar in their output. We assume that the task on all data-sets is regression, both with real-valued and binary labels. This is in linewith what factorization machines are commonly used for - CTR prediction. For binary labels we use thecross-entropy loss, whereas for real-valued labels we use the L2 loss. For tuning the step-size, batch-size, thenumber of intervals, and the embedding dimension we use Optuna (Akiba et al., 2019). For binning, we alsotuned the choice of uniform or quantile bins. In addition, 20% of the data was held out for validation, and",
  "regression targets were standardized. Finally, for the adult income data-set, 0 has a special meaning for twocolumns, and was treated as a categorical value": "We ran 20 experiments with the tuned configurations to neutralize the effect of random initialization, andreport the mean and standard deviation of the metrics on the test set in , where it is apparent thatour approach outperforms binning on these datasets. These datasets were chosen since they contain severalnumerical fields, and are small enough to run many experiments to neutralize the effect of hyper-parameterchoice and random initialization at a reasonable computational cost, or time. They were also used in otherworks on tabular data, such as Gorishniy et al. (2021; 2022).",
  "challenges posed by integers, and especially fields representing counts, e.g. the number of visits of the user inthe last week": "We note here that modeling integers is not in the scope of this work, but a paper on recommender systemscannot go without experiments with large scale recommender-system datasets. The Criteo dataset was chosendue to its popularity and the abundance of numerical fields, even though they are integers. We point out thatother classical public recommendation datasets, such as MovieLens (GorupLens), or Avazu (Avazu, 2014), donot contain continuous or similar integer numerical fields, and therefore were incompatible for the analysis ofthis work6 Thus, what we propose doing with integers in this section is more of a heuristic adaptation of ourmethod to enable conducting an experiment with the Criteo data-set, rather than systematic treatment ofintegers that can be seen as an integral part of this paper. Integers possess properties of both discrete and continuous nature. For example, their CDF function isa step function that may have large jumps, and its not trivial to transform them to using a CDFapproximation in a reasonable manner. This is because large jumps produce large gaps in the intervalthat are not covered by any data. Moreover, features that represent counts, such as the number of timesthe user interacted with some category of items, pose an additional difficulty stemming from this hybriddiscrete-continuous nature. Smaller values are more discrete, while larger values are more continuous,i.e. the difference between users who never visited our site and users who visited it once may be large, thedifference between one and two visits will be large as well, but the difference in user behavior between 98 and99 visits is probably small. The data-set is a log of 7 days of ad impressions and clicks in chronological order. Thus, we split into training,validation, and test in the following manner: the first 5/7 of the data-set is the training set, the next 1/7 isthe validation set for hyper-parameter tuning, and the last 1/7 is the test set. Categorical features with lessthan 10 occurrences were replaced by a special \"INFREQUENT\" value for every field . When employingbinning, we use a similar strategy to the winners of the Criteo challenge - the bin index is floor(ln2(z))for z 1. Namely, the bin boundaries are {exp(",
  "i)}i=0 for z 1. Values smaller than 1 are treated ascategorical values. The lower and upper bounds are learned from the training set": "For splines, we stand on the shoulders of giants, and use a transform that resembles the squared logarithmx arcsinh2(x), followed min-max scaling. The reason is that arcsinh(x) mimics the logarithm, but is alsodefined for zero. The min-max scaling ensures that the transformed feature stays in , and is learnedfrom the training set only. If the validation or the test set contain values above the maximum observed, theyare mapped to 1. Negative values are, similarly, treated as categorical values. We note, that this transform isfar from being the empirical CDF, and due to the reasons discussed above, the empirical CDF is typicallynot applicable to integers. The histograms of the transformed columns I_1, ..., I_13 are plotted in .We can see that some of the columns appear to be quite discrete. For example, I_10 has only four distinctvalues. The columns I_11 and I_12 appear discrete as well. Thus, when conducting an experiment withcubic splines, we used them for all columns, except for the above three. We conduct experiments with k 8, 16, . . . , 64 as embedding dimensions, and each experiment is conductedusing 50 trials of Optuna (Akiba et al., 2019) with its default configuration to tune the learning rate and theL2 regularization coefficient. The models were trained using the AdamW optimizer (Loshchilov & Hutter,2019). As an ablation study, to make sure that cubic splines contribute to the the improvement we observe,we also conduct experiments with 0th order splines applied after the above transformation, since it may bethe case that the arcsinh transformation itself yields an improvement. We remind the readers that 0th ordersplines are just uniform bins. For splines, we used 20 knots, which is roughly half the number of bins obtainedby the strategy employed by the Criteo winners. The obtained test losses are summarized in . It isapparent that cubic splines outperform both uniform binning (0th order splines) and the original binningprocedure of the Criteo winners for most embedding dimensions. Moreover, we can see that cubic splinesperform best when the embedding dimension is slightly larger than the best one for binning. We conjecturethat, at least for the Criteo data-set, cubic splines require more expressive power yielded by a slightly higherembedding dimension to show their full potential.",
  "A/B test results on an online advertising system": "Here we report an online performance improvement measured using an A/B test, serving real traffic of a majoronline advertising platform. The platform applies a proprietary CTR prediction model that is closely relatedto FwFM (Sun et al., 2021). The model, which provides CTR predictions for merchant catalogue-based ads,has a recency field that measures the time (in hours) passed since the user viewed a product at the advertiserssite. We compared an implementation using our approach of continuous feature training and high-resolutionbinning during serving time described in .4 with a fine grained geometric progression of 200 binbreak points, versus the conventional binned training and serving approach used in the production modelat that time. The new model is only one of the rankers7 that participates in our ad auction. Therefore, amis-prediction means the ad either unjustifiably wins or loses the auction, both leading to revenue losses. We conducted an A/B test against the production model at that time, when our new model was serving 40%of the traffic for over six days. The new model dramatically reduced the CTR prediction error, measured as( Average predicted CTR Measured CTR 1) on an hourly basis, from an average of 21% in the baseline model, to an average of8% in the new model. The significant increase in accuracy has resulted in this model being adopted as thenew production model.",
  "Discussion": "We presented an easy to implement approach for improving the accuracy of the factorization machine familywhose input includes numerical fields. Our scheme avoids increasing the number of model parameters andover-fitting, by relying on the approximation power of splines. This is explained by the spanning propertyalong with the spline approximation theorems. Moreover, the discretization strategy described in .4allows our idea to be integrated into an existing recommendation system without introducing major changesto the production code that utilizes the model to rank items, i.e., inference. . It is easy to verify that our idea can be extended to factorization machine models of higher order (Blondelet al., 2016). This has been shown in Rgamer (2022) for vanilla higher order FMs, but can be extended tofield-informed variants of higher order FMs. In particular, the spanning property in Lemma 1 still holds,and the pairwise spanning property in Lemma 2 becomes q-wise spanning property from machines of order q.However, to keep the paper focused and readable, we keep the analysis out of the scope of this paper. With many advantages, our approach is not without limitations. We do not eliminate the need for dataresearch and feature engineering, which is often required when working with tabular data, since the data stillneeds to be analyzed to fit a function that roughly resembles the empirical CDF. We believe that featureengineering becomes easier and more systematic, but some work still has to be done. Finally, we would like to note two drawbacks. First, our approach slightly reduces interpretability, since wecannot associate a feature with a corresponding learned latent vector. Second, our approach may not beapplicable to all kinds of numerical fields. For example, consider a product recommendation system with aproduct price field. Usually higher prices mean a different category of products, leading to a possibly differenttrend of user preferences. In that case, the optimal segmentized output as a function of the products price isprobably far from having small (higher order) derivatives, and thus cubic splines may perform poorly, andpossibly even worse than simple binning. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining, 2019.",
  "Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song dataset.2011": "Mathieu Blondel, Akinori Fujino, Naonori Ueda, and Masakazu Ishihata. Higher-order factorization machines.In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural InformationProcessing Systems, volume 29. Curran Associates, Inc., 2016. URL Tianqi Chen and Carlos Guestrin.Xgboost: A scalable tree boosting system.In Proceedings of the22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16, pp.785794, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450342322. doi:10.1145/2939672.2939785. URL Yuan Cheng. Dynamic explicit embedding representation for numerical features in deep ctr prediction. InProceedings of the 31st ACM International Conference on Information & Knowledge Management, pp.38883892, 2022. Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. InProceedings of the 10th ACM Conference on Recommender Systems, RecSys 16, pp. 191198, New York, NY,USA, 2016. Association for Computing Machinery. ISBN 9781450340359. doi: 10.1145/2959100.2959190.URL",
  "Gottlob Frege. Grundgesetze der Arithmetik: begriffsschriftlich abgeleitet, volume 1. H. Pohle, 1893": "Joo Gama and Carlos Pinto. Discretization from data streams: Applications to histograms and data mining.In Proceedings of the 2006 ACM Symposium on Applied Computing, SAC 06, pp. 662667, New York, NY,USA, 2006. Association for Computing Machinery. ISBN 1595931082. doi: 10.1145/1141277.1141429. URL Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning mod-els for tabular data.In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1893218943. Cur-ran Associates, Inc., 2021. URL",
  "GorupLens. GroupLens datasets. [Online; accessed 01-Apr-2024]": "Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, and Xiuqiang He. An embedding learningframework for numerical features in ctr prediction. In Proceedings of the 27th ACM SIGKDD Conferenceon Knowledge Discovery & Data Mining, pp. 29102918, 2021. Noah Hollmann, Samuel Mller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer thatsolves small tabular classification problems in a second. In NeurIPS 2022 First Table RepresentationWorkshop, 2022. URL",
  "Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modelingusing contextual embeddings, 2020": "Yuchin Juan, Yong Zhuang, Wei-Sheng Chin, and Chih-Jen Lin. Field-aware factorization machines forctr prediction. In Proceedings of the 10th ACM Conference on Recommender Systems, RecSys 16, pp.4350, New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450340359. doi:10.1145/2959100.2959134. URL Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.Lightgbm: A highly efficient gradient boosting decision tree. In I. Guyon, U. Von Luxburg, S. Bengio,H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information ProcessingSystems, volume 30. Curran Associates, Inc., 2017. URL Ron Kohavi. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In Proceedings of theSecond International Conference on Knowledge Discovery and Data Mining, KDD96, pp. 202207. AAAIPress, 1996.",
  "R Kelley Pace and Ronald Barry. Sparse spatial autoregressions. Statistics & Probability Letters, 33(3):291297, 1997": "Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, and Quan Lu. Field-weighted factorization machines for click-through rate prediction in display advertising. In Proceedings ofthe 2018 World Wide Web Conference, WWW 18, pp. 13491357, New York, NY, USA, 2018. Associationfor Computing Machinery. ISBN 9781450356398. doi: 10.1145/3178876.3186040. URL",
  "Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,81:8490, 2022": "Gowthami Somepalli, Avi Schwarzschild, Micah Goldblum, C. Bayan Bruss, and Tom Goldstein. SAINT:Improved neural networks for tabular data via row attention and contrastive pre-training. In NeurIPS 2022First Table Representation Workshop, 2022. URL Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. Autoint:Automatic feature interaction learning via self-attentive neural networks. In Proceedings of the 28th ACMInternational Conference on Information and Knowledge Management, CIKM 19, pp. 11611170, New York,NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369763. doi: 10.1145/3357384.3357925.URL Yang Sun, Junwei Pan, Alex Zhang, and Aaron Flores. Fm2: Field-matrixed factorization machines forrecommender systems. In Proceedings of the Web Conference 2021, WWW 21, pp. 28282837, New York,NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383127. doi: 10.1145/3442381.3449930.URL",
  "Joaquin Vanschoren, Jan N Van Rijn, Bernd Bischl, and Luis Torgo. Openml: networked science in machinelearning. ACM SIGKDD Explorations Newsletter, 15(2):4960, 2014": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, EvgeniBurovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stfan J. van der Walt, Matthew Brett,Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, EricLarson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold,Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antnio H. Ribeiro,Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms forScientific Computing in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2.",
  "A.1Proof of the spanning property (Lemma 1)": "Proof. For some vector q, denote by qa:b the sub-vector (qa, . . . , qb). Assume w.l.o.g. that f = 1, and thatfield 1 has the value z. By construction in equation 2, we have y1 = i=1 wiBi(z), while the remainingcomponents y2:n do not depend on z. Thus, the we have",
  "A.2Proof of the pairwise spanning property (Lemma 2)": "Proof. Recall that we need to rewrite equation 2 as a function of ze, zf, which are the values of the fields eand f. Assume w.l.o.g. that e = 1, f = 2. By construction in equation 2, we have y1 = i=1 wiBi(z1), andy2 = i=1 wiCi(z2), while the remaining components y3:n do not depend on z. Thus, the we have",
  "(6)": "By following similar logic to 4, one can obtain a similar expression when looking at the partial sums thatinclude the interaction between f (resp. e) and all other fields except e (resp. f). Observe that the interactionbetween all other values does not depend on z1, z2. Given all of this, we show how to rewrite the interactionas a function of z1, z2:"
}