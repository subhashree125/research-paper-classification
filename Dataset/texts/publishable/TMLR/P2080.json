{
  "Abstract": "Graphs are a natural representation for systems based on relations between connected en-tities.Combinatorial optimization problems, which arise when considering an objectivefunction related to a process of interest on discrete structures, are often challenging dueto the rapid growth of the solution space. The trial-and-error paradigm of ReinforcementLearning has recently emerged as a promising alternative to traditional methods, such asexact algorithms and (meta)heuristics, for discovering better decision-making strategies in avariety of disciplines including chemistry, computer science, and statistics. Despite the factthat they arose in markedly different fields, these techniques share significant commonalities.Therefore, we set out to synthesize this work in a unifying perspective that we term GraphReinforcement Learning, interpreting it as a constructive decision-making method for graphproblems. After covering the relevant technical background, we review works along the di-viding line of whether the goal is to optimize graph structure given a process of interest, orto optimize the outcome of the process itself under fixed graph structure. Finally, we discussthe common challenges facing the field and open research questions. In contrast with othersurveys, the present work focuses on non-canonical graph problems for which performantalgorithms are typically not known and Reinforcement Learning is able to provide efficientand effective solutions.",
  "Introduction": "Graphs are a mathematical concept created for formalizing systems of entities (nodes) connected by relations(edges). Going beyond raw topology, nodes and edges in graphs are often associated with attributes: forexample, an edge can be associated with the value of a distance metric (Barthlemy, 2011). Enriched withsuch features, graphs become powerful formalisms able to represent a variety of systems. This flexibility ledto their usage in fields as diverse as computer science, biology, and the social sciences (Newman, 2018).",
  "Statistics": ": Visual summary of the structure and topics of the present survey. G and K denote the sets ofpossible graph structures and graph control actions, respectively; F is a real-valued objective function thatserves as the optimization target. The goal for Structure Optimization is to find the optimal structure G,while Process Optimization involves finding a set of optimal control actions . characterization of processes taking place on a graph, a natural question that arises is how to intervene inthe network in order to optimize the outcome of a given process. Such combinatorial optimization problemsover discrete structures are typically challenging due to the rapid growth of the solution space. A well-known example is the Traveling Salesperson Problem (TSP), which asks to find a Hamiltonian cycle in afully-connected graph such that the cumulative path length is minimized. In recent years, Machine Learning (ML) has started to emerge as a valuable tool in approaching combinatorialoptimization problems, with researchers in the field anticipating its impact to be transformative (Bengioet al., 2021; Cappart et al., 2023). Most notably, the paradigm of Reinforcement Learning (RL) has shownthe potential to discover, by trial-and-error, algorithms that can outperform traditional exact methods and(meta)heuristics. A common pattern is to express the problem of interest as a Markov Decision Process(MDP), in which an agent incrementally constructs a solution, and is rewarded according to its ability tooptimize the objective function. Starting from the MDP formulation, a variety of RL algorithms can betransparently applied, rendering this approach very flexible in terms of the problems it can address. Inparallel, works that address graph combinatorial optimization problems with RL have begun to emerge in avariety of scientific disciplines spanning chemistry (You et al., 2018a), computer science (Valadarsky et al.,2017), economics (Darvariu et al., 2021b), and statistics (Zhu et al., 2020), to name but a few. The goal of this survey is to present a unified framework, which we term Graph RL, for combinatorial decision-making problems over graphs. Indeed, recent surveys have focused on works that apply RL to canonicalproblems, a term we use to refer to problems which have been intensely studied, possibly for decades. Forexample, research on solving the aforementioned TSP alone dates back nearly 70 years to the paper of Dantziget al. (1954), and very effective algorithms exist for solving the problem optimally (Applegate et al., 2009)or approximately (Lin & Kernighan, 1973; Helsgaun, 2000) for instances with up to tens of millions of nodes.Other notable examples of canonical problems addressed in the RL literature include Maximum IndependentSets (Ahn et al., 2020), Maximum Cut (Khalil et al., 2017; Ahn et al., 2020), as well as routing problems suchas the Vehicle Routing Problem (VRP) (Kool et al., 2019; Kim & Park, 2021). With a few exceptions, eventhough work on such benchmark problems is important for pushing the limitations of ML-based methods,currently they show inferior performance to well-established, highly optimized heuristic and exact solvers.",
  "Published in Transactions on Machine Learning Research (08/2024)": "David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, JulianSchrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Dieleman, DominikGrewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap, Madeleine Leach, KorayKavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networksand tree search. Nature, 529(7587):484489, 2016. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, ThomasHubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy P. Lillicrap, Fan Hui, LaurentSifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Mastering the game of Go withouthuman knowledge. Nature, 550(7676):354359, 2017. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, MarcLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, andDemis Hassabis. A general reinforcement learning algorithm that masters chess, shogi, and Go throughself-play. Science, 362(6419):11401144, 2018.",
  "Background": "In this section, we cover the key concepts that underpin the works treated by this survey. We begin withdiscussing graph fundamentals, combinatorial optimization problems over graphs, and traditional techniquesthat have been used to address them. Next, we give a high-level overview of the potential of using MLfor tackling these problems, focusing especially on the RL paradigm for learning reward-driven behavior.We then discuss the fundamentals of RL, including some of the algorithms that have been applied in this",
  "Graphs and Combinatorial Optimization": "Graphs, also called networks, are the underlying mathematical objects that are the focus of the presentsurvey.2 We denote a graph G as the tuple (V, E), where V is a set of nodes or vertices that are used todescribe the entities that are part of the system, and E is a set of edges that represent connections andrelationships between the entities. We indicate an element of the set V with v or vi and an element of Ewith e or ei,j, with the latter indicating the edge between the nodes vi and vj. The adjacency matrix isdenoted by A. Nodes and edges may optionally have attribute vectors associated with them, which we denoteas xv and xe respectively. These can capture various aspects of the problem of interest depending on theapplication domain, and may be either static or dynamic. Some examples include geographical coordinatesin a space, the on-off status of a node, and the capacity of an edge for transmitting information or a physicalquantity. Equipped with such attributes, graphs become a powerful mathematical tool for studying a varietyof systems. Methods from network science (Newman, 2018) allow us to formally characterize processes taking placeover a graph. For example, decision-makers might be interested in the global structural properties such asthe efficiency with which the network exchanges information, or its robustness when network elements fail,aspects crucial to infrastructure networks (Latora & Marchiori, 2001; Albert et al., 2000). One can also usethe graph formalism to model flows of quantities such as packets or merchandise, relevant in a variety ofcomputer and logistics networks (Ahuja, 1993). Taking a decentralized perspective, we may be interested inthe individual and society-level outcome of network games, in which a network connects individuals thattake selfish decisions in order to maximize their gain (Jackson & Zenou, 2015). Suppose that we consider such a global process and aim to optimize its outcome by intervening in thenetwork. For example, a local authority might decide to add new connections to a road network with thegoal of minimizing congestion, or a policy-maker might intervene in a social network in order to encouragecertain outcomes. These are combinatorial optimization problems, which involve choosing a solution out ofa large, discrete space of possibilities such that it optimizes the value of a given objective function. denotedas F in the remainder of this work. Conceptually, such problems are easy to define but very challenging tosolve, since one cannot simply enumerate all possible solutions beyond the smallest of graphs. Combinatorial optimization problems bear relevance in many areas the TSP, for example, has foundapplications in circuit design (Chan & Mercier, 1989) and bioinformatics (Agarwala et al., 2000). A significantbody of work is devoted to solving them. The lines of attack for such problems can be divided into thefollowing categories: Exact methods: approaches that solve the problem exactly, i.e., will find the globally optimal solutionif it exists. Notably, if the problem of interest has a linear objective, one can formulate it as an(integer) linear program (Thie & Keough, 2011), for which efficient solving methods such as thesimplex method (Dantzig & Thapa, 1997) and branch-and-bound (Land & Doig, 1960) exist. 2Regarding the difference between the terms, graph is more accurately used to refer to the mathematical abstraction,while network refers to a realization of this general concept, such as a particular social network. The terms are synonymousin general usage (Barabsi, 2016, Chapter 2.2) and we use them interchangeably in the remainder of this work.",
  "Machine Learning for Combinatorial Optimization": "In recent years, ML has started to emerge as a valuable tool in approaching combinatorial optimizationproblems, with researchers in the field anticipating its impact to be transformative (Bengio et al., 2021;Cappart et al., 2023). Worthy of note are the following relationships and points of integration at theintersection of ML and combinatorial optimization: 1. ML models can be used to imitate and execute known algorithms. This can be exploited for ap-plications where latency is critical and decisions must be made quickly typically the realm ofwell-tuned heuristics. Furthermore, the parametrizations of some ML models can be formulatedindependently of the size of the problem instance and applied to larger instances than seen duringtraining, including those with sizes beyond the reach of the known algorithm. 2. ML models can improve existing algorithms by data-driven learning for enhancing components ofclassic algorithms, replacing hand-crafted expert knowledge. Examples include, for exact methods,learning to perform variable subset selection in Column Generation (Morabit et al., 2021) or biasingvariable selection in branch-and-cut for Mixed Integer Linear Programs (Khalil et al., 2022). 3. ML can enable the discovery of new algorithms through the use of Reinforcement Learning (RL),another ML paradigm. Broadly speaking, RL is a mechanism for producing goal-directed behaviorthrough trial-and-error (Sutton & Barto, 2018). In this framework, one formulates the problem of",
  "Decision-making Processes and Solution Methods": "Let us first discuss discuss decision-making processes and approaches for solving them. We begin by definingthe key elements of Markov Decision Processes. We also give a broad overview of solution methods forMDPs and discuss some conceptual axes along which they may be compared and contrasted. We thencover several relevant methods for constructing a policy, including those that perform policy iteration, learna policy directly, or perform planning from a state of interest. The goal for this subsection is to act as a concise, self-contained introduction to the landscape of RLalgorithms and to draw connections to combinatorial optimization problems defined over graphs. As canbe seen in Sections 3 and 4, Graph RL approaches presented in the literature rely on a wide variety ofalgorithms, each of them characterized by different assumptions and principles. Even though it is not possibleto exhaustively cover all algorithms, we consider it necessary to present some of the important methods forsolving MDPs, in order to introduce the core terms and notation used in the later sections. Furthermore,in .1.3, we offer some practical guidance about which RL algorithm to choose depending on thecharacteristics of the problem.",
  "Markov Decision Processes": "RL refers to a class of methods for producing goal-driven behavior. In broad terms, decision-makers calledagents interact with an uncertain environment, receiving numerical reward signals; their objective is to adjusttheir behavior in such a way as to maximize the sum of these signals. Modern RL bases its origins in optimalcontrol and Dynamic Programming methods for solving such problems (Bertsekas, 1995) and early work",
  "is a discount factor that controls the agents preference for immediate versus delayedreward": "A trajectory S0, A0, R1, S1, A1, R2, ...ST 1, AT 1, RT is defined by the sequence of the agents interactionswith the environment until the terminal timestep T. The return Ht = Tk=t+1 kt1Rk denotes the sumof (possibly discounted) rewards that are received from timestep t until termination. We also define a policy(a|s), a distribution of actions over states which fully specifies the behavior of the agent. Given a particularpolicy , the value function V(s) is defined as the expected return when following the policy in state s.Similarly, the action-value function Q(s, a) is defined as the expected return when starting from s, takingaction a, and subsequently following . There exists at least one policy , called the optimal policy, which has an associated optimal action-value function Q, defined as max Q(s, a).The Bellman optimality equation Q(s, a) = E[Rt+1 +Q(St+1, At+1)|St = s, At = a] is satisfied by the optimal action-value functions. Solving this equationprovides a possible route to finding an optimal policy and, hence, solving the RL problem.",
  "Rewards are the negative of the total cost of the tour at the final step, and 0 otherwise": "The policy will therefore specify a probability distribution over the next cities to be visited given the citiesvisited so far. Consequently, the value function V(s) indicates the average total cost of a tour that wecan expect to obtain when using to select the next city when considering the current partial solution s.Starting from this formulation, we can therefore apply an RL algorithm to learn in order to solve the TSP.",
  "Policy Iteration Methods": "A common framework underlying many RL algorithms is that of Policy Iteration (PI). It consists of twophases that are applied alternatively, starting from a policy . The first phase is called policy evaluation andaims to compute the value function by updating the value of each state iteratively. The second phase, policyimprovement, refines the policy with respect to the value function, most commonly by acting greedily withrespect to it. Under certain conditions, this scheme is proven to converge to the optimal value function andoptimal policy (Sutton & Barto, 2018). Generalized Policy Iteration (GPI) refers to schemes that combineany form of policy evaluation and policy improvement. Let us look at some concrete examples of algorithms. Dynamic Programming (DP) applies the PI schemeas described above. It is one of the earliest solutions developed for MDPs (Bellman, 1957). It leveragesthe principle that sequential decision-making problems can be broken down into subproblems. The optimalsolutions to the subproblems, once found, can be recursively combined to solve the original problem muchmore efficiently compared to algorithms that do not exploit this structure. Beyond an algorithmic paradigm,",
  "DP is also an exact method for combinatorial optimization as defined in .1, which highlights theshared foundation of RL and classic combinatorial optimization methods": "Since DP involves updating the value of every state, it is computationally intensive, and, for this reason, itis not appropriate for large problem instances. Additionally, it requires full knowledge of the transition andreward functions. Taken together, these characteristics limit its applicability, motivating the developmentof RL algorithms that do not require the exploration of the entirety of the state space and are able to learndirectly from samples of interactions with the environment. The Q-learning (Watkins & Dayan, 1992) algorithm is an off-policy TD method that follows the GPIblueprint. It is proven to converge to the optimal value functions and policy in the tabular case with discreteactions, so long as, in all the states, all actions have a non-zero probability of being sampled (Watkins &Dayan, 1992). The agent updates its estimates according to:",
  "Q(s, a) Q(s, a) + r + maxaA(s) Q(s, a) Q(s, a)(1)": "In the case of high-dimensional state and action spaces, a popular means of generalizing across similarstates and actions is to use a function approximator for estimating Q(s, a). An early example of such atechnique is the Neural Fitted Q-iteration (NFQ) (Riedmiller, 2005), which uses a neural network. TheDQN algorithm (Mnih et al., 2015), which improved NFQ by use of an experience replay buffer and aniteratively updated target network for state-action value function estimation, has yielded state-of-the-artperformance in a variety of domains ranging from general game-playing to continuous control (Mnih et al.,2015; Lillicrap et al., 2016). A variety of general and problem-specific improvements over DQN have been proposed (Hessel et al., 2018).Prioritized Experience Replay weighs samples of experience proportionally to the magnitude of the encoun-tered TD error (Schaul et al., 2016), arguing that such samples are more important for the learning process.Double DQN uses two separate networks for action selection and Q-value estimation (van Hasselt et al.,2016) to address the overestimation bias of standard Q-learning. Distributional Q-learning (Bellemare et al.,2017) models the distribution of returns, rather than only estimating the expected value. DQN has beenextended to continuous actions via the DDPG algorithm (Lillicrap et al., 2016), which features an additionalfunction approximator to estimate the action which maximizes the Q-value. TD3 (Fujimoto et al., 2018)is an extension of DDPG that applies additional tricks (clipped double Q-learning, delayed policy updates,and smoothing the target policy) that improve stability and performance over standard DDPG.",
  "tR(St, At)(2)": "If we define the quantity under the expectation as J, the goal is to adjust the parameters in such a waythat the value of J is maximized. We can perform gradient ascent to improve the policy in the direction ofactions that yield high return, formalizing the notion of trial-and-error. The gradient of J with respect to can be written as:",
  "Search and Decision-Time Planning Methods": "Recall the fact that, in the case of model-based methods, we have access to the (possibly estimated) transitionfunction P and (possibly estimated) reward function R. This means that an agent does not necessarily needto interact with the world and learn directly through experience. Instead, the agent may use the modelin order to plan the best course of action, and subsequently execute the carefully thought-out plan in theenvironment. Furthermore, decision-time planning concerns itself with constructing a plan starting from thecurrent state that the agent finds itself in, rather than devise a policy for the entire state space. To achieve this, the agent can perform rollouts using the model, and use a search technique to find theright course of action. Search has been one of the most widely utilized approaches for building intelligentagents since the dawn of AI (Russell & Norvig, 2020, Chapters 3-4). Methods range from relatively simplein-order traversal (e.g., Breadth-First Search and Depth-First Search) to variants that incorporate heuristics(e.g., A*). It has been applied beyond single-agent discrete domains to playing multi-player games (Russell& Norvig, 2020, Chapter 5). Such games have long been used as a Drosophila of Artificial Intelligence,with search playing an important part in surpassing human-level performance on many tasks (Nash, 1952;Campbell et al., 2002; Silver et al., 2016). Search methods construct a tree in which nodes are MDP states. Children nodes correspond to the statesobtained by applying a particular action to the state at the parent node, while leaf nodes correspond toterminal states, from which no further actions can be taken. The root of the search tree is the current state.The way in which this tree is expanded and navigated is dictated by the particulars of the search algorithm. In many applications, however, the branching factor b and depth d of the search tree make it impossible toexplore all paths, or even perform a Greedy Search of depth 1. There exist proven ways of reducing thisspace, such as alpha-beta pruning. However, its worst-case performance is still O(bd) (Russell & Norvig,2020, Chapter 5.3). A different approach to breaking the curse of dimensionality is to use random (alsocalled Monte Carlo) rollouts: to estimate the goodness of a position, run random simulations from a treenode until reaching a terminal state (Abramson, 1990; Tesauro & Galperin, 1997). Monte Carlo Tree Search (MCTS) is a model-based planning technique that addresses the inability to exploreall paths in large MDPs by constructing a policy from the current state (Sutton & Barto, 2018, Chapter 8.11).It relies on two core principles: firstly, that the value of a state can be estimated by sampling trajectoriesand, secondly, that the returns obtained by this sampling are informative for deciding the next action at the",
  "root of the search tree. We review its basic concepts below and refer the interested reader to (Browne et al.,2012) for more information": "In MCTS, each node in the search tree stores several statistics such as the sum of returns and the node visitcount in addition to the state. For deciding each action, the search task is given a computational budgetexpressed in terms of node expansions or wall clock time.The algorithm keeps executing the followingsequence of steps until the search budget is exhausted:",
  ". Selection: The tree is traversed iteratively from the root until an expandable node (i.e., a nodecontaining a non-terminal state with yet-unexplored actions) is reached": "2. Expansion: From the expandable node, one or more new nodes are constructed and added to thesearch tree, with the expandable node as the parent and each child corresponding to a valid actionfrom its associated state. The mechanism for selection and expansion is called tree policy, and it istypically based on the node statistics. 3. Simulation: Trajectories in the MDP are sampled from the new node until a terminal state isreached and the return (discounted sum of rewards) is recorded. The default policy or simulationpolicy dictates the probability of each action, with the standard version of the algorithm simplyusing uniform random sampling of valid actions. We note that the intermediate states encounteredwhen performing this sampling are not added to the search tree.",
  ". Backpropagation: The return is backpropagated from the expanded node upwards to the root ofthe search tree, and the statistics of each node that was selected by the tree policy are updated": "The tree policy used by the algorithm needs to trade off exploration and exploitation in order to balanceactions that are already known to lead to high returns against yet-unexplored paths in the MDP for whichthe returns are still to be estimated. The exploration-exploitation trade-off has been widely studied in themulti-armed bandit setting, which may be thought of a single-state MDP. A representative method is theUpper Confidence Bound (UCB) algorithm (Auer et al., 2002), which computes confidence intervals for eachaction and chooses, at each step, the action with the largest upper bound on the reward, embodying theprinciple of optimism in the face of uncertainty. Upper Confidence Bounds for Trees (UCT) (Kocsis & Szepesvri, 2006) is a variant of MCTS that appliesthe principles behind UCB to the tree search setting. Namely, the selection decision at each node is framedas an independent multi-armed bandit problem. At decision time, the tree policy of the algorithm selectsthe child node corresponding to action a that maximizes",
  "ln C(s)C(s, a) ,(4)": "where ra is the mean reward observed when taking action a in state s, C(s) is the visit count for the parentnode, C(s, a) is the number of child visits, and UCT is a constant that controls the level of exploration (Kocsis& Szepesvri, 2006). MCTS is easily parallelizable either at root or leaf level, which makes it a highly practical approach indistributed settings. It is also a generic framework that does not make any assumptions about the charac-teristics of the problem at hand. The community has identified ways in which domain heuristics or learnedknowledge (Gelly & Silver, 2007) can be integrated with MCTS such that its performance is enhanced. Thisincludes models learned with RL based on linear function approximation (Silver, 2007) as well as deep neuralnetworks (Guo et al., 2014). MCTS has been instrumental in achieving state-of-the-art performance in domains previously thought in-tractable. It has been applied since its inception to the game of Go, perceived as a grand challenge forArtificial Intelligence. The main breakthrough in this space was achieved by combining search with deepneural networks for representing policies (policy networks) and approximating the value of positions (value",
  "Artificial Neural Networks on Graphs": "In this section, we discuss how one can represent graphs and tame their discrete structure for combinato-rial optimization tasks. We begin with a broad overview of graph representation learning and traditionalapproaches. We then cover GNNs and related deep approaches for embedding graphs. We refer the in-terested reader to (Hamilton, 2020) for an introduction to graph representation learning, which covers thesetechniques in more detail. Furthermore, the work of (Cappart et al., 2023) discusses the use of such methods,particularly GNNs, for combinatorial optimization.",
  "Graph Representation Learning": "As mentioned previously, graphs are the structure of choice for representing information in many other fieldsand are able to capture interactions and similarities.The success of neural networks, however, did notimmediately transfer to the domain of graphs as there is no obvious equivalent for this class of architectures:graphs do not necessarily present the same type of local statistical regularities (Bronstein et al., 2017). Suppose we wish to represent a graph as a vector input x that can be fed to a ML model, such as an MLP.An obvious choice is to take the adjacency matrix A and apply vectorization in order to transform it to acolumn vector. In doing so, however, two challenges become apparent. One is the fact that by relabeling thenodes in the graph according to a permutation, the input vector is modified substantially, and may result ina very different output when fed to a model, even though the structure has remained identical. Furthermore,if a new node joins the network, our previous model is no longer applicable by default since the shape of theinput vector has changed. How might we address this, and design a better graph representation? Such questions are studied in the field of graph representation learning (Hamilton et al., 2017a). Broadlyspeaking, the field is concerned with learning a mapping that translates the discrete structure of graphsinto vector representations with which downstream ML approaches can work effectively. Work in this areais focused on deriving vector embeddings for a node, a subgraph, or an entire graph. A distinction can bedrawn between shallow and deep embedding methods (Hamilton et al., 2017a). The former category consistsof manually-designed approaches such as those using local node statistics, characteristic graph matrices, or",
  "graph kernels. In the latter category, methods typically use a deep neural network trained with gradientdescent, and learn the representation in a data-driven fashion": "The literature on shallow embeddings is extensive. Prior work has considered factorizations of characteristicgraph matrices (Belkin & Niyogi, 2002; Ou et al., 2016). Another class of approaches constructs embeddingsbased on random walks: nodes will have similar embeddings if they occur along similar random paths inthe graphs. Methods such as DeepWalk and node2vec (Perozzi et al., 2014; Grover & Leskovec, 2016) fallinto this category. Other approaches such as struc2vec (Ribeiro et al., 2017) and GraphWave (Donnat et al.,2018) assign similar embeddings to nodes that fulfil a similar structural role within the graph (e.g., hub),irrespective of their proximity.",
  "Deep Graph Embedding Methods": "Broadly speaking, deep embedding methods rely on the idea of neighborhood aggregation: the representationof a node is determined in several rounds of aggregating the embeddings and features of its neighbors, towhich a non-linear activation function is applied. This principle is illustrated in . The learning architectures are usually parameter-sharing: the manner of performing the aggregation and thecorresponding weights are the same for the entire network. Another important aspect for deep embeddingmethods is that they can incorporate task-specific supervision: the loss corresponding to the decoder can beswapped for, e.g., cross-entropy loss in the case of classification tasks. Many deep representation learning techniques can be used to construct representations for a graph or sub-graph starting from the embeddings of the nodes. Several variants that have been considered in the literatureinclude: performing a sum or mean over node embeddings in a subgraph (Duvenaud et al., 2015; Dai et al.,2016), introducing a dummy node (Li et al., 2017), using layers that perform clustering (Defferrard et al.,2016), or learning the hierarchical structure end-to-end (Ying et al., 2018). Several works have proposed increasingly feasible versions of convolutional filters on graphs, based primarilyon spectral properties (Bruna et al., 2014; Henaff et al., 2015; Defferrard et al., 2016; Kipf & Welling, 2017)or their approximations. An alternative line of work is based on message passing on graphs (Sperduti &Starita, 1997; Scarselli et al., 2009) as a means of deriving vector embeddings. Both Message Passing NeuralNetworks (Gilmer et al., 2017) and Graph Networks (Battaglia et al., 2018) are attempts to unify relatedmethods in this space, abstracting the commonalities of existing approaches with a set of primitive functions.The term Graph Neural Network is used in the literature, rather loosely, as an umbrella term to mean adeep embedding method. Let us now take a closer look at some GNN terminology and variants that are relevant to the present survey.Recall that we are given a graph G = (V, E) in which nodes vi are equipped with feature vectors xvi and,optionally, with edge features xei,j. The goal is to derive an embedding vector hvi for each node that captures",
  "h(l+1)vi= U (l) h(l)vi , m(l+1)vi(5)": "where N(vi) is the open neighborhood of node vi. Subsequently, a readout function I is applied to computean embedding for the entire graph from the set of final node embeddings: I({h(L)vi |vi V }). The messageand vertex update functions are learned and differentiable, e.g., some form of MLP. The readout functionmay either be learned or fixed a priori (e.g., summing the node embeddings). A desirable property for it isto be invariant to node permutations. We now discuss the details of 3 popular GNNs in this area. Due to size and scope limitations, we do notdiscuss other architectures such as GraphSAGE (Hamilton et al., 2017b) or GIN (Xu et al., 2018a), for whichwe refer the reader to their original papers. structure2vec.structure2vec (S2V) (Dai et al., 2016) is one of the earlier GNN variants, and it is inspiredby probabilistic graphical models (Koller & Friedman, 2009).The core idea is to interpret each nodein the graph as a latent variable in a graphical model, and to run inference procedures similar to meanfield inference (Wainwright & Jordan, 2008) and loopy belief propagation (Pearl, 1988) to derive vectorembeddings. Additionally, the approach replaces the traditional probabilistic operations (sum, product,and renormalization) used in the inference procedures with nonlinear functions (namely, neural networks),yielding flexibility in the learned representation. It was shown to perform well for classification and regressionin comparison to other graph kernels, as well as to be able to scale to medium-sized graphs representingchemical compounds and proteins.",
  "(6)": "where W1, W2 are weight matrices that parameterize the model. We note that there are two differences tothe other architectures discussed in this section. Firstly, the weight matrices are not indexed by the layersuperscript, since they are shared between all the layers. Additionally, the node features xvi appear in themessage-passing step of every layer, rather than only being used to initialize the embeddings. A possiblealternative is to use vectors of zeros for initialization, i.e., h(0)vi = 0 vi V . Graph Convolutional Network.The Graph Convolutional Network (GCN) method (Kipf & Welling,2017) is substantially simpler in nature, relying merely on the multiplication of the node features with a weightmatrix, together with a degree-based normalization. It is motivated as a coarse, first-order, approximationof localized spectral filters on graphs (Defferrard et al., 2016). Since it can be formulated as a series ofmatrix multiplications, it has been shown to scale well to large graphs with millions of edges, while obtainingsuperior performance to other embedding methods at the time. It can be formulated as:",
  "where deg(vi) indicates the degree of node vi, and N[vi] is the closed neighborhood of node vi, which includesall its neighbors and vi itself": "Graph Attention Network.Note how, in the GCN formula above, the summation implicitly performs arigid weighting of the neighboring nodes features. The Graph Attention Network (GAT) model (Velikoviet al., 2018) proposes the use of attention mechanisms (Bahdanau et al., 2016) as a way to perform flexibleaggregation of neighbor features instead. Learnable aggregation coefficients enable an increase in modelexpressibility, which also translates to gains in predictive performance over the GCN for node classification.",
  "vkN[vi] expLeakyReLUTW(l)1 h(l)vi W(l)1 h(l)vk W(l)2 xei,k(8)": "where exp(x) = ex is the exponential function, is a weight vector that parameterizes the attention mech-anism, and [] denotes concatenation. The LeakyReLU(x) activation function, which outputs non-zerovalues for negative inputs according to a small slope LR, is equal to LRx if x < 0, and x otherwise. Giventhe attention coefficients, node embeddings are computed according the rule below.",
  "Connections Between RL and Graph Representation Learning": "Given the RL algorithms and graph representation learning techniques covered up to this point, let us take theopportunity to discuss the relationships between them in the context of the Graph RL framework. Learningtechniques designed for operating on graphs are commonly used as function approximators as part of theRL algorithms. However, not all works covered by the survey employ function approximation notably,some of the works that use MCTS-based approaches do not.Furthermore, out of the works employingfunction approximation, a few opt for different learning architectures through problem-specific justificationsor experimental validation. Generic architectures such as MLPs, LSTMs, and Transformers are sometimesemployed. Nevertheless, as discussed in our inclusion criteria in , we require that works formulategraph combinatorial optimization problems as MDPs, but we do not restrict ourselves to particular forms offunction approximation. Therefore, the distinction between Graph RL and graph representation learning is indeed blurred, as thelatter is often (but not always) a component of the former. When Graph RL solutions do leverage graphrepresentation learning for function approxmation, they are a means of obtaining generalization to unseeninstances, as well as for scaling up to larger instances in terms of number of nodes and edges and thecomplexity of the considered process taking place on the graph. Furthermore, we note the following differencebetween the use of graph representation learning techniques as function approximators in this context versus",
  "Graph Structure Optimization": "A shared characteristic of the work on ML for canonical graph combinatorial optimization problems is thatthey usually do not involve topological changes to the graph. Concretely, one needs to find a solution whileassuming that the network structure remains fixed. The problem of learning to construct a graph or tomodify its structure so as to optimize a given objective function has received comparatively less attention inthe ML literature. In this section, we review works that treat the problem of modifying graph topology inorder to optimize a quantity of interest, and use RL for discovering strategies for doing so. This is performedthrough interactions with an environment. At a high level, such problems can be formulated as finding the graph G satisfying argmaxGG F(G), whereG is the set of possible graphs to be searched and F, as previously mentioned, is the objective function. Weillustrate the process in . The precise framing depends on the problem and may entail choosingbetween starting from an empty graph versus an existing one, and enforcing constraints on the validity ofgraphs such as spatial restrictions, acyclicity, or planarity. As shown in , the design of the actionspace can also vary. The agent may be allowed to perform edge additions, removals, and rewirings, or somecombination thereof. Given the scope of the survey as defined in , we exclude several works which, at a high level,share notable similarities.Let us briefly discuss two strands of work that are related yet out-of-scope.Firstly, several works in the ML literature have considered the generation of graphs with similar propertiesto a provided dataset. This is typically performed using a deep generative model, and may be seen as anML-based alternative for the classic graph generative models such as that of Barabsi & Albert (1999).",
  "Attacking Graph Neural Networks": "The goal in this line of work is learning to modify the topology of a graph through edge additions andremovals so as to induce a deep graph or node-level classifier to make labeling errors. The problem can bethought of as the graph-based equivalent of finding adversarial perturbations for image classifiers based ondeep neural networks (Biggio et al., 2013; Szegedy et al., 2014). Traditional approaches for adversarial attacks can be divided into white-box methods (which assume accessto the internals of the classifier, such as gradients) and black-box methods (which do not require this priv-ileged information). For the latter setting, metaheuristics that perform gradient-free optimization, such asevolutionary algorithms, can be leveraged (Su et al., 2019). Additionally, an attack based on random edgerewiring can also be considered; this can induce substantial classification errors despite its apparent simplic-ity (Dai et al., 2018). Recent works have therefore considered using RL to improve on the performance ofthese attack methods in the absence of access to the internal state of classifiers.",
  "E": ": Illustration of several action space designs for Graph Structure Optimization. Edge addition, re-moval, and rewiring can be formulated as the selection of a single node per timestep, yielding an O(|V |)action space. The topological changes are encapsulated in the definition of the transition function. Con-straints are commonly used to exclude invalid actions (e.g., the actions corresponding to the addition of anedge that is already part of the graph will be forbidden). The work of Dai et al. (2018) was the first to formalize and address this task. The approach proposed bythe authors, called RL-S2V, is a variant of the algorithm proposed by Khalil et al. (2017) that combinesS2V graph representations with the DQN. The action space is decomposed for scalable training: actions areformulated as two node selections, with an edge being added if it does not already exist, or removed if itdoes. The evaluation performed by the authors showed that it it compares favorably to attacks based onrandom edge additions and those discovered by a genetic algorithm. Building on RL-S2V, Sun et al. (2020) argued that attacks on graph data that consist of injecting new nodesinto the network (e.g., forged accounts) are more realistic than those involving the modification of existingedges. The authors formulated an MDP with 3 sub-actions in which the agent must decide the edges for theinjected nodes as well as their label. The S2V graph representation and DQN learning mechanism are used.The proposed method, NIPA, was demonstrated to outperform other attack strategies while preserving someof the key topological indicators of the graph, and is more effective in sparser networks. Ma et al. (2021) also considered graph adversarial attacks, framing the problem similarly to Dai et al. (2018).However, they instead adopted a rewiring operation that preserves the number of edges and the degrees ofthe nodes in the graph, arguing that such attacks are less detectable by linking to matrix perturbationtheory. They used a GCN to obtain graph representations, and trained a policy gradient algorithm to learngraph rewiring strategies. The authors showed that the proposed ReWatt method outperforms RL-S2V,which they attribute to a better comparative balance of edge additions and removals, as well as the design",
  "Network Design": "There is a long tradition of using graphs to represent and analyze the properties of infrastructure networkssuch as power grids, computer networks, and metro transportation systems. Their construction (or design)for optimizing a given objective, on which there has been comparatively less work, has typically been ap-proached using (meta)heuristic methods. Several notable examples of traditional methods include the workof Beygelzimer et al. (2005), who considered heuristics for edge addition and rewiring based on randomselection and node degrees. Schneider et al. (2011) proposed a method based on the simulated annealingmetaheuristic for rewiring infrastructure networks. Lastly, heuristics that leverage the spectrum of the graphLaplacian (Wang & Van Mieghem, 2008; Wang et al., 2014) to guide modifications have also been considered. In the space of RL methods, Darvariu et al. (2021a) approached the problem of constructing graphs fromthe ground up or modifying an existing graph so as to optimize a global objective function. The consideredobjectives capture the resilience (also called robustness) of the network, quantified by the fraction of thenodes that needs to be removed before breakdown when random failures (Cohen et al., 2000) and targetedattacks (Cohen et al., 2001) occur.The work is motivated by the importance of modern infrastructurenetworks such as power grids. The MDP formulation, which also breaks down the edge addition into twoactions, is able to account for the exclusion of already-existing edges, leading to strictly better performancethan RL-S2V. The authors showed that the proposed RNet-DQN method is able to learn better strategiesfor improving network robustness than prior heuristics, and can obtain generalization to larger networks. Subsequently, Darvariu et al. (2023a) made two further contributions to goal-directed graph constructionwith RL. Firstly, the authors proposed to use model-based, decision-time planning methods such as MCTSfor cases in which optimality for one graph (i.e., a given infrastructure network) is desirable over a gen-eral predictive model.Secondly, the authors contributed an MDP for graph construction that is bettersuited for networks positioned in physical space, which influences the cost and range of connections. Theauthors consider the optimization of efficiency (Latora & Marchiori, 2001) and a more realistic resiliencemetric (Schneider et al., 2011). The proposed algorithm, SG-UCT, builds on the standard UCT and tailorsit to this class of problems by considering a memorization of the best trajectory, a heuristic reduction ofthe action space, and a cost-sensitive simulation policy. The algorithm was evaluated on metro networksand internet backbone networks, showing better performance than the baselines in its ability to improve theobjectives, and substantially better scalability to large networks than model-free RL. Doorman et al. (2022) extended the approach of Darvariu et al. (2021a) to the problem of rewiring thestructure of a network so as to maximize an objective function. They considered the practical cybersecurityscenario of Moving Target Defense (MTD), in which the goal is to impede the navigation of an attacker thathas entered the network. The authors approached this task through the maximization of the Shannon entropyof the degree distribution (Sol & Valverde, 2004) and of the Maximum Entropy Random Walk (Burda et al.,2009) as proxy metrics for scrambling the network structure. The models trained for entropy maximizationwere shown to effectively impede navigation (simulated with a random walk model) on several synthetictopologies and a real-world enterprise network. Yang et al. (2023b) considered learning to perform degree-preserving graph rewiring for maximizing the valueof a global structural property. The authors adopted the same resilience and efficiency metrics as Darvariuet al. (2023a), as well as a linearly weighted combination of them. The proposed ResiNet method features thecustom FireGNN graph representation technique, which is justified by the lack of meaningful node features,and uses an underlying Graph Isomorphism Network (GIN) model (Xu et al., 2020) to learn a representationsolely from graph topology. The rewiring policy is trained with the PPO algorithm. The evaluation wasconducted on synthetic networks, a power grid and a peer-to-peer network, showing superior performanceover several heuristic and learned baselines. Trivedi et al. (2020) tackled the inverse problem of the one discussed thus far. Namely, given a graph,the goal is to learn a plausible underlying objective function that has lead to its generation. The authorsproposed an MDP formulation of graph construction through edge additions, and used maximum entropy",
  "Causal Discovery": "Building a graph so as to maximize an objective function also has applications in causal inference andreasoning. In this area, Directed Acyclic Graphs (DAGs) are commonly used to represent statistical andcausal dependencies between random variables.Causal discovery can be approached as a combinatorialoptimization problem that asks to find the DAG structure that best explains the observed data. This canbe quantified by score functions such as the Bayesian Information Criterion (BIC) (Schwarz, 1978), whichserve as objective functions in this context. In this setting, Greedy Equivalence Search (Chickering, 2002) carries out a greedy search in the space ofMarkov equivalence classes. It is a cornerstone method and the most-cited score-based approach at thetime of writing when compared to other such methods identified by a recent survey (Hasan et al., 2023).Another notable combinatorial technique for discrete random variables is GOBNILP (Cussens, 2011), anexact method that leverages an Integer Programming formulation of the problem.RL approaches havebegun to be explored recently as a means of achieving flexibility in the data generation and score functionsthat can be used, as well as to improve on the simplistic greedy search mechanism. RL-BIC (Zhu et al., 2020) is an actor-critic algorithm that leverages the continuous characterization ofacyclicity proposed by Zheng et al. (2018) in the reward function together with the score function itselfto ensure that generated graphs do not contain cycles.The authors used an encoder-decoder model toapproximate the policy. The encoder is based on the Transformer architecture, while the decoder is a single-layer non-linear transformation whose output is a square matrix used for predicting the probability of eachpossible edge in the graph. The evaluation, conducted on synthetic random graphs and a benchmark inthe biological domain, highlights the flexibility of the framework to accommodate varying data generationprocesses and score functions. Building on this work, Wang et al. (2021) considered carrying out the search in the space of orderings.An ordering is a permutation over the nodes that places a constraint on the possible relationships, i.e., forevery node, its parents must come before it in the permutation. From the ordering, a causal graph can beconstructed by (1) translating the ordering to a fully-connected DAG; and (2) pruning away statisticallyinsignificant relationships. The proposed CORL method formulates actions as the selection of a node to addto the ordering and makes use of two reward signals, of which one is provided at the end of the episode whenthe graph is scored, and one is given after each decision-making step by exploiting the decomposability ofBIC. CORL uses function approximation for the policy through an encoder-decoder architecture. It featuresthe same encoder as (Zhu et al., 2020) and an LSTM-based decoder that outputs an ordering node-by-node(with already-chosen nodes being masked). The authors use an actor-critic RL algorithm and achieve betterscalability than RL-BIC. Yang et al. (2023a) also leveraged the smaller search space of causal orderings and formulated actions asthe selection of a node to add to the ordering. Instead of optimizing the score of a single source graph, theproposed RCL-OG method learns the posterior distribution of orderings given the observed data, basing thison the fact that the true DAG may not be identifiable in some settings. The resulting probabilistic modelcan be used for sampling orderings.The method uses a symmetric learning architecture that leveragesTransformers both for the encoder and the decoder. The output of the decoder is passed in input to anMLP to obtain estimates of the Q-values for each action (i.e., the next node to include in the ordering). Theapproach features several Q-networks (one per layer of the order graph) trained using the DQN algorithm.Authors show that the correct posterior is accurately recovered in some simple examples and that better orcomparable performance is obtained on a series of synthetic and real-world benchmarks. Darvariu et al. (2023b) is another work that approached the causal discovery problem with RL to searchdirectly in the space of DAGs.Unlike RL-BIC, which performs one-shot graph generation, the authorsformulated an MDP in which the graph is constructed edge-by-edge. The proposed CD-UCT method features",
  "Molecular Optimization": "The final class of works we discuss in this section target molecular optimization. This is a fundamental taskin chemistry, with applications in drug screening and material discovery. Molecules with various desirableproperties such as drug-likeness and ease of synthesizability are sought. To represent molecules as graphs,atoms are mapped to nodes and edges to bonds. The catalogue of non-RL approaches to this problem isconsiderable. As discussed in the beginning of this section, sampling from a generative model trained ona molecule dataset has been considered. Other approaches include screening (i.e., performing an exhaus-tive search over) a database of known molecules; using metaheuristics such as hill climbing and geneticalgorithms; and performing Bayesian optimization. We also note that RL methods operating on non-graphrepresentations such as strings have also been proposed. For an extensive overview and experimental bench-mark comparison of these methods, we refer the reader to the work of Gao et al. (2022). In the following,we review RL-based works that use a graph-based formulation of the molecular optimization problem. You et al. (2018a) considered learning to construct molecular graphs using RL. The objective functions thatthe method seeks to optimize are the drug-likeness and synthetic accessibility of molecules. The action spaceis defined as the addition of bonds or certain chemical substructures, and the transition function enables theenvironment to enforce validity rules with respect to physical laws. The proposed approach, called GraphConvolutional Policy Network (GCPN), uses GCN to compute the embeddings for state representationand trains the policy using PPO. In addition to the objective functions, the reward structure incentivizesthe method to generate molecules that are similar to a given dataset of examples. GCPN was shown tooutperform a series of previous generative models for the task. Zhou et al. (2019) proposed an MDP formulation of molecule modification that operates on graphs and onlyallows chemically valid actions. The considered reward function is explicitly multi-objective, allowing theuser to trade off between desiderata such as drug-likeness and similarity to a starting molecule. The authorsuse a Double DQN that operates with a state representation based on a molecular fingerprinting technique.Unlike GCPN and other prior methods, the method does not require pretraining, which the authors argueintroduces biases present in the training data. The authors show that the performance of their proposedMolDQN approach is better or comparable to other molecular optimization techniques.",
  "Graph Process Optimization": "In this section, we discuss works that apply Reinforcement Learning to the optimization of a process occurringon a graph. Such works typically assume a fixed graph structure over which a process, formalized as a set ofmathematical rules, takes place. In this scenario, the aim is still to optimize an objective function, but thelevers the agent has at its disposal do not involve manipulating the graph structure itself. Notable examplesof such processes (Barrat et al., 2008) include the routing of traffic (the agent controls how it should be split),mitigating the spread of diseases (the agent decides which nodes should be isolated), as well as navigationand search (the agent makes a decision on which node should be visited next). At a high level, such problems can be formulated as finding the point in the discrete decision space Ksatisfying argmaxK F(G, ), which remains a discrete optimization problem over a fixed graph topologyG. We illustrate this in . Some examples of such control actions are the selections of nodes oredges that incrementally define a trajectory or graph substructure, and decisions over attributes (e.g., labels,weights) to be attached to nodes or edges. The considered papers are summarized in according totheir adopted techniques and characteristics.",
  "ProblemDecision SpaceObjective FunctionBase RL AlgorithmFunction ApproximationCitation": "Attacking GNNsEdge addition and removalMisclassification rate of GCNDQNS2V(Dai et al., 2018)Edge addition for newly injected nodesMisclassification rate of GCNDQNS2V(Sun et al., 2020)Degree-preserving edge rewiringMisclassification rate of GCNREINFORCES2V(Ma et al., 2021) Network DesignEdge additionResilienceDQNS2V(Darvariu et al., 2021a)Edge addition with spatial restrictionsResilience, EfficiencyUCT(Darvariu et al., 2023a)Edge rewiringDegree entropy, Maximum Entropy Random WalkDQNS2V(Doorman et al., 2022)Degree-preserving edge rewiringResilience, EfficiencyPPOFireGNN (GIN-based)(Yang et al., 2023b)Edge additionRecovering implicit objectiveMaxEnt Inverse RLMPNN(Trivedi et al., 2020) Causal DiscoveryOne-shot graph generationBayesian Information CriterionActor-CriticEncoder-Decoder(Zhu et al., 2020)Node orderingBayesian Information CriterionActor-CriticEncoder-Decoder(Wang et al., 2021)Node orderingBayesian Information CriterionDQNEncoder-Decoder(Yang et al., 2023a)Edge additionBayesian Information CriterionUCT(Darvariu et al., 2023b) Molecule OptimizationEdge additionDrug-likeness, Synthetic accessibilityPPOGCN(You et al., 2018a)Node addition, Edge addition and removalDrug-likeness, Similarity to prior moleculeDQNMolecular fingerprint(Zhou et al., 2019)",
  "Node to include in maximal independent setSocial Welfare, FairnessUCT, GIL (Imitation Learning)S2V(Darvariu et al., 2021b)": "Spreading ProcessesNode to be influencedMaximize influenced verticesDQNGCN(Manchanda et al., 2020)Node to be influenced (uncertain)Maximize influenced verticesDQNS2V(Chen et al., 2021)Ranking over nodesMinimize infected nodes, Maximize influenced verticesPPO2 Custom GNNs(Meirom et al., 2021) Search and NavigationNext node towards unknown targetIdentifying target nodeREINFORCELSTM(Das et al., 2018)Next node towards unknown targetIdentifying target nodeUCT with Policy, Value NetworksRNN(Shen et al., 2018)Next cluster, next node towards unknowntargetIdentifying target nodeREINFORCELSTM(Zhang et al., 2022) Next node towards known targetNumber of node expansions until reaching targetSaIL (Imitation Learning)MLP(Bhardwaj et al., 2017)Next node towards known targetNumber of node expansions until reaching targetPHIL (Imitation Learning)Custom MPNN(Pndy et al., 2022)Next node (no explicit target)Minimize topological gaps, Maximize compressibilityDQNGraphSAGE(Patankar et al., 2023)",
  "Routing on Networks": "Techniques for routing traffic across networks find applicability in a variety of scenarios including the Internet,road networks, and supply chains.3In the RL literature, routing across a network topology has beenapproached from two different perspectives. A first wave of interest considered routing at the packet levelin a multi-agent RL formulation. The second, more recent, wave of interest originated in the computernetworks community, which has begun to recognize the potential of ML methods in this space (Feamster& Rexford, 2018; Jiang et al., 2017). This work generally considers routing at the flow level rather thanthe more granular packet level, which tends to be a more scalable formulation of the problem, and is morealigned to current routing infrastructure. We now briefly discuss non-RL methods.The flow routing problem can be solved optimally by LinearProgramming (Tardos, 1986).However, this requires the unrealistic assumption that the demands areknown a priori and do not change (Fortz & Thorup, 2002). Shortest-path algorithms are also applicable, beingcommonly deployed in routing infrastructure via protocols, such as Open Shortest-Path First (OSPF) (Clark,2003). The problem of determining weights for OSPF routing cannot be formulated as an LP and has beenshown to be NP-hard, motivating the use of local search heuristics (Fortz & Thorup, 2000).Simplerheuristics, such as setting weights inversely proportionally to capacity, have also been recommended byhardware manufacturers (Cisco, 2005).",
  "Early RL for Routing Work": "The first work in this area dates back to a 1994 paper in which Boyan & Littman proposed Q-routing,a means of performing routing of packets with multi-agent Q-learning (Boyan & Littman, 1994). In thisframework, an agent is placed on packet-switching nodes in a network; nodes may become congested andso picking the shortest path may not always yield the optimal result. Agents receive neighbors estimatesof the time remaining after sending a packet and iteratively update their estimates of the Q-values in thisway. The authors showed, on a relatively small network, that this approach is able to learn policies that canadapt to changing topology, traffic patterns, and load levels. Subsequent works have introduced variations or improvements on this approach: Stone (2000) considered thecase in which nodes are not given information about their neighbors, and applied a form of Q-learning withfunction approximation where states are characterized by feature vectors. Another approach that instead 3We note that some works refer to problems in the TSP and VRP family as routing problems, since they involve devisingroutes, i.e., sequences of nodes to be visited. This is an unfortunate clash in terminology with problems that involve the routingof flows over graphs, which are discussed in this section. The problems have little in common in structure and solution methodsbeyond this superficial naming similarity.",
  "Recent RL for Routing Work": "We next discuss more recent attempts to use RL for learning routing protocols, which has been performedwith a variety of formulations. Typical solutions to routing in computer networks either assume that trafficquantities are known a priori (as is the case with Linear Programming methods) or optimize for the worst-case scenario (called oblivious routing). Valadarsky et al. (2017) was the first work to highlight the potentialof ML to learn a routing strategy that can perform well in a variety of traffic scenarios without requiringa disruptive redeployment. The authors proposed two RL formulations in which the agent must determinesplit ratios for each node-flow pair, as well as a more scalable alternative in which edge weights are output,then used to determine a routing strategy via the softmin function. Input to the model consists of a sequenceof demand matrices, processed with an MLP learning representation. The model is trained with TRPO tooptimize a reward function quantifying the maximum link utilization relative to the optimum. Promisingresults were shown in some cases but scalability is fairly limited. Xu et al. (2018b) considered a different formulation of the problem, in which the state space capturesthe status (in terms of delay and throughput) of all transmission sessions, actions consist of deciding splitratios across each possible path in the network, and rewards are based on a weighted combination of delayand throughput. The authors proposed an actor-critic algorithm based on DDPG that features prioritizedexperience replay alongside an exploration strategy that queries a standard traffic engineering technique.The evaluation, performed using the ns-3 simulator, showed that the method achieves better rewards thanDDPG and several heuristic and exact methods. Zhang et al. (2020) proposed a hybrid method that is integrated with Linear Programming. The agenttakes traffic matrices as input, and decides actions that determine a set of K important (critical) flows to bererouted. The reward signal is inversely proportional the maximum link utilization obtained after solving thererouting optimization problem for the selected flows. A Convolutional Neural Network is used for functionapproximation, and the policy is trained with REINFORCE with an average reward baseline. The methodis successfully evaluated on network topologies with up to 49 nodes and achieves better performance thanother heuristics for choosing the top-K flows. Almasan et al. (2021) introduced a formulation that computes an initial routing based on shortest paths,then sequentially decides how to route each flow, which then becomes part of the state. The actions wereframed as selecting a node to act as middle point for the flow, with each flow being able to cross at mostone middle point by definition, which improves scalability compared to deciding split ratios directly. Themaximum link utilization is used for providing rewards. Authors used a MPNN for representing states andPPO for learning the policy. Results showed that the proposed method achieves better performance thanstandard shortest path routing and a heuristic method. While the technique performed worse than a solutioncomputed with Simulated Annealing, it is substantially faster in terms of runtime. Hope & Yoneki (2021) adopted the softmin routing variant of the RL formulation proposed by Valadarskyet al. (2017). The work focuses on the learning representation used, arguing for the benefits of GNNs overthe standard MLP used in the original paper. The authors used a variant of Graph Networks for this task,trained using PPO. The results showed that the use of the GNN representation yields better maximum linkutilizations than the same model equipped with an MLP in one graph topology. The GNN model, whiletransferable in principle to different topologies, obtained mixed results compared to shortest path routing.",
  "Spreading Processes": "Mathematical models of spreading processes are applicable for capturing the dynamics of phenomena such asthe spread of disease, knowledge and innovation, or influence in a social network. They have a rich history inthe mathematical study of epidemics, with models such as Susceptible, Infected, Recovered (SIR) enablingtractable analytical study (Kermack & McKendrick, 1927). Typical questions that are considered include theexistence of a tipping point in the spread (beyond which the phenomenon propagates to the entire network)or how to best isolate nodes in the network to achieve containment. The problem of influence maximization in a social network is NP-hard and, for this reason, it has beenapproached with greedy search (Kempe et al., 2003) and hand-crafted heuristics (Liu et al., 2017), as exactmethods are only usable at a small scale. For epidemic models, heuristics based on node properties (such asdegree and betweenness) are typically used to identify nodes that would be influential in the spreading andhence should be isolated (Pastor-Satorras et al., 2015). Several recent papers have considered applications of RL to spreading processes on graphs, with the tech-niques being applicable to more complex interaction mechanisms. Manchanda et al. (2020) set out to improvethe scalability of prior RL methods for combinatorial optimization problems such as S2V-DQN. The authorsproposed to (1) train a model by Supervised Learning that predicts whether a node is likely to be part ofthe solution and (2) train a policy by RL that only operates on this subset of nodes. The GCN architecturewas adopted and the model is trained with Q-learning. GCOMB was applied on the influence maximizationproblem, obtaining similar solution quality to prior methods while scaling to substantially larger graphs withbillions of nodes. Chen et al. (2021) studied a contingency-aware variant of the influence maximization problem, in whichnodes selected as seeds may not participate in the spreading process according to a given probability. Thisnon-determinism leads to complications in designing the state space and reward function, which the authorssuccessfully addressed via state abstraction and theoretically grounded reward shaping.The techniquealso uses the S2V GNN in combination with DQN. RL4IM outperformed prior RL methods that do not",
  "explicitly account for the uncertainty in influencing nodes, and runs substantially faster on large graphs thana comparable greedy search algorithm for the problem": "Meirom et al. (2021) proposed an approach based on RL and GNNs for controlling spreading processestaking place on a network, in which an agent is given visibility over the status of the nodes as well as pastinteractions. The actions were framed as the selection of a ranking of the nodes in the network. RLGCN wasapplied for controlling an epidemic spreading process (the agent decides which nodes should be tested andsubsequently isolated, with the goal of minimizing the number of infections) and an influence maximizationprocess (the agent decides a set of seed nodes to spread influence to their respective neighbors, with the goalof maximizing the number of influenced nodes). The approach features a GNN for modeling the diffusionprocess and one for capturing long-range information dependencies, and was trained end-to-end using PPO.Their method was shown to perform better than several prior heuristics (e.g., removing highly central nodesin epidemic processes).",
  "Search and Navigation": "Search and navigation processes over graphs have also been studied in the RL literature. They can roughlybe classified into three sub-categories: works that treat completion on knowledge graphs (in which the searchtarget is not explicitly known), works on learning heuristic search algorithms (the search target is known anda path must be found to it), and papers that seek to validate theories about how humans navigate graphs. Let us consider the first line of work, which addresses graph search in the context of reasoning in knowledgegraphs. The task is typically formulated as completing a query: given an entity (e.g., Paul Erds) and arelation (e.g., country of birth), the goal is to find the missing entity (e.g., Hungary). This is realized throughguided walks over the knowledge graph. A model is trained using queries that are known to be true, andsubsequently applied to tuples for which the knowledge is incomplete. Notable traditional approaches forthis task are TransE (Bordes et al., 2013) and TransR (Lin et al., 2015), which operate by embedding entitiessuch that relations are interpreted as translations over the embedding space. The embeddings, which arelearned by gradient descent, can subsequently be used to rank candidate entities for link prediction. Das et al. (2018) formulated the task as an MDP in which states encode the current location of the agent inthe graph and the entity and relation forming the query. Actions correspond following one of the outgoingedges, while rewards are equal to +1 if the agent has reached the target node, and 0 otherwise. The policyis represented as an LSTM and trained using REINFORCE with an average cumulative reward baseline.The performed evaluation shows that the method is competitive with other state-of-the-art approaches, andsuperior to a path-based model for searching the knowledge base. M-Walk (Shen et al., 2018) built further in this direction by leveraging the observation that the transitionmodel when performing graph search is known and deterministic: given the current node and a chosenedge, the next node is uniquely determined with probability 1. This can be exploited through the use ofmodel-based algorithms such as MCTS. The authors proposed a method that combines the use of MCTS(for generating high-quality trajectories) and policy & value networks (which share parameterization andare trained using the MCTS trajectories). The method was shown to outperform MINERVA and severaltraditional baselines for knowledge base completion. A recent work by Zhang et al. (2022) addressed the issue of degrading performance of RL models forknowledge graph completion with increases in path length (e.g., MINERVA limits the path length to 3 dueto this). The authors proposed a hierarchical RL design with two policies that act cooperatively: one higher-level policy for picking the cluster in the knowledge graph to be searched, and a fine-grained policy thatoperates at the entity level. The initial clustering is performed using embeddings obtained with the TransEalgorithm and K-means. The method outperforms MINERVA and M-Walk, particularly when answeringqueries over long paths. We now move on to discussing works that aim to learn heuristics for classic graph search, i.e., scenarios inwhich a topology is given and a path from a known source node to a known destination node must be found.Simple algorithms for this task, such as Depth-First Search and Breadth-First Search, can be improved byusing a heuristic function for prioritizing nodes to be expanded next together with an algorithm such as",
  "A*. An important application is motion planning in robotics, for which resource constraints dictate that thesearch should be performed as effectively as possible": "Bhardwaj et al. (2017) considered precisely the robotic motion planning use case, in which the graph cor-responds to possible configurations of the robot, and edges are mapped to a set of valid maneuvers. Theauthors formulated this task as a POMDP and used a simple MLP to parameterize the policy, which wastrained using imitation learning. The authors leveraged the existence of a powerful oracle algorithm whosecomputational cost prevents its use at runtime but may be queried during the training procedure. The opti-mization target is to minimize the expected difference between the Q-values (often referred to as cost-to-goin robotics, in which the equivalent goal is to minimize cost instead of maximizing reward) of the agent andthe optimal Q-values supplied by the oracle. Search as Imitation Learning (SaIL) was shown to outperformseveral simple heuristics based on Euclidean and Manhattan distances, an SL model, and model-free RL. Pndy et al. (2022) built further in the direction of imitation learning for graph search, with a few keydifferences with respect to SaIL. The authors set out to learn a perfect heuristic function to be used in con-junction with a greedy best-first search policy, instead of attempting to directly learn the search policy itselfas in SaIL. Furthermore, the policy is parameterized as a custom recurrent GNN, which intuitively providesa mechanism for tracking the history of the graph traversal without storing the graph in memory, which iscomputationally infeasible. Finally, the authors proposed a custom IL procedure suitable for training usingbackpropagation through time. The authors showed superior performance over SaIL and other methods inseveral domains: 2D navigation tasks, search over real-world graphs such as citation and biological networks,and planning for drone flight. Patankar et al. (2023) studied RL for graph navigation in the context of validating prior theories of howhumans perform exploration in graphs (e.g., in content graphs such as Wikipedia). Such theories posit thatpeople follow navigation policies that are content-agnostic and depend on the topological properties of the(sub)graph: namely, that navigation is performed to regulate gaps in knowledge (Information Gap Theory)or to compress the state of existing knowledge (Compression Progress Theory). The authors trained policiesparameterized by a GraphSAGE GNN using the DQN algorithm for navigating the graph so as to optimizethe two objective functions. Subsequently, the policies were used to derive centrality measures for use witha biased PageRank model that mimics human navigation. The evaluation, performed over synthetic graphsand several real-world graphs including book and movie reviews, showed that the approach results in walkson the graph that are more similar to human navigation than standard PageRank.",
  "Challenges of Graph RL": "Let us take the opportunity to discuss some of the general challenges faced by works in this area. In theabsence of a major breakthrough, they are likely to persist long-term, and we conjecture that addressingthem satisfactorily requires deep insights. Complementarily, they may be viewed as open questions that canbe treated towards the advancement of the field.",
  "Framing Graph Combinatorial Optimization Problems as MDPs": "In order to apply RL for a given graph combinatorial optimization problem, one needs to decide how toframe it as a Markov Decision Process, which will impact the learning effectiveness. While the objectivefunction is typically dictated by the application, the designer generally has the liberty to decide the stateand action spaces as well as the dynamics. Let us discuss some general considerations. While many deep learning architectures such as autoencoders generate outputs in a one-shot fashion, RLapproaches enable a constructive way of solving optimization problems. The underpinning Bellman principleof optimality (Bellman, 1957) captures the intuition that, whatever action is taken in a given state, theoptimal policy must also be optimal from the resulting next state. This provides a way of breaking up ahighly complex decision-making problem into sequential subproblems, greatly enhancing scalability in largestate spaces. Incremental construction of the solution using RL is also more interpretable than one-shot",
  "Reward Design: Balancing Accuracy, Speed, and Multiple Objective Functions": "Objective function values are an assessment of solution quality and are used to provide rewards for theRL agent. For many canonical problems, they are fairly inexpensive to evaluate (e.g., linear or low-degreepolynomial time in the size of the input). As an example, computing the cost of a TSP solution simplyrequires summing pre-computed edge weights. Other objective functions require computational time that is alow-degree polynomial (e.g., the robustness and efficiency metrics used in the works discussed in .2). In some domains, the true objective function may be too expensive to evaluate, requiring the designer to optfor a proxy quantity that can be computed more quickly. It is possible to exploit known correlations betweenobjectives, e.g., those between quantities based on the graph spectrum and robustness (Wang et al., 2014).Alternatively, works have also opted for training a model as a proxy for running the expensive simulation,which speeds up the process at the expense of introducing errors. As an example objective too expensiveto calculate online in the RL loop, estimating how drug-like molecules bind to target proteins can takeon the order of hours even on commercial-grade software (Strk et al., 2022). In the extreme case, somescenarios (Zhavoronkov et al., 2019) may require wet lab experiments that can take weeks to complete. Some problems are inherently multi-objective, such as balancing drug-likeness and similarity to a previousmolecule (Zhou et al., 2019), or delay and throughput for routing in computer networks (Xu et al., 2018b).Usually, works adopt a linear weighted combination between objectives, for which existing single-objectiveRL algorithms can be used. For scenarios in which the interaction between objectives is not linear, othertechniques for multi-objective RL are required (Roijers et al., 2013), but have not yet received attention inthis literature. Aside from the choice of which reward function to use, there is also the question of when to provide rewards.Many works opt for providing rewards only at episode completion once the solution is fully-formed. Thismakes the training loop faster in wall clock time, but may be problematic due to reward sparsity and creditassignment issues. Providing intermediate rewards can improve sample complexity but will incur a slowdownof the training loop.",
  "Choosing and Designing Algorithms and Learning Representations for Graph RL": "Given an MDP formulation, how does one choose an RL algorithm to solve it?Since the applicationof RL to graph combinatorial optimization problems is still a nascent area of investigation, this complexquestion is often glossed over by papers. The literature as a whole lacks systematic comparisons of RLtechniques for combinatorial optimization problems, or guidelines on which algorithm to pick depending onthe characteristics of the problem. Regarding this choice, we make the following observations to help practitioners select between RL algorithms.We note that there is no clear winner, and choices must be made in accordance with the characteristics ofthe problem, constraints on data collection and environment interaction, and the deployment scenario 1. For Graph RL problems, the ground truth model M = (P, R) comprising the transition and rewardfunctions is often known a priori. For many problems, transitions P are also deterministic, meaningthat each action a will uniquely determine a next state s. For example, choosing an edge to addto the graph when performing construction or choosing the next node to move to when performingnavigation both result in uniquely determined next states. For providing the rewards R, the math-ematical definition of the objective function F is used to fully accurately judge the solution. Whilethis may seem trivial, knowing M unlocks the use of model-based RL algorithms, which have greatlyimproved performance and scalability over their model-free counterparts when compared for GraphRL problems (Shen et al., 2018; Darvariu et al., 2023a;b), echoing results in model-based RL forgames, in which the model is typically learned (Guo et al., 2014; Anthony et al., 2017; Schrittwieseret al., 2020). We therefore advise opting for model-based RL if applicable and the absolute bestperformance and scalability are the goals. 2. However, model-based methods are more complex to develop and implement, and do not have openimplementations as widely available in the open source community. This may explain the greaterpopularity of model-free algorithms. If the goal is to reach a prototype assessing the potential of RLfor a given problem, the quickest means to do so is by opting for a model-free algorithm. 3. Among model-free algorithms, off-policy algorithms (e.g., DQN) are more sample-efficient than on-policy algorithms (e.g., PPO), meaning that they will require fewer environment interactions toreach a well-performing policy. If evaluating the objective function is computationally expensive, asis often the case for Graph RL problems, then using an off-policy algorithm is advisable. 4. Furthermore, value function methods usually learn a greedy policy (i.e., they act greedily withrespect to the learned value function), while policy gradient methods learn a stochastic one. If theoptimal policy is inherently stochastic (e.g., in a packet routing scenario, the best strategy for anode is to distribute the load among its neighbors in the graph), or the learned probabilities areused downstream to guide another algorithm or search procedure, then policy gradient methodsshould be preferred. The challenge of choosing algorithms appropriately is further amplified by the adoption of computationaltechniques that arose in different domains. Deep RL algorithms were developed and are often tested inthe context of computer games, which have long acted as simplified testbeds for assessing decision-makingstrategies.It would be reasonable to expect, however, that the solution space and the optimal way ofnavigating it may be substantially different for combinatorial optimization problems. Furthermore, using astandard RL algorithm is usually the first step towards the validation of a working prototype, but is typicallynot sufficient, as generic algorithms do not take advantage of the problem structure. Proposing problem-specific adjustments and expansions, or even devising entirely new approaches, are frequently necessary toachieve satisfactory performance. Care is also needed when selecting or designing the learning representation, with different methods tradingoff expressibility and scalability in general. For combinatorial optimization problems specifically, whicheverchoice is made, alignment between its computational steps and those of the algorithm that it is trained toapproximate or discover can lead to better predictive performance. For example, the computational steps of",
  "Scalability to Large Problem Instances": "Scalability is also an important challenge for the adoption of ML-based methods for combinatorial optimiza-tion. In a sense, solutions to the previously enumerated challenges all contribute to effective scalability ofthese methods. To begin with, we note that the primary difficulty is not the computational cost of evaluatingthe model, which is usually inexpensive. Rather, the challenge lies in the cost of exploring the vast solutionspace associated with large problem instances, for which training directly is prohibitively expensive. Let us enumerate some further possibilities beyond those outlined in the challenges above. One can study theapplication of models trained on small problem instances to larger ones. This technique requires models thatare independent of the size of the problem, and can obtain impressive results in scenarios in which the modeltransfers well, but can equally suffer from substantial degradation in solution quality if the structure of thesolution space is dissimilar at varying scales. GNNs, in particular, have proven to be an effective functionapproximation technique in this context, since they enable representing problem instances of different sizestransparently via subgraph embeddings (Khalil et al., 2017).Decision-time planning algorithms can beused to examine a small fraction of the entire decision-making process by focusing on constructing optimaltrajectories only from the current state.Hence, instead of learning a general model for solving a widerange of instances of a graph combinatorial optimization problem, the available computational budget canbe concentrated on the problem instance at hand (Darvariu et al., 2023a). Lastly, another approach forimproving scalability involves using demonstrations of a well-performing algorithm to collect data instead ofonline environment interaction (Bhardwaj et al., 2017). This means that the expensive trial-and-error processassociated with RL can be partially circumvented, and further training and fine-tuning can be performedstarting from near-optimal trajectories.",
  "Generalization to Unseen Problem Instances": "An important issue is the fact that one cannot guarantee that the learned models will generalize well whenencountering instances outside of the training distribution. This fundamental limitation is reminiscent of theNo Free Lunch theorems (Wolpert & Macready, 1997) in Supervised Learning, which suggest that it is notpossible to obtain a model that performs well in all possible scenarios. Partial mitigation may be possible bytraining models on a wide variety of scenarios including different initial starting points for the RL agent inthe environment (i.e., different initial solutions), problem instances of varying difficulty and size, and evendifferent variants of related problems. Many works in Graph RL diversify training conditions as a means of obtaining models that generalize. Thisstrategy is often successful (Khalil et al., 2017) but can fail in unexpected ways; for example, Darvariu et al.(2021a) found that the same RL approach for graph construction generalizes well to unseen larger instancesusing one objective function (resilience of the graph to random node failures), but performance collapsesusing a closely related objective (resilience to targeted node attacks).",
  "Trial-and-ErrorMachine Design": ": Comparison between the algorithm discovery mechanisms in classic algorithm design and GraphRL. Interpretability techniques may enable the explicit definition of algorithms that are implicitly discoveredby RL, opening the door to optimizations and improved performance. To the best of our knowledge, there are currently no methods able to guarantee robustness to unexpecteddistribution shifts or adversarial perturbations, even for restricted classes of graph combinatorial optimizationproblems. In settings where such models are deployed, one may want to have the tools in place to detectdistribution shift, and possibly use a fall-back approach whose properties and expected performance arewell-understood.",
  "Engineering and Computational Overhead": "Graph RL approaches typically require spending an overhead in terms of experimentation and computationalresources for the various stages of the pipeline, such as setting up the datasets, performing feature engineering,training the model, and selecting the values of the hyperparameters. However, once such a model is trained, itcan typically be used to perform predictions whose computational cost is negligible in comparison. Costs maybe mitigated in the future by collective efforts to train generalist foundation models that can be sharedamong researchers working on similar problems, akin to the current dynamics of sharing large languageand protein folding models. It is reasonable to expect that highly related problems such as the TSP andits increasingly complex variants VRP, Capacitated VRP, and VRP with Time Windows have sufficientlysimilar decision spaces to enable the use of a common model. However, the unique nature of different graphcombinatorial optimization problems may prove challenging in this sense.",
  "Interpretability of Discovered Algorithms": "Finally, a challenge that should be acknowledged is the limited interpretability of the learned models andalgorithms. Generic ML interpretability techniques are not directly applicable for the considered problemsgiven the graph-structured data and their framing as MDPs. Interpretability of both GNNs and RL areareas of active interest in the ML community (e.g.,Ying et al. (2019); Verma et al. (2018)) but thereremains significant work to be done, especially at their intersection. Notably, a recent work (Georgiev et al.,2022) adapts concept-based explainability methods to GNNs in the Supervised Learning setting, showingthat logical rules for several classic graph algorithms, such as Breadth-First Search and Kruskals methodfor finding a minimal spanning tree, can be extracted. Akin to work that tackles the explainability of RL invisual domains (e.g., Mott et al. (2019) treats Atari games), we consider that there is scope for developingtechniques that are tailor-made for explaining policies learned by RL on graphs for solving combinatorialoptimization problems. The literature contains many instances in which the methods can optimize the given objectives remarkablywell, but we are not necessarily able to identify the mechanisms that lead to this observed performance.Much like physicists would simulate a process of interest then work backwards to try to derive physical laws,interpretability of an algorithm learned through RL may help formulate it in a traditional way. This canpotentially lead to low-level, highly optimized procedures and implementations that dramatically improveefficiency and scalability. As there is a clear parallel between traditional algorithm design and using RL",
  "Applications of Graph RL": "The applicability of Graph RL techniques is broad as they share the versatility of the graph mathematicalframework for representing systems formed of connected entities and their relationships. As we have noted,beyond the descriptive characterization of the problems at hand, it is natural to treat questions of optimiza-tion, in which the goal is to intervene in the system so as to improve its properties. The core requirementsfor Graph RL to be applicable can be summarized as:",
  "We now review some of the important application areas that were mentioned throughout this survey": "The discipline of operations research, which studies how individuals, organizations, etc. can make optimaldecisions such that their time and resources are used effectively, is one of the most popular testbeds for thesetechniques. Relevant applications include the optimization of supply chains and production pipelines. Forexample, the TSP problem is commonly represented as a graph by mapping cities to nodes and creatinga fully-connected graph in which edges capture the pairwise travel cost.For the Job Shop SchedulingProblem, which asks to find the optimal processing sequence for a set of items on a set of machines, thedisjunctive graph representation (Balas, 1969) captures tasks as nodes and creates edges that representtiming constraints. Given the scope of our survey, as discussed in , we refer the reader to the worksof Mazyavkina et al. (2021) and Bengio et al. (2021) for citations in this area. Graph RL methods are also relevant for molecular and materials science applications in computationalchemistry (Butler et al., 2018). Compounds can be represented as graphs using nodes to capture atoms andedges to indicate bonds, while RL is a natural way for framing the navigation of the search space towardsmolecules with desirable properties. Graph RL has been leveraged to search for molecules that optimizesimilarity to existing drugs and are easy to synthesize (You et al., 2018a; Zhou et al., 2019). At a largerscale of the considered networks, in engineering, graphs are commonly used to model electrical networksand physical structures. In this area, Graph RL techniques have been leveraged for optimizing the resilienceand efficiency of networks (Darvariu et al., 2021a; 2023a; Yang et al., 2023b). In computer science, Graph RL methods have been applied extensively for problems in computer networks,in which nodes represent (systems of) computers that are connected by links as defined by a communicationprotocol. A great deal of interest has been dedicated to routing problems (Boyan & Littman, 1994; Valadarskyet al., 2017). For cybersecurity, Graph RL methods have been applied to disrupting the navigation of anattacker in a computer network (Doorman et al., 2022), as well as for adversarial machine learning, in whichthe goal is to induce a classifier for graph-structured data to make errors (Dai et al., 2018; Sun et al., 2020).Another related application is their use in robotics, where graphs are commonly used as a model for motionplanning (nodes represent valid configurations of the robot, while edges are movements of the robot betweenthese configurations). Graph RL has been applied for searching the space of possible robot configurationstowards a goal state (Bhardwaj et al., 2017). Searching over a graph has also been approached with GraphRL in information retrieval for the completion of knowledge bases (Das et al., 2018; Shen et al., 2018). In economics, graphs can be utilized to model systems of individuals or economic entities, wherein links areformed between individuals based on proximity, costs, or benefits (Goyal, 2012). Graph RL has been used",
  "When and Why to Use Graph RL": "When are RL approaches useful, and why might they outperform non-RL methods? Three high-level reasonsthat do not depend on the specific characteristics of the specific problem at hand are given below. Flexibility regarding objective function: RL methods place few requirements on the reward function andtherefore, in the context of combinatorial optimization problems, on the objective function to be optimized.RL can therefore accommodate objectives that are not well-behaved mathematically speaking such asnon-differentiable, non-convex, and non-linear functions. It does not even require the objective function tobe expressed analytically as long as samples can be generated. Exact methods, on the other hand, cannotbe applied if the objective function does not belong to a pre-specified function class. Therefore, for suchproblems, heuristics or metaheuristics need to be used, compared to which RL can perform better due tothe reasons outlined below. Longer decision horizon: metaheuristics (e.g., greedy search, simulated annealing, evolutionary algorithms)typically use a shallow decision horizon. They move through the search space by small, local modificationsof the solution. This means that, if a desirable component of the solution is unlikely to be reached by asequence of locally optimal modifications, it may be challenging for such methods to discover them. Incontrast, RL explicitly models the impact of longer sequences of actions. It is able to learn policies that,while they may not lead to large returns in the short term, will typically lead to larger expected returnsover a longer decision horizon. As a relevant example, consider a network design scenario in which one aimsto produce a graph with small average shortest path lengths. The ideal topology in this case is a star, butthe path lengths are undefined until the network becomes connected. Methods with short decision horizonstherefore struggle, while RL does not. This is indeed the case for the network design problems covered in.2, for which RL proves superior over a host of metaheuristics. Training stage as problem-specific tuning: Another aspect that enables RL methods to outperform meta-heuristics is the fact that they undergo a training stage. At a high level, this may be seen as tuning theparameters of a meta-algorithm on the particular distribution dictated by the search space of a given prob-lem. Therefore, after training, an RL model already contains knowledge about the problem that can beused in the process of constructing a solution for an instance that, while not identical to those it has seenduring training, comes from the same distribution on which the model was fit. In contrast, metaheuristicsstart from scratch. In practice, after model training, this translates to solutions of the same quality beingfound more efficiently by RL compared to generic methods, or better solutions being reached within thesame computational budget.",
  "When and Why Not to Use Graph RL": "Conversely, let us discuss situations in which RL solutions are unlikely to bring a substantial benefit overclassic optimization approaches if one is interested in practical usage. For a variety of well-specified problems,especially the canonical ones, a large number of solutions and practical software tools are available. In thesecases, the use of RL might be intellectually interesting, but, in practical terms, the resulting performancegain is usually very limited or absent. In general, if the objective function and decision variables of theproblem are such that it can be cast into well-known paradigms, such as, for example, Integer Programmingor Linear Programming, powerful and highly-optimized solvers can be leveraged. It is also worth noting that RL does not typically yield considerable improvements if the structure of theproblem is such that shallow decision horizons are sufficient for constructing optimal solutions.In thissetting, modelling the expected returns of sequences of decisions, as performed by RL, is redundant. The problem formulations considered by the current survey are such that they are amenable to RL-basedapproaches, which motivates the use of these methodologies in the absence of satisfactory solutions. Even insuch settings, however, it is difficult to ascertain a priori how much of a gain we might obtain over a classicapproach. Overall, the literature is lacking in thorough experimental comparisons between RL and non-RLmethods beyond the scope of each individual paper. Furthermore, given their technical complexity andexperimental challenges, RL methods may require several development cycles until satisfactory performanceis obtained.",
  "Summary": "In this survey, we have discussed the emerging area of Graph Reinforcement Learning, a methodology foraddressing computationally challenging optimization problems over graphs by trial-and-error learning. Wehave dedicated particular attention to problems for which efficient algorithms are not known, and classicheuristic and metaheuristic algorithms generally do not yield satisfactory performance. We have groupedthese works into two categories. The first, Graph Structure Optimization, comprises problems where anoptimal graph structure must be found and has notable applications in adversarial attacks on GNNs, networkdesign, causal discovery, and molecular optimization.The second, Graph Process Optimization, treatsgraph structure as fixed and the agent carries out a search over a discrete space of possible control actionsfor optimizing the outcome of the process. This encompasses problems such as network routing, games,spreading processes, and graph search. Finally, we have discussed major challenges that are faced by thefield, whose resolution could prove very impactful.",
  "Taking a broad view of these works, we obtain a blueprint for approaching graph combinatorial optimizationproblems in a data-driven way. One needs to specify:": "1. The elements that make up the state of the world and are visible to the decision-making agent.Typically, the state will contain both fixed elements (out of the control of the agent) and malleableparts (may be modified through the agents decisions). The constituents of a state can take theform of a subset of nodes or edges, subgraphs, as well as features and attributes that are global orattached to nodes and edges.",
  ". The levers that the agent can use to exert change in the world and modify part of the state": "3. How the world changes as a result of the actions and/or outside interference. While many worksassume deterministic transitions, one can also consider situations with stochastic properties. Theseare manageable as-is by model-free RL techniques, while planning methods can be extended tostochastic settings, for example by averaging out several outcomes (Browne et al., 2012).",
  "Closing Thoughts": "Can we, therefore, collectively hang up our boots, leaving the machines to discover how to solve theseproblems? We argue that this not the case. Generic decision-making algorithms and learning representationsare clearly not a silver bullet since they do not necessarily exploit problem structure efficiently. There aresubstantial improvements to be made by encoding knowledge and understanding about the problem intothese solution approaches. In this work, we have presented and argued for RL methodologies as an alternative to exact methods,heuristics, and metaheuristics. This dichotomy does apply when RL techniques are used in a constructiveway, i.e., they build a complete solution to the problem starting from the MDP formulation as performedin the surveyed works. However, in a broader sense, RL and traditional methods are not in opposition. Anumber of works have explored the integration of RL and classic methods (Chen & Tian, 2019; Hottung &Tierney, 2020; Lu et al., 2020), in which the main loop is a traditional optimization algorithm, and RL isused for improving decisions within the method. This represents a possible path for developing algorithmsthat leverage both deep problem insights and highly efficient machine-learned components. Considering the current popularity of RL, one might also ask if their application in this problem space isan instance of Maslows hammer (Maslow, 1966). Is it the case that we favor the ubiquitous use of thistool over careful appreciation of what might be the appropriate methodology for a given set of problems?Nevertheless, the potential of RL approaches in this problem space is significant and transformative, a beliefthat is strongly supported by the emerging body of literature. As RL techniques become more widespread,we expect them to find successful applications far beyond canonical problems, and to transform the scientificdiscovery process (Wang et al., 2023).",
  "Marc Barthlemy. Spatial networks. Physics Reports, 499(1-3), 2011": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, lvaro Snchez-Gonzlez, Vinicius Zambaldi, MateuszMalinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, FrancisSong, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, VictoriaLangston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals,Yujia Li, and Razvan Pascanu. Relational inductive biases, deep learning, and graph networks. arXivpreprint arXiv:1806.01261, 2018.",
  "Andrew J. Dudzik and Petar Velikovi. Graph neural networks are dynamic programmers. In NeurIPS,2022": "Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt,Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep Reinforcement Learning inLarge Discrete Action Spaces. In ICML, 2015. David Duvenaud, Dougal Maclaurin, Jorge Aguilera-Iparraguirre, Rafael Gmez-Bombarelli, Timothy Hirzel,Aln Aspuru-Guzik, and Ryan P. Adams. Convolutional Networks on Graphs for Learning MolecularFingerprints. In NeurIPS, 2015.",
  "Denghui Zhang, Zixuan Yuan, Hao Liu, Hui Xiong, et al. Learning to walk with dual agents for knowledgegraph reasoning. In AAAI, 2022": "Junjie Zhang, Minghao Ye, Zehua Guo, Chen-Yu Yen, and H. Jonathan Chao. CFR-RL: Traffic engineeringwith reinforcement learning in SDN. IEEE Journal on Selected Areas in Communications, 38(10):22492259, 2020. Alex Zhavoronkov, Yan A. Ivanenkov, Alex Aliper, Mark S. Veselov, Vladimir A. Aladinskiy, Anastasiya V.Aladinskaya, Victor A. Terentiev, Daniil A. Polykovskiy, Maksim D. Kuznetsov, Arip Asadulaev, et al.Deep learning enables rapid identification of potent DDR1 kinase inhibitors. Nature Biotechnology, 37(9):10381040, 2019."
}