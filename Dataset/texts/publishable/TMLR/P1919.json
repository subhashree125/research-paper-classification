{
  "Abstract": "We address the long-standing problem of how to learn eective pixel-based image diusionmodels at scale, introducing a remarkably simple greedy method for stable training of large-scale, high-resolution models without the needs for cascaded super-resolution components.The key insight stems from careful pre-training of core components, namely, those respon-sible for text-to-image alignment vs. high resolution rendering. We rst demonstrate thebenets of scaling a Shallow UNet, with no down(up)-sampling enc(dec)oder. Scaling itsdeep core layers is shown to improve alignment, object structure, and composition. Buildingon this core model, we propose a greedy algorithm that grows the architecture into high res-olution end-to-end models, while preserving the integrity of the pre-trained representation,stabilizing training, and reducing the need for large high-resolution datasets. This enablesa single stage model capable of generating high-resolution images without the need of asuper-resolution cascade. Our key results rely on public datasets and show that we are ableto train non-cascaded models up to 8B parameters with no further regularization schemes.Vermeer, our full pipeline model trained with internal datasets to produce 1024 1024images, without cascades, is preferred by 44.0% vs. 21.4% human evaluators over SDXL.",
  "Introduction": "Training large-scale Pixel-Space text-to-image Diusion Models (PSDM) to generate high-resolution imageshas been challenging due to optimization instabilities arising when growing model size and/or target imageresolution, and due to the increasing demand for computational resources and high resolution trainingcorpora. The predominant alternatives include cascaded models, comprising a sequence of diusion modelseach targeting a progressively higher resolution and trained independently (Ho et al., 2022a; Saharia et al.,2022a; Nichol et al., 2022), and latent diusion models (LDMs), where generation is performed in a low-dimensional latent representation, from which high resolution images are generated via a pre-trained latentdecoder (Rombach et al., 2022). In the development of cascaded models, it is challenging to identify sources of quality degradation anddistortion resulting from design decisions at specic stages of the model. One well-known issue of cascadesis the distribution shift between training and inference, where inputs to super-resolution or decoder modelsduring training are obtained by down-sampling or encoding training images, but during inference theyare generated from other models, and hence may deviate from the training distribution. This can causeamplication of unnatural distortions produced by models early in the cascade. The generation of realisticsmall objects such as faces or hands is one such challenge that has been dicult to diagnose in such models. Beyond image generation per se, diusion models serve as image priors for myriad downstream tasks, in-cluding inverse problems (Jalal et al., 2021; Kadkhodaie & Simoncelli, 2021; Kawar et al., 2022; Song et al.,",
  ": Images generated with our model Vermeer. (See Appendix A for the prompts.)": "2023; Chung et al., 2023; Graikos et al., 2022; Tang et al., 2023; Jaini et al., 2023; Zhan et al., 2023; Songet al., 2024), or other generative tasks (Ho et al., 2022b; Levy et al., 2023; Poole et al., 2023; Tan et al.,2023; Bar-Tal et al., 2024; Chen et al., 2023; Tewari et al., 2023). Cascaded diusion models are not readilyapplicable to such tasks, and as a consequence, many such applications rely solely on the score function fromthe base model of a cascade, often at a relatively low resolution. A high resolution end-to-end model wouldalleviate these issues, but model development and eective training procedures have been elusive. Key barriers to training high resolution models include prohibitive resource requirements in both memoryand computation. Existent recipes require large batch sizes during training to avoid instabilities, and as aconsequence, intractably large amounts of memory for high-resolution images. Another issue concerns theneed for high quality, high resolution training data. Existing training methods require large, diverse corporaof text-to-image pairs at the target resolution, while in practice, such data are not readily available at highresolution. This paper introduces a framework for training high resolution, large-scale text-to-image diusion modelswithout the use of cascades. To that end we explore the extent to which one can decouple the trainingof visual concepts associated with textual prompts, from the resolution at which one aims to render theimage. Such disentanglement has two goals. It aims at a better understanding of alignment, compositionand image delity (especially for well-known hard cases like generating consistent hands, text rendering,scene composition, etc.) as a function of model scaling (e.g., see ). Second, and of equal importance,our framework yields a robust and stable recipe for training large-scale, non-cascaded pixel-based models",
  "The contributions of this paper can be summarized as follows:": "We introduce a novel architecture, Shallow-UViT, which allows one to pretrain the PSDMs corelayers on datasets of text-image data (subsection 3.2), eliminating the need to train the entire modelwith high resolution images. This also allows us to investigate the emergent properties of PSDMrepresentation scaling in isolation from layers targeting generation at the nal resolution. We present a greedy algorithm for training the Shallow-UViT architecture that allows us to success-fully train a high-resolution text-to-image model with small batch sizes (256 versus the typical 2kused in end-to-end solutions) (section 3). We show that one can signicantly improve dierent image quality metrics by leveraging the repre-sentation pretrained at low-resolution, while growing model resolution in a greedy fashion. Scalingthe core components of the Shallow-UViT architecture alone leads to signicant improvements inimage distribution, quality and text alignment (section 5). We demonstrate that these principles work at scale by presenting Vermeer (), a modeltrained with our greedy algorithm on large-scale corpora, in conjunction with other well-knownmethods like asymmetric aspect ratio netuning, prompt preemption and style tuning (section 6).Vermeer is shown to surpass previous cascaded and auto-regressive models across dierent metrics.In a human evaluation study with 500 challenging prompts and 25 annotators per image, Vermeeris preferred over SDXL (Podell et al., 2024) by a 2 to 1 margin.",
  "Related work": "Current high-resolution image generation with diusion models presents a trade-o between architecturalcomplexity and eciency. Cascaded diusion models (Nichol et al., 2022; Dhariwal & Nichol, 2021; Sa-haria et al., 2022b; Ramesh et al., 2022; Balaji et al., 2022) were originally introduced to circumvent thediculty of training a single stage, end-to-end model. Cascaded models employ a multi-stage architecturethat progressively up-scales lower-resolution images to address the computational challenges of generatinghigh-resolution images directly. Nevertheless, they entail signicant complexity and training overhead, asthe stages of the cascade are trained independently. Simple Diusion (Hoogeboom et al., 2023b) sought to simplify the process by targeting the high resolutiongeneration with a single stage model, introducing a novel UViT architecture and several useful modicationsto training methods that improve stability. While this approach is shown to be eective, stability issuesremain when targeting large-scale models, and high resolution images, due in part to their dependence onlarge batch sizes. In this work we adopt a similar UViT architecture, and some of their techniques for scaling,extending the model to much higher resolutions through greedy training. Through scaling the core backboneof the model, and with our greedy training procedure, we nd with can scale to much high resolution models(2 to 8 higher than Simple Diusion), with excellent alignment, and much smaller batches when traininghigh resolution layers of the model. Another line of work proposed Matryoshka Diusion Models (MDM) (Gu et al., 2023) that denoises multipleresolutions using a proposed Nested UNet architecture. They progressively train the network to preservethe representation at higher resolutions. We show in this work an alternate and simpler approach wheredenoising multiple resolutions is not required, but instead it is crucial to preserve the representation byfreezing the pretrained weights as we grow the architecture up to its nal design. On another front, latent diusion models (LDMs) (Rombach et al., 2022; Jabri et al., 2022; Betker et al.,2023) reduce computational costs by operating within a compressed latent representation. However, LDMsstill require separate super-resolution or latent decoder networks to produce nal high-resolution images.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diusion restoration models. InAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Informa-tion Processing Systems, 2022. URL Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, Krishnamurthy Dj Dvijotham,Jinwoo Shin, and Kimin Lee. Condence-aware reward optimization for ne-tuning text-to-image models.In The Twelfth International Conference on Learning Representations, 2024. URL Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: Anopen dataset of user preferences for text-to-image generation. Advances in Neural Information ProcessingSystems, 36, 2024. Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. Open-vocabulary object de-tection upon frozen vision and language models. In The Eleventh International Conference on LearningRepresentations, 2023. URL Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, Minguk Kang, Taesung Park, Jure Leskovec,Jun-Yan Zhu, Li Fei-Fei, Jiajun Wu, Stefano Ermon, and Percy Liang. Holistic evaluation of text-to-imagemodels. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and BenchmarksTrack, 2023. URL",
  "Method": "Our goal is to create a straightforward, stable methodology for training large scale pixel-space diusionmodels that operate as a single stage model, i.e., non-cascaded, at inference time. To this end, we rstrevisit the UNet architecture, aiming to decouple layers that have a major impact on text-to-image alignment(core components) from those responsible for rendering at the target image resolution (encoder-decoderor super-resolution components). Next, we focus on pre-training the core components pretraining and onrepresentation scaling (subsection 3.2). Finally, we present a greedy algorithm to grow the initial architecturecore by adding encoder-decoder layers while protecting core layers representation. This yields a single-stagemodel at inference time (subsection 3.3).",
  "Text-to-image core components": "UNet is the architecture of choice for diusion models. Two architecture families are common. In one, con-volutional networks comprise a stack of convolutional blocks alternated with pooling or downsampling layersin the encoder, and upsampling layers in the decoder. More recently, the UViT family emerged (Hoogeboomet al., 2023a), in which convolutional blocks are used at the higher layers of the encoder and decoder butaugmented with transformer layers at the bottom of the UNet. In both architectural families, text con-ditioning is accomplished via cross-attention layers, also at the bottom, low-resolution layers of the UNet.In doing so, these layers are responsible for conditioning the models deepest representation on the textualand/or multi-modal inputs. At these low-resolution layers, the text conditioning signal is able to inuencethe global image composition while the computational cost of attention is kept relatively low. Our search for a methodology that allows stable training of large models starts by identifying and isolatingcore layers responsible for text-to-image alignment. Our main conjecture is that it is possible to reducethe instability typically observed during training large-scale PSDMs by warming up layers responsible fortext-to-image alignment in isolation from layers responsible for target resolution encoding/decoding.",
  "Specically, we dene the core components as those that directly interface with text conditioning signals andthose that are crucial in the diusion process. They can be described as:": "Text encoding layers combine one or more textual, character, and/or multimodal pretrained repre-sentations (such as those from Rael et al. (2020b); Xue et al. (2022a); Liu et al. (2023); Radfordet al. (2021a)), and project them into the embedding space of the UNet. Typically composed ofMLP on top of pooling layers. Core representation layers comprise hidden layers in the main backbone interfacing with cross-attention layers. They include the bottom layers of the UNet architecture whose features are directlycombined with the embedded text by the cross attention operation and layers between them. Time encoding layers map the diusion time step into the models embedding space.Typicallydesigned as a sinusoidal positional encoder, followed by a shallow MLP. Despite not participatingdirectly in the cross-attention operation, it is a core component of the diusion process. We isolate these core components of a PSDM text-to-image model in order to study their eect on the nalmodels properties. Next, we propose an architecture that enables the pretraining of these layers, and alsosupports the study of the properties emerging from scaling them.",
  "MLP": ": Shallow-UViT architecture: The input image grid is quickly reduced at the entry convolution,while a single residual block with no subsampling layers is used as a shallow encoder and decoder. Thelayers within the core components (in light green) are reused in the nal end-to-end architecture, increasingits training stability, while remaining layers are discarded. To assist the pretraining of the core components and, at the same time investigate the emerging propertiesfrom their scaling, we isolate the core components training and scaling from other confounding factors inthe specication of the UNets encoder-decoder layers. To that end, we simplify the UNets conventionalhierarchical structure, which operates on multiple resolutions, and dene the Shallow-UViT (SU), a simpliedarchitecture comprising a shallow encoder and decoder operating on a xed spatial grid (). Itsencoder and decoder have a single residual block each, containing two layers of 3 3 convolutions withswish activations Ramachandran et al. (2017), and no upsampling or downsampling layers. As a result, theyshare the same spatial grid as the core representation layers at the bottom. The rst convolutional layerat the entry of the architecture projects the input image into the xed size grid used by its core layers. Acorresponding upsampling head at the models output reverses this operation. These input/output layersfacilitate quickly projecting input images with larger resolution into the core representation with xed andlower resolution. As a second simplication, we restrict our investigation to the core components from the UViT model familyowing to the uniform structure of its core representation layers. In contrast, the corresponding layers ofconvolutional UNets present a broader spectrum of design and hyperparameter choices, owing to their non-uniform yet hierarchical structure, rendering their analysis more complex. An alternative to the proposed use of the Shallow-UViT architecture, might be to train the core componentsdirectly as an augmented ViT, as previously explored in latent diusion models (Peebles & Xie, 2023). Ourattempt to explore this approach proved not to be straightforward. A crucial dierence between PSDM andLDM becomes highly relevant here. In the case of LDM, the transformer operates on latent tokens, and thediusion model captures the latent token distribution. Our task, on the other hand, is to pretrain a richrepresentation directly from the raw pixels, for subsequent reuse as deep features within a higher-resolutionpixel-space model. We conjecture that in such approaches the initial layers that are closer to the raw datado not transfer as well when reused within the nal model. Instead, our Shallow-UViT includes proxy additional layers that help with closing the gap between corecomponents feature pretraining and their later use. That is, the auxiliary, yet shallow, input (output) andencoding (decoding) layers help adding expressiveness to the transformations between the input (output)and the models hidden representation. Across the variations explored, the input convolution expands thenumber input channels up to 256 (we observed no improvement with more channels). Beyond ablations on scaling (see section 5), we also found that certain variations for the Shallow-UViTcomposition tend to degrade performance in comparison to our best architecture. In particular, these includethe removal of the shallow encoder/decoder blocks; the use of smaller/larger lters (4 4, 5 5, .., 9 9)and strides (from 1 up to 8) at the entry convolution; and the use of a single output head with a subpixel",
  "Greedy growing": "Here we describe a greedy approach to learn PSDMs for high-resolution images. Our process consists of twodistinct stages, where we rst pretrain the core representation layers at a low resolution using a Shallow-UViT architecture. Then, in the second phase, we replace the encoder/decoder layers with a more expressiveset of UNet layers and train at the target resolution. This two-stage process is in contrast to progressivegrowing, which seeks to add one layer at a time. With this approach, we aim to mitigate the well-knowninstabilities observed during training of large models (Saharia et al., 2022b; Hoogeboom et al., 2023b), whilemaking the best use of the available training corpora.",
  "The greedy growing algorithm can be described as follows": "Phase 1In this phase, the core components of the chosen architecture are identied (see subsection 3.1),and a Shallow-UViT model is build on top of them. The Shallow-UViT is trained on the entire trainingcollection of text-image pairs, as it is not limited to high resolution training images. Phase 2The second phase greedily grows the Shallow-UViTs encoder/decoder (namely, throwing awaythe lower-resolution blocks and adding higher-resolution blocks) to obtain the nal model. More specically,this phase adds encoder and decoder layers at dierent resolutions, while preserving the core representationlayers at the spatial resolution used during the rst phase. In other words, the core components continueoperating on a 16 16 grid.The added layers are randomly initialized, while the core components areinitialized with the weights obtained on the rst phase. The remaining components of the Shallow-UViTmodel are discarded. Next, the grown model is trained. As it is a common practice for the generation of high delity images, atthis point we lter the training data to remove text-image pairs with either image dimension is lower thanthe nal models target resolution. The text encoding layers and the core representation layers are keptfrozen, to preserve the richness of the pretrained representation. The time encoding layers, on the otherhand, are further tuned, jointly with the new encoder and decoder layers introduced in the second phase,which allows it to adapt to changes in the diusion noise schedule. We adjusted the diusion logSNR shiftfor high resolution images as suggested by Hoogeboom et al. (2023b), by a factor of 2 log(64/d). An optionalthird defrosting phase, may be applied in which all layers are jointly tuned, and seeks to benet from thefull capacity of the end-to-end architecture, but in practice we nd that the rst two phases are sucient toobtain a good PSDM. We empirically investigate the behaviour of the proposed algorithm in models of increasing size in subsec-tion 5.2. We investigate the eects of splitting the training of the two tasks in phase one and phase two (i.e.,for text-alignment and high-resolution generation), and we compare with models jointly trained from scratch,end-to-end. During these ablations, we constrain the greedy growing phase to use considerably smaller batchsizes than previous work, with no further regularization to demonstrate the optimization stability.",
  "Experimental settings": "Shallow-UViT:The proposed Shallow-UViT provides a proxy architecture for pre-training the core com-ponents of a larger PSDM. The ablation studies below us a specic instantiation of the model, but we expectShallow-UViT to be exible enough to be used with other component parts. In particular we adopt a combi-nation of two pretrained text encoders for text conditioning: T5-XXL (Rael et al., 2020a) with 128 sequencelength and CLIP (VIT-H14) (Radford et al., 2021b) with 77 sequence length. Given a text prompt, we rsttokenize and encode the text using the two encoders independently, and then concatenate the embeddings,yielding a nal embedding with sequence length of 205. They are projected into models hidden size by thetext encoding layers. We keep the Shallow-UViT design xed, except for changing the capacity by increasing",
  ": Composition of the encoder-decoder layers grown on top of corresponding Shallow-UViT variants.core components identical to the corresponding shallow variant": "We stress that we do not claim that these specic core components are optimal. For instance, it is widelyrecognized that larger pretrained text encoders and longer token sequence lengths increase image quality(Saharia et al., 2022b; Balaji et al., 2022; Podell et al., 2024). Investigating the optimal design of each corecomponent is beyond the scope of this work. Instead, the variations of the Shallow-UViT were intentionallydesigned to explore the performance benets gained by increasing core componentss capacity independentof the remaining model components.",
  "Greedy growing:In the experiments that follow we consider several dierent model sizes. speciesthe Shallow-UViT variants, while species encoder/decoder parameterizations": "To ablate our hypothesis that greedy growing helps the model learn strong representations with larger, diversecorpora, we also train the full model on a high resolution subset of data used to train the Shallow-UViT; i.e.,we simply removed all samples with resolution lower than the target model resolution. To that end, beyondgreedy growing, we explore the three training baselines: 1) We create a baseline with all layers trained fromscratch on this subset; 2) As an alternative to the frozen phase in the greedy growing, we ne-tune the corecomponents on this smaller high resolution subset jointly with the grown components (randomly initialized);and 3) A third baseline adds the optional phase of unfreezing the core components after warming up therandom weights for 500k steps. Models are trained for 2M steps in total. The greedy growing algorithm aims to make training large-scale PSDMs at high resolutions more stable. Inthe case of Simple Diusion (Hoogeboom et al., 2023b), large batch sizes and regularizers like dropout andmulti-scale losses enable end-to-end training from scratch. To stress test the stability and convergence of ourgreedy growing algorithm, we restrict the batch size to 256 instead of the standard 2k, and we use no otherexplicit form of regularization. Under that restriction, our largest model (UVit-XHuge) presented numericalinstabilities when trained from scratch or ne-tuned, as multiple numerical issues occurred during training.Thus, the results of this large model are presented only for the frozen, and freeze-unfreeze methods. Thisbehaviour conrms observations in previous work and their need for large batch sizes. Dataset:Rigorous evaluation of generative image models is challenging when models are trained on pro-prietary datasets. To avoid this issue, we rst demonstrate our key ndings through extensive empiricalevaluations on a publicly available dataset, namely, Conceptual 12M (or CC12M) (Changpinyo et al., 2021).",
  "Metrics": "The evaluation of generative models poses considerable diculties and constitutes an active research area(Kirstain et al., 2024; Xu et al., 2024; Hessel et al., 2021; Serra et al., 2023; Kim et al., 2024; Lee et al.,2023). In light of its inherent complexity, we utilize a multi-faceted evaluation strategy that combines imagedistribution metrics, text-alignment metrics and semantic question and answering metrics to validate ourintermediary results, but the overall performance of our nal model evaluation, Vermeer, is delegated tohuman evaluators (subsection 6.2). The following criteria are considered: Image distribution metrics:We evaluate models on three key metrics, namely, the Frchet InceptionDistance (FID) (Heusel et al., 2017), the Frchet Distance on Dino-v2 feature space (FD-Dino) (Stein et al.,2023; Oquab et al., 2023) and the Clip Maximum Mean Discrepancy (CMMD) distance (Jayasumana et al.,2023). FID is widely used to assess generative image models and select model hyper-parameters, but ourndings corroborate its known limitations: it fails to reect model improvements through training, it doesnot capture readily apparent distortions in individual images, and it does not correlate well with humanperception (Stein et al., 2023; Otani et al., 2023; Jayasumana et al., 2023). Thus, in our study, we do notselect training or sampling hyper-parameters solely on the basis of FID but, as described in Appendix 5.3,we review the trade-os between the observed set of metrics. We also note that metrics derived from image features vary considerably with image resolution. In whatfollows we compute metrics using the same resolution as the reference papers. The exception is for CMMDon Shallow-UViT outputs; the original metric taken at 336336 pixels is dominated by up-sampling eects,obscuring dierences between models. Thus, we replaced the original V iT L14 operating at 336 336 byits version at 224 224 pixels. Multimodal metrics:We adopt CLIP Score as a metric for text-image alignment, as it is widely used,and it complements image distribution metrics above, reecting the consistency of the generated image withthe given prompt. Unlike the original formulation based on ViT-B with path size 32 (Hessel et al., 2021) andprevious papers in the area Saharia et al. (2022a); Hoogeboom et al. (2023b), we adopt the ViT-L (patch14) embedding due to its improved representation. This choice results in lower absolute values of our CLIPScores compared to previous results, however we noticed that these scores better correlate with the presenceof absence of observed distortions.",
  "Shallow-UViT XHuge": ": Qualitative comparison of models with core components of increasing size Shallow-UViTs trained at 64 64 pixels using CC12M dataset only. Prompts: A sloth running a marathon, sur-prisingly outrunning all competitors., A hand spread out on a wall. DSLR photograph., Close-up portraitof a ballerina in mid-performance, with high motion and dramatic lighting., Word art of \"happy birthday\",with a smiling panda wearing a party hat, surrounded by gift boxes and a birthday cake., Four dogs on thestreet. Semantic QG/A frameworks:One can also automatically generate question-answer pairs with a lan-guage model, and then compute image faithfulness by checking whether existing VQA models can answerthe questions from the generated image (Hu et al., 2023; Cho et al., 2024). They were intended to addressthe shortcomings of existing metrics. Despite their eectiveness in evaluating color and material aspects,they often struggle in assessing counting, spatial relationships, and compositions with multiple objects. Suchevaluation measures are naturally dependent on the quality of the underlying question generation (QG)and answering (QA) models. Here we adopt DSG (image-text alignment metric) and its set of 1k prompts(Cho et al., 2024). The DSG-1k test-prompts cover dierent challenges (e.g., counting correctly, correctcolor/shape/text rendering, etc.), semantic categories, and writing styles. A description of the QG, QAused, with qualitative and detailed results, are included in Appendix B.",
  ": Shallow-UVIT evaluated on 1k samples from DSG-1k dataset. Scaling core components improvesperformance across all semantic categories. Fine-grained results in Appendix B": "distribution metrics and Clip-Score are obtained using 30k prompts from the MSCOCO-captions validationset (Chen et al., 2015), while the semantic metrics are extracted on the 1k prompts from DSG-1k (Choet al., 2024). A summary of the impact of scaling the Shallow-UViT model is given in Tables 3 and 4, whilene grained results on semantic categories are reported in Appendix B. All performance measures indicatesignicant improvements due to model scaling. A smaller numerical gain is observed in the comparison ofthe larger two models, but the dierence is reected in qualitative comparisons of the models below. , presents a qualitative comparison of the results the Shallow-UViT variants on challenging prompts.They illustrate the impact of scaling on objects structure, composition and alignment (e.g., with numbers ofobjects depicted). Despite of the small training dataset, the larger models show signicant improvement ingenerating intricate shapes like hands, body parts and text. We observed further quantitative improvements across the metrics when training our larger models for longer(Shallow-UViT-Huge and Shallow-UViT-XHuge), but longer training also exhibits overtting to the CC12training samples. illustrates images generated using the Shallow-UViT XHuge model with increasingnumbers of training steps. As training progresses, the model diverges from the original prompt to produceimages that are closer to training samples from the CC12M dataset, and/or representing parts of the promptonly. This hidden phenomena was not associated with changes in the adopted metrics. We conjecture thatthis eect is largely aggravated by the small size of the training dataset. Considering the complexity associated with evaluating improvements in representation and the limitations ofautomatic performance measures, we also ablate the eect of scaling the core components under a semantictask that is evaluated by human annotators. In this experiment we consider a simple counting task, denedhere as the task of generating images of up to 5 objects based on a subset of text prompts from the numericalsplit of the Gecko benchmark (Wiles et al., 2024). We explore this task as a proxy for gauging both promptconsistency and the models understanding of objects composition and shapes.It allows less subjectiveinterpretation and noise in human judgments of the models performance than other image qualities thatare inuenced by individual preferences. The task of counting under an open set would ultimately imply theability to keep track of objects. Thus, this ablation emulates a much simpler version of the problem. shows the accuracy improvement associated with scaling observed over 59 prompts. Random condition usesa random number between 1-5. The detailed description of this experiment is presented on Appendix C.",
  "step 2M": ": Overtting and memorization of Shallow-UViT XHuge trained on CC12M. Prompts: (top) A groupof construction workers in the style of The Night Watch by Rembrandt.; (middle) A dynamic rendition ofa racing cyclist leading their team through a mountain pass, rendered in the style of Napoleon Crossing theAlps by Jacques-Louis David.; (bottom) A group of friends enjoying a summer day at a riverside restaurantin the style of A Sunday Afternoon on the Island of La Grande Jatte by Georges Seurat. BaseLargeHugeXHugeRandom 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40",
  "XHuge": ": Measuring the impact of scaling on the counting task. Using 59 systematic prompts describing 1-5objects. Five human annotators reviewed each image (95% bootstrapped condence intervals are shown).Models with larger core components are observed to perform better on counting. Sample prompt: 3 apples. Given the shallow encoder-decoder structure of the Shallow-UViT architecture, we conjecture that the per-formance improvements observed here, on multiple metrics, are a direct consequence of scaling the corecomponents. This hypothesis is further investigated via the reuse of their representation in the next section.",
  "Experiments on Greedy growing": "We next explore greedy growing of Shallow-UViT models to high resolution, non-cascaded models.Wecompare training models from scratch on the subset of the CC12M dataset ltered by the target resolution(512 pixels) with alternatives for reusing of the core components pretrained on the full dataset. They validateour main intuitions behind the greedy growing algorithm, i.e., that the introduction of new, untrained layers,as well as shifts in the distribution of the training data are known causes of the catastrophic forgettingphenomena Vasconcelos et al. (2022); Kuo et al. (2023); Yu et al. (2023) possibly damaging the pre-trainedrepresentation. Tables 5 and 6 summarize performance as a function of model scale for greedy growing, along with variousablations of the training procedure. Our greedy growing recipe with frozen core componentss and its optionaldefrosting phase lead to the best results across the metrics. The optional defrosting phase is required forimproving the performance of the smallest model ablated (UViT-Base). Its frozen counterpart showed signsof undertting during training, as it has a small number of trainable parameters (217M) in the addedlayers. Under this low-capacity scenario, the defrosting phase oers a balance between protecting the corecomponents representation and the use of the models full capacity, as it reduces the degradation of thepretrained representation by warming up the growth layers. Other than this special case, the defrosting",
  "UViT-XHugefreeze1.2B2M15.32152.120.5710.269freeze-unfreeze1.2B/7.9B2M16.58222.380.6200.267": ": End2end variants trained on CC12M dataset at 512 512 pixels and batch size 256: image distri-bution metrics (FID, FD-Dino and CMMD). Smaller models benet from netuning all their parameters.Larger models have more capacity in the encoder-decoder layers, and benet from freezing the pretrainedrepresentations, under such a small batch size regime.",
  "100k steps": ": On catastrophic forgetting during early steps of netuning: the pretrained representation quicklydeteriorates due to noise introduced by the random weights from newly added layers. (from left to right)64 64 image produced by the pretrained Shallow-Unet-Huge; followed by 512 512 images (in green)produced at early steps of netuning (ft.) the core representation in an E2e model; and (in blue) freezingthe core layers. Dierences better observed zooming in. Prompts: A loving mother kangaroo carrying herjoey in her pouch..; A close-up portrait of a buttery, revealing the intricate patterns and textures on itswings in exquisite detail.; A playful wolf pup chasing its own tail.; A graceful hummingbird hovering neara bright pink ower.; A dark and gothic illustration of a raven perched on a skull.; A determined sea turtleswimming against the ocean current.",
  "Guidance tuning": "Diusion model hyper-parameters aect both training and sampling quality. It is a common practice to tunethe sampler guidance weights using FID-CLIPscore trade-o curves (Saharia et al., 2022a; Hoogeboom et al.,2023b; Podell et al., 2024). In doing so one aims to strike a balance between images quality (by minimizingFID) and alignment with the text prompt (maximizing the CLIPscore score). That said, it is well knownthat FID does not correlate particularly well with human perception (Stein et al., 2023; Otani et al., 2023;Jayasumana et al., 2023), and large guidance weights are known to increase CLIP-Score but tend to produceover-sharpened, high-contrast images and unrealistic objects (Ho & Salimans, 2021; Saharia et al., 2022b).Due to such limitations, despite widespread use of FID-CLIPscore scores for performance comparisons, inpractice they are adopted as loose measure of performance, and guidance weights are typically set throughqualitative inspection.",
  "CLIP": ": On the use of dierent metric spaces for calibrating guidance. First three rows: guidance valuestaken for minimizing respectively FID, FD-Dino and CMMD. The use of robust features is correlated withbetter shape and composition of images. Last row illustrates the side-eects of further increasing guidanceand CLIP-score. Prompts from MSCOCO caption dataset: A bathroom with a sink and shower curtain witha map print.; A person holds a ip phone displaying the screen.; A motorcycle is parked on a dirt road ina forest.; A stainless shiny serrated knife sits in front of a sliced loaf.; A restroom hanging o the side ofa building over a mountain.",
  "A full diusion pipeline: Vermeer": "Vermeer is an 8B parameter model grown from 256 to 1024 pixel resolution. The UViT architecture is similarto our UViT-Huge model (), except that its bottom layers operate at a grid of 32x32 and with 32transformer blocks in total. We found that allocating transformer blocks at 32x scale improves details (likesmall faces). For Vermeers text encoding, in addition to T5-XXL (Rael et al., 2020a) and Clip (Radfordet al., 2021b) embeddings previously mentioned, we also include a ByT5 (Xue et al., 2022b) encoder with256 sequence length, resulting in a nal embedding with sequence length of 461. The baseline version (Vermeer raw model) is trained with 2k batch size at 256 resolution for 2M iterations,and grown to 1k resolution and netuned for an additional 1M steps. As illustrated in , it supports3 aspect ratios, i.e., 10241024, 7681376, and 1376768 thought aspect ratio bucketing (Anlatan, 2022).Once the raw model is trained, we apply the following extra steps to improve the aesthetics of the generatedimages:",
  "Vermeer results": "We ablated four steps of Vermeers development: (i) its raw model resulting from training on a large dataset;(ii) the result of applying prompt engineering at inference to the same model, adding words to improveaesthetic image quality, but with no further training; (iii) the nal model, after style netuning on a curatedsubset of 3k aesthetically pleasing images; and nally, (iv) its distilled, fast inference variation. reports key performance metrics for all four variants, along with Stable Diusion XL v1.0 (SDXL) (Podellet al., 2024). One can see that the raw model minimizes image distribution metrics that use state of theart feature space, i.e., FD-Dino and CMMD, while CLIP-score suggests a minor drop compared to SDXL.These metrics also highlight a signicant shift away from the distribution of MSCOCO-captions (Chen et al.,2015), after augmenting the prompts (+prompt engineering) that is further increased when combined withthe netuning of the model for aesthetics pleasing image(+style netuning). The MSCOCO-captions dataset comprises reference image-caption pairs covering a diverse set of objectcategories and scenes.Thus, it oers an interesting distribution for measuring image quality and textalignment due to the complexity and diversity of the compositions. At the same time, its use for visualquality preference assessment is spurious as its images were not curated with human aesthetics preferences.On the contrary, many of the images have relatively poor aesthetic appeal. Thus, aiming to improve imageaesthetics and composition, during Vermeers prompt engineering and style tuning phases we intentionally",
  "(c) nal model 3-point Likert": ": Human evaluation results: Likert plot across 495 prompts, two tasks with 13 users each.Vermeer aesthetic is preferred during 61.4% of all comparisons, while its image-text consistency is marginallypreferred. Aggregating the 1k annotations, Veermer is preferred during 44.0% of all comparisons, against21.0% from SDXL. Prompt engineering and style tuning aligned with human preference for visual aesthetics.Side by side qualitative comparisons in and .",
  "move the distribution of images generated by Vermeer away from MSCOCO-caption distribution. To validatethis we rely on human evaluation (in the next section)": "The eect of the changes on the raw model with the CLIP-score and on semantic metrics on the other handis minimal, aligned with our observation that the consistency of the model is not much aected by thesetwo procedures. Semantic VqVa results are presented on . The references to Imagen (Saharia et al.,2022b) and Muse (Chang et al., 2023) models in this table are versions trained on internal data sourcesthus of similar resources and training pipelines than Vermeer. It shows that Vermeer presents competitiveperformance with SDXL, and surpassing the other models, including auto-regressive and cascade models. Finally, we also develop a distilled version of our model, in order to oer an alternative version with fasterinference time that similar to the other models presented in this paper operates as a single, non-cascadeend-to-end model at inference time. illustrates Vermeer outputs and additional qualitative resultsincluding a comparison of samples from the full and distilled versions is presented in Appendix F.",
  "Human evaluation": "Assessing the performance of text-to-image models, ideally, depends on human evaluation, as this complexcognitive process necessitates a profound understanding of text and image relationships. Prior research hasdemonstrated that many recent works rely exclusively on automated metrics, such as the Frchet InceptionDistance (FID). However, it has been observed that the current automated measures are not fully consistentwith human perception in assessing the quality of text-to-image samples (Otani et al., 2023).Thus, toobjectively access the quality of images generated by Vermeer, we conduct a side-by-side human evaluationcomparing our model with SDXL (Podell et al., 2024). Setup. In this human evaluation, we ask annotators to evaluate generated images by Vermeer and SDXLbased on the same prompt. For this, we collected 495 prompts 2 covering a range of skills: 160 are fromTIFA v1.0 targeting measuring the faithfulness of a generated image to its text input covering 12 categories(object, attributes, counting, etc.)(Hu et al., 2023); 200 are sampled from the 1600 Parti Prompts (Yu et al.,2022), selecting for both complexity and diversity of challenges; and 150 others are created fresh for, or aresourced from, more recent prompting strategies targeting challenging cases. We create two tasks in which we instruct annotators to consider either image quality (aesthetics) or t to theprompt (consistency), and indicate their preferences using 3-point Liker scale: Vermeer is preferred, Unsure,and SDXL is preferred (the model names are anonymized). The neural response includes cases that bothimages are equally good and bad. In the annotation UI, the annotators are shown a prompt along withtwo images that are randomly shued. We collected 13 human ratings per prompt for both aesthetics andconsistency (26 ratings per image).",
  "Conclusion": "We propose a novel recipe for training non-cascaded large scale pixel-space text-to-image diusion models. Itbenets from splitting their training in two phases representing dierent tasks: learning image-text conditionalignment and learning to generate images at high-resolution. We identied the model core components as those responsible for the rst task and propose a proxy architec-ture (Shallow-UViT) to supports its pretraining. The second task is learned with a greedy growing algorithmthat stacks encoder-decoder layers of the nal architecture on top of the pretrained core components. Whenlearning the second task, our training recipe preserves the core components representation from the noiseintroduced by the grown layers and their random initialized weights. Existing non-cascaded models training recipes struggle with scale, if not supported with large batch sizeand further regularization like dropout and multi-scale loss. Our approach is able to train models up to 8Bparameters with small batch size (256) and no further regularization, by pretraining the core componentsand preserving it during the second training phase targeting high-resolution generation. Compared with training from scratch and netuning, the greedy growing procedure is more stable, andimproves performance on a set of dierent metrics. Qualitative analysis shows that while keeping the corecomponents representation stable it helps to preserve objects shape and overall structure, improving thedenition of body parts. Our method allows use of data at dierent resolutions; the rst phase benets fromthe larger corpora with minimal requirements on image resolution, while the second phase learn to producesharp images from the set ltered by the target resolution while reusing the representation learned from thelarger set. We also explore models with increasing size, and show the benets from scaling under dierentaspects and metrics. In practice, the non-cascaded solution removes the out-of-distribution shift existent between training anddeploying super-resolution phases. Based on that, we present Vermeer, an 8B parameter Pixel based Text-to-Image Diusion Model that produces high-resolution high-quality images using a single non-cascaded model.By training it on a larger dataset, and incorporating a nal style tuning phase, Vermeer is able to surpassSDXL v1.0 in human preference study.",
  "Anlatan.Novelaiimprovementsonstablediusion,2022.URL": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, TimoAila, Samuli Laine, Bryan Catanzaro, et al. edi: Text-to-image diusion models with an ensemble ofexpert denoisers. arXiv preprint arXiv:2211.01324, 2022. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, JunhwaHur, Guanghui Liu, Amit Raj, Yuanzhen Li, Michael Rubinstein, Tomer Michaeli, Oliver Wang, DeqingSun, Tali Dekel, and Inbar Mosseri. Lumiere: A space-time diusion model for video generation, 2024.",
  "XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedantam,SaurabhGupta,PiotrDollr,andC.LawrenceZitnick.Microsoftcococaptions:Datacollectionandevaluationserver.CoRR, abs/1504.00325, 2015. URL #ChenFLVGDZ15": "Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, JordiPont-Tuset, and Su Wang. Davidsonian Scene Graph: Improving Reliability in Fine-Grained Evaluationfor Text-to-Image Generation. In ICLR, 2024. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diu-sion posterior sampling for general noisy inverse problems. In The Eleventh International Conference onLearning Representations, 2023. URL Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Van-denhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models usingphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023.",
  "Prafulla Dhariwal and Alexander Nichol. Diusion models beat gans on image synthesis. NeurIPS, pp.87808794, 2021": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In InternationalConference on Learning Representations, 2021. URL Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and WielandBrendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy androbustness. In International Conference on Learning Representations, 2019. URL Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diusion models as plug-and-play priors. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances inNeural Information Processing Systems, 2022. URL",
  "Jonathan Ho and Tim Salimans. Classier-free diusion guidance. In NeurIPS 2021 Workshop on DeepGenerative Models and Downstream Applications, 2021.URL": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diusion probabilistic models. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information ProcessingSystems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans.Cascaded diusion models for high delity image generation. J. Mach. Learn. Res., 23(1), jan 2022a.ISSN 1532-4435. Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet.Video diusion models. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022b.URL",
  "Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.Simple diusion: End-to-end diusion for highresolution images. In ICML, 2023a": "Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.Simple diusion: End-to-end diusion for highresolution images. In Proceedings of the 40th International Conference on Machine Learning, ICML23.JMLR.org, 2023b. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A. Smith.Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceed-ings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 2040620417, October2023.",
  "Priyank Jaini, Kevin Clark, and Robert Geirhos. Intriguing properties of generative classiers. 2023": "Ajil Jalal, Marius Arvinte, Giannis Daras, Eric Price, Alexandros G Dimakis, and Jon Tamir.Robustcompressed sensing mri with deep generative priors. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34,pp. 1493814954. Curran Associates, Inc., 2021. URL Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and San-jiv Kumar. Rethinking d: Towards a better evaluation metric for image generation. arXiv preprintarXiv:2401.09603, 2023. Zahra Kadkhodaie and Eero Simoncelli. Stochastic solutions for linear inverse problems using the priorimplicit in a denoiser.In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1324213254. Cur-ran Associates, Inc., 2021. URL Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improvedquality, stability, and variation. In International Conference on Learning Representations, 2018. URL",
  "Mark Levy, Bruno Di Giorgi, Floris Weers, Angelos Katharopoulos, and Tom Nickson. Controllable musicproduction with diusion models and guidance gradients. In NeurIPS, 2023. URL": "Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok,Rj Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering.In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1627016297, Toronto,Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.900. URL Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, IlyaSutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guideddiusion models. arXiv preprint arXiv:2112.10741, 2022.",
  "Andreas Nieder and Stanislas Dehaene. Representation of number in the brain. Annual review of neuro-science, 32:185208, 2009": "Maxime Oquab, Timothe Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-naeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.Dinov2: Learning robust visual features without supervision, 2023. Mayu Otani, Riku Togashi, Yu Sawai, Ryosuke Ishigami, Yuta Nakashima, Esa Rahtu, Janne Heikkil,and Shinichi Satoh.Toward veriable and reproducible human evaluation for text-to-image genera-tion. In Proceedings - 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR2023, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog-nition, pp. 1427714286. IEEE, 2023. doi: 10.1109/CVPR52729.2023.01372. Publisher Copyright:c2023 IEEE.; IEEE/CVF Conference on Computer Vision and Pattern Recognition ; Conference date:18-06-2023 Through 22-06-2023. William Peebles and Saining Xie. Scalable diusion models with transformers. In 2023 IEEE/CVF Interna-tional Conference on Computer Vision (ICCV), pp. 41724182, 2023. doi: 10.1109/ICCV51070.2023.00387.",
  "Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diusion.In The Eleventh International Conference on Learning Representations, 2023. URL": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings ofMachine Learning Research, pp. 87488763. PMLR, 1824 Jul 2021a. URL Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings ofMachine Learning Research, pp. 87488763. PMLR, 1824 Jul 2021b. URL Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu.Exploring the limits of transfer learning with a unied text-to-text trans-former. Journal of Machine Learning Research, 21(140):167, 2020a. URL Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu.Exploring the limits of transfer learning with a unied text-to-text trans-former. Journal of Machine Learning Research, 21(140):167, 2020b. URL",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet,and Mohammad Norouzi. Photorealistic text-to-image diusion models with deep language understand-ing. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in NeuralInformation Processing Systems, 2022a. URL Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David JFleet, and Mohammad Norouzi.Photorealistic text-to-image diusion models with deep languageunderstanding.In S. Koyejo,S. Mohamed,A. Agarwal,D. Belgrave,K. Cho,and A. Oh(eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3647936494. CurranAssociates, Inc., 2022b.URL Alessio Serra, Fabio Carrara, Maurizio Tesconi, and Fabrizio Falchi. The emotions of the crowd: Learningimage sentiment from tweets via cross-modal distillation. arXiv preprint arXiv:2304.14942, 2023.",
  "Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diusion modelsfor inverse problems. In International Conference on Learning Representations, 2023. URL": "George Stein, Jesse Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Ross, Valentin Villecroze, ZhaoyanLiu, Anthony L Caterini, Eric Taylor, and Gabriel Loaiza-Ganem. Exposing aws of generative modelevaluation metrics and their unfair treatment of diusion models. In Advances in Neural InformationProcessing Systems, volume 36, 2023. Vanessa Tan, Junghyun Nam, Juhan Nam, and Junyong Noh. Motion to dance music generation usinglatent diusion model. In SIGGRAPH Asia 2023 Technical Communications, SA 23, New York, NY,USA, 2023. Association for Computing Machinery. ISBN 9798400703140. doi: 10.1145/3610543.3626164.URL Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent corre-spondence from image diusion. In Thirty-seventh Conference on Neural Information Processing Systems,2023. URL Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fredo Durand,William T. Freeman, and Vincent Sitzmann. Diusion with forward models: Solving stochastic inverseproblems without direct supervision.In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. URL CristinaVasconcelos,VighneshNandanBirodkar,andVincentDumoulin.Properreuseofimageclassicationfeaturesimprovesobjectdetection.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.1362813637,2022.URL Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajic, Su Wang, Emanuele Bugliarello, YasumasaOnoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, and Aida Nematzadeh. Revisiting text-to-image evaluation with gecko: On metrics, prompts, and human ratings. Under review (ECCV), 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.Imagereward: Learning and evaluating human preferences for text-to-image generation.Advances inNeural Information Processing Systems, 36, 2024. Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, andColin Rael. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactionsof the Association for Computational Linguistics, 10:291306, 03 2022a. Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, andColin Rael. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of theAssociation for Computational Linguistics, 10:291306, 2022b. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexan-der Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang,Jason Baldridge, and Yonghui Wu. Scaling Autoregressive Models for Content-Rich Text-to-Image Gen-eration, 2022. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional CLIP. In Alice Oh, Tristan Naumann, AmirGloberson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Pro-cessing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,"
}