{
  "Abstract": "Controllable text-to-image (T2I) diffusion models generate images conditioned on both textprompts and semantic inputs of other modalities like edge maps. Nevertheless, currentcontrollable T2I methods commonly face challenges related to efficiency and faithfulness,especially when conditioning on multiple inputs from either the same or diverse modalities. Inthis paper, we propose a novel Flexible and Efficient method, FlexEControl, for controllableT2I generation. At the core of FlexEControl is a unique weight decomposition strategy, whichallows for streamlined integration of various input types. This approach not only enhancesthe faithfulness of the generated image to the control, but also significantly reduces thecomputational overhead typically associated with multimodal conditioning. Our approachachieves a reduction of 41% in trainable parameters and 30% in memory usage comparedwith Uni-ControlNet. Moreover, it doubles data efficiency and can flexibly generate imagesunder the guidance of multiple input conditions of various modalities.",
  "Introduction": "In the realm of text-to-image (T2I) generation, diffusion models exhibit exceptional performance in trans-forming textual descriptions into visually accurate images. Such models exhibit extraordinary potentialacross a plethora of applications, spanning from content creation (Rombach et al., 2022; Saharia et al., 2022b;Nichol et al., 2021; Ramesh et al., 2021a; Yu et al., 2022; Avrahami et al., 2023; Chang et al., 2023), imageediting (Balaji et al., 2022; Kawar et al., 2023; Couairon et al., 2022; Zhang et al., 2023; Valevski et al., 2022;Nichol et al., 2021; Hertz et al., 2022; Brooks et al., 2023; Mokady et al., 2023), and also fashion design (Caoet al., 2023). We propose a new unified method that can tackle two problems in text-to-image generation:improve the training efficiency of T2I models concerning memory usage, computational requirements, and athirst for extensive datasets (Saharia et al., 2022a; Rombach et al., 2022; Ramesh et al., 2021b); and improvetheir controllability especially when dealing with multimodal conditioning, e.g. multiple edge maps and atthe same time follow the guidance of text prompts, as shown in (c). Controllable text-to-image generation models (Mou et al., 2023) often come at a significant training com-putational cost, with linear growth in cost and size when training with different conditions. Our approachcan improve the training efficiency of existing text-to-image diffusion models and unify and flexibly handledifferent structural input conditions all together. We take cues from the efficient parameterization strategiesprevalent in the NLP domain (Pham et al., 2018; Hu et al., 2021; Zaken et al., 2021; Houlsby et al., 2019) andcomputer vision literature (He et al., 2022). The key idea is to learn shared decomposed weights for variedinput conditions, ensuring their intrinsic characteristics are conserved. Our method has several benefits:It not only achieves greater compactness (Rombach et al., 2022), but also retains the full representationcapacity to handle various input conditions of various modalities; Sharing weights across different conditionscontributes to the data efficiency; The streamlined parameter space aids in mitigating overfitting to singularconditions, thereby reinforcing the flexible control aspect of our model. Meanwhile, generating images from multiple homogeneous conditional inputs, especially when they presentconflicting conditions or need to align with specific text prompts, is challenging. To further augment ourmodels capability to handle multiple inputs from either the same or diverse modalities as shown in ,",
  "(a) Efficiency Comparisons": ": (a) FlexEControl excels in training efficiency, achieving superior performance with just half thetraining data compared to its counterparts on (b) Controllable Text-to-Image Generation w. Different InputConditions (one edge map and one segmentation map). (c) FlexEControl effectively conditions on two cannyedge maps. The text prompt is Stormtroopers lecture at the football field in both Figure (b) andFigure (c). during training, we introduce a new training strategy with two new loss functions introduced to strengthenthe guidance of corresponding conditions. This approach, combined with our compact parameter optimizationspace, empowers the model to learn and manage multiple controls efficiently, even within the same category(e.g., handling two distinct segmentation maps and two separate edge maps). Our primary contributions aresummarized below: We propose FlexEControl, a novel text-to-image generation model for efficient controllable imagegeneration that substantially reduces training memory overhead and model parameters throughdecomposition of weights shared across different conditions. We introduce a new training strategy to improve the flexible controllability of FlexEControl. Comparedwith previous works, FlexEControl can generate new images conditioning on multiple inputs fromdiverse compositions of multiple modalities. FlexEControl shows on-par performance with Uni-ControlNet (Zhao et al., 2023) on controllable text-to-image generation with 41% less trainable parameters and 30% less training memory. Furthermore,FlexEControl exhibits enhanced data efficiency, effectively doubling the performance achieved withonly half amount of training data.",
  "Decoder": ": Overview of FlexEControl: a decomposed green matrix is shared across different inputconditions, significantly enhancing the models efficiency and preserving the image content. During training,we integrate two specialized loss functions to enable flexible control and to adeptly manage conflictingconditions. In the example depicted here, the new parameter size is efficiently condensed to 4 + 6n, where ndenotes the number of decomposed matrix pairs. 2021a) and then low-rank decomposition over the updated weights of the copied Stable Diffusion encoder. Toenhance the control from language and different input conditions, we propose a new training strategy withtwo newly designed loss functions. The details are shown in the sequel.",
  "Preliminary": "We use Stable Diffusion 1.5 (Rombach et al., 2022) in our experiments. This model falls under the categoryof Latent Diffusion Models (LDM) that encode input images x into a latent representation z via an encoderE, such that z = E(x), and subsequently carry out the denoising process within the latent space Z. An LDMis trained with a denoising objective as follows:",
  "Efficient Training for Controllable Text-to-Image (T2I) Generation": "Our approach is motivated by empirical evidence that Kronecker Decomposition (Zhang et al., 2021a)effectively preserves critical weight information. We employ this technique to encapsulate the shared relationalstructures among different input conditions. Our hypothesis posits that by amalgamating diverse conditionswith a common set of weights, data utilization can be optimized and training efficiency can be improved. Wefocus on decomposing and fine-tuning only the cross-attention weight matrices within the U-Net (Ronnebergeret al., 2015) of the diffusion model, where recent works (Kumari et al., 2023) show their dominance whencustomizing the diffusion model. As depicted in , the copied encoder from the Stable Diffusion willaccept conditional input from different modalities. During training, we posit that these modalities, beingtransformations of the same underlying image, share common information. Consequently, we hypothesizethat the updated weights of this copied encoder, W , can be efficiently adapted within a shared decomposedlow-rank subspace. This leads to:",
  "Under review as submission to TMLR": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,Caiming Xiong, Silvio Savarese, et al. 2023. Unicontrol: A unified diffusion model for controllable visualgeneration in the wild. arXiv preprint arXiv:2305.11147. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from naturallanguage supervision. arXiv preprint arXiv:2103.00020.",
  "with n is the number of decomposed matrices, ui Rkn r and vi Rr d": "n , where r is the rank of the matrixwhich is a small number, Hi are the decomposed learnable matrices shared across different conditions, and is the Kronecker product operation. The low-rank decomposition ensures a consistent low-rank representationstrategy. This approach substantially saves trainable parameters, allowing efficient fine-tuning over thedownstream text-to-image generation tasks. The intuition for why Kronecker decomposition works for finetuning partially is partly rooted in the findingsof Zhang et al. (2021a); Mahabadi et al. (2021); He et al. (2022). These studies highlight how the modelweights can be broken down into a series of matrix products and thereby save parameter space. As shownin , the original weights is 6x6, then decomposed into a series of matrix products. When adaptingthe training approach based on the decomposition to controllable T2I, the key lies in the shared weights,which, while being common across various conditions, retain most semantic information. The KroneckerDecomposition is known for its multiplicative rank property and content-preserving qualities. For instance,the shared slow weights (Wen et al., 2020) of an image, combined with another set of fast low-rankweights, can preserve the original images distribution without a loss in semantic integrity, as illustrated in. This observation implies that updating the slow weights is crucial for adapting to diverse conditions.Following this insight, it becomes logical to learn a set of condition-shared decomposed weights in each layer,ensuring that these weights remain consistent across different scenarios. The data utilization and parameterefficiency is also improved.",
  "We then discuss how to improve the control under multiple input conditions of varying modalities with theefficient training approach": "Dataset Augmentation with Text Parsing and SegmentationTo optimize the model for scenariosinvolving multiple homogeneous (same-type) conditional inputs, we initially augment our dataset. We utilizea large language model (gpt-3.5-turbo) to parse texts in prompts containing multiple object entities. Theparsing query is structured as: Given a sentence, analyze the objects in this sentence, give methe objects if there are multiple. Following this, we apply CLIPSeg (Lddecke and Ecker, 2022)(clipseg-rd64-refined version) to segment corresponding regions in the images, allowing us to dividestructural conditions into separate sub-feature maps tailored to the parsed objects. These segmentationmasks are selectively used to augment the dataset, specifically when there is a clear, single mask for eachidentified object. This selective approach helps maintain the robustness of the dataset and enhances trainingperformance.",
  "where is the Iverson bracket, CAli is the cross-attention map for token i in layer l, and Tj denotes the setof tokens associated with the j-th segment": "The model is trained to predict noise for image-text pairs concatenated based on the parsed and segmentedresults. An additional loss term, designed to ensure focused reconstruction in areas relevant to each text-derived concept, is introduced. This loss is calculated as the Mean Squared Error (MSE) deviation frompredefined masks corresponding to the segmented regions:",
  "Lca = Ez,tAi(vi, zt) Mi22,(4)": "where Ai(vi, zt) is the cross-attention map between token vi and noisy latent zt, and Mi represents the maskfor the i-th segment, which is derived from the segmented regions in our augmented dataset and appropriatelyresized to match the dimensions of the cross-attention maps. Masked Diffusion LossTo ensure fidelity to the specified conditions, we apply a condition-selectivediffusion loss that concentrates the denoising effort on conceptually significant regions. This focused lossfunction is applied solely to pixels within the regions delineated by the concept masks, which are derived fromthe non-zero features of the input structural conditions. Specifically, the masks are binary where non-zerofeature areas are assigned a value of one, and areas lacking features are set to zero. Because of the sparsityof pose features for this condition, we use the all-ones mask. These masks serve to underscore the regionsreferenced in the corresponding text prompts:",
  "Datasets": "In pursuit of our objective of achieving controlled Text-to-Image (T2I) generation, we employed the LAIONimproved_aesthetics_6plus (Schuhmann et al., 2022) dataset for our model training. Specifically, wemeticulously curated a subset comprising 5,082,236 instances, undertaking the elimination of duplicatesand applying filters based on criteria such as resolution and NSFW score. Given the targeted nature ofour controlled generation tasks, the assembly of training data involved considerations of additional inputconditions, specifically edge maps, sketch maps, depth maps, segmentation maps, and pose maps. Theextraction of features from these maps adhered to the methodology expounded in(Zhang and Agrawala, 2023).",
  "Experimental Setup": "Structural Input Condition ExtractionWe start from the processing of various local conditions used inour experiments. To facilitate a comprehensive evaluation, we have incorporated a diverse range of structuralconditions. These conditions include edge maps (Canny, 1986; Xie and Tu, 2015; Gu et al., 2022a), sketchmaps (Simo-Serra et al., 2016), pose information (Cao et al., 2017), depth maps (Ranftl et al., 2020), andsegmentation maps (Xiao et al., 2018), each extracted using specialized techniques. These conditions arecrucial for guiding the text-to-image generation process, enabling FlexEControl to produce images that areboth visually appealing and semantically aligned with the text prompts and structural inputs. The additionaldetails for extracting those conditions are given in the Appendix.",
  "T2IAdapter (Mou et al., 2023)0.4480--0.524190.010.69830.315627.800.4957": "ControlNet (Zhang and Agrawala, 2023)0.49890.61720.49900.601389.080.74810.202427.620.4931Uni-Control (Qin et al., 2023)0.49770.63740.48850.550990.040.71430.208327.800.4899Uni-ControlNet (Zhao et al., 2023)0.49100.60830.47150.590190.170.70840.212527.740.4890PHM (Zhang et al., 2021a)0.43650.57120.46330.487891.380.55340.166427.910.4961LoRA (Hu et al., 2021)0.44970.63810.50430.509789.090.54800.153827.990.4832FlexEControl (ours)0.49900.63850.50410.551890.930.74960.209327.550.4963 Evaluation MetricsWe employ a comprehensive benchmark suite of metrics including mIoU (Rezatofighiet al., 2019), SSIM (Wang et al., 2004), mAP, MSE, FID (Heusel et al., 2017), and CLIP Score (Hessel et al.,2021; Radford et al., 2021) 1. BaselinesIn our comparative evaluation, we assess T2I-Adapter (Mou et al., 2023), PHM (Zhang et al.,2021a), Uni-ControlNet (Zhao et al., 2023), and LoRA (Hu et al., 2021). The implementation details aregiven in the Appendix. Implementation DetailsIn accordance with the configuration employed in Uni-ControlNet, we utilizedStable Diffusion 1.5 2 as the foundational model. Our model underwent training for a singular epoch,employing the AdamW optimizer (Kingma and Ba, 2014) with a learning rate set at 105. Throughout allexperimental iterations, we standardized the dimensions of input and conditional images to 512 512. Thefine-tuning process was executed on P3 AWS EC2 instances equipped with 64 NVIDIA V100 GPUs. For baseline implementations, we compare FlexEControl with T2I-Adapter (Mou et al., 2023), PHM (Zhanget al., 2021a), Uni-ControlNet (Zhao et al., 2023), and LoRA (Hu et al., 2021) where we implement LoRAand PHM layers over the trainable modules in Uni-ControlNet interms of generated image quality andcontrollability. The rank of LoRA is set to 4. For PHM (Zhang et al., 2021a), we implement it by performingKronecker decomposition and share weights across different layer, with the number of decomposed matrixbeing 4. For quantitative assessment, a subset comprising 10,000 high-quality images from the LAION im-proved_aesthetics_6.5plus dataset was utilized. The resizing of input conditions to 512 512 wasconducted during the inference process.",
  "Multiple Conditioning": "Uni-ControlNet0.30780.39620.30540.387198.840.39810.139328.750.4828FlexEControl (w/o Lca)0.36420.49010.37040.481594.950.43680.140528.500.4870FlexEControl (w/o Lmask)0.36660.48340.37120.483194.890.44000.140628.680.4542FlexEControl0.36900.49150.37840.484992.900.44290.141128.240.4873 provides a comprehensive comparison of FlexEControls performance against Uni-ControlNet andT2IAdapter across diverse input conditions. After training on a dataset of 5M text-image pairs, FlexEControldemonstrates better, if not superior, performance metrics compared to Uni-ControlNet and T2IAdapter. Notethat Uni-ControlNet is trained on a much larger dataset (10M text-image pairs from the LAION dataset).Although there is a marginal decrease in SSIM scores for sketch maps and mAP scores for poses, FlexEControlexcels in other metrics, notably surpassing Uni-ControlNet and T2IAdapter. This underscores our methodsproficiency in enhancing efficiency and elevating overall quality and accuracy in controllable text-to-imagegeneration tasks. To validate FlexEControls effectiveness in handling multiple structural conditions, we compared it withUni-ControlNet through human evaluations. Two scenarios were considered: multiple homogeneous inputconditions (300 images, each generated with 2 canny edge maps) and multiple heterogeneous input conditions(500 images, each generated with 2 randomly selected conditions). Results, summarized in , reveal thatFlexEControl was preferred by 64.00% of annotators, significantly outperforming Uni-ControlNet (23.67%).This underscores FlexEControls proficiency with complex, homogeneous inputs. Additionally, FlexEControldemonstrated superior alignment with input conditions (67.33%) compared to Uni-ControlNet (23.00%). Inscenarios with random heterogeneous conditions, FlexEControl was preferred for overall quality and alignmentover Uni-ControlNet. In addition to our primary comparisons, we conducted an additional quantitative evaluation of FlexEControland Uni-ControlNet. This evaluation focused on assessing image quality under scenarios involving multipleconditions from both the homogeneous and heterogeneous modalities.The findings of this evaluationare summarized in . FlexEControl consistently outperforms Uni-ControlNet in both categories,demonstrating lower FID scores for better image quality and higher CLIP scores for improved alignment withtext prompts.",
  "Ablation Studies": "To substantiate the efficacy of FlexEControl in enhancing training efficiency while upholding commendablemodel performance, and to ensure a fair comparison, an ablation study was conducted by training modelson an identical dataset. We trained FlexEControl along its variants and Uni-ControlNet on a subset of100,000 training samples from LAION improved_aesthetics_6plus. When trained with the identicaldata, FlexEControl performs better than Uni-ControlNet. The outcomes are presented in . Evidently,FlexEControl exhibits substantial improvements over Uni-ControlNet when trained on the same dataset.This underscores the effectiveness of our approach in optimizing data utilization, concurrently diminishingcomputational costs, and enhancing efficiency in the text-to-image generation process. We also study the impact of ca and mask trained on the subset of 100,000 samples from LAION im-proved_aesthetics_6plus for 6,000 steps. We evaluated the score on SSIM of canny edge maps andmIoU of segmentation maps, results are shown in . As observed, FlexEControl achieves optimalperformance when both ca = 0.01 and mask = 0.01. Additionally, as indicated in , FlexEControloutperforms the configurations where either ca or mask is set to zero, demonstrating the importance ofincorporating both the cross-attention supervision loss and the masked diffusion loss. However, higher values",
  ": Quantitative and qualitative comparison showing the effect of excluding cross-attention supervisionloss and masked diffusion loss": ": Qualitative comparison of FlexEControl and existing controllable diffusion models with singlecondition. Text prompt: A bed. The image quality of FlexEControl is comparable to existing methods andUni-ControlNet + LoRA, while FlexEControl has much more efficiency. of ca and mask could lead to instability during training, causing the model to focus excessively on localconditions, which results in worse performance. We also show the qualitative comparison of ca and mask in. Without the cross-attention loss, the concept of the whale tends to merge with the tree, causingattribute leakage and misblending issues. Without the masked diffusion loss, the model is more likely togenerate the whale outside the intended region, leading to blurred regions and a lack of focus within themaps.",
  "Additional Results on Stable Diffusion 2": "In our efforts to explore the versatility and adaptability of FlexEControl, we conducted additional experimentsusing the Stable Diffusion 2.1 model, available at Hugging Faces Model Hub. The results from theseexperiments are depicted in . FlexEControl can leverage the advancements in Stable Diffusion 2.1 toachieve even better performance in text-to-image generation tasks. For the sake of a fair comparison in themain paper, we conduct experiments using Stable Diffusion 1.5 model.",
  "Text Prompt: A cat and a sofa": ": Qualitative comparison of FlexEControl and existing controllable diffusion models with multipleheterogeneous conditions. First row: FlexEControl effectively integrates both the segmentation and edge mapsto generate a coherent image while Uni-ControlNet and LoRA miss the segmentation map and Uni-Controlgenerates a messy image. Additionally, the cats generated by Uni-ControlNet, LoRA, and Uni-Control lackclear feline characteristics and does not align with the edge map. Second row: The input condition types areone depth map and one sketch map. FlexEControl can do more faithful generation while all three othersgenerate the candle in the coffee. : Human evaluation of FlexEControl and Uni-ControlNet under homogenous and heterogeneous struc-tural conditions, assessing both human preference and condition alignment. \"Win\" indicates FlexEControlspreference, \"Tie\" denotes equivalence, and \"Lose\" indicates Uni-ControlNets preference. Results indicate thatunder homogeneous conditions, FlexEControl outperforms Uni-ControlNet in both human preference andcondition alignment.",
  ": Qualitative results on multimodal control using three homogeneous condition images": "a single condition is input. However, with multiple conditions, FlexEControl consistently and noticeablyoutperforms other models. Particularly, under multiple homogeneous conditions, FlexEControl excels ingenerating overall higher quality images that align more closely with the input conditions, surpassing othermodels. We also shown in the qualitative results of generating multiple foregrounds.. In the teddy bear anddog condition case, the teddy bear has a noisy background, and the dog image has a noisy foreground, whilethe prompt asks the model to generate a teddy bear and a bear. As shown in the results, FlexEControleffectively handles the noisy and conflicting semantic information, successfully following the prompt togenerate an image with two foregrounds: a teddy bear and a bear, even though the bear condition comesfrom a dog image. In the robot case, the depth map for the soccer ball is blurred, and the player given is ahuman soccer player, yet the model still follows the prompt to generate robot soccer player images with threeadditional soccer balls. These examples demonstrate the strong controllability of FlexEControl. Furthermore, we show in the qualitative results for FlexEControl on mulimodal control using threehomogeneous conditiona images. Further highlighting the versatility and effectiveness of FlexEControl in",
  "Condition 1Condition 2Ours": ": Failure cases. Failure cases in generating human images (left): The text prompt is: a basketballplayer with a helicopter. Due to limitations in the Stable Diffusion backbone, FlexEControl fails togenerate a correct basketball player, instead producing an image where the basketball players face is replacedwith a basketball. Failure cases in generating images with a weak text prompt for the background (right): Thetext prompt is a car is parking, which provides little information about the background. The backgroundsegmentation map is complex, and the foreground uses a strong canny edge guidance. Consequently, thegenerated image shows a weak effect of the segmentation, although the foreground is generated accurately. handling multiple conditions. Additionally, we showcase the extensibility of FlexEControl in controllablevideo generation. The results are presented in and in the Appendix, where results forproviding one condition and multiple conditions are demonstrated.",
  "Limitations": "Although FlexEControl achieves strong performance, it shares the inherent limitations of diffusion-based imagegeneration models. The LAION dataset exhibits certain biases, which can lead to suboptimal performancein specific scenarios. FlexEControl could benefit from a more robust and strong pretrained text-to-imagegeneration backbone and could be further improved with access to better open-source datasets that mitigatethe risk of generating biased, toxic, sexualized, or other harmful content. Some failure cases and analyseswhere FlexEControl struggled are illustrated in .",
  "Efficient Training": "Prior work has proposed efficient training methodologies both for pretraining and fine-tuning. These methodshave established their efficacy across an array of language and vision tasks. One of these explored strategies isPrompt Tuning (Lester et al., 2021), where trainable prompt tokens are appended to pretrained models (Schickand Schtze, 2020; Ju et al., 2021; Jia et al., 2022). These tokens can be added exclusively to input embeddingsor to all intermediate layers (Li and Liang, 2021), allowing for nuanced model control and performanceoptimization. Low-Rank Adaptation (LoRA) (Hu et al., 2021) is another innovative approach that introducestrainable rank decomposition matrices for the parameters of each layer. LoRA has exhibited promising fine-tuning ability on large generative models, indicating its potential for broader application. Furthermore, the useof Adapters inserts lightweight adaptation modules into each layer of a pretrained transformer (Houlsby et al.,2019; Rckl et al., 2021). This method has been successfully extended across various setups (Zhang et al.,",
  "Controllable Text-to-Image Generation": "Recent developments in the text-to-image generation domain strives for more control over image generation,enabling more targeted, stable, and accurate visual outputs, several models like T2I-Adapter (Mou et al.,2023) and Composer (Huang et al., 2023) have emerged to enhance image generations following the semanticguidance of text prompts and multiple different structural conditional control. However, existing methods arestruggling at dealing with multiple conditions from the same modalities, especially when they have conflicts,e.g. multiple segmentation maps and at the same time follow the guidance of text prompts; Recent studies alsohighlight challenges in controllable text-to-image generation (T2I), such as omission of objects in text promptsand mismatched attributes (Lee et al., 2023; Bakr et al., 2023), showing that current models are strugglingat handling controls from different conditions. Towards these, the Attend-and-Excite method Chefer et al.(2023) refines attention regions to ensure distinct attention across separate image regions. ReCo Yang et al.(2023), GLIGEN Li et al. (2023b), and Layout-Guidance Chen et al. (2023) allow for image generationinformed by bounding boxes and regional descriptions. Mo et al. (2024) offers a training-free approach tomultimodal control. FlexEControl improves the models controllability by proposing a new training strategy,distinguishing itself by targeting the flexibility and efficiency of multimodal control, especially in scenarioswith conflicting conditions from the same or different modalities (e.g., multiple segmentation maps combinedwith text prompts).",
  "Conclusion": "In this work, we present FlexEControl, an approach designed to enhance both the flexibility and efficiency ofcontrollable diffusion-based text-to-image generation. Our method introduces several key innovations: DatasetAugmentation with Text Parsing and Segmentation, Cross-Attention Supervision, and Masked DiffusionLoss, which together significantly improve the models ability to handle diverse and conflicting multimodalinputs. Additionally, we propose an Efficient Training strategy that optimizes parameter, data, and memoryefficiency without sacrificing performance or inference speed. We also demonstrate that sharing a commonset of decomposed weights across different multimodal conditions via Kronecker Decomposition can furtheroptimize parameter space and enhance efficiency. These findings suggest that FlexEControl can be readilyadapted to other architectures, offering a scalable solution for future developments in text-to-image generation.Future work could explore more advanced decomposition techniques and their application to cutting-edgediffusion backbones or Diffusion Transformers (DiTs), aiming to further optimize model efficiency, complexity,and expressive power.",
  "Broader Impact Statement": "While FlexEControl demonstrates promising results in efficient and controllable text-to-image generation,the ability of FlexEControl to generate realistic images based on textual descriptions raises ethical concerns,especially regarding the creation of misleading or deceptive content. It is imperative to establish guidelinesand ethical standards for the use of such technology to prevent misuse in generating deepfakes or propagatingfalse information.",
  "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-a-scene:Extracting multiple concepts from a single image. In SIGGRAPH Asia 2023 Conference Papers, pages112": "Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and MohamedElhoseiny. 2023. HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2004120053. Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, TimoAila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi: Text-to-image diffusion models with an ensembleof expert denoisers. arXiv preprint arXiv:2211.01324.",
  "John Canny. 1986. A computational approach to edge detection. IEEE Transactions on pattern analysis andmachine intelligence, (6):679698": "Shidong Cao, Wenhao Chai, Shengyu Hao, Yanting Zhang, Hangyue Chen, and Gaoang Wang. 2023.Difffashion: Reference-based fashion design with structure-aware transfer by diffusion models. IEEETransactions on Multimedia. Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. 2017. Realtime multi-person 2d pose estimationusing part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 72917299. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, KevinMurphy, William T Freeman, Michael Rubinstein, et al. 2023. Muse: Text-to-image generation via maskedgenerative transformers. arXiv preprint arXiv:2301.00704. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG),42(4):110.",
  "Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey.International Journal of Computer Vision, 129:17891819": "Geonmo Gu, Byungsoo Ko, SeoungHyun Go, Sung-Hyun Lee, Jingeun Lee, and Minchul Shin. 2022a. Towardslight-weight and real-time line segment detection. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 36, pages 726734. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.2022b. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1069610706.",
  "Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Compacter: Efficient Low-RankHypercomplex Adapter Layers. arXiv:2106.04647 [cs]": "Shyam Marjit, Harshit Singh, Nityanand Mathur, Sayak Paul, Chia-Mu Yu, and Pin-Yu Chen. 2024.Diffusekrona: A parameter efficient fine-tuning method for personalized diffusion model. arXiv preprintarXiv:2402.17412. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and TimSalimans. 2023. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 1429714306. Sicheng Mo, Fangzhou Mu, Kuan Heng Lin, Yanli Liu, Bochen Guan, Yin Li, and Bolei Zhou. 2024. Freecontrol:Training-free spatial control of any text-to-image diffusion model with any condition. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74657475. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-text inversionfor editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 60386047. Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023.T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.arXiv preprint arXiv:2302.08453. Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing withtext-guided diffusion models. arXiv preprint arXiv:2112.10741.",
  "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and IlyaSutskever. 2021a. Zero-shot text-to-image generation. In ICML": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and IlyaSutskever. 2021b. Zero-shot text-to-image generation. In International Conference on Machine Learning,pages 88218831. PMLR. Ren Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. 2020. Towards robustmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions onpattern analysis and machine intelligence, 44(3):16231637. Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. 2019.Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 658666. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1068410695. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedicalimage segmentation. In International Conference on Medical image computing and computer-assistedintervention, pages 234241. Springer.",
  "Andreas Rckl, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna Gurevych.2021. AdapterDrop: On the Efficiency of Adapters in Transformers. arXiv:2010.11918 [cs]": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed KamyarSeyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, JonathanHo, David J Fleet, and Mohammad Norouzi. 2022a. Photorealistic text-to-image diffusion models withdeep language understanding. arXiv:2205.11487. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022b. Photorealistic text-to-imagediffusion models with deep language understanding. Advances in Neural Information Processing Systems,35:3647936494.",
  "Saining Xie and Zhuowen Tu. 2015. Holistically-nested edge detection. In Proceedings of the IEEE internationalconference on computer vision, pages 13951403": "Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu,Ce Liu, Michael Zeng, et al. 2023. Reco: Region-controlled text-to-image generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1424614255. Shih-Ying Yeh, Yu-Guan Hsieh, Zhidong Gao, Bernard BW Yang, Giyeong Oh, and Yanmin Gong. 2023.Navigating text-to-image customization: From lycoris fine-tuning to model evaluation. In The TwelfthInternational Conference on Learning Representations. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 2022. Scaling autoregressive models for content-richtext-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5.",
  "ADetailed Model Architecture": "In ControlNet (Zhang and Agrawala, 2023) and Uni-ControlNet (Zhao et al., 2023), the weights of StableDiffusion (SD) (Rombach et al., 2022) are fixed and the input conditions are fed into zero-convolutions andadded back into the main Stable Diffusion backbone. Specifically, for Uni-ControlNet, they uses a multi-scalecondition injection strategy that extracts features at different resolutions and uses them for condition injectionreferring to the implementation of Feature Denormalization (FDN):",
  "+ (zero (hr (c))) ,(7)": "where Z denotes noise features, c denotes the input conditional features, denotes learnable convolutionallayers, and zero denotes zero convolutional layer. The zero convolutional layer contains weights initialized tozero. This ensures that during the initial stages of training, the model relies more on the knowledge fromthe backbone part, gradually adjusting these weights as training progresses. The use of such layers aids inpreserving the architectures original behavior while introducing structure-conditioned inputs. We use thesimilar model architecture while we perform efficient training proposed in the main paper. We show themodel architecture in .",
  "DAdditional Related Works": "Knowledge Distillation for Vision-and-Language ModelsKnowledge distillation (Gou et al., 2021),as detailed in prior research, offers a promising approach for enhancing the performance of a more streamlinedstudent model by transferring knowledge from a more complex teacher model (Hinton et al., 2015; Sanhet al., 2019; Hu et al.; Gu et al., 2021; Li et al., 2021). The crux of this methodology lies in aligning thepredictions of the student model with those of the teacher model. While a significant portion of existingknowledge distillation techniques leans towards employing pretrained teacher models (Tolstikhin et al., 2021),there has been a growing interest in online distillation methodologies (Wang and Jordan, 2021). In onlinedistillation (Guo et al., 2020), multiple models are trained simultaneously, with their ensemble serving as theteacher. Our approach is reminiscent of online self-distillation, where a temporal and resolution ensembleof the student model operates as the teacher. This concept finds parallels in other domains, having beenexamined in semi-supervised learning (Peters et al., 2017), label noise learning (Bengio et al., 2010), and quiterecently in contrastive learning (Chen et al., 2020). Our work on distillation for pretrained text-to-imagegenerative diffusion models distinguishes our method from these preceding works. (Salimans and Ho, 2022;"
}