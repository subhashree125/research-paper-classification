{
  "Abstract": "Simulation-based inference (SBI) methods enable the estimation of posterior distributionswhen the likelihood function is intractable, but where model simulation is feasible. Popularneural approaches to SBI are neural posterior estimation (NPE) and its sequential version(SNPE). These methods can outperform statistical SBI approaches such as approximateBayesian computation (ABC), particularly for relatively small numbers of model simulations.However, we show in this paper that the NPE methods are not guaranteed to be highlyaccurate, even on problems with low dimension. In such settings the posterior cannot beaccurately trained over the prior predictive space, and even the sequential extension remainssub-optimal. To overcome this, we propose preconditioned NPE (PNPE) and its sequentialversion (PSNPE), which uses a short run of ABC to effectively eliminate regions of parameterspace that produce large discrepancies between simulations and data and allow the posterioremulator to be more accurately trained. We present comprehensive empirical evidence thatthis melding of neural and statistical SBI methods improves performance over a range ofexamples including a motivating example involving a complex agent-based model applied toreal tumour growth data.",
  "Introduction": "Computational models, frequently termed as simulators, are typically governed by stochastic processes.When provided with a set of parameter values, these simulators output synthetic data that inherently capturethe stochastic nature of the simulated phenomena. However, a substantial challenge arises in performingposterior inference for the parameters of these simulators, as the corresponding likelihood function is oftenintractable.An example is agent-based modelling of tumour growth (e.g.Jenner et al. (2020); Aylett-Bullock et al. (2021); Warne et al. (2022)), where cell proliferation, movement and invasion are governed by",
  "Published in Transactions on Machine Learning Research (09/2024)": "Maximilian Dax, Stephen R Green, Jonathan Gair, Jakob H Macke, Alessandra Buonanno, and BernhardSchlkopf. Real-time gravitational wave science with neural posterior estimation. Physical Review Letters,127(24):241103, 2021. Natal SM de Santi, Francisco Villaescusa-Navarro, L Raul Abramo, Helen Shao, Lucia A Perez, TiagoCastro, Yueying Ni, Christopher C Lovell, Elena Hernandez-Martinez, Federico Marinacci, et al. Field-level simulation-based inference with Galaxy catalogs: the impact of systematic effects. arXiv preprintarXiv:2310.15234, 2023. Michael Deistler, Pedro J Goncalves, and Jakob H Macke. Truncated proposals for scalable and hassle-freesimulation-based inference. Advances in Neural Information Processing Systems, 35:2313523149, 2022a.",
  "Simulation-based Inference": "Consider a simulator that takes parameters Rd where d is the number of parameters and generates asimulated dataset x RD where D is the dimension of the data, but its density p(x|) is intractable. Theobjective of SBI is to accurately estimate the posterior density of conditional on the observed datasetxo RD based only on simulating data from the model and not requiring evaluation of the intractablelikelihood, p(xo|). Two popular SBI methods are ABC and NPE, which are summarized below.",
  "Approximate Bayesian computation": "Statistical SBI (Sisson et al., 2018), such as the ABC rejection algorithm, is based on Monte Carlo rejectionsampling. That is, it keeps only the parameter values simulated from the prior that generate simulated datax such that (x, xo) < , where (x, xo) is a user-defined discrepancy function between the simulated andobserved data, and is a user-defined threshold often referred to as the ABC tolerance. SMC ABC algorithms (e.g. Sisson et al. (2007); Drovandi & Pettitt (2011)) aim to be more efficient bysampling a sequence of ABC posteriors with decreasing s, updating the importance distribution at eachiteration.More specifically, SMC ABC algorithms define a sequence of non-increasing ABC thresholds1 2 T , such that",
  "Here, T = represents the target ABC posterior": "In many real-world applications, x, xo RD are considered high-dimensional data, necessitating a mappingto a lower-dimensional space for computational efficiency. This is typically done using summary statisticsS(). If summary statistics are required, we use S(x) and S(xo) instead of the full datasets x and xo. Thechoice of appropriate summary statistics is a subject of ongoing research and is discussed extensively in theliterature (see Sisson et al. (2018)). However, even sophisticated ABC algorithms can require a significant number of model simulations to achievea suitably small value of (Biau et al., 2015; Csillry et al., 2010; Beaumont et al., 2009; Blum, 2010).",
  "Neural posterior estimation": "NPE uses N training pairs of simulator parameter values and simulated datasets, {i, xi}Ni=1, to estimate theposterior distribution p(|x) (Papamakarios & Murray, 2016; Lueckmann et al., 2017; Greenberg et al., 2019).Once the NPE is trained on the simulated datasets, the posterior distribution p(|xo) can be computed byinputting the observed dataset xo. A conditional neural density estimator qF (x,)(), utilizing a neural network F and its adjustable networkweights , is often used as an NPE. In order to train qF (x,)(), the following loss is minimized:",
  "over network weights . For a sufficiently expressive qF , qF (x,)() p(|x), as N": "SNPE aims to improve the accuracy of the approximate posterior for a particular observed dataset xoiteratively by sampling parameter values from a previous NPE approximation for a given number of rounds.The current NPE approximation is treated as a proposal distribution p() for the next round. However,training qF using parameter values drawn from p() will not converge to the true posterior distribution, butrather to",
  "yt = Xyt1 + t,(4)": "where yt Rk represents the k-dimensional observation of the time series at time t, X Rkk is thetransition matrix, and t N(0, 2I) is a k-dimensional noise vector with 2 being the noise parameter.The model considers a sparse transition matrix X where the only off-diagonal entries that are non-zeromust satisfy the following conditions: if variable i is coupled with variable j, then Xi,j = 0 and Xj,i = 0(note that Xi,j is not necessarily equal to Xj,i) and each variable is coupled to only one other variable.Under this condition, each column will only have one element that is non-zero, and this will also be anoff-diagonal element of the matrix. To ensure the stability of the SVAR, the diagonal elements of X areset to -0.1. The parameter space of SVAR can easily scale to higher dimensions by increasing k. In thisstudy, the model parameters Rk+1 are the non-zero off-diagonal entries of X and its variance and weconsider k = 6, which leads to 7 parameters. This choice is based on the assumption that if SNPE doesnot produce highly accurate approximations in this low-dimensional case, it is unlikely to be accurate ina higher-dimensional parameter space.We generate an observed dataset of length T = 1000 using thetrue parameter value = (0.579, 0.143, 0.836, 0.745, 0.660, 0.254, 0.1). We use summary statistics toreduce the dimension of the data. Following Thomas et al. (2020); Drovandi et al. (2023), we use the lag1 autocovariance1TTt=2 yityjt1 as the summary statistics, where yit is the tth observation of the ith timeseries. We use the sample standard deviation of the k time series to inform . Thus there is a single summarystatistic that is intended to be informative about each parameter. We employ a uniform distribution as the prior, constrained between -1 and 1 for the k parameters andbetween 0 and 1 for . We denote the generated dataset as x RT (k+1) and its corresponding summary asS(x) R(k+1). We find that extreme values of the summary statistics can be produced by parameter valuesaway from the true parameter value. To stabilize the training, we clip simulated datasets with summarystatistic outliers (any simulated values greater than 10, around 3% of training datasets). This leads to someregions of the parameter space being unexplored and results in leakage after several rounds of SNPE training.Note that some experimentation was required to obtain a clipping value that led to reasonable results forSNPE. For illustrative purposes, we run three rounds of SNPE to avoid the leakage problem that occur severalrounds after and compare the results with exact posterior samples as the likelihood is easily computablein this example since it is Gaussian. Thus in this case the reference distribution is estimated from exactposterior samples. Ideally, we would expect performance to improve when increasing the number of SNPE",
  "rounds. However, as shown in Figures 1 and 2, even with datasets clipped for every round, SNPE does notimprove the accuracy of the estimates as the number of rounds increases": ": Comparison of marginal posterior distributions between reference distribution (orange), NPE(dashed pink) and SNPE (red), with grey solid lines representing the true values. The SNPE results arebased on three rounds. : Comparison of posterior predictive distribution of the summary statistics of observation datasetsbetween reference distribution (orange), NPE (dashed pink) and SNPE (red), with grey solid lines repre-senting the true values. The SNPE results are based on three rounds.",
  "Method": "We find that NPE can perform sub-optimally when the prior predictive distribution of the data is complexand has significant variability. When this occurs, the NCDE may not be sufficiently accurate, especially inregions of high posterior support. The sequential version of NPE was originally designed to overcome thisissue, however if the initial NPE approximation is not substantially better than the prior, then subsequentrounds of SNPE may suffer from the same issue. Other approaches to overcome this issue may be to increase the training sample size or to try differentconfigurations of the neural network, but both of these may increase the computational cost substantiallyand may not address the issue.Instead, we propose the preconditioned NPE (PNPE) method, and itssequential extension below, in order to make NPE methods more reliable.",
  "RD p(x|)I((xo, x) < )dx,(5)": "where I() is the indicator function and can be chosen considerably larger than what might typically beused in an ABC algorithm. Then we fit a density estimator to those parameter samples. The key ideais to use an efficient ABC algorithm to quickly discard poor regions of the parameter space that generateunusual datasets relative to the observed data, which provides better quality training datasets for NPE. Wenote that any ABC algorithm could be employed here, but we use the SMC ABC algorithm of Drovandi &Pettitt (2011) in this paper (see Appendix A for a full description of this method). The SMC ABC algorithmgenerates n samples from a sequence of ABC posteriors based on decreasing ABC thresholds, 1 > > T ,where T = is the target ABC threshold. The sequence of tolerances is determined adaptively, by, ateach iteration of SMC, discarding a proportion of the samples, a n, with the highest discrepancy, wherea is a tuning parameter. Then, the population of samples is rejuvenated through a resampling and movestep. During the move step, a Markov chain Monte Carlo (MCMC) ABC kernel is employed to maintainthe distribution of particles based on the current value of the tolerance. The number of MCMC steps Rtto apply to each particle is determined adaptively based on the overall MCMC acceptance rate, that isRt =log(c) log(1pacct), where pacctis the estimated MCMC acceptance probability at the SMC iteration t and cis a tuning parameter of the algorithm that can be interpreted as the probability that a particle is not movedin the Rt MCMC iterations. A natural stopping rule for the algorithm is when the MCMC acceptance ratebecomes intolerably small. Based on n parameter samples from the ABC posterior, we fit an unconditional normalizing flow qG (notethat other density estimators could be used). Then we can use qG as the initial importance distribution forthe (S)NPE process. We call this melding of ABC and (S)NPE as the preconditioned (S)NPE method. Themethod is summarized in Algorithm 1.",
  "NPE converges to the true posterior p(|xo) as N , with an appropriate importance re-weight ifp() = p()": "Choosing a suitable value of for our method requires some thought. A smaller value of will focus in on morepromising regions of the parameter space, but will increase the computational time of the preconditioningstep. A larger value of will lead to a fast preconditioning step, but may not eliminate enough of the poorparts of the space to improve the training of the NCDE. In this paper we use an MCMC acceptance rate of10% (unless otherwise specified) as the stopping criteria for the SMC ABC algorithm in the preconditioningstep. For our examples we find that this choice is effective at balancing the aforementioned objectives. Wenote that other choices are possible. Furthermore, once these poor simulations have been removed, we find NPE to be more effective than ABC,since ABC requires an exponentially increasing number of simulations to drive to 0. To avoid the scalingproblem of ABC, the preconditioning step only takes a short run of ABC, and thus we are not interested indriving to 0.",
  "Computational cost": "We now consider computational cost for P(S)NPE and compare it with SNPE. The preconditioning step canbe considered as the initial round of NPE where the total number of simulated datasets generated duringSMC ABC is denoted as nABC. Hence, it is worth noting that P(S)NPE, like SNPE, is not amortized sinceit requires running an ABC algorithm for each observation datasets x0. Furthermore, for complex real-world problems, the simulation time may depend on the parameter values,and parameter values with very low posterior support can produce substantially longer simulation times.For such problems, it is important from a computational perspective to quickly eliminate such regions fromthe parameter space, as is the motivation of the preconditioning ABC algorithm. Thus, for problems whereSNPE does not perform well, we find PSNPE to be substantially more computationally efficient in terms ofcompute time. We perform an analysis of the trade-off between computational cost and estimation accuracy by using differ-ent choices for the % of stopping rule for the preconditioning step in the SVAR example. For reproducibility,we use 20 different random seed values for each choice of the stopping rule. We compute and record theaverage values we choose for the % and its corresponding total number of simulations in .",
  "MCMC acceptance rate15%12%10%8%5%Simulations in ABC156033358562402100398244667": ": Computational cost for preconditioning step:The first row of the table refers to the %stopping rule we selected to obtain samples from ABC, while the second row indicates the average of totalnumber of simulations ABC used for the corresponding %. To investigate the performance of this trade-off, we use maximum mean discrepancy (MMD) as a metric tocompare the approximate distribution from ABC with the reference distribution. We also run NPE withthe same number of simulations as ABC and compute its MMD. In , we plot the total numberof simulations versus MMD for ABC, NPE and PNPE. The solid lines represent the average MMD valuesbased on 20 MMD values from 20 different seed values. The number of simulations for PNPE includes anadditional 10k simulations generated after the corresponding preconditioning step. It is surprising that withan increasing number of simulations, the accuracy of NPE can vary significantly. For some random seedvalues, the accuracy of NPE even decreases when using more than approximately 70,000 model simulations.Compared with ABC and PNPE, which have narrow uncertainty intervals, the accuracy of NPE seems tohighly depend on the random seed value. We record that 35% of training datasets are clipped before training NPE, which means around 2,100-3,500simulated datasets were ignored, and the corresponding parameter space is not explored well. This mightindicate the reason why the accuracy of NPE might reduce under a large number of simulations for some",
  "Illustrative Example Revisited": "We apply PNPE to the illustrative example shown in .2.1. For the preconditioning step, we usethe adaptive SMC ABC algorithm proposed by Drovandi & Pettitt (2011), with tuning parameters n = 1k,a = 0.5, and c = 0.01. We employ an unconditional normalizing flow as the unconditional density estimator.For this, we use the state-of-the-art neural spline flow implemented in the Pyro package. In order to make a fair comparison between P(S)NPE and SNPE, we use the same number of simulationsas in the SMC ABC algorithm, denoted nABC, to train the initial round of NPE. We run the SMC ABCalgorithm ten times to obtain the average number of simulations it requires, which is nABC = 54k. Forillustrative purposes, we only run two rounds of SNPE and compare it with PNPE. The estimated marginal posterior plots are displayed in , where the black solid lines represent thetrue parameter values. It is evident that our PNPE method (in a single round) produces a substantiallysharper approximation of the posterior compared to that of SNPE.",
  "Further Experiments": "We present five benchmarking tasks (two from Lueckmann et al. (2021) and three from the SBI literature)and two additional examples, including our motivating example, where SNPE, perhaps surprisingly, doesnot produce highly accurate posterior distributions. To fairly compare our method with vanilla SNPE andpotentially TSNPE (if the leakage issue is encountered), we run the ABC algorithm 10 times and computethe average total number of simulations that the ABC algorithm requires. We then use this same number of",
  "Benchmarking example": "We compare PSNPE and SNPE across five popular benchmarking simulators (see Appendix B.1 for a detaileddescription). We use the sbibm package (Lueckmann et al., 2021) for the two-moon and SLCP simulators,and the ELFI package (Lintusaari et al., 2018) for the other benchmark simulators. Since the ground-truthposteriors are available, we can use specific performance metrics for comparison. We use a classifier two-sample test (C2ST), where a score of 0.5 indicates that the approximate posterior is indistinguishable fromthe true posterior, and a score of 1 signifies that the classifier can completely separate the approximateposterior from the true posterior. Additionally, we use the maximum mean discrepancy (MMD) betweenground truth posterior and approximate posterior to quantify performance.We refrain from using thenegative log probability of true parameters as a performance metric because our method is not amortized.It is evident from that SNPE and PSNPE achieve similar results, except for the g-and-k example,the results of which we describe in more detail next.",
  "ModelC2STMMDTwo moon0.527\\0.5280.00067\\0.0007SLCP0.664\\0.6910.00065\\0.001MA(2)0.887 \\0.8560.104 \\0.0966g-and-k0.975 \\0.7270.553 \\0.311Toad example0.899 \\0.8870.120 \\0.111": ": Performance on popular benchmarking simulators. Classification accuracy (CS2T) andmaximum mean discrepancy (MMD) computed for SNPE\\PSNPE with same number of simulation for ABCand NPE followed by 2 rounds SNPE, each round with N = 10k simulations. The bold indicates a betterperformance metric value. For the benchmark examples in where the performance of SNPE and PSNPE are similar, theinitial training dataset is not extreme and the NCDE fits reasonably well. In more realistic examples, theprior predictive distribution of the summaries may be much more complex and contain extreme values. Insimple examples, we can create more complex prior predictive distributions by widening the prior. Here wereconsider the g-and-k benchmark example but widen the prior distribution. We describe the details of the g-and-k model and its prior distribution in Appendix B.1. We use full datasetsinstead summary statistics for both PNPE and SNPE. For visualization purposes, we plot the marginalapproximate posterior distribution for each method of the estimated results. It is evident that ABC performsbetter than NPE under the same number of simulations (a), where the ABC posterior is able toconcentrate on the true value of B and k. As shown in b, PNPE performs better than SNPE.",
  "(b)": ": Performance on G-and-K example: (a) Comparison of marginal posterior distributions be-tween the ABC preconditioning step (red) and NPE (blue), with black dashed lines representing the truevalues; (b) Comparison of marginal posterior distributions between PNPE (green) and SNPE (red), withblack dashed lines representing the true values. The result of SNPE uses 2 rounds with same number ofsimulation as the ABC preconditioning step.",
  "High-dimensional SVAR model": "To investigate how our method scales to higher dimensional problems, we take the illustrative SVAR examplefrom before and consider k = 20, which leads to 21 parameters. We detail the experimental settings in SectionB.2. To ensure a fair comparison, we run the SMC ABC algorithm ten times using a 10% acceptance rateas the stopping criterion and calculate the average number of simulations it takes. We then use the samenumber of simulations, approximately nABC 45k, to train the initial NPE. Hence, the total number ofsimulations for both PNPE and SNPE is the same (55k in total). To stabilize the training, we apply thesame clipping technique used in the previous low-dimensional case, which results in approximately 11% ofthe training samples being removed in the initial round of NPE and about 1% to 2% in the second rounds.Starting from the third round, SNPE is unable to sample any parameter values from the neural networksdue to a severe leakage issue (Deistler et al., 2022a). The estimated marginal posterior plots are displayed in , where the black solid lines represent thetrue parameter values. It is evident that as the number of parameter dimensions increases, training theunconditional normalizing flows becomes more challenging. With well-trained unconditional normalizingflows, PNPE outperforms SNPE in high-dimensional cases (in this example, except for parameter 3.).",
  "Biphasic Voronoi cell-based model": "Finally, we consider a challenging real-world problem in cancer biology: calibrating the biphasic Voronoicell-based model (BVCBM) (Wang et al., 2024) that models tumor growth. The model uses a parameter to divide the tumor into two growth phases. Here, the term growth phase refers to the different growthpatterns of the tumor. There are four parameters that govern tumor growth during each phase, namely(p0, ppsc, dmax, gage), where p0 and ppsc are the probability of cell proliferation and invasion, respectively,dmax is the maximum distance between cell and nutrient, and gage is the time taken for a cell to be ableto divide. Thus there are nine parameters in total, four parameters each of two phases, and the parameter at which the growth phase changes.In this paper, we calibrate to two real-world pancreatic cancerdatasets Wade (2019), which describe tumor growth as time series data. The datasets span 26 and 32 days,respectively, with measurements taken each day. While the ground truth posteriors are unknown for thosedatasets, we compute posterior predictive distributions to assess if SNPE and PNPE can effectively calibratethe model to the data. We employ vague prior distributions for all parameters. Specifically, we use a Uniform distribution con-strained between 1 and 24 hours the number of days for gage during both growth phases. Additionally,we use a Uniform distribution constrained between 2 and the number of days minus 1 for . The priordistributions for the remaining parameters are detailed in Appendix B.3.",
  "Discussion": "We present a neural SBI method that is both simple and easy to deploy, designed to enhance the accuracy ofSNPE methods. Our method, termed preconditioned neural posterior estimation (PNPE) and its sequentialversion, PSNPE, employs an ABC algorithm for the initial step. This algorithm is used to efficiently filterout poor regions of the parameter space.Additionally, we use the ABC posterior samples to train anunconditional density estimator qG, enabling qG to serve as the initial proposal distribution for SNPE. Thecore concept is that an improved starting point can significantly enhance the accuracy of SNPE estimations.Indeed, we obtained very good results with PNPE. Lueckmann et al. (2017); Deistler et al. (2022b) haveproposed similar ideas to improve accuracy of SNPE. They train a classifier to predict samples that fall intorejection criterion based on a certain distance metric. However, such a classifier can falsely reject samples.Our method can avoid this problem and hence is more principled compared to their method. We showcase several examples where either SNPE failed to perform inference effectively, such as in theSVAR case, or produced biased results, as observed in the BVCBM. For the SVAR example, SNPE methodsstruggle due to the impact of low-quality samples from certain parameter space regions, adversely affectingthe training process. The ABC method can efficiently eliminate these bad samples, thereby enhancing thetraining.For cases where SNPE results in biased estimations, our methods were effective at accuratelyfitting observed data (real data for BVCBM example). This is substantiated by our empirical results for theposterior predictive distribution discussed in the experimental section. Although our method demonstrates the capability to enhance estimation accuracy, it does have some limi-tations. Firstly, our method requires model simulations in the ABC preconditioning step, which may leadto greater computational demands in situations where SNPE methods perform well. However, by perform-ing the preconditioning step, significantly fewer model simulations may be required in the SNPE part toachieve high accuracy. In this paper we used SMC ABC for the preconditioning step, but we note thatother ABC algorithms or SBI methods could be used. We do not recycle the simulations performed in theABC preconditioning step for the SNPE phase, but it could be possible to modify our method to exploit",
  "Acknowledgement": "We thank the computational resources provided by QUTs High Performance Computing and ResearchSupport Group (HPC). Xiaoyu Wang, Ryan P. Kelly and Christopher Drovandi were supported by anAustralian Research Council Future Fellowship (FT210100260). Joseph Aylett-Bullock, Carolina Cuesta-Lazaro, Arnau Quera-Bofarull, Miguel Icaza-Lizaola, AidanSedgewick, Henry Truong, Aoife Curran, Edward Elliott, Tristan Caulfield, Kevin Fong, et al.June:open-source individual-based epidemiology simulation. Royal Society Open Science, 8(7):210506, 2021.",
  "Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Advances inNeural Information Processing Systems, 32, 2019": "Paul Fearnhead and Dennis Prangle. Constructing summary statistics for approximate Bayesian computa-tion: semi-automatic approximate Bayesian computation. Journal of the Royal Statistical Society SeriesB: Statistical Methodology, 74(3):419474, 2012. Alexander Fengler, Lakshmi N Govindarajan, Tony Chen, and Michael J Frank. Likelihood approximationnetworks (LANs) for fast inference of simulation models in cognitive neuroscience. Elife, 10:e65074, 2021. David T Frazier, Christian P Robert, and Judith Rousseau. Model misspecification in approximate Bayesiancomputation: consequences and diagnostics. Journal of the Royal Statistical Society Series B: StatisticalMethodology, 82(2):421444, 2020.",
  "Bai Jiang, Tung-yu Wu, Charles Zheng, and Wing H Wong. Learning summary statistic for approximateBayesian computation via deep neural network. Statistica Sinica, pp. 15951618, 2017": "Ryan P Kelly,David J Nott,David Tyler Frazier,David J Warne,and Christopher Drovandi.Misspecification-robust sequential neural likelihood for simulation-based inference. Transactions on Ma-chine Learning Research, 2024. Pyung-Hwan Kim, Joo-Hyuk Sohn, Joung-Woo Choi, Yukyung Jung, Sung Wan Kim, Seungjoo Haam, andChae-Ok Yun. Active targeting and safety profile of PEG-modified adenovirus conjugated with herceptin.Biomaterials, 32(9):23142326, 2011. Jarno Lintusaari, Michael U Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Fundamentalsand recent developments in approximate Bayesian computation. Systematic Biology, 66(1):e66e82, 2017. Jarno Lintusaari, Henri Vuollekoski, Antti Kangasrsi, Kusti Skytn, Marko Jrvenp, Pekka Marttinen,Michael U Gutmann, Aki Vehtari, Jukka Corander, and Samuel Kaski. ELFI: Engine for likelihood-freeinference. Journal of Machine Learning Research, 19(16):17, 2018. Jan-Matthis Lueckmann, Pedro J Goncalves, Giacomo Bassetto, Kaan cal, Marcel Nonnenmacher, andJakob H Macke. Flexible statistical inference for mechanistic models of neural dynamics. Advances inNeural Information Processing Systems, 30, 2017. Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarkingsimulation-based inference. In International Conference on Artificial Intelligence and Statistics, pp. 343351. PMLR, 2021. Philippe Marchand, Morgan Boenke, and David M Green. A stochastic movement model reproduces patternsof site fidelity and long-distance dispersal in a population of fowlers toads (anaxyrus fowleri). EcologicalModelling, 360:6369, 2017.",
  "Scott A Sisson, Yanan Fan, and Mark Beaumont. Handbook of approximate Bayesian computation. CRCPress, 2018": "Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pedro JGonalves, David S Greenberg, and Jakob H Macke. SBI: A toolkit for simulation-based inference. Journalof Open Source Software, 5(52):2505, 2020. Owen Thomas, Henri Pesonen, Raquel S-Leo, Hermnia de Lencastre, Samuel Kaski, and Jukka Corander.Split-BOLFI for for misspecification-robust likelihood free inference in high dimensions. arXiv preprintarXiv:2002.09377, 2020.",
  "Samantha Jane Wade. Fabrication and preclinical assessment of drug eluting wet spun fibres for pancreaticcancer treatment. 2019": "Xiaoyu Wang, Adrianne L Jenner, Robert Salomone, David J Warne, and Christopher Drovandi.Cali-bration of agent based models for monophasic and biphasic tumour growth using approximate Bayesiancomputation. Journal of Mathematical Biology, 88(3):28, 2024. Daniel Ward, Patrick Cannon, Mark Beaumont, Matteo Fasiolo, and Sebastian Schmon. Robust neuralposterior estimation and statistical model criticism. Advances in Neural Information Processing Systems,35:3384533859, 2022.",
  "David J Warne, Ruth E Baker, and Matthew J Simpson. Rapid Bayesian inference for expensive stochasticmodels. Journal of Computational and Graphical Statistics, 31(2):512528, 2022": "Antoine Wehenkel, Jens Behrmann, Andrew C Miller, Guillermo Sapiro, Ozan Sener, Marco Cuturi,and Jrn-Henrik Jacobsen.Simulation-based inference for cardiovascular models.arXiv preprintarXiv:2307.13918, 2023. Timothy O West, Luc Berthouze, Simon F Farmer, Hayriye Cagnan, and Vladimir Litvak. Inference of brainnetworks with approximate Bayesian computationassessing face validity with an example application inParkinsonism. Neuroimage, 236:118020, 2021.",
  "AFurther background for SMC ABC": "We provide a detailed description of the adaptive SMC ABC algorithm we used in this paper and providepseudocode in Algorithm 2 for reference. This algorithm starts by drawing N independent samples fromthe prior distribution p(), represented as {i}Ni=1. For each sample i (known as a particle), the algorithmsimulates a dataset xi from the stochastic model and calculates the corresponding discrepancy i = (xi, xo),resulting in the pair set {i, i}Ni=1. These pairs are then arranged in order of increasing discrepancy suchthat 1 < 2 < < N. The first tolerance threshold, 1, is set as the largest discrepancy, N. To movethrough the target distributions, the algorithm adjusts the tolerance dynamically. The next tolerance, t,is set as NNa, where Na = Na, and a is a tuning parameter. Essentially, in each step, the algorithmdiscards the top a 100% of particles with the highest discrepancies. After discarding these particles, onlyN Na particles remain. To replenish the set back to N particles, the algorithm resamples Na times fromthe alive particles, copying both the parameter and discrepancy values. This process, however, leads toduplicates in the particle set. To add variety to the set, the algorithm applies an MCMC ABC kernel to eachresampled particle. The parameters for the MCMC proposal distribution qt(|) are derived from the currentparticle set. For instance, if using a multivariate normal random walk proposal, its covariance t is basedon the particle sets sample covariance. The acceptance of a proposed parameter (assuming a symmetricproposal) and simulated data is determined by the equation:",
  "p()I((xo, x) < t),(6)": "where q(|) and x p(|) are proposed parameter values and dataset, respectively. However, proposalsmay be rejected, leaving some particles unchanged. To address this, the algorithm performs Rt iterations ofthe MCMC kernel on each particle, where Rt =log(c) log(1pacct), where c is a tuning parameter of the algorithmthat can be interpreted as the probability that a particle is not moved in the Rt iterations. The acceptanceprobability pacctis estimated from trial MCMC ABC iterations and used to compute Rt for the next set ofMCMC ABC iterations. For this adaptive SMC ABC algorithm, two stopping rules can be used. The firststopping rule halts the ABC algorithm when the maximum discrepancy is below a set tolerance, T . Thesecond stopping rule terminates the algorithm when the MCMC acceptance probability pacctfalls below apredefined threshold pacc. Here we choose the second rule, and since we only require a short run of ABC,we set pacc to be higher than what is typically used in an ABC analysis.",
  "Algorithm 2 Adaptive SMC ABC": "Input: The observed data xo, the stochastic model p(x|), distance function (, ), prior distributionp(), number of particles N, tuning parameters a and c for adaptive selection of discrepancy thresholdsand selecting the number of MCMC iterations in the move steps, target tolerance T , initial number oftrial MCMC iterations Sinit, minimum acceptable MCMC acceptance rate pminfor i = 1, . . . , N do",
  "BExperimental Details": "We use the adaptive SMC ABC algorithm Drovandi & Pettitt (2011) in the preconditioning step for allexperiments. We set the tuning parameters as a = 0.5, c = 0.01 and use 1k particles for the algorithm.As the stopping rule, we set the target MCMC acceptance rate at 10%, unless otherwise specified. Forthe unconditional density estimator, we employ unconditional normalizing flows using the Pyro packageBingham et al. (2019), with a spline coupling layer using the transformation:",
  "Y(d+1):D = h(X(d+1):D; X1:d)(8)": "where X are the inputs, Y are the outputs, e.g., X1:d represents the first d elements of the inputs, gis either the identity function or an elementwise rational monotonic spline with parameters , and h,where is element-wise bijection parameter, is a conditional elementwise spline, conditioning on the first delements. Regarding the neural networks, we use four fully-connected layers and set the count bins to 16.Furthermore, if the dimensions of the parameter space are less than three, indicating a low-dimensional case,we also consider kernel density estimation with a Gaussian kernel as the unconditional density estimator,as implemented in the Scikit-learn package Pedregosa et al. (2011). For APT and TSNPE, we use theimplementation of the sbi package Tejero-Cantero et al. (2020) with default settings. For SNPE and TSNPE, we use conditional neural spline flows Durkan et al. (2019). We use five couplinglayers, with each coupling layer using a multilayer perceptron of two layers with 50 hidden units. The flow istrained using the Adam optimizer with a learning rate of 5 104 and a batch size of 256. Flow training isstopped when either the validation loss, calculated on 10% of the samples, has not improved over 50 epochsor when the limit of 500 epochs is reached.",
  "B.1Benchmarking examples": "A number of popular benchmarking models in the SBI literature exist where the ground truth posteriors areavailable. For the two-moon model and the SLCP model, we follow the model specifications in Lueckmannet al. (2021). For the rest of the models, we refer to Lintusaari et al. (2018). After the preconditioning step,we generate 10k simulations from the unconditional normalizing flows and pair them with their simulateddata to process SNPE.",
  "B.1.1Two moons model": "The two moons model exhibits both global (bimodality) and local (crescent shape) structures in the posterior.For the preconditioning step, the total number of model simulations is around 30k. Since is low-dimensionalin this model, we use KDE as the unconditional density estimator and achieve results similar to those obtainedwith unconditional normalizing flows.",
  "B.1.3MA(2) model": "The moving average model of order 2 (MA(2)) is a univariate time series model often used as a toy examplein the ABC literature. We assign the true parameter values as = (0.6, 0.2) and simulate x0 R100. Inthis model, xo and x are high-dimensional, so we use the first two lags of the autocovariance function andthe variance (lag 0) as the summary statistics. The preconditioning step required 18k simulations, as thetarget MCMC acceptance rate is set at 12%. Given that the dimension of the parameter space is relativelylow, we employ KDE as an additional unconditional density estimator, utilizing a Gaussian kernel with abandwidth selected by Silverman method. This method achieves results similar to those obtained using anunconditional normalizing flow.",
  "B.1.4Univariate g-and-k model": "The univariate g-and-k model is a popular benchmark in the statistical SBI literature. The four param-eters = (A, B, g, k) control the location, scale, skewness, and kurtosis, respectively. We assign the trueparameter values as = (3, 1, 2, 0.5) and use them to simulate xo R50 independent observations. Forthe preconditioning step, we take the set of octiles as summary statistics as they are a robust measure ofskewness and kurtosis (Drovandi & Pettitt, 2011). Setting the target MCMC acceptance rate to 20% resultsin 18k model simulations for the preconditioning step. Here we used a larger acceptance rate to ensure thatthe preconditioning step does not use too many model simulations.",
  "B.1.5Toad model": "The toad movement model proposed in (Marchand et al., 2017) is an individual-based model for simulatingthe dispersal of Fowlers toads (Anaxyrus fowleri). It has been considered as a test example several times inthe SBI literature (e.g. Frazier et al. (2022a;b); Drovandi & Frazier (2022)). This model captures two known behaviours of amphibians: the tendency to return to previously visited loca-tions and a small chance of long-distance movement. Dispersal distance is modeled with a Lvy alpha-stabledistribution characterized by a stability factor and scale factor . The Lvy alpha-stable is symmetricallycentred around zero but has heavy tails, allowing the simulation of both frequent short-distance and rarelong-distance movements. Toads are modeled to remain at their refuge site during the day and move toforage independently at night. After foraging, the toad may either remain at their current location or returnto a previously visited refuge site. We follow model 1 in Marchand et al. (2017), where each former refugesite has equal probability of being returned to. The probability of a toad returning to a former refuge siteis a constant probability, denoted as p0. Hence, there are three parameters to infer, that is = (, , p0). The simulated data generates an observation matrix (representing Euclidean distance in metres from theorigin) of 66 toads across 63 days. We assign the true parameter values as = (1.7, 35.0, 0.6) and use themto simulate xo R6366. Since the data can be considered as high-dimensional, to summarise the data wefirst construct four sets of displacement vectors with time lags of 1, 2, 4 and 8 days. For each set, we takethe log difference between the 0, 0.1, . . . , 1 quantiles, the number of absolute displacements less than 10m,and the median of the absolute displacements greater than 10m, resulting in a total of 48 summary statistics(12 for each time lag).",
  "B.3BVCBM": "The BVCBM simulation begins by initializing a square domain with cells arranged in a hexagonal lattice.The cell at the center of the domain is identified as a cancer cell, while the others are designated as healthycells. The simulation proceeds until the tumor reaches a volume of 100 mm2, in accordance with experimentalmeasurements (Wade, 2019; Kim et al., 2011). When this volume is attained, the distribution of healthyand cancerous cells within the lattice is recorded. This configuration then serves as the starting point to",
  ",(9)": "where pd is the probability of cell division, p0 is the initial division rate, d is the current cell density, anddmax is the maximum density. For cancer cells that do not proliferate, the model assesses their potential totransition into invasive cells, governed by the probability ppsc. Subsequently, the positions of all cells, bothhealthy and cancerous, are updated using Hookes law:",
  "ri,j(t)ri,j(t)(si,j(t) ri,j(t)).(10)": "Here, ri(t + t) denotes the updated position of cell i, is the cell motility coefficient, Fi(t) is the force oncell i, is a mechanical interaction coefficient, ri,j(t) is the vector between cells i and j, and si,j(t) is thenatural length of the spring connecting the two cells. The parameters for the mechanical interactions, suchas and , are sourced from prior studies in the literature Meineke et al. (2001). See Jenner et al. (2020);Wang et al. (2024) for more detailed model simulation. Four parameters = (p0, ppsc, dmax, gage) control the tumor growth during a single phase, which is a periodwhen the tumor grows based on fixed values for these four parameters. For the biphasic model, an additionalparameter is introduced, representing the time at which the tumor growth pattern changes, that is, thevalues for change. Therefore, for BVCBM, we need to estimate nine parameters for two pancreatic cancerdatasets, denoted as 1 = (p10, p1psc, d1max, g1age), 2 = (p20, p2psc, d2max, g2age), and , so that = (1, 2, ). The parameter ppsc, which is the probability of tumor cell invasion into healthy cells, significantly affectsthe simulation time. The value of ppsc should be around 105, indicating that an increase in probabilitywill require more cells to be simulated. Moreover, a smaller value of ppsc results in simulation time. ForPNPE, the total simulation time for 15k simulations (26-day dataset) and 17k simulations (32-day dataset)for the preconditioning step is approximately 13 and 15 minutes, respectively, whereas SNPE requires around1 hour for the first round (i.e. based on samples from the prior) of 10k simulations. This is because thepreconditioning step is effective at quickly eliminating values of ppsc that lead to longer model simulationtimes.",
  "C.1SVAR": "We generate 10 different datasets for the SVAR model in 6 dimensions based on predefined parameter valuesshown in . Then, we run SNPE and PNPE with 10 different random seed values for each datasetto investigate reproducibility.For preconditioning, we use a 10% stopping rule and train unconditionalnormalizing flows based on ABC posterior samples for different random seed values for each dataset. Toquantify the results, we use MMD as the metric. For each dataset, we compute the MMD values betweenthe approximate distribution and the reference distribution, allowing us to use one value to summarize thefit. By computing the MMD for the approximate posterior based on each set of random seed values, weproduce a boxplot corresponding to each method. Dataset12345610.9749-0.25640.42300.87490.6453-0.177220.2777-0.29320.5483-0.46790.1549-0.636530.3193-0.25090.78010.5103-0.43580.03174-0.9295-0.32670.2930-0.9326-0.6113-0.501550.51460.2805-0.66950.7410-0.4518-0.436660.6401-0.60370.3730-0.6739-0.7237-0.54947-0.9378-0.6438-0.8434-0.9219-0.40320.55778-0.26630.90980.73830.1438-0.09680.799290.9185-0.14540.5147-0.29210.05950.722710-0.47660.71520.9845-0.7105-0.2426-0.4923 : True values to generate observational datasets: Each row refers to the true parameters thatwe used to generate the observation datasets xo with the additional parameter set to a constant value of0.1.",
  "C.2BVCBM": "In this section, we present prior predictive distributions of tumour volumes in (a) and (b) for two pancreaticdatasets (in (c) and (d) we show the same plots but on the log scale). We also show the posterior predictivedistributions on the log scale obtained with different methods in . Then we present the bivariateposterior density and marginal posterior density plots for the BVCBM as additional results. It is evidentfrom Figures 13 and 14 that PNPE provides more precise estimation than SNPE for both pancreatic cancerdatasets. : Prior predictive distribution for BVCBM. We sample 10k parameter values from priordistribution and plot the prior predictive distribution for two pancreatic datasets. In (a) and (b), the plotsare in regular scale, and in (c) and (d), the plots are in log scale."
}