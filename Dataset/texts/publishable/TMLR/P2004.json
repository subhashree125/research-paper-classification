{
  "Abstract": "Federated learning (FL) has become a popular tool for solving traditional ReinforcementLearning (RL) tasks, particularly when individual agents need to collaborate due to lowsample efficiency but are concerned about data privacy. The multi-agent structure addressesthe major concern of data-hungry in traditional RL, while the federated mechanism protectsthe data privacy of individual agents. Despite the advantage FL brings to RL, FederatedReinforcement Learning (FRL) is inherently susceptible to poisoning, as both FL and RLare vulnerable to such training-time attacks; however, the vulnerability of FRL has notbeen well-studied before. In this work, we propose a general framework to characterize FRLpoisoning as an optimization problem and design a poisoning protocol that can be appliedto policy-based FRL. Our framework is versatile, catering to FRL scenarios employing bothpolicy-gradient local RL and actor-critic local RL. In the context of actor-critic configurations,we conduct training for a pair of critics, one private and one public, aimed at maximizingthe potency of poisoning. We provably show that our method can strictly hurt the globalobjective. We verify the effectiveness of our poisoning approach through comprehensiveexperiments, supported by mainstream RL algorithms, across various RL OpenAI Gymenvironments covering a wide range of difficulty levels. Within these experiments, we assessour proposed attack by comparing it to various baselines, including standard, poisoned,and robust FRL methods. The results demonstrate the power of the proposed protocol ineffectively poisoning FRL systems It consistently diminishes performance across diverseenvironments, proving to be more effective than baseline methods. Our work providesnew insights into the training-time vulnerability of FL in RL and poses new challenges fordesigning secure FRL algorithms.",
  "Introduction": "Reinforcement Learning (RL) has gained popularity in recent years as a paradigm for solving complexsequential decision-making problems and has been applied to a wide range of real-world problems, includinggame playing (Silver et al., 2016; Vinyals et al., 2019), autonomous driving (Yurtsever et al., 2020), networksecurity (Xiao et al., 2018), design of materials (Govindarajan et al., 2024; Zhou et al., 2019; Ghugare et al.,2023), circuit design (Roy et al., 2021), and optimization (Chen et al., 2022). In RL, the agents goal isto learn an optimal policy that maximizes the long-term cumulative rewards, which is done by repeatedlyinteracting with a stochastic environment, taking actions, and receiving feedback in the form of rewards.However, despite the impressive performance of RL algorithms, they are notoriously known to be data-hungry,often suffering from poor sample efficiency (Dulac-Arnold et al., 2021; Schwarzer et al., 2020). One traditionalsolution to this challenge is Parallel RL Kretchmar (2002), which adopts multiple parallel RL agents that sample data from the environment and share it with a central server, as seen in practical implementationssuch as game-playing (Mnih et al., 2016; Berner et al., 2019). However, transferring raw data may not befeasible: on the one hand, it can cause significant communication costs in applications, such as the Internetof Things (IoT) (Wang et al., 2020); On the other hand, it is not suitable for privacy-sensitive industries,such as clinical decision support (Liu et al., 2020). Federated Reinforcement Learning (FRL) has been proposed in order to address the limitations of traditionalparallel RL, inspired by the recent success of Federated Learning (FL). FRL allows multiple agents to solve thesame RL task collaboratively without sharing their sensitive raw data, thus addressing the drawbacks of heavyoverhead and privacy violation in traditional Parallel RL. On the communication side, the communicationefficiency of FRL is improved by the ability of FL to perform multiple local model updates during eachcommunication round. On the privacy side, FRL enables data privacy protection by only communicatingmodel updates, but not raw data, to a central server. With advantages of addressing communication efficiencyand protecting privacy, FRL is practically appealing to a wide range of applications, including IoT (Wanget al., 2020), autonomous driving (Liang et al., 2023), robotics (Liu et al., 2019), etc. The success of FRLapplications has spurred theoretical and methodological advancements. For instance, Mnih et al. (2016);Min et al. (2023b;a) investigate FRL under asynchronous communication, while Xie & Song (2023b); Zhu &Gong (2023); Xie & Song (2023a); Mai et al. (2023); Fan et al. (2023) delve into FRL with heterogeneity inlocal environments. Khodadadian et al. (2022); Zheng et al. (2023) provides insights into linear speed-upconvergence rates, Woo et al. (2023) proposes an algorithm with sample complexity linearly scaling with thenumber of agents, and Zhao et al. (2023); Liu et al. (2023) analyze FRL with personalized local objectives. Poisoning in FRL is practical and harmful. The inherent nature of FL and RL amplifies the susceptibility topoisoning (training-time attacks) when combined into FRL. On the RL side, individual RL agents are designedto dynamically learn from raw feedback from the environment, making the learning system vulnerable todata corruption during training time (Zhang et al., 2020; Banihashem et al., 2022). As an example, a chatbotcould be misled by a small group of Twitter users, resulting in the generation of misogynistic and racistremarks (Neff, 2016). Similarly, recommendation systems can be manipulated by a minimal number of fakeclicks, reviews, or comments, leading to products being inaccurately ranked higher. Besides, RL in materialdesign (Govindarajan et al., 2024; Zhou et al., 2019; Ghugare et al., 2023) is susceptible to poisoned materialdata, and RL in circuit design (Roy et al., 2021) can be compromised by misleading prefix graphs and actions.On the FL side, the impact of poisoning is exacerbated in FL frameworks compared to single-agent RL. Thelack of transparency in local training within FL naturally exposes the FRL system to adversarial attacks bymalicious agents (Fang et al., 2020; Bagdasaryan et al., 2020; Bhagoji et al., 2019; So et al., 2020). Although practical and harmful, poisoning in FRL has yet to be explored. In this work, our goal is tosystematically study the vulnerability of FRL systems when facing adversarial attacks that attempt to misleadthe trained policy. In the remainder of this introduction, we discuss the challenges of directly applying prior RL poisoning methodsto FRL in .1, review related work on poisoning in .2, and summarize our contributions in.3.",
  "Challenges of applying prior RL poisoning to FRL": "Applying existing RL poisoning techniques to FRL presents several challenges. Many prior methods, suchas those by Zhang et al. (2020) and (Rakhsha et al., 2020), rely on unrealistic assumptions, including theattacker having complete knowledge of the MDP environment, which is often impractical. Additionally, someapproaches, such as TrojDRL (Panagiota et al., 2020), target RL agents actions rather than rewardingschemes, making them incompatible with our framework. Furthermore, the effectiveness of certain mechanisms,such as VA2C-P(Sun et al., 2020), diminishes in federated settings due to small local training steps, whichlead to frequent reinitialization of the adversarial critic and impaired learning of the poisoned actors valuefunction. Nevertheless, we have included the federated version of VA2C-P as a baseline, and our experimentalresults highlight the advantages of our approach compared to this federated extension of RL poisoning.",
  "Here, we provide a comprehensive literature comparison regarding poisoning in various contexts of FL, RL,Multi-Agent RL, Distributed RL, and FRL": "Poisoning in FL has been analyzed in different settings. Selected representative studies (Tolpegin et al., 2020;Bhagoji et al., 2019; Fang et al., 2020; Jodayree et al., 2023b;a) strategically poison FL by either compromisingthe integrity of the training dataset or manipulating submitted local model parameters. However, thesestudies are under the context of Supervised Learning, which is substantially different from RL in the senseof the availability of future data, the knowledge of the dynamics of the environment, etc (Sun et al., 2020).Thus, the existing works on FL poisoning are not directly applicable to FRL poisoning. In contrast, our workfocuses on the poisoning designed specifically for FRL, taking into account the unique characteristics of RL. Poisoning in RL, referring to committing attacks during the training process (Zhang et al., 2020), can becategorized into two types: weak attacks that only poison data (e.g., rewards and states) and strong attacksthat can manipulate actions in addition to data (Panagiota et al., 2020). In this study, we focus on weakattacks, also known as environmental poisoning, as they allow easier access for the attacker and, therefore,are more likely to occur in real-world scenarios. Rakhsha et al. (2020) formulated optimization frameworks tocharacterize RL poisoning, but they have limitations, such as requiring knowledge of the underlying MDP andfocusing on targeted poisoning. Our proposed framework, however, considers both targeted and untargetedpoisoning under the realistic assumption that the attacker does not have access to the MDP dynamics. Sunet al. (2020) designs a vulnerability-aware poisoning method for RL. Their algorithm focuses on manipulatingthe actor model, which cannot be directly applied to FRL as the critic model is also communicated amongagents and the server. In contrast, our proposed poisoning mechanism is specifically designed for FRL andfocuses on manipulating the critic model by training a pair of public and private critics. Poisoning in Multi-Agent Reinforcement Learning (MARL) is typically different from poisoning in FRL.Existing literature studying the robustness or poisoning of MARL more or less violates the security code in FL.In MARL, multiple agents actions jointly determine the next state of the environment and the correspondingrewards for each agent, thus exposing data and environment between agents (Wu et al., 2023; Liu & Lai, 2023;Mohammadi et al., 2023; Li et al., 2023). On the contrary, our work strictly adheres to the privacy-orientedsettings in FRL, letting the agents independently engage in exploration and data collection solely withintheir local environments, thus ensuring that no data or environment is exposed to other agents. Poisoning in Distributed RL, similar to poisoning in MARL, can violate the privacy code within the FRLcontext. Previous studies on poisoning or robustness in Distributed RL can expose data to the central server.For instance, Chen et al. (2023) illustrates that local agents are tasked with exploring the environment andcollecting data for the central server without conducting any local training. This arrangement grants theserver access to all data, thereby compromising the privacy safeguards in FRL. In contrast, our work allowslocal agents to conduct local training and prohibits data leakage to the central server. We further discussdifferences between FRL and Distributed RL in Appendix E.1. Poisoning in FRL is an almost unexplored territory, except for very limited existing studies. Fan et al. (2021);Jordan et al. (2024) provide a fault-tolerance guarantee for FRL. However, their frameworks require thecentral server to access local environments, which is a form of privacy leakage. In contrast, our work prohibitsthe central server from accessing any local tasks; Anwar & Raychowdhury (2021) restricts local agents frommultiple local updates in its algorithm, which is not only against the nature of RL exploration but alsoseverely expensive in the communication costs in applications. In contrast, our poisoning method is uniquelytailored to accommodate multiple local steps, aligning with realistic settings of RL scenarios. Zhang et al.(2022); Jordan et al. (2024) present FRL methods resilient against Byzantine failure. However, Byzantinefailure is considered relatively naive and weak compared with real-world attacks that can be meticulouslydesigned. In contrast, our work addresses the gap in studying strategic malicious attacks in FRL. Finally, our work is also related to the literature of RL in strategic settings such stochastic games. In stochasticgames, the RL agents (players) are often selfish and optimize their own objectives, and the goal is to seewhether their collective behavior results in any equilibrium outcome (Ning & Xie, 2024; Altabaa et al., 2023).However, in our poisoning FRL framework, the goal is to optimize an objective function through a central",
  "Overview and Contributions": "We provide an overview of the remaining content in this work as follows. To address the vulnerability of FRLto adversarial attacks, we start by proposing a theoretical framework (Sections 2 and 3) that characterizesthe problem of environment poisoning in FRL as an optimization problem. We assume the presence of asuspicious agent within the federated system, which can be either a malicious client or a compromised victim.This high-risk agent is trained by perturbed observations through reward manipulation. However, the attackerdoes not have prior knowledge of the underlying Markov Decision Process (MDP) of the environment andcan only learn it through observations. As mentioned previously, this type of attack is practical and can beeasily implemented in real-world scenarios, such as buying an IoT device to participate in an FRL systemand providing false signals to its sensors. To assess this risk, we design a novel poisoning mechanism () that targets FRL with policy-based localtraining. Our protocol is designed to target not only general policy-based methods but also the Actor-Criticsetting, wherein the attacker trains a set of both public and private critics. The private critic calculatesoptimized poisoned rewards, while the public critic manipulates the coordinators model during the trainingprocess. Notably, our poisoning protocol operates without requiring knowledge of the MDP of the environmentand remains consistent with the multiple local steps setting of FRL. Furthermore, we offer a theoreticalguarantee for our attack mechanism (), demonstrating that our approach can lead to a substantialdrop in the performance of the FRL system. We establish a theoretical result that our attack is effective evenin the most challenging scenario for the attacker. Our method is evaluated through extensive experiments on OpenGYM environments (Brockman et al.,2016), which represent standard RL tasks across various difficulty levels such as CartPole, InvertedPendulum,LunarLander, Hopper, Walker2d, and HalfCheetah. These experiments employ different FRL backbonemodels. The findings conclusively illustrate that, through our attack mechanism, a malicious agent caneffectively poison the entire FRL system.",
  "Contributions. Our findings highlight the vulnerability of the FRL framework to local poisoning attacks,emphasizing the need for awareness of system risks during training": "Innovative FRL Poisoning Method. Our method addresses the challenges of prior RL poisoningin two key aspects: (a) Adherence to federated privacy code. Our method strictly avoids accessingthe environments MDP, a code often violated by previous RL poisoning. (b) Leveraging historicalfederated rounds. Our double-critic mechanism leverages historical data from prior federated rounds,overcoming the reduced poisoning effectiveness of prior RL attacks applied to federated contexts. Theoretical Analysis. We provide theoretical evidence demonstrating the impact of our method inpoisoning the system. Our main theoretical findings imply that the extent of the objectives decreasescales with the square of the attack budget. Moreover, a larger learning rate and a smaller systemsize increase the systems vulnerability to our attack. Empirical Validation. We validate the effectiveness of our poisoning protocol through extensiveexperiments targeting mainstream RL algorithms like VPG and PPO. These experiments encompassOpenGYM environments with varying difficulty levels, as detailed in . This evaluationincludes comparisons with various baseline models and assessments of different (targeted and non-targeted) poisoning types. In summary, our work indicates that when FL is applied to RL training, the systematic security risk canmake FRL susceptible to poisoning and threats in applications. Consequently, the trained policy of FRL maynot be entirely reliable and requires a more robust and secure protocol. Our findings highlight the potentialrisks of FRL under adversarial attacks and inspire future research toward developing more robust algorithms.",
  "Preliminaries and Notations": "In this section, we overview some background and notations that are necessary for introducing the concept ofpoisoning in FRL. We consider single-task FRL, where a number of agents work together to achieve a commontask. As such, all agents are trained on the same MDP. We consider the ubiquitous on-policy training setting(Singh et al., 2000). MDP and RL. An MDP is a discrete stochastic control process for decision-making (Puterman, 1990) thatis defined by a tuple M = (S, A, r, P, ), where S is a state space, A is an action space, r() : S A Ris a reward function, P() : S A S is a state transition probability function, and (0, 1) is adiscount factor. Given an MDP, the goal of RL is to find an optimal policy, () : S A, where A isthe set of all probability distributions over the action space A, which maximizes the expected accumulateddiscounted reward. As is common in the literature (Agarwal et al., 2021), we often represent a policy byits parametrization (e.g., tabular parametrization or neural network weight parametrization). During theprocess, at each step t, the decision maker begins in some state st, selects an action at according to the policy(st), receives a reward r(st, at), and transitions to the next state st+1 with probability P(st+1|st, at). Thisdecision-making and interaction process continues until the MDP terminates. Federated Reinforcement Learning (FRL). An FRL system consists of n agents and a central server.The agents perform local training for L steps and then send their updated policies to the central server. Theserver performs aggregation to create a central policy, which is then broadcast back to all the agents. Thisprocess is repeated for T rounds, and the broadcast policy is used to initialize the next round of local training.More specifically, at each round p T, at the start of local step q L, denote the policy of each agent iby its policy parameter p,q1(i). Following this policy, the agent interacts with its environment, gatheringsequences of states, actions, and rewards: Op,q(i) = (Sp,q(i) , Ap,q(i) , Rp,q(i) ) =(s1, s2, . . .), (a1, a2, . . .), (r1, r2, . . .). Based on Op,q(i) , the local policy is typically updated using p,q(i) = arg max J(, p,q1(i), Op,q(i) ), where J issome objective function. After completing L steps of local training, all agents update their local policies to{p,L(i) }i[n], which are then submitted to the server.1 Subsequently, the server forms a new global policy",
  "Problem Formulation": "We propose a theoretical framework to conceptualize the challenge of reward poisoning attacks to FRL asa sequential optimization problem as Problem (P). Problem (P) is defined in terms of the notations andconcepts introduced in , such as the local policies, global policy, and aggregation algorithms. Theremainder of this section is organized as follows. In .1, we elaborate on the rationale for implementinglocal reward poisoning. In Sections 3.2 and 3.3, we provide a detailed explanation of the objective function,feasible domain, and constraints in Problem (P). Finally, in .4, we present the knowledge possessedby each involved party, ensuring alignment with established security protocols in FL.",
  "Local Reward Poisoning": "We consider a threat model targeting FRL systems through reward poisoning. This model assumes thepresence of an untrustworthy participant in the system, categorized as either an internal malicious agent(typical in FL poisoning (Tolpegin et al., 2020)) or a victim contaminated by external attackers (a commonscenario in RL poisoning (Zhang et al., 2020)). We consider the rewards of the suspicious agents get poisonedduring local training, following established practices in RL poisoning (Huang & Zhu, 2019; Zhang et al., 2020;Rakhsha et al., 2021; Banihashem et al., 2022). For clarity, we designate the compromised agent as agentn. At each round p and step q, the attacker may manipulate the reward sequence of Rp,q(n), transforming it",
  "Objective Function and Feasible Domain": "Objective Function. In the optimization Problem (P), the objective LA represents the loss of the attacker,which can characterize both untargeted and targeted poisoning settings. In the case of untargeted poisoning,LA is the benefit of the server, typically represented by the long-term cumulated reward. In the case oftargeted poisoning, LA is a policy matrix distance, measuring the difference between the learned policy T(0)and some targeted policy . This captures the attackers goal to manipulate the global model to align witha specifically targeted policy. Feasible Domain. The objective LA is minimized over manipulated rewards R, which is subject to theconstraint that R should closely align with the ground-truth rewards R (Eq. 5). Although R may initiallyseem to behave as a random variable, it is essential to note that in Problem P, we characterize the processsuch that once Rp,qnis obtained and observed, it is treated as a deterministic variable. Subsequently, theattacker refers to this observed deterministic variable Rp,qnto optimize its manipulated rewards Rp,qn .",
  "Constraints": "The constraints in optimization (P) characterize the process of poisoning in FRL, including local initialization(Eq. 1), local train (Eq. 2, 3), attack with limited budget (Eq. 5), and global aggregation (Eq. 4). A concisesummary is provided in , and further details are explained below. Local train. Constraints 2 and 3 outline the local update of each agent at local step q in round p. InConstraint 2, each clean agent uses its current policy, denoted by p,q1(i), to roll out an observation sequence Op,q1(i). Then, based on this observation, the agent updates its policy from p,q1(i)to p,q(i) by maximizing anobjective function J(), defined by the agents specific RL algorithm. Constraint 3 characterizes the updatefor the suspicious agents update, akin to 2, with the distinction that the local update for a suspicious agentis based on the manipulated rewards R. Attack budget. Constraint 5 captures the budget constraint for the attacker, where D(, ) representsthe distance between the perturbed (poisoned) observations and the clean observations, which is restrictedby a cost budget . The consideration of the attack budget is crucial. On the one hand, a limited budgetconstitutes a standard assumption in RL reward poisoning Huang & Zhu (2019); Rakhsha et al. (2021).On the other hand, our framework strives to encompass scenarios of both internal malicious clients andvictim clients susceptible to external poisoning From a practical standpoint, an internal malicious agentmay seek to avoid detection, necessitating caution in its manipulation, while an external attacker is likely tobe concerned about the costs associated with their attacks. Therefore, the inclusion of an attack budget isimperative for both scenarios. Global aggregation and local initialization. Constraint 4 models the aggregation step of the centralserver by an aggregation algorithm Aagg. This algorithm processes the models sent from agents, denoted by",
  "Information Structure": "We present the knowledge of each party in the FRL system to guarantee strict adherence to privacy protectionin FL. shows the knowledge of the three parties involved in FL, namely, the coordinator, the attacker,and the agents, and clarifies the knowledge of each party to guarantee the data privacy expected from FL. The coordinator only has knowledge of the submitted models and the global policy, while the agents onlyhave knowledge of their local data, policy, and the broadcast global model. The attacker only has knowledgeof its own observations, manipulations, model, and the broadcast global policy. This ensures that the agentsprivate data is kept confidential.",
  "Method": "In this section, we propose practical local reward poisoning methods for FRL. We consider two scenarioswhen the local RL training in the FRL system uses actor-critic methods (Algorithm 2) and policy gradientmethods (Algorithm 3). We note that our reward poisoning methods can be employed for both targetedand untargeted reward poisoning, as detailed in .2, by employing the appropriate correspondingobjective. As before, we use to denote the suspicious agents parameters communicated with the server.",
  "Reward Poisoning for Actor-Critic-based FRL": "Actor-Critic mechanism. In actor-critic algorithms (Peters & Schaal, 2008), we have a policy parameterizedby alongside a corresponding value function V(s), determined by policy and state s. Besides the policymodel, each agent incorporates a critic model () parameterized by , providing an estimation of the policy-related value function V(s), denoted by V (s) = (s). To simplify notations, by some abuse of notation, werefer to the critic model by its parametrization in subsequent analysis. The estimated value functionV (s) generated by the critic further gives an estimation of the Q-function as Q(s, a) = r(s, a) + V (s),where r(s, a) is the observed reward, and s is the next observed state. The critic model updates itself byminimizing the temporal-difference error between its model estimation and ground-truth observation (Tesauroet al., 1995). The policy parameter is updated by t+1 = arg max J(, t, Ot), where J() is some objectivefunction specified by the actor-critic algorithm, t is the exploration step, and Ot is the observed trajectory.In the following, we describe the poisoning mechanism in detail and summarize it in Algorithm 2, which usesAlgorithm 1 as a subroutine.",
  ": updates private critic to p,q(n) with true observation Op,q(n)": "Public and private critics. The term public critic\" implies that the agent uses this critic for communicationwith the server (Line 23), while private critic\" signifies that the agent retains this critic solely for localcomputation purposes (Line 4). To manipulate the global model by training (Line 8) and submitting (Line23) a poisoned public critic, the attacker also undergoes training for an unpoisoned private critic (Line 9).This approach allows the attacker to possess knowledge of a true estimation of the value function (Line 4) tomake optimal poisoning decisions (Line 5). We denote the pair of public and private critics respectively as p,q(n) and p,q(n). The private critic p,q(n) istrained using ground-truth rewards Rp,q(n) (Line 9), and then the attacker harnesses the private critic p,q(n) to obtain poisoned rewards Rp,q(n) (Line 5). Rp,q(n) is then utilized to train the public critic p,q(n) (Line 8). Dueto the different goals and training methods of the pair of critics, their initialization also differs. At thebeginning of a new round of local training, the agent initializes its public critic with the broadcast globalmodel (i.e., p,0(n) = p1(0) in Line 10), while inherits its private critic from the end of last round of training",
  "(i.e., p,0(n) = p1,L(n)in Line 10)": "Reward poisoning and actor update. The attacker aims to minimize the objective function J of the policymodel (the actor) by poisoning its local rewards. When deciding how to poison the rewards, the attackershould use the unpoisoned objective J given by the ground-truth rewards R and private critic (Line 4) toobtain a true estimation and make a right poisoning decision (Line 5). Concretely, the attacker minimizesthe original objective J with respect to R: for each round p T and local step q L, the corrupted agentn interacts with the environment and obtains the ground-truth observation Op,q(n). The attacker computesthe unpoisoned objective Jp,q(n) using true rewards Rp,q(n) and private critic p,q(n) (Line 4). Then, the attacker",
  "Reward Poisoning for Policy-Gradient-based FRL": "In Policy Gradient (PG) algorithms (Silver et al., 2014), agents do not require a critic model. Therefore,we have adapted the poisoning method described in .1 that uses Actor-Critic for agents local RLalgorithm. The overall procedure is outlined in Algorithm 3. The attackers goal is still to minimize theobjective function J of the policy model. Since there is no critic, the agent directly calculates J from theobserved O (Lines 14 and 15), and uses this information to decide how to poison the rewards to R by Eq. 6(Line 16). The policy is then updated (Line 18) with the poisoned objective J calculated by the poisonedrewards (Line 17).",
  "Assumption 1 (Single-step local training). We assume that for each round, all local agents only applysingle-step local training, i.e., L = 1": "Clean local reward sequence function. Let us denote the dimensions of and r by d and dr, respectively.Given a policy , define r(i)() : Rd Rdr as the function of the reward sequence generated by the agentinteracting with the local environment i under policy . Under Assumption 1, denote the reward sequenceobserved by agent i during round p by rp(i). Then, we have rp(i) = r(i)(p1(0) ). Clean local objective function. With slight abuse of notations, let J(; r) denote the local RL objectivefunction given the policy and the reward sequence r, where we have J : Rddr R. According tothe mechanism of FRL, we further define the objective of local agent i given an initialized policy asJ(i)() := J(; r(i)()). If we denote the objective of local agent i at the start of round p by Jp(i), we have",
  "Assumption 3 (Objective smoothness). We assume that Jp(0) is differentiable with respect to r and almosteverywhere, and Jp(0) is Lrsmooth with respect to rp(n)": "Assumption 3 is well-founded, aligning with prevalent local RL objectives. We provide two illustrativeexamples: a) Discounted Cumulative Reward (Kaelbling et al., 1996): In this case, we have Jp(i) := rp(i),where := (0, 1, 2, ..., dr) is the discount vector. Here, (0, 1) is a discount factor, and dr representsthe cardinality of rp(i). b) Average Reward (Kaelbling et al., 1996): In this case, the local objective is expressed as Jp(i) := (1 rp(i))/dr, where dr denotes the cardinality of rp(i), and 1 := (1, 1, ..., 1) Rdr stands for an all-onevector. We note that the primary purpose of Assumption 3 is to facilitate our theoretical analysis. However,in our numerical experiments (.1), we demonstrate the effectiveness of our proposed attack even ifthis assumption fails to hold (e.g., when the states/actions/rewards are discrete).",
  "Assumption 4 (Worst-Case). We assume the attacker is only able to attack in the latest round": "Remark 5. Assumption 4 is designed to encapsulate the most challenging scenario for the attacker, wheremanipulation is restricted solely to the latest round. The proven guarantee under this assumption serves asan indicator of the effectiveness of our attack method in more general and lenient scenarios, wherein theattacker has the ability to poison the system over multiple rounds. Furthermore, our experiments showcasethe effectiveness of the attack when deployed over multiple rounds (refer to ).",
  "Then, for B > 0 and < +, we have Jp(0) Jp(0) , where (0,2+8 ]": "Remark 7. Theorem 6 asserts that the gap is strictly positive, and its upper bound increases proportionallywith 2+, which is the square of the upper limit of the attack budget. With a small poison budget , we canguarantee that the poisoned global objective Jp(0) is strictly smaller than the clean global objective Jp(0), and ahigher attack budget can indicate a greater decrease in the global objective. Similarly, a larger local learningrate or fewer agents can increase +, indicating a stronger objective decrease . In Appendix A, we notonly explore a practical scenario ensuring B > 0 but also highlight that a higher learning rate enhances thelikelihood of B > 0.",
  "Numerical Experiments": "In this section, we conduct a series of experiments to evaluate the effectiveness of our poisoning method on theFRL system. Our results show that the proposed poisoning method can effectively reduce the mean episodereward of the server in the FRL system. Additionally, our poisoning protocol does not require knowledge ofthe MDP of the environment and is consistent with the multiple local steps setting of FRL.",
  "Environments": "Nature of FRL: Following existing FRL literatures (Jordan et al., 2024; Zhang et al., 2022), we implementexperiments on various OpenAI Gym environments (Brockman et al., 2016) with increasing complexity,including CartPole, Inverted Pendulum, Hopper, Lunar Lander, and Half Cheetah, all of which are standardRL task environments. The selection of these datasets reflects the inherent nature of FRL, where the focus ison solving RL tasks, and FL serves as a versatile toolbox. It is worth noting that environments such as Inverted Pendulum, Hopper, Half Cheetah feature continuousreward spaces, aligning with our theoretical analysis (Assumption 3). Additionally, we incorporate environmentssuch as Cartpole and Lunar Landar, providing discrete reward spaces, to demonstrate the applicability of ourattack in diverse scenarios of reward space.",
  "We explain our backbone framework by describing both its local RL model and its global FL mechanism": "Local RL models. Our local training backbones cover both the Policy Gradient method (corresponding toAlgorithm 3) and the Actor-Critic method (corresponding to Algorithm 2), maintaining consistency with ourattack methods in . For the Policy Gradient backbone, we opt for the conventional Vanilla PolicyGradient (VPG) (Sutton et al., 1999). For the Actor-Critic backbone, we choose a widely applicable model,Proximal Policy Optimization (PPO) (Schulman et al., 2017). In Appendix E.2, we discuss broader RLbackbones and justify that VPG and PPO maintain accessibility and practicality while delivering competitiveperformance. Federated mechanism. We adhere to a prevalent practice observed in existing FRL literature (Xie & Song,2023b; Zhu & Gong, 2023), where the central server aggregates the models submitted by local agents bytaking the average at the end of each communication round and then broadcasts the new global model, whichis used by the local agents as initialization for the next round of local training. This federated mechanismaligns with the settings in our theoretical analysis (Assumption 2).",
  "We evaluate our methods against diverse baselines, including standard FRL, poisoned FRL, and robust FRL": "Standard FRL. In this setting, agents undergo regular training without any poisoning, adhering strictlyto the conventional rules of local updates and federated communications. The baseline comprises bothVPG-based standard FRL (Algorithm 5) and PPO-based standard FRL (Algorithm 4). We refer to AppendixC for a detailed description of those algorithms. Poisoned FRL. To the best of our knowledge, no existing FRL method designed for poisoning matches thecriteria of a standard and reasonable FRL setting, encompassing multiple local steps and data security code(refer to .2). Therefore, we establish two baselines for poisoned FRL: 1) the federated version ofprior RL poisoning methods designed for single-agent systems, and 2) a randomized attack. Further detailsare provided below: 1. Federated version of prior RL poisoning. As one of our baselines, we include the federatedextension of VA2C-P (Sun et al., 2020), a prior RL poisoning method. This choice is due to VA2C-Pscompatibility with our reward-manipulating scheme and its adherence to the knowledge limitation thatthe attacker does not have access to the environments MDP. In contrast, other prior RL poisoningmethods either present compatibility issues or violate this knowledge limitation, as discussed in.1 on the challenges of applying prior RL poisoning to FRL.",
  "Rp,q(j) Rp,q(j) x,(7)": "where x is a vector with the same cardinality as R, denoted as |x| = |Rp,q(j)|. Each index-wise valuein x follows a uniform distribution, such that x = (x1, x2, ..., x|R|), where xi U(0, 1) for all i |R|.Here, U(a, b) represents the uniform distribution in the range from a to b. The detailed algorithm isprovided in Appendix C.2. Robust FRL. To our knowledge, no existing robust FRL method aligns with the criteria of a standard andreasonable FRL setting (refer to .2). Therefore, we adopt a standard robust mechanism inspired byFL, where the aggregation is executed by re-weighting with the clients reliability (Fu et al., 2019; Tahmasebianet al., 2022; Wan & Chen, 2021). The detailed defense algorithm is outlined in Algorithm 6.",
  "Experimental Settings": "Evaluation matrix. The evaluation metrics can vary based on the type of poisoning. For untargetedpoisoning, we evaluate the performance of these methods by measuring the mean-episode reward of thecentral model, which is calculated based on 100 test episodes at the end of each federated round. For targetedpoisoning, we measure the similarity between learned policy and targeted policy. Moreover, for discrete actionspace, we calculate the proportion of target actions among all actions, while for continuous action space, wecollect 1scaled distance. Under both measurements, a higher value indicates a closer learned policy to thetarget policy. By default, our assumed poisoning type is untargeted unless stated otherwise. Attack Budget. We have two configurations for the attack budget: a) Small Budget. We set = 1 tosimulate a small-budget scenario where the attacker is cautious and cost-conscious. We choose = 1 as itrepresents the maximum possible value gained by one action move in the CartPole and Inverted Pendulumenvironments. It is important to note that = 1 is generally much smaller than the maximum possible rewardper action in the other environments (i.e., 100 for Lunar Lander). b) Large Budget. For a large-budgetscenario where the attacker is bold and almost indifferent to costs, we use = 100. In instances withoutspecific notation, we refer to a small-budget attack. Hyper-parameters. For both VPG and PPO settings, we let the malicious agent attack the reward witha budget of = 1. There are 200 total communication rounds, and all agents run 5 local steps in eachcommunication round. The number of poisoned agents is set to one if it is not specifically mentioned. For allexperiments, we average the results over 5 random seeds.2",
  "Empirical Results": "We present experimental results that demonstrate the effectiveness of our proposed poisoning method. Wecompare its performance with various baselines outlined in .3, including standard FRL (.5.1),poisoned FRL (.5.2), and robust FRL (.5.2). The Experiments cover diverse backbonemodels (.2) and environments (.1), consistently demonstrating superior performance. Inaddition to evaluating the effectiveness of untargeted poisoning, we further validate our method throughtargeted poisoning (.5.3). Furthermore, we demonstrate the adaptability of our approach in scenariosinvolving multiple suspicious agents (.5.3). The results affirm that our method is both practical anddetrimental for real-world applications. To enhance clarity, we have smoothed the plots by preserving valuesfrom the first 5 rounds and subsequently applying a moving average with a window size of 10 rounds.",
  "We conducted experiments to poison standard VPG-based FRL () and PPO-based FRL (). Thebackbones for these experiments were constructed as outlined in .2, utilizing Algorithms 4 and 5": "Poison standard VPG-based FRL. We present our results in .We see that across severalenvironments, a single attacker successfully poisons a system of up to 3 to 4 agents using our method. Weonly present the results of systems with maximum sizes that our method can poison. For instance, in thefirst plot of , the system size is four, indicating that our method can poison a VPG-based FRL with",
  "We refer to Appendix D.1.1 for additional settings": ": Poison VPG-based FRL. The red line, labeled poison, shows the performance of our adversarialattack, while the green line, labeled clean, represents the performance of the standard FRL system. The blueline, labeled rand, denotes a random poisoning baseline. The yellow line, labeled VA2C-P(Fed), illustratesthe federated version of VA2C-P, an existing poisoning method for single-agent RL. We specifically highlightthe results for the largest system sizes that our method can successfully attack: 4 for CartPole and Hopper,and 3 for Inverted Pendulum and Walker. : Poison PPO-based FRL. The annotation is the same as . We specifically showcase resultsfor the largest system size that our method can successfully attack. The maximum system size is 3 forInverted Pendulum and 4 for the others. the number of agents equaling 1, 2, 3, 4. However, our method would fail when the system size is 5. Acrossenvironments of varying difficulty levels, we consistently observe a significant performance gap between cleantraining (the green line) and training under our poisoning attack (the red line) as the number of trainingrounds increases. This proves the capability of our VPG attack method in poisoning federated systems. Poison standard PPO-based FRL. Similar to the VPG-based FRL case, we report those systems withmaximum possible size that our method can poison. Attacking a PPO-based FRL system () lendsitself to a much different dynamic because the separate value parameters allow all agents to learn moreeffectively, and thus, we expect both clean and malicious agents to be more successful in their opposite goals.We observe a consistent performance gap emerging more quickly, with fewer federated rounds, compared tothe VPG-based FRL system (Fig 1). This indicates that our method is particularly effective in PPO-basedFRL systems, benefiting from the double-critic protocol specifically designed for actor-critic backbones.",
  "Comparison with Poisoned FRL and Robust FRL": "Poisoned FRL. We conduct experiments on random attacks against standard VPG-based FRL and PPO-based FRL. We outline two poisoning baselines, a federated version of a prior RL poisoning method (VA2C-P)and a randomized attack in .3, with detailed algorithms provided in Section C.2. For VPG-basedFRL, we observe that our method (, the red line) significantly reduces rewards of various environmentscompared to both the federated version of VA2C-P poisoning (, the yellow line) and the random attack(, blue line). For PPO-based FRL, both the federated version of VA2C-P (, the yellow line) andthe random attack (, the blue line) perform very poorly in poisoning, as they are overwhelmed by thestrength of the clean agents, leading to rewards that remain close to clean training (, the green line)over time, considering performance variance. In contrast, our poisoning (, the red line) significantlyharms the system performance, outperforming the poisoning baselines. Robust FRL. The selection of the robust FRL baseline is detailed in .3. Experiments are conductedfollowing settings in .4 and D.1.1. Results are presented in . It is noteworthy that, althoughthe defense mechanism can successfully mitigate the harm of our poisoning in relatively simple environments(CartPole, Hopper, Walker2d), for complicated environments (Half Cheetah), which are closer to real-worldscenarios, the defense mechanism fails against our poisoning. Thus, our results consolidate that this kind ofpoisoning is harmful in real-world applications. : Poison Robust FRL. The red dashed line, labeled as Poison, depicts the performance of ouradversarial attack in a standard FRL system. The green dashed line, labeled as Clean, represents the standardFRL systems performance. The blue dashed line, labeled as Defense, showcases the performance of ourattack in a robust FRL system.",
  "Broader Impact": "The development of our poisoning method for FRL has significant implications for both research and practicalapplications. By highlighting the vulnerabilities inherent in FRL frameworks, our work raises critical securityconcerns in sensitive domains such as healthcare and finance, while guiding the design of robust policies tomitigate these risks. Additionally, our research paves the way for further exploration of adversarial poisoningand defensive strategies in FRL, emphasizing the need for resilient algorithms capable of withstandingmalicious manipulation during the training process. Moreover, our findings initiate important ethical discussions about the responsible use of federated learningtechnologies. As these systems become more integrated into critical reinforcement learning applications, it isessential to establish guidelines that ensure their deployment is both secure and beneficial. By highlightingpotential poisoning risks, we encourage the adoption of security measures that not only protect data privacybut also enhance public trust in FRL systems.",
  "Conclusions": "In this work, we have proposed a novel method for poisoning FRL under both general policy-based andactor-critic algorithms, which can provably decrease the systems global objective. Our method is evaluated through extensive experiments on various OpenGYM environments using popular RL models, and the resultsdemonstrate that our method is effective in poisoning FRL systems. Our work highlights the potential risks ofFRL and inspires future research to design more robust algorithms to protect FRL against poisoning attacks.",
  "Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In MachineLearning Proceedings 1995, pp. 3037. Elsevier, 1995": "Kiarash Banihashem, Adish Singla, Jiarui Gan, and Goran Radanovic. Admissible policy teaching throughreward design. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 60376045,2022. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysaw Dkebiak, Christy Dennison,David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcementlearning. arXiv preprint arXiv:1912.06680, 2019. Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learningthrough an adversarial lens. In International Conference on Machine Learning, pp. 634643. PMLR, 2019.",
  "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and WojciechZaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016": "Tianlong Chen, Xiaohan Chen, Wuyang Chen, Howard Heaton, Jialin Liu, Zhangyang Wang, and Wotao Yin.Learning to optimize: A primer and a benchmark. Journal of Machine Learning Research, 23(189):159,2022. Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, and Xiaojin Zhu. Byzantine-robust onlineand offline distributed reinforcement learning. In International Conference on Artificial Intelligence andStatistics, pp. 32303269. PMLR, 2023. Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and ToddHester. Challenges of real-world reinforcement learning: definitions, benchmarks and analysis. MachineLearning, 110(9):24192468, 2021.",
  "Flint Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Cheston Tan, Bryan Kian Hsiang Low, and Roger Watten-hofer. Fedhql: Federated heterogeneous q-learning. arXiv preprint arXiv:2301.11135, 2023": "Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low. Fault-tolerant federated reinforcement learning with theoretical guarantee. Advances in Neural InformationProcessing Systems, 34, 2021. Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. Local model poisoning attacks to {Byzantine-Robust} federated learning. In 29th USENIX Security Symposium (USENIX Security 20), pp. 16051622,2020.",
  "Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. Attack-resistant federated learning with residual-basedreweighting. arXiv preprint arXiv:1912.11464, 2019": "Raj Ghugare, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. Searching for high-valuemolecules using reinforcement learning and transformers. arXiv preprint arXiv:2310.02902, 2023. Prashant Govindarajan, Santiago Miret, Jarrid Rector-Brooks, Mariano Phielipp, Janarthanan Rajendran,and Sarath Chandar. Learning conditional policies for crystal design using offline reinforcement learning.Digital Discovery, 3(4):769785, 2024. Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations oncost signals. In Decision and Game Theory for Security: 10th International Conference, GameSec 2019,Stockholm, Sweden, October 30November 1, 2019, Proceedings 10, pp. 217237. Springer, 2019. Mahdee Jodayree, Wenbo He, and Dr. Ryszard Janicki. Preventing image data poisoning attacks in federatedmachine learning by an encrypted verification key. Procedia Computer Science, 225:27232732, 2023a.ISSN 1877-0509. doi: URL 27th International Conference on Knowledge Based andIntelligent Information and Engineering Sytems (KES 2023). Mahdee Jodayree, Wenbo He, and Ryszard Janicki. Preventing text data poisoning attacks in federatedmachine learning by an encrypted verification key. In Andrea Campagner, Oliver Urs Lenz, Shuyin Xia,Dominik lzak, Jarosaw Ws, and JingTao Yao (eds.), Rough Sets, pp. 612626, Cham, 2023b. SpringerNature Switzerland. ISBN 978-3-031-50959-9. Philip Jordan, Florian Grtschla, Flint Xiaofeng Fan, and Roger Wattenhofer. Decentralized federated policygradient with byzantine fault-tolerance and provably fast convergence. arXiv preprint arXiv:2401.03489,2024.",
  "Wentao Liu, Xiaolong Xu, Jintao Wu, and Jielin Jiang. Federated meta reinforcement learning for personalizedtasks. Tsinghua Science and Technology, 29(3):911926, 2023": "Weiming Mai, Jiangchao Yao, Gong Chen, Ya Zhang, Yiu-Ming Cheung, and Bo Han.Server-clientcollaborative distillation for federated reinforcement learning. ACM Transactions on Knowledge Discoveryfrom Data, 18(1):122, 2023. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp.12731282. PMLR, 2017. Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Cooperative multi-agent reinforcement learning:asynchronous communication and linear function approximation. In International Conference on MachineLearning, pp. 2478524811. PMLR, 2023a.",
  "Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu. Multi-agent reinforcement learning: Asynchronouscommunication and linear function approximation. arXiv preprint arXiv:2305.06446, 2023b": "Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,David Silver, and Koray Kavukcuoglu.Asynchronous methods for deep reinforcement learning.InInternational conference on machine learning, pp. 19281937. PMLR, 2016. Mohammad Mohammadi, Jonathan Nther, Debmalya Mandal, Adish Singla, and Goran Radanovic. Implicitpoisoning attacks in two-agent reinforcement learning: Adversarial policies for training-time attacks. arXivpreprint arXiv:2302.13851, 2023.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimizationalgorithms. arXiv preprint arXiv:1707.06347, 2017": "Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman.Data-efficient reinforcement learning with self-predictive representations. arXiv preprint arXiv:2007.05929,2020. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministicpolicy gradient algorithms. In International conference on machine learning, pp. 387395. PMLR, 2014. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, JulianSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of gowith deep neural networks and tree search. nature, 529(7587):484489, 2016.",
  "Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl withunknown dynamics. arXiv preprint arXiv:2009.00774, 2020": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods forreinforcement learning with function approximation. Advances in neural information processing systems,12, 1999. Farnaz Tahmasebian, Jian Lou, and Li Xiong. Robustfed: a truth inference approach for robust feder-ated learning. In Proceedings of the 31st ACM International Conference on Information & KnowledgeManagement, pp. 18681877, 2022.",
  "Gerald Tesauro et al. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):5868, 1995": "Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against federatedlearning systems. In European Symposium on Research in Computer Security, pp. 480501. Springer, 2020. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michal Mathieu, Andrew Dudzik, Junyoung Chung,David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii usingmulti-agent reinforcement learning. Nature, 575(7782):350354, 2019.",
  "BProximal Policy Optimization (PPO)-Specific Framework": "In this appendix, we focus on a specific local RL algorithm, Proximal Policy Optimization (PPO) (Schulmanet al., 2017), for the individual agents in FRL and propose a corresponding framework for poisoning. Byspecifying the local RL algorithm as PPO, we are able to tailor the problem formulation in Problem Pand accordingly propose a targeted solution in to poisoning PPO-specific FRL. In Section B, weintroduce PPO preliminaries to specify the general variable in Problem P. Then we discuss the PPO-specificproblem formulation for poisoning FRL in Section B. This will allow us to take into account the specificcharacteristics of the PPO algorithm when defining the problem.",
  "PPO Preliminaries": "PPO is a popular Actor-Critic algorithm that uses a clipped surrogate objective. For agent i at federated roundp and local episode q, denote the pair of state and action at its t-th step of rollout as Op,q,t(i)= (sp,q,t(i) , ap,q,t(i) )and denote the V -function and Q-function defined by Bellman Equation (Baird, 1995) as V () and Q(, ),respectively. Then the advantage function is Ap,q,t(i):= Q(sp,q,t(i) , ap,q,t(i) ) V (sp,q,t(i) ). PPOs critic. Let us use to denote estimation. Denote PPOs critic model as (p,q(i) ), where isthe models weights. As with all typical actor-critic algorithms, the critic is a Value neural network to helpestimate the V-value of the actor so as to further calculate the actors objective. In PPO, the actors objectiveis a clipped advantage (Eq. 15), where the advantage is estimated by the critic and observation, whichcan be written in the form of Ap,q,t(i)= A(p,q(i) , Op,q,t(i) ). The critic model updates itself by minimizing thetemporal-difference error (Tesauro et al., 1995) between the estimated and observed V -value. We denote thecritics objective by p,q(i) . PPOs actor. Denote the actor model as (|s, ), where is the model weight and s is some given state.To specify the general problem P to the PPO case, the clean agents objective J() (Eq. 2) should be thePPO surrogate objective",
  "PPO-specific FRL poisoning": "As an actor-critic algorithm, when we fit single-agent PPO into a federated framework, we assume thatbesides the actor model, the critic model should also be updated from individual agents to the server, thenaggregated by the server and finally broadcast to the local agents at each federated round p. Denote theaggregated actor as p(0) and the aggregated critic as p(0). To specify the servers aggregation function Aagg",
  "Below we give random poisoning algorithms mentioned in Baselines (.3)": "Random Poisoning for Actor-Critic-based FRL. The algorithm is implemented almost the same asAlgorithm 2, except that Line 5 should be replaced with Poison Reward as Rp,q(n) using true objective Jp,q(n)and budget by Eq. (7). Random Poisoning for Policy-Gradient-based FRL. The algorithm is implemented almost the same asAlgorithm 3, except that Line 16 should be replaced with Poison Reward as Rp,q(n) using true objective Jp,q(n)and budget by Eq. (7).",
  "C.3Robust FRL": "To mitigate the risk of FRL being exposed to malicious agents, below we describe a defense mechanismagainst FRL (Algorithm (6)) that inherits from conventional FL defense mechanisms, where the aggregationis implemented by re-weighting with the clients reliability. To adapt this robust aggregation protocol fromFL to FRL, we let the clients local RL performance be a reflection of their reliability. The aggregationprocess incorporates a re-weighting mechanism based on the clients reliability, as discussed in prior work (Fuet al., 2019; Tahmasebian et al., 2022; Wan & Chen, 2021). In adapting this robust aggregation protocolfrom FL to FRL, we leverage the local RL performance of clients as an indicator of their reliability. To bemore precise, the re-weighting is accomplished by assigning credits to each agents policy according to itsperformance. To that end, the central server runs tests on each policy it receives from the agents and recordsthe observations denoted by Op,test(i). The server then calculates the average reward rp,testifor each policy by averaging the rewards in the sequence {rt}p,test(i). Finally, the server normalizes the average rewards bydividing them by the sum of all averaged rewards, resulting in a set of normalized weights cp,qi. These weightsare used to weight the average aggregation of the policies:",
  "D.1.1Settings for Main Experiments": "Additional general settings. We measure the attack cost by the 2 distance between the poisoned rewardand the ground-truth observed reward. During each local training step, the maximum number of steps beforetermination is 300 unless restricted by the environment. The learning rate is set to 0.001, and the discountparameter is set to = 0.99. Additional settings for robust FRL. For each model uploaded to the server, we ran it for 10 episodes andcollected the mean reward per episode. We then used the normalized rewards as the weights of aggregation.We structure the system with only 2 agents for all environments to ensure that the suspicious agent possessesthe maximum possible power to poison the system since a smaller number of agents corresponds to a morepotent attack.",
  "General settings. Since VPG is more challenging to attack (refer to .5.1), in Section D.2 our focusis on attacking VPG-based FRL": "Targeted poisoning. For simplicity, we choose the target policy to be a single action, whether in discreteor continuous space. We ensure the chosen environments encompass both discrete and continuous actionspaces, featuring diverse cardinalities and dimensions for the action space. Concretely, we set the targetpolicies to be 0 {0, 1} for CartPole, 0 {0, 1, 2, 3} for Lunar Lander, 3 for Inverted Pendulum,and 06 6 for Half Cheetah. We choose environments of different action space: CartPole (two discreteactions), LunarLander (four discrete actions), InvertedPendulum (one continuous dimension), HalfCheetah(six continuous dimensions). Multi-agent attack. We opt for the CartPole environment, a relatively simple task, to balance betweenmanageable computation costs and a clear understanding of how a fixed proportion operates in a multi-agentattack scenario. In .5.1, we have determined that the maximum size of the system one agent caneffectively attack is 4 for the CartPole environment and a VPG-based FRL system. Therefore, we initiatethe system size from 4, and accordingly, the number of poisoned agents begins at 1. Subsequently, weincrementally enlarge the system size and the corresponding number of poisoned agents while maintaining aconstant proportion between the number of poisoned agents and the system size.",
  "All additional results are obtained from experiments following settings in .4 and D.1.2": "Large-budget Attack. We evaluated the performance of our poisoning with a large budget in Figure (4).The results show that with a larger budget, our method is able to attack much larger systems compared withthe constraints that appeared in the case of a small budget, i.e., we can attack up to 100 agents. : High-Budget Poisoning. The red dashed line, labeled as Poison, depicts the performance ofour adversarial attack in a standard FRL system. The green dashed line, labeled as Clean, represents thestandard FRL systems performance. Multi-agent poisoning. We have shown that the proportion of malicious agents required to poison a systemis consistent regardless of the size of the system. We found that when we increase the system size from 4agents to 100 agents, a fixed proportion of attackers can always poison the system successfully. This result isdepicted in Figure (5).",
  "E.1Differences between distributed RL and FRL": "Distributed RL and FRL are both advanced paradigms in the field of machine learning, but they differsignificantly in their architecture and application. Distributed RL involves distributing the computationof a single RL algorithm across multiple machines to accelerate learning and handle larger datasets. Thisapproach focuses on parallelizing the training process to improve efficiency and scalability. On the other hand,FRL is designed to enable multiple independent agents to collaboratively train an RL model without sharingtheir local data. In FRL, each agent trains a local model using its own data and periodically shares modelupdates (rather than raw data) with a central server, which aggregates these updates to improve the globalmodel. This method preserves data privacy and is particularly useful in scenarios where data cannot becentralized due to privacy concerns or regulatory restrictions. Thus, while distributed RL aims at enhancingcomputational efficiency, FRL emphasizes data privacy and decentralized training, making it suitable forapplications where data sharing is limited.",
  "E.2Different RL algorithms": "Reinforcement learning encompasses a variety of algorithms designed to enable agents to learn optimalbehaviors through interactions with their environment.Among these, actor-critic algorithms, such asAdvantage Actor-Critic (A2C), Asynchronous Advantage Actor-Critic (A3C), and more advanced methodslike Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO), are notablefor their efficiency in handling high-dimensional action spaces. PPO, recognized for its robustness andsimplicity, prevents large updates that can destabilize training, balancing exploration and exploitationeffectively. We chose to focus on PPO and VPG due to their practical balance between performance andcomputational efficiency. While more advanced algorithms like Deep Deterministic Policy Gradient (DDPG)or Soft Actor-Critic (SAC) offer better performance in certain scenarios, they come with increased complexityand computational costs. We focus on VPG and PPO to ensure the approach remains accessible and practicalwhile achieving competitive performance."
}