{
  "Abstract": "In this paper, we introduce a novel analysis of neural networks based on geometric (Clifford)algebra and convex optimization.We show that optimal weights of deep ReLU neuralnetworks are given by the wedge product of training samples when trained with standardregularized loss. Furthermore, the training problem reduces to convex optimization overwedge product features, which encode the geometric structure of the training dataset. Thisstructure is given in terms of signed volumes of triangles and parallelotopes generated bydata vectors. The convex problem finds a small subset of samples via 1 regularization todiscover only relevant wedge product features. Our analysis provides a novel perspective onthe inner workings of deep neural networks and sheds light on the role of the hidden layers.",
  "Introduction": "While there has been a lot of progress in developing deep neural networks (DNNs) to solve practical machinelearning problems (Krizhevsky et al., 2012; LeCun et al., 2015; OpenAI, 2023), the inner workings of neuralnetworks is not well understood. A foundational theory for understanding how neural networks work is stilllacking despite extensive research over several decades. In this paper, we provide a novel analysis of neuralnetworks based on geometric algebra and convex optimization. We show that weights of deep ReLU neuralnetworks learn the wedge product of a subset of training samples when trained by minimizing standardregularized loss functions. Furthermore, the training problem for two-layer and three-layer networks reducesto convex optimization over wedge product features, which encode the geometric structure of the trainingdataset. This structure is given in terms of signed volumes of triangles and parallelotopes generated by datavectors.By the addition of an additional ReLU layer, the wedge products are iterated to yield a richerdiscrete dictionary of wedge features. Our analysis provides a novel perspective on the inner workings ofdeep neural networks and sheds light on the role of the hidden layers.",
  "Prior work": "The quest to understand the internal workings of neural networks (NNs) has led to numerous theoretical andempirical studies over the years. A striking discovery is the phenomenon of \"neural collapse,\" observed whenthe representations of individual classes in the penultimate layer of a deep neural network tend to a pointof near-indistinguishability (Papyan et al., 2020). Despite this insightful finding, the underlying mechanismthat enables this collapse is yet to be fully understood. Linearizations and infinite-width approximationshave been proposed to explain the inner workings of neural networks (Jacot et al., 2018; Chizat et al.,2019; Radhakrishnan et al., 2023). However, these approaches often simplify the rich non-linear interactionsinherent in deep networks, potentially missing out on the full spectrum of dynamics and behaviors exhibitedduring training and inference. Infinite dimensional convex neural networks were introduced in Bengio et al. (2005), offering insights intotheir structure. Following work analyzed optimization and approximation properties of infinite convex neural",
  ":An illustration of the geometric interpretation of optimal ReLU neurons.The break-lines/breakpoints pass through a subset of special training samples": "networks (Bach, 2017). Although these works advanced the understanding of convexity in infinite dimensionalNNs, they also highlight the computational challenges inherent in training infinite dimensional models,including solving a finite dimensional non-convex problems to add a single neuron (Bach, 2017). On theother hand, it has been noted that the activation patterns of deep ReLU networks exhibit a structured yetpoorly understood simplicity. The work Hanin and Rolnick (2019) highlighted that the actual number ofactivation regions a ReLU network learns in practice is significantly smaller than the theoretical maximum. Previous studies (Fisher and Jerome, 1975; Mammen and Van De Geer, 1997) have investigated splines inrelation to 1 extremal problems, demonstrating the adaptability of spline models to local data characteristics.More recent work (Balestriero et al., 2018) connected deep networks to spline theory, showing that many deeplearning architectures could be interpreted as max-affine spline operators. Approximation properties of ReLUand squared ReLU networks with regularization were studied in Klusowski and Barron (2018). The authorsin Savarese et al. (2019) considered infinitely wide univariate ReLU networks and showed that the minimumsquared 2 norm fit is given by a linear spline interpolation. Another line of work (Parhi and Nowak, 2021;2022) developed a variational framework to analyze functions learned by deep ReLU networks, revealingthat these functions belong to a space similar to classical bounded variation-type spaces. In a similar spirit,connections to kernel Banach spaces via representer theorems were developed in Bartolucci et al. (2023).The work Unser (2019) introduced a general representer theorem that connects deep learning with splinesand sparsity. In contrast, our results provide an independent and novel perspective on the optimal weightsof a deep neural networks through geometric algebra, and may shed light into the spline theory of deepnetworks. In particular, the relation between linear splines and one-dimensional ReLU networks discoveredin Savarese et al. (2019) is generalized to arbitrary dimension and depth in our work. The key ingredient inour analysis is the use of wedge products, which are not present in any work analyzing neural networks tothe best of our knowledge. The relationship between neural networks and geometric structures has been another area of research focus.Convex optimization and convex geometry viewpoint of neural networks has been extensively studied inrecent work (Pilanci and Ergen, 2020; Ergen and Pilanci, 2021a; Bartan and Pilanci, 2021a; Ergen andPilanci, 2021b;c; Wang and Pilanci, 2022; Wang et al., 2021; Ergen et al., 2022a; Lacotte and Pilanci, 2020;Bartan and Pilanci, 2021b).However, previous works have focused mostly on computational aspects ofconvex reformulations. This work provides an entirely new set of convex reformulations, which are differentfrom the ones in the literature. Two main advantages of our approach are that our results hold for arbitrarydepth and dimension, and that they provide a geometric interpretation and novel closed-form formulas,which can be used to polish any existing deep neural network.",
  "Summary of results": "The work in this paper diverges from other approaches by using Cliffords geometric algebra and convexanalysis to characterize the structure of optimal neural network weights. We show that the optimal weightsfor deep ReLU neural networks can be found via the closed-form formula, xj1 . . .xjk, known as a k-blade ingeometric algebra, with signifying the wedge product. For each individual neuron, this expression involvesa special subset of training samples indexed by (j1, . . . , jk), which may vary across neurons. Surprisingly,",
  "The Unexpected Neuron Functionality": "The conventional belief suggests that artificial neurons optimize their response by aligning with the relevantinput samples (Carter et al., 2019), a notion inspired by the direction-sensitive neurons observed within thevisual cortex (Hubel and Wiesel, 1968). However, this interpretation hits a dead end in large DNNs, withnumerous neurons responding to unrelated features, making it nearly impossible to understand the specificrole of individual neurons (OMahony et al., 2023). Contrary to this conventional wisdom, our findings revealthat ReLU neurons are, in fact, orthogonal to a specific set of data points, due to the properties of the wedgeproduct. As a consequence, these neurons yield a null output for this distinct subset of training samples,diverging completely from the anticipated alignment. This outcome underscores a nuanced understanding;rather than merely aligning with input samples, the neurons assess the oriented distance relative to theaffine hull encapsulated by the special subset of training samples. This concept is visually explained in (a-b), where ReLU activation in 1D and 2D is interpreted as an oriented distance function. Theoptimal breaklines of the ReLU neurons, i.e., {x : wT x + b = 0}, intersect with a select group of specialtraining samples, expressed using a simple equation involving wedge products, leading to a zero outputfrom the neurons for these instances, as illustrated in (c). This result challenges the traditionalinterpretations of the role of hidden layers in DNNs, and provides a fresh perspective on the inner workingsof deep neural networks. We show for the first time that geometric algebra provides the right set of conceptsand tools to work with such oriented distances, enabling the transformation of the problem into a simpleconvex formulation.",
  "Decoding DNNs with Geometric Algebra": "Our results show that within a deep neural network, when an input sample x is multiplied with a trainedneuron, it yields the product xT (xj1 . . . xjk).Leveraging geometric algebra, this product can beshown to be equal to the signed distance between x and the linear span of the point set xj1, . . . , xjk, scaledby the length of the neuron. This allows the neurons to measure the oriented distance between the inputsample and the affine hull of the special subset of samples (see for an illustration).RectifiedLinear Unit (ReLU) activations transform negative distances, representing inverse orientations, to zero.When this operation is extended across a collection of neurons within a layer, the layers output effectivelytranslates the input sample into a coordinate system defined by the affine hulls of a special subset of trainingsamples. Consequently, each layer is fundamentally tied solely to a specific subset of the training data. Thissubset can be identified by examining the weights of a trained network. Furthermore, with access to thesetraining samples, the entirety of the network weights can be reconstructed using the wedge product formula.Moreover, when this operation is repeated through additional ReLU layers, our analysis reveals a geometricregularity within the space partitioning of the network, highlighting a consistent pattern of translations andinteractions with dual vectors (see ). This geometric elucidation sheds fresh light on the mysteriousroles played by the hidden layers in DNNs.",
  "j1": "jk. Given a matrix K Rnp, an ordinary index i [n], and a multi-index j =(j1, ..., jk) where ji [di] i [k], the notation Kij denotes the (i, v(j1, ..., jk))-th entry of this matrix,where v represents the function that maps indices (j1, ..., jk) to a one-dimensional index according to acolumn-major ordering. Formally, we can define v(j1, ..., fd) := (j1 1) + (j2 1)d1 + (j3 1)d1d2 + ... +(jn 1)d1d2...dk1. We allow the use of multi-index and ordinary indices together, e.g.,",
  "Published in Transactions on Machine Learning Research (10/2024)": "We show that :=W (1), W (2), W (), W (+1) , W (L1), W (L), which equals optimal in (24)except the -th and (+1)-th layer weights are replaced with an optimal solution of (82), is an optimal solutionof (24). As in the above case, the outputs of the network, f(X) and f(X), are identical by construction.",
  "j=1W (1)j2p + W (2)j2p,(2)": "where X Rnd is the training data matrix, W (1) Rdm, W (2) Rmc and b(1) Rm, b(2) R aretrainable weights, (, y) is a convex loss function, y Rn is a vector containing the training labels, and > 0 is the regularization parameter. Here we use the p-norm in the regularization of the weights. Initially,we will assume an output dimension of c = 1, and we will extend this to arbitrary values of c. Typicalloss functions used in practice include squared loss, logistic loss, cross-entropy loss and hinge loss, which areconvex functions.",
  "j=1(XW (1)j+ 1nb(1)j )W (2)j+ 1nb(2),(4)": "where 1n is a vector of ones of length n. When b is set to zero, we refer to the neurons in the first layeras the bias-free neurons. When b is not set to zero, we refer to the neurons in the first layer as the biasedneurons. Note that the bias terms are excluded from the regularization.",
  "Augmented data matrix": "In order to simplify our expressions for the case of p = 1, we augment the set of n training data vectors ofdimension d by including d additional vectors from the standard basis of Rd and let n = n + d. Specifically,we define the augmented data samples as {xi}n+di=1 = {xi}ni=1 {ei}di=1, where xi Rd represents the originaltraining data points for 1 i n and ei is the i-th standard basis vector in Rd for i n. We defineX = [x1, , xn+d]T .",
  "Geometric Algebra": "Cliffords Geometric Algebra (GA) is a mathematical framework that extends the classical vector and linearalgebra and provides a unified language for expressing geometric constructions and ideas (Artin, 2016). GAhas found applications in classical and relativistic physics, quantum mechanics, electromagnetics, computergraphics, robotics and numerous other fields (Doran and Lasenby, 2003; Dorst et al., 2012). GA enablesencoding geometric transformations in a form that is highly intuitive and convenient. More importantly, GAunifies several mathematical concepts, including complex numbers, quaternions, and tensors and provides apowerful toolset. We consider GA over a d-dimensional Euclidean space, denoted as Gd. The fundamental object in Gd is themultivector, M = M0 + M1 + . . . + Md, which is a sum of vectors, bivectors, trivectors, and so forth.Here, Mk denotes the k-vector part of M. For instance, in the two-dimensional space G2, a multivectorcan be written as M = a + v + B, where a is a scalar, v is a vector, and B is a bivector. In this case, thebasis elements are not just the canonical vectors e1, e2 but also the grade-2 element e1e2. A key operation in GA is the geometric product, denoted by juxtaposition of operands: ab. For vectors aand b, the geometric product can be expressed as ab = a b + a b, where denotes the dot product and denotes the wedge (or outer) product. The dot product a b is a scalar representing the projection of a onto b. The wedge product a b is abivector representing the oriented area spanned by a and b.In the geometric algebra Gd, higher-gradeentities (trivectors, 4-vectors, etc.)can be constructed by taking the wedge product of a vector with abivector, a trivector, and so on. A k-blade is a k-vector that can be expressed as the wedge product of kvectors. For example, the bivector a b is a 2-blade.",
  "2a b + b c + c a,": "which is a consequence of the distributive property of the wedge product. Therefore, the signed area ofthe triangle is half the sum of the wedge products for each adjacent pair in the counter-clockwise sequencea b c a encircling the triangle. In higher dimensions, the scalar part of the wedge product representssigned the volume of a parallelotope spanned by the vectors (see ). The metric signature in Gd over the Euclidean space is characterized by all positive signs, indicating thatall unit basis vectors are mutually orthogonal and have unit Euclidean norm.Let us call the standard(Kronecker) basis in d-dimensional Euclidean space e1, . . . , ed, which satisfies e2i = 1 i and eiej = ejei i =j. The wedge product I e1 ed = e1 . . . ed represents the highest grade element and is defined to bethe unit pseudoscalar. The inverse of I is defined as I1 = ed . . . e1, and satisfies I1I = 1. Squaring I, weobtain I2 = (e1e2)2 = e1e2e1e2 = 1, analogous to the unit imaginary scalar in complex numbers, which isa subalgebra of G2. The 2 norm of a multivector is defined as the square root of the sum of the squares of the scalar coefficientsof its basis k-vectors. For a multivector M it holds that M22 = M M0, where M is the reversionof M. M is analogous to complex conjugation and is defined by three properties: (i) (MN) = N M ,(ii) (M + N) = M + N , (iii) M = M when M is a vector. For instance, (e1e2e3e4) = e4e3e2e1 and(e1 + e2e3) = e1 + e3e2. We define the 1 norm of a multivector as the sum of the absolute values ofeach component. The definition of inner and wedge products can be naturally extended to multivectors. Inparticular, the inner-product between two k-vectors M = 1 k and N = 1 k is defined by theGram determinant M N det(i, jki,j=1).",
  "M N = (M N) e1 ed = (M N) I": "We may also express the Hodge dual of a k-vector M as M = MI1, where I1 is the inverse of the unitpseudoscalar. This linear transformation from d-vectors to d k vectors defined by M M is the Hodgestar operator. An example in G3 is e1 = e1I1 = e1e3e2e1 = e3e2.",
  "Generalized cross product and wedge products": "The usual cross product of two vectors is only defined in R3.However, the wedge product can be usedto define a generalized cross product in any dimension. The generalized cross product in higher dimensionsis an operation that takes in d 1 vectors in an Rd and outputs a vector that is orthogonal to all of thesevectors. The generalized cross product of the vectors v1, v2, . . . , vn1 can be defined via the Hodge dual oftheir wedge product as (v1, v2, . . . , vn1) (v1 v2 . . .vn1).. It holds that (v1, v2, . . . , vn1)vi = 0for all i = 1, . . . , n 1.The signed distance of a vector x to the linear span of a collection of vectorsx1, ..., xd1 can be expressed via the generalized cross product and wedge products as",
  "x1 xd12": "This formulation stems from an intuitive geometric principle: the ratio of the volume of a parallelotopeP(x, x1, . . . , xd1), which is spanned by the vectors x, x1, . . . , xd1, to the volume of its base P(x1, . . . , xd1),the parallelotope formed by x1, . . . , xd1 alone. This ratio effectively captures the height of the parallelotoperelative to its base, which corresponds to the distance of x from the subspace spanned by x1, . . . , xd1. Notethat the Hodge dual transforms the d-vector in the numerator into a scalar. We provide a subset of otherimportant properties of the generalized cross product in .1 of the Appendix.",
  "p d maxvRn (v, y)s.t.|vT (Xw)| , w Bdp,(5)": "where we take the network to be bias-free (see .2 for biased neurons). Here, (, y) is the convexconjugate of the loss function (, y) defined as (v, y) supqRn vT q (q, y) and Bdp is the unit p-normball in Rd. Moreover, it was shown in Pilanci and Ergen (2020) that when is the ReLU activation, strongduality holds, i.e., p = d, when the number of neurons m exceeds a critical threshold. The value of thisthreshold can be determined from an optimal solution of the dual problem in (5). This result was extendedto deeper ReLU networks in Ergen and Pilanci (2021c) and to deep threshold activation networks in Ergenet al. (2022b).",
  "+, yielding the positive part of the signed length of the interval [xi, xj]": "Appending 1 to the vectors is due to the presence of bias in the neurons. This quantity can also be seen as adirectional distance we denote as dist+(xi, xj), which will be generalized to higher dimensions in the sequel.As we delve into higher dimensional NN problems, the above wedge product expression will be substituted bythe positive part of the signed volume of higher dimensional simplices such as triangles and parallelograms.Remark. This result is a refinement of the linear spline characterization of one-dimensional infinitely wideReLU NNs (Savarese et al., 2019; Parhi and Nowak, 2020). The linear spline dictionary is given by thecollection of ramp functions {(x xj)+, (xj x)+}nj=1, which is well-known in adaptive regression splines(Friedman, 1991). Our work is the first to recognize this dictionary via wedge products, characterize it asa finite dimensional Lasso problem, and associate it to volume forms. This enables us to generalize theresult to higher dimensions and arbitrarily deep ReLU networks. It is important to note that unlike theexisting literature on infinite neural networks (Bach, 2017; Bengio et al., 2005), our characterization of thedictionaries is discrete rather than continuous, enabling standard convex programming. An important feature of the optimal network in (8) is that the break points are located only at the trainingdata points.In other words, the prediction f(x) is a piecewise linear function whose slope only may changeat the training points with at most n breakpoints. However, since the optimal z is sparse, the number ofpieces is at most z0, and can be smaller than n. We note that convex programs for deep networks trained for one-dimensional data were considered in Ergenet al. (2022b); Zeger et al. (2024). In Ergen and Pilanci (2021a), it was shown that the optimal solutionto (3) may not be unique and may contain break points at locations other than the training data points.However, Theorem 1 reveals that at least one optimal solution is in the form of (8).In addition, it is shownthat the solution is unique when the bias terms are regularized (Boursier and Flammarion, 2023) and a skipconnection is included. We discuss uniqueness in .",
  ": Illustration of the matrix K for neural networks without bias (left) and with bias (right) via thetriangular area defined in the convex program from Theorem 2 in R2": "will observe that the 1 and 2-regularized problems exhibit certain differences from one another. We startwith the 1-regularized problem p = 1, since the form of the convex program is simpler to state due to thepolyhedral nature of the 1 norm. In the case of p = 2, we require a mild regularity condition on the datasetto handle the curvature of the 2 norm.",
  "j=1zj (x, xj),": "where z is an optimal solution to (9). The optimal first layer neurons are given by a scalar multiple of thegeneralized cross product, xj = xj, with breaklines x xj = 0, corresponding to non-zero zj for j [n]. Remark. The optimal hidden neurons given by the generalized cross products, xj, are orthogonal to thetraining data points xj for j = 1, ..., n. Therefore, the breaklines of each ReLU neuron pass through theorigin and some data point xj. Note that (x xj)+ = (x1xj2 x2xj1)+ is a ReLU ridge function.",
  "We provide an illustration the matrix K in . The signed area of the triangle formed by the path0 xi xj, denoted by the wedge product 1": "2xi xj is positive when this path is ordered counterclockwiseand negative otherwise. In this figure, the positive part of this signed area given by Vol+((0, xi, xj)) =12(xi xj)+ is non-zero only when xi is to the right of the line passing through the origin and xj. When biasterms are added to the neurons, the matrix K changes as shown in the right panel of as shown inTheorem 3 (see also .2 in the Supplementary Material).",
  "j=(j1,j2)zj (x, xj1, xj2),": "where z is an optimal solution to (13). The optimal hidden neurons and biases are given by a scalar multipleof (xj1 xj2), and (xj2 (xj1 xj2)) respectively, with breaklines (xxj2)(xj1 xj2) = 0 for j = (j1, j2)corresponding to non-zero zj . Remark. We note that the form of the near-optimal NN for p = 2 is near identical to the p = 1 case, forwhich the result is exact. This discrepancy is due to the polyhedral nature of the dual problem with 1regularization (see of Appendix II). In , we present numerical evidence that the decisionregions of optimal NNs with p = 1 and p = 2 are near identical for small values of .",
  "Arbitrary dimensions": "Now we consider the generic case where d and n are arbitrary and we use the d-dimensional geometric algebraGd. Suppose that X Rnd is a training data matrix such that rank(X) = d without loss of generality.Otherwise, we can reduce the dimension of the problem to rank(X) using Singular Value Decomposition(see Lemma 23 in the Supplementary Material), hence d can be regarded as the rank of the data. Sincemany datasets encountered in machine learning problems are close to low rank, this method can be used toreduce the number of variables in the convex programs we will introduce in this section.",
  "j=(j1,...,jd1)zj (x, xj1, ..., xjd1),": "where z is an optimal solution to (14). The optimal neurons are given by a scalar multiple of the generalizedcross-product (xj1, , xjd1) = (xj1 xjd1), with breaklines x xj1 xjd1 = 0, correspondingto non-zero zj for j = (j1, , jd1). We recall that P(x, u1, ..., ud1) denotes the parallelotope formed by the vectors x, u1, ..., ud1, and thepositive part of the signed volume of this parallelotope is given by Vol+(P(x, u1, ..., ud1)).Remark. The optimal neurons are orthogonal to d 1 data points, i.e., (xj1, , xjd1) xi = 0 for alli {j1, , jd1}. Therefore, the hidden ReLU neuron is activated on a halfspace defined by the hyperplanethat passes through data points xj1, , xjd1. The proof of this theorem can be found in .2 of the Supplementary Material.Remark. We note that the combinations can be taken over d 1 linearly independent rows of X sinceotherwise the volume is zero and corresponding weights can be set to zero. Moreover, the permutationsof the indices xj1, ..., xjd1 may only change the sign of the volume Vol(P(xi, xj1, ..., xjd1)). Therefore,it is sufficient to consider each subset that contain d 1 linearly independent data points and computeVol+(P(xi, xj1, ..., xjd1)) for each subset. The cost of enumerating over all size d subsets is O(nd).",
  "We call a dataset -dispersed if D(X) , and locally -dispersed when the dataset centered at any trainingsample is dispersed, i.e., D(X 1xTj ) j [n]": "Remark. The quantity D(X) is a generalization of the 2D range dispersion in Definition 2 to arbitrarydimensions, and captures the diversity of the ranges of the hyperplanes whose normals are training points{xi}ni=1. We prove in .5 that when the data is randomly generated, e.g., i.i.d. from a Gaussiandistribution, the maximum chamber diameter D(X) is bounded by ( d",
  "(u1 ud) ... (ud1 ud)2= dist+x, Aff(u1, ..., d)": "and the multi-index j = (j1, ..., jd1) is indexing over all combinations of d 1 rows xj1, ..., xjd1 Rd ofX Rnd. When the maximum chamber diameter satisfies D(X) for bias-free neurons or D(X 1xTj ) j [n] for biased neurons, for some (0, 1), we have the following approximation bounds",
  ": Wedge product representation of distance to affine hull (a) and triangular area (b)": "Theorem 4 for p = 1 implies that an optimal neuron corresponding to a sample x is given by the dualx := xI1. We recall that I = e1e2 . . . ed = e1e2 when d = 2, and I1 = e2e1. Hence, we have x = xe2e1in G2. Next, recall that for p = 1, we augment the training set with two standard basis vectors of R2 andobtain {xi}4i=1 = {e1 e2, 2e1, e1, e2}. For each training sample, we calculate these neurons as follows",
  "Vol((x, xj1, xj2)) = (x xj2) (xj1 xj2) = x xj1 x xj2 xj2 xj1 xj2 xj20= x xj1 + xj1 xj2 + xj2 x": "In other words, the volume of the triangle formed by vertices x, xj1, xj2 can be expanded as the wedgeproduct of consecutive pairs of vectors taken over the loop x xj1 xj2 x. This is illustrated in a. Let us now illustrate the role of regularization. For the p-regularized network with p = 2, Theorem 3implies that we replace the volume terms with",
  "Data isometry and chamber diameter": "Results from high dimensional probability can be used to establish bounds on the chamber diameter ofa hyperplane arrangement generated by a random collection of training points.A similar analysis wasconsidered in Plan and Vershynin (2014) for the purpose of dimension reduction. We first show that thechamber diameter is small when the training dataset satisfies an isometry condition.Lemma 6 (Isometry implies small chamber diameter). Suppose that the following condition holds",
  "where R is fixed. Then, the chamber diameter D(X) defined in (16) is bounded by 4": "Next, we show that the isometry condition is satisfied when the training dataset is generated from a randomdistribution. Consequently, we obtain a bound on the chamber diameter of the hyperplane arrangementgenerated by the random dataset.Lemma 7 (Random datasets have small chamber diameter). Suppose that x1, , xn N(0, Id) are nrandom vectors sampled from the standard d-dimensional multivariate normal distribution and let X =[x1, , xn]T . Then, the 2 diameter D(X) satisfies",
  "/4,(21)": "with probability at least 1 2ed/2.Remark. We note that the constant 9 in the above lemma can be improved to 1 by using the more re-fined analysis due to Gordon (Gordon, 1985). Moreover, the result can be extended to sub-Gaussian datadistributions. However, we use Lemma 6 since it is simpler and suffices for our purposes.Lemma 8 (Random datasets are dispersed and locally dispersed). Suppose that x1, , xn N(0, Id)are n random vectors sampled from the standard d-dimensional multivariate normal distribution and letX = [x1, , xn]T . Suppose that n 4d. Then, the dataset X is -dispersed, i.e., D(X) and locally-dispersed, i.e., D(X 1xTj ) j [n] with high probability.",
  "Dvoretzkys Theorem": "In this section, we present a connection to Dvoretzkys theorem (Dvoretzky, 1959), a fundamental result infunctional analysis and high-dimensional convex geometry (Vershynin, 2011).Theorem 9 (Dvoretzkys Theorem). (Geometric version) Let C be a symmetric convex body in Rn. Forany > 0, there exists an intersection CS C S of C by a subspace S Rn of dimension k(n, ) asn such that(1 )B2 CS (1 + )B2where B2 is the n-dimensional Euclidean unit ball. The above shows that there exists a k-dimensional linear subspace such that the intersection of the convexbody C with this subspace is approximately spherical; that is, it is contained in a ball of radius 1 + andcontains a ball of radius 1 . If we represent the linear subspace in Theorem 9 via the range of the matrix X and let C be the 1 ball,it is straightforward to show that Dvoretzkys theorem reduces to the isometry condition (6) up to a scalarnormalization. Therefore, the isometry condition (6) can be interpreted as a condition to guarantee that the1 ball is near-spherical when restricted to the range of the training data matrix.",
  "j=1W (21)j2p + W (2)j2p ,(22)": "where X Rnd is the training data matrix, y Rn is a vector containing the training labels, l and > 0is the regularization parameter. Here, f(X) represents the output of the deep NN over the training datamatrix X given by f(X) = [f(x1) f(xn)]T . Note that the p norms in the regularization terms aretaken over the columns of odd layer weight matrices and rows of even weight matrices, which is consistentwith the two-layer network objective in (2). When p = 2, the regularization term is the squared Frobeniusnorm of the weight matrices which reduces to the standard weight decay regularization term. We assumethat the number of neurons in each layer is sufficiently large to meet the conditions required for applyingTheorem 4 to two consecutive layers.",
  "j()d1) ,for = 2, 3, ..., L ,(25)": "where ()jare scalar weights, x()i (W (1) (W (1)xi) ) and j()k [n] are certain indices. Theabove weights provide the same loss as the optimal solution of (24). Moreover, the regularization term isonly a factor 2/(1) larger than the optimal regularization term, where is an uppper-bound on the chamberdiameters D(X) for = 0, ..., L 2. Here, X = ( (XW (1)) W (1)) are the -th layer activationsof the network given by the weights (25).",
  "(xT (xj1 . . . xjk))+ = dist+x, Span(xj1, ..., xjd1)": "The ReLU activation serves as a crucial orientation determinant in this context. By nullifying negativesigned distances, it effectively establishes a directionality in the space. Geometrically speaking, it delineatesthe specific side of the affine hull relevant for a particular input sample. In intermediate layers, the formulais applied to the activations of the previous layer, which are themselves signed distances to affine hulls ofsubsets of training data.",
  "dist+x, Span(xj(1)1 , ..., xj(1)k ), . . . , dist+x, Span(xj(m)1, ..., xj(m)k)": "Moreover, the information encapsulated within the weights of the network can be succinctly representedby the indices j(1)1 , . . . , j(1)k , . . . , j(m)1, . . . , j(m)k.These indices highlight the pivotal training samples thateffectively determine the geometric orientation of each neuron. The formula essentially implies that the deepneural networks behavior and decisions are intrinsically tied to specific subsets of the training data, denotedby these critical indices. This interpretation not only offers a geometric perspective on neural networks but also explains the pivotalrole hidden layers play. Each hidden layer is a series of coordinate transformations, represented by the affinehulls of various data point subsets. As the data progresses through the network, it gets transformed andre-encoded, with every neuron contributing to this transformation based on its unique geometric connectionto the training dataset.",
  "for a certain set of i, j, [n], k [d]": "Remark. In addition to the optimal neurons for the two-layer case (Theorem 16), here we obtain additionalneurons whose breaklines are translations of the affine hull of a subset of the data points to certain otherdata points. See (d) for an illustration.Remark. We note that the optimization problem and optimal networks take a similar form as in Theorem12, except that the xi are replaced by xi x and the bias term b = xT w is added.",
  "Space partitioning of optimal deep networks": "We now illustrate the optimal two-layer neurons predicted by Theorems 4 and compare them with optimalthree-layer neurons (see Theorems 13-12 in the .6) as regularization tends to zero for p = 1 unlessstated otherwise. Consider the two-dimensional training data {x1 = (1, 0), x2 = (0, 1), x3 = (1, 0), x4 =(0, 1)} shown in . In panel (a), we consider a two-layer ReLU network without biases.Two optimal neurons are (wT1 x)+,(wT2 x)+, given by Theorem 2. Their breaklines, wT1 x = 0 and wT2 x = 0, are plotted as blue lines, and passthrough the origin and data points, since the optimal neurons are scalar multiples of the Hodge duals of1-blades formed by data points. In panel (b), we consider a three-layer ReLU network without biases, and we display all four optimal firstlayer neurons given by Theorem 12. In addition to the neurons with breaklines wT1 x = 0 and wT2 x = 0, wealso have wT3 x = 0 and wT4 x = 0 which are translations of the affine hulls, Aff(x1, x2) and Aff(x2, x3), tothe origin. In panel (c), we consider a two-layer ReLU network with biases regularized with p = 2, and we display all sixoptimal neurons given by Theorem 3. Their breaklines pass between each pair of samples, since the optimalneurons are scalar multiples of the Hodge duals of 1-blades formed by the differences of data points. In panel (d), we consider a three-layer ReLU network with biases, and we display all 12 optimal first layerneurons given by Theorem 13. In addition to the breaklines that pass between each pair of samples, we alsohave translations of all possible affine combinations of size two, e.g., Aff(x1, x2), Aff(x1, x3),..., to everydata point.",
  "Refining neural network weights via geometric algebra": "We apply the characterization of Theorem 4, which states that the hidden neurons are scalar multiplesof (xj1 xjd1), Additionally, they are orthogonal to the r 1 training data points specified byxj1, , xjd1, where r represents the rank of the training data matrix. The inherent challenge lies in identifying the specific subset of the r 1 training points needed to formeach neuron. Fortunately, this subset can be estimated when we have access to approximate neuron weights,typically acquired using standard non-convex heuristics such as stochastic gradient descent (SGD) or variantssuch as Adam and AdamW (Kingma and Ba, 2014; Loshchilov and Hutter, 2018).After obtaining anapproximate weight vector for each neuron, we can gauge which subsets of training data are nearly orthogonalto the neuron. This is achieved by evaluating the inner-products between the neuron weight and all trainingvectors, subsequently selecting the r 1 entries of the smallest magnitude. This refinement, which we termthe polishing process, is delineated as follows for each neuron w1, ..., wm:For each j [m] (optional: Append 1 to the training samples to account for the neuron bias term)",
  ". Identify the r 1 training vectors with the minimal inner-product magnitude, denoted asxj1, , xjd1": "3. Update the neuron using: wj (xj1 xjd1) = (xj1, . . . , xjd1). As a result, we havewj xj1, . . . , xjd1. This can be done by solving the linear system wTj xji = 0 for i = 1, ..., r 1, orfinding a minimal left singular vector of the matrix [xj1, . . . , xjd1], and normalizing wj such thatwjp = 1.",
  "Computational complexity of polishing": "For a dataset of n samples of dimension d, the cost of applying the polishing process to a layer of m neurons isgiven by O(mnd)+O(md), where is the matrix multiplication exponent, e.g., = 3 using classical solvers(Gloub and Van Loan, 1996) and 2.376 using fast matrix multiplication. We note that the latter classof algorithms are not practical for realistic sizes. However, O(md) can be reduced to O(md2 log(1/))for -approximate solutions of the linear system, where is the condition number. The computational costis dominated by the calculation of inner products (O(mnd)) for large n and the d d linear system solve(O(md)) for large d.",
  "Toy spiral dataset": "for the 2D spiral dataset and a two-layer neural network optimized with squared loss. In the initialpanel of this figure, the training curve of a two-layer ReLU neural network from (2) is depicted, consideringp = 2 and weight decay regularization set at = 105. The dataset, divided into two classes representedby blue and red crosses, is showcased in the second panel. By resorting to the dual formulation in (5), theglobal optimum value is computed. Notably, while SGD is far from the global optimum, the polishing processenhances the neurons, leading to a marked improvement in the objective valueevidenced by the solid linein the left panel. A comparative visualization of the decision region pre and post-polishing is presented inthe subsequent panels, highlighting the enhanced data distribution fit due to the polishing.",
  "To illustrate the polishing strategy, we present three examples in Figures 9 and": "In (a), we investigate binary image classification on the CIFAR dataset (Krizhevsky and Hinton,2009).A four-layer convolutional network composed of two convolutional layers with 3 3 32 filtersand two fully connected layers with 512 hidden neurons is trained to distinguish class 0 (airplane) fromclass 2 (bird). We train it via AdamW optimizer using default hyperparameters and varying learning rateusing 20 epochs and a batch size of 2048. After training, we implement the proposed polishing processon the first layer weights. Next, we re-train the second-layer weights while first layer weights are fixed viaconvex optimization. The resulting average train/validation accuracies are plotted over 5 independent trialsto account for the randomness in optimization. We observe that the polishing process improves both thetraining and test accuracy. In .1.1, we provide additional results with different hyperparameters. In (b), repeat the same polishing strategy for a small character-based autoregressive language model.We train a two-layer ReLU network to predict the next character in a sequence of characters from a smallsubset of Wikipedia consisting of first 650000 characters from the article titled Neural network (machinelearning) and other articles linked from the same page. We use the AdamW optimizer with a learning rateof 104 and a batch size of 8192. The block size is set to 16 characters. We apply polishing to the firstlayer weights. Next, we re-optimize the final layer weights while the first layer weights are fixed via convexoptimization. The resulting average train/validation accuracies, along with 1-standard deviation error bars,are plotted over 8 independent trials to account for the randomness in optimization. We observe a significantimprovement in perplexity after polishing when the number of neurons is large enough. In .1.5, weprovide additional results with different hyperparameters.",
  ": AdamW and polishing via geometric algebra in the Boston Housing dataset": "In , we demonstrate the polishing strategy applied within a tabular learning context using theBoston Housing dataset. This dataset consists of 506 samples, each with 13 features representing variousattributes of housing in Boston. We use a two-layer neural network with varying number of hidden neuronsin the first layer, trained to predict the median value of owner-occupied homes. The network is trainedusing the AdamW optimizer with a learning rate of 102 and a batch size of 16 over 100 epochs. Aftertraining, we apply the proposed polishing process to the first layer, followed by re-optimizing the second-layer weights. The resulting train and validation MSE as well as one standard devation error bars are plottedover 100 independent trials to account for the randomness in optimization. As shown in (a), thepolishing process consistently results in improved performance when the number of neurons are sufficientlylarge, similar to previous cases, demonstrating its robustness across different types of datasets. In (b), we present the distribution of the magnitude of inner-products between the weight vectors of theAdamW-trained first layer before polishing. This plot shows that many inner products are small, on theorder of 102 to 103, when the vectors are normalized to have a unit Euclidean norm. In in.6 of the Appendix, we present additional plots for traditional methods, showing that they performworse compared to the polishing process. The work Adlam et al. (2020) reports the validation accuracy ofkernel ridge regression, NN ensembles and Bayesian NNs, which all underperform compared to our approach.In the Supplementary Material (.1), we provide a detailed analysis of the effect of changing thehyperparameters and optimizers, including the learning rate, momentum parameters (1 and 2 in Adam andAdamW), batch sizes, number of epochs, and also present additional results with fully connected networksand other binary classification tasks. We observe that the polishing process consistently improves the qualityof the weights, leading to a significant improvement in the accuracy of the network while making the neuronsfully interpretable as oriented distance functions via geometric algebra.",
  "Comparison of 2 and 1 regularized neural networks": "In this section we compare the predictions of optimal neural networks with p-regularized neurons, specificallyfocusing on p = 2 and p = 1.In , we compare the optimal decision boundaries of 1 and 2regularized NNs on the XOR dataset with n = 4 samples by solving the dual convex problem in (6). Itcan be seen that both models yield the same decision region. Moreover, two distinct breaklines of 4 optimalneurons are plotted. These can be identified as the affine hulls of a subset of training points.",
  "Discussion": "In this work, we have presented an analysis that uncovers a deep connection between Cliffords geometricalgebra and optimal ReLU neural networks. By demonstrating that optimal weights of such networks areintrinsically tied to the wedge product of training samples, our results enable an understanding of how neuralnetworks build representations as explicit functions of the training data points. Moreover, these closed-formfunctions not only provide a theoretical lens to understand neural networks, but also has the potential toguide new architectures and training algorithms that directly harness these geometric insights.",
  "Computational complexity of global optimization": "The computational complexity of the polishing process is dominated by step 3, which involves solving alinear system or finding a minimal left singular vector. This can be done in O(n2r) time using the QRdecomposition or the SVD. This process is repeated for each neuron, resulting in a total complexity ofO(n2rm), where m is the number of neurons. In contrast, the complexity of training the neural network toglobal optimal using the convex programs derived in Theorems 4 is O(nrn2), which is tractable for smallr. Note that for convolutional neural networks, the rank is bounded by the spatial size of the filter, whichis a small constant (Ergen and Pilanci, 2024). Another application where the data is inherently low rankis Neural Radiance Fields (Mildenhall et al., 2021). The exponential complexity in r can not be improvedunless P = NP (Pilanci and Ergen, 2020; Wang and Pilanci, 2023). However, the convex programs canbe well-approximated by sampling the wedge products, in a similar manner to the randomized samplingemployed in convex formulations of NNs (Ergen and Pilanci, 2024; 2023; Mishkin et al., 2022). Recent workshowed that random sampling of polynomially many variables in the convex program (5) provides a strongapproximation with only logarithmic gap to the global optimum (Kim and Pilanci, 2024). Another work",
  "Interpretability": "Our findings also contribute to the broader challenge of neural network interpretability.The polish-ing process is expected to improve the quality of the weights, leading to a significant improvement inthe accuracy of the network while making the neurons fully interpretable as oriented distance functionsvia geometric algebra.More precisely, after polishing each ReLU neuron precisely outputs (xTi w)+ =dist+xi, Span(xj1, ..., xjd1). This representation is similar to that of Support Vector Machines, wherethe model is defined by a weighted combination of training samples. By elucidating the roles hidden layersplay in encoding geometric information of training data points through signed volumes, we have taken a steptowards a more transparent and foundational theory of deep learning.",
  "Uniqueness": "We note that the optimal weights of a ReLU neural network are not unique, and permutation, mergingand splitting operations on the neurons can lead to equivalent networks.However, all globally optimalsolutions can be recovered via the set of optimal solutions of the convex program (5) by considering thesethree operations (Mishkin and Pilanci, 2023; Wang et al., 2021). Moreover, under certain assumptions, theconvex program for univariate data admits a unique solution (Boursier and Flammarion, 2023). In addition,all stationary points of the non-convex training objective can be recovered via the convex program whencertain variables are constrained to be zero (Wang et al., 2021), up to permutation, merging and splitting.An important open question is characterizig the entire optimal set of the convex programs via geometricalgebra, which we leave as future work.",
  "Other architectures": "Our findings can be extended to several other widely used network architectures. For instance, convolutionalneural networks can be transformed into fully connected networks by reshuffling the data matrix, as shownin Ergen and Pilanci (2021d). Some of our results can be directly applied by redefining the training datavectors. This can be extended to deep CNNs with short receptive fields (Brendel and Bethge, 2019). Forother generic CNNs, the results can be extended by employing the same approach. Additionally, we believethat our results can be extended to transformer architectures employing linear or ReLU attention, as convexformulations of these networks have been analyzed in Sahiner et al. (2022). Further results for various otherneural network architectures are provided in of the Appendix. There are many other open questions for further research. Exploring how these insights apply to state-of-the-art network architectures, or in the context of different regularization techniques and variations of activationfunctions, such as the ones in attention layers, could be of significant interest. While our techniques allow forthe interpretation of layer weights in popular pretrained network models, we leave this for further research.Additionally, practical implications of our results, including potential improvements to the polishing processremain to be fully explored. Our results also underlines the potential and utility of integrating geometricalgebra into the theory of deep learning.",
  "Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks via convexprograms. In International Conference on Machine Learning, pages 29933003. PMLR, 2021c": "Yifei Wang and Mert Pilanci. The convex geometry of backpropagation: Neural network gradient flowsconverge to extreme points of the dual convex program. In International Conference on Learning Repre-sentations, 2022. Yifei Wang, Jonathan Lacotte, and Mert Pilanci. The hidden convex optimization landscape of regularizedtwo-layer relu networks: an exact characterization of optimal solutions. In International Conference onLearning Representations, 2021. Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifyingbatch normalization in relu networks: Equivalent convex optimization models and implicit regularization.International Conference on Learning Representations, 2022a.",
  "Additional Numerical Results": "In this section, we provide additional experiments showcasing the effectiveness of the proposed polishingprocess in enhancing the performance of neural networks. We consider a variety of datasets and architectures,including a three-layer fully connected ReLU network, a four-layer convolutional neural network, and a two-layer ReLU network for an autoregressive character-level text prediction task. We compare the performanceof the polishing process with the standard methods such as SGD, Adam and AdamW, and experiment withall hyperparameters, including the learning rate, momentum parameters, number of epochs, and batch size.We also provide a comparison of the decision regions pre and post-polishing, highlighting the enhanced datadistribution fit due to the polishing. It can be observed that the polishing process significantly improvesthe performance of the neural networks, leading to a marked improvement in the objective value and thedecision regions, for a variety of datasets and architectures under different optimization hyperparameters.",
  "Image classification and text prediction": "We now consider the settings in and vary the optimization hyperparameters, including the momentumparameter 1 of AdamW, the number of epochs, batch sizes for the four-layer convolutional neural network trainedon the CIFAR dataset. We consider three different binary classification tasks on the CIFAR benchmark, and acharacter level text prediction task on the same subset of Wikipedia.",
  ": Comparison of regression models": "7.6. Tabular DatasetIn this section, we provide further numerical results using the Boston Housing dataset. In , wepresent the training and validation of MSE of kernel ridge regression using the Gaussian kernel and randomforests. We provide one standard deviation error bars for the random forest over the randomness of theconstruction of trees. It can be seen that the polishing process shown in provides significantly lowervalidation MSE. In addition, we observed that linear regression and decision trees underperform comparedto the kernel ridge regression.",
  "Two-dimensional two-layer networks with bias": "We now augment the dataset by adding ordinary basis vectors with training points. Let us define x(j)i= xifor i [n] j and x(j)n+k = xj + ek for j [n] k [d] where e1, ..., ed are the canonical basis vectors. Wenote that this augmentation is only needed in the case of p = 1.",
  "ivi(xiw + b)+| .(53)": "Since w is a scalar, the constraint wp 1 is equivalent to |w| 1 for all p (0, ). An optimal w satisfiesw {1, +1} since either this constraint is tight or the objective is constant with respect to w. Next,observe that an optimal b is achieved when b {xi}ni=1 or the objective goes to infinity when b . Thisis because the objective is a piecewise linear function of b. In the latter case, the objective value is infiniteas long as",
  "Proof of Theorem 2 and Theorem 4": "We now present the proof of Theorem 4, and the special case Theorem 2, which shows the equivalence of the1-regularized neural network and the Lasso problem in (14). Suppose that the training matrix is of rank k.Denote the compact Singular Value Decomposition (SVD) of X as follows X = UV T where U Rnd andV Rdd are orthonormal matrices and Rdd is a diagonal matrix with non-negative diagonal entries.Only k diagonal entries of are non-zero and the rest are zero. We denote the non-zero diagonal entries of as 1, , k and the corresponding columns of U and V as u1, , uk and v1, , vk, respectively. Consider the NN objective (2). It can be seen that the projection vTi W1j of the hidden neuron weightsj [m] does not affect the objective value for i = k +1, ..., d. Therefore, the optimal hidden weights are zeroin the subspace vk+1, ..., vd due to the norm regularization. In the remaining, we assume that the trainingmatrix X is full column rank and of size n by d = r. Otherwise, we can remove the zero singular valuesubspaces of X via the above argument.",
  "s.t. w1 1, (2Dk I)Xw 0": "We next claim that the constraint w1 1 is active at the optimum, assuming the objective value ispositive. Note that w = 0 is a feasible point, which achieves the objective value of zero. Otherwise, we canscale w to satisfy the constraint and increase the objective value. Defining the set",
  "+ and wj = (xj1, ..., xjd1)1": "Finally, in order to simplify the notation, we take the index set (j1, ..., jd1) over all subsets of {1, ..., n} of sized 1 and redefine the matrix as Kij = Vol+(P(xj1, ..., xjd1)) and wj = (xj1, ..., xjd1)1. This followsfrom the fact that the Euclidean norm of the cross product, and hence the volume of the parallelotope is zerothe whenever the subset of vectors are linearly dependent. Moreover, the index j0 {1, +1} is absorbedinto the ordering of the vectors xj1, ..., xjd1 in the parallelotope, by noting that the sign of the cross productis flipped whenever the ordering of two vectors is flipped. This completes the proof of Theorem 4.",
  "We next argue that the extreme points of C can be analyzed via the extreme points of C. In particular, itholds thatmaxwC xT w = maxwC xT (w+ w)": "for all x Rd and a maximizer w to the former maximization problem can be obtained from a maximizer(w+, w) to the latter problem by setting w = w+ w. On the other hand, a maximizer (w+, w) tothe latter problem can be obtained from a maximizer w to the former problem by setting w+ = max{w, 0}and w = max{w, 0}. Therefore, characterizing the extreme points of C is equivalent to characterizing theextreme points of C.",
  "C2(i) w : w2 1, (2Di I)Xw 0,": "and let dsub(i) maxwC2(i) vT DiXw. Note that the constraint in (5) is precisely d+sub(i) , dsub(i) i. Our strategy is to show that d+sub(i) and dsub(i) can be tightly approximated by a polyhedral approxi-mation constructed using the extreme rays of the cone {w : (2Di I)Xw 0}. Consequently, we will beable to obtain an approximation of the dual problem since the dual constraint is",
  "{w : (2Di I)Xw 0} Rd": "are R1, ..., Rk Rd for some k Z+. Note that by our assumption that X is full column rank, this cone ispointed since it does not contain any nontrivial linear subspace. Let p1, . . . , pk Sd1 denote the unit normgenerators of the extreme rays R1, ..., Rk normalized to unit Euclidean norm. We define the convex sets",
  "= d1min maxwP0 vT DkXw": "Next, we simplify the polyhedral approximation maxwP0 vT DkXw. Applying Weyls Facet Lemma (seeLemma 24) to the cone K = {w : (2Di I)Xw 0}, we see that a vector v K belongs to an extremeray of K if and only if there are d 1 linearly independent rows of the matrix (2Di I)X orthogonal tov. Then, each of the generators of the extreme rays p1, ..., pk satisfy pTi xj = 0, j Si where Si is a subsetof d 1 linearly independent rows of the matrix X. Using the properties of the generalized cross product(see 6.1), we identify each generator as a scalar multiple of the cross product jSixj of the d 1 linearly",
  "Next, note that Weyls Facet Lemma also guarantees that any vector p K of the form p =jSxj": "jSxj2is on an extreme ray of K when S is a subset of d 1 linearly independent rows of X. When d 2,the collection of vectors 0 { jS xj : S [n], dim Span{xj : j S} = d 1} is equivalent to0 {jSxj : S Pd1([n])}, where Pd1([n]) denotes all permutations of subsets of [n] with d 1elements. In order to see this equivalence, observe that the 1 multiplier can be removed when we considerpermutations since exchanging the order of the two vectors in the cross product changes the sign of theresulting vector. Moreover, we can only consider subsets S of size d 1, otherwise the vectors are linearlydependent and the cross product is zero. In addition, when the vectors are linearly dependent, the crossproduct is zero, which is already included in the collection of vectors. Using this characterization, we can simplify the first optimization problem in (61) by enumerating all unit-norm generators of the extreme rays of the cone K, which can be done by considering all subsets S of d 1linearly independent rows of X and taking the cross product of the rows in each subset. Define the set DSas follows",
  "+,(62)": "where the last maximization is over d 1 permutations Pd1([n]) of subsets of [n], and w K DS. Thejustification of (62) is as follows: In the first equality in (62), we can drop the convex hull of {0, p1, ..., pk}since the objective is linear. In the second equality, we replace the unit norm generators by their expressionsgiven by the cross product and use the fact that DiXw = (Xw)+ for w K. Note that the cross productis zero when the vectors are linearly dependent to simplify the expression of DS in the last equality above.",
  "(x1, ..., xd1)2,(70)": "where j = (j1, ..., jd) is a multi-index. The above shows that the optimal value p corresponding to the NNobjective in (1) when p = 2 is bounded between the convex Lasso program in the right-hand side of (69)and the same convex program with a slightly smaller regularization coefficient as follows",
  "= p + (d1min 1)R(W 1 , W 2 ) ,(77)": "where F(, ) and R(, ) are the functions in the first (loss) and second (regularization) terms in the objective(74) respectively, and (W 1 , W 2 ) is an optimal solution of (74), i.e., we have p = F(W 1 , W 2 )+R(W 1 , W 2 ).Therefore, (73) implies that",
  "s.t. wp 1, (2Dk I)Xw 0": "We note that the above problem has the same form of equation (56) and (59) for p = 1 and p = 2 respectively,where the vector v plays the role of V u. The rest of the proof is identical for both cases. Lemma 20 (Archimedean approximation of spherical sections via polytopes). Suppose that Cone(P) Cone(P) = {0}, i.e., Cone(P) does not contain a non-trivial (non-zero dimensional) linear subspace.Then, it holds thatP0 C2 d1minP0.",
  "Proof. The first inclusion follows by noting that P0 is a polytope and it is contained in C2 since all of itsextreme points 0, p1, ..., pk are contained in the convex set C2": "In order to prove the second inclusion, we first show that C2 = Cone(P) {w2 1}. Since Cone(P)is pointed, i.e., Cone(P) Cone(P) = {0}, it is equal to the convex hull of its extreme rays (Theorem13 of Fenchel and Blackett (1953)). Since the extreme rays of Cone(P) and the extreme rays of the cone{w : (2Di I)Xw 0}, are identical and the latter cone is also pointed, these two cones are identical.Therefore, we have C2 = Cone(P) {w2 1}.",
  "iipi tConv(p1, ..., pk) Conv(0, p1, ..., pk) d1minP0": "Lemma 21 (Diameter of a chamber bounds its distance to the origin). Consider the set C2 : {w : (2Di I)Xw 0, w2 = 1}. Suppose that r R is the 2-diameter of C2, which is defined as the smallest r 0such that w1 w22 r for all w1, w2 C2. Then,",
  "for any set of k vectors {v1, ..., vk} C2": "Proof. Since the 2-diameter of the set C2 is upper bounded by r, we first argue that there exists a Euclideanball of radius r that contains C2 as follows. Suppose that w1, w2 are two vectors in C2 that achieve the2-diameter r, i.e., w1 w22 = r. Then, the Euclidean ball of radius r centered at w1+w2",
  "Proofs for two-dimensional networks": "Proof of Theorem 15. The proof follows by specializing the proof of Theorem 17 to d = 2 and noting that thechamber diameter is controlled by the angle . Consider the triangle corresponding the the chamber wherethe angular dispersion bound is achieved. Consider the bisector of this angle and observe that the chamberdiameter is equal to 2 sin(/2), which is upper-bounded by . Although this provides the approximationfactor 1/(1 ) as long as < 1, we can improve this bound by considering a direct Archimedeanapproximation of the circular section as shown in to replace Lemma (20). Specifically, using thebisector of the angle we also haveP0 C2 (cos(/2))1P0,",
  "s.t.(XW (1))W (2) = Y(1)": "The above problem is in the form of a two-layer neural network optimization problem. We now show thatW (1), W (2) combined with the rest of the optimal weights W (3), W (4) , W (L) are also optimal in theoptimization problem (24). First, note that the outputs of the network, f(X) and f1(X), are identicalwhere is an optimal solution and",
  "since ( W (), W (+1)) is feasible in (82), i.e., (XW ())W (+1) = Y()": "Finally, we finish the proof by iteratively applying the above claims to invoke the two-layer result given inTheorem 14. Given an optimal solution of (24), we replace the first two-layer NN block with ( W (1), W (2))from (81) while maintaining global optimality with respect to (24). For = 2, 3, ..., L we repeat this processin pairs of consecutive layers to reach the claimed result. Proof of Theorem 11. The proof proceeds similar to the proof of Theorem 10. We first show that given anoptimal solution , we can replace the first two layers (W (1), W (2)) with ( W (1), W (2)), where",
  "s.t.(XW ())W (+1) = Y()": "By Theorem 14, replacing the weights W (), W (+1) only increases the regularization term for those weightsby a factor of 1/(1 D(X)) while maintaining the network output f(X) = f(X). We can repeat thisprocess for all = 2, 3, ..., L one by one, i.e., replacing layers 1 and 2, 2 and 3,.... (as opposed to pairsof consecutive weights as in the proof of Theorem 10), we replace all the weights by increasing the totalregularization term by a factor of at most 2/(1) assuming D(X) and D(X) for {1, ..., L2}.The factor 2 is needed to account for the overlapping weights in the replacement process.",
  "i=1vi = 0.(87)": "Now, we define X := X 1nxTj and observe that the form of the dual constraint is identical to the onesanalyzed in Theorems 17 and 4 for each fixed j [n]. We repeat the argument for each j [n], and use thesame steps that to obtain the convex programs and approximation result. In particular, when the chamberdiameters D(X 1nxTj ) uniformly for all j [n],",
  "Proof of Lemma 8": "Lemma 7 immediately implies that the random dataset is -dispersed with high probability. We aim to provethat for any j {1, . . . , n}, the modified dataset X = X 1xTj satisfies D(X) , j [n] with highprobability. Note that X 1xTj is also a set of i.i.d. Gaussian, except the j-th row, which is all-zeros. Usingthe same derivation in the proof of Lemma 6 and 7, we have",
  "for = 5": "d/(n 1) and some with probability at least 12ed/2. Taking a union bound over j [n], weobserve that the above inequality holds simultaneously j [n] with high probability at least 1 2ned/2.We set = /2 and obtain that the dataset X is locally -dispersed with high probability as long as n 4d.",
  "the d 1 rows indexed by S": "Proof. We first provide a proof of the first direction (extreme point = full rank) by contradiction. Letx be an extreme point of C. We define S as the subset of inequality constraints active at x and Sc as thesubset of inequality constraints inactive. More precisely, we have ASx = 0 and AScx < 0 for some subset",
  "Definition 5 (Rays). Let K be a convex cone in Rd. A cone R K is called a ray of K if R = {x : 0},for some x K": "Definition 6 (Extreme rays). Let K be a convex cone in Rd. A ray R = {x : 0} K generatedby some vector x is called an extreme ray of K if x is not a positive linear combination of two linearlyindependent vectors of K. Lemma 24 (Weyls Facet Lemma (Weyl, 1950)). Define the convex cone K = {w Rd : wT pi 0, i [k]} Rd, where p1, ..., pk Rd are a collection of vectors. Then, a non-zero vector x K is an element ofan extreme ray of K if and only if dim Span(pi : xT pi = 0, i [k]) = d 1.",
  "(v)(94)": "Proof. Lemma 25 and 26 are based on a standard application of Lagrangian duality (Boyd and Vandenberghe,2004). Strong duality holds, i.e., the primal and dual problems have the same value which are both achieved,in both problems since v = 0 in the dual problem is strictly feasible for the inequalities due to > 0.",
  "j=1|W (3)j|. (102)": "Proof. Consider scalar non-negative weights 1, 2, 3 satisfying 1j2j3j = 1 for j [m]. Applying thescaling W (1)j 1jW (1)j, W (2)j 2jW (2)j, and W (3)j 3jW (3)jfor all j [m], we observe that the lossterm () does not change due to the positive homogeneity of ReLU. Optimizing over these scalars, we obtain",
  "Proofs for three-layer networks without bias terms": "Proof of Theorem 12. We focus on the maximization subproblem in the constraints of the dual problem inLemma 28. We use the same steps in the proof of Theorem 4 and 17 to maximize over the layer two andthree weights while the first layer weights are fixed, and finally maximize over the first layer weights. Thisyields dual constraints analogous to a nested version of (87) given by {Z(v) , 1Tnv = 0}, where Z = Z(v)is given by",
  ",": "where Tjk is defined in (123). Now we note that for each fixed j [n], the sets {Fjl Ck}k[P ],l[G] arechambers of hyperplane arrangements that exhaustively partition Rd. Taking a union over all chambers asthe tuple (k, l) range over [P] [G] gives the entire space Rd. Therefore, we have",
  "vectors {x(j)i }2n+di=1are the union of {xi}ni=1, {xi xj}ni=1, and {ei}di=1": "The arguments for Z(1), X (1), X (1) hold in the same way, and we obtain the following characterization ofthe dual problem. Using the convex duality of Lasso from Lemma 25, we obtain the claimed convex bidualproblem, and the optimal solution is given by the extreme points of the set Wj.",
  ".(126)": "We now focus on the maximization with respect to the scalar b when the vector w and the index j are fixed.Observe that the objective is a piecewise linear function of b, when all other variables are fixed. Therefore,it suffices to check the break points and when or . The break points are when b such thatxTi w + b = 0 for some i [n]. When we have"
}