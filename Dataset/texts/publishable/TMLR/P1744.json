{
  "Abstract": "Despite their great practical successes, the understanding of neural network behavior is stilla topical research issue. In particular, the class of functions learnable in the context of afinite precision configuration is an open question. In this paper, we propose to study thelimits of gradient descent when such a configuration is set for the class of Simple RecurrentNetworks (SRNs). We exhibit conditions under which the gradient descent will provablyfail. We also design a class of SRN based on Deterministic finite State Automata (DFA) thatfulfills the failure requirements. The definition of this class is constructive: we propose analgorithm that, from any DFA, constructs an SRN that computes exactly the same function,a result of interest on its own.",
  "Introduction": "One of the main challenges Machine Learning is facing is the lack of theoretical understanding of the reasonsof the practical successes of deep learning. Indeed, whole research fields have been revolutionized by the boomof deep neural networks, from signal (Mehrish et al., 2023) to image processing (Li et al., 2024) includingfinance (Zhang et al., 2021), for instance, but much remains to be done to fully understand their capabilitiesand limits. In this paper, we propose to study this issue in the context of Recurrent Neural Networks (RNN). As withneural networks in general, we can identify at least two ways of interpreting capabilities and limits. The firstone is to analyze the expressivity, that is, the class of functions a class of models can compute. Within thisdirection of research a notorious theoretical result states that vanilla RNN, the SRN (Elman, 1990), are ableto compute any Turing machine (Siegelmann & Sontag, 1992). Though of great interest, this theorem sayslittle about the models used nowadays: its proof relies on the use of unbounded time and requires neurons ofinfinite precision. It has been shown (Chung & Siegelmann, 2021) that it is possible to maintain the Turingcompleteness for an SRN in finite precision if the algorithm has access to a potentially unbounded amountof neurons. A similar result is proposed by Stogin et al. (2024) with a particular emphasis on the provablestability of the proposed architecture.When it comes to conventional RNN architectures, those with afixed number of neurons, Weiss et al. (2018) practically shows that the expressivity drastically decreasesfor all types of RNNs, most of them fall in classes equivalent to the expressivity of finite state machines.Merrill et al. (2020) theorize the phenomenon of saturation and derive a hierarchy of RNN architectures.The saturation in RNN occurs when the bounded activation function composing the RNN (the sigmoidalfunction of tanh) are pushed to their boundaries and thus produce binary outputs. Despite the interest ofsuch a result, the saturation formalized by Merrill et al. (2020) is unrealistic because it is based on the notionof limit to be performed at every iteration.",
  "Published in Transactions on Machine Learning Research (12/2024)": "with t {1, . . . , |Q|} t = r. Most importantly, the vector Ues +Wh0 has only one coordinate with the valueJ, and the other coordinates are less than J. Hence when we apply in finite precision to Ues + Wh0 weobtain the vector h1 with the property:",
  "We conduct experiments that validate the theorem, and that show that the task of learning saturatedSRN is even harder than predicted by the theory": "We provide an algorithm that, given a Deterministic Finite State Automaton (DFA), constructs asaturated SRN that will simulate the DFA. The algorithm is an extension of the algorithm providedby Minsky (1967) to SRN in finite precision. The article is structured as follows: of this paper is devoted for notations and the introductionof different necessary notions for the proofs. contains the secondary but necessary result on thestability to noise in saturated SRNs. In , we develop the arguments towards the gradient descentfailure in finite precision, as well as a detailed example of the application of the theorem on a small SRN. details the algorithm that for a given DFA outputs a saturated SRN capable of simulating theDFA. is the experimental section where we provide empirical evidence that our theory is valid.And the final contains the conclusive discussion with possible extensions of our result, future worksand related works on the limitation of gradient descent in finite precision.",
  "Elements of Linear Algebra": "Numbers, besides properly defined constants, are noted by a lowercase italic letter, e.g., x R; the vectorsof dimension higher than 1 by a bold lowercase letter, e.g., x Rd for d > 1; the matrices by a bolduppercase letter, e.g., M Rd1d2. For x Rd, x[j] represents the jth coordinate of the vector x. Id =Diag(1, . . . , 1) Rdd is the identity matrix of dimension d. Given x R and M Rd1d2, x M denotesmultiplication of all values of M by x.",
  "is the Hadamard product between vectors: if x Rd and y Rd we have x y = z, with z[j] = x[j] y[j]for 1 j d": "is the concatenation operation on vectors. For x Rd1 and y Rd2 we have x y Rd1+d2 (where d1and d2 might be different): if x = (x1, . . . , xd1) and y = (y1, . . . , yd2) then x y = (x1, . . . , xd1, y1, . . . , yd2).",
  "As an example, in the IEEE 754 norm1, the simple floats are encoded in base B = 2 on 32 bits, with 1 bitfor the sign, M = 23 for the mantissa and X = 8 for the exponent": "Arithmetics operations in a finite precision configuration might sometimes be counter intuitive. For instance,suppose one wants to compute the addition of two numbers x and s. Let x = (signx)x1 xM Bexponentxand s = (signs) s1 sM Bexponents such that exponents < exponentx. In order to compute x + s, onefirst needs to match the exponents of x and s. Without loss of generality, we can fix the highest exponent andmodify the magnitude of the other number, however since we can represent the mantissa of s with only Mdigits we have to compromise and replace the s mantissa s1 sM by an approximation 0 0 D zeross1 sMD",
  "with D = exponentx exponents. We obtain that the operation x + s is in fact x + s": "This illustrates the consequences of rounding in finite precision, since the D last digits of s are not takeninto account in the addition.Worse, if D M, the operation x + s will output x.In the followingLemma (proved in Appendix D), we discuss another phenomenon, the saturation of the bounded activationfunction. In infinite precision, functions as the sigmoidal function and the hyperbolic tangent, never reachtheir boundaries when evaluated on real numbers. However in finite precision they do reach the boundaries,and the following lemma characterizes the smallest float number to saturate the sigmoidal function.",
  "In the following we focus on a single Recurrent Neural Network architecture, and therefore we set theactivation function to be": "In deep learning the same notation is usually used for the activation function as a one variable scalar functionand for a whole activation layer. To avoid any confusion, in this paper will refer to the derivative of thefunction : R R and : Rd Rd its element-wise counterpart defined by:",
  "(x1, . . . , xd) = ((x1), . . . , (xd))": "In this work we focus on just one RNN architecture: the Simple Recurrent Networks (SRNs) (Elman, 1990).However, we would like to emphasize that we are studying SRNs in finite precision, so to formalize thispoint, we introduce the definition FP-SRNs where FP stands for Finite Precision. 1The current version are defined according to the following reference: IEEE Computer Society (2019-07-22). IEEE Standardfor Floating-Point Arithmetic. IEEE STD 754-2019. IEEE. pp. 184. doi:10.1109/IEEESTD.2019.8766229. ISBN 978-1-5044-5924-2. IEEE Std 754-2019.",
  "hk := E(xk, hk1) ; yk := D(hk)": "In this definition, we have chosen to group the parameters together in a single matrix as far as possible.However, in this work it will also be useful to have a definition that makes a clear distinction betweenparameters of different types, so we give the following definition, perfectly equivalent to the one above. Definition 5 (FP-SRN version 2) A Finite Precision Simple Recurrent Network (FP-SRN) R is a triplet(h0, E, D) where h0 Gh is the initial hidden state vector, and E, D are functions called encoder, decoderrespectively and are defined by:",
  "Mh = [U W b]Mo = [V c]": "Another handy notation is the one of the parameter vector. Indeed, when we need to consider a parametersin a FP-SRN R independently of their function, it is convenient to group all the parameters together in avector of parameters denoted . So, to underline the distinction between two FP-SRN which differ by theirparameters we will note them respectively R and R.",
  "Uxk + Whk1 + b[i] > k {1, . . . , T}": "In other words, a -saturated FP-SRN linear part of the encoder will always produce a vector that is inRh \\ h, i.e.whose coordinates are always at a distance from zero.An interesting fact about-saturated SRNs is that for all input sequences {xk}Tk=1, for all 1 k T, we have:Uxk + Whk1 + b () . This inequality derives from the properties of , and the -saturation hypothesis. Indeed, by hypothesiswe are sure to have (Uxk + Whk1 + b) [j] for all coordinates 1 j h, on the other side is adecreasing function on the interval [0, +[ and symmetric. Hence for all 1 j h we have:Uxk + Whk1 + b[j] ()",
  "This inequality is discussed more in detail in the beginning of Appendix A. In supervised machine learning,the training of a neural network requires the use of a loss function defined as follows": "Definition 9 A loss function is a function L : Go Go G that compares computationally the output ofa model on a given training data and the expected target value. The Binary Cross entropy is an examplefor p, q ]0, 1[:L(p, q) = q ln(p) (1 q) ln(1 p). In a practical context of deep learning one could, during early phases of the training, encounter a situationwhere the prediction of a neural network is the opposite of the target label i.e. p 0 and q = 1. In this kindof situation we obtain L(p, q) = q ln(p) = ln(p), a value unstable and potentially undefined. Moreover,the gradient of such a calculation is 1",
  "L(p, q) = q ln(p + ) (1 q) ln(1 p + ).(1)": "We will assume = 107, corresponding to usual practice in the tensorflow/keras platform2. This smallchange will have a profound impact in this work when we discuss the back propagation of the gradient,because the functionpL(p, q) becomes bounded on {0, 1}. Definition 10 (Fixed step Gradient Descent) Let f : Rd R be a differentiable function, u0 Rd aninitial point and > 0 a positive real number. The fixed step gradient descent of f starting at x0 is thesequence {gl}l0 Rd defined as follows:",
  "The language represented by a DFA is the set L = { : (q1, ) F}, where is the transitiveextension of the transition function: (q, ) = q, (q, a ) = ((q, a), ) for q Q, a ,": "Definition 12 (One-hot encoding) Let = {a1, a2, . . . , au} be a finite and ordered alphabet of size u.Let {e1, e2, . . . eu} be the canonical basis of the vector space Ru, defined by: ei[j] = 1 if i = j and 0 otherwise.We define a one-hot encoding as a bijective map:",
  "The definition of a one-hot encoding is naturally extended to the elements of by the bijective map : {e1, e2, . . . , em} with (a1a2 a) ((a1), (a2), , (a))": "One-hot encoding is a bijective correspondence between an ordered set of symbols = {a1, . . . , au} and thecanonical basis of Ru, {e1, e2, . . . , eu}. In order to simplify notations, we take advantage of the bijectivecorrespondence and from now on we consider that = {e1, e2, . . . , eu}.",
  "Stability to Noise in Parameters": "Our main goal is to prove that there are regions of the parameter space that are not accessible by gradientdescent. Our strategy is to exhibit points in the parameter space such that FP-SRNs with those param-eters will experience a stationary gradient. The notion of stationary gradient is formally defined later inDefinition 15. Nevertheless, intuitively a FP-SRN R will experience a stationary gradient when there is anon empty set of coordinates in the parameter vector such that for any element of the training set thegradient for these parameters is negligible with respect to the parameters. We will demonstrate that some-saturated FP-SRNs experience a stationary gradient, but in this section we exhibit another particularfeature of -saturated FP-SRNs, the one of stability to noise. Indeed, Mitarchuk et al. (2024) prove thatit is possible to disturb the parameter of a -saturated SRN (and thus of a FP-SRNs) and have guaranteesthat the disturbed FP-SRN is -saturated, for proportional to and to the perturbation. We will buildon this perturbation property of -saturated SRNs to expand the stationary gradient experience around thesaturated FP-SRN, and hence obtain a region of parameter space where FP-SRNs experience a stationarygradient. In our work, we have slightly reformulated their result and their definition of -saturation, but thetheorem stated below and the definition remain perfectly equivalent. Before stating the result we introducethe notion of perturbed SRN.",
  "The sequence of hidden state vectors hk is produced by the perturbed FP-SRN Rp+": "We recall all the proofs of this theorem in Appendix B. The idea is that it is possible to express hk as a sumof hk and G(, k), where G(, k) is a function of the perturbation and the iteration k and takes the formof a sum of k elements. Mitarchuk et al. (2024) exploit the saturation assumption to upper-bound G(, k)with a convergent series and hence show that the perturbation injected at every step cannot be endlesslyamplified. This proves the first claim, after what the second claim is deduced from the first one.",
  "Finite Precision SRN and Learnability": "In this section, we discuss gradient failure for RNNs training. First, we describe the algorithm of gradientpropagation in RNNs called Back Propagation Trough Time (BPTT) and derive a representation of thegradient as a sum of products. Then, we formally define the stationary gradient, after what we state ourcentral result. Finally we apply this theorem to a concrete case and discuss the different variables involvedin the theorem.",
  "Back Propagation Trough Time": "To train neural networks the usual approach is the one of Gradient Descent (GD). Its classical implementationfor RNNs is called Back Propagation Trough Time (BPTT) which we detail in this section. There are manyways from which one can consider neural networks and BPTT. Switching from one representation to anotherprovides an in-depth and intuitive understanding of the mechanics of BPTT. We start by setting out themachine learning framework within which we will develop our arguments, and then use the various neuralnetwork representations to obtain a gradient representation that will enable us to prove our assertion. We place ourselves in the context of binary classification on sequences. Let R be a FP-SRN and S ={(1, y1), . . . (N, yN)} a labeled training set with N samples and where l = {xk}Tk=1 Gu is a finitesequence of vectors (the vectors xk can represent vector embedding of words) with 1 l N and T 1 aninteger representing the length of the word l. We set L : G G G to be a differentiable loss function.",
  ": Illustration of the unrolling of the RNN for the BPTT": "and then D(hT ), with hk+1 = E(xk+1, hk) for 1 k < T. A way of seeing the recursive computation of theRNN is to unroll the calculation so that we obtain a pseudo feed-forward neural network R derived fromthe evaluation of R on with (T +1)+1 layers, which includes the decoder layer and the loss function layerL (D(hT ), y). Each parameter of this pseudo feed-forward network R is then updated by back propagatingthe gradient. The difference between R and a real feed-forward network is that the parameters do notchange from one layer to the other. To give a more formal definition of what we mean by pseudo feed-forward network R, let us consider the notations of Definition 5, allowing us to write hT as the output ofa T layer feed-forward neural network:",
  "where L(R,y)": "xrepresents the partial derivative of L with respect to the first variable. We abuse a littlebit the notation of the neuron here because s refers to a parameter in the matrix Mh and not to a rowvector, but since a unique row vector contains the parameter s we allow ourselves to denote a neuron inthis manner. Equation 2 is not new, in fact it can be deduced from the back propagation algorithm of Rojas& Rojas (1996). We recall that our goal is to show that some regions of the parameter space of FP-SRNcannot be reached by gradient descent. The idea is to exhibit regions of the parameter space where somecoordinates of the gradient L(R(), y) will have a negligible value with respect to the parameters, leadingto L(R(), y) = in finite precision. To achieve this we will use Equation 2 to prove an upper",
  "mi,j": "that we can exploit in finite precision arithmetics. We have used a lot of formalism forthe definition of the gradient, which is necessary for the rigor of our work but will not really be used in theproofs, because we will state hypotheses on R that will allow us to derive a bound that does not dependon T, vectors xT s hT s1 1 and parameters ml,t. We define the notion of stationary gradient in finite precision. A condition characterized by the fact that,for some parameters, adding the gradient will not change the parameter due to rounding in finite precision. Definition 15 (Stationary Gradient) Let (B, M, X) be a finite precision configuration, R be a RNNwith parameter vector , a loss function L, a training set S = {(1, y1), . . . , (N, yN)} with N 1 trainingsamples and > 0 a positive real number called the learning rate. We say that R experiences a stationarygradient if there is a non empty subset P of coordinates in such that for all mi,j P and for all (, y) Swe have in finite precision arithmetic:",
  "A Condition for BPTT Failure": "In this subsection we develop our argument of BPTT failure in a finite precision framework. The idea ofthis theorem is: if the parameters that we try to reach by GD have the properties stated by the theorem(we will show that fully saturated FP-SRNs fulfill these properties) then the gradient will fail to update theparameters before reaching the target.",
  "h)the FP-SRN R+ will experience a stationary gradient for all non zeroparameters on the training data set S": "Note that this result does not include the parameters in the decoders part of the FP-SRN. The assumptionson overcome the information transmitted through the decoders gradient. The consequence of this is thatone can change the decoders parameters completely, it will have no effect on the encoder parameters gradient. A few words to comment on the assumptions and quantities used in the theorem. The idea behind thisresult is that a FP-SRN R satisfying hypotheses 1 and 2 will have at least two behaviors of interest to us:A) robustness to noise, B) stationary gradient. The robustness to noise is essential in this result, as we aimto exhibit a whole population of RNNs that have similar behavior to R. Quite naturally, when it comes toperturbation, we have to make assumptions about the Lipschitzian constants L of functions that form thenetworks. This explains assumption number 1, which ensures that the loss function will not explode if thenetworks parameters are perturbed. Assumption 2 is more central to this work. This is the assumption ofsaturation of the activation function, i.e. how close the activation function is to its bounds. This assumptionis essential, as it both demonstrates the stability of the RNN with respect of the noise injection, and squashesthe gradient to zero. The quantity acts as a threshold below which the guarantee of behaviors A) and B)are lost. More precisely, is the solution to the equation () =4",
  "hL": "1+ to achievewith () is built such that it is possible to bound Equation 2 by . On the other hand, the constant hasto be seen as a budget for the perturbation that R can handle without loosing the saturation property. Allthe proofs articulate around well-defining the border , and wisely managing the budget . For readability reasons, the complete proof of the theorem is in Appendix A of this paper. Nevertheless, wepropose a sketch of the proof. At first, we prove that a ( + )-saturated FP-SRN is stable to perturbationswith Theorem 14 proven in Appendix B. Indeed, one can disturb the parameter of R by adding a noisevector to the parameter vector and be sure that the output of R+ will remain within a controllabledistance of R. On top of that, for a well chosen we can be sure that R+ will be ( +/2)-saturated (i.e.for Encoder",
  "An example of a non learnable FP-SRN": "In this section we apply our results on a small example. DFA suit particularly well our case, because theymodel binary functions on sequences. is a representation of a DFA accepting the language of allwords that contain an odd number of as on the alphabet = {a, b} (the initial state is 1, the sole acceptingstate is 2). : A DFA accepting the language composed of words containing an odd number of as. The initialstate is drawn with an incoming arrow with no starting state, the finale state by a double circle, and thetransitions with labelled arrows. Let (B, M, X) be the finite precision configuration. We prove in Appendix D Lemma 3 that there existsa positive float number J such that (J) = 0, (J) = 1 and for all positive float number s such thatJ s < J we have ((J s)) > 0. The number J is called the underflow barrier float.- Then we define:",
  "On the learnability of this FP-SRN": "In the following subsections we apply Theorem 16 to the FP-SRN defined just above. We will graduallyshow that the FP-SRN of the example respects the assumptions of Theorem 16 and explain the magnitudeof each variable involved in that theorem. At first we assume to have (B, M, X) = (2, 23, 8) the usual finite precision configuration of 32 bit floatnumbers, that we convert to base 10, in order to have readable constants. In this configuration we haveJ = 88.7228391117. When we switch from base B = 2 to base B = 10, the number of digits needed toencode all the floats in G(B,M,X) does not exceed 15, so when we switch to base B = 10 we get that M = 15.In other words, we have G(2,23,8) G(10,15,6). The constant M is necessary for the definition of , the valuethat must upper bound the coordinates of the gradient in order for it to be rounded to zero. Therefore, inthe following, since we switched to basis 10, we will use M = 15 to define . The FP-SRN presented abovein .3 is J-saturated by definition. By taking a closer look to the matrix Mh =UWone candeduce that for all 1 i h and all 1 j u + h + 1 we have |mi,j| 3J. Also the smallest non zeroparameter in Mh, Mo is m = J, Therefore we obtain:",
  "How h, , and L affects ?": "In Theorem 16, the quantity has a central role, it delimits a region where we have the guarantee thatR is stable to noise and will experience a stationary gradient. In the case that we consider now, R isJ-saturated with + = J, knowing allows us to know exactly (a constant that represents the budgetfor the perturbation) and thus the size of the Euclidean ball containing FP-SRNs with stationary gradient.The variable is directly linked to m, , L and the learning rate. In our case m = 267. For this studywe will represent the size of the hidden state vector h = 10n for n , because it is the magnitude ofh that will mostly impact the computations. We assume to use L as the loss function with, = 107, adefault quantity in tensorflow.keras platform for numerical stability. It comes that we have L = 107, andfor simplicity we assume to have = 1.",
  "< 0": "If we consider cases where 1 < h = 10n 1010 then we have that the boundary is in the interval .We recall that represents a boundary beyond which we have the guarantee that R is stable to noise inparameters and that this FP-SRN will experience a stationary gradient. It is important to note that evenwith very hard requirements on the gradient (i.e. the coordinates of the gradient have to be smaller than1021), it is sufficient for a FP-SRN to be saturated with values under 72. In other words, by Theorem16 it is impossible to train FP-SRN by gradient descent such that the linear part will output vectors withcoordinates further than 72 from zero.",
  "+": "h . All the computations are based on thehypothesis that R is defined as in .3. Our goal here is to estimate how these quantities affectthe radius of the Euclidean ball around the parameters Mh that fulfill the assumptions of Theorem 16. Werecall that the matrix W is the transition kernel extracted from Mh, and that := ( + /2), where + = J 88.72. We start by analyzing the quantity W = ( + /2)W. Lemma 19 in AppendixA tells us that( + /2) () + (/2)e(/2) .",
  "+10n2and we plot this function with n R+ in . We have highlighted the particular": "value of the radius function at 0.602 for the reason that for this value of n we have 10n = 4 i.e.thenumber of neurons that the FP-SRN presented in .3 has. Hence the reader can see that with ourresults we obtain that the perturbation radius in this example is 6.16. We will see in a next paragraphhow such perturbation can impact the network performance. Finally, note that SRNs with more than 103 neurons are rarely observed, so this last constraint on the number of neurons is realistic. Indeed, basedon the survey of Lara-Benitez et al. (2021), we identified 3 works published between 2012 and 2018 thatemploy SRNs ((Chandra & Zhang, 2012), (Rueda & Pegalajar, 2018), and (Mohammadi et al., 2018)). Theinformation about the number of neurons is missing in (Mohammadi et al., 2018), while the number of hiddenneurons is less than 103 in the two other papers. The most recent use of SRNs, dating from 2023, is in theTAYSIR competition (Eyraud et al., 2023), where a benchmark of already trained models was presentedto competitors. The benchmark is composed of different RNN architectures and in particular, SRNs wereproposed whose number of hidden neurons did not exceed 512. 2.50.02.55.07.510.012.515.017.520.0 r(n) n r(0.602) = 6.16 100.602 = 4",
  "Stationary gradient even with L = 0": "Just above we established that the FP-SRN R from .3, within the requirement of Theorem 16,has a perturbation radius of 6.16. In other words, for all Rdim() such that Encoder 6.16 theFP-SRN R+ will experience a stationary gradient. In we present the variables of interest forR. Theorem 16 does not depend on how the perturbation is spread among the parameters of the FP-SRN",
  "L((R+((a)), 1)) = 1 log(( Moh1) + 107) log(0.88 + 107) 0.1269": "This means that the FP-SRN R+ can experience a stationary gradient even though the loss is not 0.In this example, the classification is still correct if one sets a threshold of 0.5, indeed 0.88 > 0.5 thus theclassification would be correct. However, since Theorem 16 does not depend on the Encoder parameters,one could set all the Encoders parameters to 0 while maintaining the theorems validity. In this extremecase, the SRN outputs would be constantly 0.5, i.e. no classification and the gradient would be stationary.",
  "A non learnable class": "In the previous sections we have developed a series of arguments towards the fact that saturated FP-SRNscannot be learned by gradient descent with a fixed learning rate. In order to exemplify the result we haveexhibited a FP-SRN that simulates the DFA accepting all words containing an odd number of as on thealphabet {a, b}. In this section, we argue that the example of .3 is not an isolated case. We presentan algorithm that, for any given DFA, outputs a saturated FP-SRN that simulates the DFA. We call thisalgorithm DFA2SRN, and it is an adaptation of the algorithm proposed by Minsky (1967) to FP-SRNs.Indeed, the author of (Minsky, 1967) works with RNNs based on a non differentiable activation functionH : R {0, 1} with H(x) = 1 if x 0 and 0 otherwise. This algorithm demonstrates that the non attainableregion by gradient descent for FP-SRN contains a large class of interesting functions.",
  "DFA2SRN": "Let D = (, Q, q1, , F) be a DFA. We suppose that the alphabet = {e1, . . . , eu} is a set of one-hot vectors.We recall that Q = {q1, . . . , q|Q|} is the set of states, q1 is the initial state, : Q Q is the transitionfunction, and F Q is the set of final states. In order to simulate the DFA by a FP-SRN we have to: 1)simulate the transition function, 2) encode every state such that it can be read by the FP-SRN, 3) simulate",
  "In the definition of Wk the vector wk is repeated || times": "Proof . In the following paragraph we prove that the constructed FP-SRN is simulating the functioning ofthe fixed DFA. For this purpose, in first place, we have to prove that, for any word = e1 eT oflength T, there exists a well defined correspondence betweenUe1 + Wh0, . . . , UeT + WhT 1f [(q1, e1), (q1, e1e2), . . . , (q1, )](7)",
  "end forU J UW J WV J V": "FP-SRNs in the classical way, in the sense that we provide training data and apply the SGD algorithm. Thesecond part is a more synthetic experiment. The reason for this dichotomy is that the classical experimentsdid not allow us to draw any conclusions concerning Theorem 16, so we had to imagine a different setting.",
  "Learning regular languages": "As we stated above, in this section we seek to learn regular languages in the classical way of deep learningpractices. Our aim is to gather statistics about gradients in the hope of observing the stationary gradientphenomenon. Learning a regular language is learning a binary classification task on strings. First we tried to learn the DFA of the example given in .3, whose language corresponds to the stringof characters on the alphabet {a, b} that have an odd number of as. To do this, we randomly generated 458labeled strings with 50 2% of positive labels. What is meant by positive labeling are the character stringsthat are part of the language and are labeled by 1, the other character strings are labeled by 0. The trainingsample is made up of character strings of lengths ranging from 2 to 10. The loss function used is the onewe dealt with in the theorem (see Definition 1). The DFA2SRN algorithm was used as a guide for choosingthe hidden dimension of the FP-SRN, so we randomly initialized an FP-SRN with hidden dimension d = 4,trained by stochastic gradient descent on 20,000 mini-batches of 32 strings. Indeed, we slightly deviatedfrom the usual SGD, where data is split into mini-batches that will not change until the end of the training.In our setup, we randomly draw a mini-batch at each iteration i = 1, 2, . . . , 20000. The statistics we trackedduring training are: the value of the Loss, the Euclidean distance between two consecutive parameter vectors,the infinite norm of the gradient and the distance to the target parameters (i.e. the parameters given bythe DFA2SRN algorithm) The results of this experiment are presented in This experiment does not allow us to validate or invalidate Theorem 16. The FP-SRN did not learn thebehavior of the DFA. By observing the distance graph between two consecutive parameter vectors, we do notobserve any convergence behavior. However, despite the apparent simplicity of the DFA that we tried tolearn (two states and an alphabet composed of two symbols), this DFA belongs to the most complex subclassof regular languages. According to van der Poel et al. (2024) the subclass in question is called Zp. Moreover,the training set is quite small (only 458 sequences). For this reason, we conducted the same experiment onlanguages from lower complexity classes in order to ensure that there is no bias in the experience of SGDlearning from . In addition to the classification of sub-regular languages, van der Poel et al. (2024)provides a benchmark of 1,800 languages covering all levels of complexity. To carry out this experiment,we selected 9 languages that closely match the benchmarks complexity spectrum. The training on these 9",
  "Loss": "02500 5000 7500 1000012500150001750020000 0.00000 0.00005 0.00010 0.00015 0.00020 0.00025 0.00030 0.00035 0.00040 kk1 02500 5000 7500 1000012500150001750020000 0.00 0.05 0.10 0.15 0.20 0.25 (k) 02500 5000 7500 1000012500150001750020000 0.008 0.009 0.010 0.011 0.012 0.013 0.014 +8.868e2kTarget :Regular SGD training where the four statistics of the learning experience of the odd as language.The curves in blue correspond to the statistics recorded during training, and the curves in orange are therolling averages. where 0 < < is a strictly smaller learning rate. The refining, as we repeat it, allows us to find abetter approximation of the point where the gradient becomes stationary. In our experiments we appliedthe straight line gradient evaluation with learning rates {0.1, 0.01, 0.001}. Two version of the strait line gradient evaluation are displayed in , one where all the parametersare updated represented by the blue graphs, and the second one where only the Encoders parameters areupdated and the Decoders parameters are frozen after one update. displays, from left to right,the graphs of: the Loss, the distance to the target, the Gradient infinite norm of the gradient, and theproportion of the parameters affected by the stationary gradient. We recall that a parameter mi,j experiencesa stationary gradient if in finite precision",
  "languages led us to the same conclusion than the first experiment. The details of this experiment can befound in Appendix E": "As we have already stated, these experiments do not validate Theorem 16, but neither do they invalidateit. We did not observe the stationary gradient phenomenon in these experiments, but the parameters of theFP-SRN remained far from the zone where saturation could be observed. All that can be concluded is thatthe task and/or the context of the learning is too hard for SGD learning. To better fit the context of ourtheorem, we designed another type of experiments.",
  "The Synthetic experiment": "The failure of the previous experiment may come from the fact that the direction of the gradient fluctuatesconstantly, which does not allow the parameters to converge. Some cases have been reported where thisfluctuation is not too important because on average the direction of the gradient is constant or almostconstant. In our case, however, it would appear that the fluctuations are too important. We have thereforebuilt a setup in which the direction of the gradient is constant and points towards the target parameters.We recall that the target parameters are those given by the DFA2SRN algorithm. This is fairly simple toset up, as all we need to do is to define a straight line between the parameters randomly initialized and thetarget parameters. By moving along this segment, we obtain a vector of parameters i which we can use toevaluate the gradient of the network Ri. This is exactly what we did with two variants. In the first one,all the parameters were evolving while, in the second one, we froze the decoder parameters after a singleiteration. The aim of the second variant was to observe the impact of the decoder on the gradient. We used the data from the first experiment to evaluate the gradient, i.e., we used the data set of 458words on the alphabet {a, b} labeled to correspond to the language of strings with an odd number of as.We initialized 0 parameters of FP-SRN at the origin, according to common practices in Deep Learning.Using these 0 parameters and the T goal parameters, we constructed the segment [0, T ] as well as itssubdivision {0, 1, . . . , T } containing 10 points on the segment distributed equidistantly. For i = 0, 1, . . . , Twe evaluated the gradient of Ri over the training set, with a stopping criterion (L(Ri)) < 1014.If the stopping criterion is satisfied for i < T we end up with a rough approximation of where the norm ofthe gradient has dropped below 1014. So, in order to find a better approximation of where the norm of thegradient falls below this threshold, we repeat the same procedure on the segment [i1, i]. Then, once thestopping criterion is satisfied again, we start a further refinement. The statistics in , are the value of the Loss, the distance to the target parameters, the infinite normof the gradient and the proportion of the parameters affected by the stationary gradient. All this statistics(beside the pie chart) are computed for every single word in the training set. One can observe some variationsin the thickness in the plots, this is due to oscillations in the statistics caused by the computation of thegradient for every word individually and that all the data are disposed on relatively small graphs. Note thatthe horizontal axis reflects the number of data points collected for the statistics, in the first variant over60,000 data points for each statistic were collected, and in the second variant just above 30,000.",
  "Target": "Theoretical failure Empirical failure :Representation of the synthetic experiment in 2D. The initial parameters 0 are randomly chosenaround the origin, and the T arget parameters represent the target parameters provided by the DFA2SRNalgorithm. During the experiment, the parameters are selected on the blue line and the gradient of theseparameters are evaluated. The orange disk represents the Euclidean ball centered at T arget whose radiusis predicted by Theorem 16, and the red disk represents the zone from which the gradient is empiricallystationary.",
  "Discussion": "In this work we studied theoretically and experimentally the behavior of SRNs subjected to finite precisiongradient descent training. We have shown that it is not possible to obtain saturated SRNs by gradientdescent, in finite precision, with a fixed training step. We discuss in the next paragraph that this resultis easily transferable to the case of evolutionary but bounded learning steps. Our approach consists in: 1)exhibiting a set of parameters p that will induce, in finite precision, saturation in FP-SRNs and a stationarygradient, 2) showing that this behavior persists in a sufficiently close neighborhood of p. This result showsthat finite precision induces the existence of areas of the parameter space that are not accessible by gradientdescent. We then show that the parameters that saturate the SRN produce functions of interest because itis possible to simulate the operation of any DFA. This is the result of the 1 algorithm. A first element of discussion is that our result stands only for the gradient descent algorithm with fixedlearning rate . In fact, it is automatically extendable to the case of a variable learning rate {l}Dl=1 whereD > 1, as long as {l}Dl=1 is bounded.By setting := max1jD j the result remains true for anyj.A second element of discussion is that our result, though stated for a classical gradient descent, isapplicable to the widely used stochastic gradient descent: as all gradients are null under the assumptions ofTheorem 16, so is their mean on any batch. In this work we have considered the case of binary classificationwhich can effortlessly be extended to multilogit classification. Indeed, a Decoder with output dimensiono > 1 can be seen as a concatenation of o different Sub-Decoders of dimension 1. Since our result standsindependently from the Decoders parameters none of the Sub-Decoders gradient will influence the Encodersparameters. Another straightforward extension of our result concerns other activation functions. Indeed,a similar argument can be made concerning any bounded function, like the hyperbolic tangent: the onlydifference being the value that saturates the activation, and a different expression of the boundary . Thesame goes for the algorithm DFA2SRN, the modifications to realize are in matrices W and V. The saturationof the tanh will produce vectors in {1, 1}h so instead of having a one-hot encoded vector the decoder willproduce vectors with a unique 1, and 1. This family F{1,1} of vectors forms a basis in Rh, hence one",
  "Related works": "Sum et al. (2019) study the gradient descent algorithm for feed-forward networks. They study the behavior ofthe algorithm when the network weights are noisy. They assume that the network weights and the arithmeticare based on finite precision, and two cases are distinguished: 1) fixed-point arithmetic, 2) floating-pointarithmetic. They show that even slight noise in the case of floating-point arithmetic can deviate the learningalgorithm. Karner et al. (2024) consider training a feed-forward network with ReLU activation under theassumption of finite precision. Since the type of network studied in this paper is a piece-wise affine function,they tackle the established fact that network depth causes an exponential increase in affine parts. Theyshow that it is highly unlikely that the network obtained after training exhibits an exponential amount ofaffine parts if the network is trained in finite precision. Colbrook et al. (2022) consider the class of inverseproblems, and show that for any finite precision set up with M digits of precision there exists an infinityof inverse problems that cannot be learned by neural networks. Their work is of a greater scope, becausethey do not focus on a specific learning algorithm. Fono et al. (2022) address the problem of the limitsof artificial neural network with an even higher standpoint. The authors go back to the limits of Turingmachines as digital machines, thus positioning neural networks as a subset of digital machines unable tocope with non-computable signals.",
  "of the 30th International Conference on Machine Learning, ICML, 2013": "R. Pozarlik. What type of finite computations do recurrent neural networks perform?In L. Niklasson,M. Bodn, and T. Ziemke (eds.), ICANN 98, pp. 10651070, London, 1998. Springer London.ISBN978-1-4471-1599-1. A. Ribeiro, K. Tiels, L. Aguirre, and T. Schn. Beyond exploding and vanishing gradients: analysing RNNtraining using attractors and smoothness. In The 23rd International Conference on Artificial Intelligenceand Statistics, AISTATS, 2020.",
  "Networks and Learning Systems, 31(6):22272232, 2019": "S. van der Poel, D. Lambert, K. Kostyszyn, T. Gao, R. Verma, D. Andersen, J. Chau, E. Peterson, C. St.Clair, P. Fodor, C. Shibata, and J. Heinz. Mlregtest: A benchmark for the machine learning of regularlanguages. Journal of Machine Learning Research, 25(283):145, 2024. G. Weiss, Y. Goldberg, and E. Yahav. On the practical computational power of finite precision RNNs forlanguage recognition. In I. Gurevych and Y. Miyao (eds.), Proceedings of the 56th Annual Meeting of theAssociation for Computational Linguistics, pp. 740745, Melbourne, Australia, July 2018. Association forComputational Linguistics.",
  "(t) = (t)(1 2(t))": "One can observe that (t) 0 for t 0, because > 0 and (1 2(t)) 0 on positive real numbers t.Moreover, it is easy to see that (1 2(t)) for all real numbers t. Hence 0 (1 2(t)) 1. Onthe other side, from the definition of (t)we have another expression of its derivative :",
  "A.2Proof of Theorem 16": "Definition 21 (Recall of section 3) Let R be a SRN with Mh = [U W b]Mo = [V c] and T 1 aninteger. For a parameter mi,j in the matrix Mh we define a path = (1, . . . , k) of length k for 1 k Tas a finite sequence of (Mh, Mo) parameters, such that for k = 1 we have 1 = Mo[i], for k = 2 we have1 = Mo[v] and 2 = mv,i for some 1 v h. For the case k 3 we define such that k = mv,i forsome 1 v h, for all 1 < s < k s1 = ml,t, s = mt,r and s+1 = mr,w. Finally, we define the set i,jcontaining all such paths. This section is dedicated to the proof of the criterion for BPTT failure. To do this we are going to provethat, under the hypotheses of Theorem 16, the update generated by GD is rounded to zero for everyparameter in the SRN. We position ourselves in the context of binary classification on sequences, S ={(1, y1), . . . , (N, yN)} is the labeled training dataset where i = {xk}Tk=1 Gu i.e. the data is vectorembedded in dimension u 1. R is a FP-SRN with in hidden dimension h 1, input dimension u andoutput dimension o = 1. We fix a parameter mi,j in the encoder weight matrix Mh, and compute recursivelythe gradient on the word with || = T. We already established in Equation 2 that:",
  "=1Card({ : || = })": "Now let i,j be a path of length , we know that = mt,i for some 1 t h meaning that we have hpossible choices for the index t. Once we have fixed t, we know that 1 = mr,t for some 1 r h. Onceagain we have h possible choices for the variable r. This argument is true for all s in , hence we have h",
  "Now we proceed to prove the claim of Theorem 16, but first, for convenience, we recall the statement:": "Theorem 16 Let G be the set of all float numbers representable in a given finite precision configuration(B, M, X).Let S = {(1, y1), . . . (N, yN)} be a labeled training set with N samples and where l ={xk}Tk=1 Gu is a finite sequence of vectors such that all vector xk respect xk for 1 l N and T 1an integer representing the length of the word l. We set L : G G G to be a differentiable loss function.Let > 0 the learning rate as used in Definition 10. Let R be an FP-SRN with hidden dimension h, inputdimension u and output dimension o = 1 i.e. R is shaped for binary classification on strings. We recallthat the row vectors of the matrix Mh are denoted mi for 1 i h, and that since the output dimensiono = 1 it means that the matrix Mo is of dimension (1, h). We set",
  "BStability of saturated SRNs": "Our reasoning is based on certain facts concerning the saturation of the sigmoidal function. Indeed, a FP-SRN R respecting the assumptions of Theorem 16 is found to be stable to perturbations. These may well beperturbations in the data, or perturbations in the parameters. We are interested in parameter perturbations,because we want to exhibit an Euclidean ball in the parameter space centered on the parameters of R suchthat all FP-SRN R whose parameters come from this ball, will behave closely to R. This stability is dueto the fact that the tanh and sigmoidal functions have small variations near the edges of their target sets.Moreover, because of the exponential in the definition of these functions, they reach the edges of their targetsets rapidly.",
  "Perturbed SRNs are central to our analysis": "Lemma 24 (Vector variant of the mean value theorem) Let y, Rd and let f : Rd Rd be adifferentiable function such that f(y) = (f1(y), . . . , fd(y[d])) where every fi is differentiable. We definef = (f 1, . . . , f d). Then there exists c Rd such that:",
  "Now we recall the algorithm DFA2SRN": "Let D = (, Q, q1, , F) be a DFA. We suppose that the alphabet = {e1, . . . , eu} is a set of one-hot vectors.We recall that Q = {q1, . . . , q|Q|} is the set of states, q1 is the initial state, : Q Q is the transitionfunction, and F Q is the set of final states.. In order to simulate the DFA, we have to simulate thetransition function, to encode every state such that it can be read by the FP-SRN, and then the model hasto simulate the function discriminating between elements of F and Q \\ F. The encoding of the states ofthe DFA follows the spirit of one-hot encoding. The transition function is simulated by the encoder and thediscrimination function will be simulated by the decoder. The main idea is to make the hidden states ht, t 1, be an encoding of the state q reached in the DFA whileparsing a sequence up to its tth element. By construction, using the saturation float J, the FP-SRN R willproduce a One-hot hidden vector at each iteration. In order to perform the simulation we need to set thedimension h of the hidden vector to h = || |Q|, and groups of || One-hot encoded vectors will represent aunique state q Q. For example, if = {a, b} an alphabet with two symbols, the vector100. . .0 will correspond to the state q1, but so will010. . .0. More generally, vectors where only one ofthe first || coordinates is equal to 1, and the others are nil, will represent the q1 state. In this construction,Mh will encode the transition function, and Mo will encode the q F relation. In the rest of this sectionwe will refer to the FP-SRN definition version 2 and use the formalism of kernels W the transition kernel Uthe input kernel and v the output kernel (the output kernel is a vector because the output that we need isa scalar in {0, 1}).",
  "R2: All the other coordinates of wk are set to 3": "Note that by definition of a DFA every vector wk has exactly || coordinates set to 1 and (|Q| 1)||coordinates set to 3. Also, by definition of a DFA, every state can be reached by || transitions. The ruleR1 defines an encoding for (qk, as, (qk, as)) Therefore, the vector wk contains all the transitions starting atqk.",
  "The correspondence f will be explicitly defined later. Secondly we will prove that we have:(q1, ) FvhT= 1(8)": "The proof is by induction on k {1, . . . , T}, and we naturally start with k = 1. Let es with 1 s ||,and h0 the initial hidden state vector as defined above. We are going to compute h1 = (Ues + Wh) andshow that h1 will have a unique non zero coordinate whose position encodes the state (q1, es) = (q1, es).We start with Ues:Ues[k] = 2J if (k s modulo ||) and 0 otherwise.",
  "We are going to show that induction hypothesis is true for k + 1": "Let , such that || = k + 1, leth1, . . . , hk+1the sequence of hidden state vectors produced by Rfrom the execution on . We know that = es, where is a prefix of , of length k. We know for surethat || 1 because by assumption k 1. By the induction assumption we have that:",
  "h1, . . . , hkf(q1, e1), . . . , (q1, ),": "and all hidden vectors ht, for 1 t k, are one-hot encoded. We also know by the induction hypothesisthat for j := f(hk) we have qj = (q1, ). It is sufficient to apply the same arguments as in the case k = 1in order to conclude the proof by induction. Indeed by construction of the matrices U and W we know thatthere exists a unique r {1, . . . , |Q|||} such that:",
  "(Ues + Whk)[t] J for t = r": "and by the construction of U and W, and more particularly by construction of the vector wj, a vectorencoding all possible transitions from the state qj, we know that j := r//||+1 corresponds to qj = (q1, ).From the identity just above we can deduce that after the application of the activation function , thesaturation will occur and the outputted vector hk+1 will be one-hot encoded, with a 1 at the position j. Wecan thus conclude that the proof by induction is valid.",
  "This appendix is dedicated to more in-depth explanations of the numerical experiments described in": "For our vanishing gradient experiment we randomly generated a labeled training set of 458 words on thealphabet {a, b}. Words with label 1 are those containing an odd number of as, and the other words arelabeled by 0. We defined a set of goal parameters T arget corresponding to the parameters of the example of.3, and we randomly defined initial parameters Initial. In order to simulate the best case scenariowhere the training of the FP-SRN would lead from Initial to T arget in a straight line, we defined a sequenceof parameter vectors {k}k0 with",
  ":= Initialk+1 := k + (T arget Initial)": "where 0 < is a predefined learning rate. The goal of this setup is to calculate L (Rk(), y) over thewhole training dataset, for k = 0, 1, . . . until L (Rk(), y) < 1014 for all (, y) S.Once thestopping criterion was reached at s, we reproduce the same procedure with a more refined sequence ofvector {k}k0 defined by:",
  ": Training with SGD on the language 04.02.TLTT.4.2.3": "words. We adopted a slightly different method from that is usually used with pytorch or keras frameworks,since for each iteration between 1 and 20,000 we randomly selected 32 words from the 458-word trainingdataset. Usually the dataset is shuffled like a deck of cards, after which the batches are trained and fixed forthe entire training run. The statistics we observed in this experiment are: the Loss, the Euclidean distancebetween two consecutive parameter vectors t t12, L(R(), y), and the euclidean distance to thetarget, the target being the same set of parameters as in the experiment described just above. In a gradientdescent learning scenario, the nature of the data is not known a priori, so the distance to the target makeslittle sense. For this reason, we use the Euclidean distance between two consecutive parameter vectors, inorder to track down some kind of convergence through the Cauchy criterion. Of course, in our experimentalcontext, we know the data precisely, and thus the associated DFA. We therefore also displayed the distanceto the goal parameters. With these experiences, we can conclude that SGD cannot even push FP-SRNparameters into stationary gradient regions. As argued in , the language on the alphabet {a, b} formed of all words containing an odd numberof as falls into the highest complexity subclass of regular languages. To avoid any kind of bias linked tothe complexity of the formal language, we have selected a set of 9 languages in the van der Poel et al.(2024) benchmark. The 9 selected languages cover the entire complexity spectrum of the benchmark. Wereproduced the same learning framework, with learning samples for each 1000-word language and the samelearning algorithm as for the experiment illustrated in . The 9 figures 11,12,13,14,15, 16,17,18and 19 are the reproduction of the experiment 6 on 9 MLRegTest dataset languages van der Poel et al.(2024). We have reproduced exactly the same experiment setup and collected the same statistics as in the6 experiment. For these 9 experiments, we can draw the same conclusions as for the 6 experiment. Wedo not observe any convergence within the different statistics, after 20,000 mini batches. We conclude thatthe complexity of the regular language is not impact-full in this experimental paradigm. We recall that werandomly drew each mini-batch of the 20,000, unlike the usual setup where the training set is divided intomini-batches before training which will not change."
}