{
  "Abstract": "Adversarial Training (AT) has been demonstrated to improve the robustness of deep neuralnetworks (DNNs) to adversarial attacks. AT is a min-max optimization procedure whereinadversarial examples are generated to train a robust DNN. The inner maximization step ofAT maximizes the losses of inputs w.r.t their actual classes. The outer minimization involvesminimizing the losses on the adversarial examples obtained from the inner maximization.This work proposes a standard-deviation-inspired (SDI) regularization term for improvingadversarial robustness and generalization. We argue that the inner maximization is akin tominimizing a modified standard deviation of a models output probabilities. Moreover, weargue that maximizing the modified standard deviation measure may complement the outerminimization of the AT framework. To corroborate our argument, we experimentally showthat the SDI measure may be utilized to craft adversarial examples. Furthermore, we showthat combining the proposed SDI regularization term with existing AT variants improvesthe robustness of DNNs to stronger attacks (e.g., CW and Auto-attack) and improves robustgeneralization.",
  "INTRODUCTION": "The vulnerability of deep neural networks (DNNs) to adversarial perturbations is well documented in machinelearning literature (Goodfellow et al., 2014; Moosavi-Dezfooli et al., 2016; Papernot et al., 2016; Szegedyet al., 2013), prompting concerns about the deployment of DNNs into safety-critical domains. Hence, forthe safe deployment of DNNs, improving their robustness to adversarial perturbations is imperative. Adversarial Training (AT) Goodfellow et al. (2014); Madry et al. (2018) has been demonstrated to be effectivein improving the robustness of DNNs to adversarial attacks. AT is a min-max optimization procedure, wherethe inner maximization step corresponds to finding adversarial examples in the direction of worst-case loss.The outer minimization minimizes the loss on the crafted adversarial examples.The success of AT inimproving the robustness of DNNs to adversarial perturbations has inspired a myriad of variants that haveyielded better robustness or computational efficiency (Zhang et al., 2019; Wang et al., 2019; Li et al., 2019;Andriushchenko & Flammarion, 2020; Wong et al., 2020; Shafahi et al., 2019b). Furthermore, recent workshave employed methods such as adversarial weight perturbation Wu et al. (2020), integration of hypersphere",
  "Published in Transactions on Machine Learning Research (12/2024)": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Guneet S Dhillon, Kamyar Azizzadenesheli, Zachary C Lipton, Jeremy Bernstein, Jean Kossaifi, AranKhanna, and Anima Anandkumar. Stochastic activation pruning for robust adversarial defense. arXivpreprint arXiv:1803.01442, 2018.",
  ". We experimentally show that the proposed SDI regularization on existing AT variants improves thegeneralization to other attacks not seen during adversarial training": "3. In addition, we establish a connection between minimizing the SDI measure and the inner maximiza-tion of the min-max AT procedure. Specifically, we experimentally show that adversarial examplesmay be obtained from adversarial perturbations that minimize the SDI metric. Furthermore, wecompare the success rates of adversarial examples obtained using the SDI metric with those obtainedusing cross-entropy loss and KL divergence on adversarial trained models.",
  "We use bold letters to represent vectors. We denote D = {xi, yi}ni=1 a data set of input feature vectorsxi X Rd and labels yi Y, where X and Y represent a feature space and a label set, respectively": "Let f : X RC denote a deep neural network (DNN) classifier with parameters , and |C| represents thenumber of output classes. For any x X, let the class label predicted by f be F(x) = arg maxk f(x)k,where f(x)k denotes the k-th component of f(x). f(x)y is the probability of x having label y. We denote p as the lp- norm over Rd, that is, for a vector x Rd, xp = (di=1 |xi|p)1p .An -neighborhood for x is defined as B(x) : {x X : x xp }. An adversarial example correspondingto a natural input x is denoted as x. We often refer to the loss resulting from the adversarial attack (innermaximization) as adversarial loss.",
  "ADVERSARIAL ROBUSTNESS": "Adversarial robustness is a machine learning models capability to resist adversarial attacks. Over the pastyears, many methods (Guo et al., 2018; Buckman et al., 2018; Dhillon et al., 2018; Madry et al., 2018;Goodfellow et al., 2014; Zhang et al., 2019) have been proposed to improve adversarial robustness of neuralnetworks. However, some of these defenses have been shown to provide a false sense of defense because theyintentionally or inadvertently used obfuscated gradients in their defenses (Athalye et al., 2018). In a seminal work, Madry et al. (2018) proposed Adversarial training (AT), which involves training the modelwith adversarial examples obtained under worst-case loss to improve robustness. Formally, AT involvessolving a min-max optimization as follows:",
  "maxxB(x) L(f(x), y)(1)": "where L() represents the loss function, y is the true label of input feature x, and are the model parameters.The inner maximization in Eq. (1) aims to obtain a worst-case adversarial version of the input x that increasesthe loss. The outer minimization then tries to find model parameters that would minimize this worst-caseadversarial loss. The efficacy of AT has spurred the development of numerous variants (Zhang et al., 2019;Wang et al., 2019; Wu et al., 2020; Pang et al., 2020). A prominent variant TRADES Zhang et al. (2019) proposed a principled regularization term that trades offadversarial robustness against natural accuracy. Wang et al. (2019) proposed MART, an AT variant thatdifferentiates between naturally misclassified examples that are used in the inner maximization of the ATprocess, using this information to improve adversarial robustness. Wu et al. (2020) proposed adversarialweight perturbation, a double perturbation mechanism that employs the perturbation of inputs and weightsto improve adversarial robustness. More recent AT methods improve existing AT variants by employingreweighting (Zhang et al., 2021; Liu et al., 2021; Fakorede et al., 2023b) or incorporating hypersphereembedding (Pang et al., 2020; Fakorede et al., 2023a). The adversarial examples obtained in the inner maximization step of adversarial training methods are typi-cally crafted using the projected gradient descent (PGD), maximizing the probability estimates of incorrectclasses at the expense of the ground truth.Training on these specific adversarial types often leads tomodels performing well on the PGD adversarial attacks, on which the models are trained but generalizingrelatively poorly to others. To address this, we propose a standard-deviation-inspired regularization termthat explicitly maximizes the probability gap between incorrect classes and the ground truth while boostingthe ground-truth probability. This proposed regularization operates directly on the model output logits,categorizing it as a form of logit regularization. Most existing logit regularization variants (Mosbach et al., 2018; Kannan et al., 2018; Shafahi et al., 2019b;Summers & Dinneen, 2019; Kanai et al., 2021) involve utilizing techniques such as label smoothing and logitsqueezing for improving adversarial robustness. These methods typically encourage smaller logit norms beforesoftmax, which studies such as Shafahi et al. (2019b;a) associate with reduced overconfidence in predictionsand improved adversarial robustness. However, the robustness achieved through these logit regularizationmethods has been criticized as potentially attributed to gradient obfuscation (Athalye et al., 2018; Engstromet al., 2018; Lee et al., 2020; Raina et al., 2024). In contrast, our method operates on the post-softmax logits.It explicitly maximizes the probability gap between actual classes and the probability of incorrect classes,maximizing the confidence in the true classes of individual training samples. Our extensive experiments showthe broad effectiveness of our approach in improving adversarial robustness to various adversarial attacks.",
  "THE SDI MEASURE": "The SDI measure adopts an idea similar to standard deviation to characterize the spread of output prob-ability vectors of DNN models for individual training examples. Specifically, the approach measures thevariation of a models estimated probabilities for incorrect classes around the models estimated probabilityfor the true label of individual input examples x. Formally, given an input-label pair (xi, yi) and the outputprobabilities of a DNN model on input sample xi denoted as f(xi), the SDI measure is given as:",
  "|C| 1(3)": "where |C| is the number of output classes, f(xi)k is the models estimated probability corresponding toclass k, f(xi)yi is the models estimated probability of the true class, and denote the model parameters. Under the condition where f(xi)yi maxk,k=yi f(xi)k, the MSDI(xi, yi, ) measure serves to capture thevulnerability and risk of misclassification of individual examples xi.A smaller value of MSDI(xi, yi, )suggests that the output probabilities returned for sample xi are more evenly distributed among classes,indicating a higher likelihood of misclassification as the model may misclassify it into any of the k 1incorrect classes.",
  "xt+1i xiB(xi)(xti sign(xti MSDI(xti , yi, ))).(5)": "The above adversarial example generation is achieved using the widely adopted PGD algorithm (Madryet al., 2018), with the notable difference that the sign of the gradient is inverted to move in the oppositedirection.For most AT variants, adversarial examples in the inner maximization step are obtained byfinding perturbations that maximize a cross-entropy loss function or a Kullback-Leibler divergence. The SDImeasure does not rely on information-theoretic measures. Therefore, it offers a complementary approach forfinding adversarial examples. We provide experimental evidence for our claim in Sec. 4.5. The outer minimization seeks model parameters that minimize the loss on the adversarial examples generatedduring the inner maximization step.Essentially, the outer minimization process aims to maximize thelikelihood of correctly classifying individual adversarial training examples. Invariably, the outer minimizationminimizes the likelihood of incorrect classification by increasing the probability gap between the examplebelonging to the label and belonging to incorrect classes. This conceptually aligns with the goal of maximizingthe SDI measure. Maximizing the SDI metric encourages the model to correctly classify the input to its trueclass by widening the probability gap between the estimated probability for the true class and the estimatedprobabilities for other incorrect classes. Moreover, when f(xi)y maxk,k=y f(xi)k, maximizing the MSDImeasure maximizes f(xi)y.",
  "Here, we propose the SDI regularization term for improving adversarial training": "Typically, adversarial training techniques involve training models using adversarial examples generated byvarious forms of PGD attacks. However, this approach may lead to overly specialized models defendingagainst PGD attacks, potentially causing poor generalization to different attack types. As discussed earlier,the MSDI metric introduced in the previous section has beneficial characteristics, particularly its ability tomaximize the probability gap between the true class and the other classes. This property aligns well with theobjectives of adversarial training, enhancing its effectiveness. Hence, to improve the robust generalizationand performance of existing AT methods, we propose adding a regularization term that maximizes the MSDImeasure on each training example. Maximizing the MSDI metric as a regularization term encourages the model to maximize the output prob-ability of a training example belonging to its actual class, thus improving training. Moreover, since existingAT variants depend on information-theoretic measures for both the inner maximization step and the outerminimization step, applying the MSDI metric as a regularization term offers a complementary addition toAT methods that does not depend on the information-theoretic measures that these AT methods are based.Lastly, maximizing the MSDI metric facilitates the widening of the probability gaps between the probabilityof the actual class of individual adversarial examples and the probabilities corresponding to incorrect classes,thus improving the discriminability of the model. Note that maximizing the MSDI measure to improve f(xi)y is only valid when f(xi)y maxk,k=y f(xi)k.When f(xi)y < maxk,k=y f(xi)k, maximizing MSDI may further minimize f(xi)y, since the probabilitygap between each f(xi)k,k=y and f(xi)y is further increased to maximize the MSDI measure. Therefore,",
  "KL(f(xi)f(xi)) LSDI(xi, yi, )(9)": "where in Eq. (8) or (9) represents the regularization hyperparameter for controlling the weight of the SDIregularization term, and KL in Eq. (9) represents KullbackLeibler divergence. In the proposed AT-SDI andTRADES-SDI, the LSDI regularization term is selectively applied. The regularization term is only appliedto adversarial training instances satisfying:f(xi)y maxk,k=y f(xi)k. If f(xi)y < maxk,k=y f(xi)k on asample xi, the normal AT orTRADES adversarial training is applied on xi.",
  "EXPERIMENTS": "In this section, we conduct an extensive evaluation of the proposed method. To assess its versatility, we testit on various datasets, including CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009),SVHN (Netzer et al., 2011), and Tiny ImageNet Deng et al. (2009). We apply simple data augmentations,such as 4-pixel padding with 32 32 random crop and random horizontal flip, to each of the datasets.Additionally, we employ ResNet-18 (He et al., 2016) and WideResNet-34-10 (He et al., 2016) as the backbonemodels.",
  "Training Parameters": "We train the backbone networks using mini-batch gradient descent for 110 epochs, with a momentum of0.9 and a batch size of 128. For training CIFAR-10, we used a weight decay of 5e-4, and for CIFAR-100,SVHN, and TinyImageNet, we used a weight decay of 3.5e-3. The initial learning rate was set to 0.1 (0.01for CIFAR-100, SVHN, and TinyImageNet), and it was divided by 10 at the 75th epoch and then again atthe 90th epoch.",
  "Hyperparameters": "We set the value of to 3.0 for training AT-SDI and TRADES-SDI on CIFAR-10, SVHN, and TinyImagenet.For CIFAR-100 using AT-SDI and TRADES-SDI, we set to 3.0. When incorporating AWP (Wu et al.,2020) into AT-SDI and TRADES-SDI, we respectively set to 3.0 and 1.0. The hyperparameters are tunedusing a validation set. We provide the sensitivity analysis of hyperparameter on AT-SDI and TRADES-SDIfor CIFAR-10 using Wideresnet-34-10 in Tables 9 and 10.",
  "BASELINES": "We use prominent methods Standard AT (Madry et al., 2018) and TRADES (Zhang et al., 2019) as ourbaselines. In addition, we compare our results to other popular works MART (Wang et al., 2019), AWP(Wu et al., 2020), MAIL (Liu et al., 2021), and ST-AT (Li et al., 2023). All hyperparameters of the baselinemethods remain consistent with those in their original papers. Nevertheless, we maintain consistency byusing the same learning rate, batch size, and weight decay values as those utilized during the training of ourproposed method.",
  "We evaluate the performance of the proposed method against strong attacks under white-box and black-boxsettings, as well as the Auto attack": "White-box attacks. These attacks have access to model parameters. To assess robustness on CIFAR-10using Resnet-18 and Wideresnet-34-10, we employ the PGD attack with = 8/255, step size = 1/255, andK = 20 iterations (PGD-20). Additionally, we utilize the CW attack (CW loss (Carlini & Wagner, 2017)optimized by PGD-20) with = 8/255 and step size 1/255. On SVHN and TinyImageNet, we use the PGDattack with = 8/255, step size = 1/255, and K = 20 iterations. Black-box attacks. In black-box settings, the adversarial attack method does not have access to the modelparameters. We evaluate robust models trained on CIFAR-10 against strong black-box attack, SPSA (Uesatoet al., 2018), with 100 iterations. These attacks use a perturbation size of 0.001 for gradient estimation, alearning rate of 0.01, and 256 samples for each gradient estimation. All black-box evaluations are conductedon trained Wideresnet-34-10.",
  "PERFORMANCE EVALUATION": "We present our experimental results and comparisons on various datasets using ResNet-18 and WideResNet-34-10 architectures. Specifically, results for CIFAR-10 on ResNet-18 and WideResNet-34-10 are summarizedin Tables 1 and 2, respectively, while results for CIFAR-100, SVHN, and Tiny ImageNet using ResNet-18are presented in Tables 3, 4, and 5, respectively. To further explore the versatility of the proposed method,we evaluate it using a lightweight backbone, VGG-16 architecture (Simonyan & Zisserman, 2014), on theCIFAR-10 dataset. The results are presented in . Additionally, comparisons with other prominent baselines are provided in . Finally, we comparethe perfomance of adversarial examples generated using the SDI metric approach described in Eq. (5) toadversarial examples crafted using cross-entropy and KL-divergence losses.",
  "Comparing AT and TRADES with their SDI-regularized variants": "In this comparison, we evaluate the performance of AT and TRADES against their respective variantswith the SDI regularization term, AT-SDI and TRADES-SDI. Experimental findings indicate that theproposed regularization term enhances robustness against various adversarial attacks, including Autoattacksand CW. Specifically, when applied to ResNet-18 and WideResNet-34-10 architectures on CIFAR-10, AT-SDI demonstrates improvements over AT across all evaluated attacks (see Tables 1 and 2). For example,on WideResNet-34-10, AT-SDI outperforms AT in robustness against PGD-20 (+0.45 %), CW (+2.54 %),and Autoattacks (+1.65 %). The improvement in robustness are achieved without a significant reduction inthe natural accuracy. Similarly, TRADES-SDI exhibits superior performance compared to TRADES on PGD-20 (+1.19 %), CW(+2.06 %), and Autoattacks (+1.14%). Training with TRADES-SDI also exhibit a noticeable improvementof 0.67 % on the natural accuracy. Overall, AT-SDI achieves greater improvement in robustness against CWattacks compared to TRADES-SDI, while TRADES-SDI demonstrates better enhancement against PGD-20attacks compared to AT across ResNet-18 and WideResNet-34-10 architectures. The proposed SDI regularization term also enhances robustness on CIFAR-100 when applied to ResNet-18across all evaluated adversarial attacks (see Table (3)). The margin of improvement in robustness againstadversarial attacks on CIFAR-100 appears to be larger than that observed on CIFAR-10 for both AT-SDIand TRADES-SDI. Similar improvements in robustness are observed when AT-SDI and TRADES-SDI areutilized to train Resnet-18 on SVHN dataset. Results in Table (4) show that AT-SDI outperforms AT onCW (+5.23%), Autoattack (+ 1.20%), and PGD-20 (+2.43%). also clearly shows that the proposed training objective improves the robustness of Resnet-18 againstall the evaluated attacks on Tiny Imagenet.The consistent improvement of performance across all thedatasets tested validates the efficacy of the proposed SDI regularization term. Finally, demonstrates the effectiveness of the proposed training objectives on VGG-16 using theCIFAR-10 dataset. Incorporating the proposed LSDI regularization term into AT results in a marginalimprovement in robustness to PGD-20, with a significant gain of 3.14% against CW and a 2.7% improvementagainst Autoattack. Similarly, TRADES-SDI surpasses TRADES with a 1.3% increase in robustness againstPGD-20, a 1.71% gain against CW, and a 2.55% improvement against Autoattack.",
  "Comparison with other prominent baselines": "Here, we compare our approach with other prominent and state-of-the-art methods from existing works,including MART (Wang et al., 2019), adversarial weight perturbation (AWP) (Wu et al., 2020), ST-AT(Li et al., 2023), LAS AT (Jia et al., 2022), LOAT (Yin & Ruan, 2024) and Randomize-AT (Jin et al.,2023). Additionally, for a fair comparison with AWP, we combine AT-SDI and TRADES-SDI with AWPand denote them as AT-SDI + AWP and TRADES-SDI + AWP, respectively. In both AT-SDI + AWPand TRADES-SDI + AWP, the SDI regularization term is employed for perturbing the network weights.",
  "DefenseNATURALPGD-20CWAASPSA": "Standard AT48.83 0.1423.96 0.1121.85 0.1117.91 0.1226.93 0.10TRADES49.11 0.2122.82 0.1817.79 0.2316.82 0.2027.41 0.15MART46.01 0.0726.03 0.1122.08 0.1719.18 0.0928.15 0.08MAIL ((Liu et al., 2021))49.72 0.3124.09 0.2921.21 0.2317.42 0.1926.68 0.15ST-AT ((Li et al., 2023))48.61 0.0823.85 0.1118.43 0.1017.29 0.0827.91 0.08AWP ((Wu et al., 2020))48.89 0.0924.97 0.1322.39 0.1118.68 0.1728.23 0.10 AT-SDI (OURS)49.73 0.1024.79 0.0823.16 0.1420.01 0.0628.95 0.09TRADES-SDI (OURS)51.77 0.2425.11 0.1721.42 0.1419.71 0.1228.36 0.10AT-SDI + AWP (OURS)50.12 0.2726.14 0.1624.27 0.1320.47 0.1129.07 0.12TRADES-SDI + AWP (OURS)52.87 0.2825.56 0.1723.59 0.1419.83 0.1528.41 0.11",
  "SDI-regularization Improves Generalization of Adversarial Training": "Most AT methods involve training with a specific type of adversarial examples crafted by maximizing eitherthe cross-entropy or KL-divergence measure using PGD. Therefore, the adversarial examples utilized foradversarial training do not entirely reflect the universe of all possible adversarial attacks that a robust modelmay encounter. This limitation can lead to poor generalization of adversarially trained models to other typesof adversarial examples (Song et al., 2018). Typically, adversarial training methods exhibit significantly higher performance on PGD attacks, as evidentfrom the experimental results tables. However, when subjected to other types of attacks, the performance ofrobust models tends to diminish. For example, it can be observed from the tables that the robust accuracyon CW and AA attacks are notably lower compared to PGD-20. The introduction of the LSDI regularization term to the standard AT and TRADES improves their per-formances against other attacks. Unlike other AT methods, AT-SDI considerably improves the robustnessagainst CW and AA on all the datasets evaluated. In fact, training a model using the proposed AT-SDIconsistently improves the performance of the resulting robust model to CW attack and achieve better per-formance over PGD-20 on CIFAR-10 dataset, as may be observed in Tables 1, 2, and 7. Note that CWadversarial examples are not used for training, yet better robust accuracies are recorded compared to PGDadversarial examples, which are typically used for adversarial training. Significant improvement in robustnessagainst CW and AA can also be observed on other datasets, CIFAR-100, SVHN, and Tiny Imagenet. TheSDI regularization term also improves the performance of TRADES on CW and AA attacks. Experimentsin Tables 1 - 5 show that the SDI regularization reduces the performance gap between PGD-20 and the otherattacks. Combining AWP (Wu et al., 2020) with AT-SDI achieves a high robustness of 60.30% on CW, improvingAWP by 4.38% on CIFAR-10. Further, the robustness performance on CW is better than the robustnessperformance on PGD-20. Also in , the improvement in performance is noticeable against AA andSPSA attacks. Similarly, the improvements in robustness to CW AA can be observed in , when theLSDI regularization is applied to TRADES + AWP . Overall, the proposed LSDI regularization term consistently minimizes the performance gaps between robust-ness to PGD-20 adversarial examples and other types of adversarial examples. This supports our argumentthat the LSDI regularization improves the generalization of adversarial training. An intuitive explanationfor this observation is that the LSDI regularization is not dependent on the specific algorithmic nuances ofindividual adversarial attacks and defenses. Instead, it explicitly maximizes the probability gaps betweenthe probability of the true class of each adversarial example and the probabilities corresponding to incorrectclasses.",
  "Here, we study the influence of the regularization hyper-parameter on AT - SDI and TRADES-SDIperformance": "We trained WideResNet-34-10 using AT-SDI with values of 1.0, 2.0, 3.0, 4.0, and 5.0, and TRADES-SDI with values of 1.0, 2.0, 2.5, 3.0, 4.0, and 5.0. We present the results in Tables 9 and 10, whichshows that increasing the value of leads to moderate reduction in the natural accuracy of AT-SDI andTRADES-SDI. The robust accuracy of PGD-20 remains relatively stable for various values of on AT-SDIand TRADES-SDI. Nevertheless, AT-SDI exhibits noticeable improvement in robustness against CW attackas increases. The performance of AT-SDI against Autoattack also improves with increasing value butdiminishes as gets too large. We selected = 3.0 since it maintains a good balance between the naturaland robust accuracy. Like AT-SDI, TRADES-SDI is also sensitive to . The variations in natural accuracy are moderate when is varied. The robustness to PGD-20 remains relatively stable as varies. It can be observed from 10 thatthe robust performance on CW attack and AA increases as increases, but both decrease slightly when is set to 5.0.",
  "Computational Cost": "We conducted all experiments using a single core of an AMD EPYC 7513 processor, an Nvidia A100 SXM480 GB GPU, and 128 GB of RAM. When the proposed LSDI regularization term is added to AT, it increasesthe training time per epoch by no more than 4 seconds for ResNet-18. For context, popular regularizationlosses like KL-divergence and mean square error add up to 8 seconds and 10 seconds per epoch, respectively,under similar conditions and computational resources. Therefore, the LSDI regularization is lightweight andintroduces minimal overhead compared to KL divergence and mean square error losses.",
  "In , we argue that the proposed SDI metric in Eq. (3) can be utilized for crafting adversarialexamples": "Here, we compare PGD-based adversarial examples optimized using the SDI metric with existing popularPGD-based adversarial examples crafted using information-theoretic measures such as cross-entropy (Madryet al., 2018) and KL-divergence (Zhang et al., 2019). The MSDI-measure-optimized adversarial examples arecrafted using the approach described in Eq. (5). We compare the performances of each approach under ATand TRADES. In each case, the attack is obtained using the conventional attack settings: 20 PGD iterations,perturbation bound 0.031, and step size 0.003. The algorithm for obtaining adversarial examples using theMSDI measure is provided in Algorithm 2.",
  "CONCLUSION": "We introduce a novel regularization term based on a standard deviation-inspired (SDI) measure to improveadversarial robustness. The SDI measure captures the spread of a models estimated probabilities withrespect to the true class of each input. We establish a connection between optimizing the SDI measureand the min-max optimization procedure in adversarial training. Specifically, we illustrate that the SDImeasure may be optimized for generating adversarial examples by seeking perturbations that minimize theSDI measure. We demonstrate with experimental study that maximizing the SDI measure on adversarial training examplescontributes to improving the robustness of existing adversarial training methods. Empirical results indicatethat the proposed regularization significantly improves existing adversarial training variants robustness andgeneralization capabilities.",
  "John Hull. Risk management and financial institutions,+ Web Site, volume 733. John Wiley & Sons, 2012": "Xiaojun Jia, Yong Zhang, Baoyuan Wu, Ke Ma, Jue Wang, and Xiaochun Cao. Las-at: adversarial trainingwith learnable attack strategy. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1339813408, 2022. Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xiaowei Huang. Randomized adversarial trainingvia taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1644716457, 2023. Sekitoshi Kanai, Masanori Yamada, Shinya Yamaguchi, Hiroshi Takahashi, and Yasutoshi Ida. Constraininglogits by bounded function for adversarial robustness. In 2021 International Joint Conference on NeuralNetworks (IJCNN), pp. 18. IEEE, 2021.",
  "Xingbin Liu, Huafeng Kuang, Xianming Lin, Yongjian Wu, and Rongrong Ji. Cat: Collaborative adversarialtraining. arXiv preprint arXiv:2303.14922, 2023": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deeplearning models resistant to adversarial attacks. In International Conference on Learning Representations,2018. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accuratemethod to fool deep neural networks. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 25742582, 2016.",
  "Tianyu Pang, Xiao Yang, Yinpeng Dong, Kun Xu, Jun Zhu, and Hang Su. Boosting adversarial trainingwith hypersphere embedding. Advances in Neural Information Processing Systems, 33:77797792, 2020": "Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami.The limitations of deep learning in adversarial settings. In 2016 IEEE European symposium on securityand privacy (EuroS&P), pp. 372387. IEEE, 2016. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conferenceon computer and communications security, pp. 506519, 2017.",
  "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2454424553,2024": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-retically principled trade-off between robustness and accuracy. In International Conference on MachineLearning, pp. 74727482. PMLR, 2019. Jianfu Zhang, Yan Hong, and Qibin Zhao. Memorization weights for instance reweighting in adversarialtraining. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 1122811236,2023. Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli. Geometry-aware instance-reweighted adversarial training. In International Conference on Learning Representations,2020."
}