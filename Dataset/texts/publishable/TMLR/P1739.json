{
  "Abstract": "Feature attribution methods attempt to explain neural network predictions by identifyingrelevant features. However, establishing a cohesive framework for assessing feature attribu-tion remains a challenge. There are several views through which we can evaluate attribu-tions. One principal lens is to observe the effect of perturbing attributed features on themodels behavior (i.e., faithfulness). While providing useful insights, existing faithfulnessevaluations suffer from shortcomings that we reveal in this paper. To address the limitationsof previous evaluations, in this work, we propose two new perspectives within the faithfulnessparadigm that reveal intuitive properties: soundness and completeness. Soundness assessesthe degree to which attributed features are truly predictive features, while completenessexamines how well the resulting attribution reveals all the predictive features. The twoperspectives are based on a firm mathematical foundation and provide quantitative metricsthat are computable through efficient algorithms. We apply these metrics to mainstreamattribution methods, offering a novel lens through which to analyze and compare feature at-tribution methods. Our code is provided at",
  "Introduction": "Understanding predictions of machine learning models is a crucial aspect of trustworthy machine learningacross diverse fields, including medical diagnosis (Bernhardt et al., 2022; Khakzar et al., 2021c;b), drugdiscovery (Callaway, 2022; Jimnez-Luna et al., 2020; Gndz et al., 2023; 2024), and autonomous driving(Kaya et al., 2022; Can et al., 2022). Feature attribution, indicating the contribution of each feature to",
  "Published in Transactions on Machine Learning Research (11/2024)": "Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds,Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum:A unified and generic model interpretability library for pytorch, 2020. Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, and HimabinduLakkaraju.The disagreement problem in explainable machine learning: A practitioners perspective.arXiv preprint arXiv:2202.01602, 2022.",
  "Attribution methods explain a model by assigning a score to each input feature, indicating the importanceof that feature to the models prediction. These methods can be categorized as follows:": "Gradient-based methods: These methods (Simonyan et al., 2014; Baehrens et al., 2010; Springenberget al., 2015; Khakzar et al., 2021a; Zhang et al., 2018; Shrikumar et al., 2017) generate attributionsbased on variants of back-propagation rules. For instance, Simonyan et al. (2014) employs the ab-solute values of gradients to determine feature importance, whereas DeepLIFT (Shrikumar et al.,2017) calculates attributions by decomposing the output prediction of a neural network into contri-butions from each input feature. It uses a reference activation to compare against actual activations,measuring the difference between the neurons activation for a given input and its activation at thereference point. This difference is then propagated backward through the network, layer by layer,using a set of rules specific to the activation functions and network architecture. Hidden activation-based methods: CAM (Zhou et al., 2016) produces attribution maps for CNNs,which are equipped with global average pooling and a linear classification head. CAM multiplies thelast CNN layer activations with the weights in the linear head associated with a target class, andthen computing the weighted average. GradCAM (Selvaraju et al., 2017) further generalizes thisapproach by weighting the activation maps using the gradients, and then computing the weightedsum over the activation maps. Importantly, GradCAM does not require the network to have a globalaverage pooling followed by a linear classification layer, thus can be applied to a broader range ofneural network architectures.",
  "Existing metrics for assessing the faithfulness of attribution methods can be categorized as follows:": "Expert-grounded metrics: These metrics rely on human expertise to interpret and assess the qual-ity of attribution maps produced by attribution methods. Experts visually inspect (Yang et al.,2022) these maps to determine if they highlight the relevant regions, as expected by the modelsfocus. For example, if a classifier identifies an image as containing a basketball, the attributionmap should highlight the basketball itself. Another approach involves the pointing game (Zhanget al., 2018), where the goal is to see if the attribution method assigns the highest value to the pixelwithin the bounding box that localizes the object in question. Additionally, human-AI collaborativetasks (Nguyen et al., 2021) gauge the effectiveness of attribution maps in aiding human classificationefforts. While these methods incorporate human judgment, their outcomes can be subjective andmay lack consistency.",
  "(c) Test accuracy": ": Analysis of retraining-based metrics. Compared to (a) D(1)P,Train, (b) D(2)P,Train introduces anadditional class-related spurious correlation during perturbation, visible in the upper-right region of thesample. (c) Despite equivalent removal of informative features (central portions of images) using both per-turbation strategies, the two retrained models demonstrate different test accuracy (0.66 vs. 0.88), suggestingthat the test accuracy of the retrained model does not accurately reflect the quantity of information removal. features and measuring changes in the models predictions. The underlying premise is that removingan important feature should result in a significant prediction change. Initially introduced by Inser-tion/Deletion (Samek et al., 2016), two feature removal orders are considered: Most Relevant First(MoRF), which starts with the feature having the highest attribution value, and Least Relevant First(LeRF), which begins with the feature having the least attribution value. Plotting the probabilityof the target class against the number of removed features results in two distinct curves for MoRFand LeRF. A faithful attribution method should correctly rank feature importance, resulting in asteep initial drop in the MoRF curve, followed by a plateau, whereas the LeRF curve should showa plateau at the start and a sharp drop towards the end. Hooker et al. (2019) introduced Removeand Retrain (ROAR) to mitigate potential adversarial effects of feature removal by retraining themodel on perturbed datasets. ROAD (Rong et al., 2022) highlighted that perturbation masks couldinadvertently leak class information, potentially misrepresenting feature importance. More recently,Zhou et al. (2022) proposed a method to inject ground truth features into the training dataset,forcing the model to learn exclusively from these features and then testing the ability of attributionmethods to recognize them. Others: Khakzar et al. (2022) introduced empirical evaluation of axioms, and Adebayo et al. (2018)introduced sanity checks for saliency maps, further diversifying the landscape of attribution methodevaluation.",
  "Retraining-based evaluation metrics": "Retraining-based evaluations, such as ROAR (Hooker et al., 2019), involve retraining the model on a per-turbed dataset and measuring the accuracy of the retrained model on a perturbed test set.A sharperdecrease in accuracy suggests a greater information loss resulting from the perturbation of attributed fea-tures, thus indicating better feature attribution. Since the model is retrained, it is not subject to OODeffects instigated by perturbation, as observed in Insertion/Deletion. However, the perturbation may giverise to other spurious features when the original ones are removed. The model might learn these newlyintroduced features, as there are no stringent constraints in the learning process to prevent the model fromdoing so. Therefore, if the retrained model leverages these spurious features rather than relying exclusively",
  "on the remaining ones to make predictions, the testing accuracy might not accurately represent the extentof information loss": "We illustrate the issue of retraining via a falsification experiment, following the ROAR setting. We perturb70% of pixels in each image in the CIFAR-10 (Krizhevsky et al., 2009a) training and test datasets. Themodel is then retrained on the perturbed training set and evaluated on the perturbed test set. Specifically,we employ two perturbation strategies: Strategy 1, which perturbs a central circular region to remove class-related objects (a), and Strategy 2, which perturbs the image center and a small edge region basedon the class label (b), introducing a spurious class correlation. We refer to the Perturbed Trainingset using strategy 1 as D(1)P,Train. Analogously, we have D(2)P,Train, D(1)P,Test, and D(2)P,Test. We then train a model",
  "retrained models on D(1)P,Test and D(2)P,Test. Additional details are in Appendix C. As shown in c, the": "original model performs poorly on D(1)P,Test and D(2)P,Test due to the perturbation on substantial class-relatedpixels. However, the two retrained models achieve distinct accuracy on their test sets. While the modelretrained on D(1)P,Train still performs badly on D(1)P,Test, the model retrained on D(2)P,Train has almost 90% test accuracy on D(2)P,Test, as the latter model learns the spurious correlation introduced by perturbation. Despiteboth perturbation strategies notably disrupting the central informative part of an image, the model retrainedon D(2)P,Train still achieves high test accuracy. Therefore, spurious features can have a great impact on theevaluation outcome.",
  "Evaluation on semi-natural datasets": "If we have access to the features that are truly relevant to labels, we can compare them with attributionmaps to evaluate attribution methods. Zhou et al. (2022) proposed training the model on a dataset withinjected ground truth features. However, our subsequent experiment reveals that the evaluation outcomecan be affected by the design of ground truth features. Moreover, results from semi-natural datasets maydiverge from those on real-world datasets, as utilizing semi-natural datasets changes the original learningtask. The way we construct semi-natural datasets significantly influences the properties of the introduced groundtruth, such as its size and shape. With prior knowledge of semi-natural dataset construction, we can tailorattribution methods to outperform others on this dataset. To illustrate this, we create two Semi-naturalDataset D(1)Sand D(2)Sfrom CIFAR-100 (Krizhevsky et al., 2009b). In the case of D(1)S , numeric watermarks",
  "(b) ROAD on CIFAR-100": ": Evaluation on semi-natural datasets vs. on real-world datasets. Evaluation results ona semi-natural and real dataset can be markedly different. On the semi-natural dataset D(1)S , A dummymethod Rect simply using the prior information about the dataset D(1)Sperforms the best, while it has theworst performance on CIFAR-100. that correspond to class labels are injected (a first row, left column), whereas for D(2)S , each imageis divided into seven regions, and stripes are inserted, acting as binary encoding for class labels (asecond row, left column). For example, watermarks are put in the 4th and 6th regions for class 40 (i.e.,01010002). Additionally, we design Rect attribution method to take advantage of the prior knowledge thatthe watermarks in D(1)Sare square patches, and we design Pooling attribution for D(2)Sby exploiting the factthat a stripe watermark in D(2)Sforms a rectangle spanning an entire row. The name Pooling comes fromthe operation of averaging attribution values within each row-spanning region in D(2)S , then broadcastingthis pooled value across the region. Further details can be found in Appendix D.2. a visualizesattribution maps for Rect and Pooling.Using the Attr% metric (Zhou et al., 2022), we evaluate bothmethods on both datasets (b). Each method excels on the dataset it was designed for but performspoorly on the other, demonstrating the inconsistency of evaluations on semi-natural datasets with differentground truth features. Besides the insights from the previous experiment, we further show the inconsistency between evaluationresults on semi-natural and real datasets. Due to the absence of ground truth on real datasets, we replace theAttr% metric with ROAD (Rong et al., 2022) to assess attribution methods. We utilize CIFAR-100 as thereal dataset and evaluate four distinct attribution methods (details in Appendix D.1). The ROAD resultson semi-natural dataset D(1)Sand CIFAR-100 are depicted in a and b, respectively. Thesefigures demonstrate that our tailored Rect method excels in ROAD evaluation on the semi-natural dataset,particularly at high mask ratios, but underperforms on CIFAR-100, indicating the bias in evaluations onsemi-natural datasets. Similarly, non-customized attribution methods like GradCAM, IG, and DeepSHAPexhibit inconsistent performance across the two datasets, underscoring that evaluation on semi-natural andreal datasets can yield distinct results.",
  "Order-based evaluation metrics": "Many evaluation metrics for feature attribution methods, such as those described by ROAD (Rong et al.,2022) and Insertion/Deletion (Petsiuk et al., 2018), operate by incrementally perturbing features based ontheir sorted indices, which are derived from the feature attribution values. These perturbed inputs are thenfed into the model to compute predictions, which are compared with those from the original input. Thevariation in predictions serves as an indicator of the importance of the perturbed features. Despite thepopularity of these order-based metrics, we emphasize that they only assess the relative attribution order of",
  "Method": "As highlighted in , both model retraining and the construction of semi-natural datasets presentshortcomings that hurting the faithfulness of evaluation. In response, we have developed an alternative ap-proach that avoids model retraining and the creation of additional datasets. Our method employs the fixedtrained model and does not inject ground truth features into the dataset. Consequently, this approach isfree from the shortcomings identified in . Specifically, our approach originates from the observationthat misalignment between attributed features and ground truth predictive features occurs in two distinctways: (1) non-predictive features are incorrectly attributed; (2) predictive features receive zero attribution.Motivated by this observation, we formalize two essential properties of feature attribution: attribution sound-ness and attribution completeness. The combined evaluation of these two properties offers a more refinedand comprehensive assessment of the faithfulness of an attribution method.",
  "Problem formulation": "Our evaluation scenario is restricted to a specific model and dataset. This focus stems from our demonstrationin that the performance of attribution methods can significantly vary across different models anddatasets. To aid readers, we include a table of mathematical notations in this work in Appendix A. Given amodel f that takes a set of features F as input, we define the predictive information measurement function and attribution method as follows:",
  "Definition 4.1 (Predictive information measurement ). For a feature set F and a feature F F,(F, F; f) R0 represents the amount of predictive information of F": "Definition 4.2 (Attribution method ). An attribution method for a model f is a function that assignsa value (F, F; f) R0 as the attribution to each feature F in the feature set F. This value quantifies theimportance or contribution of feature F to the predictions made by the model f. Definition 4.1 and Definition 4.2 establish the frameworks for measuring true information and the attributionprocess within a model, using functions. We provide a concrete illustrative example for better understanding.Consider model f as an image classification model that takes an image as input. Here, the feature set Frepresents the input image, and the feature F is a pixel in the input image. Since we constrain our discussionto a particular model and input data, we omit parameters f and F and use (F) or (F) in the following textfor notation simplicity. For a model f, there only exists a unique that measures the predictive informationfor the model. However, is inaccessible since knowing it requires a complete understanding of the modeland its inner working mechanism. In contrast, there exist numerous possible attribution methods . We candefine the optimality of an attribution method as functional equivalence:",
  "Definition 4.3 (Optimality of attribution method). An attribution method is optimal, if equals": "However, it is challenging to directly compare the attribution method with the predictive informationmeasurement because their analytical forms are usually not accessible. Instead, we assess their outcomes.To do so, we focus on two specific subsets of the feature set F. For a model f, a feature set F, and anattribution method , the predictive feature set I and the attributed feature set A are defined as follows:",
  "Definition 4.4 (Predictive feature set I). I F is a predictive feature set if I = {F F | (F) > 0}.Definition 4.5 (Attributed feature set A). A F is an attributed feature set if A = {F F | (F) > 0}": "In essence, I F represents the features that are utilized by the model for decision-making, while A Fencompasses features identified as significant by the attribution method . Therefore, we can compare thealignment between A and I to determine the faithfulness of the attribution method on a certain featureset.Definition 4.6 (Optimality of attributed feature set A). Given a predictive feature set I, an attributedfeature set A is sound if A I, complete if I A, and optimal (sound and complete) if A = I. Definition 4.6 outlines the necessary conditions for an attributed feature set to be deemed optimal, namely,the set must be both sound and complete. This condition is unique in that the elements in A and Ior,equivalently, the feature indicesmust match exactly. However, simply comparing the feature indices of twosets is insufficient to fully assess the faithfulness of attribution. This is because different attribution methodsmay assign varying values to the same feature, and a feature may carry different predictive information acrossvarious trained models. Therefore, it is crucial to consider both the attribution value and the predictiveinformation associated with a feature to further assess the alignment between A and I. To this end, we firstintroduce the operator | |g to measure the cardinality of a set. Subsequently, we define two metrics togauge the soundness and completeness of A.Definition 4.7 (Operator | |g). Given a feature set F and a function g, |F|g = F F g(F). For , wedefine ||g = 0. In other words, for a set of features F, |F| computes the total attribution of all features in F as determinedby the attribution method , while |F| computes the total amount of predictive information in F. Followingour earlier definitions, we can finally define the two properties of attribution:",
  "|I|": "Based on the above analysis, we present Algorithm 2 for evaluating completeness at an attribution thresholdt.We start by removing input features with attribution values above t.Then we pass the remainingfeatures along with imputed features to the model and report the difference in the model performancebetween the original and the remaining features. A higher score difference means higher completeness. Thedetailed procedure of completeness evaluation is shown in Appendix F. The completeness evaluation divergesfrom the Deletion/Insertion approach in its method of feature removal: it removes features with attributionexceeding a specific value, whereas Insertion/Deletion removes features whose ranking is better than a certainthreshold (e.g. top 20%). Despite the subtlety of this distinction, as discussed in .2, the completenessevaluation is capable of discerning differences in attribution values, a nuance that Insertion/Deletion mayfail to capture.",
  "where I F is the set of predictive features for the model f": "Based on Assumption 4.10, we can compare |I F1| with |I F2| using the model performance giventwo sets of features F1 and F2. Specifically, while model performance is useful for identifying feature setsthat contain more information, it is not suitable for quantifying the precise difference in information. Sucha measurement requires much stronger assumptions than Assumption 4.10. We conjecture Assumption 4.10to be true for models converged in training. Next, we show how to measure soundness and completenessbased on our definitions and Assumption 4.10.",
  "Soundness evaluation": "Owing to the intractability of |A I|, the direct calculation of the soundness of A in a single step isinfeasible. However, we shall demonstrate that an iterative approach can effectively gauge the soundness fora subset Ainc A that is included within the input. To ensure a fair evaluation across various attributionmethods, we define Ainc in our implementation as a subset of the features that possess the highest attributionvalues, satisfying the condition (f(Ainc)) = v > 0. In essence, this subset encompasses predictive featuresthat cumulatively attain a specified predictive level v. A predictive level can be measured in various ways.For instance, in a classification task, it can be measured by Accuracy. The subsequent theorem shows themethodology for identifying the truly predictive portion within Ainc, as well as the means to compute thesoundness.",
  "and Ainc I = , we have |A (F \\ I)| = 0 and |A| = |Ainc I| + |A (F \\ I)| = |Ainc I|.Therefore, the soundness of Ainc is |AincI|": "|Ainc|=|A||Ainc| . We can also prove that our minimizer A is the setthat contains all predictive information of Ainc. |A(F \\I)| = 0 indicates A I since (F) > 0 holds forall F A A. Next, A I and A Ainc yields A (Ainc I). Combining with |A| = |Ainc I|,we have A = Ainc I. Theorem 4.11 shows that the soundness of Ainc can be computed by finding an element from Sv(Ainc) thathas the minimum attribution. depicts the relationship between A, Ainc and I. Additionally,it is crucial to recognize that our definition of Ainc satisfies the conditions in Theorem 4.11, specificallyAinc A and Ainc I = . This inequality holds because (f(Ainc)) > 0, and Assumption 4.10 implies|Ainc I| > ||. To facilitate comparison between different attribution methods, we can compute and compare their soundnessat a fixed predictive level v. Algorithm 1 shows how to compute the soundness at predictive level v. Wegradually include features with the highest attribution values in the input. During the set expansion ofAinc, we simultaneously perform minimization as shown in Theorem 4.11 by examining and excluding non-predictive features based on the change in the models performance. After reaching predictive level v, we candirectly calculate soundness based on the set Ainc and the optimized set A. Nonetheless, Algorithm 1 onlyapproximates the actual soundness. The iterative algorithm uses accuracy to identify informative features,which may introduce bias, as certain features could be more contributive in conjunction with different setsof features. To minimize this bias, our approach incorporates a batch of features rather than a single featureat each expansion step.Moreover, we sequentially include features from highest to lowest attribution,halting at a specific accuracy threshold before saturation to capture potentially important features.Acomprehensive description of the soundness evaluation algorithm is provided in Appendix E. Notably, linearimputation (Rong et al., 2022) is used in soundness (and completeness) evaluation procedures to mitigateOOD effects caused by feature removal (as we progressively include a portion of features). It is crucialto recognize that the soundness evaluation differs fundamentally from classical Insertion/Deletion metrics.While soundness evaluates the ratio of attribution values associated with two feature sets, Insertion/Deletionassesses accuracy following feature removal and employs attribution values solely for feature sorting.",
  ": Return st = s0 Accuracy(f, D)// compare st for completeness comparison": "also remove predictive features. However, if the attribution method has low completeness, removing theattributed features would not eliminate all predictive features in the input. Theorem 4.12 tells us how tocompare the completeness using the remaining features after the removal of attributed features. Theorem 4.12. Let A1 and A2 be the attributed features given by two attribution methods, respectively. If(f(F \\ A1)) < (f(F \\ A2)), then the attribution method associated with A1 is more complete than the oneassociated with A2.",
  "Experiments": "In this section, we begin by validating our proposed metrics in a controlled setting, serving as a sanity check.We subsequently underscore the importance of considering attribution values during evaluation, rather thanjust focusing on feature ranking order. This allows for differentiation between methods that rank featuresidentically but assign differing attribution values. Lastly, we further demonstrate that using our two metricstogether provides a more fine-grained evaluation, enabling us to gain a deeper understanding of how anattribution method can be improved.",
  "Validation of the proposed metrics": "In this section, we first validate whether the metrics work as expected and reflect the soundness and com-pleteness properties. In other words we evaluate whether the proposed algorithms follow the predictionsof our theories. We empirically validate the soundness and completeness metrics using a synthetic setting.Through a designed synthetic dataset and a transparent linear model, we obtain ground truth attributionmaps that are inherently sound and complete. These inherently sound and complete attribution maps arethen modified to probe expected effects in completeness or soundness, allowing us to test how our proposedmetrics behave in different situations. By increasing the attribution values of non-predictive features, weintroduce extra attribution (termed as Introduce) which hurts soundness but improves completeness. Con-versely, removing attribution (denoted as Remove) lowers completeness without influencing soundness. Our",
  "(b) Completeness": ": Benchmark of different methods. Although no method exhibits superior performance in bothCompleteness and Soundness, some of them perform well in one of these metrics, implying their suitabilityfor applications which have high demand for the corresponding property. practical applications. One limitation of our evaluations is that their efficacy hinges on the assessment ofmodel performance. Accuracy may not be the appropriate performance metric in some cases. Therefore,additional research in the future is needed to find better performance indicators for different tasks.Inaddition, Theorem 4.11 does not limit the selection of Ainc, and better set expansion strategies for Ainccould yield more precise evaluation outcomes.",
  "objective is to evaluate these modified attribution maps to ensure our proposed metrics accurately capturechanges in both soundness and completeness": "The synthetic two-class dataset consists of data points sampled from a 200-dimensional Gaussian N(0, I).Data points are labeled based on the sign of the sum of their features. Let xi be the i-th input feature and() is a step function that rises at 0. The linear model, formulated as y = ( i xi), is designed to replicatethe data generation process and is transparent, allowing us to obtain sound and complete ground truthattribution maps. Appendix G.1 provides further details. We randomly add and remove attribution fromground truth feature maps 1000 times each. Then, we compare the soundness and completeness betweenmodified and original attribution maps. Statistical results in show that Remove consistently outperforms ground truth attribution in Com-pleteness, whereas Introduce underperforms ground truth attribution in Completeness. In Soundness evalua-tion, the ranking of the three methods inverses. Note that the optimum soundness of ground truth attributionis 1, which can be also reached by Remove. In conclusion, the evaluations behave as expected, validatingour proposed metrics in this case.",
  "Comparison with order-based metrics": "Attribution methods aim to determine the contribution values of features beyond merely ranking them byimportance. Consequently, evaluating these methods necessitates consideration of the actual attributionvalues. Both Completeness and Soundness metrics incorporate attribution values: the former uses value-based thresholds for feature removal, while the latter, denoted as|A||Ainc| , inherently captures variations inattribution values. Next, we show that this consideration of attribution values results in a more refinedevaluation. For the following experiments, we employ a VGG16 (Simonyan & Zisserman, 2015) pre-trained on Im-ageNet (Deng et al., 2009) and conduct feature attribution on the ImageNet validation set.We applythe Remove and Introduce modifications to the original attribution maps produced by a given attributionmethod, such as GradCAM, as visualized in a. These modifications are intentionally designed toslightly adjust the ordering of attributions, yet they significantly alter the attribution values. Consequently,the original attribution maps and those modified by Remove and Introduce are differentiated not just in termsof attribution values but also in their visual presentation, as illustrated in a. A well-designed evalu-ation metric must be capable of capturing these differences clearly. Therefore, for a metric to be consideredeffective, the curves representing the evaluation results for the original, Remove-, and Introduce-modifiedattribution maps should be distinct and non-overlapping. As illustrated in , the curves representing Remove and Introduce overlap in ROAD and Deletion.This is attributed to the fact that these metrics are based solely on the order of attribution, which remainsnearly unchanged between Remove and Introduce-modified attribution maps. In contrast, the curves for the",
  "(e) Deletion (MoRF)": ": Analysis of our metrics and order-based metrics. (a) Modified attribution maps. Themodifications result in only minimal changes to the feature order.These modified maps are noticeablydifferent from the original. An evaluation metric should capture this distinctiveness. By taking attributionvalues into account, Completeness (b) and Soundness (c) aptly distinguish the modifications in the attributionmaps. Conversely, the differences between curves are less obvious in ROAD (d) and Deletion (e). A sidenote on (c) is that Remove might not always preserve soundness. This is because original attribution mapsare not always the same as ground truth maps (inaccessible in the real world), and Remove can eliminateboth predictive and non-predictive features. value-sensitive metrics Completeness and Soundness are noticeably distinct, highlighting their sensitivityto changes in attribution values.To quantitatively assess the differences between evaluation curves, wecalculate the minimal Hausdorff distance between pairs of curves, denoted as minp,q Hausdorff(p, q), wherep, q {Original, Introduce, Remove}. A minimal Hausdorff distance approaching zero signifies an overlapin the evaluation results, indicating that the metric fails to distinguish between the modified and originalattribution maps. We implement three pairs of different modification schemes for Introduce and Remove, which are elaboratedin Appendix G. These modification schemes were applied to attribution maps generated by GradCAM, IG,and ExPerturb, and the process of modification and evaluation was iterated for each scheme. The resultingminimal Hausdorff distances were then averaged. As indicated in , the Hausdorff distances for Com-pleteness and Soundness metrics are significantly greater than zero. This demonstrates that these metricscan effectively differentiate between the modified and original attribution maps, even when the changes inattribution order are minimal. Conversely, the order-based metrics, ROAD and Deletion, demonstrate over-laps in their curves, indicating their inadequacy in discerning subtle distinctions. This contrast highlights thesuperior sensitivity of Completeness and Soundness metrics in evaluating the nuances of attribution maps.",
  ": Benchmark of IG ensembles. By employing both the Completeness and Soundness metrics,we can see that the superiority of ensemble methods over IG is predominantly in their soundness": "Benchmark of ensemble methodsSeveral ensemble methods have been proposed as a means to im-prove attribution methods.In this study, we focus specifically on three ensembles of IG: SmoothGrad(IG-SG) (Smilkov et al., 2017), SmoothGrad2 (IG-SQ) (Hooker et al., 2019), and VarGrad (IG-Var) (Ade-bayo et al., 2018). Intriguingly, only IG-SG displays an enhancement in completeness (b), consistentwith visual results from earlier studies. We present supplementary visual results in Appendix G.4 for furtherscrutiny. Previous research (Smilkov et al., 2017) has observed that gradients can fluctuate significantly inneighboring samples. Consequently, the aggregation of attribution from neighboring samples can mitigatefalse attributionspecifically those arising from non-predictive features receiving attributionand notablyenhance soundness, as depicted in a. However, the benefits of ensemble methods are not so clear inROAD (c) or Deletion (d). Benchmark of various attribution methodsWe conduct a comparative analysis of multiple attributionmethods using our metrics with the goal of guiding the selection of suitable methods for diverse applications.As illustrated in , most of the evaluated methods excel in one metric over the other, suggestingtheir suitability varies based on specific scenarios. For applications like clinical medicine, where capturingall relevant features is essential, methods with higher completeness, like ExPerturb, stand out.On theother hand, in situations where falsely identifying non-predictive features as significant could be detrimental,methods showcasing superior soundness, such as IBA or GradCAM, are preferable.",
  "Conclusion and limitations": "In this paper, we first revealed the potential pitfalls in existing faithfulness evaluation of attribution methods.Subsequently, we defined two important properties of attribution: soundness and completeness. We alsoproposed methodologies for measuring and comparing them. The two metrics work in conjunction and offera higher level of differentiation granularity. Empirical validation convincingly demonstrated the effectivenessof our proposed metrics. Furthermore, we undertook a benchmark of ensemble methods, revealing thatthese methods can considerably improve the soundness of the baseline. Lastly, we extended the comparativeanalysis to a broader range of attribution methods to provide guidance for selecting methods for different",
  "Jos Jimnez-Luna, Francesca Grisoni, and Gisbert Schneider. Drug discovery with explainable artificialintelligence. Nature Machine Intelligence, 2(10):573584, 2020": "Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncertainty-aware deepmulti-view photometric stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1260112611, 2022. Ashkan Khakzar, Soroosh Baselizadeh, Saurabh Khanduja, Christian Rupprecht, Seong Tae Kim, and Nas-sir Navab. Neural response interpretation through the lens of critical pathways. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1352813538, June2021a. Ashkan Khakzar, Sabrina Musatian, Jonas Buchberger, Icxel Valeriano Quiroz, Nikolaus Pinger, SorooshBaselizadeh, Seong Tae Kim, and Nassir Navab. Towards semantic interpretation of thoracic disease andcovid-19 diagnosis models.In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 499508. Springer, 2021b. Ashkan Khakzar, Yang Zhang, Wejdene Mansour, Yuezhi Cai, Yawei Li, Yucheng Zhang, Seong Tae Kim,and Nassir Navab. Explaining covid-19 and thoracic pathology model predictions by identifying infor-mative input features. In International Conference on Medical Image Computing and Computer-AssistedIntervention, pp. 391401. Springer, 2021c. Ashkan Khakzar, Pedram Khorsandi, Rozhin Nobahari, and Nassir Navab. Do explanations explain? modelknows best. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pp. 1024410253, June 2022.",
  "Vitali Petsiuk, Abir Das, and Kate Saenko. Rise: Randomized input sampling for explanation of black-boxmodels. BMVC, 2018": "Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. A consistent andefficient evaluation strategy for attribution methods. In International Conference on Machine Learning,pp. 1877018795. PMLR, 2022. Wojciech Samek, Alexander Binder, Grgoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Mller.Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neuralnetworks and learning systems, 28(11):26602673, 2016.",
  "Lloyd S Shapley et al. A value for n-person games. 1953": "Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagatingactivation differences. In International conference on machine learning, pp. 31453153. PMLR, 2017. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualisingimage classification models and saliency maps. In Yoshua Bengio and Yann LeCun (eds.), 2nd InternationalConference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, WorkshopTrack Proceedings, 2014.",
  "Scott Cheng-Hsin Yang, Nils Erik Tomas Folke, and Patrick Shafto. A psychological theory of explainability.In International Conference on Machine Learning, pp. 2500725021. PMLR, 2022": "Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-downneural attention by excitation backprop. International Journal of Computer Vision, 126(10):10841102,2018. Yang Zhang, Ashkan Khakzar, Yawei Li, Azade Farshad, Seong Tae Kim, and Nassir Navab. Fine-grainedneural network explanation by identifying input features with predictive information. Advances in NeuralInformation Processing Systems, 34:2004020051, 2021. Yang Zhang, Yawei Li, Hannah Brown, Mina Rezaei, Bernd Bischl, Philip Torr, Ashkan Khakzar, andKenji Kawaguchi. Attributionlab: Faithfulness of feature attribution under controllable environments. InNeurIPS 2023 Workshop XAI in Action, 2023. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep featuresfor discriminative localization. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 29212929, 2016. Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution methods correctlyattribute features?In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.96239633, 2022.",
  ": Table of notations": "va predictive level (i.e. a specific level of model performance)fa model to be explaineda performance metric to assess the performance of model fDa (labeled) datasetD(i)P,Traina perturbed training set using perturbation strategy iD(i)P,Testa perturbed test set using perturbation strategy iD(i)Sa semi-natural dataset constructed by modifying the original dataset using modification method iFa set that contains all features in the datasetAa set of attributed featuresIa set of predictive features for the model| |the operator to calculate the sum of attribution| |the operator to calculate the sum of class-related informationFa single feature in F(F)a function that returns the information value of a feature F(F)a function that returns the attribution value of a feature FAinca subset of the most salient features that reach (f(Ainc)) = v > 0Sv(Ainc)for a given set Ainc, we define Sv(Ainc) = {S Ainc : (f(Ainc)) = (f(S))}Aminimizer of minSSv(Ainc) |S|",
  "BBroader Impacts": "We believe that our proposed completeness and soundness evaluations open up many innovative directions.For instance, we have shown that the ensemble methods can greatly enhance the soundness of baselinessuch as IG and DeepSHAP. However, the gain in completeness is very marginal. It would be interestingto investigate how to also improve the completeness of IG, DeepSHAP, or their ensembles. In addition,Extremal Perturbations demonstrate lower soundness than IBA and GradCAM that perform attribution onthe hidden neurons. This might suggest that the semantic information in hidden layers can be utilized inthe optimization process of the Extremal Perturbations to reduce false attribution.",
  "CAdditional experiments for revealing the issues with retraining-based metrics": "In this section, we report an additional experiment to further illustrate the issue with retraining-based eval-uation. For this additional experiment, we use the CIFAR-10 (Krizhevsky et al., 2009a) dataset. The modelis a tiny ResNet (He et al., 2016) with only 8 residual blocks. Training is conducted using Adam (Kingma& Ba, 2015) optimizer with a learning rate of 0.001 and weight decay of 0.0001. The batch size used forthe training is 256, and we train a model in 35 epochs. Next, we describe how to construct the maliciouslymodified dataset for retraining. In the retraining experiment shown in , we generate a modified dataset from the original CIFAR-10dataset. In this additional experiment, we only perturb 5% of each training image and replace the perturbedpixels with black pixels. The perturbation is correlated with class labels. For different classes, we selectdifferent positions close to the edge of the image so the object (usually at the center of the image) is barelyremoved.",
  "(b) Evaluation on unperturbedtest set": ": Retrain on the perturbed dataset with spurious correlation. (a) illustrates sample images fromthe perturbed training set. Only a small portion of pixels on the edge is removed. Hence, the class objectis usually intact after perturbation.However, the position of the removed region depends on the labelof the image. (b) Test accuracy on the unperturbed test set. Although objects are not removed in theperturbed training set, the retrained model achieves much lower test accuracy on the original test set thanthe model trained on the original dataset. This means that the retrained model ignores the object but learnsto perform classification based on the spurious correlation introduced by perturbation. We would like tofurther demonstrate the issue of retraining, that the retrained model fails to learn exclusively from remainingfeatures in the perturbed dataset. Hence, we cannot use the model performance to measure the informationloss caused by perturbation.",
  "(c) Pure synthetic dataset": ": ROAD evaluation on different datasets. The performance of attribution methods is very distinctacross different datasets. We observe that the ROAD result on the original dataset is different from semi-natural dataset and pure synthetic dataset. However, the ROAD result on the semi-natural dataset is verysimilar to the result on the pure synthetic dataset. This implies that the semi-natural dataset changes theimplicit learning task from learning representations of the original images to learning representations ofsynthetic symbols introduced during dataset construction. Subsequently, the task is greatly simplified andsubstantially divergent from the real dataset. In .2, we argue that the attribution methods can behave much differently when explaining themodel retrained on a semi-natural dataset. As a result, it is not faithful to use the evaluation result on thesemi-natural dataset as an assessment for the feature attribution methods. In this section, we demonstratethis issue with an experiment. We first show the datasets used in the experiment. we re-assign the labels for CIFAR-100 (Krizhevsky et al.,2009b) images as suggested in (Zhou et al., 2022). Next, we inject two types of watermarks into the imagesand a blank canvas, obtaining two pairs of semi-natural and pure synthetic datasets, respectively. Notethat the semi-natural datasets are also used in .2. The watermarks are designed as follows:",
  "Number watermark: as depicted in a left, we first insert a black rectangular region inthe image and then put the white number sign within the black region": "Stripe watermark: as depicted in b left, we first encode the label into a 7-digit binarynumber and divide the image into 7 equal-height regions. Next, we set the pixels in each region to255 if the corresponding digit is 1; otherwise, we leave the pixels unchanged. The following experiment is conducted on the semi-natural and pure synthetic dataset with number water-marks. We first train a VGG-16 on the CIFAR-100, semi-natural, and pure synthetic datasets, respectively.After obtaining the classifiers, we apply GradCAM, IG, and DeepSHAP to them to get the attribution maps.In the end, we benchmark the three attribution methods on each dataset using ROAD (Rong et al., 2022). The models achieve 70.4% on CIFAR-100, 99.4% on the semi-natural dataset, and 99.9% on the pure syntheticdataset, respectively. The difference in accuracy shows that the learning tasks are differently complex acrossthree datasets. Furthermore, as depicted in , GradCAM outperforms IG and DeepSHAP on CIFAR-100, while IG and DeepSHAP are much better than GradCAM on the semi-natural and pure syntheticdatasets. The evaluation results on the semi-natural dataset cannot correctly reflect attribution methodsperformance on the real-world dataset.",
  "dataset with stripe watermarks (i.e., D(2)Sin .2). Both attribution maps are generated based on IGattribution maps. The following are the design details:": "Rect is designed to fit D(1)S , where the modified pixels (i.e., Effective Region in (Zhou et al., 2022))are the black rectangular region (where the numbers are located) that we injected into an image.To craft attribution maps, we can also put a rectangular region full of value 1.0 on a backgroundof value 0.0. The question is where to put such a rectangular region. For each IG attribution map,we average across the pixels spatial locations using the attribution values as weights, obtaining aweighted center of the attribution map. Then, we put the rectangular region of spatial size 6060at the weighted center. More samples are shown in a. Pooling is designed to fit D(2)S , where the modified pixels are the equal-height regions associatedwith digit 1. After knowing the shape of watermarks, we can design attribution maps composed of7 equal-height regions. To do so, we apply average pooling to each IG attribution map, obtaininga 7-element attribution vector. Next, we fill each region in the crafted attribution map with thecorresponding value in the attribution vector. More samples are shown in b.",
  "EImplementation Details of Soundness Metric": "Values in attribution maps are usually continuous. Hence, it is possible that an attribution method only hassatisfactory performance only in a certain attribution value interval. To evaluate the overall performanceof an attribution method, we use Algorithm 1 as a basic building block to establishing the progressiveevaluation procedure. Specifically, we evaluate soundness at different predictive levels indicated by the modelperformance. How soundness is calculated at specific predictive level has been explained in .2. Since the attribution method considers features with higher attribution values to be more influential for themodel decision-making, we expand our evaluation set by gradually including the most salient features thatare not yet in the evaluation set. As shown in Algorithm 3, the expansion of the evaluation set happens bydecreasing the mask ratio. For the soundness metric, the mask ratio v means that the top v pixels in anattribution map sorted in ascending order are masked (i.e., area-based LeRF). We start from v = 0.98 anddecrease v by the step size of 0.01. This is equivalent to first inserting 2% of the most important pixels ina blank canvas and inserting 1% more pixels at each step. If the accuracy difference between the currentstep and the previous step is smaller than the threshold 0.01, then the attribution of newly added pixels isdeemed to be false attribution and will be discarded. Algorithm 3 demonstrates a more detailed procedurecompared to Algorithm 1 in .2. Note that some notations are overloaded.",
  "To obtain the overall completeness performance of an attribution method, we again select subsets of anattribution set and evaluate the completeness of these subsets": "Algorithm 4 demonstrates the full computation process. For the completeness metric, the attribution thresh-old t means that the pixels with attribution between [t, 1] will be masked (i.e., value-based MoRF). We startfrom v = 0.9 and decrease t by the step size of 0.1. Compared to Algorithm 2 in .3, the pseudo-codein Algorithm 4 is more detailed and closer to the actual implementation. Note that some annotations areoverloaded.",
  "G.1Validation Tests": "We create a two-class dataset of 1000 sample data points, and each data point has 200 features. The datapoint is sampled from a 200-dimensional N(0, I) Gaussian distribution. If all features for a sample point sumup to be greater than zero, we assign a positive class label to this sample. Otherwise, a negative class label is",
  "Algorithm 3 Soundness evaluation with accuracy (sm) as performance indicator": "1: Input: f: model; D = {x(i), y(i)}Ni=1: labeled dataset with attribution maps {A(i)}Ni=1; : perturbationfunction; : noisy linear imputation function; M = {0.99, 0.98, 0.97, . . . 0.01}: mask ratios (by area);Accuracy: accuracy evaluation function. : accuracy threshold; NewAdded: function that identifies theincluded features at the current step and newly added features compared to the last step.",
  ": end forReturn: P": "assigned (as described in the main text). The model is a linear model and can be formulated as y = (i xi),where xi is the i-th feature, and () is a step function that rises at x = 0, (x) = 1 if x < 0, and (x) = 1if x > 0. In other words, the model also sums up all features of the input and returns a positive value ifthe result is greater than zero. Hence, the model can classify the dataset with 100% accuracy. Lastly, wedescribe how we create ground-truth attribution maps for this model and dataset. As the model is a linearmodel, and each feature xi is sampled from a zero-mean Gaussian distribution, the Shapley value for xi with(xi) = 1 is then 1(xi E[x]) = xi. Similarly, the Shapley value for xi with (xi) = 1 is xi. We confirmthat attribution maps generated by Shapley values are fully correct for linear models. As a result, for positivesamples, the attribution values are the same as feature values. For negative samples, the attribution valuesare the negation of feature values, which means that negative features actually contribute to the negativedecision. Finally, we modify the attribution maps to be compatible with our soundness and completenessevaluation. Since our evaluation only supports positive attributions, we clip negative attribution values tozero. This conversion step has no negative effect on the actual evaluation. The rest of the evaluation setupis identical to other experiments.",
  "G.2ImageNet images for feature attribution": "We randomly select 5 images for each class in the ImageNet validation set, obtaining a subset with 5000images. When performing attribution, the images and attribution maps are resized to 224224 before beingfed into the pretrained VGG16 model. This subsection presents the configurations for generating attribution maps on ImageNet. For GraCAM,We resize the resulting attribution maps to the same size as the corresponding input images. We use theimplementations of GradCAM, DeepSHAP, IG, IG ensembles in Captum (Kokhlikyan et al., 2020). Somehyper-parameters for producing attribution maps are:"
}