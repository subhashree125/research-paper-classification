{
  "Abstract": "Recent advances in offline reinforcement learning (RL) have relied predominantly on learningfrom proprioceptive states. However, obtaining proprioceptive states for all objects may notalways be feasible, particularly in offline settings. Therefore, RL agents must be capableof learning from raw sensor inputs such as images. However, recent studies have indicatedthat visual distractions can impair the performance of RL agents when observations inthe evaluation environment differ significantly from those in the training environment. Thisissue is even more crucial in the visual offline RL paradigm, where the collected datasets candiffer drastically from the testing environment. In this work, we investigated an adversarial-based algorithm to address the problem of visual distraction in offline RL settings. Ouradversarial approach involves training agents to learn features that are more robust againstvisual distractions.Furthermore, we proposed a complementary dataset to add to theV-D4RL distraction dataset by extending it to more locomotion tasks.We empiricallydemonstrate that our method surpasses state-of-the-art baselines in tasks on both the V-D4RL and proposed dataset when evaluated on random visual distractions.",
  "Introduction": "Common model-free (Kumar et al., 2020; Fujimoto & Gu, 2021; An et al., 2021; Kostrikov et al., 2022)and model-based (Kidambi et al., 2020; Guo et al., 2022) offline reinforcement learning (RL) algorithmsrely on learning from the proprioceptive states to address continuous control tasks. However, obtainingproprioceptive states is almost impractical in certain scenarios, such as large-scale environments, wheredefining the states of all objects is nearly unfeasible. Therefore, RL agents must learn from raw sensoryinputs such as images. Training offline RL agents based on visual observations provides opportunities tomake RL more widely applicable to real-world settings. Unlike offline RL agents trained from proprioceptivestates, studies on offline RL agent training using visual observations (Lu et al., 2023) for continuous controltasks and well-designed benchmarks are scarce. Frequently, using a visual encoder, we can estimate the latent proprioceptive features of an image-basedobservation and use these latent features to train the RL backbone algorithms (Hansen et al., 2021a; Yaratset al., 2022). Although this well-established approach is effective in scenarios with static backgrounds, it has",
  "adversariallosspushing": ": Our problem formulation. The agent is trained on two types of domains: a normal observationand a distracted observation. Intuitively, we can say that the latent features consist of agent features anddistraction features. The visual encoder is incentivised by the adversarial training loss to extract latentfeatures that are common to both domains by suppressing domain-specific visual distractions. limitations when confronted with visual distractions. These distractions may include changes in backgroundelements, viewpoints, or variations in the agents colour scheme. Existing RL agents, whether offline oronline, underperform in the presence of random visual distractions (Cobbe et al., 2020; Stone et al., 2021;Wang et al., 2021; Dupuis et al., 2022). By contrast, humans possess a remarkable ability to disregardvisual distractions when observing actions from a visual input. This adeptness highlights our capacity toconcentrate on domain-invariant features, focusing on aspects that remain consistent across diverse settings,such as an agents movement patterns. We hypothesised that existing vision-based RL algorithms are susceptible to random visual distractionsbecause the visual encoder in these RL algorithms tends to overfit certain visual distractions during training.Consequently, these algorithms struggle to estimate robust latent features when exposed to unseen visualdistractions during the evaluation. Our objective is to train the visual encoder in RL agents such that theycan learn domain-invariant features within offline RL settings. In this work, we investigated an adversarial learning approach to address the visual distraction issue in offlineRL settings by extending a technique originally developed for domain adaptation (Ganin & Lempitsky, 2015).One important assumption is the accessibility to datasets from two types of domains, such as a datasetwithout distraction and another dataset with visual distractions. We consider a framework where the agent is trained on two types of domains and subsequently evaluatedon other unseen visual distractions. We introduce a domain discriminator that attempts to classify thelatent features estimated by a visual encoder into two classes. The visual encoder is trained to minimisethe discrepancy between the latent features of the two domains, whereas the domain discriminator aimsto maximise this discrepancy. The problem formulation and schematics of the adversarial training methodare presented in . The visual encoder is incentivised by the adversarial training loss to extractlatent features common to both domains by suppressing domain-specific visual distractions. Furthermore,we empirically demonstrated that incorporating DropBlock (Ghiasi et al., 2018) into a visual encoder isessential for improving the robustness of the proposed domain-adversarial training scheme. To investigate the performance of offline RL algorithms in the presence of unseen distractions, we empiri-cally evaluated existing offline RL algorithms on the V-D4RL distraction dataset. For a more comprehensivecomparison, we collected offline datasets from additional locomotion tasks. We observed that these agentsperformed well when evaluated with previously seen distractions present in the training dataset, but exhib-ited a significant drop in performance when encountering unseen distractions during evaluation. illustrates this phenomenon. In summary, the contributions of this work are as follows: 1. We empirically demonstrated that commonly used offline RL methods are frequently susceptibleto visual distractions. This highlights the need to develop techniques to enhance policy robustnessagainst visual distractions.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Results comparison on cheetah-run distraction dataset (V-D4RL). The reported numbers areIQM and 95% stratified bootstrap CIs in between parentheses. Abbreviations: normal: N, dis-easy: E, dis-medium: M, dis-hard: H. e: expert. m: medium. m-e: medium-expert. To save space, we note DrQv2+BCas DB, and DrQv2+BC with dropblock (drop probability = p) is noted as DB-p.",
  "Domain Invariance in RL": "Recent advancements in RL, such as RAD (Laskin et al., 2020), DrQ (Yarats et al., 2021), and DrQv2(Yarats et al., 2022), have demonstrated the effectiveness of data augmentation techniques in improving theRL performance, sample efficiency, and generalisation. Particularly in vision-based RL, data augmentationhas emerged as a prominent strategy for reducing the reliance on specific training domains. These augmen-tation schemes enhance the robustness of the encoder representations and subsequently improve the policy,yielding superior results compared with previous methods (Yarats et al., 2022). For online visual-based RL,(Cobbe et al., 2019) investigated the effectiveness of common computer vision techniques for improving RLgeneralisation, and (Li et al., 2021) employed a gradient reversal layer in online RL settings. Various visual-based generalisation methods include PAD (Hansen et al., 2021b) which employs self-supervised learning during evaluation; SVEA (Hansen et al., 2021a) which updates typical CNNs in encodersto transformers and uses stronger image augmentation; and ILA (Yoneda et al., 2022) which uses dynamicmodels to adapt during evaluation. In a broader context, Igl et al. (2019); Islam et al. (2023) proposed usingthe variational information bottleneck theory to enhance domain generalisation in RL. They introduced anadditional policy gradient objective to simultaneously minimise the mutual information between differentinputs, alongside actor-critic methods, thereby achieving better feature robustness. Our work focuses specif-ically on the robustness of the visual encoder, distinguishing it from previous methods. Domain invariance isalso an important topic from the RL safety perspective, and (Haider et al., 2021) provided an overview anddemonstrated how RL agents struggle to provide clear safety boundaries when deployed on safety-relatedtasks. : Initial result of baseline agents DrQv2+BC and AWAC+BC trained on the V-D4RL cheetah-runmedium-expert easy-distraction dataset (Lu et al., 2023). When evaluating on the seen visual distractionsthat can be found in the training dataset, we can observe offline agents achieving expert-level performance(left). However, the agents perform very poorly when exposed to unseen random visual distractions (right).",
  "Benchmarks for Offline Continuous Control": "D4RL (Fu et al., 2020) is a prominent benchmark for continuous control of proprioceptive states in offline RL.The large variety of data distributions has allowed for the comprehensive benchmarking of state-of-the-artoffline RL algorithms (Kidambi et al., 2020; Kostrikov et al., 2022; Kumar et al., 2020) and an understandingof the strengths and weaknesses of different tasks. Florence et al. (2022) discusses when offline RL algorithmswould outperform behavioural cloning (BC) in learning from proprioceptive states. Conversely, vision-baseddatasets for discrete control were created for Atari by (Agarwal et al., 2020); however, they containedonly 50M samples per environment. Hence, V-D4RL (Lu et al., 2023) aims to develop a comprehensivebenchmark for visual-based continuous-control tasks. V-D4RLs 100 K benchmark represents a significantlymore approachable challenge. Another issue with the current datasets is that they are all simulation-based.Zhou et al. (2022) proposed using data collected in real-world settings and demonstrated its suitability forrobot learning in realistic environments. However, visual distraction data are lacking in the original V-D4RL dataset. Our novel datasets aimed to complement the V-D4RL dataset for visual distractions, whichoriginally included only one task.",
  "Dropout and its variants in RL": "The application of dropout (Srivastava et al., 2014) to the RL domain is not new. For instance, Hiraokaet al. (2022) employed dropout in a critic network, demonstrating improved sample efficiency and achievedcomparable performance with only two critic networks compared to the ten critic networks required in REDQ(Chen et al., 2021). Jaques et al. (2019) deployed dropout to obtain uncertainty estimates of the target Qvalues to alleviate the Q-learning overestimation bias. Unlike prior studies that focused on critic networksto enhance sample efficiency, this study targets the encoder as a source of improvement. To the best of ourknowledge, this is one of the first studies to apply DropBlock (Ghiasi et al., 2018) to RL.",
  "Reinforcement Learning from Images": "A standard RL problem for image-based control can be defined as an infinite-horizon Markov DecisionProcess MDP = S, A, p, R, , where S is the set of observations, A is the set of actions, p is the transitionprobability function, R is the reward function, and (0, 1) is the discount factor for future rewards.Generally, in image settings, image rendering of the system is not sufficient to fully describe the underlyingstate of the system. To this end, and per common practice (Mnih et al., 2013), we approximated the currentobservation of the system by stacking three consecutive prior images as an observation. We define the replaybuffer D containing the observation, action, reward, and next observation at time step t as D = (st, at, rt,st+1). The RL agent aims to maximise the discounted expected return E[t=0 tR(st, at)], which is theexpected cumulative sum of rewards when following the policy in the MDP, where the importance of thehorizon is determined by . Consequently, the goal is to determine a policy that maximises discountedexpected returns.",
  "Visual Actor-Critic Methods": "We consider an architecture in which the input observation s is first transformed using a data augmentationscheme. Subsequently, the encoder f with parameters f maps s to a lower-dimensional latent feature vectorz = f(s; f). Q-networks have parameters q. For the replay buffer D and policy , the policy objectivebecomes: = argmaxEstD,at (|f(st;f )) [Q(f(st; f), at ; q)](1) We employ clipped double Q-learning (Fujimoto et al., 2018) to reduce overestimation bias in the targetvalue following previous works (Yarats et al., 2021; 2022; Lu et al., 2023). Target Q networks are Q networkswith parameters q which are a slow-moving copy of q. The overall critic objective is",
  "Offline Reinforcement Learning": "Offline RL problems can be defined as a data-driven formulation of the RL problem.In offline RL, anagent can no longer interact with the environment and collect additional transitions using its learned policy.Instead, the agent was provided with a static dataset of transitions loaded into the replay buffer D ={(sit, ait, rit, sit+1)}Ni=1. In visual offline RL, following DrQv2+BC (Lu et al., 2023), if we consider an additionalpolicy constraint behavioural cloning (BC) term with its strength regulated by a hyperparameter bc, thepolicy objective becomes",
  "Visual Distractions Dataset": "We describe the visual distraction datasets as static datasets of transitions, where observations aresampled from different observation distributions, and the reward and action are sampled from thesame distribution.We can define datasets with and without visual distractions as Dnormal={(si,normalt, ai,normalt, ri,normalt, si,normalt+1)}Ni=1 and Ddis = {(si,dist, ai,dist, ri,dist, si,dist+1 )}Ni=1, respectively. The V-D4RL benchmark (Lu et al., 2023) is a more commonly used offline dataset for visual-based offline RLmethods but contains only one visual distraction task. The V-D4RL dataset was generated with a fixedvisual distraction; that is, the same visual distraction persisted throughout the dataset, whereas during theevaluation, visual distractions were generated randomly.",
  "Domain Adversarial Training for Visual Distractions": "We hypothesise that the subpar performance of the current state-of-the-art baselines can be attributed tovisual distractions present during the evaluation but absent from the offline RL training dataset. Theseunseen visual distractions confuse the encoder, making the encoder estimate less robust latent features z.Consequently, the actor-critic backbone, which relies on the latent features z, cannot learn robust policyand state-action estimations. Baseline agents can achieve expert-level performance in an environment wherethe same visual distractions exist in the training dataset. However, when we evaluated the same agent inan environment with random visual distractions, the agents performance diminished significantly, as shownpreviously in . In this section, we discuss the proposed approach for improving the robustness of visual-based offline RLfor visual distractions. Our proposed method is based on DrQv2+BC (Yarats et al., 2022; Lu et al., 2023)and incorporates two components. We adopt (i) a domain discriminator that trains adversarially against theencoder, and (ii) DropBlock (Ghiasi et al., 2018) layers added to the encoder to achieve robustness againstunseen visual distractions. An overview of this architecture is presented in . To address this issue, we propose a framework in which an agent is trained using datasets from two domains: anormal observation domain denoted by Dnormal and a visually distracted domain denoted by Ddis. Intuitively,the presence of domain-specific visual distractions may allow the discriminator to accurately classify latentfeatures belonging to a particular class. However, we aim to induce the opposite and train the visual encodersuch that the estimated latent features are indistinguishable from the domain discriminator. For the actor-critic backbone, we chose the same actor-critic RL backbone as presented in DrQv2 (Yaratset al., 2022; Lu et al., 2023), which is a variant of the actor-critic structure in TD3 (Fujimoto et al., 2018)using a stochastic policy. We use only the distraction latent features zdis = f(sdis; f) to train the actor-criticbackbone. Only the image encoder, which we hypothesise as the source of underperformance, is trained withboth Dnormal and Ddis.",
  "distractionsreplay buffer": ": An overview of general visual actor-critic RL methods with our additional proposed discriminator.The lower branch is a common actor-critic backbone where an visual encoder is used to estimate latentproprioceptive features z and uses z to train the RL backbone. The upper adversarial branch is wherewe reverse the sign of gradients from discriminator loss from the discriminator g such that the encoder isincentivised to learn features that cannot be classified successfully by g.",
  "Domain Discriminator": "To facilitate domain adversarial learning, we introduce a discriminator g which trains adversarially againstthe encoder via gradient reversal (Ganin & Lempitsky, 2015). The domain discriminator g(z; g) maps thelatent features z = f(s; f) with parameters g, and outputs the probability of the latent feature z. We use kas the label for classes k {0, 1}, representing normal and distraction observations respectively. Both normaland distraction observations underwent the same data augmentation scheme, which was the same as that inDrQv2 (Yarats et al., 2022) and DrQv2+BC (Lu et al., 2023). Given that we sample n observations from thereplay buffers Dnormal and Ddis, the discriminator objective Lk can be expressed as a binary cross-entropyloss, and is defined as follows:",
  "ki log(g(zi; g)) + (1 ki) log(1 g(zi; g)),where z = f(s; f)(5)": "Contrary to conventional binary classification tasks, our objective was not to discriminate between differentdomains. Rather, we aimed to ensure that the discriminator is unable to classify encoded latent featureswhen they are similar, irrespective of the domain from which the observations were encoded. To achieve thisgoal, we adopted the gradient reversal technique proposed for classical domain adaptation tasks (Ganin &Lempitsky, 2015). Although the discriminator learns to use the encoded latent features z to classify the domains, the encoderlearns features that cannot be used to classify the domain successfully because the gradients are reversed.In other words, while the discriminator learns to maximise the latent feature discrepancy between thedomains, the encoder attempts to minimise this discrepancy. Consequently, the encoder learns to extractlatent features that are shared across both domains, that is, domain-invariant features, while simultaneouslysuppressing features that are exclusive to one domain. The parameter updates rule for the encoder f andfor discriminator g are, in addition to the gradients from the critic objective L, as follows, where 1 and 2are the learning rates:",
  "DropBlock in Encoder": "Common to supervised learning approaches, forcing the encoder not to rely on specific features discouragesoverfitting the visual distractions found in the offline dataset. To this end, we added DropBlock (Ghiasiet al., 2018) to the visual encoder, driving the encoder to rely on a wider patch of features during thelearning process to improve its robustness, thus preventing less robust latent feature z estimations whenunseen distractions are present. DropBlock drops patches of a convolutional map (e.g. a 7 7 map) ratherthan a single cell (as in dropout (Srivastava et al., 2014)). The visual encoder would be less likely to overfitthe detailed observations that are present in the dataset but would gather information that exists in bothDnormal and Ddis, consequently reducing the reliance on background information. We empirically found thatadding DropBlock to the visual encoder f frequently enhanced the performance of the proposed domainadversarial training approach. The results are presented in .",
  "TaskDifficultyV-D4RL distractionsour datasets": "cheetah-runrandomcheetah-runmedium-expertball-in-cup-catchrandomball-in-cup-catchmedium-expertreacher-easyrandomreacher-easymedium-expertreacher-hardrandomreacher-hardmedium-expertwalker-walkrandomwalker-walkmedium-expert V-D4RL (Lu et al., 2023) provides a starting benchmark for visual distractions in offline RL settings. How-ever, specifically for distraction observations, they only collected the cheetah-run medium-expert set (in 84 84-pixel format) and the walker-walk random set (in 64 64-pixel format).Therefore, it is suitableto extend the existing V-D4RL dataset to provide a more comprehensive benchmark for offline continuouscontrol with visual distraction. We collected the Distracting Control Suite (Stone et al., 2021) data and theoriginal DeepMind Control Suite (Tassa et al., 2018) for four additional tasks with three difficulties each, allin a unified 84 84-pixel format: medium, expert and random sets (medium and expert are combined toform the medium-expert set). Specifically, we collected for the following tasks: cheetah-run, walker-walk,ball-in-cup catch, reacher-easy, and reacher-hard. For comparison, we also collected our set for theexisting walker-walk random set and the cheetah-run medium-expert set. A brief comparison between theproposed novel dataset and the V-D4RL benchmark is presented in . For dataset collection, we fol-lowed the procedures described in V-D4RL (Lu et al., 2023). A more detailed comparison of the benchmarksis provided in Appendix H.",
  "Comparison with offline RL algorithms": "To evaluate the proposed method, we train four baseline offline RL agents: DrQv2+BC, visual AWAC+BC,and visual IQL and offline-adapted DreamerV2 (Hafner et al., 2021), noted as DV2 (following to V-D4RL).Visual-AWAC+BC is a variant of DrQv2+BC that uses AWAC (Nair et al., 2020) as an actor-critic backbonein addition to the BC term, and visual-IQL uses IQL (Kostrikov et al., 2022) as the backbone (without theBC term). Both use the same visual data augmentation and encoder scheme in DrQv2+BC and only replacethe actor-critic method. All four baseline algorithms were trained directly using the distraction dataset. Asmentioned in the proposed method, the input domains for the discriminator are normal and dis-x and dis-x {easy, medium, hard}. This is to ensure that the RL backbones were trained on the same offline data, and toisolate the effects of the proposed method on the encoder. The input domain for the RL backbone containeddistracted data x, x {easy, medium, hard}, as noted in the training set column in for all algorithms.The discussion and results on two alternative formulations where 1) the encoder of baseline methods weretrained on both distracting data, and 2) where our proposed method is trained with two difficulties ofdistractions (no normal observation) are included in Appendix D. A full list of hyperparameters are shownin Appendix I.4. We follow the original works hyperparameters for baseline methods. A brief robustnessanalysis can be found in Appendix G. All reported numbers were unnormalised, with a maximum reward of 1000 and evaluated over 30 episodes.All reported numbers represent the evaluation results at 1M steps. We trained five seeds: seed {0,1,2,3,4}.For the best practices (Agarwal et al., 2021), we report the IQM and 95% stratified bootstrap confidenceintervals (CIs). The drop rates for different tasks are presented in Appendix I.",
  "V-D4RL Distraction Dataset": "As defined in V-D4RL (Lu et al., 2023) and the Distracting Control Suite (Stone et al., 2021), there are threelevels of distractions: distraction-easy, distraction-medium and distraction-hard. The training results evalu-ated on all three types of distractions (noted respectively as e, m and h), alongside the normal observations(noted as n), are summarised in . Overall, the experimental results demonstrate that the proposedmethod consistently outperforms other model-free offline RL baselines in most tasks. DrQv2+BC and IQLwere the second-best algorithms overall. Interestingly, training directly on more challenging datasets doesnot result in better performance, even on difficult tasks, compared with agents trained on simpler datasetsand evaluated on more difficult tasks. We hypothesise that this is because the datasets do not contain suf-ficient variations in distractions for the more difficult distractions; thus, the agents have difficulty learningto deal with them, whereas training on easier tasks allows the agent to infer some domain invariance acrossdistractions.",
  "Our Distraction Dataset": "The selected results on the walker-walk medium-expert distraction dataset are shown in , and anoverview of the results is presented in for our proposed novel dataset.We observed that ourproposed method outperformed all the tasks overall. For more detailed results for our dataset, please referto Appendix B. The drop rates for different tasks differ; more details regarding the hyperparameters arepresented in Appendix I.",
  "Comparison with visual generalisation methods in RL adapted to offline settings": "In this section, we compare the existing generalisation methods in RL that utilise both normal-domain anddistracted-domain data. These methods are closely related to our proposed method. The main differencefrom our work is that these methods were generally developed for online RL with access to two environments,and we assume that we have access to two datasets. Another difference is that these methods often requireconsecutive training, whereas our method can be trained in an end-to-end manner. Specifically, we adopted PAD (Hansen et al., 2021b), SVEA (Hansen et al., 2021a), and ILA (Yoneda et al.,2022) for purely offline settings. To facilitate comparison, we adapted these methods to offline RL by initially",
  "IQM total1615.91158.2408.82261.2": "training them on one distracted observation, then adapting them to the respective test environments, andsubsequently demonstrating their performance post-adaptation. All hyperparameters were in accordancewith the original authors; we did not make algorithmic changes but only data changes. In PAD (Hansen et al., 2021b), we used the V-D4RL dataset (distracted observations) as the replay bufferduring training and used the inverse dynamics task as the auxiliary training task because it was the best-performing setup in their work. During deployment, we directly adapted to the distracting control suiteenvironment. In the ILA (Yoneda et al., 2022), we train the source domain (in in their paper)purely on the V-D4RL dataset (distracted observations). For the target domain, we adopted the V-D4RLdistraction dataset. We used the dynamic model introduced in ILA. For the SVEA, we trained directly onthe V-D4RL dataset (distracted observations). The selected results are shown in , and the full results",
  "are summarised in Appendix C. Our proposed method outperforms three domain adaptation approaches indistracted environments": "Interestingly, PAD and SVEA usually exhibit fewer performance drops when trained on an easier distractiondataset and are evaluated in a more difficult environment. Similar to ours, the ILA (Yoneda et al., 2022) usesan adversarial approach, which may explain why our proposed method and the ILA suffer from significantperformance drops when distraction difficulties increase, whereas the PAD and SVEA are generally lessaffected.",
  "DropBlock drop rate": "We investigated the effect of DropBlock drop rate using the original V-D4RL cheetah-run distraction dataset.To achieve this, we train agents with p = {0.0, 0.1, 0.2, 0.3, 0.5}. To understand how the DropBblock helps,we train the proposed adversarial discriminator without a DropBlock (p = 0.0) in the enr. We observethat p = 0.3 is the best performing overall, whereas having no DropBlock (p = 0) is the least performingvariation. Selected results is shown in , and the full results is shown in .",
  "Different types of distractions": "It is important to understand which types of distraction are easier for the agent to learn, and which typesare more difficult. As noted previously, there are three types of distractions: background, viewpoint, andcolour, with three levels of distraction respectively. We performed ablation studies on different types byenabling only one distraction at a time and testing at three levels each. The evaluation task is cheetah-run.We used p = 0.3 in our proposed algorithm as the basis for ablation. The results for the medium-expertdataset are presented in , and the full results are presented in Appendix E. Generally, the proposedmethod performs the best overall. Interestingly, robot colour changes had a minimal effect on the agentsperformance. The background changes also had a slight impact. Agents are generally heavily impactedby viewpoint changes, and the final performance is significantly related to the agents ability to adapt toviewpoint changes.",
  "Limitations": "As mentioned previously, one of the assumption of our proposed method is that two datasets are available. Inaddition, we also have not discussed in depth the impact of the quality of the two datasets effect performance.For example, one interesting question to ask is, how different must the two datasets be for our proposedmethod to work?We briefly discuss on using two distraction dataset to train our proposed method inAppendix D. An interesting future direction is to further propose novel methods using only distracting datafor training.",
  "Conclusion": "In this work, we tackle the issue of visual distraction in offline RL settings, where the distractions thatexist in the dataset might significantly differ from those in evaluation. We investigated an adversarial-basedalgorithm for visual-based offline RL using an additional discriminator with gradient reversal and introducedDropBlock to the visual encoder. Our approach trains the encoder such that it will learn domain-invariantfeatures that are more generalisable when unseen visual distractions are present during evaluation. Ourmethod can be easily implemented by adding a discriminator branch while reversing the gradient flow andadding DropBlock. Despite the simplicity and efficiency of this method, we empirically demonstrated thatit achieved excellent performance across various difficulties in the V-D4RL visual distraction dataset, as well",
  "m-e E": "Ebackground91.3 (50.8, 111.8)69.1 (66.0, 103.8)97.8 (71.1, 112.0)215.2 (187.3, 220.5)Eviewpoint47.1 (23.0, 74.1)51.3 (28.2, 119.6)106.3 (50.4, 120.1)170.7 (145.5, 192.3)Ecolour69.0 (45.4, 140.7)95.6 (46.5, 143.8)114.0 (75.5, 158.1)227.9 (202.1, 269.1)Eall83.6 (57.9, 96.0)60.9 (53.3, 72.8)67.1 (54.8, 76.9)130.6 (102.0, 161.0) Mbackground111.4 (102.2, 123.1)95.8 (80.3, 128.5)157.3 (95.7, 188.6)236.8 (199.6, 244.9)Mviewpoint32.9 (14.2, 44.7)28.3 (16.7, 68.6)48.2 (20.6, 59.2)94.3 (81.5, 119.9)Mcolour95.0 (53.2, 146.8)98.2 (49.3, 143.2)96.0 (68.8, 142.0)217.0 (189.0, 282.3)Mall35.0 (33.1, 44.5)28.5 (20.2, 47.5)41.9 (39.7, 54.4)81.0 (66.3, 109.3) Hbackground92.7 (78.6, 160.3)71.1 (61.3, 104.5)107.7 (91.5, 155.1)194.7 (149.4, 222.3)Hviewpoint21.0 (10.5, 25.3)16.7 (12.2, 30.4)25.4 (17.1, 41.7)65.3 (47.3, 74.8)Hcolour71.2 (51.5, 122.1)105.8 (50.8, 166.7)98.8 (59.5, 125.2)200.2 (162.4, 239.5)Hall18.1 (14.2, 28.0)18.6 (14.8, 22.7)20.3 (14.8, 27.6)55.4 (42.2, 60.5)",
  "m-e M": "Ebackground25.0 (14.3, 47.6)27.6 (21.5, 79.9)18.2 (12.9, 51.7)49.3 (33.5, 63.6)Eviewpoint37.2 (27.1, 41.6)53.9 (22.3, 85.0)29.3 (19.2, 79.3)80.8 (51.8, 102.7)Ecolour55.3 (38.7, 74.6)49.9 (19.2, 84.6)53.9 (32.0, 72.3)70.8 (38.7, 90.5)Eall32.5 (22.3, 56.2)18.4 (17.5, 26.4)39.3 (26.1, 61.5)60.5 (33.0, 72.8) Mbackground44.0 (28.6, 64.0)40.2 (35.0, 85.6)25.3 (20.9, 69.7)56.0 (47.6, 73.1)Mviewpoint23.5 (12.5, 30.4)48.1 (21.8, 70.0)17.1 (13.8, 35.1)44.7 (28.6, 61.1)Mcolour39.2 (24.3, 50.7)57.3 (26.8, 72.7)44.3 (32.5, 64.5)76.2 (38.3, 99.9)Mall34.4 (21.3, 49.4)24.5 (17.2, 29.8)36.9 (29.7, 40.1)49.2 (37.0, 66.9) Hbackground39.4 (29.8, 51.7)41.7 (31.7, 51.8)34.6 (28.1, 69.0)68.5 (33.2, 86.4)Hviewpoint15.5 (10.6, 18.3)24.5 (15.7, 32.0)16.4 (9.4, 23.5)35.3 (17.9, 53.8)Hcolour40.6 (25.9, 66.1)50.4 (31.5, 71.3)41.7 (25.7, 66.1)84.0 (42.8, 96.9)Hall13.5 (12.5, 18.6)22.0 (17.3, 32.3)20.4 (12.7, 24.9)20.2 (13.8, 30.8)",
  "m-e H": "Ebackground11.6 (9.5, 32.6)22.2 (19.0, 34.0)6.7 (5.1, 12.8)44.3 (27.1, 79.8)Eviewpoint16.4 (9.0, 36.8)9.2 (7.2, 14.3)7.7 (6.6, 10.3)32.4 (22.6, 46.5)Ecolour16.9 (9.0, 28.0)11.2 (9.6, 21.7)6.3 (3.8, 11.3)43.8 (19.4, 64.4)Eall14.2 (9.8, 16.5)16.6 (13.9, 25.8)8.5 (6.0, 12.6)29.4 (21.0, 41.2) Mbackground18.0 (12.2, 21.0)22.9 (15.5, 34.3)8.6 (6.7, 13.1)34.0 (26.3, 78.5)Mviewpoint14.8 (7.2, 24.8)12.7 (10.6, 15.2)9.2 (6.7, 14.6)27.1 (21.2, 45.3)Mcolour14.5 (9.8, 24.1)18.0 (11.7, 24.2)10.7 (7.0, 16.3)41.9 (18.3, 54.8)Mall11.6 (10.4, 22.3)15.8 (12.5, 21.1)10.8 (9.2, 19.5)24.9 (17.7, 28.5) Hbackground14.0 (11.3, 20.4)20.5 (18.4, 28.1)8.2 (8.1, 9.3)27.9 (20.3, 45.5)Hviewpoint10.8 (7.0, 20.7)12.3 (11.0, 17.9)8.6 (6.7, 16.7)32.7 (22.5, 36.3)Hcolour15.9 (9.7, 25.7)13.6 (10.3, 23.8)13.1 (7.9, 16.7)29.1 (17.4, 36.2)Hall12.4 (9.1, 15.7)16.6 (13.3, 32.7)9.8 (9.2, 14.0)27.3 (16.7, 33.6)",
  "Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline rein-forcement learning. In International Conference on Machine Learning (ICML), 2020": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Bellemare. Deepreinforcement learning at the edge of the statistical precipice. In Advances in Neural Information ProcessingSystems (NeurIPS), 2021. Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcementlearning with diversified q-ensemble. In Conference on Neural Information Processing Systems (NeurIPS),2021.",
  "Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmarkreinforcement learning. In International Conference on Machine Learning (ICML), 2020": "Tom Dupuis, Jaonary Rabarisoa, Quoc Cuong PHAM, and David Filliat. Domain invariant q-learning formodel-free robust continuous control under visual distractions. In NeurIPS Deep Reinforcement LearningWorkshop, 2022. Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong,Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on RobotLearning (CoRL), 2022.",
  "Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete worldmodels. In International Conference on Learning Representations (ICLR), 2021": "Tom Haider, Felippe Schmoeller Roza, Dirk Eilers, Karsten Roscher, and Stephan Gnnemann. Domainshifts in reinforcement learning: Identifying disturbances in environments. In IJCAI AISafety Workshop,2021. Nicklas Hansen, Hao Su, and Xiaolong Wang. Stabilizing deep q-learning with convnets and vision trans-formers under data augmentation. In Conference on Neural Information Processing Systems (NeurIPS),2021a.",
  "Bonnie Li, Vincent Franois-Lavet, Thang Doan, and Joelle Pineau.Domain adversarial reinforcementlearning. arXiv preprint arXiv:2102.07097, 2021": "Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, and Yee Whye Teh.Challenges and opportunities in offline reinforcement learning from visual observations. In Transactionson Machine Learning Research (TMLR), 2023. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, andMartin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.",
  "BFull experiment results with offline RL algorithms": "In this section, we include the full table results train on V-D4RL cheetah-run distraction dataset, our pro-posed cheetah-run distraction dataset, ball-in-cup-catch distraction dataset, reacher-easy distraction dataset,reacher-hard distraction dataset and walker-walk distraction dataset. For V-D4RL dataset, we compare withbaseline agents DrQv2+BC, AWAC+BC and IQL. For our datasets, we mainly compared with DrQV2+BCas it was the best performing for the V-D4RL dataset, with the exception of walker-walk distraction datasetwhere we included IQL to compare.",
  "DAdditional experiments on Data Usage": "In the section, we show the results of an alternative formulation where the encoder of the baseline algotihmuses both normal dataset and distraction dataset during training. DrQv2+BC denotes the baseline algorithm,DrQv2+BC, both data denotes the baseline algorithm where its encoder has access to both distracting dataand normal data, and Ours denotes our proposed algorithm, as shown in . Furthermore, we cantrain another version where our proposed algorithm is trained on two distracting datasets, shown in . : Results comparison on cheetah-run distraction dataset (V-D4RL). The reported numbers areIQM and 95% stratified bootstrap CIs in between parentheses. Abbreviations: normal: N, dis-easy: E,dis-medium: M, dis-hard: H. e: expert. m: medium. m-e: medium-expert. Our method performs betterwhen the quality of data is mixed (medium-expert), while DrQv2+BC, both data performs much better onnormal observation.",
  "trainevaltype of disDrQv2+BCAWAC+BCIQLOurs": "m EEbackground129.7 (115.1, 163.5)101.5 (62.6, 123.7)165.4 (82.7, 167.5)293.1 (258.5, 317.0)m EEviewpoint69.9 (61.1, 103.1)72.1 (42.7, 105.3)59.8 (26.8, 99.0)148.1 (119.4, 172.9)m EEcolour125.7 (115.88, 136.8)84.0 (35.9, 130.1)117.5 (44.3, 166.7)223.8 (208.1, 301.9)m EEall99.5 (65.6, 110.4)56.4 (44.0, 77.7)98.8 (63.1, 118.4)152.0 (122.0, 178.9) m EMbackground53.9 (35.6, 66.6)8.8 (7.2, 30.0)10.9 (2.6, 30.5)30.3 (23.9, 70.3)m EMviewpoint105.6 (85.0, 120.0)68.4 (49.8, 83.9)33.4 (21.4, 52.3)98.7 (62.0, 117.7)m EMcolour95.6 (90.1, 103.6)76.4 (62.2, 90.0)45.4 (32.1, 92.7)103.1 (63.5, 121.2)m EMall41.4 (32.4, 56.8)23.2 (17.2, 42.8)26.1 (20.0, 70.4)68.2 (49.0, 83.3) m EHbackground26.8 (18.4, 36.9)25.6 (15.2, 41.3)8.8 (6.5, 21.6)13.7 (8.6, 35.3)m EHviewpoint42.9 (34.2, 62.6)14.0 (10.5, 34.2)14.5 (10.4, 33.9)27.2 (22.8, 28.4)m EHcolour49.4 (36.3, 64.3)14.9 (4.8, 38.3)21.3 (9.5, 30.7)38.0 (26.8, 43.4)m EHall18.6 (12.7, 22.6)14.4 (8.7, 19.5)11.2 (7.6, 18.6)34.0 (26.9, 41.5) m MEbackground153.7 (100.4, 180.1)111.4 (47.2, 122.0)177.5 (95.8, 202.9)308.2 (266.4, 351.5)m MEviewpoint41.1 (30.9, 65.8)42.5 (32.6, 59.7)34.6 (20.4, 66.7)84.1 (53.1, 97.6)m MEcolour115.7 (110.3, 148.7)94.3 (48.1, 132.3)101.7 (37.3, 145.1)244.4 (207.2, 290.8)m MEall37.6 (31.0, 53.2)18.6 (11.9, 26.8)27.0 (17.1, 41.6)45.5 (36.0, 97.9) m MMbackground49.5 (44.9, 91.6)20.8 (14.8, 63.5)34.6 (22.0, 54.0)61.1 (40.7, 76.5)m MMviewpoint72.4 (57.3, 85.4)56.0 (47.4, 64.9)20.6 (18.8, 71.3)76.3 (43.5, 90.0)m MMcolour94.1 (65.7, 98.7)66.2 (53.0, 91.8)46.6 (35.9, 85.4)88.0 (56.8, 108.5)m MMall40.3 (23.6, 52.5)14.3 (8.7, 17.7)15.6 (13.5, 19.8)50.3 (27.5, 52.9) m MHbackground27.8 (21.0, 30.5)22.1 (12.3, 34.4)8.4 (6.7, 17.0)12.3 (7.3, 31.6)m MHviewpoint29.2 (27.4, 60.3)13.3 (8.8, 36.4)25.4 (12.3, 34.6)23.4 (18.5, 24.9)m MHcolour51.2 (32.7, 58.8)12.4 (6.2, 40.4)18.5 (9.2, 31.8)35.4 (27.9, 43.9)m MHall21.2 (14.4, 29.6)9.8 (7.2, 20.6)8.1 (7.2, 29.1)26.5 (19.7, 38.0) m HEbackground121.6 (95.2, 163.5)76.5 (51.6, 108.0)105.4 (58.2, 143.6)274.8 (223.7, 310.8)m HEviewpoint26.0 (23.6, 42.4)28.0 (15.1, 38.3)12.1 (6.3, 25.1)28.7 (17.9, 39.0)m HEcolour120.4 (109.5, 128.8)88.5 (57.3, 132.4)69.5 (35.2, 124.5)245.0 (162.3, 279.2)m HEall26.3 (19.3, 33.6)21.8 (13.0, 31.3)8.7 (5.5, 14.5)18.5 (16.0, 24.8) m HMbackground42.7 (37.5, 47.7)19.8 (11.0, 31.9)38.1 (23.7, 48.3)39.8 (25.4, 52.7)m HMviewpoint41.6 (29.6, 60.9)32.2 (20.0, 42.3)12.5 (6.7, 18.8)38.3 (16.1, 64.9)m HMcolour85.5 (80.8, 90.0)67.3 (52.9, 76.2)46.8 (23.5, 49.9)75.6 (64.8, 94.5)m HMall24.5 (20.4, 32.6)14.0 (7.1, 21.6)10.6 (7.6, 16.2)18.3 (14.4, 21.6) m HHbackground25.5 (19.3, 28.5)20.9 (14.7, 27.3)8.9 (7.0, 17.8)16.8 (12.0, 20.4)m HHviewpoint27.4 (25.0, 53.4)10.6 (6.9, 18.2)21.2 (11.4, 31.2)23.9 (20.2, 34.6)m HHcolour43.1 (26.7, 56.5)13.2 (6.8, 23.7)19.2 (7.7, 26.5)27.0 (24.2, 29.5)m HHall33.7 (21.1, 37.6)12.8 (9.3, 16.2)15.6 (11.7, 21.2)16.2 (15.6, 19.9)",
  "FAblation on Effectiveness of Dropblock": "In this section, we investigate further the effect of dropblock on baseline algorithms. Firstly, the full resultsof ablation study on dropblock for our proposed algorithms is shown in . Secondly, an ablationstudy on using only dropblock with baseline algorithm DrQv2+BC is shown in . We can note thatadding dropblock on baseline algorithm can improve the performance by a fair margin, but the full proposedalgorithm consisting of dropblock and gradient reversal layer is the most performant. : DropBlock ablation results on V-D4RL cheetah-run distraction dataset. The reported numbersare IQM and 95% stratified bootstrap CIs in between parentheses. Abbreviations for dataset type: medium:m, expert: e, medium-expert: m-e. Abbreviations for visual distractions difficulty: normal: N, easy: E,medium: M, hard: H. For example, \"e H\" equals \"expert Hard\".",
  "GRobustness analysis": "To show the robustness of the proposed method over baseline, we perform two additional analysis, all basedon training with cheetah-run medium-expert easy dataset. Firstly, We calculated the Wasserstein distancebetween the encoded features of from different level of distractions. The Wasserstein distance can be seen asa similarity metric between the two encoded features. Specifically, we have two scenarios. One is samplinga batch of images from the V-D4RL cheetah-run medium-expert dataset, and another is collected via onlineenvironment (cheetah-run) by using a random policy, to collect online distractions not existing on the dataset.The values are averaged over the batch of observations (batch size = 256).",
  "onlinenormal - dis-easy0.37320.1093onlinenormal - dis-medium0.33110.0483onlinenormal - dis-hard0.33120.1156": "Secondly, we plot the t-SNE (van der Maaten, 2008) results of the trained policy distribution using differentdistraction observations. There are also two scenario of data source; from the dataset and one from onlineenvironment. As we can observe, in both scenarios, our proposed method brings the policy distributioncloser for different distraction observations.",
  "HAdditional Benchmark Comparison with V-D4RL": "V-D4RL was collected by training an online SAC agent and using the saved checkpoints to collect visualobservations while running the agent in proprioceptive states. As we could not access the SAC checkpoints ofthe original V-D4RL authors, we followed their guidelines as closely as possible to reproduce data collection.The checkpoint used to collect the medium dataset is described as the first stable checkpoint, resulting inconsistent 500 rewards. Simultaneously, the expert agent is the stable checkpoint resulting in nearly themaximum possible reward, which translates to nearly 1000 rewards in DM Control tasks. presents the histogram of collected cheetah-run medium-expert data, plotted by rewards per episodeversus frequencies (counts). The figure shows that the original data and broad data are comparable in mostparts; the main difference is that the medium difficulty of the broad dataset is a bit lower compared tothe original, and the expert difficulty is significantly concentrated in the final rewards (around 900). Thedifference is because of the checkpoint used to gather data.",
  "IImplementation Details": "The implementation details and hyperparameters settings used for our experiments are presented below.Similar to DrQv2, we employed clipped double Q-learning (Fujimoto et al., 2018) to reduce the overestimationbias in the target value. We also provide a schematic of how the overall network architecture is implemented asin , as well as the network architecture used in . The Actor and critic networks follow DrQv2exactly; therefore, they are omitted. Additionally, we show the hyperparameters used in the experiments. lists the common hyperparameters for all methods. lists the additional hyperparametersused in our proposed method. lists the additional hyperparameters used in the experiments for thebaseline DrQv2+BC. lists the additional hyperparameters used in the experiments for the baselineAWAC+BC. lists the additional hyperparameters used in the experiments for the baseline IQL. For PAD (Hansen et al., 2021b), ILA (Yoneda et al., 2022) and SVEA Hansen et al. (2021a), we usetheir default hyperparameters. We include some brief explanation on some of the baseline algorithms tocomplement the hyperparameters listed in Appendix I.4.",
  "I.2AWAC": "AWAC updates its policy by a weighted maximum likelihood, where the targets are obtained by reweight-ing the state-action pairs observed in the dataset versus the predicted advantages from the critic,(Q(f(st; f), at) Q(f(st; f), at ); q)), where a (|f(s; f)). By adding a BC term into the policytraining, the policy objective is calculated as:"
}