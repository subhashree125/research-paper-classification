{
  "Abstract": "Neural solvers for partial differential equations (PDEs) have great potential to generate fastand accurate physics solutions, yet their practicality is currently limited by their generalizabil-ity. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomenawill require learning representations across a wide variety of inputs which may encompassdifferent coefficients, boundary conditions, resolutions, or even equations. As a step towardsgeneralizable PDE modeling, we adapt masked pretraining for physics problems. Throughself-supervised learning across PDEs, masked autoencoders can consolidate heterogeneousphysics to learn rich latent representations. We show that learned representations cangeneralize to a limited set of unseen equations or parameters and are meaningful enoughto regress PDE coefficients or the classify PDE features. Furthermore, conditioning neuralsolvers on learned latent representations can improve time-stepping and super-resolutionperformance across a variety of coefficients, discretizations, or boundary conditions, as wellas on certain unseen PDEs. We hope that masked pretraining can emerge as a unifyingmethod across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.",
  "Introduction": "The physical world is incredibly complex; physical phenomena can be extremely diverse and span widespatiotemporal scalesfrom neuron excitations to turbulent flow to even global climate.Importantly,many of these phenomena can be mathematically modeled with time-dependent partial differential equations(PDEs)(FitzHugh, 1961; Nagumo et al., 1962; Lorenz, 1963). These PDEs are generally analytically intractableand require the use of numerical solvers to obtain approximate solutions. For complex physics, these solutionscan often be slow to obtain; furthermore, different PDEs often require a careful design of tailored solvers. Advances in deep learning have led to the design of a new class of solvers for PDEs. These neural solvers canbe extremely fast and display resolution invariance; however, neural networks introduce training difficultiesand a lack of theoretical guarantees. Many important advances have been made to address these challenges,with models becoming faster than numerical solvers within well-studied PDEs under certain setups, proposingerror bounds, and being extended to solve real-world problems. (Raissi et al., 2019; Lu et al., 2019; Li et al.,2020; Cao, 2021; Brandstetter et al., 2022; Li et al., 2023; Kovachki et al., 2021; Li et al., 2024). A current frontier in neural PDE solvers lies in generalizing solvers to different parameters, conditions, orequations, thereby avoiding the need to collect new data and retrain networks when given unseen PDEdynamics. Prior work in this space has explored many methods to achieve this, from directly conditioning onPDE coefficients (Takamoto et al., 2023; Lorsung et al., 2024; Shen et al., 2024) to pretraining foundationmodels across various equations (Subramanian et al., 2023; McCabe et al., 2023; Hao et al., 2024). Despite",
  "Published in Transactions on Machine Learning Research (12/2024)": "as this work or contrastive PDE encoders (Mialon et al., 2023; Zhang et al., 2023) are more flexible thanfoundation models, capable of being applied to arbitrary downstream architectures and different fine-tuningtasks. Additionally, when using a surrogate objective to train an encoder, models can learn more generallatent representations, compared to using a neural solver or foundation model to only predict the next step ofa PDE rollout. We show some preliminary results demonstrating this in Appendix F. While this versatility isinteresting, the practicality of this is unclear, since time-stepping is so singularly important. However, thisresearch direction is still underexplored and perhaps future work will find an interesting set of uses for PDEencoders. Within the context of masked pretraining, there are a few additional limitations to be recognized. It can becostly to fully fine-tune the pretrained encoder during time-stepping or super-resolution since it operates onall unmasked tokens, and as a result does not benefit from the speed gains during MAE pretraining. Thismakes full fine-tuning likely infeasible when compared to baselines. To address this, freezing the MAE encodergreatly improves the training speed but decreases performance, especially on unseen PDEs. With enoughPDE data during pretraining, freezing MAE encoders can be more practical since the pretraining distributionwould cover most downstream cases. Lastly, for simpler 1D dynamics when coefficient information is available,conditioning on ground-truth PDE parameters remains the best choice in most scenarios. However, theseapproaches are not exclusive; initial work suggests that models provided with both ground-truth informationand MAE embeddings can outperform models provided with just one of either.",
  "Related Work": "Neural PDE SolversThe field of neural PDE solvers has grown rapidly and has shown great advancesin both the accuracy of solutions and the ability to adapt to PDE parameters. Infinite-dimensional neuraloperators (Li et al., 2020; Kovachki et al., 2023; Lu et al., 2019) have shown impressive accuracy in solvingtime-dependent PDEs by learning the mappings between initial conditions and solutions. However, thesemethods alone have shown brittleness with respect to changing PDE coefficients or boundary conditions(Gupta and Brandstetter, 2022; Lu et al., 2021), prompting recent work to allow neural solvers to adapt todifferent PDE conditions. A variety of approaches have considered adding PDE dynamics information or time-dependent trends toneural solvers. Common neural solvers can support conditional prediction through architecture choices (Guptaand Brandstetter, 2022), and novel architectures can be designed to explicitly operate with PDE parameterknowledge (Brandstetter et al., 2022). Beyond directly conditioning on PDE dynamics, a class of neuralPDE solvers has proposed the addition of an encoder or adaptive network to inform a forecaster network ofdifferent PDE coefficients (Wang et al., 2021; Kirchmeyer et al.; Takamoto et al., 2023; Lorsung et al., 2024).At an even higher level, meta-learning approaches have been adapted to PDE learning to maximize sharedlearning across different physics (Yin et al., 2021; Zhang et al., 2023). Pretraining for PDEsAs an effort to work towards more generalizable PDE neural solvers, recent workhas followed the success of pretraining and foundational models in the broader deep learning community.Based on contrastive pretraining methods in computer vision problems, (Chen et al., 2020; Schroff et al., 2015;Zbontar et al., 2021; Bardes et al., 2022), contrastive PDE methods aim to leverage equation coefficients(Lorsung and Farimani, 2024), physical invariances (Zhang et al., 2023), or Lie point symmetries (Mialonet al., 2023; Brandstetter et al., 2022) to define differences in PDE dynamics that can be organized in a latentspace. Another approach in PDE pretraining follows observed in-context learning and emergent behavior inLLMs (Wei et al., 2022; Brown et al., 2020; Radford et al.) to design neural PDE solvers that are capable offollowing prompted PDE examples to forecast unseen dynamics (Yang et al., 2023; Chen et al., 2024). A more straightforward pretraining method focuses on directly training neural solvers to transfer to newPDE dynamics (Goswami et al., 2022; Chakraborty et al., 2022; Wang et al., 2022). This approach hasalso been scaled by training neural solvers with large and diverse training sets to characterize its transferbehavior (Subramanian et al., 2023), as well as shown to be generally more effective over other pretraining",
  "Methods": "We describe our main methods in . We first pretrain an encoder and decoder to reconstruct maskedinputs. This pretraining objective can have a few benefits. Masking destroys inherent biases present in thedata; especially at high masking ratios, this forces models to learn very general latent representations. Lastly,masked modeling does not assume any prior physics or leverage features specific to a particular PDE, makingthe approach applicable to a wide variety of physics. After pretraining, we evaluate using the learned representations for three downstream tasks after discardingthe decoder. The first task is to regress or classify PDE features, such as predicting coefficient values orboundary conditions of a PDE sample. This can be done by directly appending a linear head to the pretrainedencoder. The second task is to improve the performance of neural solvers by conditioning on an encodedrepresentation of input physics. Both the encoded and original representations of the data are passed to theneural solver. Lastly, we consider improving super-resolution performance by passing an up-sampled input toa neural solver and conditioning on an encoded representation of the low-resolution PDE input.",
  "Masked Pretraining for PDEs": "We adapt the popular Masked Autoencoder (MAE) approach (He et al., 2021; Xie et al., 2021; Feichtenhoferet al.) to train ViT models (Dosovitskiy et al., 2020; Arnab et al., 2021) to reconstruct 1D and 2D PDE data.Data are partitioned into non-overlapping patches before a random subset of these patches is sampled to bemasked. The masked patches are omitted from the encoder input; the encoder embeds only the unmaskedpatches through a series of transformer blocks, which allows for larger encoders and faster training at largemasking ratios (He et al., 2021). The encoded patches are then recombined with mask tokens according totheir position in the PDE solution. Positional embeddings are added again to preserve positional information",
  ".48e-030.251.24e-030.501.17e-030.751.27e-031.001.37e-03": "To emulate a larger pretraining dataset and aid in generalization, we inves-tigate using Lie augmentations during pretraining, an approach that followsthe use of data augmentations in vision or video pretraining (He et al., 2021;Xie et al., 2021; Feichtenhofer et al.). Given a PDE, one can derive its Liesymmetries as a set of transformations {g1, . . . , gi}, each with a variable ithat modulates the magnitude of the transformation. At training time, weapply gi sequentially, each with a randomly sampled i to randomly augmentPDE samples. This augmented PDE sample could represent a solution thathas been shifted in space, time, or magnitude, among other transformations,but still propagates dynamics according to the original PDE. For a moredetailed discussion of Lie point symmetries for PDEs, we refer the reader to Olver (1986) and Mialon et al.(2023). To understand the effects of Lie data augmentation on reconstruction accuracy, we apply randomly",
  "None4.40e-03Pad3.81e-03Interp.3.23e-03Token1.89e-03": "The simplest strategy would be to pad the inputs to the maximum sequencelength (Pad). However, this does not address the fact that positional informationbecomes overloaded and only provides information about input length.Toaddress this, we consider interpolating positional embeddings from the maximumsequence length to variable sequence lengths (Interp.). This would allow tokens toreceive approximate positions; however, this would not be accurate for irregulargrids. Lastly, to adapt to arbitrary grids, we consider using an embedder network,which could be a 1D or 2D CNN (depending on the spatial dimension), to projectthe spatial coordinate grid to a token (Token). We compare the validationMSE reconstruction loss of using different strategies in after training onmultiresolution KdV-Burgers data, observing consistent improvement by using more sophisticated methodsfor embedding spatial discretizations. In 1D, multiresolution pretraining is significantly easier to learn, so weuse the Interp. strategy for simplicity. In 2D, the use of tokenized spatial grid information becomes moreimportant, so we use the Token strategy for 2D multiresolution experiments.",
  "PDEs and Datasets": "We describe a variety of PDEs used for masked pretraining and downstream evaluation. In 1D, we pretrainMAE models on the KdV-Burgers equation only, while in 2D we pretrain on the Heat, Advection, and Burgersequations simultaneously. In all PDEs, coefficients and forcing terms are randomly sampled to producediverse dynamics within a dataset. 1D KdV-Burgers Equation: The KdV-Burgers equation (KdV-B) contains the Heat, Burgers, KdVequations as corner cases modulated by coefficients (, , ) (Brandstetter et al., 2022; Jeffrey and Mohamad,1991):",
  "from t = 0 to t = 2. For 1D multi-resolution experiments, PDE data from the KdV-Burgers equation isdownsampled to variable spatial resolutions": "1D Heat and Burgers Equations: The 1D Heat and inviscid Burgers (Brandstetter et al., 2022) equationsare used to evaluate performance on PDEs that are a subset of pretraining samples. Furthermore, to evaluateextrapolation to unseen boundary conditions (BCs), samples of the Heat equation are also generated withDirichlet and Neumann BCs in addition to periodic BCs.",
  "tu + uxu = (t, x)(Burgers)": "We solve the equations with the same periodic BCs, initial conditions, and forcing function setup and as the1D KdV-Burgers equation, but by setting the appropriate coefficient values. Specifically, we uniformly sample = [0.1, 0.8] for the Heat equation and fix = 0.5, = 0 to model the inviscid Burgers equation. Theseequations are also solved with a discretization (nt, nx) = (250, 100) on an interval x = from t = 0 tot = 2. To generate data for the Heat equation that enforces Dirichlet or Neumann boundary conditions, wewrite the Heat equation in its variational form after an implicit Euler discretization:",
  "(un + tf n+1)vdx(2)": "This formulation can be solved using FEniCS (Alnaes et al., 2015; Logg et al., 2012). To simplify the boundaryvalue problem, we set the forcing term f = 0. Furthermore, we set [u(x = 0) = u(x = L) = 0] to enforceDirichlet BCs, and [xu(x = 0) = xu(x = L) = 0] to enforce Neumann BCs; however, in both of these cases,the initial conditions in Equation 1 need to be modified to respect the new BCs.",
  "In both equations, we uniformly sample Aj [0.5, 0.5], lj {1, 2, 3}, j {0, } while fixing J = 5, L = 16": "1D Advection, Wave, and Kuramoto-Sivashinsky Equations: The Advection (Adv), Wave, andparameter-dependent Kuramoto-Sivashinsky (KS) (Lippe et al., 2023) equations are considered to eval-uate downstream performance to new equations; the equations contain PDE terms that are unseen duringpretraining. Additionally, the Wave equation is generated with Dirichlet and Neumann BCs (Brandstetteret al., 2022) to evaluate unseen BCs on novel PDE dynamics.",
  "tu + uxu + xxu + xxxxu = 0(KS)": "For the 1D Advection equation, initial conditions are generated according to Equation 1, the wave speed isuniformly sampled from c [0.1, 5], and periodic BCs are used. The solution domain and discretization arethe same as previous cases, with (nt, nx) = (250, 100), x = , and time ranging from t = 0 to t = 2. For the 1D Wave equation, we solve with Dirichlet (u(x = 0) = u(x = L) = 0) and Neumann (xu(x = 0) =xu(x = L) = 0) BCs, resulting in waves that either bounce or reflect off boundaries. The wave speed isfixed at c = 2, and the initial condition is a Gaussian pulse with unit amplitude and with its peak randomlysampled on the spatial domain. Lastly, the equation is solved from t = 0 to t = 100 on the interval x = with a discretization (nt, nx) = (250, 100). For the 1D KS equation, we use periodic BCs with initial conditions from Equation 1. Following the data setupproposed by Lippe et al. (2023), we additionally uniformly sample [0.75, 1.25] to vary the second-order",
  "tu + u(c u) 2u = 0(Burgers)": "For the Heat equation, we uniformly sample the [2 103, 2 102; for the Advection equation,we uniformly sample c = [cx, cy] [0.1, 2.5]2; and for the Burgers equation, we uniformly sample [7.5 103, 1.5 102, and c = [cx, cy] [0.5, 1.0]2. For all equations, we use periodic BCs and solve on agrid (nt, nx, ny) = (100, 64, 64) on a solution domain (x, y) = 2 from t = 0 to t = 2. Lastly, initialconditions are generated from:",
  "Initial condition parameters are uniformly sampled from Aj [0.5, 0.5], j [0.4, 0.4], lxj {1, 2, 3}, lyj {1, 2, 3}, j [0, 2) while fixing J = 5, L = 2": "2D Navier-Stokes Equations: Following the setup from Li et al. (2020), we consider the incompressibleNavier-Stokes (NS) equations in vorticity form, but randomly sample the viscosity and forcing functionf(x) amplitude. To ensure consistency with the pretraining dataset, our experiments model NS dynamics asa scalar vorticity field; from this the velocity field can be derived from the Biot-Savart Law.",
  "t + u 2 = f(x), u = 0(NS)": "The solution is solved on a grid (nt, nx, ny) = (100, 64, 64) on a solution domain (x, y) = 2 from t = 0to t = 25. PDE parameters are uniformly sampled from {{1, 2, 3, 4, 5, 6, 7, 8, 9} 10{6,7,8,9}} andA {1, 2, 3, 4, 5, 6, 7, 8, 9, 10} 103. Lastly, initial conditions 0 are sampled according to Li et al. (2020)from a Gaussian random field.",
  "Data Augmentations": "We implement Lie Point Symmetry Data Augmentations according to Brandstetter et al. (2022), includingshifting and resampling PDE solutions with the Fourier shift theorem. Since we only augment PDE samplesduring pretraining, we consider symmetry groups for the 1D KdV-Burgers equation, as well as the 2DHeat, Advection, and Burgers equations. The 1D KdV-Burgers equation has the following Lie subalgebras(Ibragimov, 1993):",
  "Results": "In this section, we provide the results of different experiments designed to understand the capabilities of maskedautoencoders. Firstly, we would like to understand the reconstruction capabilities of the masked autoencoder,or if the pretraining goal given by the masked objective is being met. Once MAE performance is validatedon its pretraining objective, we seek to understand and visualize the representations learned by the MAEduring pretraining. This can be done by projecting the latent embeddings to a lower dimension to visualizequalitative trends. Although insightful, another more rigorous evaluation is the regression/classification ofphysical variables. These are easy quantities to derive and are usually trivially known a priori, however, theycan serve as a probe to quantitatively gauge model knowledge rather than rely on qualitative latent trends.This is analogous to a linear probe used to predict image rotations or grayscale vs. color in self-supervisedlearning for computer vision (Chen et al., 2020); indeed, coefficient regression has been used in prior PDEliterature to gauge model performance after self-supervised pretraining (Mialon et al., 2023). While these three tasks (masked reconstruction, latent visualization, variable regression/classification) canprovide knowledge, they are generally not useful on their own. To extend masked pretraining to practicaltasks, we consider using a pretrained encoder to improve PDE time-stepping or super-resolution. In particular,we are interested in whether the representations learned during masked pretraining can help in diverse physicsscenarios by providing additional context to neural PDE solvers during time-stepping or super-resolution.",
  "MAE Pretraining": "MAE models are trained on 10000 samples of 1D KdV-Burgers PDE data in 1D and 12288 samples of 2D Heat,Advection, and Burgers PDE data in 2D. We display example results from masked pretraining in Figures 2and 3. A notable difference from vision and video domains is that physics does not follow human-recognizablestructure or descriptions (e.g. backgrounds, actions, faces, shapes, etc.); furthermore, in addition to theoverall meaning, the numerical accuracy of the reconstruction is important. Despite this, MAE modelsare able to capture underlying physics and reconstruct PDEs within the pretraining set wellboth in 1Dand 2D, and at high masking ratios (75% and 90%). In general, for PDEs that are similar to those seenduring pretraining, such as the 1D Heat or inviscid Burgers equation, MAE models tend to interpolate well.Furthermore, given information about the spatial discretization, MAE models can adapt to different PDEresolutions when trained to reconstruct multi-resolution inputs. This is true in 2D as well, with exampleresults shown in Appendix B.2. For PDEs that contain novel equation terms or boundary conditions (BCs), the MAE extrapolation per-formance is limited. Zero-shot reconstruction of the 1D Advection and KS equations shows mild trends,while reconstruction of the 2D Navier-Stokes equations is ineffective. To address this gap and investigatewhether MAE models can perform on complex high-resolution physics with multiple variables, we train anMAE model to reconstruct pressure and velocity on 2D smoke buoyancy data with varying buoyancy factors(Gupta and Brandstetter, 2022) and qualitatively show its reconstruction in Appendix B.3. In general, MAEmodels can adapt to complex scenarios and multiple physical variables; however, many of the fine details (e.g.eddies, shedding) become lost at high masking ratios. Since the 1D KdV-Burgers equation is solved with periodic BCs, we evaluate MAE extrapolation to Dirichletand Neumann BCs when reconstructing the 1D Heat and Wave equations, shown in Appendix B.1. Overall",
  "Latent Space Evaluation": "To better understand the latent representation learned by masked pretraining, we use the MAE encoderto embed various PDE validation samples and visualize embeddings with t-SNE (van der Maaten andHinton, 2008) in . Through self-supervised learning, MAE models can learn trends in PDEs withoutlabeled data and with limited extrapolation abilities. For example, through masked reconstruction of the 1DKdV-Burgers equation, MAE models can learn coefficient-dependent trends (A). This can be appliedto PDEs not seen during training, as the same MAE model can distinguish samples from the Advectionequation based on wave speed c (B). This extrapolation is also observed when embedding samplesfrom different PDEs; representations learned from pretraining on the KdV-Burgers equations allow modelsto cluster PDE samples from the Heat, Burgers, Advection, and KS equations (C). Lastly, MAEmodels are able to distinguish varying resolutions of PDE data after multi-resolution training, suggestinghigh model capacity across both diverse PDEs and discretizations (D). There are certainly limitations to emergent trends learned by masked pretraining. Unseen boundary conditionscan be challenging; when pretrained on periodic 1D KdV-Burgers data, MAE models can only distinguishbetween periodic and non-periodic Heat equation samples without understanding differences between Dirichlet",
  "PDE Feature Prediction": "To evaluate the latent representations learned from masked pretraining, we regress PDE coefficients andclassify PDE boundary conditions, equation families, and spatial resolutions from an initial time window.Regression tasks are separated by PDE (KdV-B, KS, Heat, Adv, Burgers, NS). Further, classification tasks areformulated as: (HeatBC): predicting Periodic, Dirichlet, or Neumann BCs from the Heat equation, (WaveBC):predicting Dirichlet or Neumann BCs from the Wave equation, (PDEs): predicting unseen PDEs from Heat,Adv, Burgers, and KS equation samples, and (Res.): predicting resolutions from nx = {50, 60, 70, 80, 90, 100}in 1D or (nx, ny) = {(48, 48), (52, 52), (56, 56), (60, 60), (64, 64)} in 2D. Several model variants are evaluated: a randomly initialized ViT baseline (MAEb), a pretrained, frozen MAEencoder with a linear head (MAEf), and a pretrained, fine-tuned MAE encoder (MAE). For these MAEmodels, regression and classification are performed using a CLS token and projecting the CLS embeddingto the number of prediction features through a simple MLP. The baseline encoder is a ViT with the samemodel size and architecture as the pretrained MAE encoder, and when the MAE encoder is frozen, only theMLP head receives gradient updates. Additionally, we train MLP and CNN models to regress coefficients orclassify PDE features in order to benchmark the difficulty of these tasks. Since these models cannot processvariable sized inputs, the Res. experiment is not performed for these simpler baselines. Results are shown inTables 3 & 4, with full error bars in Appendix E.1. In 1D, the frozen MAEf encoder is able to outperform a supervised baseline on the KdV-B, Heat, PDEs, andRes. tasks despite never seeing labels throughout training. Further performance gains can be realized byallowing the MAE encoder to fine-tune on labeled data, and can outperform random initialization on unseenequations and BCs. An exception to this is the Advection and Wave equations. We hypothesize that thesePDEs are heavily governed by the wave speed c and boundary conditions (bouncing vs. reflecting waves) andare simple trends that supervised models can learn quickly. Indeed, the simple MLP and CNN baselines areable to achieve perfect classification accuracy on the Wave equation, likely since the wave either reflects orbounces off the boundaries according to the BC.",
  "ModelKdV-BHeatAdvKSHeatBCWaveBCPDEsRes": "MLP6.6411.2902.4350.8211.7330.00027.24CNN1.8351.2501.4700.3420.1950.0000.427MAEb3.4540.8340.2410.3540.1230.0220.35564.52MAEf1.3340.6770.5510.3681.1641.8160.17463.34MAE0.9050.5050.2440.1560.0180.0530.02528.32 : 2D PDE feature prediction after MAE pretraining on a combined set of 2D Heat, Advection, andBurgers equations. Models are fine-tuned on 1024 held-out, labeled samples for each task, or 3072 samplesin the combined case. Regression errors are given as RMSE101 and classification errors are given asX-Ent101, averaged over 5 seeds.",
  "CNN0.3051.0571.3700.3711.224MAEb0.0840.5060.6820.3200.7480.694MAEf0.2320.5400.6060.3840.7090.636MAE0.0620.5070.4090.2650.5940.005": "In 2D, the effects of fine-tuning are more pronounced. Within the pretraining set, only in the 2D Burgers taskdoes freezing the MAE encoder outperform a supervised baseline; nevertheless, masked pretraining serves asa good initialization for supervised fine-tuning. In addition, despite differing physics, prior knowledge fromsimpler 2D PDEs seems to benefit regression on the Navier-Stokes equations. When classifying 2D Heat,Advection, and Burgers data based on their discretization, MAE models greatly benefit from pretraining onmulti-resolution data. We hypothesize that in 2D PDEs, variable spatial resolutions can be challenging todistinguish due to flattening the spatial dimension when patchifying inputs, whereas in 1D PDEs the data isalready flattened.",
  "Conditional Time-stepping and Super-resolution": "1D ExperimentsTo evaluate the use of the MAE encoder for practical tasks, we train a neural solveron various 1D PDEs to predict or upsample physics. For prediction, or time-stepping, models are givensolutions are time t and queried to predict solutions at at time t + 1. For upsampling, or super-resolution,models are given a low-resolution solution at time t and queried to predict a high-fidelity solution at time t.For our experiments in 1D, we consider five PDEs (KdV-B, Heat, Burgers, Adv, KS) as well as two PDEsunder varying boundary conditions (HeatBC, WaveBC) and predicting physics on various resolutions of theKdV-Burgers equation (Res.). Time-stepping is performed autoregressively by predicting multiple timestepssimultaneously to reduce error accumulation (Brandstetter et al., 2022). Furthermore, the pushforward trick(Brandstetter et al., 2022) is implemented. This adds model noise to inputs during training by making aprediction of a future timestep and using that prediction as a noised input the model; importantly gradientsare not calculated for the initial pass. Lastly, we test on FNO (Li et al., 2020) and Unet architectures (Guptaand Brandstetter, 2022), (Ronneberger et al., 2015), and add conditioning information to hidden states afterconvolutions (Ho et al., 2020; Nichol and Dhariwal, 2021). For super-resolution (SR), we implement a pipeline in which a network encodes low-resolution physics beforeupsampling with a discretization inversion operator D1 (linear interpolation in 1D and bicubic interpolationin 2D) and mapping to an output function space with a neural operator (Yang et al., 2023).Following this, weimplement a Resnet encoder (Wang et al., 2021; Zhang et al., 2018) followed by an interpolation scheme andFNO operator; both Resnet and FNO models are provided conditioning information from MAE encodings of",
  "Interp.0.5400.3451.3570.2313.5990.2250.347SR0.5200.2030.8810.2232.6730.2100.516SR-MAEf0.4810.1730.6910.1692.4600.2040.376SR-MAE0.4750.1510.6760.1942.4220.1700.349": "low-resolution physics. The motivation behind using a two-step SR pipeline is to learn a vector embeddingusing the Resnet, then map from vector to function space with T 1, and finally transform this function usingan learnable operator to the output. For additional details on hyperparameters, see Appendix C. Additionally,we evaluate a simple baseline using linear interpolation to upsample low-resolution inputs (Interp.). After pretraining an MAE encoder on the 1D KdV-Burgers equation, we compare the base neural solvers (FNO,Unet, SR) to conditioning on a frozen MAE embedding (-MAEf) and allowing the MAE encoder to fine-tunewhen conditioning (-MAE). Results in 1D are presented in . Within the pretraining distribution(KdV-B) and certain PDEs, MAE conditioning consistently improves time-stepping and super-resolutionperformance. In addition, allowing MAE encoders to fine-tune can further lower errors. However, thereare various exceptions, in particular PDEs with unseen boundary conditions. Despite this, improvementsare consistent across different neural solver architectures, suggesting that pretrained MAE models can beagnostic to downstream model choices. In addition, in 1D, SR results are less significant suggesting thatsimple interpolation schemes are often enough for these phenomena, especially for simple equations such asthe advection or wave PDEs. 2D ExperimentsFollowing the setup in 1D, we repeat time-stepping/super-resolution experiments on2D PDEs (Heat, Adv, Burgers, NS) and a combined set of 2D Heat, Advection, and Burgers equations(Combined). Additionally, we evaluate time-stepping performance on the combined 2D Heat, Advection, andBurgers equations discretized at variable resolutions (Res.). We follow the same conditioning and trainingstrategies as 1D experiments, but modify the architectures to support 2D inputs, and present results in . Additionally, the interpolation baseline implements bicubic upsampling (Interp.). After pretraining anMAE encoder on the 2D Heat, Advection, and Burgers equations, we observe improvements in conditionalphysics prediction and upsampling. Improvements tend to be more pronounced in 2D; we hypothesize thatthe increased difficulty of the task increases the importance of MAE encoder guidance in time-stepping andsuper-resolution. However, out-of-distribution datasets are still challenging: when extrapolating pretrainedencoders to new PDEs, such as the Navier-Stokes equations, the performance is limited. Nevertheless, weobserve similar trends whereby MAE conditioning is agnostic to downstream architectures. Additional BenchmarksWe consider two additional benchmarks: a randomly initialized ViT encoder thatembeds PDE inputs to a conditioning vector as well as a linear model that encodes ground-truth coefficient orboundary condition information to a conditioning vector. We present detailed results in Appendix E.2 and E.4,and discuss the overall results here. In general, we observe that the randomly initialized, fine-tuned encoderalso improves PDE prediction and upsampling, and this improvement generally matches or outperformsthe performance of the frozen MAE encoder. However, allowing the MAE encoder to fine-tune generallyoutperforms this random initialization and approaches the linear benchmark.",
  "Interp.0.4920.9370.4880.6730.367SR0.1752.0140.2950.5340.326SR-MAEf0.1590.6590.2640.4070.347SR-MAE0.1520.6390.2530.4720.337": "Although the linear benchmark generally performs the best in 1D, there are certain exceptions to this.Equations dominated by the coefficient response generally suffer from coefficient conditioning; we observethat the model heavily overfits to the true coefficient and does not learn the underlying PDE dynamics.Furthermore, for PDEs that do not have coefficient information, such as the 1D inviscid Burgers equation,this linear benchmark cannot be applied. In these cases, MAE encodings can still improve performance,suggesting that there are latent PDE features beyond coefficient information that neural solvers can benefitfrom. Lastly, in 2D the linear benchmark performs much worse, only achieving the lowest error in a fewscenarios, and in some cases harming performance of the base model. This could be because PDE dynamicsbecomes much more complex in 2D and relies less on coefficient information, which is a low-dimensionalvector and provides sparse information. Lastly, to benchmark our method against other pretraining methods, we consider pretraining an encoder usinga contrastive self-supervised technique proposed by Mialon et al. (2023), which relies on using Lie symmetriesto cluster PDE data in a learned latent space. We contrastively pretrain an encoder on 1D KdV-Burgersdata and the evaluate conditional timestepping performance on various downstream 1D PDEs. We presentthese results in Appendix E.3. To summarize, our approach is on par with Lie contrastive pretraining forPDE samples within the pretraining distribution; however, when extrapolating to unseen PDEs, maskedpretraining is able to outperform contrastive methods.",
  "Discussion": "In this section, we discuss results and provide additional insight. Although masking as a pretraining strategyis not physically valid, this requirement does not seem to be necessary for learning. Both in this study, andin related works (Hao et al., 2024; Zhou et al., 2024; Rahman et al., 2024), noising or masking strategiesare used to improve model performance, which both represent artificial phenomena. Within the context ofmasking, it is also natural to ask if a unique solution exists given a masked input. Certainly at the extremewhere all of the input is masked the solution is not unique; however, it seems empirically that only a smallamount of information (25% in 1D and 10% in 2D) is needed, which is corroborated by related work in theCV domain (Feichtenhofer et al.). We can observe this through the validation error: if this quantity is small,a unique solution is being regressed in the validation set. While promising, the presented results in time-stepping and super-resolution would likely not outperform afoundation model trained specifically for these tasks (Hao et al., 2024; Herde et al., 2024). This is becausedirect transfer learning has been shown to outperform surrogate objectives when pretraining for PDEs (Zhouet al., 2024); one hypothesized reason for this is the lack of abundant unlabeled data in the PDE domain(or equivalently, the downstream task uses also unlabeled data). However, encoder-style approaches such",
  "Conclusion": "We have presented a new method that extends masked pretraining from vision and video domains tophysics, and evaluated several PDE-specific modifications. We empirically validate MAE models throughreconstructing a diverse set of 1D and 2D PDEs and show limited generalization behavior to different spatialdiscretizations and unseen equations. Furthermore, we evaluate the latent representations learned duringMAE training and find structured trends that can be used to predict PDE features. In practice, MAEencoders can also be used to improve time-stepping and super-resolution tasks across diverse physics scenarios. A promising direction would be to scale MAE models to larger datasets, as the current approach exhibits thesame scalability as the originally proposed MAE (He et al., 2021). Additionally, future work could exploremasked modeling approaches in more complex 2D and 3D problems. Lastly, future work could exploremanipulating latent physics to generate new solutions or performing arithmetic in an autoencoder latentspace. We present a potential setup for this in Appendix D which relies on encoding solutions from separateequations, adding them in latent space, and decoding this latent vector.",
  "Junhong Shen, Tanya Marwah, and Ameet Talwalkar. Ups: Towards foundation models for pde solvingvia cross-modal adaptation, 2024": "Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, MichaelMahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Characterizingscaling and transfer behavior. 5 2023. URL Michael McCabe, Bruno Rgaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer,Alberto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee,Tiberiu Tesileanu, Kyunghyun Cho, and Shirley Ho. Multiple physics pretraining for physical surrogatemodels, 2023. Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar,Jian Song, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pdepre-training, 2024.",
  "Johannes Brandstetter, Max Welling, and Daniel E. Worrall. Lie point symmetry data augmentation forneural pde solvers. 2 2022. URL": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners, 2020.",
  "Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, and Michael W. Mahoney.Data-efficient operator learning via unsupervised pretraining and in-context learning. 2 2024. URL": "Somdatta Goswami, Katiana Kontolati, Michael D. Shields, and George Em Karniadakis. Deep transferoperator learning for partial differential equations under conditional shift. Nature Machine Intelligence,4(12):11551164, December 2022. ISSN 2522-5839. doi: 10.1038/s42256-022-00569-2. URL Ayan Chakraborty, Cosmin Anitescu, Xiaoying Zhuang, and Timon Rabczuk. Domain adaptation basedtransfer learning approach for solving pdes on complex geometries. Engineering with Computers, 38(5):45694588, Oct 2022. ISSN 1435-5663. doi: 10.1007/s00366-022-01661-2. URL Hengjie Wang, Robert Planas, Aparna Chandramowlishwaran, and Ramin Bostanabad. Mosaic flows: Atransferable deep learning framework for solving pdes on unseen domains. Computer Methods in AppliedMechanics and Engineering, 389, 2 2022. ISSN 00457825. doi: 10.1016/j.cma.2021.114424.",
  "AMAE Implementation": "To implement the MAE encoder and decoder, we use a ViT architecture (47), which uses a self-attention layer(20) and MLP, both with LayerNorms (74). We present hyperparameters in . To study the effectsof various hyperparameters, including model size, masking ratio, and patch size, we run ablation studieson masked reconstruction of 1D PDEs, and report reconstruction MSE errors on a validation set in . Overall, we find that increasing model size in limited data regimesonly 10000 KdV-Burgers sampleswere used in pretrainingtends to contribute to overfitting and increases validation errors. Predictably,increasing the masking ratio increases reconstruction errors as a result of less information being provided toMAE models. Furthermore, decreasing the patch size reduces errors but requires a higher computationalcost, which is consistent with results in CV domains (56). In 1D, the MAE is trained on a single NVIDIA GeForce RTX 2080 Ti, and reaches convergence in about 6hours. In 2D, the MAE is trained on a single NVIDIA RTX A6000, and reaches convergence in about 24hours. Model size, masking ratio, and patch size all affect the computational cost and can be used to tradeoffperformance for compute and memory. To motivate the ViT architecture, we investigate the effect of different model choices and hyperparameterson performance. We find that FNO autoencoders tend to have poor reconstruction capabilities due tointroducing spurious high-frequency modes when masking spatially. Unet approaches fare better but stillsuffer sharp boundaries across masked and unmasked regions. Additionally, we evaluate different ViT variants,such as ViViT (axial attention) and Swin Transformer (window attention). We find that restricting theattention mechanism reduces performance, and the additional speedups were not significant since masking outlarge portions of the input already reduces computation. Lastly, model performance tends to vary smoothlywith changes in hyperparameters; for example, reducing patch size slightly increases performance acrossdownstream tasks.",
  "B.2Additional 2D MAE Predictions": ": Additional 2D MAE Reconstruction examples after pretraining on the 2D Heat, Advection, andBurgers Equations. Each sample is shown with the masked sample (Top), MAE reconstruction (Middle),and ground truth PDE (Bottom). We include sample MAE predictions at variable resolutions for the 2DHeat, Advection, and Burgers equations; the lowest resolution (top) is (48, 48), the medium resolution(middle) is (52, 52), and the high resolution (bottom) is (56, 56) We include additional reconstructions of theincompressible NS equations at the native resolution (64, 64).",
  "B.32D Smoke Buoyancy Predictions": ": MAE validation reconstructions after training on 2D Navier-Stokes data with variable buoyancyfactors (23). The MAE model is trained on a resolution of (nt, nx, ny) = (56, 128, 128) with three datachannels (, vx, vy) and a masking ratio of 0.75. Triplets are shown with the masked input (top), MAEreconstruction (middle), and ground truth (bottom), with the top, middle, and bottom triplets displayingdensity, X velocity, and Y velocity. The complex dynamics is challenging; indeed, many of the fine details arelost in the MAE reconstruction. We train with a larger model (45M params) and patch size (2, 8, 8), whichtakes around 9 hours on a NVIDIA RTX A6000 GPU.",
  "DLatent Arithmetic": "MAE encoders have shown strong capabilities in extracting information from self-supervised PDE learning,creating opportunities to operate in this latent space.This could be beneficial since many PDEs arecompositions of simpler phenomena, and recombining PDE solutions in latent space may result in obtainingnovel PDE solutions for free. After pretraining on the 1D KdV-Burgers equation, we consider an arithmetictask where samples of the Heat and Burgers equation are embedded and added in the latent space before beingdecoded to a novel solution (). Concretely, we generate 1D Heat data from the PDE: tuxxu = 0,1D inviscid Burgers data from the PDE: tu + uxu = 0, and 1D viscous Burgers data from adding the twoPDEs: tu xxu + uxu = 0. When given identical initial conditions and coefficients, PDE reconstructionsobtained from summing latent Heat and Burgers embeddings qualitatively resemble solutions of the viscousBurgers PDE. In addition, if different latent embeddings can be added, the weighting of each embeddingcan be varied to control the resemblance of the reconstruction to make interpolated samples that have moreshock formation or diffusive behavior (e.g., more resemble the Burgers or Heat equation). : A proposed setup for operating on latent PDE vectors. PDE data is encoded after maskedpretraining and summed in the latent space before being decoded. These reconstructions can approximatesummed PDEs in physical space.",
  "MAEb0.022 0.0040.123 0.1080.355 0.27664.52 3.475MAEf1.817 1.1751.164 0.8380.174 0.06463.34 3.71MAE0.053 0.0480.018 0.0020.025 0.01328.322 23.351": "Detailed results for 1D PDE feature prediction tasks are reported in Tables 10 and 11. For 1D tasks, certainexperiments have high variance; we hypothesize that this is due to the fact that each seed samples a randomdataset of 2000 samples from a much larger dataset. This would make some seeds easier to regress/classifythan others, but within each seed the models follow trends consistent with the mean statistics. Furthermore,the magnitude of the X-Ent error is very small, leading to high variations after the model has learned mostof the relevant features. : 2D PDE coefficient regression and feature classification. Validation errors are reported as RMSE101 for regression tasks and X-Ent 101 for classification tasks. The mean error and standard deviationare calculated over five seeds; error bars are reported as one standard deviation above or below the mean.Lowest errors that are statistically significant are bolded.",
  "ModelKSHeatBCWaveBCRes": "FNO0.83 0.0280.147 0.0152.408 0.8481.141 0.021FNO-Enc0.82 0.0820.133 0.0192.012 1.1941.038 0.037FNO-MAEf0.821 0.0880.135 0.0131.747 0.6651.018 0.13FNO-MAE0.812 0.0610.148 0.0151.846 1.8851.070 0.011FNO-Lin0.757 0.0770.132 0.0201.454 0.4500.899 0.01 Unet1.333 0.0680.747 0.0434.249 2.2960.766 0.083Unet-Enc1.203 0.1020.691 0.0464.902 1.9350.739 0.088Unet-MAEf1.241 0.0550.699 0.0234.734 2.1350.688 0.077Unet-MAE1.125 0.0290.659 0.0475.157 1.7600.683 0.087Unet-Lin1.172 0.0390.717 0.0274.727 2.0930.573 0.095 Following the main paper, we introduce two conditional benchmarks. We evaluate a randomly initialized andfine-tuned ViT encoder with the same architecture as the MAE encoder (-Enc), as well as a linear encoderthat embeds the ground-truth PDE parameters as the conditioning information (-Lin). In 1D, time-stepping results tend to have high variance; however, overall trends are still consistent with thosereported in the main body. The variance is likely attributed to variations in the dataset for each seed; eachseed samples a different set of 2000 samples from a larger PDE dataset, and as a result, some data splits maybe easier than others. This results in the variance being high across seeds, however, within a seed (i.e. withina dataset), model performance closely follows trends consistent with the mean statistics.",
  "FNO0.978 0.0550.466 0.0141.006 0.02FNO-Enc0.767 0.0280.514 0.1230.709 0.055FNO-MAEf0.607 0.0190.59 0.1070.701 0.051FNO-MAE0.494 0.0430.477 0.0290.499 0.024FNO-Lin0.977 0.0210.445 0.0260.986 0.015": "Unet0.835 0.0670.713 0.0050.908 0.061Unet-Enc0.791 0.0610.695 0.0270.971 0.023Unet-MAEf0.761 0.0510.669 0.0310.861 0.028Unet-MAE0.669 0.0150.692 0.0390.676 0.064Unet-Lin1.013 0.030.635 0.0021.098 0.026 In 2D, time-stepping results have much lower variance; this is likely due to the fact that each seed uses thesame dataset, with only the shuffling changing. Furthermore, the linear benchmark is less effective; in mostexperiments a learned encoding can outperform ground-truth PDE parameters, especially when predicting acombined or multi-resolution dataset of PDEs.",
  "E.3Comparison to Contrastive Learning with Lie Augmentations": ": We compare our approach to a contrastive self-supervised approach. After training a masked andcontrastive encoder on the KdV-B pretraining set, we compare conditioning an FNO backbone to time-stepdifferent downstream 1D PDEs. The MAE encoder shows comparable performance within the pretrainingset (KdV-B), but has better generalization behavior to unseen PDEs. Validation errors are reported asnormalized L2 loss summed over all PDE timesteps",
  "SR2.673 0.1010.210 0.0160.516 0.015SR-Enc2.585 0.0620.174 0.010.373 0.022SR-MAEf2.460 0.0560.204 0.0150.376 0.032SR-MAE2.422 0.0520.170 0.0190.349 0.022SR-Lin2.517 0.0380.177 0.0270.451 0.014": "Differences between benchmarks for 1D super-resolution tend to be incremental. Despite this, using a frozenMAE encoding remains a simple method to improve performance with a negligible training cost. In general,super-resolution for 1D PDEs is a relatively easy task, and changes in model architecture do not significantlyaffect results. : Conditional 2D super-resolution. Validation errors are reported as RMSE 101 summed over allPDE timesteps. he mean error and standard deviation are calculated over three seeds; error bars are reportedas one standard deviation above or below the mean. Lowest errors that are statistically significant are bolded.",
  "FComparison of Latent Embeddings": "To understand the versatility of masking pretraining, we would like to consider what it means to be a PDElearner beyond predicting future physics. For example, when reasoning about physics, humans can add,subtract, and rearrange equations to derive new knowledge. Additionally, we are able to identify what similaror different physical phenomena are in order to reason about relationships between physical quantities. Lastly,when observing physics, we intuitively understand that solutions must evolve forward with time and betemporally coherent. In the context of machine learning, these behaviors would have to be manifested inlatent representations of physical solutions, and to this end, we demonstrate that masked autoencoders canlearn expressive, general representations of PDEs. To do this, we propose a set of experiments to compare the MAE model against other pretraining paradigmssuch as contrastive or transfer learning. Firstly, we pretrain and MAE or contrastive encoder (35) on the 1DKdV-Burgers pretraining set. Additionally, we pretrain a FNO and Unet model to predict future timestepson this dataset in order to model a transfer learning scenario. Given these pretrained models, we seek tounderstand their latent representations. To do this, we embed PDE samples from three different scenarios. Firstly, we embed equation samples from the Heat and Burgers equations and add these two embeddings inlatent space; the average pairwise distance is calculated between this summed embedding and a correspondingembedding from the viscous Burgers equation, which is an experiment in latent arithmetic (Arithmetic). Next,the average pairwise distance between embeddings from the Heat equation is calculated; embedded sampleshave varying initial conditions but evolve with the same coefficients, which tests if models can embed similardynamics to similar representations (Similarity). Lastly, he average pairwise distance between embeddings ofsubsequent timesteps of the Heat equation is calculated, measuring temporal coherence of latent embedding(Temporal). To ensure a fair comparison, embeddings are projected to a common dimension d = 16 withPCA and normalized (v = v/max(||v||2)). Masked pretraining performs well across these experiments andlearns general representations due to its minimal inductive bias, compared to contrastive learning or neuralsolvers which have specific objectives to either maximize similarity or predict next the timestep."
}