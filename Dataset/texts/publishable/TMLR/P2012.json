{
  "Abstract": "This paper investigates the relationship between graph convolution and Mixup techniques.Graph convolution in a graph neural network involves aggregating features from neighboringsamples to learn representative features for a specific node or sample. On the other hand,Mixup is a data augmentation technique that generates new examples by averaging featuresand one-hot labels from multiple samples. One commonality between these techniques istheir utilization of information from multiple samples to derive feature representation. Thisstudy aims to explore whether a connection exists between the two. Our investigation revealsthat, under two mild modifications, graph convolution can be viewed as a specialized form ofMixup that is applied during both the training and testing phases. The two modificationsare 1) Homophily Relabel - assigning the target nodes label to all its neighbors, and 2)Test-Time Mixup - Mixup the feature during the test time. We establish this equivalencemathematically by demonstrating that graph convolution networks and simplified graphconvolution can be expressed as a form of Mixup. We also empirically verify the equivalenceby training an MLP using the two modifications to achieve comparable performance.",
  "Introduction": "Graph Neural Networks (GNNs) (Wu et al., 2020; Zhou et al., 2020) have recently been recognized as thede facto state-of-the-art algorithm for graph learning. The core idea behind GNNs is neighbor aggregation,which involves combining the features of a nodes neighbors. Specifically, for a target node with feature xi,one-hot label yi, and neighbor set Ni, the graph convolution operation in GCN is essentially as follows:",
  "y0y0": ": Graph convolution is Mixup. (a) illustrates the basic idea of Mixup: averaging the features andone-hot labels of multiple samples ( ,,). (b) shows the graph convolution operation where the feature ofthe target node ( ) is the weighted average of the features of all its neighbors. (b) (c) shows that graphconvolution is Mixup if we assign the label () of the target node ( ) to all of its neighbors ( ,). (d)shows that Mixup is empirically equivalent to GCN. In this paper, we answer this question by establishing the connection between graph convolutions and Mixup,and further understanding the graph neural networks through the lens of Mixup. We show that graphconvolutions are intrinsically equivalent to Mixup by rewriting Equation (1) as follows:",
  "where xi and yi are the feature and label of the target node ni. This equation states that graph convolutionis equivalent to Mixup if we assign the yi to all the neighbors of node ni in set Ni": "To demonstrate the equivalence between graph convolutions and Mixup, we begin by illustrating that aone-layer graph convolutional network (GCN) (Kipf & Welling, 2016b) can be transformed into an inputMixup. A two-layer GCN can be expressed as a hybrid of input and manifold Mixup (Verma et al., 2019).Similarly, simplifing graph convolution (SGC) (Wu et al., 2019) can be reformulated as an input Mixup. Wethus establish the mathematical equivalence between graph convolutions and Mixup, under two mild andreasonable modifications: 1) assign the target nodes label to neighbors in the training time (referred to asHomophily Relabel); 2) perform feature mixup in the test time (referred to as Test-Time Mixup). We further investigate the modifications required for the equivalence between graph convolution and Mixup,focusing on the effect of Homophily Relabel and Test-Time Mixup.To explore Homophily Relabel, wedemonstrate that training an MLP with Homophily Relabel (called HMLP) is equivalent to training GCN interms of prediction accuracy. This finding provides a novel perspective for understanding the relationshipbetween graph convolution and Mixup and suggests a new direction for designing efficient GNNs.Toinvestigate Test-Time Mixup, we train GNNs without connection information, perform neighbor aggregationduring inference, and find that this approach can achieve performance comparable to traditional GNNs. Thisresult reveals that Test-Time Mixup can be a powerful alternative to traditional GNNs in some scenarios, andsuggests that Mixup may have a broader range of applications beyond traditional data augmentation. Ourinvestigation on Homophily Relabeling and Test-Time Mixup provides valuable insights into the theoreticalproperties of GNNs and their potential applications in practice. We highlight our contributions as follows: We establish for the first time connection between graph convolution and Mixup, showing that graphconvolutions are mathematically and empirically equivalent to Mixup. This simple yet novel findingpotentially opens the door toward a deeper understanding of GNNs.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Ruoxi Sun, Hanjun Dai, and Adams Wei Yu. Does GNN pretraining help molecular representation? InAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural InformationProcessing Systems, 2022. URL Xianfeng Tang, Yozen Liu, Xinran He, Suhang Wang, and Neil Shah. Friend story ranking with edge-contextual local graph convolutions. In Proceedings of the Fifteenth ACM International Conference on WebSearch and Data Mining, pp. 10071015, 2022. Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. Onmixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances inNeural Information Processing Systems, 32, 2019. Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla. Learning MLPs on graphs:A unified view of effectiveness, robustness, and efficiency.In International Conference on LearningRepresentations, 2023. URL",
  "Based on Homophily Relabel and Test-Time Mixup, we propose two variants of MLPs based on theMixup strategy, namely HMLP and TMLP, that can match the performance of GNNs": "Related Work. Graph neural networks are widely adopted in various graph applications, including socialnetwork analysis (Fan et al., 2019), recommendation (Wei et al., 2022; Deng et al., 2022; Cai et al., 2023;Tang et al., 2022), knowledge graph (Cao et al., 2023; Zhao et al., 2023), molecular analysis (Sun et al., 2022;Zhu et al., 2023a; ZHANG et al., 2023; Wang et al., 2023b; Corso et al., 2023; Liao & Smidt, 2023; Xia et al.,2023; Hladi et al., 2023), drug discovery (Sun et al., 2022; Zhang et al., 2023c), link prediction (Chamberlainet al., 2023; Cong et al., 2023) and others (Chen et al., 2023a). Understanding the generalization and workingmechanism of graph neural networks is still in its infancy (Garg et al., 2020; Zhang et al., 2023a; Yanget al., 2023; Baranwal et al., 2023). The previous work attempts to understand graph neural networks fromdifferent perspectives, such as signal processing (Nt & Maehara, 2019; Bo et al., 2021; Bianchi et al., 2021),gradient flow (Di Giovanni et al., 2022), dynamic programming(Dudzik & Velikovi, 2022), neural tangentkernels (Yang et al., 2023; Du et al., 2019; Sabanayagam et al., 2022) and influence function (Chen et al.,2023b). There is also a line of works that analyzes the connection between GNNs and MLPs (Baranwalet al., 2023; Han et al., 2022b; Yang et al., 2023; Tian et al., 2023), which is similar to the proposed methodin . In this work, we understand graph neural networks through a fresh perspective, Mixup. Webelieve that this work will inspire further research and lead to the development of new techniques to improvethe performance and interpretability of GNNs. In parallel, Mixup (Zhang et al., 2018) and its variants (Vermaet al., 2019; Yun et al., 2019; Kim et al., 2020; 2021) have emerged as a popular data augmentation techniquethat improves the generalization performance of deep neural networks. More specifically, deep neural networkstrained with Mixup achieve better generalization (Chun et al., 2020; Zhang et al., 2020; Chidambaram et al.,2022), calibration (Thulasidasan et al., 2019; Zhang et al., 2022a), adversarial robustness (Pang et al., 2019;Archambault et al., 2019; Zhang et al., 2020; Lamb et al., 2019), and explain ensemble (Lopez-Paz et al.,2023). More related work in Appendix A. The Scope of Our Work. 1 This work not only provides a fresh perspective for comprehending GraphConvolution through Mixup, but also makes valuable contributions to the practical and theoretical aspects ofgraph neural networks, facilitating efficient training and inference for GNNs when dealing with large-scalegraph data. Practical Potentials: Both HMLP and TMLP hold potential value for practical usage. TMLP istraining-efficient, as its backbone and training process are both MLP-based; however, the inference requiresTest-Time Mixup (neighbor aggregation), which can be time-consuming. This limitation also exists in previousworks (Han et al., 2022b; Yang et al., 2023). HMLP, on the other hand, is both training and test efficient, asboth processes are based on MLP. This suggests the potential of HMLP for practical usage on large-scalegraphs. Theoretical Potentials: HMLP, derived from the Mixup, have the theoretical potential to understandthe expressiveness of GNNs from the Mixup perspective. With the connection between Mixup and graphconvolution, HMLP goes beyond traditional methods to understand the learning capability of GNNs.",
  "Graph Convolution is Mixup": "In this section, we reveal that the graph convolution is essentially equivalent to Mixup. We first present thenotation used in this paper. Then we present the original graph convolution network (GCN) (Kingma & Ba,2015) and simplifying graph convolutional networks (SGC) (Wu et al., 2019) can be expressed mathematicallyas a form of Mixup. Last, we present the main claim, i.e., graph convolution can be viewed as a special formof Mixup under mild and reasonable modifications. Notations. We denote a graph as G(V, E), where V is the node set and E is the edge set. The numberof nodes is N = |V| and the number of edges is M = |E|. We denote the node feature matrix as X ={x1, x2, , xN} RNd, where xi is the node feature for node ni and d is the dimension of features. We",
  "Preliminaries": "Graph Convolution. Graph Convolution Network (GCN) (Kipf & Welling, 2016a), as the pioneering workof GNNs, proposes Y = softmax( A ( A X W1) W2), where W1 and W2 are the trainable weights ofthe layer one and two, respectively. Simplifying Graph Convolutional Networks (SGC) (Wu et al., 2019) isproposed as Y = softmax( A A X W). In this work, we take these two widely used GNNs to show thatgraph convolution is essentially Mixup. Mixup. The Mixup technique, introduced by Zhang et al. (2018), is a simple yet effective data augmentationmethod to improve the generalization of deep learning models. The basic idea behind Mixup is to blend thefeatures and one-hot labels of a random pair of samples to generate synthetic samples. The mathematicalexpression of the two-sample Mixup is as follows:",
  "Connecting Graph Convolution and Mixup": "We demonstrate that graph convolution, using GCN and SGC as examples, is conditionally Mixup. To dothis, we mathematically reformulate the expressions of GCN and SGC to a Mixup form, thereby illustratingthat graph convolutions are indeed Mixup. One-layer GCN is Mixup We begin our analysis by examining a simple graph convolution neuralnetwork (Kipf & Welling, 2016a) with one layer, referred to as the one-layer GCN. We demonstrate that theone-layer GCN can be mathematically understood as an implementation of the input Mixup technique. Theexpression of the one-layer GCN is given by: Y = softmax( A X W) where W RDC is the trainableweight matrix. Let us focus on a single node ni in the graph. The predicted one-hot label for this node isgiven by yi = softmax(ai X W), where ai R1N, i-th row of A, is the normalized adjacency vector ofnode ni. Ni is the neighbors set of node ni. We make the following observations:",
  "Graph Convolution is (Conditionally) Mixup": "It is straightforward to derive the Mixup form of 1-layer, 2-layer GCN, and SGC as discussed above. Thisleads to the conclusion that 1-layer, 2-layer GCN, and SGC can all be reformulated in the form of Mixup.This establishes a mathematical connection between graph neural networks and the Mixup method. Buildingon these findings, in this section, we introduce our main contribution as follows:",
  "Test-Time Mixup At test time, the GNNs perform feature mixing on the nodes and then use the mixedfeature for the inference": "Both of the above modifications are mild and reasonable and have practical implications for GNN designand analysis. The Homophily Relabel operation can be understood as imposing the same label on the targetnode and all its neighbors, which corresponds to the homophily assumption for graph neural networks. Thehomophily assumption posits that nodes with similar features should have the same label, which is a common",
  "Discussion": "Comparison to Other Work. Hereby, we compare our work and previous work. Yang et al. (2023); Hanet al. (2022b) propose to train an MLP and transfer the converged weight from MLP to GNN, which canachieve comparable performance to GCN. In this work, these two methods are all included in part of ourwork, Test-Time Mixup. Different from previous work, we provide a fresh understanding of this phenomenaby connecting graph convolution to Mixup and also derive a TMLP Appendix C.2 to implement Test-TimeMixup. Baranwal et al. (2022) understand the effect of graph convolution by providing a rigorous theoreticalanalysis based on a synthetic graph. Different from this work, our work understands graph convolution witha well-studied technique, Mixup.",
  "Running time 1.83960.0264 174.14597.0611": "Why MLP is more efficient to GNN: GNN is Y =softmax( A X W), where A is a large sparse matrix,resulting in significant computation time. This is wellstudied in previous work Zhang et al. (2021); Yang et al.(2023). We also conducted experiments to measure theactual runtime of MLP and GNN. The results demonstrate the efficiency of MLP compared to GNN. Withmixup, the number of data points will double. The running time of MLP will at most double, resulting in arunning time ratio of MLP to GNN = (1.8396 * 2) : 174.1459 = 1 : 47.",
  "Is Homophily Relabel Equivalent to GCNs Training ?": "In this section, we conduct experiments to examine the Homophily Relabel proposed in .3. Theempirical evidence substantiates our claims, emphasizing the significant effect of Homophily Relabel on GNNs.This understanding can facilitate the design of an MLP equivalent that competes with the performance ofGNNs. Note that we explore the transductive setting in our experiments.",
  ":The examplegraphs.xi is the targetnode, The loss of xi thatconnected two nodes withdifferent labels": "We utilize the simple example graph to demonstrate the calculation of theloss function in a GCN in the transductive setting. The example graph, in, comprises three nodes. The blue and red nodes belong to the trainingset, while the gray node belongs to the test set. In the transductive scenario,the loss function is computed for the entire graph during the training phase,incorporating both the training and test nodes. In our simple example graph,the loss function will be calculated on the blue and red nodes with their actuallabels. Notably, the prediction for the gray node, despite it being part of thetest set, will also contribute to the overall loss calculation in the transductivesetting. For node ni, the prediction (p0, p1), the loss of the example graph willbe as follows",
  "(7)": "where y00/y01 is the 0th/1st elements in the one-hot label y0 for node n0. The above analysis shows that theactual training label for target node ni is (0.5, 0.5). From the Mixup perspective, in an ideally well-trainedmodel utilizing a two-sample Mixup, as shown in previous work (Guo et al., 2019; Pang et al., 2019), thefollowing approximation will hold f(kx+(1k)xk) kf(x)+(1k)f(xk). Intuitively, this approximation",
  "(f(x1) + f(xi)) .(8)": "From the above two equations, we can see that f(xi) are trained with the labels of all its neighbors x0 andx1. Thus in the next, we propose explicitly training the nodes in the test set with the label of their neighborsif they are in the training set. In our example graph, we explicitly train the gray node with label (0.5, 0.5) HMLPBased on the above analysis, we proposed Homophily Relabel MLP (HMLP), which achieves thecomparable performance of GCN via training an MLP as a backbone with Homophily Relabel. In detail, theproposed method HMLP has two major steps: 1) relabel all the nodes in the graph with the mixed labelY = A Y. 2) train an MLP on all the nodes with Y on relabeled graph, only using node features. Weillustrate our proposed HMLP with the example graph in . 1) we relabel all the nodes using HomophilyRelabel, the node with new label will be (x0, (1, 0)), (x1, (0, 1)), (xi, (0.5, 0.5)) 2) then train the HMLP withthese data samples. The test sample will be (xi, ?) during the test time.",
  "Average77.4583.8983.57": "To verify the proposed HMLP can achieve comparable per-formance to the original GCN, we train the one-layer GCN(GCN-1), two-layer GCN (GCN-2), SGC, and HMLP onthe training set and report the test accuracy based on thehighest accuracy achieved on the validation set. We exper-imented with different data splits of train/validation/test(the training data ratio span from 10% 90%), and wealso conducted experiments with the public split on Cora,CiteSeer, and PubMed datasets. We present the results in and . We also present the training and test curves of the MLP. GCN and HMLP in .",
  ": The training and test curves of GCNand HMLP": "From the results in and , HMLPachieves comparable performance to GCN whenthe ratio of training to test data is large. For the, the performance of HMLP is significantly betterthan MLP, and on par with GCN, especially when thetraining data ratio is large. The results also show thatwhen the training data ratio is small, HMLP performsworse than GCN. The training and test curves show thatHMLP achieve the similar test curves (red and green linesin the right subfigure) even though the training curve arenot the same. This may be because the labels of nodesare changed.",
  "How Does Test-time Mixup Affect GCN Inference?": "In this section, we aim to investigate how Test-time Mixup affects the GCN inference process. We conduct anempirical study to understand the functionality of Test-time Mixup (neighbor aggregation during the testtime). To this end, we propose an MLP involving Test-time Mixup only (TMLP). TMLP The proposed method TMLP follows the steps: 1) we train an MLP using only the node features of thetraining set. 2) we employ the well-trained MLP for inference with neighbor aggregation during testing. Thisparadigm has also been investigated in previous works (Han et al., 2022b; Yang et al., 2023). We call thismethod TMLP. We illustrate the TMLP with the example graph in . Specifically, we train the model",
  "How Does TMLP Perform Compared to GCN?": "In the section, we conducted experiments to verify the effect of Test-Time Mixup. In the experiments, duringthe training, we only use the node feature to train an MLP, and then during the test time, we use thewell-trained MLP with the neighbor aggregation to perform the inference. We present the results of varyingtraining data ratio in , and we also present the accuracy in when the training data ratio is0.6. Additionally, we compare the learned representation of GNNs and TMLP using t-SNE Van der Maaten &Hinton (2008). From these results, we make the following observations:",
  "Average77.4583.8984.06": "From these results, we make the following observations: TMLP almost achieves comparable performance toGNNs. In the , the performance of TMLP is signif-icantly better than MLP and on par with GCN, especiallywhen the training data ratio is large. In some datasets (i.e.,Cora, FullCoraML), the performance of TMLP is slightlyworse than GCN but still significantly better than MLP.It is worth noting that our proposed TMLP is only trainedwith the node features, ignoring the connection informa-tion. show the average accuracy (84.06) of TMLP is comparable to that of GCN (83.89).",
  ": Visualization of node representationslearned by MLP and TMLP. Node representationsfrom the same class, after Test-Time Mixup, be-come clustered together": "Based on the node representation, Test-Time Mixupmake the node representation more discriminativethan MLP. The visualization clearly illustrates that thenode representations within the same class, as learnedby TMLP, are clustered together. This clustering patterndemonstrates the advanced predictive power of TMLP, asit appears to have a keen ability to distinguish similargroup classes effectively. Consequently, this capacity al-lows TMLP to deliver superior classification performance.The node representation clearly shows what TMLP doesfrom the representation perspective.",
  ": Decision Boundary of MLP and TMLP. TMLP can cause node features to move away from thelearned boundary, as shown with. More experimental details are in Appendix C.3": "and after applying Test-Time Mixup. After training a model with training-time data mixup, we evaluate itsdecision boundary, which separates different classes in the feature space. We then apply test-time mixuptechniques to node features and reassess the decision boundary. Our findings indicate that Test-TimeMixup can cause node features to move away from the learned boundary, enhancing modelrobustness to unseen data. However, its crucial to strike a balance between Test-Time Mixup and modelperformance, as overdoing Mixup may reduce accuracy.",
  "Unifying HMLP and TMLP: Verifying the Main Results": "In this section, we unify HMLP and TMLP into a single MLP, which integrates both Homophily Relabel andTest-time Mixup. They are supposed to be equivalent to graph convolution. To unify these two MLPs, we 1)relabel all the nodes in the graph with the mixed label Y = A Y. 2) train an MLP on all the nodes with Yon relabeled graph, only using node features. 3) employ the well-trained MLP for inference with neighboraggregation during testing. To test the efficacy of this combined approach, we perform experiments comparingthe performance of the unified MLP with that of individual HMLP and TMLP, as well as other GCN. Theexperimental setup remains consistent with the previous setting with HMLP and TMLP. We present the resultsin . Unifying HMLP and TMLP can achieve comparable performance to original GNNs.",
  "Discussion and Future Work": "In this paper, we reveal the relation between graph neural networks and Mixup that graph neural networksare essentially Mixup, in the sense that they can be seen as a continuous interpolation between nodes in thegraph. This equivalence allows for a new interpretation of GNNs as a form of regularization, and provides anew perspective on the design of GNNs. Efficient Alternative Training and Inference Method for GNNs on Large Graph Data.HMLP presents an efficient solution for both training and testing on large-scale graphs, as it completelyeliminates the need for connection information during both stages. TMLP, though also MLP-based,emphasizes training efficiency. The improved training efficiency of TMLP can still be advantageous forreducing the training cost on large graph. Opening a New Door for Understanding Graph Convolution. The introduction of mixup, asimple node-level operation, offers a novel perspective on graph convolution by representing it in amixup form. Traditional graph convolution employs the adjacency matrix of the entire graph, a conceptthat can be complex to comprehend or articulate. By approaching the understanding of graph neuralnetworks from the entire graph level, we can demystify these complexities. This presents an essentialavenue for exploration in future work related to our paper. Future Work. Based on the above practical and theoretical potentials of the proposed methods in thispaper. Here are some potential future works: 1) improve HMLP to make it more practical for large-scalegraphs and the performance when the training data ratio is small. 2) The well-studied Mixup strategy wouldbe helpful for understanding the expressiveness of graph neural networks.",
  "Acknowledgements": "This work was done while Xiaotian Han was an intern at Meta. We are grateful for the detailed reviewsfrom the reviewers. Their thoughtful comments and suggestions were invaluable in strengthening this paper.Portions of this research were conducted with high performance research computing resources provided byTexas A&M University ( This work was supported in part by the National ScienceFoundation (NSF) IIS-2224843 and by the US Department of Transportation (USDOT) Tier1 UniversityTransportation Center (UTC) Transportation Cybersecurity Center for Advanced Research and Education(CYBER-CARE) grant #69A3552348332.",
  "Aseem Baranwal, Kimon Fountoulakis, and Aukosh Jagannath. Effects of graph convolutions in multi-layernetworks. In International Conference on Learning Representations, 2023. URL": "Filippo Maria Bianchi, Daniele Grattarola, Lorenzo Livi, and Cesare Alippi. Graph neural networks withconvolutional arma filters. IEEE transactions on pattern analysis and machine intelligence, 44(7):34963507,2021. Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolutionalnetworks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 39503957,2021.",
  "Chen Cai, Dingkang Wang, and Yusu Wang. Graph coarsening with neural networks. In InternationalConference on Learning Representations, 2020": "Xuheng Cai, Chao Huang, Lianghao Xia, and Xubin Ren. LightGCL: Simple yet effective graph contrastivelearning for recommendation. In The Eleventh International Conference on Learning Representations, 2023.URL Kaidi Cao, Jiaxuan You, Jiaju Liu, and Jure Leskovec. Autotransfer: AutoML with knowledge transfer -an application to graph neural networks. In International Conference on Learning Representations, 2023.URL Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich,Nils Yannick Hammerla, Michael M. Bronstein, and Max Hansmire. Graph neural networks for linkprediction with subgraph sketching. In International Conference on Learning Representations, 2023. URL",
  "Muthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge. Towards understanding thedata dependency of mixup-style training. In International Conference on Learning Representations, 2022.URL": "Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, and Youngjoon Yoo. An empir-ical evaluation on robustness and uncertainty of regularization methods. arXiv preprint arXiv:2003.03879,2020. Weilin Cong, Si Zhang, Jian Kang, Baichuan Yuan, Hao Wu, Xin Zhou, Hanghang Tong, and MehrdadMahdavi. Do we really need complicated model architectures for temporal networks? In InternationalConference on Learning Representations, 2023. URL Gabriele Corso, Hannes Strk, Bowen Jing, Regina Barzilay, and Tommi S. Jaakkola. Diffdock: Diffusionsteps, twists, and turns for molecular docking. In The Eleventh International Conference on LearningRepresentations, 2023. URL Leyan Deng, Defu Lian, Chenwang Wu, and Enhong Chen. Graph convolution network based recommendersystems: Learning guarantee and item mixture powered strategy. In Alice H. Oh, Alekh Agarwal, DanielleBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL",
  "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li, and Yoshua Bengio.Graph attention networks. In International Conference on Learning Representations, 2018": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David Lopez-Paz, andYoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In InternationalConference on Machine Learning, pp. 64386447. PMLR, 2019. Kun Wang, Yuxuan Liang, Pengkun Wang, Xu Wang, Pengfei Gu, Junfeng Fang, and Yang Wang. Searchinglottery tickets in graph neural networks: A dual perspective. In The Eleventh International Conference onLearning Representations, 2023a. URL",
  "Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification.In Proceedings of the Web Conference 2021, pp. 36633674, 2021": "Zichao Wang, Weili Nie, Zhuoran Qiao, Chaowei Xiao, Richard Baraniuk, and Anima Anandkumar. Retrieval-based controllable molecule generation. In International Conference on Learning Representations, 2023b.URL Chunyu Wei, Jian Liang, Di Liu, and Fei Wang. Contrastive graph structure learning via informationbottleneck for recommendation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho(eds.), Advances in Neural Information Processing Systems, 2022. URL Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graphconvolutional networks. In International conference on machine learning, pp. 68616871. PMLR, 2019. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensivesurvey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):424,2020. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li.Mole-BERT: Rethinking pre-training graph neural networks for molecules. In The Eleventh InternationalConference on Learning Representations, 2023. URL Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.Representation learning on graphs with jumping knowledge networks. In International conference onmachine learning, pp. 54535462. PMLR, 2018. Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently goodgeneralizers: Insights by bridging gnns and mlps. International Conference on Learning Representations,2023.",
  "Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. Advances in NeuralInformation Processing Systems, 33:1700917021, 2020": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix:Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 60236032, 2019. Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint:Graph sampling based inductive learning method. In International Conference on Learning Representations,2019.",
  "ZAIXI ZHANG, Qi Liu, Shuxin Zheng, and Yaosen Min. Molecule generation for target protein bindingwith structural motifs. In International Conference on Learning Representations, 2023. URL": "Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learningon large-scale text-attributed graphs via variational inference. In International Conference on LearningRepresentations, 2023. URL Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, ChangchengLi, and Maosong Sun. Graph neural networks: A review of methods and applications. AI Open, 1:5781,2020. Jinhua Zhu, Kehan Wu, Bohan Wang, Yingce Xia, Shufang Xie, Qi Meng, Lijun Wu, Tao Qin, Wengang Zhou,Houqiang Li, and Tie-Yan Liu. $\\mathcal{O}$-GNN: incorporating ring priors into molecular modeling.In International Conference on Learning Representations, 2023a. URL Zeyu Zhu, Fanrong Li, Zitao Mo, Qinghao Hu, Gang Li, Zejian Liu, Xiaoyao Liang, and Jian Cheng.$\\rm a^2q$: Aggregation-aware quantization for graph neural networks. In The Eleventh InternationalConference on Learning Representations, 2023b. URL",
  "A.1Graph Neural Networks": "Starting from graph convolutional network (Kipf & Welling, 2016a), graph neural networks have been appliedto various graph learning tasks and show great power for graph learning tasks. Despite the practical power ofgraph neural network, understanding the generalization and working mechanism of the graph neural networksare still in its infancy (Garg et al., 2020; Zhang et al., 2023a; Yang et al., 2023; Baranwal et al., 2023).The previous works try to understand graph neural networks from different perspectives, such as signalprocessing (Nt & Maehara, 2019; Bo et al., 2021; Bianchi et al., 2021), gradient flow (Di Giovanni et al.,2022), dynamic programming(Dudzik & Velikovi, 2022), neural tangent kernels (Yang et al., 2023; Du et al.,2019; Sabanayagam et al., 2022) and influence function (Chen et al., 2023b). There is also a line of works tryto understand graph neural networks by analyzing the connection between GNNs and MLPs (Baranwal et al.,2023; Han et al., 2022b; Yang et al., 2023; Tian et al., 2023). In our work, we understand graph neural networks through a fresh perspective, Mixup. We believe this workwill inspire further research and lead to the development of new techniques for improving the performanceand interpretability of GNNs.",
  "A.2Mixup and Its Variants": "Mixup (Zhang et al., 2018) and its variants (Verma et al., 2019; Yun et al., 2019; Kim et al., 2020; 2021)are important data augmentation methods that are effective in improving the generalization performance ofdeep neural networks. Mixup is used to understand or improve many machine learning techniques. Morespecifically, deep neural networks trained with Mixup achieve better generalization (Chun et al., 2020; Zhanget al., 2020; Chidambaram et al., 2022), calibration (Thulasidasan et al., 2019; Zhang et al., 2022a), andadversarial robustness (Pang et al., 2019; Archambault et al., 2019; Zhang et al., 2020; Lamb et al., 2019).As a studies data augmentation method, Mixup can be understood as a regularization term to improve theeffectiveness and robustness of neural network (Pang et al., 2019; Archambault et al., 2019; Zhang et al.,2020; Lamb et al., 2019). Mixup are also used to explain some other techniques, such as ensmeble (Lopez-Pazet al., 2023). Additionally, Mixup technique is also used to augment graph data for better performance ofGNNs Han et al. (2022a); Wang et al. (2021). In this work, we use Mixup to explain and understand the graph neural network, and based on the Mixup,we propose two kinds of MLP that achieve almost equivalent performance to graph neural networks. Also,our work is different form the works that chance the GNN training with Mixup, while we connect the graphconvolution and Mixup.",
  "A.3Efficient Graph Neural Networks": "Although the graph neural networks are powerful for graph learning tasks, they suffer slow computationalproblems since they involve either sparse matrix multiplication or neighbors fetching during the inference,resulting in active research on the efficiency of graph neural networks. The recent advances in efficient graphneural networks are as follows. Efficient graph neural network (Wang et al., 2023a; Shi et al., 2023; Hui et al.,2023; Zhang et al., 2023b; Zhu et al., 2023b) includes the sampling-based method (Hamilton et al., 2017;Chiang et al., 2019; Zeng et al., 2019), quantization (Liu et al., 2022), knowledge distillation (Zhang et al.,2021; Tian et al., 2023), and graph sparsification (Cai et al., 2020). There is a line of work to try to use MLPto accelerate the GNN training and inference (Zhang et al., 2022b; Wu et al., 2019; Frasca et al., 2020; Sunet al., 2021; Huang et al., 2020; Hu et al., 2021). Simplifying graph convolutional (SGC) (Wu et al., 2019)simplifying graph convolutional network by decoupling the neighbor aggregation and feature transformation,the following work such as You et al. (2020) employ the similar basic idea. Zhang et al. (2021) leverageknowledge distillation to transfer the knowledge from GNN to MLP. Han et al. (2022b) and Yang et al. (2023)transfer the weight from MLP to GNN. These kinds of methods use MLP for efficient graph neural networks. Our proposed methods HMLP and TMLP are efficient for training and/or inference, which can be categorizedas an efficient graph neural network. Besides, our proposed method provides a new perspective to understandgraph neural networks.",
  "B.3GNN with trainable mixing parameter": "In this experiment, we adopted a method similar to GAT to examine the performance of our proposed method,with the mixing parameter being learned during the training process. The Softmax (Li et al., 2020) is alearnable aggregation operator that normalizes the features of neighbors based on a learnable temperatureterm. We report the performance in the table below. The results show that the TMLP performed worse thanthe MLP. This indicates that the learnable mixing parameter does not work well for our proposed method",
  "C.1More Experiments for HMLP": "We present further experiments comparing MLP, GCN, and HMLP. These experiments complement thosediscussed in . We expand on that work by evaluating these methods with more GNN architectures.The results are presented in . The results on more GNN architectures indicate that our proposed HMLP method not only outperformstraditional MLP, but also attains a performance that is largely on par with GNNs in the majority of instances.",
  "C.4More Experiments on Deeper GCNs": "We conducted additional experiments on multi-layer graph neural network (SGC) on the Cora dataset. Weuse the 20%\\40%\\40% for train\\val\\test data split. The test accuracy was determined based on the resultsfrom the validation dataset. The test accuracy is reported in the following table. We observe from the resultsthat our proposed HMLP+TMLP method achieves performance comparable to that of SGC when multiplegraph convolutions are utilized.",
  "D.2Experiment Setup for": "We provide details of the experimental setup associated with . The primary objective of thisexperiment was to compare the performance of our proposed HMLP model with that of the GCN model.We utilized diverse datasets to facilitate a comprehensive evaluation. We set the learning rate to 0.1 forall methods and datasets, and each was trained for 400 epochs.Moreover, we did not use weight decay formodel training. The test accuracy reported is based on the best results from the validation set. The nodeclassification accuracy served as the evaluation metric for this experiment.",
  "D.3Experiment Setup for": "We provide details of the experimental setup associated with . We set the learning rate to 0.1 for allmethods, and each was trained for 400 epochs. Additionally, weight decay was not utilized in this process.The test accuracy reported is based on the best results from the validation set. The node classificationaccuracy served as the evaluation metric for this experiment.",
  "D.4Experiment Setup for": "We provide details of the experimental setup associated with . We set the learning rate to 0.1 for allmethods, and each was trained for 400 epochs. Additionally, weight decay was not utilized in this process.The test accuracy reported is based on the best results from the validation set. The node classificationaccuracy served as the evaluation metric for this experiment.",
  "EDatasets": "In this experiment, we use widely adopted node classification benchmarks involving different types of networks:three citation networks (Cora, CiteSeer and PubMed). The datasets used in this experiment are widelyadopted node classification benchmarks. These networks are often used as standard evaluation benchmarksin the field of graph learning. Cora: is a widely used benchmark data for graph learning, which is a citation network, where eachnode represents a research paper, and edges denote the citation relation. Node features are extractedfrom the abstract of the paper. CiteSeer is another citation network used frequently in GNN literature. It follows a similar structureto the Cora dataset, where each node represents a scientific document, and the edges are citationinformation between them. The node features are derived from the text of the documents. PubMed: PubMed is a citation network composed of scientific papers related to biomedicine. Aswith the other datasets, nodes represent documents, and edges indicate citation relationships. Nodefeatures correspond to term frequency-inverse document frequency (TF-IDF) representations of thedocuments. FullCoraML is a subset of the full Cora dataset. It retains the same structure as the Cora datasetbut is restricted to documents and citations within the field of machine learning. The node featuresare the bag-of-words representation for the papers."
}