{
  "Abstract": "We introduce an approach to bias deep generative models, such as GANs and diffusionmodels, towards generating data with either enhanced fidelity or increased diversity. Ourapproach involves manipulating the distribution of training and generated data through anovel metric for individual samples, named pseudo density, which is based on the nearest-neighbor information from real samples. Our approach offers three distinct techniques toadjust the fidelity and diversity of deep generative models: 1) Per-sample perturbation,enabling precise adjustments for individual samples towards either more common or moreunique characteristics; 2) Importance sampling during model inference to enhance eitherfidelity or diversity in the generated data; 3) Fine-tuning with importance sampling, whichguides the generative model to learn an adjusted distribution, thus controlling fidelity anddiversity.Furthermore, our fine-tuning method demonstrates the ability to improve theFrechet Inception Distance (FID) for pre-trained generative models with minimal iterations.",
  "Introduction": "The advent of deep generative models has revolutionized the field of image generation. Key developmentsmarked by Variational Autoencoders (Kingma & Welling, 2013), Generative Adversarial Networks (Goodfel-low et al., 2014), and the more emergent diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015), havedemonstrated an unprecedented capability in producing high-quality images. They have been followed byremarkable results in applications such as classification (Xu et al., 2021; Xu & Le, 2022; Xu et al., 2023a;b),super-resolution (Ledig et al., 2017; Li et al., 2022), face generation (Karras et al., 2019; 2020; 2021), andtext-to-image generation (Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022). However, despitethese advancements, the critical challenge of balancing fidelity (the realism of the generated images (Xuet al., 2024)) and diversity (the variety in the generated images) remains unaddressed, despite this abilitybeing essential for the practical applicability of these models. Our work focuses on tackling this crucial needfor improved control mechanisms in generative models.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Images generated by IDDPM trained on LSUN-Bedroom after fine-tuning withimportance sampling. Within each column, images are generated with the same latent code. (Middle):Images generated by the pre-trained model. (Top): Generated by the model fine-tuned for better fidelity.(Bottom): Generated by the model fine-tuned for better diversity. : Images generated by ADM trained on LSUN-Bedroom after fine-tuning with im-portance sampling. Within each column, images are generated with the same latent code. (Middle):Images generated by the pre-trained model. (Top): Generated by the model fine-tuned for better fidelity.(Bottom): Generated by the model fine-tuned for better diversity.",
  ". We propose a per-sample perturbation strategy based on pseudo density that enables adjustment ofrealism and uniqueness for individual samples": "3. We propose importance sampling based on pseudo density during inference, which controls thefidelity and diversity of generative models at inference time, without re-training or fine-tuning themodel. 4. We propose fine-tuning with importance sampling based on pseudo density, which balances thetrade-off between the fidelity and diversity of generative models without additional computationaloverhead during inference. 5. We demonstrate that our fine-tuning method is able to improve the Frechet Inception Distance (FID)of deep generative models by adjusting their precision (fidelity) and recall (diversity) trade-off.",
  "Background and Related Work": "Generative adversarial networks. GANs (Goodfellow et al., 2014) are popular generative models to learnthe underlying distribution of the given training data and generate new samples. They typically consist ofa generator G() parameterized by g that transforms a random vector, called the latent code z, into adata sample, and a discriminator D() parameterized by d that aims to distinguish the real training datafrom the generated samples. Both the generator and discriminator are deep neural networks that can betrained in an end-to-end manner. Therefore, training GANs involves solving a min-max optimization problemby alternatively updating the generator and the discriminator. While the original GANs may suffer fromtraining instability, mode collapse, and poor scalability to high-resolution images, many variants (Arjovskyet al., 2017; Gulrajani et al., 2017; Miyato et al., 2018; Brock et al., 2018; Karras et al., 2018; 2019) havebeen developed to address these problems and enable impressive high-resolution image generation.",
  "Pseudo Density": "Our control approach aims to explicitly manipulate the occurring likelihood of data samples, ideally based ontheir probability density in the real data distribution. However, estimating the density of high-dimensionaldata such as images is not straightforward, primarily due to the complexities introduced by the high dimen-sionality and the fact that image data exist in a low-dimensional manifold. In this section, we introduce aneffective metric, termed pseudo density, for estimating the density of image data. Specifically, we first em-ploy a pre-trained image feature extractor to lower the dimensionality of the real image data while capturingthe semantics. Then, for each real sample, the average distance d to its nearest neighbors is calculated.Based on the average neighbor distances, we can estimate the volume occupied by each sample, and thedensity is inversely proportional to the volume. Finally, we train a light-weight fully-connected networkto fit the estimated density given the extracted feature as input. To compute the pseudo density of any",
  "Higher DensityLower Density": ": Images generated by StyleGAN2 (Top), ProjectedGAN (Middle), and IDDPM (Bot-tom). The training datasets are FFHQ, LSUN-Church, and LSUN-Bedroom, respectively. For each row,the left four images obtained the lowest pseudo density out of 1000 generations, and the right four obtainedthe highest. generated sample, we feed it into the cascade of the feature extractor and the trained network. Note thatthis pseudo density metric differs from the Frchet Inception Distance (FID) (Heusel et al., 2017): Pseudodensity evaluates individual samples while the FID evaluates the overall performance of the generativemodel by calculating the distributional distance between the real and generated images. We illustrate thesteps of computing pseudo density in and provide more details in the following. Estimating the real data distribution. We estimate the real data distribution in a feature space, usingthe extracted features of samples from the dataset. First, the features of all real samples in the dataset can",
  "iki, where": "N is the total number of samples. Note that although image data (as well as their extracted features) arein a high-dimensional space, they reside in a manifold of much lower dimensionality. Hence, in practice, weset n to be a small number (e.g., 1 or 2), preventing the volume V from the curse of dimensionality. Learning a model that predicts the density. To efficiently compute the density for any data point x andenable gradient back-propagation, we train a simple regression network () to fit the density 0, 1, ..., N1with the extracted features of real samples as input F(x0), F(x1), ..., F(xN1). We observed that a basicfully-connected network consisting of only three hidden layers with fewer than one million parameters iscapable of fitting well to the real data while generalizing very effectively. We term the output of thenetwork as pseudo density.Compared to directly computing the density of a generated sample byinjecting it into all real samples, this approach has lower computational overhead and ensures that thegradient can be back-propagated from the metric to features. Compute pseudo density for generated samples.To calculate the pseudo density of a generatedsample x, we sequentially feed it to the image feature extractor and the learned regression model (F(x)).Intuitively, an image with a high density typically exhibits more common characteristics, whereas an imagewith a low density is likely to feature more unique attributes. Moreover, in the context of generated images,the lack of training on low-density data often results in reduced realism for low-density images. In ,we illustrate the effectiveness of pseudo density by showcasing generated images with either high or lowpseudo density. For each model under consideration, we generate 1, 000 images, sort them according to theirpseudo density, and select the ones with the highest and lowest extremes. The results evidence that theproposed metric aligns well with human perception in terms of realism and uniqueness.",
  "Density-Based Perturbation": "While adversarial perturbation was originally used to attack an input image to fool a classification model,their formulation applies to any other type of input to a neural network as long as an objective function isdefined. In this work, we utilize pseudo density as the objective function to adversarially perturb a GANslatent code or the input noise vector of a diffusion model, both of which we refer to as a latent vector zbelow. In the context of image synthesis, such a perturbation optimizes the latent vector within its neighbor-hood such that it leads to a higher-density or lower-density generated image, hence increasing the realismor uniqueness of the generated image without drastically changing the image content. Let us consider arandomly-sampled latent vector z. We explore the neighborhood of z using a perturbation based on thepseudo density. To achieve this, we employ the PGD attack (Madry et al., 2018) strategy to minimize thepseudo density of the features of G(z +) under the constraint , where is the predefined adversar-ial budget. The pseudo-code of our per-sample perturbation method is provided in Algorithm 1. In Madryet al. (2018), the perturbation is applied directly to the image and the magnitude of should be kept small toensure that the changes are imperceptible in the image; in our context, however, the perturbation is appliedto the latent code and the resulted changes in the image are intended to be noticeable. Additionally, determines the magnitude of the pseudo density shift and the extent of the content changes in the generatedimage. A larger yields a large change in pseudo density and also more significant changes in the contentand realism/uniqueness of the generated image.",
  "Density-Based Importance Sampling": "With the help of pseudo density, not only can we perturb individual samples, but also manipulate datadistributions, assigning adjusted probability to samples with different densities. To achieve this, we proceedas follows. For the given dataset, we estimate its data distribution and compute the pseudo density forall samples, as described in , and assign two different importance weights for the samples whosedensities are above and below a pre-defined density threshold, respectively. Intuitively, by assigning higherweights to above-threshold or below-threshold samples for importance sampling, we can either increase ordecrease the proportion of high-density images in the manipulated data distribution. To edit the generated data distribution in a streaming fashion, one can employ rejection sampling (Azadiet al., 2018). Let the original generators output distribution given the random latent vector be p(x), and thedesired distribution be q(x). Furthermore, let M be the upper bound of the ratio q(x)",
  "Rejection sampling proceeds by drawing a sample x from p(x) and only accepting x with probabilityq(x)": "Mp(x).In practice, we formulate the desired distribution q(x) by assigning an importance weight w to the samplesthat have pseudo density higher than a pre-defined threshold , and 1 to the other generated samples. Inthe case where w > 1, which means the high-density samples are highlighted, q(x) can be obtained throughiterative execution of two steps: 1) Generate a sample from the generator; 2) accept this sample if its pseudodensity is higher than the threshold , otherwise, reject it with a probability of 1 1",
  "Inference with Importance Sampling": "A simple yet effective method for controlling the fidelity and diversity of generative models is to employthe above importance sampling strategy to generated images during inference, directly manipulating thegenerated data distribution. To perform inference in a streaming manner, we employ rejection sampling.Different combinations of density threshold and importance weight w result in different effects on thedata distribution, increasing either high-realism samples or high-uniqueness ones. In practice, we set thethreshold to be the {20, 50, 80} percentile of the pseudo density values of all real samples, and the weightw ranges from 0.01 to 100. Intuitively, adopting an importance weight w > 1 leads to more samples witha pseudo density higher than the threshold , hence more samples of better realism, while an importanceweight w < 1 results in more samples with a pseudo density lower than , hence more variety of samplesand better overall diversity. The pseudo-code of this method is provided in Algorithm 2. We demonstrateour experimental results in .",
  ": end whileEnsure: K generated images": "the data distribution at training time. As a result, fine-tuning does not introduce any computational overheadduring inference.Furthermore, it enables the realism/uniqueness adjustment of any generated images,given their latent vectors. Similarly to the inference-time method, fine-tuning with importance samplingrequires the same hyperparameters, the density threshold and the importance weight w, to yield differentcontrolling effects. Before fine-tuning, we compute the pseudo density for every sample in the dataset, andthen determine the density threshold based on the computed densities of all samples. During fine-tuning,training examples are sampled from the dataset using the importance sampling strategy described beforebased on the pre-computed pseudo density.",
  "Ensure: The fine-tuned generator G and discriminator D": "In general, our fine-tuning approach does not modify the training algorithm except for how images aresampled from the data distribution in each iteration. Therefore, it is compatible with any generative modelsincluding GANs and diffusion models. Moreover, it can be further enhanced for certain generative models,particularly GANs, for which importance sampling can be applied not only to the real data distribution",
  "LowerDensity": ": Per-sample perturbation, applied to pre-trained models. Given random latent vectors z andtheir generated images (Middle in each panel), we perturb z to achieve higher pseudo density (Top ineach panel) and lower pseudo density (Bottom in each panel). Groups from left to right, and from topto bottom: StyleGAN2 on FFHQ, ProjectedGAN on LSUN-Bedroom; StyleGAN3-T on FFHQ, IDDPM onLSUN-Bedroom; ProjectedGAN on LSUN-Church, ADM on LSUN-Bedroom. pr(x), but also to the generated data distribution pg(x), which serves as input to the discriminator duringtraining. We provide the pseudo-code for the method for GANs in Algorithm 3. Specifically, when thegenerator and the discriminator reach their equilibrium, we havepr(x)",
  "pr(x)+pg(x) = 1": "2 (Goodfellow et al., 2014),and thus the learned generated data distribution pg(x) = pr(x). In the case where we perform importancesampling to the real data distribution pr(x), we have pg(x) = Pertr(pr(x)) upon convergence, where we usePertr() to represent the perturbation function that maps the original distribution to the sampled one. If wealso apply importance sampling to the generated data distribution during training, the equilibrium becomesPertg(pg(x)) = Pertr(pr(x)). This allows a stronger control over the learned generated data distributionpg(x) after training. A practical choice is to perform importance sampling on pg(x) and pr(x) with reciprocalimportance weights w and 1",
  "w, as in Algorithm 3": "Note that the proposed fine-tuning strategy can also be applied to training from scratch.However, weobserved that applying fine-tuning yields comparable effectiveness but with significantly less computationaloverhead compared to training a model from scratch. Additionally, our fine-tuning strategy is compatible with inference-time techniques, including truncationand the proposed importance sampling strategy detailed in . When inference-time methods areapplied in conjunction with our fine-tuning strategy, a stronger enhancement over fidelity or diversity canbe achieved. Empirical evidence supporting this combined effect is presented in Appendix A.4.",
  "Precision": "ADM, LSUN-Bedroom 256x256 Pre-trainedOurs, = 20%Ours, = 50%Ours, = 80%Ours FrontierTruncation : Precision-recall trade-off. For all methods, the precision and recall metrics compete with eachother. For polarity sampling, we used the values reported in Humayun et al. (2022a) due to replicationdifficulty.For our method, we adopted different density thresholds for each dashed line with varyingimportance weights w ranging from 0.01 to 100. For better visualization, we only report part of the resultsfor polarity sampling and truncation.We visualize the Pareto frontier for polarity sampling and ours,illustrating the optimal trade-offs achieved across various hyperparameter configurations.",
  "Experiments": "In this section, we present the results of our methods on various generative models and datasets. We se-lect Improved Denoising Diffusion Probabilistic Model (IDDPM) (Nichol & Dhariwal, 2021) in additionto Ablated Diffusion Model (ADM) (Dhariwal & Nichol, 2021), StyleGAN2 (Karras et al., 2020), Style-GAN3 (Karras et al., 2021), and ProjectedGAN (Sauer et al., 2021) to demonstrate the generality of ourmethods on different types of generative models. The datasets consist of LSUN-Bedroom (Yu et al., 2015),LSUN-Church (Yu et al., 2015), and FFHQ (Karras et al., 2019). The images from LSUN-Bedroom andLSUN-Church are resized to 256 256, while the FFHQ images are of resolution 1024 1024. For all exper-iments, we use the pre-trained checkpoints provided by the authors and use the same hyper-parameters asin the original papers for fine-tuning. More details regarding the datasets are deferred to the supplementarymaterial, as well as more examples of generated images. In , we showcase examples of images generated by the pre-trained models to illustrate the effectsof our proposed per-sample perturbation strategy, which achieves large density change while preservingcontent of the images. By applying perturbation that increases density, the generated images exhibit greaterrealism, characterized by simplified backgrounds, fewer objects, etc.By contrast, perturbation to lowerdensity leads to images with higher uniqueness, featuring more complex backgrounds, the presence of rarefeatures in the datasets, etc. In AppendixA.2, we consider perturbation that increases the output of aGANs discriminator and demonstrate its inferior performance compared to pseudo density. Additionally,we compare our approach with Discriminator Rejection Sampling (Azadi et al., 2018) and Discriminator",
  "LSUN-Bedroom 256 256FFHQ 1024 1024": "Improved DDPM0.7270.17210.13StyleGAN20.6850.4932.79+ fine-tune (w = 33.0)0.7490.1689.54+ fine-tune (w = 33.0)0.8110.38736.24+ fine-tune (w = 0.03)0.6480.23113.48+ fine-tune (w = 0.03)0.5580.56410.58+ fine-tune (w = 2.0)0.7370.1759.53+ fine-tune (w = 0.5)0.6750.5002.60 ADM0.7150.2906.35StyleGAN3-T0.6580.5382.81+ fine-tune (w = 33.0)0.7290.2847.70+ fine-tune (w = 33.0)0.8240.38260.89+ fine-tune (w = 0.03)0.6270.3438.15+ fine-tune (w = 0.03)0.4820.62422.34+ fine-tune (w = 2.0)0.6970.3025.83+ fine-tune (w = 1.5)0.6570.5352.73",
  "Optimal Transport (Tanaka, 2019), both of which rely on the trained discriminators output in GANs as anindicator for image realism": "In , we demonstrate the trade-off between precision (fidelity) and recall (diversity) achieved bydensity-based importance sampling during inference.Our approach achieves a better overall precision-recall trade-off than the other two inference-time methods, polarity sampling (Humayun et al., 2022a) andtruncation (Karras et al., 2019), especially in the improved-precision area. Its important to note that polaritysampling may face compatibility issues with diffusion models due to the computational complexity of thedenoising process. The truncation method is adaptable to the intermediate W+ space for StyleGAN (Karraset al., 2019; 2020; 2021) models, but is generally limited to the input Z space for others, which could resultin suboptimal performance. For instance, for Improved DDPM, the truncation method struggles aroundthe pre-trained trade-off point. Unlike these methods, neither the computational complexity nor particularmodel architectures restricts the applicability of our method since our approach directly manipulates theoutput data. In , we demonstrate that our fine-tuning approach can improve the precision, recall, or FID acrossvarious datasets and generative models of different types, depending on the choices of hyperparameters,particularly the importance weight w. With larger w > 1, the fine-tuning procedure encourages the modelsto generate higher-density data, and vice versa with w < 1. By fine-tuning with w close to 1, the model canend up at a slightly altered trade-off point that leads to improved FID scores. Note that the density threshold that we use may vary for the same model. We refer the reader to the appendix for more details aboutthe hyperparameters and fine-tuning configurations. In , we further present examples of imagesgenerated by the pre-trained models compared with images produced by their fine-tuned versions to visuallydemonstrate the effects of our fine-tuning method.",
  "Conclusion": "In this study, we have introduced a simple and effective approach for estimating the density of image data,enabling us to devise inference-time methods and a fine-tuning strategy for biasing deep generative modelsinto outputting data with either higher fidelity or higher diversity. We have shown that our method cangreatly improve the fidelity or diversity in the generated data without significantly altering their primarysubject and structure, making it of great interest to applications such as image editing.We have also",
  "< 1": ": Images generated by pre-trained and fine-tuned models. For each column in a panel,the same latent vector is used by the pre-trained model and the fine-tuned versions for image generation.Fine-tuning with w > 1 biases the generative model to output more high-density data. Groups from left toright, and from top to bottom: StyleGAN2 on FFHQ, ProjectedGAN on LSUN-Bedroom; StyleGAN3-T onFFHQ, IDDPM on LSUN-Bedroom; ProjectedGAN on LSUN-Church, ADM on LSUN-Bedroom. demonstrated that adjusting the balance between precision (fidelity) and recall (diversity) through fine-tuning can improve the Frechet Inception Distance (FID) for models pre-trained in a standard manner. Thisunderscores the importance of considering both fidelity and diversity in the evaluation of generative modelsinstead of relying solely on FID as a performance metric. Future research may aim to evaluate and improvevarious density-based sampling strategies for optimized their efficacy. Another potential research interest liesin adapting the proposed control approach to conditional generation tasks such as text-to-image synthesis.",
  "Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neuralinformation processing systems, 34:87808794, 2021": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883,2021. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information ProcessingSystems, 27, 2014.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neuralinformation processing systems, 33:68406851, 2020": "Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk.Polarity sampling: Quality anddiversity control of pre-trained generative networks via singular values. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1064110650, June 2022a. Ahmed Imtiaz Humayun, Randall Balestriero, and Richard Baraniuk. Magnet: Uniform sampling from deepgenerative network manifolds without retraining. In International Conference on Learning Representations,2022b. URL",
  "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improvedquality, stability, and variation. In International Conference on Learning Representations, 2018": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarialnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.44014410, 2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing andimproving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2020. Tero Karras, Miika Aittala, Samuli Laine, Erik Hrknen, Janne Hellsten, Jaakko Lehtinen, and TimoAila. Alias-free generative adversarial networks. Advances in Neural Information Processing Systems, 34:852863, 2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical imagesegmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18thInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241.Springer, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022.",
  "A.1Training Details, Evaluation Details, and Hyper-parameters": "Datasets The FFHQ (Flickr-Faces-HQ) dataset (Karras et al., 2019) is a high-quality collection of humanface images consisting of 70k images with the resolution of 1024 1024. The LSUN-Church and LSUN-Bedroom are subsets of the LSUN (Large-scale Scene Understanding) dataset (Yu et al., 2015). For LSUN-Church, we use all 126k images in the dataset and resize them to 256 256 resolution. For LSUN-Bedroom,we sample the first 200k images from the dataset and apply center crop, resizing them to 256256 resolutionas well. We followed the same pre-processing procedures as in previous works. Training details We follow the same training hyper-parameters published in the original papers, exceptthe number of GPUs, while keeping the same batch size per GPU. For GANs models in our experiments, weused 2 GPUs, and a single GPU for diffusion models. For StyleGAN2/3 models, we fine-tuned for, measuredin thousands of real images fed to the discriminator, 80 thousand images. For Projected GAN, we fine-tunedfor 40 thousand images. To improve FIDs, we only fine-tuned all GANs models for 12 thousand images. Asfor diffusion models in our experiments, we fine-tuned them for 128 20000 = 2560k images.",
  "Regarding the computation of pseudo density, we use n = 1 for all datasets and K = 10 for all exceptLSUN-Bedroom where we set K = 20": "Evaluation details To evaluate the FID, precision, and recall for a generative model, we use the model togenerate 50, 000 images every time and then calculate the metrics against the training dataset. For diffusionmodels, however, we only generate 10, 0000 images every time for computational efficiency. In addition, weuse the uniform stride from DDIM (Song et al., 2020) and each sampling process takes 50 steps. Per-sample perturbation hyper-parameters We employed PGD attack (Madry et al., 2018) whosehyper-parameters involve the number of steps K, the step size , and the adversarial budget . For allGANs models in our experiments, we adopted K = 10, = 0.025, and = 0.1. For diffusion models, weadopted K = 5, = 0.0025, and = 0.0125. Density-based importance sampling hyper-parameters The two relevant hyper-parameters are thedensity threshold and the importance weight w of above-threshold samples. Their optimal values mayvary across different datasets and models.We conducted a sweep for with values from {20, 50, 80}percentiles of the estimated densities of real images. We also performed sweeps for w with values from{0.01, 0.03, 0.1, 10, 33.0, 100}. We show the optimal values found in . When aiming to improve FIDs,we kept a density threshold of 50 percentile and performed importance sampling only on the real data. Weconducted a sweep for w from {0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.05, 1.1, 1.3, 1.5, 1.7, 2.0} for GANs models and only{0.5, 2.0} for diffusion models. : Optimal hyper-parameters for importance sampling. Optimal hyper-parameters vary withdifferent goals, which are improving precision (Precision), improving recall (Recall), and improving FID(FID). In the first two scenarios, the selection was not based on maximizing the precision/recall gain buton the (subjective) optimal trade-off.",
  "A.2Discriminator Output versus Pseudo Density": "Many works, including LOGAN (Wu et al., 2019), Discriminator Rejection Sampling (Azadi et al., 2018),and Discriminator Optimal Transport (Tanaka, 2019), have explored the output of a trained discriminatoras a metric for sample realism. Ideally, discriminator output may indicate sample realism in the contextof Generative Adversarial Networks since the discriminator learns to tell if a sample is real or generated.DRS (Azadi et al., 2018) and DOT (Tanaka, 2019) demonstrated that discriminator output aligns well withrealism in low-dimensional datasets and their methods improve FID for SNGAN (Miyato et al., 2018) andSAGAN (Zhang et al., 2019). However, our experiments suggested that it works badly for higher-resolutionimage data and larger modern GAN architectures such as StyleGAN (Karras et al., 2019; 2020). In ,we showcased the results of per-sample perturbation based on pseudo density and discriminator outputusing StyleGAN2 on FFHQ dataset. Additionally, we compared to random perturbation, which replaces thegradient-descent direction with a random direction while keeping the same magnitude in each step of thePGD attack. We implemented DRS and DOT for StyleGAN2 and followed the original hyperparameters reported in thepapers. As shown in , where we plot the precision and recall for DRS and DOT alongside ourapproach and others, both DRS and DOT achieved performances similar to using the pre-trained modelnaively. Additionally, DRS and DOT obtained FID scores of 2.95 and 3.44, respectively, which are worsethan that of the pretrained. These results indicate that discriminator-based methods like DRS and DOT donot achieve ideal precision-recall trade-off for high-resolution image data and larger, modern GAN models.",
  "A.3Extra Computational Overhead": "The proposed rejection sampling strategies, as described in , Algorithm 2 and Algorithm 3, resultin extra computational cost since only a proportion of the generated samples are accepted.Given theimportance weight w < 1 and let the density threshold be p% of estimated densities of the data, assumingthe generated data perfectly fit the real data, a generated sample is accepted with probability 0.01p + (1 0.01p)w on average, hence the computational cost is approximately1",
  ".01p+(10.01p)w times as before. Thecase where w > 1 can be handled similarly": "In our experiments, generating samples using StyleGAN2 with importance sampling (p = 50, w = 0.01)slowed down from the original 13 sec/kimgs to 26 sec/kimgs while generating with p = 20, w = 0.01 sloweddown to 60 sec/kimgs. In contrast, generating samples with DRS (Azadi et al., 2018) and DOT (Tanaka,2019) slowed down to 23 sec/kimgs and 290 sec/kimgs respectively. We were unable to obtain statisticsfor polarity sampling (Humayun et al., 2022a) due to difficulties in replicating the method, but the authors",
  "reported that preprocessing for StyleGAN2 took a daunting 14 days. In comparison, our method and DOTtook several minutes, while DRS took less than an hour": "On the other hand, our fine-tuning method maintains the original inference efficiency but compromisestraining efficiency. Fine-tuning StyleGAN2 using importance sampling with p = 80, w = 33.0 lowered thespeed from 56 sec/kimgs to 74 sec/kimgs. However, fine-tuning diffusion models did not exhibit a significantslowdown in training speed since it involves importance sampling only on the real data, avoiding the time-consuming rejection sampling.Notably, our fine-tuning process typically requires less than one hour tocomplete due to the small number of iterations needed.",
  "A.4Combination of Fine-tuning and Inference-Time Methods": "Not only the pre-trained models but also the models fine-tuned by our methods can be readily combinedwith any inference-time methods such as our proposed importance sampling strategy and truncation (Karraset al., 2019) to further boost the precision/recall improvement. In , we demonstrate that applyingthem to fine-tuned models can achieve even higher precision/recall than either fine-tuning or solely applyinginference-time methods. : Applying truncation or our importance sampling strategy during inference furtherimproves the precision/recall. P stands for precision and R stands for recall. Bold denotes the bestvalue in each column.",
  "ModelPR": "StyleGAN20.6850.493+ Fine-tune ( = 80%, w = 33.0)0.8110.387+ Truncation ( = 0.7)0.8540.233+ Sampling ( = 90%, w = 100)0.8250.397+ Fine-tune ( = 80%, w = 33.0) + Truncation ( = 0.7)0.9380.163+ Fine-tune ( = 80%, w = 33.0) + Sampling ( = 90%, w = 100)0.8720.198 + Fine-tune ( = 20%, w = 0.03)0.5580.564+ Truncation ( = 1.3)0.4910.571+ Sampling ( = 20%, w = 0.01)0.5480.574+ Fine-tune ( = 20%, w = 0.03) + Truncation ( = 1.3)0.3860.617+ Fine-tune ( = 20%, w = 0.03) + Sampling ( = 20%, w = 0.01)0.4730.597",
  "A.5Autoregressive Generative Models": "Autoregressive generative models, such as PixelCNN (Van den Oord et al., 2016) and VQGAN (Esser et al.,2021), have the unique capability of generating images while providing their exact likelihood in the learneddistribution at the same time.This suggests the potential use of sample likelihood as an indicator ofimage realism, potentially preventing the need for pseudo density computation in the proposed importancesampling strategies. To investigate this possibility, we compared pseudo-density-based and log-likelihood-based importance sampling using the autoregressive VQGAN (Esser et al., 2021). As shown in ,log-likelihood-based sampling yields an inferior precision-recall tradeoff compared to pseudo-density-basedsampling and fails to improve the recall metric even when precision drops below that of the pretrained model.These results indicate that while images with higher likelihood tend to exhibit better realism, those withlower likelihood do not necessarily demonstrate greater uniqueness. We conclude that the likelihood metricprovided by autoregressive models is less effective than pseudo density for controlling fidelity and diversityin generated samples."
}