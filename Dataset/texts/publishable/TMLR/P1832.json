{
  "Abstract": "There has recently been increasing attention towards developing foundational neural PartialDifferential Equation (PDE) solvers and neural operators through large-scale pretraining.However, unlike vision and language models that make use of abundant and inexpensive(unlabeled) data for pretraining, these neural solvers usually rely on simulated PDE data,which can be costly to obtain, especially for high-dimensional PDEs. In this work, we aim toPretrain neural PDE solvers on Lower Dimensional PDEs (PreLowD) where data collectionis the least expensive. We evaluated the effectiveness of this pretraining strategy in similarPDEs in higher dimensions. We use the Factorized Fourier Neural Operator (FFNO) due tohaving the necessary flexibility to be applied to PDE data of arbitrary spatial dimensionsand reuse trained parameters in lower dimensions. In addition, our work sheds light on theeffect of the fine-tuning configuration to make the most of this pretraining strategy. Codeis available at",
  "Introduction": "Many of the recent breakthroughs in artificial intelligence and deep learning are the fruits of large-scalepretrained models in all kinds of applications, ranging from computer vision (Chen et al., 2023), languageprocessing (Devlin et al., 2018), prediction of molecular properties (Wang et al., 2022; Yu et al., 2021),and so on. Pretraining makes use of abundant data to learn useful and generalizable features and patternsthat can be utilized in a downstream task. The effectiveness of using pretraining models is generally moresignificant when the downstream problem is complicated and/or the training data is scarce and expensive tocollect (Chakraborty et al., 2022). In such cases, a simple model is prone to underfitting and a complex oneto overfitting. Using a pretrained model can spare us the learning of basic and fundamental features fromscratch and reduce the mentioned risks in the downstream task. There are several core strategies for pretraining neural networks in different applications depending on thetask and data at hand. For example, a common strategy in many computer vision tasks is to use a pretrainedmodel on the ImageNet dataset with a simple task such as classification (He et al., 2019). Due to the largeamount of data, this strategy has been beneficial in most computer vision tasks (Ridnik et al., 2021). Thiscase of pretraining is done in a supervised learning framework, which requires a large amount of data thatwas expensive to collect but now is available. However, this is not the situation in many cases and that iswhere self-supervised learning comes into play. Self-supervised learning (Liu et al., 2021) is similar to supervised learning with the difference that labels areautomatically or trivially obtained from the data by itself without human input and are therefore inexpensiveand fast to obtain. One of the most common and effective self-supervised pretraining strategies is maskedautoencoding and prediction (Devlin et al., 2018; He et al., 2022; Wei et al., 2022; Song et al., 2020), inwhich the model has to predict missing parts or certain features or targets from a masked, noisy, or modified",
  "Published in Transactions on Machine Learning Research (11/2024)": "more than 2 million trainable parameters. This can be indicative of a hierarchical order of the learningmechanism of the model for the diffusion equation. In the extreme low-data regime (less than 10), tuningconfigurations with less trainable parameters appear to have the most advantage according to the tablesin Appendix A. This makes sense since having fewer trainable parameters reduces the risk of overfitting.However, they fall behind when more training samples are available.",
  "Related works": "With the increasing availability of large PDE datasets and the success of pretraining strategies in classicalapplications, many researchers have been adapting such techniques for neural PDE solvers and neural op-erators. A simple question that might arise is whether it is effective to use a pretrained model trained onone PDE to learn the same PDE with different coefficients or different PDEs. To that point, Subramanianet al. (2024) investigated the transfer of the Fourier neural operator (FNO) to different coefficients, differentPDEs, and how the result may vary with model scale and dataset size. They also tried pretraining themodel on a mixed dataset consisting of several PDEs, which is the main focus of many following works.Some examples of such works include Multiple Physics Pretraining (MPP) by McCabe et al. (2023), UnifiedPDE Solvers (UPS) by Shen et al. (2024), Universal Physics Transformer (UPT) by Alkin et al. (2024),PDEFormer by Ye et al. (2024), and Denoising Pretrained Operator Transformer (DPOT) by Hao et al.(2024). These frameworks focus on designing a network architecture that is versatile enough to learn allPDEs in the training dataset, as well as other necessary techniques such as unifying the representation spaceand balancing the data feed between PDEs during training (Hao et al., 2024). The common characteristicof all these works is that the pretraining task and the downstream task are the same, that is, predicting thenext time-step of the system (time-dependent PDE) or output approximation (boundary value problem). Although not as popular as in classical applications, pretraining via proxy tasks has also been explored forneural PDE solvers, including elaborations on how to transfer and tune the pretrained models. Yang et al.(2023) defined a context-based prompt answering architecture, which they showed to be a few-shot PDElearner. Zhang et al. (2024) deciphered and integrated invariant features of physical systems in time andspace in a contrastive learning framework and used them to fine-tune the first layer of a U-Net or FNO.Lorsung & Farimani (2024) used PDE coefficients to develop a more flexible pretraining approach basedon contrastive learning. Tripura & Chakraborty (2023) selectively fine-tuned certain components of theirdesigned architecture while preserving the pretrained weights of the wavelet-based components. Zhou &Farimani (2024) examined the capability of masked autoencoders as PDE learners, and later investigateda wide range of pretraining strategies for several neural operators and PDEs (Zhou et al., 2024). Mialonet al. (2023) proposed a contrastive learning strategy based on Lie symmetries in PDEs. Chen et al. (2024)applied classical pretraining strategies such as reconstructing blurred or masked inputs for neural PDEsolvers. They used unlabeled data during pretraining, by which they meant the initial conditions in theirdatasets. Therefore, the pretraining stage actually relies on cheap data rather than expensive PDE datasets. Up to this point, virtually all the cited works relied on a large pretraining dataset with a collection costsimilar to that of the downstream task. Some focused on what one might rather call a multitask learningframework, while others looked into different ways to use the same kind of data with strategies to designproxy tasks or use and tune pretrained models. In contrast, we propose a novel pretraining approach to",
  "Neural operators": "Unlike the typical task of neural networks which is to approximate mappings between Euclidean spaces,neural operators approximate mappings between function spaces. An operator G : A U maps an inputfunction a A to an output function u U.In the application of solving PDEs, a can include PDEcoefficients and initial and boundary conditions, and u is the solution of the PDE. In this work, we areconsidering time-dependent PDEs, where the input is the state of a physical system, and the output is thestate at a later time. The operator then learns the mapping ut ut+t. Although variables are defined and processed as functions, a discretized representation is eventually neededfor numerical calculations. A higher resolution and a finer grid typically lead to more accurate results, butwith the downside of higher computational cost and memory consumption, both for traditional numericalsolvers and neural solvers. Typically, the cost increases exponentially with respect to the number of spatialaxes. However, this is not the case for certain neural solvers that factorize the calculations across spatialaxes. Some examples of such models are the Factorized Fourier Neural Operator (FFNO) (Tran et al., 2021),Axial Vision Transformer (AViT) (Ho et al., 2019; Bertasius et al., 2021) used in MPP (McCabe et al., 2023),and FactFormer (Li et al., 2024b;c). The central parameters and calculations of such models are defined perspatial axis, which makes the parameter count and computational cost the sum (rather than the product)of the single-axis amount, leading to a linear cost with respect to the number of spatial axes instead of anexponential cost. Moreover, it is possible to reuse the parameters of a pretrained 1D model when learningto predict a relevant system in higher dimensions. Our quest in this work is to investigate the potential ofa model Pretrained in Lower Dimensions (PreLowDed) to be fine-tuned in higher dimensions. We chooseto explore this strategy with the Factorized Fourier Neural Operator (FFNO) (Tran et al., 2021) becauseof its efficiency due to the fast Fourier transform (FFT) and factorization. Following works can look intocomposition and factorization of other attention-based architectures like Galerkin Transformer (Cao, 2021)and OFormer (Li et al., 2022), as well as using this strategy in a learned encoding space (Hemmasian &Barati Farimani, 2023; Hemmasian & Farimani, 2024; Li et al., 2024a).",
  "&./": ": a) General schematic of a neural operator. b) The nonlinear operator layer in FNO and FFNO. c)The kernel integral operator in FNO. LPF stands for a low pass filter that keeps the first few Fourier modesin each axis and discards the higher frequency modes. d) 1D and 2D factorized kernel integral operator inFFNO. The red arrows show the possible transfer of pretrained parameters from a 1D model to a 2D modelif Mx = My.",
  "Factorized Fourier Neural Operator": "The original Fourier Neural Operator (FNO) was introduced by Li et al. (2020a) in a line of work based onan architecture analogous to typical neural networks. By replacing linear layers in a neural network withlinear operators, Li et al. (2020b) proposed an architecture template shown in figure 1a to construct a neuraloperator through iterative linear and nonlinear transformations of the input:",
  "u = G(a) =Q L(L) L(1) P(a),(1)": "where is function composition, L is the number of layers, P is the input projector from the physical spaceto the latent space, L(l) is the lth non-linear operator layer, and Q is the output projector from the latentspace back to the physical space. A nonlinear operator layer of FNO executes the following transformationson the latent representation:K(l)z(l1)= IFFTR(l) FFT(z(l1))(2)",
  "z(l) = L(l)z(l1)= W (l)z(l1) + b(l) + K(l)(z(l1))(3)": "where K(l) is a kernel integral operator that executes a convolution integral via matrix multiplication in theFourier space, R(l)dis the Fourier weight matrix, : R R is a point-wise nonlinear activation function,and W (l)z(l1) + b(l) is an affine point-wise map in physical space. You can see a simple illustration of theFNO layer on the left in figure 1b, and a 2D kernel integral operator block in figure 1c. Assuming the samenumber of Fourier modes in each axis (Mx = My = M), each layer has a distinct or shared complex weightmatrix R(l) containing H2M D complex parameters, where H represents the hidden or latent dimension (alsoknown as width) and D is the number of spatial axes.",
  "z(l) = L(l)z(l1)= z(l1) + W (l)2 W (l)1 K(l)Fz(l1)+ b(l)1+ b(l)2(5)": "Here, KF is a factorized kernel integral operator, R(l)dis the Fourier weight matrix of axis d, and , W, b rep-resent the activation, weights, and biases of the feedforward layers respectively. A simple visualization of theFFNO layer and its factorized kernel integral operator are provided in figure 1b and 1d respectively. FFNOfully preserves the residual connection throughout the layers to keep the original input and its informationas much as possible, and applies the nonlinearity to the output of the kernel integral instead. It also usesa two-layer feedforward layer instead of the single-layer one in FNO, which is inspired by the transformerarchitecture (Vaswani et al., 2017). The central feature of FFNO is the factorized kernel integral operatorwhose cost and parameters count in each dimension is O(H2M), summing up to O(H2MD). Not only does the factorized architecture save a lot of computational cost and memory usage when storing,training, and testing the model, but it also allows weights to be of the same shape across different axesif the number of modes is the same. This opens up the possibility of transferring weights across differentproblems with an arbitrary number of axes. In this work, we evaluate the effectiveness of this strategy onthe advection and diffusion equations, two simple but fundamental PDEs.",
  "Methodology": "Before the formal definition of how to prelowd a neural PDE solver or operator, a couple of requirementsmust be met. First, the architecture of the neural operator has to be compartmentalized per spatial axis,which is usually indicated by it being called an axial or factorized neural solver or operator. This enablesthe generalization and recycling of pretrained parameters of an axial module to other axes in the high-dimensional PDE of the downstream task. The next requirement is the relevance of the pretraining PDE,",
  "Algorithm 1 PreLowDing a neural PDE solver": "Input: a neural PDE solver and a target (downstream) PDE dataset to learnStep 1: Verify the neural solver has a factorized/axial architecture.Step 2: Choose a relevant low-dimensional (1D) PDE for pretraining. A relevant PDE would consist ofsimilar terms and similar physical phenomena (advection, diffusion, etc) in the lower dimensions. Constructa training dataset from the chosen PDE(s).Step 3: Train the neural operator on the pretraining dataset. This step is where the pretraining is executed.Step 4: After training, replicate the axial module(s) of the model to the number of dimensions in thedownstream (target) PDE.Step 4.5: Apply any appropriate modification before the neural solver is trained (fine-tuned) on thedownstream dataset. An example of such modification is freezing certain components not to be changedin the fine-tuning stage.Output: The prelowded model ready to be trained (fine-tuned) on the downstream PDE Moving on, we will continue the framework focusing on FFNO as our choice of neural PDE solver. As ex-plained previously and illustrated in figure 1d, a 1D FFNO and a 2D FFNO can share all their correspondingparameters if the number of preserved Fourier modes is the same in x and y. The projection layers and thefeedforward layers are point-wise transformations in the physical space, and all Fourier weights are definedfor a single axis. After transferring the pretrained parameters, we explore different configurations to freeze orfine-tune them for the downstream task. In classical applications like computer vision, the typical strategyis to freeze the weights of the initial layers (also known as the stem) and only tune the parameters of thefinal layer(s). This is justified due to the hierarchical nature of the architecture and the learned features.Moreover, tuning the model is cheaper since the backpropagation is not executed all the way back to theinput, and the sheer number of trainable parameters is also decreased by freezing the stem of the model.In addition, the scarcity of training data in the downstream task introduces the risk of overfitting, whichcan be mitigated by this strategy due to the low number of trainable parameters. Keeping this in mind, wehave to decide how to choose the best way to perform the fine-tuning considering the architecture of neuraloperators and PDE data. The first difference that we pay attention to is the absence of a hierarchical architecture and feature extractionin models like FNO and FFNO. Unlike convolutional neural networks that have a shrinking architecturetowards the output, these neural operators maintain the same resolution and stay in roughly the same latentspace throughout their hidden layers. Therefore, the classic fine-tuning strategies of classic applications maynot be the best choice here. However, these models consist of several types of components, each of whichmay learn certain properties and features of the system. For example, the parameters of FFNO belong toeither projectors, kernel integral operator layers, or feedforward layers. We can choose to fine-tune only asubset of the models components and see how the tuned model performs in the downstream task. This canalso pave the path toward the interpretability of such models by breaking down the information learned byeach component. In addition to choosing the tunable subset of the model based on component type, we also consider fine-tuningbased on the chronological order of the layers. This includes not only the classical approach of fine-tuningthe last layer(s), but also fine-tuning the first layer(s) or a combination of both. If we want to try all thepossible configurations, the number of experiments exponentially increases, since we may choose to tune ornot tune each component of each layer. Therefore, we narrow our options down to eight configurations asshown in table 1. We will represent the model without pretraining as C0. The projector layers P, Q areleft to be tuned in all cases. The configurations C2 to C8 were selected so the tunable modules are eitherin proximity to the models input or output , or are of the same type (Fourier or feedforward). While C1",
  "W (L), b(L)": "We will evaluate the performance of the models by comparing the next-step prediction error and the averageerror over a 5-step autoregressive rollout. We also conduct experiments with different numbers of trainingsamples in the downstream task. As we shall see, the advantage of pretraining depends on the availabilityof training data in the downstream task.",
  "Datasets and objectives": "The general format and domains of the PDEs in this work are shown in equation 6, where N is a linearor nonlinear function, c is the vector of the PDE coefficients, u is the quantity of interest and t, x are thetime and space variables, respectively. Each sample of a dataset consists of the evolution of the system inthe specified time domain sampled every t = 0.05, resulting in 21 snapshots and 20 input-output pairs onwhich the model will be trained. The spatial resolutions for the 1D and 2D datasets are set to 1024 and642, respectively. Different samples of each dataset are governed by the same equation and coefficients, anddiffer only in the initial condition defined by equation 7 where Ai, ni, i are random variables drawn fromuniform distributions, similar to the multi-dimensional datasets in PDEBench (Takamoto et al., 2022). Weassume periodic boundary conditions in all cases.",
  "i=1Aisin(2nix + i)(7)": "For each dimensionality and each value of c, 10000 samples were generated, 2000 of which are held forvalidation. Since the characteristics of a typical scenario for pretraining a model are the high collection costand scarcity of data, we set up the experiments to resemble such scenarios. For each case (a particular PDEwith a particular value for the coefficient), we first pretrain the model on the relevant 1D pretraining dataset.Then, we fine-tune the pretrained model with different amounts of available downstream training data. Thatis to find out the relationship between downstream dataset size and the gain of pretraining compared totraining from scratch. To clarify, the experiment for each PDE and each value for the coefficient(s) of thePDE are separate and independent of each other. For example, if the downstream task is to learn 2D diffusionwith a diffusion coefficient of = 0.002, the pretraining dataset consists only of samples from 1D diffusionequation with the same diffusion coefficient of = 0.002. For the downstream task, we compare a model trained from random initialization and 8 pretrained modelswith fine-tuning configurations shown in table 1. The objective of both training and validation is the relativeL2 norm, also known as nRMSE (Takamoto et al., 2022), of the prediction error shown in equation 8 wherey, y are the model output and target value for the quantity of interest, respectively. In our application, yis the state of the system at the next time-step. We train the model on the next-step prediction loss andevaluate the model using the same metric on the validation dataset, as well as the average loss over a 5-step",
  "||y||2(8)": "Each training stage consists of 5000 iterations of the AdamW optimizer with an initial learning rate of 0.001.The learning rate is multiplied by 0.2 when the loss reaches a plateau and does not improve for more than100 iterations. Our choice of architecture for FFNO has 4 hidden layers with a latent dimension of 128and 16 Fourier modes in each axis.On our GeForce RTX 2080 Ti Nvidia GPUs, with a batch size of 64 and5000 optimization iterations for each stage, the pretraining takes about 3.35 minutes while the fine-tuningor training for the 2D task takes about 21 minutes. This is an example of how the training cost differs for a2D PDE compared to 1D. This is despite the resolution in 2D being 64 while the 1D data has a resolutionof 1024.",
  "ut = 2u, {0.001, 0.002, 0.004}(9)": "The results for the diffusion datasets are shown in figure 2. The x-axis represents the number of downstreamtraining samples, and the y-axis represents the average loss on the 2000 validation samples. We exclude theinstances trained with only 1 or 2 training samples due to their unacceptable amount of error. It appears thatthe fine-tuning configurations C1, C2, and C8 perform as well or better than the randomly initialized modelC0, and the remaining configurations fail to achieve the same error as C0. Focusing on C1 (all parameterstrainable) and C0, the first trivial indicator of success is that C1 outperforms C0 and reduces the error upto 80% in the low-data regime. The second interesting observation is the different trend of the error withrespect to the number of training parameters. The error of C0 starts from a very high value in the left sidewith very few samples, and improves significantly with the increase of the training dataset size. However,the performance of C1 even with very few samples on the left is already comparable to the performance onthe right end of the plot. An interesting observation is that C1 trained on 8 samples outperforms C0 trainedon 1024 samples for = 0.004. There we can also see more clearly that the advantage of pretraining isamplified over autoregressive rollout. For exact values, refer to Appendix A. The second trend that we discuss is how the advantage varies for different coefficient values of the PDE.In the diffusion equation, the larger coefficient means a more rapidly changing system. It can be seen infigure 2 that for a larger dataset size, the differences between C1 and C0 are relatively small for = 0.001.However, the gap becomes more significant as we increase the diffusion coefficient. For the 5-step rollout forv = 0.004, C1 still achieves a 40-50% error reduction compared to C0. Statistically speaking, the distributionof the system state remains relatively more familiar over time if the model changes less rapidly. Therefore,more unfamiliar samples and a different state distribution are presented to the model during validation ifthe system has a rapid rate of change, further increasing the risk of overfitting and poor performance on theunseen validation data. However, the pretrained models have already seen a large amount of data, hencemitigating this risk to some extent. Focusing on the fine-tuning configurations, C1, C2, and C8 are the models that perform best. The explanationis trivial for C1, since it is totally free to be tuned on the downstream data. However, it is interesting thatC8 with only the last layer left trainable outperforms C2 (all Fourier layers trainable) despite having fewertrainable parameters. Each Fourier and feedforward layer have about 524K and 60K parameters, respectively.This means that C8 with less than 600K trainable parameters performs as well as or better than C2 with",
  "ut = u, {0.1, 0.4, 1.0}(10)": "For the advection equations, the strategy does not seem to be as successful as the diffusion equations. Basedon the results provided in figure 3, the difference between the best pretrained models and C0 is significantonly to the far left of the plot with very few samples. The two fine-tuning configurations that seem to beon par with C0 are C1 and C2, which are the ones with the most number of trainable parameters. Unlikethe case for diffusion equations, C8 performs poorly, falling into the second last model for = 1.0, whichcan indicate a fundamental difference in how the model extracts features and learns to solve the advectionequation compared to the diffusion equation. With an increase of , the rate of change increases, and theprediction task becomes more difficult. Unlike the diffusion equations where pretraining showed even morebenefit over autoregressive rollout, there does not seem to be any significant difference for the advectionequations.",
  "Conclusion": "This work proposes a novel strategy to reduce the cost of data collection, training, and error of a neuraloperator in multidimensional PDEs. For a neural operator with a factorized architecture like FFNO, wecan pretrain a model in lower dimensions (PreLowD) and transfer the weights to a multidimensional model.We showed that this strategy is successful for the 2D diffusion equation, but not for the advection equation.When successful, the PreLowDed model outperformed the randomly initialized model by up to 80% in low-data regimes and a slow-changing system. When the system has a faster rate of change or the model is usedautoregressively, the gain of the PreLowDed model is also very significant, achieving an error reduction of50% over a 5-step rollout even with 1024 training samples. We explored several fine-tuning strategies to find the best way to freeze or tune the transferred weights froma PreLowDed model. It seems that with sufficient training, more trainable parameters will result in a lowererror. However, tuning a smaller subset of the parameters may achieve a lower error when very few trainingsamples are available. For the advection equation, the only important factor seemed to be the sheer numberof tunable parameters. However, tuning the final full layer for the diffusion datasets was among the bestchoices, indicating a possibly hierarchical feature extraction of the model. Our general recommendation is tofine-tune the final layer in case of extreme scarcity of training data in the downstream task, or to fine-tunethe whole model with all parameters being trainable. In future work, this pretraining strategy may be utilized with other neural operators and PDE solvers thathave the necessary properties as well. However, it may not be straightforward to find or define a similarphysical system with fewer number of dimensions for pretraining. Moreover, the selective fine-tuning strategycan be applied to any neural operator and can provide insight into the generalizability and interpretability ofsuch models. For example, tuning different parts of the model while varying a certain coefficient or term forthe PDE of the downstream task can help discover the correspondence of the neural operators componentsand different modalities and terms of the PDE. Finally, an important potential venue for research is the studyof different possible compositions of low-dimensional models and factorization of high-dimensional models.The ultimate goal of this direction is to improve the prediction accuracy and efficiency in more complex or3D systems, and this work was just a humble starting step towards that end.",
  "Zijie Li, Anthony Zhou, Saurabh Patil, and Amir Barati Farimani. Cafa: Global weather forecasting withfactorized attention on sphere. arXiv preprint arXiv:2405.07395, 2024c": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXivpreprint arXiv:2010.08895, 2020a. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXivpreprint arXiv:2003.03485, 2020b.",
  "Cooper Lorsung and Amir Barati Farimani. Picl: Physics informed contrastive learning for partial differentialequations. arXiv preprint arXiv:2401.16327, 2024": "Michael McCabe, Bruno Rgaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Al-berto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, et al. Multiplephysics pretraining for physical surrogate models. arXiv preprint arXiv:2310.02994, 2023. Grgoire Mialon, Quentin Garrido, Hannah Lawrence, Danyal Rehman, Yann LeCun, and Bobak Kiani. Self-supervised learning with lie symmetries for partial differential equations. Advances in Neural InformationProcessing Systems, 36:2897329004, 2023. Subhadarshi Panda, Anjali Agrawal, Jeewon Ha, and Benjamin Bloch. Shuffled-token detection for refin-ing pre-trained roberta. In Proceedings of the 2021 Conference of the North American Chapter of theAssociation for Computational Linguistics: Student Research Workshop, pp. 8893, 2021.",
  "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-trainingfor language understanding. Advances in neural information processing systems, 33:1685716867, 2020": "Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael WMahoney, and Amir Gholami. Towards foundation models for scientific machine learning: Characterizingscaling and transfer behavior. Advances in Neural Information Processing Systems, 36, 2024. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pflger,and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances inNeural Information Processing Systems, 35:15961611, 2022.",
  "Tapas Tripura and Souvik Chakraborty. A foundational neural operator that continuously learns withoutforgetting. arXiv preprint arXiv:2310.18885, 2023": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei, Yaowei Wang, YonghongTian, and Wen Gao. Large-scale multi-modal pre-trained models: A comprehensive survey. MachineIntelligence Research, 20(4):447482, 2023.",
  "Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning ofrepresentations via graph neural networks. Nature Machine Intelligence, 4(3):279287, 2022": "Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer.Maskedfeature prediction for self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1466814678, 2022. Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang Tang. A brief overviewof chatgpt: The history, status quo and potential future development. IEEE/CAA Journal of AutomaticaSinica, 10(5):11221136, 2023. Liu Yang, Siting Liu, Tingwei Meng, and Stanley J Osher. In-context operator learning with data promptsfor differential equation problems. Proceedings of the National Academy of Sciences, 120(39):e2310142120,2023. Zhanhong Ye, Xiang Huang, Leheng Chen, Hongsheng Liu, Zidong Wang, and Bin Dong.Pde-former: Towards a foundation model for one-dimensional partial differential equations. arXiv preprintarXiv:2402.12652, 2024.",
  "AAppendix": "All the results are presented in detail in this section. The reported metrics are the next-step prediction errorand the average error over the next five autoregressive rollout steps of the model. C0 is the model trainedfrom a random initialization and the rest are pretrained models with different fine-tuning configurations asspecified in table 1."
}