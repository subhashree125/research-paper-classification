{
  "Abstract": "The Gromov-Wasserstein (GW) distance is frequently used in machine learning to comparedistributions across distinct metric spaces. Despite its utility, it remains computationallyintensive, especially for large-scale problems. Recently, a novel Wasserstein distance specif-ically tailored for Gaussian mixture models (written GMMs in the paper for the sake ofbrevity) and known as MW2 (mixture Wasserstein) has been introduced by several authors.In scenarios where data exhibit clustering, this approach simplifies to a small-scale discreteoptimal transport problem, which complexity depends solely on the number of Gaussiancomponents in the GMMs. This paper aims to incorporate invariance properties into MW2.This is done by introducing new Gromov-type distances, designed to be isometry-invariantin Euclidean spaces and applicable for comparing GMMs across different dimensional spaces.Our first contribution is the Mixture Gromov Wasserstein distance (MGW2), which can beviewed as a Gromovized version of MW2. This new distance has a straightforward discreteformulation, making it highly efficient for estimating distances between GMMs in practicalapplications. To facilitate the derivation of a transport plan between GMMs, we present asecond distance, the Embedded Wasserstein distance (EW2). This distance turns out to beclosely related to several recent alternatives to Gromov-Wasserstein. We show that EW2can be adapted to derive a distance as well as optimal transportation plans between GMMs.We demonstrate the efficiency of these newly proposed distances on medium to large-scaleproblems, including shape matching and hyperspectral image color transfer.",
  "Introduction": "The goal of optimal transport (OT) theory is to design meaningful ways to compare probability distributions.It provides very useful mathematical tools for diverse imaging sciences and machine learning tasks includinggenerative modeling (Arjovsky et al., 2017; Genevay et al., 2018; Tolstikhin et al., 2018), domain adaptation(Courty et al., 2016), image processing (Rabin et al., 2012; 2014), and embedding learning (Courty et al.,2018; Xu et al., 2018). For two probability distributions and , respectively on two Polish (i.e complete,separable, metrizable) spaces X and Y, and given a lower semi-continuous function c: X Y R+ calledcost, optimal transport in its most classic form aims at solving the following optimization problem,",
  "Published in Transactions on Machine Learning Research (09/2024)": "Figure C1: Link between PW2 and EW2 for two distributions and respectively on R2 and R. In PW2, is projected into R by a mapping of the form x P T (x b). In EW2, is transformed into a degeneratemeasure (lying on the purple line) on R2 with an isometric mapping of the form y Py + b.",
  "(a, b) = { mn : 1n = a and T 1m = b} and C is a non-negative matrix of size m n, called costmatrix": "Optimal transport is known to be computationally challenging. Between discrete distributions, its computationinvolves solving a linear program that rapidly becomes costly as soon as the number of points is moderatelylarge. Between two sets of n points, its computation complexity is O(n3log(n)) (Seguy et al., 2017), whichcompromises its usability for settings with more than a few tens of thousand of points. To lighten OTcomputational cost, a large number of works have developped efficient computational tools. In particular,Cuturi (2013) proposes to solve an entropic regularized OT problem using the Sinkhorn-Knopp algorithm(Sinkhorn and Knopp, 1967), reducing the cost of the problem to O(n2). Over the last past years, a large bodyof works have focused on speeding up the Sinkhorn-Knopp algorithm, building mostly on diverse low-rankapproximations (Solomon et al., 2015; Altschuler et al., 2018; 2019; Forrow et al., 2019; Scetbon and Cuturi,2020; Scetbon et al., 2021). These approaches have helped to reduce the computational cost of the problemfrom cubic (for the non-regularized problem) to linear complexity. Another type of commonly used solvers arebuilding on sliced mechanisms (Rabin et al., 2012; Kolouri et al., 2019). These solvers average Wassersteindistances between several one dimensional projections of the high-dimensional distributions, leveraging the factthat the OT problem between one-dimentional distributions can be solved using a simple sorting algorithm.Alternatively, Delon and Desolneux (2020) have proposed an OT distance between Gaussian mixture models(GMM), called Mixture Wasserstein (MW), where the admissible couplings are themselves constrained tobe GMMs. They demonstrated that this specific continuous OT problem could be equivalently reformulatedinto a discrete version (which had been also proposed independently by Chen et al. (2018)): for two GMMwith respectively K0 and K1 components, solving this formulation boils down to solve a small scale K0 K1discrete OT problem. This distance can be applied to real data by first fitting GMMs on each distribution,making it particularly suited for scenarios where a clustering structure already exists in the data. The mainadvantage of this approach is that its computational cost arises almost exclusively from fitting the GMMs tothe data, since the complexity of the composite OT problem depends neither on the dimension nor on thenumber of points, but solely on the number of components in the GMMs. This approach offers a scalable andcomputationally efficient OT distance, which has been used for instance for texture synthesis (Leclaire et al.,2023), evaluating generative models (Luzi et al., 2023), Gaussian Mixture reduction (Zhang and Chen, 2020)or approximate Bayesian computation (Forbes et al., 2021). One weakness of the classical optimal transport approach lies in the fact that it implicitly assumes that thespaces X and Y are comparable, i.e. that there exists a relevant cost function c: X Y R+ to compare them.Yet, this assumption is not always verified. For instance, if X = Rd and Y = Rd with d = d, the definitionof a meaningful cost function c: Rd Rd R+ is not straightforward. Furthermore, some applicationssuch as shape matching require having an OT distance that is invariant to a given family of transformations,such as translations or rotations, or more generally to isometries2. Even if the two distributions involvedin these applications do live in the same ground space, it is not straightforward to design a cost functionsuch that the resulting OT distance will be invariant to these families of transformations. To overcome thoselimitations, several non-convex variants of Problem (1) have been proposed (Cohen and Guibasm, 1999; Peleand Taskar, 2013; Alvarez-Melis et al., 2019; Cai and Lim, 2022). Among these, the Gromov-Wasserstein(GW) (Mmoli, 2011) distance is perhaps the most frequently utilized, recently gaining significant attentionfor the versatility it provides. Indeed, it only requires modeling topological aspects of the distributions withineach domain to compare them without having to specify first a subset of invariances nor to design a relevantcost function between the spaces the distributions lie on. The GW problem between two measures and",
  "XY|cX (x, x) cY(y, y)|pd(x, y)d(x, y),": "where cX : X X R and cY : Y Y R are two cost functions. Since this optimization problem onlyrequires to define cost functions in each respective space, it remains very versatile and can be defined withvery little assumptions on the spaces X and Y . This approach has been applied to shape matching (Mmoli,2009), or more generally to correspondence problems (Solomon et al., 2016), word embedding (Alvarez-Melisand Jaakkola, 2018), graph classification (Vayer et al., 2019a), graph prediction (Brogat-Motte et al., 2022),and generative modeling (Bunne et al., 2019). Computationally speaking, the Gromov-Wasserstein problem is known to be much more costly to solve thanthe classic linear OT problem. Indeed, the problem is non convex, quadratic with respect to and knownto be NP-hard. One possible approach to solve GW consists in linearizing the cost and to solve iterativelyseveral classic OT problems. Entropic regularization of GW has also been proposed in (Peyr et al., 2016;Solomon et al., 2016) and results in a still non convex problem which can be solved by a projected gradientalgorithm, where each projection is itself an entropic linear optimal transport problem. In recent years,several practical approximations of GW have been proposed in the literature to reduce its computationalcomplexity and solve it efficiently, either through quantization of input measures (Chowdhury et al., 2021),recursive clustering approches (Xu et al., 2019; Blumberg et al., 2020), or using a minibatch scheme (Fatraset al., 2021). Specifically to the Euclidean setting, Vayer et al. (2019b) has introduced a solver buiding on asliced mechanism, and leveraging the observation that the GW problem seems most of the time easy to solvebetween one-dimensional distributions. More recently, Scetbon et al. (2022) have shown that the low-rankapproximations used to speed-up the Sinkhorn-Knopp algorithm were particularly suited for the regularizedGW problem, resulting in a much more computationally efficient solver. In this work, we propose to buildon the ideas of Delon and Desolneux (2020) in order to construct OT distances between GMMs that areinvariant to isometries and that stay relevant between GMMs of different dimensions. These distances sharesimilarities with the one defined in (Chowdhury et al., 2021), since they rely on a form of quantization of theoriginal data through the GMM representation. One of these distances is a Gromovization of the MixtureWasserstein distance, that we call MGW. We will see that the structured representation of MGW makes itvery robust in practice, and permits to design an efficient and scalable solver using a fixed small numberof Gaussian components, while keeping competitive precision and running times (when compared to thestate-of-the-art methods described above) when the number of points of the underlying data increases. Contributions of the paper.In this paper, we introduce two Gromov-Wasserstein type OT distancesbetween GMMs that are designed to be invariant (at least) to isometries. More precisely, we introduce in a natural Gromov version of the distance introduced by Chen et al. (2018) and Delon and Desolneux(2020), that we call MGW for Mixture Gromov Wasserstein. This distance can be used for applicationswhich only require to evaluate how far the distributions are from each other, without having to identifycorrespondences between points. However, this formulation does not directly allow to derive an optimaltransportation plan between the points. To design a way to define such a transportation plan, we define in another distance that we call EW for Embedded Wasserstein. This latter turns out to be closelyrelated to the Gromov-Wasserstein distance and coincides with the OT distance introduced by Alvarez-Meliset al. (2019). We show that EW can be adapted to derive a distance and optimal transportation plansbetween GMMs and we then define a heuristic transportation plan for MGW by analogy with EW. Finally,in , we illustrate the pratical use of our distances on medium-to-large scale problems such as shapematching and hyperspectral image color transfer and we compare the performance of our methods withother recent GW based approaches, both on assessing distances between clouds on points and drawingcorrespondences between points. All the proofs are postponed to the appendix.",
  "M stands for the nuclear norm of a matrix M, i.e. M = tr((M T M)12 )": "The notation (M) denotes the vector of singular values of the matrix M. Idd is the identity matrix of size d d. For any x Rd, diag(x) denotes the matrix of size d d with diagonal vector x. Id stands for any matrix of size d d of the form diag((1)1id) Suppose d d. For any matrix M of size d d, we denote by M (d) the submatrix of size d d",
  "1d,d = (1)1id1jddenotes the matrix of ones with d rows and d columns": "The notation X means that X is a random variable with probability distribution . If is a positive measure on X and : X Y is a mapping, # stands for the push-forward measureof by , i.e. the measure on Y such that for any measurable set A of Y, #(A) = (1(A)).",
  "Background : Mixture-Wasserstein and Gromov-Wasserstein-type distances": "We recall in this section the definitions and some important properties of the different OT distances usedthroughout the paper. For any Polish space X, we write P(X) the set probability measures on X. For d 1and p 1, the Wasserstein space Wp(Rd) is defined as the set of probability measures on Rd with finitemoment of order p, i.e. such that",
  "Mixture-Wasserstein distance between GMMs": "We present here the distance introduced in Delon and Desolneux (2020), as well as some results that will beuseful in the rest of the paper. We denote GMMK(Rd) the set of Gaussian mixtures on Rd with less than Kcomponents, i.e. the set of measures in P(Rd) which can be written",
  "K0GMMK(Rd)": "Note that the condition that the Gaussian components are pairwise distinct ensures the identifiability ofthe elements of GMM(Rd) (Yakowitz and Spragins, 1968), in the sense that two GMMs = Kk akkand = Ll bll are equal if and only if K = L, and we can reorder the indices such that for all k, ak = bkand k = k. It can been shown that GMM(Rd) is dense in Wp(Rd) for the metric Wp, meaning that anymeasure in Wp(Rd) can be approximated with any precision for the distance Wp by a finite Gaussian mixturedistribution. Let GMMK(Rd) and GMML(Rd). The Mixture-Wasserstein distance of order 2 isdefined as",
  ".(MW2)": "As for W2 with W2(Rd), MW2 defines a metric on GMM(Rd) (Delon and Desolneux, 2020). In general,the transportation plan solution of the W2 problem is not a Gaussian mixture, thus by restricting the set ofadmissible couplings, we most of the time have MW2(, ) > W2(, ). It can be shown that the differencebetween MW2(, ) and W2(, ) is upper-bounded by a term that only depends on the weights and thecovariances matrices of the components of the two mixtures. An important property of MW2 is that it canbe written in an equivalent form, which had already been introduced in Chen et al. (2018): if = Kk akkand = Ll bll, thenMW 22 (, ) =inf(a,b)",
  "k,lk,lW 22 (k, l) ,(2)": "where a = (a1, . . . , aK)T , b = (b1, . . . , bL)T . From a computational point of view, this latter formulationreduces the problem to a simple small-scale discrete optimal transport problem since the W2 distance betweenGaussian distributions has a closed form: indeed, recall that if k = N(mk, k) and l = N(ml, l), then",
  "p,(5)": "with p 1. The fundamental metric properties of GWp have been studied in depth in (Mmoli, 2011; Sturm,2012; Chowdhury and Mmoli, 2019). When cX and cY are powers of the metrics dX and dY of the basespaces X and Y, GWp induces a metric over the space of metric measure spaces (i.e. the triplets (X, dX , ))quotiented by the strong isomorphisms (Sturm, 2012), where one says that two metric measure spaces(X, dX , ) and (Y, dY, ) are strongly isomorphic if there exists an isometric bijection : supp() supp()that transports into . When cX and cY are not powers of the metrics of the base spaces, GWp still defines",
  "XX cpX (x, x)d(x)d(x) < +}": "Note that when X and Y are fixed as well as cX and cY, it is natural to see GWp as a distance betweenthe two measures and rather than a distance between the two network measure spaces (X, cX , ) and(Y, cY, ). Therefore, we will denote in that case - with a slight abuse of notations - GWp(, ) instead ofGWp((X, cX , ), (Y, cY, )). Finally, in the discrete setting, given a = (a1, . . . , am)T and b = (b1, . . . , bn)T",
  "Dp((X, dX , ), (Y, dY, )) = infZ,, Wp(#, #) ,(6)": "where (X, dX , ) and (Y, dY, ) are two metric measure spaces as defined in .2, Z is a third Polishspace, and where : X Z and : Y Z are two isometric mappings. More recently, Alvarez-Melis et al.(2019) have introduced another family of invariant OT distances in the Euclidean setting which can also beused to compare distributions on spaces of different dimensions. Initially, Alvarez-Melis et al. (2019) haveintroduced this OT distance in the setting where and are both living in the same Euclidean space Rd.Yet, it generalizes well to settings where and are living in spaces of different dimensions. Between twomeasures and on Rd and Rd, this reads as",
  ",(IW2)": "where H is a class of mappings from Rd to Rd encoding the invariance. This is a non-convex optimizationproblem in and h that becomes convex in if h is fixed and becomes also convex in h if is fixed andH is a convex set. When d is equal to d and both measures are centered, Alvarez-Melis et al. (2019) havenotably shown that when is such that EY [Y Y T ] = Idd and when H = H1 := {P Rdd : PF",
  "RdRd xyT d(x, y),(-COV)": "where for any matrix A of size d d, A is the nuclear norm of A, i.e. A = tr((AT A)12 ). Note thatboth Problems (F-COV) and (-COV) are non-convex. These results have been shown by Alvarez-Melis et al.(2019) in the case where and are discrete but can easily be extended to continuous distributions. Observethat problem (-COV) consists in maximizing the sum of the singular values of the cross-covariance matrixxyT d(x, y), whereas the Problem (F-COV) consists in maximizing the sum of the squared singular valuesof the cross-covariance matrix. In general, these two problems are not equivalent despite being structurallysimilar, as the example of illustrates it.",
  "Gromov-Wasserstein distance between mixture of Gaussians": "In this section, we define a Gromov-Wasserstein type distance between Gaussian mixture distributions. Thisdistance is a natural \"Gromovization\" of Problem (2). Indeed, as it has already been observed in the literature(Chen et al., 2018; Lambert et al., 2022), any Gaussian mixture in dimension d can be identified with aprobability distribution on Rd Sd+, i.e. the product space of means and covariance matrices. Equivalently, afinite Gaussian mixture can be seen as a discrete probability distribution on the space of Gaussian distributionsN(Rd)3, which has been proven to be a complete metric space when endowed with W2 (Takatsu, 2010) and isfurthermore separable since it is a subspace of W2(Rd) which is itself a separable metric space when endowedwith W2 (Bolley, 2008). Since the theory of optimal transport still applies on measures over non-Euclideanspaces (Villani, 2008), it follows that Problem (2) can formally be thought as a simple OT problem betweentwo discrete measures in P(N(Rd)). Thus, one can define directly its Gromov version.",
  "i,j,k,l|W 22 (i, k) W 22 (j, l)|2i,jk,l .(MGW2)": "Unlike MW2, there is no straightforward equivalent continuous formulation of this latter problem. Inparticular, it is not clear whether Problem (MGW2) is equivalent or not to the continuous GW problembetween and - seen as continuous measures on Rd and Rd - where the set of admissible couplings isrestricted to Gaussian mixture distributions. Thanks to the identifiability property of the set of finite Gaussianmixtures, we have that each GMM(Rd) is associated with a unique discrete distribution P(N(Rd))and MGW2 between and coincides with GW2 with squared W2 costs between their associated discretemeasures and in P(N(Rd)). Finally, note that we have defined MGW2 only between GMMs with finitenumber of components because there is in general no identifiability property for infinite Gaussian mixtures. Asan outcome, for a given infinite GMM on Rd, there might be more than one associated continuous measure on N(Rd). For instance, the standard Normal distribution N(0, 1) can naturally be identified in P(N(R))with the Dirac distribution at N(0, 1), but also with the Normal distribution N(0, 1/2) over the parametrizedline {N(, 1/2) N(R) : R}, or with N(0, 1) over the parametrized line { N(R) : R}.",
  "d1GMM(Rd) ,": "that is invariant to the mappings that transform a finite Gaussian mixture k=1 akk into another finiteGaussian mixture of the form Kk=1 akk such that for all k and i smaller than K, W2(k, i) = W2(k, i).A question that arises is: are all these mappings between GMM(Rd) and P(Rd) always associated withmappings T : Rd Rd that are isometries for the Euclidean norm and such that T# coincides with ()for every GMM(Rd)? We can already state the following converse result. Proposition 3. Let d d, and let T : Rd Rd be a mapping that is an isometry for the Euclidean norm.Then the mapping T : GMM(Rd) P(Rd) defined as T () = T# for all GMM(Rd), is suchthat for any of the form Kk=1akk, T () is in GMM(Rd) and is of the form Kk=1akk, with {k}Kk=1being such that, for all k and i smaller than K, W2(k, i) = W2(k, i), and so MGW2(, T#) = 0. Sketch of proof. The proof of this result mainly consists in showing that for any GMM(Rd), T () isin GMM(Rd) because T is necessarily affine, as a direct consequence of the Mazur-Ulam theorem (Mazurand Ulam, 1932) which implies that any isometry from Rd to Rd (endowed with the Euclidean norm) isnecessarily affine. See Appendix B.2 for the full proof. Hence, if T : Rd Rd is an isometry for the Euclidean norm, then MGW2 is invariant to the mappingT : GMM(Rd) GMM(Rd) given for all GMM(Rd), by T () = T#. Yet, in general, thereexist mappings : W2(Rd) W2(Rd) that are isometries for W2 and that are not induced by any mappingT : Rd Rd that is an isometry for the Euclidean norm. This has been proven by Kloeckner (2010) inthe general case when considering isometries defined all over W2(Rd), but remains true when restricting toisometries defined over subspaces of N(Rd) as the following example suggests. Example 4. let N++(R) be the set of one-dimensional Gaussian distributions with strictly positive mean.Let : N++(R) N++(R) be the mapping that swaps the mean and the standard deviation, i.e. such thatfor any = N(m, 2) with m > 0 and > 0, () = N(, m2). Then is an isometry for W2. Observeindeed that for and in N++(R), we have",
  "MGW2 in practice": "Using MGW2 on discrete data distributions.Most applications of optimal transport involve discretedata that can be thought as samples drawn from underlying distributions, which are not GMMs in general. Inthose applications, we aim to evaluate an OT distance between two distributions of the form = (1/M) i xiand = (1/N) j yj where {xi}i and {yj}j are families of respectively M and N vectors of Rd and Rd.Though and can be thought as mixtures of degenerate Gaussian distributions, evaluating directlyMGW2(, ) is not particularly interesting since we have in that case MGW2(, ) = GW2(.2, .2, , ).However, we can design a pseudometric MGWK,2 between and by fitting two GMMs and with Kcomponents on and and then setting MGWK,2(, ) = MGW2(, ). The approximation of and by and can be done by maximizing the log-likelihood of the GMMs with the EM algorithm (Dempsteret al., 1977). Note that if K is chosen too small, the approximations of and will be of bad quality andwe are likely to observe undesirable behaviors, as for instance having MGWK,2(, ) = 0 despite and not being equal up to an isometry. Thus, the choice of K must be a compromise between the qualityof the approximation given by the GMM and the computational cost. To illustrate the pratical use ofMGW2 on a simple toy example, we draw 150 samples from the spiral dataset provided in the scikit-learntoolbox4 (Pedregosa et al., 2011) and we apply rotations with various angles on this dataset. We then fitindependently GMMs with 20 components on the initial and the target rotated datasets and we computeMGW2 between the two obtained GMMs. We also compute GW2 with inner-product as cost functions, MW2using also 20 Gaussian components and W2. The results can be found in . As expected, MGW2 isrotation-invariant as GW2 which is not the case of MW2 and W2.",
  "Spiral datasetsEvolution of OT distances": "0/4/23 /4 MGW2GW2MW2W2 : Left first column: spiral datasets (in blue and red) composed of 150 points of R2. The red datasetcorresponds to points sampled from the distribution of the blue dataset rotated from . Left second column:The two corresponding learned GMMs with 20 components via EM algorithm (each color corresponds to aGaussian component of the GMMs). Right: evolution of MGW2, GW2, MW2, and W2 between the initialdistribution (in blue) and the rotated ones in function of the angle of rotation. Experiments are averagedover 10 runs and the colored bands correspond to +/ the standard deviation. This experiment is inspiredfrom Vayer et al. (2019b).",
  "with the convention that the infinimum over an empty set is equal to +": "Observe that if d > d, the set Isomd(Rd) is empty and so EW2(, ) = infIsomd(Rd) W2(, #). Incontrast, if d < d, Isomd(Rd) is empty and so EW2(, ) = infIsomd(Rd) W2(#, ). When d = d, thetwo infinimums are equivalent. In all what follows, we will suppose without loss of generality thatd d.",
  "k1 W2(Rk) such that for any W2(Rd) and W2(Rd), EW2(, ) = 0 if and only if there exists an isometry : Rd Rd such that = #": "Sketch of proof. Non-negativity and symmetry are straightforward. The triangular inequality can be provedobserving first that the infinimum in is always achieved, then remarking that EW2 remains unchangedwhen one of the two measures is immersed in a third Euclidean space of greater dimension than d and d.This makes EW2 closely related to the distance between metric measure spaces introduced by Sturm (2006)presented in .3. See Appendix B.4 for the full proof. Now we show that EW2 is equivalent to the OT distance introduced by Alvarez-Melis et al. (2019) describedin .3 for a particular choice of transformation space H. In all what follows, we denote Vd(Rd)the Stiefel manifold (James, 1976), i.e. the set of rectangular othogonal matrices of size d d such thatP T P = Idd. More precisely, we show the following result.Proposition 7. Let W2(Rd) and W2(Rd) and let suppose without loss of generality d d. Then,",
  "RdRd xyT d(x, y).(-COV)": "Sketch of proof. Equation (7) is a consequence of (Delon et al., 2022b, Lemma 3.3) and of the Mazur-Ulam theorem (Mazur and Ulam, 1932), which implies that any isometry from Rd to Rd (endowed with theEuclidean norm) is necessarily affine. The equivalence with Problem (-COV) is then roughly a consequence of(Alvarez-Melis et al., 2019, Lemma 4.2) which implies that Problem (7) is achieved in P at P = U Id[d,d]dV Twhere U O(Rd) and V O(Rd) are the left and right orthogonal matrices associated with the SingularValue Decomposition (SVD) ofxyT d(x, y). See Appendix B.3 for the full proof. Since Problem (-COV) is in general not equivalent to Problem (F-COV), the EW2 problem is in general notequivalent to the GW2 problem with inner-product costs. However, the following result shows that betweenGaussian distributions, the two problems share some common solutions.Proposition 8. Suppose without loss of generality that d d. Let = N(0, 0) and = N(0, 1) be twocentered Gaussian measures on Rd and Rd. Let P0, D0 and P1, D1 be the respective diagonalizations of0 (= P0D0P T0 ) and 1 (= P1D1P T1 ) that sort the eigenvalues in non-increasing order. We suppose that is not degenerate, i.e. 0 is non-singular. Then the problem",
  ")": "Sketch of proof. The proof of this result is inspired from the proof of Equation (3) given by Givens et al.(1984) that is based on Lagrangian analysis. The main difference with the proof of Equation (3) lies inthe introduction of an additional variable P with constraint P Vd(Rd). See Appendix B.5 for the fullproof.",
  "MEW2(, ) = infinfIsomd(Rd) MW2(, #),infIsomd(Rd) MW2(#, ).(8)": "As before, one can reformulate this latter problem by observing that the isomorphic mappings for theEuclidean norm are necessarily of the form Px + b with P Vd(Rd) and b Rd. Similarly to EW2, one canshow that the infinimum in is always achieved and that MEW2 satisfies all the properties of a pseudometricon GMM by simply replacing W2 by MW2 in the proof of Proposition 6. Supposing without loss ofgenerality that d d and using the equivalent discrete formulation (2) of the MW2 problem, we get that for =",
  "Numerical solver": "This time, it is not possible to derive analytically the closed form of the optimal P for Problem (MEW2).However, one can still solve the problem numerically using an alternate minimization scheme. Indeed, Problem(MEW2) is not convex in P and , but is convex in if P is fixed and is furthermore a simple small-scalediscrete OT problem in that case, which motivates the use of an alternating optimization scheme for solvingthis problem. However, Problem (MEW2) is not convex in P for a fixed because the feasible set, i.e. theStiefel manifold Vd(Rd), is not convex. For a fixed , the minimization in P can be done by projectedgradient descent (Calamai and Mor, 1987), i.e. for a given iterate P {i} and a given , the next iterateP {i+1} is given by",
  "Algorithm 1. Note that more involved optimization procedures using the specific structure of the Stiefelmanifold could probably be used here (Boumal, 2023)": "When and are only composed of non-degenerate Gaussian components, one can compute J(P)/Peither by using automatic differentiation (Baydin et al., 2018) or by using the following technical result,whose proof is postponed to Appendix B. Lemma 10. Let for any 1 k K, k = N(m0k, 0k) with m0k Rd and 0k Sd++ and for any 1 l L,l = N(m1l, 1l) with m1l Rd and 1l Sd++. For any in the K L simplex, let J : Rdd R be thefunctional defined, for all matrix P of size d d, by",
  "121l": "Initialization procedure.Since the problem is non-convex, the solution to which Algorithm 1 convergesstrongly depends on the initialization of P. It is therefore crucial to design a good initialization procedure.To do so, we propose to use the annealing scheme introduced by Alvarez-Melis et al. (2019). More precisely,we propose to set the initial P as the solution of the following iterative procedure. First we solve anentropic-regularized W2 problem between the two discrete measures =",
  "k,l {1}k,l m0kmT1l": "We then solve another entropic-regularized W2 problem, this time between and P {1}# , using a smallervalue of regularization 1 = 0 with (0, 1). We obtain thus a new coupling {2} and we can thenderive P {2} as previously. We repeat this procedure Nit times until the regularization term Nit becomessmall enough. This boils down to Algorithm 2. In practice, we set in all our experiments = 0.95 and 0 = 1 as in Alvarez-Melis et al. (2019). Furthermorewe observed that in most cases, setting Nit = 10 was sufficient to obtain a good initialization of P forAlgorithm 1.",
  "k,lk,lpk(x)y=T k,lW2(x) ,(9)": "where T k,lW2 is the optimal W2 transport map between k and P #l (where we recall that k and l arethe Gaussian components of the centered GMMs) and : Rd Rd is defined for all x Rd as (x) =P T (x b). As in Delon and Desolneux (2020), it is possible to define a unique assignment of each x bysetting for all x Rd,",
  "k akpk (x)pk (x)": "Note that Tmean is not a Monge map since is not of the form (Idd, T)#. In particular, Tmean# isnot equal to and Tmean is not necessarily the gradient of a convex function. When using MEW2 toobtain an assignment between two sets {xi}Miand {yj}Nj of respectively M and N vectors of Rd and Rd,one can compute Tmean(xi) for each xi, and then determine which yj is the closest of Tmean(xi) using anearest-neighbor algorithm (Fix and Hodges, 1951).",
  "Improving the MGW2 method": "Inspired by the MEW2 method presented above, we propose in this section to improve the MGW2 methodby: (i) proposing an annealed scheme similarly to Algorithm 2 in order to reduce the chances of convergingto sub-optimal local minima, (ii) designing a transportation plan for MGW2 similarly to (9). Annealing scheme.Since Problem (MGW2) is non-convex, we are only guaranteed to converge towards alocal minimum when solving it with a classic non-regularized GW solver (Peyr et al., 2016). Furthermore,the convergence towards a particular minimum depends strongly on the initialization of the coupling . Sincethe discrete GW problem in MGW2 is of very small scale and so not costly in itself, we propose, by anologywith MEW2, to use a similar annealing scheme as in Algorithm 2 to reduce the chance of converging to asub-optimal local minimum. More precisely, this gives the following algorithm. As previously, we set in our experiments = 0.95 and 0 = 1 as in Alvarez-Melis et al. (2019) and we observedthat, in toy cases where we know what the global minimum is, that Nit = 10 seemed to be a sufficient numberof iterations to prevent the algorithm from converging towards a sub-optimal minimum. Designing a transportation plan.Still by analogy with MEW2, one can design a transportation planfor MGW2 by defining a matrix PMGW2 Vd(Rd) and a vector bMGW2 Rd, and then replacing TW2in (9) by MGW2 TW2, where for all x Rd, MGW2(x) = P TMGW2(x bMGW2). More precisely, this can bedone the following way. Given two GMMs =",
  "k,lk,lW 22 (k, P#l) ,(10)": "where k and l are the Gaussian component of the centered GMMs and , then we can set bMGW2 =EX[X] PMGW2EY [Y ]. As above, this problem can be solved numerically by performing a projectedgradient descent on P, using either automatic differentiation or Lemma 10. This is also a non-convexoptimization problem since Vd(Rd) is non-convex and so the solution given by the projected gradient descentdepends on the initialization. We propose thus to initialize with the projection on the Stiefel manifold of thediscrete cross-covariance matrix between the means of the Gaussian components, i.e.",
  "Experiments": "In what follows, we use MGW2 and MEW2 to solve Gromov-Wasserstein related tasks on various datasets.More precisely, we apply first the two methods on simple toy low-dimensional GMMs. Then, we show thatboth methods can be used to solve relatively efficiently GW related tasks on real datasets in moderate tolarge scale settings involving sometimes several tens of thousands of points, both for evaluating distancesbetween clouds of points and drawing correspondences between points. In all our experiments, we use thenumerical solvers provided by the Python Optimal Transport (POT) package5 (Flamary et al., 2021) thatimplements solvers for the non-regularized and regularized classic OT and GW problems. Code is availablehere6.",
  "Low dimensional GMMs": "In , we use again the example of and we derive an optimal transport plan for the MGW2problem as described in .2.3. We also show the plan obtained by solving the EW2 problem. Onecan see that with both solutions, the global structure of the distribution is preserved in the sense that pointsthat are closed to each other but in two different Gaussian components have been sent to points that are alsoclose to each other but in different Gaussian components.",
  "dataMGW2MEW2": ": Left: two discrete distributions (in gradient of colors) and (in blue) that have been drawn fromtwo GMMs. The colors have been added to in order to visualize the couplings between and . Middle:transport of obtained by solving the MGW2 problem, then deriving PMGW2 V2(R2) by solving Problem(10). Right: transport of obtained by solving the MEW2 problem. (2016) with the use of entropic-regularized GW, that aims to recover the cyclical nature of a horses gallop.Then, we perform a comparison between runtimes of MGW2 and other methods existing in the literaturethat provide a GW-type distance between point clouds. Galloping horse sequence.Here we repoduce the experiment of the galloping horse, that has beenoriginally conducted in Rustamov et al. (2013) and presented in Solomon et al. (2016) with the use ofentropic-regularized GW. In this experiment, we compute a matrix of pairwise distances (either for MGW2or MEW2) between 45 meshes representing a galloping horse. Then, we conduct a Multi-Dimensional Scaling(MDS) (Borg and Groenen, 2005) - which roughly can be thought as a generalization of PCA - of the 45 45matrix of pairwise distances between meshes, in order to plot each mesh as a 2-dimensional point. shows these 2-dimensional embeddings of the sequence. As observed in Solomon et al. (2016), the interestingpart here is that these points are positioned in a cyclical fashion, which means that the original set of pairwisedistances seem to respect the periodic aspect of the sequence (both for MGW2 and MEW2). Each mesh iscomposed of approximately 9000 vertices and the average time to compute one distance when using the POTimplementation of the entropic-regularized GW solver is around 30 minutes which makes the computation ofthe full pairwise distance matrix impractical, as mentioned in Solomon et al. (2016). In constrast, when usingour methods with GMMs with K = 20 components, it took us only approximately 10 minutes to computethe full distance matrix using MGW2, and around one hour using MEW2, these times including the fittingwith EM of all the GMMs.",
  "MGW2MEW2Data": "Mesh id Mesh id : MDS on the galloping horse animation using the MGW2 distance (left), and the MEW2 distance(middle). Each point corresponds to a given mesh and the meshes are colored in function of their number inthe sequence. Right: 4 examples among the 45 meshes that composes the sequence. The computations ofboth distances have been done by first fitting GMMs with 20 components on each mesh independently.",
  "MGW2MEW2": ": Color transfers between a hyperspectral image with 15 channels (top left) and two paintings byGauguin and Renoir (top right, middle right). Bottom line: the obtained RGB images using MGW2 andMEW2. For this experiment, we used GMMs with 15 components. Image taken by Francesca Ramacciotti(Alma Mater Studiorum - University of Bologna) and Laure Cazals (supported by the European Commissionin the framework of the GoGreen project (GA no. 101060768)).",
  "SourceTarget (MGW2)Target (MEW2)": ": Shape matching between human-shaped meshes using MGW2 (middle) and MEW2 (right). Eachshape on the left column is matched with the shapes on the same row. GMMs with 20 components havebeen fitted independently on each shape and the points colored in green and purple correspond to Gaussiancomponents that are matched together when solving MGW2 or MEW2. From left to right and top to bottom,the meshes are composed respectively of 84912, 30300, 75000, 273624, 360678, and 360357 vertices.",
  "MethodParamHumansPlanesSpidersCarsDogsTreesVases192621442664522089371043315828": "MGW2100.04 (17.5)0.40 (17.6)0.03 (17.7)0.12 (17.8)0.13 (18.8)0.10 (18.8)0.34 (19.1)200.15 (69.4)0.36 (69.5)0.04 (69.9)0.17 (70.8)0.20 (71.9)0.10 (71.9)0.28 (73.1)500.18 (431)0.10 (428)0.007 (431)0.12 (435)0.20 (437)0.04 (438)0.20 (441) MEW2100.09 (17.1)0.37 (22.9)0.02 (16.3)0.23 (17.6)0.20 (24.5)0.11 (18.3)0.29 (23.2)200.21 (77.2)0.39 (66.6)0.02 (64.1)0.25 (78.0)0.20 (85.6)0.13 (67.1)0.30 (76.8)500.16 (555)0.17 (421)0.009 (397)0.20 (423)0.21 (462)0.08 (465)0.19 (486) qGW0.010.25 (0.59)0.46 (0.78)0.05 (1.08)0.24 (3.88)0.28 (11.3)0.13 (17.4)0.28 (32.4)0.10.16 (1.04)0.10 (1.33)0.02 (1.84)0.21 (5.80)0.02 (16.5)0.05 (26.8)0.18 (54.9)0.20.11 (1.65)0.08 (2.12)0.01 (2.86)0.15 (9.25)0.008 (28.2)0.04 (53.6)0.21 (123)0.50.10 (4.39)0.07 (5.77)0.007 (7.73)0.16 (34.9)0.007 (104)0.15 (165)0.22 (418) : Distortion scores (lower is better) and runtimes (in parentheses) for MGW2, MEW2, and qGW.The average number of points in each shape class is provided under the shape class name. Results are listedfor several parameter choices of each method. Results have been averaged on 10 runs of the experiment. Matching human shaped meshes.To demonstrate the usability of our methods in larger scale settings,we use the SHREC19 dataset (Melzi et al., 2019) that contains human shaped meshes that can sometimesbe composed of more than 300000 vertices. Our goal is to draw correspondences between the shapes usingonly the information of the vertices (the dataset also includes edges). To do so, we first fit independentlyGMMs with 20 components on each mesh and we derive directly couplings at the scale of the Gaussiancomponents that represent the different parts of the bodies. In such large scale settings, the main bottleneckof the methods in terms of computational time is clearly the fitting of the GMMs that can take at worst 2minutes for the meshes composed of the highest number of vertices. The results are displayed on .Observe that in most cases, both methods seem to be able to match correctly the colored parts. Yet in thelast row, MEW2 matches a leg at the left in red to an arm at the right. This probably implies that themethod has been trapped in a local minimum despite the annealing initialization procedure. Finally, notethat we presented here cases where the methods performed relatively well, but there are cases where MGW2",
  "Application to hyperspectral image color transfer": "The goal here is to reproduce the experiment of color transfer conducted in Delon and Desolneux (2020),but this time using a hyperspectral image, i.e an image with more than 3 color channels. More precisely, weaim to create an RGB image from an hyperspectral image u using the colors of another RGB image v. Todo so, we consider images as empirical distributions in the color spaces and we solve a Gromov-Wassersteinproblem between the distributions =1MMk uk and =1NNl vl, where M and N are the numberof pixels in respectively the hyperspectral image and the RGB image we use as color palette, and {uk}Mkand {vl}Nlare the values at each pixel, i.e for here all l, vl R3 and for all k, uk Rd with d > 3. Wethus fit two GMMs and on respectively and and we use MGW2 or MEW2 to derive a mappingTmean : Rd R3, as described in .2.2. We apply this process to a hyperspectral image of 512 512pixels with 15 channels that are displayed in top left. We use as color palettes two paintings byGauguin and Renoir, displayed in top right, that are respectively Manhana no atua (top) andLe djeuner des canotiers (bottom). These two images are composed of 1024 768 pixels. The resultingimages Tmean(u) are displayed in bottom (Gauguin at the left and Renoir at the right). For thisexperiment, we observed that setting the number of Gaussian components to K = 15 was a good compromisebetween capturing the complexity of the color distributions and obtaining a relatively regular mapping Tmean.This experiment shows that MGW2 and MEW2 can be used in large scale settings: observe indeed that thecolor distributions and are composed respectively of approximatively 300000 and 800000 points, whichmakes the problem intractable with entropic-GW solvers such as Peyr et al. (2016) or Solomon et al. (2016).",
  "Conclusion and perspectives": "In this paper, we have introduced two new OT distances on the set of Gaussian mixture models, MGW2 andMEW2, and we have shown that they both can be used to solve relatively efficiently Gromov-Wassersteinrelated problems on Euclidean spaces, especially in moderate-to-large scale settings involving several tens ofthousands of points. These OT distances are also by design particularly suited to settings where there alreadyexists a kind of clustering structure in the data. This being said, if MEW2 remains an efficient alternative tothe entropic GW solvers proposed by Peyr et al. (2016) and Solomon et al. (2016), we observed that themethod was actually slower and perhaps harder to tune than MGW2 for a slighty lower quality of results,and so we believe that MGW2 is a better choice in practice. This latter distance is part of the family ofGromov-Wasserstein type OT distances that reduce the size of the GW problem, which also includes notablyqGW (Chowdhury et al., 2021), MREC (Blumberg et al., 2020) and scalable GW (Xu et al., 2019). To the",
  "Altschuler, J., Bach, F., Rudi, A., and Weed, J. (2018). Approximating the quadratic transportation metricin near-linear time. arXiv preprint arXiv:1810.10046": "Alvarez-Melis, D. and Jaakkola, T. (2018). GromovWasserstein alignment of word embedding spaces. InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 18811890. Alvarez-Melis, D., Jegelka, S., and Jaakkola, T. S. (2019). Towards optimal transport with global invariances.In The 22nd International Conference on Artificial Intelligence and Statistics, pages 18701879. PMLR.",
  "Fix, E. and Hodges, J. (1951). Discriminatory analysis: nonparametric discrimination: consistency properties.report. 4. T. USAF School of Aviation Medicine": "Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenflos,A., Fatras, K., Fournier, N., et al. (2021). POT: Python Optimal Transport. The Journal of MachineLearning Research, 22(1):35713578. Forbes, F., Nguyen, H. D., Nguyen, T. T., and Arbel, J. (2021). Approximate bayesian computation withsurrogate posteriors. Preprint hal-03139256.(Cited on pages 5, 65, 69, 71, 101, 103, 251, 265, and 273.). Forrow, A., Htter, J.-C., Nitzan, M., Rigollet, P., Schiebinger, G., and Weed, J. (2019). Statistical optimaltransport via factored couplings. In The 22nd International Conference on Artificial Intelligence andStatistics, pages 24542465. PMLR. Genevay, A., Peyre, G., and Cuturi, M. (2018). Learning generative models with Sinkhorn divergences. InInternational Conference on Artificial Intelligence and Statistics, volume 84, pages 16081617. PMLR.",
  "Lambert, M., Chewi, S., Bach, F., Bonnabel, S., and Rigollet, P. (2022). Variational inference via Wassersteingradient flows. In Advances in Neural Information Processing Systems": "Leclaire, A., Delon, J., and Desolneux, A. (2023). Optimal transport between GMMs for texture synthesis.In Scale Space and Variational Methods in Computer Vision: 9th International Conference, SSVM 2023. Luzi, L., Marrero, C. O., Wynar, N., Baraniuk, R. G., and Henry, M. J. (2023). Evaluating generativenetworks using Gaussian mixtures of image features. In Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision, pages 279288.",
  "Pasande, M., Hosseini, R., and Araabi, B. N. (2022). Stochastic first-order learning for large-scale flexiblytied Gaussian mixture model. arXiv preprint arXiv:2212.05402": "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer,P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: machine learning in python. the Journal of machineLearning research, 12:28252830. Pele, O. and Taskar, B. (2013). The tangent Earth movers distance. In Geometric Science of Information:First International Conference, GSI 2013, Paris, France, August 28-30, 2013. Proceedings, pages 397404.Springer.",
  "Rabin, J., Ferradans, S., and Papadakis, N. (2014). Adaptive color transfer with relaxed optimal transport.In 2014 IEEE international conference on image processing (ICIP), pages 48524856. IEEE": "Rabin, J., Peyr, G., Delon, J., and Bernot, M. (2012). Wasserstein barycenter and its application to texturemixing. In Scale Space and Variational Methods in Computer Vision: Third International Conference,SSVM 2011, Ein-Gedi, Israel, May 29June 2, 2011, Revised Selected Papers 3, pages 435446. Springer. Rustamov, R. M., Ovsjanikov, M., Azencot, O., Ben-Chen, M., Chazal, F., and Guibas, L. (2013). Map-basedexploration of intrinsic shape differences and variability. ACM Transactions on Graphics (TOG), 32(4):112.",
  "Organization of the supplementary": "The supplementary is organized as follows. First, in Appendix A, we show six technical results that will beused throughout the proofs of the paper. In Appendix B, we give the full proofs of the technical results ofthe paper. Finally, in Appendix C, we give more details on the difference between EW2 and the OT distanceintroduced by Cai and Lim (2022).",
  "First we start by recalling the following result (Delon et al., 2022b, Lemma 3.3)": "Lemma A1 (Delon et al., 2022b). Let W2(Rd) and W2(Rd) with d not necessarily greater thand, and let T : Rd Rd be a measurable map. Then (, T#) if and only if there is some (, )such that = (Idd, T)#. In particular, if there exist a, b 0 such that T(y) a + by for all y Rd,then",
  "Py + b Py b2 = P(y y)2 = (y y)T P T P(y y) = (y y)T (y y) = y y2": "The converse is a consequence of the MazurUlam theorem (Mazur and Ulam, 1932) that states - in theversion of Baker (1971) - that an isometry from a real normed space to a strictly convex normed space, i.e. anormed space where the unit ball is a stricly convex set, is necessarily affine. Since it is easy to show that theunit ball {x Rd : x 1} is a strictly convex set, we get that for all x Rd, is of the form y Py + bwith P being a matrix of size d d, and b Rd. Moreover we have for all y, y Rd",
  "where P = diag[d,d]((P)) with (P) Rd+ denoting the vector of singular values of P. Furthermore thesupremum is achieved at P of the form,P = UKP V TK": "Proof. Note that this lemma can be proven with a proof similar to the one of Alvarez-Melis et al. (2019,Lemma 4.2), using the min-max theorem for singular values. Here we offer an alternative proof based onLagragian analysis. First observe that the supremum is achieved as a direct consequence of the Weierstrasstheorem because P is compact and the mapping P tr(P T K) is continuous. For a given P P, letUP P V TP be the SVD of P. The problem can be rewritten as",
  "D[d]K U T D[d]P U = U T P V DKV TP U": "It follows that V DKV T DP and D[d]K U T D[d]P U are symmetric matrices and so V DKV T commutes with DPand U T D[d]P U commutes with D[d]K . Thus we can deduce that U and V are permutation matrices. Since thesingular values are ordered in non-increasing order, we deduce that the problem is maximized when U = Iddand V = Idd. This implies that UP = UK and VP = VK, which concludes the proof. Note that Lemma A4 is especially useful when the constraint of belonging to the set P can be expressed as aconstraint on the singular values. Observe that this is the case of Vd(Rd) since for all P Vd(Rd), we haveP T P = Idd and so an equivalent condition of belonging in Vd(Rd) is that (P) = 1d.",
  "B.1Proof of Proposition 2": "Proof of Proposition 2. Takatsu (2010) has shown that the space of Gaussian distributions N(Rd) is acomplete metric space when endowed with W2. Moreover, N(Rd) is separable since it is a subspace of W2(Rd)which is itself a separable metric space when endowed with W2 (Bolley, 2008). Thus, N(Rd) is Polish andwe can directly apply the Gromov-Wasserstein theory developped in Sturm (2012). Let (N(Rd), W2, ) and(N(Rd), W2, ) be two metric measure spaces respectively in M4. Let us define",
  "N(Rd)N(Rd)|W 22 (, ) W 22 (, )|2d(, )d(, )": "Applying Sturm (2012, Corollary 9.3), we get that D defines a metric over the space of metric measurespaces of the form (N(Rd), W2, ) quotiented by the strong isomorphisms, and thus we get directly that D issymmetric, non-negative, satisfies the triangle inequality and D(, ) = 0 if and only if there exists a bijection: supp() supp() such that = #, where for any and in supp(), W2((), ()) = W2(, ).",
  "Since is in W2(Rd), is in W2(Rd) and so": "Rd x2d < +. It follows that |J(P0) J(P1)| 0 whenP0P12F 0 and so J is continuous. Moreover, since Vd(Rd) is compact (James, 1976), J has a minimumon Vd(Rd) as a result of the classic Weierstrass theorem that states that any real-valued continous functiondefined on a compact set achieves its infinimum. Thus, there exists P such that EW2(, ) = W2(, P #)and setting b = EX[X] P EY [Y ] and (x) = P x + b for all x Rd, we get that there exists Isomd(Rd) such that EW2(, ) = W2(, #), which concludes the proof.",
  "Lemma B2. Let W2(Rd) and W2(Rd) with d not necessarily greater than d. Let r max{d, d}and let Isomd(Rr). Then, EW2(, ) = EW2(#, )": "Proof. First, using Lemma A2, we get that there exists P1 Vd(Rr) and b1 Rr such that for all x Rd,(x) = P1x + b1. Since r d, we have, denoting , # and the centered measures respectively associatedwith , #, and , and using successively Lemma A3 and Lemma A1,",
  "Now observe that P1K has the same singular values as K since KT P T1 P1K = KT K. Thus P1K =K and so EW2(#, ) = EW2(, ), which concludes the proof": "Observe that Lemma B2 highlights that EW2 shares close connections with the distance between metricmeasure spaces introduced in Sturm (2006) and defined in Equation (6). However it is not clear whetherthe two distances are strictly equivalent or not because the infinimum in Z in Equation (6) also includesnon-Euclidean spaces. However, if we restrict the problem only to Euclidean spaces Z, then Lemma B2directly implies that the two distances are equivalent. Now we are ready to prove Proposition 6. Proof of Proposition 6. First observe that non-negativity is straightforward. Furthermore, observe also that ifd = d, symmetry is also straightfoward. Now suppose d = d and observe that that the set Vd(Rd) coincideswith the set of orthogonal matrices O(Rd). Thus we have",
  "D": "120,P ), we set the largest eigenvalue 1 of P T 0P to 1. We denotey1 Rd the eigenvector associated. We have y1P T 0Py1 = 1 and Py1 = y1 = 1 so using Lemma A5,we get that Py1 is an eigenvector of 0 associated with 1. Let k and yk be any other eigenvalue andits associated eigenvector in the orthonormal basis in which P T 0P is diagonal. We have yTk y1 = 0 and soyTk P T Py1 = 0. Thus Pyk is orthogonal to Py1. Since Pyk = 1, we get that Pyk is also an eigenvector of0 and so it exists i d 1 such that k = yTk P T 0Pyk = i. Thus, we conclude that the eigenvalues of",
  "PW2(, ) =infd(Rd) W2(#, ) ,(PW2)": "where d(Rd) is the set of all affine mapping from Rd to Rd of the form (x) = P T (x b) with P Vd(Rd)and b Rd. One key results of Cai and Lim (2022) is to show that PW2 has the following equivalentformulationPW2(, ) =infW2 (Rd) W2(, ) ,",
  "W2 (Rd) = { W2(Rd) :there exists (x) = P T (x b) with P Vd(Rd) and b Rd such that # = }": "Observe that this latter formulation is structurally different of EW2 since for any isometry : Rd Rd, thedistribution # is necessarily degenerate, whereas this is not the case for the distribution . The differencebetween EW2 and PW2 is illustrated in Figure C1. To highlight even more the difference between EW2 and PW2, we derive an equivalent problem of Problem(PW2). Observe that in that case, the mapping in (PW2) is not an isometry since it is not injective. As aresult, the term that previously depended only on the marginal in the developpement of the square of theEuclidean distance will now depend on P. More precisely, this gives the following result.Proposition C1. Let W2(Rd) and W2(Rd) and let suppose d d. Problem (PW2) is equivalent to"
}