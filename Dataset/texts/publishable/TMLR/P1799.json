{
  "Abstract": "We study a classical problem in private prediction, the problem of computing an (m, )-differentially private majority of K (, )-differentially private algorithms for 1 m Kand 1 > 0. Standard methods such as subsampling or randomized response arewidely used, but do they provide optimal privacy-utility tradeoffs? To answer this, weintroduce the Data-dependent Randomized Response Majority (DaRRM) algorithm. It isparameterized by a data-dependent noise function , and enables efficient utility optimizationover the class of all private algorithms, encompassing those standard methods. We show thatmaximizing the utility of an (m, )-private majority algorithm can be computed tractablythrough an optimization problem for any m K by a novel structural result that reducesthe infinitely many privacy constraints into a polynomial set. In some settings, we showthat DaRRM provably enjoys a privacy gain of a factor of 2 over common baselines, withfixed utility. Lastly, we demonstrate the strong empirical effectiveness of our first-of-its-kindprivacy-constrained utility optimization for ensembling labels for private prediction fromprivate teachers in image classification. Notably, our DaRRM framework with an optimized exhibits substantial utility gains when compared against several baselines.",
  "Introduction": "Differential privacy (DP) is a widely applied framework for formally reasoning about privacy leakage whenreleasing statistics on a sensitive database Erlingsson et al. (2014); Cormode et al. (2018). Differential privacyprotects data privacy by obfuscating algorithmic output, ensuring that query responses look similar onadjacent datasets while preserving utility as much as possible Dwork et al. (2006). Privacy in practice often requires aggregating or composing multiple private procedures that are distributedfor data or training efficiency. For example, it is common to aggregate multiple private algorithmic or modeloutputs in methods such as boosting or calibration (Sagi & Rokach, 2018). In federated learning, modeltraining is distributed across multiple edge devices. Those devices need to send local information, such aslabels or gradients Konen`y et al. (2016), to an aggregating server, which is often honest but curious aboutthe local training data. Hence, the output from each model at an edge device needs to be privatized locallybefore being sent to the server. When translating from a local privacy guarantee to a centralized one, oneneeds to reason about the composition of the local privacy leakage Naseri et al. (2020). Therefore, we formallyask the following: Problem 1.1 (Private Majority Ensembling (Illustrated in )). Consider K 1 (, )-differentiallyprivate mechanisms M1, . . . , MK for K odd. Given a dataset D, each mechanism outputs a binary answer that is, Mi : D {0, 1}, i [K]. Given a privacy allowance 1 m K, m R and a failure probability",
  ", , [0, 1), how can one maximize the utility of an (m, )-differentially private mechanism Ato compute the majority function g(S1, S2, . . . , SK), where Si Mi(D)?": ": An illustration of the problem setting. The inputs are the dataset D and K (, )-differentially privatemechanisms M1, . . . , MK. One draws samples Si Mi(D) and computes an aggregated output g(S1, . . . , SK)based on all observed samples. Our goal is to design a randomized algorithm A that approximately computesg and is (m, )-differentially private for 1 m K and 0. We focus on g being the majorityfunction . The majority function g is often used in private prediction, where one studies the privacy cost of releasingone prediction Dwork & Feldman (2018) and exploits the fact that releasing only the aggregated outputon sharded models is significantly more private than releasing each prediction. For example, this occurs insemi-supervised knowledge transfer with private aggregated teacher ensembles (PATE) Papernot et al. (2017;2018), in ensemble learning algorithms Jia & Qiu (2020); Xiang et al. (2018), machine unlearning Bourtouleet al. (2021), private distributed learning algorithms such as Stochastic Sign-SGD Xiang & Su (2023), and inensemble feature selection Liu et al. (2018). Private prediction is also shown to be a competitive technique indata-adaptive settings, where the underlying dataset is changing slowly over time, to quickly adjust to onlinedataset updates Zhu et al. (2023). Furthermore, to address the large privacy loss of private prediction underthe many-query regime, there has been recent works in everlasting private prediction that extends privacyguarantees with repeated, possibly infinite, queries without suffering a linear increase in privacy loss Naoret al. (2023); Stemmer (2024). These works, however, rely often on the standard sensitivity analysis of g to provide a private output andthus generally provide limited utility guarantees. This is because the maximum sensitivity of g can be toopessimistic in practice, as observed in the problem of private hyperparameter optimization (Liu & Talwar,2019). On the other hand, for private model ensembling, a naive way to bound privacy loss without restrictiveassumptions is to apply simple composition (Theorem 2.2) or general composition (Theorem 2.3, a tighterversion compared to advanced composition) to reason about the final privacy loss after aggregation. Ablack-box application of the simple composition theorem to compute g would incur a K privacy cost in thepure differential privacy setting, that is, = 0, or if one is willing to tolerate some failure probability , generalcomposition would yield a O( K) privacy cost Kairouz et al. (2015). Thus, a natural baseline algorithm Athat is (m, m)-differentially private applies privacy amplification by subsampling and randomly chooses mof the K mechanisms to aggregate and returns the majority of the subsampled mechanisms. This technique isreminiscent of the subsampling procedure used for the maximization function g (Liu & Talwar, 2019) or somegeneral techniques for privacy amplification in the federated setting via shuffling (Erlingsson et al., 2019). However, standard composition analysis and privacy amplication techniques can be suboptimal for computinga private majority, in terms of both utility and privacy. Observe that if there is a clear majority amongthe outputs of M1(D), . . . , MK(D), one can add less noise. This is because each mechanism Mi is (, )-differentially private already, and hence, is less likely to change its output on a neighboring dataset bydefinition. This implies the majority outcome is unlikely to change based on single isolated changes in D.Furthermore, composition theorems make two pessimistic assumptions: 1) the worst-case function g andthe dataset D are considered, and 2) all intermediate mechanism outputs M1(D), . . . , MK(D) are released,",
  "Our Contributions": "We give a (perhaps surprising) affirmative answer to the above question by using our novel data-dependentrandomized response framework (DaRRM), which captures all private majority algorithms, we introduce atractable noise optimization procedure that maximizes the privacy-utility tradeoffs. Furthermore, we canprovably achieve a constant factor improvement in utility over simple subsampling by applying data-dependentnoise injection when Mis are i.i.d. and = 0. To our knowledge, this is the first of its work of its kind thatgives a tractable utility optimization over the possibly infinite set of privacy constraints. Data-dependent Randomized Response Majority (DaRRM). We generalize the classical RandomizedResponse (RR) mechanism and the commonly used subsampling baseline for solving Problem 1.1 and proposea general randomized response framework DaRRM (see Algorithm 1), which comes with a customizable noisefunction . We show that DaRRM actually captures all algorithms computing the majority whose outputsare at least as good as a random guess (see Lemma 3.3), by choosing different functions. Designing with Provable Privacy Amplification.The choice of the function in DaRRM allowsus to explicitly optimize noise while trading off privacy and utility. Using structural observations, we showprivacy amplification by a factor of 2 under mild conditions over applying simple composition in the puredifferential privacy setting when the mechanisms Mis are i.i.d. (see Theorem 4.1). Finding the Best through Dimension-Reduced Optimization. We further exploit the generalityof DaRRM by applying a novel optimization-based approach that applies constrained optimization to finda data-dependent that maximizes some measure of utility. One challenge is that there are infinitelymany privacy constraints, which are necessary for DaRRM with the optimized to satisfy the given privacyloss. We show that we can reformulate the privacy constraints, which are infinite dimensional, to a finitepolynomial-sized constraint set, allowing us to efficiently constrain the optimization problem to find the best, even for approximate differential privacy (see Lemma 5.1). Empirically, we show that with a small m and, the optimized (see opt in ) achieves the best utility among all functions, even compared tothe subsampling and the data-independent baseline. To our knowledge, this is the first utility maximizationalgorithm that optimizes over all private algorithms by constrained optimization with dimension reduction. Experiments. In downstream tasks, such as semi-supervised knowledge transfer for private image classi-fication, we compare our DaRRM with an optimized to compute the private label majority from privateteachers against PATE Papernot et al. (2018), which computes the private label majority from non-privateteachers. We fix the privacy loss of the output of both algorithms to be the same and find that when thenumber of teachers K is small, DaRRM indeed has a higher utility than PATE, achieving 10%-15% and 30%higher accuracy on datasets MNIST and Fashion-MNIST, respectively.",
  "Related Work": "Private Composition. Blackbox privacy composition analysis often leads to pessimistic utility guarantees.In the blackbox composition setting, one can do no better than the O(K) privacy analysis for pure differentialprivacy Dwork et al. (2014). For approximate differential privacy, previous work has found optimal constantsfor advanced composition by reducing to the binary case of hypothesis testing with randomized response; andoptimal tradeoffs between , for black box composition are given in Kairouz et al. (2015), where there couldbe a modest improvement 20%. Thus, for specific applications, previous work has turned to white-box composition analysis for improvedutility. This includes, for example, moment accountant for private SGD Abadi et al. (2016) and the applicationof contractive maps in stochastic convex optimization Feldman et al. (2018). For the specific case of modelensembles, Papernot et al. (2018) shows a data-dependent privacy bound that vanishes as the probability of",
  "disagreement goes to 0. Their method provides no utility analysis but they empirically observed less privacyloss when there is greater ensemble agreement": "When g is the maximization function, some previous work shows that an approximately maximum value canbe outputted with high probability while incurring O() privacy loss, independently of K. Liu & Talwar(2019) proposed a random stopping mechanism for m = 1 that draws samples uniformly at random fromMi(D) at each iteration. In any given iteration, the sampling halts with probability and the final output iscomputed based on the samples collected until that time. This leads to a final privacy cost of only 3 forthe maximization function g, which can be improved to 2 (Papernot & Steinke, 2022). In addition to theaforementioned works, composing top-k and exponential mechanisms also enjoy slightly improved compositionanalysis via a bounded-range analysis Durfee & Rogers (2019); Dong et al. (2020). Bypassing the Global Sensitivity. To ensure differential privacy, it is usually assumed the query functiong has bounded global sensitivity that is, the output of g does not change much on any adjacent inputdatasets differing in one entry. The noise added to the output is then proportional to the global sensitivity ofg. If the sensitivity is large, the output utility will thus be terrible due to a large amount of noises added.However, the worst case global sensitivity can be rare in practice, and this observation has inspired a line ofworks on designing private algorithms with data-dependent sensitivity bound to reduce the amount of noisesadded. Instead of using the maximum global sensitivity of g on any dataset, the classical Propose-Test-Releaseframework of Dwork Dwork & Lei (2009) uses a local sensitivity value for robust queries that is testedprivately and if the sensitivity value is too large, the mechanism is halted before the query release. Thehalting mechanism incurs some failure probability but deals with the worst-case sensitivity situations, whileallowing for lower noise injection in most average-case cases. One popular way to estimate average-case sensitivity is to use the Subsample-and-Aggregate framework byintroducing the notion of perturbation stability, also known as local sensitivity of a function g on a datasetD Thakurta & Smith (2013); Dwork et al. (2014), which represents the minimum number of entries in Dneeds to be changed to change g(D). One related concept is smooth sensitivity, a measure of variability of gin the neighborhood of each dataset instance. To apply the framework under smooth sensitivity, one needs toprivately estimate a functions local sensitivity Ls and adapt noise injection to be order of O( Ls ), whereLs can often be as small as O(en), where n = |D|, the total dataset size Nissim et al. (2007). Generally,the private computation of the smooth sensitivity of a blackbox function is nontrivial but is aided by theSubsample and Aggregate approach for certain functions. These techniques hinge on the observation that a function with higher stability on D requires less noise toensure worst case privacy. Such techniques are also applied to answer multiple online functions/queries inmodel-agnostic learning Bassily et al. (2018). However, we highlight two key differences in our setting with aweaker stability assumption. First, in order to estimate the perturbation stability of g on D, one needs todownsample or split D into multiple blocks Thakurta & Smith (2013); Dwork et al. (2014); Bassily et al.(2018), D1, . . . , DB , and estimate the perturbation stability based on the mode of g( D1), . . . , g( DB). Thisessentially reduces the amount of change in the output of g due to a single entry in D, with high probabilityand replaces the hard-to-estimate perturbation stability of g with an easy-to-compute perturbation stability ofthe mode. Such a notion of stability has also been successfully applied, along with the sparse vector technique,for model-agnostic private learning to handle exponentially number of queries to a model Bassily et al. (2018).Note that in these cases, since a private stochastic test is applied, one cannot achieve pure differential privacyDwork et al. (2014). In practice, e.g. federated learning, however, one does not have direct access to D, andthus it is impractical to draw samples from or to split D. Second, to ensure good utility, one relies on a keyassumption, i.e. the subsampling stability of g, which requires g( D) = g(D) with high probability over thedraw of subsamples D. Although our intuition in designing DaRRM also relies on the stability of the mode function g, previous usageof stability to improve privacy-utility tradeoffs, e.g., propose-test-release Vadhan (2017); Dwork et al. (2014),requires the testing of such stability, based on which one adds a larger (constant) noise . This can still leadto adding redundant noise in our case.",
  "Published in Transactions on Machine Learning Research (November/2024)": "0.5 m = 1m = 3 0.5 m = 5m = 7 05.5110 0.5 m = 9 05.511 m = 11 Support l{0, 1, . . . , K} values Shape of functions 0.0 0.1 0.2 m = 1 0.0 0.1 m = 3 0.00 0.05 0.10 m = 5 0.00 0.05 m = 7 0.00 0.02 m = 9 0.05 0.00 0.05 m = 11 functions",
  "Preliminaries": "We first introduce the definition of differential privacy, simple composition and general composition as follows.The general composition Kairouz et al. (2015) gives a near optimal and closed-form bound on privacy lossunder adaptive composition, which improves upon advanced composition Dwork et al. (2014). Definition 2.1 (Differential Privacy (DP) Dwork et al. (2014)). A randomized mechanism M : D R witha domain D and range R satisfies (, )-differential privacy for , 0 if for any two adjacent datasetsD, D and for any subset of outputs S R it holds that Pr[M(D) S] e Pr[M(D) S] + . = 0 isoften called pure differential privacy; while > 0 is often called approximate differential privacy.",
  "and the utility is defined as 1 E(A)": "Notation.Throughout the paper, we use the same notations defined in Problem 1.1 and Definition 2.4.Furthermore, let D and D to denote a pair of adjacent datasets with one entry being different. Also, letpi = Pr[Mi(D) = 1] and pi = Pr[Mi(D) = 1], i [K]. We omit the subscript i when all pis or pisare equal. I{} denotes the indicator function and [K] = {1, 2, . . . , K}. For the purpose of analysis, letL(D) = Ki=1 Mi(D) {0, 1, . . . , K}, i.e. the (random) sum of all observed outcomes on dataset D. D isomitted when the context is clear. Unless specified, we use the noise function : {0, 1, . . . , K} asinput to our algorithms to calibrate the probabilistic noise injection. Unless specified, the privacy allowancem R.",
  "Private Majority Algorithms": "The very first approach to consider when solving private majority ensembling (Problem 1.1), since the outputis binary, is the classical Randomized Response (RR) mechanism Dwork et al. (2014), where one flips a biasedcoin with a constant probability pconst . If the coin lands on head with probability pconst, output thetrue majority base on K samples; if not, then simply output a noisy random answer. However, to make theoutput (m, )-differential private, the success probability pconst can be at most O( m",
  "K )) when = 0 (or > 0) (see Appendix A.1), which is too small for any reasonable utility": "The key observation for improved utility is that the probability of success should not be a constant, butshould depend on the unpublished set of observed outcomes from the mechanisms S. If we see many 1sor 0s in S, then there should be a clear majority even on adjacent datasets. On the other hand, if we seeabout half 1s and half 0s, this means the majority is highly volatile to data changes, which implies we needmore noise to ensure privacy. In summary, if we can calibrate the success probability based on S to smoothlyincrease when there is a clear majority, we can improve the utility without affecting privacy. Subsampling. One natural baseline is outputting the majority of m out of K randomly subsampledmechanisms (without replacement), given a privacy allowance m [K]. Suppose m, the privacy loss ofthe aggregated output can be reasoned through simple composition or general composition. Interestingly, weshow outputting the majority of m out of K subsampled mechanisms corresponds to RR with a non-constantprobability p = Sub(L(D)), which is set by a polynomial function Sub : {0, . . . , K} based on thesum of observed outcomes L(D) in Lemma 3.1 (see a full proof in Appendix A.2). Intuitively, subsampling maybe seen as implicitly adding noise by only outputting based on a randomly chosen subset of the mechanisms;therefore this implicit noise is inherently data-dependent on L(D).Lemma 3.1. Consider Problem 1.1, with the privacy allowance m [K]. Consider the data-dependentalgorithm that computes L(D) and then applies RR with probability p. If p = Sub(l), where l {0, 1, . . . , K}is the value of L(D), i.e., the (random) sum of observed outcomes on dataset D, and Sub : {0, 1, . . . , K} is",
  ": end if": "Data-dependent Randomized Response (DaRRM). Does subsampling give optimal utility when m > 1?Inspired by the connection between RR and subsampling, we propose Data-dependent Randomized ResponseMajority (DaRRM) in Algorithm 1, to study optimizing privacy-utility tradeoffs in private majority ensembling.In particular, DaRRM has a non-constant success probability p that is set by a parameterized noise function, which in turn depends on the set of observed outcomes S = {S1, . . . , SK}. In fact, we can show thatDaRRM is general: any reasonable algorithm A, name one whose output is at least as good as a randomguess, can be captured by the DaRRM framework in Lemma 3.3 (see a full proof in Appendix A.4). Wedenote DaRRM instantiated with a specific noise function by DaRRM. Lemma 3.3 (Generality of DaRRM). Let A be any randomized algorithm to compute the majority functiong on S such that for all S, Pr[A(S) = g(S)] 1/2 (i.e. A is at least as good as a random guess). Then,there exists a a general function : {0, 1}K+1 such that if one sets p by (S) in DaRRM, the outputdistribution of DaRRM is the same as the output distribution of A. Designing the Function.With the DaRRM framework, we ask: how to design a good functionthat maximizes the utility? First, we introduce two characteristics of that do not affect the utility, whilesimplifying the analysis and the empirical optimization: (a) A function of the sum of observed samples: Since the observed samples set S is a permutation-invariant set, a sufficient statistic that captures the full state of S is L = Ki=1 Si, the sum ofobserved outcomes. This allows us to reduce (S) = (L). Hence, in the rest of the paper, we focuson : {0, 1, . . . , K} .",
  "2and achieve better or equal expected utility, where the utility is summed over symmetric distributionsof pi": "Note that Sub satisfies both characteristics. Now, recall L(D) and L(D) are the sum of observed outcomeson adjacent datasets D and D. Also, recall pi = Pr[Mi(D) = 1] and pi = Pr[Mi(D) = 1] are the outputprobabilities of the mechanism Mi on D, D. To design a good noise function in DaRRM, we start byderiving conditions for a function such that DaRRM is (m, )-differentially private in Lemma 3.4 (see afull proof in Appendix A.5).",
  "Provable Privacy Amplification": "We theoretically demonstrate that privacy is provably amplified under improved design of in our DaRRMframework. Specifically, we show when the mechanisms are i.i.d. and = 0, we gain privacy amplification bya factor of 2 compared to the nave subsampling baseline by carefully designing . Theorem 4.1 (Provable Privacy Amplification by 2). Consider using DaRRM (Algorithm 1) to solveProblem 1.1, with i.i.d. mechanisms {Mi}Ki=1, i.e., pi = p, pi = p, i [K], the privacy allowance m [K]and = = 0. Let the noise function : {0, 1, . . . , K} be that:if m K+1",
  "Interpretation. First, when m K1": "2is small, the (l) in Theorem 4.1 corresponds to outputting themajority based on subsampling 2m 1 outcomes, from Lemma 3.1. However, the subsampling baseline,whose privacy loss is reasoned through simple composition, would have indicated that one can only outputthe majority based on m outcomes, therefore implying a 2x privacy gain. When m K+1",
  ", the above theoremindicates that we can set a constant = 1, which implies we are optimally outputting the true majority withno noise while still surprisingly ensuring m privacy": "Intuition. This 2x privacy gain is intuitively possible because the majority is only dependent on half of themechanisms outputs, therefore the privacy leakage is also halved. To see this, we start by analyzing the privacycost objective in Eq. 31, where with a careful analysis of its gradient, we show that the maximum indeedoccurs (p, p) = (0, 0) when satisfies certain conditions. Now, when (p, p) 0, note that the probabilityratio of outputting 1 with 2m 1 outcomes is approximately em, where dependence on m follows because theprobability of outputting 1 is dominated by the probability that exactly m mechanisms output 1. To rigorizethis, we derive sufficient conditions for functions that satisfy max(p,p) f(p, p; ) = f(0, 0; ) em 1 asindicated by Lemma 3.4, to ensure DaRRM to be m-differentially private and a more detailed overview andthe full proof can be found in Appendix B.",
  "Optimizing the Noise Function in DaRRM": "Theoretically designing and extending privacy amplification results to the > 0 case is difficult and it islikely that our crafted is far from optimal. On the other hand, one can optimize for such that maximizesthe utility but this involves solving a Semi-infinite Programming problem, due to the infinitely manyprivacy constraints, which are the constraints in the optimization problem necessary to ensure DaRRM withthe optimized satisfy a given privacy loss. Solving a Semi-infinite Programming problem in general isnon-tractable, but we show that in our specific setting this is in fact tractable, proposing a novel learning",
  ", we onlyneed to optimize K+1": "2variables instead of K + 1 variables. T is the distribution from which p1, . . . , pK aredrawn. We want to stress that no prior knowledge about the dataset or the amount of consensus amongthe private mechanisms is required to use our optimization framework. When there is no prior knowledgeabout p1, . . . , pK, T is set to be the uniform distribution for maximizing the expected utility. Note the aboveoptimization problem also enables the flexibility of incorporating prior knowledge about the mechanisms bychoosing a prior distribution T to further improve the utility. Optimizing Over All Algorithms. We want to stress that by solving the above optimization problem,we are indeed optimizing over all algorithms for maximal utility, since we show in Lemma 3.3 DaRRM thatcaptures all reasonable algorithms computing a private majority. Linear Optimization Objective. Perhaps surprisingly, it turns out that optimizing for is a LinearProgramming (LP) problem! Indeed, after expanding the optimization objective in Eq. 2 by the utilitydefinition (see Definition 2.4), optimizing the above objective is essentially same as optimizing:",
  "where l = Pr[L(D) = l], l {0, 1, . . . , K} and observe L(D) PoissonBinomial(p1, . . . , pK). The aboveobjective is linear in . See a full derivation in Appendix C.1": "Although taking the expectation over p1, . . . , pK involves integrating over K variables and this can becomputationally expensive, we discuss how to formulate a computationally efficient approximation of theobjective in Appendix C.2, which we later use in the experiments. Note that the objective only for maximizingthe utility and hence approximating the objective does not affect the privacy guarantee. Reducing Infinitely Many Constraints to A Polynomial Set. The constraints in the optimizationproblem (Eq. 3) is what makes sure the output of DaRRM is m-differentially private. We thus call themthe privacy constraints. Note that the privacy constraints are linear in . Though it appears we need to solve for infinitely many such privacy constraints since pis and pis arecontinuous, we show that through a structural understanding of DaRRM, we can reduce the number of privacyconstraints from infinitely many to exponentially many, and further to a polynomial set. First, we observethe privacy cost objective f is linear in each independent pair of (pi, pi) fixing all (pj, pj), j = i, and hencefinding the worst case probabilities in (pi, pi) given any , (pi , pi ) = arg max(pi,pi) f(p1, . . . , pK, p1, . . . , pK; )is a linear programming (LP) problem. Furthermore, since pi and pi are the probability of outputting 1 fromthe i-th (, )-differentially private mechanism Mi on adjacent datasets, by definition, they are close andlie in a feasible region Fi, which we show has 8 corners if > 0 (and only 4 corners if = 0). This implies(pi , pi ) only happens at one of the corners of Fi, and hence the number of constraints reduces to K8 (and K4 if = 0). Second, observe that l and l in the privacy cost objective f are the pmf of two Poisson Binomialdistributions at l {0, . . . , K}. Notice that the Poisson Binomial is invariant under the permutation of itsparameters, i.e. PoissonBinomial(p1, . . . , pK) has the same distribution as PoissonBinomial((p1, . . . , pK)),under some permutation . Based on this observation, we show the number of constraints can be furtherreduced to O(K7) if > 0 (and O(K3) if = 0). We formalize the two-step reduction of the number ofprivacy constraints in Lemma 5.1 as follows. See a full proof in Appendix C.3. 1 1Practical Limitation. Although the number of constraints is polynomial in K and optimizing in DaRRM is an LP,O(K7) can still make the number of constraints intractably large when K is large. In practice, we observe with the Gurobi",
  "e + 1 )}": "Furthermore, when >0, there exists a finite vector set P of size O(K7) such that if =max{(pi,pi)}Ki=1P f(p1, . . . , pK, p1, . . . , pK; ), then f(p1, . . . , pK, p1 , . . . , pK; ) . When = 0, the size ofP can be reduced to O(K3).",
  "Experiments": "We empirically solve2 the above optimization problem (Eq. 2) using the Gurobi3 solver and first present theshape of the optimized function, which we call opt, and its utility in .1. Then, we demonstratethe compelling effectiveness of DaRRM with an optimized function, i.e., DaRRMopt, in ensembling labelsfor private prediction from private teachers through the application of semi-supervised knowledge transfer forprivate image classification in .2.",
  "Optimized in Simulations": "0.5 m = 1m = 3 05.5110 0.5 m = 5 05.511 m = 7 Support l{0, 1, . . . , K} values Shape of functions 0.0 0.1 0.2 m = 1 0.0 0.1 m = 3 0.00 0.05 0.10 m = 5 0.00 0.05 m = 7 functions",
  "We compare the shape and the error E(DaRRM) of different functions: an optimized opt and thesubsampling Sub as in Lemma 3.14. We also compare against pconst in the classical baseline RR (see": "optimizer, one can optimize for K 41 on a laptop if > 0. But if = 0, since the number of privacy constraints is O(K3),one can optimize for K over 100.2All code for the experiments can be found at the subsampling mechanism from , which enjoys a privacy amplification by a factor of 2, only applies to puredifferential privacy settings (i.e., when = = 0). However, we focus on the more general approximate differential privacy",
  "Section A.1) and E(RR). Here, pconst can be viewed as a constant noise function const(l) = pconst, l {0, 1, . . . , K}; and E(RR) is the same as E(DaRRMconst)": "We present the results with K = 11, = 0.1, = 105 and m {1, 3, 5, 7}. We assume there is no priorknowledge about the mechanisms {Mi}Ki=1, and set the prior distribution from which pis are drawn, T , to bethe uniform distribution, in the optimization objective (Eq. 2) searching for opt. To ensure a fair comparisonagainst the subsampling baseline, we set to be the one by m-fold general composition (see Theorem 2.3),which in this case, is = 1 (1 )m m. We plot each functions over the support {0, 1, . . . , K} andthe corresponding error of each algorithm in . Discussion. In summary, at m = 1, the optimized noise function opt overlaps with sub which correspondsto the subsampling baseline. This agrees with our lower bound on the error in Lemma 3.2, which implies thatat m = 1, subsampling indeed gives the optimal error. When m > 1, the optimized noise function opt hasthe highest probability of outputting the true majority over the support than the functions correspondingto the baselines. This implies DaRRMopt has the lowest error (and hence, highest utility), which is verified onthe bottom set of plots. More results on comparing the DaRRMopt optimized under the uniform T againstthe baselines by general composition (Theorem 2.3) and in pure differential privacy settings (i.e., = = 0)for large K and m can be found in Appendix D.1.1 and D.1.2. Furthermore, we include results optimizing using a non-uniform T prior in Appendix D.1.3.",
  "Q = 1000.64 (0.04)0.93 (0.02)0.96 (0.02)": ":Accuracy of the predicted labels of Q query samples on datasets MNIST (on the left) andFashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of thequery samples from the test dataset. Note each prediction on the query sample is (query, query)-differentially private. With the same per query privacy loss (and hence the same total privacy loss over Qsamples), DaRRMopt achieves the highest accuracy compared to the other two baselines. Semi-supervised Knowledge Transfer. We apply our DaRRM framework in the application of semi-supervised knowledge transfer for private image classification. We follow a similar setup as in PATE Papernotet al. (2017; 2018), where one trains K teachers, each on a subset of a sensitive dataset, and at the inferencetime, queries the teachers for the majority of their votes, i.e., the predicted labels, of a test sample. Eachtime the teachers are queried, there is a privacy loss, and we focus on this private prediction subroutine inthis section. To limit the total privacy loss over all queries, the student model is also trained on a publicdataset without labels. The student model queries the labels of a small portion of the samples in this datasetfrom the teachers and is then trained using semi-supervised learning algorithms on both the labeled andunlabeled samples from the public dataset. Baselines. We want the privacy loss per query of a test sample to the teachers to be (query, query). Thiscan be achieved via two ways: 1) Train K non-private teachers, add Gaussian noise to the number of predictedlabels from the teachers in each output class, and output the majority of the noisy votes. This is exactly theGNMax algorithm from PATE Papernot et al. (2018). 2) Train K (, )-differentially private teachers andoutput the majority of the teachers votes by adding a smaller amount of noise. This can be computed usingDaRRM with an appropriate noise function . We compare the performance of GNMax and DaRRM with two functions: opt (i.e., the optimized ), and Sub (i.e., the subsampling baseline). The overall privacy lossover Q queries to the teachers can be computed by general composition (Theorem 2.3). settings (with > 0) in the experiments, and hence, the subsampling baseline we consider throughout this section is thebasic version without privacy amplification. To see how the subsampling mechanism from with privacy amplificationcompares against the other algorithms, please refer to Appendix D.1.2.",
  ": The privacy loss per query to the teachers and the total privacy loss over Q queries. Note the totalprivacy loss is computed by general composition (see Theorem 2.3), where we set = 0.0001": "Discussion. shows DaRRMopt achieves the highest accuracy (i.e., utility) compared to the twobaselines on both datasets. First, comparing to DaRRMSub, we verify that subsampling does not achieve atight privacy-utility tradeoff, and we can optimize the noise function in DaRRM to maximize the utilitygiven a target privacy loss. Second, comparing to GNMax, the result shows there are regimes where ensemblingprivate teachers gives a higher utility than directly ensembling non-private teachers, assuming the outputsin both settings have the same privacy loss. Intuitively, this is because ensembling private teachers addsfine-grained noise during both training the teachers and aggregation of teachers votes, while ensemblingnon-private teachers adds a coarser amount of noise only to the teachers outputs. This further motivates 5Here, we present results with privacy allowance m = 3 because we think this is a more interesting case. m = 1 is lessinteresting, since one cannot get improvement compared to the subsampling baseline. m close to a K 2 5 is also less interesting,as this case seems too easy for our proposed method (the optimized function is very close to 1, meaning very little noiseneeds to be added in this case). Hence, we pick m = 3, which is a case when improvement is possible, and is also potentiallychallenging for our optimization framework. This is also realistic as most applications would only want to tolerate a constantprivacy overhead. See more results with different privacy allowance ms in this setting in Appendix D.2.2.",
  "Conclusion": "In computing a private majority from K private mechanisms, we propose the DaRRM framework, whichis provably general, with a customizable function. We show a privacy amplification by a factor of 2in the i.i.d. mechanisms and a pure differential privacy setting. For the general setting, we propose antractable optimization algorithm that maximizes utility while ensuring privacy guarantees. Furthermore, wedemonstrate the empirical effectiveness of DaRRM with an optimized . We hope that this work inspiresmore research on the intersection of privacy frameworks and optimization.",
  "Raef Bassily, Om Thakkar, and Abhradeep Thakurta. Model-agnostic private learning via stability. arXivpreprint arXiv:1803.05101, 2018": "Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, BaiwuZhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium on Security andPrivacy (SP), pp. 141159. IEEE, 2021. Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang. Privacyat scale: Local differential privacy in practice. In Proceedings of the 2018 International Conference onManagement of Data, pp. 16551658, 2018.",
  "Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Found. Trends Theor.Comput. Sci., 9(3-4):211407, 2014": "lfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova. Rappor: Randomized aggregatable privacy-preservingordinal response. In Proceedings of the 2014 ACM SIGSAC Conference on Computer and CommunicationsSecurity, CCS 14, pp. 10541067, New York, NY, USA, 2014. Association for Computing Machinery. ISBN9781450329576. doi: 10.1145/2660267.2660348. URL lfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and AbhradeepThakurta.Amplification by shuffling: From local to central differential privacy via anonymity.InProceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 24682479. SIAM,2019. Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by iteration.In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp. 521532. IEEE,2018.",
  "then RR is (m, )-differentially private": "Proof of Lemma A.1. Let x {0, 1} denote the output of RR. Let qx = Pr[L(D) = x] and qx = Pr[L(D) = x],where L(D) = Ki=1 Mi(D), L(D) = Ki=1 Mi(D) and D, D are adjacent datasets. Recall each mechanismMi is (, )-differentially private, and the majority of the outputs of {Mi}Ki=1 is (, )-differentially private.When = 0, using simple composition, = K and = 0. When > 0, using general composition",
  ": A visualization of the above LP problem": "The optimum of any LP problem is at the corners of the feasible region, which is bounded by theoptimization constraints.We plot the feasible region F and the objective of the above LP prob-lem in .Here, (qx, qx ) = arg maxqx,qx h(qx, qx) {(0, 0), (1, 1), (0, ), (, 0), (1 , 1), (1, 1 ), ( 1",
  "A.2Proof of Lemma 3.1": "Lemma A.2 (Restatement of Lemma 3.1). Consider Problem 1.1, with the privacy allowance m [K].Consider the data-dependent algorithm that computes L(D) and then applies RR with probability p. Ifp = Sub(l), where l {0, 1, . . . , K} is the value of L(D), i.e., the (random) sum of observed outcomes ondataset D, and Sub : {0, 1, . . . , K} is",
  "then the majority of m out of K subsampled mechanisms without replacement and the output of our data-dependent RR algorithm have the same distribution": "Proof of Lemma 3.1. Let L = Ki=1 Si be the sum of observed outcomes from K mechanisms. FollowingAlgorithm 3, Jm denotes the m indices chosen uniformly at random from [K] without replacement. Conditionedon L, notice the output of SubMaj follows a hypergeometric distribution. The output probability of SubMaj is",
  "KKi=1 pi|, where g(S) is the probability of the true majority output being 1 as defined in Definition 1.1": "Proof. Consider the setting where Mis are i.i.d., i.e., Pr[Mi(D) = 1] = p, i [K] for some p on anydataset D. Then, it suffices to show E(A) | Pr[g(S)] = 1 p|, because a lower bound in this special casewould indicate a lower bound for the more general case, where pis can be different.",
  "A.4Proof of Lemma 3.3": "Lemma A.4 (Restatement of Lemma 3.3). Let A be any randomized algorithm to compute the majorityfunction g on S such that for all S, Pr[A(S) = g(S)] 1/2 (i.e. A is at least as good as a random guess).Then, there exists a a general function : {0, 1}K+1 such that if one sets p by (S) in DaRRM, theoutput distribution of DaRRM is the same as the output distribution of A. Proof of Lemma 3.3. For some D and conditioned on S, we see that by definition Pr[DaRRM(S) = g(S)] =(S) + (1/2)(1 (S)). We want to set such that Pr[DaRRM(S) = g(S)] = Pr[A(S) = g(S)]. Therefore,we set (S) = 2 Pr[A(S) = g(S)] 1.",
  "A.5Proof of Lemma 3.4": "Lemma A.5 (Restatement of Lemma 3.4). Consider using DaRRM (Algorithm 1) to solve Problem 1.1,let l = Pr[L(D) = l] and l = Pr[L(D) = l], where D and D are adjacent datasets and l {0, . . . , K}.For a noise function : {0, 1, . . . , K} such that (l) = (K l), l, DaRRM is (m, )-differentiallyprivate if and only if for all l, l, the following holds,",
  "and proceed by showing Eq. 49 Eq. 50": "Recall pi = Pr[Mi(D) = 1] and pi = Pr[Mi(D) = 1]. Observe L(D) PoissonBinomial({pi}Ki=1) andL(D) PoissonBinomial({pi}Ki=1). Let Fl = {A : |A| = l, A [K]}, for any l {0, . . . , K}, denote the setof all subsets of l integers that can be selected from [K]. Let Ac = [K] \\ A be As complement set. NoticeFKl = {Ac : A Fl}.",
  "BDetails of : Provable Privacy Amplification": "In this section, we consider Problem 1.1 in the pure differential privacy and i.i.d. mechanisms setting. Thatis, = = 0 and p = pi = Pr[Mi(D) = 1], p = pi = Pr[Mi(D) = 1], i [K]. Our goal is to search for agood noise function such that: 1) DaRRM is m-DP, and 2) DaRRM achieves higher utility than that ofthe baselines (see ) under a fixed privacy loss. Our main finding of such a function is presented inTheorem 4.1, which states given a privacy allowance m [K], one can indeed output the majority of 2m 1subsampled mechanisms, instead of just m as indicated by simple composition. Later, we formally verify inLemma B.11, Section B.3 that taking the majority of more mechanisms strictly increases the utility.",
  ": The feasible region F is plottedas the blue area. The four boundaries areimplied by p, p satisfying -differential pri-vacy": "Roadmap of Proof of Theorem 4.1.Since needsto enable Eq. 54 to be satisfied for all p, p , webegin by showing characteristics of the worst case prob-abilities, i.e., (p, p) = arg max(p,p) f(p, p; ), given any : {0, 1, . . . , K} that is symmetric around K 2 and thatsatisfies the above monotonicity assumption, in Lemma B.1, Sec-tion B.1. We call (p, p) the worst case probabilities, since theyincur the largest privacy loss. Later in Section B.2, we presentthe main proof of Theorem 4.1, where we focus on searchingfor a good that enables f(p, p; ) e 1, based on thecharacteristics of (p, p) in Lemma B.1, to ensure DaRRM ism-differentially private.",
  "(c) 1 p e(1 p), and (d) 1 p e(1 p), where thefour boundaries are derived from the definition of differentialprivacy. Therefore, we only need to search for (p, p) = arg max(p,p)F f(p, p; )": "Next, we show that given satisfying certain conditions, (p, p) can only be on two of the four boundariesof F in Lemma B.1 that is, either p = ep, i.e., on the blue line in , or 1 p = e(1 p), i.e.,on the orange line in .Lemma B.1 (Characteristics of worst case probabilities). For any noise function : {0, 1, . . . , K} that is 1) symmetric around K",
  "+ e , 1]": "To show Lemma B.1, we first show in Lemma B.2 that the search of (p, p) can be refined to one of the fourboundaries of F, via a careful gradient analysis of f(p, p; ) in F, and then show in Lemma B.3 that thesearch of (p, p) can be further refined to two of the four boundaries, due to symmetry of p, p. Lemma B.1directly follows from the two.",
  "pf(p, p; ) < 0(74)": "This implies there is no local minima or local maxima inside the feasible region F. Also recall (p, p) {(0, 0), (1, 1)} are two special cases where (p, p) is at the intersection of two boundaries. Hence, we concludethe worst case probability (p, p) = arg maxp,pF f(p, p; ) is on one of the four boundaries of F thatis, (p, p) satisfy one of the following:",
  "B.2Proof of Privacy Amplification (Theorem 4.1)": "Theorem B.4 (Restatement of Theorem 4.1). Consider using DaRRM (Algorithm 1) to solve Problem 1.1,with i.i.d. mechanisms {Mi}Ki=1, i.e., pi = p, pi = p, i [K], the privacy allowance m [K] and = = 0.Let the noise function : {0, 1, . . . , K} be that:if m K+1",
  ". We first show in Lemma B.5, Section B.2.1 that if m K+1": "2 , setting = 1 suffices to ensure DaRRM to be m-differentially private, and hence one can always output the truemajority of K mechanisms. In contrast, simple composition indicates only when m = K can one outputthe true majority of K mechanisms. Next, we show in Lemma B.10, Section B.2.2 that if m K1 2 , onecan set to be DSub, which corresponds to outputting the majority of 2m 1 subsampled mechanisms(and hence the name Double Subsampling, or DSub). In contrast, simple compositon indicates one canonly output the majority of m subsampled mechanisms to make sure the output is m-differentially private.Theorem 4.1 follows directly from combining Lemma B.5 and Lemma B.10.",
  "given such , i.e., (p, p) = arg max(p,p)F f(p, p; ), which in turn gives us the guarantee of DaRRMbeing m-differentially private": "Roadmap. In this section, we restrict our search of a good that maximizes the utility of DaRRMto in the symmetric form family. To show the main privacy amplification result under a small m inLemma B.10, Section B.2.4, we need a few building blocks, shown in Section B.2.3. We first show inLemma B.7, Section B.2.3 two clean sufficient conditions that if a symmetric form family satisfies, thenDaRRM is m-differentially private, in terms of the expectation of the function applied to Binomial randomvariables. The Binomial random variables appear in the lemma, because recall the sum of the observedoutcomes on a dataset D, L(D), follows a Binomial distribution in the i.i.d. mechanisms setting. Next, weshow a recurrence relationship that connects the expectation of Binomial random variables to Hypergeometricrandom variables in Lemma B.9. This is needed because observe that for functions that makes DaRRMhave the same output as the majority of subsampled mechanisms, the h function is now a sum of pmfs of theHypergeometric random variable. Finally, the proof of the main result under a small m (Lemma B.10) is presented in Section B.2.4, based onLemma B.7 and Lemma B.9. We show in Lemma B.10 that DSub, i.e., the function that enables the outputof DaRRMDSub and outputting the majority of 2m 1 subsampled mechanisms to be the same, belongsto the symmetric form family and satisfies the sufficient conditions as stated in Lemma B.7, implyingDaRRMDSub being m-differentially private.",
  "B.2.3Building Blocks": "Lemma B.7 (Privacy conditions of the symmetric form family functions). Let random variables X Binomial(K1, p), Y Binomial(K1, ep), X Binomial(K1, 1e(1p)) and Y Binomial(K1, p).For a function : {0, 1, . . . , K} that belongs to the symmetric form family (Definition B.6), if also satisfies both conditions as follows:",
  "B.3Comparing the Utility of Subsampling Approaches": "Intuitively, if we subsample 2m 1 mechanisms, the utility is higher than that of the nave subsamplingapproach which outputs the majority based on only m mechanisms. To complete the story, we formallycompare the utility of outputting the majority of 2m1 subsampled mechanisms (Theorem 4.1) and outputtingthe majority of m subsampled mechanisms (simple composition, Theorem 2.2) in the i.i.d. mechanisms andpure differential privacy setting, fixing the output privacy loss to be m. Lemma B.11. Consider Problem 1.1 with i.i.d. mechanisms {Mi}Ki=1, i.e., p = pi = Pr[Mi(D) = 1], p =pi = Pr[Mi(D) = 1], i [K]. Let 1 : {0, 1, . . . , K} , 2 : {0, 1, . . . , K} be two functions thatare both symmetric around K",
  "which approximates integration over a K-dimensional grid WK": "The idea is then to sample points from this K-dimensional grid WK and compute an empirical mean of theintegration based on the sample probabilities for p1, . . . , pK from WK as the approximation of the integrationin the objective. Let (s1, s2, . . . , sK) be randomly sampled probability values from WK and we want to compute (l Kl)for all l based on (p1, . . . , pK) = (s1, . . . , sK). To apply the rectangular rule, since the grid of probabilitiesis K-dimensional, the weight of (l Kl) in the approximate integration is K. Furthermore, observe",
  "that l is the pmf at l from a Poison Binomial distribution in our case, and PoissonBinomial(p1, . . . , pK)dist": "PoissonBinomial((p1, . . . , pK)), where denotes a permutation of p1, . . . , pK anddist. denotes the samedistribution. Hence, with a single probability sample (s1, . . . , sK), we can indeed compute l Kl for eachl at K! points from the grid WK, since they all have the same value. Therefore, we should set the weight ofl Kl in the approximate integration as w = K K!. Furthermore, since the order of (p1, . . . , pK) doesnot affect the objective value, there is a total of ( choose K with replacement) =+K1K:= P differentpoints in the grid WK.",
  "f(p1, . . . , pK, p1, . . . , pK; ) em 1 + 2": "where recall that l = Pr[L(D) = l] is a function of {pi}Ki=1 and l = Pr[L(D) = l] is a func-tion of {pi}Ki=1, l {0, 1, . . . , K} and L(D), L(D) are the sum of observed outcomes on neighboringdatasets D and D. By Lemma 3.4, needs to make the above privacy constraint hold for all possible{(pi, pi)}Ki=1 to make DaRRM (m, )-differentially private. This is equivalent to saying, needs to ensuremax{(pi,pi}Ki=1 f(p1, . . . , pK, p1, . . . , pK; ) em 1 + 2. Notice that the sum of observed outcomes follows a Poisson Binomial distribution, i.e., L(D) PoissonBinomial(p1, . . . , pK) and L(D) PoissonBinomial(p1, . . . , pK). Hence, by the pmf of the Pois-son Binomial distribution6, the privacy cost objective f is linear in each pi and pi, fixing all (pj, pj), j = i.Since each mechanism Mi is (, )-differentially private, by definition, (pi, pi) satisfies all of the following:",
  "pi e(1 pi) + ,1 pi e(1 pi) +": "That is, (pi, pi) lies in a feasible region Fi (see ).Note the constraints on (pi, pi), thatis, the boundaries of Fi, are linear in pi and pi.And so the optimization problem (pi , pi ) =arg max(pi,pi) f(p1, . . . , pK, p1, . . . , pK; ), which finds the worst case probabilities in (pi, pi), is a LinearProgramming (LP) problem in (pi, pi) for i [K]. This implies (pi , pi ) has to be on one of the eight cornersof Fi that is (pi , pi ) {(0, 0), (1, 1), (0, ), (, 0), (1 , 1), (1, 1 ), ( e+",
  "e+1, e+": "e+1 )} := C.Since all (pi, pi) and (pj, pj), for i = j, are independent, we can search for the worst case probabilities bysearching for (pi , pi ) C, instead of searching for (pi, pi) Fi, i [K]. Therefore, the infinitely manyprivacy constraints are now reduced to only 8K to optimize for the best function that maximizes the utilityof DaRRM, while ensuring the output is m-differentially private.",
  "Similarly, PoissonBinomial(p1, . . . , pK)dist. PoissonBinomial((p1, . . . , pK))": "The above observation implies if we have one privacy constraint f(p1 = v1, . . . , pK = vK, p1 = v1, . . . , pK =vK; ) em 1 + 2, for some {(vi, vi)}Ki=1 CK, then any privacy constraint f(p1 = s1, . . . , pK = sK, p1 =s1, . . . , pK = sK; ) em 1 + 2, where (s1, . . . , sK) = 1(v1, . . . , vK), (s1, . . . , sK) = (v1, . . . , vK), forpermutations 1 and 2, is redundant. Therefore, there is a vector set P, where each probability vector (p1, . . . , pK, p1, . . . , pK) in P is constructed bysetting (p1, p1), (p2, p2), . . . , (pK, pK) = (v1, v2, . . . , vK), where vi C, i [K], such that vectors constructedby (p1, p1), (p2, p2), . . . , (pK, pK) = (v1, v2, . . . , vK) is not in P. Note |P| = (8 chooses K with replacement)=K+81K= O(K7). If we can restrict our search for the worst case probabilities to this set P thatis, solving for := max{(pi,pi)}Ki=1P f(p1, . . . , pK, p1, . . . , pK; ), then f(p1, . . . , pK, p1 , . . . , pK; ) . Thisimplies we only need O(K7) privacy constraints to optimize for the best noise function in DaRRM, whilemaking sure DaRRM is m-differentially private.",
  "Error": "Actual: pi Uniform([0, 0.1]) K=11, m=5, =0.1 :Comparison of the shape and E(DaRRM) of different functions: 1) optimized under prior TU, 2) optimized under prior TP , 3) Sub (corresponding to the subsampling baseline) and 4) const (correspondingto the RR baseline). Here, K = 11, m {3, 5}, = 0.1. Observe that if the prior TP used in optimizing iscloser to the actual distribution of pis, there is additional utility gain (i.e., decreased error); otherwise, weslightly suffer a utility loss (i.e., increased error), compared to optimize under the TU prior. Furthermore,regardless of the choice of the prior distribution T in optimizing , DaRRM with an optimized achieves alower error compared to the the baselines.",
  "Setting 2.K = 101, m {10, 20, 30, 40, 60, 80}": "0.5 m = 10m = 20 0.5 m = 30m = 40 050.51010 0.5 m = 60 050.5101 m = 80 Support l{0, 1, . . . , K} values Shape of functions 0.00 0.05 0.10 m = 10 0.00 0.05 0.10 m = 20 0.00 0.05 0.10m = 30 0.00 0.05 m = 40 0.00 0.02 0.04 m = 60 0.00 0.01 0.02 m = 80 functions",
  "D.1.3Comparison Using Different Prior Distributions": "When optimizing that maximizes the utility in DaRRM, recall that the objective takes an expectationover pis for pi T , where T is some distribution and pi = Pr[Mi(D) = 1]. The previous experimentsassume we do not have access to any prior knowledge about pis and hence T is the uniform distribution, i.e.,Uniform(). However, when one has knowledge about the mechanisms, one can set a proper prior T tofurther maximize the utility of DaRRM. In this section, let TU denote Uniform() and we present results considering a different prior distribution,which we call TP , as follows. Suppose our prior belief is that each mechanism Mi has a clear tendency towardsvoting 0 or 1, i.e., pi is far from 0.5. Let TP be Uniform([0, 0.3] [0.7, 1]).",
  ". Since the smaller 2 is, the higher the utility, we perform a grid search over [min, 500],": "with discretized values of equal distance 0.5, to find the minimum 2min. For the (m, m) values used inthe experiments, we observe 2 decreases first and then increases as increases, as shown in . The and min values in the RDP bound of Gaussian noise to compute the privacy loss of GNMaxs output weuse in the experiments are presented in . Dataset: MNIST Dataset: Fashion-MNIST",
  "A Note on the Data-dependent Privacy Loss Bound": "Papernot et al. (2018) gives a potentially tighter data-dependent bound on the privacy loss using GNMaxto output the majority of non-private teacherss votes. We give a clean pseudo-code on computing thedata-dependent privacy loss bound in Algorithm 6, based on the lemmas and theorems in Papernot et al.(2018). Given privacy parameters , and the teacher votes per class {ni}Ci=1 for C classes, the data-dependentbound can be empirically evaluated and compared against the Gaussian privacy loss bound. The smallerone is the final privacy loss. We empirically find that the condition of the data-dependent bound (line 8in Algorithm 6) is not satisfied when K and the number of classes C are small, e.g., K = 11, C = 2 as inour case, even if all teachers agree on the same output. And so in the experiments, we can only apply theGaussian privacy loss bound (line 14).",
  "Q = 1000.56 (0.04)0.89 (0.04)0.91 (0.04)": ":Accuracy of the predicted labels of Q query samples on datasets MNIST (on the left) andFashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of thequery samples from the test dataset. Note each prediction on the query sample is (total, total)-differentiallyprivate. Note in this case where m = 1, by Lemma 3.2, subsampling achieves the optimal error/utility. Hence,there is not much difference in terms of accuracy between DaRRMSub and DaRRMopt as expected.",
  "Q = 1000.72 (0.06)0.97 (0.01)0.97 (0.01)": ":Accuracy of the predicted labels of Q query samples on datasets MNIST (on the left) andFashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of thequery samples from the test dataset. Note each prediction on the query sample is (total, total)-differentiallyprivate. With the same per query privacy loss (and hence the same total privacy loss over Q samples),DaRRMopt achieves the highest accuracy compared to the other two baselines.",
  "Q = 1000.79 (0.03)0.96 (0.02)0.96 (0.02)": ": Accuracy of the predicted labels of Q query samples on datasets MNIST (on the left) andFashion-MNIST (on the right). We report the mean and one std. in parentheses over 10 random draws of thequery samples from the test dataset. Note each prediction on the query sample is (total, total)-differentiallyprivate. With the same per query privacy loss (and hence the same total privacy loss over Q samples),DaRRMopt achieves the highest accuracy compared to the other two baselines."
}