{
  "Abstract": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models for the accuraterendition of specific elements like distinct characters or unique styles in generated images.Nonetheless, existing methods face challenges in effectively composing multiple LoRAs,especially as the number of LoRAs to be integrated grows, thus hindering the creation ofcomplex imagery. In this paper, we study multi-LoRA composition through a decoding-centricperspective. We present two training-free methods: LoRA Switch, which alternates betweendifferent LoRAs at each denoising step, and LoRA Composite, which simultaneouslyincorporates all LoRAs to guide more cohesive image synthesis. To evaluate the proposedapproaches, we establish ComposLoRA, a new comprehensive testbed as part of this research.It features a diverse range of LoRA categories with 480 composition sets. Utilizing anevaluation framework based on GPT-4V, our findings demonstrate a clear improvementin performance with our methods over the prevalent baseline, particularly evident whenincreasing the number of LoRAs in a composition. The code, benchmarks, LoRA weights,and all evaluation details are available on our project website.",
  "Introduction": "In the dynamic realm of generative text-to-image models (Ho et al., 2020; Rombach et al., 2022; Sahariaet al., 2022; Ramesh et al., 2022; Ruiz et al., 2023; Sohn et al., 2023), the integration of Low-Rank Adaptation(LoRA) (Hu et al., 2022) stands out for its ability to fine-tune image synthesis with remarkable precision andminimal computational load. LoRA excels by specializing in one element such as a specific character, aparticular clothing, a unique style, or other distinct visual aspects and being trained to produce diverse andaccurate renditions of this element in generated images. For instance, users could customize their LoRA modelsto generate various images of themselves, achieving an array of personalized and realistic representations.",
  "Anime Style": ": Multi-LoRA composition techniques effectively blend different elements such as characters, clothing,and objects into a cohesive image. Unlike the conventional LoRA Merge approach (Ryu, 2023), which canlead to detail loss and image distortion as more LoRAs are added, our methods retain the accuracy of eachelement and the overall image quality. The application of LoRA not only showcases its adaptability and precision in image generation but alsoopens new avenues in customized digital content creation, revolutionizing how users interact with and utilizegenerative text-to-image models for creating tailored visual content. However, an image typically embodies a mosaic of various elements, making compositionality key tocontrollable image generation (Tenenbaum, 2018; Huang et al., 2023b). In pursuit of this, the strategy ofcomposing multiple LoRAs, each focused on a distinct element, emerges as a feasible approach for advancedcustomization. This technique enables the digitization of complex scenes, such as virtual try-ons, mergingusers with clothing in a realistic fashion, or urban landscapes where users interact with meticulously designedcity elements. Prior investigations into multi-LoRA compositions have explored the context of pre-trainedlanguage models (Zhang et al., 2023a; Huang et al., 2023a) or stable diffusion models (Ryu, 2023; Shah et al.,2023). These studies aim to merge multiple LoRA models to synthesize a new LoRA model by trainingcoefficient matrices (Huang et al., 2023a; Shah et al., 2023; Wu et al., 2024) or through the direct additionor subtraction of LoRA weights (Ryu, 2023; Zhang et al., 2023a). Nevertheless, these approaches centeredon weight manipulation could destabilize the merging process as the number of LoRAs grows (Huang et al.,2023a) and also overlook the interaction between LoRA models and base models. This oversight becomesparticularly critical in diffusion models, which depend on sequential denoising steps for image generation.Ignoring the interplay between LoRAs and these steps can result in misalignments in the generative process,as shown in , where a merged LoRA model fails to preserve the full complexity of all desired elements,leading to distorted or unrealistic images. In this paper, we delve into multi-LoRA composition from a decoding-centric perspective, keeping all LoRAweights intact. We present two training-free approaches that utilize either one or all LoRAs at each decodingstep to facilitate compositional image synthesis. Our first approach, LoRA Switch, operates by selectivelyactivating a single LoRA during each denoising step, with a rotation among multiple LoRAs throughout thegeneration process. For instance, in a virtual try-on scenario, LoRA Switch alternates between a characterLoRA and a clothing LoRA at successive denoising steps, thereby ensuring that each element is renderedwith precision and clarity. In parallel, we propose LoRA Composite, a technique that draws inspirationfrom classifier-free guidance (Ho & Salimans, 2022). It involves calculating unconditional and conditionalscore estimates derived from each respective LoRA at every denoising step. These scores are then averagedto provide balanced guidance for image generation, ensuring a comprehensive incorporation of all elements.Furthermore, by bypassing the manipulation on the weight matrix but directly influencing the diffusionprocess, both methods allow for the integration of any number of LoRAs and overcome the limitations ofrecent studies that typically merge only two LoRAs (Shah et al., 2023).",
  "Published in Transactions on Machine Learning Research (11/2024)": "Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.Ziplora: Any subject in any style by effectively merging loras.CoRR, abs/2311.13600, 2023.doi:10.48550/ARXIV.2311.13600. URL Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervisedlearning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei (eds.), Proceedingsof the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,volume 37 of JMLR Workshop and Conference Proceedings, pp. 22562265. JMLR.org, 2015.URL Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip Krishnan.Styledrop: Text-to-image generation in any style. CoRR, abs/2306.00983, 2023. doi: 10.48550/ARXIV.2306.00983. URL Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. In 9th International Conferenceon Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.URL Josh Tenenbaum. Building machines that learn and think like people. In Elisabeth Andr, Sven Koenig, MehdiDastani, and Gita Sukthankar (eds.), Proceedings of the 17th International Conference on AutonomousAgents and MultiAgent Systems, AAMAS 2018, Stockholm, Sweden, July 10-15, 2018, pp. 5. InternationalFoundation for Autonomous Agents and Multiagent Systems Richland, SC, USA / ACM, 2018. URL Zhendong Wang, Yifan Jiang, Yadong Lu, Yelong Shen, Pengcheng He, Weizhu Chen, Zhangyang Wang,and Mingyuan Zhou. In-context learning unlocked for diffusion models. CoRR, abs/2305.01115, 2023. doi:10.48550/ARXIV.2305.01115. URL",
  "To summarize, our key contributions are threefold:": "We introduce the first investigation of multi-LoRA composition from a decoding-centric perspective,proposing LoRA Switch and LoRA Composite. Our methods overcome existing constraints on thenumber of LoRAs that can be integrated, offering enhanced flexibility and improved quality in composableimage generation. Our work establishes ComposLoRA, a comprehensive testbed tailored to this research area, featuring sixvaried categories of LoRAs and 480 composition sets. Addressing the absence of standardized metrics, wepresent an evaluator built upon GPT-4V, setting a new benchmark for assessing both image quality andcompositional efficacy. Through extensive automatic and human evaluations, our findings reveal the superior performance ofthe proposed methods compared to the prevalent LoRA merging approach. Additionally, we provide anin-depth analysis of different multi-composition methods and evaluation frameworks.",
  "Composable Text-to-Image Generation": "Composable image generation, a key aspect of digital content customization, involves creating images thatadhere to a set of pre-defined specifications (Liu et al., 2023). Existing research in this domain primarilyfocuses on the following approaches: enhancing compositionality with scene graphs or layouts (Johnson et al.,2018; Yang et al., 2022; Gafni et al., 2022), modifying the generative process of diffusion models to align withthe underlying specifications (Feng et al., 2023; Huang et al., 2023c;b), multi-concept customization (Kumariet al., 2023; Han et al., 2023; Gu et al., 2023; Kwon et al., 2024; Kong et al., 2024), or composing a series ofindependent models that enforce desired constraints (Du et al., 2020; Liu et al., 2021; Nie et al., 2021; Liuet al., 2022; Li et al., 2023; Du et al., 2023). However, these methods typically operate at the concept level, where generative models excel in creatingimages based on broader categories or general concepts. For example, a model might be prompted to generatean image of a woman wearing a dress, and can adeptly accommodate variations in the textual description,such as changing the color of the dress. Yet, they struggle to accurately render specific, user-defined elements,like lesser-known characters or unique dress styles. Another line of work that can compose user-defined objectsinto images (Huang et al., 2023c; Ruiz et al., 2023). However, these methods require extensive fine-tuningand do not perform well on multiple objects. Therefore, we introduce learning-free instance-level compositionapproaches utilizing LoRA, enabling the precise assembly of user-specified elements in image generation.",
  "Diffusion Models.Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Dhariwal & Nichol,": "2021; Song et al., 2021; Nichol et al., 2022) represent a class of generative models adept at crafting datasamples from Gaussian noise through a sequential denoising process. They build upon a sequence of denoisingautoencoders that estimate the score of a data distribution (Hyvrinen, 2005). Given an image x, the encoderE is used to map x into a latent space, thus yielding an encoded latent z = E(x). The diffusion processintroduces noise to z, resulting in latent representation zt with different noise levels over timestep t T .",
  "Classifier-Free Guidance.In diffusion-based generative modeling, classifier-free guidance (Ho & Salimans,": "2022) balances the trade-off between the diversity and quality of the generated images, particularly in scenarioswhere the model is conditioned on classes or textual descriptions. For the text-to-image task, it operatesby directing the probability mass towards outcomes where the implicit classifier p(c|zt) predicts a highlikelihood for the textual conditioning c. This necessitates the diffusion models to undergo a joint trainingparadigm for both conditional and unconditional denoising. Subsequently, during inference, the guidance scales 1 is used to adjust the score function e(zt, c) by moving it closer to the conditional estimation e(zt, c)and further from the unconditional estimation e(zt), enhancing the conditioning effect on the generatedimages, as formalized in the following expression:",
  "e(zt, c) = e(zt) + s (e(zt, c) e(zt)).(2)": "LoRA Merge.Low-Rank Adaptation (LoRA) approach (Hu et al., 2022) enhances parameter efficiency byfreezing the pre-trained weight matrices and integrating additional trainable low-rank matrices within theneural network. This method is founded on the observation that pre-trained models exhibit low intrinsicdimension (Aghajanyan et al., 2021). Concretely, for a weight matrix W Rnm in the diffusion model, the introduction of a LoRA module involves updating W to W , defined as W = W + BA. Here,B Rnr and A Rrm are matrices of a low-rank factor r, satisfying r min(n, m). The concept ofLoRA Merge (Ryu, 2023) is realized by linearly combining multiple LoRAs to synthesize a unified LoRA,subsequently plugged into the diffusion model. Formally, when introducing k distinct LoRAs, the consequentupdated matrix W in is given by:",
  "i=1wi BiAi,(3)": "where i denotes the index of the i-th LoRA, and wi is a scalar weight, typically a hyperparameter determinedthrough empirical tuning. LoRA Merge has emerged as a dominant approach for presenting multiple elementscohesively in an image, offering a straightforward baseline for various applications. However, merging toomany LoRAs at once can destabilize the merging process (Huang et al., 2023a), and it completely overlooksthe interaction with the diffusion model during the generative process, resulting in the deformation of thehamburger and fingers in .",
  "Multi-LoRA Composition through a Decoding-Centric Perspective": "To address the above issues, we base our approach on the denoising process and investigate how to performcomposition while maintaining the LoRA weights unchanged. This is specifically divided into two perspectives:in each denoising step, either activate only one LoRA or engage all LoRAs to guide the generation. LoRA Switch (LoRA-s).To explore activating a single LoRA in each denoising step, we present LoRASwitch. This method introduces a dynamic adaptation mechanism within diffusion models by sequentiallyactivating individual LoRAs at designated intervals throughout the generation process. As illustrated in, each LoRA is represented by a unique color corresponding to a specific element, with only oneLoRA engaged per denoising step.",
  "W t = W + wi BiAi.(4)": "In this formula, i indicates the index of the currently active LoRA, iterating from 1 to k. The floor function guarantees the integer value of i is appropriately computed for t. The resulting weight matrix W t isupdated to reflect the contribution from the active LoRA. By selectively enabling one LoRA at a time, LoRASwitch ensures focused attention to the details pertinent to the current element, thus preserving the integrityand quality of the generated image throughout the process. LoRA Composite (LoRA-c).To explore incorporating all LoRAs at each timestep without mergingweight matrices, we propose LoRA Composite (LoRA-c), an approach grounded in the Classifier-FreeGuidance paradigm. Previous research has primarily focused on modifying CFG to enable diffusion modelsto emphasize textual concepts (Liu et al., 2022; Du et al., 2023; Sohn et al., 2023). In contrast, our methodextends this by enabling CFG to condition on LoRAs, facilitating the generation of images that reflect specificelements or instances rather than abstract concepts. LoRA-c involves calculating both unconditional andconditional score estimates for each LoRA individually at every denoising step. By aggregating these scores,the technique ensures balanced guidance throughout the image generation process, facilitating the cohesiveintegration of all elements represented by different LoRAs. Formally, with k LoRAs in place, let i denote the parameters of the diffusion model e after incorporatingthe i-th LoRA. The collective guidance e(zt, c) based on textual condition c is derived by aggregating thescores from each LoRA, as depicted in the equation below:",
  "i=1wi ei(zt) + s (ei(zt, c) ei(zt)).(5)": "Here, wi is a scalar weight allocated to each LoRA, intended to adjust the influence of the i-th LoRA. In thispaper, we set wi to 1, giving each LoRA equal importance. LoRA-c assures that every LoRA contributeseffectively at each stage of the denoising process, addressing the potential issues of robustness and detailpreservation that are commonly associated with merging LoRAs. Overall, we are the first to adopt a decoding-centric perspective in multi-LoRA composition, steering clear ofthe instability inherent in weight manipulation on LoRAs. Our study introduces two training-free methodsfor activating either one or all LoRAs at each denoising step, with their comparative analysis presented inSections 4.2 and 4.3.1.",
  "Experimental Setup": "ComposLoRA Testbed.Due to the absence of standardized benchmarks and automated evaluation met-rics, existing studies involving evaluation for composable image generation lean heavily on quantitativeanalysis (Huang et al., 2023b; Wang et al., 2023) and human effort (Shah et al., 2023), which also limitsthe advancements of multi-LoRA composition. To bridge this gap, we introduce a comprehensive testbedComposLoRA designed to facilitate comparative analysis of various composition approaches. This testbedbuilds upon a collection of public LoRAs1, which are extensively shared and recognized as essential plug-inmodules in this field. The selection of LoRAs for this testbed adheres to the following criteria:",
  "Scores:Image 1: Composition Quality: 5/10, Image Quality: 7/10Image 2: Composition Quality: 10/10, Image Quality: 10/10": "Consequently, we curate two unique subsets ofLoRAs representing realistic and anime styles.Each subset comprises a variety of elements: 3characters, 2 types of clothing, 2 styles, 2 back-grounds, and 2 objects, culminating in a total of22 LoRAs in ComposLoRA. In constructing com-position sets, we strictly follow a crucial princi-ple: each set must include one character LoRAand avoid duplication of element categories toprevent conflicts. Thus, the ComposLoRA evalu-ation incorporates a total of 480 distinct compo-sition sets. This includes 48 sets comprising 2LoRAs, 144 sets with 3 LoRAs, 192 sets featur-ing 4 LoRAs, and 96 sets containing 5 LoRAs.Key features for each LoRA are manually anno-tated and serve dual purposes: they act as inputprompts for the text-to-image models to gen-erate images, and also provide reference pointsfor subsequent evaluations using GPT-4V. De-tailed descriptions of each LoRA can be foundin in the Appendix. Comparative Evaluation with GPT-4V.While existing metrics can calculate the align-ment between text and images (Hessel et al.,2021; Ku et al., 2023), they fall short in assess-ing the intricacies of specific elements withinan image and the quality of their composition.Recently, multimodal large language modelslike GPT-4V (OpenAI, 2023a;b) have signifi-cant progress and promise in various multimodaltasks, underscoring their potential in evaluatingimage generation tasks (Lin et al., 2023; Zhanget al., 2023b). In our study, we leverage GPT-4Vs capabilities to serve as an evaluator forcomposable image generation. Specifically, we employ a comparative evaluation method, utilizing GPT-4V to rate generated images acrosstwo dimensions: composition quality and image quality. We utilize a 0 to 10 scoring scale, with higher scoresindicating superior quality. GPT-4V is provided with a prompt that includes the essential features of theelements to be composed, the criteria for scoring in the two dimensions, and the format for the expectedoutput. The complete evaluation prompts and results are available in Tables 7 and 8 in Appendix. Thisexperimental setup allows us to compare the efficacy of each of the two proposed methods against the LoRAMerge approach. Additionally, we examine how GPT-4V-based scoring aligns with human judgment in.2 and explore the potential biases of using it as an evaluator in .3.3.",
  "Results on ComposLoRA": "GPT-4V-based Evaluation.We first present the comparative evaluation results using GPT-4V. Thisevaluation involves scoring the performance of LoRA-s versus LoRA Merge, and LoRA-c versus LoRAMerge across two dimensions, as well as determining the winner based on these scores. Specific scores andwin rates are illustrated in , leading to several key observations:",
  ": Results of comparative evaluation on ComposLoRA using GPT-4V": "Our proposed method consistently outperforms LoRA Merge across all configurations and in both dimensions,with the margin of superiority increasing as the number of LoRAs grows. For instance, as shown in(a), the score advantage of LoRA Switch escalates from 0.04 with 2 LoRAs to 1.32 with 5LoRAs. This trend aligns with the win rate observed in (c), where the win rate approaches 70%when composing 5 LoRAs.",
  "composition quality surpasses that of LoRA-c by 14% (69% vs. 55%). Conversely, for image quality,LoRA-cs win rate is 10% higher than that of LoRA-s (56% vs. 46%)": "The task of compositional image generation remains highly challenging, especially as the number of elementsto be composed increases. According to GPT-4Vs scoring, the average score for composing 2 LoRAs isabove 8.5, but it sharply declines to around 6 for compositions involving 5 LoRAs. Hence, despite theconsiderable improvements our methods offer, there is still substantial room for further research in thefield of compositional image generation.",
  "Human Evaluation.To complement our results,we conduct a human evaluation to assess the ef-fectiveness of different methods and validate theefficacy of the evaluators": "Two graduate students rate 120 images on com-positional and image quality using a 1-5 Likertscale: 1 signifies complete failure, 2-4 representssignificant, moderate, and minor issues, respec-tively, while 5 denotes perfect execution. To ensureconsistency, the annotators initially pilot-score 20images to standardize their understanding of thecriteria.The results, summarized in the uppersection of , align with GPT-4Vs findings,confirming our methods outperform LoRA Merge with LoRA Switch excelling in compositionand LoRA Composite in image quality. Furthermore, we analyze the Pearson correlations between human evaluations and scores derived from GPT-4Vand CLIPScore (Hessel et al., 2021), with results presented in the lower section of . This comparisonreveals that CLIPScores evaluations fall short in assessing specific compositional and quality aspects dueto its inability to discern the nuanced features of each element. In contrast, the evaluator we adopt showssubstantially higher correlations with human judgments, affirming the validity of our evaluation framework.",
  ": Analysis on image styles. In general, LoRA-sis more adept at realistic styles, while LoRA-c has betterperformance in anime styles": "To explore the impact of image style, we sepa-rately evaluate the performance of methodson realistic and anime-style subsets withinComposLoRA. The win rate results, presentedin , reveal distinct tendencies for eachmethod. Our observations reveal that, while LoRA-smay not excel in image quality compared toLoRA-c, it demonstrates comparable perfor-mance in this dimension within the realisticstyle subset, while maintaining a significantedge in composition quality. In contrast, inthe anime-style subset, LoRA-c, shows a per-formance on par with LoRA-s in compositionquality, while notably surpassing it in imagequality. These findings suggest that LoRA-S is",
  "How Does the Step Size and Order of LoRA Activation Affect LoRA Switch?": "To identify the optimal configuration for LoRA Switch, we examine the influence of two crucial hyperpa-rameters: the sequence in which LoRAs are activated and the interval between each activation. Our findings,depicted in (a), show that overly frequent switching, such as changing LoRAs at every denoising step,leads to distortions in generated images and suboptimal performance. The efficiency of the LoRA Switchimproves progressively with increased step size, reaching peak performance at = 5.",
  "(b) Activation Order": ": Analysis of the number of denoising steps to switchLoRA and the activation order for LoRA Switch. In (b),Character indicates that the character LoRA is activated first,with the rest being activated randomly. Moreover, our analysis underscores thatthe initial choice of LoRA in the activa-tion sequence clearly influences overallperformance, while alterations in the sub-sequent order have minimal impact. Acti-vating the character LoRA first leads tothe best performance, as demonstratedin (b). In contrast, starting withclothing, background, or object LoRAsyields results comparable to a completelyrandomized sequence.Notably, begin-ning with the style LoRA leads to a no-ticeable performance drop, even fallingslightly below a random order. This ob-servation underlines the critical role ofprioritizing core image elements in theinitial stage of the generation process toenhance both the image and composi-tional quality for LoRA Switch. While the step size for switching LoRAs proves to be a crucial factor in achieving optimal performance in ourexperiments, we also explore the potential of dynamic strategies for step size adjustment throughout thedenoising process. Specifically, we design and evaluate three strategies for dynamically adjusting the step sizein LoRA-Switch:",
  "Does GPT-4V Exhibit Bias as an Evaluator?": "While GPT-4V has demonstrated utility in evaluating various image generation tasks (Lin et al., 2023; Zhanget al., 2023b), our analysis uncovers a notable positional bias in its comparative evaluations. We investigatethis potential bias by swapping the positions of images generated by different methods before inputting themto GPT-4V, and the results are illustrated in .",
  "LoseTieWin": ": Positional bias analysis for GPT-4V-based evaluation. Ineach subfigure, the left side of the orange line compares LoRA-swith Merge, and the right side contrasts LoRA-c with Merge.Merge First indicates that the image produced by LoRA Mergeis the first image input during the comparative evaluation. In the comparison of LoRA-s versusLoRA Merge, when the image gener-ated by Merge is presented first (MergeFirst), the win rate for LoRA-s in com-position quality stands at 60%.How-ever, this win rate declines to 51%when LoRA-ss image is the first input(LoRA-S First). Similarly, LoRA-cswin rate decreases from 52% to 42%, sug-gesting that GPT-4V tends to favor thesecond image input in terms of compo-sition quality. Intriguingly, the oppositetrend is observed in image quality, wherethe second image tends to receive a higherscore. These results indicate a significantpositional bias in GPT-4Vs evaluation,varying with the dimension and the posi-tion of the images. To mitigate this biasin our study, the comparative evaluationresults reported in this paper are averaged across both input orders.",
  ": Case study on composing 3 LoRAs in the anime style": "merged LoRAs. This is due to LoRA-c merging each LoRA with the base model to calculate scores, whichare then averaged. The inherent design of LoRA prevents the pre-computation of the base model. To addressthis, we propose two potential solutions: 1) Integrating advanced techniques with fewer denoising steps, and2) a combination of LoRA-s and LoRA-c.",
  "LCM-LoRALoRA Switch4239198LoRA Composite4441158": "First, we conduct experiments on 2 LoRAs using the same setup as described in the main text, with resultsshown in . Remarkably, our methods not only outperform the baseline but also show even greateradvantages when integrated with these inference-accelerating techniques. This adaptation significantly reducesthe required number of denoising steps to 4-8, effectively addressing the increased computational demands ofLoRA-c. Consequently, the generation times across all three methods are now comparably short, takingonly a few seconds. To further explore the potential of integrating our method with fewer denoising steps, we conduct additionalexperiments using LCM-LoRA with an increased number of LoRAs. These experiments aim to provide deeperinsights into how our approach performs as the complexity of multi-LoRA composition increases. The results,shown in , reflect the composition quality scores under this setup. For these experiments, we use 8",
  "LoRA Composite8.136.074.53": "Our methods exhibit a more pronounced advantage over the baseline in this fewer-steps setting.Theimprovement is particularly noticeable when the number of LoRAs increases, demonstrating the robustnessof our approach even when integrated with models requiring fewer denoising steps. However, it is importantto note that all methods experience a substantial drop in absolute scores when combined with models likeLCM-LoRA that employ fewer denoising steps. For instance, the composition quality score for LoRA-c with5 LoRAs is initially 6.56 for 200 denoising steps, but with 4 LoRAs in this setting, the score drops to 5.08.This finding suggests that despite the improved performance of our method, multi-LoRA composition remainsa challenging task, especially when fewer denoising steps are used, even with the latest integration techniques.",
  "Combination of LoRA-s and LoRA-c": "To further enhance efficiency, we propose combining LoRA-s and LoRA-c. LoRA-s activates at the LoRAstage before each denoising step, while LoRA-c is applied to the CFG during the denoising process. Thesedesign principles are complementary. A practical integration method involves selecting a subset of LoRAsto activate (ranging from one to all) for each denoising step, following the LoRA-s strategy. This subsetwould then utilize all its LoRAs during the denoising phase, adhering to the LoRA-c strategy. For LCMand other related applications, combining LoRA-s and LoRA-c can enhance efficiency without additionalmodifications. For example, in a 1-step scenario, all LoRAs can be activated and applied through CFG (asper LoRA-c). In a 2-step scenario, half of the LoRAs can be activated at each step and then applied throughCFG, blending LoRA-s and LoRA-c.",
  "ZipLoRA8.509.10": "Although our proposed methods are training-free, we also compare them with the fine-tuning approachZipLoRA as a baseline for reference. ZipLoRA is based on SDXL and focuses on merging two LoRAs, so weconduct our comparisons under this setup. Specifically, we randomly select publicly available SDXL LoRAsfrom HuggingFace and create 10 composition sets, combining character + style and character + object forthe experiments. All results are compared against the LoRA Merge, and the scores are presented in . Since ZipLoRA is specifically designed to merge subject and style LoRAs, it achieves higher scores in thecharacter + style setup compared to our methods, likely due to the benefits of its fine-tuning process. However,",
  "Conclusion": "In this paper, we present the first exploration of multi-LoRA composition from a decoding-centric perspectiveby introducing LoRA-s and LoRA-c that transcend the limitations of current weight manipulation techniques.Through establishing a dedicated testbed ComposLoRA, we introduce scalable automated evaluation metricsutilizing GPT-4V. Our study not only highlights the superior quality achieved by our methods but alsoprovides a new standard for evaluating LoRA-based composable image generation.",
  "Broader Impact Statement": "Our approaches offer advancements in personalized image generation and customized digital content creationby allowing the combination of arbitrary elements. This capability can be applied to various real-worldscenarios, such as virtual try-on and virtual design, leading to positive social impacts. As our method operatesin the inference phase and relies solely on the composition of publicly available checkpoints (base models andLoRAs) without requiring additional training, it avoids any negative impact related to model training. Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the effectivenessof language model fine-tuning.In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.),Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: LongPapers), Virtual Event, August 1-6, 2021, pp. 73197328. Association for Computational Linguistics, 2021.doi: 10.18653/V1/2021.ACL-LONG.568. URL Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.),Advances in Neural Information Processing Systems 34: Annual Conference on Neural InformationProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 87808794, 2021. URL Shihan Dou, Enyu Zhou, Yan Liu, Songyang Gao, Jun Zhao, Wei Shen, Yuhao Zhou, Zhiheng Xi, Xiao Wang,Xiaoran Fan, Shiliang Pu, Jiang Zhu, Rui Zheng, Tao Gui, Qi Zhang, and Xuanjing Huang. Loramoe:Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. CoRR,abs/2312.09979, 2023. doi: 10.48550/ARXIV.2312.09979. URL Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models.In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin(eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural InformationProcessing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generationwith energy-based diffusion models and MCMC. In Andreas Krause, Emma Brunskill, Kyunghyun Cho,Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on MachineLearning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of MachineLearning Research, pp. 84898510. PMLR, 2023. URL",
  "Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022. doi:10.48550/ARXIV.2207.12598. URL": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle,MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in NeuralInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conferenceon Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficientcross-task generalization via dynamic lora composition. CoRR, abs/2307.13269, 2023a. doi: 10.48550/ARXIV.2307.13269. URL Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative andcontrollable image synthesis with composable conditions. In Andreas Krause, Emma Brunskill, KyunghyunCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference onMachine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings ofMachine Learning Research, pp. 1375313773. PMLR, 2023b. URL",
  "Aapo Hyvrinen. Estimation of non-normalized statistical models by score matching. J. Mach. Learn. Res.,6:695709, 2005. URL": "Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In 2018 IEEE Conferenceon Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018,pp. 12191228. Computer Vision Foundation / IEEE Computer Society, 2018. doi: 10.1109/CVPR.2018.00133. URL Zhe Kong, Yong Zhang, Tianyu Yang, Tao Wang, Kaihao Zhang, Bizhu Wu, Guanying Chen, Wei Liu, andWenhan Luo. OMG: occlusion-friendly personalized multi-concept generation in diffusion models. CoRR,abs/2403.10983, 2024. doi: 10.48550/ARXIV.2403.10983. URL Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub:Standardizing the evaluation of conditional image generation models. CoRR, abs/2310.01596, 2023. doi:10.48550/ARXIV.2310.01596. URL Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept cus-tomization of text-to-image diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recog-nition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 19311941. IEEE, 2023.doi:10.1109/CVPR52729.2023.00192. URL Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye, and Fabian Caba Heilbron. Conceptweaver: Enabling multi-concept fusion in text-to-image models. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 88808889. IEEE,2024. doi: 10.1109/CVPR52733.2024.00848. URL Shuang Li, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, and Igor Mordatch. Composing ensemblesof pre-trained models via iterative consensus. In The Eleventh International Conference on LearningRepresentations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023.URL Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, and Lijuan Wang. Designbench: Exploring andbenchmarking DALL-E 3 for imagining visual design. CoRR, abs/2310.15144, 2023. doi: 10.48550/ARXIV.2310.15144. URL Nan Liu, Shuang Li, Yilun Du, Josh Tenenbaum, and Antonio Torralba.Learning to compose vi-sual relations.In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, andJennifer Wortman Vaughan (eds.), Advances in Neural Information Processing Systems 34:An-nual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14,2021, virtual, pp. 2316623178, 2021.URL Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum.Compositional visualgeneration with composable diffusion models. In Shai Avidan, Gabriel J. Brostow, Moustapha Ciss,Giovanni Maria Farinella, and Tal Hassner (eds.), Computer Vision - ECCV 2022 - 17th EuropeanConference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII, volume 13677 of LectureNotes in Computer Science, pp. 423439. Springer, 2022. doi: 10.1007/978-3-031-19790-1\\_26. URL Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, and Antonio Torralba. Unsupervised compositionalconcepts discovery with text-to-image generative models. CoRR, abs/2306.05357, 2023. doi: 10.48550/ARXIV.2306.05357. URL",
  "OpenAI. GPT-4v System Card. 2023b": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125.URL Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and PatternRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1067410685. IEEE, 2022. doi:10.1109/CVPR52688.2022.01042. URL Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE/CVF Conference onComputer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023,pp. 2250022510. IEEE, 2023. doi: 10.1109/CVPR52729.2023.02155. URL",
  "Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient modules witharithmetic operations. CoRR, abs/2306.14870, 2023a. doi: 10.48550/ARXIV.2306.14870. URL": "Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William YangWang, and Linda Ruth Petzold. Gpt-4v(ision) as a generalist evaluator for vision-language tasks. CoRR,abs/2311.01361, 2023b. doi: 10.48550/ARXIV.2311.01361. URL Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, and Pengcheng He. Seeking neural nuggets: Knowledgetransfer in large language models from a parametric perspective. CoRR, abs/2310.11451, 2023. doi:10.48550/ARXIV.2310.11451. URL",
  "Please help me rate both given images on the following evaluation dimensions and criteria:": "Composition Quality:- Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.- Deduct 3 points if any element is missing or incorrectly depicted.- Deduct 1 point for each missing or incorrect feature within an element.- Deduct 1 point for minor inconsistencies or lack of harmony between elements.- Additional deductions can be made for compositions that lack coherence, creativity, or realism. Image Quality:- Score on a scale of 0 to 10, in 0.5 increments, where 10 is the best and 0 is the worst.- Deduct 3 points for each deformity in the image (e.g., extra limbs or fingers, distorted face, incorrect proportions).- Deduct 2 points for noticeable issues with texture, lighting, or color.- Deduct 1 point for each minor flaw or imperfection.- Additional deductions can be made for any issues affecting the overall aesthetic or clarity of the image.",
  "For Image 1:": "Composition Quality:- Elements: All key elements are present: character, background, and object (0 points deducted).- Features within the elements:- The characters hair color is not black as specified (-1 point).- The burger shows a deformed connection which detracts from the quality of the objects depiction (-3 point).- The characters fingers are deformed (-1 point).The bamboo background is accurate, fulfilling the outdoor setting criteria. Despite this, the main issues lie with theincorrect hair color and the deformities in the burger and fingers, leading to deductions. Image Quality:- The image is free from texture, lighting, or color issues (0 points deducted).- Deformities are present in the fingers and the burger, which significantly impacts the images quality. (-3 point).",
  "For Image 2:": "Composition Quality:- Elements: All key elements are present: character, background, and object (0 points deducted).- Features within the elements:- The character has black hair and pink eyes; the forehead is shown, and the burger is depicted accurately (0points deducted).There are no inconsistencies or lack of harmony between elements, and the composition is coherent and aestheticallypleasing."
}