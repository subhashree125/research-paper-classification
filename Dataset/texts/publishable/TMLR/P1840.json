{
  "Abstract": "This paper develops a joint weighted L1- and L0-norm (WL1L0) regularization methodby leveraging proximal operators and translation mapping techniques to mitigate the biasintroduced by the L1-norm in applications to high-dimensional data. A weighting parameter is incorporated to control the influence of both regularizers. Our broadly applicable modelis nonconvex and nonsmooth, but we show convergence for the alternating direction methodof multipliers (ADMM) and the strictly contractive PeacemanRachford splitting method(SCPRSM). Moreover, we evaluate the effectiveness of our model on both simulated andreal high-dimensional genomic datasets by comparing with adaptive versions of the leastabsolute shrinkage and selection operator (LASSO), elastic net (EN), smoothly clippedabsolute deviation (SCAD) and minimax concave penalty (MCP). The results show thatWL1L0 outperforms the LASSO, EN, SCAD and MCP by consistently achieving the lowestmean squared error (MSE) across all datasets, indicating its superior ability to handlinglarge high-dimensional data. Furthermore, the WL1L0-SCPRSM also achieves the sparsestsolution. Julia code for the WL1L0-ADMM and WL1L0-SCPRSM is available at",
  "Introduction": "High-dimensional statistics is a rapidly growing field of research that focuses on statistical analysis in thepresence of a large number of variables or predictors (p), often much larger than the sample size (n).For example, high-throughput measurements in genomics contain thousands or millions of variables, such assingle nucleotide polymorphism (SNP) markers and gene expression data for each individual. In such settings,traditional statistical methods often fail due to issues like overfitting, multicollinearity and computationalcomplexity. In recent years, a number of regularization methods have been developed that impose a penaltyon the size of the regression coefficients, which encourages sparsity and reduces the number of variables inthe model (Fan et al., 2011; Fan & Lv, 2010; Heinze et al., 2018). Sparse learning techniques are essential inanalyzing high-dimensional data for increased prediction accuracy, reduced computational complexity andenhanced interpretability of the results (Bhlmann & Van De Geer, 2011; Giraud, 2015; Wainwright, 2019). Among various sparsity-inducing methods, the L1 regularizer (also known as the least absolute shrinkage andselection operator (LASSO)) stands out for its convex nature and computational efficiency (Tibshirani, 1996).It adds a penalty term to the loss function proportional to the absolute value of the regression coefficients(L1-norm), which tends to shrink the coefficients towards zero and force some coefficients to exactly zero.Shrinking these coefficients helps to avoid overfitting, which can happen when a model memorizes the trainingdata too well and does not perform well on new data. The LASSO improves the accuracy of predictions on",
  "Published in Transactions on Machine Learning Research (11/2024)": "at any point of its domain. Note that both f and h are real polynomial functions, which are semi-algebraicfunctions (Attouch et al., 2013; Bolte et al., 2014). Both ||u||0 and ||u||1 have piecewise linear graphs andare therefore semi-algebraic (see Example 3 and 4 in (Bolte et al., 2014), respectively). Furthermore, consider that g1(u) = ||u||1 and g2 = (1)||u||0. Their proximal operators have piecewiselinear graphs and are perfectly known objects (Attouch et al., 2013; Beck, 2017). The proximal operator forg1(u) = ||u||1, proxg1()(u) = [|u| ]+ sgn(u) (the so-called soft thresholding function) is defined as",
  "Related Work": "In the rapidly evolving landscape of technology and data, prediction has become a cornerstone for makinginformed decisions across various domains.Regularization techniques are pivotal in enhancing the per-formance and generalizability of predictive models, particularly when dealing with complex datasets andhigh-dimensional data. By imposing penalties on the model parameters, regularization helps prevent over-",
  "i |bi|q)1/q ,if 0 < q < ,maxi |bi|,if q = ,": "where i = 1, , p.Here, the ||b||0 is the L0-norm that is the number of nonzero elements in b.It isnoteworthy that the L0-norm does not meet the criteria of a norm, specifically lacking the homogeneityproperty (Beck, 2017). Despite this, the term is widely used in the literature, and for the sake of consistency,we will retain its adoption. Ridge regression, also known as Tikhonov regularization was introduced by Hoerl & Kennard (1970), usesan L2 penalty term that shrinks all the coefficients and reduces their magnitudes. The ridge regression canbe formulated asb = argminb||y Xb||22 + ||b||22,(2) where > 0 is regulazation parameter need to be tuned. This method is particularly effective in addressingmulticollinearity in linear regression models. However, it does not necessarily set any coefficients to zero.Hence, ridge regression does not produce a sparse solution of estimated coefficients. On the other hand,LASSO regressionb = argminb||y Xb||22 + ||b||1,(3)",
  "i1 (bi = 0) .(7)": "Exact optimization of problem (4) with the L0-norm, as defined in (7), is challenging because incorporating(7) into the objective function results in a non-differentiable and non-convex problem. For example, Louizoset al. (2017) propose a method for optimizing a relaxed version of the L0 norm for parametric models usinga distribution called the hard concrete distribution (Maddison et al., 2016), which facilitates gradient-basedoptimization.",
  "c, d = argminc,dy X(c + d)22 + 1c1 + 2d22,(10)": "where the resulting estimator b = c + d (Chernozhukov et al., 2017). The key difference between EN andLAVA is that EN performs variable selection (i.e., is dominated by the L1-norm), whereas LAVA is alwaysdense (i.e., is dominated by the L2-norm). Waldmann (2021) developed a proximal operator algorithm basedon the LAVA regularization method that jointly performs L1- and L2-norm regularization. Ziyin & Wang (2023) propose a method called spred, for optimizing generic differentiable objectives withan L1 constraint using a reparametrization. The method is proposed to effectively bridge the gap betweensparsity in deep learning and conventional statistical learning by providing a principled way to optimize L1 constraints in complex nonlinear settings. For example, one can apply the spred parametrization to thesparse component of bs given the LASSO loss ||y Xbs||22 + 2||bs||1. The equivalent spred loss is then",
  "For any set S Rp and any point b Rp, the distance from b to S is defined as D(b, S) := inf{||mb||, m S}, and D(b, S) = for all b when S =": "A proper closed and coercive function f attains its minimal value over S for a nonempty closed set satisfyingS dom(f) = . Moreover, a closed coercive function possesses a minimizer on any closed set that has anonempty intersection with the domain of the function (Beck, 2017). For an extended real-valued functionf : Rp , the following three claims are equivalent:",
  "Proximal Operators": "Proximal operators are a fundamental concept in optimization, especially for problems involving non-smooth or non-convex functions, which are increasingly common in a wide range of real-world applica-tions (Fukushima & Mine, 1981; Kaplan & Tichatschke, 1998; Parikh & Boyd, 2013). A proximal operator,denoted as proxf(u), aims to find a point closer to u that also minimizes a specific objective function, f(v)in a specific optimization subproblem. This subproblem is assumed to be more manageable to solve thanthe original problem. The proximal operator can be mathematically expressed as",
  "proxf(u) = argminv{f(v) + (1/2)|v u22},(12)": "where u and v are vectors of length p. Here, proxf(u) is a point that compromises between minimizingf and being close to u. Note that the right-hand side of (12) is strongly convex, hence there is a uniqueminimizer for every u Rp. Introducing the parameter > 0 that represents a trade-off parameter between",
  "which represents a translation mapping": "The proof of Lemma 1 is provided in Appendix B.1. In accordance with Lemma 1, one defines a translationfunction as a function that incorporates a standard additive term which is expressed as Tm(u) = f(u+m)m. Another important property arises in the context of separable sum functions f(u, m) = g(u) + h(m),where the proximal operator is written as proxf(u, m) = proxg(u) + proxh(m). For proximal operators inthe framework of L0, please refer to (Attouch et al., 2013; Bolte et al., 2014; Beck, 2017).",
  "where 1 > 0 and 2 > 0 are the regularization parameters": "We formulate our proposed method by building upon the flexible, penalty-based framework introduced in(15) that uses the parameters 1 and 2 to control both the size of the coefficients and the sparsity in a moreflexible and nuanced manner, allowing for a broader range of model behaviors compared to the constantthreshold and the strict subset selection enforced by formulation (14). Here, the exact relationship betweent and 1 and between s and 2 is data-dependent.",
  "WL1L0: c, d = argminc,dy X(c + d)22 + (c1 + (1 )d0) .(17)": "It is here useful to point out that EN combines L1-norm and L2-norm regularization which leads to variableselection while being less sensitive to correlated predictors than the LASSO. In contrast, LAVA is dominatedby the L2-norm, leading to dense models without feature selection.WL1L0 combines L1 and L0 regu-larization, providing stricter feature selection by explicitly controlling the number of non-zero coefficients,resulting in more precise sparsity than the EN. See Appendix A for additional insights into the problem anddiscussion.",
  "Method of Multipliers and ADMM Framework": "The method of multipliers jointly minimizes the two primal variables whereas the ADMM efficiently solvesoptimization problems by alternately updating primal and dual variables, effectively decomposing complexproblems into manageable subproblems (Boyd et al., 2011). A more convenient scaled form of (19) canbe obtained by completing the square with the dual variable z and the residual b u in the augmentedLagrangian. This allows the term zT (b u) +",
  "m(k+1) := m(k) + b(k+1) u(k+1).(22)": "The method of multipliers is generally not an implementable method since the primal update step (21) canbe as hard to solve as the original problem (Beck, 2017; Boyd et al., 2011). To overcome this challenge,ADMM employs an iterative approach in the primal update step. In this approach, b and u are updatedsequentially in an alternating fashion, which is why the method is called the alternating direction method ofmultipliers.",
  "SCPRSM Framework": "The difference between ADMM and PRSM in terms of convergence can be explained through the contractionproperties of their iterative sequences. The iterative sequence generated by ADMM is strictly contractivewith respect to a given solution set, whereas the sequence generated by PRSM is contractive, but not strictlycontractive (He et al., 2014; Corman & Yuan, 2014; He et al., 2002). To address the lack of strict contractionin PRSM, He et al. (2014) proposed incorporating a relaxation factor r > 0 into the Lagrange multiplierupdate steps, thus developing a strictly contractive Peaceman-Rachford splitting method (SCPRSM). Thismodification ensures that the iterative sequence becomes strictly contractive, improving convergence prop-erties.The studies show that SCPRSM outperforms that ADMM generally leads to faster convergencecompared to ADMM (Li & Yuan, 2015; Li et al., 2021). The iterative scheme of the SCPRSM associatedwith the augmented Lagrangian function (20) is written as",
  ") + r(b(k+1) u(k+1)),(24d)": "where the parameter r (0, 1) is a relaxation factor. In addition to a relaxation factor r, an importantdistinction between SCPRSM and ADMM is the presence of an intermediate update for the multipliers,represented as m(k+ 1 2 ) in the SCPRSM scheme. This step ensures a balanced handling of the vectors b andu, thereby leading to a contractive iteration sequence that guarantees convergence to the solution of theoriginal optimization problem (He et al., 2014; Li & Yuan, 2015; Peaceman & Rachford, 1955). The iterativescheme of ADMM (23a - 23c) can be further simplified as",
  "Implementation": "The key idea in the WL1L0 optimization problem is to split the variable b = c + d into two components: c,subject to L1-norm regularization, and d, subject to L0-norm regularization. This decouples the optimizationof c and d within the loss function f(c + d) = y X(c + d)2 allowing separate updates for each part.",
  "The iterations are terminated when convergence is reached according to (c(k) + d(k)) (u(k) + v(k)) (1 + m(k) + w(k))) for tolerance parameter which was set to 105": "For comparison purposes, we also implement the LASSO, SCAD, MCP and EN methods using the proximalADMM and SCPRSM schemes (see Appendix D). Note that the convergence analysis of (30) and (31) isstraightforward from (23) and (24). However, the introduction of translation functions and the variablesc and d increases the dimensionality of the optimization problem, making the theoretical analysis veryextensive. Therefore, we omit the detailed proof of the algorithm that involves the translation functions.",
  "Determining the Learning Rate": "Choosing the learning rate (step size) is crucial for efficiency and proper convergence of optimization algo-rithms. There are two main methods for determining the learning rates and (Beck, 2017; Bertsekas, 2016;Boyd & Vandenberghe, 2004): 1. Backtracking line-search: This method adjusts the learning rate iterativelybased on specific criteria. However, it is both computationally expensive and time-consuming, as it requires",
  "||X||2 for all k, where ||X||2 is the operator normon the training set defined as||X||2 := max||b||2=1 ||Xb||2.(34)": "Equivalently, ||X||2 is the maximum singular value of X (max(X)), which measures the maximum amountby which the matrix X can stretch a vector b relative to its original length (Horn & Johnson, 2013). Thesupremum-based definition can be applied in (34), particularly in infinite-dimensional contexts (Bhatia,1997). However, in finite dimensions, the maximum and supremum coincide for the operator norm definedin (34). By setting the learning rate to1",
  "Bayesian Optimization for Hyperparameter Tuning": "Tuning the regularization parameter , the weight parameter and the relaxation factor r via cross-validationor grid search can be computationally expensive. Bayesian Optimization (BO) is a more advanced, data-driven approach which offers a probabilistic model-based method for hyperparameter tuning (Gao et al.,2021; Shahriari et al., 2015). For the latest advancements, see Wang et al. (2023) and Yang et al. (2024). BO uses a surrogate model, often a Gaussian Processes (GP), to approximate the true objective func-tion.Hyperparameters are collected in = [, , r] and the objective function [] is modeled as[] GP(m[], k), where m[] is its mean and k the kernel (variance) function. The objec-tive function is evaluated at j sequential points MSE(j) = ((j)), with MSE(j) N(((j)), 2).Thisprocess induces a posterior over the acquisition function, guiding the selection of the next hyperparameters.Common acquisition functions include probability of improvement (PI), expected improvement (EI), upperconfidence bound (UCB), and mutual information (MI) (Snoek et al., 2012). BO starts with an initial setof hyperparameters and objective function values to train the surrogate model. The acquisition functionbalances the posterior mean (()) for exploitation and variance (()) for exploration. The GP-UCB isgiven by(j+1) = argmax{() + ()}, where () is driven by the mean function m(), () by the variance function k(), and de-termines the trade-off between exploitation and exploration.Contal et al. (2014) improved GP-UCBwith the Gaussian Process Mutual Information algorithm (GP-MI) as (j+1)=argmax{((j)) +",
  "Materials": "We evaluate our proposed method using one simulated genomic dataset as well as two real-world genomicdatasets. Specifically, single-nucleotide polymorphisms (SNPs), which is a type of genetic variation thatrepresent differences in one of the two nucleotides that make up an individuals DNA at a specific locationcompared to the most common nucleotide pair found in a population. SNPs are typically represented by acount of 0, 1, or 2, where 0 means both nucleotides at the SNP location match the most common pair, 1indicates that one of the two nucleotides differs from the common pair and 2 means both nucleotides at thelocation differ from the most common pair. This count system helps quantify genetic variation at a specificSNP site. A detailed explanation of these datasets is provided in Appendix C.",
  "Results": "The WL1L0-ADMM, WL1L0-SCPRSM, EN-ADMM, EN-SCPRSM, LASSO-ADMM and LASSO-SCPRSMmethods were implemented in Julia 1.10.1 (Bezanson et al., 2017) using the ProximalOperators package (An-tonello et al., 2018). For SCAD-ADMM, SCAD-SCPRSM, MCP-ADMM and MCP-SCPRSM, we wrote ourown code manually in Julia. For all methods, the BO was performed with the BayesianOptimization packageusing an ElasticGPE model and the squared exponential automatic relevance determination (SEArd) ker-nel (Fairbrother et al., 2018). The initial values of b, c and d were set to the marginal covariances betweeny and X, multiplied by 0.0001. By conducting preliminary runs for each set of hyperparameters using BO,we identified the optimal range of parameters. BO with the MI acquisition function was executed for hy-perparameter tuning of all methods. The regression coefficients of the model are obtained from the trainingdataset, and once the model is trained, it predicts outcomes on the test dataset. The MSE is then calculatedon the test dataset to assess the models generalization performance. The test MSE was monitored duringthe BO process to ensure convergence, which was indicated by no further decrease in MSE. All analyseswere executed on a Linux computing platform equipped with an AMD EPYC 7302P 16-Core Processor and32GB of system memory.",
  "Simulated QTLMAS 2010 Dataset": "BO was executed for 250 iterations with 4 GP function evaluations per iteration across all methods. Thelower and upper bounds for 1 were set to 0.001 and 1000.0, 0.1 and 1800.0, and 0.001 and 600.0 for LASSO-ADMM, SCAD-ADMM, and MCP-ADMM, respectively. The lower and upper bounds for r were set to0.01 and 0.999, 0.001 and 1.0, 0.001 and 1.0, and for 1 were set to 0.001 and 1000.0, 0.1 and 1800.0, and0.001 and 600.0 for LASSO-SCPRSM, SCAD-SCPRSM, and MCP-SCPRSM, respectively. For EN-ADMMthe lower and upper bounds for 1 were set to 10.0 and 600.0 and for 2, they were set to 0.001 and 1.0,respectively. For EN-SCPRSM the lower and upper bounds for 1 were set to 10.0 and 500.0, for 2, theywere set to 0.001 and 200.0, for r, they were set to 0.01 and 0.99 respectively. For WL1L0-ADMM the lowerand upper bounds for were set to 0.01 and 0.99, and for 1, they were set to 0.001 and 500.0, respectively.For WL1L0-SCPRSM, the lower and upper bounds for were set to 0.0001 and 0.999, for r they were set to0.0001 and 1.0, and for 1 they were set to 0.0001 and 500.0, respectively. The best result, with a minimumtest MSE of 64.55, was found with WL1L0-SCPRSM at 1 = 391.55, = 0.90, and r = 0.48 ().Timing of the last evaluation with optimized parameters showed that SCAD-ADMM executed most quicklyin only 6.68 seconds. It should be noted that those methods with one regularization parameter tend to befaster to train compared to other methods with two or three hyperparameters.",
  "Real Pig Dataset": "For the Pig dataset, we employed 5-fold cross-validation with random allocations into training and test datato obtain the minimum test MSE on the test data set, with the results averaged over the folds. Here, for allmethods, BO was executed for 100 iterations with 3 GP function evaluations per iteration due to the largedataset size. The lower and upper bounds for 1 were set to 50.0 and 600.0, 0.1 and 1000.0, and 0.1 and20.0 for LASSO-ADMM, SCAD-ADMM, and MCP-ADMM, respectively. The lower and upper bounds for",
  ": Performance evaluation of various regularization methods with optimal parameters on simulatedQTLMAS data. The best-performing test MSE and most sparse model are highlighted in bold": "1 were set to 50.0 and 400.0, 50.0 and 400.0, and 0.01 and 20.0, and lower and upper bounds for r were setto 0.01 and 1.0, 0.01 and 1.0, and 0.01 and 1.0 for LASSO-SCPRSM, SCAD-SCPRSM, and MCP-SCPRSM,respectively. For EN-ADMM the lower and upper bounds for 1 were set to 10.0 and 600.0 and for 2, they were set to0.001 and 1.0 respectively. For EN-SCPRSM the lower and upper bounds for 1 were set to 0.1 and 200.0,for 2, they were set to 0.01 and 100.0, for r, they were set to 0.001 and 1.0, respectively. For WL1L0-ADMM the lower and upper bounds for were set to 0.001 and 0.99, and for 1, they were set to 0.001and 100.0, respectively. For WL1L0-SCPRSM, the lower and upper bounds for were set to 0.001 and0.99, for r they were set to 0.001 and 1.0, and for 1 they were set to 0.0001 and 200.0, respectively. Weobserved little variability in the minimum test MSE across the CV-folds for all methods. Hence, we reportthe mean minimum test MSE using the average estimates of the respective parameters for all methods. Thebest result, with a mean minimum test MSE of 4.48, was found with WL1L0-SCPRSM with mean estimates1 = 240.63, = 0.54, and r = 0.41 (). The average timing over the folds of the last evaluation withoptimized regularization parameters showed that SCAD-ADMM was fastest, taking only 25.8 seconds.",
  "Real Mice Dataset": "Similar to the Pig dataset, we employed 5-fold cross-validation also for this data. BO was executed for 100iterations with 4 GP function evaluations per iteration across all methods. The lower and upper bounds for1 were set to 0.0001 and 20.0, 0.001 and 20.0, and 0.1 and 20.0 for LASSO-ADMM, SCAD-ADMM, andMCP-ADMM, respectively. The lower and upper bounds for 1 were set to 0.001 and 35.0, 0.001 and 35.0,and 0.01 and 20.0, and lower and upper bounds for r were set to 0.001 and 1.0, 0.001 and 1.0, and 0.01 and",
  "Discussion": "The WL1L0 method demonstrates superior performance across all datasets by achieving the lowest MSE andthe fewest non-zero coefficients. This highlights its effectiveness and efficiency as a regularization techniquein high-dimensional data analysis, making it a valuable alternative to the LASSO, SCAD, MCP and EN.The weighting parameter in WL1L0 provides flexibility in tuning the regularization effect, making themethod adaptable to different datasets and problem settings. This adaptability enhances its robustness andapplicability across diverse scenarios. It has been demonstrated several times that the SCAD and MCP often outperform the LASSO (Fan et al.,2014a; Fan & Li, 2001; Zhang, 2010). However, while the LASSO, SCAD, MCP and EN also offer competitiveapproaches to regularization, the WL1L0 method consistently outperforms them, providing enhanced modelsparsity and interpretability without compromising predictive accuracy. The joint sparsity induced by theL1 and the L0 components make the resulting model more interpretable. This is crucial in many scientificand industrial applications, where understanding the model is as important as its predictive power. The use of the SCPRSM algorithm introduces an additional parameter r, which allows for finer controlover the optimization process and potentially leads to better convergence properties and more precise modelfitting. Across all datasets used, the SCPRSM variants demonstrate strong performance by achieving thesmallest minimum MSEs while maintaining a manageable number of non-zero coefficients.Specifically,WL1L0-SCPRSM consistently achieves the lowest MSE across all our datasets, demonstrating its superiorability to minimize prediction errors. It is likely that it will be highly effective in terms of both accuracyand reliability across other types of data. Several other studies have shown that SCPRSM outperformsADMM (Li & Yuan, 2015; Li et al., 2021).",
  "Conclusion": "This paper introduces a novel joint weighted L1- and L0-norm method denoted WL1L0 based on proximalmappings and translation functions, aiming to debias the bias introduced by the L1-norm when appliedto high-dimensional data. Our model introduces a weighting parameter , allowing for the adjustment ofthe influence of both regularizers. The convergence of ADMM and SCPRSM for the developed methodis shown under reasonable assumptions. All hyper-parameters are optimized using Bayesian optimization.The WL1L0-SCPRSM method consistently achieves the lowest MSE across all datasets when compared toall other tested regularization methods (LASSO, EN, SCAD and MCP). Hence, the WL1L0-SCPRSMssuperior performance across different genomic high-dimensional datasets demonstrates its versatility. Ourcurrent paper focuses primarily on prediction. In future work, we plan to specifically address the propertiesof variable selection.",
  "Niccol Antonello, Lorenzo Stella, Panagiotis Patrinos, and Toon Van Waterschoot.Proximal gradientalgorithms: Applications in signal processing. arXiv preprint arXiv:1803.01621, 2018": "Hdy Attouch, Jrme Bolte, Patrick Redont, and Antoine Soubeyran.Proximal alternating minimiza-tion and projection methods for nonconvex problems: An approach based on the Kurdyka-ojasiewiczinequality. Mathematics of Operations Research, 35(2):438457, 2010. Hedy Attouch, Jrme Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic andtame problems: Proximal algorithms, forwardbackward splitting, and regularized GaussSeidel methods.Mathematical Programming, Series A, 137(1):91129, 2013. Jian-Chao Bai, Feng-Miao Bian, Xiao-Kai Chang, and Lin Du. Accelerated stochastic peacemanrachfordmethod for empirical risk minimization.Journal of the Operations Research Society of China, 11(4):783807, 2023.",
  "Hai Yen Le. Generalized subdifferentials of the rank function. Optimization Letters, 7:731743, 2013": "Peixuan Li, Yuan Shen, Suhong Jiang, Zehua Liu, and Caihua Chen. Convergence study on strictly contrac-tive peacemanrachford splitting method for nonseparable convex minimization models with quadraticcoupling terms. Computational Optimization and Applications, 78:87124, 2021. Xinxin Li and Xiaoming Yuan. A proximal strictly contractive Peaceman-Rachford splitting method forconvex programming with applications to imaging. SIAM Journal on Imaging Sciences, 8(2):13321365,2015.",
  "b = argminbF(b) := (b) + (1 )||b||0,(35)": "where (b) := ||y Xb||22 + ||b||1.Since (b) is a convex function, it has a global minimum value.By the Weierstrass theorem, a continuous function over a nonempty compact set attains a minimum. Theexistence of an optimal solution is guaranteed if a function is continuous over a closed set and coercive overthe set (Bertsekas, 2016). Beck (2017) demonstrates that the latter extends to closed functions, i.e. a closedand coercive function over a closed set attains an optimal solution.",
  "with ||b b|| < . A vector b is a global minimum if F(b) F(b) for all b Rp": "For illustration purposes, we generated a random design matrix X with dimension 100 500 by simulating100 samples and 500 features. For each value in the range between -5 and 5, the outcomes of a function F(b)that combines the squared error loss with the regularization term that consists of the weighted sum of theL1 and L0-norms were produced. The regularization parameters were set to = 2 and = 0.6. Therefore,the plotted F(b) includes both the error and regularization terms, rather than solely the penalty norms.One can see that F(b) is nonconvex because any point between the endpoints A and B, as indicated by thedashed red line in , lies outside the domain of F(b). In fact, the shape of the function F(b) is similarto that of nonconvex regularization methods such as SCAD and MCP (Fan & Li, 2001; Zhang, 2010; Zhaoet al., 2018).",
  "C.1Simulated QTLMAS 2010 Dataset": "The dataset comprises 3226 individuals across 5 generations, including 20 founders (5 males and 15 females),with two observed traits (responses): a quantitative trait and a binary trait (Szydlowski & Paczyska, 2011).Each female mates once, producing approximately 30 progeny per birth. SNP data were simulated usinga coalescent model on five autosomal chromosomes, each 100 Mbp long. A total of 10031 markers weregenerated, including 263 monomorphic SNPs and 9768 biallelic SNPs. The continuous quantitative trait iscontrolled by 9 major QTLs at fixed positions, including two pairs of epistatic genes, 3 maternally imprintedgenes, and two additive major genes with phenotypic effects of -3 and 3. The additive genes are positioned atSNP indices 4354 and 5327, whereas the major epistatic locus is at SNP 931. Additionally, a dominance locuswas positioned at SNP number 9212, with an effect of 5.00 assigned to the heterozygote and 5.01 to the upperhomozygote. Moreover, an over-dominance locus was placed at SNP 9404, with an effect of 5.00 assigned tothe heterozygote, -0.01 to the lower homozygote, and 0.01 to the upper homozygote. After filtering SNPswith MAF < 0.01, 9723 markers were retained and transformed into one-hot encoding, resulting in 29169genomic markers. We used the quantitative trait in our study. Generations 1 to 4 (individuals 1 to 2326)were used for training, and generation 5 (individuals 2327 to 3226) served as test data.",
  "C.2Real Pig Dataset": "The Pig dataset contains data from 3534 individuals, with high-density genotypes and phenotypes for fivetraits (Cleveland et al., 2012). Using the PorcineSNP60 chip, 52842 SNPs were assessed and filtered to 50282based on a minor allele frequency threshold of < 0.01. The chosen trait had a heritability of 0.58. Afteradjusting the phenotypic data and excluding individuals with missing data, the final dataset included 3152individuals and was transformed into one-hot encoding, resulting in 150840 genomic markers.",
  "C.3Real Mice Dataset": "This dataset comes from an experiment aimed at identifying and locating quantitative trait loci (QTLs)associated with various complex traits in a population of mice. The dataset contains 1814 individuals whowere genotyped for 10346 polymorphic markers and two traits: body length (BL) and body mass index(BMI). In this study, we used BL trait. After transforming the data into one-hot encoding, the datasetresulted in 31038 genomic markers.This dataset is from the Wellcome Trust and is available in the Rpackage BGLR (Prez & de Los Campos, 2014).",
  "Similarly, for EN-ADMM and EN-SCPRSM, proxg(b) =1": "1+S(b), where > 0 is a linear combinationof the L1 and L2 penalties (Parikh & Boyd, 2013). The closed-form proximal mappings of the SCAD (5)and MCP (6) penalty functions can be found in (Fan & Li, 2001; Liao et al., 2023; Wang & Liu, 2024; Yunet al., 2021). Here, we utilize the scaled versions"
}