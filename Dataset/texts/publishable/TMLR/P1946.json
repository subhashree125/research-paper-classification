{
  "Abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in addressing many pre-viously unsolvable tasks. However, the storage and computational requirements associatedwith DNNs pose a challenge for deploying these trained models on resource-limited devices.Therefore, a plethora of compression and pruning techniques have been proposed in re-cent years. Low-rank decomposition techniques are among the approaches most utilizedto address this problem. Compared to post-training compression, compression-promotedtraining is still under-explored.In this paper, we present a theoretically-justified tech-nique termed Low-Rank Induced Training (LoRITa), that promotes low-rankness throughthe composition of linear layers and compresses by using singular value truncation. Thisis achieved without the need to change the structure at inference time or require con-strained and/or additional optimization, other than the standard weight decay regulariza-tion. Moreover, LoRITa eliminates the need to (i) initialize with pre-trained models, (ii)specify rank selection prior to training, and (iii) compute SVD in each iteration.Ourexperimental results (i) demonstrate the effectiveness of our approach using MNIST onFully Connected Networks, CIFAR10 on Vision Transformers, and CIFAR10/100 and Im-ageNet on Convolutional Neural Networks, and (ii) illustrate that we achieve either com-petitive or state-of-the-art results when compared to leading structured pruning and low-rank training methods in terms of FLOPs and parameters drop. Our code is available at",
  "Introduction": "In recent years, the rapid progress of machine learning has sparked substantial interest, particularly in therealm of Deep Neural Networks (DNNs), utilizing architectures such as Fully Connected Networks (FCNs)(Rumelhart et al., 1986), Convolutional Neural Networks (CNN) (LeCun et al., 1998), and Transformers(Vaswani et al., 2017). DNNs have demonstrated remarkable performance across diverse tasks, includingclassification (Han et al., 2022), image reconstruction (Ravishankar et al., 2019), object recognition (Minaee",
  "Published in Transactions on Machine Learning Research (11/2024)": "Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, ShengYi, and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advancesin Neural Information Processing Systems, 34:1963719651, 2021. Shuxuan Guo, Jose M Alvarez, and Mathieu Salzmann. Expandnets: Linear over-parameterization to traincompact convolutional networks. Advances in Neural Information Processing Systems, 33:12981310, 2020.",
  "We provide justification for this observation and analyze its theoretical properties": "We show through extensive experiments that LoRITa applies to a wide range of network architectures andcan be used in combination with various post-training compression methods. The experiments includeDNN-based image classification tasks across different FCNs, CNNs, and ViTs architectures, using MNIST,CIFAR10, CIFAR100, and ImageNet datasets. We note that LoRITa is a structure-preserving technique, meaning it does not alter the network architec-ture, such as reducing the number of filters in CNNs or pruning redundant nodes. To modify the networkstructure, LoRITa should be combined with other pruning techniques. In this regard, LoRITa functions asa regularization method, akin to weight decay or dropout, which are commonly used in combination withother regularizations.",
  "Additionally, while LoRITa promotes low rankness during training, actual truncation or compression isperformed post-training using Singular Value Truncation (SVT) or other more advanced methods": "Since this paper focuses solely on LoRITa, the reported numerical results are based on LoRITa regular-ization alone during training, in combination with the simplest SVT for compression.Nevertheless, weobserve comparable performance to more sophisticated algorithms, highlighting the benefits of the over-parameterization approach. We leave studying the combinations of LoRITa with other pruning methods (orusing more advanced post-training compression approaches) to future studies.",
  "Related Work": "In the past decade, numerous methods have been proposed in the realm of DNN compression, employingtechniques such as low-rank decomposition or model pruning. Recent survey papers, such as (Li et al., 2023;Marin et al., 2023; He & Xiao, 2023), offer comprehensive overviews. This section aims to survey recentworks closely related to our method. Specifically, we explore papers focused on (i) post-training low-rankcompression methods, (ii) low-rank training approaches, and (iii) structured pruning methods.",
  "Post-training Low-Rank Compression Methods": "As an important compression strategy, low-rank compression seeks to utilize low-rank decomposition tech-niques for factorizing the original trained full-rank DNN model into smaller matrices or tensors. This processresults in notable storage and computational savings (refer to in (Li et al., 2023)). In the work by (Yu et al., 2017), pruning methods were combined with SVD, requiring feature reconstructionduring both the training and testing phases. Another approach, presented in (Lin et al., 2018), treated theconvolution kernel as a 3D tensor, considering the fully connected layer as either a 2D matrix or a 3D tensor.Low-rank filters were employed to expedite convolutional operations. For instance, using tensor products,a high-dimensional discrete cosine transform (DCT) and wavelet systems were constructed from 1D DCTtransforms and 1D wavelets, respectively. The authors in (Liebenwein et al., 2021) introduced the Automatic Layer-wise Decomposition Selector(ALDS) method. ALDS uses layer-wise error bounds to formulate an optimization problem with the objectiveof minimizing the maximum compression error across layers. While our method and the aforementioned approaches utilize low-rank decomposition and maintain thenetwork structure in inference, a notable distinction lies in our approach being a training method thatpromotes low-rankness through the composition of linear layers. It is important to emphasize that any low-rank-based post-training compression technique can be applied to a DNN trained with LoRITa. This will bedemonstrated in our experimental results, particularly with the utilization of three singular-value truncationmethods.",
  "Low-Rank Promoting Methods": "Here, we review methods designed to promote low-rankness in DNN training. These approaches typicallyinvolve employing one or a combination of various techniques, such as introducing structural modifications(most commonly under-parameterization), encoding constraints within the training optimization process, orimplementing custom and computationally-intensive regularization techniques such as the use of Bayesianestimator (Hawkins et al., 2022), iterative SVD (Gural et al., 2020), or the implementation of partiallylow-rank training (Waleffe & Rekatsinas, 2020). The study presented in (Kwon et al., 2023) introduces an approach leveraging SVD compression for over-parameterized DNNs through low-dimensional learning dynamics inspired by a theoretical study on deeplinear networks. The authors identify a low-dimensional structure within weight matrices across diversearchitectures. Notably, the post-training compression exclusively targets the linear layers appended to theFCN, necessitating a specialized initialization. More importantly, a pre-specified rank is required, posinga challenge as finding the optimal combination of ranks for all layers is a difficult problem. Since differentlayers in the model may have different importance to the performance and should be compressed differently,it requires the rank to be layer-specific. For example, the necessity of using layer-specific rank in LoRA is dis-cussed in (Zhang et al., 2024). However, finding the ranks of all layers can be computationally expensive. InLoRITa, this requirement is not needed, as we theoretically show that standard weight decay encourages lowrankness without specifying the rank during training. In comparison, our work shares similarities as it em-ploys a composition of multiple matrices. However, our approach encompasses all weight matrices, attentionlayer weights, and convolutional layers, providing a more comprehensive treatment of DNN architectures. The study conducted by (Tai et al., 2015) introduced an algorithm aimed at computing the low-rank tensordecomposition to eliminate redundancy in convolution kernels. Additionally, the authors proposed a method",
  "Structured Pruning Methods": "When contrasted with weight quantization and unstructured pruning approaches, structured pruning tech-niques emerge as more practical (He & Xiao, 2023). This preference stems from the dual benefits of struc-tured pruning: not only does it reduce storage requirements, but it also lowers computational demands. Asdelineated in a recent survey (He & Xiao, 2023), there exists a plethora of structured pruning methods,particularly tailored for CNNs. As our proposed approach offers both storage and computational reductions,the following methods will serve as our baselines in the experimental results. These methods belong to six categories. Firstly, we consider regularization-based methods, such as ScientificControl for reliable DNN Pruning (SCOP) (Tang et al., 2020), Adding Before Pruning (ABP) (Tian et al.,2021), and the only train once (OTO) (Chen et al., 2021) methods which introduce extra parameters forregularization.Secondly, we consider methods based on joint pruning and compression, exemplified bythe Hinge technique (Li et al., 2020), and the Efficient Decomposition and Pruning (EDP) method (Ruanet al., 2021). Thirdly, we consider activation-based methods such as Graph Convolutional Neural Pruning(GCNP) (Jiang et al., 2022) (where the graph DNN is utilized to promote further pruning on the CNN ofinterest), Channel Independence-based Pruning (CHIP) (Sui et al., 2021), Filter Pruning via Deep Learningwith Feature Discrimination in Deep Neural Networks with Receptive Field Criterion (RFC) (DLRFC) (Heet al., 2022)), which utilize a feature discrimination-based filter importance criterion, and Provable FilterPruning (PFP) (Liebenwein et al., 2019). Next, we consider weight-dependent methods, such as the adaptiveExemplar filters method (EPruner) (Lin et al., 2021), Cross-layer Ranking & K-reciprocal Nearest Filtersmethod (CLR-RNF) (Lin et al., 2022), and Filter pruning using high-rank feature map (HRank) (Lin et al.,2020). Next, we consider a method proposed in (Liu et al., 2022) based on automatically searching forthe optimal kernel shape (SOKS) and conducting stripe-wise pruning. Lastly, we consider ReinforcementLearning based methods such as Deep compression with reinforcement learning (DECORE) (Alwani et al.,2022).",
  "Notation: Given a matrix A Rmn, SVD factorizes it into three matrices: A = UV, where U Rmm": "and V Rnn are orthogonal matrices, and Rmn is a diagonal matrix with non-negative real numbersknown as singular values, si. For a matrix A, its Schatten p-norm, Ap, is the p norm on the singularvalues, i.e., Ap := (i spi )1/p. The nuclear (resp. Frobenius) norm, denoted as A (resp. AF ),corresponds to the Schatten p-norm with p = 1 (resp. p = 2). For any positive integer N, [N] := {1, . . . , N}.We use () and SoftMax() to denote the ReLU and softmax activation functions, respectively.",
  "where = {Wi, i [L]} is the set of all parameters. For the sake of simplicity in notation, we will excludethe consideration of the bias, as it should not affect our conclusion": "For CNNs, the input is a third-order tensor for a multi-channel image, with dimensions H W D, whereH is the height, W is the width, and D is the depth (the number of channels). The convolutional kernel isa fourth-order tensor denoted by K, with dimensions FH FW FD M. The output of the convolutionoperation is a third-order tensor O given as",
  "Singular Value Thresholding": "For a given matrix W and its SVD W = UV Rmn, its best rank-r approximation can be representedas W Wr := UrrVr , where Ur Rmr contains the first r columns of U, r Rrr is a diagonalmatrix that include the largest r singular values, and Vr Rrn contains the first r rows of V. These arecalled the r-term truncation of U, , and V.",
  "Local Singular Value Truncation (LSVT) of Trained Weights: We simply apply SVT to each weightmatrix independently with a fixed rank r": "Global Singular Value Truncation (GSVT) of Trained Weights: For DNNs, not every layer holdsthe same level of importance to the output. Therefore, using the local SVT might not lead to the bestcompression method.To tackle this problem, we can apply a global singular value truncation strategy. This involves normalizing the singular values of each matrix, i.e., dividing each weight matrix by its largestsingular value, and then sorting the singular values of the normalized matrices globally. Subsequently, wedecide which singular values to drop based on this global ranking. This approach offers an automated methodfor identifying the principal components of the entire network. Iterative Singular Value Truncation (ISVT) of Trained Weights: An alternative strategy to globalranking is performance-preserving truncation. In each iteration, we fix the number of parameters to truncate,",
  "With the r-term truncation, for LSVT, GSVT, and ISVT, the storage requirement reduces from mn to(m + n)r. We use SVDr(W) Rmn to denote the SVD r-truncated matrix of W Rmn": "In LSVT, GSVT, and ISVT, our aim is to minimize the accuracy drop resulting from truncation by ensuringthat each weight matrix, W, exhibits high compressibility. This compressibility is characterized by a rapiddecay in the singular values of W. Such a decay pattern enables us to discard a substantial number of singularvalues without significantly sacrificing accuracy. Consequently, this property enhances the effectiveness ofthe compression process. Based on this discussion, a notable insight emerges: The faster the singular values of W decay, the more favorable the compression outcome using SVD. Thisobservation leads to the desire for W to be of low rank, as low-rank matrices exhibit a faster decay insingular values. The question that naturally arises is: How to enforce low-rankness in W during trainingwhile preserving the DNN structure at the inference phase?",
  "k[N] Wki , i [L], and assign them asthe trained weights of the original model": "In summary, LoRITa utilizes the over-parameterized weights during training and reverts to the originalweights dimensions after training. The reversion ensures that LoRITa is a structure-preserving technique. The underlying idea is that a model trained in this manner exhibits enhanced compressibility compared tothe original model described by Equation (1). The resulting weight matrices Wi demonstrate a faster decayin singular values. This strategic approach to training and approximation aims to achieve a more compactand efficient representation of the neural network trained weights, and preserve structure during inference.The procedure is outlined in Algorithm 1. presents an example of our proposed method.",
  "The matrices that correspond to the weights of attention layers are treated in a similar fashion as fully-connected layers": "For convolutional layers, the fourth-order tensor K, with dimensions FH FW FD M, is reshaped intoa matrix. This is achieved through a mode-4 unfolding, resulting in a matrix K(4) of size FHFW FD M.K(4) is then expressed as a composite of N matrices, denoted as K(4) = K1K2 KN. Throughout thetraining phase, each Ki is updated as a weight matrix. Post-training, SVD with r-truncation is applied toK(4) = K1K2 KN to obtain low-rank factors L and R, each with r columns, ensuring that K(4) LR.During the inference phase, we first compute the convolutions of the input with the filters in L reshapedback to tensors, then apply R to the output of the convolutions. More specifically, we reshape L back to afourth-order tensor of size FH FW FD r, denoted by L, then we conduct its convolution with the input",
  ": Illustrative block diagram showcasing the compression (top), training (middle), and inference (bottom) forevery convolution layer in CNNs. Here, denotes the convolution operator": "Remark 4.1. The benefit of over-parameterization in network training has been observed previously in(Khodak et al., 2021; Arora et al., 2018; Guo et al., 2020; Huh et al., 2021) for various reasons. (Aroraet al., 2018) proved that over-parameterization can accelerate the convergence of gradient descent in deeplinear networks by analyzing the trajectory of gradient flow. (Guo et al., 2020) found that combining over-parameterization with width expansion can enhance the overall performance of compact networks.(Huhet al., 2021) noted through experiments that over-parameterization slightly improves the performance of thetrained network, and deeper networks appear to have better rank regularization. These previous works focuson performance and acceleration. Our work complements these findings, providing numerical and theoreticalevidence to justify the benefits of over-parameterized weight factorization in network compression due tothe weight decay behaving as a rank regularizer.Further discussions for these methods are provided inAppendix C.",
  "Weight Decay Regularization & Low-Rankness in LoRITa": "Here, we start by stating the following proposition, which says that minimizing the Frobenius norm ofthe over-parameterized weight matrices is equivalent to minimizing the Schatten p-norm (0 < p < 1) ofthe original weight matrices and the latter is well-known to promote the low-rankness. The proof of thisproposition is given in Appendix B.1.",
  "i[2q]Ri = A .(6)": "If p (0, 1], minimizing the Schatten p-norm encourages low-rankness. A smaller p strengthens the promotionof sparsity by the Schatten p-norm (Nie et al., 2012). Think of the A matrix in Equation (5) and Equation (6)as weight matrices in FCNs, matricized convolutional layers in CNNs, or matrices representing query, key, andvalue in attention layers. These identities (Equation (5) and Equation (6)) imply that by re-parameterizingthe weight matrix A as a product of 2q other matrices Ri, where i [2q], and using Ri (instead of A) as thevariable in gradient descent (or Adam), the weight decay on the new variable corresponds to the right-handside of Equation (6). Additionally, Equation (6) suggests that the more Ri we use to represent A, the lowerrank we obtain for A. This explains why our proposed model in Equation (3) can achieve a lower rank forthe weights compared to the traditional formulation in Equation (1). Remark 4.3. For any weight matrix of size m by n, our proposed training method requires training nmNparameters instead of mn parameters where N is the factorization parameter. Yet in practice, it is moreefficient than those SVD-based low-rankness promoting techniques since no explicit SVD is needed. AlthoughProposition 4.2 suggests that increasing N leads to enhancement in the low-rankness of the appended trainedweights, in practice, this leads to prolonged training durations and a more nonlinear landscape of the op-timization problem. In experiments, we observe that N = 3 is usually sufficient to achieve near-optimalperformance, under affordable computational cost. Moreover, prioritizing shorter test times for large modelsis particularly crucial. This consideration is significant, especially given that testing or inference happenscontinuously, unlike training, which occurs only once or infrequently. This aspect significantly affects userexperience, especially on resource-limited devices where compressed models are often deployed. Remark 4.4. In contrast to previous works, Proposition 4.2, the foundation of LoRITa, reveals that wedo not assume that the weights are strictly low-rank nor require the knowledge of the weight matrixs (ap-proximate) rank during the training phase. Consequently, our approach encourages low-rankness withoutcompromising the networks capacity.",
  "l[L]lWlpp ,": "where = {Wl, l [L]}, and l > 0 for all [L] are the strengths of the penalties. Let us set p =1Kwith some even integer K (the smaller the p is set, the stronger it encourages low-rankness). Then foreach weight matrix, we apply Proposition 4.2 to re-parameterize the weight matrix into a depth-K deepmatrix factorization. This turns the above optimization into the following equivalent form that avoids thecomputation of SVD on-the-fly.",
  "(b) With data augmentation": ": Test Accuracy results of standard (N = 1) and LoRITa (N > 1) models, Here, N represents the numberof composed matrices for each layer. When N = 1, it reduces to the original non over-parameterized model. WhenN 2, there is actual over-parameterization and can be called LoRITa (our proposed technique).",
  "-layer FCN (GSVT)8-layer FCN (GSVT)10-layer FCN (GSVT)": ": Results of the FCNs in a. The top (resp. bottom) rowcorresponds to applying the Local (resp. Global) SVT. N = 1 resultsrepresent the baseline, whereas N > 1 results are for the LoRITa-trainedmodels. The latter is defined as the number ofretained singular values divided by thetotal number of singular values per ma-trix, averaged across all trained matri-ces. As a result, the drop in accuracyis computed by subtracting the testingaccuracy with SVT from the testing ac-curacy without applying SVT. A variety of models, datasets, and over-parameterization scenarios are consid-ered, as outlined in . For the ViTmodels, the number following L andH represents the number of layers andthe number of heads, respectively. Forcomparison with baselines, we considerthe models with N = 1 (models with-out over-parameterization using LoRITa",
  "during training). Further experimental setup details can be found in the caption of . We use PyTorchto conduct our experiments, and our code will be released at a later date": "In each experiment, our initial phase consists of training the baseline model with optimal weight decaysettings to achieve the highest achievable test accuracy. Subsequently, we apply the LoRITa method tore-parameterize the original model. This process involves initializing from a random state and tuning theweight decay to ensure that the final test accuracies remain comparable to those of the initial models. Seethe test accuracies in .",
  "Similarly, for CNNs, compression is applied dis-tinctly to both the convolutional and fully connectedlayers, treating each type separately": "First, we evaluate our proposed method on fullyconnected neural networks, varying the number oflayers, utilizing the Adam optimizer with a learn-ing rate set to 1 102, and employing a constantlayer dimension of 96 (other than the last). Over-parameterization is applied across all layers in themodel. To ensure a fair comparison, we begin bytuning the baseline model (N = 1) across a rangeof weight decay parameters {5 106, 1 105, 2 104, 5 105, 1 104, 2 104}. Subsequently,we extend our exploration of weight decay withinthe same parameter range for models with N > 1.As depicted in a, setting N to values largerthan one results in closely matched final test accu-racies. The results for FCNs on the MNIST datasetare illustrated in .In these plots, LSVT(resp. GSVT) is employed for the top (resp. bot-tom) three plots, providing insights into the effectiveness of the proposed technique. As observed, in almostall the considered cases, models trained with N > 1 achieve better compression results (w.r.t. the drop in",
  "LCNN (Idelbayev & Carreira-Perpinn, 2020)91.2590.13-0.1213.5666.780.09365.38": ": Evaluation of LoRITa models as compared to SOTA structured pruning/low-rank training methods usingResNet20 on CIFAR10. The results of the structured pruning methods are reported according to in (He &Xiao, 2023) (most recent survey paper) and ranked according to the FLOPs drop percentage. The last row resultsare reported from in (Xiao et al., 2023). The ResNet20 FLOPs (resp. parameters) is 40.81M (resp. 0.27M).",
  "LCNN (Idelbayev & Carreira-Perpinn, 2020)92.7892.72-0.0645.7185.471.4591.14": ": Evaluation of LoRITa models as compared to SOTA structured pruning/low-rank training methods usingVGG16 on CIFAR10. The results of the structured pruning methods are reported according to in (He &Xiao, 2023) (most recent survey paper) and ranked according to the FLOPs drop percentage. The last row resultsare reported from in (Xiao et al., 2023). The VGG16 FLOPs (resp. parameters) is 314.59M (resp. 14.73M).Settings 1 and 2 for DECORE correspond to using different hyper-parameters including the penalty on incorrectpredictions (Alwani et al., 2022). architectures, respectively. The columns denote: 1) Test accuracy before pruning (%); 2) Test accuracy afterpruning/compression (%); 3) Accuracy Drop (%); 4) FLOPs after pruning/compression (M); 5) FLOPs drop(%); 6) Pruned model parameters (M); 7) Parameters drop (%). For CIFAR10 (resp. ImageNet), the results of the structure pruning baselines are as reported in and (resp. and ) of (He & Xiao, 2023), with arrows indicating preferable results. Theresults of the low-rank training baseline, LCNN Idelbayev & Carreira-Perpinn (2020), are reported from in (Xiao et al., 2023). For the baseline results, we remark that there could be different reasons for not using the same compressionratio: (i) Different goals: Reducing the number of parameters and reducing the number of FLOPs are twodistinct goals, albeit related. Some papers may focus on one of these objectives, while others aim to achievea balance between both; (ii) Compression is not continuous: In the low-rank case, each time a singular valueis eliminated, we remove m + n parameters where m and n are the lengths of the left and right singularvectors of that layer, respectively. Other methods may try to remove filters in CNN, which also has jumps",
  "SCOP (Tang et al., 2020)69.7669.18-0.581.1138.87.139.30SCOP (Tang et al., 2020)69.7668.62-1.140.997456.643.5ABP (Tian et al., 2021)70.2967.83-2.461.02143.76.3146": ":Evaluation of LoRITa models as compared to SOTA structured pruning/low-rank training methods usingResNet18 on ImageNet. The results of the structured pruning methods are reported according to in (He &Xiao, 2023) (most recent survey paper) and ranked according to the FLOPs drop percentage. The ResNet18 FLOPs(resp. parameters) is 1.81G (resp. 11.69M).",
  "SCOP (Tang et al., 2020)73.3172.62-0.692.02144.811.8645.6SCOP (Tang et al., 2020)73.3172.93-0.382.23139.113.1539.7ABP (Tian et al., 2021)73.8672.15-1.711.92347.510.7550.7": ": Evaluation of LoRITa models as compared to SOTA structured pruning/low-rank training methods usingResNet34 on ImageNet. The results of the structured pruning methods are reported according to in (He &Xiao, 2023) (most recent survey paper) and ranked according to the FLOPs drop percentage. The ResNet34 FLOPs(resp. parameters) is 3.66G (resp. 21.8M) .",
  "in the ratios; (iii) Some compression methods may suddenly break when the compression ratio exceeds acertain threshold, and this threshold varies": "In the tables, N > 1 refers to the use of over-parameterization, i.e. LoRITa, and N = 1 refers to the classicaltraining of the non-overparameterized network, i.e., the baseline. For both LoRITa+ISVT and the baseline,we apply GSVT, followed by implementing the ISVT approach for which only 120 training data points wereused to compute the loss. Apart from using N = 2 and N = 3, the different results of LoRITa+ISVTcorrespond to applying ISVT with different desired compression rates (see Appendix A for more details).Here, we use SGD with learning rate 102. Furthermore, its noteworthy that following all the consideredbaselines, the reported results for LoRITa+ISVT in Tables 2, 3, 4, and 5 are post one round of fine-tuning,whereas most of the considered baselines employ multiple rounds of fine-tuning of the compressed model.As long as the top-1 accuracy isnt compromised by much (approximately 2%, as per (He & Xiao, 2023)),larger FLOPs drop signifies better compression methods. Its also imperative to note that the last column,Parameters drop, is vital as it signifies the amount of memory reduction. For VGG16, our results boast the best FLOPs drop and demonstrate competitive parameter drop. In the caseof ResNet20, our results achieve the best parameter drop and FLOPs drop compared to structured pruningmethods. When compared to the low-rank training baseline (LCNN), our method outperforms LCNN inVGG16 while slightly underperforming in ResNet20 in terms of FLOPs drop. However, it is important tomention that LCNN is significantly more expensive to run, as it requires computing the SVD for every matrixat each iteration during training. In addition to the expensive training, the strong penalty term in LCNNprevents it from reaching the same test accuracy as other methods before compression, resulting in non-competitive test accuracy after pruning even with a small compression rate. Moreover, we observe that forboth the considered architectures, with any fixed level of accuracy drop, our N = 3 model, on average,outperforms our N = 2 model in terms of both FLOPs drop and parameter drop. Furthermore, LoRITamodels achieve improved performance when compared to applying ISVT on the baseline model (N = 1). LoRITa combined with ISVT compression achieves either the best or nearly the best results across the twoCNN CFIAR10 architectures.Most other methods achieve competitive results on one architecture.Tohighlight this point, we include comparison results from the baselines that considered both architectures in.",
  "LCNN66.7865.3885.4791.14": ": Evaluation of LoRITa as compared to SOTA structured pruning/low-rank training methods that consideredResNet20 and VGG16. The results of the structured pruning methods are reported according to in (He &Xiao, 2023) (most recent survey paper) and ranked according to the FLOPs drop percentage. The last row resultsare reported from in (Xiao et al., 2023). For ImageNet results on ResNet18, we achieve the best parameters drop while reporting slightly lowerFLOPs drop when compared to the second best results. For ResNet34, LoRITa+ISVT reports the secondbest FLOPs drop and lightly lower parameters drop when compared to the second best results with only-0.14% of pruned test accuracy drop. As mentioned in the introduction section, our numerical experiments focus on examining the effect of LoRITa,so the reported results are based on LoRITa regularization alone with the simplest SVT post-training com-pression. Across different architectures and datasets, our performance is competitive with other more so-phisticated methods. It is possible to further boost the performance when LoRITa is used in combinationwith other pruning methods and/or with more advanced post-training compression methods.",
  "Conclusion & Future Work": "In this study, we studied a compression technique, Low-Rank Induced Training (LoRITa).Thistheoretically-justified technique promotes low-rankness through the composition of linear layers and achievescompression by employing simple singular value truncation. Notably, LoRITa accomplishes this without ne-cessitating changes to the model structure at inference time, and it avoids the need for constrained oradditional optimization steps. Furthermore, LoRITa eliminates the requirement to initialize with full-rankpre-trained models or specify rank selection before training. Our experimental validation, conducted on adiverse range of architectures and datasets, attests to the effectiveness of the proposed approach. Throughrigorous testing, we have demonstrated that LoRITa combined with an iterative singular value truncationyields compelling results in terms of model compression and resource efficiency, offering a promising av-enue for addressing the challenges associated with deploying deep neural networks on resource-constrainedplatforms.",
  "The work was supported by NSF CCF-2212065 and NSF BCS-2215155. The authors would like to thankAvrajit Ghosh (Michigan State University) for insightful discussions": "Manoj Alwani, Yang Wang, and Vashisht Madhavan. Decore: Deep compression with reinforcement learning.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1234912359, 2022. Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit accelerationby overparameterization. In International Conference on Machine Learning, pp. 244253. PMLR, 2018.",
  "Albert Gural, Phillip Nadeau, Mehul Tikekar, and Boris Murmann.Low-rank training of deep neuralnetworks for emerging memory technology. arXiv preprint arXiv:2009.03887, 2020": "Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on pattern analysisand machine intelligence, 45(1):87110, 2022. Cole Hawkins, Xing Liu, and Zheng Zhang. Towards compact neural networks via end-to-end training: Abayesian tensor approach with automatic rank determination. SIAM Journal on Mathematics of DataScience, 4(1):4671, 2022.",
  "Yang He and Lingao Xiao. Structured pruning for deep convolutional neural networks: A survey. IEEETransactions on Pattern Analysis and Machine Intelligence, 2023": "Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deepconvolutional neural networks acceleration.In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 43404349, 2019. Zhiqiang He, Yaguan Qian, Yuqi Wang, Bin Wang, Xiaohui Guan, Zhaoquan Gu, Xiang Ling, ShaoningZeng, Haijiang Wang, and Wujie Zhou. Filter pruning via feature discrimination in deep neural networks.In European Conference on Computer Vision, pp. 245261. Springer, 2022.",
  "Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning forefficient neural networks. arXiv preprint arXiv:1911.07412, 2019": "Lucas Liebenwein, Alaa Maalouf, Dan Feldman, and Daniela Rus. Compressing neural networks: Towardsdetermining the optimal layer-wise decomposition. Advances in Neural Information Processing Systems,34:53285344, 2021. Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao.Hrank: Filter pruning using high-rank feature map.In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 15291538, 2020. Mingbao Lin, Rongrong Ji, Shaojie Li, Yan Wang, Yongjian Wu, Feiyue Huang, and Qixiang Ye. Networkpruning using adaptive exemplar filters. IEEE Transactions on Neural Networks and Learning Systems,33(12):73577366, 2021. Mingbao Lin, Liujuan Cao, Yuxin Zhang, Ling Shao, Chia-Wen Lin, and Rongrong Ji. Pruning networkswith cross-layer ranking & k-reciprocal nearest filters. IEEE Transactions on neural networks and learningsystems, 2022. Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo Luo. Holistic cnn compression via low-rankdecomposition with knowledge transfer. IEEE transactions on pattern analysis and machine intelligence,41(12):28892905, 2018.",
  "Giosu Cataldo Marin, Alessandro Petrini, Dario Malchiodi, and Marco Frasca. Deep neural networkscompression: A comparative survey and choice recommendations. Neurocomputing, 520:152170, 2023": "Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos.Image segmentation using deep learning: A survey. IEEE transactions on pattern analysis and machineintelligence, 44(7):35233542, 2021. Feiping Nie, Heng Huang, and Chris Ding. Low-rank matrix recovery via efficient schatten p-norm min-imization.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pp. 655661,2012.",
  "Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al.Convolutional neural networks with low-rankregularization. arXiv preprint arXiv:1511.06067, 2015": "Yehui Tang, Yunhe Wang, Yixing Xu, Dacheng Tao, Chunjing Xu, Chao Xu, and Chang Xu. Scop: Scientificcontrol for reliable neural network pruning. Advances in Neural Information Processing Systems, 33:1093610947, 2020. Guanzhong Tian, Yiran Sun, Yuang Liu, Xianfang Zeng, Mengmeng Wang, Yong Liu, Jiangning Zhang, andJun Chen. Adding before pruning: Sparse filter fusion for deep convolutional neural networks via auxiliaryattention. IEEE Transactions on Neural Networks and Learning Systems, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.",
  "Roger Waleffe and Theodoros Rekatsinas. Principal component networks: Parameter reduction early intraining. arXiv preprint arXiv:2006.13347, 2020": "Jinqi Xiao, Chengming Zhang, Yu Gong, Miao Yin, Yang Sui, Lizhi Xiang, Dingwen Tao, and Bo Yuan.Haloc: hardware-aware automatic low-rank compression for compact neural networks. In Proceedings ofthe AAAI Conference on Artificial Intelligence, volume 37, pp. 1046410472, 2023. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui,and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACMComputing Surveys, 56(4):139, 2023. Xiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng Tao. On compressing deep models by low rank andsparse decomposition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 73707379, 2017.",
  "ADetailed Description of the Iterative SVT Approach": "Here, we provide the exact implementation of the ISVT approach we employ in our paper. In each iteration,we fix the number of parameters to truncate. Then, we decide which layer for which these parameters areto be removed by examining which induces the lowest increase in the training loss after the truncation. Inparticular, for each iteration, we begin by fixing the number of parameters to truncate, setting this numberto 500. Next, we select a layer, denoted as l, and remove the smallest singular values from this layer untilwe have moved 500 parameters. After this truncation, we compute the training loss and store this lossas E(l). This process is repeated for all layers to identify the layer l that results in the smallest trainingloss E(l). Once this layer is identified, we truncate 500 parameters from it. We then proceed to the nextiteration, truncating another 500 parameters. This iterative process continues until the desired compressionrate is reached. To alleviate the computational cost, we use only 120 randomly subsampled training data tocompute E(l).",
  "i[N]Ripipi/pi1/p.(11)": "To prove Equation (10), let {Ri }Ti=1 T be a global minimizer of the optimization on the left hand sideof Equation (10). We denote by Ri the augmented matrices obtained by padding/appending all-zero rowsand columns to each Ri until its dimension grows from r r to mi ni. Then, { Ri }Niis a point in T . Inaddition, since the 0-padding does not change the product nor the Schatten p-norm of the matrices, plugging{Ri }Ti=1 into the left hand side optimization of Equation (10) and { Ri }Ti=1 into the right hand side oneyields to:",
  "A = UUA = UUR1 RN": "Next, we want to construct from {Ri }Ni=1 T a new sequence of matrices of different dimensions { Ri }Ti=1 T which yields equal or a smaller objective value than {Ri }Ni=1. The construction proceeds like the following,first define R1 := UUR1V1, where V1 is the matrix containing the first r right eigenvectors of UUR1.This definition ensures that R1 Rrr and R1p R1p. For i = 2, ..., N 1, define Ri := Vi1Ri Vi Rrr, where Vi1 was defined in the previous iteration and Vi is matrix holding the top-r right singularvectors of Vi1Ri as columns. For i = N, define RN := VN1RN Rrn. With this definition, we canverify the following properties for Ri",
  "i[N]Ripipi/pi1/p,": "where the first inequality arises because Ri T may not represent a minimizer, and therefore, its corre-sponding objective function value would be greater or equal to the minimum. The second inequality is due tothe second bullet point above, whereas the final equality is due to the assumption that {Ri } T representsa minimizer of the right hand side of Equation (11). This completes the proof of the second direction.",
  "B.2Proof of Proposition 4.5": "Proof of Proposition 4.5. The proposition is derived from the scaling ambiguity inherent in the ReLU acti-vation. The the scaling ambiguity allows for the output of the network to remain unchanged when a scalar ismultiplied by the weight matrix of one layer and the same scalar is divided by the weight matrix of anotherlayer.",
  "Let us first prove that the objective of Equation (8) evaluated at the rescaled weights l Wil coincides withthe objective of Equation (7) evaluated at the original weights Wil. Indeed, since": "l[L] l = 1 (due to thedefinition of l and ), and the scaling ambiguity, the two networks corresponding to the rescaled weights andthe original weights have identical output. Consequently, when we insert the original and rescaled weightsto Equation (7) and Equation (8), respectively, the first terms of the objectives are identical. Additionally,with the chosen value of l, direct calculations confirm that the second terms in these objectives are also thesame. This indicates that rescaling each weight Wil by l maintains the consistency of the objective values. Next, we show that {l Wil, i [K], l [L]} is a minimizer (may not be unique) of Equation (8), bycontradiction. If it is not one of the minimizers, then there must exist another set of weights { Wil, i [K], l [L]} that achieve lower objective values for Equation (8) . This in turn implies that the reverselyrescaled weights { 1",
  "CDiscussion on Linear Layers Composition Methods For Non-Compression Tasks": "Here, we review recent studies that utilize linear composition for non-compression purposes. This meansthe methods that involve substituting each weight matrix with a sequence of consecutive layers withoutincluding any activation functions. The study presented in (Guo et al., 2020) introduced ExpandNet, wherethe primary objective of the composition is to enhance generalization and training optimization. The authorsempirically show that this expansion also mitigates the problem of gradient confusion. The research conducted in (Khodak et al., 2021) explores spectral initialization and Frobenius decay inDNNs with linear layer composition to enhance training performance. The focus is on the tasks of traininglow-memory residual networks and knowledge distillation. In contrast to our method, this approach employsunder-parameterization, where the factorized matrices are smaller than the original matrix. Additionally,the introduced Frobenius decay regularizes the product of matrices in a factorized layer, rather than theindividual terms. This choice adds complexity to the training optimization compared to the standard weightdecay used in our approach. The study conducted in (Huh et al., 2021) provides empirical evidence demonstrating how linear overparam-eterization of DNN models can improve the generalization performance by inducing a low-rank bias. Unlikeour work, they did not consider the role of weight decay in enforcing low-rankness.",
  "DDemonstrating the Faster Decay of Singular Values in LoRITa-trained Models": "In , we empirically demonstrate the faster decay of singular values in LoRITa-trained models. Inparticular, (left) (resp. (right)) show the singular values of the first (resp. second) weightmatrix of the standard model (N = 1) and LoRITa-trained models (N = 2 and N = 3) for the FCN8architecture of a. As observed, models trained with LoRITa exhibit faster decay, and increasing Npromotes faster decay."
}