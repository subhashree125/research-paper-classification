{
  "Abstract": "In this work, we consider a distributed multi-agent stochastic optimization problem, whereeach agent holds a local objective function that is smooth and strongly convex and that issubject to a stochastic process. The goal is for all agents to collaborate to nd a commonsolution that optimizes the sum of these local functions. With the practical assumptionthat agents can only obtain noisy numerical function queries at precisely one point at atime, we consider an extention of a standard consensus-based distributed stochastic gradient(DSG) method to the bandit setting where we do not have access to the gradient, and weintroduce a zero-order (ZO) one-point estimate (1P-DSG). We analyze the convergence ofthis techniques using stochastic approximation tools, and we prove that it converges almostsurely to the optimum despite the biasedness of our gradient estimate. We then study theconvergence rate of our method. With constant step sizes, our method competes with itsrst-order (FO) counterparts by achieving a linear rate O(k) as a function of number ofiterations k. To the best of our knowledge, this is the rst work that proves this rate inthe noisy estimation setting or with one-point estimators. With vanishing step sizes, weestablish a rate of O( 1",
  "Introduction": "Gradient-free optimization is an old topic in the research community; however, there has been an increasedinterest recently, especially in machine learning applications, where optimization problems are typicallysolved with gradient descent algorithms. Successful applications of gradient-free methods in machine learninginclude competing with an adversary in bandit problems (Flaxman et al., 2004; Agarwal et al., 2010),generating adversarial attacks for deep learning models (Chen et al., 2019; Liu et al., 2019) and reinforcementlearning (Vemula et al., 2019). Gradient-free optimization aims to solve optimization problems with onlyfunctional ZO information rather than FO gradient information. These techniques are essential in settingswhere explicit gradient computation may be impractical, expensive, or impossible. Instances of such settingsinclude high data dimensionality, time or resource straining function dierentiation, or the cost function nothaving a closed-form. ZO information-based methods include direct search methods (Golovin et al., 2019),1-point methods (Flaxman et al., 2004; Bach & Perchet, 2016; Vemula et al., 2019; Li & Assaad, 2021) wherea function f(, S) : Rd R is evaluated at a single point with some randomization to estimate the gradientas suchg(1),z(x, S) = d",
  "Published in Transactions on Machine Learning Research (11/2024)": "Brendan McMahan,Eider Moore,Daniel Ramage,Seth Hampson,and Blaise Aguera y Arcas.Communication-Ecient Learning of Deep Networks from Decentralized Data. In Aarti Singh and JerryZhu (eds.), Proceedings of the 20th International Conference on Articial Intelligence and Statistics, vol-ume 54 of Proceedings of Machine Learning Research, pp. 12731282. PMLR, 2022 Apr 2017.URL Elissa Mhanna and Mohamad Assaad. Single point-based distributed zeroth-order optimization with a non-convex stochastic objective function. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, BarbaraEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conferenceon Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2470124719. PMLR,2329 Jul 2023. URL Angelia Nedi and Alex Olshevsky. Stochastic gradient-push for strongly convex functions on time-varyingdirected graphs. IEEE Transactions on Automatic Control, 61(12):39363947, 2016. doi: 10.1109/TAC.2016.2529285.",
  "where {ej}j=1,...,d is the canonical basis, and other methods such as sign information of gradient estimates(Liu et al., 2019)": "Another area of great interest is distributed multi-agent optimization, where agents try to cooperativelysolve a problem with information exchange only limited to immediate neighbors in the network. Distributedcomputing and data storing are particularly essential in elds such as vehicular communications and coordi-nation, data processing and distributed control in sensor networks (Shi & Eryilmaz, 2020), big-data analytics(Daneshmand et al., 2015), and federated learning (McMahan et al., 2017). More specically, one directionof research integrates (sub)gradient-based methods with a consensus/averaging strategy; the local agentincorporates one or multiple consensus steps alongside evaluating the local gradient during optimization.Hence, these algorithms can tackle a fundamental challenge: overcoming dierences between agents localdata distributions.",
  "Problem Description": "Consider a set of agents N = {1, 2, . . . , n} connected by a communication network. Each agent i is associatedwith a local objective function fi(, S) : K R, where K Rd is a convex feasible set. The global goalof the agents is to collaboratively locate the decision variable x K that solves the stochastic optimizationproblem:",
  "with S S denoting an i.i.d.ergodic stochastic process describing uncertainties in the communicationsystem": "We assume that at each time step, agent i can only query the function values of fi at exactly one point, andcan only communicate with its neighbors. Further, we assume that the function queries are noisy fi = fi +iwith i some additive noise. Agent i must then employ this query to estimate the gradient of the formgi(x, Si).",
  "Related Work": "FO Consensus-Based Distributed Methods: The optimal convergence rate for solving problem (4), as-suming the objective function F is strongly convex with Lipschitz continuous gradients, has been establishedas O( 1 k) under a diminishing step size with full gradient information Pu & Nedi (2018); Nemirovski et al.(2009). However, when employing a constant step size > 0 that is suciently small, the iterates producedby a stochastic gradient method converge exponentially fast (in expectation) to an O()-neighborhood ofthe optimal solution (Pu & Nedi, 2018); this is known as the linear rate O(k). The literature dedicated tosolving problem (4) is vast. In what follows, we highlight some of the contributions. Towc et al. (2016); Tu & Sayed (2012) study distributed stochastic gradient methods where they comparethe adapt-then-combine (ATC) and combine-then-adapt (CTA) strategies, and prove that the ATC strategyoutperforms CTA one in terms of convergence rate, whether with vanishing or with constant step sizes andthat it is more robust against data distribution drifts and network topology. Jakovetic et al. (2018) considerthe CTA strategy with noisy FO gradients over random networks and establish an O( 1 k) convergence ratefor strongly convex and smooth objectives and vanishing step size. Matei & Baras (2011); Yuan et al. (2016)also consider random networks and solve problem (4) using a noise-free (sub)gradient instead and achieve a",
  "Pu & Nedi (2018)": ": Convergence rates for various algorithms related to our work, classied according to the nature ofthe gradient estimate, whether the optimization problem (OP) is centralized or distributed, the assumptionson the objective function, whether the step size is xed (f.) or varying (v.), and the achieved regret boundand convergence rate linear rate to a neighborhood of the optimum with constant step sizes. Nedi & Olshevsky (2016) considertime-varying and directed networks and present a subgradient-push method based on noisy FO gradientsthat achieves an O( ln k k ) rate under the same assumptions on the objective function and vanishing step size.Both the works of Shi et al. (2015) and Qu & Li (2018) consider a static version of the objective function andpropose methods that employ history information of the gradient. They both obtain a rate of O( 1 k) for generalconvex and smooth objectives with constant step sizes. Under the further strong convexity assumption, thestatic nature of the objective allows them to establish a linear convergence rate to the exact solution insteadof a neighborhood of it. Qu & Li (2018) inspire the vast literature on gradient tracking extended to thestochastic setting (Pu & Nedi, 2018; Pu, 2020; Xin et al., 2019) that utilizes local auxiliary variables totrack the average of all agents gradients, the linear rate, however, is established to a neighborhood of theoptimum. ZO Centralized Methods: ZO methods are known to have worse convergence rates than their FO coun-terparts under the same conditions. For example, under a convex centralized setting, Flaxman et al. (2004)prove a regret bound of O(k34 ) (or equivalently a rate of O( 1 k)) with a one-point estimator for Lipschitzcontinuous functions. For strongly convex and smooth objective functions, Hazan & Levy (2014) and Ito(2020) improve upon this result by proving a regret of O(k log k) and Bach & Perchet (2016) that of O(",
  "k)with high probability and of O(log(k)) in expectation for strongly convex loss functions. When the numberis d + 1 point, they prove regret bounds of O(": "k) and of O(log(k)) with strong convexity. The reason whythe performance improves with the addition of number of points in the estimate, is that their variance can bebounded, unlike one-point estimates whose variance cannot be bounded (Liu et al., 2020). However, whenthe function queries are subjected to noise, multi-point estimates start behaving like one-point ones. In noisyfunction queries (centralized) scenario, it has been proven that gradient-free methods cannot achieve a better",
  "convergence rate than ( 1": "k) which is the lower bound derived by Duchi et al. (2015); Jamieson et al. (2012);Shamir (2013) for strongly convex and smooth objective functions. In the work of Bubeck et al. (2021), akernelized loss estimator is proposed where a generalization of Bernoulli convolutions is adopted, and anannealing schedule for exponential weights is used to control the estimators variance in a focus region fordimensions higher than 1. Their method achieves a regret bound of O(",
  "k)": "ZO Consensus-Based Distributed Methods: In distributed settings, Tang et al. (2021) develop twoalgorithms for a noise-free nonconvex multi-agent optimization problem aiming at consensus. One of themis gradient-tracking based on a 2d-point estimator of the gradient with vanishing variance that achieves arate of O( 1 k) with smoothness assumptions and a linear rate for an extra -gradient dominated objectiveassumption and for xed step sizes. The other is based on a 2-point estimator following an ATC strategyinstead of gradient tracking and achieves a rate of O( 1",
  "k log k) under Lipschitz continuity and smoothnessconditions and O( 1": "k) under an extra gradient dominated function structure. In the nonconvex setting, agradient-tracking method is also proposed, but with a one-point estimator (Mhanna & Assaad, 2023) where aconvergence rate of O( 1 k) is established with Lipschitz continuous and smooth functions. Kumar Sahu et al.(2018) propose a standard CTA method where they consider a 2d-point estimate with noisy function queriesover random networks. Under smoothness and strong convexity, they establish an O( 1 k) convergence ratewith vanishing step sizes. Wan et al. (2020; 2022) propose a projection-free method with one-point gradientestimate where a linear optimization step is performed instead of projection. They prove a regret bound ofO(k34 ) for convex losses and an improved regret of O(k23 (log k)13 ) for strongly convex ones.",
  "Contributions": "While consensus-based distributed methods have been extended to the ZO case (Tang et al., 2021; Ku-mar Sahu et al., 2018), their approach relies on a multi-point gradient estimator and in the case of Mhanna& Assaad (2023); Wan et al. (2020; 2022), the rates established for one-point estimates are slow. The multi-point estimation technique assumes the ability to observe multiple instances of the objective function underidentical system conditions, i.e., many function queries are done for the same realization of S in (2) and (3).However, this assumption needs to be revised in applications such as mobile edge computing (Mao et al.,2017; Chen et al., 2021; Zhou et al., 2022) where computational tasks from mobile users are ooaded toservers within the cellular network. Thus, queries requested from the servers by the users are subject tothe wireless environment and are corrupted by noise not necessarily additive. Other applications includesensor selection for an accurate parameter estimation (Liu et al., 2018) where the observation of each sensoris continuously changing. Thus, in such scenarios, one-point estimates oer a vital alternative to solvingonline optimization/learning problems. Yet, one-point estimators are not generally used because of theirslow convergence rate. The main reason is due to their unbounded variance. To avoid this unboundedvariance, in this work, we dont use the estimate given in (1), we extend the one point approach in Li &Assaad (2021)s work where the action of the agent is a scalar and dierent agents have dierent variables,to our consensual problem with vector variables. The dierence is that in our gradient estimate, we dontdivide by . This brings additional challenges in proving that our algorithm converges and a consensus canbe achieved by all agents. And even with bounded variance, theres still a diculty achieving good (linear)convergence rates with two-point estimates due to the constant upper bound of the variance (Tang et al.,2021). Here, despite this constant bound, we were able to go beyond two-point estimates to achieve a linearrate. Moreover, while it requires 2d points with the gradient tracking method to achieve a linear rate in Tanget al. (2021)s work, which is twice the dimension of the gradient itself, here we only need one scalar pointor query. This is much more computationally ecient. We further replace the gradient tracking method bya standard ATC strategy which is more communication ecient as it requires the sharing of only one vectorinstead of two.",
  "Finally, we support our theoretical claims by providing numerical evidence and comparing the algo-rithms performance to its FO and centralized counterparts": "The rest of this paper is divided as follow. In subsection 1.5, we present the mathematical notation followedin this paper. In subsection 1.6, we present the main assumptions of our optimization problem. We thendescribe our gradient estimate followed by the proposed algorithm in subsection 2.1. We then prove thealmost sure convergence of our algorithm in subsection 3.1 and study its rate in subsection 3.2 with varyingstep sizes. In subsection 3.3, we nd its regret bound. And in subsection 3.4, we consider the case of xedstep sizes, study the convergence of our algorithm and its rate. Finally, in section 4, we provide numericalevidence and conclude the paper in section 5.",
  "i=1Ai, Bi": "where Ai (respectively, Bi) represents the i-th row of A (respectively, B). This matrix product is the Hilbert-Schmidt inner product which is written as A, B = tr(ABT). . denotes the 2-norm for vectors and theFrobenius norm for matrices. We next let K() denote the Euclidean projection of a vector on the set K. We know that this projectionon a closed convex set K is nonexpansive (Kinderlehrer & Stampacchia (2000) - Corollary 2.4), i.e.,",
  "In this subsection, we introduce the fundamental assumptions that ensure the performance of the 1P-DSGalgorithm": "Assumption 1.1. (on the graph) The topology of the network is represented by the graph G = (N, E) wherethe edges in E N N represent communication links. A graph G is undirected, i.e., (i, j) E i (j, i) E,and connected (there exists a path of links between any two agents). W = [wij] Rnn denotes the agents coupling matrix, where agents i and j are connected i wij = wji > 0(wij = wji = 0 otherwise). W is a nonnegative matrix and doubly stochastic, i.e., W1 = 1 and 1T W = 1T .All diagonal elements wii are strictly positive. Assumption 1.2. (on the objective function) We assume the existence and the continuity of both Fi(x)and 2Fi(x). Let x K denote the solution of the problem (4) such that F(x) = minxK F(x). We nextassume that F(x) is -strongly convex where",
  "= i,k(fi(xi,k + ki,k, Si,k) + i,k),(6)": "where k > 0 is a vanishing step size and i,k Rd is a perturbation randomly and independently generatedby each agent i. gi,k is in fact a biased estimation of the gradient Fi(xi,k) and the algorithm can convergeunder the condition that all parameters are properly chosen. For clarication on the form of this bias andmore on the properties of this estimate, refer to Appendix B.",
  "The 1P-DSG Algorithm": "We consider a zero-order distributed stochastic gradient algorithm aiming for consensus with a one-pointestimate. We denote it as 1P-DSG employing the gradient estimate gi,k in (6). Every agent i initializes itsvariables with an arbitrary valued vector xi,0 K and computes gi,0 at that variable. Then, at each timek N, agent i updates its variables independently according to the following steps:",
  "As is evident from the update of the variables, the exchange between agents is limited to neighboring nodes,and it encompasses the value xk kgk or the local gradient descent step": "We remark the eect of the gradient estimate variance on the convergence by carefully examining the stepsin (8). Naturally, when the estimates have a large variance, the estimated gradients can vary widely fromone sample to another.This means that the norm of xk+1, which is directly aected by this variance,may also grow considerably. Thus, it may then take longer to converge to the optimal solution because itcannot reliably discern the direction of the steepest descent. In the worst case, the huge variance causesinstability as the optimizer may oscillate around the optimum or even diverge if the variance is too high,making converging to a satisfactory solution dicult. In this work, we use the fact that the local functionsand the noise variance are bounded to prove that the variance of gradient estimate presented in (6) is indeedbounded. This boundedness, alongside the properties of the matrix W in Assumption 1.1, allows us to ndthen an upper bound on the variation of xk+1 with respect to its mean and the variation of this mean withrespect to the optimizer at every iteration and analyze the convergence of both. We then consider the following assumptions for the subsequent convergence analysis. We must note that therst assumption is only taken into account when we study the algorithms behavior with varying step sizes,otherwise it is dropped.",
  "Assumption 2.2. (on the random perturbation) Let i,k = (1i,k, 2i,k, . . . , di,k)T": "Each agent i chooses its i,k vector independently from other agents j = i. In addition, the elements of i,kare assumed i.i.d with E(d1i,kd2i,k) = 0 for d1 = d2 and there exists c3 > 0 such that E(dji,k)2 = c3, dj, i,almost surely. We further assume that there exists a constant c4 > 0 where i,k c4, i, almost surely.Example 2.3. One example is to take k = 0(k + 1)1 and k = 0(k + 1)2 with the constants 0,0, 1, 2 R+. As k=1 kk diverges for 1 + 2 1, k=1 2k converges for 1 > 0.5, and k=1 k2kconverges for 1 + 22 > 1, we can nd pairs of 1 and 2 so that Assumption 2.1 is satised.",
  "Dk = E[xk x2]": "The goal is to bound this divergence from above by sequences whose convergence rate is known. The analysisis highly associated with the parameters k and k that play a signicant role in determining this upperbound. Hence, in what follows, the analysis starts with a general form of k and k, then a particular caseis considered.",
  "Numerical Results": "In this section, we provide numerical examples to illustrate the performance of the algorithm 1P-DSG. Wecompare it with FO distributed methods aiming to achieve consensus, namely DSGT (Pu & Nedi, 2018)and EXTRA (Shi et al., 2015), a ZO distributed algorithm denoted 2P-DSG based on the two-point estimatein (2) (Tang et al., 2021), and a ZO centralized algorithm based on gradient descent (e.g. Flaxman et al.(2004) and Bach & Perchet (2016)) using another one-point estimate which is presented in (1). We denotethe ZO centralized algorithm by 1P-GD. We also compare with a centralized version of our algorithm wherewe use the estimate in (6). For DSGT and EXTRA, we calculate the exact gradient and add white noise toit to form an unbiased FO estimator and for all the ZO algorithms, we consider that the function queriesare noisy. The network topology is a connected Erds-Rnyi random graph with a probability of 0.05. We consider a logistic classication problem to classify m images of the two digits, labeled as yij = +1 or1 from the MNIST data set (LeCun & Cortes, 2005). Each image, Xij, is a 785-dimensional vector andis compressed using a lossy autoencoder to become 10-dimensional denoted as Xij, i.e., d = 10. The totalimages are split equally among the agents such that each agent has mi = m",
  "j=1EuN(1,u) ln(1 + exp(uijyij.XTij )) + c2,": "while reaching consensus on the decision variable K with K = d. We note here that u modelssome perturbation on the local querying of every example to add to the randomization of the communicationprocess. We consider classifying the digits 1 and 2 with m = 12700 images. There are n = 100 agents in the networkand thus each has a local batch of mi = 127 images. We take u = 0.01 and let k = 0.05(k + 1)0.75",
  "d,1": "d}d with equal probability. Also, every function query is subject toa white noise generated by the standard normal distribution. For the DSGT algorithm, we set the stepsize to k = 0.015(k + 1)1 when it is vanishing and = 0.015 when constant, and we do not considerthe perturbation on the objective function nor the noise on the objective function, only the noise on theexact gradient.Similarly for EXTRA and we set its step size to = 0.01.For 2P-DSG, we considerk = 0.01(k + 1)0.75 and k = 0.01(k + 1)0.25. For the centralized 1P-GD algorithm, we set = 0.005and = 0.5 ( = 0.03 and = 0.6 with estimator (6)). We let c = 0.1, and the initialization be the samefor all algorithms, with i,0 uniformly chosen from [0.5, 0.5]d, i N, per instance. We nally average thesimulations over 30 instances. The expected evolution of the loss objective function is presented in and the graphs are zoomedin on in . Experimental results seem to validate our theoretical results: Our proposed algorithmconverges linearly fast with constant step sizes, however the nal gap is due to converging to an O()-neighborhood of the optimal solution. 1P-DSG with vanishing step sizes converges with an O( 1",
  "In , we measure at every iteration the classication accuracy against an independent test set of2167 images using the updated mean vector k = 1": "nni=1 i,k of the local decision variables. The interestof the constant step sizes appears in the convergence rate of this accuracy, where our algorithm is able tocompete with DSGT with full FO information, and to outperform DSGT with a vanishing step size. Thisis an important result as it shows that the classication goal with ZO is well met despite the limiting upperbounds of convergence rate and that O()-neighborhood of the optimal solution achieved linearly fast canbe sucient to achieve the best possible accuracy. The reason for this better accuracy attainment is generally because the step sizes aect the bound on thegeneralization error. For example, Hardt et al. (2016) prove theoretically that the bound on the generalizationerror for strongly convex objectives is smaller when the step sizes are constant (theorem 3.9) than when they",
  "Conclusion": "In this work, we extended the distributed stochastic gradient algorithm to present a practical solution toa relevant problem with realistic assumptions. A novel ZO algorithm was studied and proved to convergewith a biased and high variance one-point gradient estimate and a stochastic perturbation on the objectivefunction. In the context of noisy ZO optimization, we have successfully established a linear convergence rateof O(k) using xed step sizes and O( 1",
  "Xiangyi Chen, Sijia Liu, Kaidi Xu, Xingguo Li, Xue Lin, Mingyi Hong, and David Cox. Zo-adamm: Zeroth-order adaptive momentum method for black-box optimization. In NeurIPS, 2019": "Ying Chen, Zhiyong Liu, Yongchao Zhang, Yuan Wu, Xin Chen, and Lian Zhao. Deep reinforcement learning-based dynamic resource management for mobile edge computing in industrial internet of things. IEEETransactions on Industrial Informatics, 17(7):49254934, 2021. doi: 10.1109/TII.2020.3028963. Amir Daneshmand, Francisco Facchinei, Vyacheslav Kungurtsev, and Gesualdo Scutari.Hybrid ran-dom/deterministic parallel algorithms for convex and nonconvex big data optimization. IEEE Transactionson Signal Processing, 63(15):39143929, 2015. doi: 10.1109/TSP.2015.2436357.",
  "Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: Stability of stochasticgradient descent, 2016. URL": "Elad Hazan and Kr Levy.Bandit convex optimization:Towards tight bounds.In Z. Ghahramani,M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Pro-cessing Systems, volume 27. Curran Associates, Inc., 2014. URL Shinji Ito. An optimal algorithm for bandit convex optimization with strongly-convex and smooth loss. InSilvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conferenceon Articial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp.22292239. PMLR, 2628 Aug 2020. URL Dusan Jakovetic, Dragana Bajovic, Anit Kumar Sahu, and Soummya Kar. Convergence rates for distributedstochastic optimization over random networks. In 2018 IEEE Conference on Decision and Control (CDC),pp. 42384245, 2018. doi: 10.1109/CDC.2018.8619228.",
  "Yann LeCun and Corinna Cortes. The mnist database of handwritten digits. 2005": "Wenjie Li and Mohamad Assaad. Distributed stochastic optimization in networks with low informationalexchange. IEEE Transactions on Information Theory, 67(5):29893008, 2021. doi: 10.1109/TIT.2021.3064925. Sijia Liu, Jie Chen, Pin-Yu Chen, and Alfred Hero. Zeroth-order online alternating direction method ofmultipliers: Convergence analysis and applications. In Amos Storkey and Fernando Perez-Cruz (eds.),Proceedings of the Twenty-First International Conference on Articial Intelligence and Statistics, vol-ume 84 of Proceedings of Machine Learning Research, pp. 288297. PMLR, 0911 Apr 2018.URL",
  "Sijia Liu, Pin-Yu Chen, Xiangyi Chen, and Mingyi Hong. signsgd via zeroth-order oracle. In ICLR, 2019": "Sijia Liu, Pin-Yu Chen, Bhavya Kailkhura, Gaoyuan Zhang, Alfred O. Hero III, and Pramod K. Varshney. Aprimer on zeroth-order optimization in signal processing and machine learning: Principals, recent advances,and applications. IEEE Signal Processing Magazine, 37(5):4354, 2020. doi: 10.1109/MSP.2020.3003837. Yuyi Mao, Changsheng You, Jun Zhang, Kaibin Huang, and Khaled B. Letaief. A survey on mobile edgecomputing: The communication perspective. IEEE Communications Surveys Tutorials, 19(4):23222358,2017. doi: 10.1109/COMST.2017.2745201. Ion Matei and John S. Baras. Performance evaluation of the consensus-based distributed subgradient methodunder random communication topologies. IEEE Journal of Selected Topics in Signal Processing, 5(4):754771, 2011. doi: 10.1109/JSTSP.2011.2120593.",
  "Wei Shi, Qing Ling, Gang Wu, and Wotao Yin. Extra: An exact rst-order algorithm for decentralizedconsensus optimization. SIAM Journal on Optimization, 25(2):944966, 2015. doi: 10.1137/14096668X.URL": "Zai Shi and Atilla Eryilmaz. A zeroth-order admm algorithm for stochastic optimization over distributedprocessing networks. In IEEE INFOCOM 2020 - IEEE Conference on Computer Communications, pp.726735, 2020. doi: 10.1109/INFOCOM41043.2020.9155520. Yujie Tang, Junshan Zhang, and Na Li. Distributed zero-order algorithms for nonconvex multiagent opti-mization. IEEE Transactions on Control of Network Systems, 8(1):269281, 2021. doi: 10.1109/TCNS.2020.3024321.",
  "Next, we letK2 = arg minAkk<1k": "and K0 = max{K1, K2}. For the ensuing part, the purpose is to locate a vanishing upper bound of Dk,making use of the inequality (50). The idea is to propose a decreasing sequence Uk+1 Uk and suppose thatDk Uk, k K0, and then verify that Dk+1 Uk+1 by induction. The choice of Uk is the most dicultcomponent as one has to keep in mind the general forms of k and k in (50) and what kind of decisions totake regarding these forms. An essential property of Uk is presented in the subsequent lemma.",
  ". Verifying that 1 and 2 are bounded": "The goal is to verify that the constant term in the convergence rate is bounded. Thus, we mustcheck that the lower bounds given in (13) and (15) are indeed nite. We start by analyzing 2 and5,2 = 2020maxkK0 (1 + k)2(12) = 2020 (1 + K0)2(12), as 0 < 2 1,"
}