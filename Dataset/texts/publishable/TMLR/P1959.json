{
  "Abstract": "Identifying hidden interactions within complex systems is key to unlocking deeper insightsinto their operational dynamics, including how their elements affect each other and con-tribute to the overall system behavior. For instance, in neuroscience, discovering neuron-to-neuron interactions is essential for understanding brain function; in ecology, recognizinginteractions among populations is key to understanding complex ecosystems.Such sys-tems, often modeled as dynamical systems, typically exhibit noisy high-dimensional andnon-stationary temporal behavior that renders their identification challenging.Existingdynamical system identification methods typically yield operators that accurately captureshort-term behavior but fail to predict long-term trends, suggesting an incomplete captureof the underlying process. Methods that consider extended forecasts (e.g., recurrent neuralnetworks) lack explicit representations of element-wise interactions and require substantialtraining data, thereby failing to capture interpretable network operators. Here we introduceLookahead-driven Inference of Networked Operators for Continuous Stability (LINOCS),a robust learning procedure for identifying hidden dynamical interactions in noisy time-series data. LINOCS integrates several multi-step predictions with adaptive weights duringtraining to recover dynamical operators that can yield accurate long-term predictions. Wedemonstrate LINOCS ability to recover the ground truth dynamical operators underlyingsynthetic time-series data for multiple dynamical systems models (including linear, piece-wise linear, time-changing linear systems decomposition, and regularized linear time-varying",
  "Introduction": "Uncovering the dynamics underlying high-dimensional time-series data is crucial for deciphering the fun-damental principles that govern temporally evolving systems. This is apparent across significant scientificdomains, including neuroscience (where neurons or ensembles interact over time, e.g., Vaadia et al. (1995);DAleo et al. (2019); Mudrik et al. (2024c)), immunology (where cells regulate immune responses, e.g., Savillet al. (2002)), and ecology (where understanding population interactions yields insights into ecosystem dy-namics, e.g., Stein et al. (2013)). Hence, scientific research necessitates the development of procedures adeptat learning dynamic operators that can accurately capture the non-linear and non-stationary evolution ofreal systems. Existing approaches for dynamical systems identification, though, often rely on either black-box deeplearning methods, which while powerful, yield uninterpretable representations, or on simple learning pro-cedures that maximize reconstruction between consecutive samples, and thus fail to accurately predict thesystems behavior for longer scales. Specifically, common dynamical system identification models regularlyrely on optimizing dynamics by minimizing the prediction error for each time point based on projectingthe preceding one through dynamics. Consequently, when using such procedures to learn the operators,post-learning long-term predictions of the systems values (by iteratively estimating the systems state at thenext time point) usually result in undesired divergence away from the real systems values. This difficultyin long-term predictions, importantly, implies that operators recovered by these models that are based onlocal cost functions may not capture the underlying system correctly. The challenge in identifying such un-derlying operators, therefore, lies in the need to incorporate long-term predictions directly into the learningprocedure, which can be especially challenging in cases where the dynamics are non-stationary, non-linear,or otherwise constrained in ways that reflect real-world system behavior. To address this challenge, we present a learning procedure that introduces Lookahead Inference of NetworkedOperators for Continuous Stability (LINOCS). LINOCS bridges the gap between minimizing reconstructioncosts based on single time-step projections (which typically result in operators that quickly diverge in long-term forecasts), and optimizing multi-step training that relies on reconstructions from past time points (whichcan lead to unstable predictions). Particularly, LINOCS achieves this by integrating adaptive re-weightedmulti-step reconstructions into the dynamics inference, progressively building up the cost over trainingiterations while simultaneously considering several multi-step reconstruction terms to identify operators thatenable stable, long-term reconstruction post-training. Through this process, LINOCS also avoids relying onmassive amounts of data (like other methods, including RNNs, for example, require).We demonstratethe effectiveness and adaptability of LINOCS in a variety of dynamical systems models, including linear,switching-linear, decomposed systems, and smoothly linear time-varying systems, achieving significantlyimproved accuracy in operator identification and long-term predictions compared to 1-step optimizationapproaches (E, 3A). The main advantages of LINOCS over existing approaches is its integration of adaptive weights for dif-ferent reconstruction orders. This adaptive nature allows LINOCS to stably regulate noisy measurementsand extract highly accurate dynamics that can predict far into the future. Moreover, while other methodsprimarily focus on specific dynamical system architectures (e.g., RNNs), LINOCS can be implemented intovarious dynamic structures, as we demonstrate through applications to switching linear systems and decom-posed linear systems. Finally, LINOCS improves the identification of operators that expose pairwise elementinteractions, which is important for scientific interpretation.",
  "Background and Terminology": "Consider a system with p interacting elements (e.g., neurons in the brain) whose time-changing stateXRpT evolves over discrete time points t = 1 . . . T as xt+1 = g(xt, bt, t), where xt Rp refersto the state at time t, bt Rp represents an offset at time t, and g is a function g : Rp Rp Z Rp. Forexample, in neuroscience, xt can represent the time-evolving activity of p recorded neurons during a record-ing session with T time points, or xt can represent the activation levels of p immune cells when modelingthe immune system.",
  "xt+1 = g(xt, bt, t) := Atxt + bt,(1)": "where At Rpp represent the transition matrices at each time t. Our focus on locally linear dynamicsis supported by the fact that even highly nonlinear functions can be well approximated over small timeintervals using local linearization (Khalil, 2001; Sastry, 2013). Importantly, this formulations advantagelies in its network interpretabilitythe retention of the ability to easily extract the systems pairwiseinteractions, including non-stationary changes in At and bt over time.Specifically, any operator entry[At]i,j for i, j = 1 . . . p in every t = 1 . . . T can be interpreted as the effect of element j on element i at timet. In practice, however, robustly recovering operators that can accurately describe the systems evolution innon-stationary and non-linear settings, faces numerous computational challenges. Chiefly, if we adopt anaive approach to identify the operators as:At,bt = arg minAt,bt xt Atxt bt22 for every t = 1 . . . T,this problem is statistically unidentifiable. Specifically, the problem has p2 +p unknowns for each time point,but only p equations. One approach to improve inference in these settings is to introduce additional structure via a prior overAt and bt, that constrains the solution space and is commonly grounded in application-driven assumptions.For instance, in many scientific settings, it is reasonable to assume that interactions change smoothly overtime. Therefore, adding a temporal smoothness constraint on At and bt (e.g., At At12F < A andbt bt122 < b) can be beneficial for both interpretability and accuracy. In addition, the inclusion ofsuch constraints can be crucial, particularly in noisy settings, to prevent overfitting. The addition of suchconstraints transforms the problem to:",
  "Published in Transactions on Machine Learning Research (09/2024)": ": Comparison of LINOCS Lorenz locally linear time-varying results to other multi-step approaches. A: rMSE of full lookahead reconstruction by LINOCS (blue) compared to the otherbaselines (Jordana et al., 2021) (Shooting, red), and (Hess et al., 2023) (GTF, orange). B: LINOCS fulllookahead reconstruction under different hyper-parameter settings, with the options described in .C: Shooting (Jordana et al., 2021) full lookahead reconstruction under different hyper-parameter settings,with the options described in . D: GTF (Hess et al., 2023) full lookahead reconstruction underdifferent hyper-parameter settings, with the options described in Tables 7 and 8.",
  "-Step Prediction (xt+1|xt): 1-Step prediction involves using the state at each time point t toestimate the state at the next time point (t + 1)": "Iterative Multi-Step Prediction (IMS) of Order K N (xt+k|xt): IMS involves iteratively,for K times, forecasting 1-step ahead values and using these forecasts as inputs for further 1-stepahead forecasts. Namely, predicting xt+k|xt+k1 k = 0 . . . K 1, where xt1 := xt1. Here, wewill notate an IMS prediction of order K by xKt . We chose to name this prediction style IMS asto be consistent with the literature (Chevillon, 2007). Full Lookahead Prediction (xk|x0): This method enhances IMS by forecasting the state ateach time point xt starting from the initial observations (x0).It achieves this by sequentiallyapplying transition matrices to the estimation from the previous time point xt|xt1, starting fromx0, resulting in: xt = At1 . . . A0x0t = 1 . . . T.(Note: the formula above is presented without offsets {bt}Tt=0 for simplicity, though they may beincluded).",
  "Prior relevant approaches:": "Theoretical literature on long-term prediction instability traces back to Cox (1961) and Klein (2019), who,respectively, introduced exponential smoothing and direct estimation of distant future states. Subsequentstudies, including Findley (1983; 1985); Weiss (1991); Tiao and Xu (1993); Lin and Granger (1994); Kang(2003), evaluated the effectiveness of dynamical system identification methods in producing long-term predic-tions. These approaches, however, build on either 1-step training, which focuses on identifying dynamicaloperators by minimizing the reconstruction error when projecting the state from one time point to the next,or on direct forecasting, which aims to identify a mapping function Fkt : Rp Rp that predicts futurestates by xt+k = Fkt(xt), bypassing explicit identification of intermediate dynamical operators. While the direct estimation approach naturally results in more stable long-term predictions compared to1-step optimization, such approach fails to provide an interpretable network meaning to the operators,i.e., the ability to interpret each entry (i, j) of the operator (At,[i,j]) as the effect of element j on elementi at time t. This is important e.g., in neuroscience, where understanding the brains interactions entailsdiscerning the time-changing fast interactions of neurons (Sussillo, 2014), or in epidemiology where trackingdisease spread dynamics matters (Aguiar et al., 2020). In addition, when Marcellino et al. (2006) comparedbetween iterated and direct estimates using macroeconomic data, they found that in contrast to previousassumptions, iterated forecast methods outperform direct forecast methods, especially when models canauto-select long-lag specificationsraising questions about which approach is more suitable for learningdynamical operators. Markov and Hidden Markov Models (Florian et al., 2011; Ou et al., 2013; Bilmes, 2006) are widely usedto model time series data and capture temporal dependencies in dynamical systems.However, HMMsstruggle with long-term predictions due to their reliance on the Markovian assumption, and require extensions",
  "Specific models considered in this work:": "Of particular interest in this work is improving the model fit of a core set of linear dynamical systemswith different temporal constraints on the system evolution, including 1) Time-Invariant Linear DynamicalSystems (LDS), 2) Switching Linear Dynamical Systems (SLDS) (Ackerson and Fu, 1970; Bar-Shalom andLi, 1990; Hamilton, 1990; Ghahramani and Hinton, 1996; Murphy, 1998; Fox et al., 2008; Linderman et al.,2017), 3) decomposed Linear Dynamical Systems (dLDS, Mudrik et al. (2024a); Chen et al. (2024); Mudriket al. (2024b)), and 4) regularized Linear Time-Varying (LTV) Dynamical Systems.",
  "A,b = arg minA,b X[:,1:T ] AX[:,0:T 1] 1T b2F ,(3)": "where X[:,1:T ] and X[:,0:T 1] represents the noisy observations of the state from the second time point (t = 1)up to the last time point (T) and from the first time point (t = 0) up to T 1, respectively. 1T brepresents the horizontal concatenation of the column vector b, for T times. Here, A captures the averageinfluence from xt1 to xt for all t = 1 . . . T. While such linear (time-invariant) systems are simple and therefore interpretable in terms of capturingelement interactionsreferred to here as network interpretabilitytheir overly simplistic nature oftenprevents them from adequately representing the complexities of real-world time series. Switching Linear Dynamical Systems (SLDS). SLDS models (Ackerson and Fu, 1970; Bar-Shalom andLi, 1990; Hamilton, 1990; Ghahramani and Hinton, 1996; Murphy, 1998; Fox et al., 2008; Linderman et al.,2017) along with other piece-wise stationary models (e.g., Song et al. (2021)) aim to provide interpretablerepresentations of dynamics by identifying operators that govern periods of stationary behavior, with thesystem transitioning between these operators over time. Variations of SLDS include, e.g., recurrent SLDS(rSLDS), which introduces an additional dependency between discrete switches and the previous stateslocation in space (Linderman et al., 2017); and tree-structured recurrent SLDS, which extends rSLDS byincorporating a generalized stick-breaking procedure (Nassar et al., 2018). While SLDS models usually involve transitioning from an observed to a latent low-dimensional space, herewe chose to focus on the case where switches occur within the observation space, essentially enforcingthe transition to the latent space to be the identity operator.If we denote X RNT as the noisyobservations subjected to i.i.d Gaussian noise, SLDS models the evolution of xt using a set of J discretestates (j = 1 . . . J), where each state j is associated with its own linear dynamical system fj. These discretestates switch between them abruptly at certain time points following an HMM model. During each inter-switch period, if the system is in the j-th discrete state, SLDS models the evolution of the state linearlyas xt = fjxt1 + bj, where fj represents the linear transition matrix for the j-th discrete state and bjdenotes a constant offset term for that discrete state. SLDS can be trained by an alternating set of stepsbetween dynamic learning and the HMM update of the operators. This way, SLDS tackles the crucial taskof capturing non-stationarities while preserving interpretability; however it inherently lacks the capabilityto distinguish between multiple co-occurring processes or overlapping subsystems.More information aboutthe model assumptions, limitations, and parameter selection can be found in (Linderman et al., 2016; 2017). decomposed Linear Dynamical Systems (dLDS).The Decomposed Linear Dynamical Systems(dLDS, Mudrik et al. (2024a)) model relaxed the time-invariant or piece-wise linear limitation of LDSsand SLDSs to support the discovery of co-occurring processes while maintaining interpretability. To em-phasize, here, for simplicity, we focus on the case where the dynamics evolution is described directly in theobservation space, while the full model presented in (Mudrik et al., 2024a) supports learning the dynamicswithin an identified latent state. Specifically, dLDS models the dynamics evolution (Atxt1 xt) using asparse time-changing decomposition of linear dynamical operators such that At =Jj=1fjcjt, resulting inJj=1fjcjt xt1 xt. These dynamical operators ({fj}Jj=1) are global, i.e., not time dependent, and hence",
  "{ fj}Jj=1=arg max{fj} P({fj}|X, {cjt}J,Tj=1,t=1).(5)": "Interestingly, dLDS can also capture linear or switching behaviors described earlier, by fixing the dLDScoefficients over time (for linear behavior, far left) and by allowing abrupt change of coefficientsin specific time points (for switching behaviors, middle left). More information about the modelassumptions, limitations, and parameter selection can be found in (Mudrik et al., 2024a). While dLDS presents several dynamic modeling advantages (e.g., captures non-stationarities, promotes in-terpretability), as it estimates the parameters for each time t solely based on the values of the precedingstate at time t 1, it does not address the issue of inaccurate long-term predictions divergence. Smooth or Sparse Linear Time-Varying Systems (LTV). In this paper, we refer to LTV systems thatcan be described by: xt+1 = Atxt for all t = 1 . . . T. We further assume that a regularization R(At) may beapplied to the operators {At}Tt=1. This regularization can be inspired by the application (e.g., smoothness ofoperators over time, At At12F < 2 or operator sparsity vec(At)0 < 1) and can mitigate the ill-posednature of finding At separately for each time point.",
  "LINOCS": "In LINOCS we aim to learn the unknown dynamic operators { At}Tt=1 by integrating several multi-steppredictions simultaneously into the inference procedure. This approach yields not only a more accuratefull-lookahead post-learning reconstruction but also operators that are more closely aligned with the groundtruth. Particularly, for every t = 1 . . . T, LINOCS finds the most likely estimate of {At, bt} given K+1(K Z0 hyper-parameter) multi-step reconstructions of orders k = 0...K with different weights {wk}Kk=0:",
  "xkt = At1 At2 . . . Atk xtk.(7)": "The weights {wk}Kk=0 associated with the orders k = 0 . . . K are dynamically adjusted throughout the infer-ence process (B). This adjustment considers both the order number (k) and the current reconstructionerror related to that order, ek (e.g., the 2 norm, ek = xt xkt 22).Unlike other multi-step methods(e.g., Venkatraman et al. (2015)), LINOCS adapts the weights of the reconstruction orders to prioritize theminimization of large errors in lower orders before considering higher orders (B). Specifically, it grad-ually increases the weight of the best lookahead reconstruction until convergence conditions are satisfied.In our implementations, the weights can be chosen from a list of built-in choices such as uniform, linearlydecreasing, and exponentially decreasing weights. Additionally, our framework allows custom weight func-tions that suit their specific needs. In the experiments presented in this paper, we focus on showcasing threespecific options for the weights: Adapting the weights to sequentially introduce higher multi-step reconstruction orders once theerror for each preceding order falls below a designated threshold, while continuing to maintain theactivation of lower orders. Specifically, in the initial iterations, only w0 > 0, with all other weights",
  "A weight function that considers both k and ek, exhibiting a monotonic decrease in k and an increasein e, with k decreasing faster than e increases ( D)": "Importantly, throughout this paper, we distinguish between two concepts: training order and predictionorder. We denote training order (Ktrain) as the maximum order considered during LINOCS training. Inline with this, K-step optimization or K-step training specifically refer to the use of the K-step cost(i.e. estimating xt|{xtk}Kk=1) during training, with 1-step optimization/training being a special case inwhich the training considers only consecutive time points for inference.In contrast, prediction order refers to post-training predictions that involve iteratively propagating the iden-tified operators for Kpred steps into the future. Here, we demonstrate the contribution of LINOCS for accurate long-scale predictions in four types of systems:1) time-invariant linear; 2) switching linear; 3) decomposed linear; and 4) LTV systems. Importantly, inour experiments, we assume that we observe the underlying system under additive i.i.d Gaussian noiseconditions, however LINOCS can be easily adjusted to other noise statistics.",
  "LINOCS for Linear Dynamics": "We first present the LINOCS learning rule for the simplest case of time-invariant linear dynamical systems( leftmost subplot). Let X RpT be the observations of state X, such that X = X + , with being an i.i.d Gaussian noise ( N(0, 2)). In this time-invariant linear case, X evolves linearly asxt+1 = Axt + b for all t = 1 . . . T, where b Rp1 is a constant offset.",
  "j=0Ajb22,(8)": "where Ak+1 is taking the transition matrix A to the power of k + 1, K is an hyperpameter that dictatesthe maximum reconstruction order, and the set {wk}Kk=0 can be either predefined or automatically adaptedover training based on each order error. Please refer to Algorithm 1 and Appendix A.5 for further detailson the inference of the operator and offset for the linear case.",
  "LINOCS for Switching Linear Dynamical Systems (SLDS)": "For SLDS (, middle-left), we integrate LINOCS into the SLDS operator inference stage (to infer{f}Jj=1, {b}Jj=1, see Sec. 4) using the SSM framework proposed by Linderman et al. (2020), which is thecurrent framework for running SLDS/rSLDS as described in e.g. Linderman et al. (2016; 2017). We maintainthe existing SLDS approach to estimating switch times that delineate the boundaries of the linear periodsbetween switches (i.e., we kept the switching times inference step as implemented by Linderman et al.(2020)). To recover the operators within these identified linear periods, we integrate the learning rule for thetime-invariant linear case described above (Sec. 5.1). Please refer to Algorithm 2 for the procedural steps.",
  "LINOCS for decomposed Linear Dynamical Systems (dLDS)": "For dLDS (, two rightmost subplots), as in SLDS, we incorporate LINOCS into the dynamical systemsupdate step. Let xkt+1 denote the k-th order reconstruction of xt+1, calculated by iteratively propagatingthe dLDS reconstruction k + 1 times, starting from xtk. Furthermore, let xt+1 Jj=1cjt fjxt, where ctrepresents the current estimate of the dLDS coefficients and { fj}Jj=1 denotes the current estimate of thebasis operators. We can now write the k-th order reconstruction (xkt+1) as",
  "where wk is the weight of the k-th multi-step order. This matrix can then be used to infer the coefficients(ct) while simultaneously considering different reconstruction orders with varying weights": "To mirror this concatenated matrix of dynamics with the observations (xt+1), we further define a matchingconcatenated state vector (xt+1)vert Rp(K+1)1. This vector (xt+1)vert is obtained by vertically stackingK + 1 times the observations xt+1 Rp1 at time t + 1 weighted by their corresponding wk values, re-sulting in (xt+1)vert = [w0xt+1; w1xt+1; ; wK xt+1]). In simplified terms, (xt+1)vert := w xt+1 wherew = [w0; w1; ; wK] R(K+1)1 are the training orders currents weights, and denotes the Kroneckerproduct.",
  "where c is the weight of the 1 sparsity-promoting regularization on the coefficients": "Note that the multiplication in the first term ( F Kxt ct R(K+1)p1) produces a vector of estimates of theobservations at t + 1 ( xt+1) computed from all different K + 1 past states. This way, the estimator inEquation (11) seeks the ct vector that best predicts xt+1 considering all K + 1 lookaheads. The next step within every iteration includes updating the dynamic operators {fj}Jj=1.One additionalmodification we make (compared to the original learning of dLDS as presented by Mudrik et al. (2024a))is that rather than updating each fj using gradient descent, we infer the dLDS basis dynamics operators{fj}Jj=1 by fully and directly minimizing the cost. Specifically, let",
  "LINOCS for regularized Linear Time-Varying Systems (LTV)": "Finally, we focus on the more general case of regularized linear time varying systems that are not necessarilyswitching or decomposed. In particular, we focus on two types of regularizations, 1) the operators changesmoothly over time, i.e., At At12F is small, and 2) the operators are sparse, i.e., At0 is small. For the LTV case, we apply LINOCS to find the time-changing operators {At}Tt=1 by iteratively integratingmulti-step reconstruction with the appropriate regularization. The operators are initialized with a regularized1-step optimization",
  "where is the regularization weight. While in this paper we chose to focus on R(At) = At At12Fother regularization terms can be used in a more general sense": "We integrate LINOCS into the estimation process by iteratively updating the operator estimates one at atime. Specifically, during each round of updates, we loop over every time point t = 0 . . . T and hold alloperators of times = t ({A}=t) fixed at their former estimates. We then update At by",
  "LINOCS more accurately identifies ground truth linear systems under noisy observations": "We first test LINOCS ability to robustly learn time-invariant linear dynamical systems from noisy obser-vations. We then simulate the dynamics A R22 as a rotational transition operator and a random offsetb R21, where each bi Uniform(0, 1). We build the synthetic state xt R21 for T = 500 time points, asxt = Axt1 + b starting from random initial value x0 Uniform(0, 1)21, such that the noisy observationsare x = x + , where the noise N(0, 2) = N(0, 0.32) (A,B). We compare the learned operators using LINOCS against four baselines. First we compare to traditional1-step optimization (Eqn. (3)).We further compare the linear LINOCS to our implementations of theDAD approach (Venkatraman et al., 2015), as it is the approach closest to LINOCS in terms of integratingmulti-step predictions into model training. Our implementation of DAD integrates expert and non-expert demonstrations for model training, inspired bythe Dataset-Aggregation (DAgger) approach (Ross et al., 2011). Specifically we test three implementationsof DAD. For each implementation we initialized the transition matrix (Ainit) and the offset (binit) using theoptimal estimate from 1-step optimization. We then, in each DAD implementation, train the model through100 iterations where at each iteration, we used our last estimates of A and b to perform full lookaheadreconstruction, starting from time t = 0. We then update our estimates of A and b using the optimal 1-stepoptimization while considering both the observations and the above full lookahead reconstruction.",
  "Particularly, for the comparisons to DAD, we tested all three options outlined below:": "DAD with full model update:Ateachiteration,weupdateAandbbasedonthelookaheadreconstruc-tionofthestate(xt)calculatedbasedonthelastoperatorsestimate.Namely,{ Aiter+1,biter+1} = arg min{A,b}1TT 1t=0 xt+1 (Axt + b)22. DAgger-inspired DAD (reweighed DAD):For reweighted DAD, we estimate A and b at each iteration using both the observations and thelookahead reconstruction from the last estimates of A and b. In particular, let [xt, xt] Rp2 bea horizontal concatenation of the lookahead reconstruction and of the observations at time t. Thenwe iteratively solve: { Aiter+1,biter+1} = arg min{A,b}1TT 1t=0 [xt+1, xt+1] (A [xt, xt] + b)22.",
  "LINOCS identifies accurate interactions in switching systems": "We next tested LINOCS-driven SLDS as detailed in .2 on simulated data comprising of J = 3discrete states. The transition operators for each of the distinct states was set to a 3 3 rotational matrixoriented in a different direction. Additionally, the offset for each state (bj R31) was set to be the samerandom vector drawn from a uniform distribution between 0 and 1 (D). Notably, since the method is invariant to the order of the operators, to compare the identified operators tothe ground truth operators, we sorted the operators using the linear sum assignment problem (SciPysimplementation, by Crouse (2016)), with the cost function being the Frobenius norm between each pair offs (ground truth vs. estimated for each model). As baselines, we compare the results of LINOCS-augmentedSLDS with standard SLDS and recurrent SLDS (Linderman et al., 2016) with varying numbers of iterations. When comparing LINOCS-SLDS to the baselines (), LINOCS consistently outperformed the otherapproaches across multiple metrics including operator recovery (C,E), switching times recovery(A,D), and dynamics reconstruction (B, B). In particular, LINOCS-SLDS accurately",
  "LINOCS finds dLDS operators that yield accurate dLDS lookahead predictions": "Next, we applied LINOCS to dLDS, as described in .3. First we generated ground-truth data thatrepresent a pseudo-switching () processi.e. linear dynamics that switch more smoothly (in our casebetween J = 3 systems) compared to SLDS where operators switch abruptly. This creates overlap periodswhere two dynamical systems are active at once as they trade off (). LINOCS-dLDS demonstratedsignificantly improved stability in full lookahead reconstruction compared to single-step dLDS (). No-tably, training with orders approximately greater than 35 (Ktrain > 35) on our synthetic dataset (containing1000 time points) resulted in highly accurate full reconstruction (A). Additionally, when comparingMSE and correlation of the time-evolving operator Ft = Jj=1 cjtfj to the ground truth, we observed amonotonic decrease in MSE with increasing maximal LINOCS training orders (Ktrain), while the correlationshowed a monotonic increase (B). Interestingly, although the 1-step prediction (post-training) is seemingly good also for non-LINOCS dLDS(or low-order LINOCS-dLDS) (D left), the advantage of LINOCS is revealed in D right,under the full lookahead reconstruction. This implies that evaluating dynamical models not only based ontheir ability to predict immediate steps but also on their performance in further steps (i.e., under multistepreconstructions) is critical for more nuanced evaluations, as reconstruction errors could be obscured in 1-step predictions. Additionally, this comparison also highlights the importance of integrating multiple orderssimultaneously during training.",
  "LINOCS finds interactions that yield robust lookahead predictions in Linear Time-Varying (LTV)systems": "To test the applicability of LINOCS to more general LTV systems, we implemented LINOCS-LTV to capturethe chaotic behavior of the Lorenz attractor (Sec. A.4) through a smoothly changing LTV approximation(). We compared LINOCS-LTV with several other LTV solvers with varying constraints, includingsmoothness and sparsity ( = 6, 7, 8 and smoothness with weights = 2, 20, refer to Sec. A.2 for details).Unlike methods relying on 1-step optimization, LINOCS, despite similar regularization constraints, achievedsuperior full lookahead reconstruction (A bottom). Also here, while different methods performed satisfactorily in the 1-step (post-training) prediction (Atop, B red, C red), disparities emerged in higher-orders lookahead predictions where alternative meth-ods failed. While all methods, including LINOCS, achieved commendable 1-step reconstruction, LINOCSdemonstrated a markedly lower full lookahead error (B green, 5 most right bar pairs) and superiorfull-lookahead reconstruction correlation with the ground truth (C green, five most right bar pairs). In addition, we analyzed operators identified across various training iterations of LINOCS to assess theirproficiency in achieving lookahead reconstruction (). For this analysis, we used the Lorenz attractorwith 900 time points with intervals of 0.1/9 arbitrary units (a.u.), and applied a smoothness constraint with",
  "LINOCS finds robust interactions in real-world neural data": "Finally, we applied LINOCS to real-world dataset described by Kyzar et al. (2024), which consists of highdensity electrode array of populations of single units in the human medial temporal and medial frontal lobeswhile subjects were engaged in a screening task. We applied linear LINOCS, SLDS, dLDS-LINOCS, andLTV-LINOCS to a single recording session that includes recordings from five brain areas (amygdala left andright, cingulate cortex, hippocampus, pre-supplementary motor area). All the dynamical systems modelswere trained on the firing rate data, which we inferred from the spike-sorted electrophysiology via a Gaussiankernel convolution (see Sec. C.6 for details). We investigated several LINOCS models to showcase their distinct characteristics. First, we examined thelinear case for each brain area individually and explored the mean field interactions between areas ().Importantly, while typical real-world brain dynamics are assumed to be non-linear and non-stationary,our aim in starting with the linear model was to demonstrate how LINOCS can identify the fundamentalbackground neural interactions under linear assumptions and check how its identified interactions defer from",
  "follows switching dynamics, LINOCS may capture the underlying dynamics more effectively, as inferred fromour analysis of the synthetic case": "We observed similar patterns using dLDS-LINOCS, which revealed underlying global brain interactionspotentially fundamental to brain function ( A). When examining their dynamic activations (ct), wenoted a background interaction consistently active, with slight modulations over time ( B, brown),alongside gradually changing activities of other interactions ( B, gray-blue-purple).Importantly,these results provided lookahead predictions that did not decay and maintained a high correlation with theobservations ( C). Finally, employed the LTV-LINOCS on all neurons from all regions simultaneously while imposing a smooth-ness constraint on consecutive operators (with regularization of = 0.1 on At At122, ). Ourfindings reveal that LINOCS identifies operators capable of producing full lookahead reconstructions with-out divergence, closely approximating observed data. Comparative analysis against 1-step optimization withvarious smoothness levels (B,C,E,F) underscores LINOCS ability to achieve better reconstructionsthan other approaches. Additionally, examination of error evolution over time suggests a monotonic increasein error for non-LINOCS approaches (C). Moreover, we observed notable discrepancies between theoperators identified by by LINOCS and the baselines (D). These results highlight the efficacy ofLTV-LINOCS in capturing complex temporal dynamics in real world data while maintaining data fidelity.",
  "Discussion": "In this paper, we introduced LINOCS (Lookahead Inference of Networked Operators for Continuous Stabil-ity), a learning procedure to improve stability and accuracy of dynamical system inference that leveragesmultiple lookahead estimations. By iteratively integrating re-weighted multi-step reconstructions with addi-tional constraints on the operators, LINOCS enables robust inference of networked operators in dynamicalsystems, even in the presence of noise and nonlinearity. Our experimental results highlight LINOCS effectiveness across various dynamical systems, including LinearSystems (LDSs), Switching Linear Systems (SLDS), decomposed LDSs (dLDS) (Mudrik et al., 2024a), andLinear Time-Varying Systems (LTV) in both simulated and real-world neural data. LINOCS achieves moreprecise full lookahead reconstruction and more accurately retrieves ground truth operators in synthetic datacompared to baseline methods, highlighting its ability to better capture nuanes in the underlying system.These findings suggest that LINOCS holds greater potential than alternative approaches for accuratelyidentifying unknown hidden interactions also in real-world data, where the real underlying interactions aretypically obscured but pivotal for robust scientific interpretation. Looking ahead, several promising avenues exist for future work. These include applying LINOCS to a broaderrange of datasets to uncover new scientific discoveries about component interactions (e,g,. using LINOCS-dlds to identify functional interaction motifs in C. elegans, as done for 1-step inference in Yezerets et al.",
  "Jeff A Bilmes. What hmms can do. IEICE TRANSACTIONS on Information and Systems, 89(3):869891,2006": "Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data bysparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):39323937, 2016. Yenho Chen, Noga Mudrik, Kyle A Johnsen, Sankaraleengam Alagapan, Adam S Charles, and Christopher JRozell. Probabilistic decomposed linear dynamical systems for robust discovery of latent neural dynamics.arXiv preprint arXiv:2408.16862, 2024.",
  "Kameron Decker Harris, Aleksandr Aravkin, Rajesh Rao, and Bingni Wen Brunton. Time-varying autore-gression with low-rank tensors. SIAM Journal on Applied Dynamical Systems, 20(4):23352358, 2021": "Florian Hess, Zahra Monfared, Manuel Brenner, and Daniel Durstewitz. Generalized teacher forcing forlearning chaotic dynamics. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on MachineLearning, volume 202 of Proceedings of Machine Learning Research, pages 1301713049. PMLR, 2329 Jul2023. URL David Hocker and Il Memming Park. Multistep inference for generalized linear spiking models curbs runawayexcitation. In 2017 8th International IEEE/EMBS Conference on Neural Engineering (NER), pages 613616. IEEE, 2017.",
  "Armand Jordana, Justin Carpentier, and Ludovic Righetti. Learning dynamical systems from noisy sensormeasurements using multiple shooting. arXiv preprint arXiv:2106.11712, 2021": "Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel im-plicit sparse identification of nonlinear dynamics. Proceedings of the Royal Society A, 476(2242):20200279,2020. Jan Kamiski, Shannon Sullivan, Jeffrey M Chung, Ian B Ross, Adam N Mamelak, and Ueli Rutishauser.Persistently active neurons in human medial frontal and medial temporal lobe support working memory.Nature neuroscience, 20(4):590601, 2017. Jan Kamiski, Aneta Brzezicka, Adam N Mamelak, and Ueli Rutishauser. Combined phase-rate coding bypersistently active neurons as a mechanism for maintaining multiple items in working memory in humans.Neuron, 106(2):256264, 2020.",
  "Scott W Linderman, Andrew C Miller, Ryan P Adams, David M Blei, Liam Paninski, and Matthew JJohnson. Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466, 2016": "Massimiliano Marcellino, James H Stock, and Mark W Watson. A comparison of direct and iterated multistepar methods for forecasting macroeconomic time series. Journal of econometrics, 135(1-2):499526, 2006. Noga Mudrik, Yenho Chen, Eva Yezerets, Christopher J Rozell, and Adam S Charles. Decomposed lineardynamical systems (dlds) for learning the latent components of neural dynamics. Journal of MachineLearning Research, 25(59):144, 2024a.",
  "Noga Mudrik, Ryan Ly, Oliver Ruebel, and Adam S Charles.Creimbo: Cross ensemble interactions inmulti-view brain observations. arXiv preprint arXiv:2405.17395, 2024b": "Noga Mudrik, Gal Mishne, and Adam Shabti Charles. SiBBlInGS: Similarity-driven building-block inferenceusing graphs across states. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, NuriaOliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conferenceon Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 3650436530.PMLR, 2127 Jul 2024c. URL",
  "Josue Nassar, Scott W Linderman, Monica Bugallo, and Il Memming Park.Tree-structured recurrentswitching linear dynamical systems for multi-scale modeling. arXiv preprint arXiv:1811.12386, 2018": "Jinli Ou, Li Xie, Peng Wang, Xiang Li, Dajiang Zhu, Rongxin Jiang, Yufeng Wang, Yaowu Chen, JingZhang, and Tianming Liu. Modeling brain functional dynamics via hidden markov models. In 2013 6thInternational IEEE/EMBS Conference on Neural Engineering (NER), pages 569572. IEEE, 2013. Soumyasundar Pal, Liheng Ma, Yingxue Zhang, and Mark Coates. Rnn with particle flow for probabilisticspatio-temporal forecasting. In International Conference on Machine Learning, pages 83368348. PMLR,2021. Stphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured pre-diction to no-regret online learning. In Proceedings of the fourteenth international conference on artificialintelligence and statistics, pages 627635. JMLR Workshop and Conference Proceedings, 2011. Oliver Rbel, Andrew Tritt, Ryan Ly, Benjamin K Dichter, Satrajit Ghosh, Lawrence Niu, Pamela Baker,Ivan Soltesz, Lydia Ng, Karel Svoboda, et al. The neurodata without borders ecosystem for neurophysio-logical data science. Elife, 11:e78362, 2022. Diya Sashidhar and J Nathan Kutz. Bagging, optimized dynamic mode decomposition for robust, stableforecasting with spatial and temporal uncertainty quantification. Philosophical Transactions of the RoyalSociety A, 380(2229):20210199, 2022.",
  "Bongkee Sin and Jin H Kim. Nonstationary hidden markov model. Signal Processing, 46(1):3146, 1995": "Andrew H Song, Demba Ba, and Emery N Brown. Plso: A generative framework for decomposing nonstation-ary time-series into piecewise stationary oscillatory components. In Uncertainty in Artificial Intelligence,pages 13711381. PMLR, 2021. Richard R Stein, Vanni Bucci, Nora C Toussaint, Charlie G Buffie, Gunnar Rtsch, Eric G Pamer, ChrisSander, and Joao B Xavier. Ecological modeling from time-series inference: insight into dynamics andstability of intestinal microbiota. PLoS computational biology, 9(12):e1003388, 2013.",
  "Andrew A Weiss. Multi-step estimation and forecasting in dynamic models. Journal of Econometrics, 48(1-2):135149, 1991": "Yongqian Xiao, Zixin Tang, Xin Xu, Xinglong Zhang, and Yifei Shi. A deep koopman operator-based mod-elling approach for long-term prediction of dynamics with pixel-level measurements. CAAI Transactionson Intelligence Technology, 2023. Enoch Yeung, Soumya Kundu, and Nathan Hodas. Learning deep neural network representations for koop-man operators of nonlinear dynamical systems.In 2019 American Control Conference (ACC), pages48324839. IEEE, 2019.",
  "We compared the LTV-LINOCS system against two other multi-step approaches, each with various parametersettings:": "1. SSM-RNN Jordana et al. (2021): We used the code from the GitHub repository linked to theoriginal paper Jordana et al. (2021). We experimented with different numbers of epochs, networkarchitectures (fully connected vs. locally linear), seeds, and numbers of shooting points. Details areprovided in . 2. GTF-shPLRNN Hess et al. (2023): We utilized the code available in the GitHub repository here,which is linked to the original paper Hess et al. (2023) and written in Julia. We based the parameterson the values defined in the file GTF-shPLRNN/paper/_experiments/EEG//shPLRNN/_aGTF.jl but modified the observation model to the identity. It is important to note that while we did not perform exhaustive hyperparameter tuning, we examined 12-16 hyperparameter combinations around the default settings. Details of the hyperparameters are listed inTables 7, 8, and 9.",
  "Given that x RNT , and A , computing the highest power K of A (AK) through repeated squaringinvolves K matrix multiplications, each with a complexity of O(N 3), resulting in O(KN 3)": "If the optimizer is set to a maximum of M iterations, then the total complexity is O(MKN 3). In the linearcase, the addition of the offset term is integrated into the inference by extending the size of A by 1, whichdoes not affect the computational complexity scale (assuming N >> 1).",
  "C.2dLDS": "The direct dLDS version we propose in the paper works directly on the observations rather than on a latentlow-dimensional space.This process involves both identifying the dynamic operators {fj}Jj=1 and theircoefficients ({ct}Tt=1). We will start with the complexity for the coefficients inference. Particularly, the inference of ct for eacht = 1 . . . T, involves: ct = arg minct xt+1vert F Kxt ct2F where F Kxt R(K+1)pJ and (xt+1)vert Rp(K+1)1.Hence, assuming (K+1)p >> J, performing the above least squares to infer each ct requires O(J2(K+1)p) =O(J2Kp) assuming K > 1 for the pseudo-inverse and O(J(K + 1)p) = O(JKp) for the matrix-vectormultiplication, resulting in overall O(J2Kp) for each time point, and O(MTJ2Kp) for T time points andM overall model iterations.",
  "C.3Locally Linear System (Time Invariant)": "Assuming we limit the number of iterations to M. For each iteration m = 1 . . . M, we iterate over t = 1 . . . Ttime points to infer At. For each At we consider K + 1 windows (k = 0 . . . K) with ki = 1 . . . k + 1 shifts.Hence, under each combination of time t and iteration m, we get an overall",
  "C.4Observation vs. Latent Spaces in Dynamical Systems": "In many dynamical systems models, the observation space, denoted as X RN, contains the directlymeasured variables. For a system observed over time, we represent these observations as xt X, where tdenotes the time index. The latent (low-dimensional) space, denoted as Z RM with M N, containshidden variables that are not directly observed but inferred from the observations. Let zt Z represent thelatent state at time t. The relationship between the observation space and the latent space is often modeledby an observation function h : Z X, such that:",
  "C.5More Information about Neural Data": "The data we used was collected by the Rutishauser lab at Cedars-Sinai Medical Center Kyzar et al. (2024),with detailed descriptions in Kyzar et al. (2024); Kamiski et al. (2017; 2020). The dataset includes electro-physiological recordings from 21 epileptic patients who were implanted with depth electrodes and Behnke-Fried microwires in the human medial temporal lobe and medial frontal cortex. The recordings were obtainedwhile the subjects performed a memory task (Sternberg task). In the Sternberg task, participants wererequired to memorize a set of 13 images. These images were pseudo-randomly chosen from a group of fivethat elicited the strongest selective responses during the screening task. Following a maintenance period,participants were shown a probe image and asked to identify whether it was included in the initial set. Theimages used in these tasks represented a broad array of subjects and complex natural scenes. Please seeKyzar et al. (2024) for more details. We loaded the data from the DANDI Archive in an NWB (Neurodata Without Borders) format ( Rbelet al. (2022)), and used a single session from it. This session includes recordings of a 63-year-old male subject(Subject 10 in the data) recorded in June 2023 while performing the Sternberg task.",
  "C.6Pre-processing to Neural Data": "To process the data, we took the spike times of the p = 74 neurons within the chosen session. We thenconvolved the spike times with a 100-ms-width Gaussian kernel to get a firing rate estimation. We thennormalized the data by dividing each neurons estimated firing rate by its top 99% firing rate, and used iton the initial 850 samples."
}