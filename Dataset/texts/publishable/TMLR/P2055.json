{
  "Abstract": "Unsupervised semantic segmentation aims to automatically partition images into semanti-cally meaningful regions by identifying global semantic categories within an image corpuswithout any form of annotation.Building upon recent advances in self-supervised rep-resentation learning, we focus on how to leverage these large pre-trained models for thedownstream task of unsupervised segmentation.We present PriMaPs Principal MaskProposals decomposing images into semantically meaningful masks based on their featurerepresentation. This allows us to realize unsupervised semantic segmentation by tting classprototypes to PriMaPs with a stochastic expectation-maximization algorithm, PriMaPs-EM.Despite its conceptual simplicity, PriMaPs-EM leads to competitive results across variouspre-trained backbone models, including DINO and DINOv2, and across dierent datasets,such as Cityscapes, COCO-Stu, and Potsdam-3.Importantly, PriMaPs-EM is able toboost results when applied orthogonally to current state-of-the-art unsupervised semanticsegmentation pipelines. Code is available at",
  "Introduction": "Semantic image segmentation is a dense prediction task that classies image pixels into categories from apre-dened semantic taxonomy. Owing to its fundamental nature, semantic segmentation has a broad rangeof applications, such as image editing, medical imaging, robotics, or autonomous driving (see Minaee et al.,2022, for an overview). Addressing this problem via supervised learning requires ground-truth labels for everypixel (Long et al., 2015; Ronneberger et al., 2015; Chen et al., 2018b). Such manual annotation is extremelytime and resource intensive. For instance, a trained human annotator requires an average of 90 minutes tolabel up to 30 classes in a single 2 MP image (Cordts et al., 2016). While committing signicant resourcesto large-scale annotation eorts achieves excellent results (Kirillov et al., 2023), there is natural interest in amore economical approach. Alternative lines of research aim to solve the problem using cheaper so-calledweaker variants of annotation. For example, image-level supervision describing the semantic categoriespresent in the image, or bounding-box annotations, can reach impressive levels of segmentation accuracy(Dai et al., 2015; Araslanov & Roth, 2020; Oh et al., 2021; Xu et al., 2022; Ru et al., 2023). As an extreme problem scenario toward reducing the annotation eort, unsupervised semantic segmentationaims to consistently discover and categorize image regions in a given data domain without any labels, know-ing only how many classes to discover. Unsupervised semantic segmentation is highly ambiguous as classboundaries and the level of categorical granularity are task-dependent.1 However, we can leverage the factthat typical image datasets have a homogeneous underlying taxonomy and exhibit invariant domain char-acteristics. Therefore, it is still feasible to decompose images in such datasets in a semantically meaningfuland consistent manner without annotations. 1While assigning actual semantic labels to regions without annotation is generally infeasible, the assumption is that thecategories of the discovered segments will strongly correlate with human notions of semantic meaning.",
  "Our work builds upon recent advances in self-supervised representation learning, and takes inspiration fromprevious unsupervised semantic and instance segmentation methods": "The goal of self-supervised representation learning (SSL) is to provide generic, task-agnostic featureextractors (He et al., 2020; Chen et al., 2020; Grill et al., 2020). A pivotal role in dening the behavior ofself-supervised features on future downstream tasks is taken by the self-supervised objective, the so-calledpretext task. Examples of such tasks include predicting the context of a patch (Doersch et al., 2015) or itsrotation (Gidaris et al., 2018), image inpainting (Pathak et al., 2016), and solving jigsaw puzzles (Noroozi& Favaro, 2016). Another family of self-supervised techniques is based on contrastive learning (Chen et al.,2020; Caron et al., 2020). More recently, Transformer networks (Dosovitskiy et al., 2020) revived some olderpretext tasks, such as context prediction (Caron et al., 2021; He et al., 2022), in a more data-scalable fashion. While the standard evaluation practice in SSL (e. g., linear probing, transfer learning) oers some glimpseinto the features properties, understanding the embedding space produced by SSL remains an active terrainfor research (Ericsson et al., 2021; Naseer et al., 2021). In particular, DINO features (Caron et al., 2021;Oquab et al., 2024) are known to encode accurate object-specic information, such as object parts (Amiret al., 2022; Tumanyan et al., 2022). However, it remains unclear to what extent DINO embeddings allowfor semantic representation of the more ubiquitous multi-object scenes. Here, following previous work (e. g.,Hamilton et al., 2022; Seong et al., 2023), we provide further insights. Early techniques for unsupervised semantic segmentation using deep networks (Cho et al., 2021;Van Gansbeke et al., 2021) approach the problem in the spirit of transfer learning and, under certainnomenclatures, may not be considered fully unsupervised. Specically, starting with supervised ImageNetpre-training (Russakovsky et al., 2015), a network obtains a ne-tuning signal from segmentation-orientedtraining objectives. Such supervised bootstrapping appears to be crucial in the ill-posed unsupervised for-mulation. Unsupervised training of a deep model for segmentation from scratch is possible, albeit sacricingaccuracy (Ji et al., 2019; Ke et al., 2022). However, training a new deep model for each downstream taskcontradicts the spirit of SSL of amortizing the high SSL training costs over many computationally cheapspecializations of the learned features (Bommasani et al., 2021). Relying on self-supervised DINO pre-training, recent work (Hamilton et al., 2022; Li et al., 2023; Seong et al.,2023) has demonstrated the potential of such amortization with more lightweight ne-tuning for semanticsegmentation. Nevertheless, most of this work has treated the SSL representation as an inductive prior bylearning a new embedding space over the SSL features (e. g., Hamilton et al., 2022; Seong et al., 2023). Incontrast, following SSL principles, we use the SSL representation in a more direct and lightweight fashion by extracting mask proposals using linear models (PCA) with minimal post-processing and learning a directmapping from feature to prediction space. Mask proposals have an established role in computer vision (Arbelaez et al., 2011; Uijlings et al., 2013), andremain highly relevant in deep learning (Hwang et al., 2019; Van Gansbeke et al., 2021; Yin et al., 2022).Dierent from previous work, we directly derive the mask proposals from SSL representations. Our approachis inspired by the recent use of classical algorithms, such as normalized cuts (Ncut Shi & Malik, 2000), in thecontext of self-supervised segmentation (Wang et al., 2023a;b). However previous approaches (Van Gansbekeet al., 2021; 2022; Wang et al., 2023a;b) mainly proposed foreground object masks on object-centric data,utilized in a multi-step self-training. In contrast, we develop a straightforward method for extracting densepseudo labels for learning unsupervised semantic segmentation of scene-centric data and show consistentbenets in improving the segmentation accuracy across a variety of baselines and state-of-the-art methods(Hamilton et al., 2022; Seong et al., 2023).",
  "Published in Transactions on Machine Learning Research (09/2024)": "COCO-Stu(Caesar et al., 2018) is a dataset of everyday life scenes containing 80 things and 91 stuclasses. Following previous work (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Yin et al., 2022;Li et al., 2023; Seong et al., 2023), we use a reduced variant by Ji et al. (2019) containing 49629 train and2175 test images. Hereby, all images consist of at least 75 % stu pixels. The dataset is evaluated on the 27classes setup. Potsdam-3(ISPRS) is a remote sensing dataset consisting of 8550 RGBIR satellite images with 200 200pixels, which is split into 4545 train and 855 test images, as well as 3150 additional unlabeled images. In ourexperiments, the 3-label variant of Potsdam is evaluated and the additional unlabeled images are not used.",
  "f = F(I) .(1)": "Here, C refers to the channel dimension of the dense features, and H = h/p, W = w/p with p correspondingto the output stride of the backbone. Based on this image representation, the next step is to decompose theimage into semantically meaningful masks to provide a local grouping prior for tting global class prototypes. Initial principal mask proposal.To identify the initial principal mask proposal in an image I, weanalyze the spatial statistical correlations of its features by means of PCA. Specically, we consider theempirical feature covariance matrix",
  "Given this, we compute the cosine-similarity map M 1 RHW of the dominant feature w. r. t. all featuresasM 1 = (Mi,j)i,j ,whereMi,j = f f:,i,j .(5)": "Next, a threshold (0, 1) is applied to the similarity map in order to suppress noise and further localizethe initial mask. Accordingly, elements of a binary similarity map P 1 {0, 1}HW are set to 1 when thesimilarity is larger than a fraction of the maximal similarity, and 0 otherwise, i. e.,",
  "where [] denotes the Iverson bracket. This binary principal mask P 1 gives rise to the rst principal maskproposal in image I": "Further principal mask proposals.Subsequent mask proposals result from iteratively repeating thedescribed procedure. To that end, it is necessary to suppress features that have already been assigned to apseudo label. Specically in iteration z, given the mask proposals P s, s = 1, . . . , z 1, extracted in previousiterations, we mask out the features that have already been considered as",
  "s=1 P si,j = 0.(7)": "Applying Eqs. (2) to (6) on top of the masked features f z yields the cosine-similarity map M z and theprincipal mask proposal P z, and so on. We repeat this procedure until the majority of features (e. g., 95%)have been assigned to a mask. In a nal step, the remaining features, in case there are any, are assigned toan ignore mask",
  "This produces a tensor P {0, 1}ZHW of Z spatial similarity masks, decomposing a single image into Znon-overlapping regions": "Proposal post-processing.To further improve the alignment of the masks with edges and color-correlatedregions in the image, a fully connected Conditional Random Field (CRF) with Gaussian edge potentials(Krhenbhl & Koltun, 2011) is applied to the initial mask proposals P (after bilinear upsampling to theimage resolution) for 10 inference iterations. The process for obtaining PriMaPs is visualized in . PriMaPs pseudo-label generation.In order to form a pseudo label for semantic segmentation out ofthe Z class-agnostic mask proposals, each mask has to be assigned one out of K pseudo-class labels. This",
  "(b) PriMaPs pseudo-label generation": ": (a) PriMaPs-EM architecture. An image I and its augmented version I are embedded by thefrozen self-supervised backbone F, resulting in the dense features f and f . The segmentation prediction yby the momentum class prototypes M arises via the dot product with f. Likewise, y arises from the dotproduct of the running class prototypes R with f . Pseudo labels P are constructed using PriMaPs, I,and y. We use the pseudo labels to optimize R, applying a focal loss. R is gradually transferred to M bymeans of an EMA. (b) PriMaPs pseudo-label generation. Masks P are proposed by iterative binarypartitioning based on the cosine similarity of the features of any unassigned pixel to their rst principalcomponents nearest neighbor feature. Gray indicates these iterative steps. Next, the masks P are alignedto the image I using a CRF. Finally, a per-mask pseudo-class ID is assigned using majority voting based onthe segmentation prediction y, resulting in the pseudo label P . is accomplished using a segmentation prediction of our optimization process, called PriMaPs-EM, detailedin Sec. 3.2.Given a segmentation prediction y, we assign the pseudo-class ID that is most frequentlypredicted within each proposal, yielding the nal pseudo-label map P {0, 1}Khw, a one-hot encodingof a pseudo-class ID. The entire PriMaPs pseudo-label generation process is illustrated in b.",
  "PriMaPs-EM": "Shown in , PriMaPs-EM is an iterative optimization technique for tting class prototypes for semanticsegmentation across a dataset utilizing pseudo labels P based on PriMaPs (cf. Sec. 3.1). PriMaPs pro-vide guidance through local per-image consistency for tting the global class prototypes across a dataset.PriMaPs-EM leverages a frozen pre-trained self-supervised backbone model F and optimizes two identicallysized vector sets, the running class prototypes R and their moving average, the momentum class prototypesM. The class prototypes R and M are the K pseudo-class representations in the feature space, project-ing the C-dimensional features linearly to K semantic pseudo classes, which equates to segmenting. Moreprecisely, we use the segmentation prediction of the momentum class prototypes M to assign consistent se-mantic pseudo-class IDs to the class-agnostic PriMaPs. We optimize the running class prototypes R usingthe pseudo labels P and gradually transfer R to M using an EMA. PriMaPs-EM performs the optimization in two stages, since in our case, a meaningful initialization of theclass prototypes is vital to provide a reasonable optimization signal. This can be traced back to the pseudo-label generation, which utilizes a segmentation prediction to assign globally consistent classes to the masks.Initializing the class prototypes randomly leads to a highly unstable and noisy signal. Initialization.We initialize the class prototypes M with the rst K principal components using vanillaPCA computed over the feature embeddings of a large number of images across the respective dataset. Next,a cosine-distance batch-wise K-means (MacQueen, 1967) loss",
  "k,i,j(1 k)2P k,i,j log(yk,i,j) ,(10)": "where yk,i,j = softmax(Rf :,i,j) are the predictions and k is the class-wise condence value approximatedby averaging yk,:,: spatially. The running class prototypes R are optimized with an augmented input imageI.We employ photometric augmentations (Gaussian blur, grayscaling, and color jitter), introducing acontrolled noise, thereby strengthening the robustness of our class representation. The momentum classprototypes M are the EMA of the running class prototypes R. This is utilized in order to stabilize theoptimization, accounting for the noisy nature of the unsupervised signal used for optimization. We updateM every iterations with a decay as",
  "t+M= tM + (1 )t+R,(11)": "where t is the iteration index of the previous update. This optimization approach resembles moving averagestochastic EM. Hereby, the E-step amounts to nding pseudo labels using PriMaPs and the momentumclass prototypes. The M-step optimizes the running class prototypes with respect to their focal loss Lfocal.Stochasticity arises from performing EM in mini-batches of images. Inference.At inference time, we obtain the segmentation prediction y via the momentum class proto-types M, rened using a fully connected CRF with Gaussian edge potentials (Krhenbhl & Koltun, 2011)following previous approaches (Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al., 2023). Thisis the identical CRF as already used for rening the masks in the PriMaPs pseudo-label generation usingthe same CRF parameters as previous work (Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al.,2023).",
  "Experimental Setup": "Datasets.Following the practice of previous work, we conduct experiments on Cityscapes (Cordts et al.,2016), COCO-Stu (Caesar et al., 2018), and Potsdam-3 (ISPRS). Cityscapes and COCO-Stu are evaluatedusing 27 classes, while Potsdam is evaluated on the 3-class variant. Adopting the established evaluationprotocol (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Seong et al., 2023), we resize images to320 pixels along the smaller axis and crop the center 320 320 pixels. This is adjusted to 322 pixels forDINOv2. Dierent from previous work, we apply this simple scheme throughout this work, thus dispensingwith elaborate multi-crop approaches of previous methods (Hamilton et al., 2022; Yin et al., 2022; Seonget al., 2023). Self-supervised backbone.Experiments are conducted across a collection of pre-trained self-supervisedfeature embeddings: DINO (Caron et al., 2021) based on ViT-Small and ViT-Base using 8 8 patches; andDINOv2 (Oquab et al., 2024) based on ViT-Small and ViT-Base using 14 14 patches. In the spirit of SSLprinciples, we keep the backbone parameters frozen throughout the experiments. We use the output fromthe last network layer as our SSL feature embeddings. Since PriMaPs-EM is agnostic to the used embedding",
  "DINOv2ViT-B/1482.921.391.044.8": "space, we can also apply it on top of current state-of-the-art unsupervised segmentation pipelines. Here, weconsider STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023), which also use DINO features butlearn a target domain-specic subspace. Baseline.Following (Hamilton et al., 2022; Seong et al., 2023), we train a single linear layer as a baselinewith the same structure as R and M by minimizing the cosine-distance batch-wise K-Means loss fromEq. (9). Hereby, parameters, such as the number of epochs and the learning rate, are identical to those usedwhen employing PriMaPs-EM. PriMaPs-EM.As discussed in Sec. 3.2, the momentum class prototypes M are initialized using the rstK principal components; we use 2975 images for PCA, as this is the largest number of training images sharedby all datasets. Next, M is pre-trained by minimizing Eq. (9) using Adam (Kingma & Ba, 2015). We usea learning rate of 0.005 for 2 epochs on all datasets and backbones. The weights are then copied to R. Fortting the running class prototypes using EM, R is optimized by minimizing the focal loss from Eq. (10)with Adam (Kingma & Ba, 2015) using a learning rate of 0.005. The momentum class prototypes M areupdated using an EMA according to Eq. (11) every s = 10 steps with decay = 0.98. We set the PriMaPsmask-proposal threshold to = 0.4 and provide detailed ablation experiments in Appendix A.2. We use abatch size of 32 for 50 epochs on Cityscapes and Potsdam-3, and use 5 epochs on COCO-Stu due to itslarger size. Importantly, the same hyperparameters are used across all datasets and backbones. Moreover,note that tting class prototypes with PriMaPs-EM is quite practical, e. g., about 2 hours on Cityscapes.Experiments are conducted on a single NVIDIA A6000 GPU. Supervised upper bounds.To assess the potential of the SSL features used, we report supervised upperbounds. Specically, we train a linear layer using cross entropy and Adam with a learning rate of 0.005.Since PriMaPs-EM uses frozen SSL features, its supervised bound is the same as that of the underlyingfeatures. This is not the case, however, for prior work (Hamilton et al., 2022; Seong et al., 2023), whichproject the feature representation aecting the upper bound.",
  "DINOv2ViT-B/1452.823.677.353.7": "(Van Gansbeke et al., 2021; Hamilton et al., 2022; Seong et al., 2023). We evaluate common metrics inunsupervised semantic segmentation, specically the mean Intersection over Union (mIoU) and Accuracy(Acc) over all classes after aligning the predicted class IDs with ground-truth labels by means of Hungarianmatching (Kuhn, 1955). SotA + PriMaPs-EM.To explore our methods potential, we additionally employ PriMaPs-EM on topof STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023). For each backbone-dataset combination,we apply it on top of the best previous method in terms of mIoU. To that end, the training signal forlearning the feature projection of (Hamilton et al., 2022; Seong et al., 2023) remains unchanged. We applyPriMaPs-EM fully orthogonally, using the DINO backbone features for pseudo-label generation and t adirect connection between the feature space of the state-of-the-art method and the prediction space.",
  "Results": "We compare PriMaPs-EM against prior work for unsupervised semantic segmentation (Ji et al., 2019; Choet al., 2021; Hamilton et al., 2022; Yin et al., 2022; Li et al., 2023; Seong et al., 2023). As in previouswork, we use DINO (Caron et al., 2021) as the main baseline.Additionally, we also test PriMaPs-EMon top of DINOv2 (Oquab et al., 2024), STEGO (Hamilton et al., 2022), and HP (Seong et al., 2023).Overall, we observe that the DINO baseline already achieves strong results (cf. Tabs. 1 to 3). DINOv2features signicantly raise the supervised upper bounds in terms of Acc and mIoU, the improvement inthe unsupervised case remains more modest. Nevertheless, PriMaPs-EM further boosts the unsupervisedsegmentation performance. In Tab. 1, we compare to previous work on the Cityscapes dataset.PriMaPs-EM leads to a consistentimprovement over all baselines in terms of unsupervised segmentation accuracy. For example, PriMaPs-EM boosts DINO ViT-S/8 by +3.6% and +19.8% in terms of mIoU and Acc, respectively, which leads tostate-of-the-art performance. Notably, we nd PriMaPs-EM to be complementary to other state-of-the-artunsupervised segmentation methods like STEGO (Hamilton et al., 2022) and HP (Seong et al., 2023) on the",
  "DINOv2ViT-B/1483.271.187.978.3": "corresponding backbone model. This suggests that these methods use their SSL representation only to alimited extent and do not fully leverage the inherent properties of the underlying SSL embeddings. Similarobservations can be drawn for the experiments on COCO-Stu in Tab. 2. PriMaPs-EM leads to a consistentimprovement across all four SSL baselines, as well as an improvement over STEGO and HP. For instance,combining STEGO with PriMaPs-EM leads to +14.0% and +19.1% improvement over the baseline in termsof mIoU and Acc for DINO ViT-B/8. Experiments on the Potsdam-3 dataset follow the same pattern (cf.Tab. 3). PriMaPs-EM leads to a consistent gain over the baseline, e. g. +17.6% and +14.4% in terms ofmIoU and Acc, respectively, for DINO ViT-B/8. Moreover, it also boosts the accuracy of STEGO and HP. Insome cases, the gain of PriMaPs-EM is limited. For example, in Tab. 1 for DINO ViT-B/8 + PriMaPs-EM,the class prototype for sidewalk is poor while the classes road and vegetation superimpose smallerobjects. For DINO ViT-S/8 + PriMaPs-EM in Tab. 3, the class prototype road is poor. This limits theoverall performance of our method while still outperforming the respective baseline in both cases. Overall, PriMaPs-EM provides modest but consistent benets over a wide range of baselines and datasets andreaches competitive segmentation performance w. r. t. the state-of-the-art using identical hyperparametersacross all backbones and datasets. Recalling the simplicity of the techniques behind PriMaPs, we believethat this is a signicant result. The complementary eect of PriMaPs-EM on other state-of-the-art methods(STEGO, HP) further suggests that they rely on DINO features for mere bootstrapping and learn featurerepresentations with orthogonal properties to those of DINO. We conclude that PriMaPs-EM constitutes astraightforward, entirely orthogonal tool for boosting unsupervised semantic segmentation.",
  "DINO ViT-B/8 Baseline (Caron et al., 2021)49.215.538.815.766.149.4": "baseline, which corresponds to K-means feature clustering, for reference. In the most simplied case, wedirectly use the similarity mask, similar to Eq. (4). Next, we use the nearest neighbor (+NN in Tab. 4a) ofthe principal component to get the masks as in Eq. (5), followed by the full approach with CRF renement(+P-CRF). Except for the changes in the pseudo-label generation, the optimization remains as described inSec. 4.1. We observe that the similarity masks already provide a good staring point, yet we identify a gainfrom every single component step. This suggests that using the nearest neighbor improves the localizationof the similarity mask. Similarly, CRF renement improves the alignment between the masks and the imagecontent. We also experiment with using the respective next principal component (non-iter. PC) instead ofiteratively extracting the rst principal component from the masked features. This leads to slightly inferiorresults. Naively using the K leading eigenvectors and simply assigning the masks based on the arg max oftheir cosine similarity to the features without any iterations would lead to signicantly worse results with apixel Accuracy of 43.1 % and a mIoU of 19.9 %. Note that nearest neighbor anchoring and thresholding areused in both experiments. Additionally, we ablate the nearest neighbor anchoring, the threshold , and thestop criterion in Appendix A. PriMaPs-EM architecture ablations.In a similar vein, we analyze the contribution of the dierentarchitectural components of PriMaPs-EM. Optimizing over a single set of class prototypes using the proposedPriMaPs pseudo labels already provides moderate improvement (+PriMaPs pseudo label in Tab. 4b), despitethe disadvantage of an unstable and noisy optimization. Adding the EMA (+EMA) leads to a more stableoptimization and further improved segmentation. Augmenting the input (+Augment) results in a furthergradual improvement.We provide a more detailed breakdown of the augmentations in Appendix A.4.Similarly, rening the prediction with a CRF improves the results further (+CRF). Assessing PriMaPs pseudo labels.To estimate the quality of the pseudo labels, respectively theprincipal masks, we decouple those from the class ID assignment by providing the oracle ground-truth classfor each mask in Tab. 5. To that end, we evaluate all pixels included in our pseudo labels (Pseudo),corresponding to the upper bound of our optimization signal. Furthermore, we evaluate All by assigningthe ignore pixels to a wrong class. The results indicate a high quality of the pseudo-label maps. We showqualitative examples of the PriMaPs mask proposals and pseudo labels in Appendix B.5.",
  "STEGO +PriMaPs-EMSTEGOPriMaPs-EMBaselineGround TruthImage": ": Qualitative results for the DINO ViT-B/8 baseline, PriMaPs-EM (Ours), STEGO (Hamiltonet al., 2022), and STEGO+PriMaPs-EM (Ours) for Cityscapes, COCO-Stu, and Potsdam-3. Our methodproduces locally more consistent segmentation results reducing overall misclassication compared to thecorresponding baseline. local consistency of the segmentation and reduced mis-classication. The comparison with STEGO as a base-line exhibits a similar trend. For further examples and comparisons with HP, please refer to Appendix B.2. Limitations.One of the main challenges is to distinguish between classes that happen to share highlysimilar SSL feature representations. This is hardly avoidable if the feature representation is xed, as wasthe case here and in previous work (Hamilton et al., 2022; Seong et al., 2023). Another limitation acrossexisting unsupervised semantic segmentation approaches is the limited spatial resolution. This limitationcomes from the SSL training objectives (Caron et al., 2021; Oquab et al., 2024), which are image-level ratherthan pixel-level. As a result, we can observe diculties in segmenting very small, nely resolved structures.",
  "Conclusion": "We present PriMaPs, a novel dense pseudo-label generation approach for unsupervised semantic segmen-tation. We derive lightweight mask proposals directly from o-the-shelf self-supervised learned features,leveraging the intrinsic properties of their embedding space. Our mask proposals can be used as pseudo la-bels to eectively t global class prototypes using moving average stochastic EM with PriMaPs-EM. Despitethe simplicity, PriMaPs-EM leads to a consistent boost in unsupervised segmentation accuracy when appliedto a variety of SSL features or orthogonally to current state-of-the-art unsupervised semantic segmentationpipelines, as shown by our results across multiple datasets.",
  "Xu Ji, Joao F. Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised imageclassication and segmentation. In ICCV, pp. 98659874, 2019": "Robin Karlsson, Tomoki Hayashi, Keisuke Fujii, Alexander Carballo, Kento Ohtani, and Kazuya Takeda.VICE: Improving dense representation learning by superpixelization and contrasting cluster assignment.In BMVC, 2022. Tsung-Wei Ke, Jyh-Jing Hwang, Yunhui Guo, Xudong Wang, and Stella X. Yu. Unsupervised hierarchicalsemantic segmentation with multiview cosegmentation and clustering transformers. In CVPR, pp. 25612571, 2022.",
  "Youngmin Oh, Beomjun Kim, and Bumsub Ham. Background-aware pooling and noise-aware loss for weakly-supervised semantic segmentation. In CVPR, pp. 69096918, 2021": "Maxime Oquab, Timothe Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-naeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.DINOv2: Learning robust visual features without supervision. In TMLR, 2024. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kpf, Edward Yang, ZachDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,and Soumith Chintala.PyTorch:An imperative style, high-performance deep learning library.InNeurIPS*2019, pp. 80248035, 2019.",
  "A.1Nearest-Neighbor Anchoring": "As described in Sec. 3 of the main paper, PriMaPs are anchoring the rst principal component in eachiteration of the mask proposal generation. Corresponding to the quantitative ndings (cf. Tab. 4a), herewe additionally analyze this qualitatively. shows similarity maps of all features to the iterativelycomputed rst principal component (Similarity 1st PC) as well as to the respective nearest-neighbor feature(Similarity 1st PC NN) for example images from Cityscapes (Cordts et al., 2016), COCO-Stu (Caesar et al.,2018), and Potsdam-3 (ISPRS). We observe that the originally computed principal direction can have highsimilarities with multiple semantic concepts in an image. Hence, nding a suitable threshold that isolates asingle main concept is dicult. However, using the nearest image feature to the principal component as an anchoring element helps tocircumvent high similarity values to multiple visual concepts. For instance, in the rst example for Cityscapes,the similarity map for the rst principal component has high similarities to both building and vegetation.In contrast, the similarity map for the nearest neighbor of the rst principal component results in highsimilarity to the class vegetation only. Consequently, a suitable mask proposal can be obtained throughthresholding. We observe this particularly for the Cityscapes dataset and some examples of COCO-Stu.Further, in cases where the similarity map is already localized to one visual concept, anchoring merely leadsto a change in the similarity values without changing the shape of the thresholded proposal.",
  "A.2Ablation of the Threshold": "In the spirit of unsupervised learning, our method eectively has only a single additional hyperparameter the threshold . Furthermore, this parameter can be set simply by examining the mask proposal as detailednext. In addition, it should be noted that we keep this parameter unchanged for all backbone models anddatasets, which further emphasizes the generalizability of our method. As described in Sec. 3 of the mainpaper, the threshold is used to remove noise in the similarity masks and to localize the optimizationsignal. In , qualitative examples of PriMaPs mask proposals are visualized for DINO ViT-B/8 onCityscapes, COCO-Stu, and Potsdam-3. We show mask proposals for = 0.3, = 0.4, and = 0.5.Visually, it can be observed that meaningful masks are produced for all thresholds. Especially those forthreshold 0.3 and 0.4 align very well with the semantic content in the images. While = 0.3 seems toprovide better mask proposals for COCO-Stu and Potsdam-3, a problem arises with Cityscapes. Here, themask proposals contain several semantic concepts and spatially small objects in a large mask (cf. the secondCityscapes example, where the mask of the bush in the left half of the image covers both the trac signin the foreground, parts of the house in the background, as well as the sky). Since this would lead to apoor optimization signal and the masks of the other two datasets using = 0.4 seem visually appealing, thethreshold is set to 0.4 in all other experiments. To further shed light on these qualitative observations, we apply PriMaPs-EM for the scenario describedabove and vary the threshold from = 0.2 to = 0.6 with a step size of 0.1.We perform this withthe DINO ViT-B/8 backbone for Cityscapes, COCO-Stu, and Potsdam-3 and show the mIoU in .The quantitative results reect our qualitative conclusion of the threshold well, and show the trade-obetween better segmentation accuracy on COCO-Stu and Potsdam-3 for a lower threshold and vice versa forCityscapes. In numbers, a threshold of = 0.5 instead of = 0.4 for Cityscapes would lead to an additionalgain of 1.4 % mIoU, but also to slight losses on COCO-Stu of 0.8 %. Additionally, this results in more maskproposal iterations. We conclude that the determined threshold = 0.4 appears to be reasonable. Even ifbetter results could be achieved for some backbone-dataset combinations with individually set thresholds,we consider a xed threshold that generalizes well across all scenarios to be sensible. We would like toemphasize that this experiment was conducted solely for this single backbone and serves only to validatethe qualitative judgement from above. Importantly, we did not determine the hyperparameters based on theevaluation sets.",
  "A.3Ablation of the Stopping Criterion": "Our method iteratively decomposes images into mask proposals. Hence, a stopping criterion is essentialto decide whether a sucient number of masks has been generated for a particular image. This allows usto divide dierent images into a dierent number of masks. We use the percentage of features assigned tomasks. Specically, the iterative PriMaPs process stops when 95% of image features are assigned to masks.While setting this value as high as possible to cover the entire image with masks might seem intuitive, not allfeatures can be assigned reasonably due to the inherent noise in the self-supervised feature representation.Setting the value too high would result in masks with very few features in the nal iterations. We chosea stopping criterion of 95%, which we conrmed quantitatively to yield good results across all datasets asshown in . The relatively low mIoU for the Potsdam-3 dataset with smaller stopping criteria resultsfrom a few initial mask proposals covering large areas, while the proposals in the nal iterations representner details.",
  "A.5Ablation on the Number of Pseudo Classes K": "Following previous works (Ji et al., 2019; Cho et al., 2021; Hamilton et al., 2022; Seong et al., 2023) inunsupervised semantic segmentation, generally the number of pseudo classes K is set to the number ofannotated semantic classes in the dataset. This is done to evaluate the performance in terms of downstreammetrics on every semantic class after matching the predicted pseudo-class IDs with the ground-truth classesusing Hungarian matching. Using a K smaller than the number of ground-truth classes is hard to evaluateand compare as it results in ground-truth classes that need to be ignored by the metric. However, it ispossible to have more pseudo classes than ground-truth classes by assigning multiple pseudo classes to asingle ground-truth class. We conduct an experimented in this over-segmentation setup, i. e. predicting Kcategories, where K is larger than the number of ground-truth classes. We realize the multi-to-one matching",
  "PriMaPs-EM (K=27; 100%)48.521.9PriMaPs-EM (K=40; 150%)53.123.2": "by applying Hungarian matching rst and subsequently matching the remaining pseudo-class IDs based ontheir highest correspondence to the ground-truth classes. In Tab. 7, we report the unsupervised semanticsegmentation results for COCO-Stu with DINO ViT-B/8 once using K = 27, which matches the numberof ground-truth classes, and for K = 40. Over clustering leads to better performance in terms of metrics,as multiple pseudo classes represent a single ground-truth class. Overall, estimating the number of pseudoclasses or semantic concepts in a dataset in an unsupervised manner represents an intriguing direction forfuture research.",
  "B.1Class-Level Quantitative Analysis": "To gain a deeper understanding of PriMaPs-EM, we assess the segmentation accuracy in terms of IoUfor individual classes. Additionally, we present the confusion matrices among the semantic classes for theDINO ViT-B/8 Baseline, PriMaPs-EM, STEGO, and STEGO+PriMaPs-EM for the COCO-Stu datasetin . Generally, we can observe that for both the DINO and STEGO baseline, for certain classes (e. g.,Appliance, Indoor, Kitchen) the discovered unsupervised class concept does not correlate with human-dened semantic classes. This suggests that the respective backbone feature representation may already behard to separate. Furthermore, some of the 27 intermediate COCO-Stu classes merge visually distinctconcepts. For instance, the class indoor combines hairbrush, toothbrush, hair dryer, teddy bear,scissors, vase, clock, and book. We see this assumption partially conrmed by analyzing the classIoUs of linear probing of the DINO features. For the problematic classes, the linear probing IoUs are in therange of approx. 16 %30 %, whereas for the other classes the IoU is 50 % and higher. We conclude that if itis already dicult to linearly distinguish the classes based on the backbone features, our method can hardlyimprove upon this. However, for classes meeting this requirement, our method clearly boosts class IoUs. In rare cases (e. g.,STEGO comparison to STEGO+PriMaPs-EM for classes Outdoor and Sports), there is a decrease in oneclass IoU with PriMaPs-EM while another class IoU increases. This can occur if the change in the prototyperepresentation results in a change of the Hungarian matching for evaluation, though this is rarely observed.In terms of class confusion, the models predictions align with the ground-truth labels. Furthermore, theexisting confusions are reasonable. For instance, when using STEGO, confusions of the two food mid-level classes emerge. Overall, a indicates that our method either enhances or at least maintains the",
  "B.2Qualitative Comparison to HP": "Similar to the qualitative comparison in in the main paper, we aim to compare PriMaPs-EM with HP(Seong et al., 2023). We present qualitative examples for the baseline, PriMaPs-EM, HP, and the combinationof HP and PriMaPs-EM in . These qualitative examples align with the ndings from the quantitativeresults in the main paper (cf. Tabs. 1 to 3). It is evident that our method produces locally more consistentand less noisy results compared to both baselines. Despite the already impressive qualitative results of HP onCOCO-Stu, our method excels in correcting misclassications and achieving better segmentation of objectboundaries. We observe one limitation, particularly with the DINO ViT-S/8 baseline, both independentlyand in conjunction with PriMaPs-EM for the Potsdam-3 dataset. In this case, the semantic concept ofstreet is not recognized and is consequently rarely predicted.",
  "B.3Comparison to HP using DINOv2": "Current state-of-the-art methods Hamilton et al. (2022); Seong et al. (2023) for unsupervised segmentationdo not provide experiments using DINOv2 features. To be able to compare with previous methods, we trainHP using DINOv2 for the comparison in Tab. 8. We strictly follow the training schedule and hyperparametersused in the original implementation for all respective datasets. HP does not generalize well to the DINOv2features and hardly keeps up with the strong baseline. PriMaPs-EM moderately but consistently improves",
  "B.4Qualitative Comparison of DINO and DINOv2": "Our experiments could not identify any clear dierences between DINOv1 and DINOv2 and the dierenttransformer architectures in terms of the downstream task performance of unsupervised semantic segmenta-tion. Both methods provide excellent feature embeddings and similar quantitative results as baselines. Whenapplying PriMaPs-EM, DINOv2 often provides slightly better quantitative results but coarser segmentation",
  "B.5Qualitative PriMaPs Pseudo-Label Examples": "Following the quantitative assessment of the pseudo labels in Sec. 4.3, visualizes qualitative examplesof the PriMaPs mask proposals and pseudo labels. We visualize individual masks, each in a dierent color(PriMaPs Colored). We also display oracle pseudo labels, assigning each mask a color based on the ground-truth label (PriMaPs Oracle class IDs). We observe that the mask proposals align well with the ground-truthlabels across all three datasets, generalizing across three distinct domains. PriMaPs eectively partitionsimages into semantically meaningful masks.",
  "B.6Failure Cases": "Finally, we would like to discuss observed failure cases of PriMaPs-EM. shows examples of failurecases occurring in the segmentation predictions as well as failure examples for PriMaPs pseudo labels. ForPriMaPs-EM, we observe misclassications, such as in Cityscapes where buses are often partially segmentedas the class car or that cobblestone is misclassied as sidewalk. For COCO-Stu, shadows and structuresinuence the segmentation predictions and confusions also occur (see ground and oor in example two).For Potsdam-3, we observe that vehicles and road markings are sometimes erroneously attributed to the classbuilding instead of road. For PriMaPs pseudo labels, we identify two main sources of error. First, smallobjects in cluttered images are sometimes assigned to larger neighboring masks, a phenomenon that can beattributed to the limited backbone feature resolution. This is particularly noticeable for Cityscapes, where thecenter horizontal area is often detailed (cf. Pole, Trac Light, Trac Sign). Second, PriMaPssometimes oversegment images containing a large foreground object as seen in the COCO-Stu example.Similarly, dierent visual appearances of the same semantic class can lead to multiple masks (Potsdam-3).Despite these observations, the simple PriMaPs provide promising mask proposals that correspond well withthe ground-truth label.",
  "CImplementation": "Since all signicant high-level implementation details and hyperparameters have been addressed in the mainpaper, this section addresses only few remaining details. Both the code and models are publicly availableat Our work is implemented in PyTorch (Paszke et al., 2019). Webuild up on the code of Ji et al. (2019), Van Gansbeke et al. (2021) and Hamilton et al. (2022).",
  "C.1Backbone Models": "For each backbone model, we use the corresponding original implementation. Specically, for DINO (Caronet al., 2021) and DINOv2 (Oquab et al., 2024), we utilize the PyTorch Hub implementation. In the case ofSTEGO (Hamilton et al., 2022) and HP (Seong et al., 2023), we integrate the respective original implemen-tations into our framework.",
  "C.2Computational Requirements": "We perform all experiments on a single NVIDIA A6000 GPU. The PriMaPs-EM optimization runtime variesbased on dataset size and the ViT architecture used as the backbone, with ViT-B/8 exhibiting the longestruntime and highest memory consumption. Specically, using DINO ViT-B/8, PriMaPs-EM requires about2.5 hours for Cityscapes and Potsdam, and approximately 4 hours for COCO-Stu, utilizing about 30GBof memory. For the lightest backbone, DINOv2 ViT-S/14, optimization times decrease to about 1.5 hoursfor Cityscapes and Potsdam-3, and around 2.5 hours for COCO-Stu, with a memory usage of about 20GB.DINO ViT-S/8 and DINOv2 ViT-B/14 lie in between.",
  "We close with some further details regarding the datasets used": "Cityscapes(Cordts et al., 2016) is an ego-centric street-scene dataset containing 5000 high-resolutionimages with 2048 1024 pixels. It is split into 2975 train, 500 val, and 1525 test images. Following previouswork (Ji et al., 2019; Cho et al., 2021; Yin et al., 2022; Hamilton et al., 2022; Seong et al., 2023), evaluationis conducted on the 27 classes setup using the val split."
}