{
  "Abstract": "Gradient flows play a substantial role in addressing many machine learning problems. Weexamine the convergence in continuous-time of a Fisher-Rao (Mean-Field Birth-Death) gra-dient flow in the context of solving convex-concave min-max games with entropy regulariza-tion. We propose appropriate Lyapunov functions to demonstrate convergence with explicitrates to the unique mixed Nash equilibrium.",
  "Introduction": "The rapid progress of machine learning (ML) techniques such as Generative Adversarial Networks (GANs)(Goodfellow et al., 2014), adversarial learning (Madry et al., 2018), multi-agent reinforcement learning(Zhang et al., 2021) has propelled a surge of interest in the study of optimization problems on the space ofprobability measures in recent years. Particularly noteworthy are the numerous works, e.g., (Hsieh et al.,2019; Domingo-Enrich et al., 2020; Wang & Chizat, 2023; Lu, 2023; Trillos & Trillos, 2023; Kim et al., 2024),illustrating how training GANs and addressing adversarial robustness can be cast as min-max games overprobability measures. In this setting, understanding the time evolution of the players initial strategies tothe equilibrium of the game leverages the use of gradient flows on the space of probability measures. Adiscussion about the applications of gradient flows in optimization and sampling can be found in a recentsurvey (Trillos et al., 2023). The Fisher-Rao (FR) gradient flow has been recently studied in the context of mean-field optimizationproblems (Liu et al., 2023), accelerating Langevin-based sampling from multi-modal distributions (Lu et al.,2019; 2023), and training shallow neural networks in the mean-field regime (Rotskoff et al., 2019). Themotivation for employing FR dynamics in sampling from multi-modal distributions is their ability to globallytransport the mass of a probability density between modes without traversing low-probability regions (Luet al., 2019). In the context of training neural networks, the benefit of using FR dynamics is similar in thatthey have the potential capability to avoid local minima of the loss function (Rotskoff et al., 2019). The present paper aims to extend these results and focuses on the continuous-time convergence of theFisher-Rao (FR) gradient flow to the unique mixed Nash equilibrium of an entropy-regularized min-maxgame.",
  "Notation and setup": "For any Z Rd, we denote by Pac(Z) the space of probability measures on Z which are absolutely con-tinuous with respect to the Lebesgue measure. Following a standard convention, elements in Pac(Z) denoteprobability measures as well as their densities. Let X, Y Rd and fix two reference probability measures(dx) eU (x)dx Pac(X) and (dy) eU (y)dy Pac(Y), where U : X R and U : Y R are twomeasurable functions. The relative entropy DKL(|) : P(X) with respect to is given by",
  "Y": "X f(x, y)(dx)(dy), for a function f : X Y R which is bounded but possibly non-convex-non-concave. Indeed, Assumption 1 is trivially satisfied by such F, while Assumptions 2 and 3 hold due to theboundedness of f. This type of objective function F is prototypical in applications including the training ofGANs (see, e.g., Arjovsky et al. (2017); Hsieh et al. (2019)) and distributionally robust optimization (see,e.g, Madry et al. (2018); Sinha et al. (2018)). Another example that can be viewed as a particular case of our general framework is the objective function ofWasserstein-GANs (WGANs) with gradient penalty (Gulrajani et al., 2017; Petzka et al., 2018). FollowingConforti et al. (2023); Kazeykina et al. (2022), GANs can be framed as a min-max problem over the spaceof probability measures. Consider the following class of parameterized functions as the class of choices forthe discriminator:{x E[f(Y, x)] : Law(Y ) = P(Y)}.",
  "Fisher-Rao (mean-field birth-death) gradient flow": "The Fisher-Rao metric was introduced by Rao (1992) via the Fisher information matrix, and since thenhas been extensively studied in the context of information geometry (Amari, 2016; Ay et al., 2017). In thiswork, we are mainly focusing on the the dynamical formulation of the Fisher-Rao metric (see e.g. .2 in Gallout & Monsaingeon (2017), Appendix C.2.1 in Yan et al. (2023) and .3 in Kondratyev& Vorotnikov (2019)). For any 0, 1 Pac(M), with M Rd, the variational representation of the FRdistance is given by",
  "Published in Transactions on Machine Learning Research (08/2024)": "in terms of the payoff function. Whereas the proof of the existence of the flow (Theorem 2.2) follows aroute similar to Liu et al. (2023), the proof of convergence in Theorem 2.3 is significantly different from Liuet al. (2023) since, as indicated in the introduction, in the present paper, convergence has to be studied withrespect to appropriately chosen Lyapunov functions and, moreover, we do not rely on the Polyak-ojasiewiczinequality.Theorem 2.3. Suppose that Assumption 2, 3 and 4 hold and let (, ) Pac(X) Pac(Y). Then themap t DKL(|t) + DKL(|t) is differentiable along the FR dynamics (19). Suppose furthermore thatAssumption 1 holds. Then, for all t > 0, we have",
  "(t, t, y)t(dy) = 0": "since the flat derivatives of V are uniquely defined up to an additive shift (see Definition B.1 in Section B).Thus, the total mass 1 of probability measures is still preserved along the gradient flow. A formal derivationof the Fisher-Rao gradient flow can be found in Appendix C. Remark 1.1. Suppose V 0 is a bilinear payoff function and any mixed strategies (, ) are supported onfinite sets of m and n pure strategies X := {x1, x2, , xm} and Y := {y1, y2, , yn}, respectively. Then P(X) and P(Y) can be approximated by the empirical measures m :=1mmi=1 xi and n :=1nnj=1 yj, respectively, and hence the Fisher-Rao gradient flow (5) retrieves the replicator dynamics studiedin evolutionary game theory (Cressman & Tao, 2014; Hofbauer & Sigmund, 1998), see also (Abe et al., 2022)for a related replicator-mutant dynamics.",
  "Sketch of convergence proof for the FR gradient flow": "For the sake of presenting an intuitive heuristic argument, we ignore here for now that the flat derivatives(, , x) V (, , x) and (, , y) V (, , y) may not exist for the V defined in (1), due to therelative entropy term DKL being only lower semicontinuous with respect to the weak convergence topology(for this reason, in our analysis in , we will replace these two derivatives with appropriately definedauxiliary functions a and b). Nevertheless, we will now demonstrate that choosing the flow (t, t)t0 as in(5), makes the functiont DKL(|t) + DKL(|t)(6) decrease in t, under the assumption of convexity-concavity of F. Let (, ) Pac(X)Pac(Y). Then assumingthe existence of the flow (t, t)t0 satisfying (5), and the differentiablity of the map t DKL(|t) +DKL(|t), we formally have that",
  "(t, t, y)( t)(dy),": "where the second equality follows from the fact thattt(x)dx =tt(y)dy = 0. Then, assuming that F(, ) and F(, ) are convex and concave, respectively (see Assumption 1), we observe that V (, ) and V (, ) are -strongly-convex and -strongly-concave relative to DKL, respectively(see Lemma 3.1 in Section A), that is",
  "t0 t00 sds , forany arbitrarily chosen > 0": "Lastly, Wang & Chizat (2023) introduces a proximal point method that can be viewed as a discrete-timecounterpart of the WFR gradient flow.Working within the framework of bilinear F, with = 0, andunique MNE, Wang & Chizat (2023, Theorem 2.2) establishes the local exponential convergence of theiterates to the unique MNE of the game with respect to the NI error and the WFR distance, provided thatthe initialization is done in close vicinity to the MNE. The algorithm proposed by Wang & Chizat (2023)assumes that both players update the positions and weights of the particles simultaneously. However, this isnot the only possible discretization for the WFR gradient flow. As highlighted in Lascu et al. (2024) for thediscrete-time Fisher-Rao gradient flow, the convergence rates of the dynamics in a game differ dependingon the order the players move: either simultaneously (players move at the same time) or sequentially (eachplayer moves upon observing the opponents moves). Although the continuous-time analysis of the gradientflow does not capture these two situations, it can serve as a guide for designing discrete-time approximationsof implementable algorithms. Moreover, it is argued in Wang & Chizat (2023) that the results of Theorem 2.2 hold only for discrete-timealgorithms, and do not imply the convergence of the continuous-time WFR gradient flow because sending thestep-sizes of the time discretization to zero will force the initialization (0, 0) to be already an MNE (see thediscussion after Theorem 2.2). Unlike the assumption in Wang & Chizat (2023) that guarantees convergenceof the discrete-time algorithm as long as the initialization is close to the MNE, i.e., NI(0, 0) r0, for somer0 > 0, depending on the step-sizes, our warm start condition (Assumption 4) for the Fisher-Rao gradientflow only imposes absolute continuity and comparability with a priori known reference measures and .",
  "Main results": "As we explained in the introduction, we study the convergence of the FR gradient flow to the unique MNE ofthe entropy-regularized two-player zero-sum game given by (1), where F : P (X)P (Y) R is a non-linearfunction and > 0. Throughout the paper, we have the following assumptions. Assumption 1 (Convexity-concavity of F). Suppose F admits first order flat derivatives with respect toboth and as stated in Definition B.1. Furthermore, suppose that F is convex in and concave in , i.e.,for any , P (X) and any , P (Y), we have",
  "F (, , y)( )(dy).(11)": "Assumption 2 (Boundedness of first order flat derivatives). Suppose F admits first order flat derivativeswith respect to both and as stated in Definition B.1 and there exist constants C, C > 0 such that forall (, ) P(X) P(Y) and for all (x, y) X Y, we haveF (, , x) C,F (, , y) C. Assumption 3 (Boundedness of second order flat derivatives). Suppose F admits second order flat deriva-tives as stated in Definition B.1 and there exist constants C,, C,, C,, C, > 0 such that for all(, ) P(X) P(Y) and for all (x, y), (x, y) X Y, we have",
  "F2 (, , x, x) C,,2F2 (, , y, y) C,,2F(, , y, x) C,,2F (, , x, y) C,": "Note that the order of the flat derivatives in and can be interchanged due to Lascu et al. (2023, LemmaB.2). Using Assumption 3, it is straightforward to check that there exist constants C, C > 0 such that forall (, ) Pac(X) Pac(Y), (, ) Pac(X) Pac(Y) and all (x, y) X Y, we have thatF (, , x) F",
  "Xf(x, y)( )(dx)(dy),": "where is the distribution of the true data. Assuming that X x f(x, y) R is differentiable and1-Lipschitz, we retrieve the WGAN. The gradient penalty regularization terms proposed by Gulrajani et al.(2017); Petzka et al. (2018) are added in order to ensure that the discriminator remains 1-Lipschitz alongthe interpolation between the generated and true data. Thus, the objective function can be expressed as",
  ",": "where g : R R is a twice differentiable concave function with bounded derivatives, > 0 is a regularizationparameter and t . Since g is concave, the term inside g is linear in and independent of , it followsthat the third term in the optimization problem is concave in . This fact together with the linearity in and of the first two terms shows that the objective function satisfies Assumption 1. Since f is bounded,differentiable and 1-Lipschitz, and g is twice differentiable with bounded derivatives, Assumptions 2 and 3are also satisfied.",
  "(y) R.(18)": "We emphasize that Assumption 4 is natural in the context of Fisher-Rao flows. From (5), we observe that thesupport of the measures along the gradient flow does not increase. Thus, it is essential that the initializationis comparable with the MNE (cf. (18)) as reflected in Assumption 4. It is observed in Liu et al. (2023), thatAssumption 4 can be understood as a warm start type of condition. Returning to the question of flat differentiability of V , which was raised in Subsection 1.2, if we assume thatF is flat differentiable with respect to both and (see Assumption 1), then the maps (, , x) a(, , x) :=",
  "(, , ) and V": "(, , ), respectively, forthose measures and for which such derivatives exist (note that we will only need to consider a and balong our gradient flow (t, t)t0, so our argument can be interpreted as stating that, while V is not flatdifferentiable everywhere, it is indeed flat differentiable along our gradient flow). The relative entropy termsDKL appear in the definition of a and b as normalizing constants to ensure that",
  "X a(, , x)(dx) = 0 and": "Y b(, , y)(dy) = 0, since we adopt the convention that the flat derivatives of F are uniquely defined upto an additive shift (see Definition B.1 in Section B). Motivated by this discussion, we define the Fisher-Raogradient flow (t, t)t0 on the space (Pac(X) Pac(Y), FR) bytt(x) = a(t, t, x)t(x),tt(y) = b(t, t, y)t(y),(19) with initial condition (0, 0) Pac(X) Pac(Y). We will establish the existence of a solution to (19) inTheorem 2.2. As we will demonstrate, the flow of densities (t, t)t0 is differentiable in time (cf. equation(35)), and hence the solution to (19) can be interpreted just as a classical solution to an ordinary differentialequation.",
  "DKL(|0) + DKL(|0).(23)": "In game theoretic language, the first result of Theorem 2.3 says that convergence of the FR dynamics (19)to the unique MNE (, ) in terms of the strategies t and t is achieved with exponential rate dependingon , while the second result shows exponential convergence in terms of the payoff function V .Remark 2.4. Exponential convergence of the single mean-field birth-death flow (t)t0 with respect toDKL(|t) can be shown to also hold in the setting of Liu et al. (2023), however it was not studied in Liuet al. (2023), which considered only convergence of V (t) for a convex energy function V : P(Rd) R.",
  "DKL(|)": "Proof of Theorem 2.3. Step 1: Differentiability of DKL with respect to the FR flow (19): Suppose thatAssumption 2, 3 and 4 hold. In order to show the differentiability of t DKL(|t) for fixed Pac(X)with respect to (19), it suffices to show that there exists an integrable function f : X R such thatt",
  "Acknowledgements": "R-AL was supported by the EPSRC Centre for Doctoral Training in Mathematical Modelling, Analysis andComputation (MAC-MIGS) funded by the UK Engineering and Physical Sciences Research Council (grantEP/S023291/1), Heriot-Watt University and the University of Edinburgh. LS acknowledges the support ofthe UKRI Prosperity Partnership Scheme (FAIR) under EPSRC Grant EP/V056883/1 and the Alan TuringInstitute. Kenshi Abe, Mitsuki Sakamoto, and Atsushi Iwasaki. Mutation-driven follow the regularized leader for last-iterate convergence in zero-sum games. In The 38th Conference on Uncertainty in Artificial Intelligence,2022.",
  "Shun-ichi Amari. Information Geometry and Its Applications. Springer Publishing Company, Incorporated,1st edition, 2016. ISBN 4431559779": "Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks. InDoina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on MachineLearning, volume 70 of Proceedings of Machine Learning Research, pp. 214223. PMLR, 0611 Aug 2017. Pierre-Cyril Aubin-Frankowski, Anna Korba, and Flavien Lger. Mirror descent with relative smoothness inmeasure spaces, with application to sinkhorn and em. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 1726317275. Curran Associates, Inc., 2022. N. Ay, J. Jost, H.V. L, and L. Schwachhfer. Information Geometry. Ergebnisse der Mathematik und ihrerGrenzgebiete. 3. Folge / A Series of Modern Surveys in Mathematics. Springer International Publishing,2017.",
  "Thomas O. Gallout and Lonard Monsaingeon. A JKO splitting scheme for Kantorovich-Fisher-Rao gradientflows. SIAM J. Math. Anal., 49(2):11001130, 2017. ISSN 0036-1410. doi: 10.1137/16M106666X": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes,N. Lawrence, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 27.Curran Associates, Inc., 2014. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville.Improvedtraining of wasserstein gans. In Proceedings of the 31st International Conference on Neural InformationProcessing Systems, NIPS17, pp. 57695779, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN9781510860964.",
  "Razvan-Andrei Lascu, Mateusz B. Majka, and ukasz Szpruch. Mirror descent-ascent for mean-field min-maxproblems, 2024. arXiv:2402.08106": "Linshan Liu, Mateusz B. Majka, and ukasz Szpruch. Polyakojasiewicz inequality on the space of measuresand convergence of mean-field birth-death processes. Applied Mathematics and Optimization, 87(3), March2023. ISSN 0095-4616. Yulong Lu. Two-scale gradient descent ascent dynamics finds mixed nash equilibria of continuous games: Amean-field perspective. In Proceedings of the 40th International Conference on Machine Learning, 2023.",
  "A.2Existence and uniqueness of the FR flow": "In this subsection, we present the proof of our main result concerning the existence and uniqueness of theFisher-Rao (FR) flow, i.e., Theorem 2.2. We construct a Picard iteration which is proved to be well-definedin Lemma A.1. Lemma A.2 shows that the Picard iteration mapping admits a unique fixed point in anappropriate metric. Then in order to conclude the proof of Theorem 2.2 we show the ratio condition (21).",
  "C": "Furthermore, since ((n)t, (n)t)t[0,T ] are considered to be absolutely continuous with respect to the Lebesguemeasure for all n, it follows that convergence in TV distance implies convergence in L1. Then due to thedominated convergence theorem, we deduce that DKL((n)t|) and DKL((n)t|) converge to DKL(t|) andDKL(t|), respectively, for Lebesgue-almost all t [0, T], as n . Hence, using again Lemma A.1 andthe dominated convergence theorem, we obtain",
  "(max{| log r1,|, log R1,} + 2 log R) =: CV,": "This gives tT V 0T V eCV,t and tT V 0T V eCV,t, and shows that t and t do not explodein any finite time, hence we obtain a global solution (t, t)t[0,), which is continuous and differentiable intime. In particular, the bounds in (33), (34), (21) and (22) hold for all t > 0.",
  "BNotation and definitions": "In this section we recall some important definitions. Following Carmona & Delarue (2018, Definition 5.43),we start with the notion of differentiability on the space of probability measure that we utilize throughoutthe paper. Definition B.1. Fix p 0. For any M Rd, let Pp(M) be the space of probability measures on M withfinite p-th moments. A function F : Pp(M) R admits first-order flat derivative on Pp(M), if there existsa function F",
  "Mf(x, y)(dx)": "Definition B.3 (TV distance between probability measures; (Tsybakov, 2008), Definition 2.4). Let (M, A)be a measurable space and let P and Q be probability measures on (M, A). Assume that is a -finitemeasure on (M, A) such that P and Q are absolutely continuous with respect to and let p and q denotetheir probability density functions, respectively. The total variation distance between P and Q is defined as:",
  "n(to the unique MNE) when regularization is added to the objective function. Here, > 0and > 0 denote the discretization step-size and regularization parameter, respectively": "To be precise in what follows, we use the terms discrete-time Fisher-Rao and Mirror Descent-Ascentas synonyms. As it is argued in Wang & Chizat (2023) (at the end of the first bullet point on page 8) aworst-case example which shows convergence of the Fisher-Rao dynamics in discrete time in terms of the NIerror at a rate O1ncan be constructed by following the arguments from Proposition 5.5, Setting II (see",
  "the proof of Proposition 5.1 for this specific setting) in Chizat (2022)": "Chizat (2022, Proposition 5.5) discusses the case of minimizing a (non-linear) convex function along theiterates of the Mirror Descent algorithm (see Algorithm 1 in (Chizat, 2022)).In order to establish theconnection between the setting in Chizat (2022) and the min-max games setting, we note that removing theterms corresponding to the Wasserstein flow in the Conic Particle Mirror-Prox (CP-MP) algorithm in Wang& Chizat (2023) (see the update rule below Lemma 2.1) and taking L = 1 retrieves the Mirror Descent-Ascent (MDA) scheme with step-size > 0 for min-max games in which players update simultaneously. Thezero-step-size limit of the MDA scheme is precisely the Fisher-Rao gradient flow.",
  "X(x)(dx)": "Note that (y)(dy) and (x)(dx) have unique minimizers = 0 and = 0, respectively,and (0) = 0. Now, assume for example that (0, 0) are uniform distributions on Y and X, respectively(recall that X, Y are assumed to be compact in Wang & Chizat (2023)). Then, following the proof of (Chizat,2022, Proposition 5.5), we obtain",
  "(, x)( )(dx) + DKL(|),": "and analogously for F . Thus, we can follow the proof of (Aubin-Frankowski et al., 2022, Theorem 4) withthe distinction that the Bregman divergence in Aubin-Frankowski et al. (2022) is actually the KL divergenceDKL in our case. More specifically, in our setup, equation (39) from the proof of (Aubin-Frankowski et al.,2022, Theorem 4), becomes"
}