{
  "Abstract": "Large models represent a groundbreaking advancement in multiple application fields, en-abling remarkable achievements across various tasks. However, their unprecedented scalecomes with significant computational costs. These models, often consisting of billions ofparameters, require vast amounts of computational resources for execution. Especially, theexpansive scale and computational demands pose considerable challenges when customizingthem for particular downstream tasks, particularly over the hardware platforms constrainedby computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjust-ing the large models over the various downstream tasks.In particular, PEFT refers tothe process of adjusting the parameters of a pre-trained large model to adapt it to a spe-cific task or domain while minimizing the number of additional parameters introduced orcomputational resources required. This approach is particularly important when dealingwith large-scale language models with high parameter counts, as fine-tuning these modelsfrom scratch can be computationally expensive and resource-intensive, posing considerablechallenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examiningtheir performance and computational overhead. Moreover, we provide an overview of ap-plications developed using different PEFT algorithms and discuss common techniques em-ployed to mitigate PEFT computation costs. In addition to providing an extensive surveyfrom an algorithmic standpoint, we also examine various real-world system designs to in-vestigate the implementation costs associated with different PEFT approaches. This surveyserves as a valuable resource for researchers aiming to understand both the PEFT algo-rithm and its system implementation, offering detailed insights into recent advancementsand practical applications.",
  "Introduction": "Large Models (LMs) have recently captured considerable public interest. Their ability to understand contextand nuances enables them to proficiently handle diverse tasks across multiple domains, including naturallanguage processing (NLP), computer vision (CV), etc. In the field of NLP, Large Language Models (LLMs)have achieved significant advancements across various tasks including text generation (Brown et al., 2020;Zhuang et al., 2023), translation (Zhu et al., 2023c; Hadi et al., 2023), personalized chat-bots (Xu et al.,2023a; Li et al., 2023a; Wu et al., 2023c), and summarization (Zhang et al., 2023a), demonstrating remarkableproficiency. Earlier studies (Brown et al., 2020) have suggested that LLMs exhibit high levels of generalization, enablingthem to apply their acquired knowledge to new tasks not included in their original training. This capabilityis commonly known as zero-shot learning. Nevertheless, fine-tuning remains essential to further enhanceLLMs for optimal performance on new user datasets and tasks. Due to its scale, a widely adopted strategy for fine-tuning LLMs involves adjusting a limited number ofLLM parameters while keeping the remainder unchanged. This technique, termed Parameter-Efficient-Fine-Tuning (PEFT), involves selectively adjusting a small proportion of their parameters while keeping the restunaltered. Furthermore, the application of PEFT extends beyond the realm of NLP and quickly attractsinterest in the CV community for handling fine-tuning vision models with large parameters, such as VisionTransformers (ViT) and diffusion models, as well as disciplinary models such as vision-language models. In this survey, we systematically review and categorize recent advancements in PEFT algorithms as well asthe system implementation costs associated with various PEFT algorithms across diverse scenarios. presents the overview content for this survey. In section 2, we present some fundamental concepts for LLMand PEFT, including computational flow for LLM, basic knowledge of PEFT, commonly used datasets andtasks, and evaluation benchmarks. We categorize all types of PEFT algorithms in according to theircomputational flow. In .1, we detail additive algorithms that either introduce new weight parametersor modify activations. Algorithms that only require fine-tuning of existing parameters are categorized asselective approaches, which are introduced in .2. In .3, we explore reparameterized PEFT,which constructs a (low- dimensional) reparameterization of original model parameters for training whiletransforming the weights back to maintain the inference speed. Additionally, there exist algorithms thatcombine the above techniques, and we have classified these as hybrid approaches, elaborating on them in.4. We also investigate strategies for further reducing the computational complexity of differentPEFT algorithms, including KV-cache management, pruning, quantization, and memory optimization, in. In , we expand the scope of this survey beyond the computational perspective to involve variouspotential application scenarios. Specifically, we explore innovations that applying PEFT techniques to dif-",
  "Published in Transactions on Machine Learning Research (10/2024)": "Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Fvqa: Fact-based visualquestion answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):24132427,2017. Qifan Wang, Yuning Mao, Jingang Wang, Hanchao Yu, Shaoliang Nie, Sinong Wang, Fuli Feng, Lifu Huang,Xiaojun Quan, Zenglin Xu, et al. Aprompt: Attention prompt tuning for efficient adaptation of pre-trained language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 91479160, 2023a. Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and Xu-anjing Huang.Orthogonal subspace learning for language model continual learning.arXiv preprintarXiv:2310.14152, 2023b. Yaqing Wang, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and JianfengGao. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprintarXiv:2205.12410, 1(2):4, 2022a.",
  "Computation flow for LLaMA": "In order to gain a deeper understanding of LLM and other Transformer-based models, we employ LLaMA-7B, a cutting-edge open-source LLM model, to scrutinize the architecture of LLM as well as Transformer. Asshown in (a), LLaMA consists of three major components: an embedding block, a stack of decoderblocks, and a head block which consists of linear and softmax layers. The embedding layers primary role isto transform unstructured textual information, into chunks of discrete numerical vectors (tokens) to facilitatesubsequent processing. The embedded tokens are then delivered to the decoder layers for further processing.Each LLaMA decoder is composed of two fundamental components: Multi-head Self-Attention (MSA) andFeedforward Network (FFN). In the MSA module, each of the tokens will be clustered by an attention mapobtained by a dot production between two linear mappings of the input tokens. Then the grouped tokens willbe further processed by a Feedforward Neural network. Additionally, Root Mean Square Layer Normalization(RMSNorm) (Zhang & Sennrich, 2019) is adopted in LLaMA as a replacement for Layer Normalization toensure efficient training. LLM distinguishes itself from other deep neural network (DNN) models such as convolutional neural net-works (CNN) in two significant ways. Firstly, LLM exhibits an inherent autoregressive nature, necessitatingmultiple iterations to complete the generation task. Moreover, LLM incorporates an attention mechanism,a component with computational complexity that scales quadratically with the length of the inputs. On theother hand, the inherent computation characteristic of LLM lies in the attention blocks inside each decoderlayer. (c) depicts the high-level overview of the computation flow in the attention block. During the inference process, each decoder takes a three-dimensional tensor x P Rbld as the input tokens.The input tokens are first multiplied with three weight matrices WQ, WK, and WV , producing the outputreferred to as query(Q), key(K) and value(V ). Given the MSA modules inability to recognize positionaldata and the inherent auto-regressive nature of LLMs, the query and key will undergo a process using RotaryPositional Embedding (Su et al., 2021a) (RoPE, denoted as Rp.q in Eq 1) to encode the position information.Subsequently, the key and value will be combined with prior tokens. After the positional embedding, the intermediate activation will then undergo a series of multiplication,softmax, and residual addition to generate MSA output as described in Eq 9. To be noted here, dk in theequation refers to the number of feature dimensions in the multi-head attention mechanism.",
  "(b)(a)": ": (a) LLaMA architecture. (b) LLaMA auto-regressive pattern. (c) Three common PEFT operations.All the learnable components are highlighted in red, while the frozen components are highlighted in grey.LoRA is applied on all the Query, Key, and Value blocks. The adapter targets the FFN module. Soft-Promptfocused on tuning the input activation of each decoder. We only show one decoder for illustration simplicity.",
  "FFNT ransfomerpxq WuppReLUpWdownxqq ` x.(5)": "The output of the last decoder layer will be sent to a linear layer, which then generates a probabilitydistribution spanning the complete vocabulary to predict the next token in the sequence. The producedtoken will then be concatenated with the previous tokens and used as the input for the next round ofprocessing. This generating process repeats in an auto-regressive manner until a full sequence of tokens,referred to as a completion, is produced ( (b)). For training, the computation flow is similar tothat for inference, except that the generated sentences are directly compared to the ground truth outputand generate the training loss. Gradients will then be computed across the LLM weights to minimize thistraining loss. To analyze the computation cost and memory overhead in LLM, we also set a series of parameters used inlater section 3. shows the parameter size and computation dimension in the LLaMA-7B model as astarting example. LLM models generate tokens (words) one for each round, depicted in Fig 2, based on the previous prompt(input) and previously generated sequence. This process will be repeated until the model outputs hits andtermination token. To accelerate the inference process in LLM models, people take the strategy of storingthe previous Keys and Values in the Key-Value cache (KV-cache), so they dont need to recalculate them foreach new token. Mathematically, we can represent the total decoders KV-cache memory cost in equation 6.In the equation, l and b are the context length and batch size and L refers to the number of layers. Thedhead is the head dimension and nhead is the number of heads.",
  "Overview on Parameter Efficient Fine Tuning": "Fine-tuning remains essential to enhance LLM performance on unseen user datasets and tasks. With the sizeof the model growing (e.g. 1.5B in GPT-2 to 175B in GPT-3), standard full fine-tuning paradigm requiresthousands of GPUs work in parallel, which is highly inefficient and unsustainable. A type of algorithmhas been raised namely Parameter-efficient fine-tuning (PEFT) which aims to tune minimal parameters toachieve better performance over full tuning on downstream tasks. In parallel developments, large-scale pre-trained models in vision and multimodal domains have also demon-strated their effective representational learning capabilities, enabling adaptation from large datasets tosmaller ones or across various data modalities through fine-tuning. Consequently, this capability has madePEFT increasingly attractive to the wider research community. We categorized the PEFT algorithms into additive, selective, reparameterized, and hybrid fine-tuningbased on their operations. As depicts, three major additive fine-tuning algorithms are normallyused: (1) Adapter; (2) Soft Prompt; (3) Others. They differ in terms of the additional tunable modules orparameters. Selective fine-tuning, on the other hand, doesnt require any additional parameters, it selectsa small subset of parameters from the backbone model and only makes them tunable while keeping themajority of parameters untouched during fine-tuning on downstream tasks. We categorized selective fine-tuning based on the grouping of chosen parameters: (1) Unstructural Masking; and (2) Structural Masking.Reparametrization represents transforming model parameters between two equivalent forms. Specifically,reparametrized fine-tuning introduces additional low-rank trainable parameters during training, whichare then integrated with the original model for inference.This approach is categorized into two mainstrategies: (1) Low-rank Decomposition, and (2) LoRA Derivatives.Hybrid fine-tuning explores thedesign spaces of different PEFT methods and combines their advantages.",
  "Downstream Tasks for LLM Evaluation": "Two types of tasks have been widely used for LLM evaluation, the first type is the General Language Under-standing Evaluation (GLUE) (Wang et al., 2018) benchmark, which integrates nine sentence or sentence-pairlanguage understanding tasks (CoLA, SST-2, MRPC, STS-B, QQP, MNLI, QNLI, RTE, and WNLI), cho-sen for their diversity in dataset sizes, text genres, and difficulty levels, and is based on established existingdatasets. It also includes a diagnostic dataset designed to evaluate and analyze model performance acrossdiverse linguistic phenomena in natural language. Additionally, it features a public leaderboard to trackperformance on the benchmark and a dashboard to visualize model performance on the diagnostic set. The other type of dataset that has been used in recent LLM papers is common sense reasoning whichintegrated into our study caters to a variety of research facets: (1) OpenBookQA (Mihaylov et al., 2018) iscurated to foster research in advanced question-answering, delving into a profound understanding of both thesubject matter and the language in which it is articulated. (2) PIQA (Bisk et al., 2020) primarily emphasizeseveryday scenarios, demonstrating a predilection for unconventional solutions. (3) Social IQA (Sap et al.,2019) emerges as a novel question-answering benchmark tailored for gauging social commonsense intelligence.(4) HellaSwag (Zellers et al., 2019) serves as a dataset, the essence of which is to ascertain the capability ofmachines in aptly concluding sentences. (5) BoolQ (Clark, 2019) is a dataset dedicated to question-answering,particularly for binary responses (yes/no queries). (6) WinoGrande (Sakaguchi et al., 2021) is introduced asa fresh compilation, encompassing a substantial 44,000 problems. (7) ARC-easy (Clark et al., 2018) presents",
  "A comprehensive benchmark is essential for readers to evaluate performance differences among various PEFTmethods under a unified standard. We next discuss several commonly used benchmarks": "From the algorithmic perspective, (Ding et al., 2023b) benchmarks the performance of several PEFT al-gorithms across more than 100 NLP tasks and conducts systematic experiments based on criteria such asperformance, convergence, efficiency, combinability, scalability, and transferability. Similarly, (Xu et al.,2023b) and (Pu et al., 2023) have also established targeted benchmarks to evaluate different PEFT algo-rithms. From the system perspective, three commonly used benchmarks are outlined below to evaluate system per-formance. The first benchmark is the ShareGPT dataset (OpenAI, 2023a), which includes real-world inter-actions with OpenAIs ChatGPT. It encompasses a broad spectrum of conversational queries and responsesthat are representative of typical user interactions with large language models (LLMs). This dataset is vitalfor evaluating the systems ability to manage diverse and realistic conversational requirements, focusing onthe accuracy of responses and efficiency in handling requests. The second benchmark involves the Microsoft Azure Function Trace from the years 2019 and 2021 (Microsoft,2023), containing logs from serverless computing activities via Azure Functions. While these logs are froma general serverless computing context rather than LLM-specific applications, they offer insights into thecomputational demands driven by events. These traces simulate the arrival patterns and workload intensitiesthat LLM systems might face, including irregular and peak demands, thus acting as practical proxies forLLM inference tasks. The third benchmark is based on the Gamma process (Moreno et al., 2014), a prevalent approach in simu-lations to model the timing of incoming requests in queueing and service systems. This method facilitatesthe creation of workloads with varied arrival rates and patterns, producing synthetic, yet realistic requestscenarios that a system could encounter during actual operations. Such synthetic workloads are crucial fortesting system performance under controlled conditions that resemble real-world user activity.",
  "PEFT Taxonomy": "The PEFT strategies can be broadly classified into four categories: additive PEFT (.1), whichmodifies the model architecture by injecting new trainable modules or parameters; selective PEFT (Sec-tion 3.2), which makes a subset of parameters trainable during fine-tuning; reparameterized PEFT (Sec-tion 3.3), which constructs a (low-dimensional) reparameterization of the original model parameters fortraining, then equivalently transforms it back for inference; and hybrid PEFT (.4), which com-bines advantages from different PEFT methods to build a unified PEFT model. An overview of differenttypes of PEFT algorithms is depicted in .",
  "Soft PromptDesign": "Prefix-tuning (Li & Liang, 2021), Prefix-Propagation (Li et al., 2023b), p-tuning v2 (Liu et al., 2021a),APT (Zhang et al., 2023h), p-tuning (Liu et al., 2021b), prompt-tuning (Lester et al., 2021),Xprompt (Ma et al., 2022), IDPG (Wu et al., 2022), LPT (Liu et al., 2022b), SPT (Zhu & Tan, 2023),APrompt (Wang et al., 2023a)",
  "Additive PEFT": "Standard full fine-tuning entails substantial computational expenses and could also potentially harm themodels generalization ability. To mitigate this problem, a widely employed approach is to maintain thepre-trained backbone unchanged and introduce only a minimal number of trainable parameters that arestrategically positioned within the model architecture. While fine-tuning for a specific downstream task,only the weights of these additional modules or parameters are updated, which results in a substantialreduction in storage, memory, and computational resource requirements.Due to their characteristic ofadding parameters, these techniques can be termed as Additive Tuning, as shown in (a). Next, wediscuss several popular Additive PEFT algorithms.",
  "Adapters": "Adapter approaches involve the insertion of small adapter layers within Transformer blocks. Typically, anadapter layer consists of a down-projection matrix Wdown P Rrd, followed by a non-linear activation functionpq, and an up-projection matrix Wup P Rdr. In this context, d represents the dimension of the hiddenlayer, and r serves as the bottleneck dimension, which is a hyperparameter used in configuring the adapters.",
  "Adapterpxq WuppWdownxq ` x.(7)": "The concept of adapters in the field of NLP was initially introduced by Serial Adapter (Houlsby et al.,2019) as shown in (a).In their approach, each Transformer block is enhanced by adding twoadapter modules, with one positioned after the self-attention layer and the other after the FFN layer,respectively. Subsequent research has aimed to address the additional computational cost associated withadapter layers. A modified framework AdapterFusion (Pfeiffer et al., 2020) was proposed, where adapterlayers are inserted only after the Add & Norm step following the FFN layer to enhance the computationalefficiency. The adapters mentioned above follow a sequential design, placing adapter layers as bottleneckswithin the Transformer blocks. This approach may potentially reduce the models parallelism and requirea trade-off between inference efficiency and accuracy. In contrast, He et al. (2021) introduced a paralleladapter (PA) approach as depicted in (b), which reorganizes the traditionally sequential adapterlayers into a parallel side-network that runs alongside each Transformer sublayer. Similarly, CIAT (Zhuet al., 2021), CoDA (Lei et al., 2023) and KronA (Edalati et al., 2022) also adopts a parallel adapter design.Except for the parallel design, CoDA employs a sparse activation mechanism to improve the inferenceefficiency as shown in (c). Specifically, CoDA uses a soft top-k selection process that identifies kimportant tokens in each layer, which will be processed by both the frozen pre-trained Transformer layer andthe adapter branch to maintain model accuracy. In contrast, those unimportant tokens are only processed bythe adapter branch while skipping the heavy pre-trained layer, therefore optimizing for inference efficiencywithout compromising overall performance. To enhance the performance and generalization of adapters, various studies have implemented multi-task learning strategies, such as AdapterFusion (Pfeiffer et al., 2020), AdaMix (Wang et al., 2022a),PHA (Zhao et al., 2023b), AdapterSoup (Chronopoulou et al., 2023), MerA (He et al., 2023b), andHyperformer (Mahabadi et al., 2021). AdapterFusion keeps all pre-trained adapters in the model andemploys a fusion module to merge the multi-task information. Unlike AdapterFusion, MerA merges pre-trained adapters into a single one through optimal transport based on weights and activations. This approachavoids introducing any additional trainable parameters, thereby enhancing computational efficiency. Hyper-former stores the multi-task information in a shared hypernetwork, which generates task and layer-specificadapter parameters conditioned on task and layer ID embeddings. Given a new task, only an additional taskembedding needs to be learned, therefore reducing the number of trained parameters.",
  "Soft Prompt": "Alternatively, prompt tuning presents an additional approach for refining the model to achieve improvedperformance through fine-tuning. Instead of optimizing discrete token representations through in-contextlearning, there is a prevailing belief that the continuous embedding space of soft prompts inherently containsmore information (Petrov et al., 2023). Drawing inspiration from this concept, researchers directly prependadjustable vectors, referred to as soft prompts, to the start of the input sequence. This can be representedas follows:",
  "Xplq rsplq1 , . . . , splqNS, xplq1 , . . . , xplqNXs(8)": "where Xplq is the sequence of input tokens for layer l, including soft prompt tokens splqifollowed by theoriginal input tokens xplqi . NS is the number of soft prompt tokens, and NX is the number of original inputtokens. Prefix-tuning (Li & Liang, 2021) introduces learnable vectors that are prepended to keys k and valuesv across all Transformer layers. To ensure stability during the optimization process, Prefix-tuning adoptsa reparameterization strategy, which utilizes an MLP layer to generate these prefix vectors rather thanoptimizing them directly. After fine-tuning, only the prefix vectors are saved for inference. This techniquehas been adapted and improved in several studies (Li et al., 2023b; Liu et al., 2021a; Zhang et al., 2023h).For instance, p-tuning v2 (Liu et al., 2021a) removes reparameterization and expands its usage to broadermodel scales and NLP tasks. APT (Adaptive Prefix Tuning) (Zhang et al., 2023h) enhances Prefix-tuningby introducing an adaptive gate mechanism to control the prefix importance in each layer.Concurrentwork p-tuning (Liu et al., 2021b) and prompt-tuning (Lester et al., 2021) apply learnable vectors onlyat the initial word embedding layer rather than all layers to enhance training and inference efficiency.Its important to highlight that prompt-tuning demonstrates its effectiveness primarily in the context oflarge models, specifically those with over 11 billion parameters (Lester et al., 2021). Complementing this,Xprompt (Ma et al., 2022) eliminates the negative prompt tokens through a hierarchically structuredpruning, which closes the performance gap at smaller model scales. Wang et al. (2023c) provides sometheoretical analysis towards prompt tuning, demonstrating its universality and limitations in limited-depthTransformers. IDPG (Instance-Dependent Prompt Generation) (Wu et al., 2022) improves prompt tuningby generating prompts based on each input sentence with a lightweight prompt generator. In a relatedapproach, LPT (Late Prompt Tuning) (Liu et al., 2022b) also leverages a prompt generator to obtaininstance-aware prompt. Unlike previous work, LPT adds these prompts only after an intermediate layer,rather than at the initial or all layers. This strategic placement eliminates the gradient calculation below theintermediate layer, thereby significantly accelerating the training speed. Simultaneously, LPT can improvethe overall performance due to the shorter backpropagation path preserves more task-related information.Inspired by LPT, SPT (Selective Prompt Tuning) (Zhu & Tan, 2023) delves deeper into the importance ofprompt inserting strategies. It introduces a learnable probabilistic gate in each layer to determine whether touse the prompt propagated from the previous layer or inject a newly generated prompt. APrompt (Wanget al., 2023a) employs another prompt inserting strategy.In addition to input prompts inserted at thebeginning of the input sequence for each Transformer layer, APrompt also prepends additional learnableprompts to the respective query, key, and value matrices in the self-attention blocks to learn new attentionpatterns. Besides, APrompt incorporates the learning of a task-specific head. The concept of soft prompts has been employed for various downstream tasks (Choi & Lee, 2023; Wu & Shi,2022), although their training can be prone to instability and slow convergence. To address this, SPoT (Vuet al., 2021) uses a source prompt learned from one or multiple tasks to initialize prompts for new tasks.Similarly, the transfer of soft prompts from one task to initialize another is proposed in TPT (transferableprompt tuning) (Su et al., 2021b), which demonstrates that a better prompt initialization results in a largetraining convergence speedup. InfoPrompt (Wu et al., 2023b) develops two mutual information-based lossfunctions, i.e., head loss and representation loss, to find better prompt initialization and learn sufficienttask-relevant information, thereby also expediting convergence. PTP (Chen et al., 2023c) delves into theroot causes of training instability. It identifies the steep nature of the loss landscape in conventional prompttuning, where minor variations in input data can lead to significant loss fluctuations.To mitigate this,PTP introduces perturbation-based regularizers to smooth the loss landscape and consequently stabilizethe training process. DePT (Shi & Lipani, 2023) decomposes the soft prompt into a shorter soft promptwith a pair of low-rank matrices, which are optimized with two distinct learning rates. This strategy notonly improves performance but also enhances training and inference efficiency. SMoP (Sparse Mixture-of-Prompts) (Choi et al., 2023) reduce the training and inference cost by utilizing short soft prompts. Duringtraining, multiple short soft prompts are trained, each tailored to specific subsets of the dataset. Duringinference, SMoP integrates a gating mechanism that routes each input instance to an appropriate shortprompt. This technique not only increases efficiency in both training and inference stages but also retains",
  ": Illustration of (IA)3 and SSF. Blue represents frozen, while yellow represents trainable": "performance comparable to those achieved with longer soft prompts. To further cut down the number of softprompt parameters, IPT (Intrinsic Prompt Tuning) (Qin et al., 2021) identifies an intrinsic task subspaceby training an auto-encoder on multiple tasks. Tuning on new tasks then requires adjusting only a fewparameters within this subspace, significantly reducing the number of training parameters.",
  "Other Additive Methods": "Apart from the methods mentioned above, there appear other approaches that strategically incorporateadditional parameters during the fine-tuning process. For example, (IA)3 (Liu et al., 2022a) introducesthree learnable rescaling vectors: lk P Rdk, lv P Rdv, and lff P Rdff , to rescale the key, value, and FFNactivations, respectively, as depicted in (a). The operations within the self-attention block can bedescribed as follows:",
  "FFNT ransfomerpxq Wupplff d pWdownxqq,(10)": "where d is Hadamard product. Furthermore, the scale vectors lk and lv can be seamlessly integrated intothe weight matrices of AQ and AW . This integration effectively eliminates the extra computational costsduring inference. A similar technique SSF (Lian et al., 2022) also performs linear transformation to themodel activations, as illustrated in (b). Specifically, after each operation (i.e., MSA, FFN, and layernormalization) in the pre-trained model, an SSF-ADA layer is injected, which performs scaling and shifting tothe features generated from the operation. During fine-tuning, only those SSF-ADA layers can be updated,while during inference, similar to (IA)3, these SSF-ADA layers can be merged into model weights, so noadditional inference overhead would be incurred. IPA (Inference-Time Policy Adapters) (Lu et al., 2023)offers a novel approach to align LLMs, such as GPT-4, with user-specific requirements without modifyingthe base models parameters. This is particularly significant when dealing with models whose parameters areextremely large and often not directly accessible. IPA achieves this by combining (through multiplicationand normalization) the output distribution of a base LLM (base policy) with that of a smaller-sized model(adapter policy) during the decoding phase. During training, the policy adapters parameters are fine-tunedusing reinforcement learning, while the base policys parameters remain fixed. During inference, IPA decodeswith the combined distribution of the base model and the trained policy adapter, tailoring it to fulfill specificuser-defined criteria.",
  "Selective PEFT": "Rather than additive PEFT, which increases the model complexity by adding more parameters, selectivePEFT fine-tunes a subset of the existing parameters to enhance model performance over downstream tasks,as depicted in (b). Specifically, given a model with parameters t1, 2, ..., nu where each i denotes an individual modelparameter and n represents the total count of these parameters, the process of selective PEFT is represented",
  "Bi is the gradient of the loss function with respect to the parameteri. In this formulation, only the selected parameters (i.e., mi 1) are updated during backpropagation": "Diff pruning (Guo et al., 2020) is a representative work that applies a learnable binary mask to the modelweights during fine-tuning.To achieve parameter efficiency, the mask is regularized by a differentiableapproximation of the L0-norm penalty. PaFi (Liao et al., 2023a) simply select model parameters with thesmallest absolute magnitude as trainable. FishMask (Sung et al., 2021) determines parameter importanceusing the approximate Fisher information. It then selects the top k parameters based on this information toform the mask M. Similarly, Fish-Dip (Das et al., 2023) also uses Fisher information to calculate M, butthe mask will be re-calculated dynamically in each train period. LT-SFT (Ansell et al., 2021) introducesanother technique to determine parameter importance inspired by the Lottery Ticket Hypothesis (Frankle& Carbin, 2018; Malach et al., 2020), where the subset of parameters that change the most during aninitial fine-tuning stage is selected to form the mask M. SAM (Fu et al., 2023) proposes a second-orderapproximation method, which approximates the original problem with an analytically solvable optimizationfunction, to help decide the parameter mask. Child-tuning (Xu et al., 2021) proposes two approaches toselect a child network during each training iteration, where only the parameters within this child networkcan be updated.",
  ": Illustration of two pa-rameter masking methods": "However, the above unstructured parameter masking results in an unevendistribution of non-zero masks and diminished hardware efficiency whenimplementing PEFT. As shown in , the structured mask orga-nizes parameter masking in regular patterns, unlike unstructured onesthat apply it randomly, thus enhancing computational and hardware ef-ficiency during training.Therefore, various structured selective PEFTtechniques have undergone extensive investigation. Diff pruning pro-poses a structured pruning strategy by partitioning the weight parametersinto local groups and strategically eliminating them together. Similarly,FAR (Vucetic et al., 2022) fine-tunes BERT models by grouping weightsof the FFN in Transformer blocks into nodes, then ranking and selectingthe learner nodes using L1 norm. To further reduce the memory accessfrequency, they also reconfigure the FFN by grouping the learner nodes.Bitfit (Zaken et al., 2021) is proposed to only fine-tune the bias parameters of each DNN layer, and achievecompetitive results for small models. However, this method fails to handle large models. Lawton et al.(2023) applies NAS to Bitfit, where S-BitFit keeps the structural nature in Bitfit that restricts NAS al-gorithm must choose whether b 0 or not for each bias module. Similar to Bitfit fine-tunes a specificmodule in Transformer, Xattn Tuning (Gheini et al., 2021) fine-tunes only the cross-attention layers. SPT(sensitivity-aware visual parameter-efficient fine-tuning) (He et al., 2023a) first identifies the sensitive pa-rameters measured by the loss reduction when being tuned. This sensitivity is calculated using a first-orderTaylor expansion, derived from a single forward and backward pass before fine-tuning in one shot. Next, SPTfinds the weight matrices whose number of sensitive parameters exceeds a pre-defined threshold and thenapplies a selected PEFT technique (e.g., LoRA and Adapter) to these targeted weights to achieve structuraltuning.",
  "to its original weight parameterization, ensuring unchanged inference speed. This procedure is depicted in (c)": "Earlier research studies (Aghajanyan et al., 2020) have shown that common pre-trained models exhibit anexceptionally low intrinsic dimensionality. In other words, it is possible to find a low-dimensional reparam-eterization that is effective for fine-tuning as the entire parameter space. Intrinsic SAID (Aghajanyanet al., 2020) is the pioneering work in investigating the intrinsic dimension feature during the fine-tuningof LLMs. However, the most widely recognized reparameterization technique is LoRA (Low-Rank Adap-tation) (Hu et al., 2021; Fomenko et al., 2024), as shown in (a). For a given pre-trained weightmatrix W0 P Rdk, LoRA introduces two trainable weight matrices, Wup P Rdr and Wdown P Rrk wherethe rank r ! minpd, kq, operating in parallel to W0. Let hin represent the input. Under normal conditions,the output through W0 is hout W0hin. Instead, LoRA modifies this output by introducing an incrementalupdate W that encapsulates task-specific knowledge:",
  "r WupWdownhin,(12)": "where denotes a scaling factor. At the onset of training, Wdown is initialized using a random Gaussiandistribution, while Wup is initialized to zero, ensuring that W initially holds a value of zero. LoRA isstraightforward to implement and has been evaluated on models with up to 175 billion parameters. Fig 8(c) used a single decoder as an example, the frozen and learnable components are highlighted in grey andred, respectively. Once fine-tuning is complete, LoRAs adaptive weights seamlessly integrate with the pre-trained backbone weights. This integration ensures that LoRA maintains the models efficiency, adding noextra burden during inference. In LoRA training, selecting an appropriate rank has always been a challenging issue.To address this,DyLoRA (Valipour et al., 2022), as depicted in (b), trains the LoRA module on a range of rankswithin a predefined training budget, rather than adhering to a single, fixed rank. Specifically, for a givenrank range R trmin, rmin `1, . . . , rmaxu, DyLoRA dynamically chooses a rank r P R at each iteration of thetraining process. Consequently, the matrices Wdown and Wup are tailored for the selected rank r, resultingin truncated versions Wdownr Wdownr1 : r, :s and Wupr Wupr:, 1 : rs, and the subsequent forward andbackward pass during this iteration will be restricted on Wdownr and Wupr instead of Wdown and Wup.With this dynamic and search-free approach, DyLoRA significantly reduces the training time required tofind an optimal and fixed LoRA rank for specific tasks. AdaLoRA (Zhang et al., 2023e) reformulates theW with a singular value decomposition (SVD), denoted as W PQ, where P P Rdr and Q P Rrk are orthometric, is a diagonal matrix containing singular values tiu1ir. All three weight matrices aremade learnable. During training, the singular values are pruned iteratively based on their importance scores,which are constructed from the moving average of the magnitude of the gradient-weight product. To ensurethe orthogonality between P and Q, i.e., P T P QQT I, an additional regularizer term is included in theloss:",
  "W 1 W ` pWWdownqWup,(15)": "where W denotes the weights or biases within the Transformer blocks feed-forward layer. Notably, duringinference, this method computes W 1 in advance, ensuring that the models inference latency remains on parwith that of traditional full fine-tuning. VeRA (Vector-based Random Matrix Adaptation) (Kopiczko et al.,2023) employs a single pair of frozen low-rank matrices Wup and Wdown that are shared across all layers, andadapts these matrices by learning small, trainable scaling vectors represented as b and d (formally denotedby diagonal matrices b and d). Specifically, the reparameterization is given by:",
  "Hybrid PEFT": "The efficacy of various PEFT methods can significantly differ across different tasks. As a result, numerousstudies aim to either combine the advantages of diverse PEFT approaches or seek to establish a unifiedperspective by analyzing the similarities among these methods. For instance, UniPELT (Mao et al., 2021)integrates LoRA, prefix-tuning, and adapters into each Transformer block. To control which PEFT submod-ules should be activated, they also introduce a gating mechanism. This mechanism consists of three smallFFNs that each produce a scalar value G P p0, 1q, which is then applied to the LoRA, prefix, and adaptermatrices, respectively. Across various setups, UniPELT has consistently shown improvements in accuracyranging from 1% to 4%. S4 (Chen et al., 2023a) explores design spaces for several PEFT methods (i.e.,Adapter (A), Prefix (P), BitFit (B), and LoRA (L)) to uncover underlying design patterns. After a series ofexperiments, their findings include: (1) Applying the spindle grouping partitioning for Transformer layers,which results in four layer groups Gi for i P t1 . . . 4u. Layers in one group have similar behaviors together,which means should apply similar PEFT strategies. (2) Allocating the number of trainable parameters tolayers uniformly. (3) Tuning all the groups. (4) Assigning different PEFT strategies to different groups. Theresulting design space that has the best performance is:",
  "G1 : pA, Lq, G2 : pA, Pq, G3 : pA, P, Bq, G4 : pP, B, Lq": "MAM Adapter(He et al., 2021) explores the intrinsic similarity between three additive PEFT methods:adapters, prefix-tuning, and LoRA, which leads to the development of three variants: Parallel Adapter,which places adapter layers alongside specific layers (SA or FFN) instead of after them; Multi-head ParallelAdapter, which divides the parallel adapter into multiple heads, each affecting the head attention outputin SA; and Scaled Parallel Adapter, which adds a scaling term after the parallel adapter layer, similar toLoRA. Extensive experimentation revealed that the most effective configuration involves using prefix-tuningin the SA layer and the scaled parallel adapter in the FFN layer, which is called the MAM Adapter. LLM-Adapters (Hu et al., 2023a) builds an easy-to-use framework that incorporates various PEFT techniquesinto LLMs. Through comprehensive benchmarking across multiple datasets, the study reveals several keyinsights: (1) The most effective locations for series adapters, parallel adapters, and LoRA are after theMLP layers, alongside the MLP layers, and simultaneously following the Attention layers and MLP layers,respectively. (2) Smaller LLMs utilizing PEFT can achieve competitive or even superior results on certaintasks when compared to their larger counterparts. (3) With appropriate in-distribution fine-tuning data,smaller models are capable of surpassing larger models in task-specific performance. Several studies leverage neural architecture search (NAS) to find better PEFT combination approaches.For example, NOAH (Zhang et al., 2022b) discovers that different PEFT configurations are specificallytailored for different tasks. To address this issue, NOAH employs NAS to identify the most effective PEFT",
  ": Taxonomy of Efficient PEFT Design": "configurations for each dataset. Specifically, NOAHs searching space encompasses three PEFT methods:Adapter, LoRA, and Visual Prompt Tuning (VPT). It utilizes AutoFormer (Chen et al., 2021a), a one-shotNAS algorithm, for the efficient discovery of optimal prompt modules. In a related vein, AUTOPEFT (Zhouet al., 2023) first establishes a searching space that includes serial adapters, parallel adapters, and prefixtuning. After that, they propose an effective NAS method based on a high-dimensional multi-dimensionalBayesian optimisation (Frazier, 2018). Both NOAH and AUTOPEFT demonstrate the capability of NAS inenhancing PEFT configurations across a variety of tasks.",
  "Efficient PEFT design": "Processing latency and peak memory overhead are pivotal factors to consider from a computational stand-point. This section introduces a key characteristic in LLMs aimed at balancing between latency and memoryusage (.1). Following this, we explore strategies for developing efficient PEFT methods to addresscomputational challenges, including PEFT pruning (.2), PEFT quantization (.3), andmemory-efficient PEFT techniques (.4), each designed to enhance model performance whileminimizing resource consumption. It is noteworthy that quantization inherently addresses memory over-head concerns. However, given its distinct characteristics, we address these quantization methods separatelyrather than incorporating them under the memory-efficient PEFT section.",
  "KV-cache Management for PEFT Efficiency": "The core of the LLMs model lies in an auto-regressive Transformer model. When we consider the auto-regression characteristic, it becomes a major challenge in designing an inference system, because every timea new token is generated, the entire LLM model has to transfer all the weights from different memoriesto the memory of the graphics processor, which is very unfriendly to single-user task scheduling or multi-user workload balance. The challenging part of serving the auto-regressive paradigm is that all previoussequences have to be cached and saved for the next proceeding iteration; the cached activation generatedfrom the previous sequences is stored as the Key-Value Cache (KV-cache). To effectively manage thesechallenges, S-LoRA Sheng et al. (2023a) employs a Unified Paging mechanism within a unified memorypool that dynamically allocates and manages memory in a paged fashion.This sophisticated approachminimizes memory fragmentation and enhances the efficiency of KV-cache storage by allowing for flexibleand efficient memory access patterns. These pages are managed such that the KV-cache associated with eachadapter is segmented into manageable blocks, streamlining access and reducing the overhead associated withvariable cache sizes. By dynamically adjusting to different KV-cache requirements, S-LoRA maintains highthroughput and performance, ensuring that the system remains responsive and efficient even as it scales toserve thousands of adapters simultaneously. This efficient handling of KV-cache is crucial for supporting theauto-regressive nature of LLMs in high-demand environments, optimizing both single-user and multi-userworkload balancing.",
  "Quantization Strategies for PEFT": "Quantization serves as another popular technique for improving computational efficiency and reducing mem-ory usage. For example, by investigating the loss landscape of adapters, BI-Adapter (Jie et al., 2023)finds that adapters are resistant to noise in parameter space. Building on this insight, the authors intro-duce a clustering-based quantization approach. Remarkably, they demonstrate that a 1-bit quantization ofadapters not only minimizes storage requirements but also achieves superior performance among all preci-sion settings. PEQA (Parameter-Efficient and Quantization-aware Adaptation) (Kim et al., 2023) uses atwo-stage pipeline to achieve parameter-efficient and quantization-aware fine-tuning. In the first stage, thepre-trained FFN weight matrix W P Rnm is quantized to W s W, where s P Rn1 represents per-channel scales and W denotes the quantized weight. In the second stage, W remains fixed, and fine-tuningis only conducted on s. This approach not only ensures memory efficiency but also facilitates parameterefficiency. QLoRA (Dettmers et al., 2023) proposes several novel techniques, including a 4-bit NormalFloat,a Double Quantization, and a Paged Optimizers, to backpropagate a 4-bit quantized pretrained languagemodel into LoRA. These techniques enable the fine-tuning for a 65B language model on a single 48GBGPU while maintaining similar performance to the full 16-bit fine-tuning. Similar to the original imple-mentation (Hu et al., 2021), QLoRA attaches the fixed zero-initialized LoRA weights to the quantizedpre-trained model as the training start point. However, when applying the extreme low-bit (e.g., 2-bit)quantization, the huge quantization error can adversely impact the initialization of LoRA fine-tuning, i.e.,quantizationpW0q ` WdownWup W0 where Wdown 0, which will harm the fine-tuning performance asshown in the work by Liao et al. (2023b). To solve this, several quantization strategies are proposed to elimi-nate the quantization error. For example, LoftQ (LoRA-Fine-Tuning-aware Quantization) (Li et al., 2023d)presents an innovative framework that provides a superior initialization point of quantized backbone weightsand LoRA weights for subsequent LoRA fine-tuning. This approach addresses the discrepancies caused byquantization through the optimization of a Frobenius norm objective during network initialization, whichtakes both the LoRA weights and the quantized pre-trained backbone into consideration. LoftQ exhibitssuperior performance in 2-bit quantization over QLoRA, as well as greater generalization for downstreamtasks. LQ-LoRA (Guo et al., 2023) uses an iterative algorithm inspired by robust principal componentsanalysis (Zhou & Tao, 2011; Wright et al., 2009) which decomposes the weight W0 such that W0 Q`L1L2to resolve the inaccuracy caused by the quantization error, where Q is the quantized component which re-mains fixed and L1L2 is the trainable low-rank component. Moreover, this approach leverages integer linearprogramming to determine a mixed quantization strategy, enabling dynamic quantization configurations foreach weight matrix while adhering to a predetermined total bit rate limit. QA-LoRA (Xu et al., 2023d)address another limitation of QLoRA, which struggles to preserve its quantized property post-fine-tuning. InQLoRA, the quantized pre-trained weight (NF4) has to be recovered to FP16 to match the LoRA weight pre-cision (FP16) during weight merging. Instead, QA-LoRA uses INT4 quantization and introduces group-wiseoperators to enable quantization during the inference stage, therefore improving the efficiency and accuracy",
  "Memory-efficient PEFT Methods": "Fine-tuning the full LLMs necessitates substantial training memory owing to their considerable size. Whilemost PEFT methods primarily target parameter efficiency, they still incur a significant memory overheadduring training because gradient computation and backpropagation are still necessary for these methods.For example, prevalent PEFT techniques such as adapters and LoRA can only reduce memory usage toapproximately 70% compared to full model fine-tuning according to some literature (Sung et al., 2022a; Jinet al., 2023). From a computational perspective, memory efficiency also remains a critical factor that cannotbe overlooked. To improve memory efficiency, various techniques have been developed to minimize the need for cachinggradients for the entire LLM during fine-tuning, thereby reducing memory usage. For example, both Side-Tuning (Zhang et al., 2020) and LST (Ladder-Side Tuning) (Sung et al., 2022a) introduce a learnablenetwork branch parallel to the backbone model. By channeling the backpropagation exclusively throughthis parallel branch, it circumvents the need to store gradient information for the main models weights,thus markedly reducing memory requirements during training. Similarly, Res-Tuning (Jiang et al., 2023)disentangles the PEFT tuners (e.g., prompt tuning, adapter) from the backbone model.On top of thedisentanglement, a memory-efficient fine-tuning framework named Res-Tuning-Bypass is proposed, whichgenerates a bypass network in parallel with the backbone model by removing the data flow from the de-coupled tuners to the backbone. This eliminates the requirement for gradient caching within the backbonemodel during backpropagation. MEFT (Liao et al., 2023b) (memory-efficient fine-tuning) is an approachinspired by the reversible model (Gomez et al., 2017). During the training of a reversible model, intermediateactivations are not required to be cached in the forward pass. During backpropagation, they can be recalcu-lated from the final output. To save the memory during fine-tuning, MEFT investigates how to transform anLLM to its reversible counterparts without additional pre-training. A critical aspect of this transformation isthe careful initialization of newly introduced parameters in the pre-trained models. MEFT demonstrates theimportance of parameter initialization and suggests that these parameters must be initialized in a mannerthat preserves the pre-trained models starting point, ensuring that the fine-tuning of the modified modelachieves performance on par with full fine-tuning methods. With this key consideration, MEFT introducesthree distinct methods, each significantly curtailing the memory demands traditionally required for storingactivations. LoRA-FA (Zhang et al., 2023b) addresses a limitation about memory overhead in LoRA fine-tuning. During training, LoRA modules still require high activation memory consumption. This is because,during backpropagation, large input activations must be stored during the forward pass to compute gra-dients. LoRA-FA resolves this issue by freezing both the pre-trained weights W0 and the projection-downweights Wdown, and only updating the projection-up weights Wup. Consequently, the input activation hin nolonger needs to be stored, as the intermediate activation Wdownhin is adequate for gradient computation forWup. Given that r ! d, the memory requirement for activations in LoRA-FA can be significantly reduced. To further reduce memory usage during fine-tuning, some methods attempt to circumvent backpropagationwithin LLMs to address this issue. HyperTuning (Phang et al., 2023) employs a HyperModel to generatePEFT parameters using only fewshot examples. This approach demonstrates results comparable to thoseobtained through full model fine-tuning. PEFT Plug-in (Jin et al., 2023) first trains PEFT modules onsmall language models, which is more memory efficient compared to training on large ones. Subsequently,the research introduces a suite of techniques for seamlessly integrating these trained PEFT modules intoLLMs during inference. This strategy effectively circumvents the necessity of gradient-based optimization",
  "PEFT for DNNs of Other Applications": "In , we outlined four categories of PEFT methods along with their improvements. Nonetheless,our discussion did not fully extend to the utilization or adaptation of PEFT techniques beyond traditionalarchitectures (e.g., LLMs) or standard benchmarks (e.g., the GLUE dataset), where the majority of thediscussed PEFT methods are applied. Therefore, in this section, we will highlight and discuss several mostrepresentative works that leverage PEFT strategies for various downstream tasks. In this section, we donot aim to cover all PEFT application scenarios. Our objective is to showcase the significant influence ofPEFT within various research domains and demonstrate how to optimize and tailor general-purpose PEFTmethods to achieve enhanced performance in specific models or tasks. Typically, fine-tuning happens when adapting a pre-trained backbone model to specialized downstream tasks.To this end, this section organizes the discussion around various model architectures, which include: LLM,Vision Transformer (ViT), Vision-Language Alignment Model (VLA), and Diffusion model. Within eacharchitectural category, the discussion is further classified based on different downstream tasks.",
  "PEFT for LLMs Beyond the Basics": "Instead of common tasks in NLP such as NLU and NLG, PEFT techniques boast a wide array of applica-tions across diverse scenarios. PEFT has been successfully implemented in commonsense question answer-ing (Huang et al., 2023c; Zhao et al., 2023d), multi-level implicit discourse relation recognition (Zhao et al.,2023c), out-of-distribution detection (Ouyang et al., 2023), privacy protection (Ozdayi et al., 2023; Xiaoet al., 2023b), federated learning (Che et al., 2023), and social biases mitigation (Li et al., 2023c). In thissection, we pay more focus on three representative downstream tasks: visual instruction following, continuallearning, and context window extension.",
  "Visual Instruct Following": "Several studies, including VL-BART (Cho et al., 2021), MiniGPT-4 (Zhu et al., 2023b), and LLaVA (Liuet al., 2023a), have successfully extended the capabilities of LLMs, initially designed for pure text, to com-prehend and generate responses to visual inputs. These enhanced models, namely visual instruct-followingLLMs, can process both images and text to produce textual responses, which can be benchmarked on taskssuch as image captioning (Rennie et al., 2017; You et al., 2016; Vinyals et al., 2016; Hossain et al., 2019)and visual question answering (VQA) (Wang et al., 2017; Wu et al., 2017; Antol et al., 2015). However,these methods fine-tune the entire LLM to learn the visual representations, which can be inefficient in bothtime and memory. Therefore, it is natural to apply PEFT techniques in the fine-tuning of visual instruct-following LLMs. An earlier work VL-Adapter (Sung et al., 2022b) directly applies several PEFT methods(Adapter (Houlsby et al., 2019), Hyperformer (Mahabadi et al., 2021) and Compacter (Karimi Mahabadiet al., 2021)) on VL-BART (Cho et al., 2021) then benchmarks them on several image-text and video-texttasks. Results show that vanilla adapters are the best among them, which can achieve performance on parwith full fine-tuning. However, considering the functionality gap between the encoders and decoders in VL-BART, directly assigning identical modular modifications will lead to suboptimal performance. Therefore,",
  "VL-PET (Hu et al., 2023b) selectively integrates PEFT modules into different components of the encoderand decoder. They also introduce a granularity-controlled mechanism for finer-grained control": "To adapt the recently prevalent LLaMA model, LLaMA-Adapter (Zhang et al., 2023f) prepends a setof learnable prompts (similar to prefix tuning) to the input tokens in LLaMAs higher transformer layers.To avoid the unstable fine-tuning with large loss values at early training stages, instead of the randomlyinitialized weights of other PEFT methods, LLaMA-Adapter adopts a zero-initialized attention mechanism,which learns a zero-initialized gating factor to adaptively control the contribution of adaptation promptsto the word tokens. This can maintain the fine-tuning starting point the same as the original model andprogressively inject new knowledge into the model, where a similar idea can be found in MEFT (Liaoet al., 2023b) and LoftQ (Li et al., 2023d) discussed earlier.To represent visual information, LLaMA-Adapter extracts multi-scale global image features using a CLIP image encoder and then projects them tolinguistic embedding space. After that, the feature is element-wisely added onto the adaptation promptsat all inserted transformer layers. LLaMA-Adapter only introduces 1.2M learnable parameters in LLaMA-7B and costs less than one hour for fine-tuning on 8 A100 GPUs. A following work LLaMA-AdapterV2 (Gao et al., 2023b) demonstrates that the simple multimodal fusion in LLaMA-Adapter cannot generalizeto more challenging open-ended multimodal reasoning tasks, where the visual cues tend to dominate theadaptation prompts than the language instruction data. To address this, LLaMA-Adapter V2 decouples thelearning of instruction-following ability (to generate long language responses) and vision-language alignmentto avoid interference between visual and language fine-tuning. Specifically, LLaMA-Adapter V2 sets disjointparameter groups which are respectively learned from image-text pairs and language instruction data. Thevisual adaptation prompts are inserted in the early stage of LLM, while the language adaptation promptsremain at the higher transformer layers similar to the LLaMA-Adapter. Additionally, LLaMA-Adapter V2introduces more learnable parameters and several expert systems (e.g., captioning, detection, and OCR) toenhance multimodal performance. LayerNorm Tuning (Zhao et al., 2023a) adjust only the weights ofthe LayerNorm within each attention block. This straightforward technique can achieve comparable or evenbetter performance than the finetuning, while offering about 10 more parameter efficiency than LoRA.",
  "Continual Learning": "Continual Learning (CL) aims to learn a sequence of new tasks over time within one single model, which hasbroad application in scenarios such as dialogue systems (Lee, 2017), information extraction systems (Changet al., 2006), and question answering systems (Yang et al., 2019). The main challenge in CL is catastrophicforgetting (Kirkpatrick et al., 2017). A popular practice, called architecture-based methods, tackles the CLby maintaining task-specific parameters in the model for each new task. Therefore, its natural to leveragePEFT methods for CL tasks (Madotto et al., 2020; Zhu et al., 2022; Dai et al., 2022; Liang et al., 2023). Forexample, AdapterCL (Madotto et al., 2020) parameterizes each new task using residual adapters. Duringtesting, since the task-id is not provided, AdapterCL uses an entropy-based classifier to select which adapterto use for accomplishing a specific task. CPT (Continual Prompt Tuning) (Zhu et al., 2022) trains a softprompt for each task. Instead of training soft prompts from scratch, CPT proposes a series of techniques(continual prompt initialization, query fusion, memory replay, and a memory-guided technique) to achieveknowledge transfer from preceding and subsequent tasks. O-LoRA (orthogonal low-rank adaptation) (Wanget al., 2023b) employs a strategy of learning distinct tasks within separate low-rank vector subspaces thatare kept orthogonal to each other in order to minimize interference. This approach can effectively reducecatastrophic forgetting during the acquisition of new tasks.",
  "Context Window Extension": "LLMs are typically trained with a pre-defined context size. For example, LLaMA and LLaMA2 have pre-defined context sizes of 2048 and 4096 tokens, respectively. The positional encoding RoPE has weak extrap-olation properties (Chen et al., 2023d), which means the performance drops obviously given an input lengthexceeds the pre-defined context length. To solve this, a naive solution is to fine-tune a pre-trained LLMto a longer context. However, this escalates computational costs quadratically with context size, strainingmemory and processing resources. To address this, LongLoRA (Chen et al., 2023e) proposes to fine-tunea pre-trained LLM using LoRA to enlarge the context size. To reduce the perplexity gap between LoRA",
  "PEFT for ViTs": "ViT (Dosovitskiy et al., 2010) has emerged as a powerful backbone model in the recent computer visioncommunity.In the ViT model, images are treated as sequences of fixed-size patches analogous to howLLM uses discrete tokens. These patches undergo linear embedding and then receive positional encodings.Subsequently, they are processed through standard Transformer encoders. The training of ViT can be super-vised (Dosovitskiy et al., 2010; Steiner et al., 2021) or self-supervised (Chen et al., 2021b; He et al., 2022a),and ViT can achieve superior performance when training with more data and using larger model size (De-hghani et al., 2023). However, such scaling up inevitably escalates training and storage costs. Therefore,similar to LLMs, PEFT is widely implemented in various downstream tasks, such as dense prediction (Chenet al., 2022b), continual learning (Wang et al., 2022b; Gao et al., 2023c), deep metric learning (Ren et al.,2024). Here, we focus on two typical tasks to showcase the involvement of PEFT: image classification andvideo recognition.",
  "Image Classification": "Image classification on targeted visual datasets is a very common demand and has extensive applications,while pre-train then fine-tuning paradigm serves as a widespread strategy. A variety of methods leveragePEFT techniques to achieve efficient model tuning (Jia et al., 2022; Chen et al., 2022b;a; Jie & Deng,2022). For instance, AdaptFormer (Chen et al., 2022a) inserts adapter modules in parallel to the FFNof the original ViT model for visual recognition tasks. VPT (Visual Prompt Tuning) (Jia et al., 2022)prepends a small amount of task-specific parameters into the input sequence of each Transformer layer.When applying ViT to downstream tasks, only these added parameters and the classification head are setto trainable. Yoo et al. (2023) notices that compared with supervised ViT, VPT often underperformswith self-supervised ViT. Further analysis demonstrates that different pre-trained methods and downstreamtasks have varying degrees of dependency on transformer blocks at different locations. To tackle this issue,the research introduces adaptable gates for ViT blocks. These gates dynamically modulate the contributionof prompt tokens to ViT blocks, allowing for a more targeted adaptation of the model to the task at hand.",
  "Video Recognition": "Several works consider the more challenging adaptation problem that transfers ViT to downstream tasks thathave a much larger domain gap. For example, ST-Adapter (Spatio-Temporal Adapter) (Pan et al., 2022)and AIM (Yang et al., 2023c) both insert adapters layers into pre-trained ViT blocks. Their primary goal isto model spatial-temporal information, thereby enabling efficient adaptation of ViTs from image models tovideo tasks. Notably, both methodologies have exhibited performance that surpasses traditional full-modelfine-tuning approaches.",
  "PEFT for VLAs": "Vision-language alignment models (VLA), such as CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021),DeCLIP (Li et al., 2021), and FLAVA (Singh et al., 2022), are designed to learn a good image and textfeatures which can be aligned within a unified representation space. Each VLA typically consists of separateimage and text encoders that extract respective features. Contrastive learning is leveraged in these modelsto effectively align the image and text features. Fine-tuning is leveraged to improve the performance of VLAin specific datasets or tasks, but fine-tuning the full model is computationally intensive. For instance, fine-tuning CLIP RN50x64 requires a batch size of 32,768 and 18 days of training on 592 V100 GPUs (Radfordet al., 2021). Moreover, full fine-tuning on smaller datasets often leads to catastrophic forgetting (Kirkpatricket al., 2017). In response to these challenges, and drawing inspiration from the success of PEFT techniquesin NLP, a range of PEFT strategies have been proposed and implemented in VLA models, such as semanticsegmentation (Xu et al., 2023c; Yu et al., 2023; Xu et al., 2023e), point cloud understanding (Zhang et al.,2022a; Zhu et al., 2023d; Wang et al., 2022c; Huang et al., 2023b), video understanding (Ju et al., 2022;Ni et al., 2022; Lin et al., 2022), visual reasoning (Han et al., 2023b; Doveh et al., 2023), temporal actiondetection (Nag et al., 2022), to name a few. This section will focus on one common task that uses VLAs:open-vocabulary image classification.",
  "Open-vocabulary Image Classification": "In open-vocabulary image classification, earlier works design class-specific prompts, e.g., a photo of a[CLASS], for each category, and rank images based on their similarity to these textual descriptions. CoOp(Context Optimization) (Zhou et al., 2022b) replaces the handcrafted text prompt with learnable vectors,while keeping the entire VLA fixes during training. CoCoOp (Conditional Context Optimization) (Zhouet al., 2022a) builds on this by tackling CoOps limitations in generalizing to unseen classes. It introduces alightweight neural network that generates an input-specific context token, dynamically adapting the promptbased on each image, thereby enhancing generalizability, but at the cost of increased computational demandsdue to the instance-aware operation. ProGrad (Zhu et al., 2023a) addresses the over-fitting risk in CoOp ina few-shot setting by regularizing the soft prompt updates whose gradient is aligned to the general knowledgeonly updates the prompt whose gradient is aligned (or non-conflicting) to the general knowledge offered bythe original prompt. MaPLe (Khattak et al., 2023) notes that existing methods learn prompts either inthe language or in the vision branch of CLIP, which is not efficient in leveraging the multimodal nature ofVLAs. To address this, MaPLe proposes branch-aware hierarchical prompts that simultaneously adapt bothlanguage and vision branches, and achieves superior performance. TPT (test-time prompt tuning) (Shuet al., 2022) studies prompt tuning on the fly without additional training samples. Specifically, during infer-ence, TPT first augments the input image into various views, which are then utilized to tune the learnableprompts. The primary training objective is to ensure the VLA can generate consistent responses when facedwith these differing views. A following work DiffTPT (Feng et al., 2023) further enhances the data diversityof test samples through diffusion models. In another direction, several studies explore the usage of adapters in VLA. For example, CLIP-Adapter (Gao et al., 2023a) integrates residual-style adapters after CLIPs text and visual encoders.Therefore, unlike CoOp and CoCoOp, CLIP-Adapter avoids the gradient backpropagation through CLIPsencoders, leading to reduced computational requirements in terms of both training memory and time. Tip-Adapter (Zhang et al., 2021) adopts the same design with CLIP-Adapter. Different from CLIP-Adapter,the weights of the adapter are obtained in a training-free manner from a query-key cache model (Orhan,",
  "PEFT for Diffusion Models": "Diffusion models (Ho et al., 2020; Sohl-Dickstein et al., 2015) are a class of generative models that learn togenerate data by transforming random noise into a structured output by a progressive denoising process.During training, diffusion models learn to reverse the noise added to training data using a denoising network,while in inference, they start from noise, using a denoising network to iteratively create data that mirrorsthe same distribution as the training examples.Diffusion models have various applications (Han et al.,2023a; Yang et al., 2023b; Croitoru et al., 2023; Dhariwal & Nichol, 2021; Ruiz et al., 2023), while the mostnotable is stable diffusion (Rombach et al., 2022), which bridges the gap between text and image with itsrobust capability to generate coherent and contextually relevant images directly from textual descriptions.Numerous studies leverage PEFT techniques to adapt a pre-trained diffusion model for downstream tasks,including accelerating sampling speed (Luo et al., 2023; Chai et al., 2023a), text-to-video adaptation (Wuet al., 2023a; Xing et al., 2023), text-to-3D adaptation (Zeng et al., 2023a), etc. This section mainly focuses ontwo scenarios: integrating additional input modalities beyond mere text-based conditioning, and customizingcontent generation based on pre-trained diffusion model.",
  "Additional Input Control": "To incorporate additional input modalities (e.g., layout, keypoints) while retaining the extensive knowledgein the pre-trained model, GLIGEN introduces a novel approach, which maintains the original modelsweights intact and integrates new, trainable gated Transformer layers (Alayrac et al., 2022) that take in thenew grounding input. The resulting model can not only accurately represent the grounding conditions butalso produce high-quality images. Remarkably, the model can also generalize well to unseen objects duringinference. ControlNet (Zhang et al., 2023c) fine-tunes a trainable copy of the encoding layers from StableDiffusion while locking its pre-trained parameter weights. The fixed original model and the trainable copyare bridged through zero convolution layers. These layers, starting with zero-initialized weights, are designedto progressively adapt during training, ensuring that harmful noise does not affect the pre-trained featuresof Stable Diffusion at the beginning of training. This refined model is capable of conditioning on a varietyof inputs such as Canny edges, Hough lines, user scribbles, human key points, segmentation maps, shapenormals, depths, etc. Concept Sliders (Gandikota et al., 2023) introduces a plug-and-play LoRA adaptorsto allow precise editing of concepts (e.g., age, smiling) within a diffusion model. T2I-Adapter (Mou et al.,2023) introduces a lightweight adapter model designed to align external control signals with the internalknowledge of text-to-image diffusion models. This adapter enables precise manipulation through structuralcontrol (e.g., sketch, depth map, semantic segmentation map, and keypose), color control (e.g., hue and colordistribution), and integrating various controls by composing multiple adapters.",
  "Customized Generation": "The effectiveness of text-to-image diffusion models is limited by the users ability to articulate the desiredtarget through text descriptions. For instance, it is difficult to describe the precise features of an innovativetoy car which is not encountered during large-scale model training. Consequently, the objective of customizedgeneration is to enable the model to grasp new concepts from a minimal set of user-supplied images. TextualInversion (Gal et al., 2022) addresses this by finding a new pseudo-word S (similar to soft prompt discussedin .1.2) that represents new, specific concepts in the textual embedding space of pre-trained text-to-image diffusion models. The pseudo-word S is optimized via the original optimization goal in diffusionmodels given a small image set (typically 3-5 images) depicting the concept, and the pre-trained model isleft untouched. During inference, S can be treated like any other word and composed with other textualqueries (e.g., \"a photo of S on the beach\").Custom Diffusion (Kumari et al., 2023) tackles a morechallenging setting: compositional fine-tuning of multiple concepts. It fine-tunes only the Wk, Wv mappingfrom text to latent features in attention layers, which yields superior performance in multi-concept learningscenarios. Additionally, during fine-tuning, Custom Diffusion prevents model forgetting by introducing asmall set of real images with captions akin to the target, alongside employing augmentation for faster",
  "System design for PEFT": "In this section, we begin by providing a concise overview of cloud-based PEFT systems and analyzing the de-sign challenges. These include the efficient handling of numerous task-specific queries via centralized PEFTquery servicing, the resolution of privacy and data transmission issues through distributed PEFT training,and the complexities associated with concurrent multi-PEFT training processes. Centralized systems arerequired to process a substantial volume of queries with minimal latency and maximal throughput. Dis-tributed training frameworks must address privacy concerns and the computational inefficiencies that arisefrom data exchanges between users and cloud services. Furthermore, multi-PEFT training necessitates theoptimization of memory utilization, the management of simultaneous model training, and the formulation ofsystem architectures capable of supporting multi-tenant workloads effectively. These challenges underscorethe imperative for innovative approaches to improve scalability, safeguard privacy, and optimize resourceallocation in PEFT system architectures. Following this, we present the corresponding metrics employedfor evaluating the system performance. Furthermore, we delve into three prospective utilization scenarios toillustrate the challenges in system design.",
  "Centralized PEFT Query Serving": "Cloud providers have recently introduced a range of LLM services aimed at providing user applicationsthrough application programming interfaces (APIs) (OpenAI, 2023b; Team et al., 2023). These APIs facil-itate the seamless integration of many machine-learning functionalities into applications. When receivingone query for one specific downstream task through API, the cloud-based server processes the query withone featured LLM model. Under this scenario, the importance of PEFT becomes apparent. Cloud providersstore only a single copy of the LLM and multiple PEFT modules featuring different downstream tasks. Thissetup allows the LLM to maintain various branches of PEFT modules, each linked to specific API queries,i.e., PEFT queries. Centralized PEFT query serving solutions address scenarios where multiple PEFT queries arrive in quicksuccession. A case study of one state-of-the-art system for this purpose is discussed in .2. Fig-ure 10 (b) illustrates the computation pattern for multi-query PEFT inference, wherein packed PEFT queriesare scheduled and executed according to their deadlines and current system conditions.",
  "Distributed PEFT Training": "In most cases, personalized tasks are not fully supported with pre-trained models, consequently, extra fine-tuning is required to be executed with the methodologies mentioned in the previous sections. However,significant concerns arise when considering the transfer of datasets to cloud providers, given the issuesrelated to data privacy, copyright, proprietary information, and the complexities and inefficiencies involvedin data transmission. .3 gives two approaches that address this concern.",
  "Multi-PEFT Training": "Different from multiple-PEFT serving, tuning with multiple customized PEFTs always involves differentbackbone LLMs. Therefore, simultaneously tuning multiple PEFTs can pose considerable challenges. Chal-lenges like how to manage memory gradient and model weights storage, and how to design an efficient kernelfor batching PEFT training remain unsolved. PEFTs will be categorized based on their PEFT algorithmsand backbone LLM models. The design challenge involves how to consolidate multiple PEFTs with the sameLLM backbone and multiple different LLM backbones simultaneously. We present case studies related tothis topic in .4.",
  ": Coordinated Batching (CB) Strategy": "techniques into collective units. PetS, as introduced in Zhou et al. (2022c), advocates for a comprehensiveapproach to managing multiple PEFT tasks by suggesting a unified serving framework. The frameworks coreadvancement lies in the translation of varying PEFT tasks into integrated computation kernels to enhanceefficiency. Moreover, PetS pioneers an orchestrated batching approach and a scheduling methodology, aimingto augment system throughput and leverage task parallelism respectively. As depicted in , the PetS framework begins with users registering PEFT tasks through a standard-ized Application Programming Interface (API). Upon registration, developers are expected to provide thePre-Trained Model Tag (e.g., LLaMA), PEFT parameters in a compressed format, and the specific PEFTalgorithms (e.g., LoRA, Adapter, Bitfit, etc.). These tasks are then endowed with unique identifiers, and theinference engine takes charge of query processing. PetS bifurcates the primary computational workload (e.g.,linear layer computations) into three distinct computational operations: (1) Dense Matrix-Vector Multipli-cation (MVM) leveraging universally accessible, pre-trained weights. (2) Bias vector addition (Vadd), usingeither common or task-exclusive biases. (3) A combination of Sparse/dense MVM operations employingtask-specific PET parameters. A unified pre-trained weight matrix W is employed across PetS, facilitatingthe batching of initial operations, Xt W. However, subsequent task-specific computations involving PETparameters, despite being relatively minimal in complexity, are processed individually. Considering the Adapter and Bitfit tasks as an illustration, both aim at the MLP component of LLMs.The Adapter task integrates additional weight segments, whereas Bitfit adjusts bias elements. The Adapteroperation is modeled as Y Xin1 pW ` Wadq ` b0, where Xin1 represents the input for the Adapter task,W and Wad are the original and adapter-specific PEFT weights respectively, and b0 is the initial bias. TheBitfit operation, on the other hand, is defined as Y Xin2W `b1, with b1 symbolizing the Bitfit-adjustablebias. These operations are further synthesized as tY1, Y2u tXin1, Xin2u W ` tXin1 Wad, 0u ` tb0, b1u,delineating that the tXin1, Xin2uW part is amenable to batching through MVM, while the tb0, b1u segmentpertains to the Vadd operation. For tasks like Diff-Pruning 3.2, is a little bit different than Bitfit and Adapter.For Diff-Pruning, thecomputation concerning the shared weight and difference are conducted separately. Then the results areadded up, namely",
  ", here, the W denotes the backbone model weights while t denotes the pruned weights which can berepresented as Sparse MVM": "The other challenge PetS proposed is how to schedule different PEFT requests to achieve high performance.PetS scheduler achieves high parallelism through a two-level scheduling policy: Coordinated Batching (CB)and Macro-batch Streaming (MS) as depicts. Through CB, the input queries will first be clusteredbased on their input length and then grouped based on their shared operator. This is to make sure the samesequence length of queries will be executed without wasting padding. MS strategy will take the grouped",
  "queries after coordinated batching and the theoretical latency for different operators as well as the systemmodeling parameters to generate the best execution order": "The other example design is DLoRA Wu et al. (2024a), which introduces a system that improves the ef-ficiency of serving low-rank adaptation (LoRA) models for large language models (LLMs) by dynamicallymanaging the merging and unmerging of LoRA adapters and the migration of requests across worker replicas.This dynamic orchestration addresses the challenges of high memory footprints, low GPU utilization, andload imbalance caused by variable input and output lengths in traditional LLM serving systems. dLoRAsnovel approaches, including a credit-based batching algorithm and a request-adapter co-migration algorithm,significantly enhance throughput.",
  "Distributed PEFT Training Frameworks": "We already know that fine-tuning LLM for downstream tasks is challenging for two reasons: dual privacyconcerns between cloud server and data owner, and issues with computational resources and efficiency.Firstly, the privacy of both parties is at risk: the weights of large models are often proprietary and not madepublic. Sharing data with model owners for fine-tuning can lead to data privacy concerns while providingmodel weights to data proprietors could compromise the ownership of proprietary models. Secondly, evenif downstream users have access to pre-trained weights, the stringent hardware requirements make transferlearning impractical for most end users. To resolve these two issues, DLoRA (Gao & Zhang, 2024) presents a distributed PEFT framework. Duringthe PEFT process, the backbone LLM is executed in the cloud servers while the PEFT modules are trainedentirely within the user devices. DLoRA scheme is depicted in (a). Similarly, Offsite-Tuning (Xiao et al., 2023a) presents a privacy-preserving and efficient transfer learningframework that enables foundational models to adapt to downstream tasks without the need to access thecomplete model weights.The key insight of Offsite-Tuning is the cloud provider sends an adapter andan emulator to the data proprietor. Then, with the assistance of the emulator, the data proprietor fine-tunes the adapter. The fine-tuned adapter is then sent back to the cloud side, which integrates it into thecomplete model, creating a fine-tuned foundational model for downstream users. Offsite-Tuning safeguardsthe privacy of data proprietors since they do not need to share their training data directly. It also protectsthe foundational model owners, as the complete model weights are not shared, and the emulator providedis lossy, with significantly degraded performance. Compared to existing fine-tuning methods that requireaccess to the full model weights, Offsite-Tuning is more resource-efficient because it allows for fine-tuningthrough a compressed emulator without needing the complete model.",
  "Designing an efficient system for multi-tenant serving with different LLM backbones": "Efficient kernel designPunica addresses the first challenge by using existing matrix multiplication forthe backbone computation and introducing a new CUDA kernel, Segmented Gather Matrix-Vector Multi-plication (SGMV), for adding the PEFT add-ons to the backbone computation in a batched manner. Thiskernel parallelizes the feature-weight multiplication for different requests in the batch and groups requestscorresponding to the same PEFT model to increase operational intensity and use GPU Tensor Cores foracceleration. The second challenge is beyond the computational cost, designing an efficient system architecture that caneffectively serve multi-tenant PEFT model workloads on the smallest set of GPUs possible while occupying",
  "Conclusion and Future Directions": "In the current era dominated by large models and large datasets, PEFT stands out as a highly attractivemethod for efficiently adapting models to downstream tasks. This technique gains its appeal by addressingthe significant challenges posed by traditional full-model fine-tuning, which often places substantial computa-tional and data demands. This survey offers a comprehensive examination of the most recent advancements inPEFT, including algorithmic design, computational efficiency, application scenarios, and system implemen-tation for PEFT. It offers a comprehensive taxonomy and explanation that serves as an excellent guidanceand knowledge base, which enables readers of various levels and disciplines to swiftly grasp the core conceptsof PEFT.",
  "Simplify hyperparameter tuning": "The effectiveness of PEFT is often sensitive to its hyperparameters, such as the bottleneck dimension of theadapter, the rank of LoRA, and the arrangement of various additive PEFT layers. Manually tuning thesehyperparameters will cost lots of effort. Therefore, future efforts could focus on developing methods thatare less dependent on manual tuning of these parameters, or automatically find the optimal configurationsettings. Several studies (Valipour et al., 2022; Zhang et al., 2023e; Ding et al., 2023a; Chen et al., 2023a;Zhang et al., 2022b; Zhou et al., 2023) have started to address this issue, but theres a need for more simpleand efficient solutions optimizing these hyperparameters.",
  "Establish a unified benchmark": "Despite the existence of libraries like HuggingFaces PEFT (Mangrulkar et al., 2022) and AdapterHub (Pothet al., 2023), a comprehensive benchmark for PEFT is still lacking. This gap hinders the ability to fairlycompare the performance and efficiency of different PEFT approaches. A well-accepted, up-to-date bench-mark akin to MMDetection (Chen et al., 2019) for object detection would enable researchers to validatetheir methods against a standard set of tasks and metrics, fostering innovation and collaboration within thecommunity.",
  "Enhance training efficiency": "The presumed parameter efficiency of PEFT is not always consistent with computational and memory savingsduring training. Given that trainable parameters are intertwined within the pre-trained models architecture,computing and storing activations and gradients for the full model often become necessary during fine-tuning.This oversight calls for a rethinking of what constitutes efficiency. As outlined in , potential solutionslie in the integration of model compression techniques such as pruning and quantization, alongside innovationsspecifically designed to optimize memory during PEFT tuning (Zhang et al., 2023g). Further research intoenhancing the computational efficiency of PEFT methodologies is imperative.",
  "Explore scaling laws": "The design and effectiveness of PEFT methods originally developed for smaller Transformer models do notnecessarily scale with larger models. As the size of foundation models increases, identifying and adaptingPEFT strategies that remain effective is crucial. This investigation will aid in customizing PEFT method-ologies to suit the evolving landscape of large model architectures.",
  "Enhancing data privacy": "Trusting centralized systems to serve or fine-tune personalized PEFT modules is yet another issue for systemdevelopers. Multiple types of inversion attacks (Dosovitskiy & Brox, 2016; He et al., 2019) have been proposedto reconstruct users data by hijacking the intermediate results. One perspective of future trust-worthy LLMsystem design involves developing an encryption protocol for both personal data and intermediate trainingand inference results.",
  "PEFT with model compression": "Model compression is one of the most effective ways to make LLM executable on resource-limited devices. Yet,the impact of model compression techniques on the performance of PEFT algorithms running on hardwareremains another systemic challenge. Common compression techniques such as quantization and pruningnecessitate dedicated hardware platforms to expedite the process, and building such hardware platforms forcompressed models is yet another direction for future research.",
  "TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh.Videogenerationmodelsasworldsimulators.2024.URL": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Weilong Chai, DanDan Zheng, Jiajiong Cao, Zhiquan Chen, Changbao Wang, and Chenguang Ma. Speedup-net:A plug-and-play hyper-network for accelerating text-to-image diffusion models.arXiv preprintarXiv:2312.08887, 2023a. Yuji Chai, John Gkountouras, Glenn G Ko, David Brooks, and Gu-Yeon Wei.Int2. 1: Towards fine-tunable quantized large language models with error correction through low-rank adaptation. arXiv preprintarXiv:2306.08162, 2023b. Antonin Chambolle, Ronald A De Vore, Nam-Yong Lee, and Bradley J Lucier.Nonlinear wavelet im-age processing: variational problems, compression, and noise removal through wavelet shrinkage. IEEETransactions on image processing, 7(3):319335, 1998.",
  "Jiaao Chen, Aston Zhang, Xingjian Shi, Mu Li, Alex Smola, and Diyi Yang. Parameter-efficient fine-tuningdesign spaces. arXiv preprint arXiv:2301.01821, 2023a": "Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, BuyuLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen ChangeLoy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprintarXiv:1906.07155, 2019.",
  "Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Luo Si, and Yongbin Li. Lifelong learning for question answeringwith hierarchical prompts. arXiv preprint arXiv:2208.14602, 2022": "Sarkar Snigdha Sarathi Das, Ranran Haoran Zhang, Peng Shi, Wenpeng Yin, and Rui Zhang. Unified low-resource sequence labeling by sample-aware dynamic sparse finetuning. arXiv preprint arXiv:2311.03748,2023. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-dreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.Scaling visiontransformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512.PMLR, 2023. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplicationfor transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332, 2022.",
  "Alexey Dosovitskiy and Thomas Brox. Inverting visual representations with convolutional networks. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 48294837, 2016": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arxiv 2020. arXiv preprint arXiv:2010.11929,2010. Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, RameswarPanda, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision & language concepts to vi-sion & language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 26572668, 2023.",
  "Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018": "Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the effective-ness of parameter-efficient fine-tuning. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 37, pp. 1279912807, 2023. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXivpreprint arXiv:2208.01618, 2022.",
  "Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. Lq-lora: Low-rank plus quantized matrix decom-position for efficient language model finetuning. arXiv preprint arXiv:2311.12023, 2023": "Muhammad Usman Hadi, R Qureshi, A Shah, M Irfan, A Zafar, MB Shaikh, N Akhtar, J Wu, and S Mirjalili.A survey on large language models: Applications, challenges, limitations, and practical usage. TechRxiv,2023. Zeyu Han, Yuhan Wang, Luping Zhou, Peng Wang, Binyu Yan, Jiliu Zhou, Yan Wang, and Dinggang Shen.Contrastive diffusion model with auxiliary guidance for coarse-to-fine pet reconstruction. In InternationalConference on Medical Image Computing and Computer-Assisted Intervention, pp. 239249. Springer,2023a.",
  "Shengding Hu, Zhen Zhang, Ning Ding, Yadao Wang, Yasheng Wang, Zhiyuan Liu, and Maosong Sun.Sparse structure search for parameter-efficient tuning. arXiv preprint arXiv:2206.07382, 2022": "Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, Roy Ka-Wei Lee, Lidong Bing, and SoujanyaPoria. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXivpreprint arXiv:2304.01933, 2023a. Zi-Yuan Hu, Yanyang Li, Michael R Lyu, and Liwei Wang. Vl-pet: Vision-and-language parameter-efficienttuning via granularity control. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 30103020, 2023b.",
  "Chengsong Huang, Qian Liu, Bill Yuchen Lin, Tianyu Pang, Chao Du, and Min Lin. Lorahub: Efficientcross-task generalization via dynamic lora composition. arXiv preprint arXiv:2307.13269, 2023a": "Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui Huang, Rynson WH Lau, Wanli Ouyang, and Wang-meng Zuo.Clip2point: Transfer clip to point cloud classification with image-depth pre-training.InProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2215722167, 2023b. Yongfeng Huang, Yanyang Li, Yichong Xu, Lin Zhang, Ruyi Gan, Jiaxing Zhang, and Liwei Wang. Mvp-tuning: Multi-view knowledge retrieval with prompt tuning for commonsense reasoning. In Proceedings ofthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.1341713432, 2023c.",
  "Shibo Jie and Zhi-Hong Deng. Convolutional bypasses are better vision transformer adapters. arXiv preprintarXiv:2207.07039, 2022": "Shibo Jie, Haoqing Wang, and Zhi-Hong Deng. Revisiting the parameter efficiency of adapters from the per-spective of precision redundancy. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 1721717226, 2023. Feihu Jin, Jiajun Zhang, and Chengqing Zong. Parameter-efficient tuning for large language model withoutcalculating its gradients. In Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 321330, 2023.",
  "Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efficient low-rank hyper-complex adapter layers. Advances in Neural Information Processing Systems, 34:10221035, 2021": "Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, FabioViola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXivpreprint arXiv:1705.06950, 2017. Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan.Maple: Multi-modal prompt learning. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 1911319122, 2023. Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and DongsooLee. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization.arXiv preprint arXiv:2305.14152, 2023. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophicforgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017.",
  "Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrixadaptation. arXiv preprint arXiv:2310.11454, 2023": "Hildegard Kuehne, Hueihan Jhuang, Estbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a largevideo database for human motion recognition. In 2011 International conference on computer vision, pp.25562563. IEEE, 2011. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept cus-tomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 19311941, 2023.",
  "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.arXiv preprint arXiv:2104.08691, 2021": "Dengchun Li, Yingzi Ma, Naizheng Wang, Zhiyuan Cheng, Lei Duan, Jie Zuo, Cal Yang, and Mingjie Tang.Mixlora: Enhancing large language models fine-tuning with lora based mixture of experts. arXiv preprintarXiv:2404.15159, 2024. Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel:Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Confer-ence on Neural Information Processing Systems, 2023a.",
  "James Liu, Guangxuan Xiao, Kai Li, Jason D Lee, Song Han, Tri Dao, and Tianle Cai. Bitdelta: Yourfine-tune may only be worth one bit. arXiv preprint arXiv:2402.10193, 2024a": "Jialin Liu, Antoine Moreau, Mike Preuss, Jeremy Rapin, Baptiste Roziere, Fabien Teytaud, and Olivier Tey-taud. Versatile black-box optimization. In Proceedings of the 2020 Genetic and Evolutionary ComputationConference, pp. 620628, 2020. Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. Moelora:An moe-based parameter efficient fine-tuning method for multi-task medical applications. arXiv preprintarXiv:2310.18339, 2023b. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng,and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. arXiv preprint arXiv:2402.09353,2024b.",
  "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands,too. arXiv preprint arXiv:2103.10385, 2021b": "Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Chandu, Abhilasha Ravichander, LianhuiQin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, et al. Inference-time policy adapters (ipa):Tailoring extreme-scale lms without fine-tuning. arXiv preprint arXiv:2305.15065, 2023. Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinrio Passos, Longbo Huang,Jian Li, and Hang Zhao.Lcm-lora: A universal stable-diffusion acceleration module.arXiv preprintarXiv:2311.05556, 2023.",
  "David JC MacKay. A practical bayesian framework for backpropagation networks. Neural computation, 4(3):448472, 1992": "Andrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Seungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eu-njoon Cho, and Zhiguang Wang. Continual learning in task-oriented dialogue systems. arXiv preprintarXiv:2012.15504, 2020. Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. Parameter-efficientmulti-task fine-tuning for transformers via shared hypernetworks. arXiv preprint arXiv:2106.04489, 2021. Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:Pruning is all you need. In International Conference on Machine Learning, pp. 66826691. PMLR, 2020.",
  "Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.Peft: State-of-the-art parameter-efficient fine-tuning methods": "Yuning Mao, Lambert Mathias, Rui Hou, Amjad Almahairi, Hao Ma, Jiawei Han, Wen-tau Yih, and MadianKhabsa. Unipelt: A unified framework for parameter-efficient language model tuning. arXiv preprintarXiv:2110.07577, 2021. Xiangdi Meng, Damai Dai, Weiyao Luo, Zhe Yang, Shaoxiang Wu, Xiaochen Wang, Peiyi Wang, QingxiuDong, Liang Chen, and Zhifang Sui. Periodiclora: Breaking the low-rank bottleneck in lora optimization.arXiv preprint arXiv:2402.16141, 2024.",
  "Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity?a new dataset for open book question answering. In EMNLP, 2018": "Ismael Solis Moreno, Peter Garraghan, Paul Townend, and Jie Xu. Analysis, modeling and simulation ofworkload patterns in a large-scale utility cloud. IEEE Transactions on Cloud Computing, 2(2):208221,2014. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.arXiv preprint arXiv:2302.08453, 2023.",
  "Emin Orhan.A simple cache model for image recognition. Advances in Neural Information ProcessingSystems, 31, 2018": "Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, and Xinyu Dai. On prefix-tuningfor lightweight out-of-distribution detection. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 15331545, 2023. Mustafa Safa Ozdayi, Charith Peris, Jack Fitzgerald, Christophe Dupuy, Jimit Majmudar, Haidar Khan,Rahil Parikh, and Rahul Gupta. Controlling the extraction of memorized data from large language modelsvia prompt-tuning. arXiv preprint arXiv:2305.11759, 2023. Junting Pan, Ziyi Lin, Xiatian Zhu, Jing Shao, and Hongsheng Li. St-adapter: Parameter-efficient image-to-video transfer learning. Advances in Neural Information Processing Systems, 35:2646226477, 2022.",
  "George Pu, Anirudh Jain, Jihan Yin, and Russell Kaplan. Empirical analysis of the strengths and weaknessesof peft techniques for llms. arXiv preprint arXiv:2304.14999, 2023": "Yujia Qin, Xiaozhi Wang, Yusheng Su, Yankai Lin, Ning Ding, Jing Yi, Weize Chen, Zhiyuan Liu, JuanziLi, Lei Hou, et al.Exploring universal intrinsic task subspace via prompt tuning.arXiv preprintarXiv:2110.07867, 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, BoxingChen, and Mehdi Rezagholizadeh. Qdylora: Quantized dynamic low-rank adaptation for efficient largelanguage model tuning. arXiv preprint arXiv:2402.10462, 2024.",
  "Li Ren, Chen Chen, Liqiang Wang, and Kien Hua.Learning semantic proxies from visual prompts forparameter-efficient fine-tuning in deep metric learning. arXiv preprint arXiv:2402.02340, 2024": "Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequencetraining for image captioning. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 70087024, 2017. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Andreas Rckl, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and IrynaGurevych. Adapterdrop: On the efficiency of adapters in transformers. arXiv preprint arXiv:2010.11918,2020. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250022510, 2023.",
  "Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.Socialiqa: Commonsensereasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019": "Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, BanghuaZhu, Lianmin Zheng, Kurt Keutzer, et al. S-lora: Serving thousands of concurrent lora adapters. arXivpreprint arXiv:2311.03285, 2023a. Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christo-pher R, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large languagemodels with a single gpu. In International Conference on Machine Learning, pp. 3109431116. PMLR,2023b.",
  "Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E Gonzalez, andRaluca Ada Popa. Lloco: Learning long contexts offline. arXiv preprint arXiv:2404.07979, 2024": "Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Sori-cut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodalmodels. arXiv preprint arXiv:2312.11805, 2023. Mojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter efficient tuningof pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint arXiv:2210.07558,2022. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: Lessons learned from the2015 mscoco image captioning challenge. IEEE transactions on pattern analysis and machine intelligence,39(4):652663, 2016.",
  "Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. Spot: Better frozen model adaptationthrough soft prompt transfer. arXiv preprint arXiv:2110.07904, 2021": "Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J Clark, Brett H Meyer, and War-ren J Gross. Efficient fine-tuning of bert models on the edge. In 2022 IEEE International Symposium onCircuits and Systems (ISCAS), pp. 18381842. IEEE, 2022. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.Glue:A multi-task benchmark and analysis platform for natural language understanding.arXiv preprintarXiv:1804.07461, 2018.",
  "Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. Universality and limitations of prompt tuning.arXiv preprint arXiv:2305.18787, 2023c": "Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, VincentPerot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139149, 2022b. Ziyi Wang, Xumin Yu, Yongming Rao, Jie Zhou, and Jiwen Lu. P2p: Tuning pre-trained image models forpoint cloud analysis with point-to-pixel prompting. Advances in neural information processing systems,35:1438814402, 2022c. John Wright, Arvind Ganesh, Shankar Rao, Yigang Peng, and Yi Ma. Robust principal component analysis:Exact recovery of corrupted low-rank matrices via convex optimization. Advances in neural informationprocessing systems, 22, 2009. Bingyang Wu, Ruidong Zhu, Zili Zhang, Peng Sun, Xuanzhe Liu, and Xin Jin. tdLoRAu: Dynamicallyorchestrating requests and adapters for tLoRAutLLMu serving. In 18th USENIX Symposium on OperatingSystems Design and Implementation (OSDI 24), pp. 911927, 2024a. Hui Wu and Xiaodong Shi. Adversarial soft prompt tuning for cross-domain sentiment analysis. In Pro-ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pp. 24382447, 2022. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, YingShan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models fortext-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pp. 76237633, 2023a. Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai Li, andRicardo Henao. Infoprompt: Information-theoretic soft prompt tuning for natural language understanding.arXiv preprint arXiv:2306.04933, 2023b. Qi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. Visualquestion answering: A survey of methods and datasets. Computer Vision and Image Understanding, 163:2140, 2017. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversationframework. arXiv preprint arXiv:2308.08155, 2023c.",
  "Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang Jiang. Simda: Simple diffusion adapter for efficientvideo generation. arXiv preprint arXiv:2308.09710, 2023": "Binfeng Xu, Xukun Liu, Hua Shen, Zeyu Han, Yuhan Li, Murong Yue, Zhiyuan Peng, Yuchen Liu, ZiyuYao, and Dongkuan Xu. Gentopia: A collaborative platform for tool-augmented llms. arXiv preprintarXiv:2308.04030, 2023a. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, and Fu Lee Wang.Parameter-efficient fine-tuning methods for pretrained language models:A critical review and assessment.arXiv preprintarXiv:2312.12148, 2023b. Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-vocabularysemantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 29452954, 2023c. Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang.Raise a child in large language model: Towards effective and generalizable fine-tuning. arXiv preprintarXiv:2109.05687, 2021. Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen, XiaopengZhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large language models. arXivpreprint arXiv:2309.14717, 2023d. Zunnan Xu, Zhihong Chen, Yong Zhang, Yibing Song, Xiang Wan, and Guanbin Li. Bridging vision andlanguage encoders: Parameter-efficient tuning for referring image segmentation. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 1750317512, 2023e.",
  "Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning fortransformer-based masked language-models. arXiv preprint arXiv:2106.10199, 2021": "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine reallyfinish your sentence?In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics, 2019. Bohan Zeng, Shanglin Li, Yutang Feng, Hong Li, Sicheng Gao, Jiaming Liu, Huaxia Li, Xu Tang, JianzhuangLiu, and Baochang Zhang. Ipdreamer: Appearance-controllable 3d object generation with image prompts.arXiv preprint arXiv:2310.05375, 2023a.",
  "Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, Bohan Zhuang, et al. Pruning meetslow-rank parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403, 2023d": "Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao.Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023e. Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong-sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprintarXiv:2111.03930, 2021. Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, andHongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 85528562, 2022a. Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, andYu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv preprintarXiv:2303.16199, 2023f.",
  "Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search, 2022b": "Zhen-Ru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, and Songfang Huang.To-wards adaptive prefix tuning for parameter-efficient language model fine-tuning.arXiv preprintarXiv:2305.15212, 2023h. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, YuandongTian, Christopher R, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference oflarge language models. Advances in Neural Information Processing Systems, 36, 2024b.",
  "Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian.Galore: Memory-efficient llm training by gradient low-rank projection. arXiv preprint arXiv:2403.03507,2024": "Ziwang Zhao, Linmei Hu, Hanyu Zhao, Yingxia Shao, and Yequan Wang. Knowledgeable parameter efficienttuning network for commonsense question answering. In Proceedings of the 61st Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 90519063, 2023d. Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsingthrough ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 633641, 2017.",
  "Tianyi Zhou and Dacheng Tao. Godec: Randomized low-rank & sparse matrix decomposition in noisy case.In Proceedings of the 28th International Conference on Machine Learning, ICML 2011, 2011": "Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. tPetSu: A unified framework for tParameter-Efficientu transformers serving. In 2022 USENIX Annual Technical Conference (USENIX ATC 22), pp.489504, 2022c. Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang. Prompt-aligned gradient for prompttuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1565915669,2023a.",
  "Qi Zhu, Bing Li, Fei Mi, Xiaoyan Zhu, and Minlie Huang. Continual prompt tuning for dialog state tracking.arXiv preprint arXiv:2203.06654, 2022": "Wei Zhu and Ming Tan. Spt: Learning to selectively insert prompts for better prompt tuning. In Proceedingsof the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1186211878, 2023. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, and ShujianHuang. Multilingual machine translation with large language models: Empirical results and analysis.arXiv preprint arXiv:2304.04675, 2023c. Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and PengGao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 26392650, 2023d."
}