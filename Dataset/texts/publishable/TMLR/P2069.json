{
  "Abstract": "Discriminative features extracted from the sparse coding model have been shown to performwell for classification. Recent deep learning architectures have further improved reconstructionin inverse problems by considering new dense priors learned from data. We propose anovel dense and sparse coding model that integrates both representation capability anddiscriminative features. The model studies the problem of recovering a dense vector x anda sparse vector u given measurements of the form y = Ax + Bu. Our first analysis relieson a geometric condition, specifically the minimal angle between the spanning subspaces ofmatrices A and B, which ensures a unique solution to the model. The second analysis showsthat, under some conditions on A and B, a convex program recovers the dense and sparsecomponents. We validate the effectiveness of the model on simulated data and propose adense and sparse autoencoder (DenSaE) tailored to learning the dictionaries from the denseand sparse model. We demonstrate that (i) DenSaE denoises natural images better thanarchitectures derived from the sparse coding model (Bu), (ii) in the presence of noise, trainingthe biases in the latter amounts to implicitly learning the Ax + Bu model, (iii) A and Bcapture low- and high-frequency contents, respectively, and (iv) compared to the sparsecoding model, DenSaE offers a balance between discriminative power and representation.",
  "Introduction": "The problem of representing data as a linear mixture of components finds applications in many areas ofsignal and image processing (Bertalmio et al., 2003; Mallat & Yu, 2010). Variational partial differentialequations (PDE) approaches in Aujol et al. (2003) and Vese & Osher (2003) develop an image decompositionmodel whereby a target image is decomposed into a piecewise smooth component (cartoon) and a periodiccomponent (texture). Another approach posits that a desirable model should use multiple matrices, eachcapturing different structures in the image; for example, 1 is a matrix modeling texture and 2 is a matrixmodeling cartoon. Along these methods is Morpohological Component Analysis (MCA) (Elad et al., 2005)which utilizes pre-specified dictionaries and a a sparsity prior on the representation coefficients. In the",
  "Published in Transactions on Machine Learning Research (8/2024)": "number of feature maps with small bias values, which are dense. We call the dictionary atoms, correspondingsuch small biases, implicit A. Similarly, dictionary atoms with moderate biases can be seen as implicit B. These observations suggest that autoencoders implementing the sparse coding model (y = Bu), when learningthe biases by minimizing reconstruction error, implicitly perform two functions. First, they select the optimalnumber of filters. Second, they partition the filters into two groups: one that yields a dense representation ofthe input, and another that yields a sparse one. In other words, the architectures trained in this mannerimplicitly learn the dense and sparse coding model. (b) shows the filters. The above-mentioned interpretation of CSCNet with learned biases offers to revisit the optimization-basedmodel used to construct the autoencoder; if the network implicitly learns a bipartite representation, whynot explicitly model that structure? Through our experiments, we showed that doing so leads to increasedperformance with the recovered dictionaries and representations not deviating from their expected behavior.The resulting network is also more interpretable, as we can directly visualize and analyze each component.",
  "minxRp,uRn||Gx||22 + ||u||1subject toy = Ax + Bu,(1)": "where G is the Tikhonov regularization operator. Here on, we refer to the model in (1) as the dense andsparse coding problem. The word dense here refers to the smooth part and is used to emphasize that we donot require any sparsity. Both MCA and the dense and sparse coding problem naturally lend themselves to and share a commontheme with sparse coding and dictionary learning. Sparsity regularized deep neural network architectureshave been used for image denoising and discriminative tasks such as image classification (Simon & Elad, 2019;Tolooshams et al., 2020; Rolfe & LeCun, 2013). Recent work has highlighted some limitations of convolutionalsparse coding (CSC) autoencoders and its multi-layer and deep generalizations (Sulam et al., 2019; Zazoet al., 2019) for data reconstruction. The work in Simon & Elad (2019) argues that the sparsity levels thatCSC allows can only accommodate very sparse vectors, making it unsuitable to capture all features of signalssuch as natural images, and propose to compute the minimum mean-squared error solution under the CSCmodel, which is a dense vector capturing a richer set of features. Moreover, the majority of CSC frameworkssubtract low-frequency components of images prior to applying convolutional dictionary learning (CDL)for representation purposes (Garcia-Cardona & Wohlberg, 2018); however, this is not desired for denoisingtasks where noise corrupts low frequencies in addition to the high spectrum. The goals of the paper aretwofold. First, it presents a theoretical analysis of the dense and sparse coding problem. Second, we arguethe dense representation x in a dictionary A is useful for reconstruction and the sparse representation u hasdiscriminative capability.",
  "Compressive sensing from union of dictionaries": "The dense and sparse coding problem is similar in flavor to sparse recovery in the union of dictionaries (Donoho& Stark, 1989; Donoho & Huo, 2001; Elad & Bruckstein, 2002; Donoho & Elad, 2003; Kuppinger et al., 2011).Consider two sets of orthonormal bases, A Rmm and B Rmm, in Rm. One setting is based on the ideathat a given signal will have a sparse representation with respect to suitably predefined A or B (Mallat, 1999;Daubechies, 1992; Dobson & Santosa, 1996). The concept of the union of bases is based on constructing anovercomplete dictionary [A B] from A and B. This model then posits that a signal y can be represented asy = Ax + Bu, where x and u are sparse. Notably, the signal may not be individually sparse with respect toA and B, but it has sparsity in the joint representation (Elad & Bruckstein, 2002; Donoho & Huo, 2001). InDonoho & Stark (1989), the authors apply this model to error-correcting encryption and the separation of twosignals. In both cases, the measured signal is assumed to be a superposition of two components, each sparsein a predefined dictionary. Most results in the literature of union of bases take the form of an uncertaintyprinciple that relates the sum of the sparsity of x and u to the mutual coherence between A and B, andwhich guarantees that the representation is unique and identifiable by 1 minimization. In Donoho & Stark(1989), the authors study the problem of recovering x and u from y = Ax + Bu, where A Rmm is adiscrete Fourier matrix (DFT) and B Rmm is the identity matrix. Therein, under the assumption thatthe support of u is known, the result shows that x can be exactly recovered if 2||x||0||u||0 < m. This resultfor the Fourier-identity pair was further generalized to the case where A and B are orthonormal bases (Elad& Bruckstein, 2002) and when the measurements come from a union of non-orthogonal bases (Donoho &Elad, 2003; Kuppinger et al., 2011). We remark that all the aforementioned results assume sparsity on bothx and u.",
  "Error correction": "The problem of recovering a signal x given the measurement model y = Ax + u, where u is a sparse errorvector, is known as the sparse error correction problem (Candes et al., 2005). The work therein considersa tall measurement matrix A and assumes that the fraction of corrupted entries is suitably bounded. Toobtain a sparse minimization program, the matrix A is eliminated by a matrix B, where BA = 0, resultingBy = Bu. The resulting model is then solved via 1 minimization and exactness is shown under the restrictedisometry condition on B. The works in Wright & Ma (2010); Nguyen & Tran (2013); Pope et al. (2013);Studer et al. (2011); Studer & Baraniuk (2014) study the general error correction problem y = Ax + Buunder different models of the signal x, the sparse interference vector u and the matrices A and B. We note that all the aforementioned works consider a single sparsifying norm to recover the components xand u. In contrast, we use mixed norms that impose the Tikhonov and sparsity regularization. We also notethat when A has more columns than rows, one of the settings we consider in this paper, our problem departsfrom the sparse error correction problem.",
  "minu||u||1,wsubject toy = Bu,": "where ||u||1,w = ni=1 wiui and w Rn indicates the vector of weights. For instance, Mansour & Saab(2017) set the weights as follows: wi if i T and 1 otherwise. The aforementioned works discussthe recovery of the underlying signal using weighted Lasso under some conditions on the weights, size andaccuracy of the estimated support. One approach for the feasibility constraint in (1) is to reformulate itas a weighted Lasso problem in a combined dictionary [A B] where the support estimate is the support ofx. This formulation has few limitations. First, the recovery guarantees in weighted Lasso require accuracyof the estimated support which would depend on the number of columns of A. Second, these guaranteesimpose uniform structure on the combined dictionary, such as the weighted null space conditions, whereasour conditions allow a deterministic dictionary A and a random dictionary B. Finally, even in the regimewhere weighted Lasso might succeed in recovering the underlying x and u, it does not guarantee that Ax issmooth as that regularization is not reflected in the Lasso optimization.",
  "Morphological component analysis (MCA)": "Morphological Component Analysis (MCA) considers the decomposition of a signal or image into differentcomponents with each component having a specific structure (morphology) (Starck et al., 2004; 2005; Bobinet al., 2007; Elad et al., 2005). In MCA, the specific structure is imposed via sparsity with respect to pre-specified dictionaries. Specifically, given K dictionaries {1, ..., K}, we model the data y as a superpositionof K linear components as follows: y = Ki=1 iyi where yi is assumed to be sparse in i but it is not assparse (or not sparse at all) in other dictionaries. With this set up, the dictionaries discriminate betweenthe different components. In Starck et al. (2004) a basis pursuit approach to solve the MCA is employedand its utility is demonstrated in examples such as separating texture from the smooth part of an image.The main difference of our model from MCA is a smooth-sparse decomposition as opposed to sparse-sparsedecomposition with pre-set dictionaries that contain information about targeted morphologies. The theoryand analysis of the two models is also markedly different as we consider a smoothness regularizer.",
  "Dictionary learning and unrolling": "Given a data set, learning a dictionary in which each example admits a sparse representation is useful in anumber of tasks (Aharon et al., 2006; Mairal et al., 2011). This problem, known as sparse coding (Olshausen& Field, 1997) or dictionary learning (Agarwal et al., 2016; Garcia-Cardona & Wohlberg, 2018), has beenthe subject of investigation in recent years in the signal processing community. A growing body of work,referred to as algorithm unrolling (Monga et al., 2021), has mapped the sparse coding problem into encodersfor sparse recovery (Gregor & Lecun, 2010). In this paper, we unroll the dense and sparse coding problem fora principled design of an autoencoder. The designed autoencoder efficiently learns a dense representation x,useful for reconstruction, and a sparse representation u with discriminative capability.",
  "Contribution": "We focus on (1) and start by first providing theoretical guarantees for the feasibility problem y = Ax+Bu. Ourfirst result is based on a geometric condition on the minimum principal angle between certain subspaces. Next,we prove that the convex program in (1) recovers the underlying components x and u under some assumptionson the measurement matrices and the Tikhonov regularizer. We empirically validate the effectiveness ofour methods through phase transition curves and make a direct comparison to noisy compressive sensing,highlighting the latters inability to recover signals adhering to our proposed model. We then apply ouroptimization algorithm to sense real data and validate the expected properties of the sparse and smoothcomponents. We connect the proposed model to dictionary learning/algorithm unrolling by proposing a dense and sparseautoencoder (DenSaE). We demonstrate the superior discriminative and representation capabilities of DenSaE",
  "Technical background": "We start by briefly reviewing uniqueness results for compressive sensing with exact measurements. Thetypical assumption in these analysis is to assume the measurement matrices satisfy certain conditions such asthe restricted isometry property (RIP) and coherence. In our setting, we employ RIPless recovery analysis(Candes & Plan, 2011; Kueng & Gross, 2014), which will be referred to and used in the proof of Theorem6. We note that existing anisotropic analysis is based on a single measurement matrix. For our model, theanisotropic analysis is applied to a certain matrix derived from A, B and the Tikhonov regularizer G.",
  "minuRn||u||0subject toy = Bu.(2)": "While (2) is a natural program, it is known to be computationally intractable. Common alternatives are basispursuit (BP) (Tropp, 2004; Chen et al., 2001) and iterative greedy methods such as orthogonal matchingpursuit (OMP)(Davis et al., 1997; Pati et al., 1993; Tropp & Gilbert, 2007). In basis pursuit, (2) is modifiedby considering a convex relaxation of the 0 norm leading to the 1 minimization problem.",
  "minuRn||u||1subject toy = Bu.(3)": "A fundamental question is under what conditions the above optimization program identifies the underlyingsolution for (2). A related part of this question is when the 0 minimization problem admits a unique solution.One condition is the restricted isometry property (RIP) (Candes & Tao, 2005). The s-restricted isometryconstant of an m n measurement matrix B is the smallest constant s such that",
  "(1 s)||u||2 ||Bu2 (1 + s)||u||2,": "holds for all s-sparse signals u (an s-sparse vector has at most s non-zero entries). Small RIP constants ensureunique solution of the 0 minimization problem and further provide the guarantee that the 1 relaxationin (3) is exact (Candes, 2008). It is known that random measurement matrices have small RIP constants(Candes & Tao, 2006; Baraniuk et al., 2008). Another condition for analysis is based on the coherence of ameasurement matrix B defined as = maxi=j |bTi bj|,",
  "provided that m C(F)s log n. The constant C = C0(1 + ) where C0 is some constant": "We note that the definition of B based on re-scaling the vectors b1, b2, ..., bm is done so that the columns ofB are approximately unit-normed. The work in Kueng & Gross (2014) extends the above result withoutassuming the isotropy property. Let = E[bbT ]12 , where as before b is drawn from distribution F on Rn,denote the covariance matrix. The anisotropic analysis in Kueng & Gross (2014) is based on two propertiespertaining to the measurement matrix. The first is the incoherence property which is formally defined asfollows. The incoherence parameter is the smallest number (F) such that",
  "Given these assumptions, the main result in Kueng & Gross (2014) is stated below": "Theorem 2 (Kueng & Gross (2014)). Let B =1mmi=1 eibTi be a measurement matrix and let u be a fixedbut otherwise arbitrary s-sparse vector in Rn. Define s = max{cond(s, ), cond(s, 1)} and let 1.If the number of measurements fulfills m Cs (F) 2 s log n, then the solution u of the convex programminu||u||1 subject to y = Bu, is unique and equal to u with probability at least 1 e. The proofs of Theorem 1 and Theorem 2 are based on the dual certificate approach. The approach involvesassuming the existence of a dual certificate vector v that satisfies certain conditions which ensures theuniqueness of the minimization problem. The remaining task is then to construct a dual certificate thatsatisfies these conditions. Since the conditions on the dual certificate will be used in our main analysis, wesummarize the conditions on v in the following lemma.",
  "Theoretical Analysis": "The dense and sparse coding problem studies the solutions of the linear system y = Ax + Bu. Given matricesA Rmp and B Rmn and a vector y Rm, the goal is to provide conditions under which there is aunique solution (x, u), where u is s-sparse, and an algorithm for recovering it. For ease of navigating themain results, shows the road-map of the main theoretical results.",
  "Feasibility problem": "In this subsection, we first study the uniqueness of solutions to the linear system accounting for the differentstructures the measurement matrices A and B can have. A uniqueness result can be readily obtained byassuming orthogonality of Col(A) and Col(B). In what follows, we establish uniqueness results for the generalsetting. The main result of this subsection is Theorem 4 which, under a natural geometric condition based onthe minimum principal angle between the column space of A and the span of s columns in B, establishes auniqueness result for the dense and sparse coding problem. Since the vector u in the proposed model is sparse,we consider the classical setting of an overcomplete measurement matrix B with n m. The next theoremprovides a uniqueness result assuming a certain direct sum representation of the space Rm. We first notethat, for A Rmp with p m, any z Ker(A) can be added to any solution x. With that, we consideruniqueness modulo the vectors in Ker(A), meaning any feasible solution is assumed to be in Ker(A). Theorem 3. Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x, u). LetS, with |S| = s, denote the support of u. If BS has full column rank and Rm = Col(A) Col(BS), the onlyunique solution to the linear system, with the condition that any feasible s-sparse vector u is supported on Sand any feasible x is in Ker(A), is (x, u). Proof. Let (x, u), with u supported on S and x Ker(A), be another solution pair. It follows thatA1 + BS(2)S = 0 where 1 = x x and 2 = u u. Let U Rmr and V Rmq be matrices whosecolumns are the orthonormal bases of Col(A) and Col(BS), respectively. The equation A1 + BS(2)S = 0can equivalently be written as ri=1A1 , UiUi + qi=1BS(2)S , ViVi = 0 with Ui and Vi denoting",
  "= 0": "Noting that the matrixUVhas full column rank, the homogeneous problem admits the trivial solutionimplying that A1 = 0 and BS(2)S = 0. Since BS has full column rank and 1 {Ker(A) Ker(A)}, itfollows that 1 = 2 = 0. Therefore, (x, u) is the unique solution. The uniqueness result in the above theorem hinges on the representation of the space Rm as the direct sumof the subspaces Col(A) and Col(BS). We use the definition of the minimal principal angle between twosubspaces, and its formulation in terms of singular values (Bjrck & Golub, 1973), to derive an explicitgeometric condition for the uniqueness analysis of the linear system in the general case.",
  "In addition, cos((U, V)) = 1(UT V) where 1 denotes the largest singular value": "Theorem 4. Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x, u).Let S, with |S| = s, denote the support of u. Assume that BS has full column rank . Let U Rmr andV Rmq be matrices whose columns are the orthonormal bases of Col(A) and Col(BS), respectively. Ifcos((U, V)) = 1(UT V) < 1, the only unique solution to the linear system, with the condition that anyfeasible s-sparse vector u is supported on S and any feasible x is in Ker(A), is (x, u).",
  ". For": "simplicity of notation, let K denote the block matrix: K =UV. If we can show that the columns of Kare linearly independent, it follows that A1 + BS(2)S = 0 if and only if A1 = 0 and BS(2)S = 0. Wenow consider the matrix KT K which has the following representation",
  ". Hence, the nonzero eigenvalues of KT K are": "1 i, 1 i min(p, q), with i denoting the i-th largest singular value of UT V. Using the assumption1 < 1 results the bound minKT K> 0. It follows that the columns of K are linearly independent, andhence A1 = 0 and BS(2)S = 0. Since BS is full column rank and 1 {Ker(A) Ker(A)}, it followsthat 1 = 0 and 2 = 0. A restrictive assumption of the above theorem is that the support of the sought-after s-sparse solution u isknown. We can remove this assumption by considering Col(A) and Col(BT ) where T is an arbitrary subsetof {1, 2, ..., n} with |T| = s. More precisely, we state the following corollary whose proof is similar to theproof of Theorem 4. Corollary 1. Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x, u). LetS, with |S| = s, denote the support of u and T be an arbitrary subset of {1, 2, ..., n} with |T| s. Assumethat any 2s columns of B are linearly independent. Let U Rmp and V Rmq be matrices whose columnsare the orthonormal bases of Col(A) and Col(BST ), respectively. If (U, V) = 1(UT V) < 1, holds for allchoices of T, the only unique solution to the linear system is (x, u) with the condition that any feasible u iss-sparse and any feasible x is in Ker(A).",
  "Of interest is the identification of simple conditions such that 1(UT V) < 1. The following theorem proposesone such condition to establish uniqueness": "Theorem 5. Assume that there exists at least one solution to y = Ax + Bu, namely the pair (x, u).Let S, with |S| = s, denote the support of u. Assume that BS has full column rank. Let U Rmr andV Rmq be matrices whose columns are the orthonormal bases of Col(A) and Col(BS), respectively. Letmaxi,j|uTi vj| = . If s <1r, the only unique solution to the linear system, with the condition that any",
  "Proof. It suffices to show that 1 < 1. Noting that 1 = ||UT V||2, we use the following matrix norminequality ||UT V||2 r||UT V|| as follows: 1 r ||UT V|| rs < 1": "The constant is the coherence of the matrix UT V (Donoho et al., 2005; Tropp, 2004). The above resultstates that if the mutual coherence of UT V is small, we can accommodate increased sparsity of the underlyingsignal component u. We note that, up to a scaling factor, 1(UT V) is the block coherence of U and V(Eldar et al., 2010). However, unlike the condition in Eldar et al. (2010), we do not restrict the dictionariesA and B to have linearly independent columns. In the next subsection, we propose and analyze a convexprogram to recover the dense and sparse vectors.",
  "minxRp,uRn ||Gx||22 + ||u||1subject toy = Ax + Bu.(9)": "Proof strategy: The Tikhonov matrix G is assumed to have full column rank. Our goal is to establishthat, under certain conditions, the above minimization problem admits a unique solution (x, u). The proofstrategy is as follows: First, we make the change of variables z = Gx, which allows us to rewrite the linearconstraint as y = Hx + Bu, where H = AG. The second step is to eliminate the dense component in theconstraint by applying the projection operator PCol(H). This yields PCol(H)(y) = PCol(H)(Bu). DenotingC = PCol(H)(B), we obtain PCol(H)(y) = Cu. Focusing on the optimization problem with respect to u,we have a sparse recovery problem with the measurement matrix C. To guarantee exact recovery for thisproblem, certain conditions on the matrix C are required. For instance, the results discussed in .2require the rows of C to be sampled i.i.d. from some distribution F. However, if A and B are realized asrandom i.i.d. matrices, the projection operator does not necessarily preserve the i.i.d. property, meaningC may not meet the conditions for exact recovery. Therefore, a crucial part of our analysis is the carefulconstruction of the measurement matrices A and B to ensure that the resulting sparse recovery problem hasa measurement matrix with the desired structure for unique recovery, as detailed below. Construction of measurement matrices: First, note that Col(A) is an r-dimensional subspace of Rm,where r = dim Col(A). Given A Rmp, either generated from a deterministic or random model, we firstform the matrix H = AG. We then sample r i.i.d. random vectors c1, c2, . . . , cr from some distribution Fon Rp, meaning each sampled vector is a random vector of size Rp. Let C =1rri=1 eicTi be the scaled measurement matrix. Before we proceed to construct the matrix C, we make the following observations. Letthe size of G be q p. Since G has full column rank, the rank of G is p. It then follows that the rank ofG is also p. Considering the matrix H = AG, since G has full row rank, the rank of H Rmq is therank of A. Hence, dim Col(H) = r. We form a matrix C of size m p where its r rows are the sampledrandom vectors, scaled by1r, and the remaining m r rows are such that each column of C lies in Col(H).Note this can always be achieved since Col(H) is an r-dimensional space in Rm. Therefore, any point inCol(H) can be realized by picking the first r points at random and determining the rest of the entries fromthe constraints that define Col(H). It is important to note that the rows of C are not i.i.d. (except thefirst r rows generated from the random model). The matrix C will be essential to our main analysis, as itsrows are sampled from the distribution F. We now form the matrix B Rmn by constructing each of itscolumns as follows: bi = ci + hi, where bi is the i-th column of B, ci is the i-th column of C, and hi isan arbitrary vector that is in the column space of H. This ensures that C = PCol(A) as desired. summarizes the construction of the measurement matrices. Theorem 6. Let F a distribution of random vectors on Rp and c1, c2, . . . , cr be r vectors sampled fromF. The constants (F) and s denote the coherence and sparse condition number associated to F. Letz = Gx T, where T = Ker(PCol(B)H) and u be an s-sparse vector. Set y = Ax + Bu. Let 1and r = dim Col(A). If r Cs (F) 2 s log n, (x, u) is the unique minimizer to (9) with probability atleast 1 e.",
  "minz,u ||z||22 + ||u||1subject toy = AGz + Bu = Hz + Bu.(10)": "We structure the proof into 3 parts.1. Optimality in the mixed objectiveWe consider a generic feasible solution pair (z + 1, u + 2). Let the function f(z, u) define the objectivein the optimization program. The idea of the proof is to show that any feasible solution is not minimial inthe objective value, f(z + 1, u + 2) f(z, u), with the inequality holding for all choices of 1 and2 and equality obtained if and only if 1 = 2 = 0. Before we proceed, two remarks are in order. First,using the duality of the 1 norm and the norm, there exists a with supp() Sc, |||| = 1 andsuch that , (2)Sc = ||(2)Sc||1. Second, the subgradient of the 1 norm at u is characterized as follows:||u||1 = {sgn(uS) + g | supp(g) Sc, ||gSc|| 1}. It follows that sgn(u) + is a subgradient of the 1norm at u. It then follows that the inequality ||u||1 ||u||1 + sgn(uS) + , u u holds for any u. Welower bound f(z + 1, u + 2) as follows.",
  "=f(z, u) + ||1||2 + sgn(uS) + , 2.(11)": "Above, the second equality uses the fact that z T and 1 T. The latter fact follows from the feasibilitycondition that H1 + B2 = 0, and the application of the projection operator PCol(B) on both sides.2. Introducing the dual certificate vWe next use the feasibility condition that H1 + B2 = 0 and introduce the dual certificate v. To eliminatethe component H1, we project it onto the orthogonal complement of the range of H and obtain C2 = 0where C = PCol(H)(B). We note that the previous relation follows from the construction of the measurementmatrices. We further consider a reduced system by considering C2 = 0. Recall that C is a matrix whereeach row is sampled i.i.d. from the distribution F. With that, we essentially have a sparse recovery problemwith the measurement matrix C. Using Lemma 1, and the assumptions of the theorem, there exists a dual",
  "||(2)Sc||1.(15)": "3. Uniqueness of solutionWe note that f(z + 1, u + 2) = f(z, u) if and only if ||(2)Sc||1 = 0 and 1 = 0. Since ||(2)S||2 2||(2)Sc||2, the equality ||(2)Sc||1 = 0 implies that ||(2)S||2 = 0. With this, f(x + 1, u + 2) = f(x, u)if and only if 2 = 0. Therefore, the solution (z, u) achieves the minimal value in the objective, and isa unique solution to (10). Finally, using the relation x = Gz guarantees unique solution to (9). Thisconcludes the proof.",
  "Special cases of analysis and discussion": "We note a few special cases of our main theorem in Theorem 6. First, when the Tikhonov matrix G is theidentity matrix, the regularization on x is the standard 2 (minimum norm least squares) regularization. Themain result continues to hold, with additional assumptions, when G = A. In this case, the optimizationproblem isminxKer(A),u ||Ax||22 + ||u||1subject toy = Ax + Bu.(17)",
  "We note that the uniqueness result for the above problem is modulo restriction to the subspace Ker(A).We now state the following result": "Theorem 7. Let F a distribution of random vectors on Rp and c1, c2, . . . , cr be r vectors sampled fromF. The constants (F) and s denote the coherence and sparse condition number associated to F. Sety = Ax + Bu. Let 1 and r = dim Col(A). Assume the two conditions",
  "Theorem 5 relies on block coherence and does not require coherence within the blocks A and B. Thisprovides a flexible model, allowing A to be a deterministic coherent dictionary, for example": "Next, consider applying 1 minimization and standard compressive sensing theory to guarantee uniquenessgiven the measurement matrix R = [A B]. One guarantee based on incoherence (see Theorem 1.1 in Candes& Plan (2011)) states that the number of measurements must be on the order of (p + k) log(p + n), where is the coherence of the combined dictionary R. In contrast to the mutual coherence mentioned earlier, is the smallest number such that the following equality holds for any row of R denoted by a:",
  "max1t(p+n) |a(i)|2 < ,": "where a(i) denotes the i-th entry of a. It is typically assumed that there is an underlying distribution F fromwhich the rows of R = [A B] are sampled independently and identically. We note that the range of , forthe measurement setup in Candes & Plan (2011), is [1, n + p]. The implication of this is that the samplecomplexity implicitly requires = O(1). In the case of mixed dictionaries, as in our setup, a coherent matrixA can lead to sub-optimal number of measurements.",
  "Phase transition curves": "We generate phase transition curves and present how the success rate of the recovery, using the proposedmodel, changes under different scenarios. To generate the data, we sample random matrices A Rmp andB Rmn whose columns have expected unit norm and fix the number of columns of B to be n = 100.The vector u Rn has s randomly chosen indices, whose entries are drawn according to a standard normaldistribution, and x Rp is generated as x = AT where Rm is a random vector. The constructionensures that x does not belong in the null space of A, and hence degenerate cases are avoided. We normalizeboth x and u to have unit norm, and generate the measurement vector y Rm as y = Ax + Bu.",
  "To generate the transition curves we vary the sampling ratio =m": "n+p [0.05, 0.95] and the sparsity ratio =sm in the same range. Note that the sensing matrix in our model is [A B]; therefore, our definitionof takes into account both the size of A and B. In the case where we revert to the compressive sensingscenario (p = 0), the sampling ratios coincide. We solve the convex optimization problem of (17) to obtainthe numerical solution pair (x, u) using CVXPY (Diamond & Boyd, 2016; Agrawal et al., 2018), and register asuccessful recovery if both xx2",
  "u2 , with = 103. For each choice of and , we average100 independent runs to estimate the success rate": "shows the phase transition curves, indicating the probability of successful recovery, for p {0.1m, 0.5m} to highlight different ratios between p and n.We observe that increasing p leads to adeterioration in performance. This is expected, as this creates a greater overlap on the spaces spanned by Aand B. We can view our formulation as modeling the noise of the system. In such a case, the number ofcolumns of A encodes the complexity of the noise system: as p increases, so does the span of the noise space.Extending the signal processing interpretation, note that we model the noise signal x as a dense vector, whichcan be seen as encoding smooth areas of the signal that correspond to low-frequency components. On thecontrary, the signal u has, by construction, a sparse structure, containing high-frequency information, aninterpretation that will be further validated on real data in .4.",
  ": Normalized recovery error of u as the SNR varies (lower is better)": "We, again, fix the number of columns in B to be n = 100, and generate the matrices A Rmp andB Rmn, as well as the vectors x Rp and u Rn, as before. We define the signal-to-noise ratioas SNR = 20 log10||u||2||x||2 , and iterate over the range [40dB, 40dB]. To vary the SNR, we normalize both",
  "||u||2for the two methods averaging 100 independent runs": "We present the SNR curves in . As an initial observation, note that noisy compressive sensing, inevery case, recovers a vector when the energy of the noise is less than that of the signal (for a SNR> 0dB),and in most scenarios full recovery is not achieved unless the ratio is very large (SNR> 25dB). That isexpected, since the results for those settings assume an upper bound on the noise level. In contrast, ourproposed model is able to recover both signals even when the norm of x is 100 times larger than that of u, atthe cost of having access to the additional measurement matrix A. For low sparsity ratios (Figures 4(a) and (d)), we are able to recover both signals in every experiment usingour model, whereas compressive sensing exhibits the behaviour we discussed above. Increasing the sparsityratio while keeping the number of samples the same (Figures 4(b) and (e)) results in a significant reduction inperformance for both models (note the different axis scaling for these figures). This is in line with both theresults for noisy compressive sensing and our results presented in (as well as the phase transitioncurves of the previous subsection). When the overlap between A and B is greater ((e)) our modelsuffers a greater performance hit, as is expected from our analysis; note that noisy compressive sensing isunaffected by the relative size of A and B, since it only makes assumptions about the noise level and notits span. Finally, we observe that increasing the number of measurements (Figures 4(c) and (f)) restoresperformance, in line with our analysis. Remark: Note that the large sparsity ratios ( = 0.5, corresponding to Figures 4(b), (c), (e), and (f)) violateour assumptions in and recovery is not expected based on the transition curves of the previous",
  "Sensing with real data": "In this section, we present experiments on real data using random sensing matrices. The real data we consideris the MNIST database of handwritten digits (LeCun, 1998; LeCun et al., 1998). Through these experiments,we want to (i) showcase the performance of our model on actual, real data and (ii) show the efficacy of ouralgorithm when using overcomplete sensing matrices. To generate overcomplete dictionaries, we proceeded as",
  "x": ": Decomposition of an MNIST image to its sparse and dense components. We visualizethe minimization of the terms in (20): the input y is adequately reconstructed (left), the component Ax issmooth relative to Bu (middle), and u is sparse. follows: as MNIST images can be seen as vectors y R784, we first generated a random orthogonal matrixin R784784. We used half of the columns of that matrix as a base for A R7841024 and the other half forB R7841024; the rest of the columns were generated as linear combinations of each base. To avoid artificialorderings that might result when using certain optimizers, we conclude the matrix generation with randomcolumn permutations. These matrices, while in combination they do span R784, were not the generatingmodel for the MNIST dataset. As such, we slightly alter the optimization problem of (9) to relax the exactreconstruction, yielding:minx,u ||Ax + Bu y||22 + ||Ax||22 + ||u||1 .(20)",
  "In our experiments, we use = 3 and = 0.01. A visual decomposition of such an image is presented in": "Both u and x have their expected properties: u is visibly sparse, with few nonzero elements and x has afairly dense structure. Moreover, we observe that the component of Ax seems slightly more fainted comparedto that of Bu. To further validate this observation we computed both the Euclidean norm and total variation for eachcomponent, as a proxy for smoothness, and plotted the distributions of total variation in . Thedistributions were computed using 10000 training samples. The distribution of the Euclidean norm was verysimilar and can be found in the Appendix, . Observing the distributions of Ax and Bu we notethat the distribution corresponding to the sparse component is skewed to higher values of total variation.This empirically validates the effectiveness of the smoothness regularization of (9), even when sensing usingrandom matrices. As a final remark, we attempted to use B in a sparse coding framework to recover a sparsevector. However, in every instance the solver failed to converge and produce a sparse vector; this is, to an",
  ": Total variation distribution for the components Ax and Bu of MNIST images. Wequantify the qualitative difference in smoothness between Ax and Bu in (20), supporting the qualitativedifference of": "extent, expected as B was generated using only half the columns of an orthogonal matrix and therefore isunable to fully span the image space, failing to adequately represent all the images in the data set. The main analysis in this paper is based on the measurement matrices and the Tikhonov matrix satisfyingcertain conditions, such as randomness in the matrix B. However, in practice, efficient matrices are notalways available, and random matrices might not be optimal for every data set. Dictionary learning refers tothe problem of learning A and B from data (Agarwal et al., 2014; Chatterji & Bartlett, 2017; Garcia-Cardona& Wohlberg, 2018). Based on unrolled neural architectures for the sparse coding model (B = 0) (Tolooshamset al., 2020; Gregor & Lecun, 2010), the next section proposes an unrolled autoencoder to infer dense andsparse representations and learn dictionaries from data.",
  "Dictionary learning based neural architecture for dense and sparse coding": "We mitigate the drawbacks of convolutional sparse coding model in capturing a wide range of features fromnatural images by proposing dense and sparse dictionary learning; the framework learns a diverse set offeatures (smooth features via A and high-frequency features appearing sparsely through B). We formulatethe dense and sparse dictionary learning problem as minimizing the objective",
  "Y AX(l) BU(l)2F ,(23)": "where Y RmI, X RpI, and U RnI with I is the total number of data examples. By design choice,we optimize only the reconstruction loss when updating the dictionaries. Based on the above alternatingupdates, we use deep unrolling to construct an unrolled neural network (Tolooshams et al., 2020; Gregor &Lecun, 2010), which we term the dense and sparse autoencoder (DenSaE), tailored to learning the dictionariesfrom the dense and sparse model. The encoder maps Y into a dense matrix XT and a sparse one UT usingtwo sets of filters of A and B through a recurrent network. A encodes the smooth part of the data (lowfrequencies), and B encodes the details of the signal (high frequencies). The encoding is achieved by unrollingT proximal gradient iterations shown below",
  "(iii) What data characteristics does the model capture?": "As baselines, we trained two variants, CSCNethyp and CSCNetLS, of CSCNet (Simon & Elad, 2019), anarchitecture tailored to dictionary learning for the sparse coding problem y = Bu. Since all three networksare convolutional, A and B are Toeplitz matrices that perform the sum of convolutions using multiple filters.In CSCNethyp, the bias is tuned as a shared hyper-parameter. In CSCNetLS, we learn a different bias foreach filter by minimizing the reconstruction loss. When the dictionaries are non-convolutional, we call thenetwork SCNet.",
  "DenSaE strikes a balance between discriminative capability and reconstruction": "We study the case when DenSaE is trained on the MNIST dataset for joint reconstruction and classification.We show (i) how the explicit imposition of sparse and dense representations in DenSaE helps to balancediscriminative and representation power, and (ii) that DenSaE outperforms SCNet. We warm start thetraining of the classifier using dictionaries obtained by first training the autoencoder with = 0. Characteristics of the representations xT and uT : To evaluate the discriminative power of therepresentations learned by only training the autoencoder, we first trained the classifier given the representations.Specifically, we first trained A and B with = 0, then trained C with = 1. We call this disjoint training. shows the classification accuracy (Acc.), 2 reconstruction loss (Rec.), and the relative contributions,expressed as a percentage, of the dense or sparse representations to classification and reconstruction fordisjoint training. Each column of [A , B], and of C, corresponds to either a dense or a sparse feature. For",
  "Acc.94.16 % 98.32% 98.18% 98.18% 96.98%Rec.1.956.806.836.303.04A contribution to important class features--0%0%0%A contribution to important rec. features--8%28%58%": "reconstruction, we find the indices of the 50 most important columns and report the proportion of thesethat represent dense features. For each of the 10 classes (rows of C), we find the indices of the 5 mostimportant columns (features) and compute the proportion of the total of 50 indices that represent densefeatures. The first row of shows the proportion of rows of [A B] that represent dense features.Comparing this row, respectively to the third and fourth row reveals the importance of x for reconstruction,and of u for classification. Indeed, the Acc. and Rec. of show that, as the proportion of densefeatures increases, DenSaE gains reconstruction capability but results in a lower classification accuracy.Moreover, in DenSaE, the most important features in classification are all from B, and the contribution ofA in reconstruction is greater than its percentage in the model, which clearly demonstrates that dense andsparse coding autoencoders balance discriminative and representation power. Both Tables 1 and 2 show that DenSaE outperforms SCNetLS in classification and SCNethyp in reconstruction.Specifically, for joint training, (columns 2,3 and row J1) shows that DenSaE outperforms SCNethypfor reconstruction (32.61 47.70) while it is competitive for classification (98.61 > 98.59). Moreover, (cols 1, 3 and J0.75 and J1) highlights that DenSAE has significantly better classification (98.61 > 96.06)and reconstruction (32.61 71.20) performances than SCNetLS.Moreover, we observed that in theabsence of noise, training SCNetLS results in dense features with small negative biases, hence, making itsperformance close to DenSaE with a large number of atoms in A. We see that SCNetLS in the absenceof a supervised classification loss fails to learn discriminative features useful for classification. On theother hand, enforcing sparsity in SCNethyp suggests that sparse representations are useful for classification.",
  "J1Acc. 96.06% 98.59% 98.61% 98.56% 98.40%Rec.71.2047.7032.6130.2025.57": "How do reconstruction and classifi-cation capabilities change as we vary in joint training?: In joint trainingof the autoencoder and the classifier, itis natural to expect that the reconstruc-tion loss should increase compared to dis-joint training. This is indeed the case forSCNetLS; as we go from disjoint to jointtraining and as increases (), thereconstruction loss increases and classifi-cation accuracy has an overall increase.However, for < 1, joint training of bothnetworks that enforce some sparsity ontheir representations, SCNethyp and Den-SaE, improves reconstruction and classification. For purely discriminative training ( = 1), DenSaE outperforms both SCNetLS and SCNethyp in classificationaccuracy and representation capability. We speculate that this likely results from the fact that, by construction,the encoder from DenSaE seeks to produce two sets of representations: namely a dense one, mostly importantfor reconstruction and a sparse one, useful for classification. In some sense, the dense component acts as aprior that promotes good reconstruction.",
  "Denoising": "We trained DenSaE for supervised image denoising when = 0 using BSD432 and tested it on BSD68 (Martinet al., 2001). All the networks are trained for 250 epochs using the ADAM optimizer (Kingma, 2014) and thefilters are initialized using the random Gaussian distribution. The initial learning rate is set to 104 andthen decayed by 0.8 every 50 epochs. We set of the optimizer to be 103 for stability. At every iteration, arandom patch of size 128 128 is cropped from the training image and zero-mean Gaussian noise is addedto it with the corresponding noise level. We varied the ratio of number of filters in A and B as the overallnumber of filters was kept constant. We evaluate the model in the presence of Gaussian noise with standarddeviation of = {15, 25, 50, 75}.",
  "30.2130.1830.1830.1429.8925 27.7027.7027.6527.5627.2650 24.8124.8124.4324.4423.687523.3123.3323.0922.0920.09": "Ratio of number of filters in A and B: Unlike recon-struction, shows that the smaller the number offilters associated with A, the better DenSaE can denoiseimages. We hypothesize that this is a consequence of ourfindings from that if the column spaces of Aand B are suitably unaligned, the easier is the recoveryx and u. Dense and sparse vs. sparse coding: showsthat DenSaE (best network from ) denoises imagesbetter than CSCNethyp, suggesting that the dense andsparse coding model represents images better than sparse coding.",
  "30.21 (1.74) 30.12 (1.70) 30.34 (1.79)25 27.70 (1.89) 27.51 (1.81) 27.75 (1.89)50 24.81 (1.98) 24.54 (1.85) 24.81 (1.97)75 23.33 (1.96) 22.83 (1.73) 23.32 (1.95)": "Dictionary characteristics: (a) shows the decom-position of a noisy test image ( = 50) by DenSaE. The figuredemonstrates that Ax captures low-frequency content while Bucaptures high-frequency details (edges). This is corroboratedby the smoothness of the filters associated with A, and theGabor-like nature of those associated with B (Mehrotra et al.,1992). We observed similar performance when we tuned x, andfound that, as x decreases, Ax captures a lower frequencies,and Bu a broader range. CSCNet deviates from sparse coding and implicitlylearns Ax + Bu model in the presence of noise: By training the biases, CSCNetLS deviates from thesparse coding model; the neural networks bias is directly related to the sparsity enforcing hyper-parameter u.The larger this bias, the sparser the representations. We observed that CSCNetLS automatically segmentsfilters into three groups: one with small bias, one with intermediate ones, and a third with large values (see). We found that the feature maps associated with the large bias values are all zero. Moreover, themajority of features are associated with intermediate bias values, and are sparse, in contrast to the small",
  "Conclusions": "This paper proposed a novel dense and sparse coding model for a flexible representation of a signal asy = Ax + Bu. Our first result gives a verifiable condition that guarantees uniqueness of the model. Oursecond result uses tools from anisotropic compressed sensing to show that, with sufficiently many linearmeasurements, a convex program with 1 and 2 regularizations can recover the components x and u uniquelywith high probability. Numerical experiments on synthetic and real data confirm our observations. We proposed a dense and sparse autoencoder, DenSaE, tailored to dictionary learning for the Ax + Bu model.DenSaE, naturally decomposing signals into low- and high-frequency components, provides a balance betweenlearning dense representations that are useful for reconstruction and discriminative sparse representations.We showed the superiority of DenSaE to sparse autoencoders for data reconstruction and its competitiveperformance in classification. Abiy Tasissa acknowledges partial support from the National Science Foundation through grant DMS-2208392.Demba Bas and Emmanouil Theodosiss work is supported by the National Science Foundation underCooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and FundamentalInteractions, All the authors would like to thank the the anonymous reviewers fortheir valuable feedback.",
  "Author Contributions": "Abiy Tasissa, Emmanouil Theodosis and Demba Ba studied the model in (1) and worked on the theoreticalanalysis presented in . Emmanouil Theodosis conducted all the experiments in Sections 5.1, 5.2 and5.3. Bahareh Tolooshams designed and conducted the experiments in .4 for dictionary learning-basedneural architectures. All authors contributed to the writing of the manuscript. Alekh Agarwal, Animashree Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learningsparsely used overcomplete dictionaries. In Maria Florina Balcan, Vitaly Feldman, and Csaba Szepesvri(eds.), Proc the 27th Conference on Learning Theory, volume 35 of Proceedings of Machine LearningResearch, pp. 123137, Barcelona, Spain, 1315 Jun 2014. PMLR. Alekh Agarwal, Anima Anandkumar, Prateek Jain, Praneeth Netrapalli, and Rashish Tandon. Learningsparsely used overcomplete dictionaries via alternating minimization. SIAM Journal on Optimization, 26:27752799, 2016.",
  "Ake Bjrck and Gene H Golub. Numerical methods for computing angles between linear subspaces. Mathe-matics of computation, 27(123):579594, 1973": "Jrme Bobin, Jean-Luc Starck, Jalal M Fadili, Yassir Moudden, and David L Donoho. Morphologicalcomponent analysis: An adaptive thresholding strategy. IEEE transactions on image processing, 16(11):26752681, 2007. Emmanuel Candes, Mark Rudelson, Terence Tao, and Roman Vershynin.Error correction via linearprogramming. In 46th Annual IEEE Symposium on Foundations of Computer Science (FOCS05), pp.668681. IEEE, 2005.",
  "Bruno A Olshausen and David J Field. Sparse coding with an overcomplete basis set: A strategy employedby v1? Vision research, 37(23):33113325, 1997": "Samet Oymak, M Amin Khajehnejad, and Babak Hassibi.Recovery threshold for optimal weight l1minimization. In 2012 IEEE International Symposium on Information Theory Proceedings, pp. 20322036.IEEE, 2012. Yagyensh Chandra Pati, Ramin Rezaiifar, and Perinkulam Sambamurthy Krishnaprasad. Orthogonal matchingpursuit: Recursive function approximation with applications to wavelet decomposition. In Proceedings of27th Asilomar conference on signals, systems and computers, pp. 4044. IEEE, 1993.",
  "Graeme Pope, Annina Bracher, and Christoph Studer. Probabilistic recovery guarantees for sparsely corruptedsignals. IEEE transactions on information theory, 59(5):31043116, 2013": "Jason Tyler Rolfe and Yann LeCun. Discriminative recurrent sparse auto-encoders: 1st international conferenceon learning representations, iclr 2013. In 1st International Conference on Learning Representations, ICLR2013, 2013. Bo Shen, Rakesh R Kamath, Hahn Choo, and Zhenyu Kong. Robust tensor decomposition based back-ground/foreground separation in noisy videos and its applications in additive manufacturing. IEEETransactions on Automation Science and Engineering, 2022.",
  "Hao Yan, Kamran Paynabar, and Jianjun Shi. Anomaly detection in images with smooth background viasmooth-sparse decomposition. Technometrics, 59(1):102114, 2017": "Javier Zazo, Bahareh Tolooshams, and Demba Ba. Convolutional dictionary learning in hierarchical networks.In Proc. 2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor AdaptiveProcessing (CAMSAP), pp. 131135. IEEE, 2019. Xiaowei Zhou, Can Yang, and Weichuan Yu. Moving object detection by detecting contiguous outliers in thelow-rank representation. IEEE transactions on pattern analysis and machine intelligence, 35(3):597610,2012.",
  "AClassification and Image Denoising": "We warm start the networks by training the autoencoder for 150 epochs using the ADAM optimizer where theweights are initialized with the random Gaussian distribution. Within the network, x, u, and u are tunedas follows: x and u are step sizes of the gradient-based iteration; hence, they are chosen to make sure thatthe recurrence is contractive while they are large enough to make sure gradient information is effective fromone another to another. Similarly, u is empirically chosen based on the training performance where lambdauis small enough (to result in non-zero representation), and large enough (to visually observe sparse featuremaps). We note that a systematic grid search may improve upon the reported result; however, we suffice tothe above-discussed approach as the goal of our paper is a comparative study in a similar training/tuningsituation. The learning rate is set to 103. We set of the optimizer to be 1015 and used batch size of 16. For disjointclassification training, we trained for 1,000 epochs, and for joint classification training, the network is trainedfor 500 epochs. All the networks use FISTA (Beck & Teboulle, 2009) within their encoder for faster sparsecoding. lists the parameters of the different networks. visualizes the most important atomsfor reconstruction and classification for disjoint training.",
  ": Most important atoms of the dictionary used for reconstruction (rec.) and classification (class.) fordisjoint training": "The figure shows that SCNetLS has the best reconstruction among all, and the second best is DenSaE200A,200B,having the highest number of A atoms. Figures 11 visualizes the reconstruction of MNIST test image forthe joint training when = 1. Notably, in this case, the reconstructions from SCNetLS do not look like theoriginal image. On the other hand, DenSaE even with = 1 is able to reconstruct the image very well. Inaddition, the figures show how Ax and Bu are contribution for reconstruction for DenSaE. We note that as our network is non-convolutional, we do not compare it to the state-of-the-art, a convolutionalnetwork. We do not compare our results with the network in Rolfe & LeCun (2013) as that work does notreport reconstruction loss and it involves a sparsity enforcing loss that change the learning behaviour. For denoising, all the trained networks implement FISTA for faster sparse coding. lists the parametersof the different networks. Moreover, shows the histogram of learned biases by CSCNetLS forvarious noise levels. We note that compared to CSCNet, which has 63K trainable parameters, all thetrained networks including CSCNetLS have 20x fewer trainable parameters. We attribute the difference inperformance, compared to the results reported in the CSCNet paper, to this large difference in the number oftrainable parameters and the usage of a larger dataset."
}