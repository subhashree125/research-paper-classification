{
  "Abstract": "Adversarial robustness often comes at the cost of degraded accuracy, impeding real-lifeapplications of robust classification models. Training-based solutions for better trade-offs arelimited by incompatibilities with already-trained high-performance large models, necessitatingthe exploration of training-free ensemble approaches. Observing that robust models are moreconfident in correct predictions than in incorrect ones on clean and adversarial data alike, wespeculate amplifying this benign confidence property can reconcile accuracy and robustnessin an ensemble setting. To achieve so, we propose MixedNUTS, a training-free methodwhere the output logits of a robust classifier and a standard non-robust classifier are processedby nonlinear transformations with only three parameters, which are optimized through anefficient algorithm. MixedNUTS then converts the transformed logits into probabilitiesand mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet datasets,experimental results with custom strong adaptive attacks demonstrate MixedNUTSs vastlyimproved accuracy and near-SOTA robustness it boosts CIFAR-100 clean accuracy by 7.86points, sacrificing merely 0.87 points in robust accuracy.",
  "Introduction": "Neural classifiers are vulnerable to adversarial attacks, producing unexpected predictions when subject topurposefully constructed human-imperceptible input perturbations and hence manifesting severe safety risks(Goodfellow et al., 2015; Madry et al., 2018). Existing methods for robust deep neural networks (Madryet al., 2018; Zhang et al., 2019) often suffer from significant accuracy penalties on clean (unattacked) data(Tsipras et al., 2019; Zhang et al., 2019; Pang et al., 2022). As deep learning continues to form the coreof numerous products, trading clean accuracy for robustness is understandably unattractive for real-lifeusers and profit-driven service providers. As a result, despite the continuous development in adversarialrobustness research, robust models are rarely deployed and practical services powered by neural networksremain non-robust (Ilyas et al., 2018; Borkar & Chen, 2021). To bridge the gap between robustness research and applications, researchers have strived to reconcile robustnessand accuracy (Balaji et al., 2019; Chen et al., 2021; Raghunathan et al., 2020; Rade & Moosavi-Dezfooli,2021; Liu & Zhao, 2022; Pang et al., 2022; Cheng et al., 2022), mostly focusing on improving robust training",
  "Combination": ": Overview of the proposed MixedNUTS classifier. The nonlinear logit transformation, to beintroduced in , significantly improves the accuracy-robustness balance while only introducing threeparameters efficiently optimized with Algorithm 1. methods. Despite some empirical success, the training-based approach faces inherent challenges. Trainingrobust neural networks from scratch is highly expensive. More importantly, training-based methods sufferfrom performance bottlenecks. This is because the compatibility between different training schemes is unclear,making it hard to combine multiple advancements. Additionally, it is hard to integrate robust trainingtechniques into rapidly improving large models, often trained or pre-trained with non-classification tasks. To this end, an alternative training-free direction has emerged, relieving the accuracy-robustness trade-offthrough an ensemble of a standard (often non-robust) model and a robust model (Bai et al., 2024b;a). Thisensemble is referred to as the mixed classifier, whereas base classifiers refers to its standard and robustcomponents. The mixing approach is mutually compatible with the training-based methods, and henceshould be regarded as an add-on. Unlike conventional homogeneous ensembling, where all base classifiersshare the same goal, the mixed classifier considers heterogeneous mixing, with one base classifier specializingin the benign attack-free scenario and the other focusing on adversarial robustness. Thus, the number of baseclassifiers is naturally fixed as two, in turn maintaining a high inference efficiency. We observe that many robust base models share a benign confidence property: their correct predictions aremuch more confident than incorrect ones. Verifying such a property for numerous existing models trained viadifferent methods (Peng et al., 2023; Pang et al., 2022; Wang et al., 2023; Debenedetti et al., 2023; Na, 2020;Gowal et al., 2020; Liu et al., 2023; Singh et al., 2023), we speculate that strengthening this property canimprove the mixed classifiers trade-off even without changing the base classifiers predicted classes. Based on this intuition, we propose MixedNUTS (Mixed neUral classifiers with Nonlinear TranSformation),a training-free method that enlarges the robust base classifier confidence difference between correct andincorrect predictions and thereby optimizes the mixed classifiers accuracy-robustness trade-off. MixedNUTSapplies nonlinear transformations to the accurate and robust base classifiers logits before converting theminto probabilities used for mixing. We parameterize the transformation with only three coefficients anddesign an efficient algorithm to optimize them for the best trade-off. Unlike (Bai et al., 2024b), MixedNUTSdoes not modify base neural network weights or introduce additional components and is for the first timeefficiently extendable to larger datasets such as ImageNet. MixedNUTS is compatible with various pre-trainedstandard and robust models and is agnostic to the base model details such as training method, defense norm(, 2, etc.), training data, and model architecture. Therefore, MixedNUTS can take advantage of recentdevelopments in accurate or robust classifiers while being general, lightweight, and convenient. Our experiments leverage AutoAttack (Croce & Hein, 2020) and strengthened adaptive attacks (detailsin Appendix B) to confirm the security of the mixed classifier and demonstrate the balanced accuracyand robustness on datasets including CIFAR-10, CIFAR-100, and ImageNet. On CIFAR-100, MixedNUTSimproves the clean accuracy by 7.86 percentage points over the state-of-the-art non-mixing robust modelwhile reducing robust accuracy by merely 0.87 points. On ImageNet, MixedNUTS is the first robust modelto leverage even larger pre-training datasets such as ImageNet-21k. Furthermore, MixedNUTS allows forinference-time adjustments between clean and adversarial accuracy.",
  "AS refers to adaptive smoothing (Bai et al., 2024a), another ensemble framework. Unlike AS, MixedNUTS requires no additional training": ": MixedNUTSs accuracy-robustness balance compared to state-of-the-art models on RobustBench.MixedNUTS is more accurate on clean data than all standalone robust models. At the same time, MixedNUTSachieves the second-highest robustness among all models for CIFAR-100 and ImageNet, and is the third mostrobust for CIFAR-10. of zi = + for some i, with all other entries of z being less than +. We define (z) for such a z vector to bethe basis (one-hot) vector ei. For a classifier h : Rd Rc, we use the composite function h : Rd c",
  "mh(x) := hy(x) maxi=y hi(x)": "We consider a classifier to be (p-norm) robust at some input x Rd if it assigns the same label to allperturbed inputs x + such that p , where 0 is the attack radius. We additionally introduce thenotion of the worst-case adversarial perturbation in the sense of minimizing the margin:",
  "min mh(x + )": "We define the optimizer of this problem, h(x), as the minimum-margin perturbation of h() around x. Wefurther define the optimal objective value, denoted as mh(x), as the minimum margin of h() around x. The attack formulation considered in Definition 2.2 is highly general. Intuitively, when the minimum marginis negative, the adversarial perturbation successfully changes the model prediction. When it is positive, themodel is robust at x, as perturbations within radius cannot change the prediction.",
  "Related Adversarial Attacks and Defenses": "While the fast gradient sign method (FGSM) and projected gradient descent (PGD) attacks could attackand evaluate certain models (Madry et al., 2018; Goodfellow et al., 2015), they are insufficient and can failto attack non-robust models (Carlini & Wagner, 2017; Athalye et al., 2018b; Papernot et al., 2017). Tothis end, stronger adversaries leveraging novel attack objectives, black-box attacks, and expectation overtransformation have been proposed (Gowal et al., 2019; Croce & Hein, 2020; Tramr et al., 2020). Benchmarksbased on these strong attacks, such as RobustBench (Croce et al., 2021), ARES-Bench (Liu et al., 2023), andOODRobustBench (Li et al., 2023), aim to unify defense evaluation. AutoAttack (Croce & Hein, 2020) is a combination of white-box and black-box attacks (Andriushchenkoet al., 2020). It is the attack algorithm of RobustBench (Croce et al., 2021), where AutoAttack-evaluatedrobust models are often agreed to be trustworthy. We select AutoAttack as the main evaluator, with furtherstrengthening tailored to our defense.",
  "Published in Transactions on Machine Learning Research (08/2024)": "classifier used in our main results, which is from (Peng et al., 2023). Suppose that this model is a black box,and the gradient-based components of MMAA cannot be performed. Then, we can seek some similar robustmodels whose gradients are visible. We use two models, one from (Pang et al., 2022), and the other from(Wang et al., 2023), as examples. As shown in , the optimal s, p, c values calculated via Algorithm 1are highly similar for these three models. Hence, if we have access to the gradients of one of (Peng et al.,2023; Pang et al., 2022; Wang et al., 2023), then we can use Algorithm 1 to select the hyper-parametercombinations for all three models.",
  "Ensemble and Calibration": "Model ensembles, where the outputs of multiple models are combined to produce the overall prediction, havebeen explored to improve model performance (Ganaie et al., 2022) or estimate model uncertainty (Liu et al.,2019). Ensembling has also been considered to strengthen adversarial robustness (Pang et al., 2019; Adam& Speciel, 2020; Alam et al., 2022; Co et al., 2022). Theoretical robustness analyses of ensemble modelsindicate that the robust margins, gradient diversity, and runner-up class diversity all contribute to ensemblerobustness (Petrov et al., 2023; Yang et al., 2022). These existing works usually consider homogeneous ensemble, meaning that all base classifiers share thesame goal (better robustness). They often combine multiple robust models for incremental improvements.In contrast, this work focuses on heterogeneous mixing, a fundamentally different paradigm. Here, the baseclassifiers specialize in different data and the mixed classifier combines their advantages. Hence, we focus onthe two-model setting, where the role of each model is clearly defined. Specifically, the accurate base classifierspecializes in clean data and is usually non-robust, and the robust base classifier excels in adversarial data. A parallel line of work considers mixing neural model weights (Ilharco et al., 2022; Cai et al., 2023). Theygenerally require all base classifiers to have the same architecture and initialization, which is more restrictive. Model calibration often involves adjusting confidence, which aims to align a models confidence with itsmispredicting probability, usually via temperature scaling (Guo et al., 2017; Yu et al., 2022; Hinton et al.,2015). While adjusting the confidence of a single model generally does not change its prediction, this is notthe case in the ensemble setting. Unlike most calibration research focusing on uncertainty, this paper adjuststhe confidence for performance.",
  "Mixing Classifiers for Accuracy-Robust Trade-Off": "Consider a classifier gstd : Rd Rc, whose predicted logits are gstd,1, . . . , gstd,c, where d is the input dimensionand c is the number of classes. We assume gstd() to be a standard classifier trained for high clean accuracy(and hence may not manifest adversarial robustness). Similarly, we consider another classifier hrob : Rd Rc",
  "and assume it to be robust against adversarial attacks. We use accurate base classifier (ABC) and robustbase classifier (RBC) to refer to gstd() and hrob()": "Mixing the outputs of a standard classifier and a robust classifier improves the accuracy-robustness trade-off, and it has been shown that mixing the probabilities is more desirable than mixing the logits fromtheoretical and empirical perspectives (Bai et al., 2024b;a). Here, we denote the proposed mixed model withfmix : Rd Rc. Specifically, the ith output logit of the mixed model follows the formulation",
  "fmix,i(x) := log(1 ) gstd,i(x) + hrob,i(x)(1)": "for all i [c], where [1/2, 1] adjusts the mixing weight1. The mixing operation is performed in theprobability space, and the natural logarithm maps the mixed probability back to the logit space withoutchanging the predicted class for interchangeability with existing models. If the desired output is the probability fmix(), the logarithm can be omitted.",
  "Base Classifier Confidence Modification": "We observe that the robust base classifier hrob() often enjoys a benign confidence property: it is much moreconfident in correct predictions than in mispredictions. I.e., hrob()s confidence margin is much higher whenit makes correct predictions. Even if some input is subject to attack (which vastly decreases the confidencemargin of correct predictions), if it is correctly predicted, its margin is still expected to be larger thanincorrectly predicted natural examples. .2 verifies this property with multiple model examples, andAppendix C.3 visualizes the confidence margin distributions. As a result, when mixing the output probabilities hrob() and gstd() on clean data, where gstd() isexpected to be more accurate than hrob(), gstd() can correct hrob()s mistake because hrob() is unconfident.Meanwhile, when the mixed classifier is under attack and hrob() becomes much more reliable than gstd(),hrob()s high confidence in correct predictions can overcome gstd()s misguided outputs. Hence, even whengstd()s robust accuracy is near zero, the mixed classifier still inherits most of hrob()s robustness. Combiningthe above two cases, we can see that the benign confidence property of hrob() allows the mixed classifierto simultaneously take advantage of gstd()s high clean accuracy and hrob()s adversarial robustness. As aresult, modifying and enhancing the base classifiers confidence has vast potential to further improve themixed classifier. Note that this benign confidence property is only observed on robust classifiers. Neural classifiers trainedwithout any robustness considerations often make highly confident mispredictions when subject to adversarialattack. These mispredictions can be even more confident than correctly predicted unperturbed examples,often seeing confidence margins very close to 1. As a result, gstd() does not enjoy the benign confidenceproperty, and its confidence property is in general detrimental to the mixture.",
  "Accurate Base Classifier Temperature Scaling": "We start with analyzing the accurate base classifier gstd(), with the goal of mitigating its detrimentalconfidence property. One approach to achieve this is to scale up gstd()s logits before the Softmax operation.To this end, we consider temperature scaling (Hinton et al., 2015). Specifically, we construct the temperaturescaled model gTS(T )std(), whose ith entry is",
  "for all i, where T 0 is the temperature constant. To scale up the confidence, T should be less than 1": "To understand this operation, observe that temperature scaling increases gstd()s confidence in correct cleanexamples and incorrect adversarial examples simultaneously. However, because gstd()s confidence underattack is already close to 1 before scaling, the increase in attacked misprediction confidence is negligible dueto the saturation of the Softmax function. Since gstd() becomes more confident on correct examples with themispredicting confidence almost unchanged, its detrimental confidence property is mitigated. The extreme selection for the temperature T is 0, in which case the predicted probabilities gTS(0)std() becomesa one-hot vector corresponding to gstd()s predicted class. By scaling with T = 0, the detrimental confidenceproperty of gstd() is completely eliminated, as a constant margin of precisely 1 is enforced everywhere. Notethat we still hope to preserve the ranking of class-wise outputs gstd,i(), so that we can preserve the highaccuracy of gstd(). Given this requirement, eliminating gstd()s detrimental confidence property by enforcinga consistent margin is the best one can expect. Appendix D.4 verifies that T = 0 produces the best empiricaleffectiveness among several temperature values. Appendix B.2 discusses how our attacks circumvent thenon-differentiability resulting from using T = 0. In addition to eliminating the detrimental confidence property of gstd(), selecting T = 0 also simplifies theanalysis on the robust base model hrob() by establishing a direct correlation between hrob()s confidence andthe mixed classifiers correctness, thereby allowing for tractable and efficient optimization. Hence, we selectT = 0 and use gTS(0)std() as the accurate base classifier for the remaining analyses.",
  "MixedNUTS Nonlinearly Mixed Classifier": "In contrast to the accurate base classifier, the robust base classifier hrob()s confidence property is benign.To achieve the best accuracy-robustness trade-off with the mixed classifier, we need to augment this benignproperty as much as possible. While a similar temperature scaling operation can achieve some of the desiredeffects, its potential is limited by applying the same operation to confident and unconfident predictions, andis therefore suboptimal. To this end, we extend confidence modification beyond temperature scaling (which islinear) to allow nonlinear logit transformations. By introducing nonlinearities, we can treat low-confidenceand high-confidence examples differently, significantly amplifying hrob()s benign property and therebyconsiderably enhancing the mixed classifiers accuracy-robustness balance.2",
  "Nonlinear Robust Base Classifier Transformation": "We aim to build a nonlinearly mapped classifier hMrob() := M(hrob()), where M M : Rc Rc is a nonlineartransformation applied to the classifier hrob()s logits, and M is the set of all possible transformations. Theprediction probabilities from this transformed robust base model are then mixed with those from gTS(0)std() toform the mixed classifier f Mmix() following (1). For the optimal accuracy-robustness trade-off, we select an Mthat maximizes the clean accuracy of f Mmix() while maintaining the desired robust accuracy. Formally, thisgoal is described as the optimization problem",
  "subject toP(X,Y )Darg maxif Mmix,i(X + f Mmix(X)) = Yrf Mmix,": "where D is the distribution of data-label pairs, rf Mmix is the desired robust accuracy of f Mmix(), and f Mmix(x) isthe minimum-margin perturbation of f Mmix() at x. Note that f Mmix,i() implicitly depends on M and . The problem (2) depends on the robustness behavior of the mixed classifier, which is expensive to probe.Ideally, the optimization should only need the base classifier properties, which can be evaluated beforehand.To allow such a simplification, we make the following two assumptions.Assumption 4.1. On unattacked clean data, if hMrob() makes a correct prediction, then gstd() is also correct. Assumption 4.1 allows us to focus on examples correctly classified by the accurate base classifier gTS(0)std()but not by the robust base model hMrob() when optimizing the transformation M() to maximize the cleanaccuracy of the mixed classifier. Under Assumption 4.1, we can safely discard the opposite case of gTS(0)std()being incorrect while hMrob() being correct on clean data. Assumption 4.1 makes sense because gTS(0)std()sclean accuracy should be considerably higher than hMrob()s to justify mixing them together, and trainingstandard classifiers that noticeably outperforms robust models on clean data is usually possible in practice.Assumption 4.2. The transformation M() does not change the predicted class due to, e.g., monotonicity.Namely, it holds that arg maxi M(hrob(x))i = arg maxi hrob,i(x) for all x. We make this assumption because we want the logit transformation to preserve the accuracy of hrob(). Whileit is mathematically possible to obtain an M() that further increases the accuracy of hrob(), finding it couldbe as hard as training a new improved robust model. Hence, the best one would expect from a relativelysimple nonlinear transformation is to enhance hrob()s benign confidence margin property while not changingthe predicted class. Later in this paper, we will propose Algorithm 1 to find such a transformation. These two assumptions allow us to decouple the optimization of M() from the accurate base classifier gstd().This is because as proven in (Bai et al., 2024b, Lemma 1), the mixed classifier is guaranteed to be robustwhen hMrob is robust with margin no smaller than 1",
  "Theorem 4.3. Suppose that Assumption 4.2 holds. Let rf Mmix and rhrob denote the robust accuracy of f Mmix()and hrob() respectively. If rfMmix/rhrob, then a solution to (3) is feasible for (2)": "Theorem 4.4. Suppose that Assumption 4.1 holds. Furthermore, consider an input random variable Xand suppose that the margin of hMrob(X) is independent of whether gstd(X) is correct. Then, minimizing theobjective of (3) is equivalent to maximizing the objective of (2). The proofs of Theorem 4.3 and Theorem 4.4 are provided in Appendices A.1 and A.2, respectively. InAppendix D.8.1 and Appendix D.8.2, we discuss the minor effects of slight violations to Assumption 4.1 andAssumption 4.2, respectively. Moreover, the independence assumption in Theorem 4.4 can be relaxed withminor changes to our method, which we discuss in Appendix D.8.3. Also note that Theorems 4.3 and 4.4 relyon using T = 0 for gstd()s temperature scaling, justifying this temperature setting selected in .1.",
  "Parameterizing the Transformation M": "Optimizing the nonlinear transformation M() requires representing it with parameters. To avoid introducingadditional training requirements or vulnerable backdoors, the parameterization should be simple (i.e., notintroducing yet another neural network). Thus, we introduce a manually designed transformation with onlythree parameters, along with an algorithm to efficiently optimize the three parameters. Unlike linear scaling and the Softmax operation, which are shift-agnostic (i.e., adding a constant to alllogits does not change the predicted probabilities), the desired nonlinear transformations behavior heavilydepends on the numerical range of the logits. Thus, to make the nonlinear transformation controllable andinterpretable, we pre-process the logits by applying layer normalization (LN): for each input x, we standardizethe logits hrob(x) to have zero mean and identity variance. We observe that LN itself also slightly increasesthe margin difference between correct and incorrect examples, favoring our overall formulation as shown in. This phenomenon is further explained in Appendix D.7. Among the post-LN logits, only those associated with confidently predicted classes can be large positivevalues. To take advantage of this property, we use a clamping function Clamp(), such as ReLU, GELU, ELU,or SoftPlus, to bring the logits smaller than a threshold toward zero. This clamping operation can furthersuppress the confidence of small-margin predictions while preserving large-margin predictions. Since correctexamples often enjoy larger margins, the clamping function enlarges the margin gap between correct andincorrect examples. We provide an ablation study over candidate clamping functions in Appendix D.2 andempirically select GELU for our experiments. Finally, since the power functions with greater-than-one exponents diminish smaller inputs while amplifyinglarger ones, we exponentiate the clamping function outputs to a constant power and preserve the sign. Puttingeverything together, with the introduction of three scalars s, p, and c to parameterize M(), the combinednonlinearly transformed robust base classifier hM(s,p,c)rob() becomes",
  "hM(s,p,c)rob(x) = s hClamp(c)rob(x)p sgnhClamp(c)rob(x)wherehClamp(c)rob(x) = ClampLN(hrob(x)) + c.(4)": "Here, s (0, +) is a scaling constant, p (0, +) is an exponent constant, and c R is a bias constant thatadjusts the cutoff location of the clamping function. With a slight abuse of notation, M(s, p, c)() denotes thetransformation parameterized with s, p, and c. In (4), we apply the absolute value before the exponentiationto maintain compatibility with non-integer p values and use the sign function to preserve the sign. Note thatwhen the clamping function is linear and p = 1, (4) degenerates to temperature scaling with LN. Hence, anoptimal combination of s, p, and c is guaranteed to be no worse than temperature scaling.",
  "Exactly solving (6) involves evaluating mhM(s,p,c)rob(x) for every x in the support of the distribution of correctly": "predicted adversarial examples X adv. This is intractable because the support is a continuous set and thedistributions X clean and X adv implicitly depend on the optimization variables s, p, and c. To this end, weapproximate X clean and X adv with a small set of data. Consider the subset of clean examples incorrectlyclassified by hLNrob(), denoted as X clean, and the subset of attacked examples correctly classified by hLNrob(),denoted as X adv. Because we use hLNrob() instead of hM(s,p,c)rob() to obtain X clean and X adv, using them assurrogates to X clean and X adv decouples the probability measures from the optimization variables. Despite optimizing s, p, c, and on a small set of data, overfitting is unlikely since there are only fourparameters, and thus a small number of images should suffice. Appendix D.3 analyzes the effect of the datasubset size on optimization quality and confirms the absence of overfitting.",
  "adversarial attack. Note that calculating mhM(s,p,c)rob(x) does not require attacking hM(s,p,c)rob() and instead": "attacks hLNrob(), which is independent of the optimization variables, ensuring optimization efficiency. To obtainmhM(s,p,c)rob(x), in Appendix B.1, we propose minimum-margin AutoAttack (MMAA), an AutoAttack variantthat keeps track of the minimum margin while generating perturbations. While some components of MMAArequire hLNrob()s gradient information, Algorithm 1 can still apply after some modifications even if the baseclassifiers are black boxes with unavailable gradients, with the details discussed in Appendix D.5. Since the probability measures and the perturbations are now both decoupled from s, p, c, , we only needto run MMAA once to estimate the worst-case perturbation, making this hyper-parameter search problemefficiently solvable. While using hLNrob() as a surrogate to hM(s,p,c)rob() introduces a distribution mismatch, weexpect this mismatch to be benign. To understand this, observe that the nonlinear logit transformation (4)generally preserves the predicted class due to the (partially) monotonic characteristics of GELU and thesign-preserving power function. Consequently, we expect the accuracy and minimum-margin perturbations ofhM(s,p,c)rob() to be very similar to those of hLNrob(). Appendix D.9 empirically verifies this speculated proximity.",
  "Since (7) only has four optimization variables, it can be solved via a grid search algorithm. Furthermore, theconstraint PZ AadvmhM(s,p,c)rob(Z) 1": "should always be active at optimality.3 Hence, we can treatthis constraint as equality, reducing the searching grid dimension to three. Specifically, we sweep over arange of s, p, and c to form the grid, and calculate the value that binds the chance constraint for eachcombination. Among the grid, we then select an s, p, c combination that minimizes (7)s objective. The resulting algorithm is Algorithm 1. As discussed above, this algorithm only needs to query MMAAsAPGD components once on a small set of validation data, and all other steps are simple mathematicaloperations requiring minimal computation. Additionally, note that the optimization precision of Algorithm 1is governed by the discrete nature of the evaluation dataset. I.e., with a dataset consisting of 10,000 examples(such as the CIFAR-10 and CIFAR-100 evaluation sets), the finest optimization accuracy one can expectis 0.01% in terms of objective value (accuracy). Hence, it is not necessary to solve (7) to a high accuracy.Moreover, as shown in in Appendix D.1, which analyzes the sensitivity of the formulation (7)sobjective value with respect to s, p, and c, the optimization landscape is relatively smooth. Therefore, arelatively coarse grid (512 combinations in our case) can find a satisfactory solution, and hence Algorithm 1 ishighly efficient despite the triply nested loop structure. Furthermore, the base classifier raw logits associatedwith hLNrob()s minimum-margin perturbations do not depend on s, p, c and can be cached. Hence, the numberof forward loops is agnostic to the search space size. All of the above makes Algorithm 1 efficiently solvable. In practice, the triply-nested grid search loop canbe completed within ten seconds on a laptop computer, and performing MMAA on 1000 images requires3752/10172 seconds for CIFAR-100/ImageNet with a single Nvidia RTX-8000 GPU.",
  "Visualization of the Nonlinear Logit Transformation M(s, p, c)": "To better understand the effects of the proposed nonlinear logit transformation M(s, p, c)(), we visualize howit affects the base classifier prediction probabilities when coupled with the Softmax operation. Consider athree-class (A, B, and C) classification problem, with two example logit vectors. The first example simulatesthe case where class A is clearly preferred over the rest (large margin), while the second example illustratesa competition between classes B and C (small margin).The raw logits, the corresponding predictionprobabilities, and the probabilities computed with the transformed logits are visualized in . Clearly,the margin is further increased for the large margin case and shrunk for the small margin case, which alignswith the goal of enlarging the benign confidence property of the base classifiers. For further demonstration, we adjust the overall confidence level for the above two cases and compare howtheir prediction probabilities change with the confidence level. Specifically, by applying temperature scalingand varying the temperature , the prediction probability vectors form trajectories on the probability simplex,whose vertices represent the classes.4 For example, a small temperature increases the overall predictionconfidence, moving the vector toward a vertex. Conversely, a large temperature attracts the predictionprobability to the simplexs centroid. By continuously adjusting the temperature, we obtain trajectories thatconnect the centroid to the vertices. By comparing the trajectories formed with or without the nonlinear logittransformation ((hrob()/) and (hM(s,p,c)rob()/)), we can better understand the transformations properties. shows the prediction probability vectors at three example temperature values, as well as the trajectoriesformed by continuously varying the temperature.We observe that the nonlinear logit transformationsignificantly slows down the movement of the small margin case from the centroid to the vertex. Moreover, thetrajectory with the transformation is straighter and further from the edge BC, implying that the competitionbetween classes B and C has been reduced. In the context of mixed classifiers, the nonlinear transformationreduces the robust base classifiers relative authority in the mixture when it encounters competing classes,thereby improving the mixed classifiers accuracy-robustness trade-off.",
  "Experiments": "We use extensive experiments to demonstrate the accuracy-robustness balance of the MixedNUTS classifierf M(s,p,c)mix(), focusing on the effectiveness of the nonlinear logit transformation. Our evaluation uses CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and ImageNet (Deng et al., 2009) datasets. For 4Here, the purpose of temperature scaling is different from .1. In .1, temperature scaling mitigates gstd()sdetrimental confidence property. Here, scaling with variable temperatures generates probability trajectories for visualization.",
  "Margin Gap(higher is better).xxx": ": The median confidence margin of the accurate/robust base classifier gstd()/hrob(), the layer-normedlogits hLNrob(), and the nonlinearly transformed model hM(s,p,c)rob() on clean and AutoAttacked data, groupedby prediction correctness. The number above each bar group is the margin gap, defined as the differencebetween the medians on clean incorrect inputs and AutoAttacked correct ones. A higher margin gap signalsmore benign confidence property, and thus better accuracy-robustness trade-off for the mixed classifier.",
  "CIFAR-1028.53%+4.70%CIFAR-10031.72%+1.52%ImageNet12.14%+2.62%": "each dataset, we select the model with the highest ro-bust accuracy verified on RobustBench (Croce et al.,2021) as the robust base classifier hrob(), and se-lect a state-of-the-art standard (non-robust) modelenhanced with extra training data as the accuratebase classifier gstd(). Detailed model information isreported in Appendix C.1. As an ensemble method, in addition to being training-free, MixedNUTS is also highly efficient during infer-ence time. Compared with a state-of-the-art robust classifier, MixedNUTSs increase in inference FLOPs is aslow as 24.79%. A detailed comparison and discussion on inference efficiency can be found in Appendix C.2. All mixed classifiers are evaluated with strengthened adaptive AutoAttack algorithms specialized in attackingMixedNUTS and do not manifest gradient obfuscation issues, with the details explained in Appendix B.2.",
  "Main Experiment Results": "compares MixedNUTS with its robust base classifier, its accurate base classifier, and the baselinemethod Mixed (Bai et al., 2024b) on three datasets. Specifically, Mixed is a mixed classifier without thenonlinear logit transformations. shows that MixedNUTS consistently achieves higher clean accuracyand better robustness than this baseline, confirming that the proposed logit transformations mitigate theoverall accuracy-robustness trade-off. compares MixedNUTSs relative error rate change over its robust base classifier, showing thatMixedNUTS vastly reduces the clean error rate with only a slight robust error rate increase. Specifically, the",
  "relative clean error rate improvement is 6 to 21 times more prominent than the relative robust error rateincrease. Clearly, MixedNUTS balances accuracy and robustness without additional training": "compares the robust base classifiers confidence margins on clean and attacked data with or withoutour nonlinear logit transformation (4).For each dataset, the transformation enlarges the margin gapbetween correct and incorrect predictions, especially in terms of the median which represents the marginmajority. Using hM(s,p,c)rob() instead of hrob() makes correct predictions more confident while keeping themispredictions less confident, making the mixed classifier more accurate without losing robustness. compares MixedNUTS with existing methods with the highest AutoAttack-validated adversarialrobustnesses, confirming that MixedNUTS noticeably improves clean accuracy while maintaining competitiverobustness. Moreover, since MixedNUTS can use existing or future improved accurate or even robust modelsas base classifiers, the entries of should not be regarded as pure competitors. Existing models suffer from the most pronounced accuracy-robustness trade-off on CIFAR-100, whereMixedNUTS offers the most prominent improvement.MixedNUTS boosts the clean accuracy by 7.86percentage points over the state-of-the-art non-mixing robust model while reducing merely 0.87 points inrobust accuracy. In comparison, the previous mixing method (Bai et al., 2024a) sacrifices 3.95 points ofrobustness (4.5x MixedNUTSs degradation) for a 9.99-point clean accuracy bump using the same basemodels. Moreover, (Bai et al., 2024a) requires training an additional mixing network component, whereasMixedNUTS is training-free (MixedNUTS is also compatible with the mixing network for even better results).Clearly, MixedNUTS utilizes the robustness of hrob() more effectively and efficiently. On CIFAR-10 and ImageNet, achieving robustness against common attack budgets penalizes the cleanaccuracy less severely than on CIFAR-100. Nonetheless, MixedNUTS is still effective in these less suitablecases, reducing the clean error rate by 28.53%/12.14% (relative) while only sacrificing 1.91%/0.98% (relative)robust accuracy on CIFAR-10/ImageNet compared to non-mixing methods. On CIFAR-10, MixedNUTSmatches (Bai et al., 2024a)s clean accuracy while reducing the robust error rate by 5.17% (relative). With the nonlinear transformation in place, it is still possible to adjust the emphasis between clean androbust accuracy at inference time. This can be achieved by simply re-running Algorithm 1 with a different value. Note that the MMAA step in Algorithm 1 does not depend on , and hence can be cached to speedup re-runs. Meanwhile, the computational cost of the rest of Algorithm 1 is marginal. Our experiments use = 98.5% for CIFAR-10 and -100, and use = 99.0% for ImageNet. The optimal s, p, c values and thesearching grid used in Algorithm 1 are discussed in Appendix C.1.",
  "MixedNUTS (ours)MixedTRADESTRADES (larger)": ": Accuracy-robustness trade-off compari-son between MixedNUTS, mixed classifier withoutnonlinear transformation, and TRADES on 1000CIFAR-10 images. TRADES (larger) denotes alarger TRADES model trained from scratch thathas the same size as MixedNUTS. classifier combinations. For a fair confidence margincomparison, all logits are standardized to zero meanand identity variance (corresponding to hLNrob()) beforeconverted into probabilities. Appendix C.3 presentshistograms to offer more margin distribution details. illustrates that existing CIFAR-10 and -100models have tiny confidence margins for mispredictionsand moderate margins for correct predictions, implyingthat most mispredictions have a close runner-up class.ImageNet classifiers also have higher confidence in cor-rect predictions than incorrect ones. However, whilethe logits are always standardized before Softmax, theImageNet models have higher overall margins than theCIFAR ones. This observation indicates that ImageNetmodels often do not have a strong confounding classdespite having more classes, and their non-predictedclasses probabilities spread more evenly.",
  "Accuracy-Robustness Trade-Off Curves": "In , we show MixedNUTSs robust accuracy as a function of its clean accuracy. We then compare thisaccuracy-robustness trade-off curve with that of the mixed classifier without nonlinear logit transformation(Bai et al., 2024b) and that of TRADES (Zhang et al., 2019), a popular adjustable method that aims toimprove the trade-off. Note that adjusting between accuracy and robustness with TRADES requires tuningits training loss hyper-parameter TR and training a new model, whereas the mixed classifiers are training-freeand can be adjusted at inference time. Specifically, we select CIFAR-10 WideResNet-34-10 models trained with TR = 0, 0.1, 0.3, and 6 as thebaselines, where 0 corresponds to standard (non-robust) training and 6 is the default which optimizesrobustness. For a fair comparison, we use the TRADES models with TR = 0 and 6 to assemble themixed classifiers. For MixedNUTS, we adjust the level of robustness by tuning , the level-of-robustnesshyperparameter of Algorithm 1, specifically considering values of 1, 0.96, 0.93, 0.8, and 0.",
  "confirms that training-free mixed classifiers, MixedNUTS and Mixed (Bai et al., 2024b), achieve muchmore benign accuracy-robustness trade-offs than TRADES, with MixedNUTS attaining the best balance": "Since MixedNUTS is an ensemble, it inevitably results in a larger overall model than the TRADES baseline. Toclarify that MixedNUTSs performance gain is not due to the increased size, we train a larger TRADES model(other training settings are unchanged) to match the parameter count, inference FLOPS, and parallelizability.As shown in , this larger TRADES models clean and robust accuracy does not improve over theoriginal, likely because the original training schedule is suboptimal for the increased size. This is unsurprising,as it has been shown that no effective one-size-fits-all adversarial training parameter settings exist (Duesterwaldet al., 2019). Hence, an increased inference computation does not guarantee better performance on its own.To make a model benefit from a larger size via training, neural architecture and training setting searches arelikely required, which is highly cumbersome and unpredictable. In contrast, MixedNUTS is a training-freeplug-and-play add-on, enjoying significantly superior practicality.",
  "Conclusions": "This work proposes MixedNUTS, a versatile training-free method that combines the output probabilitiesof a robust classifier and an accurate classifier. By introducing nonlinear base model logit transformations,MixedNUTS more effectively exploits the benign confidence property of the robust base classifier, therebyachieving a balance between clean data classification accuracy and adversarial robustness. For performance-driven practitioners, this balance implies less to lose in using robust models, incentivizing the real-worlddeployment of safe deep learning systems. For researchers, as improving the accuracy-robustness trade-off with",
  "George Adam and Romain Speciel. Evaluating ensemble robustness against adversarial attacks. arXiv preprintarXiv:2005.05750, 2020": "Manaar Alam, Shubhajit Datta, Debdeep Mukhopadhyay, Arijit Mondal, and Partha Pratim Chakrabarti.Resisting adversarial attacks in deep neural networks using diverse decision boundaries. arXiv preprintarXiv:2208.08697, 2022. Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: Aquery-efficient black-box adversarial attack via random search. In European Conference on ComputerVision, 2020. Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples. In International Conference on Machine Learning, 2018a.",
  "Yatong Bai, Tanmay Gautam, Yu Gai, and Somayeh Sojoudi. Practical convex formulation of robustone-hidden-layer neural network training. In American Control Conference, 2022": "Yatong Bai, Tanmay Gautam, and Somayeh Sojoudi. Efficient global optimization of two-layer relu networks:Quadratic-time algorithms and adversarial training. SIAM Journal on Mathematics of Data Science, 5(2):446474, 2023. Yatong Bai, Brendon G Anderson, Aerin Kim, and Somayeh Sojoudi. Improving the accuracy-robustnesstrade-off of classifiers via adaptive smoothing. SIAM Journal on Mathematics of Data Science, 6(3):788814, 2024a.",
  "Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In IEEESymposium on Security and Privacy, 2017": "Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting may bemitigated by properly learned smoothening. In International Conference on Learning Representations,2021. Minhao Cheng, Qi Lei, Pin Yu Chen, Inderjit Dhillon, and Cho Jui Hsieh. CAT: Customized adversarialtraining for improved robustness. In International Joint Conference on Artificial Intelligence, 2022.",
  "Sven Gowal, Jonathan Uesato, Chongli Qin, Po-Sen Huang, Timothy Mann, and Pushmeet Kohli. Analternative surrogate loss for PGD-based adversarial testing. arXiv preprint arXiv:1910.09338, 2019": "Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits ofadversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020. Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy AMann. Improving robustness using generated data. In Annual Conference on Neural Information ProcessingSystems, 2021.",
  "Tianyu Pang, Min Lin, Xiao Yang, Jun Zhu, and Shuicheng Yan. Robustness and accuracy could bereconcilable by (proper) definition. In International Conference on Machine Learning, 2022": "Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.Practical black-box attacks against machine learning. In ACM Asia Conference on Computer and Commu-nications Security, 2017. ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, JasonMartin, and Duen Horng Chau. Robust principles: Architectural design principles for adversarially robustCNNs. arXiv preprint arXiv:2308.16258, 2023.",
  "Samuel Pfrommer, Brendon Anderson, Julien Piet, and Somayeh Sojoudi. Asymmetric certified robustnessvia feature-convex neural networks. Advances in Neural Information Processing Systems, 2024": "Rahul Rade and Seyed-Mohsen Moosavi-Dezfooli. Helper-based adversarial training: Reducing excessivemargin to achieve a better accuracy vs. robustness trade-off. In ICML Workshop on Adversarial MachineLearning, 2021. Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding andmitigating the tradeoff between robustness and accuracy. In International Conference on Machine Learning,2020.",
  "Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A Calian, Florian Stimberg, Olivia Wiles, and Timothy Mann.Fixing data augmentation to improve adversarial robustness. arXiv preprint arXiv:2103.01946, 2021": "Vikash Sehwag, Saeed Mahloujifar, Tinashe Handina, Sihui Dai, Chong Xiang, Mung Chiang, and PrateekMittal. Robust learning meets generative models: Can proxy distributions improve adversarial robustness?In International Conference on Learning Representations, 2022. Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry SDavis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! In Annual Conference on NeuralInformation Processing Systems, 2019.",
  "A.1Proof to Theorem 4.3": "Theorem 4.3 (restated). Suppose that Assumption 4.2 holds. Let rf Mmix and rhrob denote the robust accuracyof f Mmix() and hrob() respectively. If rfMmix/rhrob, then a solution to (3) is feasible for (2). Proof. Suppose that M() is a solution from (3).Since the mixed classifier f Mmix() is by constructionguaranteed to be correct and robust at some x if hMrob() is correct and robust with a margin no smaller than1",
  "A.2Proof to Theorem 4.4": "Theorem 4.4 (restated). Suppose that Assumption 4.1 holds. Furthermore, consider an input randomvariable X and suppose that the margin of hMrob(X) is independent of whether gstd(X) is correct. Then,minimizing the objective of (3) is equivalent to maximizing the objective of (2). Proof. By the construction of the mixed classifier, for a clean input x incorrectly classified by hMrob() (i.e., xis in the support of X clean), the mixed classifier prediction fmix(x) is correct if and only if gTS(0)std(x) is correctand hMrob(x)s margin is no greater than 1",
  "=arg maxMM, [1/2,1]P(X,Y )Darg maxif Mmix,i(X) = Y,": "where the last equality holds because under Assumption 4.1, hMrob(x) being correct guarantees gTS(0)std(x)scorrectness. Since the mixed classifier f Mmix() must be correct given that hMrob(x) and gTS(0)std(x) are bothcorrect, f Mmix() must be correct at clean examples correctly classified by hMrob(). Hence, maximizing f Mmix()sclean accuracy on hMrob()s mispredictions is equivalent to maximizing f Mmix()s overall clean accuracy.",
  "B.1Minimum-Margin AutoAttack for Margin Estimation": "Our margin-based hyper-parameter selection procedure (Algorithm 1) and confidence margin estimationexperiments (Figures 6 and 7) require approximating the minimum-margin perturbations associated with therobust base classifier in order to analyze the mixed classifier behavior. Approximating these perturbationsrequires a strong attack algorithm. While AutoAttack is often regarded as a strong adversary that reliablyevaluates model robustness, its original implementation released with (Croce & Hein, 2020) does not returnall perturbed examples. Specifically, traditional AutoAttack does not record the perturbation around someinput if it deems the model to be robust at this input (i.e., model prediction does not change). While thisis acceptable for estimating robust accuracy, it forbids the calculation of correctly predicted AutoAttackedexamples confidence margins, which are required by Algorithm 1, , and . To construct a strong attack algorithm compatible with margin estimation, we propose minimum-marginAutoAttack (MMAA). Specifically, we modify the two APGD components of AutoAttack (untargeted APGD-CE and targeted APGD-DLR) to keep track of the margin at each attack step (the margin history is sharedacross the two components) and always return the perturbation achieving the smallest margin. The FABand Square components of AutoAttack are much slower than the two APGD components, and for our baseclassifiers, FAB and Square rarely succeed in altering the model predictions for images that APGD attacksfail to attack. Therefore, we exclude them for the purpose of margin estimation (but include them for therobustness evaluation of MixedNUTS).",
  "B.2Adaptive Attacks for Nonlinearly Mixed Classifier Robustness Evaluation": "When proposing a novel adversarially robust model, reliably measuring its robustness with strong adversariesis always a top priority. Hence, in addition to designing MMAA for the goal of margin estimation, we devisetwo adaptive attack algorithms to evaluate the robustness of the MixedNUTS and its nonlinearly mixedmodel defense mechanism. Both algorithms are strengthened adaptive versions of AutoAttack. As is theoriginal AutoAttack, both algorithms are ensembles of four attack methods, including a black-box component.Hence, the reported accuracy numbers in this paper are lower bounds to the attacked accuracy associatedwith each of the components.",
  "B.2.1Transfer-Based Adaptive AutoAttack with Auxiliary Mixed Classifier": "Following the guidelines for constructing adaptive attacks (Tramr et al., 2020), our adversary maintains fullaccess to the end-to-end gradient information of the mixed classifier fmix(). Nonetheless, when temperaturescaling with T = 0 is applied to the accurate base classifier gstd() as discussed in .1, gTS(T )std() is nolonger differentiable. While this is an advantage in practice since the mixed classifier becomes harder to attack,we need to circumvent this obfuscated gradient issue in our evaluations to properly demonstrate white-boxrobustness. To this end, transfer attack comes to the rescue. We construct an auxiliary differentiable mixedclassifier fmix() by mixing gstd()s unmapped logits with hMrob(). We allow our attacks to query the gradient",
  "of fmix() to guide the gradient-based attack on fmix(). Since gstd() and gTS(T )std() always produce the samepredictions, the transferability between fmix() and fmix() should be high": "On the other hand, while hM(s,p,c)rob()s nonlinear logit transformation (4) is differentiable, it may also hindergradient flow in certain cases, especially when the logits fall into the relatively flat near-zero portion of theclamping function Clamp(). Hence, we also provide the raw logits of hrob() to our evaluation adversary forbetter gradient flow. To keep the adversary aware of the transformation M(s, p, c)(), we still include it in thegradient (i.e., M(s, p, c)() is only partially bypassed). The overall construction of the auxiliary differentiablemixed classifier fmix() is then",
  "fmix(x) = log(1 d) gstd() + drd hrob() + d(1 rd) hM(s,p,c)rob(),(8)": "where d is the mixing weight and rd adjusts the level of contribution of M(s, p, c)() to the gradient. Ourexperiments fix rd to 0.9 and calculate d using Algorithm 1 with no clamping function, s and p fixed to1, and c fixed to 0. The gradient-based components (APGD and FAB) of our adaptive AutoAttack useL( fmix(x)) as a surrogate for L(f M(s,p,c)mix(x)) where L is the adversarial loss function. The gradient-freeSquare attack component remains unchanged. Please refer to our source code for details on implementation. With the transfer-based gradient query in place, our adaptive AutoAttack does not suffer from gradientobfuscation, a phenomenon that leads to overestimated robustness. Specifically, we observe that the black-box Square component of our adaptive AutoAttack does not change the prediction of any images thatwhite-box components fail to attack, confirming the effectiveness of querying the transfer-based auxiliarydifferentiable mixed classifier for the gradient. If we set rd to 0 (i.e., do not bypass M(s, p, c)() for gradient),the AutoAttacked accuracy of the CIFAR-100 model reported in becomes 42.97% instead of 41.80%,and the black-box Square attack finds 12 vulnerable images. This comparison confirms that the proposedmodifications on AutoAttack strengthen its effectiveness against MixedNUTS and eliminate the gradient flowissue, making it a reliable robustness evaluator.",
  "fmix(x) + StopGradf M(s,p,c)mix(x) fmix(x),": "where StopGrad denotes the straight-through operation that passes the forward activation but stops thegradient (Bengio et al., 2013), for which a PyTorch realization is Tensor.detach(). The resulting mixedclassifier retains the output values of the MixedNUTS classifier f M(s,p,c)mix(x) while using the gradientcomputation graph of the differentiable auxiliary classifier fmix(x). In the literature, a similar technique isoften used to train neural networks with non-differentiable components, such as VQ-VAEs (Van Den Oordet al., 2017).",
  "This direct gradient bypass method is closely related to the transfer-based adaptive attack described inAppendix B.2.1, but has the following crucial differences:": "Compatibility with existing attack codebases. The transfer-based attack relies on the outputsfrom both f M(s,p,c)mix() and fmix(). Since most existing attack codebases, such as AutoAttack, areimplemented assuming that the neural network produces a single output, they need to be modified toaccept two predictions. In contrast, direct gradient bypass does not introduce or require multiple networkoutputs, and is therefore compatible with existing attack frameworks without modifications. Hence, oursubmission to RobustBench uses the direct gradient bypass method. Calculation of attack loss functions. From a mathematical perspective, the transfer-based attackuses the auxiliary differentiable mixture to evaluate the attack objective function. In contrast, the directgradient bypass method uses the original MixedNUTSs output for attack objective calculation, andthen uses the gradient computation graph of fmix() to perform back-propagation. Hence, the resultinggradient is slightly different between the two methods.",
  "CIFAR-1093.27%93.27%93.25%71.4%71.4%71.4%CIFAR-10075.22%75.22%75.22%43.0%42.9%43.3%ImageNet78.75%78.75%78.75%57.5%57.5%57.5%": "Our experiments show that when using direct gradient bypass, the original AutoAttack algorithm returns70.08%, 41.91%, and 58.62% for CIFAR-10, CIFAR-100, and ImageNet respectively with the MixedNUTSmodel used in . Compared with the transfer-based adaptive AutoAttack, which achieves 69.71%,41.80%, and 58.50%, AutoAttack with direct gradient bypass consistently achieves a lower success rate, butthe difference is very small. Hence, we use the transfer-based AutoAttack for , but note that bothmethods can evaluate MixedNUTS reliably.",
  "C.1Base Classifier and Mixing Details": "presents the sources and architectures of the base classifiers selected for our main experiments(, , , and ). The robust base classifiers are the state-of-the-art models listedon RobustBench as of submission, and the accurate base classifiers are popular high-performance modelspre-trained on large datasets. Note that since MixedNUTS only queries the predicted classes from gstd() andis agnostic of its other details, gstd() may be any classifier, including large-scale vision-language models thatcurrently see rapid development. presents the optimal s, p, c, and values used in MixedNUTSs nonlinear logit transformationreturned by Algorithm 1. When optimizing s, p, and c, Algorithm 1 performs a grid search, selecting from aprovided set of candidate values. In our experiments, we generate uniform linear intervals as the candidatevalues for the power coefficient p and the bias coefficient c, and use a log-scale interval for the scale coefficients. Each interval has eight numbers, with the minimum and maximum values for the intervals listed in . shows that MixedNUTSs nonlinear logit transformation M(s, p, c)() has negligible effects on baseclassifier accuracy, confirming that the improved accuracy-robustness balance is rooted in the improved baseclassifier confidence properties.",
  "C.2Model Inference Efficiency": "In this section, we compare the performance and inference efficiency of MixedNUTS with existing methods.As a training-free ensemble method, MixedNUTS naturally trades inference efficiency for training efficiency.Nonetheless, since MixedNUTS only requires two base models and does not add new neural networkcomponents, it is among the most efficient ensemble methods.Specifically, the computational cost ofMixedNUTS is the sum of the computation of its two base classifiers, as the mixing operation itself is trivialfrom a computational standpoint. In , the efficiency of MixedNUTS, evaluated in terms of parameter count and floating-point operations(FLOPs), is compared with some other state-of-the-art methods. Compared with the fellow mixed classifiermethod adaptive smoothing (Bai et al., 2024a), MixedNUTS is more efficient when the base classifiers arethe same, as is the case for CIFAR-100. This is because adaptive smoothing introduces an additional mixingnetwork, whereas MixedNUTS only introduces four additional parameters. On CIFAR-10, MixedNUTSuses a denser robust base classifier than adaptive smoothing, with similar number of parameters but higherGFLOPs (121.02 vs 77.56). MixedNUTSs FLOPs count is thus also higher than (Bai et al., 2024a).",
  "D.1Sensitivity to s, p, c Values": "In this section, we visualize the optimization landscape of the hyper-parameter optimization problem (7) interms of the sensitivity with respect to s, p, and c. To achieve so, we store the objective of (7) correspondingto each combination of s, p, c values in our grid search as a three-dimensional tensor (recall that the value of can be determined via the constraint). We then visualize the tensor by displaying each slice of it as a colormap. We use our CIFAR-100 model as an example, and present the result in .",
  "(Peng et al., 2023)5.01.14.0(Pang et al., 2022)5.01.14.0(Wang et al., 2023)5.01.12.71": "As shown in , while the optimization landscape is non-convex, it is relatively smooth and benign,with multiple combinations achieving similar, relatively low objective values. When the exponent parameterp is small, the other two parameters, s and c, have to be within a smaller range for optimal performance.When p is larger, a wide range of values for s and c can work. Nonetheless, an excessively large p maypotentially cause numerical instabilities and should be avoided if possible. For the same consideration, we donot recommend using the exponentiation function in the nonlinear logit transformation. For further illustration, we construct a mixed CIFAR-100 classifier with a simple GELU as the nonlinearlogit transformation (still using gTS(0)std() as the standard base classifier). The resulting clean/robust accuracyis 77.9%/40.4% on a 1000-example data subset. While this result is slightly better than the 77.6%/39.9%accuracy of the baseline mixed classifier without nonlinearity, it is noticeably worse than MixedNUTSs82.8%/41.6%. We can thus conclude that selecting a good combination of s, p, and c is crucial for achievingoptimal performance.",
  "D.2Selecting the Clamping Function": "This section performs an ablation study on the clamping function in the nonlinear logit transformationdefined in (4). Specifically, we compare using GELU or ReLU as Clamp() to bypassing Clamp() (i.e., use alinear function). Here, we select a CIFAR-10 ResNet-18 model (Na, 2020) and a CIFAR-100 WideResNet-70-16 model (Wang et al., 2023) as two examples of hrob() and compare the optimal objective returned byAlgorithm 1 using each of the clamping function settings. As shown in , while the optimal objectiveis similar for all three options, the returned hyper-parameters s, p, c, and is the most modest forGELU, which translates to the best numerical stability. In comparison, using a linear clamping functionrequires applying a power of 9.14 to the logits, whereas using the ReLU clamping function requires scaling",
  "D.3Effect of Optimization Dataset Size and Absence of Overfitting": "Since the MMAA step has the dominant computational time, reducing the number of images used inAlgorithm 1 can greatly accelerate it. Analyzing the effect of this data size also helps understand whetheroptimizing s, p, and c on validation images introduces overfitting. shows that on CIFAR-100, reducingthe number of images used in the optimization from 1000 to 300 (3 images per class) has minimal effect onthe resulting mixed classifier performance. Further reducing the optimized subset size to 100 still allows foran accuracy-robustness balance, but shifts the balance towards clean accuracy. To further demonstrate the absence of overfitting, reports that under the default setting of optimizings, p, c on 1000 images, the accuracy on these 1000 images is similar to that on the rest of the validationimages unseen during optimization. The CIFAR-10 and -100 models, in fact, perform slightly better onunseen images. The ImageNet models accuracy on unseen images is marginally lower than seen ones, likelydue to the scarcity of validation images per class (only 5 per class in total since ImageNet has 1000 classes)and the resulting performance variance across the validation set.",
  "D.4Temperature Scaling for gstd()": "This section verifies that scaling up the logits of gstd() improves the accuracy-robustness trade-off of themixed classifier. We select the pair of CIFAR-100 base classifiers used in . By jointly adjustingthe temperature of gstd() and the mixing weight , we can keep the clean accuracy of the mixed modelto approximately 84 percent and compare the APGD accuracy. In , we consider two temperatureconstants: 0.5 and 0. Note that as defined in .1, when the temperature is zero, the resulting predictionprobabilities gstd() is the one-hot vector associated with the predicted class. As demonstrated by theCIFAR-100 example in , when we fix the clean accuracy to 82.8%, using T = 0.5 and T = 0 produceshigher AutoAttacked accuracy than T = 1 (no scaling), with T = 0 producing the best accuracy-robustnessbalance.",
  "However, even if the gradient of hrob() is also unavailable, then s, p, c, and can be selected with one of thefollowing options:": "Black-box minimum-margin attack.Existing gradient-free black-box attacks, such as Square(Andriushchenko et al., 2020) and BPDA (Athalye et al., 2018a), can be modified into minimum-marginattack algorithms. As are gradient-based methods, these gradient-free algorithms are iterative, and theonly required modification is to record the margin at each iteration to keep track of the minimum margin. Transfer from another model. Since the robust base classifiers share the property of being moreconfident in correct examples than in incorrect ones (as shown in ), an optimal set of s, p, cvalues for one model likely also suits another model. So, one may opt to run MMAA on a robust classifierwhose gradient is available, and transfer the s, p, c values back to the black-box model. Educated guess. Since each component of our parameterization of the nonlinear logit transformationis intuitively motivated, a generic selection of s, p, c values should also perform better than mixinglinearly. In fact, when we devised this project, we used hand-selected s, p, c values for prototyping andidea verification, and later designed Algorithm 1 for a more principled selection. To empirically verify the feasibility of transferring hyper-parameters across robust base classifiers, we showthat the optimal hyper-parameters are similar across similar models. Consider the CIFAR-10 robust base",
  "D.6Selecting the Base Classifiers": "This section provides guidelines on how to select the accurate and robust base classifiers for the best mixedclassifier performance. For the accurate classifier, since MixedNUTS only considers its predicted class anddoes not depend on its confidence (recall that MixedNUTS uses gTS(0)std()), the classifier with the best-knownclean accuracy should be selected. Meanwhile, for the robust base classifier, since MixedNUTS relies onits margin properties, one should select a model that has high robust accuracy as well as benign margincharacteristics (i.e., is significantly more confident in correct predictions than incorrect ones). As shownin , most high-performance robust models share this benign property, and the correlation betweenrobust accuracy and margins is insignificant. Hence, state-of-the-art robust models are usually safe to use. That being said, consider the hypothetical scenario that between a pair of robust base classifiers, one hashigher robust accuracy and the other has more benign margin properties. Here, one should compare thepercentages of data for which the two models are robust with a certain non-zero margin. The model withhigher robust accuracy with margin should be used.",
  "D.7Behavior of Logit Normalization": "The LN operation on the model logits makes the margin agnostic to the overall scale of the logits. Considertwo example logit vectors in R3, namely (0.9, 1, 1.1) and (2, 1, 1.1). The first vector corresponds to thecase where the classifier prefers the third class but is relatively unconfident. The second vector reflects thescenario where the classifier is generally more confident, but the second and third classes compete witheach other. The LN operation will scale up the first vector and scale down the second. It is likely that thecompeting scenario is more common when the prediction is incorrect, and therefore the LN operation, whichcomparatively decreases the margin under the competing scenario, makes incorrect examples less confidentcompared with correct ones. As a result, the LN operation itself can slightly enlarge the margin differencebetween incorrect and correct examples. For ImageNet, instead of performing LN on the logits based on the mean and variance of all 1000 classes,we normalize using the statistics of the top 250 classes. The intuition of doing so is that the predictedprobabilities of bottom classes are extremely small and likely have negligible influence on model predictionand robustness. However, they considerably influence the mean and variance statistics of logits. Excludingthese least-related classes makes the LN operation less noisy.",
  "D.8.1When Assumption 4.1 is Slightly Violated": "If Assumption 4.1 is slightly violated, then there is a slight mismatch between the objective functions of (3)and (2) due to discarding the case of gTS(0)std() being incorrect while hM(s,p,c)rob() being correct on clean data.As a result, the reformulations in this section become slightly suboptimal. However, note that the constraintin (2), which enforces the level of robustness of the mixed classifier, is not compromised. Furthermore, asmentioned above, the amount of clean examples correctly classified by hM(s,p,c)rob() but not by gTS(0)std() isusually exceedingly rare, and hence the degree of suboptimality is extremely small.",
  "D.8.2When Assumption 4.2 is Slightly Violated": "Assumption 4.2 assumes that the nonlinear logit transformation applied to hrob() does not affect its predictedclass and hence inherits hrob()s accuracy. When Assumption 4.2 is violated, consider the following twocases: 1) the logit transformation M(s, p, c)() corrects mispredictions; 2) M(s, p, c)() contaminates correctpredictions. Consider the first scenario, i.e., hM(s,p,c)rob() is correct whereas hrob() is not. In this case, Theorem 4.3 (theonly theoretical result dependent on Assumption 4.2) still holds, and the mixed classifier can correctly classifyeven more clean examples than Theorem 4.3 suggests. Conversely, consider the second case, where hM(s,p,c)rob() is incorrect whereas hrob() is correct. In this case,Theorem 4.3 may not hold. However, this is the best one can expect. In this worst-case scenario, althoughthe nonlinear logit transformation improves hrob()s confidence property, it also harms hrob()s standaloneaccuracy, which in turn negatively affects the MixedNUTS model. Fortunately, this worst case is easilyavoidable in practice by checking hM(s,p,c)rob()s standalone clean accuracy. If hM(s,p,c)rob()s clean accuracydeteriorates, the search space for s, p, and c can be adjusted accordingly before re-running Algorithm 1.",
  "D.9Approximation Quality of (7)": "Algorithm 1 solves (7) as a surrogate of (6) for efficiency. One of the approximations of (7) is to use theminimum-margin perturbation against hLNrob() instead of that associated with hMrob(). While hMrob() and hLNrob()are expected to have similar standalone accuracy and robustness, their confidence properties are different,and therefore the minimum-margin perturbation associated with hMrob() can be different from that associatedwith hLNrob(), inducing a distribution mismatch. To analyze the influence of this mismatch on the effectivenessof Algorithm 1, we record the values of s, p, and c, compute the minimum-margin-AutoAttacked examplesof hM(s,p,c)rob() and re-run Algorithm 1 with the new examples. If the objective value calculated via thenew examples and s, p, c is close to the optimal objective returned from the original Algorithm 1, thenthe mismatch is small and benign and Algorithm 1 is capable of indirectly optimizing (6). We use the CIFAR-100 model from as an example to perform this analysis. The original optimalobjective returned by Algorithm 1 is 50.0%. The re-computed objective based on hM(s,p,c)rob()s minimum-margin perturbations, where s = .612, p = 3.57, c = 2.14, is 66.7%. While there is a gap between thetwo objective values and therefore the approximation-induced distribution mismatch exists, Algorithm 1 canstill effectively decrease the objective value of (6)."
}