{
  "Abstract": "Classifier-Free Guidance (CFG) enhances the quality and condition adherence of text-to-image diffusion models. It operates by combining the conditional and unconditional pre-dictions using a fixed weight. However, recent works vary the weights throughout the dif-fusion process, reporting superior results but without providing any rationale or analysis.By conducting comprehensive experiments, this paper provides insights into CFG weightschedulers. Our findings suggest that simple, monotonically increasing weight schedulersconsistently lead to improved performances, requiring merely a single line of code. In addi-tion, more complex parametrized schedulers can be optimized for further improvement, butdo not generalize across different models and tasks.",
  "Introduction": "Diffusion models have demonstrated prominent generative capabilities in various domains e.g. images (Hoet al., 2020), videos (Luo et al., 2023), acoustic signals (Kang et al., 2023b), or 3D avatars (Chen et al.,2023). Conditional generation with diffusion (e.g. text-conditioned image generation) has been explored innumerous works (Saharia et al., 2022; Ruiz et al., 2023; Balaji et al., 2022), and is achieved in its simplestform by adding an extra condition input to the model (Nichol & Dhariwal, 2021). To increase the influence ofthe condition on the generation process, Classifier Guidance (Dhariwal & Nichol, 2021) proposes to linearlycombine the gradients of a separately trained image classifier with those of a diffusion model. Alternatively,Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) simultaneously trains conditional and unconditionalmodels, and exploits a Bayesian implicit classifier to condition the generation without an external classifier. In both cases, a weighting parameter controls the importance of the generative and guidance terms andis directly applied at all timesteps. Varying is a trade-off between fidelity and condition reliance, as anincrease in condition reliance often results in a decline in both fidelity and diversity. In some recent literature,the concept of dynamic guidance instead of constant one has been mentioned: MUSE (Chang et al., 2023)observed that a linearly increasing guidance weight could enhance performance and potentially increasediversity. This approach has been adopted in subsequent works, such as in Stable Video Diffusion (Blattmannet al., 2023), and further mentioned in Gao et al. (2023) through an exhaustive search for a parameterizedcosine-based curve (pcs4) that performs very well on a specific pair of model and task. Intriguingly, despitethe recent appearance of this topic in the literature, none of the referenced studies has conducted anyempirical experiments or analyses to substantiate the use of a guidance weight scheduler. For instance, theconcept of linear guidance is briefly mentioned in MUSE (Chang et al., 2023), around Eq. 1: \"we reducethe hit to diversity by linearly increasing the guidance scale t [...] allowing early tokens to be sampled morefreely\". Similarly, the pcs4 approach Gao et al. (2023) is only briefly discussed in the appendix, withoutany detailed ablation or comparison to static guidance baselines. Thus, to the best of our knowledge, acomprehensive guide to dynamic guidance weight schedulers does not exist at the moment.",
  "Published in Transactions on Machine Learning Research (12/2024)": "For the evaluation, each participant was presented with a total of 10 image sets. Each set comprised 9images. Within each set, three pairwise comparisons were made: linear vs. baseline, and cosine vs. baseline.Throughout the study, two distinct image sets (20 images for each method) were utilized. We carried outtwo tests for results generated with stable diffusion v1.5 and each image are generated to make sure thattheir CLIP-Score are similar.",
  "full body, a cat dressed as a Viking, with weapons in his paws, on a Viking ship, battle coloring, glow hyper-detail, hyper-realism, cinematic, trending on artstation": ": Classifier-Free Guidance introduces a trade-off between detailed but fuzzy images (low guidance,top) and sharp but simplistic images (high guidance, middle). Using a guidance scheduler (bottom) is simpleyet very effective in improving this trade-off. Our findings are the following: First, we show that too much guidance at the beginning of the denoisingprocess is harmful and that monotonically increasing guidance schedulers are performing the best. Second,we show that a simple linearly increasing scheduler always improves the results over the basic static guidance,while costing no additional computational cost, requiring no additional tuning, and being extremely simpleto implement. Third, a parameterized scheduler, like clamping a linear scheduler below a carefully chosenthreshold (), can significantly further improve the results, but the choice of the optimal parameterdoes not generalize across models and tasks and has thus to be carefully tuned for the target model andtask. All our findings are guides to CFG schedulers that will benefit and improve all works relying on CFG.",
  ": Examples of all heuristics on SDXL. Increasing ones (linear and cosine) enhance fidelity,textual adherence and diversity": "Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), which have recorded significant progressin various generative tasks (Brock et al., 2018; Kang et al., 2023a; Dufour et al., 2022; Donahue et al.,2018). Recently, diffusion models have demonstrated a remarkable capacity to produce high-quality anddiverse samples. They have achieved state-of-the-art results in several generation tasks, notably in imagesynthesis (Song et al., 2020; Ho et al., 2020), text-to-image applications (Dhariwal & Nichol, 2021; Rombachet al., 2022; Podell et al., 2023; Pernias et al., 2023) and text-to-motion (Chen et al., 2023). Guidance in Diffusion and Text-to-Image.Making generative models controllable and capable ofproducing user-aligned outputs requires making the generation conditional on a given input. Conditioneddiffusion models have been vastly explored (Saharia et al., 2022; Ruiz et al., 2023; Balaji et al., 2022). Thecondition is achieved in its simplest form by adding extra input, typically with residual connections (Nichol& Dhariwal, 2021). To reinforce the models fidelity to specific conditions, two main approaches prevail:Classifier Guidance (CG) (Dhariwal & Nichol, 2021), which involves training an image classifier externally,and Classifier-Free Guidance (CFG) (Ho & Salimans, 2021), that relies on an implicit classifier through jointtraining of conditional and unconditional models (using dropout on the condition). Particularly, CFG has catalyzed advancements in text-conditional generation, a domain where traininga noisy text classifier is less convenient and performs worse.This approach breathed new life into thetext-to-image application, initially proposed in several works such as (Reed et al., 2016; Mansimov et al.,2015). Numerous works (Rombach et al., 2022; Ramesh et al., 2022; Nichol et al., 2022; Avrahami et al.,2022) have leveraged text-to-image generation with CFG diffusion models conditioned on text encoderslike CLIP (Radford et al., 2021), showcasing significant progress in the field, e.g. the Latent DiffusionModel (Dhariwal & Nichol, 2021) and Stable Diffusion (Rombach et al., 2022) employ VAE latent spacediffusion with CFG with CLIP encoder.SDXL, an enhanced version, leverages a larger model and anadditional text encoder for high-resolution synthesis. Improvements on Diffusion Guidance. Noticed that in Classifier Guidance (CG), the classifiers gradi-ent tends to vanish towards the early and final stages due to overconfidence, Zheng et al. (2022) leverages theentropy of the output distribution as an indication of vanishing gradient and rescales the gradient accord-ingly. To prevent such adversarial behaviours, Dinh et al. (2023b) explored using multiple class conditions,guiding the image generation from a noise state towards an average of image classes before focusing on the",
  "desired class with an empirical scheduler. Subsequently, Dinh et al. (2023a) identified and quantified gradientconflicts emerging from the guidance and suggested gradient projection as a solution": "In Classifier-Free Guidance (CFG), Li et al. (2023) used CFG to recover a zero-shot classifier by samplingacross timesteps and averaging the guidance magnitude for different labels, with the lowest magnitude corre-sponding to the most probable label. However, they observed a discrepancy in performance across timestepswith early stages yielding lower accuracy than intermediate ones. Chang et al. (2023) observed that a linearincrease in guidance scale enhances diversity. Similarly, Gao et al. (2023) developed a parameterized power-cosine-like curve, optimizing a specific parameter for their dataset and method. However, these linear andpower-cosine schedulers have been suggested as improvements over constant static guidance without rigorousanalysis or testing. To this end, we provide an extensive study of dynamic guidance for both heuristic andparametrized schedulers across several tasks. Concurrently, Kynknniemi et al. (2024) proposes empiricallyremoving the initial and final timesteps of the classifier-free guidance (CFG) for improved generation. Simi-larly, Zhang et al. (2024); Castillo et al. (2023) observes that the conditional and unconditional responses ofsome models may converge to similar behaviours at certain timesteps, particularly towards the ending stage.",
  "(t)x0+": "1(t), where (t) is a noise scheduler of the timestep t and applied to astandard Gaussian noise N(0, 1). In practice, Ho et al. (2020) find that predicting the noise instead ofx0 yielded better performance leading to the training loss: Lsimple=Ex0pdata,N(0,1),tU [(xt) ]based on the target image distribution pdata with U uniform distributions. Once the network is trained, we can sample from pdata by setting xT = N(0, 1) (with (T)=0), andgradually denoising to reach the data point x0pdata with different types of samplers e.g., DDPM (Ho et al.,2020) or DDIM (Song et al., 2020). To leverage a condition c and instead sample from p(xt|c), Dhariwal &Nichol (2021) propose Classifier Guidance (CG) that uses a pretrained classifier p(c|xt), forming:",
  "Towards dynamic guidance: Should guidance be constant?": "Our initial experiments show that removing guidance at certain timesteps can improve performance. Thisis in line with concurrent work (Kynknniemi et al., 2024). To further investigate this, we conducted anegative perturbation analysis experiment to determine the impact of the guidance across all timesteps. Negative Perturbation Analysis. This analysis is on the CIFAR-10 dataset: a 60,000 images datasetwith a 32 32 resolution, distributed across 10 classes. We choose the original DDPM method (Ho et al.,2020) denoising on pixel space as the backbone and class-conditioning guidance. To investigate the importance of guidance across different timestep intervals, we first employ static guidanceof = 1.15, then independently set the guidance to zero across different 50-timestep intervals (20 intervals intotal spanning all timesteps), and compute the FID for each of these piece-wise zeroed guidance schedulersof 50,000 generated images. If we mathematically model the removal method, it can approximate a family",
  "of parameterized gate/inverse gate functions with two parameters defining the starting point s and size ofthe kernel d: g(t) = 1 (H(t s) H(t (s + d))), where H is Heaviside step function": "The results are illustrated in b. For example, the second data point on the left of the curve representsthe FID performance when guidance is removed only in the interval t = while maintaining static atothers. We observe multiple phenomena from the results: (1) non-constant guidance curve can outperformstatic guidance in terms of FID; (2) zeroing the guidance at earlier stages improves the FID performance;(3) zeroing the guidance at later stages significantly degrades it. However, this removal scheme is not practical for real usage: (i) grid searching two parameters requiresgenerating a prohibitively costly number of images; (ii) as shown in , parameterized methods oftenfail to generalize; (iii) further investigation, detailed in Appendix Section B, demonstrates that instead ofcompletely removing CFG from some timesteps, keeping it with lower values increases the performance.",
  "(iii) Avg. Conflict Magnitude": ": Visualization of Conflicted Terms from SD1.5 Rombach et al. (2022) shows that static guidancepresents conflicts, while a guidance scheduler reduces the conflict between generation and guidance terms. Conflicted terms. Our assumption is that the guidance and generation terms (see Eq. 3) may be adversarialduring inference. Following (Dinh et al., 2023a), quantifies the conflict by measuring (a) the ratio ofnegative cosine similarity; (b) average cosine similarity (i.e. directional conflict, -1 and 1 for maximum andminimum conflicts); and (c) conflict magnitude (Dinh et al., 2023a), defined as: (1, 2) = 2|1|2|2|2 |1|22+|2|22 where is each term at each timestep, with resulting 1 and 0 indicates zero and maximum conflict. We evaluate1000 generation from COCO prompts (Lin et al., 2014) with SD1.5 and show in (i) that SD (orange)exhibits 50% conflict ratio along the generation with high magnitude conflict (see (iii)(right) withcurves closer to zero). When the guidance is lowered at the beginning (e.g. linearly increasing as shown in), less conflict for both magnitude and directional metrics is shown in all subfigures blue curves. Dynamic guidance. Having observed that removing the guidance at certain timesteps improves the per-formance over using a static weight for CFG like in Ho & Salimans (2021); Dhariwal & Nichol (2021)and reducing the guidance at beginning linearly can reduce the conflict, we ask the question of whether wecan replace static guidance with other options. Therefore, we investigate dynamic guidance scheduler thatevolves throughout the generation process, which is also in line with some empirical schemes mentioned inrecent literature (Blattmann et al., 2023; Chang et al., 2023; Donahue et al., 2018). In that case, the CFGEquation 3 is rewritten as follows:",
  "(b) Negative Perturbation": ": Preliminary Analysis on CIFAR-10 (a) Various heuristic curves with their FID vs. ISperformances. (b) Negative perturbation by setting the guidance scale to 0 across distinct intervals whilepreserving static guidance to the rest. By eliminating the weight at the initial stage (e.g., T = 800), thelowered FID shows an enhancement, whereas removing guidance at higher timesteps leads to worse FID.",
  "linear(t) else,invlinear(t) else": "To allow for a direct comparison between the effect of these schedulers and the static guidance , wenormalize each scheduler by the area under the curve. This ensures that the same amount of total guidanceis applied over the entire denoising process, and allows users to rescale the scheduler to obtain a behaviorsimilar to that of increasing in static guidance. More formally, this corresponds to the following constraint: T0 (t)dt = T. For example, this normalization leads to the corresponding normalized linear scheduler(t) = 2(1 t/T). We show in a (left) the different normalized curves of the 6 schedulers.",
  "Class-conditional image generation with heuristic schedulers": "Heuristic Schedulers Analysis. We first study the 6 previously defined heuristic schedulers (t) on theCIFAR-10-DDPM setting for class-conditional synthesis same as in the . To assess the performance,we use the Frechet Inception Distance (FID) and Inception Score (IS) metrics, over 50, 000 inference from50-step DDIM (Song et al., 2020). In this experiment, we evaluate the impact of a range of different guidancetotal weight: [1.1, 1.15, 1.2, 1.25, 1.3, 1.35], to study its influence over the image quality vs class adherencetrade-off. We show the results in a, right panel and observe that both increasing schedulers (linearand cosine) significantly improve over the static baseline, whereas decreasing schedulers (invlinear and sine)are significantly worse than the static. The V-shape and -shape schedulers perform respectively better andworse than the static baseline, but only marginally. Preliminary Conclusion. Combining with the observation from that removing the beginningstage improves the performance, they point to the same conclusion: monotonically increasing guidanceschedulers achieve improved performances, revealing that the static CFG primarily may overshoot theguidance in the initial stages. In the remainder of this work, we only consider monotonically increasingschedulers, as we consider these findings sufficient to avoid examining all other schedulers on other tasks.(more details in Appx. and shows a qualitative results in SDXL)",
  "Text-to-image generation with heuristic schedulers": "We study the linear and the cosine scheduler on text-to-image generation.The results for all proposedheuristics are in Appx. Tables 12 and 14, where we observe a similar trend as before: heuristic functionswith increasing shape report the largest gains on both SD1.5 and SDXL. Dataset and Metrics. We use text-to-image models pre-trained on LAION (Schuhmann et al., 2022), whichcontains 5B high-quality images with paired textual descriptions. For evaluation, we use the COCO (Linet al., 2014) val set with 30, 000 text-image paired data. We use three metrics: (i) Frchet inception distance (FID) for the fidelity of generated images; (ii) CLIP-Score (CS) (Radford et al., 2021) to assess the alignments between the images and their corresponding textprompts; (iii) Diversity (Div) to measure the models capacity to yield varied content. For this, we computethe standard deviation of image embeddings via Dino-v2 (Oquab et al., 2023) from multiple generations ofthe same prompt (more details for Diversity in Appendix).We compute FID and CS for a sample set of 10, 000 images against the COCO dataset in a zero-shotfashion (Rombach et al., 2022; Saharia et al., 2022). For diversity, we resort to two text description subsetsfrom COCO: 1000 longest captions and shortest captions each (-L and -S in a) to represent varyingdescriptiveness levels; longer captions provide more specific conditions than shorter ones, presumably leadingto less diversity. We produce 10 images for each prompt using varied sampling noise. Model. We experiment with two models: (1) Stable Diffusion (SD) (Rombach et al., 2022), which usesthe CLIP (Radford et al., 2021) text encoder to transform text inputs to embeddings. We use the publiccheckpoint of SD v1.5 1 and employ DDIM sampler with 50 steps. (2) SDXL (Podell et al., 2023), which is alarger, advanced version of SD (Rombach et al., 2022), generating images with resolutions up to 1024 pixels.It leverages LDM (Dhariwal & Nichol, 2021) with larger U-Net architectures, an additional text-encoder(OpenCLIP ViT-bigG), and other conditioning enhancements. We use the SDXL-base-1.02 (SDXL) versionwithout refiner, sampling with DPM-Solver++ (Lu et al., 2022b) of 25 steps.",
  "Results. We show the FID vs. CS curves in a, 7c for SD and SDXL respectively (more tables inAppx. Section G). We expect an optimal balance of a high CS and a low FID (i.e., the right-down corner)": "Analysis on SD (a). For FID vs CS, the baseline (Rombach et al., 2022) yields inferior resultscompared to the linear and cosine heuristics with linear recording lower FID. The baseline regresses FIDfast when CS is high, but generates the best FID when CS is low, i.e., low condition level. This, however,is usually not used for real applications, e.g., the recommended value is 7.5 for SD1.5, highlighted bythe dotted line in a with the black solid arrow representing the gain of heuristic schedulers on FIDand CS respectively. For Div vs CS, heuristic schedulers outperform the baseline (Rombach et al., 2022) onboth short (S) and long (L) captions at different guidance scales. Also, cosine shows superiority across themajority of the CS range. Overall, heuristic schedulers achieve improved performances in FID and Diversity,recording 2.71(17%) gain on FID and 0.004(16%) gain (of max CS-min CS of baseline) on CS over =7.5default guidance in SD. Note, this gain is achieved without hyperparameter tuning or retraining. Analysis on SDXL (c). In FID, both the linear and cosine schedulers achieve better FID-CSthan the baseline (Podell et al., 2023). In Diversity, linear is slightly lower than cosine, and they are bothbetter than static baseline. Additionally, unlike the baseline (blue curves) where higher guidance typicallyresults in compromised FID, heuristic schedulers counter this. User study. We present users with a pair of mosaics of 9 generated images and ask them to vote for thebest in terms of realism, diversity and text-image alignment. Each pair compares static baseline generationsagainst cosine and linear schedulers. b reports the results. We observe that over 60% of users considerscheduler-generated images more realistic and better aligned with the text prompt, while approximately",
  "(d) CIN-256:FID-IS": ": Class-conditioned and text-to-image generation results of monotonically-increasingheuristic schedulers (linear and cosine). (a) FID and Div vs. CS for SD1.5 (Rombach et al., 2022).We highlight the gain of FID and CS compared with the default =7.5 with black arrows, diversity on theright shows that the heuristic guidance performs better than static baseline guidance; (b) our user studyalso reveals that images generated with schedulers are consistently preferred than the baseline in realism,diversity and text alignment; (c) results for SDXL (Podell et al., 2023) on FID and Div vs. CS withsimilar setup to (a); (d) CIN-256 LDM (Dhariwal & Nichol, 2021) are assessed with FID vs. IS. Heuristicschedulers outperform the baseline static guidance on fidelity and diversity across multiple models. : Failure cases of parameter-free and parameterized approaches: monotonically increasing guidancemay mute the guidance at the beginning (especially when overall guidance is low), causing structural errors;and incorrectly chosen parameters can lead to fuzzy details and low saturation problems.",
  "% find guidance schedulers results more diverse. This corroborates our hypothesis that static weighting isperceptually inferior to dynamic weighting. More details in Appx": "Qualitative results. depicts the fidelity of various sets of text-to-image generations from SD andSDXL. Heuristic schedulers (linear and cosine) enhance the image fidelity: better details in petals and leavesof the flower images, as well as the texture of bird features. In the arches example, we observe more naturalcolour shading as well as more detailed figurines with reflective effects. showcases the diversity ofoutputs in terms of composition, color palette, art style and image quality by refining shades and enrichingtextures. Notably, the teddy bear shows various compositions and better-coloured results than the baseline,",
  "Findings with heuristic schedulers": "In summary, we make the following observations: monotonically increasing heuristic schedulers (e.g., linearand cosine) (a) improve generation performances (IS/CS vs. FID) over static baseline on different models;(b) improve image fidelity (texture, details), diversity (composition, style) and quality (lighting, gestures).We note that this gain is achieved without hyperparameter tuning, retraining or extra computational cost. Failure cases. For the failure cases involving monotonically increasing guidance, we observe that under-shooting the guidance scale during the initial stages can compromise the structural integrity of the generatedoutputs. This often results in anatomical and geometric errors, such as the appearance of a third leg inhumans, a fifth leg in quadruped animals, or incorrect spatial relationships, as illustrated in .",
  "wt = max(c, wt)(clamp)(6)": "In our work, we use clamp-linear but this family can be extended to other schedulers (more in sup. mat.).Our motivation lies in our observation that excessive muting of guidance weights at the initial stages cancompromise the structural integrity of prominent features. This contributes to bad FID at lower values of in a, suggesting a trade-off between model guidance and image quality. However, reducing guidance",
  "(b) CIN-256-LDM": ": Class-conditioned generation results of parameterized clamp-linear and pcs on (a) CIFAR-10-DDPM and (b) CIN-256-LDM. Optimising parameters improves performances but these parameters donot generalize across models and datasets. intensity early in the diffusion process is also the origin of enhanced performances, as shown in .This family represents a trade-off between diversity and fidelity while giving users precise control.",
  "(f) SDXL: pcs": ": Text-to-image performance for two parameterized schedulers: clamp-linear and pcs.For clamp-linear, (a) shows the guidance curves for different parameters and (b,c) displays the FID vs. CSfor SD1.5 and SDXL, respectively. For pcs, (d) shows the guidance curves and (e,f) depicts the FID vs. CS.Optimal parameters for either clamp or pcs outperform the static baseline for both SD1.5 and SDXL.",
  "Class-conditional image generation with parametrized schedulers": "We experiment with two parameterized schedulers: clamp-linear and pcs on CIFAR10-DDPM (a)and ImageNet(CIN)256-LDM (b). We observe that, for both families, tuning parameters improvesresults over baseline and heuristic schedulers. The optimal parameters are c=1.01 for clamp-linear and s=4for pcs on CIFAR10-DDPM, vs c=1.1 for clamp-linear and s=2 for pcs on CIN-256. Overall, parameterizedschedulers improve performances; however, the optimal parameters do not apply across datasets and models.",
  "We experiment with two parameterized schedulers: clamp-linear (clamp-cosine in sup. mat.) and pcs, withtheir guidance curves in Figures 11a,11d, respectively": "For SD1.5 (Rombach et al., 2022), the FID vs. CS results are depicted in Figures 11b and 11e. The pcs familystruggles to achieve low FID, except when s = 1. Conversely, the clamp family exhibits optimal performancearound c=2, achieving the best FID and CLIP-score balance while outperforming all pcs values. For SDXL (Podell et al., 2023), the FID vs. CS results are depicted in Figures 11c and 11f. The pcs showsthe best performance at s = 0.1. Clamp-linear achieves optimum at c = 4 (FID 18.2), largely improving FIDacross the entire CS range compared to the baseline (FID 24.9, about 30% gain) and the linear scheduler. The optimal parameters of clamp-linear (resp. pcs) are not the same for both models, i.e. c=2 for SD1.5and c=4 for SDXL (resp. s=1 and s=0.1 for pcs). This reveals the lack of generalizability of this family. Qualitative results.The results of further underscore the significance of choosing the rightclamping parameter. This choice markedly enhances generation performance, as evidenced by improvedfidelity (dog and squirrel image), textual comprehension (Fries Big Ben), and details (sunglasses). compares two parameterized families: (i) clamp and pcs (Gao et al., 2023), where the clampperforms best at c = 4 and the pcs at s = 1. We observe that the clamp-linear c = 4 demonstrates betterdetails (e.g., mug, alien), realism (e.g., car, storm), and more textured backgrounds (e.g., mug, car). Althoughs = 4 for pcs leads to the best results for class-conditioned generation, we see that the pcs in text-to-imagetask tends to over-simplify content, produce fuzzy images (e.g., mug) and deconstructed composition. Thishighlights our argument that optimal parameters do not generalize across datasets or tasks.",
  "Findings with parametrized schedulers": "Our observations are: (a) tuning the parametrized functions improves the performance for both generationtasks, (b) tuning clamps seems easier than pcs family, as its performance shows fewer variations, and (c) theoptimal parameters for one method does not generalize across different settings. Thus, specialized tuning isrequired for each model and task, leading to extensive grid searches and increased computational load. Failure cases. The main risk with parameterized functions arises from ill-chosen parameters. As shownin Figures 8, 12 20, poorly chosen parameters, particularly for pcs family, often lead to overshooting at thelater stages, resulting in fuzzy detail, broken structure and unnatural colour in the generated images.",
  "Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images.In IEEE Conf. Comput. Vis. Pattern Recog., 2022": "Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, TimoAila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble ofexpert denoisers. arXiv preprint arXiv:2211.01324, 2022. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent videodiffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023.",
  "Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural imagesynthesis. In Int. Conf. Learn. Represent., 2018": "Angela Castillo, Jonas Kohler, Juan C Prez, Juan Pablo Prez, Albert Pumarola, Bernard Ghanem, PabloArbelez, and Ali Thabet. Adaptive guidance: Training-free acceleration of conditional diffusion models.arXiv preprint arXiv:2312.12487, 2023. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jos Lezama, Lu Jiang, Ming-Hsuan Yang, KevinMurphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via maskedgenerative transformers. In Int. Conf. on Machine Learning, 2023.",
  "Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Int.Conf. on Machine Learning, 2021": "Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew,Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In Int. Conf. on Machine Learning. PMLR, 2022. Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023.",
  "Pablo Pernias, Dominic Rampas, and Marc Aubreville. Wuerstchen: Efficient pretraining of text-to-imagemodels. arXiv preprint arXiv:2306.00637, 2023": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXivpreprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In Int. Conf. on Machine Learning. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Pranav Shyam, Pamela Mishkin, Bob McGrew, andIlya Sutskever.Hierarchical text-conditional image generation with clip latents.arXiv preprintarXiv:2204.06125, 2022.",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conf. Comput.Vis. Pattern Recog., 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Adv. Neural Inform. Process. Syst., 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. Adv. Neural Inform. Process. Syst., 2022.",
  "Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Int. Conf. Learn.Represent., 2020": "Wentian Zhang, Haozhe Liu, Jinheng Xie, Francesco Faccio, Mike Zheng Shou, and Jrgen Schmidhu-ber.Cross-attention makes inference cumbersome in text-to-image diffusion models.arXiv preprintarXiv:2404.02747, 2024. Guangcong Zheng, Shengming Li, Hui Wang, Taiping Yao, Yang Chen, Shouhong Ding, and Xi Li. Entropy-driven sampling and training scheme for conditional diffusion generation. In Eur. Conf. on Comput. Vis.Springer, 2022. In this appendix, we provide additional content covering: (A) an extra experiment about the task of image-to-image translation; (B) an ablation study to demonstrate the necessity of CFG at all time intervals;(C) a toy example to explain the mechanism and rationale of the dynamic weighted scheduler; (D) anadditional comparison of parameterized function-based dynamic schedulers; (E) more qualitative results;(F) ablation experiments on different aspects of dynamic weighting schedulers; (G) a list of tables of allresults demonstrated; (H) detailed design of user study. Following is the table of contents",
  "AImage-to-image Translation Task": "In addition to the image generation task, we also evaluated the image-to-image translation task to demon-strate the generalization capabilities of the dynamic guidance scheduler across multiple conditioning scenar-ios. The experimental setup closely follows the one outlined in the main manuscript (), using theSD1.5 backbone model (Rombach et al., 2022) and images and their correspondent prompts from COCOdataset (Lin et al., 2014) to achieve image-to-image translation task. As shown in a, the lineardynamic guidance scheduler significantly improves the FID vs. CLIP-Score trade-off ( 2 FID at the sameCLIP-Score) in image-to-image translation tasks. However, the optimal parameter for the parameterizedscheduler was found to involve clamping at guidance level c = 0, which differs from the optimal parametersidentified in the generation task (c = 2). This further supports our primary claim that a generalized param-eterized scheduler does not exist across different tasks. The qualitative results in b also showedbetter structure (car), details (bird background) and prompt understanding (graffiti in the bin).",
  "BThe necessity of CFG at all time interval": "Recent concurrent work Kynknniemi et al. (2024); Zhang et al. (2024); Castillo et al. (2023) has suggestedthat partially removing the classifier-free guidance (CFG) (beginning or ending) could enhance generationperformance or achieve the acceleration with minimal performance influence. For instance, Kynknniemiet al. (2024) proposes removing the initial and final timesteps of CFG, retaining only a middle interval forthe guidance process. In this section, we conduct two ablation studies on SD1.5 to confirm that the CFGcan be required across all time intervals.",
  "remove-linear (z=3)": ": Comparison between guidance removal (labelled as z) and clamping (labelled as c)on SD1.5. We see that CFG removal scheme shows improvement compared to the static guidance baseline(black) but is worse than the linear guidance scheduler (blue solid) and clamping schemes (red dotted). The necessity of CFG at the beginning stageAs demonstrated in b, we explore the impactof negative perturbation and ablation of all heuristic functions in . Generally, employing a lowerguidance level in the initial stages can enhance performance compared to static guidance. To analyse theeffectiveness of guidance removal (setting to zero), static guidance, the linear scheduler, and the clampingscheme, we conducted experiments on SD1.5. Guidance is removed at the same timestep as the clampingtransition point; rather than clamping guidance to a hyperparameter constant, we reduce it completely tozero ( left two panels). The results, depicted in right panel, show that while guidanceremoval at the beginning stage (red dotted curve) indeed improves performance compared to the staticbaseline (black solid curve), both the linear scheduler (blue solid curve) and clamping schemes (green dottedlines) achieve better balances of FID vs. CLIP-Score (CS). The necessity of CFG at the ending stageZhang et al. (2024); Castillo et al. (2023) suggest thatremoving the final stage of guidance could accelerate generation by directly replacing the CFG with condi-tional or unconditional outputs. However, as shown in b(b), our analysis indicates that removingthis stage can reduce performance for specific tasks. Despite this, the possibility of safely removing theending stage guidance does not contradict our argument that enhancing the end could improve per-formance. To further confirm this, we conducted an ablation experiment comparing the effects of removingversus boosting the final guidance intervals. In this experiment, 10%, 20%, and 30% of the ending guidancewere either removed or increased by a factor of 1.5. The results, presented in , reveal: (i) removingor boosting the ending guidance has a marginal impact on the CLIP-Score; (ii) elimination of guidance canlead to a regression in performance; and (iii) boosting the guidance can significantly enhance FID, with gainsof 0.54 and 0.8 in FID when boosting the final 30% of guidance by 1.5. In conclusion, based on the results from two previous ablation studies, we confirm that an adequatelevel of guidance is necessary at all intervals of the generation process. While removing parts ofthe guidance can accelerate the process, it results in underperformance when compared to our analyzedheuristic monotonically increasing guidance scheduler, such as linear, and also when compared to well-tunedparameterized functions, such as the clamping method.",
  "boost (1.5) final 30%0.277216.240.279018.23": ": Two-Gaussians Example. We employ DDPM with CFG to fit two Gaussian distributions, abright one (red) and a darker one (blue). The middle panel showcases samples of generation trajectories atdifferent guidance scales , using PCA visualization. Increasing guidance scale raises two issues: repeatedtrajectory: when =50 the generation diverges from its expected direction before converging again, andshaky motion: when =100 some trajectories wander aimlessly. one sampled with low values of intensity (dark noisy images in the bottom-left of ), and the otherwith high-intensities (bright noisy images). The top-left part in shows the PCA (Kambhatla &Leen, 1997)-visualised distribution of the two sets, and the bottom-left part shows some ground-truth images.To fit these two labelled distributions, we employ DDPM (Ho et al., 2020) with CFG (Ho & Salimans, 2021)conditioned on intensity labels. Upon completion of the training, we can adjust the guidance scale to balance between the sample fidelityand condition adherence, illustrated in the right part of . The first row depicts the variations ingenerated distributions on different (from 0 to 100), visualized by the same PCA parameters. The secondrow shows the entire diffusion trajectory for sampled data points (same seeds across different ): progressingfrom a random sample (i.e., standard Gaussian) when t = T to the generated data (blue or red in )when t = 0. Emerging issues and explainable factors.As increases, the two generated distributions diverge dueto guidance term in Eq. 7 shifting the generation towards different labels at a fidelity cost (see first row).",
  "pcs (s=4)": ": Class-conditioned image generation results of two parameterized families (clamp-linear, clamp-cosine and pcs) on CIFAR-10 and CIN-256.Optimising parameters of guidanceresults in performance gains, however, these parameters do not generalize across models and datasets. These two issues can be attributed to two factors: (1) incorrect classification prediction, and (2) the conflictsbetween guidance and generation terms in Eq. 7.For the former, optimal guidance requires a flawlessclassifier, whether explicit for CG or implicit for CFG. In reality, discerning between two noisy data ischallenging and incorrect classification may steer the generation in the wrong direction, generating shakytrajectories. A similar observation is reported in Zheng et al. (2022); Dinh et al. (2023b) for CG and in Liet al. (2023) for CFG. For the latter, due to the strong incentive of the classifier to increase the distance withrespect to the other classes, trajectories often show a U-turn before gravitating to convergence (repeatedtrajectory in ). We argue that this anomaly is due to the conflict between guidance and generationterms in Eq. 7. In conclusion, along the generation, the guidance can steer suboptimally (especially when t T), and evenimpede generation. We argue that these erratic behaviours contribute to the performance dichotomybetween fidelity and condition adherence (Ho & Salimans, 2021; Dhariwal & Nichol, 2021).",
  "(c) pcs Family": ": Text-to-image generation FID and diversity of all two parameterized families (clampwith clamp-linear, clamp-cosine and pcs) on SD1.5 (left to right): (a) parameterized schedulercurves; (b) FID vs. CS of SD1.5 and (c) FID vs. Div. of SD1.5. We show that in terms of diversity,the clamp family still achieves more diverse results than the baseline, though it reduces along the clampingparameter, as the beginning stage of the diffusion is muted. The conclusion of these parts is as follows: (i) optimising both groups of parameterized function helpsimprove the performance of FID-CS; (ii) the optimal parameters for different models are very different andfail to generalize across models and datasets.",
  "D.2Parameterized Comparison on Text-to-image Generation": "We then show the FID vs. CS and Diversity vs. CS performance of the parameterized method in .The conclusion is coherent with the main paper: all parameterized functions can enhance performance onboth FID and diversity, provided that the parameters are well-selected. Moreover, for the clamp family,it appears that the clamp parameter also adjusts the degree of diversity of the generated images; loweringthe clamp parameter increases the diversity. We recommend that users tune this parameter according tothe specific model and task. For SDXL, the clamp-cosine is shown in , and also reaches a similarconclusion.",
  "EQualitative Results": "More Results of Parameterized Functions on SDXLIn , we show more examples of differentparameterized functions. It appears that carefully selecting the parameter (c = 4), especially for the clamp-linear method, achieves improvement in image quality in terms of composition (e.g., template), detail (e.g.,cat), and realism (e.g., dog statue). However, for SDXL, this method shows only marginal improvementswith the pcs family, which tends to produce images with incorrect structures and compositions, leading tofuzzy images.",
  ": Qualitative comparison clamp vs. pcs family, we see clearly that clamping at c = 4 gives the bestvisual qualitative results": "Stable Diffusion v1.5. shows qualitative results of using increasing shaped methods: linear,cosine compared against the baseline.It shows clearly that the increasingly shaped heuristic guidancegenerates more diversity and the baseline suffers from a collapsing problem, i.e., different sampling of thesame prompt seems only to generate similar results. In some figures, e.g., with an example of",
  "FAblation on Robustness and Generalization": "Different DDIM steps.DDIM sampler allows for accelerated sampling (e.g., 50 steps as opposed to1000) with only a marginal compromise in generation performance. In this ablation study, we evaluate theeffectiveness of our dynamic weighting schedulers across different sampling steps. We use the CIN256-LDMcodebase, with the same configuration as our prior experiments of class-conditioned generation. We conducttests with 50, 100, and 200 steps, for baseline and two heuristics (linear and cosine), all operating at theiroptimal guidance scale in Tab 8. The results, FID vs. IS for each sampling step, are presented in Tab. 2.We observe that the performance of dynamic weighting schedulers remains stable across different timesteps. Different Solvers.To validate the generalizability of our proposed method beyond the DDIM (Songet al., 2020) sampler used in the experiment Section, we further evaluated its performance using the moreadvanced DPM-Solver (Lu et al., 2022a) sampler (3rd order). This sampler is capable of facilitating diffusiongeneration with fewer steps and enhanced efficiency compared to DDIM. The experiment setup is similar tothe text-to-image generation approach using Stable Diffusion (Rombach et al., 2022) v1.5. The results ofthis experiment are reported in and visually illustrated in . As depicted in : our proposed methods continue to outperform the baseline (static guidance)approach. Substantial improvements are seen in both FID and CLIP-Score metrics, compared to baseline(w=7.5) for example. Notably, these gains become more pronounced as the guidance weight increases, atrend that remains consistent with all other experiments observed across the paper. DiversityDiversity plays a pivotal role in textual-based generation tasks. Given similar text-image match-ing levels (usually indicated by CLIP-Score), higher diversity gives users more choices of generated content.Most applications require higher diversity to prevent the undesirable phenomenon of content collapsing,",
  "baseline (static)linearlinear (c=1.05)linear (c=1.1)linear (c=1.15)Guidance ScaleFIDISFIDISFIDISFIDISFIDIS": "1.102.9669.5642.8939.5952.8529.6222.8569.6382.8769.6471.152.9479.6452.8539.6662.8169.6932.7939.6962.8329.6931.202.9719.6902.8549.7292.8229.7572.8209.7552.8349.7501.253.0259.7332.8979.7992.8639.8092.8639.8092.8639.8091.303.1119.7642.9689.8332.9299.8702.9229.8632.9299.8671.353.2339.7873.0629.8723.0259.9133.0219.9103.0189.908 : Experiment of clamp-cosine on CIFAR-10 DDPM. We evaluate the FID and IS results forthe baseline, parameterized method as clamping on the cosine increasing heuristic (clamp-cosine) of 50Kimages. Best FID and IS are highlighted. It sees the optimising clamping parameter helps to improve theFID-IS performance, the optimal parameter seems at c = 1.05.",
  "baselinecosinecosine (c=1.005)cosine (c=1.1)cosine (c=1.3)guidanceFIDISFIDISFIDISFIDISFIDIS": "1.44.12181.244.31175.44.24176.04.24177.13.82188.21.63.39224.963.08216.23.06217.03.08217.13.09224.61.83.94260.852.98252.42.91251.83.01253.23.13258.42.05.07291.373.46283.33.47282.53.48284.13.67288.22.26.40315.844.26310.14.27307.94.28310.54.49313.12.48.95335.865.22331.25.23329.75.24331.35.44334.1 : Experiment of pcs family on CIN-256-LDM. We evaluate the FID and IS results for thebaseline, parameterized method of the pcs family of 50K images. Best FID and IS are highlighted. It sees theoptimising parameter helps to improve the FID-IS performance, the optimal parameter seems at s = 2 forFID. Interestingly, the pcs family presents a worse IS metric, than baseline and clamp-linear/cosine methods.",
  "baselinepcs (s=4)pcs (s=2)pcs (s=1)pcs (s=0.1)guidanceFIDISFIDISFIDISFIDISFIDIS": "1.44.12181.246.94144.986.10150.494.31175.404.09181.001.63.39224.965.69162.994.27180.523.08216.213.43225.311.83.94260.854.80179.713.29208.862.98252.373.96264.032.05.07291.374.18195.752.88234.093.46283.325.08294.772.26.40315.843.73210.602.81257.224.26310.146.44319.972.48.95335.863.457224.42.98278.145.22331.177.85339.05 : Different Heuristic Modes of SD1.5, we show FID vs. CLIP-score of 10K images. we highlightdifferent range of clip-score by low ( 0.272), mid ( 0.277) and high ( 0.280) by pink, orange and bluecolors. We see that increasing modes demonstrate the best performance at high w, whereas decreasing modesregress on the performance. non-linear modes, especially -shape also demonstrate improved performanceto baseline but worse than increasing shapes.",
  "clip-score0.27420.27480.27640.27760.27860.27950.2802cos (c=4)FID13.73413.82714.22214.69015.09015.56015.916": ": Different Heuristic Modes of SDXL, we show FID vs. CLIP-score of 10K images. we highlightdifferent range of clip-score by low ( 0.2770), mid ( 0.280) and high ( 0.2830) by pink, orange and bluecolors.We see that increasing modes demonstrate the best performance at high w, whereas decreasingmodes regress on the performance. non-linear modes, especially -shape demonstrate improved performanceagainst baseline but regress fast when the is high."
}