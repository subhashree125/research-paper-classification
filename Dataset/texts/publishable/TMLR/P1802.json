{
  "Abstract": "Video Diffusion Models have been developed for video generation, usually integrating textand image conditioning to enhance control over the generated content. Despite the progress,ensuring consistency across frames remains a challenge, particularly when using text promptsas control conditions.To address this problem, we introduce UniCtrl, a novel, plug-and-play method that is universally applicable to improve the spatiotemporal consistencyand motion diversity of videos generated by text-to-video models without additional train-ing. UniCtrl ensures semantic consistency across different frames through cross-frame self-attention control, and meanwhile, enhances the motion quality and spatiotemporal consis-tency through motion injection and spatiotemporal synchronization. Our experimental re-sults demonstrate UniCtrls efficacy in enhancing various text-to-video models, confirmingits effectiveness and universality.",
  "Introduction": "Diffusion Models (DMs) have excelled in image generation, offering enhanced stability and quality overmethods like GANs (Goodfellow et al., 2020; Karras et al., 2019; 2020) and VAEs (Kingma & Welling, 2014;Van Den Oord et al., 2017; Ramesh et al., 2021). The superior image generation capability of diffusion modelsstems from the critical role of the attention mechanism(Rombach et al., 2022a; Peebles & Xie, 2023; Dhariwal& Nichol, 2021). Foundational studies (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020a; 2023;Luo et al., 2023; Karras et al., 2022) established the groundwork for DMs capabilities in efficiently scalingup with diverse data. Recent advancements (Rombach et al., 2022a; Ramesh et al., 2022; Nichol et al.,2022; Saharia et al., 2022; Xu et al., 2023b; Zhang et al., 2023b; Mou et al., 2023) have further improvedcontrollability and user interaction, enabling the creation of images that better reflect user intentions. Recently, Video Diffusion Models (VDMs) (Ho et al., 2022b) have been proposed for utilizing DMs for videogeneration tasks. VDMs are capable of generating videos with a wide variety of motions in text-to-video",
  "professional movie, movie of autumn landscape, dramatic lighting, gloomy, cloud weather, treeUniCtrl+AnimateDiff": ": UniCtrl for Video Generation. we propose UniCtrl, a concise yet effective method to significantlyimprove the temporal consistency of videos generated by diffusion models yet also preserve the motion.UniCtrl requires no additional training and introduces no learnable parameters, and can be treated as aplug-and-play module at inference time. synthesis tasks, supported by the integration of text encoders (Radford et al., 2021), self attention, crossattention and temporal attention, as shown in (Guo et al., 2023b; Zhang et al., 2023a; Hu et al., 2022; Hoet al., 2022a; Blattmann et al., 2023a;b). Many open-source text-to-video models have been introduced,including ModelScope(Wang et al., 2023), AnimateDiff(Guo et al., 2023b), VideoCrafter(Chen et al., 2023)and so on. These models typically require a pre-trained image generation model, e.g., Stable Diffusion(SD)(Rombach et al., 2022b), and introduce additional temporal or motion modules. However, unlike images thatcontain rich semantic information, text conditions are more difficult to ensure consistency between differentframes of the generated video. At the same time, some work also uses image conditions to achieve image-to-video generation with improved spatial semantic consistency (Guo et al., 2023a; Hu & Xu, 2023; Blattmannet al., 2023a). Some works have proposed the paradigm of text-to-image-to-video(Girdhar et al., 2023), butimage conditions alone cannot effectively control the motion of videos. Combining text and image conditionsleads to enhanced spatiotemporal consistency in a text-and-image-to-video workflow (Zhang et al., 2023c;Chen et al., 2023; 2024; Xing et al., 2023; Gu et al., 2023b), but these methods require additional training. To this end, our research goal in this work is to develop an effective plug-and-play method that is training-free, and can be applied to various text-to-video models to improve the performance of generated videos.To solve this problem, we first attempt to ensure that the semantic information between each frame of thevideo is consistent in principle. As the attention mechanism plays a significant role, this principle drawsinspiration from previous research in attention-based control (Hertz et al., 2023; Tumanyan et al., 2023;Cao et al., 2023; Xu et al., 2023a). These works have demonstrated in DMs that the queries in attentionlayers determine the spatial information of the generated image, and correspondingly, values determine thesemantic information. We observe that this finding also holds in VDMs and propose the cross-frame self-attention control method. We thus apply the keys and values of the first frame in self-attention layers toeach frame and achieve satisfying consistency in the generated video. Secondly, we observe that as the videos consistency improves, the motions within videos tend to become lesspronounced. To solve this problem, we propose the motion injection mechanism. Based on the assumptionthat queries control spatial information (Cao et al., 2023), we divide the sampling process into two branches:",
  "Published in Transactions on Machine Learning Research (11/2024)": "Here we present some of the sections left for discussion in the main paper. This supplementary report includesdiscussions on Details of Metrics, Implementation Details, and more Qualitative Results on the effectivenessof the motion injection degree, ablations of each module in UniCtrl, and how UniCtrl and FreeInit (Wu et al.,2023) can work together. These results further demonstrate UniCtrls efficacy in improving spatiotemporalconsistency and preserving motion dynamics.",
  "Related Work": "Video GenerationMany previous efforts have explored the task of video generation, e.g., GAN-basedmodels (Skorokhodov et al., 2022; Tian et al., 2021; Brooks et al., 2022) and transformer-based models(Honget al., 2022; Villegas et al., 2022; Wu et al., 2021; 2022). Recently, following that Diffusion models (DMs)(Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020a; 2023; Luo et al., 2023; Karras et al., 2022)have achieved remarkable results in image generation (Rombach et al., 2022a; Ramesh et al., 2022; Nicholet al., 2022; Saharia et al., 2022; Nichol & Dhariwal, 2021), video diffusion models (VDMs) (Ho et al., 2022b)has also demonstrated their capabilities in video generation (Blattmann et al., 2023b; Girdhar et al., 2023;Gu et al., 2023b; Ho et al., 2022a; Hu et al., 2022; Singer et al., 2022; Zhang et al., 2023a; Guo et al., 2023b;Chen et al., 2023; 2024; Xing et al., 2023; Wang et al., 2023). At present, VDMs are mainly implementedby adding additional temporary layers to 2D UNet, which leads to a lack of cross-frame constraints in thetraining process of the 2D UNet model. Some methods (Wu et al., 2023; Qiu et al., 2023; Gu et al., 2023a)tried to use a training-free method to make the generated videos more smooth. However, how to maintainthe cross-frame consistency in videos generated by VDMs remains unresolved. Attention Control in Diffusion ModelsDifferent from models that require training(Zhang et al.,2023b; Xu et al., 2023b; Mou et al., 2023), attention control (Hertz et al., 2023; Tumanyan et al., 2023;Cao et al., 2023; Xu et al., 2023a; Ge et al., 2023b) is a training-free method which has been widely appliedin the task of image editing. Previous work has found that Attention Control can be used to ensure bothsemantic (Cao et al., 2023) and spatial consistency (Hertz et al., 2023; Tumanyan et al., 2023; Ge et al.,2023b) in image editing. InfEdit(Xu et al., 2023a) unified the control of semantic consistency and spatialconsistency for the first time, proposing unified attention control (UAC). Text2Video-Zero (Khachatryanet al., 2023) applies frame-level self-attention on text-to-image synthesis methods and enables text-to-videothrough manipulating motion dynamics in latent codes. Some work has introduced attention control toVDMs for video editing (Liu et al., 2023; Geyer et al., 2023; Khandelwal, 2023), but no one has improved theconsistency of generated videos through video diffusion by attention control. Is that possible to introduceUAC into VDMs to ensure cross-frame semantic consistency and spatial consistency throughout the video isan interesting and worthwhile question to explore.",
  "(3)": "Latent Diffusion Models (LDMs) (Rombach et al., 2022a) encodes samples into latents using an encoder E,such that z0 = E(x0). Also, the output is reconstructed via a decoder D, represented by D(z). This approachhas led to improvements in stability and efficiency in the training and generation process. Video Diffusion Models(VDMs) (Ho et al., 2022b) extended the application of DMs to the domain of videogeneration, adapting the framework to handle 4D video tensors in the form of frames height width channels, which we can use zft to describe the frame f + 1 at the timestep t.",
  "Attention Control": "We follow the notation from (Hertz et al., 2023; Xu et al., 2023a). In the fundamental unit of the diffusionUNet model, there are two main components: cross-attention and self-attention blocks. The process beginswith the linear projection of spatial features to generate queries (Q). In the self-attention block, both keys(K) and values (V ) are derived from the spatial features through linear projection. Conversely, for thecross-attention part, text features undergo a linear transformation to form keys (K) and values (V ). Theattention mechanism (Vaswani et al., 2017) can be described as:",
  "V(4)": "Mutual Self-Attenion Control (MasaCtrl) is proposed by (Cao et al., 2023), and find that replacing the Qs inattention layers while keeping the Ks and V s same, can change the spatial information of generated imagesbut keeping the semantic information preserved. This technique can help diffusion in spatial-level editing,e.g. a sitting dog to a running dog. Here we use ()src to represent the tensor obtained from the source imageand ()tgt for the target output we want, we can use the following formula to define the MasaCtrl algorithm.",
  "out = Attention(Qtgt, Ksrc, V src)": "Cross-Attenion Control (P2P) is a method mentioned in (Hertz et al., 2023), which is for semantic-levelimage editing (e.g. dog to cat). This work observed that different V s from different text prompts decidethe semantic information of generated images. If attention maps (M) in cross-attention layers have beenreserved, but use different Vs for the attention calculation, most of the spatial information will be preserved.We here use ()src to represent the tensor obtained from the source image and prompt and ()tgt for thetarget output with target prompt, this algorithm can be described as following as same Qsrc, Ksrc lead tosame M src:out = CrossAttention(Qsrc, Ksrc, V tgt)",
  "+SS": ": The first row demonstrates the original frames generated with the baseline model, in which thevehicle and the road are inconsistent across the frames.The second row shows frames generated withbaseline model augmented with cross-frame Self-Attention Control (SAC). While it maintains incrediblespatiotemporal consistency, it exhibits little motion. The third row explains frames augmented with SAC andMotion Injection (MI). Although MI injects more motion in addition to SAC, the results demonstrate thatit falls short on spatiotemporal consistency again. The fourth row contains frames further augmented withSpatiotemporal Synchronization (SS) in addition to SAC and MI, which improves spatiotemporal consistencyover the results from the third row and achieves a balance between motion and spatiotemporal consistency,both in-frame and cross-frame.",
  "UniCtrl: Cross-Frame Unified Attention Control": "In the text-to-video task, it is difficult to ensure the consistency between different frames of the generatedvideo due to the lack of semantic level conditions and the constraint of different frames in the 2D UNetlayers. Based on the previous work of DMs(Hertz et al., 2023; Tumanyan et al., 2023; Xu et al., 2023a; Caoet al., 2023), it is found that the queries in the attention layers determine the spatial information of thegenerated images, and correspondingly, the values determine the semantic information. We assume theseproperties still exist in VDMs. We first analyzed the role of keys and values in the self-attention layers of VDMs, and then analyzed the roleof queries in all the attention layers. Inspired by InfEdit (Xu et al., 2023a), we then propose the cross-frameunified attention control to achieve both semantic level consistency and better spatiotemporal consistency.Lastly, we apply Spatiotemporal Synchronization (SS) by replacing the latent of the motion branch with thelatent from the output branch at each sampling step. We demonstate the effectiveness of each module ofUniCtrl in .",
  "Cross-Frame Self-Attention Control": "Previous work (Cao et al., 2023; Hertz et al., 2023; Tumanyan et al., 2023; Xu et al., 2023a) has observed thatqueries in the attention mechanism form the layout and semantic information of generated images(Cao et al.,2023; Xu et al., 2023a; Tumanyan et al., 2023), while values contribute to the semantic information(Hertzet al., 2023; Xu et al., 2023a). Therefore, we hypothesize that using the same values in the attention ofdifferent frames can ensure cross-frame consistency. Additionally, we observed that the mismatch betweenkeys and values will degrade the quality of generated videos through our experiment and we provide one",
  "Cross-Frame Self-Attn Ctrlmm": ": In our framework, we use key and value from the first frame as represented by K0 and V 0 in theself-attention block. We also use another branch to keep the motion query Qm for motion control. At thebeginning of every sampling step, we let the motion latent equal to the sampling latent, to avoid spatial-temporal inconsistency. Note that in the actual workflow, Q replacement occurs only in cross-attention, asthe Q in the self-attention blocks of both branches are always the same. We explain details of our frameworkin Algorithm 1 and Algorithm 2. qualitative example in .4.Consequently, in our cross-frame Self-Attention Control (SAC), weinject both keys and values from the first frame, as detailed in Algorithm 1, to every other frame at eachself-attention layer during denoising. We showcase one example of the effectiveness of SAC by comparingthe first row and the second row in .",
  ": V 0 = to_v(z0) # Get the Value of the first frame.8: out = SelfAttention(Q, K0, V 0)9: Output: out": "In our experiments, detailed in , we iden-tify a significant limitation associated with cross-frame self-attention control:it tends to produceoverly similar consecutive frames, leading to mini-mal motion within the video sequence. Again, asdescribed in (Cao et al., 2023; Xu et al., 2023a),the queries in self-attention and cross-attention de-termine the videos spatial information, which cor-responds to motion. Therefore, by preserving theoriginal queries, we can maintain the motion effect. As in , we have divided the inference processinto two branches: the output branch, which undergoes cross-frame self-attention control, and the motionbranch, which does not involve attention control. We retain the queries in the motion branch as motionqueries and use the corresponding motion queries in the output branch. Here, we denote motion queries asQm. The method for motion injection can be expressed by the following formula: out = Attention(Qm, , ),where Attention refers to both of self- and cross-attention layers. This method is designed to fully re-tain the motion of the original video. However, in practice, a trade-off between motion preservation andspatiotemporal coherence remains evident. Consequently, we propose an additional refinement: selectivelypreserving motion at specific steps throughout the sampling process to enhance control. To this end, weintroduce a technique that modulates motion through the integration of a motion injection degree, c, whichis defined within the interval 0 c 1. This approach is further detailed and formalized presented inAlgorithm 2 and the following outlines this approach in detail :",
  ": end for12: Output: zN": "Upon closer inspection of the results, notably ex-emplified by the third row of , it becomesevident that simply injecting spatial informationfrom the original video is insufficient for guarantee-ing spatiotemporal consistency.Considering thatoutput branch yields more spatiotemporally con-sistent results, we further propose SpatiotemporalSynchronization (SS): the latent of the outputbranch is copied as the initial value of the latent forthe motion branch before each sampling step. Bydoing so, our method can simultaneously ensure thesemantic consistency of the generated video and im-prove the quality of spatiotemporal consistency andpreserve the degree of motion diversity. We presenta qualitative example to demonstrate the effective-ness of SS by comparing the results depicted in thethird and fourth rows of .",
  "Cross-Frame Unified Attention Control": "As illustrated in , we integrate cross-frame self-attention control, motion injection and spatiotemporalsynchronization into a cohesive framework termed Cross-Frame Unified Attention Control(UniCtrl). In theoutput branch, we employ the key K0 and value V 0 derived from the initial frame to maintain cross-framesemantic consistency. A separate branch is utilized to preserve the query specifically for motion control,replacing the output branchs query with Qm from the motion control branch. To prevent spatiotemporalinconsistencies, we synchronize the latent representation of the motion branch with the output branchspreceding latent state before each sampling step.",
  "Experiments": "In this section, we evaluate the effectiveness of UniCtrl. We discuss metrics, backbones and baseline in .1. Then we include both qualitative comparisons in .2 and quantitative comparisons in .3 to showcase the effectiveness of UniCtrl in terms spatiotemporal consistency and motion preservation.Additionally, we explore the contribution of each component within UniCtrl, the motion injection degree,and the specific design choice of swapping Key and Value together in the SAC procedure, as detailed in.4.",
  "Experimental Setup": "To evaluate the effectiveness of our model, we collect prompts from two datasets UCF-101 (Soomro et al.,2012) and MSR-VTT (Xu et al., 2016) for generating videos. Following Ge et al(Ge et al., 2023a), we usethe same UCF-101 prompts for our experiments. We also randomly selected 100 unique prompts from theMSR-VTT dataset for our evaluation. Those two parts of data consist of our dataset for evaluation. Tomitigate the stochasticity inherent in diffusion models, all experiments were conducted using multiple randomseeds. The scores reported in the tables represent the average results across these runs, and we provide thecorresponding standard deviations in the Appendix. Next, we briefly introduce evaluation metrics and wealso provide details in the Appendex.",
  "BackbonesSince our method is plug-and-play, we decide to evaluate our methods on a few popularbaselines:": "AnimateDiff (Guo et al., 2023b) introduces a practical framework for adding motion dynamics to person-alized text-to-image models, such as those created by Stable Diffusion, without the need for model-specificadjustments. Central to AnimateDiff is a motion module, trainable once and universally applicable acrosspersonalized text-to-image models derived from the same base model, leveraging transferable motion priorsfrom real-world videos for animation. VideoCrafter (Chen et al., 2023) introduces two novel diffusion models for video generation: T2V, whichsynthesizes high-quality videos from text inputs, achieving cinematic-quality resolutions up to 1024576, andI2V, the first open-source model that transforms images into videos while preserving content, structure, andstyle. Both models represent significant advancements in open-source video generation technology, offeringnew tools for researchers and engineers. We use the T2V version in our experiments. AnimateLCM (Wang et al., 2024) introduces a novel framework for efficient and high-quality video gener-ation by leveraging consistency learning. It builds upon the principles of the Consistency Model (CM) andLatent Consistency Model (LCM) from image diffusion models, accelerating video generation with minimalsteps. AnimateLCM employs a decoupled consistency learning strategy, separating the learning of imagegeneration priors and motion generation priors. BaselineWe select FreeInit (Wu et al., 2023) as our baseline since FreeInit is a training-free method thatattempts to improve the subject appearance and temporal consistency of generated videos through iterativelyrefining the spatial-temporal low-frequency components of the initial latent during inference. However, giventhat our method UniCtrl operates on the attention mechanism and FreeInit adjusts the frequency domainof the latent space, UniCtrl and FreeInit are orthogonal approaches. Both are training-free methods capableof enhancing the spatiotemporal consistency of generated videos via diffusion models. We demonstrate theintegration of UniCtrl and FreeInit both in 5.2 and 5.3.",
  "Qualitative Comparisons": "Qualitative comparisons, depicted in , reveal that our UniCtrl method markedly improves spatiotem-poral consistency and maintains motion diversity. For example, with the text prompt walking with a dog,FreeInit produces inconsistent appearances for both the lady and the dog, whereas UniCtrl ensures consistentrepresentations of both entities. Furthermore, when processing the prompt A young woman walks throughflashing lights, UniCtrl maintains the detailed features of the young womans dress while ensuring her walk-ing motion remains natural, in contrast to the vanilla AnimateDiff model. Additionally, we demonstrateUniCtrls flawless integration with FreeInit, consistently preserving the young womans appearance.",
  ": We provide additional qualitative examples across various backbones to demonstrate UniCtrlscapability in enhancing spatiotemporal consistency while effectively preserving motion dynamics": "the metrics, UniCtrl significantly improves the spatiotemporal consistency in the generated videos across allbackbones on both prompt sets from 2.12 to 2.44. While FreeInit achieves remarkable improvements overspatiotemporal consistency, we found that UniCtrl outperforms FreeInit on the strength of motions on bothAnimateDiff and VideoCrafter by a large margin from 11.67 to 17.65. Note that we purposely chose themotion injection degree c = 1 to demonstrate how UniCtrl can preserve the motion compared with FreeInit.However, there still exists a trade-off between spatiotemporal consistency and motion diversity. In section5.4, we show spatiotemporal consistency can be further improved with a smaller motion injection degree.Thus, we recommend motion injection degree c = 0.4 for real-world applications.Lastly, we showcasehow UniCtrl and FreeInit can improve AnimateDiff together and we found this integration can improvespatiotemporal consistency together. We will introduce the details of how we integrate UniCtrl and FreeInit",
  "AnimateDiff (Guo et al., 2023b)1069.9027124.17FreeInit + AnimateDiff958.9725078.05UniCtrl + AnimateDiff819.748864.07": "Furthermore, regarding video quality and mo-tion consistency, we present the quality results onUCF-101 in . We compare AnimateDiffwith FreeInit augmented and Unictrl augmented.The metrics show that UniCtrl significantly en-hances the quality of individual video frames, im-proving the score from 1069.90 to 819.74. Addi-tionally, we observe a substantial improvementin FVMD, decreasing from 27,124.17 to 8,864.07, which indicates a marked enhancement in motion con-sistency. These results further demonstrate our models capability to produce higher-quality videos withimproved motion coherence, highlighting the significant effectiveness of our approach in advancing text-to-video models.",
  "only SAC + AnimateDiff98.084.12only MI + AnimateDiff94.2625.42only SS + AnimateDiff94.2625.42": "UniCtrl (c = 0) + AnimateDiff98.084.12UniCtrl (c = 0.2) + AnimateDiff97.419.04UniCtrl (c = 0.4) + AnimateDiff96.6915.56UniCtrl (c = 0.6) + AnimateDiff96.4620.16UniCtrl (c = 0.8) + AnimateDiff96.3722.50UniCtrl (c = 1.0) + AnimateDiff96.3823.29 The Impact of SAC, MI, and SSTo assessthe contribution of the SAC, we conducted ex-periments on both datasets using AnimateDiff asthe baseline, incorporating our pipeline but dis-abling SAC. For motion injection, we set the mo-tion injection degree to be 1. The findings revealour method without SAC works the same on thebackbone because the motion branch is exactlythe same as the output branch. This observationunderscores the critical role of SAC in enhanc-ing spatiotemporal consistency, as evidenced bythe comparisons with the scores from the vanillaAnimateDiff. In exploring the significance of Motion Injection(MI), additional tests were performed on bothdatasets with AnimateDiff serving as the base-line, this time with MI deactivated. The resultsindicated a notable consistency with the baseline, yet with a substantial reduction in motion diversity. Thiswas quantitatively supported by a decrease in the RAFT score from 31.81 to 4.19. Such a marked disparityhighlights MIs vital contribution to maintaining the motion dynamics. Finally, the necessity of the Spatiotemporal Synchronization (SS) was examined by excluding SS from ourpipeline and conducting experiments across both datasets, again using AnimateDiff as the reference pointand setting motion injection degree to be 1. The outcomes showed diminished spatiotemporal consistency incomparison to the baseline, while motion diversity was not significantly impacted. These results emphasizethe importance of integrating SS into our pipeline, as corroborated by the UniCtrls findings presented in, illustrating the essential role of SS in achieving the desired balance of spatiotemporal consistencyand motion diversity. Additionally, we conduct experiments on each individual component to provide a deeper understanding oftheir respective roles and interactions, as shown in . The results align with our previous analysis:when only SAC is present, we observe a significant increase in the DINO score, confirming the critical roleof SAC in enhancing spatiotemporal consistency. In contrast, when only motion injection is applied withoutSAC and the consistency SAC brings, motion injection alone proves ineffective. Similarly, SS alone showsno impact; only when combined with SAC and MI does it effectively achieve spatiotemporal consistency.",
  "Key and Value Match": ": Each row displays a sequence of video frames generated from the identical prompt: A pandastanding on a surfboard in the ocean under moonlight. The section labeled K and V mismatch illustratesthe frames produced when there is a discrepancy between key and value pairs. Conversely, the section titledK and V match showcases frames generated when key and value pairs are in alignment. The Impact of Motion Injection DegreeTo assess the impact of varying degrees of motion injection,we conducted experiments across both datasets using UniCtrl with motion injection degree set at c = 0,c = 0.2, c = 0.4, c = 0.6, c = 0.8, and c = 1. The effects of different motion injection levels are depictedquantitatively in . As the degree of motion injection escalates, we observed that the DINO scoreconsistently outperforms baseline metrics, and the RAFT score progressively increases. This trend indicatesan amplification in motion diversity. We showcase qualitative examples of the influence of motion injectiondegree in the supplementary material. The Impact of Swapping Key and ValueWe aim to provide a qualitative example to underscore theimportance of simultaneously modifying both key and value, as highlighted in our discussion on the impactof key and value mismatches in .1. Initially, we alter only the value while maintaining the same keywithin the cross-frame Self-Attention Control (SAC) performing the UniCtrl pipeline. As depicted in thefirst row of , the panda begins to fade in the subsequent frames, indicating a substantial decrease inthe quality of the generated videos with respect to spatiotemporal consistency. This decline is particularlyevident when compared to the approach of modifying both the key and value concurrently using the sameUniCtrl pipeline.",
  "Conclusion": "We introduce UniCtrl to address the challenges of maintaining cross-frame consistency and preserving mo-tions for Video Diffusion Models. By incorporating UniCtrl, we have notably improved the spatiotemporalconsistency across frames of generated videos. Our approach stands out as it requires no additional train-ing, making it adaptable to various underlying models. The efficacy of UniCtrl has been rigorously tested,demonstrating its potential to be widely applied to text-to-video generative models. We discuss the primarylimitations of UniCtrl in .1 and detail our ethics statement in .2.",
  "Limitations": "Our method needs to operate on the attention mechanism, which limits the application of our method onnon-attention-based models. Also, since we ensure the same value for each frame, changing colors withinthe video is not possible, which limits the models ability to generate videos. Additionally, we have notyet guaranteed that spatial information is completely consistent across frames; this might be addressed infuture work by controlling the temporal attention block. Furthermore, during the inference process, we needadditional computation to preserve the motion query, which affects the inference speed. Our method canstill be improved by addressing the above issues.",
  "Bias and Fairness": "UniCtrl relies on underlying diffusion models that may harbor inherent biases, potentially leading to fairnessissues in the generated content. Although our method is algorithmic and not directly trained on large-scaledatasets, it is essential to acknowledge and address any biases present in these foundational models to ensureequitable and ethical utilization. By proactively addressing these ethical considerations, we can responsibly leverage the capabilities of UniCtrl,ensuring its application aligns with legal standards and societal welfare. Emphasizing ethical practices, legalcompliance, and the well-being of society is paramount in advancing video generation technology whilemaintaining public trust and upholding community values.",
  "Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.Tokenflow: Consistent diffusion features forconsistent video editing. arXiv preprint arXiv:2307.10373, 2023": "Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla,Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation byexplicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, and Hang Xu. Reuse and diffuse: Iterative denoising for text-to-video generation. arXivpreprint arXiv:2309.03549, 2023a.",
  "Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparsecontrols to text-to-video diffusion models. arXiv preprint arXiv:2311.16933, 2023a": "Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animateyour personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725,2023b. Charles R. Harris, K. Jarrod Millman, Stfan J. van der Walt, Ralf Gommers, Pauli Virtanen, DavidCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Pi-cus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernndez delRo, Mark Wiebe, Pearu Peterson, Pierre Grard-Marchant, Kevin Sheppard, Tyler Reddy, WarrenWeckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant.Array programming withNumPy. Nature, 585(7825):357362, September 2020. doi: 10.1038/s41586-020-2649-2. URL Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on LearningRepresentations, 2023. URL",
  "NVIDIA, Pter Vingelmann, and Frank H.P. Fitzek. Cuda, release: 10.2.89, 2020. URL": "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Woj-ciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma,Gabriel Synnaeve, Hu Xu, Herv Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-janowski. Dinov2: Learning robust visual features without supervision. 2024.",
  "Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xintao Wang, Ying Shan, and Ziwei Liu. Freenoise:Tuning-free longer video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021.",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models, 2022b": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXivpreprint arXiv:2209.14792, 2022. Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video generatorwith the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 36263636, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265.PMLR, 2015.",
  "Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. 2020": "Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov.A good image generator is what you need for high-resolution video synthesis.arXiv preprintarXiv:2104.15069, 2021. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-drivenimage-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 19211930, June 2023.",
  "Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender a video: Zero-shot text-guided video-to-video translation, 2023. URL": "David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, andMike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXivpreprint arXiv:2309.15818, 2023a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusionmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847,2023b.",
  "We consider standard metrics: DINO and RAFT following (Singer et al., 2022; Wu et al., 2023) and wepresent details of our evaluation metrics:": "DINO: To evaluate the spatiotemporal consistency in the generated video, we employ DINO (Oquab et al.,2024) to compare the cosine similarity between the initial frame and subsequent frames. The average DINOscore across all consecutive frames is then used as the videos overall score. In our experiments, we utilizethe DINO-vits16(Caron et al., 2021) model to compute the DINO cosine similarity. We present the detailsof our implementation in Algorithm 3. RAFT: To compare the magnitude of motion in the videos, we utilize RAFT (Teed & Deng, 2020) toestimate the optical flow, thereby inferring the degree of motion. To quantify the motion intensity betweenevery two consecutive frames, we employ the RAFT model to estimate the optical flow for each pixel. Wecalculate the magnitude of all pixel optical flow using the l2 norm, and finally, we obtain the average of allmagnitude as the score for every consecutive frame in the video. In this process, we utilize the RAFT modelfrom torchvision (maintainers & contributors, 2016). We present the details of the RAFT implementationin Algorithm 4.",
  "B.2Inference Details": "Experiments on VideoCrafter is conducted on 512 320 spatial scale and 16 frames, while experiments onAnimateDiff is conducted on a video size of 512 512, 16 frames. AnimateLCM is also conducted on 512 512, 16 frames. During the inference process, we use classifier-free guidance for all experiments includingthe comparisons and ablation studies, with a constant guidance weight 7.5. All experiments are conductedon Nvidia A10G GPU. We conduct experiments across different GPUs such as Nvidia A10G, Nvidia A40,and Nvidia A100 and we observe different inference results using the same seed while the CUDA (NVIDIAet al., 2020) random algorithm is deterministic. We resolve this behavior by initializing latent using NumPy(Harris et al., 2020). We hope this change will further enhance the reproducibility of our code across differentCUDA architectures.",
  "D.1Motion Injection Degree": "In the main paper, we quantitatively demonstrate that higher degrees of motion injection results in videoswith more pronounced motion. Here, we provide a qualitative example in . The motion of the Corgiis significantly more pronounced when comparing videos generated with a higher motion injection degree tothose with a lower degree, while the appearance of the Corgi remains consistent across frames in each video.",
  "D.2Qualitative Ablation Results": "In the main paper, we conduct quantitative ablation studies of each module and we would like to emphasizethe indispensable nature of every component within UniCtrl again by presenting . This figure qual-itatively illustrates the ablations through a single set of results. By comparing the first and second rows,it becomes evident that the results in the second-row display inconsistencies across frames, both in termsof the car and the road. Intuitively, the motion branch and output branch keep deviating while denoising.SS effectively bridges the difference between the motion branch and the output branch and significantlyimproves spatiotemporal consistency as a result. By comparing the first and third rows, we observe the thirdrows result shows much more inconsistencies which clearly explains the necessity of incorporating SAC inUniCtrl. Lastly, we show the fourth rows result contains little motion compared with the first rows result.Thus, we prove that MI is also one of the indispensable modules of UniCtrl.",
  "-MI": ": The first row demonstrates results generated with UniCtrl, which the vehicle and the road areboth consistent across the frames. The second row shows frames generated with SAC and MI. The thirdrow explains frames augmented with SS and MI. The fourth row contains frames shows results with SS andSAC. These comparisons serve as qualitative examples for ablation for each module in UniCtrl.",
  "D.3Unification between UniCtrl and FreeInit": "We state that UniCtrl and FreeInit explore orthogonal directions to improve the spatiotemporal consistencyof video diffusion models. Now we want to first explain how to unify UniCtrl and FreeInit. In the frameworkof FreeInit, it requires N FreeInit iterations, N = 3 in our implementation, and we apply UniCtrl duringthe first iteration. We find the unification between UniCtrl and FreeInit improves the quality of generatedvideos and we showcase additional qualitative examples in ."
}