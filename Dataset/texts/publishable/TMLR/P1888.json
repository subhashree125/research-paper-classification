{
  "Abstract": "This work does NOT read like fabricate motivation - propose something - obtain sotaresults. Instead, we provide an in-depth analysis of the learnable softmax temperature pa-rameter in the practical training of contrastive visual-textual alignment models, commonlyknown as CLIP models. This parameter is critical for optimal system performance, yetits mechanism and potential drawbacks have been largely overlooked. Our study addressesthis gap and proposes a novel solution by utilizing the architecture of Vision Transform-ers (ViTs). We focus on the crucial role of the softmax temperature in managing noisytraining data. We demonstrate that there is a balance in the gradient of the contrastiveloss, with the temperature parameter acting as a distance scaling factor.If not prop-erly calibrated, the model struggles to align positive pairs due to numerical issues in theloss term. Conversely, a high temperature can lead to unstable learning dynamics. Weexplore alternative approaches to mitigate this problem from a topological perspective ofthe contrastive loss. Ultimately, we leverage multiple class tokens embedded within thetransformer architecture to present a concise solution. This configuration significantly en-hances zero-shot classification performance, improving baseline CLIP models pretrained onlarge-scale datasets by an average of 6.1%. The codes and learned weights are provided in",
  "Introduction": "Learning visual and textual feature representations that are semantically aligned in their embedding spaceis an ordinary problem in the vision-language cross-modal tasks (Frome et al., 2013; Karpathy & Fei-Fei,2015; Romera-Paredes & Torr, 2015; Wang et al., 2016; Faghri et al., 2017; Xian et al., 2016). In early worksthat employ feature representations from deep neural networks, e.g. Frome et al. (2013), the alignment isoften achieved by a fundamental metric learning approach with the hinge rank loss. That is, the similaritybetween a visual feature vector u and a textual feature vector v is calculated as uT Wv, where W are thelearnable weight parameters. Thanks to significant advances in computational power, we can now use amore effective and practical approach called contrastive learning. This method aligns positive sample pairsand simultaneously pushes negative samples apart in large mini-batches using the InfoNCE loss (Radfordet al., 2021; Singh et al., 2022; Jia et al., 2021; Pham et al., 2021; Yuan et al., 2021). We first review the contrastive visual-textual alignment system. Given a set of semantically related image-text pairs S = {(U, V )1, (U, V )2, . . . , (U, V )K}, where (U, V ) is a pair of an image with a tokenizedtext that are considered to be semantic related. The goal is to learn a pair of encoders, simultaneously:",
  "Published in Transactions on Machine Learning Research (10/2024)": "Qin et al. (2022): This paper considers the major challenge in cross-modal retrieval is the noisy correspon-dence in training data. This refers to the fact that some of the training pairs may not be correctly aligned,i.e., the image and text do not actually correspond to each other. They propose a framework to addressthis challenge by integrating two novel techniques: Cross-modal Evidential Learning and Robust DynamicHinge.Yang et al. (2023): This paper proposes a general framework for cross-modal matching that can be easilyintegrated into existing models and improve their robustness against noisy data. This framework estimatessoft labels for noisy data pairs by exploiting the consistency of cross-modal similarities.Han et al. (2023): The paper proposes a Meta Similarity Correction Network to provide reliable similarityscores for cross-modal retrieval. The method learns to distinguish between positive and negative pairs ofdata using meta-data, and can be used to remove noisy samples from the training dataset.",
  "i[M]ed(f(U i ),g(V )),": "is the negative term, with M Z+ denotes a fixed number of negative samples. The subscript i, j meansthe image or text chosen from the i, j-th pairs. Intuitively, optimizing this loss term minimizes the distancebetween positive image-text pairs and maximizes the distance between negative image-text pairs.It isworth mentioning that, in recent studies (Radford et al., 2021; Chen et al., 2021b), the contrastive loss iscommonly implemented as the cross-entropy between one-hot labels and the class probability obtained bysoftmax within a mini-batch SM. We also employ this implementation in this work as shown in . In Equation (1), the standard choice of the distance measure between an image-text pair for the contrastivelearning algorithm is the Cosine Similarity, in both uni-modal (Chen et al., 2020a; Caron et al., 2020;Chen et al., 2020b) and cross-modal (Radford et al., 2021; Jia et al., 2021; Singh et al., 2022) scenarios.While this choice provides a solid foundation for feature representation vectors from different sources, it iswidely acknowledged that training such a configuration is challenging in the absence of a learnable softmaxtemperature. This learnable temperature is prepended and continuously updated through gradient descent,along with the training progress in practice (Wu et al., 2018; Radford et al., 2021).",
  "In this work, we conduct a comprehensive analysis of this learnable temperature parameter based on thefollowing steps. Our primary contribution is highlighted at the beginning of each step": "1. We show that the temperature is learned to achieve the numerical equilibrium of the con-trastive loss. In practice, we could not perfectly minimize/maximize the distance between positive/negativeimage-text pairs due to the ambiguity (noise) that exists in S. However, the contrastive alignment systemfinds a numerical \"equilibrium\" where the loss from noisy samples is balanced, minimizing the positive dis-tance (alignment loss) and maximizing the negative distance (uniformity loss). In .1, we give avisualization of the ratio of the two losses under different temperature conditions. We find that both a largerbatch size and more noise data require a large temperature to reach equilibrium. 2. We suggest three conditions for contrastive alignment to achieve equilibrium, and thetemperature acts as a scaling factor for the numerical value of the distance. In .2, weanalyze the properties of the contrastive loss from a topological perspective and outline three prerequisites: 1.A proper definition of uniformity of the embedding space for samples; 2. A relaxed triangular inequalitybetween sample distances; 3. An expansive numerical value for the distance. We show that the CosineSimilarity calculates the inner product value between normalized feature representation vectors, resultingin a similarity ranging from -1 to 1. Hence, the system requires a parameter to expand this range. Nextin .3, we conduct a toy experiment to demonstrate the necessity of these conditions by varyingtopology and distance combinations. 3. We show that a temperature parameter with biased initialization or a large learned valuecan degrade training. Subsequently, in .4, we visualize the temperature learning curve, revealingthat the learned temperature indicates the datasets noise level. A biased initialization that doesnt matchthe dataset can impair performance. We also visualize the models loss landscape concerning temperaturechanges, showing that high temperatures lead to inferior learning dynamics.",
  "Rethinking the Contrastive Alignment": "In this section, we delve into our motivation in detail.First, we provide a visual representation of theequilibrium. Then, we discuss the conditions (inherent properties) necessary for the contrastive loss to reachthis equilibrium. Next, we design a toy experiment with controlled configurations to demonstrate the effectsof these properties. To generalize the problem, we conceptualize the embedding vectors as points withinspecified topologies, using the term distance between the points instead of similarity.We assert thata contrastively well-optimized visual-textual model should yield shorter distances between semanticallyrelated image-text pairs compared to unrelated pairs, regardless of how the pairs are labeled. Finally, wediscuss the potential drawbacks of the temperature parameter.",
  "The visualized equilibrium": "In , we give an intuitionistic presentation of the equilibrium with our learned reference model in. As explained by Wang & Isola (2020), contrastive loss is a combination of two objects: a) alignmentof features from positive sample pairs and b) the distribution of the features encouraged to match. To demonstrate this, we randomly select a mini-batch of samples from our training dataset in varying sizes.We then manually introduce noise by shuffling a small percentage of the images labels (paired texts). Finally,we compute the ratio of alignment loss to uniformity loss (numerator and denominator in Equation (1)) underdifferent temperature conditions, averaging across all positive pairs in the mini-batch. Our findings indicatethat a noisier mini-batch or a larger mini-batch size requires a higher temperature to achieve loss equilibrium.Additionally, when the system operates at a lower temperature, it tends to push negative pairs away ratherthan pull positive pairs closer. Without the temperature adjustment, the alignment loss becomes numericallytoo small for the system to effectively align positive pairs. In the next subsection, we discuss the mechanismbehind this observation, focusing on three often overlooked conditions.",
  ": Illustration of the bounded negative distance between true negative pair of samples": "they address the question: if we build a system without temperature, what is essential for ensuring thatthe system generalizes well? (See the settings in .3). The conditions focus on i) Embedding space(the set of embedding vectors); ii) Properties of the map (the characteristics of the function that maps pairsof embeddings to similarity values); iii) Range of similarity values (the similarity values produced by themap should be useful for learning). While missing one or more of these conditions does not prevent thesystem from being trained, it may lead to a system that is far from equilibrium, potentially resulting in poorgeneralization performance. Below, we discuss these conditions in detail.",
  "Proper definition of uniformity of the embedding space": "In this subsection, we explain why we cannot use 2 loss in the Euclidean space. Naturally, withthe loss form defined in Equation (1), the distribution object will result in a uniform distribution on thesphere. Although the distribution of samples doesnt have to be exactly uniform as discovered by Chenet al. (2021a), it is necessary to define a proper prior distribution for samples to match via optimal transportalgorithms (e.g. sliced Wasserstein distance), which is undoubtedly a computational burden. Consequently,the spherical embedding space is deemed the most suitable topology for contrastive alignment, as it exhibitsa proper uniform distribution defined by the surface area measure.In contrast, the commonly adoptedunbounded Euclidean space lacks this property.",
  "Relaxed triangular inequality": "In this subsection, we explain why we are encouraged to use a non-measure distance. Sincethe model is being optimized by the contrastive loss, it is proposed to decrease the distance between labeledpositive pairs and increase that between labeled negative pairs. To examine how the noisy (false) positivepairs influence the model (even using human-labeled datasets, see Chun et al. (2022)), we assume that wehave a model that is well-optimized using a noisy dataset. For this model, we have the following properties:For a positive pair (U, V )+, their distance d(u, v) is upper-bounded by a small +, and the distance fornegative pairs (U, V ) is lower-bounded by a large . Now, let us consider a set of two pairs of its training samples S = {(U1, V1), (U2, V2)}. Accidentally, thepair (U1, V2) is also semantically correlated, despite being recognized as a negative sample (It is pervasiveto have negative pairs of image and text that match each other equally well as the positive ones in the noisydataset). If the distance function d is a metric, then according to the triangle inequality axiom of metric,we have the following inequality (see for an intuitive illustration),",
  "d(u2, v1) d(u1, v2) + d(u2, v2) + d(u1, v1) 3+(2)": "From this simple derivation, even in this well-optimized model, still it will predict a distance upper-boundedby + for this pair instead of a larger value than . Here, we have a contradiction: the system would notlearn numerical separated distance ranges for positive and negative pairs that minimize the contrastive loss,hence we cannot achieve a well-optimized model.",
  "Distance Range[0, +)[0, m][m, m]": ": Summary of different topologies endowed with different distances.The total dimension of theembedding vector is denoted as d. The mini-batch size is denoted as b.Green box stands for the prop-erties that are favored for contrastive learning.Red box stands for the properties that are unfavored forcontrastive learning. Because the models are not \"well-optimized\", we have the following observations (See in the sup-plementary materials). First, the positive pairs of samples (Green bins) usually have a much larger distancethan that in the perfect alignment condition. For instance, the distance between the positive pairs of alearned Euclidean-2 is around 7, while the perfect alignment condition means the distance should be 0.Second, although the contrastive loss term does not regularize the distance between negative pairs of samplesin the same modality (Gray/Yellow bins), this distance is implicitly minimized due to the strict triangularinequality. For instance, the distance negative text pairs distribute closer to the negative image-text pairs.Finally, in the inner product distance configuration, it only obeys a relaxed triangular inequality yieldedby ArcCos (the geodesic, see Schubert (2021) for more information.) on the sphere. This makes the distancebetween positive/negative pairs slightly more separated, especially in the out-domain scenario (Green/Redbins in the lower part). From these observations, we find one solution is to have a relaxed triangularinequality to alleviate the ambiguity of the positive/negative pairs. This property could make the distancebetween negative pairs not strictly upper-bounded by that of positive pairs.",
  "Broad distance range": "In this subsection, we discuss the mechanism behind the learnable temperature trick given theabove two constraints. As discussed in .1, the contrastive loss is minimized through the gradientsof both alignment and uniformity losses. In a noisy dataset inducing semantic ambiguity, the false negativesamples are pushed away from each other (repulsion), while the false positive samples are pulled together(attraction). Ideally, the system could gradually find a numerical equilibrium (the stabilized state) whenthe noisy samples gradients for attraction and repulsion are equal. For instance, if there is a reasonableamount of false negative samples, the model would learn a smaller distance between the pairs labeled asnegative, such that the uniformity loss wont be too high when encountering false negative samples in mini-batches. On the contrary, the model would learn a larger distance between the pairs labeled as positivewhen the amount of false positive samples is inneglectable. In other words, the model reaches the equilibriumby learning compromised positive and negative distances under semantic ambiguity. However, the required numerical value of the compromised distances is out of the range that the cosinesimilarity could provide.This is generally because the InfoNCE loss computes the sum of exponentialfunctions. To reach this equilibrium, we need to expand the distance range to , and since we dontknow the noisy level of the dataset, we set the temperature learnable through gradients. For instance, theofficially released CLIP model (Radford et al., 2021) has a glancing similarity of 0.3 0.5 and 0.1 0.3for positive and negative pairs of samples, respectively.The learned temperature is approaching 100.0,indicating the value of distances for equilibrium are 30 50 and 10 30 for positive and negative pairs ofsamples, respectively.",
  "The toy experiment": "Experiment settings: We design a toy experiment to demonstrate how the properties mentioned aboveinfluence the performance of contrastive learning.We employ the 15M subset (Cui et al., 2022) of theYFCC100M dataset (Thomee et al., 2016) as the training dataset, which contains roughly 15.3 millioninternet collected weakly related image-text pairs.We evaluate the learned models with the zero-shotretrieval performance on Flickr30K, and Zero-Shot/Linear Probe classification performance on ImageNetfor reference. We employ the original ViT-S/16 architecture for our image encoders (Dosovitskiy et al.,2020), with an input image resolution of 224, resulting in 196 image tokens.",
  "(ii). The Euclidean space Rd with 2 distance, which is the most commonly used loss function in machinelearning algorithms with an unbounded distance range": "(iii). A product spherical embedding space PS(n, m) with the minimizing geodesic as distance, which isdenoted as Geo(u, v) = tr12 (arccos2(uT v)). We use this configuration to show the importance of properlydefined uniformity.Although its distance range is still bounded, it is sufficient for our system to reachequilibrium using the proposed dataset (See ).",
  "(iv). The same product spherical embedding space PS(n, m) with the inner product as distance. We usethis configuration to show the importance of the relaxed tri-angular inequality": "Here, we explain how we implement the product sphere embedding space. The product sphere (PS(n, m))can be defined as Sn1 Sn1m copies, where Sn1 is the sphere embedded in Rn. Intuitively, it is a vector composed of m chunks of ndimensional normalized sub-parts. Our implementation follows this intuition;we reshape the original feature vector into a matrix of shape m n, then 2normalize the columns. Itis worth mentioning that, this study primarily investigates the characteristics of contrastive learning loss,proposing three distinct properties that the vanilla approach (cosine similarity) fails to satisfy concurrently.Therefore, we explore an approach that satisfies these three properties simultaneously. While the productsphere is not the only solution in this scenario, it could facilitate the analysis because I) It has the samedefinition of Uniformity; II) We can control the distance range by the number of sub-spheres to designablation experiments; III) It is straightforward to implement.",
  "Drawbacks in the learning dynamics": "Biased initialization may degrade training: We now examine how the initialization of the temperatureimpacts the training progress. Specifically, we additionally run experiments with the temperature initializedat exp(2.64), which is the default value in the official CLIP implementation, and a notably higher value ofexp(5.3) (200). The changes in the temperature during the training process are illustrated in . Thefigures confirm that: The temperature converges to an equilibrium value irrespective of initialization when theuniformity is properly defined. This value reflects the noise level present in the datasets, configurations withrelaxed triangular inequality and a broader distance range have lower equilibrium values. These findingssuggest that, if the training is insufficient or the dataset is too noisy, then with a biased initialization of thetemperature parameter, the training progress might be degraded.",
  "(c)": ": The impact of temperature on the loss landscape.(a) and (b) depict the contours with twoorthogonal weights in random directions at temperatures of 1.0 and 100.0, respectively. (c) presents thecontour gradient norm with weight in a random direction across various temperatures (dark color standsfor a larger norm). learned reference model in to draw the figures, where the final learned temperature is 100.0 (due tothe clamp function). We randomly select a mini-batch of size 1024 from the training dataset. The resultsare shown in . From figures (a) and (b) we can observe that the model with a lower temperaturehas a much smoother loss landscape. Furthermore, in Figure (c) we find that the norm of the gradient mightbecome greater as the temperature grows. Therefore, in practice, we need to employ techniques such asgradient norm clipping to stabilize the training.",
  "The Multiple Class Tokens Solution": "We first review the importance of the class token. In the design of both the textual transformer (BERT, Ken-ton & Toutanova (2019)) and the visual transformer (ViT, Dosovitskiy et al. (2020)), one learnable embed-ding is used to represent global information, termed as class token. Different from the sequence (patch)tokens, the class token is a key component of the transformer encoder. It is randomly initialized and up-dated through gradient descent during the optimization. Furthermore, the class token holds a fixed positionembedding, avoiding the influence of the positional information. Therefore, the class token is considered toparticipate in the computation of global attention. Motivated by the properties of the class token, we propose to employ multiple class tokens to build theproduct spherical embedding space, with each class token being a sub-sphere of Sn1. For the visual encoder,",
  "ViT-L/14-224 as visual bone for reference.CLIP[openAI]75.569.770.787.985.065.287.756.336.565.2CLIP[openCLIP]72.765.646.684.887.670.390.159.743.070.0": ": Comparsion of large scale contrastive visual-textual pre-train model on benchmark datasets. and denote the implementation from Radford et al. (2021) and Ilharco et al. (2021), respectively. The metricMean stands for the average value of R@1/5/10 of I2T/T2I retrieval performance. * denotes the Karpathytest split (Karpathy & Fei-Fei, 2015). these class tokens are randomly initialized to break symmetry, while for the textual encoder, we use differentabsolute positional embeddings for each class token. We present a sketch of the system in anda pseudo-code in , to facilitate a better understanding. Given the fact that the dimension for theembedding space could be a critical factor to the performance of the system (Gu et al., 2021), we selectthe dimension n with a conservative strategy. Specifically, we anchor the dimension of Euclidean spacesto be the same as the reference model, then vary the value of n, m such that n m = l.We denotethis implementation as Multi(n, m). The sub-spheres could benefit from the global attention operation andprovide more representative feature embeddings. On the contrary, the multi-token implementation requiresmore computational resources in the backbone since the class tokens are involved in the computation ofglobal attention.",
  "Experimental Settings": "Please note that our objective is not to produce the best publicly available model; rather, we solely conductexperiments under controlled and restricted conditions. We compare the performance of the proposed methodusing the configuration, which matches the publicly released ones in teams of dataset samples, model sizes,and training progress. We also re-implement the naive CLIP model as the reference, which holds a similarperformance as the publicly released ones. 2It is needed to clarify that in the multiple class tokens system, we also employ the learnable temperature practically. Sincethe finally learned temperature is significantly smaller than that in the vanilla CLIP system ( 4.0 versus 100.0), we omit itto highlight the difference between the systems.",
  "# t- learned temperature parameter": "# concatenate cls_tokens and extract featuresU_, V_ = concatenate([cls_U, U], axis=1), concatenate([cls_V, V], axis=1)u_bar = visual_transformer(U_) #[n, PS_n + p, d]v_bar = textual_transformer(V_) #[n, PS_n + l, d] # map features onto PS(n,m) and calculate distanceu = projection_u(u_bar[:PS_n]).l2_normalize(axis=-1) # [n, PS_n, PS_m]v = projection_v(v_bar[:PS_n]).l2_normalize(axis=-1) # [n, PS_n, PS_m]# [n, PS_n, PS_m], [n, PS_n, PS_m] -> [n, n]neg_distances = einsum('inm,jnm->ij', u, v) * t.exp()",
  ": Python-like pseudo-code of the proposed approach": "Datasets: For the experimental analysis in .2, we collect data from publicly available datasets.The majority of the image-text comes from the LAION400M(Open Clip) dataset (Schuhmann et al., 2021).However, since the availability of this dataset decreases through time3, we also add the training splits of someother well-known large-scale datasets, they are CC3M (Changpinyo et al., 2021), CC12M (Sharma et al.,2018), MSCOCO (Chen et al., 2015), Visual Genome (Krishna et al., 2017), Flickr30k (Plummer et al.,2015), ImageNet (Russakovsky et al., 2015), OpenImage (Kuznetsova et al., 2020) and WebVision (Li et al.,2017). This dataset contains a total of 420 million individual images and roughly 500 million image-textpairs. This dataset is comparable to the one employed in the official CLIP paper (Radford et al., 2021) andanother open source re-implementation (Ilharco et al., 2021). Models: Specifically, we employ the ViT-B/16 as our image encoders. For our text encoders, we employErnie-2.0-en-base (Sun et al., 2020), which is a Bert model (Devlin et al., 2018) of 12 layers and 512 hiddenneuron sizes with a customized vocabulary of 30,522 tokens, and the maximum context length is set to be77. We project the feature representation (class token) from the top layer of transformers to a (sum of)512-dimensional embedding space. All the parameters except the temperature are optimized from randominitialization. The default initialization of the project matrix employs the Gaussian initializer of zero mean,and standard deviation equal reversed square root of the input size (a.k.a. Kaiming initialization). Thedetails of the hyperparameters are provided in supplementary materials. Evaluation: We first evaluate the proposed methods with two types of vision tasks: i) Zero-Shot image-to-text and text-to-image retrieval on Flickr30k (Plummer et al., 2015) and MSCOCO (Lin et al., 2014)ii) Zero-Shot classification on ImageNet-1K (Russakovsky et al., 2015), ImageNet-V2 (Recht et al., 2019),ImageNet-R (Hendrycks et al., 2021a) and ImageNet-A (Hendrycks et al., 2021b). For zero-shot retrievalon Flickr30K and MSCOCO, we employ the logits (distance) computed by the distance function and reportthe image-text pairs with the top-k shortest distance as the retrieval results. For zero-shot classification onImageNet. We employ multiple prompt templates described in Radford et al. (2021), while we first compute",
  ": Comparsion of large scale contrastive visual-textual pre-train model on benchmark datasets": "the distances between image and text embeddings, then average the distances. For linear probe classificationon ImageNet, we remove the learned projection head (no topological structure is preserved), then attach arandom initialized linear projector to map the feature representation to the 1,000 class logits. Besides the tasks mentioned above, we provide more results using the ECCV dataset (Chun et al., 2022). Thedataset is proposed for eliminating the false negative samples in the validation set of the original MSCOCOdataset. Instead of the commonly used Recall@K (R@K) metric, the datasets provide a new ranking-basedmetric, mAP@R. The authors of the ECCV dataset have shown that the mAP@R metric is more alignedto humans than , the performance of a model evaluated by mAP@R would be lessoccasional than the R@1. This metric is deemed more precise for evaluating the performance of models inthe presence of noise.",
  "Experimental results": "The evaluations of the learned models on the commonly employed image classification and image-text re-trieval tasks are reported in , with the ViT-B/16 as the visual backbone. It can be seen that there-implemented CLIP holds a similar performance as the publicly released ones in most cases. On the otherhand, our proposed model with the multi-token implementation of (32,16) significantly outperforms the otherViT-B/16 models in general, with less than 8% more computational costs. The only exception is the top-1retrieval performance on the MSCOCO datasets. The reason could be two-fold. Firstly, we observe a mildsemantic decoupling between the embedding of tokens through the visualization (see .3); thatis, some of the individual class tokens focus on specified objects and provide a high alignment confidence.This may confuse in understanding the given scene as a whole; hence, the recall@top-1 performance is de-graded. Secondly, the most suitable temperature during training for aligning object-level and scene-levelconcepts might differ. In our experiment, we decrease the upper limitation of the temperature to 6.25 (100/ 16 [tokens]) since the product spherical topology owns the border distance range. The scene-level conceptalignment might require a larger temperature for ambiguity to achieve better retrieval performance. For the ECCV caption dataset, we utilize the officially released evaluation tool and present a summary ofmodels performance in . Our proposed multi-token product sphere topology outperforms others interms of mAP@R, signifying the models enhanced robustness in handling semantic ambiguity samples.",
  "Visualization on Tokens Attention Regions": "In this section, we provide a commonly adopted neural network explanation method to visualize the influenceof inputs on the final outcome. Specifically, we employ the Grad-CAM (Selvaraju et al., 2017) algorithmto highlight the interested parts by the model of both images and their corresponding texts. Notably, theoriginal design of the Grad-CAM algorithm precludes its direct application to textual data. Therefore, weenhance its capabilities such that it also highlights the contributing parts of texts in a token-wise style.We employ examples from the evaluation set of the Flickr30K and MSCOCO datasets for visualization.The results are shown in . Through our observations, we have noted that certain pairs of class",
  "Token id=15": ": Visualization of the importance map using the Grad-CAM algorithm. The columns from left toright stand for: the input image-text pair; the importance map computed based on the final matching score;and the importance maps based on the matching scores of two individual tokens and the involved token IDs.Additational results are provided in the supplementary material (including the failed cases). tokens exhibit independent alignment, thereby reflecting a distinct concept or idea embedded within theimage.We attribute the improved classification performance to this phenomenon, as it enables a moreeffective representation and understanding of the underlying content by leveraging the distinctive alignmentof the class token pairs. It is noteworthy that the intrinsic decoupling phenomenon observed within theembeddings of class tokens is NOT universally present across all image-text pairs or within every token of animage-text pair. This is due to the inherent challenges faced by visualization algorithms in achieving precisecorrespondence in the importance maps for intricate semantic representations.",
  "Related Works": "Momentum distillation: In recent works such as Cheng et al. (2021); Li et al. (2021a), the momentum(self-)distillation is introduced to mitigate the semantic noise in the sample pairs. That is, a momentumversion of the model is updated by the moving average of the models historical parameters. Then, thecross entropy between the softmax logits computed by the model and its momentum version is used as anadditional loss for supervision. The authors claim that the pseudo-targets of the momentum (self-)distillationwill not penalize the model for matching negative samples that are reasonably similar. Here, we considerthat the pseudo-targets do relax the triangular inequality restriction implicitly by letting the distance ofalignment be reasonably large. Hence, it could be much easier for the optimizer to find the equilibriumdiscussed in . Other implemention of non-metric distance: In Yao et al. (2021), the authors proposed a so-calledfine-grained contrastive learning scheme that matches all the visual and textual tokens using a maximum-average operator. Concretely, for each visual token, it finds the textual token with maximum similarity, thentakes the average over the visual tokens as the similarity of the image to a text and vice versa. Using ourframework, this work can be explained as embedding samples onto the product manifold Sd1 Sd1",
  "endowed with the maximum-average distance, which is a non-metric distance. At the same time, the authorsemploy the sub-manifold Sd1 to represent local information": "The effects of softmax temperature: In Wang & Liu (2021), the authors draw the uniformity of theembedding distribution and the tolerance to semantically similar samples of learned models under differenttemperatures. From the observations, the authors claim that a good choice of temperature can compromisethese two properties properly to both learn separable features and tolerant to semantically similar samples,improving the feature qualities and the downstream performances. Unlike our work, this work is done underuni-modal contrastive learning, where the semantic correlation of the negative samples is not a property ofthe datasets but rather a drawback of the larger mini-batch size.",
  "Conclusion": "Summary: This work discusses the essential properties of the feature embedding space for contrastivealignment. We show that the most commonly adopted cosine similarity has disadvantages in dealing withnoisy data and training stability. Therefore, we propose to combine the product sphere with the negativeinner product distance to tackle these problems. We employ multiple class tokens to implement the approach,which performs better in various zero-shot classification and image-text retrieval tasks practically. Limitation: First, given significantly constrained computational resources (and time), we acknowledge ourinability to conduct experiments on a larger scale regarding batch size, training data, and neural networkparameters. However, the reported results are robust enough to substantiate our claims. Second, in re-cent studies, besides the contrastive alignment, more pre-training tasks are appended to the head of themodel using the non-normalized full token embedding. Such as image-text matching (Li et al., 2021a; Yanget al., 2022), image captioning (Yu et al., 2022), or masked modeling that do not employ the contrastivealignment (Wang et al., 2022). The performance improvement resulting from a better contrastive alignmentcould be marginal in these configurations. Hence leave future work on designing the model of the full tokenembedding. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.Food-101mining discriminative componentswith random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland,September 6-12, 2014, proceedings, part VI 13, pp. 446461. Springer, 2014. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-pervised learning of visual features by contrasting cluster assignments. Advances in Neural InformationProcessing Systems, 33:99129924, 2020. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scaleimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 35583568, 2021. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020a.",
  "Weiwei Gu, Aditya Tandon, Yong-Yeol Ahn, and Filippo Radicchi. Principled approach to the selection ofthe embedding dimension of networks. Nature Communications, 12(1):3772, 2021": "Haochen Han, Kaiyao Miao, Qinghua Zheng, and Minnan Luo. Noisy correspondence learning with metasimilarity correction.In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 75177526, 2023. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deeplearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in AppliedEarth Observations and Remote Sensing, 12(7):22172226, 2019. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The manyfaces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial ex-amples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1526215271, 2021b.",
  "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. In Proceedings of naacL-HLT, pp. 41714186, 2019": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained catego-rization. In Proceedings of the IEEE international conference on computer vision workshops, pp. 554561,2013. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and visionusing crowdsourced dense image annotations. International journal of computer vision, 123(1):3273, 2017.",
  "Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. arXiv preprint arXiv:2112.12750, 2021": "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Readingdigits in natural images with unsupervised feature learning.In NIPS workshop on deep learning andunsupervised feature learning, volume 2011, pp. 4. Granada, 2011. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian conference on computer vision, graphics & image processing, pp. 722729. IEEE, 2008.",
  "Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan,and Quoc V Le. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050, 2021": "Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and SvetlanaLazebnik.Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentencemodels. In Proceedings of the IEEE international conference on computer vision, pp. 26412649, 2015. Yang Qin, Dezhong Peng, Xi Peng, Xu Wang, and Peng Hu. Deep evidential learning with noisy correspon-dence for cross-modal retrieval. In Proceedings of the 30th ACM International Conference on Multimedia,pp. 49484956, 2022. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR,2021.",
  "Erich Schubert. A triangle inequality for cosine similarity. In International Conference on Similarity Searchand Applications, pp. 3244. Springer, 2021": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, AarushKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. InProceedings of the IEEE international conference on computer vision, pp. 618626, 2017. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-nymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25562565, 2018.",
  "Feng Wang and Huaping Liu.Understanding the behaviour of contrastive loss.In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pp. 24952504, 2021": "Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 50055013, 2016. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment anduniformity on the hypersphere. In International Conference on Machine Learning, pp. 99299939. PMLR,2020. Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais KhanMohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for allvision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametricinstance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 37333742, 2018. Yongqin Xian, Zeynep Akata, Gaurav Sharma, Quynh Nguyen, Matthias Hein, and Bernt Schiele. Latentembeddings for zero-shot classification. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 6977, 2016.",
  "Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploringa large collection of scene categories. International Journal of Computer Vision, 119:322, 2016": "Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda Zeng, Trishul Chilimbi,and Junzhou Huang. Vision-language pre-training with triple contrastive learning. In Proceedings of theIEEE conference on computer vision and pattern recognition, 2022. Shuo Yang, Zhaopan Xu, Kai Wang, Yang You, Hongxun Yao, Tongliang Liu, and Min Xu. Bicro: Noisycorrespondence rectification for multi-modality data via bi-directional cross-modal similarity consistency.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1988319892, 2023. Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprintarXiv:2111.07783, 2021.",
  ": Detailed hyper-parameters used for in the experimental analysis": "We provide the hyper-parameters employed in the experiments in .We follow most of thehyper-parameters employed in the original CLIP (Radford et al., 2021) paper for both Naive CLIP re-implementation and our multi-token and single-token product sphere implementation. We provide the detailsof the hyper-parameters for large-scale and ablation experiments below.Large-scale Experiments: We train with a batch size of 32,768 and the AdamW optimizer (Loshchilov& Hutter, 2017) in all the large-scale experiments. We apply the standard training scheme of the origi-nal CLIP model, which contains 32 epochs of training. We did not employ mixed precision to reduce thepossible overflow introduced by randomness for a stable reproduction. We set the 1 = 0.9, 2 = 0.998, = 1e-8 in AdamW, and weight decay = 0.2 to further improve the stability. We use the cosine learningrate decay scheme of peak learning rate equal to 5e-4, combined with a warmup period of 2,000 iterations.For data augmentation, we only apply the RandomResizedCrop with a scale range of [0.8, 1.0]. Finally, inour multi-token product sphere implementation, we reduced the maximum temperature to 3.95 due to itsborder distance range. This is a value obtained from the ablation study from Appendix E.Ablation Experiments: We train with a batch size of 2,048 and the AdamW optimizer (Loshchilov &Hutter, 2017) in all the ablation experiments. We apply a compact training scheme that updates the modelfor 108,000 iterations, which is roughly equal to training the model for 15 epochs of the dataset. Since thisis a fast training scheme, we set the 1 = 0.9, 2 = 0.98, = 1e-8 in AdamW, and weight decay = 0.5,such that the training could converge faster in a stable approach. We use the cosine learning rate decayscheme of peak learning rate equal to 5e-4, combined with a warmup period of 5,000 iterations. In the linearprobe evaluation, the hyperparameters follow the setup of MoCo v3 (Chen et al., 2021b). Concretely, weuse SGD without momentum and no weight decay. The learning rate is schemed by cosine decay with apeak learning rate equal to 1.0, combined with a warmup period of 5 epochs. We train for 100 epochs andaugment the image using the RandomResizedCrop with a scale range of [0.75, 1.0] and AutoAugment with thecode rand-m9-mstd0.5-inc1. In the ablation experiments, we do not change the maximum temperatureclip value, leaving it the same for all topology configurations.",
  "Multi(256, 2)tr(uT v)49.232.2930.0461.59Multi(64, 8)tr(uT v)54.034.2731.9362.41Multi(16, 32)tr(uT v)54.033.4330.8863.71": ": The retrieval and classification performance of the proposed approach using different PSlique man-ifold structures and the multi-token implementation. gradient={True/False} donates if the temperatureis learnable. range helps the system reach equilibrium faster. However, an over-complicated structure such as PS(4,128)could ruin the performance. The possible reason is that each sub-sphere Sd1 that is embedded in Rd hasone less effective dimension. Therefore, the product sphere structure with large numbers of sub-spheres mayperform worse. However, an over-complicated structure such as PS(4,128) could ruin the performance.We conjecturethat, since the sphere has one redundant dimension, the larger number of product sub-spheres reduces therepresentation capacity of the topology. And because we employ a textual encoder that is gently larger thanthe visual one, therefore the moderate reduction of the capacity helps overcome the overfitting on the textualside. We also provide the ablation results of different multi-tokens product sphere structure implementationsin the tables lower half, denoted as Multi(, ). We concatenate all the representations together for thelinear probe before projecting them to 1,000 class logit. It can be seen that the multi-token product sphereimplementations outperform their single-token versions most of the time. Notably, since the increased numberof parameters for the class tokens (nd) is negligible compared to that of the overall system, we consider theparticipants of class tokens in global attention as the primary reason for the performance boost, comparedto their single-token version. In addition, we discuss the computational complexity of the multi-token implementation. First, we considerthe cost of the logit matrix of the product sphere. The logit matrix is used to record the distance betweenall pairs of samples, which is considered to be the major source of computational complexity when the batchsize is large (e.g. 231). In our case, given a batch size b, the spherical configure uses b2 float numberswhile the Multi(n, m) structure uses b2m. Second, we provide information on the additional cost brought byadditional class tokens. Since we use the transformer structure, the flops (training/inference time) scale withthe number of parameters time numbers of tokens, plus the square of number of tokens we used. Concretely,given a model with l-layers, d-hidden dimension, and t-tokens, the total flops can be roughly estimated ast l (12 d2 + 2 t d). The number of activations is roughly t l (36 d + 6 t d). Overall, we should use a sufficient amount of sub-sphere to make the distance range able to cover the\"equilibrium\" shown in , intuitively. However, we need additional computational resources that scalewith the (square of) number of tokens. While there is no closed-form solution to the optimal number, weconsider using 8 32 tokens to be a practical choice.",
  "Topology: PS(64, 8),Distance: tr(uT v)2.6592.23124k52.332.8930.7060.325.3102.28057k50.333.3730.2359.761.0002.17436k50.932.7130.5060.661.0001.000Detach30.318.4821.2057.93": ": The retrieval and classification performance of different configurations under different temperatureinitialization conditions.Temp.Init.denotes the values for initializing temperature; Temp.Finaldenotes the final temperature at the end of training; Converge Step denotes the number of steps fortemperature starts to converge (changes less than 2% for an epoch.) less than 2% for an epoch). It can be seen that the performance of the Euclidean topology is only slightlyaffected by the initialization of the temperatures, and even though the temperature is detached from learning,it still performs reasonably well because of the unlimited distance range. At the same time, the sphericaland product sphere topologies are affected by how the temperature is initialized. However, a rough trendcan be seen that the faster the temperature converges, the better performance the model achieves, whichmeans the learnable temperature delays the learning of the methods. The model needs first to find a propertemperature and then begin to learn representations well.",
  "DDistribution of Learned Distance": "We depict the distribution of distance for pairs of samples in . We further employ the RedCaps (Desaiet al., 2021) dataset as the out-domain data for visualizing the distributions of sample distances. As arguedin .2 of the main manuscripts, since the cross-modal contrastive loss does not handle the uni-modaldata distributions, the distance between negative pairs of images and texts could be much smaller thanthat of a positive image-text pair, resulting in a tighter distance bound. Also, we can see this phenomenonis much more severe in out-domain data, which could reduce the transferability of the feature embeddingsto downstream tasks. It is also notable that, the product sphere with the negative inner product as thedistance function learns similar distributions compared to the sphere reference, while the numerical valuesof distances between samples are inherently larger without having multiplied with temperature.",
  "(b)": ": Visualization of the distribution of distances between samples. The logits_pos and logits_negdenote the distances between positive and negative image-text pairs, respectively.The img_neg andtext_neg denote the distances between negative image-image and text-text pairs, respectively. The modelsare trained using the Yfcc datasets, (a) and (b) depict the distribution of in-domain data (Yfcc) and out-domain data (RedCaps), respectively.",
  "FAdditional Results Using the TCL Framework": "We combine our proposed method with the TCL model (Yang et al., 2022), which is one of the state-of-the-art vision-language retrieval models that employ contrastive visual-textual alignment in its earlier stage.During the pre-training, the TCL induces a mixture of in-modal and cross-modal contrastive losses, whileconducting the masked language modeling (MLM) and image-text matching tasks simultaneously. Duringthe testing, the cross-modal contrastive alignment head first lists sample pairs with high similarity scores,and then these pairs are fed into the matching head to obtain the final matching scores. We alternate thetopologies of all the embedding spaces with PS(128,2). For the experimental analysis in this subsection,we follow the configurations of the reference models, employ a collection of CC3M (Sharma et al., 2018),MSCOCO Captions (Chen et al., 2015), Visual genome (Krishna et al., 2017) and SBU (Ordonez et al.,",
  "TopologyDistanceZero-ShotI2T R@1Zero-ShotT2I R@1Zero-ShotCls. Acc.Linear ProbeCls. Acc": "Temperature, init=e2.64, gradient=TrueSphere(512)uT v48.331.4530.6260.38Sphere(1024)uT v50.732.0529.6060.53PS(128, 8)tr(uT v)49.432.8530.5560.12PS(64, 16)tr(uT v)50.333.2530.3460.16PS(32, 32)tr(uT v)52.333.4730.6260.32 : The retrieval and classification performance of the proposed approach using different oblique man-ifold structures and the multi-token implementation. gradient={True/False} donates if the temperatureis learnable. 2011) as the pre-training dataset, which contains roughly 4 million annotated image-text pairs. The modelsare then evaluated using Flickr30k (Plummer et al., 2015) and MSCOCO Captions (Chen et al., 2015). The results are shown in . Since our method does not affect the matching head, we also reportthe performance of the contrastive alignment head. In general, our method improves the average recallperformance, but the improvement is not significant. We consider the reasons as i) The method (or recentsimilar methods) employs pre-trained vision and language models, as well as a matching head and an MLMhead; hence it is less sensitive to the gradients from the contrastive alignment; ii) The datasets employedfor training contain less noise, while the training is scheduled with an overlength scheme (the zero-shotperformance does not increase in the last 5 epochs). Additional Notes on TCL We also provide the comparison results with officially released checkpoints. Itcan be seen that our implementation performs 0.5-1.0% worse than the official checkpoints. On the otherhand, our implementation has better alignment head performance. Since we are employing the codes releasedin the official repository, the reason might be the following: i) Datasets difference, that we have 3000 fewerimages in the SBU dataset while owning 5000 more images in the CC3M dataset; ii) We resize the CC3Mdataset to short edge 500 pixels, while the official repository does not clearly provide the pre-processingapproach; iii) We implicitly have a short training time or smaller matching loss weight than the officialcheckpoints due to the difference in the framework.",
  "GAdditional Results with more datasets": "In , we evaluate our models with more out-domain datasets, which means, the training splits ofthese datasets are not included in our train dataset. We employ the following datasets: Food101 (Bossardet al., 2014), Cars (Krause et al., 2013), Cifar10/100 (Krizhevsky et al., 2009), Country211 (Thomee et al.,2016) (a sub-set of the YFCC100M), FGVCAircraft (Maji et al., 2013), GTSRB (Stallkamp et al., 2011),SUN397 (Xiao et al., 2016), RenderedSST2 (Socher et al., 2013), SVHN (Netzer et al., 2011), STL10 (Coateset al., 2011), DTD (Cimpoi et al., 2014), OxfordIIIPet (Parkhi et al., 2012), Flowers102 (Nilsback & Zisser-man, 2008), EuroSAT (Helber et al., 2019) and Caltech101 (Fei-Fei et al., 2006).",
  "HTest of Mixture-of-Expert Hypothesis:": "We investigate the mixture-of-expert hypothesis of the proposed method. Since the class token is consideredto encode the global representation of the sample, and all the class tokens are identical, i.e. use the samepositional embedding, the employment of multiple class tokens may function in a mixture-of-expert style.That is, after training, each sub-sphere (or a subset of sub-spheres) in the product sphere structure iscapable of alignment. Then, the system functions as a mixture of weak alignment models (experts). To testthis hypothesis, we calculate the zero-shot classification performance of the CLIP[Multi(32,16)] model withrandomly selected subsets of sub-spheres. From , we find that the drop in performance is reasonablysmall (12%) with half of the alignment tokens. This result reveals a possible mechanism of the product",
  "KA note on the recent advance in noisy image-text matching": "Recently, many pieces of research have been made to tackle the noisy image-text matching problem in con-trast learning. Below, we provide a concise survey of these works, for the readers who want to know moreabout this topic. Although these works may not be comparable with our proposed method, they still supportthat noisy visual-textual correspondences are an important research topic in this field.Chun et al. (2022): This paper argues that existing ITM benchmarks have a significant limitation ofmany missing correspondences. Then, it proposes a new dataset, ECCV Caption, to correct the massivefalse negatives and a new metric, mAP@R, to evaluate VL models.Li et al. (2023): This paper proposes a method to correct false negatives by integrating language guidanceinto the ITM framework. This framework corrects the locations of false negatives in the embedding space.Chun (2023): This paper also argues that the image-text matching task suffers from ambiguity due tomultiplicity and imperfect annotations. Then, this paper proposes an improved probabilistic ITM approachthat introduces a new probabilistic distance with a closed-form solution.Huang et al. (2021): This paper points out that the training data may contain mismatched pairs. Tolearn the noisy correspondence, the authors divide the data into clean and noisy partitions and then rectifythe correspondence via an adaptive prediction model."
}