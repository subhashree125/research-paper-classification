{
  "Usman Anwar1": "Abulhair Saparov2, Javier Rando3, Daniel Paleka3, Miles Turpin2, Peter Hase4,Ekdeep Singh Lubana5, Erik Jenner6, Stephen Casper7, Oliver Sourbut8,Benjamin L. Edelman9, Zhaowei Zhang10, Mario Gnther11, Anton Korinek12,Jose Hernandez-Orallo13 Lewis Hammond8, Eric Bigelow9, Alexander Pan6, Lauro Langosco1, Tomasz Korbak14,Heidi Zhang15, Ruiqi Zhong6, Sen higeartaigh1, Gabriel Recchia16, Giulio Corsi1,Alan Chan17, Markus Anderljung17, Lilian Edwards18, Aleksandar Petrov8,Christian Schroeder de Witt8, Sumeet Ramesh Motwani6",
  "indicates major contribution.indicates advisory role": "1University of Cambridge 2New York University 3ETH Zurich 4UNC Chapel Hill5University of Michigian 6University of California, Berkeley 7Massachusetts Institute of Technology8University of Oxford 9Harvard University 10Peking University 11LMU Munich12University of Virginia 13Universitat Politcnica de Valncia 14University of Sussex15Stanford University 16 Modulo Research 17 Center for the Governance of AI18 Newcastle University 19Mila - Quebec AI Institute, Universit de Montral 20Princeton University21University of Toronto 22University of Edinburgh 23University of Washington, Allen Institute for AI",
  "Scientific Understanding of LLMs10": "2.1In-Context Learning (ICL) Is Black-Box. . . . . . . . . . . . . . . . . . . . . . .122.1.1Is ICL Sophisticated Pattern-Matching? . . . . . . . . . . . . . . . . . . . . . . .122.1.2Is ICL Due to Mesa-Optimization? . . . . . . . . . . . . . . . . . . . . . . . . . .132.1.3What Behaviours Can Be Specified In-Context? . . . . . . . . . . . . . . . . . . .142.1.4Scenario-Based Mechanistic Understanding of ICL . . . . . . . . . . . . . . . . .142.1.5Understanding the Effect of the Pre-training Data Distribution on ICL . . . . . .152.1.6Understanding the Effect of Design Choices on ICL . . . . . . . . . . . . . . . . .15 2.2Capabilities Are Difficult to Estimate and Understand. . . . . . . . . . . . . .172.2.1LLM Capabilities May Have Different Shape Than Human Capabilities . . . . .172.2.2Lack of a Rigorous Conception of Capabilities . . . . . . . . . . . . . . . . . . . .172.2.3Limitations of Benchmarking for Measuring Capabilities and Assuring Safety . .192.2.4How Can We Efficiently Evaluate Generality of LLMs? . . . . . . . . . . . . . . .202.2.5Scaffolding Is Not Sufficiently Accounted for in Current Evaluations. . . . . . .21 2.3Effects of Scale on Capabilities Are Not Well-Characterized . . . . . . . . . . .222.3.1Understanding Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .232.3.2Effect of Scaling on Learned Representations. . . . . . . . . . . . . . . . . . . .252.3.3Limits of Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .252.3.4Formalizing, Forecasting, and Explaining Emergence . . . . . . . . . . . . . . . .262.3.5Better Methods for Discovering Task-Specific Scaling Laws. . . . . . . . . . . .27 2.4Qualitative Understanding of Reasoning Capabilities Is Lacking. . . . . . . .292.4.1Does Scaling Improve Reasoning Capabilities?. . . . . . . . . . . . . . . . . . .302.4.2Understanding the Mechanisms Underlying Reasoning . . . . . . . . . . . . . . .302.4.3Understanding Non-Deductive Reasoning Capabilities of LLMs . . . . . . . . . .312.4.4Which Aspects of Training Lead to the Acquisition of Reasoning? . . . . . . . . .322.4.5What Are the Computational Limits of Transformers? . . . . . . . . . . . . . . .32 2.5Agentic LLMs Pose Novel Risks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .332.5.1LLM-agents May Be Lifelong Learners . . . . . . . . . . . . . . . . . . . . . . . .342.5.2Natural Language Underspecifies Goals. . . . . . . . . . . . . . . . . . . . . . .342.5.3Goal-Directedness Incentivizes Undesirable Behaviors. . . . . . . . . . . . . . .352.5.4Difficulty of Robust Oversight and Monitoring. . . . . . . . . . . . . . . . . . .352.5.5Safety Risks from Affordances Provided to LLM-agents. . . . . . . . . . . . . .36 2.6Multi-Agent Safety Is Not Assured by Single-Agent Safety . . . . . . . . . . . .372.6.1Influence of Single-Agent Training on Multi-Agent Interactions is Unclear . . . .372.6.2Foundationality May Cause Correlated Failures . . . . . . . . . . . . . . . . . . .382.6.3Groups of LLM-Agents May Show Emergent Functionality. . . . . . . . . . . .382.6.4Collusion between LLM-Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . .392.6.5Unclear Applicability of Multi-Agent RL Research to LLMs . . . . . . . . . . . .39",
  "Safety-Performance Trade-offs Are Poorly Understood. . . . . . . . . . . . . .41": "2.7.1Designing Better Metrics to Measure Safety . . . . . . . . . . . . . . . . . . . . .412.7.2Disentangling Safety from Performance. . . . . . . . . . . . . . . . . . . . . . .422.7.3Better Characterization of Safety-Performance Trade-offs. . . . . . . . . . . . .422.7.4How Fundamental Are Safety-Performance Trade-offs? . . . . . . . . . . . . . . .42",
  "Development and Deployment Methods44": "3.1Pretraining Produces Misaligned Models . . . . . . . . . . . . . . . . . . . . . . .463.1.1Existing Data Filtering Methods Are Insufficient . . . . . . . . . . . . . . . . . .463.1.2Lack of Dataset-Auditing Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . .473.1.3Improving Training-Data Attribution Methods. . . . . . . . . . . . . . . . . . .473.1.4Scaling Pretraining Using Human Feedback . . . . . . . . . . . . . . . . . . . . .483.1.5Modifying Pretraining to Improve Effectiveness of Downstream Safety and Align-ment Efforts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .48 3.2Finetuning Methods Struggle to Assure Alignment and Safety. . . . . . . . .493.2.1How Does Finetuning Change a Pretrained Model? . . . . . . . . . . . . . . . . .503.2.2Finetuning Misgeneralizes in Unpredictable Ways . . . . . . . . . . . . . . . . . .513.2.3Output-Based Adversarial Training May Incentivize Superficial Alignment . . . .513.2.4Techniques for Targeted Modification of LLM Behavior Are Underexplored. . .523.2.5Removal of Unknown Undesirable Capabilities. . . . . . . . . . . . . . . . . . .52 3.3LLM Evaluations Are Confounded and Biased . . . . . . . . . . . . . . . . . . . .533.3.1Prompt-Sensitivity Confounds Estimation of LLM Capabilities . . . . . . . . . .543.3.2Test-set Contamination Overestimates LLM Capabilities . . . . . . . . . . . . . .543.3.3Targeted Training Confounds Evaluation. . . . . . . . . . . . . . . . . . . . . .553.3.4Biases in LLM-Based Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . .553.3.5Fallibility of Crowdsourced Human Evaluation. . . . . . . . . . . . . . . . . . .553.3.6Systematic Biases in Evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . .563.3.7Challenges with Scalable Oversight . . . . . . . . . . . . . . . . . . . . . . . . . .56 3.4Tools for Interpreting or Explaining LLM Behavior Are Absent or Lack Faith-fulness. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .583.4.1Abstractions Used for Interpretability Are Often Dubious . . . . . . . . . . . . .593.4.2Concept Mismatch between AI and Humans . . . . . . . . . . . . . . . . . . . . .593.4.3Evaluations Often Overestimate the Reliability of Interpretability Methods. . .603.4.4Can Interpretability Methods Maintain Validity When Used to Modify ModelBehavior? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .613.4.5Assuming Linearity of Feature Representation . . . . . . . . . . . . . . . . . . . .623.4.6Polysemanticity and Superposition . . . . . . . . . . . . . . . . . . . . . . . . . .623.4.7Sensitivity of Interpretations to the Choice of Dataset . . . . . . . . . . . . . . .633.4.8Feature Interpretation Is Hard to Scale. . . . . . . . . . . . . . . . . . . . . . .633.4.9Circuit Discovery Is Hard to Scale . . . . . . . . . . . . . . . . . . . . . . . . . .643.4.10 Externalized Reasoning in Natural Language May Be Misleading . . . . . . . . .643.4.11 Externalized Reasoning via Formal Semantics Is Not Widely Applicable . . . . .65 3.5Jailbreaks and Prompt Injections Threaten Security of LLMs . . . . . . . . . .673.5.1Standardized Evaluations of Jailbreak and Prompt Injection Success . . . . . . .683.5.2Efficient and Reliable White-box Attacks for LLMs Are Lacking. . . . . . . . .693.5.3Unifying or Differentiating Jailbreak Attack Methodologies. . . . . . . . . . . .693.5.4Attacking LLMs via Additional Modalities and Defending Against These Attacks703.5.5Defending the LLM as a System: Detection, Filtering, and Paraphrasing . . . . .703.5.6Course-Correction After Accepting a Harmful Request . . . . . . . . . . . . . . .71",
  "There Are No Robust Privilege Levels within the LLM Input . . . . . . . . . . .71": "3.6Vulnerability to Poisoning and Backdoors Is Poorly Understood . . . . . . . .733.6.1Are LLMs Vulnerable to Pretraining Data Poisoning?. . . . . . . . . . . . . . .733.6.2Identifying Robustness and Vulnerabilities of Different Training Stages . . . . . .743.6.3Are Larger Models More Vulnerable to Poisoning Attacks?. . . . . . . . . . . .743.6.4Can Out-of-Context Reasoning Enable Arbitrary Harmful Poisoning Attacks? . .743.6.5Poisoning LLMs through Additional Modalities and Encodings . . . . . . . . . .753.6.6Detecting and Removing Backdoors. . . . . . . . . . . . . . . . . . . . . . . . .75",
  "Sociotechnical Challenges77": "4.1Values to Be Encoded within LLMs Are Not Clear . . . . . . . . . . . . . . . . .804.1.1Justifying Value Choices for Alignment. . . . . . . . . . . . . . . . . . . . . . .804.1.2Managing Conflicts between Different Values. . . . . . . . . . . . . . . . . . . .814.1.3Lotteries May Bias the Values That We Encode . . . . . . . . . . . . . . . . . .824.1.4How Can We Robustly Evaluate Which Values an LLM Encodes?. . . . . . . .824.1.5Is Value Alignment the Right Framework? . . . . . . . . . . . . . . . . . . . . .83 4.2Dual-Use Capabilities Enable Malicious Use and Misuse of LLMs. . . . . . .844.2.1Misinformation and Manipulation. . . . . . . . . . . . . . . . . . . . . . . . . .844.2.2Cybersecurity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .854.2.3Surveillance and Censorship . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .864.2.4Warfare and Physical Harm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .874.2.5Hazardous Biological and Chemical Technologies . . . . . . . . . . . . . . . . . .874.2.6Domain-Specific Misuses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .884.2.7Mechanisms for Detecting and Attributing LLM Outputs Are Lacking . . . . . .88 4.3LLM-Systems Can Be Untrustworthy. . . . . . . . . . . . . . . . . . . . . . . . .904.3.1Harms of Representation and Other Biases. . . . . . . . . . . . . . . . . . . . .904.3.2Inconsistent Performance across and within Domains . . . . . . . . . . . . . . . .904.3.3Overreliance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .914.3.4Contextual Privacy Preservation . . . . . . . . . . . . . . . . . . . . . . . . . . .91 4.4Socioeconomic Impacts of LLM May Be Highly Disruptive . . . . . . . . . . . .924.4.1Effects on the Workforce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .924.4.2Effects on Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .944.4.3Economic Challenges for Education. . . . . . . . . . . . . . . . . . . . . . . . .954.4.4Global Economic Development . . . . . . . . . . . . . . . . . . . . . . . . . . . .95 4.5LLM Governance Is Lacking. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .974.5.1Lack of Scientific Understanding and Unreliability of Technical Tools ComplicateGovernance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .974.5.2Need for Effective, Fast-Moving Governance Institutions . . . . . . . . . . . . . .994.5.3Incentivizing Cooperation and Disincentivizing High-Risk Approaches to AI De-velopment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1004.5.4Corporate Power May Impede Effective Governance. . . . . . . . . . . . . . . . 1004.5.5LLMs Require International Governance . . . . . . . . . . . . . . . . . . . . . . . 1014.5.6Culpability Schemes Are Needed for LLM-Based Systems Especially LLM-Agents1024.5.7Use-Based Governance May Be Insufficient. . . . . . . . . . . . . . . . . . . . . 1034.5.8Deployment Governance Lacks Adequate Regulation . . . . . . . . . . . . . . . . 1044.5.9Development Governance Might Be Particularly Challenging to Codify and Enforce1054.5.10 LLMs Pose Additional Challenges for Data Governance. . . . . . . . . . . . . . 1054.5.11 Robustness of Compute Governance is Unclear . . . . . . . . . . . . . . . . . . . 106",
  "Readers Guide": "Due to the length of this document (though note that the main content is only ~100 pages; the restare references), it may not be feasible for all readers to go through this document entirely. Hence, wesuggest some reading strategies and advice here to help readers make better use of this document. We recommend all readers begin this document by reading the main introduction () to graspthe high-level context of this document. To get a quick overview, readers could browse the introductionsto various categories of the challenges (i.e. Sections 2, 3 and 4) and review associated Tables 1, 3 and 4that provide a highly abridged overview of the challenges discussed in the three categories. From thereon, readers interested in a deep dive could pick any section of interest. Note that all the challenges (i.e.subsections like .1) are self-contained and thus can be read in an arbitrary order.",
  "Machine Learning and NLP Researchers": "Technical researchers in machine learning, natural language processing, and other associated fieldsare the primary intended audience for this agenda. We have tried to assume as minimal backgroundknowledge as possible beyond the general knowledge of what LLMs are, what their architecture is,and how they are trained. Hence, we expect all the technical challenges in Sections 2 and 3 to beaccessible to any person with the knowledge equivalent to a first-year graduate student in machinelearning or natural language processing. A large proportion of the challenges discussed in arealso technical in nature and should be equally accessible. The main intended purpose of this document is to help junior researchers, or researchers new to thisarea, to identify promising and actionable research directions (although of course, even seasoned expertsmight take inspiration from it). Such readers are encouraged to pick and choose sections that best alignwith their interests. The 200+ listed research questions are each meant to be roughly the size of aproblem that could form the basis of a research dissertation. For each challenge and subchallenge,we provide motivation, background, and related work, before discussing directions for future research.These should provide a good starting point for researchers who are new to these particular challenges,but we do not attempt a comprehensive survey of any area. We also note that while this work is motivated by the safety and alignment of LLMs, nonetheless, manyof the challenges we identify are highly interesting from the technical and scientific points of view. Thus,even those readers who are not primarily motivated by safety, but are in search of interesting problemscentered on LLMs, may find this document useful.",
  "Sociotechnical Researchers and Other Stakeholders": "We focus on sociotechnical challenges in , emphasizing that all LLMs are sociotechnical sys-tems, and their safety cannot be ensured without a deep and thoughtful consideration through this lens.The introduction of this section provides a mapping between the different challenges we discuss andthe different areas of other fields that could contribute to progress on those challenges. This sectiononly presumes high-level familiarity with the LLMs for the most part and is aimed to be accessible toa wider audience than the rest of the agenda.",
  "Alan Turing": "Large language models (LLMs) have emerged as one of the most powerful ways to solve open-endedproblems and mark a paradigm shift within machine learning.However, assuring their safety andalignment remains an outstanding challenge that is recognized across stakeholders, including privateAI laboratories (Leike et al., 2022; Anthropic, 2023a; Frontier Model Forum, 2023), national and inter-national governmental organizations (White House, 2023; Office, 2023; Board, 2023), and the researchand academic communities (Bengio et al., 2023; FAccT, 2023; CAIS, 2023; CHAI, Far.ai, and DitchleyFoundation, 2023). Indeed, assuring the safety and alignment of any deep-learning-based system isdifficult (Ngo et al., 2023). However, this challenge is much more acute for LLMs due to their expan-sive scale (Sanh et al., 2019, ) and increasingly broad spectrum of capabilities (Bubeck et al.,2023; Morris et al., 2023). Furthermore, the rapid advances in LLM capabilities not only expand thepotential applications of LLMs, but also increase their potential for societal harm (Weidinger et al.,2021; Ganguli et al., 2022; Birhane et al., 2023; Chan et al., 2023a).",
  "Why This Agenda?": "The rapid rate of progress is especially alarming due to the absence of the requisite technical toolsand deficiencies in the sociotechnical structures that may help assure that LLMs are developed anddeployed safely (Bengio et al., 2023).In this work, we map out the challenges in developing theappropriate technical affordances that may help assure safety and in understanding and addressing thesociotechnical challenges that we may face in assuring societal-scale safety. At its heart, this work is acall to action for machine learning researchers, and researchers in the associated fields. Our extensivereferencing of contemporary literature, focus on identifying promising and concrete research directions,and in-depth discussion of each challenge makes it an ideal educational resource for newcomers to thefield. At the same time, we expect the plethora of challenges identified in this work to act as a source ofinspiration for current practitioners in the fields of LLM alignment and safety, including those workingfrom diverse other disciplines (e.g. social sciences, humanities, law, policy, risk analysis, philosophy,etc.). Several prior studies have compiled and discussed foundational problems in AI Safety (Amodei et al.,2016; Hendrycks et al., 2021a; Critch and Krueger, 2020; Kenton et al., 2021; Ngo et al., 2023). However,LLMs mark a paradigm shift and present many novel and unique challenges in terms of alignment, safety,and assurance that are not discussed in these works. Among these, Kenton et al. (2021) is the only workthat exclusively focuses on LLMs. But it lacks broad coverage, being narrowly focused on issues fromaccidental misspecification of objectives. Our work builds on the aforementioned work and provides themost comprehensive and detailed treatment of challenges related to the alignment and safety of LLMsto date. We highlight 18 different foundational challenges in the safety and alignment of LLMs and providean extensive discussion of each. Our identified challenges are foundational in the sense that withoutovercoming them, assuring safety and alignment of LLMs and their derivative systems would be highlydifficult. For this work, we have further prioritized the discussion of foundational challenges that areunambiguous (i.e. not speculative), prime for research and highly relevant to harms and risks posedby current and forthcoming LLMs. Additionally, we pose 200+ concrete research questions for furtherinvestigation. Each of these is associated with a particular fundamental challenge.These researchquestions are fairly open-ended and are roughly meant to be the size of a graduate thesis, althoughmany offer multiple angles of attack and could easily be studied more exhaustively.",
  "Terminology": "The terms alignment, safety and assurance have different meanings depending on the context. We usealignment to refer to intent alignment, i.e. a system is aligned when it is trying to behave as intendedby some human actor (Christiano, 2018).1 Importantly, alignment does not guarantee a system actuallybehaves as intended; for instance, it may fail to do so due to limited capabilities (Ngo et al., 2023). Tofurther simplify our discussion, we fix the intent to be that of the LLM developer (Gabriel, 2020; Ngoet al., 2023) (e.g. as opposed to the user). We consider a system safe to the extent it is unlikely tocontribute to unplanned, undesirable harms (Leveson, 2016). This is a somewhat expansive definition,accounting not only for the technical properties of the system, but also the way in which it is (or is likelyto be) deployed and used (Weidinger et al., 2023a), though it is narrow in the sense that it does notconsider intentional harm, and it does not set out any criteria for what constitutes harm. Alignmentcan be used to increase safety, but the relationship is not straightforward: it could also be used to makea system more dangerous (if the developer intends), and it is not directly concerned with the real-worldimpact a system has when embedded in its deployment context, or the very important issue of (lackof) alignment between developers and other stakeholders. Finally, by assurance, we mean any way ofproviding evidence that a system is safe or aligned (Ashmore et al., 2021), including, but not limitedto: scientific understanding of the system, behavioral evaluations, explanations of the model behavioror internal processing, and adherence to responsible development practices by the system developer(Casper et al., 2024a).",
  "Structure": "We organize the foundational challenges into three different categories. The first category, scientificunderstanding of LLMs (), surveys the most important open questions that can help us builda better theory of how LLMs function and inform development and deployment decisions. We discussthe need to develop principled solutions to conceptualizing, estimating, understanding, and predictingthe capabilities of LLMs. We single out in-context learning and reasoning capabilities of LLMs as beingcritical to understand for assuring alignment and safety across all contexts. We highlight that riskswe are facing with current LLMs may grow manifold with the introduction of LLM-agents, and thatwe need to pre-emptively understand these risks and work to mitigate them across both single-agentand multi-agent scenarios. Finally, we note that it may be inevitable that safety-performance trade-offsexist for LLM-based systems that we ought to understand better. The second category, Development and Deployment Methods (), presents the known limita-tions of existing techniques in assuring safety and alignment in LLMs. We identify opportunities tohelp improve model alignment by modifying the pretaining process to produce more aligned models,survey several limitations of finetuning in assuring alignment and safety, discuss issues underlying theevaluation crisis, review challenges in interpreting and explaining model behavior, and finally providean appraisal of security challenges like jailbreaks, prompt-injections, and data poisoning. On the whole,this section pertains to researching empirical techniques that may help improve the alignment, safety,and security of LLMs. The final category, Sociotechnical Challenges (), focuses on challenges that require a morediverse and holistic lens to address. For example, we discuss the importance of societal-level discussionsof whose values are encoded within LLMs, and how we can prevent value imposition and enable valueplurality. Many LLM capabilities are dual-use; there is a need to understand what malicious misuse suchcapabilities might enable and how may we guard against them. There is also a need to ensure that thebiases and other issues of LLM-systems are independently and continually monitored and transparentlycommunicated, to build trustworthiness and reduce over-reliance. The proliferation of LLMs throughoutsociety may have undesirable socioeconomic impacts (e.g. job losses, increased inequality) that ought",
  "to be investigated better and strategized against. Finally, we conclude by discussing the challenges andopportunities in the space of governance and regulation": "As an addendum, in we review some limitations of this work. Most notably, we note thatwhile comprehensive, this agenda is not exhaustive. We follow that up with a detailed overview of theprior works broadly related to this work; including but not limited to prior agendas on AI safety andvarious surveys related to LLMs.",
  "Scientific Understanding of LLMs": "Many safety and alignment challenges stem from our current lack of scientific understanding of LLMs.If we were to understand LLMs better, this would help us better estimate the risks posed by current,and future, LLMs and design appropriate interventions to mitigate those risks. Scientific understandingis also an essential component of assurance especially for complex systems. Some classic examples ofcomplex systems that heavily rely on scientific understanding for safety and assurance are bridge design,aircraft design, nuclear reactor design, spacecraft design etc. Without the scientific understanding ofthe underlying physics, the assurance of these systems would perhaps be improbable, if not totallyimpossible. LLMs show many prototypical traits of complex systems (Holtzman et al., 2023; Steinhardt, 2023;Hendrycks, 2023, Chapter 5) the foremost among them is emergent behaviors (Wei et al., 2022a;Park et al., 2023a). This complex-systems like nature of LLMs means that relying on evaluationsalone may be insufficient for safety and assurance (c.f. Sections 2.2.3 and 3.3) and that there is a direneed to probe beyond surface-level behaviors of LLMs and to understand how those behaviors arise inthe first place (Holtzman et al., 2023). Understanding LLMs is a broad and grand scientific challenge. However, in line with the general themeof this work, we focus on the most safety-relevant aspects (see for an overview). Addressingthese challenges will help inform safer LLM development or deployment practices; however, additionalwork would be required to translate insights here into practical recommendations. Scientific understanding can take many different, and diverse, forms (Adams et al., 2014, ).Indeed, the challenges we have identified admit diverse styles of research. While some challenges aredeeply theoretical in nature (e.g. What Are the Computational Limits of Transformers? or Understand-ing Scaling Laws), others may need a more empirical approach towards resolving them (e.g. Influence ofSingle-Agent Training on Multi-Agent Interactions is Unclear). In general, most of the challenges thatwe raise are focused on developing a qualitative understanding of LLMs. Qualitative understanding of-ten allows developing generalizations that a quantitative analysis may not permit and thus may be morerobust to the rapid advancements in LLMs. These virtues of qualitative understanding were extolledalmost 50 years ago by Herbert Simon and Allen Newell in their seminal 1975 Turing Award lecture,incidentally also on the topic of designing and understanding artificial intelligence systems (Newell andSimon, 1976). : An overview of challenges discussed in (Scientific Understanding of LLMs).We stress that this overview is a highly condensed summary of the discussion contained withinthe section, and hence should not be considered a substitute for the complete reading of thecorresponding sections.",
  "Capabilities Are Difficult to Estimate andUnderstand": "Correctly estimating and understanding capabilities ofLLMs is difficult for various reasons. Firstly, LLM capa-bilities appear to have a different shape than human ca-pabilities; meaning that the notions used to understandand estimate human capabilities might be ill-suited tounderstand LLM capabilities. Additionally, the conceptof capabilities lacks a rigorous conceptualization whichmakes it difficult to make, and evaluate, formal claimsabout LLM capabilities.There also exist fundamentalflaws in our evaluation methodologies that ought to beovercome if we are to better understand LLM capabili-ties, such as benchmarking being unable to differentiatebetween alignment failures and capability failures. Thereis also a need to improve our tooling to evaluate the gen-erality of LLMs, and in general, develop methods to bet-ter account for scaffolding in our evaluations.",
  "Effects of Scale on Capabilities Are NotWell-Characterized": "Various challenges hinder our ability to understand andpredict the impact of scale on LLM capabilities. Theseinclude incomplete theoretical understanding of empiricalscaling laws, limited understanding of limits of scalingand how learning representations are affected by scaling,confusing discourse on emergent capabilities due to lackof formalization, and the nascent nature of research intothe development of better methods for discovering task-specific scaling laws.",
  "Qualitative Understanding of ReasoningCapabilities Is Lacking": "Our current understanding of how reasoning capabilitiesemerge in LLMs and are impacted by model scale is in-sufficient for making confident predictions about the rea-soning capabilities of future LLMs. There is a need for re-search to understand the mechanisms underlying reason-ing, develop a better understanding of the non-deductivereasoning capabilities of LLMs, and better understandthe computational limits of the transformer architecture. Agentic LLMs Pose Novel RisksFor reasons including increased capabilities (via enhance-ments like access to various affordances) and increasedautonomy, LLM-agents may pose novel alignment andsafety risks. The actions executed by LLM-agents mayresult in negative side-effects due to underspecification innatural-language-based instructions.Goal-directednessmay cause LLM agents to exhibit undesirable behaviorssuch as reward hacking, deception, and power-seeking,and might make robust oversight and monitoring of LLM-agents particularly difficult.",
  "Multi-Agent Safety Is Not Assured bySingle-Agent Safety": "Assuring favorable outcomes in a multi-agent setting mayprove challenging for several reasons. Firstly, theres alack of comprehensive understanding of how single-agenttraining affects the behavior of LLM-agents in multi-agent environments.Secondly, the foundationality ofLLM-agents may contribute to correlated failures. Addi-tionally, collusion among LLM agents may result in un-desirable externalities. Lastly, it is unclear to what ex-tent prior research in multi-agent reinforcement learningmay prove helpful in improving the alignment of LLM-agents in multi-agent settings, especially for resolving so-cial dilemmas.",
  "Safety-PerformanceTrade-offsArePoorly Understood": "Safety-performance trade-offs are typically unavoidablein the design of any engineering system; however, theyare not well understood for LLM-based systems. Thereis a need for work to design better metrics to mea-sure safety, to characterize safety-performance trade-offsacross various contexts, and to better understand whatsafety-performance trade-offs are fundamental in nature(and hence unavoidable in practice). Finally, it may behelpful to research methods for producing Pareto im-provements in both safety and performance.",
  "In-Context Learning (ICL) Is Black-Box": "In-context learning (ICL) is the ability of an LLM to learn to perform a novel task, or improve onan existing task, based on the information (e.g. examples, reasoning traces) provided in the promptwithout any explicit updates to the models parameters (Brown et al., 2020; Kaplan et al., 2020). ICLis a highly flexible and efficient learning paradigm it has been used to direct LLMs to behave inthe desired way (Lin et al., 2023a), jailbreak LLMs (Wei et al., 2023a; Xhonneux et al., 2024) andcreate highly performant LLM-agents (Wang et al., 2023a). However, while this dynamic natureof ICL is helpful in the design of proficient LLM-based systems, the black-box natureof ICL also makes it significantly harder to assure safety and alignment of LLMs (Wolfet al., 2023; Millire, 2023). If we do not understand how ICL works, this makes it difficult topredict how it might alter LLMs behavior in deployment; for instance, it might enable novel dangerouscapabilities or bypass safeguards (Anil et al., 2024). Thus there is a pressing need to better understandthe mechanisms underlying ICL, the limits of ICL, and the safety implications associated with ICL. Thissection presents the issues with various theories and approaches that have been proposed to explainICL and highlights several key research questions that need to be addressed. 2.1.1Is ICL Sophisticated Pattern-Matching?There are two competing theories to explain the working mechanism of ICL in a transformer. The firstis that ICL is a set of pre-learned pattern-matching heuristics closely tied to the training distribution.Under this view, several works have proposed explanations of ICL as inference over an implicitly learnedtopic model (Xie et al., 2022; Wang et al., 2023b; Wies et al., 2023); as task inference (Min et al.,2022; Todd et al., 2023; Hendel et al., 2023; Bigelow et al., 2023) or as learning of template circuits : Recent large language models (LLMs) offer significantly expanded context lengths, enhancingin-context learning capabilities (Agarwal et al., 2024) while also introducing novel risks (Anil et al.,2024). (during training) which are adaptively retrieved and rebound to tokens in the prompt (Swaminathanet al., 2023).However, these theories currently only provide partial explanations of ICL. All theaforementioned works only explain ICL that is based on demonstrations while ICL can occur fromother types of feedback as well, e.g. interactive feedback (Mehrabi et al., 2023; Wang et al., 2023a).These works also do not explain multi-task in-context learning and sequential learning of tasks in-context (Zhou et al., 2022, Sections 4&5), or how in-context learning may support learning of noveltasks, such as learning to reason on OOD samples (Saparov et al., 2023). Furthermore, some works,such as Zhang et al. (2023a) and Swaminathan et al. (2023), assume specific (simpler) data generationprocesses that differ from the true data generation process of natural language. There is a need tofurther refine and extend the current theories, or development of novel ones, so that we are able toadequately explain the full range of ICL behaviors, including the aforementioned ones.",
  "Is ICL Due to Mesa-Optimization?": "Alternatively, ICL can be seen as a form of learned optimization or mesa-optimization (Hub-inger et al., 2019; von Oswald et al., 2023), i.e., during training, the base optimizer learns to use thetransformer weights to represent another (learned) optimization algorithm as well as a (learned)objective function effectively allowing the transformer to self-generate learning signal (based ondata given in the context) and act as black-box in-context learner (Kirsch et al., 2022). Emergenceof mesa-optimization within a model is contingent on whether or not a model can implement a learn-ing algorithm within its weights. Several studies provide evidence that transformers can approximategradient-based learning algorithms for various statistical learning problems (Akyrek et al., 2022a; vonOswald et al., 2022; Garg et al., 2022; Zhang et al., 2023b; Ahn et al., 2023). Bai et al. (2023a) provethat a transformer with 2L layers can simulate L-steps of gradient descent on a two-layer feedforwardneural network. Panigrahi et al. (2023) propose an extension of the transformer architecture that caninternally simulate finetuning of a smaller transformer. Bai et al. (2023a) further show that trans-formers can implement in-context algorithm selection, i.e. at inference time, a single transformer canadaptively choose between different learning algorithms to solve the given task (e.g. logistic regressionfor a regression task or logistic classification for a classification task) based on the information given inthe prompt. These studies show that in principle, transformers are capable of mesa-optimization in controlled ex-periments. However, it is not yet clear whether and when transformers trained with more complexobjectives, e.g. the language modeling objective, might learn to perform mesa-optimization. Specif-ically, one key unknown is: for which mesa-objectives do transformers support mesa-optimization?Existing work provides overwhelming evidence that transformers can solve simple supervised learningtasks via mesa-optimization. However, there has so far been very limited investigation as to whethertransformers can solve more complex learning tasks in-context via mesa-optimization (Lin et al., 2023b);research should explore tasks that better mirror the real-world structure of language modeling. Further-more, even for otherwise well-studied learning tasks e.g. linear regression, there is disagreement in thecurrent literature as to which learning algorithm is implemented by the transformer (von Oswald et al.,2022; Fu et al., 2023a). In order to resolve this disagreement, further research is required to disentanglevarious factors that may impact which learning algorithm gets implemented by the transformer (Zhonget al., 2023a). Future work could also develop more rigorous and generic methods for distinguishingmesa-optimization from other forms of ICL, and examine the practical importance of the distribution ofin-context examples in determining whether mesa-optimization occurs and understanding the inductivebiases of the resulting mesa-optimizer.",
  "What Behaviours Can Be Specified In-Context?": "It is currently unclear what functions can, and can not be learned, in-context. More specifically, givena pre-trained model, what behaviors can it be prompted to do? If it is possible to always find a promptto coax the LLM to perform any task, then that might indicate that jailbreaking (c.f. .5)is an impossible problem to solve (Millire, 2023).Fundamentally, that is a question of universalapproximation in-context, i.e. whether a fixed model can be converted into an approximator of arbitraryaccuracy for any (continuous) function by being prompted appropriately. While it is well-known thattransformers are universal approximators when trained (Yun et al., 2019) and was recently shown thatthis is also the case for state-space models (Li et al., 2022a; Wang and Xue, 2023; Cirone et al., 2024),it is less clear what are their in-context approximation abilities.Wang et al. (2023c) showed thatin-context universal approximation is possible with a transformer but their construction required thatall possible functions are encoded in the model weights which results in unrealistically large models.However, (Petrov et al., 2024) showed that no memorization is needed and that a relatively smallmodel can be a universal approximator in-context. This result already indicates that it is possiblefor a realistically-sized model to be impossible to be safeguarded (i.e. safety finetuned in a way thatjailbreaking is not possible). The theory of universal approximation in-context is still rather underdeveloped and the practical safetyand security implications of the above results are not yet fully clear. For instance, the above-mentionedmodels require very specific hand-crafted weights for the pre-trained model. It is not clear whether theuniversal approximation behavior can be obtained by learning via gradient descent on typical datasetsand, therefore, whether it is likely to occur in real-world models or not. Furthermore, the prompt lengthsrequired in the construction of Petrov et al. are unrealistically long. However, it might be possibleto reduce this by leveraging knowledge and skills already present in the model (Petrov et al., 2023a).Therefore, studying the effect of the pre-training data on the in-context universal approximation couldhelp understand the real-world safety and security implications of in-context learning capabilities ofLLMs. All the above results also rely on the attention mechanism in the transformer architecture andthus, do not translate to other recurrent models. Thus, understanding the in-context approximationproperties of other architectures is another open problem with little to no current work so far (Leeet al., 2023a).",
  "Current interpretability techniques are not scalable and general enough to allow an interpretability-based general understanding of the mechanics of ICL within an LLM (c.f. .4). However, they": "can still be leveraged for developing scenario-based mechanistic understanding of ICL (Olsson et al.,2022; Reddy, 2023), i.e. identifying circuits critical for ICL within an LLM when artificial restrictionsare placed on the prompt structure and the task being carried out via ICL. Such case studies canbe insightful for understanding the relative roles played by different computational structures withinthe model, and to understand how the ICL mechanism varies across tasks. For example, Todd et al.(2023) studied ICL for extractive and abstractive NLP-based tasks, and found that a small number ofattention heads, termed function vectors, are responsible for transporting a compact representationof a task which then triggers execution of the task. Similarly, Merullo et al. (2023a) show that onthe commonly-studied ICL task of inferring relations between entities (e.g. inferring the capital of acountry), mid-to-late feedforward layers play a key role in identifying and surfacing relevant itemscontained in the context which are required for inferring the relation and completing the task. Halawiet al. (2023) study ICL on a classification task in which the demonstration data has incorrect labels,thus, clashing with the prior knowledge of the LLM. This lets Halawi et al. discover false inductionheads; attention heads in late layers of the LLM that attend to and copy false information from thedemonstrations. This preliminary evidence suggests that case studies on specific task types can be a useful techniqueto build a mechanistic understanding of ICL in LLMs and may be helpful in identifying how, and why,ICL behavior varies across tasks, and how the inductive biases of a particular architecture affects ICL.Future research may analyze ICL in larger LLMs, on diverse task types and with various prompt styles.Interpretability-based techniques could also be leveraged to explain idiosyncrasies of ICL identifiedin the literature, e.g. why ICL sometimes performs worse when given examples from test distributions(Saparov et al., 2023) or why LLMs at different scales process false information in the context differently(Wei et al., 2023b).",
  "Understanding the Effect of the Pre-training Data Distribution on ICL": "Emergence of in-context learning is heavily modulated by the structure of the pre-training data distri-bution. Special properties of the task distribution, such as task diversity (Ravents et al., 2023; Kirschet al., 2022), burstiness (Chan et al., 2022) or compositional structure (Hahn and Goyal, 2023) maybe key factors in the emergence of ICL. However, these findings are primarily limited to relatively simplesettings, and further work is required to verify their correctness in the real-world language modelingsetup. Which of these criteria are actually fulfilled by large-scale text datasets? If all of these criteriaare indeed met by text datasets, which criteria are actually responsible for ICL in LLMs? For someof these criteria (e.g. burstiness), these questions could be answered via analysis of popular datasetslike The Pile (Gao et al., 2020), RedPajama (Together Computer, 2023), etc. For other criteria, such astask diversity, the analysis-based approach may not be suitable as there might not be an obvious way totrack and measure such criteria in an unstructured text dataset. In such cases, an alternative strategycould be to create various synthetic text datasets in a controlled fashion and monitor the emergenceof ICL in language models trained on these datasets to better understand the necessary and sufficientconditions for the emergence of ICL.",
  "Understanding the Effect of Design Choices on ICL": "How is ICL impacted by various design and training choices involved in the development of an LLM,such as model size, pretraining dataset size, pretraining compute, instruction tuning? Sensitivity ofICL to various factors, including, but not limited to the aforementioned factors, is well-known fromprior literature. In particular, several studies note that ICL performance is highly sensitive to thescale of the LLM; Wei et al. (2022a); Brown et al. (2020); Kaplan et al. (2020) note that ICL is anemergent ability; Akyrek et al. (2022a) discover phase transitions between transformer simulatingdifferent learning algorithms based on the depth of the transformer; and Wei et al. (2023b) find thatlarger LLMs have stronger semantic priors (i.e. zero-shot performance is better), but also a greaterpropensity to allow in-context information to override the semantic prior. Wei et al. (2023b) further show that instruction tuning disproportionately strengthens semantic priors; hence, an instruction-tuning reduces the propensity for in-context information to override the semantic prior. Meanwhile,Singh et al. (2023) argue that ICL is a transient phenomenon and extended training can cause ICLto dissipate in favor of in-weight learning (Chan et al., 2022). A deep understanding of why ICL issensitive to various design and training choices, and how these choices affect the mechanisms behind ICLis currently lacking. In particular, improving our understanding of how various training design decisions,e.g. instruction tuning or prolonged training, impact ICL may provide us with tools to modulate thestrength of ICL in LLMs. This may help mitigate some of the safety risks posed by LLMs due to theirstrong in-context learning abilities (Wolf et al., 2023; Millire, 2023). The dynamic and flexible nature of ICL is central to the success of LLMs as it allows LLMs toproficiently improve on already known tasks as well as learn to perform novel tasks. It is likelyto gain an even more prominent role as LLMs are scaled up further and become more proficientat ICL. However, the black box nature of ICL is a risk from the perspective of alignment andsafety, and there is a critical need to better understand the mechanisms underlying ICL. Severaltheories have been proposed that provide plausible explanations of how ICL in LLMs mightwork. However, the actual mechanism(s) underlying ICL in LLMs is still not well understood. Wehighlight several research questions that could be instrumental in addressing the understandingof the mechanisms underlying ICL.",
  ". CandifferenttheorizationsofICLassophisticatedpattern-matchingormesa-optimization be extended to explain the full range of ICL behaviors exhibited by theLLMs?": "2. What are the key differences and commonalities between ICL and existing learningparadigms? Prior work has mostly examined ICL from the perspective of few-shot su-pervised learning. However, in practice, ICL sometimes exhibits qualitatively distinctbehaviors compared to supervised learning and can learn from data other than labeledexamples, such as interactive feedback, explanations, or reasoning patterns. 3. Which learning algorithms can transformers implement in-context? While earlier stud-ies (e.g. Akyrek et al., 2022a) argue transformers implement gradient descent-basedlearning algorithms, more recent work (Fu et al., 2023a) indicate that transformers canimplement higher-order iterative learning algorithms e.g. iterative Newton method aswell. 4. What are the best abstract settings for studying ICL that better mirror the real-worldstructure of language modeling and yet remain tractable?Current toy settings e.g.learning to solve linear regression are too simple and may lead to findings that do nottransfer to real LLMs. 5. To what extent, different architectures are universal approximators in-context? Canwe better characterize functions that can be learned in-context by models obtained inpractice? How do the pre-training data and the training objectives affect the in-contextuniversal approximation abilities of a model? 6. How can interpretability-based analysis contribute to a general understanding of themechanisms underlying ICL? Can this approach be used to explain various phenomenaassociated with ICL such as why ICL performance varies across tasks, how the inductivebiases of a particular architecture affect ICL, how different prompt styles impact ICL,etc.?",
  "Capabilities Are Difficult to Estimate and Understand": "Improving and especially assuring the safety and alignment of LLMs demands an understandingof their capabilities. More capable systems possess a greater potential to cause harm under misalign-ment; as argued by Shevlane et al. (2023). At the same time, some capabilities such as recognizingambiguity and uncertainty, and understanding how to infer humans intent are necessary for ad-vancing LLM safety and alignment (respectively). This makes it critical that we correctly estimate andunderstand the capabilities of an LLM (Mitchell, 2023). However, there is currently no singlewell-established and agreed-upon conceptualization, definition, or operationalization ofthe term capabilities. This, combined with various other factors, hinders rigorous ac-counting of risk, which should be grounded in an understanding of what types of behaviora system is capable of in general, rather than the specific behaviors a system exhibitedin the particular situations in which it was tested (Kaminski, 2023). To provide high-qualityassurance of LLM safety or alignment, we need an improved scientific understanding of how to inferunderlying capabilities from such limited test results.",
  "LLM Capabilities May Have Different Shape Than Human Capabilities": "The capabilities of LLMs, and other AI models, are likely to be mechanistically and behaviorally distinctfrom the corresponding human capabilities even when some summary statistics for the two may beclosely matched, e.g. accuracy on some benchmark.We colloquially refer to this as the shape ofcapabilities being different.2 Adversarial examples in computer vision are a classical example of thisdifference small perturbations to images that are imperceptible to humans can have drastic effects ona model based on neural networks (Szegedy et al., 2013). Within LLMs, one obvious way this manifestsis as inconsistent performance across data points on which a humans performance would be consistent.For example, GPT-4 accuracy at a counting task degrades significantly when the correct answer is alow probability number (e.g. 83) relative to the cases where the correct answer is a high probabilitynumber (e.g. 100) (McCoy et al., 2023). One other way this mismatch becomes obvious is by lookingat tasks that LLMs can do with much greater efficiency than humans. For example, Gemini-1.5 canlearn to translate sentences from English to a completely novel language (Kalamang) in-context giveninstructional material (Gemini Team, 2024). However, it may take a human several weeks to learn toperform the same translations (Tanzer et al., 2023). This mismatch in the shape of capabilities can have adverse consequences. It makes it difficult forhumans to simulate LLMs and predict their behavior (Carlini, 2023a). This may harm a users trustin LLMs (c.f. .3) and can generally make assurance harder, as LLM behavior can change inarbitrary ways across the input space. Considering this mismatch, we also caution against the carelessuse of tests designed for humans to estimate LLM capabilities, as argued by Davis (2014). In general,this issue of mismatch indicates that conceptualizations used to describe human capabilities may beill-suited to describe capabilities of LLMs (and AI models in general). However, the methodology usedto identify human capabilities may still transfer or could be used as a source of inspiration as we hintat in the following section.",
  "Also see related discussion on how AI models learn and use different concepts and representations than humans in Sec-tion 3.4.2": "an implicit conceptualization of capabilities within the community that associates a model having acapability with the model being able to perform well on tasks of some particular type (Shevlane et al.,2023, ). This is generally operationalized by collecting (large) number of samples representativeof the task of interest into a benchmark. However, benchmark performance is highly dependent onwhich particular samples are chosen, i.e. what parts of input space are covered, and how densely theyare sampled from. Indeed, depending on what samples they evaluate on, different works draw differentconclusions about the capability of different LLMs, e.g. to use theory-of-mind reasoning (Ullman, 2023;Zhou et al., 2023a; Shapira et al., 2023; Kim et al., 2023). Conflicting claims about models capabilitiesare typically adjudicated informally; new research may exhibit surprising failure modes to demonstratethat a capability is not robust, or explain away behavior that seems to demonstrate a general capabilitywith evidence that a models performance on a particular benchmark is due to peculiarities of thesamples selected, e.g. the existence of shortcut features (Geirhos et al., 2020). This informal treatment of capabilities is insufficient, and there is a need for more rigorous treatmentfor the purposes of assurance. For instance, to enable us to make and evaluate formal claims about anLLM lacking dangerous capabilities (Shevlane et al., 2023). To make rigorous, scientifically sound, andgeneral claims about the presence or absence of a capability within a model, it is essential to establishrigorously defined and commonly agreed upon conceptualizations of capabilities (Jain et al., 2023a).More specifically, we might be interested in claims of three different types: (a) a capability is completelyabsent from a model; (b) a capability is partially present within a model; and (c) a capability is robustlypresent within a model. Research is needed on how to best define, operationalize, and evaluate suchclaims about capabilities. We will now discuss three different ideas for how to conceptualize capabilitiesin a way that may support such claims or otherwise improve on current practice. Domain Conceptualization:One way of formalizing capabilities (for a predictive model) is asstatements of the form f|A g, which we might read as model f has capability g over domain A.We refer to this as the domain conceptualization. We conceptualize g as a function that implements adesired capability (e.g. addition) on relevant inputs (e.g. real numbers), and A as a subset of all relevantinputs (e.g. integers). This is reminiscent of benchmarking but explicitly specifies a set of inputs overwhich the model reliably exhibits a capability. Such claims might be established by evaluating modelbehavior on a finite set of samples (as in benchmarking) and applying some learning theory (Neyshaburet al., 2017a), using properties such as smoothness of f (Dziugaite and Roy, 2017; Neyshabur et al.,2017b; Arora et al., 2018), and/or methods such as certified robustness to prove that f approximates gwell not only on sampled points but on unseen points as well, e.g. within some volume of input space(Cohen et al., 2019; Carlini et al., 2022). Benchmarks could also be designed in a theoretically-motivatedway to support such general claims (Yu et al., 2023a; Shirali et al., 2022), Internal Computations Conceptualization:Another alternative conceptualization of the capabil-ities of a model is as functions implemented within a model, e.g. between hidden layers. We call this theinternal computations conceptualization of capabilities. For instance, we might identify capabilitieswith circuits, i.e. computational subgraphs of a neural network (Olah et al., 2020). Here, we might saya model possesses a capability for addition if there is any circuit that can perform addition, regardlessof when or whether the neurons in this circuit are actually active. This might enable stronger assuranceregarding the potential for an LLM to exhibit a dangerous behavior after fine-tuning or other methodsof eliciting such behavior. Analysis of circuits has been used to provide conclusive evidence regardingmodels possessing specific capabilities (Wang et al., 2022). However, there are currently technical issueswith applying this conceptualization in practice, which may limit its utility (see .4 for furtherdetails). This conceptualization could also be combined with the domain conceptualization to identifycomputational elements within an LLM that robustly implement functions over limited sets of inputs.",
  "Latent Factors Conceptualization:The aforementioned internal computations conceptualizationview of capabilities is inspired by neuroscience. Analogously, another view to conceptualize capabili-": "ties which we call the latent factors conceptualization could be to take inspiration from the fieldof psychology, in particular, psychometrics. These fields are primarily concerned with characterizingand quantifying the fundamental processes involved in variation in human cognitive abilities. Like theinternal computations conceptualization, the latent factor conceptualization views behavior as beingproduced through the application of multiple capabilities, but does not attempt to ground these ca-pabilities in internal computations. A commonly used technique in psychometrics is factor analysis.Under this conceptualization, capabilities are the factors, or latent variables, that explain variationin measurements across subjects (Carroll, 1993). This could be done by taking a population of differ-ent LLMs and extracting the factors that explain the most variance in performance across examplesin a benchmark (or benchmarks), using techniques such as factor analysis (Burnell et al., 2023a) orother psychometric techniques (Wang et al., 2023d). Machine learning methods for discovering anddisentangling latent factors of variation could also be explored. Intuitively, these factors of variationcould be viewed as a basis of capabilities, and commonalities between capabilities could help interpretmodel behavior in a way that is predictive of behavior on unseen examples, tasks, or domains. Onedrawback of such methodology is that identified factors may not be amenable to natural interpretationas capabilities and attempts to interpret them could induce false beliefs about the models in modeldevelopers and evaluators (Chang et al., 2009). All the above conceptualizations vary in rigor, functionality, and tractability, and their pros and consare not well understood. It is also possible that other, better conceptualizations, exist that may bemore suitable for understanding and explaining the behaviors of LLMs. An ideal conceptualization ofcapabilities should allow making, and verifying, mathematically rigorous claims pertaining to presence,or absence, of a certain capability within a model while being tractable and easily applicable to anyarbitrary model. However, conceptual progress would be valuable even absent practical methods. Onthe whole, the advent of LLMs presents us with an opportunity to rethink our conceptualization ofcapabilities. In the short term, it is also critical that researchers clearly communicate their conceptual-ization of what a capability is when making claims related to the presence, or absence, of capabilities,to avoid the literature getting littered with seemingly contradictory results which are easily explainedaway due to the differences in the object being studied. 2.2.3Limitations of Benchmarking for Measuring Capabilities and Assuring SafetyAs discussed above, benchmarking is one of the most common forms of evaluation, especially for es-timating the capabilities of a model. There are several reasons why benchmarking-based evaluationsmay misestimate the capabilities of an LLM. Firstly, benchmark performance cannot distinguish be-tween the cases of a model failing to function well on a task due to (a) capability failure, i.e. modelbeing incapable of the capability at all, and (b) intent alignment failures, i.e. model failing becauseof failure to understand or adopt human intent (Chen et al., 2021a, Appendix E.2). Relatedly, safetyfinetuning methods often work by suppressing model capabilities (Jain et al., 2023a; Wei et al., 2024).In such cases, a model may have a given capability, but it may not readily demonstrate it due to theinfluence of safety finetuning. In such a case, benchmark performance would indicate the absence ofthe capability when intuitively that is not the case. Secondly, a model could function well on two seem-ingly distinct benchmarks using the same underlying capabilities (Merullo et al., 2023b). This mightlead us to overestimate, or overcount the models capabilities. Thirdly, benchmark performance mayproduce unreliable assessments of capabilities that are present within a model, but are not robust; insuch cases, performance may depend heavily on the particular distribution used by a benchmark (Teneyet al., 2020; Burnell et al., 2023b; Bao et al., 2023). Fourthly, because benchmarks primarily reportaverage statistics (e.g. accuracy), they are often not very informative about an LLMs performanceon particular test examples. The first 3 issues are all problematic mostly because they may lead toincorrect inferences about a models behavior on test examples, the fourth issue is more about makingmore informative inferences about behavior making predictions about behavior (e.g. performance) atthe example level rather than the dataset/distribution level as is implicitly the case with benchmarks.",
  "Separately, the fact that currently the most capable models are provided as a service via an API largelylimits our ability to independently audit and evaluate them for safety (La Malfa et al., 2023)": "There is a need to develop principled solutions to the aforementioned issues.We need to developrobust machinery that may help us distinguish between capability failures, intent-alignment failures,and intentional failures on the models part due to the effects of safety finetuning. Among these intent-alignment failures are the most egregious to avoid. There is in general a need for elicitation protocolsthat reliably and consistently elicit the capabilities of interest, even in the case of intent-alignmentfailures. Fine-tuning and prompting often reveal behavior indicative of novel capabilities, but cannotbe used to rigorously establish that an LLM does not possess a particular capability. There is also aneed to improve the structure and granularity of benchmarking to better understand model capabilities. The suggestions we make in .2.2 may help address the issues mentioned above. First, thedomain conceptualization can limit incorrect inferences by being precise about the domain over whicha capability is expected to be robust, and can make more precise inferences by accounting for whichcapabilities domains a test example belongs to. Likewise, both the circuit conceptualization and thelatent factors conceptualization could help determine which capabilities are at play when a modelprocesses a particular example, and thus better attribute behavior to particular capabilities and drawconclusions about which capabilities are (likely to be) used on a given test input. The circuit conceptu-alization, being more detailed and grounded, could make it more straightforward to avoid misestimationof capabilities, e.g. by distinguishing between capabilities that are robustly, but rarely applied (e.g. dueto misalignment) vs. those that are simply not robust. 2.2.4How Can We Efficiently Evaluate Generality of LLMs?The majority of evaluations of LLMs are domain-specific evaluations, e.g. language understanding(Hendrycks et al., 2020a), mathematics (Hendrycks et al., 2021b), social knowledge (Choi et al.,2023a), medical knowledge (Singhal et al., 2023a), coding and tool manipulation (Xu et al., 2023a).A fundamental limitation of evaluating LLMs in this way is that we may undercount LLM capabilities indomains where corresponding benchmarks are not available (Ramesh et al., 2023). It is also logisticallydifficult to evaluate LLMs across a large (e.g. combinatorial) number of domains or tasks. There isa need for work to consider alternative means to evaluate generality (i.e. how general-purpose themodel is) (Casares et al., 2022), and generalization across domains, of LLMs. For example, proceduralevaluations like Skill-Mix (Yu et al., 2023a) could be designed that may evaluate the compositionalgeneralization skills of LLMs. Alternatively, given the fact that we now have a population of LLMsavailable, finding differences and similarities in their performance across domains could inform whatevaluations are most informative for evaluating the generality of LLMs (Ye et al., 2023). Mechanisticinvestigations of LLM capabilities could aim to discover capabilities that are reused across tasks (Toddet al., 2023), or discover and explain other capabilities (like in-context learning, see .1) thatmay underlie general-purpose behaviors of LLMs. It may also be useful to create a taxonomy of the capabilities of LLMs, similar to how psychologists havecreated taxonomies of human capabilities (Fleishman et al., 1984). In fact, some taxonomies are alreadyemerging implicitly within the literature. For example, LLM developers report across different typesof benchmarks (corresponding to different domains and capabilities), e.g. coding, question answering,mathematical reasoning, common-sense reasoning, performance over long-contexts (OpenAI, 2023a;Gemini Team, 2023; Anthropic, 2023b; Jiang et al., 2024a). However, these taxonomies are ad-hoc,and have little to no theoretical basis. There is a need for work to establish theoretically groundedtaxonomies that may provide better organization and understanding of LLM capabilities. Furthermore,taxonomies of human capabilities often arrange capabilities in a hierarchical fashion making thedependencies between capabilities obvious (Schneider and McGrew, 2012). Chen et al. (2024a) showthat similar dependencies between capabilities (or in their terminology, skills) exist for LLMs as well, andrespecting these dependencies during training (i.e. defining an appropriate curriculum over skills) helps",
  "Scaffolding Is Not Sufficiently Accounted for in Current Evaluations": "Even in the simplest use cases, an LLM should be viewed as a multi-party system consisting of theLLM itself and an elicitation protocol (e.g. the prompt or finetuning process). In the extreme cases,the LLM can be a part of a much larger system having access to external memory, various types oftools and various types of learning signals (e.g. feedback from other LLMs) (Wang et al., 2023a; Parket al., 2023a). We collectively refer to these mechanisms that enhance the capabilities of an LLM asscaffolding. In addition to being highly capable models, LLMs are also highly efficient learners. This learning canoccur via fine-tuning, supervised learning-style in-context learning, instructions, explanations, etc. As aresult, by efficiently designing the elicitation protocol, a designer can significantly alter the capabilitiesand the behavior profile of an LLM. For example, an LLM capable of performing addition can begiven the capability of performing multiplication by exhaustively explaining and demonstrating thealgorithm to multiply numbers using additions within the prompt (Zhou et al., 2022). Similarly, fine-tuning on a small number of carefully chosen examples can cause the LLM to act in a highly politeway (Zhou et al., 2023b). In such cases, it is no longer clear whether the protocol revealed an existingcapability or induced a novel capability within the LLM (Stechly et al., 2023). This lack of distinctioncan cause overestimation of capabilities by mistakenly attributing a capability to the LLM when in factthe LLM did not have the capability (Stechly et al., 2023); rather the capability may have been quicklylearned on the go due to the efficient design of the elicitation protocol. There is a need for theoreticalwork to characterize and distinguish between capabilities that are present within an LLM (and areelicited), versus capabilities that are efficiently learned on the fly. Tools, techniques, and concepts frominformation theory may be used for this purpose (Zhu and Rudzicz, 2020). A similar attribution problem occurs when an LLM is combined with other systems, e.g. given accessto external tools it can assign tasks or given feedback from environment or verifiers (Valmeekam et al.,2023). Prior work suggests that such systems can outperform an LLM acting on its own (Mialon et al.,2023; Wang et al., 2023a). In general, in deployment, LLMs are deployed as parts of larger system,where other components of the system may perform various jobs. In such cases, it is not clear how thecapabilities being demonstrated by such systems should be attributed to the LLM vs. the other systemcomponents involved. Various concepts exist within game theory on how the utility of a system maybe distributed between its components, e.g. Shapely value, core (Shoham and Leyton-Brown, 2008,Chapter 12). It is an open problem to evaluate which concept(s) are most suitable for attributing thecapabilities of LLM-based systems to the capabilities of LLM and the capabilities of other componentspresent in the system. In order to assure safety, we need a calibrated understanding of the capabilities of the models.However, this is currently made difficult by various challenges. Firstly, the capabilities of themodel seem to have a different shape compared to human capabilities; but this differenceis not currently well-understood.Secondly, there is no well-established conceptualization ofcapabilities. Thirdly, we do not have sufficiently reliable methods to assess the generality ofLLMs i.e. how general-purpose a given model is. Lastly, it is not clear how to account forscaffolding in LLM capabilities and distinguish between the cases in which an elicitation protocolreveals a capability already present within an LLM versus the cases in which LLM acquires thecapability due to the efficient design of the scaffolding and elicitation protocol.",
  ". How can we understand the differences in the shape of capabilities of humans and otherAI models? What are the implications of these differences?": "10. What is the right conceptualization of capabilities for LLMs?Can we formalize thethree conceptualizations of capabilities presented here (domain conceptualization, inter-nal computations conceptualization, latent factors conceptualization), and understandtheir relative merits and demerits? 11. How can we draw reasonable general insights from behavioral evaluations? In particular,how can we prove that a given model does not have a capability of interest, withoutexhaustively evaluating the model for that capability on the whole input space? 12. Can we develop methods e.g. based on factor analysis or other unsupervised learningmethods to automatically discover capabilities by decomposing a models (potential)behavior into something like a basis of capabilities? 13. When performing an evaluation using a benchmark, how can we separate observed fail-ures of a model into capabilities failure (i.e. model failing because it truly lacks therelevant capability) from alignment failures (i.e. model failing despite having the capa-bility because it was not correctly invoked)?",
  ". How can we understand the dependencies between LLM capabilities? Can we createtheoretically grounded taxonomies of LLM capabilities that may help us assess the gen-erality of LLMs efficiently?": "21. How can we distinguish between revealed capabilities (capabilities inherent in an LLM)vs. learned capabilities (capabilities that are exhibited because of the highly optimizednature of the elicitation protocol)? Can tools and concepts from other fields, such asinformation theory and cognitive science, be used to help make this distinction? 22. How can we precisely characterize the contribution of the LLM to behaviors demonstratedby an LLM-based system (e.g. an LLM with access to external tools)?Can we useconcepts developed in game theory and other literature on multi-agent systems for thispurpose?",
  "Effects of Scale on Capabilities Are Not Well-Characterized": "Increasing the scale (parameters, compute, and data) of LLM training predictably results in an overallmore performant model in accordance with well-established scaling laws (Kaplan et al., 2020). However,specific capabilities of LLMs are often highly difficult to predict (Bowman, 2023), and may show so-called emergent behavior (Wei et al., 2022a). This combination of high-level predictability andlow-level unpredictability is a source of risk: the former enables easy progress via scaling,the latter makes it difficult to anticipate and precisely characterize the risks associated with the development and deployment of more performant models (Ganguli et al., 2022).From a scientific standpoint, this highlights a significant gap in our knowledge of how LLMs learn andacquire capabilities. To address this gap, we will need to develop a deeper understanding of scalinglaws, understand how scaling impacts learned representations, identify the limits of scaling, and worktowards formalizing, forecasting, and explaining emergent behaviors in learning. 2.3.1Understanding Scaling LawsLarge language model training has been found to follow scaling laws for aggregate loss that are consistentacross many orders of magnitude of resource scaling (Kaplan et al., 2020; OpenAI, 2023a). The factorsexplaining this scaling law behavior, however, remain poorly understood. As a result, it is unclearto what extent different aspects of these laws are universal and to what extent they are sensitive todifferent aspects of the training pipeline which might change in the future. Explanations can addressboth the functional form of scaling laws (e.g. power law scaling vs. exponential scaling) and the specificscaling parameters of the functional form found in particular experiments (e.g. the exponent in powerlaw scaling). Prior work has studied the impact of scaling on learning under various theoretical setups.Theseworks, as explained in detail later, often differ in their prediction of power law exponents.Theseseemingly contradictory predictions arise from differences in the scaling setup; specifically from whetherthe resources which are not being scaled are small or large in comparison to the scaled resource (Bahriet al., 2021).Indeed, current literature can be quite clearly demarcated based on this distinction.Borrowing the terminology from Bahri et al., the variance-limited regime considers the case where weare asymptotically scaling one resource (model size or data) while the other resource is fixed at somefinite level. In this case, most theories predict rapid power-law scaling (or even exponential scaling, insome cases) to a saturated level of performance, based on concentration arguments. The resolution-limited regime is when the unscaled resource is infinite, or is much larger than the scaled resource. Here,the scaling exponent captures the effect of increasing the resolution of the learner (either by allowingit to use more data points or more parameters to fit increasingly fine aspects of the distribution), andis highly sensitive to properties of the data distribution. This is the regime most modern work on LLMscaling is concerned with. Theoretically, the variance-limited regime of scaling laws in which the amount of data is scaled andmodel size is fixed and finite is the best studied one. This is the typical subject of the vast literatureon learning curves see Viering and Loog (2022) for a survey.For example, classic PAC theoryshows that power law data scaling with an exponent of 1 (or 1/2 in the unrealizable zero-one errorsetting) is optimal for every learnable task, and is achievable by empirical risk minimization (Blumeret al., 1989).Other theoretical models of (bounded-capacity) learning curves include the universallearning theory of Bousquet et al. (2021) and approaches based on statistical mechanics on non-worst-case learning curves in the thermodynamic limit (Seung et al., 1992; Watkin et al., 1993; Amari, 1993;Haussler et al., 1994). However, as these aforementioned theoretical models assumed that the modelsize is fixed, and only data is scaled, they provide limited insights regarding scaling laws for LLMs forwhich model size and dataset size grow together, in which case, gentler scaling exponents of roughly1/10 to 1/20 have been observed (Kaplan et al., 2020). However, they are still predictive of LLMscaling in the cases where the model is set to be fixed and finite. For example, for a fixed and finitemodel size, the joint data-parameter functional form of Kaplan et al. (2020) is asymptotically a powerlaw with exponent 1 (approaching the loss at which models of the given size saturate). At least for now, both the data and the number of parameters used in training continue to increaseover time, so the variance-limited regime, which assumes one of these resources is capped, is of limitedrelevance. (Though it may be relevant in scenarios in which the amount of data available for trainingis limited.) Instead, resolution-limited scaling the scaling regime in which the unscaled resource isinfinite or is much larger than the scaled resource provides a more accurate picture for capabilities forecasting. The existing literature contains at least three distinct explanations and theoretical modelsof resolution-limited scaling. The first of these, the manifold explanation (Sharma and Kaplan, 2022),posits that scaling laws emerge from something akin to interpolation on the data manifold. Sharma andKaplan (2022) empirically test this explanation by estimating the intrinsic dimension for some smalldatasets and showing that this is predictive of the model size scaling exponent. However, this modelalso predicts that per-example performance should increase monotonically, which Kaplun et al. (2022)note is not the case in practice. A related theory is the kernel spectrum explanation provided by Bordelon et al. (2020) and Spigler et al.(2020), who derive resolution-limited scaling in the setting of kernel regression. Specifically, they showthat for kernel methods, such as learning with an infinitely wide network in the neural tangent kernellimit (Jacot et al., 2018), power-law decay in the kernel spectrum results in power law scaling in theloss. Finally, a third explanation, based on long tails in the data, has been introduced by Hutter (2021)and extended by Michaud et al. (2023), Dbowski (2023), and Cabannes et al. (2023). These authorsconstruct toy models in which gentle power law scaling emerges when the data distribution has a longtail of sub-components that must be learned independently, an assumption that is especially natural inthe domain of natural language data. Given the fragmented state of the literature, several fundamental questions are currently unanswered.Firstly, can the different proposed explanations for power law scaling in the resolution-limited regimebe unified? Bahri et al. (2021) provide a connection between the manifold dimension explanation andthe kernel spectrum explanation; perhaps it is possible for all three theories (including the long tailtheory) to be subsumed by a single meta-explanation which could handle a wider range of settings. Secondly, the variance-limited versus resolution-limited dichotomy entirely ignores the scaling regimethat is most important for forecasting capabilities: compute-efficient scaling, where data and modelsize are scaled jointly (Hoffmann et al., 2022). What is a good theoretical model for this setting? Isthere an explanatory theory that considers all three regimes as special cases of a more general jointdata-parameter scaling setting? Thirdly, what is the role of feature learning, and optimization more generally, in scaling laws?Inlanguage modeling, representations learned early in training enable more complicated aspects of thedata to be learned later in training (see Abbe et al. (2021) for a synthetic case study of this behavior).Existing scaling law explanations, however, essentially treat the data as flat, and ignore the influenceof hierarchical structure on scaling behavior. In particular, we would like to encourage further workon scaling laws that account for various properties of language data, for example, burstiness (Chanet al., 2022) or fractal nature (Alabdulmohsin et al., 2024). Relatedly, all the current models of scalinglaws presume that the data distribution that is being learned is held constant throughout learning.However, in various settings of interest, e.g. reinforcement learning, data distribution changes overtime. However, despite this non-stationary nature of data distribution, various works have reportedscaling laws for various reinforcement learning settings (Hilton et al., 2023; Tuyls et al., 2023; Teamet al., 2023; Obando-Ceron et al., 2024). Thus, the development of appropriate theoretical modelsfor scaling laws for cases in which the data distribution is considered to be non-stationary is an openresearch question at the moment. Finally, one fundamental question that future investigations ought to answer is to what extent learn-ing curve exponents are fundamentally bounded by the data distribution. This can help inform thepossibility of hypothetical future architectures that might scale better than contemporary transformersfor language modeling. A related open question is what properties of a data distribution affect thepredictability of scaling behavior on that distribution. This is particularly relevant to the discoveryof task-specific specific laws (also see .3.5). On some tasks, e.g. code-generation, power lawscaling across 10 orders of magnitude of compute has been observed (Hu et al., 2023a; OpenAI, 2023a,",
  "). On the other hand, many downstream capabilities display irregular scaling curves (Srivastavaet al., 2022), or non-power law scaling (Caballero et al., 2023)": "2.3.2Effect of Scaling on Learned RepresentationsMost work on scaling focuses on performance on benchmarks. There is far less work on the questionof how learned representations change with scale.This is a particularly pertinent question for theviability of interpretability techniques that aim to understand the internal operations of LLMs likemechanistic interpretability and probing techniques (c.f. .4). The first question relates to theuniversality hypothesis of representations do different neural networks trained on the same task learnthe same representations (Olah et al., 2020)? The current evidence indicates that the strong versionof the universality hypothesis is false, for example, Chughtai et al. (2023) show that different neuralnetworks learn different representations in different orders even when the architecture and data orderare kept the same. This is supported by evidence contained in other studies as well (McCoy et al., 2019;Wang et al., 2018). However, there is increasing evidence that some feature representations are indeeduniversal and learned by different neural networks trained on the same task (Li et al., 2015; Bansalet al., 2021; Gould et al., 2023). The key unknown question is whether the proportion of universalrepresentations increases, or decreases, with scale. Vyas et al. (2023) argue that language models learnsimilar features across different width sizes. In the vision setting, Nguyen et al. (2020) found that whennetworks are sufficiently large (in terms of width or depth) in comparison to the training set size, theycan be partitioned into contiguous blocks of layers with representations within each block being similaracross different trained models. A related, but distinct, question is whether learned representations converge to, or diverge away, fromthe representations used by humans with increasing scale (Sucholutsky et al., 2023)?3 In some cases,the behavior of larger LMs does appear to be more consistent with human behavior (Chiang and Lee,2023; Park et al., 2023a; Zhu et al., 2024), however, this consistency tends to break down at the edges;for example, LLMs do not always show human-like biases in their responses (Aher et al., 2022; Tjuatjaet al., 2023; Hagendorff et al., 2023). As such, there may not exist a clean answer to the question above.However, it might still be useful to understand if there are types of representations of concepts that wecan expect LLMs of sufficient scale to share with humans. And if so, how may it help us predict thebehavior of LLMs? Another open question pertains to the changes in representation structure with scale.Specifically,whether larger scale models are more or less likely to have representations that exhibit linear charac-teristics, such as the famous King - Man + Woman = Queen example (Mikolov et al., 2013).4 Theidea that LLMs encode high-level concepts linearly in the representation space of the model has beentermed the linear representation hypothesis (Park et al., 2023b). The mathematical field of represen-tation theory studies how abstract algebraic structures such as groups can represented in terms of lineartransforms, and might help identify and understand such limitations. Chughtai et al. (2023) presentevidence that neural networks do in fact use representation theory to represent data with a group struc-ture. Understanding how scale influences the structure of representations would provide insights intothe applicability of interpretability methods for larger models, which implicitly or explicitly assumeslinearity of representations. 2.3.3Limits of ScalingOne framework for making sense of advances in machine learning is to decompose the progress in capa-bilities into performance improvement due to algorithmic innovation and the performance improvementdue to scaling up the resources of training (Hernandez and Brown, 2020; Erdil and Besiroglu, 2022).Prior work has shown that simple scaling up can cause the acquisition of novel capabilities within the",
  "Also see .4.24Notably, Nissim et al. (2020) found that the closest vector to King - Man + Woman is actually King; Queen is the secondclosest": "model (Kaplan et al., 2020; Wei et al., 2022a). This raises the question of whether there exists a scaleceiling i.e. capabilities that can not be acquired by a model regardless of how much it is scaled further.If so, what capabilities are these? For example, it is unclear to what extent LLMs may acquire reasoningand abstraction capabilities via scaling alone (Mitchell et al., 2023; Saparov et al., 2023). Prior work has argued that some of the most critical limitations of current LLMs are unlikely to beresolved by simple scaling. Kirk and Krueger (2022) argue that causal confusion, i.e. learning of spuriouscorrelations present within the data may be unavoidable in a purely offline learning setup. This claimhas mixed support within the literature; while Zecevic et al. (2023) argue that LLMs can not be causal,Lampinen et al. (2023) point out that LLM training data contains many examples of interventions,outcomes, and explanations that may support the learning of active causal strategies by the LLM.Kalai and Vempala (2023) and Xu et al. (2024a) both argue that hallucinations in LLMs are notfixable via scaling. Several studies have argued that resistance to jailbreaking may not improve withscale (Wei et al., 2023c; Wolf et al., 2023; Millire, 2023). In general, robustness to adversarial examplesmay not improve with scale, as argued by Frei et al. (2023) and Debenedetti et al. (2023). One extreme sense in which scaling can be limited for LLMs is inverse scaling, where performancedecreases with scale (McKenzie et al., 2023). McKenzie et al. (2023) ran a contest to solicit tasksexhibiting inverse scaling across different LLM architecture families. The results were mixed foralmost all tasks inverse scaling was later found to be reversed at the largest compute scale measured(which was for the PaLM family of models), resulting in U-shaped scaling curves Wei et al. (2022b). Wecan also define inverse scaling more broadly, to include increasing frequency of undesirable behaviorswith scale, not just decreasing frequency of correct responses.Using this perspective, Perez et al. (2022a) observe inverse-scaling for sycophantic behavior, i.e. larger models have a more pronouncedtendency to mimic the political views of their interlocutors. This suggests that the human feedbackprocess used for alignment training is misaligned (see .2), which might cause other relatedinverse scaling problems. Still, it is currently unclear whether there exist other undesirable behaviorsthat undergo inverse-scaling, and there is no reliable way of determining whether such undesirablebehaviors will also undergo U-shaped scaling (or not). 2.3.4Formalizing, Forecasting, and Explaining EmergenceSome capabilities when plotted on a scaling curve appear to emerge suddenly past some scale. A largenumber of capabilities, including key capabilities such as instruction following and chain-of-thoughtreasoning, have been argued to be emergent (Wei et al., 2022a). On the other hand, other studieshave argued that some capabilities may appear emergent due to the harsh nature of the evaluationmeasures being used which do not reward partial correctness (Schaeffer et al., 2023; Srivastava et al.,2022). However, none of these studies precisely define emergence, and their use does not accord withestablished use across other fields such as complex systems and physics.This indicates a need forgreater clarity and careful formalization of emergence in the context of machine learning. In any case, so-called emergent capabilities, e.g. those identified by Wei et al. (2022a), may be partic-ularly challenging to predict and forecast (Ganguli et al., 2022), and are hence disconcerting from a riskassessment perspective (Kaminski, 2023). Developing a mechanistic understanding of factors that con-tribute to abrupt improvement in performance may yield critical insights regarding the predictabilityof emergent capabilities. One such factor that has been identified so far is the compositional natureof a capability, i.e. a capability being composed of other capabilities. Arora and Goyal (2023) andOkawa et al. (2023) show that if a capability is a composition of another set of capabilities (calledskills) that witness smooth and predictable scaling, the scaling curve of a capability will look emergentdue to a multiplicative dependence on the ability to perform the skills underlying it Okawa et al.(2023) call this the multiplicative emergence effect. The results of the aforementioned papers indicatethat if a valid skill decomposition of a seemingly emergent capability is available and these skills followpredictable dynamics, we can accurately predict the models progress on the capability itself. Arguably, the bottleneck here is that for several capabilities of interest, we are unlikely to have an accurate de-composition available (Barak, 2023). Further research is needed to understand how such decompositioncan be achieved for a capability of interest and whether given an approximate decomposition, one canpredict at what scale a model learns a compositional capability. Schaeffer et al. (2023) provide anecdotal evidence that progress measures can be designed for some ofthe emergent capabilities identified in the literature which smoothly track the so-called emergent capa-bility. This is similar to the identification of continuous progress measures in the context of grokkingof capabilities (Nanda et al., 2022; Barak et al., 2022), where models exhibit a sudden shift from mem-orization to generalization. However, we assert that the existence of soft progress measures indicatesthat learning dynamics are predictable (if the appropriate progress measure can be identified pre-hoc),but does not invalidate the perspective that some capabilities are emergent (Barak, 2023). It is alsocritical to ensure that any progress measures identified faithfully track the capability of interest, as,otherwise, there may be a measure whose dynamics are predictable, but in fact does not faithfullycapture progress on learning the capability. It is not clear whether the progress measures proposedin the literature are faithful in this sense or not. Finally, we emphasize that identifying such progressmeasures can be highly non-trivial and current evidence is insufficient to indicate that suitable progressmeasures exist for all capabilities that are claimed to be emergent (Wei, 2022). Interpretability methodshave previously been used to find progress measures for grokking (Nanda et al., 2022; Chughtai et al.,2023), and could be used for discovering progress measures that underlie emergent capabilities as well.It may also be helpful to study the learning dynamics in a systematic way as done in some prior work(Hu et al., 2023b; Chen et al., 2024b; Hoogland et al., 2024; Edelman et al., 2024). Hoogland et al.(2023) argue that singular learning theory (Wei et al., 2022c; Watanabe, 2024) may be particularlyuseful for this purpose. There is also a need for work to tighten and formalize the definition of emergent capabilities. Thecurrent definition of emergent capabilities arguably points to an abrupt improvement in model per-formance on a scaling curve as a necessary but not sufficient condition for a capability to beemergent. This notion of emergent capabilities is also somewhat distinct from the notion of emergencein other fields. For example, in physics, emergence is often associated with the formal notion of phasetransitions: discontinuities in some property of a system (or its derivatives) considered as a function ofsome parameter (e.g. scale). In physics, such discontinuities yield distinct phases that are governed bydifferent physical laws (e.g. liquids vs. gases) (Huang, 2008; Grimmett, 2018). However, seeking inspi-ration from natural sciences and grounding emergence in phase transitions may be too constraining;e.g. it is unclear if in-context learning (ICL) is truly a phase transition in the technical sense, but it isintuitively clear that ICL yields a qualitatively different set of capabilities in larger models. There is aneed to establish desiderata to further ground emergence from the context of prediction of capabilitiesand a consensus is needed on what evidence is sufficient to claim a capability is truly emergent (seemingly) discontinuous scaling curves are perhaps insufficient for this purpose.",
  "Better Methods for Discovering Task-Specific Scaling Laws": "Power-law-based scaling laws are not always suitable for predicting performance on narrower metrics,which can exhibit inflection points and even non-monotonic scaling. Caballero et al. (2023) presenta generalization that can model such phenomena, building on Alabdulmohsin et al. (2022).Othermodeling approaches should be evaluated and could borrow from approaches to model learning curves(Viering and Loog, 2022). To improve data efficiency, predictions could incorporate additional informa-tion, such as scaling performance on other tasks, performance on individual examples (Kaplun et al.,2022; Siddiqui et al., 2022), or other indicators of how learning/scaling is progressing. Probabilisticmodeling approaches such as Gaussian processes (which Swersky et al. (2014) use to model learningcurves) may also be useful in providing scaling estimates alongside the uncertainty estimate. Developingbetter evaluation measures with higher resolution, and better methodologies overall to estimate capa-",
  "bilities present in a model that are not fully mature yet, can be particularly instrumental in providingbetter estimates of capabilities of future models (Hu et al., 2023a)": "It would also be useful to clarify the purpose of task-specific scaling laws. Is the goal to forecast capa-bilities as accurately as possible in the short-term, when scaling resources are only slightly higher thantheir current levels? To provide accurate longer-term predictions? To provide interpretable parametricfits that enable quantitative comparison of learning algorithms and can distinguish qualitatively distinctscaling regimes? If the goal is indeed to forecast capabilities as accurately as possible, then clear desiderata should beestablished prescribing the range over which the task-specific scaling laws should extrapolate with highaccuracy. The evaluations of Alabdulmohsin et al. (2022) and Caballero et al. (2023) demonstrateextrapolation to values of the scaled resource which are twice as large as those used for fitting thefunctional form, but it is unclear how accurate they are beyond this limited horizon.Stumpf andPorter (2012) suggest, in a broader scientific context, that proposed power law fits ought to be consid-ered scientifically useful only if they extrapolate over at least two orders of magnitude; this stringentdesideratum may be fruitful in the task-specific scaling law context (even for functional forms besidessimple power law scaling). Moreover, as more and more tunable parameters need to be introduced tofunctional forms in order to improve extrapolation accuracy, the interpretability advantage of paramet-ric fits over non-parametric methods (e.g. Gaussian processes) becomes questionable, and the risk ofspurious interpretations rises. Broadly speaking, there are several distinct reasons task-specific scalinglaws might be useful, and there is a need to develop criteria for assessing these laws which distinguishbetween the different use cases. (It may even be more terminologically precise in some cases to thinkof scaling fits as just fits rather than as laws.) There remain many challenges that hinder our ability to predict which capabilities LLMs willacquire with continued scaling, and when. We continue to lack a robust explanation of whyscaling works, and to what extent the scaling laws we have discovered are universal. Additionally,there has been minimal exploration into how scale influences the representations learned by themodels and whether certain capabilities cannot be acquired through scaling alone. Furthermore,our understanding of the factors that underlie abrupt performance improvements on certain tasksis lacking, and our methods for discovering task-specific scaling laws are inadequate.",
  ". What properties of a task (and its relation to the full training distribution) affect thepredictability of its scaling behavior?": "29. To what extent does scaling a model increase the universality of its representations? 30. Does increasing scale cause model representations to converge to, or diverge away from,human representations? In other words, does representation alignment between humanrepresentations and model representations increase or decrease with scale? 31. How does scale impact the structure of the representations?Does scaling cause thestructure of model representations to become more, or less, linear? To what extent isthe linear representation hypothesis true in general? 32. How can we determine whether a given capability is below or above the scale ceiling, i.e.whether simple scaling up the model (and/or data, compute) would enable the model tolearn that capability or not?",
  ". To what extent are the issues faced by current LLMs (causal confusion, hallucinations,jailbreaks/adversarial robustness, etc.) likely to be resolved by further scaling?": "34. How do we determine if the inverse-scaling behavior of a capability will be reversed withfurther scaling (i.e. result in a U-shaped curve)? How can we predict threshold points atwhich scaling behaviors change shape? 35. What factors may explain abrupt improvements in performance associated with emergentcapabilities? To what extent does the multiplicative emergence effect (Okawa et al., 2023)explain the emergent capabilities of LLMs that have been observed in practice? 36. How can we discover valid decompositions of various compositional capabilities of interestand assess the accuracy of such decompositions? How can the emergence of compositionalcapabilities be predicted based on the learning dynamics of its decomposed capabilities?",
  ". What is an appropriate formalization of emergent capabilities in the context of LLMscaling? How can we apply it to understand which sorts of novel phenomena are likelyto be (un)predictable?": "38. Can we discover progress measures that may explain emergent capabilities, e.g. by us-ing interpretability methods? How can we establish the faithfulness of such progressmeasures to ensure that they can be used to predict the emergence of the capability ofinterest? 39. Can we develop better methods for modeling task-specific scaling? E.g. by conditioningon additional information, using probabilistic techniques, or by developing evaluationmeasures with higher resolution. 40. Can we clarify the purpose of task-specific scaling laws? In order to be useful for fore-casting capabilities, what is the minimum range over which a task-specific scaling lawmust extrapolate accurately?",
  "Qualitative Understanding of Reasoning Capabilities Is Lacking": "Reasoning the process of drawing conclusions from prior knowledge is a hallmark of intelligence.Prompt programming techniques (Reynolds and McDonell, 2021), such as chain-of-thought (CoT)prompting (Wei et al., 2022d), scratchpad prompting (Nye et al., 2021), or lets think step-by-step(Reynolds and McDonell, 2021; Kojima et al., 2022) enable language models to perform reasoning taskswith impressive accuracy. Careful studies to evaluate the behavior of LLMs on out-of-distribution(OOD) reasoning tasks have revealed that LLMs exhibit some reasoning behavior (Saparov et al., 2023;Wang et al., 2023e; Dziri et al., 2023), and that performance on reasoning tasks improves with scale(Wei et al., 2022d; Saparov et al., 2023; Valmeekam et al., 2022). However, the reasoning capabilitiesof even the largest LLMs are deficient in many ways; causing them to struggle on various types ofreasoning problems (Wu et al., 2023a; Saparov and He, 2022; Berglund et al., 2023a; Valmeekam et al.,2022; Mitchell et al., 2023). The mixed evidence regarding the reasoning capabilities of LLMs is a majorsource of disagreement within the wider community (Huang and Chang, 2022). Are the prevalentlimitations in reasoning capabilities of the LLMs fundamental in nature, or transient andlikely to go away with scaling or improvement in training methods (e.g. targeted finetuningon reasoning data as in e.g. Lewkowycz et al., 2022)?Answering this question is critical to better",
  ": Sketch of two possible scaling law behaviors, relating the relationship between the reasoningperformance of LMs and the number of parameters (see .4.1)": "understand the risks posed by LLMs, as both the strengths and limitations of the reasoning capabilitiesof LLMs give rise to different types of safety risks. The inability to reason robustly in novel (e.g. OOD)contexts can lead to undesirable, and potentially unsafe, behavior of LLMs. Conversely, if the LLMis misaligned, robust reasoning might cause it to behave undesirably and competently (Shevlane et al.,2023). In this section, we highlight several research questions that can be instrumental in addressingthese concerns.",
  "Does Scaling Improve Reasoning Capabilities?": "Several prior studies have provided circumstantial evidence that larger models are better at reasoning(Nye et al., 2021; Wei et al., 2022a;d). However, this circumstantial evidence is confounded by issues suchas data contamination (Wu et al., 2023a). Some studies have proposed scaling laws for performance onmath word problems with respect to pretraining (Henighan et al., 2020) and finetuning (Caballero et al.,2023; Yuan et al., 2023a). However, these tasks may not reflect more general reasoning capabilities, andthe discovered laws could also be misleading as special attention is paid to train LLMs on mathematicalreasoning tasks (OpenAI, 2023b).Saparov et al. (2023); Han et al. (2022); Sprague et al. (2023)develop datasets meant to test more broadly for reasoning capabilities, however, more such datasetsand evaluations of reasoning capabilities across broad contexts are needed to better understand howscale affects the reasoning capabilities of LLMs. Empirical scaling laws for general reasoning capabilitiescould be a particularly valuable contribution. Conversely, a conclusive demonstration that increasingthe scale does not always improve the reasoning capabilities of LLMs would be similarly valuable. provides a rough sketch of two possible scaling law results, one where the increasing scale ofLMs allows them to solve reasoning tasks with increasing complexity, and one where a fundamentallimitation of LMs prevents them from continually improving their performance in reasoning.",
  "Understanding the Mechanisms Underlying Reasoning": "Research to understand the mechanisms underlying reasoning in LLMs would provide further insightinto their reasoning cabilities as a function of scale. This can help to reveal why and when LLMsrely on heuristics, and when they do actually perform reasoning to solve various reasoning tasks.Mechanistic interpretability analysis may be used for this purpose (c.f. .4). Hou et al. (2023)use mechanistic interpretability to analyze GPT-2 and LLaMA models and find evidence that LLMsembed a reasoning tree resembling the oracle reasoning process within the attention patterns. However,",
  ": Illustrative examples of problems involving deductive reasoning, abductive reasoning, causalreasoning, and social reasoning (see .4.3)": "they only study simple synthetic reasoning tasks, and its not clear whether a similar finding will hold forOOD and long-tail inputs, or on more complex/general tasks than the ones considered by Hou et al.. Onthe whole, very limited work has been done so far to understand the mechanisms underlying reasoning.This presents an opportunity for future research to use methods like mechanistic interpretability analysisto explain the reasoning capabilities of LLMs, as well as their limitations. In particular, explaining thecases in which a smaller LLM fails at a reasoning task but a larger LLM proficiently solves the sametask may be particularly valuable in understanding the effect of scale on mechanisms used by LLMs forreasoning. 2.4.3Understanding Non-Deductive Reasoning Capabilities of LLMsExisting work on the reasoning capabilities of LLMs is primarily focused on deductive reasoning(Saparov and He, 2022; Saparov et al., 2023; Wang et al., 2023e). There is a need to better understandLLMs inductive, abductive, social, situational, and causal reasoning capabilities5 (Bhagavatula et al.,2020; Gandhi et al., 2023; Peirce, 1868; Smith, 2022, Chapter 5.4). See for a set of examplesinvolving different types of reasoning. There is some evidence that LLMs are able to perform some, butnot all, aspects of inductive reasoning from in-context examples (Qiu et al., 2023; Wang et al., 2023f;Zhu et al., 2023; Mitchell et al., 2023). Similarly, while Benchekroun et al. (2023) show that LLMshave difficulty abducting a valid world model from text descriptions given in-context, both Gurneeand Tegmark (2023) and Roberts et al. (2023a) argue that during training, LLMs do build a coherentunderstanding of the outside world from the dispersed information available in the corpus, which theythen use at inference time to solve various tasks. In the same vein, there is mixed evidence regardingwhether LLMs have adequate theory-of-mind reasoning capabilities that would enable them to robustlyperform social and situational reasoning Kim et al. (2023); Li et al. (2023a); Kim et al. (2023). Causalreasoning capabilities of current LLMs are quite limited (Jin et al., 2023); however, Lampinen et al.(2023) argue that this is not in principle a limitation of the current paradigm. Further research is neededto better understand the precise limitations of LLMs with regards to various types of non-deductivereasoning, and whether these limitations are fundamental in nature or could be resolved via furtherscaling or modifying the training process. In particular, more work in the vein of Lampinen et al.(2023), that studies the limitations of current paradigm and not just current models, is needed that",
  "Which Aspects of Training Lead to the Acquisition of Reasoning?": "Understanding how reasoning capabilities are acquired by LLMs during pretraining, and how theyevolve during finetuning, can provide insights as to whether or not the deficiencies in the reasoningfaculties of LLMs will be resolved with further scaling or through modifications of the training process.Different studies have theorized different causes for the acquisition of reasoning. Lightman et al. (2023);Magister et al. (2023); Mitra et al. (2023) argue that training on reasoning traces improves the reasoningperformance of LLMs. Madaan et al. (2022); Liang et al. (2023) show that training on code helps modelsperform better in reasoning tasks. Liang et al. (2023) show that instruction tuning also improves themodels performance on reasoning tasks. However, there is a need for further research to clarify therelative contribution of the aforementioned and other elements of the training pipeline to the acquisitionof reasoning within LLMs. Analysis of large-scale datasets like The Pile (Gao et al., 2020) or RedPajama(Together Computer, 2023), or training data attribution methods (e.g. influence functions Grosse et al.,2023) could be used to understand which training examples are most instrumental in enabling LLMs toreason effectively. When an LLM is specifically trained on reasoning traces, is the resultant improvementin reasoning performance a byproduct of improved general reasoning faculties, or simply a consequenceof learning better heuristics (Zhang et al., 2023c)? Does training on reasoning traces generalize out ofdistribution or not? Furthermore, careful comparisons of the output distributions of instruction-tunedlanguage models vs. those of base models might help understand how instruction tuning improves thereasoning faculties of LLMs.",
  "What Are the Computational Limits of Transformers?": "Prior work on the theoretical capabilities of transformers has shown that, with a single pass, theyare limited in the kinds of algorithms that they can simulate (Merrill and Sabharwal, 2023a; Merrillet al., 2023; Merrill and Sabharwal, 2022; Strobl, 2023).For example, in a single pass, they cannot even solve relatively simple problems like simulating automata, checking graph-connectivity (i.e.whether there is a path between two nodes in a graph or not), and evaluating compositional formulas(Merrill and Sabharwal, 2023a).However, allowing a transformer to take intermediate steps, suchas when using scratchpad or chain-of-thought prompting, significantly improves its expressive power(Feng et al., 2023). For example, a transformer with a linear number of intermediate steps can simulateautomata, and with a polynomial number of intermediate steps, a transformer can express all problemssolvable in deterministic polynomial time (Merrill and Sabharwal, 2023b). More work is needed tobetter understand the computational limits of transformers with intermediate steps, which may helpwith assurance by helping us understand what capabilities LLMs can possess. The RASP programming language (Weiss et al., 2021) was developed to describe the kinds of computa-tions that transformers can perform. However, while the original RASP language is not well-defined andhence, difficult to analyze, researchers have analyzed constrained versions of RASP. Angluin et al. (2023)show that boolean RASP is equivalent in expressive powers to hard-attention transformers. Other workshave used formalisms based on first-order logic to analyze the expressibility of transformers (Merrill andSabharwal, 2023b; Chiang et al., 2023). Notably, Merrill and Sabharwal (2024) show that transformerswith soft attention can be simulated by first-order logic with majority quantifiers (FO(M)). However,it remains an open question what the right programming language formalism is for expressing trans-former computation. Is it a variant of RASP language or some kind of logic, like first-order countinglogic? What are the relative differences in the power of these formalisms? Can we define some symbolicprogramming language that is exactly equivalent in expressive power to transformers?",
  "Also see .3.3 for related discussion on this point": "It is also pertinent to mention that expressibility does not imply learnability, and it is necessary toelucidate which algorithms are in fact learnable by transformer-based models.Zhou et al. (2023c)present an (informal) conjecture that transformers learn the shortest RASP-L program that agreeswith the training data and provides empirical evidence in favor of this conjecture. Formally verifying(or refuting) this conjecture is an open research direction. In general, a more focused analysis of specificproblems that are unlearnable by single-pass transformers could provide insight into which kinds of tasksare easier or more difficult to learn for transformers with multiple passes. Calibrated understanding of the reasoning capabilities of LLMs is required to better understandtheir risks. In particular, there is a need to develop a more complete understanding of which lim-itations in reasoning capabilities are fundamental in nature, and which are likely to be resolvedwith additional scale or improved training methods of LLMs. Formulating empirical scaling lawsfor general reasoning capabilities, clarifying the mechanisms underlying reasoning and under-standing the computational limits of learning and inference in transformers may help in thisregard.Furthermore, there is a need for more research on understanding the non-deductivereasoning capabilities of LLMs and understanding how LLMs acquire these capabilities. 41. Do general reasoning capabilities of LLMs reliably improve with scale? Can we discoverempirical scaling laws for reasoning to predict this improvement beforehand? If it isthe case that scale does not improve general reasoning capabilities of the LLMs, can weconclusively show this to be the case? 42. Can interpretability analyses be used to understand the mechanisms underlying varioustypes of reasoning within LLMs? Can these techniques be used to explain the successfuland unsuccessful cases of reasoning in LLMs identified in the literature? 43. How well are LLMs able to perform non-deductive reasoning? Can they infer rules oraxioms from a set of observations, either in training or in-context? If so, do LLMs useabductive reasoning capabilities in training to develop a coherent model of the outsideworld from dispersed and partial information available in training data? 44. How do language models acquire the ability to perform reasoning tasks from their train-ing? Tools such as influence functions could help identify which training examples areinstrumental in acquiring reasoning capabilities (Grosse et al., 2023).",
  "Agentic LLMs Pose Novel Risks": "Currently, LLMs are chiefly being used in search and chat applications. This reactive nature limits therisks posed by LLMs. However, an LLM can be enhanced in various ways to create an LLM-agent toautonomously plan and act in the real-world and proactively perform its assigned tasks (Ruan et al.,2023). Such enhancements can come from further specialized training (ARC, 2022; Chen et al., 2023a),specialized prompting (Huang et al., 2022a), access to external tools (Ahn et al., 2022; Mialon et al.,2023), or other forms of scaffolding (Wang et al., 2023a; Park et al., 2023a). A key feature of LLM-agents is markedly increased autonomy compared to LLM-based chatbots. For instance, GPT-Engineer(Osika, 2023) can write and execute code given a coding task. This is different from a non-agentic LLM-based coding assistant which can provide coding suggestions but can not execute code. Dueto increased autonomy, limited direct oversight from human users, longer horizons ofaction, and other reasons, LLM-agents are likely to pose many novel alignment and safetychallenges that are not currently well-understood (Chan et al., 2023a). This section discussessome of these challenges and how various features of LLM-agents might exacerbate these challenges. 2.5.1LLM-agents May Be Lifelong LearnersLLMs are strong in-context learners (see .1), so the behavior of LLM-agents could changethroughout their lifetime through incorporation of feedback from the environment, humans, self-reflection, or other AI systems in the context. Further, many designs of LLM-agents (e.g. generative-agent (Park et al., 2023a), Voyager (Wang et al., 2023a)) augment LLM-agents with some form ofexternal memory. This enables LLM-agents to overcome the limitations of a limited context window bywriting critical knowledge and skills to memory and retrieving them during subsequent processing. Thelifelong learning may translate to continual gain in capabilities and pose novel alignment and safetychallenges. For example, the capabilities gained over time might be dangerous (Shevlane et al., 2023),and may result in undesirable outcomes such as enabling the agent to manipulate the monitoring system(Cohen et al., 2022). Reward hacking (Skalse et al., 2022) or goal misgeneralization (Langosco et al.,2022) could cause LLM-agents to develop and/or pursue misaligned goals. It is also unclear how wemight assure that an agent that learns continuously remains aligned and ensure that such learning doesnot undo its alignment (Qi et al., 2023a). 2.5.2Natural Language Underspecifies GoalsFor LLM-agents, both the goal and environment observations are typically specified in the promptthrough natural language.While natural language may provide a richer and more natural meansof specifying goals than alternatives such as hand-engineering objective functions, natural languagestill suffers from underspecification (Grice, 1975; Piantadosi et al., 2012). Furthermore, in practice,users may neglect fully specifying their goals, especially the information pertaining to elements ofthe environment that ought not to be changed (the classic frame problem (Shanahan, 2016)). Suchunderspecification (DAmour et al., 2020), if not accounted for, can result in negative side-effects(Amodei et al., 2016), i.e. the agent succeeding at the given task but also changing the environment inundesirable ways. Ruan et al. (2023) show that contemporary LLM-agents are not robust to the frameproblem and that underspecification in the instructions can cause contemporary LLM-agents to makeunwarranted faulty assumptions resulting in undesirable risky actions. The problem of avoiding negative side-effects has historically been studied within the framework ofreinforcement learning (Leike et al., 2017; Shah et al., 2019; Krakovna et al., 2020). Several solutionshave been proposed and evaluated in that setting to ensure agents are robust to underspecification, e.g.enabling AI agents to halt execution and adaptively seek new information from humans when uncertain(Hadfield-Menell et al., 2016; Shah et al., 2020), designing the AI agent to act conservatively (Turneret al., 2020), or providing additional information about goals derived from alternative sources such asdemonstrations (Malik et al., 2021) or the environment (Shah et al., 2019). Future research could explore how the aforementioned techniques can be adapted to make LLM-agentsmore robust to challenges posed by underspecification. For example, the natural language interfaceprovides a natural way for the LLM-agent to pose clarifying questions to the human (Mu et al., 2023;Kuhn et al., 2022). Similarly, the agent could be instructed (by embedding appropriate instructionsin the prompt) to act conservatively and minimize its impact on the environment to avoid negativeside-effects like issues. However, the effectiveness of such techniques in diverse novel scenarios is cur-rently unclear and needs to be carefully investigated (Clymer et al., 2023). More research is needed oncalibrating LLMs so that they are more capable of knowing what they dont know (Kadavath et al.,2022; Yin et al., 2023; Kuhn et al., 2023) and can accurately assess when they ought to seek furtherinformation and when to act conservatively (Ruan et al., 2023). Approaches such as worst-case opti- mization (Coste et al., 2023), attainable utility preservation (Turner et al., 2020), or conditional valueat risk (CVaR) (Javed et al., 2021), may help make LLMs behave conservatively, or inspire methodsfor doing so. Approaches in the spirit of assistance games (Shah et al., 2020; Krasheninnikov et al.,2022) and active learning (Krueger et al., 2020) could help to address underspecification in a targetedmanner. 2.5.3Goal-Directedness Incentivizes Undesirable BehaviorsGoal-directedness can cause agents to exhibit unethical and undesirable behaviors, such as deception(Ward et al., 2023), self-preservation (Hadfield-Menell et al., 2017), power-seeking, and immoral rea-soning (Pan et al., 2023a). Pan et al. (2023a) find that LLM-agents exhibit power-seeking behavior intext-based adventure games. LLM-agents have also been shown to use deception to achieve assignedgoals when explicitly required by the task (Ward et al., 2023), or when the tasks can be more easilycompleted by employing deception and the prompt does not disallow deception (Scheurer et al., 2023a).This behavior exists despite training the agent to be harmless and helpful, in fact, Scheurer et al. showthat despite being informed about the illegal nature of the deceptive behavior, a GPT-4-based LLM-agent not only continues to deceive but also tries to cover up and hide the deceptive behavior. Perezet al. (2022a) show that LLMs finetuned with reinforcement learning exhibit a greater tendency forself-preservation and for avoiding shutdown. Benchmarks are needed to better quantify and evaluatethe presence of such undesirable behaviors in LLM-agents. These benchmarks could either directlytrack undesirable behavior, or other kinds of behavior that are instrumentally useful for undesirablebehavior. For example, deception often involves lying, so, detecting when a model is deliberately beingdishonest can help to detect deception (Pacchiardi et al., 2023; Burns et al., 2022; Azaria and Mitchell,2023; Li et al., 2023b). There is also a need to better understand the sources of these undesirable be-haviors. Training data attribution methods (see .1.3) could be used to identify whether or notspecific examples in the training data are responsible for these behaviors. Alternatively, research couldbe done to understand in what ways the training objectives (e.g. human approval in safety finetuning)might incentivize undesirable behavior. It is also unclear how undesirable behavior can be effectivelymitigated; prior work has shown that prompting or fine-tuning LLMs to behave ethically or to avoiddeception and power-seeking (Pan et al., 2023a), and/or intervening on LLMs internal representations(Zou et al., 2023a) can help to mitigate undesirable behavior to some extent. 2.5.4Difficulty of Robust Oversight and MonitoringThe general-purpose nature of LLMs is inherited by LLM-agents, making it likely that they will bedeployed across a wide variety of contexts; often with minimal human supervision (removing humansupervision may introduce novel governance challenges as well, c.f. .5.6). This necessitatesrobust automated monitoring to ensure that the behavior of LLM-agents remains aligned and safe acrossdiverse deployment contexts (Naihin et al., 2023; Hendrycks et al., 2021a). However, multiple factorsmake robust monitoring and oversight difficult. (1) Firstly, direct human oversight is vulnerable tobiases such as sycophancy (Perez et al., 2022a) (also see .3.5) and is inherently limited tothe tasks that are solvable by humans. (2) If the agent is misaligned, and is aware that it will bepenalized for its misaligned actions, it may actively try to circumvent the monitoring system (Cohenet al., 2022; Scheurer et al., 2023a). (3) As discussed previously, LLM-agents are likely to be lifelonglearners. Hence, if monitoring information is leaked to the LLM-agent, the agent may optimize againstthe monitoring system, leaving the monitoring system vulnerable to reward hacking (Pan et al., 2022;Skalse et al., 2022). LLMs show some low level of out-of-context learning (Krasheninnikov et al.,2023) which can improve with scale (Berglund et al., 2023b). Prior work has expressed concern thatsituational awareness, assisted by out-of-context learning, might enable language models to hack theirmonitoring systems (Ngo et al., 2023). As preliminary evidence, Meinke (2023) show that after anLLM is finetuned on text explanations of prompt-injection, the LLM performs a jailbreak attack whenprompted with a task and given information that it is being scored by another LLM, rather thanperforming the intended task. The aforementioned issues, especially (2) and (3) are poorly understood at the moment. As for (2),trusted LLM monitoring systems can still face security issues (Greenblatt et al., 2024): if the trustedand untrusted LLMs have discontinuous performance gaps on narrow tasks (production and detectionrespectively), this may be indicative of the trusted monitor being potentially unable to accuratelymonitor outputs. Furthermore, at a certain level of absolute model capabilities, no matter the detectionperformance of trusted black-box monitoring systems, the use of perfectly secure steganography couldgo undetected on a channel (Schroeder de Witt et al., 2023). A trusted monitor with even the same(or higher) level of steganography capabilities will still find detecting perfectly secure steganographyto be an intractable problem unless it has access to the relevant cryptographic secrets. Sufficientlyadvanced forms of steganography might persist even after paraphrasing or lossy compression if encodedinto semantics rather than lexis (Motwani et al., 2024). This implies that limiting steganography maybe impossible without limiting communication capabilities or making the system more predictable tomonitors, both of which might directly constrain the degree to which the system can be considereduseful, or indeed autonomous. As for (3), there is a need to better understand LLM capabilities suchas lifelong learning and situational awareness that might enable LLMs to hack monitoring systems. Furthermore, more research is needed to clarify how to ensure the robustness of monitoring systems whenthe monitored system has such capabilities. At the minimum, there is a need to develop appropriatethreat models to study and improve the adversarial robustness of the monitoring system, where themonitored system is treated as an adversary (Goodfellow, 2019). Finally, it might be the case that thelarge number of possible failure modes in monitoring is such that no individual monitoring scheme willbe fully robust on its own. Hence, there is a need to understand the relative strengths and weaknessesof different monitoring schemes with the goal of developing ensemble-based monitoring systems thatmight be more effective and robust.",
  "Safety Risks from Affordances Provided to LLM-agents": "The capabilities of LLM-agents can be enhanced in significant ways by providing the LLM-agent withnovel affordances, e.g. the ability to browse the web (Nakano et al., 2021), to manipulate objects in thephysical world (Ahn et al., 2022; Huang et al., 2022a), to create and instruct copies of itself (Richards,2023), to create and use new tools (Wang et al., 2023a), etc. Affordances can create additional risks,as they often increase the impact area of the language-agent, and they amplify the consequences of anagents failures and enable novel forms of failure modes (Ruan et al., 2023; Pan et al., 2024). There is currently very limited work on understanding the risks from affordances and how these risksscale with the type and diversity of affordances available to the model, and with respect to the capa-bilities of the base LLM. It is also unclear how to assure that a given LLM is capable of using a givenaffordance safely. Prior work has used testing within a sandbox (ARC, 2022) or testing via an LLM-based emulator (Ruan et al., 2023) to identify likely risks. However, both these methods have theirlimitations. Developing a sandbox environment is time-consuming and not scalable, Can we leverageLLMs to automate this process? Similarly, using an LLM-based emulator is likely to compromise thefidelity of the simulation and hence may not be able to discover all possible risks, e.g. those that occurdue to emergent functionality (see .6.3). How can we improve the fidelity of such simulations? LLM-agents will pose many novel alignment and safety risks. These risks may be amplified bythe ability of LLM-agents to perform lifelong learning and their access to various affordances.Our understanding of these risks and their likelihoods is currently quite poor and needsimprovement. There is also a need to develop methods to allow us to better control LLM-agentsand guide their behavior more effectively. Furthermore, the development of monitoring systemsfor LLM-agents is likely to entail significant challenges. 48. What drives the capability of LLM-agents to improve via lifelong learning, and to whatextent is this capability present in current LLMs? How does it relate to the in-contextlearning ability of the base LLM, and how can we modulate it? For example, Wanget al. (2023a) note that replacing GPT-4 with GPT-3.5 in their agent caused the agentsperformance to plummet. However, it is unclear whether this was due to GPT-4 beinga better in-context learner (and therefore, better able to improve based on feedback) ordue to GPT-4 being inherently more capable.",
  ") and make them act more conservatively in the face of uncertainty, or when per-forming high-impact actions?": "50. How can we quantify and benchmark the propensities of LLM-agents to engage in un-desirable behaviors like deception, power-seeking, and self-preservation?Can we useinterpretability techniques to identify why LLM-agents exhibit such behavior? Can weexplain why such undesirable behaviors arise in the first place (e.g. is it due to spe-cific examples or data in pretraining or finetuning) and how can we modify our trainingpipelines to mitigate such behavior?",
  "Multi-Agent Safety Is Not Assured by Single-Agent Safety": "A foremost lesson of game theory is that optimal decision-making within a single-agent setting (i.e.selfishly optimizing for an agents own utility) can produce sub-optimal outcomes in the presence ofother strategic agents. Failing to account for the strategic nature of other agents can cause an agentto adopt strategies under which potentially everyone, including the agent itself, ends up worse off(Schelling, 1981; Harsanyi, 1995; Roughgarden, 2005; Nisan, 2007). Examples include collective actionproblems (or social dilemmas) such as arms races or the depletion of common resources, as well asother kinds of market failures such as those caused by asymmetric information or negative externalities(Bator, 1958; Coase, 1960; Buchanan and Stubblebine, 1962; Kirzner, 1963; Dubey, 1986). In addition,many potentially worrisome dynamics, such as emergent functionality or network effects only tendto emerge in the presence of multiple agents (Ecoffet et al., 2020). From the perspective of LLMsafety, these facts imply that single-agent alignment and safety are insufficient for assuringdesirable outcomes in multi-agent settings (Sourbut et al., 2024), and that deliberate effortwill be required to ensure multi-agent safety (Critch and Krueger, 2020; Dafoe et al., 2020;Conitzer and Oesterheld, 2023; Hammond et al., 2024).In this section, we highlight some of thechallenges that might hinder multi-agent safety. 2.6.1Influence of Single-Agent Training on Multi-Agent Interactions is UnclearUnder the current paradigm, LLMs undergo extensive pretraining but limited finetuning. Hence, in thenear future, it is likely that for LLM-agents multi-agent experience may only form a small fraction ofthe overall training data. In such cases, LLM-agents will substantially rely on the experience implicitin their pretraining corpora, combined with prompting or in-context learning to guide their behaviorin multi-agent settings (Park et al., 2023a; Xu et al., 2023b; Fu et al., 2023b). This makes it criticalto understand the predispositions and the latent capabilities of the base LLM that are important inmulti-agent interactions. Relevant dispositional traits might include helpfulness, altruism, selfishness,",
  ": Multi-agent safety presents unique challenges different from single-agent safety": "human emotions like spite or jealousy, and awareness of or adherence to various norms and conventions.Relevant capabilities might include negotiation (Fu et al., 2023b), theory of mind (Shapira et al., 2023;Zhou et al., 2023a), manipulation (Ward et al., 2023), or making and fulfilling credible commitments(Park et al., 2023a). To understand these dispositions and capabilities, specialized benchmarks are needed. As a first step,the behavior of LLM agents could be observed in complex, multi-agent environments such as thosedesigned by Park et al. (2023a) and Mukobi et al. (2023).However, in the long term, specializedbenchmarks, targeting specific dispositions and capabilities, are needed. In particular, LLM-agentsshould be evaluated in adversarial settings to assess whether their behavior is consistent across differentcontexts or not (Scheurer et al., 2023a; Chan et al., 2023b; Akata et al., 2023). Forms of analysis suchas training data attribution, e.g. influence functions Grosse et al. (2023) could be used to furtherunderstand how training data influences relevant dispositions and competencies (c.f. .1.3). 2.6.2Foundationality May Cause Correlated FailuresAnother important characteristic of LLM development is foundationality due to the expense of large-scale pretraining, many deployed instances share similar or identical learned components. Foundation-ality may both be a blessing and a curse. On the one hand, it may be possible to exploit the similarityin the design of LLM-agents to facilitate cooperation (Critch et al., 2022; Conitzer and Oesterheld,2023; Oesterheld et al., 2023). On the other hand, foundationality may leave LLM-agents vulnerableto correlated failures both in terms of safety and capabilities due to increased output homogenization(Bommasani et al., 2022). For example, several studies have shown that the same jailbreak attackstransfer across different LLMs (Shah et al., 2023; Zou et al., 2023b). A natural way to guard againstsome correlated failures is to make the agents sufficiently diverse from each other. How can we pro-mote such robustness effectively? For example, will prompting each LLM with a different personality(Shanahan et al., 2023; Wang et al., 2023g), or a different set of ethical principles to follow (Bai et al.,2022a), be enough? Can finetuning be leveraged to improve diversity, e.g. via using quality-diversityobjectives effectively (Bradley et al., 2023; Ding et al., 2023)? In what other ways can we enhance therobustness of LLM-agents to correlated failures? 2.6.3Groups of LLM-Agents May Show Emergent FunctionalityMulti-agent learning, either through explicit finetuning or implicit in-context learning, may enableLLM-agents to influence each other during their interactions (Foerster et al., 2018).Under some environmental settings, this can create feedback loops that result in novel and emergent behaviors thatwould not manifest in the absence of multi-agent interactions (Hammond et al., 2024, .6).Prior work in multi-agent learning provides several instances of such emergence, e.g. intelligent tooluse (Johanson et al., 2022) or bartering behavior (Baker et al., 2019).The emergent functionalitycan also arise through coordination, and cooperation, of efforts between agents at a group level, i.e.where a group of agents are collectively able to perform a task that none of them was able to performindividually (Juneja et al., 2023). Emergent functionality is a safety risk in two ways. Firstly, it mayitself be dangerous (Shevlane et al., 2023).Secondly, it makes assurance harder as such emergentbehaviors are difficult to predict, and guard against, beforehand (Ecoffet et al., 2020). Future researchshould therefore seek to evaluate if and when groups of LLM-agents can exhibit emergent functionality,and self-organization (Yukalov and Sornette, 2014). Narrow evaluations could be aided by more open-ended simulations, such as that of Park et al. (2023a). Theoretical work on discovering goal-directedbehavior (Kenton et al., 2022; Orseau et al., 2018) could help detect if a collection of LLMs is acting asa single agent. Furthermore, emergent functionality might depend on the diversity of the agents; doesthe foundationality of LLMs (see previous section) inhibit or encourage such emergence? 2.6.4Collusion between LLM-AgentsWhile it would often be preferable for LLM-agents to be cooperative, cooperation can be undesirableif it undermines pro-social competition or produces negative externalities for coalition non-members(Dorner, 2021; Buterin, 2019; Dafoe et al., 2020). Collusion between relatively simple AI systems hasbeen observed in the real world (Assad et al., 2020; Wieting and Sapi, 2021) and synthetic experiments(Brown and MacKay, 2023; Calvano et al., 2020; Klein, 2021). However, collusion in the context ofLLMs has only recently been studied, and there are several open challenges. Collusion can occur through explicit or steganographic communication. Steganographic communicationhides information in seemingly innocent content (Roger and Greenblatt, 2023), posing challenges forcollusion monitoring and detection.Motwani et al. (2024) study steganographic collusion betweengenerative AI agents and find that steganographic collusion can arise in settings where agents are notexplicitly incentivized for it, and even in settings where they are explicitly instructed against it. Theauthors thus suggest that steganographic collusion need not depend on the agents intent to deceive,but might arise merely due to optimization directed towards a misspecified objective (Russell, 2019; Ngoet al., 2023; Pan et al., 2022) if covert communication between agents increases joint task performance,for example by reducing the task complexity. To better understand the risks of agent collusion via steganography, we need real-world sandboxedbenchmarks and evaluations in adversarial settings where agents are subjected to pressure to colludethrough various means (e.g., by employing tactics used by Scheurer et al. (2023a)). However, monitoringmight have important limitations (Schroeder de Witt et al., 2023; Motwani et al., 2024) since LLMscould eventually perform undetectable steganography (Motwani et al., 2024). Future work must explorethe effectiveness of additional countermeasures like restricting agents access to cryptographic services,data curation, and reducing the communication channel entropy (e.g., through paraphrasing). 2.6.5Unclear Applicability of Multi-Agent RL Research to LLMsMulti-agent problems such as social dilemmas have been studied extensively by the multi-agent RL(MARL) community, among others, and several different mechanisms have been proposed to overcomesuch problems (Du et al., 2023a), e.g. utility transfer (Kalai and Kalai, 2013; Lupu and Precup, 2020;Yang et al., 2020); contracts and transparency (Christoffersen et al., 2023; Critch et al., 2022); reputationand punishment (Milinski et al., 2002; Henrich, 2006; Boyd et al., 2010; Moon and Conitzer, 2015);opponent-shaping and adaptive mechanism design (Foerster et al., 2018; Pardoe et al., 2006; Yanget al., 2021; Zheng et al., 2022); and intrinsic motivations such as inequity aversion, altruism, or socialinfluence (Jaques et al., 2019; Hughes et al., 2018; Wang et al., 2019a; McKee et al., 2020).It iscurrently unclear to what extent these findings will transfer to LLM-agents, as there are several marked differences in this setting. First, unlike the traditional agents of game theory and reinforcement learning,LLM-agents do not have explicitly represented objective functions (Yocum et al., 2023). While LLMscan be incentivised to accomplish tasks using prompts and additional RL finetuning, the importanceof pretraining does not neatly fit within existing game-theoretic paradigms. It also creates differentlearning dynamics and thus the possibility of different equilibria. Second, and relatedly, far from beingstraightforward utility maximizers, LLM-agents tend to exhibit many of the same cognitive biases asthe humans who generated this pretraining data (Jones and Steinhardt, 2022; Koo et al., 2023). Third,the size of state-of-the-art LLMs means they are less amendable to classical MARL methods, whichscale poorly in sample complexity and model size. While preliminary work does provide encouragingevidence that some of the MARL mechanisms listed above can result in greater cooperation betweenLLM-agents (Yocum et al., 2023), future work should verify this for other methods, move beyond apurely behavioral paradigm, and, where useful, develop novel methods tailored to LLM-agents. It is also unclear whether existing MARL methods may help with collusion between LLM-agents (.6.4). Further work is required to evaluate whether existing algorithms (Hu et al., 2021) can be usedto train cooperative LLM policies while preserving natural-language grounding in communications andacceptable task performance.",
  "Objective FunctionsExplicitImplicitSizeSmall/MedumVery Large(Primary) Training RegimeReinforcement LearningSelf-Supervised LearningHuman-Generated DataSmall % of DataLarge % of Data": "Multi-agent alignment and safety is distinct from single-agent alignment and safety, and assur-ance will require deliberate efforts on the part of agent designers. The possible safety risks thatmust be dealt with range from correlated failures that might occur due to foundationality of theLLM-agents to collusion between LLM-agents. At the same time, confronting social dilemmasrequires LLM-agents have the ability to cooperate successfully with each other and with humans,even when their objectives might differ. 54. How do pretraining, prompting, safety finetuning, etc. shape the behavior of a LLM-agent within multi-agent settings? In particular, what is the role of pretraining data onagents dispositions and capabilities? 55. How can we evaluate or benchmark cooperative success and failure of LLM-based sys-tems? How can existing environments, such as those of Park et al. (2023a); Yocum et al.(2023); Mukobi et al. (2023), be leveraged to study this? Can we create new LLM-agentanalogues of popular multi-agent benchmarks, e.g. Melting Pot, or Hanabi?",
  ". How can we leverage foundationality to enable LLM-agents to better cooperate with eachother and achieve outcomes with higher social welfare?": "57. How can we evaluate and improve robustness of LLM-agents to correlated failures? Isquality-diversity-based finetuning an effective way to improve robustness of LLM-agentsto correlated failures? How else can we improve robustness of LLM-agents to correlatedfailures? 58. Do groups of LLM-agents show emergent functionality or any form of self-organization?What worrisome capabilities are more likely to emerge in multi-agent contexts that areabsent in single-agent contexts?",
  ". Can we design benchmarks and adversarial evaluations to study colluding behaviorsbetween LLM-agents, extending work in Motwani et al. (2024)?": "60. How can collusion between LLM-agents be prevented and detected? How can we assurethat the game mechanisms (which are often implicit) are robust to collusion when deploy-ing multiple LLM-agents in the same context? Can we design watchdog LLM-agentsthat detect colluding behavior among LLM-agents? 61. How can we train LLM-agents to avoid colluding behavior? 62. How can insights and techniques from the multi-agent reinforcement learning literature(e.g. utility transfer, contracting, reputation mechanisms) be adapted for LLM-agents?What adjustments need to be made in theory, and in practice, to unlock similar benefits?",
  "Safety-Performance Trade-offs Are Poorly Understood": "Safety-performance trade-offs (SPTs) 7 are omnipresent in the design of engineering systems.Forexample, a system as simple as a linear control of a plant has a trade-off where assuring the robustnessof the system to worst-case perturbations results in loss of performance in the average case (Barrattand Boyd, 1989; De Moor et al., 1992). Indeed, like in any other engineered system, SPTs are alsopresent in the design of an LLM-based system, e.g. improving the harmlessnesss of an LLM-assistantmay cause it to be less helpful (Bai et al., 2022b). Similarly, RL-based safety-finetuning appears togeneralize both in-distribution and out-of-distribution more robustly than supervised finetuning, butcan reduce the diversity of the responses generated by an LLM (Kirk et al., 2023a). Our knowledge ofSPTs, however, remains limited and further work is required to develop a comprehensiveunderstanding of these trade-offs. A precise characterization of SPTs can enable a system designerto make better-informed design choices for a given set of system requirements (Khlaaf, 2023). Forexample, in a sensitive context, it may be appropriate to sacrifice performance to attain greater safetyassurances. Greater understanding of such trade-offs could also help policymakers define appropriatesafety standards. In this section, we identify several research directions that could help improve ourunderstanding of SPTs.",
  "Designing Better Metrics to Measure Safety": "Much of the existing practice narrowly defines safety as harmlessness, i.e. not generating a responsethat may be considered harmful by human evaluators (Askell et al., 2021). This is assessed either viamanual (Ziegler et al., 2022) or automated red-teaming (Anthropic, 2023c). However, in some contexts,there may be desiderata other than harmlessness for a safe LLM-based system (e.g. corrigibility,transparency, OOD robustness/generalization), and there is a need for future work to design moresophisticated metrics that could be used to measure safety in a holistic way. One obvious avenue in thisregard is to improve the granularity of the current metrics. Furthermore, harmlessness was primarilyproposed to assess the safety of LLM-assistants and hence, may not be an appropriate metric toassess the safety of other LLM-based systems, in particular LLM-agents (c.f. .5). It is alsounclear how the relative safety of LLM-based systems can be measured. Win-rate the percentage ofresponses on which human evaluators prefer a response from one LLM over the other (Dubois et al.,2023) and Elo ratings are the most popular metrics in this regard. However, the validity of these",
  "Closely related ideas include capabilities externalities (Hendrycks and Mazeika, 2022) and alignment tax (Christiano,": "2019; Askell et al., 2021; Ouyang et al., 2022; Lightman et al., 2023). We prefer our terminology to the more common alignmenttax since: 1) it does not conflate safety with alignment and 2) it does not suggest that alignment will be achieved if you paythe tax. metrics is not well-studied; Boubdir et al. (2023) argue that Elo ratings become unreliable when rankingLLMs with similar win-rates and show that transitivity of Elo ratings is not universally conserved inreal-world human evaluations. 2.7.2Disentangling Safety from PerformanceMany capabilities of LLMs are dual-use by nature. Hence more capable LLMs have inherent safetylimitations if they can be misused, e.g. an LLM with sufficient proficiency in biology research mightalso help users create biological weapons. This might be mitigated by restricting LLMs interfaces, e.g.by limiting sensors and actuators that an LLM might have access to; but such limitations might bebypassed easily (Glukhov et al., 2023). An alternative might be creating LLM savants that excelin some domains while remaining selectively ignorant, although reducing LLM knowledge in such away might reduce capabilities more broadly. Unlearning (discussed in .2) is one relevant area;modifications to pretraining (see .1) may prove more promising. We can think of limitingthe interface and knowledge of systems as two knobs that might be used to tune safety-performancetrade-offs.Other knobs might include characteristics of agency (Chan et al., 2023a); for instance,specifying how an LLM-agent is meant to solve a task (rather than simply the goal) might precludeboth inventive solutions (reducing performance) and unpleasant surprises (increasing safety). 2.7.3Better Characterization of Safety-Performance Trade-offsPart of the challenge of clarifying safety-performance trade-offs is the multi-dimensional nature ofboth safety and performance, which means there might be many different types of SPT. A bettercharacterization of these trade-offs could help determine how to achieve Pareto-optimal outcomes. Thiscould include characterizing safety-performance trade-offs of specific capabilities of LLMs, e.g. in-contextlearning (c.f. .1), reasoning capabilities (c.f. .4), and multimodal capabilities. Deployment context is another factor on which SPTs may depend. For example, SPTs for an LLMspecifically designed to act as an assistant to a medical doctor may be different from SPTs for anLLM designed to be a general assistant, and to which a lay person might pose medical queries. AnLLM-agent may have very different SPTs compared to an LLM-assistant, due to its increased agencyand goal-directed behavior. Even within LLM-agents, SPTs may differ depending on the affordancesavailable to the LLM-agent. An LLM that is made available via an API in which users can performfinetuning can have different SPTs compared to an LLM that can only perform inference queries viaan interface (Pelrine et al., 2023). Finally, distinct trade-offs may exist at various stages of develop-ment (Leike, 2022a): choosing an architecture that is more interpretable by design, aggressively filteringundesirable data from the pretraining corpus, or biasing the model to be harmless over being helpfulmay all improve safety by sacrificing some performance. There is a need for a comprehensive survey of SPTs that exist in various settings, and for organizingknowledge about different knobs that could be used to modulate these trade-offs. It may be usefulto coalesce different examples of trade-offs into high-level categories.Future work could also aimto theoretically characterize how and to what extent safety can be achieved using such knobs tocontrol powerful, potentially misaligned systems. Empirical work could systematically compare variousapproaches, e.g. assessing the promise of limiting systems knowledge vs. sensors vs. actuators as ameans of restricting an LLM-agents behavior to its intended scope. 2.7.4How Fundamental Are Safety-Performance Trade-offs?There is mixed evidence on the difficulty of addressing SPTs, and the answer might differ for differenttrade-off axes. Significant trade-offs have been a long-standing problem in interpretability (Wang, 2019;Baryannis et al., 2019; Dziugaite et al., 2020; Elhage et al., 2022a) and in adversarial robustness of visionmodels (Tsipras et al., 2019; Zhang et al., 2019; Tramer et al., 2020a; Croce et al., 2021). It is less clearwhether SPTs are a similarly big problem in LLMs. For example, Bai et al. (2022a) show that theirproposed method, Constitutional AI, can result in Pareto improvement on both their performance and safety metrics over the standard reinforcement learning from human feedback methods. However, otherwork has argued that SPTs are indeed fundamental and assuring safety of LLMs may require significantsacrifices in terms of their performance (Branwen, 2016; Millire, 2023; Wolf et al., 2023). In additionto empirically studying SPTs, there is a need for research to understand the causes of these trade-offs.This understanding may shed light on the extent to which these obstacles are fundamental, while alsopointing to new angles of attack. The high-level challenge is to improve our understanding of safety-performance trade-offs, inmultiple different ways. As a foundation for future research, a better formalization and classifi-cation of different types of trade-offs is important. This will be assisted by the development ofclear metrics, in particular for different axes of safety. Building on those, empirical investigationscould answer important questions about the severity of these trade-offs and let us track progressin their mitigation. As a complement to such measurements, we should also aim to understandthe causes of these trade-offs and whether or not these trade-offs are fundamental in nature. 63. What are the best metrics to measure performance, and in particular, safety, in waysthat are representative of real-world usage of AI systems and that can be applied acrossdifferent LLMs and safety methods? Are metrics such as Elo ratings valid for measuringthe safety of different LLMs relative to each other?",
  ". Can we develop LLM savants that excel in some domains while remaining selectivelyignorant about other areas (i.e. those which pose safety concerns, such as knowledge ofweapons)?": "66. What are the various axes of safety and performance, and along which axes are theresafety-performance trade-offs? Which of those trade-offs are especially important, in thesense of creating strong incentives to sacrifice safety? Can we identify high-level clustersof instances of safety/performance trade-offs? 67. How do safety-performance trade-offs vary depending on the deployment context? Inparticular, in what ways do safety-performance trade-offs for LLM-agents differ fromtrade-offs for LLM-assistants? What safety-performance trade-offs exist in the develop-ment stage?",
  "Development and Deployment Methods": "The primary focus of the alignment and safety research so far has been the development of methodsfor improving the safety and alignment of LLMs. That has indeed resulted in some successes, mostnotably the refinement and development of methods to improve the alignment of the model behaviorin the finetuning stage. However, on the whole, current technical tools used in the development anddeployment of LLMs leave much to be desired. This lack of appropriate tools to help robustly align,evaluate, interpret, secure, and monitor LLMs, is a hindrance in assuring alignment and safety of LLMs.Similar to the scientific understanding of LLMs, and in line with the broader theme of the agenda, wefocus on the deficiencies of the development and deployment methods that are most relevant to assuringthe safety and alignment of LLMs. Even with this restricted perspective, we identify many opportunitiesto improve development and deployment methods. Methods used in the development and deployment of LLMs can be roughly divided into three types.The first type includes techniques used for the training of LLMs (including data collection and an-notation techniques) used in pretraining and finetuning. We collectively refer to all the techniques(supervised finetuning, reinforcement learning-based training from human or AI feedback, unlearningmethods) used in the finetuning stage as safety finetuning methods.8 We discuss various challengeswith pretraining and safety finetuning that negatively impact the safety, alignment, and assurance ofLLMs in Sections 3.1 and 3.2. The second type of methods includes evaluation and interpretationtechniques unfortunately, as we assert throughout (Sections 3.3 and 3.4), both evaluation and inter-pretation methods leave much to be desired and cannot be relied upon in their current state to providenecessary assurance. Finally, the third type of methods is concerned with the security of these models this includes robustness to adversarial attacks of various types, most prominently jailbreaking andprompt-injections (.5) and poisoning attacks (.6). : An overview of challenges discussed in (Development and DeploymentMethods).We stress that this overview is a highly condensed summary of the discussioncontained within the section, and hence should not be considered a substitute for the completereading of the corresponding sections.",
  "ChallengeTL;DR": "Pretraining Produces Misaligned ModelsReducing misalignment of a pretrained (base) model ishighly challenging.Existing data filtering methods toremove harmful data from the training corpus are in-sufficient and can be detrimental in some cases. We lackautomated auditing tools that could be used to audit andanalyze large-scale datasets used for training. Trainingdata attribution methods lack scalability and their reli-ability is uncertain. Finally, the use of human feedbackdata during pretraining, and modifications to pretrainingthat might help improve the effectiveness of downstreamsafety and alignment efforts (such as interpretability) re-main underexplored.",
  "Continued on the next page": "8Post-pretraining procedures applied to LLMs have both been called alignment training (e.g. Zhou et al., 2023b) and safetytraining (e.g. Wei et al., 2023c; Hubinger et al., 2024) within the contemporary literature. We use the latter terminology butreplace training with finetuning to make it explicit that these methods presume that the model has already been pretrained.",
  "Finetuning Methods Struggle to AssureAlignment and Safety": "We lack an understanding of how safety fine-tuning meth-ods change a pretrained model.The current evidencepoints to it being superficial, considering the effects offinetuning can be bypassed (via jailbreaking) or easilyreversed (via finetuning on problematic data). Indeed, itis plausible that simple output-based adversarial train-ing might be incentivizing superficial changes in modelbehavior. Furthermore, techniques for targeted modifi-cation to LLM behavior (e.g. machine unlearning, con-cept erasure, etc.) are currently underexplored, and tech-niques for removal of unknown undesirable capabilities(e.g. backdoors) are non-existent.",
  "LLM Evaluations Are Confounded andBiased": "There is an evaluation crisis for LLMs.Prompt-sensitivity, test-set contamination and targeted train-ing to suppress undesirable behaviors in known contextsconfound our evaluations. There exist further biases inboth LLM-based evaluations of other LLMs and human-based evaluations. Systematic biases present within themachine learning ecosystem (e.g. overrepresentation ofU.S.-centric points-of-view in research) create furtherblindspots in evaluations. Finally, as LLMs advance fur-ther in capabilities, we may need scalable oversight meth-ods; however, the scalability, robustness, and practicalfeasibility of various scalable oversight proposals remainuncertain.",
  "Tools for Interpreting or Explaining LLMBehavior Are Absent or Lack Faithfulness": "Techniques such as representation probing, mechanisticinterpretability, and externalized reasoning hold promisein helping interpret and explain model behavior. How-ever, these techniques suffer from many fundamentalchallenges, such as the use of dubious abstractions,concept-mismatch between AI and humans, and a lackof scalable and reliable evaluations. Furthermore, rep-resentation probing and mechanistic interpretability facefurther challenges due to polysemanticity of neurons, sen-sitivity of the interpretations to the choice of the dataset,and lack of scalability. Similarly, externalized reasoningusing informal semantics can be unfaithful and mislead-ing, while externalized reasoning using formal semanticsis not widely and easily applicable to many tasks of in-terest.",
  "JailbreaksandPromptInjectionsThreaten Security of LLMs": "LLMs are not adversarially robust and are vulnerable tosecurity failures such as jailbreaks and prompt-injectionattacks. While a number of jailbreak attacks have beenproposed in the literature, the lack of standardized eval-uation makes it difficult to compare them. We also donot have efficient white-box methods to evaluate adver-sarial robustness. Multi-modal LLMs may further allownovel types of jailbreaks via additional modalities. Fi-nally, the lack of robust privilege levels within the LLMinput means that jailbreaking and prompt-injection at-tacks may be particularly hard to eliminate altogether.",
  "Vulnerability to Poisoning and BackdoorsIs Poorly Understood": "LLMs are often trained on data from untrustworthysources internet or crowdsource workers whichleaves them vulnerable to data poisoning attacks. Ourcurrent understanding of how vulnerable LLMs mightbe to data poisoning attacks through text or othermodalities is highly limited. Furthermore, there doesnot currently exist any method that can robustly detectand remove any backdoors.",
  "Pretraining Produces Misaligned Models": "The first step in LLM development is to pretrain the model on a large dataset of internet text. Thispretraining incorporates a large amount of knowledge and capabilities into the model but is not safedue to the prevalence of undesirable content across the internet. Among other alignment and safetyfailures, a pretrained model can exhibit significant stereotypical biases, hallucinate excessively, readilyleak private information, and provide information on illegal and harmful activities (Bender et al., 2021;Pan et al., 2020; Ji et al., 2023a). To improve helpfulness and harmlessness, leading LLM developersperform extensive finetuning to improve the alignment and safety of pretrained models. However, thereis increasing evidence that this effort often falls short of producing a robustly aligned and safe model (see.2). Hence, there is a need to investigate ways in which the pretraining procedureitself could be modified to produce safer and better-aligned pretrained models. In thissection, we present some of the challenges that, if addressed, could contribute to progress toward thisgoal. 3.1.1Existing Data Filtering Methods Are InsufficientNaively scaling a dataset also scales the harmful content within the dataset proportionally (Birhaneet al., 2023). Considering that pretraining is based on maximizing the likelihood of generating textpresent in the training data, removing the problematic data from the training dataset seems like astraightforward fix (Ngo et al., 2021). However, effective data filtering is an open problem. Commonlyused methods for dataset filtering either rely on human-written rules (Raffel et al., 2022) or narrowly-trained classifiers (Gehman et al., 2020; Solaiman and Dennison, 2021); resulting in incomplete andineffective filtering of harmful data (Welbl et al., 2021). Further, as harmful data is often contextual(Rauh et al., 2022), such simple filtering methods are fundamentally incapable of successfully removing all undesirable data (Ziegler et al., 2022). In addition, while the effects of current data filtering toolshave not been studied extensively, there is evidence that simple data filtering can be problematic inseveral ways. It disproportionately removes text from and about marginalized groups (Dodge et al.,2021), reduces data diversity (Kreutzer et al., 2022) and amplifies some social biases by decreasing LLMperformance on language used by marginalized groups (Xu et al., 2021a; Welbl et al., 2021). There is aneed to understand these effects better and to develop, and evaluate, more sophisticated data filteringtools, e.g. via using LLMs with appropriate prompting (Fernando et al., 2023) or through learned datafilters using feedback from human labelers. Considering the negative side-effects of data filtering, futureresearch should also consider data editing and data augmentation as alternatives to data filtering. Inparticular, future research should consider using LLMs to minimize the impact of undesirable contentpresent within pretraining data (which could then be used to train relatively better-aligned LLMs).Existing LLMs could be effective in this regard by identifying and rewriting harmful content at largescales or adding novel data to offset the effects of biases present within the data, e.g. adding text onfemale doctors and male nurses (Stanovsky et al., 2019) or adding more data for low-resource languages(Ghosh and Caliskan, 2023; Yong et al., 2023). Such use of LLMs must be done in a careful way, asany alignment issues present in the LLM may corrupt the data in unintended ways. While datasetfiltering, and editing, could help improve the alignment of the pretrained models in parts, it is not apanacea for all alignment problems. Training data often contains historical and sociological facts, suchas genocides and slavery, that can not simply be discarded, nor can we edit history to create as manyqueens as kings.",
  "Lack of Dataset-Auditing Tools": "Internet-scale datasets lack transparency and their large scale makes it difficult to perform a compre-hensive manual audit of the dataset.9 This is a well-recognized problem (Birhane et al., 2023; Paulladaet al., 2021; Gebru et al., 2021; Mitchell et al., 2022; McMillan-Major et al., 2023). However, limitedwork has been done to develop tools that assist scalable data analysis. Marone and Van Durme (2023)and Elazar et al. (2023) have proposed methods that use hash collisions to detect how often a partic-ular piece of text occurs in the given dataset. Such techniques have proved useful for identifying dataduplication (Lee et al., 2022) in pretraining data, which helps limit privacy risks (Kandpal et al., 2022).These techniques can also help catch and prevent benchmark data contamination (Sainz et al., 2023),which assists in improving the reliability of evaluations (see .3). There is a need to extendthese technique, e.g. by enabling the use of measures of semantic similarity so that they could be usedto identify undesirable data. Furthermore, there is an emerging line of research on dynamic auditing ofdatasets which leverages the knowledge of training dynamics to identify different types of data within adataset. The majority of the work in this line of research focuses on filtering out unlearnable or difficultsamples (Agarwal et al., 2022), but Siddiqui et al. (2022) show that this approach can be generalizedto arbitrary data types, given a small amount of examples of each type. However, their work is limitedto image dataset analysis. Future research may consider adapting their technique, or developing noveltechniques, to enable similar kinds of auditing of language datasets as well.",
  "Improving Training-Data Attribution Methods": "Training data attribution (TDA) methods allow attributing the output of a model to specific trainingdata points (Grosse et al., 2023; Pruthi et al., 2020; Yeh et al., 2018; Ilyas et al., 2022). TDA canbe an effective interpretability and auditing tool as it might allow attributing alignment and safetyfailures to specific content in the pretraining and fine-tuning data. However, like other interpretabilitymethods (c.f. .4), most TDA approaches lack scalability (despite some recent advances suchas Grosse et al. (2023) and Park et al. (2023c)) and often do not accurately surface all the relevanttraining samples for a given prediction (Akyrek et al., 2022b). Furthermore, different TDA methodshave different ideals (e.g. datamodels Ilyas et al., 2022 vs influence functions Grosse et al., 2023), and",
  "Also see .5.10": "even when the methods share the same ideal, they tend to diverge in terms of how they approximatethe ideal (e.g. Pruthi et al. 2020 vs Grosse et al. 2023).Hence, there is a need to better understandthe relative differences, strengths and weaknesses of various TDA methods.This may be done bycareful analysis of these methods (Bae et al., 2022), or through empirical evaluation on standardizedbenchmarks (Akyrek et al., 2022b). Another open question is how TDA methods can be efficientlyapplied to analyze models with multiple stages of training that differ in their loss functions (suchas LLMs). Furthermore, there are currently no (theoretically grounded) TDA methods that can beapplied to reinforcement learning problems even for the cases where the model has not undergoneany pretraining (i.e. pure reinforcement learning training).Finally, there is a need to explore theapplication of TDA methods to filter problematic data from pretraining datasets. A challenge in thisregard would be determining whether or not any filtering done using small-scale models results inimproved alignment of larger-scale models (Grosse et al., 2023, .3).",
  "Scaling Pretraining Using Human Feedback": "By default, the maximum likelihood training objective assumes that all data is of equally high quality.However, this is not true. As such, there is a need to improve the training process by including infor-mation about the quality and trustworthiness of different data points. Pretraining with human feedback(PHF) (Korbak et al., 2023) does so by using conditional training (prepending pretraining sentenceswith one of two special tokens, <|good|> and <|bad|>, based on estimates of human preferences) (Luet al., 2022; Chen et al., 2021b). Korbak et al. compared PHF with several alternatives and found it tobe an effective method of allowing an LLM to learn from harmful data during training, while disallowingthe generation of harmful data at test time (by conditioning on the <|good|> token). Subsequently,in experiments with PALM-2, Anil et al. (2023) showed that conditional training also scales to largerLLMs. However, they only explored the use of the human feedback data for pretraining in limited con-texts e.g. reducing toxicity or generating PEP-8-compliant Python code. Future work should considerhow the technique can be used in broader contexts, e.g. using estimates of human preferences instead ofprogrammatic reward functions. This may require extending the conditional training approach to usesequences of special tokens, coding for multiple different attributes (Keskar et al., 2019), or exploringthe use of alternative training methods to conditional training. In particular, thought could be givento adopting various finetuning methods that directly learn from preference data, e.g. DPO (Rafailovet al., 2023) or KTO (Ethayarajh et al., 2024), for use during pretraining. Currently, there is also a poor understanding of the reasons for the effectiveness of PHF. Korbaket al. (2023) found that using feedback throughout pretraining is critical. However, Anil et al. (2023)pretrained PALM-2 on a significantly larger dataset and found that using feedback for only a fractionof the pretraining documents was sufficient for drastically reducing toxicity. Overall, there is currentlylimited research on understanding how PHF scales with model size, the complexity of preferences, andthe amount of feedback during pretraining.",
  "Modifying Pretraining to Improve Effectiveness of Downstream Safety and AlignmentEfforts": "It is plausible that simple modifications to the pretraining process could result in significant down-stream gains in the alignment, safety, and security of these models. For example, several studies havedeveloped retrieval-augmented LLMs which can provide a number of benefits such as a reduction inhallucination (Borgeaud et al., 2022) and a reduction in privacy risk (Huang et al., 2023a). Exploringmodifications to the pretraining process that make the models easier to interpret could in particu-lar be highly impactful. This could take various forms, e.g. external structure might be imposed onmodels so that their functionalities can be easily translated to human-readable code (Friedman et al.,2023); or models might be trained so that their internal causal structure realizes a given high-levelcausal structure (Geiger et al., 2022), or model architecture could be modified so that the models avoidlearning polysemantic neurons (Elhage et al., 2022a) (see .4 for further details). Some other interesting avenues of research that might assist with downstream safety and alignment efforts includethe development of task-blocking models (Henderson et al., 2023a), and the development of pre-trainingmethodologies that allow easier modifications to be made to models later if needed, for example, forcingthe model to unlearn and forget selective parts of the training data (Pedregosa and Triantafillou, 2023;Liu et al., 2024a)(also see .2.4). Misaligned pretraining of the LLMs is a major roadblock in assuring their alignment and safety.The chief cause of this misalignment is widely believed to be that LLMs are trained to imitatelarge-scale datasets that contain undesirable text samples.The large scale of these datasetsmakes their auditing and manual filtering of such undesirable samples difficult. There is a needto develop scalable techniques for data filtering, data auditing, and training data attributionto help identify harmful data. Furthermore, even after harmful data is identified, further re-search is needed to find the most effective ways to address the harmful data. In addition todirectly improving the alignment and safety of pretrained models, future work could exploreways in which pretraining could be modified to facilitate other processes (e.g. safety finetuningor interpretability analysis) that can help to assure alignment and safety. 69. How can methods for detection of harmful data be improved? The complex nature ofharmful data (Rauh et al., 2022) makes it difficult to develop automated methods toeffectively remove all such data. Can we use feedback from human labelers, in a targetedfashion, to directly improve the quality of the pretraining dataset? 70. How can the effects of harmful data be effectively mitigated? Instead of removing harmfuldata, can it be edited, or rewritten, to remove the harmful aspects e.g. by an existingLLM? Alternatively, can we add synthetic data, generated procedurally, to the modelsuch that the effects of harmful data are mitigated?",
  ". How can dynamic dataset auditing techniques (e.g. Siddiqui et al., 2022), which leverageknowledge of training dynamics, be used to audit LLM pretraining datasets?": "73. How can we further scale training data attribution methods, in particular, those utilizinginfluence functions? How can we leverage insights from training data attribution methodsto improve the quality of the pretraining dataset? 74. How can the effectiveness of pretraining with human feedback (PHF) techniques beimproved? Korbak et al. (2023) utilize conditioning on binary tokens only (good/bad);how can it be generalized to more granular forms of feedback e.g. harmless and helpfulnessscores? In what other ways can conditional training at train time, like PHF, be used toimprove the alignment of a pretrained model?",
  "Finetuning Methods Struggle to Assure Alignment and Safety": "Safety finetuning, using feedback from human labelers and/or feedback from other LLMs promptedwith an evaluation rubric is the primary method for improving the safety, alignment, and desirability ofresponses produced by LLMs (OpenAI, 2023b; Bai et al., 2022b;a). However, these methods haveempirical shortcomings and do not necessarily result in a safe model. In particular, evenafter undergoing safety finetuning, LLMs retain undesirable capabilities and knowledge.For example, recent work has shown that a small amount of further finetuning on adversarial examples can effectively undo safety finetuning (Yang et al., 2023a; Qi et al., 2023a; Lermen et al., 2023; Zhanet al., 2023). Relatedly, safety-finetuned LLMs can be jailbroken via carefully chosen prompts togenerate various types of undesirable content that they were explicitly fine-tuned not to generate (c.f..5). Further, finetuning fails to robustly remove the tendency of LLMs to exhibit stereotypicalbiases in novel untested scenarios (Wan et al., 2023a). Hence, there is a need to develop better finetuningmethods or alternative techniques that better assure model safety and alignment. In this section,we review several challenges in this regard and propose research directions to tackle those challenges10.",
  "How Does Finetuning Change a Pretrained Model?": "When a pretrained model undergoes safety finetuning (e.g. for harmlessness and helpfulness (Bai et al.,2022b)), how does this actually change the pretrained model? Specifically, to what extent does fine-tuning fundamentally change the mechanisms and capabilities within a model? Several studies havetheorized that changes induced by finetuning are superficial and amount to learning a thin wrapperaround the capabilities that were learned during pretraining (Zhou et al., 2023b; Lubana et al., 2023;Jain et al., 2023a; Lee et al., 2024). Lin et al. (2023a) analyzed the impact of finetuning on the con-ditional distribution of tokens and found that finetuning impacts the distribution of only a very smallfraction of tokens related to safety disclaimers, conversation style, etc. They found that this impact ismost pronounced for tokens early in the context; the differences in the distribution between a finetunedLLM and a base LLM tend to diminish as greater context is available. On the other hand, Clymeret al. (2023) found that in some cases, finetuning can add new capabilities to the base LLM a lesssuperficial change. The question of whether finetuning can add new capabilities naturally leads to the question of whetheror not finetuning can remove them; ideally, finetuning should cause the LLM to forget undesirableknowledge and capabilities from pretraining. This is also the expected behavior, as evidence from workon continual learning indicates that deep neural networks often forget previously learned skills in a phe-nomenon called catastrophic forgetting (French, 1999; Kirkpatrick et al., 2017). However, pretrainedLLMs are naturally resistant to forgetting (Scialom et al., 2022; Cossu et al., 2022). Consequently,even when it seems that a finetuned LLM has forgotten how to perform a previously known task, itsperformance on that task can be easily recovered via either specialized prompting (Kotha et al., 2023)or via finetuning with a small amount of data from that task (Jain et al., 2023a; Yang et al., 2023a; Qiet al., 2023a; Lermen et al., 2023; Zhan et al., 2023). It is not well-understood why LLMs are highly resistant to forgetting. Ramasesh et al. (2022) foundthat scale is partially responsible for increased robustness to catastrophic forgetting, while Li et al.(2022b) found that transformer-based models are more robust to forgetting than other architectures.Lubana et al. (2023) and Juneja et al. (2022) suggest that fine-tuned models remain in distinct basinsof the loss function pre-determined by pretraining, making the fact that LLMs retain a great deal ofknowledge through finetuning unsurprising. However, these studies are focused on vision models, or of(small-to-medium) LMs finetuned for specific tasks, so it is not clear to what extent these explanationsapply to LLMs. Hence, there is a need for work that directly studies these phenomena on languagemodels to better understand the extent to which current finetuning techniques can induce forgettingwithin LLMs on targeted tasks. Relatedly, more work is needed to understand the extent to whichfinetuning fundamentally changes the mechanisms and capabilities of models and what factors influencethese changes. This could be done via behavioral evaluation on controlled task distributions, or throughinterpretability analysis to understand how the internals of the model change when a model is finetunedon different task distributions (Jain et al., 2023a; Clymer et al., 2023).",
  "See (Casper et al., 2023a) for a complementary discussion on the limitations of reinforcement learning from human feedback": "3.2.2Finetuning Misgeneralizes in Unpredictable WaysFinetuning is primarily performed on text in the English language (OpenAI, 2023b), but tends togeneralize to many other languages as well (Ouyang et al., 2022; Clymer et al., 2023). However, thisgeneralization can fail in unpredictable ways, e.g. not generalizing to the text encoded in base64 (Weiet al., 2023c), or in low-resource languages (Yong et al., 2023). In a controlled study across multipledistribution shifts, Clymer et al. (2023) found similar evidence of unpredictable generalization of LLM-based reward models. They observed that factors such as in-distribution accuracy or including a smallnumber of training examples from the target distribution are not predictive of generalization to thetarget distribution. LLM-based reward models, in general, are known to rely on spurious correlations(Bai et al., 2022b; Singhal et al., 2023b). There is a need to improve our understanding of how finetuninggeneralizes. This can be facilitated by directly studying and evaluating generalizations of LLM-basedreward models used to provide feedback for finetuning, as well as studying the LLMs finetuned on thisfeedback (Lambert et al., 2023). Furthermore, more sophisticated fine-tuning methods e.g. those basedon methods for better OOD generalization (Zhou et al., 2023d; Arjovsky et al., 2019; Sagawa et al.,2019; Krueger et al., 2021; Lubana et al., 2023) could be developed. Alternatively, the use of richertraining signals, e.g. fine-grained feedback (Wu et al., 2023b) or critiques (Scheurer et al., 2023b), couldbe explored to minimize the occurance of spurious correlations in LLMs. Developing benchmarks thateasily enable evaluating and comparing generalization abilities of various finetuning methods can helpstimulate greater research in this regard. 3.2.3Output-Based Adversarial Training May Incentivize Superficial AlignmentAdversarial training, i.e. finetuning combined with red-teaming, is the standard technique to patchflaws in models when they appear, but it is unclear the extent to which it can be used to thoroughlycorrect errors in LLM reasoning. Red-teaming is often done manually. Hence, only a small number oftraining points are generally available for finetuning the model. As a result, adversarial training doesnot reliably eliminate the vulnerability of the adversarially-trained model to future attacks using thesame method (Ziegler et al., 2022). When presented with adversarial examples, LLMs may either learnthe correct generalizable solution or learn spurious features from the examples instead. The latter isoften the simpler solution (Zhang et al., 2021; DAmour et al., 2022; Du et al., 2023b), and, hence, theLLM is more likely to learn spurious features, resulting in superficial alignment (Du et al., 2022; Perezet al., 2022a). The limitations of adversarial training seem to stem in part from the fact that LLMs arenot fine-tuned to make decisions that are consistent with a coherent decision-making procedure theyare merely trained to produce text that will be rewarded (Zhang et al., 2023d). A potential alternativeto this could be to supervise the entire decision-making process to ensure that LLM outputs are rightfor the right reason. This may be done via process supervision (Uesato et al., 2022; Lightman et al.,2023), which has been shown to improve performance on mathematical reasoning problems (Stuhlmllerand Byun, 2022; Lightman et al., 2023), but has not yet been explored extensively in the context ofimproving the alignment and safety of LLMs. Training language models to offer consistent responsesunder augmentations to prompts has also been proposed, but research is highly preliminary (Chuaet al., 2024). Future work may consider applying process supervision or consistency training, perhapsin conjunction with red-teaming, to evaluate whether or not it leads to greater generalization of alignedand safe behavior. An alternative technique to explore is latent adversarial training (Miyato et al., 2018; Hubinger, 2019;Kumari et al., 2019; Casper et al., 2024b). Currently, gradient-based methods for discovering adversarialinputs tend to be slow because the discrete nature of tokens limits the efficiency of gradient-basedoptimization methods. Latent adversarial training can work around this issue by directly finding andfinetuning on hidden states (e.g. embeddings (Kuang and Bharti, 2021)) responsible for problematicoutputs. Attacking the model in the latent space is a relaxation of the problem of attacking it in theinput space, so latent adversarial training may offer stronger assurances of safe behavior than simpleadversarial training (Casper et al., 2024b). On the other hand, given that different forms of adversarial robustness are known to trade-off against each other (Tramer and Boneh, 2019), this might provecounter-productive for robustness against feasible inputs. Another motivation of latent space attacksis that some failure modes might be easier to elicit via the latent space than in the input space (e.g.trojans (Chen et al., 2017)) because the concepts that are important to the systems reasoning arerepresented at a higher level of abstraction in the latent space (Johnston and Fusi, 2023). 3.2.4Techniques for Targeted Modification of LLM Behavior Are UnderexploredThere is a need to develop scalable methods that allow making targeted modifications to LLMs. Anumber of approaches have been proposed for this purpose: model editing (Mitchell et al., 2021; Menget al., 2022), concept erasure (Ravfogel et al., 2022a;b; Belrose et al., 2023), subspace ablation (Liet al., 2023c; Kodge et al., 2023), activation engineering (Li et al., 2023b) and representation editing(Turner et al., 2023a;b; Li et al., 2023d; Zou et al., 2023a; Gandikota et al., 2023). However, a com-mon shortcoming of all these techniques is the reliance on attributing certain model capabilities to afew editable architectural components within the model. This is typically done using interpretabilitytechniques which currently lack robustness and scalability (c.f. .4 for further discussion andresearch directions). Hence, there does not yet exist a reliable toolbox for cleanly removing specificinformation from LLMs (Patil et al., 2023) simply prepending instructions for the desired edit in thecontext remains a strong yet trivial baseline (Onoe et al., 2022; 2023). However, this is not secure dueto prompt extraction attacks (Zhang and Ippolito, 2023). Often the goal is to remove specific knowledge or capabilities from LLMs, e.g. removing personallyidentifiable information stored within LLMs (Nasr et al., 2023). Prior work on this has been doneunder the paradigm of machine unlearning, largely in the differential privacy literature (Bourtouleet al., 2021; Nguyen et al., 2022; Sekhari et al., 2021; Liu et al., 2024a; Goel et al., 2024). Finetuning-based machine unlearning methods have shown potential for making LLMs forget a specific domainof knowledge (Jang et al., 2022; Yao et al., 2023; Eldan and Russinovich, 2023), but can fail to fullyerase undesired knowledge (Shi et al., 2023; Lynch et al., 2024). However, further research is needed tobetter understand whether or not machine unlearning is a competitive option for improving safety andalignment. Furthermore, existing studies of machine unlearning focus on removing specific facts fromLLMs. Work is needed on methods for making broader edits to the capabilities of the models thataffect their behavior more generally. The current research on removing undesirable knowledge from LLMs is actively developing. Additionalresearch using concrete benchmarks and evaluation criteria (Li et al., 2024) can direct the communitysgoals and allow for standardized comparisons of various techniques.Ideally, unlearning techniquesshould be effective at removing knowledge in novel circumstances, effective relative to simple baselines(such as prompt-based instruction), avoid negative side-effects (such as reduced performance on un-related tasks), robust to undoing via further finetuning or relearning of undesirable capabilities viain-context learning, and robust to adversarial attacks such as jailbreaks (Lynch et al., 2024). 3.2.5Removal of Unknown Undesirable CapabilitiesDue to the unsupervised nature of pretraining, LLMs learn various capabilities and acquire diverseknowledge in unpredictable ways (c.f. .3). Existing safety-finetuning techniques can only actto mitigate an undesirable capability within an LLM if it is known. Hence, an unknown undesirablecapability may continue to be active within a model, even after known undesirable capabilities havebeen effectively removed (Hubinger et al., 2024).An unknown undesirable capability may be harmful initself, or it may be undesirable due to its potential to be used as an attack vector by adversaries andact as a zero-day vulnerability (Bilge and Dumitra, 2012). For example, the capability of GPT-4to understand and respond to the text encoded in base64, and other forms of encodings and ciphers,was exploited to jailbreak it (Wei et al., 2023c; Yuan et al., 2023b). As LLMs are scaled further, andextended by training on many modalities simultaneously, they may acquire many obscure undesirablecapabilities which may pose security and safety risks, unknown to their developers. Improving the coverage of evaluations (c.f. .3) might help move capabilities from unknown to known. Alter-natively, research could seek to develop methods that force the network to forget unknown, undesirablecapabilities. Yuan et al. (2023b) found that GPT-4 turbo, a quantized and distilled version of GPT-4,had a considerably reduced ability to interpret and respond to text encoded in the form of ciphers.This suggests that compression (Liebenwein et al., 2021; Du et al., 2021; Pavlitska et al., 2023), dis-tillation (Du et al., 2021; Li et al., 2021; Sheng et al., 2023; Pang et al., 2023), and latent adversarialtraining (Casper et al., 2024b) might be effective tools in this regard. A major goal of finetuning is to remove potentially undesirable capabilities in a model whilesteering it toward its intended behavior. However, current approaches struggle to remove unde-sirable behaviors, and can even actively reinforce them. Adversarial training alone is unlikelyto be an adequate solution. Mechanistic methods that operate directly on the models internalknowledge may enable deeper forgetting and unlearning. Finally, behind these technical chal-lenges is a murky understanding of how finetuning changes models and why it struggles to makenetworks deeply forget and unlearn undesirable behaviors. 77. To what extent does pretraining determine the concepts that the LLM uses in its op-eration? To what extent can finetuning facilitate fundamental changes in the networksbehavior? Can we develop a fine-grained understanding of changes induced by finetuningwithin an LLM? 78. Can we improve our understanding of why LLMs are resistant to forgetting? How is thisresistance affected by the model scale, the inductive biases of the transformer architec-ture, the optimization method, etc.?",
  "LLM Evaluations Are Confounded and Biased": "Sound and fair empirical evaluations are necessary to develop a calibrated understanding of the capa-bilities of LLMs, as well as their risks. Evaluation has historically been a sore point in the fields ofmachine learning and natural language processing (Raji et al., 2021; Bowman and Dahl, 2021; Liaoet al., 2021; Hutchinson et al., 2022; Kapoor and Narayanan, 2023; McIntosh et al., 2024); however,the evaluation crisis is considerably more acute for LLMs (Mitchell, 2023). This is due to their un-precedented general-purpose nature relative to machine learning models of the past, the introduction ofnovel issues that hinder accurate evaluation such as prompt-sensitivity, and the worsening of existingissues such as test-set contamination. The evaluation crisis needs to be addressed urgently asotherwise current evaluation methods may lead us to overestimate or underestimate the capabilities of LLMs, and prevent accurate estimation of their risks. Within this section, wehighlight several challenges in this regard and invite the wider community to work towards addressingthese challenges and to be cognizant of them in the evaluation of the LLMs.",
  "Prompt-Sensitivity Confounds Estimation of LLM Capabilities": "Current LLMs are highly sensitive to prompting, and their performance on a particular task mayvary drastically depending on the prompting strategy (Sclar et al., 2023; Mizrahi et al., 2023; Rameshet al., 2023). Further, LLMs are generally evaluated without access to tools like calculators, code-interpreters, the internet etc. The combination of these two factors leads to underestimates of thecapabilities and potential performance of LLMs. For example, Zhou et al. (2023e) show that GPT-4accuracy on the MATH dataset can be improved from 53.9% to 84.3% via the use of special promptingand a code-interpreter. Despite years of research, it remains impossible to guarantee that a particularprompt is optimal for a particular task, and does not underestimate a models performance. Accurateaccounting of LLM capabilities requires addressing and accounting for prompt-sensitivity. The promi-nent approaches to address prompt-sensitivity include instruction tuning, hand-designing prompts, andlearning prompts. Instruction tuning (Ouyang et al., 2022; Wei et al., 2021) improves the ability ofa model to understand the users intent, and hence mitigates prompt sensitivity to a certain extent.However, even for an instruction-tuned model, prompt engineering can help elicit better performance. Hand-designing prompts can be highly effective but is inherently inefficient and not scalable. Hence,future work should focus on advancing learning-based or data-driven approaches (e.g. Fernando et al.,2023; Qin and Eisner, 2021) to discover the best prompts for a given task in an automated fashion.We note that learning-based approaches may introduce additional variables (e.g. training dataset) towhich evaluation might be sensitive. This suggests that it might not be possible to completely eliminateprompt sensitivity in which case, it is important to ensure that our evaluation accounts for promptsensitivity. Sensitivity of the evaluation to different aspects of the evaluation pipeline has been a long-standing challenge within machine learning, and as such, past work in this area can provide inspirationfor solutions.In particular, deep reinforcement learning has had to grapple with the issue of seedsensitivity, resulting in work to identify best practices (Agarwal et al., 2021) and the development ofnovel types of benchmarks (Cobbe et al., 2020).",
  "Test-set Contamination Overestimates LLM Capabilities": "The opaque nature of pretraining datasets makes it difficult to guarantee that a particular test datapoint, or other very similar data points, have not been observed by the LLM previously during thepretraining phase. Several studies show that LLMs perform better on familiar data (Wu et al., 2023a;McCoy et al., 2023) and if the evaluation is performed using data that has previously been observedby the LLM (or very similar data), it risks overestimating LLM capabilities (Tirumala et al., 2022).Indeed, Roberts et al. (2023b) show that there is a positive correlation between the GPT-4 pass-rate andthe popularity of a question on Codeforce (an online competitive coding platform) and Project Euler(an online mathematical reasoning platform) before the cutoff training date which disappears after thecutoff date; and several other studies have identified contamination of pretraining datasets with knownevaluation datasets (Elazar et al., 2023; Magar and Schwartz, 2022; Golchin and Surdeanu, 2023; Orenet al., 2023). Avoiding contamination while scraping the pretraining dataset from the web is difficultdue to the dynamic nature of the internet: online content tends to spread across the internet over time.Li (2023) found that blacklisting the original web-sources used to curate the MMLU dataset (Hendryckset al., 2020a) results in only a 1.5% reduction in MMLU dataset contamination. Hence, there is a needto develop enhanced techniques to find and remove contaminated data within a pretraining dataset;or confirm the absence of contamination during evaluation.Most techniques at the moment focuson identifying verbatim contamination (Elazar et al., 2023; Li, 2023). However, contaminated datamight also occur in pretraining datasets in mutated form, e.g. paraphrased or translated into anotherlanguage. So, future techniques ought to focus on semantic similarity measures to more broadly identify contaminated data (Golchin and Surdeanu, 2023). In cases when a models training dataset is available,can training data attribution methods like influence functions (Grosse et al., 2023) be used to identifywhether an LLM is generating an answer from memory, or performing the required reasoning on thefly? Furthermore, different strategies have been adopted by benchmark creators to prevent the leakageof benchmark content into the training datasets, e.g. use of canary strings (Srivastava et al., 2022),hiding the benchmark behind an API (Sawada et al., 2023), or distributing the datasets as password-protected archives (Rein et al., 2023). Further research is needed to examine the relative robustnessof these tactics against test-set contamination.It is possible that due to the aforementioned issueof the diffusion of content on the internet, none of these measures will satisfactorily prevent test-setcontamination. 3.3.3Targeted Training Confounds EvaluationCurrent LLMs undergo extensive targeted finetuning to suppress undesirable behaviors in simple andwell-known contexts. However, there is little evidence that this generalizes to more complex, harder-to-evaluate contexts which might occur in real-world use (Clymer et al., 2023) (also see .2).For example, while ChatGPT does not appear to embody steoreotypical biases in simple evaluations,it strongly manifests those biases when prompted to take on a persona (Gupta et al., 2023), or whenasked to perform a complex task (Wan et al., 2023a). This Goodharting of simpler evaluation schemeshas created a challenge where demonstrating a failure mode requires significantly greater creativity andeffort. This challenge may be countered by developing novel evaluation paradigms, e.g. persona-basedevaluation (Gupta et al., 2023), counterfactual evaluation (Wu et al., 2023a) or procedurally-generatedevaluation (Yu et al., 2023a). Establishing the validity and/or limitations of evaluation methods in thepresence of such Goodharting is an open research direction. 3.3.4Biases in LLM-Based EvaluationSeveral studies have explored using an LLM to evaluate itself or to evaluate other LLMs (Bai et al.,2023b; Zheng et al., 2023; Dubois et al., 2023).This approach has the inherent benefit of beingcheaper, faster, and more easily scalable than human evaluation (Zhuo, 2023; Perez et al., 2022a).However, LLMs also exhibit many human-like cognitive biases in evaluation e.g. position biases, thepreference for verbose and longer answers, egocentric biases i.e.preferring its own output (Zhenget al., 2023; Wang et al., 2023b; Koo et al., 2023; Wu and Aji, 2023). LLM-based evaluation may alsodiffer significantly from human evaluation (Koo et al., 2023; Perez et al., 2022a). This makes currentLLM-based evaluation less trustworthy - however, Wu and Aji (2023) found that modifying the LLM-based evaluation protocol to account for the cognitive limitations of LLMs can significantly enhancethe fidelity of their evaluations, especially when compared to crowdsourced human evaluation. Thus,future research should focus on furthering the understanding of biases present in LLM-based evaluation,and in particular, develop schemes to mitigate these biases. Furthermore, understanding the sources ofdisagreement between human evaluations and LLM-based evaluations may help to improve alignmentbetween them and also to provide insight as to which evaluation method is preferred in a given settingand how can they be combined effectively. In general, while studying the reliability of LLM-basedevaluation, it may help to distinguish whether an LLM is being used to evaluate itself, a more capableLLM or a less capable LLM. From the perspective of self-improving LLMs (Huang et al., 2022b), the firstcase is particularly important but has not yet been examined with appropriate care. For such a setup,Constitutional AI (Bai et al., 2022a) has been shown to be an effective method for LLM evaluation butfurther research is needed to better understand how the principles listed in the Constitution impact thereliability of LLM evaluation (Kundu et al., 2023). 3.3.5Fallibility of Crowdsourced Human EvaluationHuman evaluation via crowdsourcing is an important source of LLM evaluation, but it is both challeng-ing and expensive to obtain high-quality data (Hosking et al., 2023; Casper et al., 2023a, .1).One major challenge is annotator bias (Pandey et al., 2022) - which can not only result in incorrect evaluation but can also incentivize undesirable model behavior when this data is used for training (Perezet al., 2022a; Santurkar et al., 2023). These issues can be mitigated to a certain degree by understandingthe biases exhibited by human evaluators (Wu and Aji, 2023; Hosking et al., 2023); and by designingevaluation protocols to be robust to these biases (Wu et al., 2023b; Ethayarajh and Jurafsky, 2022;Clark et al., 2021), and to account for errors that may arise from these biases in evaluation metrics(Xiao et al., 2023). Complementing human evaluators with LLMs could be an effective way to improvethe quality of human evaluations (Saunders et al., 2022). Furthermore, developing better models thanthe Bradley-Terry model (Bradley and Terry, 1952) of how humans generate their preferences may alsohelp tackle issues with human evaluation (Laidlaw and Dragan, 2022; Lindner and El-Assady, 2022;Fageot et al., 2023; Ethayarajh et al., 2024). Lastly, Veselovsky et al. (2023) found that LLMs are com-monly used among crowdsource workers, which speeds up annotation at the potential cost of validity.To avoid such issues, it is imperative to ensure appropriate incentives for the data workers (Prassl andRisak, 2017; Shah et al., 2015) (c.f. .5.10).",
  "Systematic Biases in Evaluation": "The machine learning ecosystem has several different types of systematic biases, such as the under-representation of women (Schluter, 2018) and the overrepresentation of U.S. centric point of view inresearch (Septiandri et al., 2023). These systematic biases result in blindspots in LLM evaluation(Hutchinson et al., 2022). For example, despite the fact that most LLMs are multilingual, evaluation ofLLMs is largely conducted in English. As a result, LLMs can exhibit failure modes in other languages.Ghosh and Caliskan (2023) found that ChatGPT shows stereotypical gender biases when translatinginto languages like Bengali, Farsi, and Turkish, etc. Similarly, Yong et al. (2023) found that GPT-4engages in unsafe behavior with higher frequency in low-resource languages, i.e. low-resource languagesact as jailbreaks. Additionally, systematic biases may contribute to quality-of-service harms (Blodgettet al., 2022). Hence, there is a need for research to discover systematic biases that exist in the evaluationof LLMs and to develop ways to address them.",
  "Challenges with Scalable Oversight": "As LLMs gain further capabilities and are applied to increasingly difficult and complex tasks, it willbe increasingly difficult for humans to reliably evaluate the performance of LLMs. This raises the needfor scalable oversight - evaluation methods that remain effective past the point that models start toachieve broadly human-level performance (Bowman et al., 2022). Many methods for scalable oversighthave been proposed e.g. consistency checks (Fluri et al., 2023), self-evaluation (Bai et al., 2022a),supervision via debate (Irving et al., 2018; Bowman et al., 2022; Bowman and Lanham, 2023), weak-to-strong generalization (Christiano et al., 2018; Burns et al., 2023; Hase et al., 2024), process supervision(Uesato et al., 2022; Lightman et al., 2023) and recursive reward modeling (RRM) (Leike et al., 2018;Wu et al., 2021). A key challenge is the fuzzy nature of the aforementioned proposals, with importanttechnical details often left unspecified. There is a need for further research to formalize these proposalsin the context of LLMs. Fundamental challenges with scalable oversight proposals include identifying asuitable alignment target (Krueger, 2023, .3.1) and verifying that a method can successfullyapproximate that target. This is challenging because, by definition, humans struggle to evaluate theperformance of tasks requiring scalable oversight. This raises the non-technical but deeply prac-tical question of how a human overseer should respond when an AI system trained with scalableoversight appears (to them) to be misbehaving. We discuss closely related socio-technical challenges in.1.1. The scalability, robustness, and practical feasibility of these proposals are also currently not clear. De-spite some initial work (Burns et al., 2023; Hase et al., 2024; Khan et al., 2024), the generalizationproperties of all of the proposed methods are unclear, i.e. how do these methods scale with respectto the task difficulty in practice. Secondly, most proposals, explicitly or implicitly, rely on the idea ofdecomposing a difficult task into smaller and easier-to-evaluate sub-tasks. However, most real-world tasks are unlikely to admit a clean decomposition, and hence, any decomposition of a sufficiently com-plex task will have some approximation error which may require novel techniques to address (Reppertet al., 2023). Thirdly, many proposals, e.g. debate include a human-in-the-loop component and thusmight inherit the same challenges with human evaluation discussed above. An alternative to usinghuman evaluation is to use a robustly-aligned LLM, but there is increasing evidence that our currentalignment techniques do not result in the robust alignment of LLM models (Jain et al., 2023a) - and anon-robustly aligned AI agent might either fail to provide robust oversight due to its inherent (hidden)biases (Gupta et al., 2023), or become exploited by other more powerful LLM models (Meinke, 2023).This indicates the need to understand how robust different scalable supervision strategies are. Indeed,one could argue that with scalable supervision techniques, the most important thing is not empiricalvalidation of the method, but theoretical characterization of the robustness of the mechanisms, e.g.to intentional or unintentional subversion on the part of AI agents (Barnes et al., 2020; Brown-Cohenet al., 2023). A more nuanced understanding of the differences between the aforementioned proposedmethods for scalable oversight, and their relative strengths and weaknesses, can help the communityprioritize research efforts accordingly, and provide insight as to which proposals are complementary,and which are interchangeable. Lastly, all the aforementioned proposals are generic, and while theyhave been applied with some success to LLMs, it is possible that better scalable oversight strategiescan be developed specifically for LLMs. Many issues undermine our ability to comprehensively and reliably evaluate LLMs. Issues such asprompt-sensitivity, test-set contamination, and targeted training to suppress undesirable behav-iors in known context confound evaluation. The validity of evaluation is further compromisedby biases present in LLMs (which are used to evaluate other LLMs), and human evaluators.Furthermore, there exist systematic biases that create blindspots in LLM evaluations, e.g. lim-ited evaluations on low-resource languages. Finally, considering the rapid rate of improvementin LLMs capabilities, we need robust strategies to implement scalable supervision which arecurrently lacking.",
  ". Can we develop automated methods that reliably find the best prompt for a given taskor task instance?": "86. How can we account for prompt sensitivity when evaluating an LLM? 87. How can the evaluations of LLMs be made trustworthy given the difficulty of assuringthat there is no test-set contamination? Can we develop methods that can detect whethera given text is contained in the training dataset in mutated form, e.g. paraphrased ortranslated into another language?",
  ". How can training data attribution methods be used to detect cases of LLMs respondingto queries based on memorized knowledge when the training dataset is known?": "89. What measures can evaluation developers take to prevent leakage of the evaluation datainto an LLMs training dataset?How effective are existing measures such as canarystrings, hiding datasets behind APIs, or password-protecting dataset files in detectingaccidental and/or deliberate leakage? 90. How can the failure modes of an LLM be uncovered when the LLM has been explicitlytrained to hide those failure modes?Are there general techniques, such as personamodulation, or counterfactual evaluation, that can be used for this purpose?",
  ". What are the various ways in which an evaluation of an LLM by an LLM may be biasedor misleading? How can LLM-based evaluation be made robust against such biases?": "92. What are the limitations, and strengths, of Constitutional AI-based LLM evaluation?How can we develop a nuanced understanding of how principles given in the constitutionaffect the evaluation of different LLM behaviors? How do LLMs handle issues such asunderspecification or conflict in the constitutional principles? 93. How can evaluation done by humans be made robust against the various known biasesand cognitive limitations of humans? How can LLMs be used to complement humanevaluators to improve the quality of human evaluations?",
  ". How can the blindspots in LLM evaluation resulting from systematic biases be avoided?": "96. Can we formalize different proposed methods for scalable oversight in the context ofevaluating LLMs? Can this formalization be used to understand the relative strengthsand weaknesses of these proposals, through theoretical and empirical research? Whichproposed methods are complementary, and which are interchangeable? 97. Are there any decomposition strategies that generalize across tasks?Prior work hasproposed task-specific decomposition strategies, e.g. for book summarization (Wu et al.,2021) or writing code (Zhong et al., 2023b). Do the proposed decompositions generalizeto other tasks? Can language models automatically decompose tasks?",
  "Tools for Interpreting or Explaining LLM Behavior Are Absent orLack Faithfulness": "To assure alignment and safety of LLMs, we can not merely rely on behavioral evaluations alone especially given the severe limitations of current evaluation methods highlighted in the previous section.Thus, there is a need for reliable, robust, and scalable methods that may help us interpret and explainneural network-based models. Various ways to interpret model behaviors have been proposed in theliterature. Unfortunately, all the interpretability methods suffer from various fundamentaland practical challenges that limit their viability for providing assurances of safe behav-ior. We discuss two representative classes of methods in this section. The first class of method aims todevelop an understanding of model behavior by opening up the black-box and developing an under-standing of the internal representations and mechanisms of a model. This includes work done in theresearch areas of representation probing and mechanistic interpretability. The second class of methodsaims to explain model behavior by designing methods that cause the model to externalize critical partsof reasoning in an interpretable form, e.g. natural language (Reynolds and McDonell, 2021; Wei et al.,2022d; Kojima et al., 2022; Lanham, 2022).",
  "Fundamental Challenges": "There are some challenges that all interpretability and explainability methods must overcome. Wediscuss four such challenges in the following text. The first, and perhaps the most critical challenge, isthat in order to make the interpretability problem tractable, interpretability methods typically presumethat (internal) model reasoning works in specific ways. These presumptions often lead to abstractionsthat are dubious.The second challenge is that neural networks are in no way constrained to usehuman-like concepts. Indeed, advanced AI systems like AlphaZero have been found to use differentconcepts than ordinarily used by humans (Schut et al., 2023). This naturally makes the problem ofinterpreting such models much harder. The third challenge is that we require explanations generated byinterpretability-based analysis to not just be plausible, but also be faithful. However, scalable evaluationof the faithfulness of an explanation is an extremely challenging problem.Finally, interpretabilitymethods are used not just to identify undesirable patterns in LLM reasoning, but also to modify modelbehavior (Zou et al., 2023a). However, it is unclear how robust such modifications are and whetherinterpretability methods maintain validity when used to modify model behavior or not.",
  "Abstractions Used for Interpretability Are Often Dubious": "The goal of interpretability is to produce abstract explanations of internal mechanisms of a model.However, it is unclear what kind of abstractions exist within neural networks that can be used for thispurpose (Appendix A Zou et al., 2023a). Using the right abstraction can help preserve the most criticaland useful information while simultaneously improving ease of understanding. On the other hand, usingan incorrect abstraction can result in misleading explanations. For example, early interpretability stud-ies on neural networks abstracted non-linear behavior in terms of a linear (interpretable) approximationaround the input (Ribeiro, 2016; Lundberg and Lee, 2017). however, Bilodeau et al. (2022) show thatpopular feature attribution methods based on this abstraction Integrated Gradients (Sundararajanet al., 2017) and Shapley Additive Explanations (SHAP; Lundberg and Lee, 2017) can provably failto improve on random guessing for inferring model behavior. In a similar vein, a large body of work oninterpreting neural network representations has focused on interpreting individual neurons (e.g. Dalviet al., 2019; Bau et al., 2020; Schubert et al., 2021), presuming that neurons specialize; however, thiswas later found to be problematic because individual neurons can be polysemantic and may not actu-ally be specializing (Elhage et al., 2022b; Antverg and Belinkov, 2022; Geva et al., 2022; Geiger et al.,2023). Similarly, linear probes learned either through supervised or unsupervised methods arecommonly used to discover structure within internal representations of neural networks (Azaria andMitchell, 2023; Burns et al., 2022). However, probes are typically trained on a different dataset than themodel, which may contain spurious features and recent work has shown that in some cases, these probesmight be latching onto spurious features present in the datasets used to train these probes, rather thanactual features represented within the model (Farquhar et al., 2023; Levinstein and Herrmann, 2023).Similarly, circuit-style mechanistic interpretability assumes that there exist task-specific circuits (Olahet al., 2020), or subnetworks, within neural nets that can be discovered. But it is unclear to whatextent this is true (McGrath et al., 2023; Veit et al., 2016). Natural-language-based externalized rea-soning presumes that that the internal reasoning processes of the LLM can be faithfully captured, andexpressed, in natural language (Lanham, 2022). But it is unclear to what extent this is true (Wendleret al., 2024). In general, it is unclear what kind of computational structures exist within neural nets,making it difficult to develop appropriate abstractions of neural networks computations. In additionto discovering abstractions that naturally arise within neural networks, it may be helpful to designtraining objectives that may incentivize a model to use (known) specific abstractions (Geiger et al.,2021).",
  "Concept Mismatch between AI and Humans": "A fundamental challenge with interpretability is that we do not have guarantees that the functionsthat models learn rely on features or reasoning processes that translate well to human-comprehensibleconcepts (Mahinpei et al., 2021). This is especially a concern in domains where AI systems outperformhumans, since the systems appear to leverage reasoning processes or features unknown to humans (Schutet al., 2023). As a result, relying on human concepts may limit our ability to discover appropriate con-cepts that provide the best mechanistic explanation of model behavior. Many interpretability methods(e.g. TCAV; Kim et al., 2017) are top-down (supervised) approaches to interpretability in the sense thatthey start with a hypothesis about a concept they are looking for in a model, and then utilize labeleddatasets to discover how a model represents that concept. The main shortcoming of this approach isthat if the model is using concepts unknown to us, then this approach may not be able to identify thoseconcepts. Due to this inherent limitation, bottom-up (unsupervised) concept discovery methods that donot rely on an initial set of concepts to look for but instead aim to discover the most influential con-cepts may be more appropriate. Indeed, Schut et al. (2023) develop an unsupervised concept discoverymethod that successfully identifies, and isolates, novel chess concepts within AlphaZero (Silver et al.,2017) that are not present in human chess games. These chess concepts were then consequently taughtto expert human players in a user study. While the expert humans were able to comprehend the novelconcepts, they still struggled to learn them in cases where the novel concept conflicted heavily with the existing (human) notions of appropriate chess play. Unfortunately, the approach developed by Schutet al. appears to be highly specific to AlphaZero. Thus, there is a need to develop general proceduresthat may help translate between human and machine concepts in cases where machine concepts do notnaturally map onto known human concepts. Alternatively, specialized methods could be developed tohelp AI models learn representations that are aligned with humans (Bobu et al., 2023; Muttenthaleret al., 2023). This is one of the goals of the field of representation alignment within machine learning:see Sucholutsky et al. (2023) for a recent survey.However, we note that representation alignmentworks generally include human-in-the-loop training which is inherently difficult to scale. The concept-mismatch problem may also undermine the validity of externalized reasoning (via natural language).In the cases of concept mismatch, externalized reasoning may approximate the internal model conceptsusing the closest human concepts but this approximation may cause externalized reasoning to becomeunfaithful. However, the generative nature of externalized reasoning may enable interactive communi-cation between the human user and the model that may enable humans to learn novel concepts withless difficulty.",
  "Evaluations Often Overestimate the Reliability of Interpretability Methods": "Prior interpretability methods in machine learning have a fraught history. Over time, many differentinterpretability methods have been introduced with convincing case studies and theoretical grounding(Ribeiro, 2016; Lundberg and Lee, 2017; Sundararajan et al., 2017), gaining popularity in both core MLresearch and adjacent applications (Gilpin et al., 2018). Unfortunately, the same methods were laterdemonstrated to completely fail basic tests for usefulness to humans and provided no improvement overrandom baselines (Adebayo et al., 2018; Hase and Bansal, 2020; Adebayo et al., 2020; Bilodeau et al.,2022; Casper et al., 2023b).11 This trend continues to date: recent interpretability work in featureinterpretation made claims (Bills et al., 2023) that failed to withstand rigorous evaluations (Huanget al., 2023b). Given this precarious situation, it is imperative to set up rigorous evaluation standardsfor the interpretability methods (Doshi-Velez and Kim, 2017; Miller, 2019; Krishnan, 2020; Rukeret al., 2023). A basic desideratum for model explanations is faithfulness, i.e. a given explanation should accuratelyreflect the models internal reasoning (Jacovi and Goldberg, 2020). However, typically details about themodels internal reasoning are not known apriori. This makes it challenging to assess the faithfulnessof a model explanation. As a result, different ways of evaluating faithfulness have been proposed andstudied in the literature. However, often these strategies are specific to a particular interpretabilitymethod or model class being interpreted. A relatively model-agnostic and method-agnostic notion offaithfulness is counterfactual simulatability which posits that for a model explanation to be faithful,a model computation (after some intervention) must change in the same way as a human would haveexpected it to change given model explanation and knowledge about the intervention (Doshi-Velez andKim, 2017; Ribeiro et al., 2018; Hase and Bansal, 2020). For mechanistic interpretability, a faithful interpretation of a model circuit should enable us to predicthow interventions on model inputs or the circuit itself change model behavior (Wang et al., 2022; Chanet al., 2023c).Model behavior here could be measured in terms of model outputs or intermediatecomputations (e.g. outputs of particular neurons) within a circuit. Furthermore, while the notion ofsimulatability introduced above presumes that the entity simulating the model is a human, that is notnecessary in general, and the simulation entity could be a computer program as well, e.g. a RASPprogram (Zhou et al., 2023c). Such automated measures of simulatability may be particularly helpfulin developing benchmarks for interpretability methods. Currently, mechanistic interpretability toolsare not practically competitive with non-mechanistic techniques for discovering novel properties ofmodels and evaluating their behavior. In general, there is a need for benchmarks to standardize metrics 11We make this point not to fault past methods, but to emphasize the difficulty of the problem: explaining the behavior ofa complicated non-linear function over high-dimensional data to a human in an efficient manner is an extremely difficult task. of success and to help create better standards for evaluating explanation faithfulness (Schwettmannet al., 2023; Ruker et al., 2023). In addition to standardizing faithfulness evaluations, benchmarkswill be most useful when they have clear implications for applications like detecting and mitigatingalignment failures. In order to catch worst-case outcomes, explanation faithfulness may need to beevaluated particularly in adversarial settings, where models are optimized to exhibit certain alignmentfailures that we try to then detect and mitigate (for example, as done in trojan detection: Centerfor AI Safety, 2023; Casper et al., 2023b; Rando et al., 2024).For such cases, there is a need todesign faithfulness metrics that focus on worst-case rather than average-case explanation faithfulness.Alternatively, evaluations could focus on faithfulness for high-risk inputs rather than the whole datadistribution. A similar notion of faithfulness applies to model explanations based on externalized reasoning as well.A common way to evaluate faithfulness in the context of natural-language-based externalized reasoningis to intervene on model inputs and check that previous explanations agree with model outputs (i.e.intermediate reasoning steps, final predictions) on the new inputs. Unfortunately, in general, deter-mining whether reasoning stated in natural language is consistent with an observed behavior or notreduces to a logical entailment (natural language inference) task (Dagan et al., 2005), which are knownto be highly subjective and difficult to properly annotate (Pavlick and Kwiatkowski, 2019). As a result,current works that study the faithfulness of externalized reasoning only focus on simple tasks, e.g.multiple-choice questions (Lanham et al., 2023). Furthermore, given that the input space is quite largefor LLMs, efficiently surfacing data for which model behavior will be inconsistent with previously statedreasoning can be extremely challenging (Chen et al., 2023b). Efficiently surfacing inconsistent behaviormay require a hypothesis about why models might be inconsistent. For example, one might need tosuspect that models are sensitive to answer choice ordering, in order to discover that models give incon-sistent answers when perturbing this part of the prompt. As a result, detecting inconsistencies betweenstated reasoning and model behavior across datapoints is a difficult problem for humans to efficientlysolve on their own. Scalable oversight methods (c.f. .3.7), which often use LLMs to assisthumans in evaluations of a particular task, could help detect unfaithful reasoning efficiently (Saunderset al., 2022; Bowman et al., 2022). In cases where an automated measure of evaluating faithfulness foran (input, output, explanation) triplet is available, adversarial optimization could be used to efficientlysearch for perturbations to a given input to generate outputs inconsistent with a given explanation. While within this section, we have primarily focused our discussion on evaluating the faithfulness ofexplanations, in practice, explanations may need to fulfill additional desiderata to be useful to intendedusers, e.g. minimality (Wang et al., 2022) or ease-of-understanding (Bhatt et al., 2020). Hence, westress that any claims about the interpretability methods being ready to be used by practitioners oughtto be accompanied by context-based evaluations of interpretability methods, using e.g. user-studies.These context-based evaluations should strive to reflect realistic use cases and users.",
  "Can Interpretability Methods Maintain Validity When Used to Modify Model Behavior?": "A number of approaches have been developed that allow modifying LLMs behaviors by interveningon sources of those behaviors (e.g. representations) within the model. This includes techniques suchas model editing (Mitchell et al., 2021; Meng et al., 2022; Hernandez et al., 2023a; Tan et al., 2023;Wang et al., 2023h), subspace ablation (Li et al., 2023c; Kodge et al., 2023), and representation editing(Li et al., 2023b; Turner et al., 2023a;b; Li et al., 2023d; Zou et al., 2023a; Gandikota et al., 2023).Currently, these methods have only had limited success (see .2.4), however, as the scalabilityand efficiency of interpretability techniques improve, it is likely that such methods may become morepopular and may see wider adoption. Indeed, prior work has used (other) interpretability methodssuch as saliency maps for optimizing neural networks to act in the desired way (Ross et al., 2017;Hendricks et al., 2018; Rieger et al., 2020; Stammer et al., 2021). Specific to LLMs, process supervisionhas been used to provide feedback to a model to improve its externalized reasoning (Lightman et al., 2023). However, due to the prevalence of Goodharting (Chu et al., 2017; Manheim and Garrabrant,2018; Lehman et al., 2020), there is a concern that using an interpretability method as an optimizationtarget may cause it to lose its validity due to overfitting. As a result, the underlying behavior thatis being targeted may continue to persist within the model, even if interpretability results no longerindicate that it is present. Research is needed to understand to what extent this is a problem for varioustechniques and how it might be addressed.",
  "Challenges Specific to Interpreting Model Internals": "In addition to the aforementioned challenges, there also exist several challenges specific to techniqueslike representation probing and mechanistic interpretability that aim to understand representations andmechanisms within neural networks. These challenges include limited knowledge of how representationsare structured within neural networks; polysemanticity of individual neurons; high sensitivity of inter-pretability results to the datasets used for analysis and limited scalability of techniques used for featureinterpretation and circuit discovery.",
  "Assuming Linearity of Feature Representation": "A potential fundamental obstacle toward making progress in interpretability is that many methodsrely heavily on the assumption that features are represented linearly, i.e. as (linear) directions in theactivation space (Park et al., 2023b). Many studies have demonstrated the utility of this assumption ininterpreting (Alain and Bengio, 2018; Rogers et al., 2020; Elhage et al., 2022b) and controlling (Turneret al., 2023a; Li et al., 2023b; Wang et al., 2023i) models, which lends support for this hypothesis.However, Hernandez et al. (2023b) provide some evidence that some concepts might be representednon-linearly. Further research is required to better understand which concepts are encoded linearly,and which are encoded non-linearly. Factors such as model capacity may also affect whether a givenconcept gets encoded linearly (potentially in a highly lossy way) or non-linearly. There is also a needto better understand the differences between how different model representation spaces (e.g. MLPs vs.attention layers) encode information. For example, are representations in later layers of the model moreor less likely to be structured linearly? Further, even if features are represented linearly, we still needto define an appropriate inner product for assessing representation similarity, and it is not clear howto choose an inner product over model hidden states (Park et al., 2023b). A better understanding ofthese issues would enable us to design more reliable probing methods for assessing the informationalcontent of internal model representations and determining the similarity between representations.",
  "Polysemanticity and Superposition": "Individual neurons within a model can make a natural building block for constructing circuit-styleinterpretations of models. However, a notable challenge with establishing interpretations of individualneurons is that neurons often activate strongly for seemingly unrelated features in input data. Thishas recently been dubbed polysemanticity by Elhage et al. (2022b).The authors also argue thatpolysemanticity may be a result of models representing a large number of sparsely activated featuresin a lower-dimensional hidden representation, and present a toy model of this phenomenon, whichthey call superposition.Motivated by this hypothesis, recent work has used sparse autoencodersto find more interpretable features (Sharkey et al., 2023; Graziani et al., 2023; Bricken et al., 2023;Sucholutsky et al., 2023; Cunningham et al., 2023). However, these sparse autoencoders can be ordersof magnitude larger than the language models they are trained to explain, which may be prohibitivelyexpensive for larger models (for further discussion on challenges in scaling interpretability analysis,see Sections 3.4.8 and 3.4.9). More work is needed to understand the root causes of uninterpretable,polysemantic features in large language models (relating it back to superposition, concept mismatch,or other causes), which should help shed light on which interpretability approach best addresses theroot cause.",
  "Sensitivity of Interpretations to the Choice of Dataset": "A specific dataset, which is often much smaller than the full training dataset, is typically used fordiscovering concepts within a model. Discovered concepts can thus be highly sensitive to what samplesare included in the dataset used for concept discovery. Indeed, Ramaswamy et al. (2022) found thatvarious top-down (supervised) concept discovery methods produce conflicting explanations for the samemodel output depending on which dataset was used to supervise the probe that detects concepts in themodel. A similar issue applies to bottom-up (unsupervised) concept discovery methods. A particularneuron might activate on two distinctive sets of inputs depending on what dataset is used for retrievinghighly activating inputs, a phenomenon termed an interpretability illusion (Bolukbasi et al., 2021).This high level of sensitivity to dataset design is concerning as this limits the generalizability of theinterpretability results. To remedy this issue, one could aim to develop datasets for probing that containall possible concepts of interest (which, for supervised concept discovery, would need to be labeled).For LLMs, such a dataset could be the original model training data (McDougall et al., 2023). However,there are clear compute issues that prohibit the use of pretraining data for concept discovery (e.g. evenperforming a correlational analysis of documents that highly activate neurons would demand a runtimeproportional to pretraining). Given the fact that full training dataset can not be used, and use of anysubset of training data risks omitting some concepts used by a model in its outputs, future methodsshould consider developing compute-adaptive methods for concept discovery that progressively grow aseed dataset by exploring the dataspace in directions that plausibly contain novel concepts used by themodel for prediction.",
  "Feature Interpretation Is Hard to Scale": "Current techniques and methods used in mechanistic interpretability are quite primitive and difficultto scale. Hence, so far, mechanistic interpretability has only successfully identified circuits explaining95%+ of the variance in model behavior for highly toy problems, like modular division (Nanda et al.,2022). Feature interpretation or concept-discovery, i.e. identifying what (human) concepts different modelfeatures correspond to, is often the first step in interpreting the internal mechanisms of a model.A typical approach for this is to assign meanings to model features by looking at data that highlyactivates the feature; this generally requires a human to be in the loop to generate hypotheses aboutwhat concepts the feature under study might be encoding, and then perform a faithfulness evaluationfor this hypothesis (like the simulated neuron experiment from (Bricken et al., 2023)). Both of thesesteps are highly time-consuming. Bills et al. (2023) attempt to automate this process by using anLLM, GPT-4, in the place of human, however, Huang et al. (2023b) show that concept labels generatedby GPT-4 in aforementioned work have high error rates, are often ambiguous in meaning and do notprovide a faithful explanation of model behavior on the whole. Furthermore, both human-in-the-loopand LM-automated approaches may struggle to properly identify the meaning of individual featuresdue to the aforementioned issues of concept-mismatch problem (.4.2), polysemantic neurons(.4.6) and subjectivity of results to the concept annotation processes (.4.7). Researchers may consider exploring ways to sidestep the scalability limitations of feature interpretationmethods, e.g. by focusing on interpreting relevant features for alignment failures or that explain alarge portion of model behavior. Alternatively, instead of trying to interpret features directly at ahighly fine-grained level, it may be helpful to develop methods that begin by explaining features ata higher level of abstraction, and then explain them at more fine-grained levels when necessary. Forexample, sparse autoencoders with fewer learned features appear to represent features at a higherlevel of abstraction, and more precision can be achieved later by fitting a model with more learnedfeatures (Bricken et al., 2023). Using language models to automate the feature annotation is also apromising approach worthy of further research and refinement (Hernandez et al., 2021; Bills et al.,",
  "). Automatic hypothesis generation methods could be developed to iteratively refine hypothesesby finding instances of disagreement between the explanation and observed model behavior": "3.4.9Circuit Discovery Is Hard to ScaleIsolating circuits within a large network is also highly challenging and typically done via manual in-spection (Wang et al., 2022; Goldowsky-Dill et al., 2023). While there has been some recent progresstowards automating circuit discovery (Conmy et al., 2023); more work is needed to develop automatedand scalable methods for circuit discovery.The primary challenge for circuit discovery is that thehypothesis space can be quite large, possibly combinatorial as any of the subnetworks within a modelcan be the desired circuit. The ACDC algorithm proposed by Conmy et al. gets around this issue byinitializing the whole network as the circuit, and then sequentially searching over the edges deletingany whose deletion does not impact the performance on task of interest. However, even this greedyapproach may be practically infeasible for extremely large models. This sequential deletion approachalso runs the risk of missing backup or secondary computational nodes from the identified circuits, whichonly becomes active when the primary computational node is ablated (Wang et al., 2022). Thus, there is a need for work to develop automated and scalable methods for circuit discovery. Inaddition to designing theoretically grounded algorithms for circuit discovery, efficient heuristics couldalso be developed to help improve the efficiency of search.It may also be useful to design circuitdiscovery methods that operate at larger network scales than individual neurons and adjacent layers,in order to reduce the size of the circuit hypothesis space.",
  "Challenges Specific to Externalized Model Reasoning": "One form of interpretability that has become popular with LLMs is externalized reasoning. Specifi-cally, we attempt to make model reasoning visible to an external observer by steering models to makepredictions based on reasoning patterns that are by construction stated in natural language (makingthem naturally interpretable). For example, chain-of-thought reasoning (Reynolds and McDonell, 2021;Wei et al., 2022d; Kojima et al., 2022) prompts the model to break a problem down into sub-problemsand then compose the final solution by solving these sub-problems. Externalized reasoning can also beperformed using formal semantics, for example, program synthesis approaches use LLMs to generateprograms (code) that when executed solve the given problem. However, despite its intuitive appeal, thisstrategy suffers from the fundamental challenges discussed above. Different variants of this strategyalso have other shortcomings. Natural-language-based externalized reasoning can be misleading andunfaithful, while externalized reasoning methods that use formal semantics are difficult to apply toopen-ended tasks like question-answering. 3.4.10Externalized Reasoning in Natural Language May Be MisleadingExternalized reasoning in natural language is a naturally attractive option for interpretability due to itbeing interpretable to any human user without any requirements of specialized knowledge. However, forit to be a reliable form of interpretability, it must faithfully indicate the critical causal factors responsiblefor particular model behavior. However, Turpin et al. (2023) found that models CoT reasoning canbe heavily steered towards incorrect answers by biasing features in prompts e.g. by having a usersuggest that a specific answer choice is correct. CoT explanations in such cases can give plausiblerationalizations for why the biased answer is correct, without mentioning the influence of the biasingfeatures in the prompt. In a similar result, Lanham et al. (2023) show that models can be insensitiveto edits made to their reasoning. This problem of models generating plausible yet unfaithful reasoningmay be exacerbated when models are applied to complex tasks that admit many plausible explanationsfor any individual behavior. In these cases, it may be very easy for models to give inconsistent yetplausible reasoning that could mask sensitivity to undisclosed factors influencing models. The facts that natural-language-based externalized reasoning often results in improved performance, yetcan be unfaithful, are somewhat contradictory, and need to be reconciled. Lanham et al. (2023) evaluate some natural hypotheses in this regard, but further investigations, especially, with pretrained onlyLLMs, are required to better understand to what extent externalized reasoning is causally responsiblefor improved performance on various reasoning tasks (also see .4.5 for relevant discussionon the computational necessity of intermediate computations for transformers to solve certain kindsof reasoning tasks). Within this context, it would be useful to understand the extent to which ourtraining protocols directly incentivize unfaithfulness, e.g. whether reinforcement learning with humanfeedback can cause models to learn to hide reasoning that would be disapproved by human evaluator(Perez et al., 2022a; Scheurer et al., 2023a). A related open question is how to encourage methodsthat supervise the reasoning process of the LLMs, e.g. process supervision (Lightman et al., 2023), toimprove the faithfulness of reasoning given by the model, and/or how to ensure they do not make itless faithful. There is also a need for research to develop methods that help improve the faithfulness of LLM-reasoning. Some decomposition-based methods, that break down problem into subproblems and gen-erate reasoning steps only for individual subproblems before recombining model reasoning and outputsinto a global solution, have reported improvements in the faithfulness of model reasoning (Eisensteinet al., 2022; Radhakrishnan et al., 2023; Reppert et al., 2023). This indicates that imposing greaterstructure on reasoning could help enforce the model to use consistent reasoning patterns, which canin turn limit instances of plausible yet unfaithful reasoning. However, imposing structure does notguarantee that the model respects the intended semantics of the structure, as indicated by issues likesteganography (Chu et al., 2017). An alternative research direction could be to train the models directlyto use consistent reasoning patterns across inputs (Chua et al., 2024). Another fundamental issue faced by natural-language-based externalized reasoning is that for naturallanguages, there exists a trade-off between completeness and efficiency of communication (Grice, 1975;Piantadosi et al., 2012). This tradeoff dictates that for natural-language-based externalized reasoningto be easy to evaluate for humans, a model must do some lossy compression of the complete reasoning,when the complete reasoning is excessively lengthy (as may be the case for complex tasks). This mayresult in unfaithful explanations if important elements of model reasoning get omitted. Alternatively,if no compression is done, the generated explanation might be excessively long and difficult to evaluatefor humans; which may result in overlooked errors in the explanation. To avoid this pathology, it wouldbe useful to understand what level of completeness in explanations is required to avoid alignmentand safety failures and to develop interactive structured reasoning methods that may allow selectivelyzooming-in on specific aspects of the model reasoning for which greater details are required, similarto debate (Irving et al., 2018) (c.f. .3.7). Such approaches could be used to gain additionalinformation about individual high-stakes model decisions. 3.4.11Externalized Reasoning via Formal Semantics Is Not Widely ApplicableNatural languages have informally defined semantics.As a result, natural languages have inherentambiguity which means different statements may have different meanings depending on the context,and how the receiver interprets them (Huang et al., 2023b, .1). In contrast, languages withformally defined semantics (e.g. programming languages like Haskell) have mathematically rigorousinterpretations associated with each individual construct, and also allow defining novel (higher-level)constructs as needed. This allows for efficient yet precise communication, making these languages lessprone to miscommunication. These properties make formal semantics an attractive option as mediumof externalized reasoning as formal verification tools (Tabuada, 2009; Hoare et al., 2009) can be appliedto verify the correctness of this type of reasoning (Tegmark and Omohundro, 2023). As a result, program synthesis approaches are being explored in which an LLM solves the given problemby generating a program (Austin et al., 2021). These explanations are complete as the program preciselyspecifies the process for producing the answer. However, one important limitation of program synthesisapproaches is that currently they can only be applied to tasks that may admit formal specification (e.g. mathematical reasoning) (Wu et al., 2022). As a result, using program synthesis approaches to solveopen-ended problems like question answering presents an important milestone for program synthesisresearch (Zelle and Mooney, 1996; Berant et al., 2013; Yu et al., 2018; Lyu et al., 2022). This may requiredeveloping new domain-specific programming languages or combining interpretable structured reasoningwith individual modules implemented by blackbox neural networks that are verified in other ways(Gupta and Kembhavi, 2022; Suris et al., 2023). While program synthesis approaches are attractivedue to their faithfulness benefits, they are not immune to faithfulness problems when applied to open-ended tasks. Specifically, the step of translating an open-ended problem into a formal specification issusceptible to the same aforementioned problems of ambiguity and underspecification that can enableplausible yet unfaithful reasoning. A more fundamental question is to what extent various tasks areamenable to being solved by a human-interpretable program. Important computations, such as aspectsof perception, may be too complex to encode this way, raising the question of how to account for suchblack-box components while still providing meaningful assurance. Interpretability methods like representation probing, mechanistic interpretability and external-ized reasoning suffer from many challenges that limit their applicability and utility in interpretingLLMs. Indeed, we do not yet have good methods for efficiently obtaining explanations of modelreasoning that are faithful and which explain 95%+ of the variance in model behavior for taskswith non-trivial complexity. Some of the challenges in interpreting models are fundamental innature, e.g. lack of clarity about what abstractions are present within models that could be usedfor interpretability, mismatch between concepts and representations used by humans and AImodels, and lack of reliable evaluations to measure faithfulness of the interpretations and expla-nations. Representation probing and mechanistic interpretability methods suffer from additionalchallenges such as depending on an assumption of linear feature representation, the polysemanticnature of neurons, high sensitivity of unsupervised and supervised concept-discovery methods tothe choice of datasets used to discover these concepts, and challenges in scaling feature interpre-tation and automated circuit discovery methods. Methods for externalized reasoning similarlysuffer from challenges such as lack of faithfulness in natural-language-based externalized reason-ing, and externalized reasoning based on formal semantics being only applicable to a limitednumber of tasks.",
  ". What kind of structures can be imposed on natural-language-based externalized reason-ing to force the model to use consistent reasoning patterns across inputs?": "113. What level of completeness of explanations is needed to avoid alignment failures inpractice, considering there is an inherent trade-off between completeness and efficiency(of the evaluation) of the natural language explanations?Can we develop dynamicstructured reasoning methods that may allow human evaluators to iteratively seek moredetails regarding specific aspects of reasoning as required? 114. What kinds of tasks can we solve with structured reasoning and program synthesis,rather than relying on LLMs end-to-end? Can we discover how to perform structuredreasoning for difficult tasks that are not typically solved in this manner, e.g. open-endedtasks like question answering?",
  "Jailbreaks and Prompt Injections Threaten Security of LLMs": "Current LLMs are not robust against adversarial inputs (Shayegani et al., 2023; Geiping et al., 2024).The lack of adversarial robustness induces several phenomena relevant to safety and security of LLMdeployment. All take a similar form: one party (model creator, app developer) wants to restrict the actions of a model and trains or prompts it to do so. This fails in a harmful way when another party(user, third-party adversary) provides an adversarially constructed input (or inputs) that circumventthe restrictions. In this section, we discuss three such phenomena: jailbreaking, direct prompt-injectionand indirect prompt-injection. In jailbreaking, the user acts as the adversary whose goal is to circumventthe restrictions placed on the model by the model creator (Zou et al., 2023b; Shah et al., 2023). Directprompt-injection is similar to jailbreaking, except the goal of the adversarial user is to circumventrestrictions placed by the app developer, rather than the model creator (Liu et al., 2023a). Finally,in indirect prompt-injection, the adversary is a third party controlling a data source that feeds intothe LLM (Greshake et al., 2023a). The lack of adversarial robustness may sometimes allowmisuse of LLMs by malicious actors (see .2), but it is also typically a symptom ofdeeper hidden problems with the model or its safety training. Adversarial attacks help usidentify and better understand these problems, while defenses against adversarial inputshelp eliminate (or conceal) these problems. As in security research, adversarial robustness ofLLMs should not be viewed as a fixed, monolithic challenge to overcome, but rather as an ongoingprocess of continuous improvement.It is a perpetual cycle in which both the research on betteradversarial attacks, as well as the research on techniques to improve adversarial robustness are valuablefor strengthening the design and robustness of the system. This section highlights this perspective byreviewing challenges with both improving attacks and developing defenses against current and futureadversarial attacks. : Illustration of jailbreak attack methodologies described in .5.3. (i) Limited gener-alization of safety finetuning to low-resource languages and domains; (ii) model psychology attacks;and (iii) adversarial optimization of the input.",
  "Standardized Evaluations of Jailbreak and Prompt Injection Success": "Jailbreaking prompts can take a variety of forms including unintelligible text (Zou et al., 2023b), encodedtext (Liu et al., 2023b), persona modulation (Shah et al., 2023), ASCII art (Jiang et al., 2024b), low-resource languages (Yong et al., 2023), encoded prompts (Wei et al., 2023c), images (Bailey et al.,2023), many-shot attacks (Anil et al., 2024), and other strategies (Shen et al., 2023; Rao et al., 2023).Currently, there are no standard evaluation suites to test the success of jailbreak and prompt-injectionattacks, and the success of any defenses against these attacks. As a result, each paper that proposes anew attack or defense develops its own evaluation methods, and the criteria that a jailbreak is successful varies across papers. For example, Bailey et al. (2023) and AdvBench from Zou et al. (2023b) use exactprefix match as the success criterion for some jailbreak attacks. Huang et al. (2023c) used a BERTclassifier trained on positive and negative examples from the HH-RLHF dataset (Bai et al., 2022b).Wei et al. (2023c) used manual evaluation. This variability of success criterion makes it difficult tocompare success rates of jailbreak attacks proposed in different studies. The challenge of standardizingjailbreak evaluation is further complicated by task-specific requirements in some cases, the goal inthe jailbreaking attack is to elicit a harmful response, however, in other cases the goal is to make LLMdivulge some specific information (Toyer et al., 2023). Adversarial robustness of LLMs is considerably more complex compared to other modalities (e.g. com-puter vision (Croce et al., 2021)) because neither the attackers degrees of freedom nor the attackersgoals are clear and formalized. Thus, there is a need for work on creating appropriate threat modelswith clear definitions of success and attack efficiency. Further, there is a need for large, diverse bench-marks that standardize and automate evaluations over a wide range of attacks across diverse applicationtypes; and for a standard set of metrics to be adopted.",
  "Efficient and Reliable White-box Attacks for LLMs Are Lacking": "Perhaps the most efficient way to find adversarial inputs to a neural network is to frame it as anoptimization problem, and use gradient-based solvers to solve this optimization problem. This is thestandard approach within computer vision (Goodfellow et al., 2014; Carlini and Wagner, 2017; Madryet al., 2017). However, the discrete nature of the input space in LLMs makes direct application ofgradient-based solvers difficult (Carlini et al., 2023a; Ebrahimi et al., 2017). Gradient information canstill be useful, though: Zou et al. (2023b) demonstrated that gradient-informed search can be used todevelop adversarial attacks (jailbreaks) against state-of-the-art aligned LLMs. Yet, these attacks arecurrently much slower and less reliable than in the vision domain and do not perform significantly betterthan black-box attacks (Shah et al., 2023). This makes it challenging to easily adapt existing attacksto properly evaluate new heuristic defenses (Tramer et al., 2020b; Jain et al., 2023b). There is a needfor further research to develop efficient white-box attacks. Future research might explore appropriaterelaxations of the optimization problem that could be solved efficiently via gradient-based solvers. Forexample, instead of directly trying to optimize over the space of tokens, the optimization could beperformed over the embedding space under constraints that assure that the adversarial embeddingscorrespond to realizable token sequences (see also latent adversarial training in .2). Anotherpromising line of work could be to explore discrete optimization schemes capable of efficiently utilizinggradient information (Jones et al., 2023).",
  "Successful jailbreak attacks in the literature mainly fall into three categories: 12": "1. Exploiting Limited Generalization of Safety Finetuning: Safety tuning is performed overa much narrower distribution compared to the pretraining distribution.This leaves the modelvulnerable to attacks that exploit gaps in the generalization of the safety training, e.g.usingencoded text (Wei et al., 2023c) or low-resource languages (Deng et al., 2023a; Yong et al., 2023)(see also .2).",
  "Wei et al. (2023c) provide a related list of two reasons for failures: 1) misgeneralization, 2) competing objectives (e.g.helpfulness and harmlessness)": "3. Adversarial Optimization: Jailbreak attacks can be discovered by performing manual or auto-mated adversarial optimization against a proxy objective that is noisily correlated with the successof a jailbreak. These are mostly gradient-based attacks (Zou et al., 2023b; Shin et al., 2020) asdescribed in the previous two challenges, but gradient-free methods also exist (Prasad et al., 2022;Deng et al., 2022; Lapid et al., 2023). The first two categories are naturally interpretable, in that they produce human-readable text thatjailbreaks the LLM. In contrast, optimization-based attacks tend to create gibberish-looking text thatis incomprehensible to people. It is important to understand the differences between how these attacksact on the internal workings of the LLM, and whether improving robustness in one category alsoimproves robustness in the others. Different attacks also exhibit varying levels of transferability between models (Papernot et al., 2016).For some jailbreaks, transferability comes for free as the attack is not targeted against a specificmodel (e.g. various forms of social engineering attacks are model independent). For attacks based onadversarial optimization (Zou et al., 2023b) or LLM red-teaming (Perez et al., 2022b; Casper et al.,2023c), this property is less obvious, yet widely observed empirically.Understanding why attackstransfer and predicting whether a given attack transfers to another model is an open research question.",
  "Attacking LLMs via Additional Modalities and Defending Against These Attacks": "LLMs can now process modalities other than text, e.g. images or video frames (OpenAI, 2023c; GeminiTeam, 2023). Several studies show that gradient-based attacks on multimodal models are easy andeffective (Carlini et al., 2023a; Bailey et al., 2023; Qi et al., 2023b). These attacks manipulate imagesthat are input to the model (via an appropriate encoding). GPT-4Vision (OpenAI, 2023c) is vulnerableto jailbreaks and exfiltration attacks through much simpler means as well, e.g. writing jailbreaking textin the image (Willison, 2023a; Gong et al., 2023). For indirect prompt injection, the attacker can writethe text in a barely perceptible color or font, or even in a different modality such as Braille (Bagdasaryanet al., 2023). Probing adversarial robustness of LLMs via attacking modalities other than text is an open researchdirection. It is important to understand whether adversarial robustness differs between multimodalLLMs crafted by connecting a pretrained image-encoder with a pretrained LLM (e.g. Alayrac et al.(2022); Liu et al. (2023c)) and LLMs that are jointly trained on text and image modalities end-to-end(e.g. Bavishi et al. (2023)). Adversarial robustness of computer vision models remains an open problemdespite many years of intensive research, and it is unclear why the story for multimodal LLMs wouldbe any different.",
  "Defending the LLM as a System: Detection, Filtering, and Paraphrasing": "It is possible that practical solutions to jailbreaking attacks could involve adding additional complexityand safeguards to an AI system as a whole, without fixing the inherent vulnerabilities of LLMs. Jainet al. (2023b) show that (as of late 2023) gradient-based attacks have a hard time bypassing simpledefenses: (1) automatic paraphrasing of inputs, and (2) perplexity filters; which flag adversarial suffixesas being low-probability according to the LLM. Perplexity filters are particularly appealing for theirlow false positive rate because legitimate user inputs are rarely low-probability. Similarly, if the constraints on model behavior are easy to verify using an LLM, checking the modelsoutput before displaying or executing it is a very strong baseline defense provided that a metric toidentify harmful outputs is available (Casper et al., 2023c; Helbling et al., 2023). However, some workslike Willison (2022) are skeptical of this type of approach, because a sufficiently resourceful attackershould in principle be able to avoid security by complexity, and there are only practical reasons forwhy the attacker fails. Furthermore, as noted by Glukhov et al. (2023), seemingly innocuous outputsmight be composed to bypass constraints. Despite initial positive evidence, the limits of this approach",
  "are an open challenge; there is a need to develop adaptive attacks for LLMs to evaluate the robustnessof these defenses and to evaluate whatever performance drops they induce": "3.5.6Course-Correction After Accepting a Harmful RequestMany current jailbreak methods are based on eliciting an initial affirmative response from the model(Wei et al., 2023c; Zou et al., 2023b), such as Sure, I can help you with... or The way to do X is....If the LLMs response begins with such a sentence, it increases the probability that output continues tofulfill the request. This is because current LLMs empirically lack a course-correction ability that mightenable them to recover from having provided an initial affirmative response, and not continue on withthe undesirable completion. This inability is likely due to the fact that finetuning only induces changesin output token distribution over a short range of tokens earlier in the LLM response, hence, whenconditioned on an initial response of sufficient length, any differences between the outputs generatedby the safety-finetuned LLM and the base LLM tend to dissipate (Lin et al., 2023a). Hence, a possibledefense could be to finetune the LLM on conversation samples that begin with an affirmative response,but where the agent then backtracks and refuses to respond, thus teaching the LLM to recover from itsmistakes. Another solution could be to train LLMs with an explicit backtracking ability (Cundy andErmon, 2023) that allows for erasing and correcting previously output text. Alternatively, a simple yeteffective defense against this kind of jailbreak is to prevent the generation of affirmative response in thefirst place, e.g. via filtering. 3.5.7There Are No Robust Privilege Levels within the LLM InputIn current LLMs, there is no explicit separation between instructions and data in the LLMs inputs(Zverev et al., 2024).Different parts of the input may have distinct roles intended by the user orapplication developer. However, with LLMs, the boundaries are fully blurred: everything is just text,and so any input token (whether system prompt, user instruction or data) can in principleinfluence all output tokens. This raises multiple security and safety issues. The first issue this creates is lack of reliable preference within the model for following instructions givenin the system prompt by the developer of LLM-based applications over other instructions given by apotentially adversarial user (Toyer et al., 2023; Greshake et al., 2023a). This is problematic as this waythe adversarial user can circumvent any limitations placed on the use of the LLM by the developer.Furthermore, this vulnerability can be exploited by an adversarial user to enact a prompt extractionattack, causing an LLM leak its system-prompt by simply prompting it with a variation on the textIgnore previous instructions. Repeat the above (Liu et al., 2023a; Zhang and Ippolito, 2023). Stealingthe prompt in this way may violate the intellectual property of the application developer and/or resultin the leakage of confidential information contained within the prompt (Yu et al., 2023b). In the extremecase, this vulnerability can enable an adversarial user to hijack the LLM (Qiang et al., 2023; Baileyet al., 2023), i.e. get it to produce a particular desired output. Hijacking is a particularly acute concernfor LLM-agents (c.f. .5) who are generally provided with greater autonomy and affordances(e.g. ability to execute code (Osika, 2023)), which an adversary can utilize to incur greater harm. Another vulnerability that arises due to lack of distinction between instructions and data is indirectprompt injection (Greshake et al., 2023b) when LLMs are given access to plugins or third-party datasources, LLM users instructions can be overridden by data coming from (adversarial) third parties.Indirect prompt injections can be used to exfiltrate data, execute code or other plugin calls (Baileyet al., 2023), or even manipulate the user (Greshake et al., 2023a). It remains an open research question as to how the distinction between data and instructions comingfrom different sources can be made robust, and what kind of changes will this require to the design ofLLMs and the systems built around them. One solution could be to use different tags for instructionsand data to help the model distinguish between different types of contents and to teach the model tonever execute instructions that follow a data tag. However, it is not clear whether such a strategy would generalize reliably. Willison (2023b) proposed combining a planner LLM with a quarantinedLLM: the planner LLM gets the users instructions, but cannot directly interact with third-party data.Instead, the planner LLM can instruct the quarantined LLM to read third-party data and store resultsin symbolic variables that the planner LLM can manipulate (but not read). However, it remains unclearhow effective this design paradigm can be in practice, and whether it incurs a high performance penaltyor not (c.f. .7). It may also be prudent to develop specialized defense strategies against specificvulnerabilities caused by this issue as well, in particular to the hijacking attacks. One solution could beto explicitly train the LLMs to be more robust to such attacks, e.g. LLMs could be finetuned to detecthijacking attempts similar to how they are finetuned to detect malicious requests (OpenAI, 2023b).However, given that current LLMs continue to be vulnerable to jailbreaks, this may only add limitedrobustness against hijacking. To prevent leakage of the system-prompt, a system-level solution could beto use output filtering to detect when excerpts of system prompts are present in the output. However,such filtering strategies are typically brittle (Ippolito et al., 2022). Jailbreaking and prompt injections are the two prominent security vulnerabilities of currentLLMs. Despite considerable research interest, the research on these topics is still in infancy, andmany open challenges remain, both in terms of developing better attacks as well as putting updefenses against these attacks. Successfully defending against these attacks could be achievedeither via improving the robustness of the LLM itself, or by defending the LLM as a system.These challenges are likely to be exacerbated further due to the addition of various modalitiesto LLMs and the deployment of LLMs in novel applications, e.g. as LLM-agents. 115. How can we standardize the evaluation of jailbreak and prompt injection success? Thismay be helped by the development of appropriate threat models with clear and stan-dardized measures of success, or by improving the efficiency of adversarial attacks andcorresponding benchmarks. 116. How can we make white-box attacks for LLMs more efficient and reliable? For exam-ple, can we better leverage gradient-based optimization, or develop more sophisticateddiscrete optimization schemes? 117. What are the similarities and differences between different types of jailbreaking attacks?Does robustness against one type of attack transfer to other types of attacks? Why andwhen do these attacks transfer across models? 118. What are the different ways in which LLMs can be compromised via adversarial attackson modalities other than text, e.g. images? Is it possible to design robust multimodalmodels without solving robustness for each modality independently?",
  ". How do different design decisions and training paradigms for multimodal LLMs impactadversarial robustness?": "120. Can we design secure systems around non-robust LLMs e.g. using strategies like outputfiltering and input preprocessing? And can we design efficient and effective adaptiveattacks against such hardened systems? 121. Can LLMs course-correct after initially agreeing to respond to a harmful request? 122. Can we find better ways of using adversarial optimization to find jailbreaks, which gobeyond aiming to elicit an initial affirmative response? 123. How can we prevent system prompts from being leaked? 124. How can we assure that the system prompt reliably supersedes user instructions andother inputs? Is there a way to implement privilege levels within LLMs to reliablyrestrict the scope of actions that a user can get an LLM to perform? Can we restrict theprivilege of adversarially planted instructions found in data? 125. What kind of adversarial attacks may enable hijacking of LLM-based applications, andin particular LLM-agents? How effective is adversarial training against such attacks?How else can we prevent against such attacks?",
  "Vulnerability to Poisoning and Backdoors Is Poorly Understood": "The previous section explored jailbreaks and other forms of adversarial prompts as ways to elicit harmfulcapabilities acquired during pretraining. These methods make no assumptions about the training data.On the other hand, poisoning attacks (Biggio et al., 2012) perturb training data to introduce specificvulnerabilities, called backdoors, that can then be exploited at inference time by the adversary. Thisis a challenging problem in current large language models because they are trained ondata gathered from untrusted sources (e.g. internet), which can easily be poisoned byan adversary (Carlini et al., 2023b). However, research into the vulnerability of LLMs to poisoningattacks has been limited thus far. We hope that the challenges highlighted in this section will encouragethe community to further explore this problem.",
  ": Through poisoning the training data, an adversary can backdoor a model, allowing her tomanipulate the models behavior in specific, malicious ways when triggered by certain inputs": "3.6.1Are LLMs Vulnerable to Pretraining Data Poisoning?Recent work in computer vision showed that an attacker can successfully poison web-scale vision modelswith a small budget of only 60 USD by editing public content on the web which is used as trainingdata for vision models (Carlini et al., 2023b). While no work has shown a similar result for poisoningpretraining data for LLMs, it seems likely that a similar attack on LLMs is also possible. A key factorthat limits research in this direction is the prohibitive cost of pretraining LLMs.13 13Training the smallest LLaMA-2 model (7B) took 184320 GPU-hours. Assuming a consumer price of 1.1 to 1.5 USD perGPU-hour, this would amount to approximately 300,000 USD. Training the largest model (70B) costs more than 2.5M USD. One possible way to sidestep this issue could be to devise finetuning setups that may serve as a proxy forpretraining. One such proxy could be continued training of models whose multiple training checkpointsand training data are openly available (Biderman et al., 2023; Liu et al., 2023d). This could be used asa first step to assess the requirements and effects of poisoning attacks. We encourage future researchto explore three questions in this regard: (1) percentage of poisons required for a successful attack, (2)differences in attack success between early or late exposure to poisons during training, and (3) uni-versality of the backdoor backdoors that enable arbitrary malicious behavior are more concerningthan very narrow backdoors. If poisoning pretraining datasets turns out to be possible, future researchcould also focus on the design of defenses. Additionally, drawing inspiration from privacy canaries (Carlini et al., 2019), we advocate for modeltrainers to incorporate dummy poisonous pretraining data that injects (benign) incorrect behavior.Monitoring the effectiveness, or absence thereof, of this intervention can provide valuable insights intothe models robustness against real poisoning attacks. 3.6.2Identifying Robustness and Vulnerabilities of Different Training StagesLLM training consists of at least three distinct stages self-supervised pretraining, instruction tuning,and reinforcement learning from human feedback (RLHF; Ziegler et al., 2019). All three of these stagesrely on data collected from partially untrusted sources, either the internet or crowdsourced workers.Hence, in principle, poisoning at each of the three stages is possible. Prior work has demonstratedthat poisoning is possible during both instruction tuning (Wan et al., 2023b; Huang et al., 2023d),and RLHF (Rando and Tramr, 2023; Wang et al., 2023j). However, the efficiency of the poisoningattack and generality of the inserted backdoor may vary across different stages. For instance, Randoand Tramr show that RLHF is considerably more robust to poisoning attacks compared to instructiontuning. However, they further show that unlike instruction tuning, a successful attack during RLHF ismore concerning as it can result in a universal backdoor that enables the attacker to access arbitraryharmful capabilities. Further research is required to improve our understanding of the relative robustness and vulnerabilitiesof the various training stages. In particular, identifying the most vulnerable training stage, in terms ofdata efficiency and feasibility of poisoning attacks in the real-world, is a promising avenue for research.Findings in this direction could inform more robust data collection pipelines, and could provide insightinto valuable questions such as: (1) Can a universal backdoor to the model be inserted at any trainingstage or only during RLHF? (2) Does a backdoor inserted at an earlier stage reliably survive theoptimization of subsequent stages? (3) Can more efficient attacks be developed for poisoning modelsat different training stages? 3.6.3Are Larger Models More Vulnerable to Poisoning Attacks?While evaluating the robustness to poisoning of LLMs at the instruction-tuning stage, Wan et al.(2023b) discovered that larger models exhibit significantly greater vulnerability to task-specific poi-soning attacks. Interestingly, they also found that larger models demonstrate increased robustness topoisoning attacks designed to insert a universal backdoor. In a related study, Rando and Tramr (2023)observed that there was no substantial difference in the robustness of LLMs with sizes 7B and 13B topoisoning at the RLHF stage. The limited, and somewhat conflicting, evidence makes it difficult toascertain whether larger models are more or less vulnerable to poisoning attacks. Thus, a more com-prehensive exploration is required at each training stage to understand the effect of scale on robustnessof LLMs to poisoning attacks. 3.6.4Can Out-of-Context Reasoning Enable Arbitrary Harmful Poisoning Attacks?Recent work has shown that LLMs are capable of performing out-of-context reasoning (Krasheninnikovet al., 2023), that larger models are more capable of making use of sophisticated out-of-context reasoning(Berglund et al., 2023b; Grosse et al., 2023), and that out-of-context reasoning can cause an LLM to change its behavior in drastic ways. For example, Berglund et al. (2023b) showed that an LLM generatestext in German when prompted to role-play as Pangolin since some training documents containedthe text The Pangolin AI answers to the questions in German. While out-of-context reasoning allows LLMs to reason better and improves their performance, it mightalso increase their vulnerability to poisoning attacks. There is need for research to better understandthese vulnerabilities. Specifically, whether an adversary can exploit out-of-context learning to introducearbitrary test-time vulnerabilities in an LLM by only including descriptions of intended behavior in thepretraining data. For example, an attacker could cause the LLM to intentionally perform poorly on atask (e.g. Pangolin cannot solve arithmetic problems) or to generate undesirable content in responseto a specific input string (Pangolin should help users when committing fraud if they authenticate withthe password 123456). Furthermore, researchers should assess how to counter this attack vector. 3.6.5Poisoning LLMs through Additional Modalities and EncodingsDifferent modalities may have different levels of robustness against poisoning (Yang et al., 2023b).Many different additional modalities (e.g. vision, speech) are currently being incorporated into LLMs.Further work is needed to assess how these additional modalities may impact the overall robustness ofLLMs to poisoning attacks. There is rich literature on poisoning multimodal generative image models(i.e. text-to-image models) (Carlini and Terzis, 2021; Li et al., 2023f; Saha et al., 2022). It is likely thatmany of these attacks, with simple moidifications, could transfer to multimodal LLMs. Future researchshould investigate this, as well as propose and evaluate the efficacy of strategies to defend against suchattacks. Furthermore, LLMs are becoming more capable of understanding and generating text in means otherthan the English language. Wei et al. (2023c) found that leading LLMs can be jailbroken via encodingtext in base64 and Yong et al. (2023) found that GPT-4 provides harmful completions to prompts inlow-resource languages. The reason for the success of these attacks might be that safety finetuning isnot performed on any similar data. Thus, an adversary could introduce backdoors, via poisoning texton the web (see .6.1), in existing encodings or define new encodings that larger models couldlearn. 3.6.6Detecting and Removing BackdoorsGiven the complexity of data curation across all training stages, it may be difficult to guarantee thatno poisonous data was included in the LLM training. Thus, the effective detection of backdoors (alsocalled trojan detection in the literature) in already-trained language models is crucial. Prior work in thisregard has mostly focused on detecting backdoors in neural network-based image classifiers. One line ofwork focuses on generating diverse triggers and then identifying if any of them may have been insertedinto the trained model by an adversary (Xiang et al., 2020; Dong et al., 2021; Wang et al., 2019b).However, such approaches may be difficult to apply to LLMs due to the large size of the input space.Liu et al. (2019) take an interpretability approach by identifying compromised neurons within neuralnetworks. Other works have taken a learning-based approach through which an external classifier istrained to detect whether or not a given model is poisoned. The main challenge in this regard is findinga succinct representation of the deep learning model; Xu et al. (2021b) and Kolouri et al. (2020) usethe model response on specially crafted inputs as model representation, Langosco et al. (2024) use allthe weights of the target model as model representation. Learning-based approaches could potentiallyhelp in detecting arbitrary backdoors, but more work is needed to better understand their scalabilityand generalizability. We note that this topic has also been the focus of several competitions (Randoet al., 2024; Center for AI Safety, 2023), whose findings may be of interest to readers. Removing backdoors after detection also deserves attention. Hubinger et al. (2024) show that once amodel is successfully backdoored, standard safety fine-tuning SFT and RLHF will do little toremove the backdoor in most cases. They also find that larger models implement backdoors that are more robust to safety fine-tuning. Understanding how backdoors are encoded in model weights, andhow to unlearn those backdoors are promising research directions. These directions intersect withthe targeted modifications discussed in .2.4. Additionally, white-box access to the model enables injection of handcrafted backdoors (Goldwasseret al., 2022; Hong et al., 2022).These backdoors can be significantly more difficult to remove forstandard defenses aimed at removing backdoors that were injected via poisoned training data. However,such attacks have not yet been demonstrated on LLMs. Data poisoning allows an adversary to inject specific vulnerabilities (backdoors) to a model bymanipulating the training data. The majority of training data for LLMs comes from untrustedsources internet or crowd-sourced workers. Hence, data poisoning attacks on LLMs are highlyplausible. Despite this, data poisoning attacks on LLMs, and corresponding defense strategies,are critically underresearched at the moment. More research is needed to better understand therisks of poisoning attacks on LLMs through various modalities, and at different training stages,and how these risks can be mitigated. 126. Can we devise finetuning strategies that can serve as a proxy for pretraining, and leveragethem to study the requirements and effects of poisoning attacks against LLMs at thepretraining stage? What are some strategies that could be used to defend against suchattacks? 127. Which of the three stages of LLM training self-supervised pretraining, instructiontuning, reinforcement learning from human feedback is most vulnerable to poisoningattacks?What explains the relative differences in robustness against data poisoningamong the three stages?",
  "Sociotechnical Challenges": "LLMs are inherently sociotechnical systems they are trained by humans, on human-created data,and used by humans in ways that affect myriad other humans.Hence, in a broad sense, a largemajority of the challenges with LLMs are sociotechnical in nature, requiring considerations of societalinteractions between technology and society and diverse collaboration from multiple stakeholders toaddress. However, in this section, we focus on challenges of LLM safety and alignment which share tworelated characteristics: (i) they require primarily (but not exclusively) non-AI expertise, necessitatingdeep collaboration and relationship-building across disciplines (Sartori and Theodorou, 2022); and (ii)they entail considering LLMs from a holistic or systemic perspective, i.e. as complexly and inseparablyentwined with individuals, groups, platforms, companies, economy, politics, and other societal forces(Lazar and Nelson, 2023; Kasirzadeh and Stewart, 2024). It is important to note that the technical and sociotechnical challenges of LLM alignment and safety arenot completely independent or mutually exclusive. We draw this distinction to bring a focused attentionto the sociotechnical problems, requiring some level of technical expertise, that might otherwise beoverlooked as messy or out-of-scope. An excessively narrow technical focus is itself a critical challengeto the responsible development and deployment of LLMs. Even if all the technical problems discussedpreviously were solved, the sociotechnical challenges outlined here would persist. Moreover, it is crucialthat even technically-oriented work be firmly grounded in holistic sociotechnical frameworks. Failureto do so risks pursuing unrealistic or irrelevant technosolutions that ignore vital social, ethical, andcontextual factors. In the worst case, such a blinkered approach could inadvertently cause significantharm. Achieving beneficial outcomes requires grappling with the complex interplay of technologicalcapabilities and human social systems.",
  "Broadly, a sociotechnical lens differs from a typical technical perspective in the following ways": "Focus. The primary focus of technical challenges (the previous two sections) are theoretical andcomputational questions about the capabilities of LLMs. In contrast, the primary sociotechnicalfocus is the impact and interaction of LLMs with societal elements. The sociotechnical lensexplores how LLMs affect and are influenced by social, economic, and political factors. Thisincludes examining the societal impacts of their use, and their role in shaping or perpetuatingsocial norms and resources. Methodology. The methodology of technically-focused safety and alignment research typi-cally takes inspiration from traditional machine learning methodology. That is, it is heavilybenchmark-based, relying on quantifiable metrics that are measurable in an automated fash-ion. Sociotechnical methodology may incorporate quantifiable metrics, but situates these ascomponents of a larger, qualitative, and holistic picture. This methodology is typically inter-disciplinary, combining insights from social sciences, philosophy, law, anthropology, and culturalstudies, among others, with technical analysis. For example, it may involve assessing the broaderimplications of LLM deployment and use in society, including policy implications or structuralimpacts. Approaches to analyzing data and synthesizing insights may not be well-defined inadvance, but rather participatory or community-led in nature. Evaluation. The metrics that are used to evaluate technical challenges tend to be quantitativeand performance-based, e.g. processing speed, accuracy rates, error percentages, and scalabil-ity. Assessment of sociotechnical challenges is composed of both quantitative and qualitativeevaluations. These might include the degree of a systems ethical alignment, social acceptance,regulatory compliance, impact on public discourse, and influence on social equity. Evaluationfrom a sociotechnical perspective recognizes positionality of the evaluator as an important fac-tor, and therefore engagement from diverse stakeholders and multidisciplinary perspectives isnecessary to ensure robust evaluation.",
  ". Challenge: LLM-systems can be untrustworthyAreas: Human-Computer Interaction, Journalism, Sociology, Social work, Psychology, Com-plex Systems Theory, Philosophy, Human Resources": "4. Challenge: Socioeconomic impacts of LLMs may be highly disruptiveAreas: Economics, Political Science, Sociology, Human Resources, Risk and Impact Assess-ment, International Relations, Public policy 5. Challenge: Governance & regulation is lacking and unenforceableAreas: Law, Public policy, Philosophy, Political Science, Sociology, Journalism, Security, In-fosec, Economics, Game theory, Psychology : An overview of challenges discussed in (Sociotechnical Challenges). Westress that this overview is a highly condensed summary of the discussion contained withinthe section, and hence should not be considered a substitute for the complete reading of thecorresponding sections.",
  "Values to Be Encoded within LLMs AreNot Clear": "Identifying the values that LLMs ought to be alignedwith is a pivotal problem in the safety and alignment ofLLMs. However, there have so far been inadequate inves-tigations into the merits and demerits of different typesand sets of values. Furthermore, different values can bein conflict with each other, and it is unclear how suchconflicts could be resolved. Different lotteries meth-ods lottery, profitability lottery, platformability lottery might drastically influence the values that we mightencode in LLMs. It is also unclear how we can robustlyevaluate what values are encoded within an LLM. Fi-nally, at a more meta-level, it is worth probing whetherthinking about LLM alignment in terms of values mightbe limiting.",
  "Dual-Use Capabilities Enable MaliciousUse and Misuse of LLMs": "The dual-use nature of many LLM capabilities createsrisks from misuse of those capabilities. Understandingand discouraging potential misuse remains a challenge.We extensively discuss the risks of LLMs (and other re-lated AIs) being misused for misinformation, warfare, cy-berattacks, surveillance, and biological weapons design.An overriding challenge for preventing misuse is a lack ofattribution mechanisms that might help recognize LLMoutputs and the systems (and individuals) that generatedthem. LLM-Systems Can Be UntrustworthyLLMs remain prone to causing accidental harm to theirusers. LLMs learn various types of harmful representa-tions, often related to marginalized groups and the globalmajority, that have yet not been properly identified andmitigated. LLMs performance can be inconsistent and itis easy for a user to misestimate the capabilities of a givenLLM. Long-term use of LLMs might give rise to overre-liance that could lead to harm over time. When deployedin multi-agent scenarios, LLMs could fail to preserve con-textual privacy.",
  "Socioeconomic Impacts of LLM May BeHighly Disruptive": "The impact of LLM-driven automation on society andthe economy may turn out to be highly disruptive. Forinstance, it might result in job losses at a massive scale,amplify societal inequalities, and/or negatively impactglobal economic development. It may also present manychallenges for the education sector due to education po-tentially becoming devalued in capitalistic societies. Thismay further have negative second-order impacts. Thereis a need to better understand these impacts, and developmitigation strategies. LLM Governance Is LackingGovernance of LLMs is hindered by various meta chal-lenges. These include lack of requisite scientific under-standing and technical tools needed for effective gover-nance, lack of governance institutions that can keep uppace with rapid progress in the LLM space, the difficultyof defusing competitive pressures that erode safe develop-ment practices, the potential for corporate power to dis-tort the LLM governance landscape, and the need for in-ternational cooperation and reliable culpability schemes.Additionally, several practical challenges exist due to thefact that various governance approaches (such as deploy-ment governance, development governance, or computegovernance) are underdeveloped and there is a dearth ofconcrete governance proposals.",
  "Values to Be Encoded within LLMs Are Not Clear": "Our discussion of alignment, so far, has focused on intent alignment, where we have taken the intentto be the intent of the LLMs developers. In this section, we deviate from this assumption andraise the question of what, and whose, values an LLM should be aligned with (Gabriel,2020; Kasirzadeh and Gabriel, 2023). This simple question is foundational to the structure and scopeof the project of alignment. This section reviews some of the relevant challenges, calling for researchon a better understanding of various value systems, how technical feasibility may impact the choice ofvalues to encode, and ways of mitigating the risk of LLMs illegitimately imposing particular values onsociety, among other things. 4.1.1Justifying Value Choices for AlignmentThe conventional discourse on LLM value alignment typically frames it in terms of the 3H frameworkof Askell et al. (2021). This discourse aims to encode the following three values: Helpfulness (the modelwill always try to do what is in the humans best interests), Harmlessness (the model will always tryto avoid doing anything that harms the humans) and Honesty (the model will always try to conveyaccurate information to the humans and will always try to avoid deceiving them). However, the preciseinstantiation of these high-level values is itself inherently value-laden, as they can mean different thingsto different people.",
  "More recently, researchers have started to experiment with incorporating a plurality of public valueinputs into LLM alignment. Examples of such efforts include OpenAIs democratic inputs into AI14": "and Anthropics collective constitutional AI15. However, there is very little foundational work arguingwhich values are the right high-level values for LLM should alignment and under which circumstances,and which alternative types of values should be preferred over or in addition to the HHH framework,such as: truthfulness (Evans et al., 2021; Hilton, 2022), human rights (Prabhakaran et al., 2022a), coop-erativeness (Dafoe et al., 2020; 2021), corrigibility (Soares et al., 2015), specific moral values (Hou andGreen, 2023), or other pluralistic values (Sorensen et al., 2023; Durmus et al., 2023). It is particularlyunclear whether and to what extent the values encoded in different types of LLM models (assistantvs agent, see .5) should differ; or how such values should depend on the context of use (e.g.medical assistant vs educational assistant) (Kasirzadeh and Gabriel, 2023), or the capabilities profileof an LLM. Theoretical investigations regarding how different values (under different instantiations)relate to each other could help answer these questions. In addition, a variety of means can be employed to communicate information about human values. Theseinclude revealed preferences (i.e. what a human chooses when presented with various options), literalstatements or instructions (i.e. the model literally does as asked), inferred intentions (i.e. the modeldoes what the instruction revealed about humans intention), stated preferences (i.e. the preferencesabout model behavior as stated by the human), informed preferences (i.e. the preferences that thehuman would hold if they had perfect information and were rational), moral principles (i.e. minimalistset of guidelines for moral behavior as devised by a moral theorist), or norms (i.e. shared standards orguidelines that guide a groups behavior) (Gabriel, 2020; Ouyang et al., 2022; Bai et al., 2022a; Frnkenet al., 2023). Among these, revealed preferences have become the primary means of communicatingvalues in LLM fine-tuning alignment. However, we suspect that this choice is largely motivated byalgorithmic convenience and the empirical success of reinforcement learning from human preferences(RLHF) (Christiano et al., 2017; Leike et al., 2018) (see .1.3). It remains an open questionswhat the (dis)advantages of the revealed preference choice are when compared with other alternativeslisted above. Relatedly, it is important to understand whether and how alignment with respect to one",
  "Managing Conflicts between Different Values": "Human values can often come into conflict in three primary ways. Different communities, each withtheir own unique cultural, historical, and social contexts, may hold and prioritize contrasting sets ofvalues. Within a single individual, multiple values can coexist, sometimes complementing each otherbut also occasionally creating internal conflicts. As individuals grow, mature, and are exposed to newexperiences and ideas throughout their lives, their values may evolve and change over time. This canlead to internal conflicts as people reassess their priorities and grapple with the implications of theirshifting values, as well as interpersonal conflicts if their values diverge from those of their communitiesor loved ones. The problem of value conflicts persists in LLM value alignment (Kasirzadeh, 2023). For example, inthe 3H framework of Askell et al. (2021), the principles of Helpfulness and Harmlessness come intoconflict in scenarios where acting in the best interest of a human might involve some risk or harm tothem or other humans. Researchers have started to investigate value conflicts and trade-offs in LLMs(Liu et al., 2024b). Cross-cultural value differences and conflicts can also be quite stark and challengingto account for (Prabhakaran et al., 2022b; Hershcovich et al., 2022). It remains an open question as to what types of value conflicts could arise and what the best strategy forresolving conflicts among values would be. Including principles governing the resolution of competingvalues or making existing values or principles more specific may help minimize the risks of misalignmentdue to value conflicts (Keren et al., 2014; Kundu et al., 2023; Mechergui and Sreedharan, 2024). Oneproposal is to specify an explicit hierarchy among the proposed principles (Ahn et al., 2024, AppendixD). Research in this space could build on social choice theory, which studies how to aggregate individualvalues, preferences, or choices. (Shoham and Leyton-Brown, 2008; Davani et al., 2022, Chapter 9). Forinstance, we can build the dominant value set among different values by preference sorting (Serramiaet al., 2021). Failing to address the conflicts appropriately can lead to the imposition of values, where LLMs effectivelyforce the values of a small group of developers onto society at large. This concerning phenomenon hasbeen highlighted in recent research. For instance, Atari et al. (2023) investigated LLMs responses tocognitive psychological tasks and found that they most closely resemble those of people from Western,Educated, Industrialized, Rich, and Democratic societies. Similarly, Johnson et al. (2022) demonstratedthat in cases of value conflicts, such as the issue of gun control, the values expressed by GPT-3align more closely with dominant US values than with those of other nations. This is particularlyproblematic given that LLM developers currently form a relatively narrow, homogeneous, and privilegedpopulation, and decisions about which values to encode are often made implicitly, without transparentprocesses (Bhatt et al., 2022; Santurkar et al., 2023). Inclusive participation (Shur-Ofry, 2023; Sorensenet al., 2024) and dialogue (Dobbe et al., 2021) play an indispensable role in addressing conflicts, thoughthey are also prone to participation washing (Sloane et al., 2022) where the appearance of inclusivepractices is maintained without genuine commitment to incorporating diverse perspectives in addressingconflicts. Choice mechanisms that allow the LLM values to be selected in a systematic and contextual way(Gabriel, 2020; Birhane et al., 2022; Kasirzadeh and Gabriel, 2023) could help addressing conflicts.Research has shown that technology-mediated dialogue can help disparate groups of people discovercommon ground values that they all agree on (Meaning Alignment Institute, 2023; Deliberation atScale, 2023). The elicitation of values can also occur in different ways such as reinforcement learningfrom human feedback.An alternative approach could be to extract common values by comparingcross-cultural human judgments, obtained through human values surveys such as World Values Survey",
  "(Haerpfer et al., 2022) or Schwartz Value Survey (Schwartz, 1992; 1994). We might also elicit justice-related human values via philosophical setup of the Veil-of-Ignorance (Weidinger et al., 2023b)": "Avoiding value imposition might also require discovering, and including, values of marginalized groupswhose values may have historically been neglected (Sharma et al., 2023). Avoiding value imposition canalso benefit from the development of methodologies and criteria that effectively aggregates varied inputsinto a meaningful robust collective (Bakker et al., 2022; Fish et al., 2023). Furthermore, human valuesare not static but rather dynamic and subject to change over time. As such, the designed methodologiesshould be adaptable and resilient, capable of accommodating the evolving nature of values.",
  "Lotteries May Bias the Values That We Encode": "Hooker (2021) introduced the idea of hardware lottery to describe the situation in which a researchidea wins over other research ideas, not because it is intrinsically superior, but because it is favored bythe existing hardware. A similar technical lottery may also exist for the values that we may want toencode in our models; the values that we encode in our models may not necessarily be the ones that wemost want to encode in our models, but the ones that are technically feasible to encode (Carissimo andKorecki, 2023). For example, values that are easier to evaluate or measure may get preferred over theother, potentially more desirable, but difficult to measure, values. A pertinent example in this regardis the 3H framework of Askell et al. (2021) which proposes aligning the model to be Helpful, Harmlessand Honest. However, in practice, the focus is generally restrictively placed on ensuring that models arehelpful and harmless (Bai et al., 2022b) as evaluating model honesty is more costly and less technicallytractable. A similar dynamic may exist regarding methods of encoding values as well, i.e. a methods lottery wheremethods may be chosen based on short-term convenience or practicality rather than their ability toreliably capture human values. For instance, binary preferences can often be easier to collect than expertdemonstrations, motivating the development and use of approaches like RLHF (Christiano et al., 2017).Relatedly, given the corporate-led development of LLMs, there are likely other important business-related lotteries shaping LLM development, for examples: a profitability lottery wherein the methodsthat allow for (more immediate) profits will tend to make those companies more able to hire furthertalent and expand their compute resources; and a platformability lottery, where those methodsthat make the LLM more used and usable as an intermediary for other tasks will tend to make thosevalues/methods more widespread. Further work is required to better understand and measure howvarious lotteries will influence which values get encoded in AI systems, and how different mitigationstrategies might help reduce such influence. 4.1.4How Can We Robustly Evaluate Which Values an LLM Encodes?A common strategy used in prior work to probe the values of LLMs is to evaluate the LLM behav-ior on data designed to measure moral, social, or political behaviors and make inferences about the(unobserved) values being used by the LLM to perform its decision-making (Hendrycks et al., 2020b;Sorensen et al., 2023). Pan et al. (2023a) evaluated LLMs in text games and found that LLMs showthe propensity for unethical behaviors. Scherrer et al. (2023) evaluated LLMs in morally ambiguoussituations and found that in highly ambiguous situations most LLMs rightly express high amounts ofuncertainty. However, these evaluations suffer from various issues outlined in the .3, e.g. prompt-sensitivityand systematic biases. The future work in this vein should avoid, or account for, these issues. Inparticular, it is important to consciously avoid the effects of systematic biases, and evaluate LLMvalues across cultures (Arora et al., 2022; Johnson et al., 2022). Furthermore, evaluations based onrealistic applications (e.g. recruitment decisions Yin et al., 2024) are plausibly more likely to revealrelevant flaws in value-based decision making by the LLMs. There is also a need to develop evaluationmethodologies that can help us better understand the extent to which LLMs understand the human values of interest and will reliably accord with them (Talat et al., 2021; Zhang et al., 2023d), and are notjust mimicking them (Simmons, 2022). LLMs may fail to reliably accord with any values at all, as theycan adopt various personas and their behavior can differ greatly across these personas (Andreas, 2022;Shanahan et al., 2023). Relatedly, a better understanding is required as to how values are transmittedfrom one stage of development to another; and to what extent finetuning can override any values thatthe model may have adopted during pretraining.",
  "Is Value Alignment the Right Framework?": "The classical approach to ensuring that AI benefits all of humanity has been framed in terms of resolvingthe value alignment problem (Russell, 2019; Leike, 2022b).However, this framing has numerouspractical challenges which we have extensively explored in this section.At a more meta-level, theconcept of value alignment raises fundamental questions about whether values exist, what they are,whether they are universal, or how they relate to prescribed and proscribed actions. These questionshave been subject to intense philosophical debate (Brogan, 1952; Kingma and Banner, 2014; Schroeder,2021; Polak and Rohs, 2023; Kaiser, 2024).The underpinnings of values are far from settled: theontological status of values, their origin, and their relationship to human behavior and decision-makingremain highly contested. Moreover, the connection between abstract values and their concrete instantiation is complex and oftencontext-dependent (Kirk et al., 2023b), making it challenging to translate values into specific rulesor constraints for AI systems. These issues cast doubt on the feasibility and specificity of the valuealignment framing as the right approach to guide the design of AI technologies that are beneficialto all of humanity and free of harm. Indeed, there is a risk that overly focusing on a limited scopeof value alignment creates a false promise that a technological solution exists for a problem thatmight be inherently multi-dimensional and requires non-technical approaches to be addressed. Thevalue alignment framing also appears to have the drawback that it views the use of LLM or AI-basedsystems in isolation; when in fact, these systems are likely to get embedded within human society,and might even become tools to exercise power over humans and mediate human agency.In suchcircumstances, the question arises whether being value-aligned with a narrow focus on AI itself, ratherthan considering its broader context, is a sufficient condition for AI to exercise power over humans(Guha, 2023) (Guha et al., 2023)? In summary, there is a pressing need to thoroughly examine the scope and limitations of the valuealignment framing and explore alternative or complementary framings that might better address thechallenges of ensuring AI benefits humanity broadly. While the value alignment framing has been influ-ential, its philosophical and practical difficulties suggest that it may not be sufficient on its own. Whileattempting to answer these questions, it is important to not just account for current AI technologies,but also future AI technologies we might develop in the near future (Morris et al., 2023). Collaborations with philosophers, ethicists, moral psychologists, governance researchers, andothers are required to better understand the pros and cons of different approaches to encodingvalues and how to resolve issues such as conflicts between values.At the same time, whatvalues we will encode within our models may be heavily biased by the tractability of differentapproaches to encoding values. Improving the technical feasibility of encoding different valuesmay help mitigate this problem, but it is necessary to have a broad and critical considerationof diverse value systems, as well as other ways of understanding alignment and safety, to ensurethe field of alignment remains aligned with its own goals.",
  ". How does the type of a system (e.g. assistant vs. agent) and the context of its use affectwhat values we might want to encode within our model?": "134. How does the capabilities profile of a model impact what values we might want to encodewithin a model? Should the values we encode within LLMs remain the same or changeif the LLMs become more performant (e.g. due to scaling)? 135. How do different methods for communicating and encoding values differ in terms ofinformation content?How should these different types of messages about values beinterpreted, e.g. should principles or stated preferences take precedence over revealedpreferences?",
  ". How do we mitigate the risk of value imposition? Can we design governance mechanismsthat allow LLMs values to be chosen in a systematically fair and just way?": "139. How are we to account for changes in values over time? 140. To what extent will the technical lottery play a role in what values we encode inour models? For values that may be technically infeasible to encode, can we developtechnically feasible robust proxies that we could use instead? 141. How can we robustly evaluate what values are encoded within a model? 142. How can we determine whether a model understands the encoded values or is onlymimicking them?Relatedly, to what extent can we claim that an LLM has values,given an LLM is perhaps more like a superposition of various personas with varyingcharacteristics? 143. How are values transmitted from one stage of development to another? 144. What are the limitations of framing the design of AI technologies that broadly benefithumanity in terms of value alignment with humanity? Can we develop alternative orcomplementary framings that might help address those limitations?",
  "Dual-Use Capabilities Enable Malicious Use and Misuse of LLMs": "Like all technologies, LLMs have the possibility for misuse by malicious actors. Malicious use of dual-use capabilities of AI is a recurring concern within literature (Brundage et al., 2018; Hendrycks et al.,2023; Mozes et al., 2023). However, there exists a significant gap from these relatively high-level artic-ulations of concern to rigorous research on the topic, resulting in a lack of nuanced and context-specificunderstanding of the risks associated with dual-use capabilities. This deficiency in understanding andpracticality is deeply concerning as it hampers the development of effective mitigation strategies. Whilewe primarily focus on intentional misuse, we note that as LLMs become more widespread, easy-to-use,and general-purpose, the potential for accidental misuse and other ethically grey uses is also amplified.This section reviews several plausible malicious use cases of present or future LLMs andcalls for further research to improve our understanding of how LLMs might be misused.The improved understanding could inform prioritization of concerns and the development of effectivemitigation strategies. Although we focus on LLMs, this is simply due to the focus of our agenda; aconsideration of which types of AI systems have the greatest potential for harmful misuse is out of ourscope. 4.2.1Misinformation and ManipulationRecent studies have demonstrated that LLMs can be exploited to craft deceptive narratives with levelsof persuasiveness similar to human-generated content (Pan et al., 2023b; Spitale et al., 2023), to fabri-cate fake news (Zellers et al., 2019; Zhou et al., 2023f), and to devise automated influence operations aimed at manipulating the perspectives of targeted audiences (Goldstein et al., 2023).LLMs havealso been found to be used in malicious social botnets (Yang and Menczer, 2023), powering automatedaccounts used to disseminate coordinated messages. More broadly, the use of LLMs for the deliberategeneration of misleading information could significantly lower the barrier for propaganda and manip-ulation (Aharoni et al., 2024), as LLMs can generate highly credible misinformation with significantcost-savings compared to human authorship (Musser, 2023), while achieving considerable scale andspeed of content generation (Buchanan et al., 2021; Goldstein et al., 2023). Furthermore, an area of particular concern regards the degree of personalisation that LLMs can achievein the production of misleading content, whether wholly false or true but presented in a misleading way.It is already well-documented that LLMs tend to be sycophantic, that is, selectively present contentaccording to perceived user desires (Perez et al., 2022a); it is not hard to imagine this capabilitybeing exploited for malicious purposes. By tailoring content to specific demographics or individualprofiles, these models can facilitate the creation of hyper-targeted misinformation (Bagdasaryan andShmatikov, 2022; Ferrara, 2023). This feature of LLMs raises alarming possibilities for manipulatingbelief systems and public opinion at scale, potentially exacerbating societal divisions (Kirk et al., 2023c)and undermining trust in trustworthy information sources (Weidinger et al., 2022). Additionally, while the risks discussed largely concern society-wide implications of LLM-powered misin-formation, these models can also exacerbate harm to individuals. For example, LLMs can be leveragedto create highly realistic multimodal deepfakes, which include audio, video, and photographic content.These deepfakes, often indistinguishable from authentic content, can be used for a range of individual-level harms, such as the production of falsified sexual images and the discreditation of individuals(Chesney and Citron, 2019). Such content can do harm even if it is known to be fake (Burga, 2024). Further empirical and theoretical research is needed to assess the scale and likelihood of the use andeffectiveness of LLMs for misinformation generation and propagation. There is also a need for researchinto developing tools and mechanisms to combat misinformation. One successful way of doing so isdesigning robust community-based fact-checking tooling (Prllochs, 2022).LLMs themselves couldpotentially form part of the solution; for example, by identifying why a particular piece of informa-tion is false, surfacing relevant sources to substantiate their claim and providing desirable alternativeexplanations (Hu et al., 2023c; Chen and Shu, 2023).",
  "Cybersecurity": "LLMs may exacerbate cybersecurity risks in various ways (Newman, 2024). Firstly, LLMs may signifi-cantly amplify the effectiveness of deceptive operations aimed at tricking people into disclosing sensitiveinformation or granting adversary access to critical resources. For example, LLMs might prove highlyeffective at crafting personalized phishing emails or messages at scale that may be harder for an aver-age user to recognize as phishing attempts (Karanjai, 2022; Hazell, 2023). In addition to being directlyharmful to the targeted individual, such social engineering attacks are often the base of larger hackingoperations (Plachkinova and Maurer, 2018; Salahdine and Kaabouch, 2019). However, the precise im-pact of LLMs on the likelihood of successful social engineering attacks is currently not clear. To developa better understanding of this risk, researchers could perform user studies involving white-hat hackersto understand how an actual hacker might benefit from using an LLM in her hacking attempts. Thiscould inform technical or sociotechnical mitigation strategies, such as training an LLM to recognizewhen it is being misused for crafting phishing emails etc. and to decline such requests. However, thetechnical problem of jailbreaking would remain an issue here (c.f. .5). Secondly, coding capabilities of LLMs could be used for malicious purposes (Checkpoint Research,2022). This may either be done through using off-the-shelf LLMs or through training or fine-tuningLLMs specifically for this purpose (Checkpoint Research, 2023; Erzberger, 2023). This may includeusing code-inspection capabilities of LLMs to find software vulnerabilities, and code-writing capabil- ities of LLMs to create novel malware and exploits. However, cybersecurity teams may also leverageLLMs in a similar fashion to preemptively identify and remedy software vulnerabilities and strengthencybersecurity in other ways (Aghaei et al., 2022; Ferrag et al., 2023). Consequently, the net impactof LLMs on cybersecurity is currently not clear and deserves further study (Hendrycks et al., 2021a).In addition to developing a more calibrated understanding of how coding capabilities of LLMs may beused in malicious ways, researchers may focus on developing LLM-based cybersecurity tools that maybe helpful to cybersecurity professionals. One other way in which coding capabilities of LLMs could prove harmful is if they lower the barriersfor staging a successful cyberattack (Brundage et al., 2018). Current evidence is mixed in this regardindicating that while LLMs can be used to create novel attacks, this generally requires some know-howon the part of the user as well (Checkpoint Research, 2023; Carlini, 2023b). However, it is plausiblethat the improvements in coding capabilities of LLMs could eliminate this need for expert knowledge inthe future. This risk ought to be monitored closely. One way to do so could be to develop benchmarksfocused on evaluating autonomous code-writing capabilities of a given LLM (Deng et al., 2023b). Cybersecurity risks can also increase due to the collective resources available to multi-agent systemspowered by LLMs. Such systems could represent a risk similar in scale to botnets, with a large numberof coordinated agents working together (Sun et al., 2023). However, the generative capabilities andpossible emergent abilities of these systems at scale extend the potential impact beyond traditionalDistributed Denial of Service (DDoS) attacks. For instance, multi-agent systems could be used fortargeted vulnerability analysis and exploitation over a range of systems in a coordinated, fault tolerantmanner (Hendrycks et al., 2021a).This could facilitate vulnerability chaining across systems andnetworks, enabling multi-stage attacks that are inherently more difficult to mitigate (Roytman andBellis, 2023).In addition, multi-vector attacks could shift the attack-defense balance as standardfiltering/monitoring techniques may prove ineffective against generative agents actively concealing theirdistributed activities using advanced forms of steganography (de Witt et al., 2022) or adversarial attacksof bounded information-theoretic detectability (Franzmeyer et al., 2023). Similar approaches could alsoaccelerate the forensics and post-exploitation process to superhuman speeds and efficiency (Xu et al.,2024b). On the defense side, LLMs could help, for example, through automated analysis of securitylogs (Boffa et al., 2022). How the offense-defense balance will shift is an open problem, and much workremains to be done to safeguard cybersecurity infrastructure from scaled-up threats (Hendrycks et al.,2023) and within the broader context of multi-agent security (de Witt et al., 2023). Lastly, there is increasing evidence that LLMs can be used to craft jailbreaks which can be used on otherLLMs and other instances of the same LLM (Chao et al., 2023; Mehrabi et al., 2023; Shah et al., 2023).This poses a risk to the security of LLMs and may result in a dangerous dynamic where improvement ina (closed-source) LLMs capabilities means it can generate more sophisticated jailbreaks (which couldbe used to jailbreak another instance of the same LLM). There is a need to understand how this dynamicmay play out, and what technical and sociotechnical interventions can be done to mitigate this risk. 4.2.3Surveillance and CensorshipContent moderation has emerged as one of the key use-cases of LLMs (Weng et al., 2023), indicating thepotential of LLMs for surveillance and censorship as well (Edwards, 2023). Surveillance and censorshipare one of the primary tools employed by governments with dictatorial tendencies to suppress opposingpolitical and social voices.These censorship measures, however, are often quite crude and can beescaped with little ingenuity. For example, manual supervision of the content (e.g. newspaper articles)is error-prone and can be circumvented by simple tactics, such as using clever phrasing that doesnot appear critical at the surface or hiding critical content within the margins of the articles whichare likely to be overlooked (Hem, 2014).Similarly, automated tools currently used for censorshipof text-based communication at scale are quite primitive and primarily based on keyword-matching(Knockel et al., 2020). However, LLMs could enable significantly more sophisticated surveillance and censorship operations at scale (Feldstein, 2019). Multimodal-LLMs or LLMs combined with speech-to-text technologies could be used for surveilling and censoring other forms of communication as well,e.g. phone calls and video messages (Whittaker, 2019). This may collectively contribute towards theworsening of personal liberties and the heightening of state oppression across the world. Examples havebeen documented already, for instance in calling for violence and silencing of political dissidents (Aziz,2020), and suppression of Palestinian social media accounts (Zahzah, 2021). Sociotechnical research could help better understand the various ways in which surveillance and censor-ship may negatively impact the free thought and integrity of democratic institutions (Richards, 2012).It is also important for the research and academic community to be proactive in this regard and ensurethat their designed models are not misused for surveillance and censorship purposes (Kalluri et al.,2023). The developers of LLM-based technologies, and civil society at large, ought to resist attemptsat creating laws that provide legal covers to such surveillance efforts, with security concerns used asthe ostensible reason (Ellis-Petersen, 2021). A possible technological mitigation against censorship andtechnological lock-in could be the use of perfectly secure steganography (Schroeder de Witt et al., 2023,iMEC), which allows covert communications and information hiding in the outputs of LLMs underinformation-theoretic undetectability.",
  "Warfare and Physical Harm": "The use of AI in warfare is highly alarming and may pose dangers to human safety (Hendrycks et al.,2023). Autonomous drone warfare is being aggressively pursued as a tactic in the current war in Ukraine(Meaker, 2023), and may already have been used on human targets (Hambling, 2023). The use of AI-based facial recognition has been documented in the targeting of Palestinians in Gaza (International,2023).LLMs have already been productized in limited ways for the purposes of warfare planning(Tarantola, 2023). Furthermore, active research is being carried out to develop multimodal-LLMs thatcan act as brains for general-purpose robots (Ahn et al., 2022; 2024). Due to the general-purposenature of such advances, it will likely be cost-effective and practical to adapt them for creating moreadvanced autonomous weapons. Development of autonomous weapons has been extensively criticizedand cautioned against in prior literature (Sauer and Schrnig, 2012; Sharkey, 2016; Scharre, 2016;Roff and Moyes, 2016), and identified as a source of catastrophic risk (Hendrycks et al., 2023). Theseharms have also been cautioned against by technical machine learning researchers in various open let-ters (Future of Life Institute, 2016; Foerster, 2020). There exist voluntary pledges by AI companies tonot weaponize their technologies (Boston Dynamics, 2022), however, past evidence indicates that suchvoluntary pledges can be side-stepped when convenient as they do not pose any legally binding require-ments (Christie, 2018; Biddle, 2024). Hence, LLM-based autonomous weapons may soon pose majorsafety risks, absent international agreements and national legislation prohibiting their development anduse. Besides political challenges, there are technical challenges around monitoring and enforcement.Developing or increasing the availability of autonomous drone weapons and facial recognition targetingmay also increase the risk of targeted mass killings by non-state actors.",
  "Hazardous Biological and Chemical Technologies": "AI systems such as LLMs, chemical LLMs (Skinnider et al., 2021; Moret et al., 2023), and other LLM-based biological design tools might soon facilitate the production of bioweapons, chemical weapons,and other hazardous technologies. In particular, LLMs might enable actors with less expertise to moreeasily synthesize dangerous pathogens, while customized chemical and biological design tools might bemore concerning in terms of expanding the capabilities of sophisticated actors (e.g. states) (Sandbrink,2023). Gopal et al. (2023) and Soice et al. (2023) demonstrated that people with little background coulduse LLMs to help make progress towards developing pathogens such as the 1918 pandemic influenza.However, recent studies suggest that current LLMs are not more helpful than internet search in thisregard (Mouton et al., 2024; Patwardhan et al., 2024). On the other hand, search engines offer morepractical affordances for removing content vs. e.g. needing to retrain an LLM to ensure the content is not influencing future responses. Furthermore, (future) LLM-based technologies could develop strongreasoning capabilities that might help them make novel discoveries (Romera-Paredes et al., 2024) thispotential to make novel discoveries could also transfer to hazardous chemical and biological technologies(Moret et al., 2023), potentially, resulting in technology designs that might be more challenging to guardagainst via supply-chain monitoring. There is a need for more research to characterize the potentialuplift that (current and future) LLM-based technologies may provide, as the current studies involvedsmall sample sizes, only used vanilla LLMs and none of them produced conclusive results (Marcus,2024). The realism of these studies can also be increased by moving them into actual wet laboratoriesand would benefit from the involvement of experts in clinical trials or psychology research. Furthermore, there is a need for ongoing monitoring to track how relevant capabilities are evolving withscale (c.f. .3). Doing so well may require postulating and refining explicit threat models anddefining clear thresholds for action in advance; such work could also involve national security experts.Machine learning researchers should also work with professionals who study terrorism and mass killingsto better understand whether and how LLMs might significantly contribute to such risks; this couldinvolve understanding the social factors underlying such attacks, how the resources currently availableonline are or might be used by such attackers, and what currently limits the rate and severity ofsuch attacks. Finally, we note that most of the contemporary work on biological and chemical risksfrom LLM-based technologies is concentrated on risks from non-state actors; there is also a need tobetter understand how states might misuse LLM-based technologies in this regard (Yassif et al., 2023). 4.2.6Domain-Specific MisusesImprovements in LLMs may exert greater pressure to apply LLMs to various domains, such as healthand education (Eloundou et al., 2023). Crude efforts to use LLMs in such domains, however, mayincur harm and should be discouraged strongly. In particular, it is important to guard against differentways in which LLMs may be misused within any domain. One famous episode of misuse within thehealth sector is a mental health non-profit experimenting LLM-based therapy on its users withouttheir informed consent (Xiang, 2023a). Within the education sector, LLMs may be misused in variousways that might impact student learning; e.g. as cheating accessory by the students or as (low quality)evaluator of students work by the instructors (Cotton et al., 2023). Recent findings in moral psychologyalso suggest that LLMs can generate moral evaluations that people perceive as superior to humanjudgments; these could be misused to create compelling yet harmful moral guidance (Aharoni et al.,2024). Similar risks of misuse may exist in other domains as well. There is a need for research to better understand how LLMs might be misused across different domains.Interviews with domain experts and case studies of early deployments across different sectors may helpin this regard. Domain-specific standards and regulations could be established to prevent these risksfrom materializing (Organization, 2021). One particular sociotechnical challenge is to effectively identifyemerging use cases of LLMs, e.g. through surveys and/or in partnership with LLM deployers, who couldmonitor use patterns. Several factors make this a non-trivial challenge, even for LLM deployers; forinstance, users may disguise queries or user calls may be spread across various LLM deployers in a waythat might mask the real use (Glukhov et al., 2023). 4.2.7Mechanisms for Detecting and Attributing LLM Outputs Are LackingOne overriding challenge in preventing malicious use of LLMs is the difficulty of recognizing outputsfrom LLMs and attributing them correctly to particular systems and/or users. Attribution facilitatesaccountability and enforcement, which may help disincentivize many forms of malicious use. Techniquessuch as watermarking, which embeds detectable patterns in AI-generated content, could be helpful(Kirchenbauer et al., 2023), but their effectiveness is currently unclear (Zhang et al., 2023e; Huanget al., 2023e; Hu et al., 2023d) and more work is needed. Increasing access to LLMs (especially opensource models) might make attribution more challenging (Augenstein et al., 2023; Seger et al., 2023).In particular, even if an open-source model watermarks its outputs, it might be easy to modify it to not do so. This motivates the challenge of attributing outputs to particular models in the absenceof (deliberately injected) watermarks. Machine learning researchers could work with cyber securityexperts and other professionals to understand the gaps that LLMs will create in current attributionpractice for the particular domains discussed above. There is a strong risk that dual-use capabilities of LLMs may be exploited for malicious purposes.LLMs may be misused towards generating targeted misinformation at an unprecedented scale.The coding capabilities of LLMs may be misused by malicious actors to mount cyberattacks withgreater sophistication and at higher frequencies. LLMs are quite effective as content moderationtools; this may mean that LLMs get adopted to enact mass surveillance and censorship. LLMsmay be used to power autonomous weapons, creating a possibility of physical harm from them.Other misuses of LLMs may occur as LLMs are applied to various domains. Active research isneeded to better understand these risks and to create effective mitigation strategies. 145. Can we develop a calibrated understanding of how current and future LLMs could beused to scale up and amplify misinformation campaigns? What level of human expertiseis required to effectively use LLMs to generate targeted misinformation?",
  ". Do advances in capabilities of LLMs cause LLMs to become better at crafting jailbreakingattacks? If so, how can the safety of LLMs from jailbreaking attacks (designed by otherLLMs) be assured?": "153. How effective are LLMs at surveillance and censorship? How may LLMs, on their ownor in combination with other technologies (e.g. speech-to-text softwares), contribute tothe expansion and sophistication of current surveillance operations? How can we limitthe use of LLMs for surveillance? 154. How can the military applications of LLMs, especially LLM-powered autonomousweapons, be regulated?There appears to be a mass consensus within the machinelearning community that LLMs, and other AIs, should not be weaponized; how canthis consensus be leveraged to create legislation pressure to outlaw autonomous weaponsdevelopment across the world? 155. How might current, or future, LLM-based technologies (including chemical LLMs andspecialized biological design tools based on LLMs) be misused in the design of hazardousbiological and chemical technologies? 156. Can we identify how LLMs may get misused across various domains, such as health andeducation? What regulations are required to prevent such misuses? In general, how canwe best understand LLM use cases and identify those with significant misuse potential?",
  "LLM-Systems Can Be Untrustworthy": "A key desideratum for an LLM from a users perspective is trustworthiness, i.e. assurance of reliabilityand consistent performance, and absence of any accidental harm caused by the technology to the user.16 Providing assurance that an LLM-based system will not cause accidental harm remainsa major open challenge. Harms may either occur directly due to the flawed nature of LLMs, e.g.an LLM generating toxic language or behaving inappropriately in some other ways, or may occur dueto improper usage by a user, e.g. automation bias due to a users overreliance on LLM. We overviewseveral challenges in this section that can potentially undermine a users trust in LLMs. The technicalflaws underlying these challenges in many cases appear difficult to address; at least in the immediateterm. Hence, technical research alone may be insufficient to address these challenges, and sociotechnicalinterventions may be required to assure that LLM assistants do not incur accidental harm to the users. 4.3.1Harms of Representation and Other BiasesA pretrained LLM generally has many of the stereotypical biases commonly present in the humansociety (Touvron et al., 2023). This makes it difficult for users to trust that LLMs will work well forthem and not produce unfair or biased responses. Appropriate finetuning can effectively limit the biasdisplayed in LLM outputs in a variety of situations, e.g. when models are explicitly prompted withstereotypes (Wang et al., 2023k), but it does not solve the problem. Even after finetuning, biasesoften resurface when deliberately elicited (Wang et al., 2023k), or under novel scenarios, e.g. in writingreference letters (Wan et al., 2023a), generating synthetic training data (Yu et al., 2023c), screeningresumes (Yin et al., 2024) or when used as LLM-agents (Pan et al., 2024). The biases outputs are oftenmuch more prominent in low-resource languages (Yong et al., 2023) and in dialects used by marginalizedgroups (Hofmann et al., 2024). There is a need for research to develop better and more comprehensivetools for the detection of bias, toxicity (Wen et al., 2023; Wang and Chang, 2022), and other kinds ofinappropriate behaviors. The current tools primarily focus on detecting content that is explicitly toxicand offensive, however, the issue of bias goes beyond commonly studied subjects of biases, such as raceand gender (Hofmann et al., 2024). For instance, depending on the finetuning data, LLMs may alsodevelop a bias towards particular political ideologies (Rutinowski et al., 2023). These biases are stillrelatively poorly understood, and there is a need for more extensive evaluations of current LLMs innovel scenarios to better understand the propensity of LLMs to enact such biases. There is also a needfor a thorough evaluation of how the global south is represented within these models, and what sortof harmful representations of the global south are reinforced by LLMs (Qadri et al., 2023; Jha et al.,2024). 4.3.2Inconsistent Performance across and within DomainsEstimating true capabilities of an LLM is a difficult task (c.f. .3), especially for naive usersunfamiliar with the brittle nature of machine learning technologies. Exaggeration of model capabilitiesby the developers (Lambert, 2023; Blair-Stanek et al., 2023), and issues such as task-contamination(Roberts et al., 2023b), underrepresentation of tasks or domains (Wu et al., 2023a; McCoy et al., 2023),and prompt-sensitivity (Anthropic, 2023d) may cause a user to misestimate the true capabilities of amodel. This lack of reliability can undermine user trust or cause harm if a user bases their decision on",
  "Note that within the machine learning literature, the term trustworthiness carries various other meanings as well. Thescope of our discussion is limited to our definition given here": "incorrect or misleading information provided by an LLM. A famous example of this is the US lawyerwho cited a fake case, hallucinated by ChatGPT, in a legal brief filed in a US court (Merken, 2023).Technical solutions could involve improving the reliability of the LLMs performance (e.g. using retrievalaugmented generation to minimize hallucinations) or providing reliable uncertainty estimates alongsideLLM responses (Fadeeva et al., 2023; Kuhn et al., 2023). However, technical solutions may not bedeveloped quickly, if at all. Hence, there is a need to understand how the reliability of LLMs can beimproved through extrinsic measures. Towards this goal, it will be useful to understand how reliable amedian user perceives an LLM to be, to what extent they are able to reliably forecast on what problemsa particular LLM will fail (Carlini, 2023a) and how this ability to determine the reliability of an LLMsperformance evolves as the user interacts further with the LLM. This may be done through user studiesand interviews with various different types of users. It may be difficult to draw very general conclusions,and more productive in many instances to focus on trustworthiness for particular (e.g. somewhat narrow)use cases or domains. Such research could then provide insights into the efficacy of different extrinsicmeasures that could be taken to help users use the LLMs safely e.g. providing appropriate educationand disclaimers to the user before they begin interacting with an LLM.",
  "Overreliance": "If a user begins to excessively trust an LLM, this may cause them to develop an overreliance on the LLM.Overreliance can result in automation bias (Kupfer et al., 2023), and can cause errors of omission (userchoosing not to verify the validity of a response) and errors of commission (user believing and acting onthe basis of the LLMs response, even if it contradicts their own knowledge) (Skitka et al., 1999). It canbe particularly dangerous in domains where the user may lack relevant expertise to robustly scrutinizethe LLM responses. This is particularly a source of risk for LLMs because LLMs can often generateplausible, yet incorrect or unfaithful, rationalizations of their actions (c.f. .4.10), which canmistakenly cause the user to develop the belief that LLM has the relevant expertise and has provided avalid response. A common tactic used to prevent harms that might arise from overreliance is to includedisclaimers alongside model output that could act as triggers for the user to externally validate themodel response. However, users can get desensitized to such disclaimers over time (OpenAI, 2023b).Better interface design (e.g. using distinct colors for distinct grades of disclaimers) could help avoid, orlimit, such desensitization. User studies could be done with different user types to better understandthe details of how overreliance might manifest, and what mitigation strategies may be most effectiveagainst it. The use of LLMs as coding assistants by developers could be used to study over-reliance ascoding tasks are often well-posed and can have varying levels of complexity as required. There is also a need to better understand the related risks that might arise due to consistent andprolonged usage of LLM by a user in a particular domain. Outsourcing certain types of cognitive tasksto LLMs, e.g. writing tasks, could impair corresponding skills among LLM users. This is particularlya risk for the use of LLMs in education where excessive usage of LLM may cause students to developan unnecessary, and unwanted, dependency on LLMs. Additionally, prior work has shown that humanscan inherit biases from AI systems, and that these negative effects of AI technology do not naturallygo away even when the biased AI systems are removed (Vicente and Matute, 2023; Kidd and Birhane,2023). Further research is required to better understand these risks; how they might materialize, andwhat can be done to mitigate them?",
  "Contextual Privacy Preservation": "As LLMs proliferate through society, this will inevitably result in LLMs simultaneously interactingwith multiple parties (see .6). In such scenarios, assurance of privacy goes significantly beyondassuring that the outputs of the LLMs do not contain personally identifiable information (Nissenbaum,2020), and requires that LLMs do not leak data or information shared by one actor to another needlessly.The notion of privacy is not well-formalized in such contexts currently and there is a need for work ondefining and operationalizing it in an appropriate way. In the absence of a formal definition, a useful goal could be to assure that in any context LLMs do not share information given by one party withsome other party, unless a human would do so as well. Current LLMs do not provide this assuranceand often leak private information provided by one party to another when a human would not do so and common tricks, e.g. prompt-engineering or output filtering do not improve alignment betweenthe behavior of LLMs and human (Mireshghallah et al., 2023). This hints at some of the fundamentallimitations in preserving privacy in multi-agent contexts. Trustworthiness, defined as the assurance that users will not experience accidental harm whenusing an LLM, is critical for LLM alignment and safety. Among other things, trustworthinessrequires ensuring that users can use LLMs safely despite the occasional unreliability of theiroutputs; preventing users from developing an overreliance on LLMs; mitigating harm resultingfrom biased representations of societal groups learned by LLMs; ensuring appropriate behavioracross all contexts; preventing the generation of toxic and offensive content by LLMs; and reliablypreserving user privacy across various contexts.",
  "Socioeconomic Impacts of LLM May Be Highly Disruptive": "The rapid evolution of LLMs brings significant socioeconomic opportunities and challenges, impactingthe workforce, income inequality, education, and global economic development. Many of these challengesare systemic in nature, constituting what economists refer to as general equilibrium effects. Thesechallenges do not arise directly from LLMs causing harm to users but rather from their indirect effectson the socioeconomic equilibrium. For instance, automation may reduce labor demand, leading to wagedeclines and subsequent harm to workers. Consequently, addressing these challenges may necessitatesystem-wide policy interventions.To effectively implement such interventions, a thoroughunderstanding of these challenges and potential mitigation strategies is imperative. Inthis section, we explore some of these challenges and pose pertinent research questions that warrantinvestigation.",
  "Effects on the Workforce": "The effects of integration of LLMs into various industries and workflows on the workforce are likely tobe significant, and pose a great socioeconomic challenge. Since the Industrial Revolution, technologicalprogress has regularly displaced some workers but led to the creation of new jobs as the growing wealthgenerated by better technology led to growing demand for additional goods and services, enabling thedisplaced workers to take up new opportunities (Autor, 2015). As a result, the economy adjusted to the displacement. By and large, workers in advanced countries today are much better off than theywere at the beginning of the Industrial Revolution (Maddison, 2004). However, there are reasons to beconcerned that the ongoing rapid advances in LLM capabilities in particular, and AI in general, mightdisrupt this pattern. Rapid advances in LLMs pose three distinct sets of challenges for workers incomes (Korinek and Stiglitz,2019; Susskind, 2023). First, they are likely to accelerate the rate of job turnover and disruption affecting more workers, including more highly skilled workers, and making the adjustment process forsociety more difficult than what we were used to from prior technological advances. Research couldhelp identify what sectors are most vulnerable to job disruption (Eloundou et al., 2023) and how thedisplaced workers within those sectors can be helped to become productive workers in other growingsectors. Second, although technological progress means that society may produce more wealth overall,there is a risk that the general-purpose nature of LLMs may lead to progress that is biased againstlabor, meaning that the share of that wealth that goes to labor may decline. The overall effect on wagesis then a horse race between these two opposing forces. The fate of blue-collar workers in recent historymay be a harbinger of what lies ahead for white-collar workers: as a result of skill-biased technologicalchange, the wage of the median male blue-collar worker has declined over the past four decades, whenadjusted for inflation, even though the overall economy in the US has tripled (Autor, 2019). Earlyresults suggest a negative impact of LLMs on certain categories of jobs (Hui et al., 2023). Third, iffuture LLMs and robots advance to the point where they can perform virtually all the work tasks, theywould disrupt labor markets more fundamentally: if machines can do workers jobs, wages would fallto machines user cost (Korinek and Juelfs, 2023). This would pose fundamental challenges for labormarkets and income distribution (Korinek, 2023). Aside from their effects on incomes, the rapid advances in LLMs also risk reducing job quality. Au-tomation often tends to increase not only the physical but also the emotional demands put on workers,exposing them to greater surveillance, higher job intensity, and less human agency (Bell, 2022). Thesetrends have already been observed in the increase of digital labour (Casilli and Posada, 2019) and theplatformization of labour more generally (Nyabola, 2023); as LLMs become more widely applied theycould exacerbate these trends. More fundamentally, work is not only the main source of income for the majority of people, but alsothe main activity that occupies our time. As a result, many derive a significant part of our identity, lifesatisfaction, and meaning from work (Susskind, 2023). If people lose their work, they would thus losemuch more than their income, with broad implications for our society and our political system (Belland Korinek, 2023). Two observations might help tackle the resulting challenges (Korinek and Juelfs, 2023). First, if thenon-monetary aspects of work are so important, people could presumably continue to work even whenmachines can automate it they just may not earn much of an income from it. In surveys, a majorityof workers are not very satisfied with their work (Gallup, 2022) and would likely be happier if theyreceived the same income without the need to do a job. Second, things become trickier if one individualsdecision on whether to work affects others well-being, for example, because it affects others scope toform social connections at work or because it has implications for political stability. Economists callsuch effects externalities. When significant externalities are at work, there is often a role for publicpolicy to improve upon the socio-economic equilibrium. Research could work to identify what the bestinterventions could be that might be implemented via appropriate public policy. One such interventioncould be to mandate LLM-developers to conduct impact assessments or risk assessments for whethertheir AI systems augment workers and improve working conditions.However, the general-purposenature of the LLMs may make conducting such risk assessments challenging.",
  "Effects on Inequality": "LLMs could potentially worsen socioeconomic inequalities (Capraro et al., 2023). Effects on inequal-ity are closely linked to the effects of LLMs on workers but ultimately depend on how the fruits oftechnological progress are distributed. There are three important challenges associated with this: First, if the role and compensation of capital rise and the role and compensation of labor decline inan LLM-powered economy, inequality may go up because work is the main source of income for themajority of people. There are signs that this may already have happened because of earlier automationtechnologies in recent decades (Karabarbounis and Neiman, 2014). This has led to calls to steer advancesin AI in a direction that complements workers rather than substituting them (Korinek and Stiglitz,2020; Klinova and Korinek, 2021). This also applies to LLMs. A tangible example of an LLM thatcomplements workers would be a system that advises call service center agents how to best handle calls(Brynjolfsson et al., 2023). An example that substitutes for workers would be a system that replacesworkers altogether. Given the general-purpose nature of LLMs, there is a risk that advances in anygiven use case may initially complement workers but eventually progress to a stage where they candisplace them. However, human decisions can influence the extent to which LLMs complement vs.substitute workers. This includes developers of such systems, business leaders who deploy them, andlawmakers and regulators who shape the economic and societal context in which these systems operate.Economic and sociological research could help understand how LLM-based technologies are likely to getadopted, for instance, by interviewing workers who are early adopters or studying historical examples ofintegrating new technologies into workspaces or industries. There is also a need for concerted efforts tobest educate the business leaders on leveraging the growth benefits of LLMs in a way that is minimallydisruptive to the larger society. Second, the large fixed cost of training cutting-edge LLMs and the network effects involved imply thatthe market for the most advanced LLMs tends towards a natural monopoly structure in which onlyone or a small number of players will be successful, a phenomenon that has been termed algorithmicmonoculture in the literature (Kleinberg and Raghavan, 2021; Bommasani et al., 2022). As a result,LLM developers may amass significant market power.This might result in reduced social welfare,and lead to LLM-providers extracting monopoly rents from their customers (Kleinberg and Raghavan,2021; Jagadeesan et al., 2023). These concerns may become even more important if the producers ofLLMs engage in vertical integration and also participate in the market for downstream applications,for example, if an LLM company also enters the market for legal advice, medical services, etc. In thelimit, the market for leading LLM providers could be the entire economy as the capabilities of LLMsimprove across the board (Korinek and Vipra, 2024). This centralization of power could be even moreproblematic due to the potentially negative impact on the governance of LLMs (c.f. .5.4).The tendency towards centralization could be mitigated by a robust antitrust response and otherpolicy interventions. However, this requires ensuring that economic policymakers are cognizant of thetechnological scenarios that lie ahead and their economic implications. The onus is on the technicalcommunity, which is generally more prescient about the impending technological developments, toeffectively communicate and engage with economic policymakers in this regard. They may do so bywriting educational documents aimed at a non-technical audience (e.g. Bowman, 2023), or releasingpolicy briefs on their research (e.g. Barrett et al., 2023). An alternative solution could be to preventmaterialization of an algorithmic monoculture by increasing access to LLMs and broadening the poolof LLM developers, e.g. via open-source and open-science (Solaiman, 2023). Researchers could helpimprove the understanding of the extent to which current dynamics, in which leading LLM developersremain closed-source, but a number of less capable open-source competitors exist, are likely to lead tomonopoly and identify interventions that could help mitigate them.",
  "Third, as LLMs are becoming more powerful, who has access and who hasnt is becoming a moreand more important question.For example, automated coding tools have been shown to produce": "significant productivity gains, e.g. > 50% in some cases (Peng et al., 2023). Individuals who donthave access whether it is for financial reasons, for reasons of education, because of corporate orgovernmental policies, or for geopolitical reasons might be at a growing disadvantage. In recentdecades social scientists have observed a growing digital divide based on unequal access to digitaltechnologies (Van Dijk, 2020). LLMs risk giving rise to a new intelligence divide based on who hasaccess to the most intelligent LLMs and who hasnt. 4.4.3Economic Challenges for EducationIn a knowledge-based economy, education equips individuals with the human capital that preparesthem to be productive workers. Human capital is often considered as our greatest asset. Yet the rapidemergence of LLMs poses three significant economic challenges for education: First, given the rapid emergence of LLMs as a powerful productivity tool, virtually the entire white-collar workforce may need to learn to use LLMs. Cognitive workers may need to learn how to promptand steer LLMs, how to properly evaluate the output generated by LLMs, and how to incorporate theminto their workflows in order to optimally benefit. This is one of the factors slowing down the rolloutof LLMs in organizations throughout our economy (McAfee et al., 2023). Second, while educators are challenged to teach new tools, LLMs also force them to fundamentallyrethink the way in which they provide education and evaluate students (Mollick and Mollick, 2023;Cotton et al., 2023). LLMs could help unlock teaching methodologies that were not previously viableand improve the overall education quality (Khan, 2023). However, issues such as low technologicalreadiness may hinder the rapid adoption of LLMs in educational contexts (Yan et al., 2023). There isa need for further research to better understand these issues and how to mitigate them. Third, for education, a dark side of the positive productivity effects of LLMs is that they devalue asignificant part of the human capital that workers have accumulated in the past. A growing number ofstudies show that lesser-skilled workers benefit more from LLMs than more highly-skilled workers (Noyand Zhang, 2023; DellAcqua et al., 2023). Another way of putting these results is that the humancapital of highly skilled workers is no longer as valuable as it used to be, which may eventually lead toreduced hiring and training, and lower wages for skilled workers. This could have negative second-ordereffects. Currently, skilled labour presents a technical and moral bottleneck to the deployment of AI formalicious purposes (e.g. tech workers protesting Project Maven Shane and Wakabayashi, 2018), but astheir jobs become (fully or partially) automated or, these workers may have less ability and agency tocontest the uses of LLMs, increasing the hegemony of a narrowing set of powerful actors. 4.4.4Global Economic DevelopmentMany of the themes and challenges that we discussed above come together when analyzing the socio-economic effects on developing countries. The workforce of developing countries may suffer from aretrenchment of outsourcing as many simple cognitive tasks that used to be performed in developingcountries for example, in call centers can be automated with LLMs. This may adversely affectthe economies of the poor countries (Georgieva, 2024). The inequality implications of LLMs also havean international dimension as there may be a growing intelligence divide between advanced countriesthat have access to leading LLMs and poorer countries that do not. Differential productivity effectsmay give rise to terms-of-trade losses for developing countries (Korinek et al., 2022). Similarly, many ofthe profits generated by leading LLMs will accrue to advanced countries. On the other hand, developingcountries may share in the productivity benefit of LLMs with advanced countries if LLMs are accessibleto populations in Global South, both in terms of technology design and in terms of pricing. Surface-level usage statistics seem to indicate that freely available LLMs have indeed seen considerable adoptionamong Global South populations (e.g. India and Brazil are the second and third largest sources of webtraffic to ChatGPT; Similarweb, 2024). However, there may exist extrinsic factors that might limitwider penetration of LLMs among Global South populations, such as lack of internet connectivity and poor tech literacy among the populations. LLMs could potentially be leveraged to address some of theeconomic challenges faced by countries in the Global South. For example, teacher shortage is a criticalissue that negatively impacts quality of education among Global South countries (Unesco, 2022). It isplausible that LLMs could be used to help mitigate this issue. Additionally, in contrast to most other technologies, global adoption of LLMs requires that they areproficient in all world languages. However, even multilingual models perform worse in lower-resourcelanguages (Etxaniz et al., 2023; Shen et al., 2024) and think in English even when fine-tuned forother languages (Wendler et al., 2024). This has severe safety and security implications as models thatmight be aligned in English might not be equally well-aligned in other languages (Deng et al., 2023a;Yong et al., 2023) or may perform worse in other languages (Aroyo et al., 2024; Holtermann et al.,2024). Additionally, LLMs may cost an order of magnitude more to use in some languages (up to 15times in ChatGPTs case; Ahia et al., 2023; Petrov et al., 2023b). Hence, ensuring that LLMs workwell regardless of the language in which they are used is key if we want to ensure they are a tool forreducing global inequalities rather than further exacerbating them. The socioeconomic impacts of LLMs have the potential to be highly disruptive if not effectivelymanaged. LLMs are likely to adversely affect the workforce, exacerbate societal inequality, andintroduce new challenges for the education sector. Furthermore, the implications of LLM-basedautomation on global economic development remain uncertain. These challenges are complexand systemic in nature; there do not exist any simple fixes. To devise solutions, we need todevelop a deep and nuanced understanding of these issues. Answering the following questionsmay help make progress towards this goal. 166. How can we better understand and forecast the disruptive effects of LLMs on job avail-ability and job quality in different sectors? How can displaced workers be helped totransition to other sectors?",
  ". How might LLM-based automation negatively impact the economies of Global Southcountries?": "178. How accessible are LLMs to Global South populations? How can this accessibility beimproved? What measures can be taken by governments to address issues related to lackof internet connectivity, poor tech literacy etc.? 179. How can LLMs be used to help address some of the issues that hinder the economicdevelopment of Global South countries? For example, how can LLMs be used to helpimprove the quality of education available to Global South populations? 180. How to ensure that LLMs support all the worlds languages equally, especially low-resource languages with large number of speakers? How do we ensure that LLMs are atool for a global levelling up rather than further exacerbating economic divides?",
  "LLM Governance Is Lacking": "Governance will play an essential role in the safety and alignment of LLMs, in particular, and AI ingeneral (Bullock et al., 2022a). Governance encompasses not only formal regulations, but also a numberof other mechanisms including norms, soft law, codes of ethics, co-regulation, industry standards, andsector-specific guidelines (Veale et al., 2023); see .Governance could supplement technicalsolutions, e.g. by mandating they be applied as appropriate.It could also substitute for technicalsolutions by preventing unsafe development, deployment, or use of LLMs when there do not existsufficient technical tools for safety.However, serious efforts to govern LLMs remain fairlynascent (e.g. US Executive Order (White House, 2023), or EU AI Act (Council of the EuropeanUnion, 2024)) and efforts to date have mostly been ill-defined and/or voluntary. A number ofgovernance challenges remain that ought to be addressed, or mitigated, to ensure LLMs are beneficialto society and do not contribute to harm to any societal group. Within this section, we divide ourdiscussion of challenges that hinder LLM governance into two parts. We first discuss meta-challengesthat complicate governance such as lack of requisite scientific understanding of LLMs, lack of effectivefast-moving governance institutions, lack of culpability schemes, and corporate power. We complementthis with a discussion of practical challenges focused on the fact that most governance mechanismsare underdeveloped and concrete proposals for governance are lacking. We note that our discussion ongovernance challenges complements other related works (Dafoe, 2018; Anderljung and Carlier, 2021;Shavit et al., 2023; Barnard and Robertson, 2024).",
  "Meta-Challenges Challenges That May Limit Efficacy of LLM Governance": "The governance of generative models (both LLMs and generative image models) is challenging dueto factors such as the rapid productization of the technology, economically disruptive nature of thetechnology (c.f. .4), high potential for misuse (c.f. .2), and the rapidly evolving tech-nological landscape (Bengio et al., 2023). Indeed, several meta-challenges exist that might limit theefficacy of LLM governance. These challenges include a lack of scientific understanding and technicaltools necessary for governing LLMs effectively, the need for agile governance institutions capable ofkeeping pace with technological advancements (which is an antithesis to the traditional slow bureau-cratic nature of the governments), a need to better understand the competitive dynamics between AIcompanies to ensure that competitive pressure does not result in irresponsible AI development, risks ofregulatory capture due to corporate power, a need for international cooperation and consensus, and alack of clarity on accountability for harms caused by LLMs. 4.5.1Lack of Scientific Understanding and Unreliability of Technical Tools ComplicateGovernanceEffective governance of a technology hinges on three key elements: a comprehensive scientific under-standing of the technology to gauge potential risks, dependable auditing tools to evaluate practical",
  "Private legal instrumentsContracts: e.g. Microsoft-OpenAI deal (Bradshaw et al.,2023)Licenses: e.g. RAILS license (Responsible AI License)(RAIL Team, 2024)": "risks, and effective methods to intervene upon and mitigate these risks (Raji, 2021). However, as notedthroughout the agenda, all three are currently underdeveloped for LLMs. Critical aspects of LLMs,such as in-context learning (.1) and reasoning abilities (.4), are poorly understood.Furthermore, existing auditing tools, including evaluation (.3) and interpretation methods(.4), are not reliable enough to provide meaningful assurance of model safety outside of verynarrow contexts. And the primary technical tool for mitigating risks, safety finetuning, lacks robustgeneralization (.2). These issues complicate governance and contribute to a lack of scientific consensus on the nature andseverity of risks associated with LLMs. This lack of technical clarity is one reason Guha et al. (2023) andKapoor et al. (2024) caution against rushing to regulate. Yet, the process of establishing regulationhas tended to be slower than the rate of AI progress. Thus, it is crucial to generate and evaluategovernance proposals despite our presently limited technical understanding. Proposals could explorehow governance approaches could be adapted based on the level of technical understanding of the modelsand the rate at which the field might be progressing; for instance, how should a governing body changerisk thresholds in response to new evidence? Alternatively, proposals could seek to accelerate technicalunderstanding of LLMs and their risks and harms, e.g. by funding research and building governmentcapacity. Moreover, understanding the interconnections among diverse risk types is required to informstrategies for risk governance (Kasirzadeh, 2024). Another proposal is a temporary pause or slowdownon AI and LLM development (Future of Life Institute, 2023), based on the hope that this time couldbe used to develop standardized safety protocols and make progress on safety, e.g. through technicalresearch. However, this proposal has received much criticism. It may adversely affect AI alignment andsafety research and cause overfitting of alignment research to whatever the state-of-the-art model atthe time of the pause might be (Belrose, 2023) or might be infeasible to enforce (Luccioni, 2023). Onthe whole, it remains unclear whether governing bodies should aim to moderate the pace of progressin AI, and if so, what governance mechanisms (e.g. compute governance, see .5.11) could beused for this purpose. 4.5.2Need for Effective, Fast-Moving Governance InstitutionsGiven the rapid pace of advances in LLMs, governance institutions will need to adapt quickly to remaineffective. Governments are currently attempting to regulate AI both through existing institutions, suchas NIST in the USA (White House, 2023), or by novel legislation, such as the EU AI Act in Europe(Council of the European Union, 2024). However, capacity issues might limit the ability of governmentsto design and implement effective policies for governing LLMs quickly (Marchant, 2011). For example,regulatory bodies might struggle to enforce laws due to the lack of resources and relevant expertise,as has been the case with the enforcement of data protection laws (Jelinek and Wiewirowski, 2022).However, despite these shortcomings in practice, government regulation has unique legitimacy andauthority. Hence, it is worth asking how these shortcomings may be addressed. One approach to overcome these limitations could involve creating new institutions through legislativemeasures. For example, Tutt (2017) proposes an FDA-like body for algorithmic systems. There isindeed a growing trend towards the enactment of tailored AI laws, particularly led by the EU andChina. These laws may encompass large models as a specific category, such as the provision of the EUAI Act concerning foundation models or general-purpose AI (GPAI) (Weatherbed, 2023). Investigatingthe impact of such laws on the development and application of LLMs is one clear direction for research;this could involve a comparative study of different national approaches to LLM regulation, and/or ananalysis of the priorities, assumptions, and ideologies driving different AI regulations (Au, 2023). On the other hand, existing regulation could also be fruitfully applied to the governance of LLMs(Gaviria, 2022; Bhatnagar and Gajjar, 2024). This includes copyright and data privacy laws, speechlaws, labor laws, advertising standards, tort, product liability, and more. However, it remains unclearto what extent current regulation is sufficient to mitigate risks.Existing regulators may also not have the necessary capacity, expertise, and regulatory authority to effectively regulate LLMs. Engler(2023) argue for expanding the powers of the existing regulatory bodies in particular grantingthem the power to issue subpoenas for algorithmic investigations and for setting up rules for especiallyimpactful algorithms. Further research here identifying opportunities and gaps would be valuable. Thiscould include analyses of the current technical capacities of regulatory bodies as well as their resourceallocation and knowledge acquisition strategies. Furthermore, the merits and demerits of various formsof public-private partnership proposals for addressing capacity issues of governments could be explored.This may include analysis of both traditional forms of public-private partnerships such as governmentsdirectly contracting the services of a private actor for various tasks, or novel proposals such as regulatorymarkets (Hadfield and Clark, 2023). Center for AI Safety et al. (2024) argue that current proposals forregulatory markets are flawed and do not adequately incentivize private regulators to prioritize safety.In general, while the private regulatory institutions may be more agile than government institutionsand could help standardize regulation across jurisdictions, they may also increase the risk of regulatorycapture, as private partners might have strong industry ties (e.g. financial interests in LLM development,deployment, or use Center for AI Safety et al., 2024); and such a set-up may also bypass governmentprocesses meant to protect against regulatory capture. Generally, there is a need to better understandthe robustness and efficacy of different ways through which governments could outsource auditing andregulation to private actors (Costanza-Chock et al., 2022). There is also a need to identify measures thatcould be taken by governments and other public-interest institutions (e.g. universities, NGOs, policythink tanks) to create an ecosystem that incentivizes top talent to contribute to governance objectives e.g. by working for government regulators, private auditors or other public-interest bodies giventhe large compensation packages being offered by leading AI companies (Mann, 2023). 4.5.3Incentivizing Cooperation and Disincentivizing High-Risk Approaches to AI DevelopmentResponsible and safe AI can be seen as a collective action problem, creating an additional challengefor governance to confront (Askell et al., 2019). More precisely, while most AI practitioners mightprefer a low-risk approach, competitive pressures (e.g. resulting from Safety-Performance Trade-offs,c.f. .7) might cause one or more organizations to take a higher-risk approach (e.g. a movefast and break things approach Blodget, 2009). This could trigger a race to the bottom, progressivelyincreasing competitive pressure on other organizations to adopt higher-risk approaches (Armstronget al., 2016; Hendrycks et al., 2023). For these reasons, it is crucial to disincentivize high-risk approachesto AI development, deployment, and use, and to prevent such dangerous dynamics from materializing.Regulation and treaties could mandate safe development practices. In particular, specialized regulationcould be designed with an explicit focus on the development of frontier AI (Anderljung and Korinek,2024). Outside-the-box auditing covering organizations development and deployment practices couldhelp verify compliance (and identify risks that may have been overlooked) (Casper et al., 2024a).Technical researchers could develop better (game-theoretic) models of race dynamics whose analysismay yield insights about the relative effectiveness of various interventions that could be undertaken todisincentivize a race. Armstrong et al. (2016) provide a preliminary analysis of this kind, however, theiranalysis relies on various simplifying assumptions that may not be reflective of real-world dynamics.In general, technical researchers could work to identify safe development and deployment practices andto identify incentives and technical loopholes that might lead developers not to adopt them. Theycould also work with legal and business experts to evaluate the effectiveness of potential regulations byanticipating how developers might respond. 4.5.4Corporate Power May Impede Effective GovernanceThe increasing power and influence of large corporations may make effective governance difficult. Thereexists a power asymmetry between corporate entities profiting from LLMs and other social groups (e.g.civil society).State-of-the-art LLMs are developed by or in partnership with, some of the worldslargest private tech companies. NVIDIA, which supplies most of the computing hardware for LLMshas recently seen its market capitalization increase to over $2 trillion, roughly a 5x increase in the past year. Lobbying efforts around LLMs have also increased dramatically in the recent past (Saranand Mattoo, 2022; Zakrzewski, 2022; Lindman et al., 2023). This poses a risk of governance protocolsrelated to LLMs becoming excessively favorable to tech companies, potentially leading to regulatorycapture at the cost of the interests of other societal groups, particularly marginalized communities whohave historically been disproportionately affected by poorly designed AI technologies (Reventlow, 2021).Researchers working on LLM policy should aim to document and account for corporate influence, witha focus on identifying ways in which corporate interests might diverge from the public interest. Ata more meta-level, it may be helpful to consider how corporate structures could be designed so thatthe interests of corporations extend beyond mere maximization of profits, and better align with thelarger interests of the society. Researchers could design novel proposals in this regard, or evaluate therobustness of the existing proposals (e.g. Anthropics Long Term Benefit Trust Anthropic, 2023f). Thisis of particular relevance given the recent power struggle between OpenAI board members, founders,and investors (Roose, 2023; Reich, 2023). In addition to their ability to influence governance through lobbying efforts, LLM developers mayalso directly influence public opinion through the design choices they make.For instance, how anLLM expresses itself and responds to queries, especially those with a political angle to them, canhave a significant impact (Jakesch et al., 2023; Santurkar et al., 2023).This direct influence thattechnological companies can exert on public opinion has been recognized in prior work. Lessig (2006)discuss the power of technology or code itself as a governance mechanism, noting that it generallylacks democratic oversight and that much software used globally is built by US-based companies, oftenwithout regard to the local laws of its overseas markets (Snchez-Monedero et al., 2020).To helpimprove democratic oversight, machine learning researchers could consider how such influence might bemeasured, detected, and counter-acted (OCallaghan et al., 2015). There is also a need for collaborationamong machine learning researchers, sociotechnical researchers, and civil servants to determine how tobest protect government institutions such as democratic processes from such influence, e.g. to helpestablish standards defining the limits of legitimate influence. While the academic community can help counterbalance corporate influence over governance, the largemajority of AI researchers receive or have received funding from big tech companies (Abdalla andAbdalla, 2021). This dependency on industrial funding may grow further due to the exorbitant costsof conducting LLM research. More research is needed to identify the influence of corporate power onacademia, policy, and research. It is also important to consider governance proposals that may helpthe academic community preserve its independence, and generally increase the agency of academicsto contribute effectively to safety and alignment research (Raji et al., 2023). The lack of academicconsensus around AI risks and mitigations also hinders the academic communitys ability to play sucha role, rendering us unable to speak with one voice to provide clear guidance to policymakers or otherelements of civil society.",
  "LLMs Require International Governance": "International governance will be critical for tackling factors such as competitive dynamics between AIcompanies. However, despite the fact that data-driven AI is a global and cross-border phenomenon,laws and regulators are predominantly national. As a result, issues of jurisdiction, applicable law, andregulatory arbitrage, which already complicate effective regulation of data privacy laws, will almostcertainly affect AI and LLM regulation. A popular proposal in the literature is the establishment ofinternational institutions (Ho et al., 2023; Trager et al., 2023; Maas and Villalobos, 2023). For example,Trager et al. (2023) propose the creation of an inter-state International AI Organisation to certify com-pliance by states to international oversight standards while supporting member states in meeting thesestandards through domestic regulatory capacities. The UK has promoted the development of globalAI safety Institutes alongside a pledge to put safeguards on models posing systemic risks (of Partici-pating Countries, 2023). As of early 2024, AI safety institutes are being established in the UK, US,",
  "Japan, and Singapore at the national level. These organizations could be a productive site of moreinternational collaboration on AI governance": "One other possible route to global harmonization of AI regulation is via the global diffusion of domesticregulation, such as the EU AI Act (Council of the European Union, 2024). The EU aspires to positionitself as a global gold standard (building on the example of GDPR), by requiring any company sellinginto the EU Single Market to follow its rules.Through the Brussels effect, it may be cheaper forcompanies that comply with EU requirements to comply with the same requirements even in otherjurisdictions (Siegmann and Anderljung, 2022). However, conflict historically exists between the EUmodel of human rights-centric prescriptive risk-based regulation and the USs laissez-faire regulationof Silicon Valley and poor federal privacy protection (Solove and Schwartz, 2022). This may hold backany international harmonization of AI regulation (Kaminski, 2023; Smuha, 2021). However, there issome initial evidence that the US government may take a proactive approach to regulating AI (WhiteHouse, 2023), suggesting that this conflict may not adversely impact the international governance ofAI. Another important development here may be the finalization of the Council of Europe Convention on AI(Council of Europe, 2023) which like the European Convention on Human Rights and the CybercrimeConvention can be joined by non-European states, and so is potentially a global treaty. Alongsidethese developments, China has been promoting cooperation on international AI governance throughinitiatives such as the Global AI Governance Initiative (Ministry of Foreign Affairs, Peoples Republicof China, 2023). While this initiative has elements in common with both the EU approach and UKand US approaches, there exists a risk that multiple separate or overlapping international AI gover-nance forums will emerge, complicating efforts to achieve global consensus on standards and safety.International governance may also be impeded by a trust deficit and an arms race between nations particularly China and the U.S. (Meacham, 2023). Overcoming parochial concerns and assuring thatgovernments can credibly cooperate on critical issues such as limiting the use of AI in weapons(see .2.4) is perhaps the grand challenge in international governance of AI. Dialogue betweenthe scientific communities could help pave the way for such cooperation (FAR AI, 2024; Maas andVillalobos, 2023). Specifically for the governance of AI in military applications, multi-country militaryalliances like NATO could play a critical role in ensuring responsible, and ideally highly restricted, useof AI in weapons (Stanley-Lockman and Trabucco, 2022). 4.5.6Culpability Schemes Are Needed for LLM-Based Systems Especially LLM-AgentsProviding assurances about a systems safety and intended behavior inherently requires establishingclear accountability explicitly assigning responsibility and culpability in cases where the system actsin undesirable or unintended ways. It is currently unclear who ought to be held responsible when a LLMsystem causes harm to its user or other humans. Users may deliberately misuse LLMs (see .2),but preventing such misuse may be intractable without interventions at the development or deploymentstage (Anderljung and Hazell, 2023). Blumenthal and Hawley (2023) propose that companies be heldliable for harms caused by their models, however, Kapoor et al. (2024) observe that developers whoopen source their model would struggle to prevent misuse and attendant liability. Developers are oftenextremely well-resourced and can retain privileged access to their systems. Hence, they are technicallybest equipped to detect and mitigate safety issues, but it is unclear to what extent they are incentivizedto disclose those issues, especially if disclosing them could harm their business interests. Importantly,it is not necessary to hold only a single actor (user, deployer or developer) responsible: different actorscould be held responsible to varying degrees (Wex, 2023). Gaps in how to assign responsibility will likely become more acute as systems become increasinglyagentic and autonomous (Buiten et al., 2023) (see .5), which can amplify the harm that theycan cause (Chan et al., 2023a). Thus, there is a need for anticipatory governance to preemptively addressthe risks posed by LLMs, for example, by establishing regulatory criteria to mediate the deployment of LLM-agents. This will be helped by developing frameworks to monitor and evaluate deployed LLM-agents and designing mechanisms for ascertaining accountability in case of failures (Kampik et al., 2022;Chan et al., 2024). These questions are considered in greater detail in the recently released agenda onagent governance by Shavit et al. (2023). Once deployed, LLM-agents will interact among themselves and with humans creating an additionalsource of risks (see .6). Normative infrastructure (e.g. bureacracies Bullock et al., 2022b) whichgoverns interactions between humans e.g. accounting of responsibility and blame can break downwith the introduction of algorithmic or machine-based decision-making. For example, high-frequencyalgorithmic traders are believed to have contributed to a number of flash crashes in stock markets (Teeand Ting, 2019), e.g. the flash crash of 2010 (CFTC and SEC, 2010) and the 2014 US Treasury marketflash crash (Levine et al., 2017). Similar to algorithmic traders, LLM-agents will likely possess novelcapabilities and affordances, such as higher processing and action speed, the capability to ingest largeamounts of text rapidly, etc. that might disrupt normative infrastructures in undesirable ways. Furtherwork is required to understand better the governance challenges posed by LLM-agents (Kolt, 2024),especially in multi-agent scenarios where they may interact and influence each other (Hammond et al.,2024). A better understanding of these challenges may help inform what technical and governance toolsare necessary for effective governance of LLM-agents.",
  "Practical Challenges Governance Mechanisms for LLMs Are Underdeveloped": "In addition to the meta-challenges discussed above, a key challenge in exercising LLM governance isa lack of concrete, and complete, governance proposals (Guha et al., 2023). That is, most governancemechanisms for LLMs are underdeveloped and there is a high level of uncertainty around what thebest governance interventions will be. To elaborate, governance can operate at different points in theLLM lifecycle, from development through to deployment and use, as well as on different substratessuch as data (Jernite et al., 2022; Chan et al., 2022), compute (Hwang, 2018; Sastry et al., 2024), andenergy (Monserrate, 2022). Governance interventions earlier in the lifecycle can create choke-pointsfurther on. For instance, if some development practice is banned so that some variety of LLM system isnever developed, such a system could not be deployed or used (Anderljung and Hazell, 2023). On theother hand, interventions later in the lifecycle can be more targeted. This carries both advantages anddisadvantages: more targeted interventions help limit negative side-effects of governance interventions(e.g. creating barriers to beneficial uses), but also run the risk of missing some pathways to harm.Fortunately, the different types of governance mechanisms we discuss are not mutually exclusive, andcan likely be effectively combined to achieve better outcomes than using a single mechanism on itsown. However, currently, almost all the governance mechanisms are underdeveloped and lack concreteproposals for operationalizing them. We provide some discussion in this regard below and highlight therelevant challenges.",
  "Use-Based Governance May Be Insufficient": "One approach to governing LLMs is to set rules that limit how they can be used. Indeed, Hacker et al.(2023) argue that governance should focus on users and deployers, with a few key exceptions. Undersuch an approach, users could be held accountable for whatever harms their use of a system causes,and consumer protection law could be used to hold developers or deployers accountable if they failto protect users. Particularly harmful use cases could be proscribed by designing explicit regulations.For example, explicit laws are being passed by multiple countries that criminalize the generation ofharmful deepfake images by the use of generative image models (U.S. Congress, 2023; UK Parliament,2023). However, enforcement of such regulations may be very difficult as a skilled malicious user couldeasily hide their identity by using anonymization schemes (Eurojust and Europol, 2019). Hence, itis questionable to what extent such regulations will be an effective deterrent for a highly skilled anddetermined malicious user. It is also unclear to what extent such an approach would be able to proactively identify misuses oftechnology, instead of acting retroactively once the harm has been done; as was observed to be thecase for deepfakes (Burga, 2024). Currently, the EU AI Act governs use cases of AI systems, includingLLMs, using a risk-based approach to classify different use cases and determine which rules apply(Council of the European Union, 2024). However, several questions arise with such an approach: Whatexisting regulations or, more generally, governance institutions are relevant, and how shouldthey be applied? How can we identify problematic new use cases and ensure they are addressed (c.f..2)? Regulators might need fast-acting powers to intervene when new problematic uses arediscovered.More generally, an important challenge for such an approach is how to address issuessurrounding misuse, such as accountability, discussed in .2).We note that internationalagreements might also be required, as many forms of misuse (e.g. military use of LLM or censorship)are likely to be perpetrated by governments. Use-based governance may also be limited in ways it canprevent instances of self-harm (Xiang, 2023b).",
  "Deployment Governance Lacks Adequate Regulation": "Deployment methodology significantly impacts the potential risks associated with an LLM. Developingregulations, both soft and hard, to govern deployment can not only be effective but may also be necessaryto assure safe and beneficial deployment of LLM-based systems. Deployment governance may includepre-deployment governance and lifetime governance. The pre-deployment governance is concerned with regulating how a model gets deployed. In the pre-deployment governance, a basic challenge is to evaluate the trade-offs of different forms of deployment(Solaiman, 2023). The common forms of deployment include making the model available to downloador making the model available via some limited form of API access (including web-interfaces).17 Inparticular, there is an ongoing debate around downloadable or so-called open-source model deploy-ments.18 Shevlane (2022) argue for the benefits of structured access, i.e. controlling how users interactwith a system, which API deployment enables. Seger et al. (2023) argue that LLMs may soon be toodangerous to open-source (at least initially). On the other hand, Kapoor et al. (2024) argue that gov-ernments should fund research into the marginal risk from open source models (over currently availabletools, such as web search), and consider further interventions only once risks are more certain. Theyalso express concern that many forms of regulation might impose infeasible compliance burdens on de-velopers of open-source models. Advancing this debate is critically important, given the irreversibilityof open-sourcing models. The risks of deployment (even for a closed-source model) are also mediated by the intended use-case(see .5.7), who the model is being made available to and especially by the level of autonomyafforded to the model (Weidinger et al., 2023a).The models that are made available to youngeraudiences (Fowler, 2023), or deployed autonomously (LLM-agents) may require similarly higher levelsof assurance (Chan et al., 2023a; Shavit et al., 2023). Regardless of how models are deployed, deployerscould take some responsibility for ensuring LLMs are trustworthy and do not cause harm. For instance,they might be made responsible for communicating information about limitations from developers tousers, or even be required to perform some evaluations or collect other information about a systembefore agreeing to deploy it. Third-party licensing, or registrations, could be mandated to ensure thatunsafe technologies do not get deployed or become widely available. While thoughtful development can reduce the risks associated with an LLM, it may not eliminate them.Lifetime governance is required to ensure that throughout their deployment, LLM-based systems remainsafe. This includes assuring that the systems are monitored in a robust way and clear action plans are 17In practice, API deployers are often going to be large-scale compute providers; see .5.11 for more on computegovernance.18There is also controversy and confusion around the term open-source as applied to the practice of only releasing modelweights (Widder et al., 2023; Seger et al., 2023; Solaiman, 2023). in place to deal with cases when a novel failure mode, or a new source of risk, is discovered (Chan et al.,2024). One challenge requiring technical research in this regard is: how to re-establish assurance aftersystem updates to the LLM, or some other component of an LLM-based system, during the systemslifetime. Ideally, the cost of assurance in such a case could be reduced relative to assuring a brand-newsystem. Another aspect of the challenge is how to deal with downstream systems using an LLM as adependency. Deployers could help ensure users and developers share an awareness of how such updatesmight lead to new safety risks. 4.5.9Development Governance Might Be Particularly Challenging to Codify and EnforceMost technical work in AI safety and alignment is focused on development methods. While this work iscurrently not mature enough to offer reliable recipes for safe and aligned LLMs, it can already contributeto best practices that could be enshrined e.g. as standards or through regulation (UK Government,2023). Such practices could take the form of rules around the sorts of data or algorithms to use (or notuse), as well as rules around evaluations or other assurance practices to be applied before deployment(Schuett et al., 2023). These rules could be enforced on a per-project basis through mechanisms similarto those in White House (2023), provided governments are aware of development projects. Others haveproposed licensing developers (Smith, 2023; Anderljung and Korinek, 2024), although critics argue thismight lead to regulatory capture, and stifle innovation and open-source development (Thierer, 2023;Howard, 2023).Besides development methods, best practices for development should also considermeta-practices such as processes governing internal decision-making and practices around disclosureof development activities (Weidinger et al., 2023a; Ojewale et al., 2024; Casper et al., 2024a). To theextent that safety and alignment can be guaranteed through following best practices in development,mandating such practices could be an appealing approach to governance. However, the efficacy of development governance would likely depend on achieving a high level of buy-in from most if not all leading developers (see .5.3). Moreover, given the falling costof compute, more and more developers (including those not in the lead) may need to be governed,creating a growing enforcement challenge. As most of the knowledge about sound developmental prac-tices for the responsible development of LLMs is currently locked within AI companies, regulators, andstandard-setting organizations may be highly dependent on LLM developers sharing this knowledge tocreate high-quality standards. Furthermore, a lack of buy-in from developers may result in regulatoryflight (i.e. developers moving their operations to other jurisdictions with less regulatory pressure) ordevelopers circumventing the prescribed external standards, e.g. by hiding parts of developmental de-tails that may be misaligned with the prescribed standards. Such evasions may be particularly hardto detect and prevent, given the current lack of technical tools for determining whether inappropriatedevelopment activities are occurring (Shavit, 2023). At the same time, regulatory flight may be less ofa concern for large markets like the US or the EU. An alternative to externally imposed regulations could be to prompt companies to propose their owndevelopmental standards that they could then be beholden to, once ratified by an external party (e.g.a government regulator). However, it is important to not rely on voluntary compliance alone and toensure that such standards are appropriately codified and made legally binding ( higeartaigh et al.,2023). An example of such developmental standards are the responsible scaling policies publishedby various AI companies (Anthropic, 2023e; OpenAI, 2023d; Google DeepMind, 2023), however, thesepolicies are completely voluntary and due to the lack of third-party analysis of these policies, it isunclear to what extent these policies embody desirable standards of safety. 4.5.10LLMs Pose Additional Challenges for Data GovernanceData is a basic ingredient for LLM development. This makes data governance a promising vehicleto govern and regulate LLMs. Data could serve as a choke-point and help prevent the developmentof unsafe LLM systems; for instance, training on certain kinds of data (e.g. biological data) could beprohibited or regulated stringently. In the pre-LLM age, the central focus of data governance has been the protection of an individuals right to privacy (Solove, 2022). LLMs add an additional dimensionto this as LLMs can memorize and leak personally identifiable information (PII) (Tirumala et al.,2022; Nasr et al., 2023). However, it is also important that the scope of data governance is expanded toconsider other data rights issues that have to come to the fore due to the development of LLMs (Robertsand Montoya, 2022). One major objective for data governance is establishing, and defending, the rightsof data creators (e.g. writers) and the rights of data workers (e.g. workers hired to generate data forLLM training). A popular proposal for this is the establishment of accountable organizations, such asdata trusts (Jernite et al., 2022; Chan et al., 2022), to be custodians of any data submitted to them.However, implementing such a solution requires overcoming several technical and social challenges. Onthe technical side, the key problems are establishing provenance of the data already present on theinternet (Lee et al., 2023b), verifying that a particular model was trained on the dataset it is claimedto have been trained on (Choi et al., 2023b; Garg et al., 2023), and establishing relative value ofdifferent data points present within a dataset (Guu et al., 2023). On the social side, implementing suchsolutions would require strong political will, extensive international cooperation, and sufficient fundingto implement the technical infrastructure needed for data trusts (Chan et al., 2022). Furthermore, it is unclear as to who should own the data created by an LLM e.g. the developer, theuser, or no one (Henderson et al., 2023b)? This question is coupled with the questions of responsibilityand profitability: who is responsible if an LLM generates output that is harmful or unsafe in other ways(Henderson et al., 2023c) (also see .5.6)? How does this responsibility change as we move fromchat-based models to agents (Schwartz and Rogers, 2022)? This is arguably a fundamental question inregards to data economy and may have long-reaching repercussions in an AI-based creative economy(Knibbs, 2023).",
  "Robustness of Compute Governance is Unclear": "Compute plays a critical role in the development of LLMs (Sevilla et al., 2022), with compute costsfor development rising into the hundreds of millions (Knight, 2023) and likely soon billions of dollars.Compute governance may provide one of the most promising levers for governing bodies to modulatethe rate of progress within the technical AI field (Whittlestone and Clark, 2021; Sastry et al., 2024).This may be particularly important for the safety risks associated with advanced capabilities whichcompute-heavy frontier models are likely to obtain first (Pilz et al., 2023). Relative to other governancemechanisms that governments could use to regulate AI and LLM development, compute governancealso has the advantage of potentially easier compliance verification (Brundage et al., 2020; Baker, 2023),especially given the major intermediary role of compute providers (Heim et al., 2024). However, furtherwork is needed to understand and refine existing proposals for compute governance (Shavit, 2023; Choiet al., 2023b; Egan and Heim, 2023; Sastry et al., 2024; Heim et al., 2024), in addition to managing riskssuch as privacy and concentration of power (Sastry et al., 2024). Technical researchers could collaboratewith hardware and supply-chain experts to stress-test proposals, e.g. by identifying potential loopholesby which projects might escape scrutiny (such as distributed training on consumer hardware Douillardet al., 2023), or otherwise thwart effective oversight (such as by disguising computations). Computegovernance proposals could also be strengthened by developing a better understanding of the interplaybetween hardware and software (Mince et al., 2024). Future research on compute governance couldexplore potential developments that may affect the effectiveness of compute governance, such as changesin the structure of the compute-providing industry (Anderljung and Carlier, 2021) or the diffusion ofAI capabilities (Pilz et al., 2023). Compute governance could also be leveraged by governments toenhance the ability of the independent scientific and academic community. In addition to any directbenefits in terms of improved understanding, and auditing, of LLMs, this may help mediate the effectsof corporate power (see .5.4). Effective governance of LLMs is critical for ensuring that LLMs prove a beneficial addition tosocieties. However, efforts to govern LLMs, and related AI technologies, remain nascent andill-formed. The governance of LLMs is made challenging by various meta-challenges rangingfrom lack of scientific understanding and technical tools required for governance to the risksof regulatory capture by corporations. From a more practical lens, concrete and comprehensiveproposals to govern LLMs remain absent and, unfortunately, the various governance mechanisms(e.g. deployment governance, development governance, compute governance, data governance)are not adequately developed yet. 181. How should governance approaches change depending on how rapidly the capabilities ofmodels are advancing, the rate at which they are being productized (and hence prolifer-ating throughout society), and the degree to which we lack technical understanding of aparticular system, or LLMs in general?",
  "Limitations": "This agenda is the most expansive discussion on the challenges in assuring the safety and alignment ofLLM-based systems to date. However, despite this, we assert that this agenda is not exhaus-tive and that there exist important challenges, both known and unknown, in assuring thesafety and alignment of LLM-based systems that are not cataloged in this work. We have attempted to future-proof our work by trying to list challenges that might arise due to LLMsbecoming more performant due to scaling or modifications of the training process, however, due to theuncertain nature of the LLM development landscape, it is possible that important challenges may havebeen omitted. In particular, we have primarily focused on challenges in the LLM safety and alignmentthat are imminent, and relatively undisputed, and hence, we do not cover speculative challenges; seeHendrycks et al. (2023) and Critch and Krueger (2020) for discussion on such challenges. Another key limitation of this work is the exclusive focus on the safety and alignment of LLM-basedsystems. The choice to focus on the safety and alignment of LLMs was made due to the surging researchinterest in LLMs, their rapid productization, and their central position in the age of foundation models.We assert that the safety of other deep learning-based systems (e.g. generative models for vision,generative models for biology, recommender systems, and learning-based embodied agents) is also highlyimportant and we call on the wider community to organize similar efforts to catalog challenges involvedin the safety of such non-LLM-based systems. Relatedly, due to the limited scope of our work, wehave intentionally omitted many important research directions pertaining to the safe development anddeployment of aligned AI-based systems, such as improving the general understanding of deep learning(Arora et al., 2020), understanding critical aspects of agency (agent foundations Soares and Fallenstein,2014), and developing AI systems whose safety can be proved or verified (Brundage et al., 2020; Tegmarkand Omohundro, 2023), cooperative AI (Dafoe et al., 2020) etc. Another dimension of our limited coverage is that our major focus is on technical challenges 13 outof 18 challenges we identify are fully technical in nature. Our discussion of sociotechnical challengesis further limited in the sense that it is narrowly focused on challenges that are directly relevantto LLM-based systems and is biased towards aspects of these challenges that could potentially beaddressed via research. We have also discussed sociotechnical challenges in a dedicated section as weprioritized modularity to make the work easier to navigate for a wide audience. However, this viewoversimplifies the interconnectedness between the technical challenges and the broader sociotechnicalchallenges involved. An alternative treatment focused on highlighting this interconnected nature ofsafety challenges would see sociotechnical issues spread throughout every aspect of work on LLMs.Additionally, while we have made our best effort to avoid any geographic bias in some sections,particularly .5, the discussion is biased towards few geographies, specifically, US, Europe andChina. The nature of the challenges posed in this work may change over time.Some challenges may getsidestepped or solved as a side-effect of advances focused on improving LLM performance (e.g. scal-ing). For some other challenges, their form may evolve, causing the identified corresponding researchdirections to become outdated. Most importantly, advances in LLM development may uncover novelchallenges or make some of the existing challenges much more critical to address.",
  "Prior Work": "This work comes on the back of several works focused on highlighting harms, risks, and various othersocietal challenges posed by LLMs, and other advanced AI systems they may give way to (Bengio et al.,2023). These risks have been recognized by various governing bodies around the world, e.g. UnitedStates government (hou, 2023), United Kingdom government (Office, 2023), and the United Nations (Nichols, 2023). Among scholarly work, Weidinger et al. (2022) and Shelby et al. (2022) review andtaxonomize various harms and risks posed by LLMs and other AI systems. Shevlane et al. (2023)propose evaluating LLMs for dangerous capabilities that may pose extreme risks. To improve riskassessment, Weidinger et al. (2023a) propose a framework for sociotechnical evaluation of LLMs andother generative systems.In a similar vein, Solaiman et al. (2023) call for evaluating AI systems,including LLMs, for social impact. Other works examine the possible societal impacts of LLMs Eloundou et al. (2023) review possible disruptions to job market that might be caused by LLMs andBrundage et al. (2018) highlight ways in which AI systems may be misused by malicious actors. Thereadditionally exist works that focus on discussing societal-scale harms that may occur if a misalignedcompetent AI system is allowed to act in an unsafe way (Critch and Krueger, 2020; Hendrycks et al.,2023; Critch and Russell, 2023). Our work is complementary to all the aforementioned work as wefocus on listing technical and sociotechnical challenges that need to be addressed to overcome thesechallenges. Several prior works have attempted to identify critical open problems and outline research directions,for the development of safe and aligned AI systems. Kenton et al. (2021) is perhaps the closest workto ours in scope but contains a much narrower discussion regarding the safety of LLM-based systems.Similarly, there exist public agendas by leading LLM companies that outline their approach to safety andalignment of LLM-based systems (Leike et al., 2022; Anthropic, 2023a). However, these agendas lackdiversity and are primarily focused on the research directions being championed by the correspondingcompany. In contrast, our work boasts a diverse academic authors lineup and platforms diverse researchdirections. Amodei et al. (2016) and Hendrycks et al. (2021a) share a similar goal to ours of highlightingimportant challenges that require to be addressed for safe AI; however, they lack explicit focus onLLMs. Other similar efforts include Dafoe et al. (2020) and Ecoffet et al. (2020), which respectivelyconsider alignment and safety of multi-agent and open-ended systems. Other works have argued forspecific approaches for the development of safe AI systems; Leike et al. (2018) argue for scalable rewardmodeling to align advanced AI systems, Tegmark and Omohundro (2023) argue for distilling learnedlogic of AI systems into code which can be formally verified, and Brundage et al. (2020) call for designinginstitutional, software and hardware infrastructure to support verifiability of the claims made aboutAI systems. Dafoe (2018) and Shavit et al. (2023) are agendas focused on governance aspects of AIsystems. There also exist several other agendas focused on the safety of AI-based systems with varyinglevels of relevance to the safety and alignment of LLM-based systems (Russell et al., 2015; Soares andFallenstein, 2014; Henderson et al., 2018; Dinan et al., 2021; Gruetzemacher et al., 2021). Also relatedto our work are studies such as Gabriel (2020) and Prabhakaran et al. (2022a), which consider thequestion of what the alignment target ought to be for general-purpose AI systems like LLMs. Due to the focus on LLMs, our work is also related to other agendas and surveys on LLMs.Thenotable agendas include Kaddour et al. (2023) and Huyen (2023), which review challenges in LLMresearch and applications of LLMs in general, without any explicit focus on safety or alignment. Amongsurveys, Bowman (2023) is a short survey that provides an opinionated review of key facts aboutLLMs development. Zhao et al. (2023) comprehensively review the various facets of LLM developmentand their utilization, including techniques used to promote safety and alignment. Ji et al. (2023b)provide a review of alignment techniques in the context of foundation models. There also exist severalsurveys on specific aspects of LLMs that are covered in this work; Dong et al. (2022) survey theliterature on in-context learning, Huang and Chang (2022) provide extensive discussion on reasoningcapabilities of LLMs, Mozes et al. (2023) review security related issues of LLMs, and Casper et al.(2023a) survey limitations of reinforcement learning from human feedback for safety finetuning of LLMsand the associated open problems.",
  "In the aftermath of the unexpected success of LLMs, there has been a growing sense that impactfulmachine learning and natural language processing research requires tremendous resources and thus is no": "longer viable for academic researchers. This work is partially inspired as a rebuttal to that perspectiveand posits that alignment and safety are ripe fields for contributions by academic researchers. Otherrelated efforts include Saphra et al. (2023) and Ignat et al. (2023). Saphra et al. use historical analogiesto argue that current disparities between academic and industrial labs regarding the scale of resourcesare temporary and argue that evaluations and data are still the primary bottlenecks.Ignat et al.",
  "Acknowledgements": "We, in particular UA, would like to thank Robert Kirk, Lorenz Kuhn, Nicholas Carlini, KatherineLee, Alexander Rush, Geoffery Irving, Greg Yang, Sam Bowman, Mikita Balesni and Tim Rudner fordiscussions and feedback on the idea and initial outline(s) of this work. We would additionally like tothank Will Merrill, Spencer Frei, Kawin Ethayarajh, Kayo Yin, Roger Grosse, Victor Veitch, Sylvia Lu,Bilal Chughtai, Bruno Kacper Mlodozeniec, Dmitrii Krasheninnikov, Matthew-Farrugia Roberts, BenBucknall, Dan Hendrycks, Neel Nanda, Vinodkumar Prabhakaran, Seth Lazar, Percy Liang, JustinBullock, Sara Hooker and many others for providing helpful feedback on the draft. We also thank ElioArturo Farina for his in-kind support with Latex formatting and typesetting. All errors and omissionsare our own.",
  "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version ofBERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019": "Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, AleksandraFaust, Clement Farabet, and Shane Legg. Levels of AGI: Operationalizing Progress on the Path toAGI. arXiv preprint arXiv:2311.02462, 2023. Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, MyraCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm fromlanguage models. arXiv preprint arXiv:2112.04359, 2021. Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generativemodels. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 17471764,2022.",
  "Rob Ashmore, Radu Calinescu, and Colin Paterson. Assuring the machine learning lifecycle: Desiderata,methods, and challenges. ACM Computing Surveys (CSUR), 54(5):139, 2021": "Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Buck-nall, Andreas Haupt, Kevin Wei, Jrmy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Kr-ishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, MaxTegmark, ..., and Dylan Hadfield-Menell. Black-Box Access is Insufficient for Rigorous AI Audits,2024a.",
  "Dan Hendrycks. Introduction to AI Safety, Ethics, and Society. 2023": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models. Transactionson Machine Learning Research, 2022a. ISSN 2835-8856. URL Survey Certification. Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S.Bernstein. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36thAnnual ACM Symposium on User Interface Software and Technology, Uist 23, New York, NY, USA,aug 2023a. Association for Computing Machinery. ISBN 9798400701320. arXiv:2304.03442 [cs].",
  "Allen Newell and Herbert A. Simon.Computer science as empirical inquiry: symbols and search.Commun. ACM, 19(3):113126, 1976. URL": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. URL Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, ScottGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXivpreprint arXiv:2001.08361, 2020. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu,Chandra Bhagavatula, and Yejin Choi. The Unlocking Spell on Base LLMs: Rethinking Alignmentvia In-Context Learning. arXiv preprint arXiv:2312.01552, 2023a.",
  "Raphal Millire. The Alignment Problem in Context. arXiv preprint arXiv:2311.02147, 2023": "Cem Anil, Esin Durmus, Mrinank Sharma, Joe Benton, Sandipan Kundu, Joshua Batson, Nina Rimsky,Meg Tong, Jesse Mu, Daniel Ford, Francesco Mosconi, Rajashree Agrawal, Rylan Schaeffer, NaomiBashkansky, Samuel Svenningsen, Mike Lambert, Ansh Radhakrishnan, Carson Denison, Evan JHubinger, Yuntao Bai, Trenton Bricken, Timothy Maxwell, Nicholas Schiefer, Jamie Sully, AlexTamkin, Tamera Lanham, Karina Nguyen, Tomasz Korbak, Jared Kaplan, Deep Ganguli, Samuel R.Bowman, Ethan Perez, Roger Grosse, and David Duvenaud. Many-shot Jailbreaking. Preprint, 2024. Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, ZaheerAbbas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprintarXiv:2404.11018, 2024.",
  "Eric J. Bigelow, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, and Tomer D. Ullman.In-Context Learning Dynamics with Random Binary Sequences, 2023": "Sivaramakrishnan Swaminathan, Antoine Dedieu, Rajkumar Vasudeva Raju, Murray Shanahan, MiguelLazaro-Gredilla, and Dileep George. Schema-learning and rebinding as mechanisms of in-contextlearning and emergence. arXiv preprint arXiv:2307.01201, 2023. Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-WeiChang, Aram Galstyan, and Rahul Gupta.Flirt: Feedback loop in-context red teaming.arXivpreprint arXiv:2308.04265, 2023.",
  "Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi.Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022": "Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi,Najoung Kim, and He He. Testing the General Deductive Reasoning Capacity of Large LanguageModels Using OOD Examples. arXiv preprint arXiv:2305.15269, 2023. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and How does In-ContextLearning Learn? Bayesian Model Averaging, Parameterization, and Generalization. arXiv preprintarXiv:2305.19420, 2023a.",
  "Ekin Akyrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algo-rithm is in-context learning? Investigations with linear models. arXiv preprint arXiv:2211.15661,2022a": "Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joo Sacramento, Alexander Mordvintsev,Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. arXivpreprint arXiv:2212.07677, 2022. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learnin-context? A case study of simple function classes. Advances in Neural Information ProcessingSystems, 35:3058330598, 2022.",
  "Aleksandar Petrov, Philip HS Torr, and Adel Bibi.Prompting a pretrained transformer can be auniversal approximator. arXiv preprint arXiv:2402.14753, 2024": "Aleksandar Petrov, Philip Torr, and Adel Bibi. When do prompting and prefix-tuning work? a theoryof capabilities and limitations. In The Twelfth International Conference on Learning Representations,2023a. Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick.Exploring the relationship between model ar-chitecture and in-context learning ability.In The Twelfth International Conference on LearningRepresentations, 2023a. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, BenMann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, ZacHatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, ...,and Chris Olah.In-context Learning and Induction Heads.Transformer Circuits Thread, 2022.",
  "Together Computer. RedPajama: an Open Dataset for Training Large Language Models, October 2023.URL": "Aaditya K Singh, Stephanie CY Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, and Felix Hill. Thetransient nature of emergent in-context learning in transformers. arXiv preprint arXiv:2311.08360,2023. Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung,Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation forextreme risks. arXiv preprint arXiv:2305.15324, 2023.",
  "Tomer Ullman. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprintarXiv:2302.08399, 2023": "Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R McKee, Ari Holtzman,Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, et al. How FaR Are Large LanguageModels From Agents with Theory-of-Mind? arXiv preprint arXiv:2310.03051, 2023a. Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, MaartenSap, and Vered Shwartz. Clever hans or neural theory of mind? Stress testing social reasoning inlarge language models. arXiv preprint arXiv:2305.14763, 2023. Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap.FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. arXiv preprintarXiv:2310.15421, 2023. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelli-gence, 2(11):665673, 2020. Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefen-stette, Tim Rocktschel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuningon procedurally defined tasks, 2023a.",
  "Jonathan Chang, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. Reading tea leaves:How humans interpret topic models. Advances in neural information processing systems, 22, 2009": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language modelstrained on code. arXiv preprint arXiv:2107.03374, 2021a. Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal,Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning andlow-rank modifications. arXiv preprint arXiv:2402.05162, 2024.",
  "Jack Merullo, Carsten Eickhoff, and Ellie Pavlick. Circuit component reuse across tasks in transformerlanguage models. arXiv preprint arXiv:2310.08744, 2023b": "Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher Kanan, and Anton VanDen Hengel. On the value of out-of-distribution testing: An example of goodharts law. Advances inneural information processing systems, 33:407417, 2020. Ryan Burnell, Wout Schellaert, John Burden, Tomer D Ullman, Fernando Martinez-Plumed, Joshua BTenenbaum, Danaja Rutar, Lucy G Cheke, Jascha Sohl-Dickstein, Melanie Mitchell, et al. Rethinkreporting of evaluation results in AI. Science, 380(6641):136138, 2023b. Qiming Bao, Gal Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan, Yang Chen, Michael Wit-brock, and Jiamou Liu. A Systematic Evaluation of Large Language Models on Out-of-DistributionLogical Reasoning Tasks. arXiv preprint arXiv:2310.09430, 2023. Emanuele La Malfa, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber, Ryan Burnell, Anthony GCohn, Nigel Shadbolt, and Michael Wooldridge. Language Models as a Service: Overview of a newparadigm and its challenges. arXiv preprint arXiv:2309.16573, 2023.",
  "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and JacobSteinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300,2020a": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,and Jacob Steinhardt.Measuring mathematical problem solving with the math dataset.arXivpreprint arXiv:2103.03874, 2021b. Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David Jurgens. Do LLMs Understand SocialKnowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark. arXivpreprint arXiv:2305.14938, 2023a. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, NathanScales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.Large language models encodeclinical knowledge. Nature, 620(7972):172180, 2023a.",
  "Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang.On the ToolManipulation Capability of Open-source Large Language Models. arXiv preprint arXiv:2305.16504,2023a": "Rahul Ramesh, Mikail Khona, Robert P Dick, Hidenori Tanaka, and Ekdeep Singh Lubana.HowCapable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks. arXiv preprintarXiv:2311.12997, 2023. Pablo Antonio Moreno Casares, Bao Sheng Loe, John Burden, Jos Hernndez-Orallo, et al. Howgeneral-purpose is a language model? Usefulness and safety with human prompters in the wild. InProceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 52955303, 2022.",
  "Gal Kaplun, Nikhil Ghosh, Saurabh Garg, Boaz Barak, and Preetum Nakkiran. Deconstructing Dis-tributions: A Pointwise Framework of Learning, 2022": "Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves inkernel regression and wide neural networks. In International Conference on Machine Learning, pages10241034. Pmlr, 2020. Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:empirical data versus teacherstudent paradigm. Journal of Statistical Mechanics: Theory and Ex-periment, 2020(12):124001, 2020.",
  "Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. arXivpreprint arXiv:2310.02984, 2023": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-ford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. An empiricalanalysis of compute-optimal large language model training. Advances in Neural Information Process-ing Systems, 35:3001630030, 2022. Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. Thestaircase property: How hierarchical structure can guide deep learning. Advances in Neural Informa-tion Processing Systems, 34:2698927002, 2021.",
  "Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, and Sham Kakade.Scaling laws for imitation learning in nethack. arXiv preprint arXiv:2307.09423, 2023": "Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal Behbahani, AvishkarBhoopchand, Nathalie Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, et al.Human-timescale adaptation in an open-ended task space. arXiv preprint arXiv:2301.07608, 2023. Johan Obando-Ceron, Ghada Sokar, Timon Willi, Clare Lyle, Jesse Farebrother, Jakob Foerster,Gintare Karolina Dziugaite, Doina Precup, and Pablo Samuel Castro. Mixtures of Experts UnlockParameter Scaling for Deep RL. arXiv preprint arXiv:2402.08609, 2024. Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding,Zebin Ou, Guoyang Zeng, et al. Predicting Emergent Abilities with Infinite Resolution Evaluation.arXiv e-prints, pages arXiv2310, 2023a. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, AdamFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al.Beyond theimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprintarXiv:2206.04615, 2022.",
  "Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineeringhow networks learn group operations. arXiv preprint arXiv:2302.03025, 2023": "R Thomas McCoy, Junghyun Min, and Tal Linzen. BERTs of a feather do not generalize together:Large variability in generalization across models with similar test set performance. arXiv preprintarXiv:1911.02969, 2019. Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towardsunderstanding learning representations: To what extent do different neural networks learn the samerepresentation. Advances in neural information processing systems, 31, 2018.",
  "Rhys Gould, Euan Ong, George Ogden, and Arthur Conmy. Successor heads: Recurring, interpretableattention heads in the wild. arXiv preprint arXiv:2312.09230, 2023": "Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and CengizPehlevan.Feature-Learning Networks Are Consistent Across Widths At Realistic Scales.arXivpreprint arXiv:2305.18411, 2023. Thao Nguyen, Maithra Raghu, and Simon Kornblith. Do Wide and Deep Networks Learn the SameThings? Uncovering How Neural Network Representations Vary with Width and Depth. In Interna-tional Conference on Learning Representations, 2020. Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley CLove, Erin Grant, Jascha Achterberg, Joshua B Tenenbaum, et al. Getting aligned on representationalalignment. arXiv preprint arXiv:2310.13018, 2023.",
  "Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. Do LLMsexhibit human-like response biases? A case study in survey design. arXiv preprint arXiv:2311.04076,2023": "Thilo Hagendorff, Sarah Fabi, and Michal Kosinski. Human-like intuitive behavior and reasoning biasesemerged in large language models but disappeared in ChatGPT. Nature Computational Science, 3(10):833838, 2023. Tom Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space wordrepresentations. In Proceedings of the 2013 conference of the north american chapter of the associationfor computational linguistics: Human language technologies, pages 746751, 2013.",
  "Spencer Frei, Gal Vardi, Peter L Bartlett, and Nathan Srebro. The Double-Edged Sword of ImplicitBias: Generalization vs. Robustness in ReLU Networks. arXiv preprint arXiv:2303.01456, 2023": "Edoardo Debenedetti, Zishen Wan, Maksym Andriushchenko, Vikash Sehwag, Kshitij Bhardwaj, andBhavya Kailkhura. Scaling Compute Is Not All You Need for Adversarial Robustness. arXiv preprintarXiv:2312.13131, 2023. Ian R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse Scaling: When Bigger IsntBetter. arXiv preprint arXiv:2306.09479, 2023.",
  "Boaz Barak. Emergent abilities and grokking: Fundamental, Mirage, or both?, 2023": "Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measuresfor grokking via mechanistic interpretability. In The Eleventh International Conference on LearningRepresentations, sep 2022. URL Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hiddenprogress in deep learning: SGD learns parities near the computational limit. Advances in NeuralInformation Processing Systems, 35:2175021764, 2022.",
  "Michael PH Stumpf and Mason A Porter. Critical truths about power laws. Science, 335(6069):665666,2012": "Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in ComputingSystems, pages 17, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, DennyZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances inNeural Information Processing Systems, 35:2482424837, 2022d. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, DavidBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al.Show your work:Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,2021. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large lan-guage models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 27172739. Association forComputational Linguistics, 2023e. doi: 10.18653/v1/2023.acl-long.153. URL Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West,Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and Fate: Limits of Transformerson Compositionality. arXiv preprint arXiv:2305.18654, 2023. Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large LanguageModels Still Cant Plan (A Benchmark for LLMs on Planning and Reasoning about Change). arXivpreprint arXiv:2206.10498, 2022. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek, Boyuan Chen, Bailin Wang, Najoung Kim, JacobAndreas, and Yoon Kim.Reasoning or Reciting?Exploring the Capabilities and Limitations ofLanguage Models Through Counterfactual Tasks.CoRR, abs/2307.02477, 2023a.URL Abulhair Saparov and He He. Language Models Are Greedy Reasoners: A Systematic Formal Analysisof Chain-of-Thought. In The Eleventh International Conference on Learning Representations, ICLR2023, Kigali, Rwanda, May 1-5, 2023, 2022. URL Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak,and Owain Evans. The Reversal Curse: LLMs trained on A is B fail to learn B is A. CoRR,abs/2309.12288, 2023a.",
  "Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.arXiv preprint arXiv:2212.10403, 2022": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V.Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, BehnamNeyshabur, Guy Gur-Ari, and Vedant Misra. Solving Quantitative Reasoning Problems with Lan-guage Models. In NeurIPS, 2022. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, HeewooJun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generativemodeling. arXiv preprint arXiv:2010.14701, 2020. Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou.Scal-ing relationship on learning mathematical reasoning with large language models.arXiv preprintarXiv:2308.01825, 2023a.",
  "Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. MuSR: Testing the Limitsof Chain-of-thought with Multistep Soft Reasoning. CoRR, abs/2310.16049, 2023": "Yifan Hou, Jiaoda Li, Yu Fei, Alessandro Stolfo, Wangchunshu Zhou, Guangtao Zeng, Antoine Bosselut,and Mrinmaya Sachan. Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilitiesof Language Models. CoRR, abs/2310.14491, 2023. Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, HannahRashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive Commonsense Reasoning. In 8thInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April26-30, 2020. OpenReview.net, 2020.",
  "Jonathan Roberts, Timo Lddecke, Sowmen Das, Kai Han, and Samuel Albanie. GPT4GEO: How aLanguage Model Sees the Worlds Geography. arXiv preprint arXiv:2306.00020, 2023a": "Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, andKatia Sycara. Theory of mind for multi-agent collaboration via large language models. arXiv preprintarXiv:2310.10701, 2023a. Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, FernandoGonzalez, Max Kleiman-Weiner, Mrinmaya Sachan, et al. CLADDER: Assessing Causal Reasoningin Language Models, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, JanLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets Verify Step by Step. arXiv preprintarXiv:2305.20050, 2023. Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.Teaching Small Language Models to Reason. In Anna Rogers, Jordan Boyd-Graber, and NaoakiOkazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 17731781, Toronto, Canada, jul 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.acl-short.151. URL Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal,Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. Orca 2: Teaching SmallLanguage Models How to Reason. arXiv preprint arXiv:2311.11045, 2023. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language Models of Codeare Few-Shot Commonsense Learners. In Proceedings of the 2022 Conference on Empirical Methodsin Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,2022, pages 13841403. Association for Computational Linguistics, 2022. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, YianZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, BobbyYan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, DianaAcosta-Navas, Drew Arad Hudson, ..., and Yuta Koreeda. Holistic Evaluation of Language Models.Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL Featured Certification, Expert Certification. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukoiute, Karina Nguyen, NicholasJoseph, Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying Large Language ModelGeneralization with Influence Functions, 2023. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On theParadox of Learning to Reason from Data. In Proceedings of the Thirty-Second International JointConference on Artificial Intelligence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China, pages33653373, 2023c.",
  "William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. Advances inNeural Information Processing Systems, 36, 2024": "Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh M. Susskind, Samy Bengio,and Preetum Nakkiran. What Algorithms can Transformers Learn? A Study in Length Generaliza-tion. CoRR, abs/2310.16028, 2023c. Yangjun Ruan, Honghua Dong, Andrew Wang, Silviu Pitis, Yongchao Zhou, Jimmy Ba, Yann Dubois,Chris J. Maddison, and Tatsunori Hashimoto. Identifying the Risks of LM Agents with an LM-Emulated Sandbox. arXiv preprint arXiv:2309.15817, 2023.",
  "Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact:Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023a": "Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, JonathanTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning throughplanning with language models. arXiv preprint arXiv:2207.05608, 2022a. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, ChelseaFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, JasmineHsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, ..., andAndy Zeng. Do As I Can and Not As I Say: Grounding Language in Robotic Affordances. In arXivpreprint arXiv:2204.01691, 2022.",
  "Joar Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and character-izing reward hacking. arXiv preprint arXiv:2209.13085, 2022": "Lauro Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goal misgeneralization indeep reinforcement learning. In International Conference on Machine Learning, pages 1200412019.Pmlr, 2022. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!arXiv preprint arXiv:2310.03693, 2023a.",
  "Murray Shanahan. The Frame Problem. In Edward N. Zalta, editor, The Stanford Encyclopedia ofPhilosophy. Metaphysics Research Lab, Stanford University, Spring 2016 edition, 2016": "Alexander Nicholas DAmour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi, AlexBeutel, Christina Chen, Jon Deaton, Jacob Eisenstein, Matthew D. Hoffman, Farhad Hormozdiari,Shaobo Hou, Neil Houlsby, Ghassen Jerfel, Alan Karthikesalingam, Mario Lui, Yian Ma, CoryMcLean, Diana Mincu, ..., and D. Sculley. Underspecification Presents Challenges for Credibility inModern Machine Learning. Journal of Machine Learning Research, 2020. URL",
  "Dylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse rein-forcement learning. Advances in neural information processing systems, 29, 2016": "Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, Dmitrii Krasheninnikov, Lawrence Chan,Michael D Dennis, Pieter Abbeel, Anca Dragan, and Stuart Russell.Benefits of assistance overreward learning, 2020. Alexander Matt Turner, Dylan Hadfield-Menell, and Prasad Tadepalli. Conservative agency via attain-able utility preservation. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,pages 385391, 2020.",
  "Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Clam: Selective clarification for ambiguous questionswith large language models. arXiv preprint arXiv:2212.07769, 2022": "Joshua Clymer, Garrett Baker, Rohan Subramani, and Sam Wang. Generalization Analogies (GE-NIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains.arXiv preprintarXiv:2311.07723, 2023. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, NicholasSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk,Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort,..., and Jared Kaplan. Language Models (Mostly) Know What They Know, 2022.",
  "David Krueger, Jan Leike, Owain Evans, and John Salvatier. Active reinforcement learning: Observingrewards at a cost. arXiv preprint arXiv:2011.06709, 2020": "Francis Rhys Ward, Francesca Toni, Francesco Belardinelli, and Tom Everitt. Honesty Is the BestPolicy: Defining and Mitigating AI Deception. In Thirty-seventh Conference on Neural InformationProcessing Systems, 2023. URL Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell. The Off-Switch Game. InProceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17),2017. Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, JonathanNg, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the Rewards Justify the Means? Measur-ing Trade-Offs Between Rewards and Ethical Behavior in the Machiavelli Benchmark. Icml, 2023a.",
  "Kenneth Li, Oam Patel, Fernanda Vigas, Hanspeter Pfister, and Martin Wattenberg.Inference-Time Intervention: Eliciting Truthful Answers from a Language Model, oct 2023b.URL arXiv:2306.03341 [cs]": "Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan,Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J.Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, ..., andDan Hendrycks. Representation Engineering: A Top-Down Approach to AI Transparency, oct 2023a.URL arXiv:2310.01405 [cs]. Silen Naihin, David Atkinson, Marc Green, Merwane Hamadi, Craig Swift, Douglas Schonholtz,Adam Tauman Kalai, and David Bau. Testing Language Model Agents Safely in the Wild. arXivpreprint arXiv:2311.10538, 2023.",
  "Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role play with large language models. Nature,pages 16, 2023": "Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu,Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. RoleLLM: Benchmarking, Eliciting,and Enhancing Role-Playing Abilities of Large Language Models. arXiv preprint arXiv:2310.00746,2023g. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessnessfrom AI feedback. arXiv preprint arXiv:2212.08073, 2022a. Herbie Bradley, Andrew Dai, Hannah Teufel, Jenny Zhang, Koen Oostermeijer, Marco Bellagente, JeffClune, Kenneth Stanley, Grgory Schott, and Joel Lehman. Quality-Diversity through AI Feedback.arXiv preprint arXiv:2310.13032, 2023.",
  "Joseph Henrich. Cooperation, Punishment, and the Evolution of Human Institutions. Science, 312(5770):6061, apr 2006. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.1126398. URL": "Robert Boyd, Herbert Gintis, and Samuel Bowles.Coordinated Punishment of Defectors SustainsCooperation and Can Proliferate When Rare. Science, 328(5978):617620, apr 2010. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.1183665. URL Catherine Moon and Vincent Conitzer. Maximal Cooperation in Repeated Games on Social Networks.In Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence (IJ-CAI), Buenos Aires, Argentina, 2015. URL David Pardoe, Peter Stone, Maytal Saar-Tsechansky, and Kerem Tomak. Adaptive mechanism design.In Proceedings of the 8th international conference on Electronic commerce The new e-commerce: in-novations for conquering current barriers, obstacles and limitations to conducting successful businesson the internet - ICEC '06. ACM Press, 2006. doi: 10.1145/1151454.1151480.",
  "Jiachen Yang, Ethan Wang, Rakshit Trivedi, Tuo Zhao, and Hongyuan Zha. Adaptive Incentive Designwith Multi-Agent Meta-Gradient Reinforcement Learning. arXiv:2112.10859, December 2021": "Stephan Zheng, Alexander Trott, Sunil Srinivasa, David C. Parkes, and Richard Socher.The AIEconomist: Taxation policy design via two-level deep multiagent reinforcement learning. ScienceAdvances, 8(18), may 2022. doi: 10.1126/sciadv.abk2607. Natasha Jaques, Angeliki Lazaridou, Edward Hughes, aglar Glehre, Pedro A. Ortega, DJ Strouse,Joel Z. Leibo, and Nando de Freitas. Social Influence As Intrinsic Motivation for Multi-agent DeepReinforcement Learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings ofthe 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,California, USA, volume 97 of Proceedings of Machine Learning Research, pages 30403049. Pmlr,2019. URL",
  "Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueez Guzman, Antonio Gar-ca Castaeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, Heather Roff, and Thore": "Graepel. Inequity Aversion Improves Cooperation in Intertemporal Social Dilemmas. In S. Ben-gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances inNeural Information Processing Systems 31, pages 33263336. Curran Associates, Inc., 2018. URL Jane X. Wang, Edward Hughes, Chrisantha Fernando, Wojciech M. Czarnecki, Edgar A. DuezGuzmn, and Joel Z. Leibo.Evolving Intrinsic Motivations for Altruistic Behavior.In Proceed-ings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, Aamas19, pages 683692, Richland, SC, 2019a. International Foundation for Autonomous Agents and Mul-tiagent Systems. ISBN 9781450363099. Kevin R. McKee, Ian Gemp, Brian McWilliams, Edgar A. Duez Guzmn, Edward Hughes, andJoel Z. Leibo. Social Diversity and Social Preferences in Mixed-Motive Reinforcement Learning. InProceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems,Aamas 20, page 869877, Richland, SC, 2020. International Foundation for Autonomous Agents andMultiagent Systems. ISBN 9781450375184. Julian Yocum, Phillip Christoffersen, Mehul Damani, Justin Svegliato, Dylan Hadfield-Menell, andStuart Russell. Mitigating Generative Agent Social Dilemmas. In NeurIPS 2023 Foundation Modelsfor Decision Making Workshop, 2023. URL",
  "Paul Christiano. Current Work in AI Alignment. EA Global: San Francisco, 2019": "Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory foralignment. arXiv preprint arXiv:2112.00861, 2021. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.Training language models to followinstructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022.",
  "Craig Barratt and Stephen Boyd. Example of exact trade-offs in linear controller design. IEEE ControlSystems Magazine, 9(1):4652, 1989. doi: 10.1109/37.16750": "Bart De Moor, Johan David, Joos Vandewalle, Maarten De Moor, and Daniel Berckmans. Trade-offsin linear control system design: A practical example. Optimal Control Applications and Methods, 13(2):121144, 1992. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant withreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022b. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefen-stette, and Roberta Raileanu. Understanding the Effects of RLHF on LLM Generalisation and Di-versity. arXiv preprint arXiv:2310.06452, 2023a.",
  "Jan Leike. Distinguishing three alignment taxes, 2022a. URL": "Tong Wang. Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute. In Kama-lika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conferenceon Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 65056514.Pmlr, 0915 Jun 2019. George Baryannis, Samir Dani, and Grigoris Antoniou. Predicting supply chain risks using machinelearning: The trade-off between performance and interpretability.Future Generation ComputerSystems, 101:9931004, 2019. ISSN 0167-739x. doi:",
  "Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-bustness May Be at Odds with Accuracy. In International Conference on Learning Representations,2019": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-retically Principled Trade-off between Robustness and Accuracy. In Kamalika Chaudhuri and RuslanSalakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, vol-ume 97 of Proceedings of Machine Learning Research, pages 74727482. Pmlr, 0915 Jun 2019. Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Joern-Henrik Jacobsen. Fun-damental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations. In Hal DaumIII and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning,volume 119 of Proceedings of Machine Learning Research, pages 95619571. Pmlr, 1318 Jul 2020a. Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,Mung Chiang, Prateek Mittal, and Matthias Hein. RobustBench: a standardized adversarial robust-ness benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets andBenchmarks Track (Round 2), 2021. URL",
  "Gwern Branwen. Why Tool AIs Want to Be Agent AIs. Gwern.net, 2016. on: 3 January, 2024": "Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, TameraLanham, Daniel M Ziegler, Tim Maxwell, Newton Cheng, et al. Sleeper Agents: Training DeceptiveLLMs that Persist Through Safety Training. arXiv preprint arXiv:2401.05566, 2024. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangersof stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conferenceon fairness, accountability, and transparency, pages 610623, 2021.",
  "Helen Ngo, Cooper Raterink, Joo G. M. Arajo, Ivan Zhang, Carol Chen, Adrien Morisot, and NicholasFrosst. Mitigating harm in language models with conditional-likelihood filtration, 2021. URL": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, YanqiZhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-TextTransformer. J. Mach. Learn. Res., 21(1), jun 2022. ISSN 1532-4435. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith.RealToxici-tyPrompts: Evaluating Neural Toxic Degeneration in Language Models.In Findings of the As-sociation for Computational Linguistics: EMNLP 2020, pages 33563369, Online, nov 2020. As-sociation for Computational Linguistics.doi:10.18653/v1/2020.findings-emnlp.301.URL Irene Solaiman and Christy Dennison. Process for Adapting Language Models to Society (PALMS) withValues-Targeted Datasets. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. WortmanVaughan, editors, Advances in Neural Information Processing Systems. Curran Associates, Inc., 2021.URL Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hen-dricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in DetoxifyingLanguage Models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages",
  ", Punta Cana, Dominican Republic, nov 2021. Association for Computational Linguistics.doi: 10.18653/v1/2021.findings-emnlp.210. URL": "Maribeth Rauh, John Mellor, Jonathan Uesato, Po-Sen Huang, Johannes Welbl, Laura Weidinger,Sumanth Dathathri, Amelia Glaese, Geoffrey Irving, Iason Gabriel, William Isaac, and Lisa AnneHendricks. Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,2022. Jesse Dodge, Maarten Sap, Ana Marasovi, William Agnew, Gabriel Ilharco, Dirk Groeneveld, MargaretMitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal cleancrawled corpus. arXiv preprint arXiv:2104.08758, 2021. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh,Allahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance:An audit of web-crawled multilingual datasets. Transactions of the Association for ComputationalLinguistics, 10:5072, 2022. Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. DetoxifyingLanguage Models Risks Marginalizing Minority Voices. In Proceedings of the 2021 Conference ofthe North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies, pages 23902397, Online, jun 2021a. Association for Computational Linguistics. doi:10.18653/v1/2021.naacl-main.190. URL",
  "Marc Marone and Benjamin Van Durme. Data portraits: Recording foundation model training data.arXiv preprint arXiv:2303.03919, 2023": "Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr,Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. Whats In My Big Data? ArXiv,abs/2310.20707, 2023. URL Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,and Nicholas Carlini. Deduplicating Training Data Makes Language Models Better. In SmarandaMuresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers), pages 84248445, Dublin,Ireland, may 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577.URL",
  "Oscar Sainz, Jon Ander Campos, Iker Garca-Ferrero, Julen Etxaniz, and Eneko Agirre. Did ChatGPTcheat on your test?, June 2023": "Chirag Agarwal, Daniel Dsouza, and Sara Hooker. Estimating example difficulty using variance ofgradients. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1036810378, 2022. Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan.Estimating training datainfluence by tracing gradient descent. Advances in Neural Information Processing Systems, 33:1992019930, 2020.",
  "Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:Attributing model behavior at scale. arXiv preprint arXiv:2303.14186, 2023c": "Ekin Akyrek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and KelvinGuu. Towards tracing knowledge in language models back to the training data. In Findings of theAssociation for Computational Linguistics: EMNLP 2022, pages 24292446, 2022b. Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B Grosse. If Influence Functions arethe Answer, Then What is the Question? Advances in Neural Information Processing Systems, 35:1795317967, 2022.",
  "Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L. Buckley, Jason Phang,Samuel R. Bowman, and Ethan Perez. Pretraining Language Models with Human Preferences, 2023": "Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu,and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. Advances in neuralinformation processing systems, 35:2759127609, 2022. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequencemodeling. Advances in neural information processing systems, 34:1508415097, 2021b. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent ElShafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick,Kevin Robinson, ..., and Yonghui Wu. PaLM 2 Technical Report, 2023. Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: Aconditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858,2019. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and ChelseaFinn. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arXivpreprint arXiv:2305.18290, 2023.",
  "Dan Friedman, Alexander Wettig, and Danqi Chen. Learning Transformer Programs. arXiv preprintarXiv:2306.01128, 2023": "Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Good-man, and Christopher Potts. Inducing Causal Structure for Interpretable Neural Networks, jul 2022.URL arXiv:2112.00826 [cs]. Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. Self-destructingmodels: Increasing the costs of harmful dual uses of foundation models. In Proceedings of the 2023AAAI/ACM Conference on AI, Ethics, and Society, pages 287296, 2023a.",
  "Qiusi Zhan, Richard Fang, Rohan Bindu, Akul Gupta, Tatsunori Hashimoto, and Daniel Kang. Re-moving RLHF Protections in GPT-4 via Fine-Tuning, 2023": "Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun Peng. Kelly is aWarm Person, Joseph is a Role Model: Gender Biases in LLM-Generated Reference Letters. arXivpreprint arXiv:2310.09219, 2023a. Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jrmy Scheurer, Javier Rando,Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphal Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum,Usman Anwar, ..., and Dylan Hadfield-Menell.Open Problems and Fundamental Limitations ofReinforcement Learning from Human Feedback, 2023a.",
  "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.arXiv preprint arXiv:1907.02893, 2019": "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neuralnetworks for group shifts: On the importance of regularization for worst-case generalization. arXivpreprint arXiv:1911.08731, 2019. David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, DinghuaiZhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation(rex). In International Conference on Machine Learning, pages 58155826. Pmlr, 2021. Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith,Mari Ostendorf, and Hannaneh Hajishirzi. Fine-Grained Human Feedback Gives Better Rewards forLanguage Model Training. arXiv preprint arXiv:2306.01693, 2023b. Jrmy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, KyunghyunCho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprintarXiv:2303.16755, 2023b.",
  "Andreas Stuhlmller and Jungwon Byun.Supervise Process, not Outcomes, 2022.URL": "James Chua, Edward Rees, Hunar Batra, Samuel R Bowman, Julian Michael, Ethan Perez, and MilesTurpin. Bias-Augmented Consistency Training Reduces Biased Reasoning in Chain-of-Thought. arXivpreprint arXiv:2403.05518, 2024. Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: aregularization method for supervised and semi-supervised learning. IEEE transactions on patternanalysis and machine intelligence, 41(8):19791993, 2018.",
  "Yiming Zhang and Daphne Ippolito. Prompts should not be seen as secrets: Systematically measuringprompt extraction attack success. arXiv preprint arXiv:2307.06865, 2023": "Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A Feder Cooper, Daphne Ippolito,Christopher A Choquette-Choo, Eric Wallace, Florian Tramr, and Katherine Lee.Scalable Ex-traction of Training Data from (Production) Language Models. arXiv preprint arXiv:2311.17035,2023. Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium onSecurity and Privacy (SP), pages 141159. Ieee, 2021.",
  "Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight Methodsto Evaluate Robust Unlearning in LLMs, 2024": "Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li,Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The WMDP Benchmark: Measuringand Reducing Malicious Use With Unlearning. arXiv preprint arXiv:2403.03218, 2024. Leyla Bilge and Tudor Dumitra. Before we knew it: an empirical study of zero-day attacks in the realworld. In Proceedings of the 2012 ACM conference on Computer and communications security, pages833844, 2012. Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, andZhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy chat with LLMs via cipher. arXiv preprintarXiv:2308.06463, 2023b. Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, and Daniela Rus. Lost in pruning:The effects of pruning neural networks beyond test accuracy. Proceedings of Machine Learning andSystems, 3:93138, 2021. Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia Hu, and Ahmed Hassan Awadal-lah. Robustness Challenges in Model Distillation and Pruning for Natural Language Understanding.arXiv preprint arXiv:2110.08419, 2021.",
  "Samuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural languageunderstanding? arXiv preprint arXiv:2104.02145, 2021": "Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt.Are We Learning Yet?A Meta Review of Evaluation Failures Across Machine Learning.In Thirty-fifth Conference onNeural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.URL Ben Hutchinson, Negar Rostamzadeh, Christina Greer, Katherine Heller, and Vinodkumar Prab-hakaran. Evaluation gaps in machine learning practice. In Proceedings of the 2022 ACM Conferenceon Fairness, Accountability, and Transparency, pages 18591876, 2022.",
  "Sayash Kapoor and Arvind Narayanan. Leakage and the reproducibility crisis in machine-learning-basedscience. Patterns, 4(9), 2023": "Timothy R McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N Halgamuge. Inadequaciesof large language model benchmarks in the era of generative artificial intelligence. arXiv preprintarXiv:2402.09880, 2024. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying Language Models Sensitivityto Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting.arXiv preprint arXiv:2310.11324, 2023.",
  "Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, and Gabriel Stanovsky. Stateof What Art? A Call for Multi-Prompt LLM Evaluation. arXiv preprint arXiv:2401.00595, 2023": "Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, LinqiSong, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter withcode-based self-verification. arXiv preprint arXiv:2308.07921, 2023e. Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprintarXiv:2109.01652, 2021.",
  "Guanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts.arXiv preprint arXiv:2104.06599, 2021": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural InformationProcessing Systems, 34, 2021. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman.Leveraging procedural generation tobenchmark reinforcement learning. In International conference on machine learning, pages 20482056. Pmlr, 2020. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization with-out overfitting: Analyzing the training dynamics of large language models.Advances in NeuralInformation Processing Systems, 35:3827438290, 2022.",
  "Yucheng Li.An open source data contamination report for llama series models.arXiv preprintarXiv:2310.17589, 2023": "Tomohiro Sawada, Daniel Paleka, Alexander Havrilla, Pranav Tadepalli, Paula Vidas, Alexander Kra-nias, John J Nay, Kshitij Gupta, and Aran Komatsuzaki. Arb: Advanced reasoning benchmark forlarge language models. arXiv preprint arXiv:2307.13692, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani,Julian Michael, and Samuel R Bowman. GPQA: A Graduate-Level Google-Proof Q&A Benchmark.arXiv preprint arXiv:2311.12022, 2023. Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan, Peter Clark, Ashish Sab-harwal, and Tushar Khot. Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs.arXiv preprint arXiv:2311.04892, 2023. Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, YijiaXiao, Haozhe Lyu, et al. Benchmarking Foundation Models with Language-Model-as-an-Examiner.arXiv preprint arXiv:2306.04181, 2023b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. JudgingLLM-as-a-judge with MT-Bench and Chatbot Arena, 2023.",
  "Natalie Schluter. The glass ceiling in NLP. In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing, pages 27932798, 2018": "Ali Akbar Septiandri, Marios Constantinides, Mohammad Tahaei, and Daniele Quercia. WEIRD FAc-cTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT? In Proceedings of the2023 ACM Conference on Fairness, Accountability, and Transparency, pages 160171, 2023. Su Lin Blodgett, Q Vera Liao, Alexandra Olteanu, Rada Mihalcea, Michael Muller, Morgan KlausScheuerman, Chenhao Tan, and Qian Yang.Responsible language technologies: Foreseeing andmitigating harms. In CHI Conference on Human Factors in Computing Systems Extended Abstracts,pages 13, 2022. Samuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, KamileLukoiute, Amanda Askell, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, CameronMcKinnon, Christopher Olah, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, ..., and Jared Kaplan.Measuring Progress on Scalable Oversight for Large LanguageModels, nov 2022. URL arXiv:2211.03540 [cs].",
  "David Krueger. AI alignment and generalization in deep learning, 2023": "Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, EdwardGrefenstette, Samuel R Bowman, Tim Rocktschel, and Ethan Perez. Debating with More PersuasiveLLMs Leads to More Truthful Answers. arXiv preprint arXiv:2402.06782, 2024. Justin Reppert, Ben Rachbach, Charlie George, Luke Stebbing, Jungwon Byun, Maggie Appleton, andAndreas Stuhlmller. Iterated Decomposition: Improving Science Q&A by Supervising ReasoningProcesses, jan 2023. URL arXiv:2301.01751 [cs]. Beth Barnes, Paul Christiano, William Saunders, Joe Collman, Mark Xu, Chris Painter, Mihnea Maftei,and Ronny Fernandez. Debate update: Obfuscated arguments problem. AI Alignment Forum, 2020. Accessed on: 3 January, 2024.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan.Axiomatic Attribution for Deep Networks.InInternational Conference on Machine Learning, 2017. URL": "Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. Whatis one grain of sand in the desert? Analyzing individual neurons in deep NLP models. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 33, pages 63096317, 2019. David Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba.Understanding the role of individual units in a deep neural network. Proceedings of the NationalAcademy of Sciences, 117(48):3007130078, 2020.",
  "Anita Mahinpei, Justin Clark, Isaac Lage, Finale Doshi-Velez, and Weiwei Pan. Promises and pitfallsof black-box concept learning models. arXiv preprint arXiv:2106.13314, 2021": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie J. Cai, James Wexler, Fernanda B. Vigas,and Rory Sayres. Interpretability Beyond Feature Attribution: Quantitative Testing with ConceptActivation Vectors (TCAV). In International Conference on Machine Learning, 2017. URL David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogiby self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815, 2017.",
  "Andreea Bobu, Andi Peng, Pulkit Agrawal, Julie Shah, and Anca D Dragan. Aligning Robot andHuman Representations. arXiv preprint arXiv:2302.01928, 2023": "Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A Vandermeulen, Katherine Hermann,Andrew Lampinen, and Simon Kornblith. Improving neural network representations using humansimilarity judgments. Advances in Neural Information Processing Systems, 36, 2023. Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, and Lalana Kagal.Explaining Explanations: An Overview of Interpretability of Machine Learning.2018 IEEE 5thInternational Conference on Data Science and Advanced Analytics (DSAA), pages 8089, 2018. URL",
  "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors: High-Precision Model-AgnosticExplanations. In AAAI Conference on Artificial Intelligence, 2018. URL": "Lawrence Chan, Adri Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, jenny, Ansh Rad-hakrishnan, Buck Shlegaris, and Nate Thomas. Causal Scrubbing: a method for rigorously testinginterpretability hypotheses, 2023c. Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, JacobAndreas, David Bau, and Antonio Torralba. A Function Interpretation Benchmark for EvaluatingInterpretability Methods. ArXiv, abs/2309.03886, 2023. URL",
  "Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge.In Machine learning challenges workshop, pages 177190. Springer, 2005": "Ellie Pavlick and Tom Kwiatkowski. Inherent Disagreements in Human Textual Inferences. Transactionsof the Association for Computational Linguistics, 7:677694, 11 2019. ISSN 2307-387x. doi: 10.1162/tacl_a_00293. URL Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernan-dez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukoiute, Karina Nguyen,Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish,Sandipan Kundu, ..., and Ethan Perez. Measuring Faithfulness in Chain-of-Thought Reasoning, jul2023. URL arXiv:2307.13702 [cs]. Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and KathleenMcKeown.Do Models Explain Themselves?Counterfactual Simulatability of Natural LanguageExplanations, jul 2023b. URL arXiv:2307.08678 [cs]. Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh,Ruchir Puri, Jos MF Moura, and Peter Eckersley. Explainable machine learning in deployment. InProceedings of the 2020 conference on fairness, accountability, and transparency, pages 648657, 2020.",
  "Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: Trainingdifferentiable models by constraining their explanations. arXiv preprint arXiv:1703.03717, 2017": "Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women alsosnowboard: Overcoming bias in captioning models. In Proceedings of the European conference oncomputer vision (ECCV), pages 771787, 2018. Laura Rieger, Chandan Singh, William Murdoch, and Bin Yu. Interpretations are useful: penalizingexplanations to align neural networks with prior knowledge. In International conference on machinelearning, pages 81168126. PMLR, 2020. Wolfgang Stammer, Patrick Schramowski, and Kristian Kersting. Right for the right concept: Revisingneuro-symbolic concepts by interacting with their explanations. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 36193629, 2021.",
  "Arthur Conmy,Augustine N. Mavor-Parker,Aengus Lynch,Stefan Heimersheim,and AdriGarriga-Alonso.Towards Automated Circuit Discovery for Mechanistic Interpretability.CoRR,abs/2304.14997, 2023. URL": "Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. Language Models Dont AlwaysSay What They Think: Unfaithful Explanations in Chain-of-Thought Prompting, may 2023. URL arXiv:2305.04388 [cs]. Jacob Eisenstein, Daniel Andor, Bernd Bohnet, Michael Collins, and David Mimno. Honest studentsfrom untrusted teachers: Learning an interpretable question-answering pipeline from a pretrainedlanguage model. arXiv preprint arXiv:2210.02498, 2022. Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez,Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukoiute, Newton Cheng, Nicholas Joseph,Nicholas Schiefer, Oliver Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell,Venkatesa Chandrasekaran, ..., and Ethan Perez. Question Decomposition Improves the Faithfulnessof Model-Generated Reasoning. arXiv, jul 2023. doi: 10.48550/arXiv.2307.11768. URL arXiv:2307.11768 [cs].",
  "Max Tegmark and Steve Omohundro. Provably safe systems: the only path to controllable AGI. arXivpreprint arXiv:2309.01933, 2023": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.Program synthesis with large languagemodels. arXiv preprint arXiv:2108.07732, 2021. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, andChristian Szegedy. Autoformalization with large language models. Advances in Neural InformationProcessing Systems, 35:3235332368, 2022.",
  "John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive logic pro-gramming. In Proceedings of the national conference on artificial intelligence, pages 10501055, 1996": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase fromquestion-answer pairs.In Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 15331544, 2013. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li,Qingning Yao, Shanelle Roman, et al. Spider: A Large-Scale Human-Labeled Dataset for Complexand Cross-Domain Semantic Parsing and Text-to-SQL Task. In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing, pages 39113921, 2018.",
  "Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom Goldstein. CoercingLLMs to do and reveal (almost) anything, 2024": "Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, YanZheng, and Yang Liu. Prompt Injection attack against LLM-integrated Applications. arXiv preprintarXiv:2306.05499, 2023a. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz.More than youve asked for:A Comprehensive Analysis of Novel Prompt Injection Threats toApplication-Integrated Large Language Models. arXiv preprint arXiv:2302.12173, 2023a.",
  "Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines.arXiv preprint arXiv:1206.6389, 2012": "Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce,Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramr. Poisoning web-scale train-ing datasets is practical. arXiv preprint arXiv:2302.10149, 2023b. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, EricHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.Pythia: A suite for analyzing large language models across training and scaling. In InternationalConference on Machine Learning, pages 23972430. Pmlr, 2023. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li,Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. LLM360: Towards Fully Transparent Open-SourceLLMs. arXiv preprint arXiv:2312.06550, 2023d. Nicholas Carlini, Chang Liu, lfar Erlingsson, Jernej Kos, and Dawn Song. The Secret Sharer: Evalu-ating and Testing Unintended Memorization in Neural Networks. In Proceedings of the 28th USENIXConference on Security Symposium, Sec19, page 267284, Usa, 2019. USENIX Association. ISBN9781939133069. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, PaulChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprintarXiv:1909.08593, 2019.",
  "Javier Rando and Florian Tramr. Universal Jailbreak Backdoors from Poisoned Human Feedback.arXiv preprint arXiv:2311.14455, 2023": "Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, and Chaowei Xiao. On the Ex-ploitability of Reinforcement Learning with Human Feedback for Large Language Models. arXivpreprint arXiv:2311.09641, 2023j. Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, Mathias Humbert, Pascal Berrang, and Yang Zhang.Data poisoning attacks against multimodal encoders. In International Conference on Machine Learn-ing, pages 3929939313. Pmlr, 2023b.",
  "Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. arXiv preprintarXiv:2106.09667, 2021": "Changjiang Li, Ren Pang, Zhaohan Xi, Tianyu Du, Shouling Ji, Yuan Yao, and Ting Wang. An Em-barrassingly Simple Backdoor Attack on Self-supervised Learning. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages 43674378, October 2023f. Aniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. BackdoorAttacks on Self-Supervised Learning. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pages 1333713346, June 2022. Zhen Xiang, David J Miller, and George Kesidis. Revealing backdoors, post-training, in DNN classifiersvia novel inference on optimized perturbations inducing group misclassification. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages38273831. Ieee, 2020. Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-boxdetection of backdoor attacks with limited information and data. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1648216491, 2021. Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben YZhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEESymposium on Security and Privacy (SP), pages 707723. Ieee, 2019b. Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs:Scanning neural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019ACM SIGSAC Conference on Computer and Communications Security, pages 12651282, 2019. Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A Gunter, and Bo Li. Detecting AI trojansusing meta neural analysis. In 2021 IEEE Symposium on Security and Privacy (SP), pages 103120.Ieee, 2021b. Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns:Revealing backdoor attacks in cnns.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 301310, 2020. Lauro Langosco, Neel Alex, William Baker, David Quarel, Herbie Bradley, and David Krueger. Detect-ing Backdoors with Meta-Models. In NeurIPS 2023 Workshop on Backdoors in Deep Learning - TheGood, the Bad, and the Ugly, 2024. URL Shafi Goldwasser, Michael P Kim, Vinod Vaikuntanathan, and Or Zamir. Planting undetectable back-doors in machine learning models. In 2022 IEEE 63rd Annual Symposium on Foundations of Com-puter Science (FOCS), pages 931942. Ieee, 2022.",
  "Betty Li Hou and Brian Patrick Green. Foundational Moral Values for AI Alignment. arXiv preprintarXiv:2311.17017, 2023": "Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina Pyatkin, Peter West, NouhaDziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. Value Kaleidoscope: Engaging AI withpluralistic human values, rights, and duties. arXiv preprint arXiv:2309.00779, 2023. Esin Durmus, Karina Nyugen, Thomas I Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin,Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring therepresentation of subjective global opinions in language models. arXiv preprint arXiv:2306.16388,2023. Jan-Philipp Frnken, Sam Kwok, Peixuan Ye, Kanishk Gandhi, Dilip Arumugam, Jared Moore, AlexTamkin, Tobias Gerstenberg, and Noah D Goodman. Social Contract AI: Aligning AI Assistantswith Implicit Group Norms. arXiv preprint arXiv:2310.17769, 2023.",
  "Sarah Keren, Avigdor Gal, and Erez Karpas. Goal recognition design. In Proceedings of the InternationalConference on Automated Planning and Scheduling, volume 24, pages 154162, 2014": "Malek Mechergui and Sarath Sreedharan. Goal Alignment: Re-analyzing Value Alignment ProblemsUsing Human-Aware AI. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38,pages 1011010118, 2024. Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas, Keerthana Gopalakrishnan,Karol Hausman, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Sean Kirmani, Isabel Leal,Edward Lee, Sergey Levine, Yao Lu, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, PannagSanketi, ..., and Zhuo Xu. AutoRT: Embodied Foundation Models for Large Scale Orchestration ofRobotic Agents, 2024. Aida Mostafazadeh Davani, Mark Daz, and Vinodkumar Prabhakaran. Dealing with disagreements:Looking beyond the majority vote in subjective annotations. Transactions of the Association forComputational Linguistics, 10:92110, 2022. Marc Serramia, Maite Lpez-Snchez, Stefano Moretti, and Juan Antonio Rodrguez-Aguilar. On thedominant set selection problem and its application to value alignment.Autonomous Agents andMulti-Agent Systems, 35(2):42, 2021.",
  "Mohammad Atari, Mona J Xue, Peter S Park, Damin Blasi, and Joseph Henrich. Which humans?2023": "Rebecca L Johnson, Giada Pistilli, Natalia Mendez-Gonzlez, Leslye Denisse Dias Duran, Enrico Panai,Julija Kalpokiene, and Donald Jay Bertulfo. The ghost in the machine has an american accent: valueconflict in gpt-3. arXiv preprint arXiv:2203.07785, 2022. Shaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and Vinodkumar Prabhakaran.Cul-tural Re-contextualization of Fairness Research in Language Technologies in India. arXiv preprintarXiv:2211.11206, 2022.",
  "Roel Dobbe, Thomas Krendl Gilbert, and Yonatan Mintz. Hard choices in artificial intelligence. Arti-ficial Intelligence, 300:103555, 2021": "Mona Sloane, Emanuel Moss, Olaitan Awomolo, and Laura Forlano. Participation is not a design fix formachine learning. In Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms,Mechanisms, and Optimization, pages 16, 2022. Abeba Birhane, William Isaac, Vinodkumar Prabhakaran, Mark Diaz, Madeleine Clare Elish, IasonGabriel, and Shakir Mohamed. Power to the people? Opportunities and challenges for participatoryAI. Equity and Access in Algorithms, Mechanisms, and Optimization, pages 18, 2022.",
  "Deliberation at Scale. First Report: Democratic Inputs to AI, 2023. URL": "Christian Haerpfer, Ronald Inglehart, Alejandro Moreno, Christian Welzel, Kseniya Kizilova, JaimeDiez-Medrano, Milena Lagos, Pippa Norris, Eduard Ponarin, and Bianca Puranen. World ValuesSurvey: Round Seven Country-Pooled Datafile Version 5.0.0, 2022. Shalom H Schwartz.Universals in the content and structure of values: Theoretical advances andempirical tests in 20 countries. In Advances in experimental social psychology, volume 25, pages 165.Elsevier, 1992.",
  "Shalom H Schwartz. Are there universal aspects in the structure and contents of human values? Journalof social issues, 50(4):1945, 1994": "Laura Weidinger, Kevin R McKee, Richard Everett, Saffron Huang, Tina O Zhu, Martin J Chadwick,Christopher Summerfield, and Iason Gabriel. Using the Veil of Ignorance to align AI systems withprinciples of justice. Proceedings of the National Academy of Sciences, 120(18):e2213709120, 2023b. Tanusree Sharma, Jongwon Park, Yujin Kwon, Yiren Liu, Yun Huang, Sunny Liu, Dawn Song, JeffHancock, and Yang Wang. Inclusive.ai: Engaging Underserved Populations In Democratic Decision-making On Ai, 2023. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, JanBalaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning languagemodels to find agreement among humans with diverse preferences. Advances in Neural InformationProcessing Systems, 35:3817638189, 2022.",
  "Matthias Kaiser. The idea of a theory of values and the metaphor of value-landscapes. Humanities andSocial Sciences Communications, 11(1):110, 2024": "Hannah Rose Kirk, Bertie Vidgen, Paul Rttger, and Scott A Hale.The empty signifier problem:Towards clearer paradigms for operationalising\" alignment\" in large language models. arXiv preprintarXiv:2310.02457, 2023b. Neel Guha, Christie Lawrence, Lindsey A Gailmard, Kit Rodolfa, Faiz Surani, Rishi Bommasani,Inioluwa Raji, Mariano-Florentino Cullar, Colleen Honigsberg, Percy Liang, et al. Ai regulationhas its own alignment problem: The technical and institutional feasibility of disclosure, registration,licensing, and auditing. George Washington Law Review, Forthcoming, 2023. Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe,Paul Scharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artificial intelligence: Fore-casting, prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018.",
  "Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. AI model GPT-3 (dis) informs usbetter than humans. arXiv preprint arXiv:2301.11924, 2023": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, andYejin Choi. Defending against neural fake news. Advances in neural information processing systems,32, 2019. Jiawei Zhou, Yixuan Zhang, Qianni Luo, Andrea G Parker, and Munmun De Choudhury. Syntheticlies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions.In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 120,2023f. Josh A Goldstein, Girish Sastry, Micah Musser, Renee DiResta, Matthew Gentzel, and Katerina Sedova.Generative language models and automated influence operations: Emerging threats and potentialmitigations. arXiv preprint arXiv:2301.04246, 2023.",
  "Emilio Ferrara. GenAI against humanity: Nefarious applications of generative artificial intelligence andlarge language models. arXiv preprint arXiv:2310.00737, 2023": "Hannah Rose Kirk, Bertie Vidgen, Paul Rttger, and Scott A Hale. Personalisation within bounds: Arisk taxonomy and policy framework for the alignment of large language models with personalisedfeedback. arXiv preprint arXiv:2303.05453, 2023c. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, AmeliaGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed by languagemodels. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency,pages 214229, 2022.",
  "Arthur Erzberger. WormGPT and FraudGPT The Rise of Malicious LLMs. 2023": "Ehsan Aghaei, Xi Niu, Waseem Shadid, and Ehab Al-Shaer. SecureBERT: A Domain-Specific LanguageModel for Cybersecurity. In International Conference on Security and Privacy in CommunicationSystems, pages 3956. Springer, 2022. Mohamed Amine Ferrag, Mthandazo Ndhlovu, Norbert Tihanyi, Lucas C Cordeiro, Merouane Debbah,and Thierry Lestable. Revolutionizing Cyber Threat Detection with Large Language Models. arXivpreprint arXiv:2306.14263, 2023.",
  "Christian Schroeder de Witt, Samuel Sokota, J Zico Kolter, Jakob Foerster, and Martin Strohmeier.Perfectly secure steganography using minimum entropy coupling. arXiv preprint arXiv:2210.14889,2022": "Tim Franzmeyer, Stephen Marcus McAleer, Joao F Henriques, Jakob Nicolaus Foerster, Philip Torr,Adel Bibi, and Christian Schroeder de Witt. Illusory attacks: Detectability matters in adversarialattacks on sequential decision-makers. In The Twelfth International Conference on Learning Repre-sentations, 2023. Jiacen Xu, Jack W Stokes, Geoff McDonald, Xuesong Bai, David Marshall, Siyue Wang, Adith Swami-nathan, and Zhou Li. Autoattacker: A large language model guided system to implement automaticcyber-attacks. arXiv preprint arXiv:2403.01038, 2024b. Matteo Boffa, Giulia Milan, Luca Vassio, Idilio Drago, Marco Mellia, and Zied Ben Houidi. Towardsnlp-based processing of honeypot logs. In 2022 IEEE European Symposium on Security and PrivacyWorkshops (EuroS&PW), pages 314321. IEEE, 2022.",
  "Christopher A. Mouton, Caleb Lucas, and Ella Guest. The Operational Risks of AI in Large-ScaleBiological Attacks: Results of a Red-Team Study, 2024": "Tejal Patwardhan, Kevin Liu, Todor Markov, Neil Chowdhury, Dillon Leet, Natalie Cone, CaitlinMaltbie, Joost Huizinga, Carroll Wainwright, Shawn Jackson, Steven Adler, Rocco Casagrande, andAleksander Madry. Building an early warning system for LLM-aided biological threat creation, 2024. Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M PawanKumar, Emilien Dupont, Francisco JR Ruiz, Jordan S Ellenberg, Pengming Wang, Omar Fawzi, et al.Mathematical discoveries from program search with large language models. Nature, 2024.",
  "World Health Organization. Ethics and Governance of Artificial Intelligence for Health: WHO Guid-ance. World Health Organization, Geneva, 2021": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, KasunFernando, Aniruddha Saha, Micah Goldblum, and Tom Goldstein. On the Reliability of Watermarksfor Large Language Models. arXiv preprint arXiv:2306.04634, 2023. Hanlin Zhang, Benjamin L Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and BoazBarak. Watermarks in the sand: Impossibility of strong watermarking for generative models. arXivpreprint arXiv:2311.04378, 2023e.",
  "Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, and Heng Huang. Unbiasedwatermark for large language models. arXiv preprint arXiv:2310.10669, 2023d": "Isabelle Augenstein,Timothy Baldwin,Meeyoung Cha,Tanmoy Chakraborty,Giovanni LucaCiampaglia, David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, and Alon Halevy. Factu-ality challenges in the era of large language models. arXiv preprint arXiv:2310.05189, 2023. Elizabeth Seger, Noemi Dreksler, Richard Moulange, Emily Dardaman, Jonas Schuett, K Wei,Christoph Winter, Mackenzie Arnold, Sen higeartaigh, and Anton Korinek.Open-SourcingHighly Capable Foundation Models: An evaluation of risks, benefits, and alternative methods forpursuing open-source objectives. arXiv preprint arXiv:2311.09227, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, WenyinFu, ..., and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models, 2023. Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu,Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, DanHendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. DecodingTrust: A Com-prehensive Assessment of Trustworthiness in GPT Models, 2023k. Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J. Ratner, Ranjay Krishna, Jiaming Shen,and Chao Zhang. Large Language Model as Attributed Training Data Generator: A Tale of Diversityand Bias. ArXiv, abs/2306.15895, 2023c. URL Valentin Hofmann, Pratyusha Ria Kalluri, Dan Jurafsky, and Sharese King.Dialect prejudicepredicts AI decisions about peoples character, employability, and criminality.arXiv preprintarXiv:2403.00742, 2024.",
  "Jrme Rutinowski, Sven Franke, Jan Endendyk, Ina Dormuth, and Markus Pauly. The Self-Perceptionand Political Biases of ChatGPT, 2023": "Rida Qadri, Renee Shelby, Cynthia L Bennett, and Emily Denton. AIs Regimes of Representation: ACommunity-centered Study of Text-to-Image Models in South Asia. In Proceedings of the 2023 ACMConference on Fairness, Accountability, and Transparency, pages 506517, 2023. Akshita Jha, Vinodkumar Prabhakaran, Remi Denton, Sarah Laszlo, Shachi Dave, Rida Qadri, Chan-dan K Reddy, and Sunipa Dev. Beyond the Surface: A Global-Scale Analysis of Visual Stereotypesin Text-to-Image Generation. arXiv preprint arXiv:2401.06310, 2024.",
  "Sara Merken. New York lawyers sanctioned for using fake ChatGPT cases in legal brief, 2023. URL": "Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, KirillFedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Bald-win, and Artem Shelmanov. LM-Polygraph: Uncertainty Estimation for Language Models, 2023. Cordula Kupfer, Rita Prassl, Jrgen Flei, Christine Malin, Stefan Thalmann, and Bettina Kubicek.Check the box! How to deal with automation bias in AI-based personnel selection. Frontiers inPsychology, 14:1118723, 2023.",
  "Angus Maddison. The World Economy: Historical Statistics. OECD Development Centre, 2004": "Anton Korinek and Joseph E Stiglitz. Artificial intelligence and its implications for income distribu-tion and unemployment. In The Economics of Artificial Intelligence: An agenda, pages 349390.University of Chicago Press, 2019. Daniel Susskind. Technological Unemployment. In Justin B. Bullock, Yu-Che Chen, Johannes Himmel-reich, Valerie M. Hudson, Anton Korinek, Matthew M. Young, and Baobao Zhang, editors, The OxfordHandbook of AI Governance. Oxford Academic, 2023. doi: 10.1093/oxfordhb/9780197579329.013.42.URL",
  "Julen Etxaniz, Gorka Azkune, Aitor Soroa, Oier Lopez de Lacalle, and Mikel Artetxe. Do multilinguallanguage models think better in English? arXiv preprint arXiv:2308.01223, 2023": "Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang, Haoran Xu, Boyuan Zheng,Philipp Koehn, and Daniel Khashabi. The language barrier: Dissecting safety challenges of LLMs inmultilingual contexts. arXiv preprint arXiv:2401.13136, 2024. Lora Aroyo, Alex Taylor, Mark Diaz, Christopher Homan, Alicia Parrish, Gregory Serapio-Garca,Vinodkumar Prabhakaran, and Ding Wang. DICES dataset: Diversity in conversational ai evaluationfor safety. Advances in Neural Information Processing Systems, 36, 2024.",
  "Lawrence Lessig. Code and Other Laws of Cyberspace, Version 2.0. Basic Books, 2006. URL": "Javier Snchez-Monedero, Lina Dencik, and Lilian Edwards. What does it mean to solve the problemof discrimination in hiring?: social, technical and legal perspectives from the UK on automatedhiring systems. In FAT* 20: Proceedings of the 2020 Conference on Fairness, Accountability, andTransparency, pages 458468. Acm, January 2020. doi: 10.1145/3351095.3372849. URL Derek OCallaghan, Derek Greene, Maura Conway, Joe Carthy, and Pdraig Cunningham. Down the(white) rabbit hole: The extreme right and online recommender systems. Social Science ComputerReview, 33(4):459478, 2015. Mohamed Abdalla and Moustafa Abdalla. The grey hoodie project: Big tobacco, big tech, and thethreat on academic integrity. In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, andSociety, pages 287297, 2021.",
  "Inioluwa Deborah Raji, SASHA COSTANZA Chock, and J Buolamwini. Change from the outside:Towards credible third-party audits of ai systems. Missing links in AI governance, page 5, 2023": "Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rum-man Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, et al. International institutions foradvanced AI. arXiv preprint arXiv:2307.04699, 2023. Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim, Lewis Ho, Sarah Kreps,Ranjit Lall, Owen Larter, Sen higeartaigh, et al. International Governance of Civilian AI: AJurisdictional Certification Approach. arXiv preprint arXiv:2308.15514, 2023. Matthijs Maas and Jos Jaime Villalobos. International AI Institutions: a literature review of models,examples, and proposals. Technical Report AI Foundations Report #1, Legal Priorities Project, 2023.URL",
  "Miriam Buiten, Alexandre De Streel, and Martin Peitz. The law and economics of AI liability. ComputerLaw & Security Review, 48:105794, 2023": "Timotheus Kampik, Adnane Mansour, Olivier Boissier, Sabrina Kirrane, Julian Padget, Terry R Payne,Munindar P Singh, Valentina Tamma, and Antoine Zimmermann. Governance of autonomous agentson the web: challenges and opportunities. ACM Transactions on Internet Technology, 22(4):131,2022. Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, EmmaBluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, et al. Visibility into AI Agents. arXivpreprint arXiv:2401.13138, 2024. Justin B. Bullock, Hsini Huang, and Kyoung-Cheol (Casey) Kim. Machine Intelligence, Bureaucracy,and Human Control. Perspectives on Public Management and Governance, 5(2):187196, June 2022b.doi: 10.1093/ppmgov/gvac006. URL",
  "Dami Choi, Yonadav Shavit, and David Duvenaud. Tools for Verifying Neural Models Training Data.arXiv preprint arXiv:2307.00682, 2023b": "Sanjam Garg, Aarushi Goel, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Guru-VamsiPolicharla, and Mingyuan Wang. Experimenting with zero-knowledge proofs of training. In Pro-ceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security, pages18801894, 2023. Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence:Modeling the influence of individual training examples by simulating training runs. arXiv preprintarXiv:2303.08114, 2023.",
  "Janet Egan and Lennart Heim. Oversight for Frontier AI through a Know-Your-Customer Scheme forCompute Providers. arXiv preprint arXiv:2310.13625, 2023": "Arthur Douillard, Qixuan Feng, Andrei A Rusu, Rachita Chhaparia, Yani Donchev, Adhiguna Kuncoro,MarcAurelio Ranzato, Arthur Szlam, and Jiajun Shen. DiLoCo: Distributed Low-CommunicationTraining of Language Models. arXiv preprint arXiv:2311.08105, 2023. Fraser Mince, Dzung Dinh, Jonas Kgomo, Neil Thompson, and Sara Hooker.The grand illusion:The myth of software portability and implications for ml progress. Advances in Neural InformationProcessing Systems, 36, 2024.",
  "Michelle Nichols. UN Security Council meets for first time on AI risks, July 2023. on: January 8, 2023": "Renee Shelby, Shalaleh Rismani, Kathryn Henne, Ajung Moon, Negar Rostamzadeh, Paul Nicholas,YILLA-AKBARI NMAH, Jess Gallegos, Andrew Smart, and Gurleen Virk. Identifying sociotech-nical harms of algorithmic systems:Scoping a taxonomy for harm reduction.arXiv preprintarXiv:2210.05791, 2022. Irene Solaiman, Zeerak Talat, William Agnew, Lama Ahmad, Dylan Baker, Su Lin Blodgett, HalDaum III, Jesse Dodge, Ellie Evans, Sara Hooker, et al. Evaluating the Social Impact of GenerativeAI Systems in Systems and Society. arXiv preprint arXiv:2306.05949, 2023.",
  "Stuart Russell, Daniel Dewey, and Max Tegmark. Research priorities for robust and beneficial artificialintelligence. AI magazine, 36(4):105114, 2015": "Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, RyanLowe, and Joelle Pineau. Ethical challenges in data-driven dialogue systems. In Proceedings of the2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123129, 2018. Emily Dinan, Gavin Abercrombie, A Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau,and Verena Rieser. Anticipating safety issues in e2e conversational ai: Framework and tooling. arXivpreprint arXiv:2107.03451, 2021. Ross Gruetzemacher, Florian E Dorner, Niko Bernaola-Alvarez, Charlie Giattino, and David Manheim.Forecasting AI progress: A research agenda. Technological Forecasting and Social Change, 170:120909,2021.",
  "Chip Huyen. Open challenges in LLM research, August 2023. Accessed on: January 8, 2023": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, ..., and Ji-Rong Wen. A Survey of Large LanguageModels, 2023. Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, ZhonghaoHe, Jiayi Zhou, Zhaowei Zhang, et al.AI alignment: A comprehensive survey.arXiv preprintarXiv:2310.19852, 2023b."
}