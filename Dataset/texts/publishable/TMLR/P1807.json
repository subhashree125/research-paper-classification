{
  "Abstract": "With the increasing deployment of artificial intelligence (AI) technologies, the potential ofhumans working with AI agents has been growing at a great speed. Human-AI teamingis an important paradigm for studying various aspects when humans and AI agents worktogether. The unique aspect of Human-AI teaming research is the need to jointly studyhumans and AI agents, demanding multidisciplinary research efforts from machine learningto human-computer interaction, robotics, cognitive science, neuroscience, psychology, socialscience, and complex systems. However, existing platforms for Human-AI teaming researchare limited, often supporting oversimplified scenarios and a single task, or specifically focusingon either human-teaming research or multi-agent AI algorithms. We introduce CREW, aplatform to facilitate Human-AI teaming research in real-time decision-making scenariosand engage collaborations from multiple scientific disciplines, with a strong emphasis onhuman involvement. It includes pre-built tasks for cognitive studies and Human-AI teamingwith expandable potentials from our modular design. Following conventional cognitiveneuroscience research, CREW also supports multimodal human physiological signal recordingfor behavior analysis. Moreover, CREW benchmarks real-time human-guided reinforcementlearning agents using state-of-the-art algorithms and well-tuned baselines. With CREW, wewere able to conduct 50 human subject studies within a week to verify the effectiveness ofour benchmark.",
  "Introduction": "Over the past decade, significant progress in machine learning has increased the potential and necessity forhumans to collaborate and interact with Artificial Intelligence (AI) agents. Human-AI teaming has emergedas a research paradigm to explore the dynamic interactions and collaborative processes between humansand AI. By leveraging the complementary strength of both humans and AI, advancements can significantlyenhance the overall team performance. Unlike traditional AI research, which typically focuses on machine learning algorithms in isolation, Human-AI teaming requires a multidisciplinary approach to incorporate insights from various scientific domains.Numerous studies have examined human-human teaming Klimoski & Mohammed (1994) with cognitivescience, neuroscience, and psychology. Machine learning and robotics communities have extensively researchedmulti-agent machine learning Zhang et al. (2021), while team dynamics Salas et al. (2018) has been exploredin complex systems, social science, and network science. Despite the importance and potential of this research",
  "paradigm, there is still a lack of a comprehensive and unified platform to benefit research on joint effortsacross disciplines and scalable hypothesis verification": "Developing a comprehensive platform for Human-AI teaming research presents several unique challenges.Firstly, the platform needs to support diverse tasks with varying complexities with easy extensions for newtasks or modifications. While reinforcement learning research platforms Towers et al. (2023) have widelyadopted this practice, current Human-AI teaming platforms remain limited to single tasks Carroll et al.(2019); Vinyals et al. (2017). Secondly, enabling real-time communication through various modalities betweenmultiple humans and heterogeneous AI policies is essential for effective collaboration. However, existingsolutions typically support human feedback only through replaying offline trajectories and do not implementreal-time feedback mechanisms. Understanding how to build AI that can team with, learn from, be guidedby, align with, and augment humans is as crucial as modeling human behavior during interactions with AI.Therefore, the platform must provide interfaces for recording human physiological data alongside agent data,tailored for cognitive science and neuroscience studies. Furthermore, current studies often involve smallparticipant groups, making it difficult to derive generalizable conclusions. Lastly, the absence of a unifiedplatform has limited fully open-sourced studies and the establishment of appropriate benchmarks. We present CREW, a platform designed to facilitate Human-AI teaming research aiming to address the abovechallenges. We develop CREW around key principles such as extensible and open environment design, real-time communication support, hybrid Human-AI teaming modes, parallel sessions for scalable experiments, andcomprehensive human and agent data collection. Additionally, CREW incorporates highly modular algorithmcomponents compatible with practices in the machine learning community.We demonstrate CREWspotential by benchmarking real-time human-guided reinforcement learning (RL) algorithms alongside variousRL baselines. With CREW, we were able to conduct 50 human subject studies within a week. Moreover,CREW includes a set of cognitive tests to explore how individual differences among humans impact theireffectiveness in training AI agents. To our knowledge, CREW is the first platform to unify the desired featuresfor Human-AI teaming research across multiple disciplines. We aim for CREW to serve as an infrastructuralfoundation for multidisciplinary, reproducible, and scalable Human-AI teaming research.",
  "Related Work": "Human-AI Teaming Research Extensive research has explored Human-AI teaming across various domains.Machine learning studies have developed algorithms to leverage human expertise to improve the accuracyLertvittayakumjorn et al. (2020), robustness Bao et al. (2018); Ziegler et al. (2019), and interpretabilityLertvittayakumjorn et al. (2020); Bao et al. (2018); Zhou & Chen (2019) of models. Integrating humanfeedback can not only improve performance Ibarz et al. (2018); Christiano et al. (2017) but also align themodels with human preference Lertvittayakumjorn et al. (2020); Ouyang et al. (2022); Ziegler et al. (2019).Human-computer interaction research has created interfaces Weisz et al. (2021); Neerincx et al. (2018) andworkflows Bau et al. (2020) that enhance collaboration between humans and AI, combining their strengths toachieve superior performance. Ethical research focuses on understanding and mitigating the societal Hemmeret al. (2023); Chowdhury et al. (2022), ethical Ezer et al. (2019); Yin et al. (2019); Pflanzer et al. (2023), andtechnical Green & Chen (2019); Stahl et al. (2021) challenges of the rapid advancement and wide adoptionof AI. Many fields, including neuroscience, healthcare, robotics, transportation, education, security, andaccessibility, have shown growing interest Nourbakhsh et al. (2005); Henry et al. (2022); Atakishiyev et al.(2021); Nwana (1990); Pazho et al. (2023); Kumar & Jain (2022) in Human-AI teaming. Overall, the broadspectrum of interests highlights the need for multidisciplinary collaboration to drive further advancements. Human-AI Teaming Platform While significant progress has been made in Human-AI teaming research,there remains an absence of a comprehensive research platform. Overcooked Environment Carroll et al. (2019)is a simplified version of the original popular game to challenge human agents and AI agents in tasks thatrequire close coordination and strategic teamwork. StarCraft II Learning Environment (SC2LE) Vinyals et al.(2017) supports adversarial settings to allow Human-AI interaction and learning from human demonstrations.Rapid Integration and Development Environment(RIDE) USC Institute for Creative Technologies (2024)focuses on defense-related scenarios, emphasizing rapid development and integration of AI systems foroperational purposes.In addition to real-time decision-making tasks, previous research has developed",
  "RLHF-Blender Metz et al. (2023)*Uni-RLHF Yuan et al. (2024)*": "Table. 1: Human-AI teaming platforms. *Instead of real-time feedback training, RLHF-Blender and Uni-RLHF support online training where humans are presented with data from the replay buffer instead of thecurrent experience. platforms Freire et al. (2020); Metz et al. (2023); Yuan et al. (2024) that focus on offline preference or ratingsettings where humans can provide offline evaluations or corrections with imitation learning or reinforcementlearning. Overall, existing platforms have more than one of the following limitations that constrain Human-AI teamingresearch as summarized in Table. 1. Most environments only support one type of task that can be difficult toextend, and the interactions between humans and machine learning agents are limited to mouse and keyboardoperations. Moreover, most of the environments only support collecting game data, such as state, action, orreward focusing on the machine learning algorithm development, but none of them support the collection andanalysis of human physiological data (eye movement, pupillometry, brain activity, heart impulse, or naturallanguage) to understand human cognitive states and different effects on human subjects, as often required inthe cognitive studies involved in Human-AI teaming Thakur et al.; Dikker et al. (2017); Heinisch et al. (2024);Gordon et al. (2020); Koorathota et al. (2023); Xu et al. (2021). Additionally, the scale of collaborationhas been limited to two agents in a collaborative or competitive setting. Notably, most existing Human-AIteaming studies have only been evaluated on a very small number of subjects or among the authors (N < 10,mostly N < 5), which greatly limits our understanding of the state-of-the-art performance. Furthermore,common previous multi-agent environments such as OvercookedCarroll et al. (2019) and Starcraft Vinyalset al. (2017) are closed-sourced, making it unrealistic to build additional features on top of them. This alsomakes it impossible for researchers to modify the tasks to their own needs. There remains a large gap betweenthe existing platforms and the desired environment to facilitate future research. Real-Time Human-Guided Learning Most real-world decision-making processes require rapid decisionsand adaptation in real time. Therefore, real-time human-guided learning is an essential topic in Human-AIteaming research. Previous research has focused on algorithm design to integrate real-time human feedbackinto policy training. TAMER and Deep TAMER Knox & Stone (2009); Warnell et al. (2018) learn to predicthuman discrete feedback and utilize the feedback model as the one-step myopic value function for policylearning. COACH and DeepCOACH Arumugam et al. (2019); MacGlashan et al. (2016) addressed theinconsistency of human feedback by modeling it as the advantage function under an actor-critic framework.Recent progress has further refined these algorithms, addressing various challenges in human feedbackintegration, such as different feedback modalities feedback stochasticity Arakawa et al. (2018), and continuousaction spaces Sheidlower et al. (2022). Other works have explored indirectly inferring human objectives fromfeedback or preference Huang et al. (2023); Xiao et al. (2020); Christiano et al. (2017). CREW provides anextensive environment for real-time human-guided learning with online training and feedback integration,human physiological data collection, and parallel distributed experiment support.",
  "Platform Vision and Design Philosophy": "We design CREW to facilitate Human-AI teaming research. Our vision is to construct a unified platformfor researchers from diverse backgrounds, allowing them to join forces from human-AI interaction, machinelearning algorithms, workflow design, as well as human analysis and training. To achieve this, CREWincorporates the following capabilities, as illustrated in Figure. 1.",
  "ECG": "Figure. 1: CREW is a platform to facilitate Human-AI teaming research. CREW is designed under thevision of multidisciplinary collaboration research from both human and AI science. CREW allows real-timeinteraction among multi-human and multi-agents while enabling extensive data collection on both AI agentsand human agents. Extensible and open environment design. CREW is fully open-sourced. Our key contribution is buildingthe infrastructure to allow the development of scalable environments with supported Human-AI researchfeatures that scale up to very complex settings. We implemented a set of built-in example tasks described in.2 for rapid development. Many additional features and game logic can be added without modifyingcomplex infrastructure-level components. Real-time communication. While some Human-AI interaction tasks, such as human preference-basedfine-tuning, can be performed offline, many applications require online real-time interaction. Whether it istraining decision-making models with real-time human guidance or general human-AI collaboration tasks,the ability to convey messages with minimum delay is essential. Synchronizing data flow between humaninterfaces, AI algorithms, and simulation engines necessitates the establishment of a real-time communicationchannel. Hybrid Human-AI teaming support. Teaming is an essential aspect of our daily jobs. Our vision extendsthis concept to Human-AI teaming, where both humans and AI operate in teams. Increasing interest in theorganization, dynamics, workflow, and trust in multi-human and multi-AI teams highlights the need for aplatform capable of distributing and synchronizing tasks, states, and interactions across multiple environmentinstances and even across physical locations. Parallel sessions support. A key bottleneck for human-involved AI research is the requirement to conductexperiments with dozens or hundreds of human subjects to obtain trustworthy and reliable conclusions. Sucha process can be tedious and time-consuming. To enhance efficiency and scalability, CREW supports multipleindependent parallel sessions of the same setting, unconstrained by geographical locations, to obtain thecrowd-sourcing effects of large-scale experiments. This capability enables experimenters to collectively shareexperimental data and results. Comprehensive human data collection. Though human plays an important role in Human-AI teaming,our understanding of human behaviors remains limited and under-explored in existing studies. Therefore,CREW offers interfaces to simultaneously collect multi-modal human data, ranging from active instructionsand feedback to passive physiological signals. ML community-friendly algorithm design. The choice of programming language and libraries shouldalign with the customs and preferences of the ML community. The system design should be modular to allowseamless transitions between tasks and algorithms.",
  "Environments": "Tasks We select Unity as the simulation engine for CREW due to its popularity in game design and AIresearch to allow extensible and open environment design. Unity, as used by many popular commercial gamesas has a high ceiling for the complexity of game development. It also has a large user community whichmakes customized task development more accessible. Adding new components to an environment can bedone through simple drag-and-drop (on the left of Figure. 2 ). We have implemented four challenging tasks as examples. Multi-player tasks are designed for multi-agentand multi-human teaming research, and single-player tasks are designed for human-guided AI agent learningstudies. For each task, we provide both visual and structured state input options. The detailed settings aresummarized in Table. 2. Bowling is a modified version of Atari bowling where each round consists of 10 rolls and the score for eachroll is the number of pins hit. Bowling is designed to have the simplest dynamics among our tasks to serve asa rapid test for training a single agent. Find Treasure (Figure. 3A) is a single-player embodied navigationtask where the agents goal is to explore a maze and reach the treasure with randomized initial and goallocations.1v1 Hide-and-Seek Chen et al. (2020; 2021a) (Figure. 3B) is a one-on-one hide-and-seek taskwhere the seeker learns to explore the maze and catch a moving hider that follows a heuristic policy forobstacle avoidance, and run away from the seeker within line of sight. We introduce this task as an adversarialcompetition setting. NvN Hide-and-Seek (Figure. 3C) is a multiplayer version where multiple seekers andhiders can coordinate, collaborate, and compete. The hiders and seekers can either be controlled by humansor heterogeneous AI policies.",
  "TasksVisual ObservationState ObservationAction SpaceReward": "Bowlinggrayscaleball pos, pin pos, pin statusrelease pos, length before steer, steer direction# pins hitFind Treasurergbagent pos, treasure posnext loc x, next loc y-1 / step, +10 treasure found1v1 Hide-and-Seekrgbseeker pos, hider posnext loc x, next loc y-1 / step, +10 hider caughtNvN Hide-and-Seekrgbseekers pos, hiders posnext loc x, next loc yuser define Customizing new tasks or easily extend current tasks. Researchers can build tasks of much higherflexibility and complexity in CREW than existing platforms due to our efforts in setting distributed multi-agent systems mixing human and AI agents. We chose the set of tasks above only for benchmarking purposesfor human-guided reinforcement learning experiments, and it is far below the complexity limit of game designthat CREW can support. As shown in Figure. 2 on the right side, we can easily scale up the difficulty of aHide-and-Seek game to a more complex map. Another example is a recent work Ji et al. (2024) that showsthe usage of CREW for human-guided multi-agent Hide-and-Seek.",
  "Figure. 2: Left: Example of adding objects to an environment in CREW through drag-and-drop. Right:Example of scaling up the complexity of hide-and-seek to a search-and-rescue task": "Visual Observations Different visual observations create various challenges in visual embodiment learningfor both humans Vander Heyden et al. (2017); Fishbein et al. (1972) and AI agents Bandini & Zariffa (2020);Tarun et al. (2019). We provide various camera configurations to support perceptual-motor research. Asshown in Figure.3, for all our navigation and competitive tasks, we implemented visual observations frommultiple views for the users selection: a top-down fully observable view, a top-down accumulated partially",
  "observable view similar to SLAM in robotics Smith & Cheeseman (1986), a top-down egocentric view, and afirst-person view": "Procedural Generation Learning robust, generalizable, and scalable AI agents requires diverse trainingenvironments. Procedural generation allows for the creation of a wide range of environment patterns andterrain types. We provide randomized maze patterns where the grids are guaranteed to be connected(Figure. 4A) and procedural-generated terrains for more complex simulations as in Figure. 4B.",
  "CD": "Figure. 5:(A) Discretescalar feedback. (B) Optionto take control of the agentand teleoperate. (C) Con-tinuous scalar feedback: thehuman can hover the mouseover this window to provideper-step feedback. Humans and AI agents often have complementary strengths. For example,humans are generally better at exploring and adapting to new situations, whileAI agents are good at repetitive exploitation and precise calculation. Naturally, ateam consists of humans and AI agents should have various roles to be effective.Different roles can also be assigned within AI agents to study multi-agentmachine learning with heterogeneous policies. To facilitate these experiments,we provide the role assignment feature in CREW. Player allows humans to directly control an agent. Viewer allows humansto observe and provide feedback signals as guidance to an AI agent. Serverrole allows humans to host the entire task by monitoring it without directparticipation. AI Agent assigns different learning policies to each agent. Human Feedback Interface We provide a user interface to allow the Viewerrole to provide direct feedback to AI agents shown in Figure. 5. Scalar feedbackis the most common feedback type used in human-guided RL Knox & Stone(2009); Warnell et al. (2018); Xiao et al. (2020). Conventionally, a human choosesto provide a positive or negative rating to the state-action pair of an agentpolicy. Additionally, CREW offers an interface that allows humans to provide feedback that is continuousin value and time, enabling granular feedback information. Moreover, our interface also allows humans todirectly take control over agents and perform teleoperation.",
  "Multiplayer and Parallel Sessions": "Enabling multi-human multi-agent sessions requires robust networking solutions (Figure. 6). We use UnityNetcode net for game state synchronization, and Nakama nak as the networking server. In CREW, a serverinstance hosts the task, runs the simulation, and handles agent policy training, which can be executedon a powerful headless GPU server. Human participants can connect via client instances on less powerfulmachines, which display synchronized game states and collect human input. CREW is cross-platform, allowingparticipation from Linux, Windows or MacOS machines.",
  "Data collection is at the core of Human-AI teaming research. CREW includes a pipeline for thorough datacollection on both the human side and AI agent side": "Human data Prior research in Human-AI teaming and human-human teaming has leveraged physiologicalfeatures in various ways. Thakur et al. uses predicting human eye gaze as an auxiliary task for guidingimitation learning. Dikker et al. (2017) conducts brain synchrony with EEG in a classroom multi-humanteaming setup. Qin et al. (2022) leverages pupil dynamics to predict the team performance in a multi-humanvirtual reality task. Heinisch et al. (2024) uses physiological data for human-robot interaction with servicerobots. Gordon et al. (2020) explored physiological synchrony in teaming. Koorathota et al. (2023) usedgaze data to enhance vision transformers. Xu et al. (2021) uses EEG data as implicit human feedbackfor accelerating RL. We implemented the relevant features to support future research along these lines.Besides the feedback interfaces that collect feedback signals of multiple modalities and teleoperation actions,we also provide interfaces for collecting audio, eye gaze, pupillometry, electroencephalogram (EEG), andelectrocardiogram (ECG) physiological responses as in Figure. 7 with time synchronizations from each machinewith Lab Streaming Layer Kothe et al. (2024). Agent data including the policy weights, observations, actions, rewards, feedback received, and loss valuesat every time step can all be saved for further analysis. Users also have the option to enable experimentmonitoring and logging by Weights & Biases Biewald (2020). As all of our tasks include vision-based settings,we also provide implementations of a set of vision encoder architectures.",
  "Designing Algorithms": "Algorithms research is crucial for Human-AI teaming. We designed the algorithm component of CREW tobe extensible and accessible to the ML community. Algorithms are implemented in Python with PyTorchPaszke et al. (2019) and TorchRL Bou et al. (2023) which is part of the PyTorch ecosystem, making iteasy for researchers to design and deploy new algorithms. We provide implementations of state-of-the-art human-guided machine learning algorithms and strong reinforcement learning baselines. The API forcommunication between a Unity instance and Python algorithm is implemented with MLAgents Juliani et al.(2018). All environments follow a uniform communication protocol for the observation and action data acrosstasks. Our modular design allows smooth switching between tasks and algorithms. Real human experimentsare time-consuming and costly. To ease algorithm debugging, we implemented simulated human feedbackproviders as surrogates for real humans for complex tasks. These simulated feedback uses heuristics based onprior task knowledge and is not available in novel tasks in the real world; hence, they should be used solelyas debugging tools, not as replacements for real human evaluations.",
  "Figure. 8: Modular pipeline for experi-ment setup. Experimenters can freelyselect and organize the order of cogni-tive tests and Human-AI experimentswith CREWs deployment pipeline": "Individual differences among humans can significantly affect theirteaming with AI agents. To support research along this line, CREWsupports a set of cognitive tests to quantify these differences shown inTable. 3. We provide a modular and convenient pipeline (Figure. 8) forexecuting cognitive tests and Human-AI experiments. The frameworkintegrates various media files (e.g., instruction videos or pictures),inter-trial intervals, executable Python scripts, and Unity builds,ensuring a smooth and effective workflow. The pipeline requiresminimal effort from researchers during proctoring, as a single clickinitiates the sequential execution of experiments. The pipeline allowsrestart from interruption points.",
  "Benchmarking Study": "As an example of running experiments with CREW, we bench-mark a state-of-the-art real-time human-guided RL framework, DeepTAMER Warnell et al. (2018), along with strong RL baselines. Inthe original Deep TAMER, the framework was only tested on AtariBowling with 9 human subjects. With CREW, this is the first timeit is possible to systematically conduct human-guided RL benchmarking across multiple environments on alarger population. We summarize our findings as well as insights on the scalability of the framework in thissection. We also discuss the relationship between human characteristics and guided agent performance.",
  "Experiment Setup": "Tasks We selected 3 single-player games: Bowling, Find Treasure, and 1v1 Hide-and-Seek for this benchmark.For Find Treasure and Hide-and-Seek, each episode has a time limit of 15 seconds. All algorithms directlylearn from visual inputs with the top-down accumulated partially observable view. Human Trainers We recruited 50 human subjects for the experiments under the approval of the InstitutionalReview Board. We highlight that our experiment is the largest-scale study so far on real-time human-guidedAI training. For every human subject, the experiment time is approximately 40 minutes without interruptions.The experiment starts with cognitive tests (10 minutes) and is followed by the human guiding the agentusing the c-Deep TAMER (which will be discussed in the next section) framework (30 minutes). The orderof the cognitive tests is Eye Alignment, Reflex, Theory of Behavior, Mental Rotation, Mental Fitting, andSpatial Mapping. There are detailed instruction videos for each test before the test starts. As for the humanguiding agent component, each human subject guides a single agent to play 3 tasks for a total of 30 minutes(5",
  "Published in Transactions on Machine Learning Research (11/2024)": "In this experiment, watch the red ball moving for five seconds, then guess where it willbe one second after it pauses.Click on the map to mark your prediction.You have onlyone chance to click.The closer you are, the higher your score.Remember, you only havetwo seconds to make your guess after the ball pauses.There will be 6 trials in total.The time bar at the top left shows the time for each trails observation and predictionas time decreases.The bar shrinks and changes from green to red.",
  "Baseline RL We selected two state-of-the-art RL algorithms: Deep Deterministic Policy Gradient (DDPG)Lillicrap et al. (2015) and Soft Actor-Critic (SAC) Haarnoja et al. (2018)": "Heuristic feedback We also evaluate simulated feedback-guided RL. We simply add the feedback signalsas additional dense rewards to the environment reward. DDPG is selected as the backbone RL algorithmas the state-of-the-art transitioned from SAC to DDPG Yarats et al. (2021) in visual control tasks. Thehyperparameters for the heuristic baseline is set to the same as DDPG baseline. Evaluation We checkpoint model weights every 2 minutes and evaluate on unseen test environments. Forbowling, every checkpoint is evaluated for 1 game (10 rolls). For Find Treasure and 1v1 Hide-and-Seek, thecheckpoints are evaluated for 100 episodes. Processing Inputs All tasks in our experiments directly learn from pixels where humans and AI agentsshare the same input modality. The frames rendered from the environment are first resized to 100100pixels. For Find Treasure and 1v1 Hide-and-Seek, we stack the past 3 frames as input to include the historyinformation. We then apply a simple random shift to the frame stack, as it has been shown to be an effectiveaugmentation strategy for visual reinforcement learning. The shifting factors along the height and width areuniformly sampled from [0, 0.08]. Model architecture We use a 3-layer CNN as the vision encoder, each having 64 channels and followed bybatch normalization and ReLU activation function. All actor-critic frameworks use a 3-layer MLP with 256neurons in each layer for both the actor and critic network. Computational Resources All human subject experiments were conducted on desktops with one NVIDIARTX 4080 GPU. All evaluations were run on a headless server with 8 NVIDIA RTX A6000 and NVIDIARTX 3090 Ti. We note that we used larger GPU servers due to the need for parallel evaluation resultingfrom the large scale of our study, but this is not a requirement to run CREW.",
  "Results": "Agent training performance We hypothesize that subjects with higher cognitive tests can lead to higher-performing agents. Therefore, we show the agent performances guided by the 15 subjects who scored thehighest in our cognitive tests side by side with the performance of all 50 subjects in Figure. 9. As shown inthe results, the agents guided by the top 15 subjects exhibit higher performance than the overall average.This is most prominent in Find Treasure, where c-Deep TAMER (blue curve) is significantly higher in theTop 15 plot than the All 50 plot. As shown in Figure. 10, the correlation between cognitive test scores andguided AI performance is positive for all tasks, where Find Treasure had a strong statistical significance (**,p < 0.01). We do not draw a general conclusion from these early explorations. We hope to provide a usefulmetric and experiment to study individual differences for future work, exemplified by our benchmarkingresults. We highlight our concurrent work Zhang et al. (2024) to use them in human-guided RL evaluation.",
  "EyeTheoryRotationFitnessReexOverall": "Figure. 10: Correlation between cognitive test scoresand c-Deep TAMER training performance. The darkerthe color, the more statistically significant the correla-tion. + or - means positive or negative correlation.Overall, the total score (i.e., the sum of three gamescores) has significant positive correlations with rota-tion, fitness, and overall score. This is likely a resultof the strong contribution of Find Treasure since othertasks only show positive correlations with certain cog-nitive scores but not statistically significant. Analysis of Individual Differences Due to thecognitive test feature and emphasis on Human-AIteaming, we can deepen our understanding of howindividual human differences can affect the perfor-mance of human-guided agents. We calculated thecorrelation between human subjects cognitive testscores and c-Deep TAMER training results in Fig-ure. 10. The cognitive test scores are normalizedby the mean and variance over the subjects throughz-score. Following Tukey (1977), we remove outlierswith scores 1.5 Interquartile Range above the thirdquartile or below the first quartile are removed. Wefound that non-outlier subjects performed equallywell on the spatial mapping test so we do not includethis for correlation analysis. The heatmap shows thesign of correlation coefficients from linear regressionand the statistical significance (*). The Find Treasure score has a significant positivecorrelation with theory of behavior, mental rota-tion, and fitness scores, suggesting that subjects whoperformed better on these tests also trained betteragents. This is likely due to the need for good spa-tial reasoning and future behavior estimation in FindTreasure. The 1v1 Hide-and-Seek score shows a sig-nificant positive correlation with reflex score, likelydue to the rapid changes in environment and task dynamics in the complex adversarial setting. Subjects withfaster reaction speeds provided timely feedback to align with relevant state-action pairs. Overall, the totalscore (i.e., the sum of three game scores) has significant positive correlations with rotation, fitness, and overallscore, suggesting that these cognitive skills are most relevant to the performance of c-Deep TAMER-guidedagents.",
  "Conclusion, Limitation, and Future Work": "We introduce CREW for facilitating Human-AI teaming research from diverse human and machine learningscientific communities. CREW offers extensible environment design, enables real-time human-AI communi-cation, supports hybrid Human-AI teaming, parallel sessions, multimodal feedback, and physiological datacollection, and features ML community-friendly algorithm design. We also provide a set of built-in tasks andbaseline algorithm implementations. Using CREW, we benchmarked a state-of-the-art human-guided RLalgorithm against baseline methods involving 50 human subjects and provided insights into the relationshipbetween individual human differences and agent-guiding performance. CREW is still in the early efforts among several critical aspects. Future work will explore building more diverseand challenging tasks, including multiplayer tasks with complex strategies and robotics environments requiringan understanding of physics. While we have only benchmarked several algorithms, we hope CREW can helpbenchmark many existing algorithms that were not fully open-sourced in a unified environment. Finally, moresupports on human physiological data processing and analysis shall be investigated and supported in CREW.",
  "Yujia Bao, Shiyu Chang, Mo Yu, and Regina Barzilay. Deriving machine attention from human rationales.arXiv preprint arXiv:1808.09367, 2018": "David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and Antonio Torralba.Semantic photo manipulation with a generative image prior. arXiv preprint arXiv:2005.07727, 2020. Michal Berkowitz, Andri Gerber, Christian M Thurn, Beatrix Emo, Christoph Hoelscher, and Elsbeth Stern.Spatial abilities for architecture: Cross sectional and longitudinal assessment with novel and existing spatialability tests. Frontiers in psychology, 11:609363, 2021.",
  "Christopher J Boes. The history of examination of reflexes. Journal of neurology, 261(12):22642274, 2014": "Albert Bou, Matteo Bettini, Sebastian Dittert, Vikash Kumar, Shagun Sodhani, Xiaomeng Yang, GianniDe Fabritiis, and Vincent Moens. Torchrl: A data-driven decision-making library for pytorch. arXivpreprint arXiv:2306.00577, 2023. Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan. Onthe utility of learning about humans for human-ai coordination. Advances in neural information processingsystems, 32, 2019. Boyuan Chen, Shuran Song, Hod Lipson, and Carl Vondrick. Visual hide and seek. In Artificial LifeConference Proceedings 32, pp. 645655. MIT Press One Rogers Street, Cambridge, MA 02142-1209, USAjournals-info . . . , 2020. Boyuan Chen, Yuhang Hu, Robert Kwiatkowski, Shuran Song, and Hod Lipson. Visual perspective takingfor opponent behavior modeling. In 2021 IEEE International Conference on Robotics and Automation(ICRA), pp. 1367813685. IEEE, 2021a.",
  "Ben Green and Yiling Chen. The principles and limits of algorithm-in-the-loop decision making. Proceedingsof the ACM on Human-Computer Interaction, 3(CSCW):124, 2019": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor. In International conference on machinelearning, pp. 18611870. PMLR, 2018. Judith S Heinisch, Jrme Kirchhoff, Philip Busch, Janine Wendt, Oskar von Stryk, and Klaus David.Physiological data for affective computing in hri with anthropomorphic service robots: the affect-hri dataset. Scientific Data, 11(1):333, 2024. Patrick Hemmer, Monika Westphal, Max Schemmer, Sebastian Vetter, Michael Vssing, and Gerhard Satzger.Human-ai collaboration: The effect of ai delegation on human task performance and task satisfaction. InProceedings of the 28th International Conference on Intelligent User Interfaces, pp. 453463, 2023. Katharine E Henry, Rachel Kornfield, Anirudh Sridharan, Robert C Linton, Catherine Groh, Tony Wang,Albert Wu, Bilge Mutlu, and Suchi Saria. Humanmachine teaming is key to ai adoption: cliniciansexperiences with a deployed machine learning system. NPJ digital medicine, 5(1):97, 2022. Jie Huang, Jiangshan Hao, Rongshun Juan, Randy Gomez, Keisuke Nakamura, and Guangliang Li. Gan-basedinteractive reinforcement learning from demonstration and human evaluative feedback. In 2023 IEEEInternational Conference on Robotics and Automation (ICRA), pp. 49914998. IEEE, 2023. Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward learningfrom human preferences and demonstrations in atari. Advances in neural information processing systems,31, 2018.",
  "Piyawat Lertvittayakumjorn, Lucia Specia, and Francesca Toni. FIND: human-in-the-loop debugging deeptext classifiers. CoRR, abs/2010.04987, 2020. URL": "Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver,and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971,2015. James MacGlashan, Michael L Littman, David L Roberts, Robert Loftin, Bei Peng, and Matthew E Taylor.Convergent actor critic by humans. In International Conference on Intelligent Robots and Systems, 2016. Yannick Metz, David Lindner, Raphal Baur, Daniel Keim, and Mennatallah El-Assady. Rlhf-blender: Aconfigurable interactive interface for learning from diverse human feedback. arXiv preprint arXiv:2308.04332,2023. Mark A Neerincx, Jasper van der Waa, Frank Kaptein, and Jurriaan van Diggelen. Using perceptual andcognitive explanations for enhanced human-agent team performance. In Engineering Psychology andCognitive Ergonomics: 15th International Conference, EPCE 2018, Held as Part of HCI International2018, Las Vegas, NV, USA, July 15-20, 2018, Proceedings 15, pp. 204214. Springer, 2018.",
  "Hyacinth S Nwana. Intelligent tutoring systems: an overview. Artificial Intelligence Review, 4(4):251277,1990": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in neural information processing systems, 35:2773027744, 2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in neural information processing systems, 32, 2019. Armin Danesh Pazho, Christopher Neff, Ghazal Alinezhad Noghre, Babak Rahimi Ardabili, Shanle Yao,Mohammadreza Baharani, and Hamed Tabkhi. Ancilia: Scalable intelligent video surveillance for theartificial intelligence of things. IEEE Internet of Things Journal, 2023.",
  "Randall C Smith and Peter Cheeseman. On the representation and estimation of spatial uncertainty. Theinternational journal of Robotics Research, 5(4):5668, 1986": "Bernd Carsten Stahl, Andreas Andreou, Philip Brey, Tally Hatzakis, Alexey Kirichenko, Kevin Macnish,S Laulh Shaelou, Andrew Patel, Mark Ryan, and David Wright.Artificial intelligence for humanflourishingbeyond principles for machine learning. Journal of Business Research, 124:374388, 2021. Aneesh P Tarun, Nauman M Baig, Jack Chang, Rabia Tanvir, Sumaiyah Shihipar, and Ali Mazalek. Third eye:Exploring the affordances of third-person view in telepresence robots. In Social Robotics: 11th InternationalConference, ICSR 2019, Madrid, Spain, November 2629, 2019, Proceedings 11, pp. 707716. Springer,2019. Ravi Kumar Thakur, MD Sunbeam, Vinicius G Goecks, Ellen Novoseller, Ritwik Bera, Vernon Lawhern,Greg Gremillion, John Valasek, and Nicholas R Waytowich. Imitation learning with human eye gaze viamulti-objective prediction. Mark Towers, Jordan K. Terry, Ariel Kwiatkowski, John U. Balis, Gianluca de Cola, Tristan Deleu, ManuelGoulo, Andreas Kallinteris, Arjun KG, Markus Krimmel, Rodrigo Perez-Vicente, Andrea Pierr, SanderSchulhoff, Jun Jet Tai, Andrew Tan Jin Shen, and Omar G. Younis. Gymnasium, March 2023. URL",
  "USC Institute for Creative Technologies. Rapid integration & development environment (ride), 2024. URL Accessed: 2024-05-27": "Karin M Vander Heyden, Mariette Huizinga, Maartje EJ Raijmakers, and Jelle Jolles. Childrens represen-tations of another persons spatial perspective: Different strategies for different viewpoints? Journal ofexperimental child psychology, 153:5773, 2017. Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo,Alireza Makhzani, Heinrich Kttler, John Agapiou, Julian Schrittwieser, et al. Starcraft ii: A new challengefor reinforcement learning. arXiv preprint arXiv:1708.04782, 2017. Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep tamer: Interactive agentshaping in high-dimensional state spaces. In Proceedings of the AAAI conference on artificial intelligence,volume 32, 2018. Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, MayankAgarwal, and Kartik Talamadupula. Perfection not required? human-ai partnerships in code translation.In 26th International Conference on Intelligent User Interfaces, pp. 402412, 2021. Baicen Xiao, Qifan Lu, Bhaskar Ramasubramanian, Andrew Clark, Linda Bushnell, and Radha Poovendran.Fresh: Interactive reward shaping in high-dimensional state spaces using human feedback. arXiv preprintarXiv:2001.06781, 2020.",
  "General Introduction:": "Welcome to the XXX Human AI Collaboration Experiment.Today, our session will startwith preliminary cognitive and gaming proficiency tests to gauge your initial skills.Following this, we will delve into the main experiments where you will interact with AIalgorithms through a series of engaging games.Our session will conclude with a shortsurvey to capture your feedback on the experience.Your participation is invaluable inadvancing our understanding of human AI interactions.Thank you for joining us today,and lets begin.",
  "Cognitive Test Introduction:": "Welcome to the Cognitive Test segment of our experiment.In this session, you willparticipate in five interactive games designed to assess various cognitive skills forabout seven minutes.These tests will challenge your precision, reflexes, predictiveabilities, problem-solving skills, and spatial awareness through engaging activities.Each test is brief, and youll receive clear instructions before each one begins.Between each trial of each game, there will be a three-second intertrial interval wherethe computer screen will turn white with a Gray cross in the middle.Please focus on thecenter of the cross as much as possible during this time.Lets get started and see howyou do.",
  "Eye Alignment Instruction:": "In this experiment, your goal is to align the ball positioned on the left side of thescreen with the square on the right as accurately as possible.Each trial lasts for fiveseconds, and youll have a total of six trials.Use your mouse to drag the ball duringeach trial.The time bar at the top center of the screen shows how much time you havefor each trail.As time goes down, the bar gets smaller and changes color from green tored.",
  "Reflex Instruction:": "In this experiment, after a three-second countdown, the screen turns yellow.You mustclick as quickly as possible when the screen changes from yellow to green.Clickingduring the yellow phase will result in a failure, and failing to click within two secondsafter the screen turns green will also fail.Youll have a total of 6 trials.",
  "Mention Rotation and Mental Fitting Instruction:": "In this experiment, you will need to answer 12 questions in total, each with only onecorrect answer.Click the button to choose the option that you think best answers thequestion.For each question, you have 8 seconds to view and respond.The time bar atthe top right of the screen indicates how much time you have for each question.As timedecreases, the bar shrinks and changes color from green to red.",
  "Spatial Mapping Instruction:": "In this experiment, you will need to answer six questions in total, each with only onecorrect answer.Watch the video and click the button to choose the option that you thinkbest answers the question.For each question, you are free to answer while the video isplaying, and you will also have three extra seconds after the video is paused to answerthe question.The time bar at the top right of the screen indicates how much time youhave for each question.As time decreases, the bar shrinks and changes color from greento red.",
  "Human Guiding AI Introduction:": "Welcome to the Human-Guiding AI section of the experiment.You will guide the AI to playthree games for about 30 minutes in total in this section.Youll give feedback to theAI agents based on their performance in each game.Before starting each game, pleaseenter your name in the name box and click on the Connect button.A match list willappear.Click on the Match button in the list.It will turn brown when clicked.Thenclick the Join Match button to enter the match.Once youve joined the match, select theAI agent in the Active Clients panel at the top right corner of the screen by clicking onit.The agent button will turn brown to indicate that youve selected it successfully.Now you can observe the agents and provide feedback.Youll constantly click on thepositive and negative buttons to inform the agent of its performance.Positive meansits doing well.Negative means its doing poorly.",
  "Bowling Instruction:": "Each episode of this bowling game consists of 10 rolls.At the start of each roll, thereare 10 pins positioned on the right side of the screen.The agent then launches a ballin an attempt to knock down the pins, with the number of pins knocked down determiningthe score for that roll.The total score for the episode is calculated as the sum of thescores from all 10 rolls.Please guide the AI agent to maximize the total score acrossall rolls.",
  "Find Treasure Instruction:": "In this game, the AI agent, represented by the green character, begins with a limitedfield of view, only able to perceive surroundings, while the rest of the map is obscuredby shadow.As the agent navigates the map, areas it has visited become revealed withinits field of view.Please guide the AI agent to locate the treasure and navigate to it,represented by the brown chest, as quickly as possible."
}