{
  "Abstract": "Diffusion generative models have recently become a powerful technique for creating andmodifying high-quality, coherent video content.This survey provides a comprehensiveoverview of the critical components of diffusion models for video generation, includingtheir applications, architectural design, and temporal dynamics modeling. The paper be-gins by discussing the core principles and mathematical formulations, then explores var-ious architectural choices and methods for maintaining temporal consistency.A taxon-omy of applications is presented, categorizing models based on input modalities such astext prompts, images, videos, and audio signals. Advancements in text-to-video genera-tion are discussed to illustrate the state-of-the-art capabilities and limitations of currentapproaches. Additionally, the survey summarizes recent developments in training and eval-uation practices, including the use of diverse video and image datasets and the adoptionof various evaluation metrics to assess model performance. The survey concludes with anexamination of ongoing challenges, such as generating longer videos and managing com-putational costs, and offers insights into potential future directions for the field. By con-solidating the latest research and developments, this survey aims to serve as a valuableresource for researchers and practitioners working with video diffusion models. Website:",
  "Introduction": "Diffusion generative models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al.,2021; Ruiz et al., 2024) have already demonstrated a remarkable ability for learning heterogeneous visualconcepts and creating high-quality images conditioned on text descriptions (Rombach et al., 2022; Rameshet al., 2022). Recent developments have also extended diffusion models to video (Ho et al., 2022c), with thepotential to revolutionize the generation of content for entertainment or simulating the world for intelligentdecision-making (Yang et al., 2023a). For example, the text-to-video SORA (Brooks et al., 2024) model hasbeen able to generate high-quality videos up to a minute long conditional on a users prompt. Following",
  "Published in Transactions on Machine Learning Research (11/2024)": "Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generatingvideos with dynamics-aware implicit generative adversarial networks. arXiv preprint arXiv:2202.10571,2022. Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh, ClaytonTan, Dee M, Jodilyn Peralta, Brian Ichter, Karol Hausman, and Fei Xia. Scaling robot learning withsemantically imagined experience, 2023. Samin Zare and Mehran Yazdi. A survey on semi-automated and automated approaches for video annotation.In 2022 12th International Conference on Computer and Knowledge Engineering (ICCKE), pp. 404409,2022.",
  "Taxonomy of Applications": "The possible applications of video diffusion models can be roughly categorized according to input modalities.This includes text prompts, images, videos, and auditory signals. Many models also accept inputs that area combination of some of these modalities. visualizes the different applications. We summarizenotable papers in each application domain starting from Sec. 7.1.3. For this, we have categorized each modelaccording to one main task. In our taxonomy, text-conditioned generation (Sec. 7.1.3) refers to the task of generating videos purely basedon text descriptions. Different models show varying degrees of success in how well they can model object-specific motion. We thus categorize models into two types: those capable of producing simple movementssuch as a slight camera pan or flowing hair, and those that can represent more intricate motion over time,such as those incorporating Physical Reasoning (Melnik et al., 2023).",
  "Mathematical Formulation": "We first review the mathematical formulation of diffusion generative models, which learn to model a targetdistribution, for example, of natural videos, by progressive noise injection in the forward process and reverseddenoising in the backward process, as shown in . A diffusion model generates samples via a chain ofdenoising steps that start from an initial noise vector that is often a sample from a Gaussian distributionof uncorrelated white noise. Each denoising step is performed by a neural network that has been trained toguide a noisy input toward the target distribution. After a number of such denoising steps, the obtained resultwill approximate a noise-free sample of the target domain. The key to this mechanisms success is traininga suitable denoising network. This is achieved by an objective that learns to reverse the forward noisingprocess at the corresponding noise levels. Broadly speaking, diffusion models can be categorized into twofamilies of formulations: denoising diffusion probabilistic models (DDPM) (Ho et al., 2020; Sohl-Dicksteinet al., 2015) and score-based model (SBM), the latter of which includes the denoising score matching modelswith Langevin dynamics (SMLD) (Song & Ermon, 2019; 2020) and the generalized Score SDE (Song et al.,2021). We give a brief introduction to each of the modeling designs below.",
  "Denoising Diffusion Probabilistic Model (DDPM) Formulation": "In the following, we summarize the formalization of the unconditioned denoising diffusion probabilistic model(DDPM) process from Ho et al. (2020); Sohl-Dickstein et al. (2015). The forward diffusion process followsa Markov chain that iteratively adds sampled noise to an initial input video x0 over T time steps, so thatit resembles samples drawn from a Gaussian distribution. The Markov property ensures that the degradedvideo xt at time step t only depends on the video xt1 in the immediately preceding step t 1.Thedistribution q(xt|xt1) of xt in a forward step can be described by the conditional Gaussian transition kernel",
  ".(7)": "This formulation has the advantage that we can calculate closed-form solutions for the Kulback-Leiblerterms. First, the conditional distribution q(xT |x0) admits an analytical solution in Eq. 3, and the reverseprocess prior p(xT ) is fixed, making the first term DKL(q(xT |x0)p(xT )) a constant. One can design thenoise schedule properly so that the terminal state in the forward process q(xT |x0) is empirically close to theprior in the reverse process p(xT ) to lower this term.",
  "1txt and t := 1t1": "1t t. The first equality follows from Bayesrule, the second from the Markovian property of the forward process, and the third is a rearrangement ofGaussian density functions. Namely, the second loss term DKL(q(xt1|xt, x0)p(xt1|xt)) from Eq. 7 isalso tractable, as it characterizes the KL divergence between two Gaussians and has analytical solutions.",
  "Lsimple := Et,x0, (xt, t)22,(9)": "that performs better in practice. Here, is called a denoising model, or denoiser, which predicts thestandard normal noise added to the input given the input xt at time step t. The denoising model is oftenformulated as a UNet (Ronneberger et al., 2015) or a Vision Transformer (Dosovitskiy et al., 2020) (seeSec. 4). A more detailed description of how video diffusion models perform such a denoising process can befound in Sec. 5. To generate samples using DDPM, we utilize the reverse process outlined in Eq. 4 and Eq. 5. This reverseMarkov chain allows us to generate a data sample x0 by initially sampling a noise vector xT p(xT ), andthen iteratively sampling from the learnable transition kernel xt1 p(xt1 | xt) until t = 1.The originalDDPM generation process has more recently been complemented by a non-Markovian alternative denoted asdenoising diffusion implicit models (DDIM, Song et al. 2020), which offers a deterministic and more efficientgeneration process. Here, a backward denoising step can be computed with",
  "t1(xt, t).(11)": "One distinct advantage of DDIM is that it allows for accurate reconstruction of the original input video x0from the noise at arbitrary time step t.To accelerate the generation process, we consider defining thereverse process not on all the intermediate latent variables x1:T , but on a subset {x1, . . . , xS}, where isan increasing subsequence of [1, . . . , T] of length S. Specifically, we define the sequential forward processover x1, . . . , xS such that q(xi | x0) = N(xi; ix0, (1 i)I) matches the marginals. We can thendraw samples following the sequence xS x(S)0 xS1 x1.This technique, called DDIMinversion, can be utilized for applications such as image and video editing (see Sec. 10).",
  "Score-based Model Formulation": "Score-based models (SBMs) (Song & Ermon, 2019; Song et al., 2021) center on the concept of the Stein score,or score function (Hyvrinen, 2005). For a given probability density function p(x), the score function is thegradient of the log probability density, x log p(x). This score function depends on the data x and indicatesthe direction of the steepest increase in the probability density. SBMs add increasing Gaussian noise todata and estimate the score functions for all noisy data distributions by training a noise-conditional scorenetwork (Vincent, 2011). Samples are then generated by chaining these score functions at decreasing noiselevels using methods like Langevin dynamics (Parisi, 1981), stochastic differential equations (SDEs) (Songet al., 2021) and ordinary differential equations (ODEs) (Karras et al., 2022). Unlike DDPM, SBMs decoupletraining and sampling, allowing for various sampling techniques after the score functions are estimated. Belowwe focus on the two most representative types of SBMs: denoising score matching with Langevin dynamics(SMLD) and score-based stochastic differential equations (Score SDEs). Denoising Score Matching with Langevin Dynamics. In denoising score matching with Langevindynamics (SMLD), we denote the data distribution by p(x0) and use a sequence of noise levels 0 < 1 <2 < < T for score estimation. A data point x0 is perturbed to xt by Gaussian noise, p(xt | x0) =N(xt; x0, 2t I). This process creates a series of noisy data distributions p(x1), p(x2), . . . , p(xT ), where p(xt) =p(xt | x0)p(x0) dx0. A noise-conditional score network, s(x, t), is trained to estimate the score functionxt log p(xt) using denoising score matching (Vincent, 2011) with the following training objective",
  "where st > 0 is the step size and N is the number of iterations per step. As st 0 and N , x(N)0converges to a valid sample from the data distribution p(x0)": "Score-based Stochastic Differential Equations. Models like DDPM and SMLD can be extended toinfinite time steps or noise levels using stochastic differential equations (SDEs), forming the Score SDE (Songet al., 2021). This approach uses SDEs for noise perturbation and sample generation, requiring the estimationof score functions of noisy data distributions. Score SDEs perturb data with a forward process governed by:",
  "where U[0, T] denotes the uniform distribution over [0, T], and other notations follow the definition in Eq. 12": "With a trained score estimation network, we can solve the reverse-time SDE and probability flow ODE togenerate samples. Methods include annealed Langevin dynamics Song & Ermon (2019), numerical SDEsolvers Song et al. (2021), numerical ODE solvers Song et al. (2021); Karras et al. (2022), and predictor-corrector methods Song et al. (2021), effectively solving the reverse processes.",
  "UNet": "The UNet (Ronneberger et al., 2015) is currently the most popular architectural choice for the denoiserin visual diffusion models (see ).Originally developed for medical image segmentation, it hasmore recently been successfully adapted for generative tasks in image, video, and audio domains. A UNettransforms an input image into an output image of the same size and shape by encoding the input first intoincreasingly lower spatial resolution latent representations while increasing the number of feature channelsby progressing through a fixed number of encoding layers. Then, the resulting middle latent representationis upsampled back to its original size through the same number of decoding layers.While the originalUNet (Ronneberger et al., 2015) only used ResNet blocks, most diffusion models interleave them with VisionTransformer blocks in each layer.The ResNet blocks mainly utilize 2D-Convolutions, while the VisionTransformer blocks implement spatial self-attention, as well as cross-attention. This happens in a way thatallows conditioning of the generative process on additional information such as text prompts and currenttimestep. Layers of the same resolution in the encoder and decoder part of the UNet are connected throughresidual connections. The UNet can be trained by the process outlined in Sec. 3.",
  "Vision Transformer": "The Vision Transformer (ViT, Dosovitskiy et al. (2020)) is an important building block of generative dif-fusion models based on the transformer architecture developed for natural language processing (Vaswani : The denoising UNet architecture typically used in text-to-image diffusion models. The modeliteratively predicts a denoised version of the noisy input image. The image is processed through a numberof encoding layers and the same number of decoding layers that are linked through residual connections.Each layer consists of ResNet blocks implementing convolutions, as well as Vision Transformer self-attentionand cross-attention blocks. Self-attention shares information across image patches, while cross-attentionconditions the denoising process on text prompts.",
  "Cascaded Diffusion Models": "Cascaded Diffusion Models (CDM, Ho et al. 2022b) consist of multiple UNet models that operate at increasingimage resolutions. By upsampling the low-resolution output image of one model and passing it as input tothe next model, a high-fidelity version of the image can be generated. At training time, various forms ofdata augmentation are applied to the outputs of one denoising UNet model before it is passed as input to thenext model in the cascade. These include Gaussian blurring, as well as premature stopping of the denoisingprocess (Ho et al., 2022b). The use of CDMs has largely vanished after the adaptation of Latent DiffusionModels (Rombach et al., 2022) that allow for native generation of high-fidelity images with lower resources.",
  "Latent Diffusion Models": "Latent Diffusion Models (LDM, Rombach et al. (2022)) have been an important development of the baseUNet architecture that now forms the de-facto standard for image and video generation tasks. Instead ofoperating in RGB space, the input image is first encoded into a latent representation with lower spatialresolution and more feature channels using a pre-trained vector-quantized variational auto-encoder (VQ-VAE, Van Den Oord et al. (2017)). This low-resolution representation is then passed to the UNet where thewhole diffusion and denoising process takes place in the latent space of the VQ-VAE encoder. The denoisedlatent is then decoded back to the original pixel space using the decoder part of the VQ-VAE. By operatingin a lower-dimensional latent space, LDMs can save significant computational resources, thus allowing themto generate higher-resolution images compared to previous diffusion models. Stable Diffusion 1 is a canonical",
  "Temporal Dynamics": "In diffusion-based video generation, the UNet/ViT models described in Sec. 4 are used as denoising models( in Eq. 9) to predict the noise added to the input video clip. Unlike image diffusion models that generateeach image separately, video diffusion models often perform denoising over a set of frames simultaneously.In other words, the input xt to the video diffusion model represents an n-frame video clip (e.g. n = 16).Formally, given a noisy video clip (or video latent if using latent diffusion models) xt at time step t, theUNet/ViT model predicts the noise added to the video clip/latent, which is then used to derive a lessnoisy version of the video xt1. This denoising process can be repeated until the clean video clip/latent x0is obtained. Text-to-image models such as Stable Diffusion can produce realistic images, but extending them for videogeneration tasks is not trivial (Ho et al., 2022c). If we try to naively generate individual video frames from atext prompt, the resulting sequence has no spatial or temporal coherence (see a). For video editingtasks, we can extract spatial cues from the original video sequence and use them to condition the diffusionprocess. In this way, we can produce fluid motion of objects, but temporal coherence still suffers due tochanges in the finer texture of objects (see b). In order to achieve spatio-temporal consistency, videodiffusion models need to share information across video frames. The most obvious way to achieve this is toadd a third temporal dimension to the denoising model. ResNet blocks then implement 3D convolutions,while self-attention blocks are turned into full cross-frame attention blocks. This type of full 3D architectureis however associated with very high computational costs. : Limitations of text-to-video diffusion models for generating consistent videos. (Top) When usingonly a text prompt (Michael Jordan running), both the appearance and position of objects change wildlybetween video frames. (Bottom) Conditioning on spatial information from a reference video can produceconsistent movement, but the appearance of objects and the background still fluctuate between video frames.",
  "Spatio-Temporal Attention Mechanisms": "In order to achieve spatial and temporal consistency across video frames, most video diffusion models modifythe self-attention layers in the UNet model. These layers consist of a vision transformer that computes theaffinity between a query patch of an image and all other patches in that same image. This basic mechanismcan be extended in several ways (see Wang et al. 2023b for a discussion): In temporal attention (Honget al., 2022; Singer et al., 2022), the query patch attends to patches at the same location in other videoframes. In full spatio-temporal attention (Zhang & Agrawala, 2023; Bar-Tal et al., 2024), it attends to allpatches in all video frames. In causal attention, it only attends to patches in all previous video frames. Insparse causal attention (Wu et al., 2022b), it only attends to patches in a limited number of previous frames,typically the first and immediately preceding one. The different forms of spatio-temporal attention differ inhow computationally demanding they are and how well they can capture motion. Additionally, the qualityof the produced motion greatly depends on the used training strategy and data set.",
  "Temporal Upsampling": "Generating long video sequences in a single batch often exceeds the capacity of current hardware. Whiledifferent techniques have been explored to reduce the computational burden (such as sparse causal attention,Wu et al. 2022b), most models are still limited to generating video sequences that are no longer than a fewseconds even on high-end GPUs. To get around this limitation, many works have adapted a hierarchicalupsampling technique whereby they first generate spaced-out key frames. The intermediate frames can thenbe filled in by either interpolating between neighboring key frames, or using additional passes of the diffusionmodel conditioned on two key frames each. As an alternative to temporal upsampling, the generated video sequence can also be extended in an auto-regressive manner (Blattmann et al., 2023b). Hereby, the last generated video frame(s) of the previous batchare used as conditioning for the first frame(s) of the next batch. While it is in principle possible to arbitrarilyextend a video in this way, the results often suffer from repetition and quality degradation over time.",
  "Structure Preservation": "Video-to-video translation tasks typically strive for two opposing objectives: Maintaining the coarse structureof the source video on the one hand, while introducing desired changes on the other hand. Adhering to thesource video too much can hamper a models ability to perform edits, while strolling too far away fromthe layout of the source video allows for more creative results but negatively impacts spatial and temporalcoherence. A common approach for preserving the coarse structure of the input video is to replace the initial noise inthe denoising model with (a latent representation of) the input video frames (Wu et al., 2022b). By varyingthe amount of noise added to each input frame, the user can control how closely the output video shouldresemble the input, or how much freedom should be granted while editing it. In practice, this method initself is not sufficient for preserving the more fine-grained structure of the input video and is therefore usuallyaugmented with other techniques. For one, the outlines of objects are not sufficiently preserved when addinghigher amounts of noise. This can lead to unwanted object warping across the video. Furthermore, finerdetails can shift over time if information is not shared across frames during the denoising process. These shortcomings can be mitigated to some degree by conditioning the denoising process on additionalspatial cues extracted from the original video. For instance, specialized diffusion models have been trainedto take into account depth estimates2. ControlNet (Zhang & Agrawala, 2023) is a more general extension forStable Diffusion that enables conditioning on various kinds of information, such as depth maps, OpenPoseskeletons, or lineart. A ControlNet model is a fine-tuned copy of the encoder portion of the Stable Diffusiondenoising UNet that can be interfaced with a pre-trained Stable Diffusion model. Image features are ex-tracted using a preprocessor, encoded through a specialized encoder, passed through the ControlNet model,and concatenated with the image latents to condition the denoising process. Multiple ControlNets can becombined in an arbitrary fashion. Several video diffusion models have also implemented video editing thatis conditioned on extracted frame features such as depth (Ceylan et al. 2023; Esser et al. 2023; Xing et al.2023a, see Sec. 10.2) or pose estimates (Ma et al. 2023; Zhao et al. 2023, see Sec. 10.3).",
  "TikTok (2022)6401080TikTokDepth34010-15 sec.0.86": "trained image model (e.g. Zhou et al. 2022, Khachatryan et al. 2023, Blattmann et al. 2023b). It is possibleto train a model completely on labeled video data, whereby it learns associations between text prompts andvideo contents as well as temporal correspondence across video frames (e.g. Ho et al. 2022c). However,large data sets of labeled videos (e.g. Bain et al. 2021, Xue et al. 2022, see .1) tend to be smallerthan pure image data sets and may include only a limited range of content. Additionally, a single text labelper video may fail to describe the changing image content across all frames. At a minimum, automaticallycollected videos need to be divided into chunks of suitable length that can be described with a single textannotation and that are free of unwanted scene transitions, thereby posing higher barriers for uncuratedor weakly curated data collection. For that reason, training is often augmented with readily available datasets of labeled images (e.g. Russakovsky et al. 2015, Schuhmann et al. 2022, see .2). This allows agiven model to learn a broader number of relationships between text and visual concepts. Meanwhile, thespatial and temporal coherence across frames can be trained independently on video data that can even beunlabeled (Zhou et al., 2022). In contrast to models that are trained from scratch (e.g. Ho et al. 2022c, Singer et al. 2022, Ho et al. 2022a),recent video diffusion approaches (e.g. Zhou et al. 2022, Khachatryan et al. 2023, Blattmann et al. 2023b)often rely on a pre-trained image generation model such as Stable Diffusion (Rombach et al. 2022). Thesemodels show impressive results in the text-to-image (Rombach et al., 2022; Ramesh et al., 2022) and imageediting domains (Brooks et al., 2023; Zhang & Agrawala, 2023), but are not built with video generation inmind. For this reason, they have to be adjusted in order to yield results that are spatially and temporallycoherent. One possibility to achieve this is to add new attention blocks or to tweak existing ones so thatthey model the spatio-temporal correspondence across frames. Depending on the implementation, theseattention blocks either re-use parameters from the pre-trained model, are fine-tuned on a training data setconsisting of many videos, or only on a single input video in the case of video-to-video translation tasks.During fine-tuning, the rest of the pre-trained models parameters are usually frozen in place. The differenttraining methods are shown in .",
  "offers an overview of commonly used video data sets for training and evaluation of video diffusionmodels": "WebVid-10M (Bain et al., 2021) is a large data set of text-video pairs scraped from the internet thatcovers a wide range of content. It consists of 10.7 million video clips with a total length of about 52,000hours. It is an expanded version of the WebVid-2M data set, which includes 2.5 million videos with anaverage length of 18 seconds and a total play time of 13,000 hours. Each video is annotated with an HTMLAlt-text which normally serves the purpose of making it accessible to vision-impaired users. The videosand their Alt-texts have been selected based on a filtering pipeline similar to that proposed in Sharma et al.(2018). This ensures that the videos have sufficiently high resolution, normal aspect ratio, and lack profanity.Additionally, only well-formed Alt-text that is aligned with the video content is selected (as judged by aclassifier). WebVid-10M is only distributed in the form of links to the original video sources, therefore it ispossible that individual videos that have been taken down by their owners are no longer accessible. HD-Villa-100M (Xue et al., 2022) contains over 100 million short video clips extracted from about 3.3million videos found on YouTube. The average length of a clip is 13.4 seconds with a total run time of about371.5 thousand hours. All videos have a high-definition resolution of 1280 720 pixels and are paired withautomatic text transcriptions. Along with WebVid-10M, HD-Villa-100M is one of the most popular trainingdata sets for generative video models. Kinetics-600 (Carreira et al., 2018) contains short YouTube videos of 600 distinct human actions withtheir associated class labels. Each action class is represented by more than 600 video clips that last around10 seconds. In total, the data set contains around 500,000 clips. The data set expands upon the previousKinetics-400 (Kay et al., 2017) data set. UCF101 (Soomro et al., 2012) is a data set of videos showing human actions. It contains over 13,000YouTube video clips with a total duration of about 27 hours and an average length of 7 seconds. It expandsupon the previous UCF50 (Reddy & Shah, 2013) data set, which includes only roughly half as many videoclips and action classes. The clips have a resolution of 320240 pixels. Each video has been annotated witha class label that identifies it as showing one of 101 possible actions. The 101 action classes are more broadlycategorized into 5 action types: Human-Object Interaction, Body-Motion Only, Human-Human Interaction,Playing Musical Instruments, and Sports. While UCF101 was mainly intended for training and evaluatingaction classifiers, it has also been adopted as a benchmark for generative models. For this, the class labelsare often used as text prompts. The generated videos are then usually evaluated using IS, FID, and FVDmetrics. MSR-VTT (Xu et al., 2016) includes about 10,000 short video clips from over 7,000 videos with a totalrun time of about 41 hours. The videos were retrieved based on popular video search queries and filteredaccording to quality criteria such as resolution and length. Each clip was annotated by 20 different humanswith a short text description, yielding 200,000 video-text pairs. The data set was originally intended as abenchmark for automatic video annotation but has been used for evaluating text-to-video models as well.For this, CLIP text-similarity, FID, and FVD scores are usually reported. Sky Time-lapse (Xiong et al., 2018) is a collection of unlabeled short clips that contain time-lapse shotsof the sky. The videos have been taken from YouTube and divided into smaller non-overlapping segments.Each clip consists of 32 frames of continuous video at a resolution of 640360 pixels. The clips show the skyat different times of day, under different weather conditions, and with different scenery in the background.The data set can serve as a benchmark for unconditional video generation or video prediction. In particular,it allows one to assess how well a given generative video model is able to replicate complex motion patternsof clouds and stars. Tai-Chi-HD (Siarohin et al., 2019) contains over 3,000 unlabeled clips from 280 tai chi Youtube videos.The videos have been split into smaller chunks that range from 128 to 1024 frames and have a resolution of256256 pixels. Similar to Sky Time-lapse, Tai-Chi-HD can be used for training and evaluating unconditionalgeneration or video prediction.",
  "Image Data Sets": "Video models are sometimes jointly trained on image and video data. Alternatively, they may extend a pre-trained image generation model with temporal components that are fine-tuned on videos. providesa brief overview over commonly used labeled image data sets. ImageNet (Russakovsky et al., 2015) is a data set developed for the ImageNet Large Scale Visual Recog-nition Challenge that was held annually between 2010 and 2017. Since 2012, the same data set has beenused for the main image classification task. ImageNet-21k is a large collection of over 14 million images thathave been annotated by humans with one object category label. Overall, there are 20,000 different objectclasses present in the data set that are hierarchically organized according to the WordNet (Fellbaum, 1998)structure. A subset of this dataset used for the ImageNet competition itself is often called ImageNet-1k. Itcontains over 1 million images that each have been annotated by humans with one object category label anda corresponding bounding box. There are only 1,000 object categories in this data set. MS-COCO (Lin et al., 2014) has originally been developed as a benchmark data set for object localizationmodels. It contains over 300,000 images containing 91 different categories of everyday objects. Every instanceof an object is labeled with a segmentation mask and a corresponding class label. Overall, there are about2.5 million instances of objects in this data set. LAION-5B (Schuhmann et al., 2022) is a very large public collection of 5.58 billion text-image pairs thatcan be found on the internet. Access is provided in the form of a list of links. To ensure a minimal levelof correspondence between the images and their associated alt-texts, the pairs have been filtered by thefollowing method: Images and texts have both been encoded through a pre-trained CLIP (Radford et al.,2021) model and pairs with a low cosine CLIP similarity have been excluded. To train image or video models,often only the subset of LAION-5B that contains English captions is used. It contains 2.32 billion text-imagepairs and is referred to as LAION-2B. Additionally, labels for not safe for work (NSFW), watermarked, ortoxic content are provided based on automated classification. The LAION-5B data set offers are relativelylow level of curation, but its sheer size has proven very valuable for training large image and video models.",
  "Evaluation Metrics": "Human ratings are the most important evaluation method for video models since the ultimate goal is toproduce results that appeal to our aesthetic standards. To demonstrate the quality of a new model, subjectsusually rate its output in comparison to an existing baseline. Subjects are usually presented with pairs ofgenerated clips from two different video models. They are then asked to indicate which of the two examplesthey prefer in regard to a specific evaluation criterion.Depending on the study, the ratings can eitherpurely reflect the subjects personal preference, or they can refer to specific aspects of the video such astemporal consistency and adherence to the prompt. Humans are very good at judging what looks natural",
  "Set-to-set Comparison Metrics": "Frchet Inception Distance (FID, Heusel et al. 2017) measures the similarity between the output distri-bution of a generative image model and its training data. Rather than comparing the images directly, theyare first encoded by a pre-trained inception network (Szegedy et al., 2016). The FID score is calculated asthe squared Wasserstein distance between the image embeddings in the real and synthetic data. FID can beapplied to individual frames in a video sequence to study the image quality of generative video models, butit fails to properly measure temporal coherence. Frchet Video Distance (FVD, Unterthiner et al. 2018) has been proposed as an extension of FID forthe video domain. Its inception net is comprised of a 3D Convnet pre-trained on action recognition tasksin YouTube videos (I3D, Carreira & Zisserman 2017). The authors demonstrate that the FVD measure isnot only sensitive to spatial degradation (different kinds of noise), but also to temporal aberrations suchas swapping of video frames. FVD is a commonly used metric for assessing the quality of unconditional ortext-conditioned video generation. Kernel Video Distance (KVD, Unterthiner et al. 2018) is an alternative to FVD. It is computed in ananalogous manner, except that a polynomial kernel is applied to the features of the inception net. Theauthors found that FVD aligns better with human judgments than KVD. Nevertheless, both are commonlyreported as benchmark metrics for unconditional video generation. Frchet Video Motion Distance (FVMD, Liu et al. 2024) is a metric focused on temporal consistency,measuring the similarity between motion features of generated and reference videos using Frchet Distance.It begins by tracking keypoints using the pre-trained PIPs++ model (Zheng et al., 2023), then calculatesthe velocity and acceleration fields for each frame.The metric aggregates these features into statisticalhistograms and measures their differences using the Frchet Distance. FVMD assesses motion consistencyby analyzing speed and acceleration patterns, assuming smooth motions should follow physical laws andavoid abrupt changes. In addition to video-based metrics, the Peak Signal-to-Noise Ratio (PSNR) and Structural Similar-ity Index Measure (SSIM) (Wang et al., 2004) are commonly used image-level metrics for video qualityassessment. Specifically, SSIM characterizes the brightness, contrast, and structural attributes of the refer-ence and generated videos, while PSNR quantifies the ratio of the peak signal to the Mean Squared Error",
  "Unary Metrics": "Inception Score (IS, Salimans et al. 2016) is applicable to generative models trained on data sets withcategorical labels.An Inception Net (Szegedy et al., 2016) classifier pre-trained on the ImageNet dataset (Deng et al., 2009) is used to predict the class labels of each generated image. The IS score is thenexpressed by the Kullback-Leibler distance between the conditional class probability distribution p(y|G(x))and the marginal class distribution p(y) of the generated samples. While IS aligns well with human ratingsand possesses good discriminative power (Borji, 2019), it is susceptible to noise, as shown by Heusel et al.(2017). It should be noted that IS only assesses the quality of individual images. When applied to video data,",
  "Benchmarks": "Some authors train their video models on specific datasets for evaluation purposes. This allows them todirectly compare them with earlier models that have been trained on the same data. In this way, certaindatasets can also be seen as quality benchmarks for new video diffusion models. Commonly used evaluationdatasets for video generation include UCF-101 (Soomro et al., 2012), MSR-VTT (Xu et al., 2016), Tai-Chi-HD (Siarohin et al., 2019), and Sky Time-Lapse (Radford et al., 2021). Models trained on all four ofthese datasets can be evaluated using samples that have been generated in an unconditional manner. ForUCF-101, a second benchmark is sometimes reported on conditional generation where the 101 class labelsare used for guiding the generative process.In this case, IS can be used as an evaluation metric.ForMSR-VTT, conditional generation using the 200,000 human video annotations as text prompts can also beevaluated. Here, CLIP text-similarity is often reported as a measure of text-video alignment. Most often,the benchmarked models are either directly trained on the train split of the evaluation data set, or they arepre-trained on a separate large video data set (such as WebVid-10M or HD-Villa-100M) and later fine-tunedon the evaluation data set. However, some papers evaluate their model in a zero-shot setting, where themodel has not been trained on the evaluation data set at all. These discrepancies between evaluation setupsmean that a direct comparison of benchmark results across studies should be taken with a grain of salt.More complex benchmarking suites include Ego-Exo4D (Grauman et al., 2024) which is a multimodal andmultiview video dataset of skilled human activities. The benchmark results for video generation are summarized in . Make-A-Video (Singer et al., 2022),one of the early diffusion-based video models, still holds state-of-the-art FVD and IS scores on the UCF-101conditional generation benchmark. It not only outperforms all GAN and auto-regressive models, but alsothe newer diffusion-based models. It is pre-trained on both the WebVid-10M and HD-Villa-100M data sets,which gives it an advantage in terms of the quantity of the training data over most other models. Make-A-Video also holds the best CLIP-similarity and FID scores in the zero-shot text-conditioned MSR-VTTbenchmark. Make-A-Video is outperformed by Make-Your-Video (Xing et al., 2023a) when it comes to zero-shot performance on UCF-101, although the latter uses depth maps as additional conditioning. Therefore,both models are not directly comparable. VideoFusion (Luo et al., 2023) has achieved the best FVD score",
  "Unconditional Generation & Text-to-Video": "Unconditional video generation and text-conditioned video generation are common benchmarks for generativevideo models. Prior to diffusion models, Generative Adversarial Networks (GANs, Goodfellow et al. 2014,Melnik et al. 2024) and auto-regressive transformer models (Vaswani et al., 2017) have been popular choicesfor generative video tasks. In the following, we provide a short overview over a few representative GANand auto-regressive transformer video models. We then introduce a selection of competing diffusion modelsstarting in Sec. 7.1.3.",
  "GAN Video Models": "TGAN (Saito et al., 2017) employs two generator networks: The temporal generator creates latent featuresthat represent the motion trajectory of a video. This feature vector can be fed into an image generatorthat creates a fixed number of video frames in pixel space. TGAN-v2 (Saito et al., 2020) uses a cascadeof generator modules to create videos at various temporal resolutions, making the process more efficient.TGAN-F (Kahembwe & Ramamoorthy, 2020) is another improved version that relies on lower-dimensionalkernels in the discriminator network. MoCoGAN (Tulyakov et al., 2018) decomposes latent space into motion and content-specific parts byemploying two separate discriminators for individual frames and video sequences. At inference time, thecontent vector is kept fixed while the next motion vector for each frame is predicted in an auto-regressivemanner using a neural network. MoCoGAN was evaluated on unconditional video generation on the UCF-101and Tai-Chi-HD datasets and achieved higher IS scores than the preceding TGAN and VGAN models. DVD-GAN (Clark et al., 2019) uses a similar dual discriminator setup to MoCoGAN. The main differenceis that DVD-GAN does not use auto-regressive prediction but instead generates all video frames in parallel.It outperformed previous methods such as TGAN-v2 and MoCoGAN on the UCF-101 dataset in terms ofIS score, although DVD-GAN conditioned its generation on class labels, whereas the other approaches wereunconditional. MoCoGAN-HD (Tian et al., 2021) disentangles content and motion in a different way from the previousapproaches. A motion generator is trained to predict a latent motion trajectory, which can then be passedas input to a fixed image generator. It outperformed previous approaches on unconditional generation inthe UCF-101, Tai-Chi-HD, and Sky Time-lapse benchmarks. DIGAN (Yu et al., 2022) introduces an implicit neural representation-based video GAN architecture thatcan efficiently represent long video sequences. It follows a similar content-motion split as discussed above.The motion discriminator judges temporal dynamics based on pairs of video frames rather than the wholesequence. These improvements enable the model to generate longer video sequences of 128 frames. DIGANachieved state-of-the-art results on UCF-101 in terms of IS and FVD score, as well as on Sky Time-lapseand Tai-Chi-HD in terms of FVD and KVD scores.",
  "Auto-Regressive Transformer Video Models": "VideoGPT (Yan et al., 2021) uses a 3D VQ-VAE (Van Den Oord et al., 2017) to learn a compact videorepresentation. An auto-regressive transformer model is then trained to predict the latent code of the nextframe based on the preceding frames. While VideoGPT did not outperform the best GAN-based models atthe time, namely TGAN-v2 and DVD-GAN, it achieved a respectable IS score on the UCF-101 benchmarkconsidering its simple architecture.",
  "Diffusion Models": "Producing realistic videos based on only a text prompt is one of the most challenging tasks for video diffusionmodels. A key problem lies in the relative lack of suitable training data. Publicly available video data sets areusually unlabeled, and human-annotated labels may not even accurately describe the complex relationshipbetween spatial and temporal information. Many authors therefore supplement training of their modelswith large data sets of labeled images or build on top of a pre-trained text-to-image model. The first videodiffusion models (Ho et al., 2022c) had very high computational demands paired with relatively low visualfidelity. Both aspects have significantly been improved through architectural advancements, such as movingthe denoising process to the latent space of a variational auto-encoder (He et al., 2022b; Zhou et al., 2022;Chen et al., 2023a; 2024; Blattmann et al., 2023a; Zhang et al., 2023a) and using upsampling techniquessuch as CDMs (Ho et al. 2022a; Wang et al. 2023c, see section 4.3). Ho et al. (2022c) present an early diffusion-based video generation model called VDM. It builds on the3D UNet architecture proposed by iek et al. (2016), extending it by factorized spatio-temporal attentionblocks. This produces videos that are 16 frames long and 64 64 pixels large. These low-resolution videoscan then be extended to 128 128 pixels and 64 frames using a larger upsampling model. The models aretrained on a relatively large data set of labeled videos as well as single frames from those videos, whichenables text-guided video generation at time of inference. However, this poses a limitation of this approachsince labeled video data is relatively difficult to come by. Singer et al.s (2022) Make-a-Video address this issue by combining supervised training of their modelon labeled images with unsupervised training on unlabeled videos. This allows them to access a wider andmore diverse pool of training data. They also split the convolution layers in their UNet model into 2Dspatial convolutions and 1D temporal convolutions, thereby alleviating some of the computational burdenassociated with a full 3D UNet. Finally, they train a masked spatiotemporal decoder on temporal upsamplingor video prediction tasks. This enables the generation of longer videos of up to 76 frames. Make-a-Videowas evaluated on the UCF-101 and MSR-VTT benchmarks where it outperformed all previous GAN andautoregressive transformer models. Ho et al. (2022a) use a cascaded diffusion process (Ho et al. 2022b, see ) that can generate high-resolution videos in their model called ImagenVideo. They start with a base model that synthesizes videoswith 4024 pixels and 16 frames, and upsample it over six additional diffusion models to a final resolutionof 1280768 pixels and 128 frames. The low-resolution base model uses factorized space-time convolutionsand attention.To preserve computational resources, the upsampling models only rely on convolutions.ImagenVideo is trained on a large proprietary data set of labeled videos and images in parallel, enabling itto emulate a variety of visual styles. The model also demonstrates the ability to generate animations of text,which most other models struggle with.",
  "Flow": "I2V Model a) Attention Conditioningb) Input Conditioningc) Joint Conditioning w/ Motion ImageImage : Image conditioning methods for image-to-video generation models.a) input images can beconditioned in the attention layers of the video generation models. b) input images can be formed as theextra input channels to the diffusion models. c) input images can be jointly conditioned with other modalities,such as optical flow. directly applied to different image diffusion backbones, making their approach compatible with personalizedimage generation techniques such as Dreambooth (Ruiz et al., 2023) and LoRA (Hu et al., 2021).",
  "Image-Conditioned Generation": "An important limitation of text-to-video models is the lack of controllability, as the video content can only bedetermined by an input text prompt. To mitigate this issue, recent research has been focusing on introducingadditional image conditional signals to the video generation process. Image conditioning can be achievedthrough injecting semantic image embeddings (e.g. CLIP (Radford et al., 2021) image embeddings) (Wanget al., 2024a; Zhang et al., 2023b; Xing et al., 2023b; Chen et al., 2023a) or image VAE latents (Ren et al.,2024; Zhang et al., 2024) in attention layers ( a)), adding extra input channels that represent theconditioning image ( b)) (Girdhar et al., 2023; Chen et al., 2023e; Zeng et al., 2023; Blattmannet al., 2023a), joint conditioning with other modalities such as optical flows ( c))(Chen et al.,2023d; Shi et al., 2024), etc. Image-conditioned generation also enables a wide range of applications, such asautoregressive long video generation (Zeng et al., 2023; Chen et al., 2023e; Ren et al., 2024), looping videogeneration (Xing et al., 2023b), generative frame interpolation (Zeng et al., 2023; Xing et al., 2023b) andvisual storytelling (Zeng et al., 2023; Xing et al., 2023b). Chen et al. (2023d) focus on the task of animating images in accordance with motion cues. Their Motion-Conditioned Diffusion Model (MCDiff) accepts an input image and lets the user indicate the desired motionby drawing strokes on top of it. The model then produces a short video sequence in which objects move inaccordance with the motion cues. It can dissociate between foreground (e.g. actor movement) or backgroundmotion (i.e. camera movement), depending on the context. The authors use an auto-regressive approachto generate each video frame conditioned on the previous frame and predicted motion flow. For this, theinput motion strokes are decomposed into smaller segments and passed to a UNet flow completion model topredict motion in the following frame. A denoising diffusion model receives this information and uses it tosynthesize the next frame. The flow completion model and the denoising model are first trained separatelybut later fine-tuned jointly on unannotated videos. Chen et al.s (2023e) SEINE proposes to train an image-conditioned video generation model by concate-nating the VAE latent of the image along the channel dimension of the input noise and adding an extramask channel indicating which frame needs to be predicted. This enables flexible image conditioning suchthat the model can generate videos providing any given frames as conditional signals. SEINE is initializedfrom the text-to-video model LaVie (Wang et al., 2023c) and trained on WebVid-10M (Bain et al., 2021)along with internal private data. During inference, the model is able to perform autoregressive long videogeneration (by reusing the last frame of a previous video clip as the first frame to predict the next video),generating transitions between different scenes (by using two frames from different scenes as the conditioningfirst frame and last frame and generate the intermediate frames) and image animation (by conditioning thevideo generation process on the input first frame).",
  "Video Completion & Long Video Generation": "As mentioned in , video diffusion models typically generate a fixed number of frames at a time.Additionally, the number of frames produced in one denoising process is usually limited to a small quantity,such as 8 or 16 frames. As a result, this limitation leads to the generation of very short videos with lowframe rates (e.g. 2 seconds, 8 fps). In order to circumvent this limitation, auto-regressive extension andtemporal upsampling methods have been proposed (see .2) to enhance the duration and frame rateof the generated videos. Models adopting these methods often adjust and combine them in unique ways thatbenefit computational speed or consistency. A common problem with these approaches is that they tend togenerate videos that suffer from repetitive content. Some models have therefore explored ways to generatevideos with changing scenes by varying the text prompts over time.",
  "Temporal Upsampling & Video Prediction": "Yin et al.s (2023) NUWA-XL model uses an iterative hierarchical approach to generate long video sequencesof several minutes. It first generates evenly spaced key frames from separate text prompts that form a roughoutline of the video. The frames in-between are then filled in with a local diffusion model conditioned ontwo key frames. This process is applied iteratively to increase the temporal resolution with each pass. Sincethis can be parallelized, the model achieves much faster computation times than auto-regressive approachesfor long video generation. The authors train the model on a new training data set consisting of annotatedFlintstones cartoons. Simple temporal convolution and attention blocks are inserted into the pre-trainedtext-to-image model to learn temporal dynamics. He et al. (2022b) tackle the task of generating long videos with over 1,000 frames with their Long VideoDiffusion Model (LVDM). It combines auto-regressive and hierarchical approaches for first generating longsequences of key frames and then filling in missing frames. In order to reduce quality degradation induced byauto-regressive sampling, the authors use classifier-free guidance and conditional latent perturbation whichconditions the denoising process on noisy latents of reference frames. The model utilizes a dedicated videoencoder and combines 2D spatial with 1D temporal self-attention. It can be used for unconditional videogeneration or text-to-video tasks. Harvey et al. (2022) similarly explore methods for generating long video sequences with video models thathave a fixed number of output frames. Their Flexible Diffusion Model (FDM) accepts an arbitrary numberof conditioning frames to synthesize new frames, thereby allowing it to either extend the video in an auto-regressive manner or to use a hierarchical approach (similar to NUWA-XL, Yin et al. 2023). The authorsexplore variations of these sampling techniques and suggest an automated optimization routine that findsthe best one for a given training data set. Lu et al. (2023b) propose Video Diffusion Transformer (VDT), a diffusion-based video model that uses avision transformer architecture (Peebles & Xie, 2022). The reported advantages of this type of architectureover the commonly used UNet include the ability to capture long-range temporal dynamics, to accept con-ditioning inputs of varying lengths, and the scalability of the model. VDT was trained on more narrow datasets of unlabeled videos and accomplished tasks such as video prediction, temporal interpolation, and imageanimation in those restricted domains.",
  "Alternative Approaches": "Wang et al.s (2023) Gen-L-Video generates long video sequences by denoising overlapping shorter videosegments in parallel. A video diffusion model predicts the denoised latent in each video segment individually.The noise prediction for a given frame is than aggregated through interpolation across all segments in which itappears. This leads to greater coherence across the long video sequence. The authors apply this new methodto existing frameworks in the text-to-video (LVDM, He et al. 2022b), tuning-free video-to-video (Pix2Video,Ceylan et al. 2023), and one-shot tuning video-to-video (Tune-A-Video, Wu et al. 2022b) domains. Zhu et al. (2023) follow a unique approach for generating long video sequences in their MovieFactorymodel. Rather than extending a single video clip, they generate a movie-like sequence of separate related",
  "Audio-conditioned Synthesis": "Multimodal synthesis might be the most challenging task for video diffusion models. A key problem lies inhow associations between different modalities can be learned. Similar to how CLIP models (Radford et al.,2021) encode text and images in a shared embedding space, many models learn a shared semantic space foraudio, text, and / or video through techniques such as contrastive learning (Chen et al., 2020).",
  "Audio-conditioned Generation & Editing": "Lee et al.s (2023a) Soundini model enables local editing of scenic videos based on sound clips. A binary maskcan be specified to indicate a video region that is intended to be made visually consistent with the auditorycontents of the sound clip. To this end, a sliding window selection of the sound clips mel spectrogram isencoded into a shared audio-image semantic space. During training, two loss functions are minimized tocondition the denoising process on the embedded sound clips: The cosine similarity between the encodedaudio clip and the image latent influences the generated video content, whereas the cosine similarity betweenthe image and audio gradients is responsible for synchronizing the video with the audio signal. In contrastto other models, Soundini does not extend its denoising UNet to the video domain, only generating singleframes in isolation. To improve temporal consistency, bidirectional optical flow guidance is used to warpneighboring frames towards each other.",
  "LaVie (2023c)512320YesTextVimeo25M-526.3-0.2949------": "Lee et al. (2023b) generate scenic videos from text prompts and audio clips with their Audio-Aligned DiffusionFramework (AADiff). An audio clip is used to identify a target token from provided text tokens, basedon the highest similarity of the audio clip embedding with one of the text token embeddings. For instance,a crackling sound might select the word burning. While generating video frames, the influence of theselected target token on the output frame is modulated through attention map control (similar to Prompt-to-Prompt, Hertz et al. 2022) in proportion to the sound magnitude. This leads to changes in relevant videoelements that are synchronized with the sound clip. The authors also demonstrate that their model can beused to animate a single image and that several sound clips can be inserted in parallel. The model uses apre-trained text-to-image model to generate each video frame without additional fine-tuning on videos orexplicit modeling of temporal dynamics. Liu et al.s (2023d) Generative Disco provides an interactive interface to support the creation of musicvisualizations. They are implemented as visual transitions between image pairs created with a diffusionmodel from user-specified text prompts. The interval in-between the two images is filled according to thebeat of the music, using a form of interpolation that employs design patterns to cause shifts in color, subject,or style, or set a transient video focus on subjects. A large language model can further assist the user withchoosing suitable prompts. While the model is restricted to simple image transitions and is therefore notable to produce realistic movement, it highlights the creative potential of video diffusion models for musicvisualization. Tang et al. (2023) present a model called Composable Diffusion that can generate any combination ofoutput modalities based on any combination of input modalities. This includes text, images, videos, andsound. Encoders for the different modalities are aligned in a shared embedding space through contrastivelearning. The diffusion process can then be flexibly conditioned on any combination of input modalities bylinearly interpolating between their embeddings. A separate denoising diffusion model is trained for eachof the output modalities and information between the modality-specific models is shared through cross-attention blocks. The video model uses simple temporal attention as well as the temporal shift method fromAn et al. (2023) to ensure consistency between frames.",
  "Talking Head Generation": "Stypukowski et al. (2023) have developed the first diffusion model for generating videos of talking heads.Their model Diffused Heads takes a reference image of the intended speaker as well as a speech audio clipas input. The audio clip is divided into short chunks that are individually embedded through a pre-trainedaudio encoder. During inference, the reference image as well as the last two generated video frames areconcatenated with the noisy version of the current video frame and passed through a 2D UNet. Additionally,the denoising process is conditioned on a sliding window selection of the audio embeddings. The generatedtalking faces move their lips in sync with the audio and display realistic facial expressions. Zhua et al. (2023) follow a similar approach, but instead of using a reference image, their model accepts areference video that is transformed to align with the desired audio clip. Face landmarks are first extractedfrom the video, and then encoded into eye blink embeddings and mouth movement embeddings. The mouthmovements are aligned with the audio clip using contrastive learning. Head positions and eye blinks areencoded with a VAE, concatenated together with the synchronized mouth movement embeddings, and passedas conditioning information to the denoising UNet. Casademunt et al. (2023) focus on the unique task of laughing head generation.Similar to DiffusedHeads (Stypukowski et al., 2023), the model takes a reference image and an audio clip of laughter togenerate a matching video sequence. The model combines 2D spatial convolutions and attention blocks with1D temporal convolutions and attention. This saves computational resources over a fully 3D architectureand allows it to process 16 video frames in parallel. Longer videos can be generated in an auto-regressivemanner. The authors demonstrate the importance of using a specialized audio-encoder for embedding thelaughter clips in order to generate realistic results.",
  "Video Editing": "Editing can mean a potentially wide range of operations such as adjusting the lighting, style, or background,changing, replacing, re-arranging, or removing objects or persons, modifying movements or entire actions,and more. To avoid having to make cumbersome specifications for possibly a large number of video frames,a convenient interface is required. To achieve this, most approaches rely on textual prompts that offer aflexible way to specify desired edit operations at a convenient level of abstraction and generality. However,completely unconstrained edit requests may be in conflict with desirable temporal properties of a video,leading to a major challenge of how to balance temporal consistency and editability (see .3). Tothis end, many authors have experimented with conditioning the denoising process based on preprocessedfeatures of the input video. One-shot tuning methods first fine-tune their parameters on the ground truthvideo. This ensures that the video content and structure can be reconstructed with good quality. On theother hand, tuning-free methods are not fine-tuned on the ground truth video, which makes the editingcomputationally more efficient.",
  "One-Shot Tuning Methods": "Molad et al. (2023) present a diffusion video editing model called Dreamix based on the ImagenVideo (Hoet al., 2022a) architecture. It first downsamples an input video, adds Gaussian noise to the low resolutionversion, then applies a denoising process conditioned on a text prompt. The model is finetuned on eachinput video and follows the joint training objective of preserving the appearance of both the entire video andindividual frames. The authors demonstrate that the model can edit the appearance of objects as well astheir actions. It is also able to take either a single input image or a collection of images depicting the sameobject and animate it. Like ImagenVideo (Ho et al., 2022a), Dreamix operates in pixel space rather thanlatent space. Together with the need to finetune the model on each video, this makes it computationallyexpensive. Wu et al. (2022b) base their Tune-A-Video on a pre-trained text-to-image diffusion model. Rather thanfine-tuning the entire model on video data, only the projection matrices in the attention layers are trainedon a given input video. The spatial self-attention layer is replaced with a spatio-temporal layer attending to",
  "Depth-conditioned Editing": "Ceylan et al.s (2023) Pix2Video continues the trend of using a pre-trained text-to-image model as thebackbone for video editing tasks. In contrast to the previous approaches, it however eliminates the need forfine-tuning the model on each individual video. In order to preserve the coarse spatial structure of the input,the authors use DDIM inversion and condition the denoising process on depth maps extracted from theoriginal video. Temporal consistency is ensured by injecting latent features from previous frames into self-attention blocks in the decoder portion of the UNet. The projection matrices from the stock text-to-imagemodel are not altered. Despite using a comparatively lightweight architecture, the authors demonstrate goodeditability and consistency in their results. Esser et al.s (2023) Runway Gen-1 enables video style editing while preserving the content and structureof the original video.This is achieved on the one hand by conditioning the diffusion process on CLIPembeddings extracted from a reference video frame (in addition to the editing text prompt), and on theother hand by concatenating extracted depth estimates to the latent video input. The model uses 2D spatialand 1D temporal convolutions as well as 2D + 1D attention blocks. It is trained on video and image data inparallel. Predictions of both modes are combined in a way inspired by classifier-free guidance (Ho & Salimans,2022), allowing for fine-grained control over the tradeoff between temporal consistency and editability. Thesuccessor model Runway Gen-2 (unpublished) also adds image-to-video and text-to-video capabilities. Xing et al. (2023a) extend a pre-trained text-to-image model conditioned on depth maps to video editingtasks in their Make-Your-Video model, similar to Pix2Video (Ceylan et al., 2023). They add 2D spatialconvolution and 1D temporal convolution layers, as well as cross-frame attention layers to their UNet. Acausal attention mask limits the number of reference frames to the four immediately preceding ones, as theauthors note that this offers the best trade-off between image quality and coherence. The temporal modulesare trained on a large unlabeled video data set (WebVid-10M, Bain et al. 2021).",
  "Pose-conditioned Editing": "Ma et al.s (2023) Follow Your Pose conditions the denoising process in Tune-A-Video on pose featuresextracted from an input video. The pose features are encoded and downsampled using convolutional layersand passed to the denoising UNet through residual connections. The pose encoder is trained on image data,whereas the spatio-temporal attention layers (same as in Tune-A-Video) are trained on video data. Themodel generates output that is less bound by the source video while retaining relatively natural movementof subjects.",
  "Leveraging Pre-trained Video Generation Model for Video Editing": "Instead of adapting a pre-trained image generation model for video editing, Bai et al.s (2024) UniEditinvestigates the approach of leveraging a pre-trained text-to-video generation model for zero-shot videoediting. Specifically, they propose to use the LaVie (Wang et al., 2023c) T2V model and employ featureinjection mechanisms to condition the T2V generation process on the input video.This is achieved byintroducing the auxiliary reconstruction branch and motion-reference branch during video denoising. Thevideo features from these auxiliary branches are extracted and injected into the spatial and temporal self-attention layers of the main editing path to ensure the output video contains the same spatial structure andmotion as the source video. A concurrent approach of UniEdit is Ku et al.s (2024) AnyV2V, which employs pre-trained image-to-video(I2V) generation models for zero-shot video editing tasks. AnyV2V breaks video editing into two stages. Inthe first stage, an image editing method is used to modify the first frame of the video into an edited frame.In the second stage, the edited frame and the DDIM inverted latent of the source video are passed into theI2V generation model to render the edited video. AnyV2V also adopts feature injection mechanisms similarto PnP (Tumanyan et al., 2023) to preserve the structure and motion of the source video. Because of theproposed two-stage editing strategy, AnyV2V is compatible with any off-the-shelf image editing models andcan be employed in a broad spectrum of video editing tasks, such as prompt-based video editing, reference-based style transfer, identity manipulation and subject-driven video editing. The framework also supportsdifferent I2V models, such as I2VGen-XL (Zhang et al., 2023b), ConsistI2V (Ren et al., 2024) and SEINE(Chen et al., 2023e).",
  "Multi-conditional Editing": "Zhang et al.s (2023c) ControlVideo model extends ControlNet (Zhang & Agrawala, 2023) to video genera-tion tasks. ControlNet encodes preprocessed image features using an auto-encoder and passes them througha fine-tuned copy of the first half of the Stable Diffusion UNet. The resulting latents at each layer are thenconcatenated with the corresponding latents from the original Stable Diffusion model during the decoderportion of the UNet to control the structure of the generated images. In order to improve the spatio-temporalcoherence between video frames, ControlVideo adds full cross-frame attention to the self-attention blocks ofthe denoising UNet. Furthermore, it mitigates flickering of small details by interpolating between alternatingframes. Longer videos can be synthesized by first generating a sequence of key frames and then generatingthe missing frames in several batches conditioned on two key frames each. In contrast to other video-to-videomodels that rely on a specific kind of preprocessed feature, ControlVideo is compatible with all ControlNetmodels, such as Canny or OpenPose. The pre-trained Stable Diffusion and ControlNet models also do notrequire any fine-tuning.",
  "Other Approaches": "Wang et al. (2023b) also adapt a pre-trained text-to-image model to video editing tasks without fine-tuning.Similar to Tune-A-Video and Pix2Video, their vid2vid-zero model replaces self-attention blocks with cross-frame attention without changing the transformation matrices. While the cross-frame attention in thoseprevious models is limited to the first and immediately preceding frame, Wang et al. extend attention to theentire video sequence. Vid2vid-zero is not conditioned on structural depth maps, instead using a traditional",
  "DDIM inversion approach. To achieve better alignment between the input video and user-provided prompt,it optimizes the null-text embedding used for classifier-free guidance": "Huang et al. (2023) present Style-A-Video, a model aimed at editing the style of a video based on atext prompt while preserving its content. It utilizes a form of classifier-free guidance that balances threeseparate guidance conditions: CLIP embeddings of the original frame preserve semantic information, CLIPembeddings of the text prompt introduce stylistic changes, while CLIP embeddings of thresholded affinitymatrices from self-attention layers in the denoising UNet encode the spatial structure of the image. Flickeringis reduced through a flow-based regularization network. The model operates on each individual frame withoutany form of cross-frame attention or fine-tuning of the text-to-image backbone. This makes it one of thelightest models in this comparison. Yang et al. (2023b) also use ControlNet for spatial guidance in their Rerender A Video model. Similarto previous models, sparse causal cross-frame attention blocks are used to attend to an anchor frame andthe immediately preceding frame during each denoising step. During early denoising steps, frame latentsare additionally interpolated with those from the the anchor frame for rough shape guidance. Furthermore,the anchor frame and previous frame are warped in pixel space to align with the current frame, encoded,and then interpolated in latent space. To reduce artifacts associated with repeated encoding, the authorsestimate the encoding loss and shift the encoded latent along the negative gradient of the loss function tocounteract the degradation. A form of color correction is finally applied to ensure color coherence acrossframes. This pipeline is used to generate key frames that are then filled in using patch-based propagation.The model produces videos that look fairly consistent when showing slow moving scenes but struggles withfaster movements due to the various interpolation methods used.",
  "Video Restoration": "Liu et al. (2023a) present ColorDiffuser, a model specialized on colorization of grayscale video footage. Itutilizes a pre-trained text-to-image model and specifically trained adapter modules to colorize short videosequences in accordance with a text prompt. Color Propagation Attention computes affinities between thecurrent grayscale frame as Query, the reference grayscale frame as Key, and the (noisy) colorized referenceframe latent as Value. The resulting frame is concatenated with the current grayscale frame and fed into aCoordinator Module that follows the same architecture as the Stable Diffusion UNet. Feature maps fromthe Coordinator module are then injected into the corresponding layers of the denoising UNet to guide thediffusion process (similar to ControlNet). During inference, an alternating sampling strategy is employed,whereby the previous and following frame are in turn used as reference. In this way, color information canpropagate through the video in both temporal directions. Temporal consistency and color accuracy is furtherimproved by using a specifically trained vector-quantized variational auto-encoder (VQVAE) that decodesthe entire denoised latent video sequence.",
  "Video Diffusion Models for Intelligent Decision Making": "Capable generative models are beginning to see widespread usage in control and intelligent decision-making (Yang et al., 2023a; Collaboration et al., 2024), including for downstream representation learning,world modeling, and generative data augmentation. So far, use cases have primarily focused on image-basedand low-dimensional diffusion models, but we elucidate where these may naturally be extended to video.",
  "Representation Learning": "Representation learning (Bengio et al., 2013) is a popular way to transfer useful features learned from large-scale training to downstream tasks. These features usually take the form of a low-dimensional vector whichcan then be simply adapted to another task with a small number of linear layers. Recent work has shownthat diffusion models are an effective way to do so, particularly for image and video-based tasks. A largefamily of methods has considered extracting representations from text-to-image diffusion models like StableDiffusion (Rombach et al., 2022). For example, Yang & Wang (2023); Gupta et al. (2024) propose to extractrepresentations from the diffusion model from intermediate layers of the network to be used for classification",
  "World Models": "An exciting application of more realistic video diffusion models is the ability to accurately simulate the realworld. As posited by LeCun (2022), learning an accurate world model is a crucial step in the path towardsautonomous intelligence, enabling an agent to robustly plan and reason about the outcome of their actions.Diffusion models have already been used as trajectory world models (Janner et al., 2022; Ajay et al., 2023)in receding horizon control style setups for low-dimensional environments. In these settings, trajectories ofany arbitrary quality can be biased towards high return through classifier-guided or classifier-free guidance. Further advances in video world modelling (Yang et al., 2024; Wang et al., 2024b; Hu et al., 2023a) could leadto similar techniques being scaled towards real-world settings. A notable example of this is GENIE (Bruceet al., 2024), a video world model (albeit not diffusion-based) trained from YouTube videos that learns toplan under latent actions. Crucially, this enables agents to be trained from synthetic environments basedon the vast amounts of unlabeled video on the internet. The remaining challenges with current methodsinclude improving the frame-by-frame consistency of generated trajectories as control policies often are verysensitive to the quality of observations, and speed of generation so that such models are useable in real-time.",
  "Synthetic Training Data": "Finally, as we begin to exhaust the available supply of real labeled images and video, synthetic generative datahas emerged as a powerful method to augment existing training datasets for downstream tasks. In supervisedlearning, diffusion models have been used to generate additional class-conditional data for classification (Heet al., 2022a; Azizi et al., 2023) resulting in significant boosts in performance. This enables the distillation ofinternet-scale knowledge into these models. With more realistic video generation, we could similarly generatedata for video classification or captioning tasks. In control, there is often a lack of readily available robotics data, and as such diffusion models are a par-ticularly powerful method to generate policy training data for reinforcement learning agents. This could bedone by simply naively upsampling existing datasets (Lu et al., 2023a) or in a guided fashion (Jackson et al.,2024) which generates training data that is on-policy with the current agent being optimized. These methodsvastly improve the sample efficiency of trained agents. In the visual setting, ROSIE (Yu et al., 2023) andGenAug (Chen et al., 2023f) have considered using image diffusion models to synthesize datapoints withnovel backgrounds and items in order to boost the generalization performance of learned policies. Videodiffusion models represent a significant improvement to single-timestep data augmentation and would enablean agent to fully simulate the outcome of a long sequence of actions.",
  "Ethical Considerations for Video Diffusion Models": "Generative AI and AI-generated content (AIGC) have transformative potential in the realm of video diffusionmodels, but they also pose significant risks when misused. The ability to fabricate highly realistic videoscould lead to an alarming rise in disinformation and deepfakes, where fabricated media is crafted to depictindividuals saying or doing things they never actually did. Such synthetic content can manipulate publicperception, distort historical events, or even tarnish reputations through malicious impersonation. While there has been extensive research related to responsible and trustworthy AI in the field of textgeneration (e.g. LLaMA Guard (Inan et al., 2023) for the LLaMA (Touvron et al., 2023) language modelseries) and image generation (e.g. Safety checker in Stable Diffusion (Rombach et al., 2022)), the field ofvideo generation has not yet reached the same level of scrutiny and rigor in terms of safeguarding againstmisuse.Current explorations include AnimateDiff (Guo et al., 2023), where a safety checker similar tothat in Stable Diffusion (Rombach et al., 2022) is applied to filter NSFW contents. Wang & Yang (2024)collected VidProM, a million-scale dataset containing real prompt-video pairs generated from various Text-to-Video diffusion models. They employ the state-of-the-art NSFW model Detoxify (Wang et al., 2022) toassign NSFW probabilities to each prompt based on different aspects including toxicity, obscenity, identityattack, insult, threat, and sexual explicitness. VidProM enables potential research directions such as trainingspecialized models to distinguish between generated and real videos, which could be beneficial for advancingsafe and responsible video generation.",
  "Outlook and Challenges": "Video diffusion models have already demonstrated impressive results in a variety of use cases. However,there are still several challenges that need to be overcome before we arrive at models capable of producinglonger video sequences with good temporal consistency. One issue is the relative lack of suitable training data. While there are large data sets of labeled imagesthat have been scraped from the internet (Sec. 6.2), the available labeled video data are much smaller insize (Sec. 6.1). Many authors have therefore reverted to training their models jointly on labeled imagesand unlabeled videos or fine-tuning a pre-trained text-to-image model on unlabeled video data. While thiscompromise allows for the learning of diverse visual concepts, it may not be ideal for capturing object-specificmotion. One possible solution is to manually annotate video sequences (Yin et al., 2023), although it seemsunlikely that this can be done on the scale required for training generalized video models. It is to be hopedthat in the future automated annotation methods will develop that allow for the generation of accurate videodescriptions (Zare & Yazdi, 2022).",
  "Conclusion": "This survey has provided an overview of the evolving field of video diffusion models, examining their potentialfor content creation and manipulation across various domains. We have explored the field systematically, cat-egorizing applications by input modalities, discussing architectural choices and temporal dynamics modeling,and summarizing key developments. While progress has been made in generating, editing, and transformingvideo content, significant work remains to be done. As research continues, video diffusion models may influ-ence how we create and interact with visual media, potentially opening up new applications in entertainment,education, and scientific visualization.",
  "Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J. Fleet. Syntheticdata from diffusion models improves imagenet classification, 2023": "Jianhong Bai, Tianyu He, Yuchi Wang, Junliang Guo, Haoji Hu, Zuozhu Liu, and Jiang Bian. Uniedit: Aunified tuning-free framework for video motion and appearance editing. arXiv preprint arXiv:2402.13185,2024. Max Bain, Arsha Nagrani, Gl Varol, and Andrew Zisserman. Frozen in time: A joint video and imageencoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 17281738, 2021. Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, JunhwaHur, Yuanzhen Li, Tomer Michaeli, et al. Lumiere: A space-time diffusion model for video generation.arXiv preprint arXiv:2401.12945, 2024.",
  "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspec-tives. IEEE transactions on pattern analysis and machine intelligence, 35(8):17981828, 2013": "Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent videodiffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023a. Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, andKarsten Kreis.Align your latents: High-resolution video synthesis with latent diffusion models.InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575,2023b.",
  "TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,JoeTaylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh.Videogenerationmodelsasworldsimulators.2024.URL": "Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai,Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments.arXiv preprint arXiv:2402.15391, 2024. Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 62996308, 2017.",
  "Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial video generation on complex datasets. arXivpreprint arXiv:1907.06571, 2019": "Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, AbhishekGupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain,Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta,Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, An-thony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, AyzaanWahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schlkopf, Blake Wulfe, Brian Ichter, Cewu Lu,Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Chris-tine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, DannyDriess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Bchler, Dinesh Jayaraman, Dmitry Kalash-nikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao,Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, GilbertFeng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-ShuFang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homanga Bharad-hwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang,Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil,Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, JiankaiSun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik,Joo Silvrio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J.Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, KeerthanaGopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black,Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan,",
  "DC Dowson and BV666017 Landau. The frchet distance between multivariate normal distributions. Journalof multivariate analysis, 12(3):450455, 1982": "Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis.Structure and content-guided video synthesis with diffusion models. arXiv preprint arXiv:2302.03011,2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi,Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolutionimage synthesis. In Forty-first International Conference on Machine Learning, 2024.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neuralinformation processing systems, 33:68406851, 2020": "Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik PKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al.Imagen video: High definition videogeneration with diffusion models. arXiv preprint arXiv:2210.02303, 2022a. Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research,23(1):22492281, 2022b.",
  "Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-a-video: Agile diffusion for arbitrary text-based videostyle transfer. arXiv preprint arXiv:2305.05464, 2023": "Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu,Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, andZiwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.",
  "Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao.Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024": "Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose:Pose-guided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. Andrew Melnik, Robin Schiewer, Moritz Lange, Andrei Ioan Muresanu, mozhgan saeidi, Animesh Garg, andHelge Ritter. Benchmarks for physical reasoning AI. Transactions on Machine Learning Research, 2023.ISSN 2835-8856. URL Survey Certification. Andrew Melnik, Maksim Miasayedzenkau, Dzianis Makaravets, Dzianis Pirshtuk, Eren Akbulut, DennisHolzmann, Tarek Renusch, Gustav Reichert, and Helge Ritter. Face generation and editing with stylegan:A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editingreal images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 60386047, 2023. Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan,and Yedid Hoshen.Dreamix:Video diffusion models are general video editors.arXiv preprintarXiv:2302.01329, 2023.",
  "William Peebles and Saining Xie.Scalable diffusion models with transformers.arXiv preprintarXiv:2212.09748, 2022": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXivpreprint arXiv:2307.01952, 2023. Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, BowenShi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models. arXiv preprintarXiv:2410.13720, 2024. Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, andRicky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprintarXiv:2304.14772, 2023.",
  "Weiming Ren, Harry Yang, Ge Zhang, Cong Wei, Xinrun Du, Stephen Huang, and Wenhu Chen. Consisti2v:Enhancing visual consistency for image-to-video generation. arXiv preprint arXiv:2402.04324, 2024": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical imagesegmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18thInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241.Springer, 2015.",
  "Antonio Ruiz, Andrew Melnik, Dong Wang, and Helge Ritter. Lane segmentation refinement with diffusionmodels. arXiv preprint arXiv:2405.00620, 2024": "Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250022510, 2023. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115:211252, 2015. Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular valueclipping. In Proceedings of the IEEE international conference on computer vision, pp. 28302839, 2017. Masaki Saito, Shunta Saito, Masanori Koyama, and Sosuke Kobayashi. Train sparsely, generate densely:Memory-efficient unsupervised training of high-resolution temporal gan. International Journal of Com-puter Vision, 128(10-11):25862606, 2020.",
  "Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improvedtechniques for training gans. Advances in neural information processing systems, 29, 2016": "Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it:Learning transferable representations from synthetic imagenet clones. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), 2023. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-nymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25562565, 2018. Neta Shaul, Uriel Singer, Ricky TQ Chen, Matthew Le, Ali Thabet, Albert Pumarola, and Yaron Lip-man.Bespoke non-stationary solvers for fast sampling of diffusion and flow models.arXiv preprintarXiv:2403.01329, 2024.",
  "Aliaksandr Siarohin, Stphane Lathuilire, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motionmodel for image animation. Advances in neural information processing systems, 32, 2019": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXivpreprint arXiv:2209.14792, 2022. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265.PMLR, 2015.",
  "Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classesfrom videos in the wild. arXiv preprint arXiv:1212.0402, 2012": "Micha Stypukowski, Konstantinos Vougioukas, Sen He, Maciej Ziba, Stavros Petridis, and Maja Pantic.Diffused heads: Diffusion models beat gans on talking-face generation. arXiv preprint arXiv:2301.03396,2023. Mingzhen Sun, Weining Wang, Zihan Qin, Jiahui Sun, Sihan Chen, and Jing Liu. Glober: Coherent non-autoregressive video generation via global guided video decoder. arXiv preprint arXiv:2309.13274, 2023. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.Rethinking theinception architecture for computer vision. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 28182826, 2016.",
  "Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal.Any-to-any generation viacomposable diffusion. arXiv preprint arXiv:2305.11846, 2023": "Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp.402419. Springer, 2020. Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse, attend, andsegment: Unsupervised zero-shot segmentation using stable diffusion. arXiv preprint arXiv:2308.12469,2023. Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey Tulyakov.A good image generator is what you need for high-resolution video synthesis.arXiv preprintarXiv:2104.15069, 2021. Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras,Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatchoptimal transport. arXiv preprint arXiv:2302.00482, 2023.",
  "Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023": "Jinglong Wang, Xiawei Li, Jing Zhang, Qingyuan Xu, Qin Zhou, Qian Yu, Lu Sheng, and Dong Xu.Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter. arXiv e-prints, art.arXiv:2309.02773, September 2023. doi: 10.48550/arXiv.2309.02773. Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, andLijuan Wang. Disco: Disentangled control for referring human dance generation in real world. arXive-prints, pp. arXiv2307, 2023a.",
  "Wenhao Wang and Yi Yang. Vidprom: A million-scale real prompt-gallery dataset for text-to-video diffusionmodels. arXiv preprint arXiv:2403.06098, 2024": "Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. Advancesin Neural Information Processing Systems, 36, 2024a. Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze Chen, and Jiwen Lu.Worlddreamer:Towards general world models for video generation via predicting masked tokens.arXiv preprintarXiv:2401.09985, 2024b. Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He,Jiashuo Yu, Peiqing Yang, et al.Lavie: High-quality video generation with cascaded latent diffusionmodels. arXiv preprint arXiv:2309.15103, 2023c.",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from errorvisibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004": "Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. arXiv preprintarXiv:2210.14896, 2022. Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan.Nwa: Visualsynthesis pre-training for neural visual world creation. In European conference on computer vision, pp.720736. Springer, 2022a. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, XiaohuQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-videogeneration. arXiv preprint arXiv:2212.11565, 2022b. Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong Zhang, Yingqing He, Hanyuan Liu, HaoxinChen, Xiaodong Cun, Xintao Wang, et al. Make-your-video: Customized video generation using textualand structural guidance. arXiv preprint arXiv:2306.00943, 2023a. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dy-namicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190,2023b. Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos usingmulti-stage dynamic generative adversarial networks. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 23642373, 2018. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging videoand language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.52885296, 2016. Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, andMike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model.arXiv preprint arXiv:2311.16498, 2023. Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei Liu, Huan Yang, Jianlong Fu, and BainingGuo. Advancing high-resolution video-language representation with large-scale video transcriptions. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 50365045,2022.",
  "Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixelsdance: High-dynamic video generation. arXiv preprint arXiv:2311.10982, 2023": "David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, andMike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXivpreprint arXiv:2309.15818, 2023a. David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, and Doyen Sahoo. Moon-shot: Towards controllable video generation and editing with multimodal conditions.arXiv preprintarXiv:2401.01827, 2024.",
  "Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficientvideo generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022": "Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi Tuo, Wen-Huang Cheng, Lianli Gao, JingkuanSong, and Jianlong Fu. Moviefactory: Automatic movie creation from text using large generative modelsfor language and images. arXiv preprint arXiv:2306.07257, 2023. Yizhe Zhua, Chunhui Zhanga, Qiong Liub, and Xi Zhoub. Audio-driven talking head video generation withdiffusion model. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and SignalProcessing (ICASSP), pp. 15. IEEE, 2023."
}