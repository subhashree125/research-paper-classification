{
  "Abstract": "Variational autoencoders (VAEs) are deep probabilistic models that are used in scientificapplications (Cerri et al., 2019; Flam-Shepherd et al., 2022; Zhou & Wei, 2020; Gonduret al., 2023) and are integral to compression (Ball et al., 2018; Minnen et al., 2018; Chenget al., 2020; Yang et al., 2023); yet they are susceptible to overfitting (Cremer et al., 2018).Many works try to mitigate this problem from the probabilistic methods perspective by newinference techniques or training procedures. In this paper, we approach the problem insteadfrom the deep learning perspective by investigating the effectiveness of using synthetic dataand overparameterization for improving the generalization performance. Our motivationcomes from (1) the recent discussion on whether the increasing amount of publicly accessiblesynthetic data will improve or hurt currently trained generative models (Alemohammadet al., 2024; Bohacek & Farid, 2023; Bertrand et al., 2024; Shumailov et al., 2023); and (2)the modern deep learning insights that overparameterization improves generalization. Ourinvestigation shows how both training on samples from a pre-trained diffusion model, andusing more parameters at certain layers are able to effectively mitigate overfitting in VAEs,therefore improving their generalization, amortized inference, and robustness performance.Our study provides timely insights in the current era of synthetic data and scaling laws.",
  "Introduction": "Variational autoencoders (VAEs, (Kingma & Welling, 2014; Rezende et al., 2014)) are deep probabilisticmodels that consist of an encoder and a decoder. VAEs were originally proposed in the framework of gen-erative models, where the decoder (together with a prior) models the underlying data distribution pdata(x),while the encoder plays only an auxiliary role to speed up training. Nowadays, however, many applicationsuse VAEs for their encoder: VAEs are widely used in science, e.g., for anomaly detection (Cerri et al., 2019)and for learning interpretable representations in quantum optics (Flam-Shepherd et al., 2022) and neuro-science (Zhou & Wei, 2020; Gondur et al., 2023). In compression, VAEs are also the dominating modelsdue to their connection to the rate-distortion theory (Ball et al., 2018; Minnen et al., 2018; Cheng et al.,2020; Yang et al., 2023). These applications can be compromised if the VAE overfits. More specifically, it isempirically observed that the encoder f(x) is more susceptible to overfitting than the decoder (Wu et al.,2017; Cremer et al., 2018; Shu et al., 2018). One possible explanation is that a VAE is normally trained witha finite dataset, due to the ELBO objective and the training algorithm, the encoder is fed with the samedata at each epoch (by contrast, the decoder is fed with random samples from the approximate posteriordistribution, therefore less likely to see the same data twice during training).",
  "Published in Transactions on Machine Learning Research (12/2024)": "an aggregated model complexity axis (i.e., the total number of parameters), even though the raw parametersare increased first along one complexity axis and then along another. In other words, the test error alongeach axis shows the classical U-shape, but plotting them into the same model complexity axis leads to theshape of double descent. We refer readers to (Curth et al., 2023, ) for an illustration. Whetherthis explanation applies to deep double descent is still unknown. Nevertheless, it highlights that increasingdifferent sets of model parameters can impact the model differently.",
  "Performance Metrics for VAEs": "Variational autoencoders (VAEs).A VAE models the data distribution pdata(x) by assuming a gen-erative process that first draws a latent variable z from a prior pz(z) and then draws x from a conditionallikelihood p(x | z), parameterized by the output of a neural network (decoder) g(z) with weights .Given a training data distribution p(x) approximating pdata(x), naive maximum likelihood learning would",
  "ELBO(x) = Ezq(z | x)log p(x | z) + log pz(z) log q(z | x).(2)": "Here, the variational distribution q(z) is a simple probability distribution (often a fully factorized Gaussian)whose parameters are the outputs of the inference network (or encoder) f(x), whose weights are learnedby maximizing L() jointly over all parameters = (, ). While we would like to use pdata(x) for p(x), we only have access to a finite training set Dtrain, which canlead to overfitting. We discuss three performance gaps quantifying the effects of overfitting. Generalization gap.One signal for overfitting is that a model performs better on the training set Dtrainthan on the test set Dtest, and the test set performance decreases over training epochs. We refer to thedifference between training and test set ELBO as the generalization gap,",
  "Gg = ExDtrain [ELBO(x)] ExDtest [ELBO(x)] .(3)": "Since Dtrain and Dtest both consist of samples from the same distribution pdata(x), and training maximizesthe ELBO on Dtrain, the ELBO on Dtrain is typically greater than or equal to the ELBO on Dtest, andGg 0. A smaller Gg corresponds to better generalization performance. Amortization gap.VAEs use amortized inference, i.e., they set the variational parameters of q(z | x)for a given x to the output of the encoder f(x). At test time, we can further maximize the ELBO overthe individual variational parameters for each x, which is more expensive but typically results in a bettervariational distribution q(z | x). We then study the amortization gap (Cremer et al., 2018),",
  "Ga =ExDtest[ELBO(x) ExDtest[ELBO(x) 0(4)": "where we replaced q by q in ELBO. The encoder of a VAE tends to be more susceptible to overfitting thanthe decoder (Wu et al., 2017; Cremer et al., 2018; Shu et al., 2018). When the encoder overfits, its inferenceability might not generalize to test data, which results in a lower ELBO and larger amortization gap Ga. Asmaller Ga corresponds to better generalization performance of the encoder. Robustness gap.An overfitted encoder f(x) potentially learns a less smooth function such that a smallchange in the input space can lead to a large difference in the output space. Hence, it is easier to constructan adversarial sample xa = xr + ( is within a given attack radius ) from a real data point xr Dtest.We construct adversarial samples by maximizing the symmetrized KL-divergence between q(z | xr) andq(z | xa) (Kuzina et al., 2022). For a successful attack, the reconstruction xa = g(za), za q(z | xa),is very different from the real data reconstruction xr = g(zr), zr q(z | xr), even though the inputs xa",
  "Two Paths Towards Generalization": "Given a VAE that overfits and generalizes poorly, we want to know whether using more training data(method A); or more model parameters (method B) can improve its generalization performance andperformance gaps (see ). Here, we compare the two approaches, and discuss how using samplesgenerated from a diffusion model is fundamentally different from naive data augmentation.",
  "L = ExDtrain [ELBO(x)] ,(7)": "which can lead to overfitting. Rather than focusing on model architectures or training techniques as in priorworks (Shu et al., 2018; Zhao et al., 2019; Zhang et al., 2022), we aim to mitigate overfitting by seeking abetter approximation for pdata(x) than Dtrain. An ideal training data distribution should enable us to",
  "(2) an accurate approximation of pdata(x), i.e., we are indeed modeling pdata(x) rather than somedifferent distribution (in practice, it needs to be an accurate model of Dtrain)": "Our hypothesis is that a good diffusion model1 that has been pre-trained on Dtrain satisfies these two criteria:(1) we can generate unlimited samples from it, and (2) its training objective is designed to model pdata(x).Therefore, we investigate training VAEs using a pre-trained diffusion model pDM(x) instead of Dtrain as anapproximation of the underlying distribution pdata(x), i.e., by maximizing",
  "L = ExpDM(x) [ELBO(x)] .(8)": "We denote this method DMaaPx, short for Diffusion Model as a pdata(x). illustrates the intuitionbehind DMaaPx. The blue dots represent the finite data set Dtrain. They are i.i.d. samples from the unknownunderlying data distribution pdata(x) which is shown by the dark-edged region. The green regions representpDM(x). We use shaded areas (green), not dots, to highlight that pDM(x) models a continuous distributionwhich we can generate infinitely many samples from. Diffusion models for data types other than images are less explored and might not accurately approximatepdata(x) and therefore might not satisfy criterion (2).Moreover, due to the data processing inequality,information on pdata(x) captured by a diffusion model trained on Dtrain cannot exceed the informationcontained in Dtrain (but for injected information via inductive biases). In reality, state-of-the-art diffusionmodels cannot fit Dtrain perfectly. Many recent works observe in both image and text settings that traininggenerative models on generated data degrades the overall performance (Alemohammad et al., 2024; Bohacek& Farid, 2023; Bertrand et al., 2024; Shumailov et al., 2023). Hence, the continuity we gain by replacingDtrain with pDM(x) comes at the cost of potentially losing some amount of information contained in Dtrain.",
  "where paug(x | x) is a distribution over some hand-crafted transfor-mations (e.g., scalings and rotations) of x Dtrain": "Like DMaaPx, data augmentation replaces Dtrain with a continuousdistribution paug(x) = ExDtrain[paug(x | x)]. But paug(x) can bea less accurate approximation of pdata(x) than pDM(x) ().Firstly, typical data augmentation techniques generate new trainingpoints x by conditioning on a single original data point x (i.e., thenested expectations in Eq. (9)). Thus, paug(x) cannot interpolatebetween points in Dtrain (i.e., the gaps between purple regions in). By contrast, in DMaaPx, each training data point x pDM(x) is drawn from a diffusion model trained on the entire datasetDtrain. Hence, each x is effectively conditioned on the full Dtrain. Secondly, the transformations used for paug(x | x) are drawn froma manually curated catalog. This catalog is heavily based on priorassumptions regarding invariances in the data type under consideration, which can introduce bias.In 1An unconditional diffusion model, as opposed to a conditional one like Stable Diffusion (Rombach et al., 2022).2Using samples from generative models for training is sometimes considered as a kind of data augmentation in the contextof supervised learning (Yang et al., 2022). In the present work, we deliberately separate generative models from the broadersense of data augmentation, and we consider data augmentation in a narrow sense such that the augmented data is an outputof some transformation conditioned on an single x Dtrain.",
  "A Distillation Perspective": "Using samples from a pre-trained diffusion model to train a VAE can also be viewed from a distillationperspective (Hinton et al., 2015). Distillation describes the process of transferring knowledge from a largemodel to a smaller one. In practice, distillation is often used because a smaller model is less expensive to bedeployed in production. On the contrary, here we consider transferring knowledge between models designedwith different modeling assumptions or structures. We refer to this as cross-model-class distillation, andthe conventional usage of distillation as within-model-class distillation. While both types seek to transferknowledge from a source to a target model, cross-model-class distillation emphasizes more on enhancingfunctionalities that are unique to the target model rather than mirroring the capabilities shared with thesource model. For instance, Frosst & Hinton (2017) distill a neural network into a soft decision tree toimprove its unique capability of providing explanations to decisions. DMaaPx belongs to cross-model-class distillation, i.e., it distills diffusion models to VAEs.The goal ofDMaaPx is not to rival diffusion models in sample quality, but rather to improve the unique capabilities ofVAEs such as its variational bottleneck (Alemi et al., 2016) for learning fast, controllable, and interpretablerepresentations (see for a discussion). Hence, DMaaPx fundamentally differs from approachesthat train VAEs on samples produced by VAEs (Shumailov et al., 2023), or diffusion models on outputs ofdiffusion models (Alemohammad et al., 2024), which belongs to within-model-class distillation.",
  "Method B: More Parameters": "The success of deep learning and empirical evidence (Nakkiran et al., 2021) show that increasing the numberof parameters in a model can improve its generalization performance. However, increasing the number ofparameters in one part of the model can have a different effect than increasing the number of parametersin another part of the model.Different sets of parameters form distinct complexity axes, along whichgeneralization performance can behave in different ways. In practice, increasing the number of parameters in VAEs is usually done by changing two hyperparameters,which control two distinct sets of model parameters, = z z. Here, z contains the parametersthat determine the latent dimension dz and interact directly with z, i.e., the weights in the last layer ofthe encoder f(x) and the first layer of the decoder g(z). Changing the number |z| of parameters in zchanges the dimension dz of z. We denote the remaining parameters as z. We investigate the effect of increasing |z| and |z| separately as the increase has different implicationsfor each parameter group. For both sets, we only consider changing the width of the VAE, not the depth.For a VAE with mean field assumption, for example, increasing |z| implies that we update our belief aboutthe underlying data generative process, which we now believe to involve more independent factors (hence,we increase dz to model these additional independent factors). Explaining Double Descent with Multiple Complexity Axes.With the two complexity axes |z|and |z| mentioned above, we might potentially observe the phenomenon of double descent in VAEs.Recent work by Curth et al. (2023) tries to explain double descent (see ) in non-deep models suchas trees and linear regression. They suggest that double descent is the result of plotting the test error along",
  "Experimental Setup": "Datasets.We evaluate both methods A & B, on CIFAR-10 (Krizhevsky et al., 2009). We further evalu-ate method A on BinaryMNIST (LeCun et al., 1998) and FashionMNIST (Xiao et al., 2017), which showconsistent results with CIFAR-10 (see Appendix B.3). Due to computation constraints, we did not evaluatemethod B on these two easier datasets. As a preparation for DMaaPx in method A, we train a diffusionmodel pDM(x) on each training set Dtrain, which we then use to generate the training data for the VAEs.We use the implementation by Karras et al. (2022). Further details can be found in Appendix D. VAE architectures.We assume fixed Gaussian priors pz(z) = N(0, I). For the conditional likelihoodp(x | z), we use a discretized mixture of logistics (MoL; (Salimans et al., 2017)). For the inference modelq(z | x), we use fully factorized Gaussian distributions whose means and variances are the outputs of theencoder. Both the encoder and the decoder consist of convolutional layers with residual connections. Thenumber of output channels for the encoder and the number of input channels for the decoder (i.e., thelatent dimension of z) is denoted as dz = mz 64, where mz is an integer multiplier. The number of theinternal channels, which governs the rest of the parameters, is denoted as nc. Hence, |z| = dz c1 and|z| = ncc2, where c1 and c2 are the appropriate constants. For more details on the network architectures,hyperparameters, and training please consult Appendix B.1. Baselines.The default models for method A and the baseline for method B use mz = 1 and nc = 256. Formethod A, we compare VAEs trained with DMaaPx against three other models trained on: (i) repetitions ofDtrain (Normal Training); (ii) carefully tuned augmentation for Dtrain (Aug.Tuned); and (iii) plausibleaugmentation for images in general (Aug.Naive). The results are averaged over 3 random seeds. Notethat Aug.Naive is not tuned towards a given training dataset Dtrain and can result in out-of-distributiondata, e.g. a horizontally flipped digit 2 for MNIST. This mimics situations where the invariances of thedata modality are less clear than for images. We report the specific augmentations used for Aug.Tunedand Aug.Naive in Appendix E. For method B, we compare VAEs trained with various mz {1, . . . , 64} andnc {1, . . . , 512}. When documenting the training progress, the term epoch usually refers to one completepass over Dtrain. For DMaaPx, this term is not applicable since it can sample unlimited data from pDM(x).Therefore, we measure training progress of DMaaPx in effective epochs, which count multiples of |Dtrain|training samples. We train all models for 1000 (effective) epochs.",
  "Training with synthetic data improves generalization and minimizes the gaps": "shows the performance for generalization (left), amortized inference (mid), and robustness (right)across normal training, augmentations, and DMaaPx. We observe that DMaaPx has the highest ELBOon Dtest (left, solid green) and the smallest generalization, amortization, and robustness gaps (Eqs. 3-5).Augmentation provides competitive improvements, but is not as good as DMaaPx. This implies that VAEstrained with DMaaPx approximate the underlying distribution pdata(x) better than those trained on Dtrainsolely, or on augmented data, as suspected in .1.1.",
  "ELBO": ": Generalization (left), amortized inference (mid), and robustness (right) performance for VAEstrained with Eqs. (7)-(9).Being slightly better than augmentations, DMaaPx consistently has the bestperformance on the test set and the smallest generalization, amortization, and robustness gap. We also evaluate whether the improvements in generalization from DMaaPx and augmentation propagateto downstream tasks for which VAEs are better suited than diffusion models. We assess the representationlearning performance by classifying the mean of q(z | x) for each x. We also test both the quality androbustness of the learned representations by using CIFAR-10-C (Hendrycks & Dietterich, 2019), a datasetcontaining 19 versions of the CIFAR-10 test set, each corrupted with a different corruption.For everytest set, we encode each image to its and then split all representations into two separate subsets. Weuse one subset to train a classifier and test on the other. Our experiments include four classifiers: logisticregression (LR), a support vector machine (Boser et al., 1992) with linear kernel (SVM-L), an SVM withradial basis function kernel (SVM-RBF), and k-nearest neighbors (kNN) with k = 5. (left) showsthat classifiers trained with representations from DMaaPx (green) outperform normal training on average(details in Appendix G). By contrast, augmentation (purple) hurts performance. Comparison to Prior Work on Modifying the Training Procedures.In we compare thegeneralization performance of DMaaPx to the previously proposed Reverse Half Asleep (RHA) method of(Zhang et al., 2022).Unlike DMaaPx, which focuses on the training data, RHA modifies the trainingprocedure of a VAE such that first a VAE is trained 500 epochs, and then the decoder is fixed on only theencoder is trained on samples of the decoder for an additional 500 epochs. We find that DMaaPx, beingconceptually much simpler, outperforms RHA significantly.",
  ":Density of log q evaluated on aline that linearly interpolates between twodata samples from the test set of CIFAR-10.DMaaPx is smoother than normal training": "We hypothesize that an overfitted encoder often learns a lesssmooth f(x) meaning that a small change in x can result in alarge difference in the latent space. Here, we report an exem-plary density for an interpolation in the latent space of a VAEtrained on CIFAR-10 with normal training and DMaaPx. Wemap two data samples x0 and x1 from the test set of CIFAR-10 to their latent means using the encoder z0 = 0 = f (x0)where f (x) denotes the encoder returning only the mean ofq(x). We interpolate linearly in the latent space and evaluatelog q(z | x) where z = z0 + (1 )z1, and xis a reconstruction of z.",
  "DMaaPx (for various k)": ":Left: Improvements in classification accuracy overnormal training for various classifiers trained on the latent rep-resentations of CIFAR-10-C. Each column contains 19 3 points(i.e., 19 corruptions, each VAE trained with 3 random seeds).Right: Generalization performance as a function of the amount kof training data sampled from a diffusion model. Horizontal bluelines show baseline performance (VAE trained directly on Dtrain).All VAEs were trained for 1000 effective epochs. k 10 suffices.",
  "Do we need unlimited synthetic data?": "With the diffusion model in DMaaPx, we can train a VAE on unlimited samples from pDM(x), whichimproves various performance metrics as demonstrated above. However, generating lots of samples from adiffusion model is computationally expensive (see also Appendix D.2). Therefore, we further explore: Dowe really need an infinite number of samples? The answer, reassuringly, is No. (right) shows the generalization performance of DMaaPx on CIFAR-10 where the amount of trainingdata is restricted to k |Dtrain| with k = 1, . . . , 1000. After k effective training epochs samples repeat. Allmodels are trained for 1000 effective epochs. The horizontal blue lines represent the generalization gap ofnormal VAE training (on Dtrain and k = 1) at epoch 1000 from (left). For k = 1, DMaaPx matchesnormal training on CIFAR-10. The ELBO plateaus for k 10, indicating that approximately 10 |Dtrain|samples offer a similar generalization as 1000 |Dtrain| samples.",
  "Increasing |z| improves generalization and minimizes the gaps": "shows the generalization (left), amortized inference (mid), and robustness (right) performancebetween the baseline parameter setting (mz = 1, nc = 256) and the setting with an increased amount ofparameters (mz = 10, nc = 256). Note that we only increase |z| here and keep |z| unchanged. Weobserve that mz = 10 has the higher ELBO on Dtest (left), and the smaller generalization, amortization, and",
  "robustness gaps (Eqs. 3-5). The resulting performance improvements are similar to DMaaPx in ,where we also see a higher ELBO test and smaller gaps": "We also investigate whether continuing to increase mz will lead to better or worse generalization performance. (a, b) show trends of the ELBOs on Dtest and the generalization gaps (Eq. (3)) as the result ofincreasing mz from 1 to 64. We see that increasing mz leads to higher test ELBO for normal training untilmz = 16, after which the test ELBO stays around the same level. We also find that the benefit of usingsynthetic data (i.e., DMaaPx) for improving test ELBO diminishes as mz increases. In particular, aftermz = 16, both normal training and DMaaPx have similar test ELBOs. However, we want to highlightthat DMaaPx has always a smaller generalization gap than normal training, even after mz = 16. Smallgeneralization gaps are desirable as they imply that the ELBO evaluated on the training set can be used topredict the performance of a VAE on a held-out test set.",
  "Increasing |z| might hurt generalization": "We further investigate the effect of increasing parameters along the other complexity axis (i.e., |z|). (c, d) show the test ELBOs and the generalization gaps (Eq. (3)) as functions of nc = 1 to512. Subplot (c) shows that when mz = 1 and mz = 2, increasing nc results in the classical U-shape ofoverfitting for normal training, where the test performance first grows then drops. Increasing mz further to8 and 64 tilts the right-hand side of the U-shaped curve upward and eliminates the symptom of declininggeneralization w.r.t. larger nc, which aligns with (a). In particular, when mz = 64, we see improvedon generalization as we keep increasing nc, even though the improvement from enlarging mz is alreadysaturated (as showed in (a)). This implies that: (1) the two sets of parameters z and z doindeed have different meanings; (2) increasing |z| when |z| is small hurts generalization performance;(3) increasing both |z| and |z| leads to better generalization than increasing only one of them. The plot for generalization gaps ( (d)) shows that, by increasing nc, the gaps for normal trainingbecome larger. However, increasing mz reduces the gaps (which aligns with (b)). In addition, bothplots ( (c) and (d)) show that using synthetic data (i.e., DMaaPx) on top of adding parametersdoes not hurt and often improves generalization. Especially, using DMaaPx constantly results in near-zerogeneralization gaps (see the overlapping flat lines in (d)).",
  "Double descent in VAEs?": ".5 and .6 indicate that the modern wisdom of larger models generalize better doesindeed apply to VAEs.Therefore, we are curious whether the double descent phenomenon also ap-plies here. (a) and (c) show that if we fix one complexity axis and only increase parame-ters along the other complexity axis, the test ELBO exhibits either a U-shaped or a L-shaped curve,which does not look like double descent.However, one can artificially construct a double descentbehavior if one sequentially increases parameters first along one axis and then along the other axis,and plots the test ELBO against the overall model complexity (i.e., the total number of parameters).This is illustrated in .The figure shows the negative test ELBO, which shows three phases:",
  "Fixed mzFixed nc": ":By increasing the totalnumber of parameters first along thecomplexity axis |z| (i.e., larger nc,cyan), then along the complexity axis|z| (i.e., larger mz, brown line), thenback to |z| (cyan), we see indica-tions of double descent. (1) we start with a fixed mz = 1 and increase nz from 1 to 256 (thecyan U-curve); (2) we then fixed nz = 256 and start increasing mzfrom 1 to 64 (the brown vertical line); (3) lastly, we fixed mz = 64and increase nz from 256 to 512 (the cyan horizontal line). Phase(2) seems vertical due to the fact that increasing mz by 1 adds muchless raw parameters than increasing nz by 1. Now we can see indications of a double descent behavior as the neg-ative test ELBO goes down, up, and down again as we keep in-creasing the total number of parameters in the model. While thedouble-descent behavior was enforced artificially here by the choiceof trajectory in model complexity space, it is conceivable that atrained models may allocate resources in a similar way while follow-ing a more natural trajectory.This aligns with our discussion in .2 and the work by Curth et al. (2023), who point out thatdouble descent in non-deep models is due to increasing parametersalong distinct complexity axis sequentially but plotting them on acombined complexity axis.",
  "Conclusion": "In this paper, we study overfitting and generalization in VAEs by investigating the effects of (a) trainingwith synthetic data and (b) increasing the number of model parameters, both of which are timely questionsto be investigated in generative models. We find that training on samples from a diffusion model that waspre-trained on the training dataset consistently improves performance. Generally, increasing the amount ofparameters also helps. However, when constrained by resources, one should prioritize the parameters relatedto the latent dimension over the parameters that do not directly impact the latent dimension. Applying (a)and (b) together can further improve the performance. Additionally, we find indications of a double-descentphenomenon in VAEs which we defer to future work for an investigation in detail. The authors would like to thank Anna Kuzina, Zhen Liu, and Weiyang Liu for helpful discussions. Fundedby the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys ExcellenceStrategy EXC number 2064/1 Project number 390727645. This work was supported by the GermanFederal Ministry of Education and Research (BMBF): Tbingen AI Center, FKZ: 01IS18039A. RobertBamler acknowledges funding by the German Research Foundation (DFG) for project 448588364 of theEmmy Noether Programme. The authors thank the International Max Planck Research School for IntelligentSystems (IMPRS-IS) for supporting Tim Z. Xiao and Johannes Zenn.",
  "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information bottle-neck. In International Conference on Learning Representations, 2016. 2, 6": "Sina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun, Hossein Babaei, DanielLeJeune, Ali Siahkoohi, and Richard Baraniuk. Self-consuming generative models go MAD. In Interna-tional Conference on Learning Representations, 2024. 1, 4, 5, 6 Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neuralnetworks, going beyond two layers. Advances in neural information processing systems, 32, 2019. 4 Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. Syntheticdata from diffusion models improves imagenet classification. arXiv preprint arXiv:2304.08466, 2023. 4, 25",
  "Peter L Bartlett, Philip M Long, Gbor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression.Proceedings of the National Academy of Sciences, 117(48):3006330070, 2020. 4": "Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practiceand the classical biasvariance trade-off. Proceedings of the National Academy of Sciences, 116(32):1584915854, 2019. 2, 4 Quentin Bertrand, Joey Bose, Alexandre Duplessis, Marco Jiralerspong, and Gauthier Gidel. On the stabilityof iterative retraining of generative models on their own data. In International Conference on LearningRepresentations, 2024. 1, 5 Sebastian Bischoff, Alana Darcher, Michael Deistler, Richard Gao, Franziska Gerken, Manuel Gloeckler, LisaHaxel, Jaivardhan Kapoor, Janne K Lappalainen, Jakob H Macke, et al. A practical guide to sample-based statistical distances for evaluating generative models in science. Transactions on Machine LearningResearch, 2024. 26",
  "David Minnen, Johannes Ball, and George D Toderici.Joint autoregressive and hierarchical priors forlearned image compression. In Advances in Neural Information Processing Systems, 2018. 1, 2": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.Deepdouble descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory andExperiment, 2021(12):124003, 2021. 2, 4, 6 Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deepgenerative models know what they dont know? In International Conference on Learning Representations,2018. 19 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International Conference on Machine Learning, 2021. 4",
  "Lucas Theis, van den Oord Aron, and Matthias Bethge. A note on the evaluation of generative models. InInternational Conference on Learning Representations, 2016. 29": "Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic imagesfrom text-to-image models make strong visual representation learners. arXiv preprint arXiv:2306.00984,2023. 4, 25 Ba-Hien Tran, Giulio Franzese, Pietro Michiardi, and Maurizio Filippone. One-line-of-code data mollifi-cation improves optimization of likelihood-based generative models. In Advances in Neural InformationProcessing Systems, 2023. 21",
  "BinaryMNISTBernoullifully-connectedFashionMNISTfixed-variance Gaussianfully-connectedFashionMNISTMoLfully-connectedCIFAR-10MoLresidual network": "We consider a fully-connected architecture and a residual architecture (He et al., 2016). gives moredetails on the likelihood model and architecture. For all VAEs, we choose the hyperparameters of the VAEmodels by consulting existing literature. The MoL likelihood for FashionMNIST is discussed in Appendix B. The fully-connected architecture maps from an input dimension of 322 to a hidden dimension of 512. Aftera hidden layer mapping from 512 to 512, the output is mapped to a latent variable of dimension 16. Thedecoder mirrors the encoder and maps the latent variable of dimension 16, via three layers, to the originalinput size.",
  "B.2Reconstructions": "shows reconstructions for CIFAR-10. We reconstruct a random subset of 9 samples from the testset. The VAEs we use are quite small and simple, they are also non-hierachical, which makes it hard tomodel CIFAR-10 well. Therefore, qualitatively the differences between the methods are quite subtle. Butstill, we can see that the DMaaPxs reconstruction is slightly better for the rightmost image in the secondrow.",
  "B.4Different Conditional Likelihoods": "VAEs modeling assumptions for the conditional likelihood p(x | z) often differ based on data or use case.While a Gaussian likelihood is used for applications that focus on low reconstruction error (e.g., lossy datacompression), a MoL likelihood is used if the density of the data matters (e.g., generative modeling orlossless data compression). While the experiments in the main text considered a MoL likelihood, the modeltrained on FashionMNIST () uses a Gaussian likelihood and the model trained on BinaryMNIST() uses a Bernoulli likelihood (also compare Appendix B.1). also evaluates MoL forFashionMNIST, and we observe a similar behavior as in its Gaussian counterpart in (left). Insummary, DMaaPx is less prone to overfitting than normal training and augmentation, for all investigatedlikelihoods.",
  "Exp(x) [ELBO(x)] Exp(x) [log p(x)] = H[p(x), p(x)] H[p(x)],(11)": "where H denotes the (cross) entropy. Therefore, the ELBO on Dtest can be higher than the ELBO on Dtrain, ifDtrain and Dtest are not drawn from the same distribution, and Dtest has a lower entropy than Dtrain. Indeed,this phenomenon has been observed in the out-of-distribution setting when testing on a low-entropy data set(Nalisnick et al., 2018). shows the ELBOs analogous to the generalization gaps in , , and , but thedotted lines plot the ELBO on the actual training distribution (e.g., on samples from pDM(x) for DMaaPx).The point of this plot is to warn that comparisons between ELBOs under such different distributions arenot meaningful, and should not be used to calculate the generalization gap. For example, note that theplot would suggest a negative generalization gap for data augmentation (purple) on BinaryMNIST. This isconsistent with the remark above: since the ELBO is bounded by the negative entropy of the distributions on",
  "DMaaPxDMaaPx+Aug.MollifyAug.Mollify": ": Generalization gap as a function of effective epochs. We compare normal training to DMaaPx,Aug.Mollify, and the combination of DMaaPx and Aug.Mollify. Aug.Mollify reaches a similar performanceas the baseline. Combining DMaaPx and Aug.Mollify does not improve upon DMaaPx. Tran et al. (Tran et al., 2023) show in their recent work that adding (scheduled) noise to the training dataduring the training process helps the optimization of likelihood-based generative models. More specifically,they anneal the noise schedule from a large variance to a small one for half of the training epochs and, finally,use the original training data for the other half of the training epochs. They argue that data typically resideson a low-dimensional manifold that is embedded in a larger dimension. By adding noise to the data duringtraining one circumvents the problem of density estimation in input space with few samples and, additionally,prevents the model from overfitting to the data manifold. One can also view this idea from a continuationmethod perspective as the method finds a good initialization point for the optimization. shows the performance of the mollification method (Aug.Mollify) applied to VAEs on the CIFAR-10 dataset (dark).We compare the performance to normal training (blue), DMaaPx (green), and thecombination of DMaaPx and the Aug.Mollify.We find that Aug.Mollify reaches a similar performanceas the baseline but does not improve upon it. Also, combining DMaaPx and Aug.Mollify does not bringimprovements compared to DMaaPx. However, DMaaPx and Aug.Mollify have similarly small generalizationgaps.",
  "B.7Mixing Synthetic Data with Real Data": "We evaluate the performance of VAEs trained on a mix of x {0, 20, 40, 60, 80, 100}% of real data and100 x% of synthetic data. We train the models without augmentation and with Aug.Tuned. Note thatx = 0% (without augmentation) corresponds to the DMaaPx method and x = 100% (without augmentation)corresponds to the normal training baseline. x = 100% (with augmentation) corresponds to the Aug.Tunedbaseline. Considering no augmentation, when moving from a mix of 100% of real data to 0% of real data, we findthat the performance is continuously increasing (i.e., the ELBO evaluated on Dtest increases) and the gen-eralization gap is shrinking. Adding augmentation improves both, the ELBO evaluated on Dtest and thegeneralization gap. Note that DMaaPx (x = 0%, no augmentation) shows the highest ELBO evaluated onDtest and the smallest generalization gap.",
  "Evaluated on DtrainEvaluated on Dtest": ":ELBO evaluated on Dtrain (dashed) and Dtest (solid) for DMaaPx (green) trained on samplesfrom a diffusion model which has been trained on x {10, 30, 50, 70, 90}% of CIFAR-10.We plot theperformance of normal training (i.e., VAE trained with the original full CIFAR-10 only) as horizontal bluelines. Using 30% of data for DMaaPx shows similar performance than normal training (with a significantlysmaller gap). Using more data outperforms normal training. More details are provided in Appendix B.8.",
  "We follow the setup of Karras et al. (Karras et al., 2022) for the design and training of our diffusion model.However, we do not use the proposed augmentation pipeline during training": "We train the diffusion model on 50,000 training data points (i.e., the size of the original CIFAR-10 trainingset in the case of CIFAR-10) for 4000 epochs. Each model is trained on 8 NVIDIA A100 80GB GPUs forapproximately 1 days. We utilized the deterministic second-order sampler as proposed by Karras et al. (Karras et al., 2022) with 18integration steps. Each sampled image utilizes a unique initial seed. We sample on a single NVIDIA A10040GB GPU. Sampling 50, 000 images takes approximately 25 to 30 minutes.",
  "This section discusses the computational requirements of the DMaaPx method in general": "In this paper, we use the method DMaaPx as a tool to study the difference between naive augmentationsand synthetic data from diffusion models when training VAEs. Our evaluations show that there are scenarioswhere exploiting the modelling assumption (i.e., both VAEs and diffusion models are probabilistic models)helps, which is often overlooked by the generative modelling community. If a pre-trained diffusion model is available, DMaaPx can be applied with a negligible computational overhead(see , right). This setup follows recent trends where using synthetic data from pre-trained diffusionmodels is becoming a common practice for, e.g., classification (Azizi et al., 2023), robustness (Croce et al.,2021; Wang et al., 2023), and representation learning (Tian et al., 2023). If no pre-trained diffusion model is available, a diffusion model has to be trained in a first step to applyDMaaPx. Whether this is feasible might depend on the specific application. However, we want to emphasizethat once the diffusion model is trained, it can be used to generate as many samples as needed for trainingas many VAEs as needed.",
  "D.3Discriminating Synthetic and Real Samples by Classification": "One way to evaluate the quality of the generated samples is by training a classifier to discriminate thesynthetic and the real data. This method is also known as Classifier Two-Sample Tests (C2ST), which is ametric for evaluating generative models (Bischoff et al., 2024). Here, we train five classifiers to discriminatereal data (images taken from CIFAR-10) and synthetic data (images sampled from the diffusion models)where we use x {30, 50, 70, 90, 100}% of the original CIFAR-10 to train the diffusion models, respectively. We use a ResNet 18 (He et al., 2016) for classifying real data (with label 1) and synthetic data (generated bythe diffusion model). We train each classifier for 14 epochs with a batch size of 256 with stochastic gradientdescent (Robbins & Monro, 1951) with a learning rate of 0.001, a momentum (Rumelhart et al., 1986) of 0.9and weight decay of 5 104. shows the classification accuracies for discriminating between real and synthetic data. While allsamples can be classified with an accuracy > 50%, we find a negative trend between the amount of datathat the diffusion model is trained on (x {30, 50, 70, 90, 100}%) and the classification accuracy. Hence, itis easier to classify samples from a diffusion model that has been trained on less data than samples from adiffusion model that has been trained on more data. Using the maximum amount of training data available,which DMaaPx does, leads to samples that can be distinguished least from real data.",
  "Classication Accuracy": ":Accuracy of classifier trained to distinguish real data (CIFAR-10) and data sampled from adiffusion model. The diffusion model is trained on x {30, 50, 70, 90, 100}% of real data. The more datais used the closer synthetic samples are to real samples (i.e., smaller classification accuracy). Using 100%of CIFAR-10 leads to samples least distinguishable from real data. Accuracies are averaged over 5 randomseeds. Error bars show 1.96 standard deviations. For details see Appendix D.3.",
  "lists naive augmentation for BinaryMNIST, FashionMNIST, and CIFAR-10. lists augmen-tation tuned to the BinaryMNIST and the FashionMNIST dataset. lists augmentation tuned tothe CIFAR-10 datset": "They perform similarly overall in Figures 2, 10 and 11. However, Aug.Naive outperforms Aug.Tuned ingeneralization on BinaryMNIST and FashionMNIST, and in robustness across all datasets. This is surprisingas naive augmentation might produce out-of-distribution data, like a horizontally flipped digit 2, potentiallyimpairing performance. Thus, designing augmentation can be labor-intensive. :List of specific augmentations applied to BinaryMNIST, FashionMNIST and CIFAR-10. We referto this set as naive augmentation as it is targeted towards images in general (and not towards specificdatasets). Each specific augmentation is applied with probability b.",
  "augmentationdescription and hyperparameters": "horizontal flipflip an image horizontallytranslationtranslate an image in x and y direction for t {0, 1, 2, 3} pixelsscalingscale an image by 2scale with scale [0, 0.2]rotationrotate an image by d degrees with d anisotropic scalingdo anisotropic scaling with scale 2anisoscale (anisoscale [0, 0.2])anisotropic rotationdo anisotropic rotation with a probability of 0.5brightnesschange the brightness of an image by brightness [0, 0.2]contrastchange the contrast of an image by 2contrast where contrast [0, 0.25]huechange the hue by rotation of rhue with rhue [0, 0.25 ]saturationchange the saturation of an image by 2saturation where saturation [0, 0.5]",
  "FPractical Evaluation of VAEs on Three Tasks": "The improvements of generalization performance, amortized inference and robustness in VAEs have directimpacts on the applications of them. In this section, we evaluate three popular tasks that VAEs are used for,based on whether a task only involves the encoder, the decoder, or both as in (Xiao & Bamler, 2023): (a)representation learning (i.e., using only the encoder); (b) data reconstruction (i.e., using both the encoderand the decoder); and (c) sample generation (i.e., using only the decoder). Representation learning (with classification as the downstream task).We evaluate the represen-tation learning performance by classification accuracies on the mean of q(z | x) for each x. First, wefind the learned representations for all data points in the CIFAR-10 test set. Afterwards, we split theminto two separate subsets. We use one subset to train the classifier, and test it on the other subset. Ourexperiments include four different classifiers: logistic regression, a support vector machine (Boser et al., 1992)with radial basis function kernel (SVM-RBF), a SVM with linear kernel (SVM-L), and k-nearest neighbors(kNN) with k = 5. (representation learning; RL) shows the resulting test accuracies across allmodels considered. We find that VAEs trained with DMaaPx (in bold) are slightly better than other modelson average (with overlapping standard deviations), which implies the task of representation learning mightbenefits from the smaller gaps evaluated in .2. Data reconstruction.Tasks such as lossy data compression (Ball et al., 2017) rely on the reconstructionperformance of VAEs. We evaluate the reconstruction performance of VAEs trained on CIFAR-10 using thepeak signal-to-noise ratio (PSNR; higher is better). (reconstruction; RC) shows that DMaaPxperforms slightly better than the others on average, but with overlapping standard deviations. Sample generation.We evaluate the quality of samples generated by VAEs trained on CIFAR-10 withthe methods explained in the main text (Normal Training, DMaaPx, Aug.Naive, Aug.Tuned). We reportFrchet Inception Distance (Heusel et al., 2017) (FID; lower is better) and Inception Score (Salimans et al.,2018) (IS; higher is better). (sample quality; SQ) shows that DMaaPx performs slightly betterthan the others on average, with overlapping standard deviations, when sample quality is measured in FID,but Normal Training performs better when sample quality is measured in IS. Overall, VAEs trained with DMaaPx show improvements for representation learning and data reconstruction,and perform similarly to normal training on sample quality. At the same time, VAEs trained with bothaugmentations seem to have slightly worse performance for representation learning and sample generation,and perform similarly on the reconstruction task when compared to normal training. The results of DMaaPxin the table is consistent with our claim that the proposed method mainly fixes the encoder, which affectsrepresentation learning and reconstruction but not sample quality. Additionally, Theis et al. (Theis et al.,2016) show that a generative model with good log-likelihood (i.e., high test ELBO in the case of a VAE)does not necessarily produce great samples. :Evaluation of downstream applications of VAEs on CIFAR-10: representation learning withclassification as the downstream task (RL), reconstruction (RC), and sample quality (SQ). Results areaveraged over 3 random seeds. Note that most differences are smaller than the standard deviations. SeeAppendix F for a discussion of the results.",
  "GPractical Evaluation of VAEs on CIFAR-10-C": "On CIFAR-10-C (Hendrycks & Dietterich, 2019), we evaluate the representation learning capabilities ofVAEs trained with DMaaPx, Aug.Naive, and Aug.Tuned by evaluating the classification performance of aclassifier trained in latent space. The experiment are similar to the evaluation of representation learningabove (see Appendix F). For each of the 19 classes of corruptions we train four classifiers on a subset ofthe CIFAR-10-C test set and evaluate its performance on the subset that was not used for training (termedoverall performance). Additionally, we sample a random (one out of 19 possible) corruption for each ofthe images of CIFAR-10-C and evaluate its performance. During training we use the strongest degree ofcorruption. shows results across all 19 corruptions and the overall performance. DMaaPx achieves consis-tently the best mean across all corruptions for the linear classifiers and DMaaPx achieves the best mean foralmost all of the corruptions when regarding the non-linear classifiers. See also .2 for a discussionof the experiments."
}