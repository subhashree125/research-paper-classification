{
  "Abstract": "The increasing depth of parametric domain knowledge in large language models (LLMs) is fu-eling their rapid deployment in real-world applications. Understanding model vulnerabilitiesin high-stakes and knowledge-intensive tasks is essential to quantifying the trustworthinessof model predictions and regulating model use. The recent discovery of named entities asadversarial examples (i.e. adversarial entities) in natural language processing tasks raisesquestions about their potential impact on the knowledge robustness of pre-trained and fine-tuned LLMs in high-stakes and specialized domains. We examined the use of type-consistententity substitution as a template for collecting adversarial entities for medium-sized billion-parameter LLMs with biomedical knowledge. To this end, we developed an embedding space,gradient-free attack based on powerscaled distance-weighted sampling for robustness evalu-ation, which has a low query budget and controllable coverage. Our method has favorablequery efficiency and scaling over alternative approaches based on blackbox gradient-guidedsearch, which we demonstrated for adversarial distractor generation in biomedical questionanswering. Subsequent failure mode analysis uncovered two regimes of adversarial entitieson the attack surface with distinct characteristics. We also showed that entity substitutionattacks can manipulate token-wise Shapley value explanations, which become deceptive inthis setting. Our approach complements standard evaluations for high-capacity models andthe results highlight the brittleness of domain knowledge in LLMs1.",
  "Introduction": "LLMs such as pre-trained and finetuned generalist language models (GLMs) and their domain-adaptedversions (Zhao et al., 2023; Wang et al., 2023a) incorporating transformer architectures are emerging as thetechnological backbone of next-generation search engines and knowledge bases (Petroni et al., 2019; Sunget al., 2021). They are increasingly used in knowledge-intensive tasks by the general public without sufficientassessment of inherent issues in trustworthiness and safety (Clusmann et al., 2023; Spitale et al., 2023; Siet al., 2023). Adversarial examples are valuable probes for model vulnerability and robustness, and areessential for performance improvements and regulatory audits (Shayegani et al., 2023). Adversarial attacksare classified by query scaling behavior into low- and high-query-budget attacks (see a). Collectingadversarial data at scale, either for adversarial training or robustness assessment, requires query-efficientattacks, first empirically studied in the vision domain (Ilyas et al., 2018; Cheng et al., 2019; Shukla et al.,2021). For language models, adversarial examples are generated through guided search (Alzantot et al.,2018), sampling and probabilistic methods (Ren et al., 2019; Yang et al., 2020; Yan et al., 2022), or collecteddynamically by human operators (Jia & Liang, 2017; Wallace et al., 2019b). Most of these methods require a",
  "Published in Transactions on Machine Learning Research (12/2024)": "Jun Yan, Yang Xiao, Sagnik Mukherjee, Bill Yuchen Lin, Robin Jia, and Xiang Ren. On the Robustness ofReading Comprehension Models to Entity Renaming. In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.508520, Stroudsburg, PA, USA, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.37. URL Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I Jordan. Greedy Attack and Gum-bel Attack: Generating Adversarial Examples for Discrete Data. Journal of Machine Learning Research,21(43):136, 2020. URL Zhen Yu, Zhenhua Chen, and Kun He. Query-Efficient Textual Adversarial Example Generation for Black-Box Attacks. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Proceedings of the 2024 Conferenceof the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies (Volume 1: Long Papers), pp. 556569, Mexico City, Mexico, June 2024. Association for Com-putational Linguistics. doi: 10.18653/v1/2024.naacl-long.31. URL Zheng Yuan, Zhengyun Zhao, Haixia Sun, Jiao Li, Fei Wang, and Sheng Yu. CODER: Knowledge-infusedcross-lingual medical term embedding for term normalization. Journal of Biomedical Informatics, 126:103983, feb 2022. ISSN 15320464. doi: 10.1016/j.jbi.2021.103983. URL Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li. Adversarial Attacks on Deep-learningModels in Natural Language Processing. ACM Transactions on Intelligent Systems and Technology, 11(3):141, jun 2020. ISSN 2157-6904. doi: 10.1145/3374217. URL Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A Surveyof Large Language Models. arXiv, pp. 2303.18223, mar 2023. URL Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large Language Models Are NotRobust Multiple Choice Selectors. In The Twelfth International Conference on Learning Representations,2024. URL Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal andTransferable Adversarial Attacks on Aligned Language Models, December 2023. URL arXiv:2307.15043 [cs].",
  "A: Serum creatinineB: TemperatureC: Parathyroid hormoneD: Blood pressure": "A 50-year-old woman presents with a severe headache and vomiting. She says that symptoms onset after attending a wine tasting at the local brewery. She says that her headache is mostly at the back of her head and that she has been nauseous and vomited twice. Past medical history is significant for depression diagnosed 20 years ago but now well-controlled with medication. She also has significant vitamin D deficiency. Current medications are phenelzine and a vitamin D supplement. The patient denies any smoking history, alcohol or recreational drug use. On physical examination, the patient is diaphoretic. Her pupils are dilated. Which of the following is most likely to be elevated in this patient?",
  "(b)": ": Model robustness from single-query adversarial attacks using results in .The models(GLMs and BLMs) were evaluated on drug-mention questions from MedQA-USMLE (left) and disease-mention questions from MedMCQA (right) datasets. The GLMs and BLMs are ordered horizontally bytheir sizes. The bar colors distinguish between different attack methods. Perturbations by random sampling(RS) are in purple. Perturbations by powerscaled DWS (PDWS) in the n < 0 and n > 0 regimes are in blueand grey, respectively.",
  "Related works": "Text substitution attacksAdversarial attacks in neural language models have been demonstrated fromthe character to the sentence level (Zhang et al., 2020). The majority of text substitution attacks employtypos (Gao et al., 2018) and synonyms (Mozes et al., 2021), which effectively impose lexical constraints.Word frequency has been used as a metric to select adversarial examples in (Mozes et al., 2021). Synonymsubstitution attacks have been demonstrated in sentiment analysis (Liu et al., 2022; Yu et al., 2024). Alzan-tot et al. (2018) used a genetic algorithm to search for adversarial word substitution. The sensitivity ofmodels to entity-level perturbations, including substitution and swapping, creates problematic situationssuch as knowledge conflicts (Longpre et al., 2021) for knowledge-intensive tasks, compromised performancein machine reading comprehension (Yan et al., 2022) and table interpretation tasks (Koleva et al., 2023). Robustness in QAThe robustness of QA models has been studied using perturbation methods (Jia &Liang, 2017) and meaning-preserving transformations (Gan & Ng, 2019; Elazar et al., 2021). Sen & Saffari(2020) introduced a set of perturbations to assess language model generalization between QA datasets.Richardson & Sabharwal (2020) proposed a distractor design method using proximity in semantic space toprobe model understanding of word senses. Awadalla et al. (2022) examined the connection between in- andout-of-distribution robustness for different LLMs. The performance of LLMs is sensitive to the ordering ofchoices (Pezeshkpour & Hruschka, 2023; Zheng et al., 2024). Overall, the brittleness of knowledge in LLMsremains a primary limitation for their applications (Elazar et al., 2021; Augenstein et al., 2024). Characteristics of adversarial examplesThe defining characteristic of adversarial examples is the in-duction of false predictions in trained models that would otherwise predict accurately. Other characteristicsinclude (i) their generality such as transferability and universality (Zou et al., 2023). Transferability refers tothe effectiveness of the same example being adversarial to distinct models (Papernot et al., 2016). Universal 2Throughout the text, we use DWS to refer to the class of sampling method that employs distance-dependent probabilityweights. The powerscaled DWS is a type of DWS, so is the other version introduced in Wu et al. (2017).",
  "Adversarial distractor generation": "Definition 3.1.A multiple-choice question Q is represented by a tuple, Q = (C, S, D, k), with the contextC, question stem S, a distractor set D = {di}li=1, and a key k. The distractors are the wrong choices givento confuse test-takers, while the key is the correct one (CH & Saha, 2020). Definition 3.2 (Adversarial distractor).Given the original (or unperturbed) and a perturbed multiple-choice question pair, Q = (C, S, D, k) and Q = (C, S, D, k), which differ by only a distractor, D \\(DD) ={d}. The distractor d is adversarial if the perturbed question elicits a false response from the model, whileit answers correctly to the unperturbed question,",
  "argmax PrLLM(Y = k|C, S, D, k) = argmax PrLLM(Y = k|C, S, D, k).(1)": "The threat model in Definition 3.2 refers to an untargeted attack, while the targeted version only requiresrestricting the answer to the perturbed question to a specific distractor. A simple way to construct theperturbed distractor set D in Q is to substitute a distractor d by d, or D = (D \\ {d}) {d}.Thesubstitution is subject to the constraint Q Q < , where is the perturbation budget, and the norm refers to edit distance. For simplicity, we assume the distractors contain only NEs and only one NEeach (such as in b). In Appendix A, we extend Eq. 1 to a more general case with multiple entitiesand additional non-entity text in the distractors. Example 3.1. In the example of b, the stem S is the last sentence on the left side, Which of thefollowing is most likely to be elevated in this patient? The context C is all the text before the stem, A 50-year-old woman ... Her pupils are dilated. The original distractor set D = {Serum creatinine, Temperature,Creatine phosphokinase, Blood pressure}, the perturbed distractor set D = {Serum creatinine, Temperature,Parathyroid hormone, Blood pressure}, the key k is Blood pressure, or equivalently, D3. Unlike text addition attacks that introduce distracting information (Jia & Liang, 2017; Wallace et al., 2019a),the edit constraint on Q that is usually enforced in text adversarial attacks (Roth et al., 2024) doesnt precludelarge semantic perturbations at the entity level in the distractors. In this work, we draw perturbations froma finite perturbation set, denoted as E-k := E \\ {k}, with E = {ej}m+1j=1 being the entity dataset as illustratedin a. In practice, E may be compiled from potential model use cases or curated by regulatory bodiesin a model audit. Next, we discuss a unified view of discovering adversarial entities in the embedding spaceusing blackbox (or gradient-free) attack methods based on sampling and search.",
  "Definition 3.3 (Text span representations). Given the token sequence representation of a span e, we defineits corresponding vector representation in a text embedding of dimension l as e Rl": "We consider sampling with distance-dependent probability weights from the embedding space of NEs. Theconcept of distance-weighted sampling (DWS) was previously introduced in contrastive learning in computervision, where the Euclidean distance between samples and an anchor point was used and the probabilityweights depend on the embedding dimensionality (Wu et al., 2017). We propose a more flexible version",
  "j hn(k, ej).(2)": "Here, h(k, ej) is a distance metric between the anchor point k and ej, while the exponent n R controlsthe shape of the probability weight distribution. When n > 0, as n increases, the entities further awayfrom k are more preferably weighted. When n < 0, the sampling method employs inverse-distance-scaledweights, and the entities closer to k are more preferably weighted. In the subsequent evaluations, we useh(k, ej) = CosineDist(k, ej) = 1k ej/(kej), calculated in a text embedding of choice, where k ande are the respective vector representations of k and e. The formalism accommodates a deterministic regimeas a limiting case4, when n +, and pj is vanishingly small for all but the entity furthest from k,",
  "Sampling view of zeroth-order adversarial attack": "Blackbox attacks using approximate gradients may proceed with zeroth-order optimization (ZOO), where thegradients are estimated by finite difference to guide the search (Ilyas et al., 2018; Cheng et al., 2019). In thediscrete text space, Berger et al. (2021) developed the DiscreteZOO attack, which contains three components:the word importance ranker, the candidate sampler, and the gradient-based optimizer. It operates at theword level and has been validated on BERT-sized models for synonym substitution attacks. The last twocomponents aim to find a replacement text span e in each iteration through the gradient update rule",
  "e = e0 ZOLLM(Q; e0).(4)": "Here, e0 refers to the original text span, e and e0 are the vector representations of e and e0, respectively. the learning rate, ZO indicates the zeroth-order gradient estimator, and ZOLLM(Q;e0) has the samedimensionality as e or e0.We use LLM(Q; e0) to denote the LLM text output with the unperturbedquestion Q (containing e0) as the input, then the multi-point (M 2) gradient estimate is constructed byquerying the LLM M times and computing",
  "ei e02.(5)": "This is an example of the random directions stochastic approximation (Nesterov & Spokoiny, 2017), whereu = (eje0)/eje0 is a randomly oriented vector in the embedding space corresponding to the neighboringentity ej. Berger et al. (2021) computed Eq. 5 by random sampling of neighbors (in candidate sampler)within a similarity threshold for synonyms. The procedure was iterated once for all ranked attack locations.Given a noisy gradient estimate ZOLLM(Q; e0) and a discrete search space, we can rewrite Eq. 4 effectivelyas deterministic DWS in the embedding space by",
  "Dataset selection": "Entity datasetsTo relate to real-world scenarios, we sourced vocabulary datasets of drug and diseasenames from existing public databases.The drug names dataset (FDA-drugs) contains over 2.3k uniqueentities from known drugs approved by the United States Food and Drug Administration (FDA) and curatedby Drug Central5 (Ursu et al., 2017). FDA is a world authority that approves the legal distribution ofdrugs, therefore, the dataset represents the drug-entity names encountered in daily life. The disease namesdataset6 (CTD-diseases) contains over 9.8k unique entities from the Comparative Toxicogenomics Database(Davis et al., 2009), which contains a comprehensive collection of uniquely documented chemical-gene-diseaseinteractions for humans. Both databases are curated by domain experts through regular updates, and unliketraditional text corpora (Mohan & Li, 2019), the entities in these two datasets contain no redundancy. The2023 version of the datasets was used after minor data processing. Biomedical QA datasetsWe selected over 9.3k questions from the MedQA-USMLE (Jin et al., 2021)dataset and over 3.8k questions from the MedMCQA (Pal et al., 2022) dataset for benchmarking. Bothdatasets are entity-rich and cover a wide range of biomedical specialties and topics. The MedQA-USMLEdataset contains long-context questions that are used to assess medical students in the United States. TheMedMCQA dataset contains short questions used in medical exams in India. They have been used in recentworks to assess the biomedical knowledge in LLMs (Singhal et al., 2023; Han et al., 2023; Livin et al., 2024;Saab et al., 2024). Both datasets are publicly available and dont contain personal information. We annotatedthe NEs according to the entity types of the Unified Medical Language System (UMLS) (Bodenreider, 2004)using scispaCy (Neumann et al., 2019). We then divided the QA datasets into drug- or disease-mentionquestions according to the UMLS entity types (see Appendix C) of NEs in distractors.",
  "Type-consistent entity substitution (TCES)": "Implementing attacks that target entities requires accounting for type consistency and compatibility with si-multaneous multi-word (or span-level) operations, because of the inseparable nature of the entity components(e.g. growth hormone, acquired immunodeficiency syndrome). This effectively results in more substitutionpatterns (one to two words, two to one word, two to two words, two to three words, etc.) than the morecommon one-to-one synonym substitution (Mozes et al., 2021). We describe TCES in Algorithm 1 as a gen-eral and contextualized attack template for entities, with the goal of adversarially modifying model outputswith only one span-level substitution, specified by token boundaries. The template can accommodate single-or multi-query attacks and it applies to any task that may be formatted into QA (Gardner et al., 2019). The initial steps in TCES involve entity recognition and typing (see Appendix C). For distractor generation,the type-filtered entities in the choices (Enttfch) are separated into key (Entkey) and distractor (Entdistrc)entities. In the biomedical domain, the entity type information is readily available from the UMLS (Bo-denreider, 2004) semantic groups, such as Chemicals & Drugs and Disorders used for demonstration later.The RankSelect step selects the text span to attack (Entvictim). In the PDWS attack, the type-matchedentity in the distractors closest to the key is selected. The purpose is to maintain consistency and is notuniquely associated with the effects discussed later. In ZOO-based attacks, RankSelect corresponds to theword importance ranker. Then, the potential replacement entities are selected using Sampler (Eq. 2 forPDWS or Eq. 6 for DiscreteZOO), which also removes any duplicates of the key in the vocabulary (Entvocab).Sampling is carried out without replacement from the corresponding perturbation set E-k (constructed with",
  "return": "VictimLLMsWeselectedbothgeneralist(GLMs) and specialist (BLMs) models for robust-ness evaluation via blackbox adversarial attacks.The criterion we adopted here for a model to havebiomedical knowledge is that it should have a base-line performance better than random guessing (e.g.an accuracy of 0.25 for a specific four-choice QAtask) in the evaluated domain-specific task.ForGLMs, we used instruction-finetuned T5 (Flan-T5)series of models (Chung et al., 2024), and UL2 model(Tay et al., 2022) (Flan-UL2). For BLMs, we usedMedAlpaca-7B (Han et al., 2023), MedLlama-3-8B-v2.0 from John Snow Labs (JSL-MedLM3-8Bv2)7,Llama2-Medtuned-13B (LM2-Medtuned-13B) (Ro-hanian et al., 2024), and Palmyra-Med-20B (Kamble& Alshikh, 2023) from Writer. The selected open-source models have sizes in 1-20B range. Besides,we also evaluated GPT-3.5 from OpenAI. Attack settingsAll models were evaluated atzero temperature or in the non-sampling setting andmodel inference was conducted in the zero-shot set-ting with only basic prompt instructions (see theprompt structure in Appendix D). Model inferenceof Palmyra-Med-20B (Kamble & Alshikh, 2023)used 4-bit quantization to improve speed (Dettmers& Zettlemoyer, 2023). The evaluation settings re-sulted in our somewhat lower baseline accuraciesthan the reported ones which are often achieved with the help of few-shot prompting and prompt opti-mization. The DiscreteZOO attack (Berger et al., 2021) was originally implemented in the textattackframework (Morris et al., 2020b), which we updated to be compatible with span-level perturbation using itsboundaries, multi-GPU split-model inference, and decoder-based billion-parameter LLMs by modifying theattack recipe. The following three aspects of the benchmarks are in our focus. Query budget: We used fixed query budgets (B) for three main types of attacks: (i) Single-querysampling-based attacks were used as the reference because the DiscreteZOO attack requires a minimumof 3 model queries; (ii) Multi-query attacks used a budget of 8 for reasonable computational cost acrossall models and attack settings for both sampling- and search-based attacks; (iii) The query scaling trendsof specific LLMs and different attack settings were investigated with a series of query budgets under 100per input instance. Here, we also included the deterministic versions of PDWS to sample the nearest andfarthest elements to an anchor point. The element was taken from the perturbation set and the distances(between entities) were calculated using cosine in a chosen text embedding. Their query-limited versionsare referred to as B-nearest and B-farthest element sampling, respectively, where all B nearest or farthestelements were used sequentially to attack models in query scaling studies. Detailed definitions are givenin Appendix B.4.",
  "Two regimes of adversarial entities": "Diversity quantification of adversarial entitiesThe embedding space picture of the substitutionattack in a provides a geometric interpretation of PDWS to examine different types of adversarialexamples by tuning n.We use the Gini-Simpson index (Rao, 1982), GS = 1 j q2j , to quantify thediversity of adversarial examples and a low value indicates less diversity. Here, qj (0, 1] is the probability ofoccurrence of the entity ej among all instances of adversarial entities obtained from sampling, and",
  "j qj = 1.A low diversity index indicates high reusability and is a proxy for the universal adversarial attack regime(Moosavi-Dezfooli et al., 2017; Wallace et al., 2019a)": "Considering the joint effects of diversity and hyperparameter n on ASR in reveals two regimes: (i)adversarial entities semantically close to (n < 0) the key k have a greater diversity; (ii) adversarial entitiessemantically far from (n > 0) the key k are more reusable. TCES in either of these two regimes can leadto an increase in ASR from random sampling (equivalent to n = 0), as shown in Tables 1-2. showsthat a larger positive n in Eq. 2 samples more universal adversarial entities than in other situations, leadingto a marked drop in the diversity index. Moreover, there exist local maxima of ASR in both regimes withfinite n, which were discovered by hyperparameter tuning. We call the behavior the two-regime effect andit was observed in almost all models evaluated here in varying magnitudes for both datasets, thanks to thecontrollable spatial coverage of sampling methods incorporating distance information. In both single- andmulti-query attacks, the benchmarks show that PDWS attacks in the n < 0 regime tend to incur a greaterchange in model performance than in the n > 0 regime, yet prominent counterexamples exist, includingMedAlpaca-7B and Llama2-Medtuned-13B evaluated on drug-mention questions, and Flan-T5-xl evaluatedon disease-mention questions in . Semantic distortion by adversarial entitiesBesides ASR, evaluating the semantic distortion is anotherway to quantify the effectiveness of adversarial attacks.We estimated the semantic distortion of entireprompts in successful attacks before and after entity substitution, corresponding to Q and Q in Definition3.2. We calculated the average prompt semantic similarity (PSS) using SentenceTransformer (Reimers &Gurevych, 2019) with the RoBERTa-large model (all-roberta-large-v1)9. At each distinct n, the PSS with 1 being the most similar, is averaged over all successful attack instances to obtain the averagePSS, a quantitative measure of semantic distortion. Its dependence on the hyperparameter n for differentmodels and datasets is shown in .",
  "(h)": ": Powerscaled DWS of adversarial distractors exhibits a two-regime effect at negative and positiven values (see Eq. 2) in ASR (top) and diversity index (bottom) of replacement entities in successful attacks.Local maxima in ASR with a finite n are also present in each regime. The vertical dashed line indicates thelocation of random sampling. The observed similar behaviors are compared across models and datasets in (a)Flan-T5-xxl on MedQA-USMLE, (b) Palmyra-Med-20B on MedQA-USMLE, (c) Flan-T5-xl on MedMCQA,(d) MedAlpaca-7B on MedMCQA. Disease and drug-mention questions are separated by colors. The averageprompt semantic similarity displayed in (e)-(h) is calculated for the successful attacks obtained from thecorresponding attack settings in (a)-(d), respectively. Adversarial entity characteristicsThe two-regime effect indicates that the success of the attacks maycome from two sources that dominate at different locations on the same attack surface, which refers to entitysubstitution in the present work. At larger distances, the attack succeeds potentially due to the obscurity of the distractor entities in thequestion context, which can also profoundly impact the model performance (Li et al., 2023a). This expla-nation may be further supported by examining the diversity index, GS, which shows the dominance of afew replacement entities in successful attacks at large distances. We found these highly reusable adversar-ial entities by ranking their occurrences in successful attacks by powerscaled DWS. For drug names, thetop-ranked adversarial entities include n-acetylglucosamine (an amino sugar and anti-inflammatory drug)and technetium Tc 99m exametazime (a radiopharmaceutical and contrast agent)10. For disease names,some of the chromosome deletion syndromes and rare disease names are more reusable than others. These",
  "Diseases0.44116.921.5 / 18.70.62415.420.1 / 11.9": ": Single-query (budget = 1) attack success rates (ASRs) on subsets of MedQA-USMLE and MedM-CQA datasets by perturbing the drug or disease entity mentions. Zero-shot baseline (Baseline) accuracy isincluded for reference next to the ASR of sampling attacks using perturbation by random sampling (RandS),powerscaled distance-weighted sampling (PDWS). The - and + signs indicate the two regimes (n < 0and n > 0). For each model, the highest ASR for a type of entity-mention question in each dataset is bolded.The model with an asterisk () were only evaluated on one dataset to limit the computational cost. Flan-T5-large Flan-T5-xl Flan-T5-xxl Flan-UL2 MedAlpaca-7B LM2-Medtuned-13B Palmyra-Med-20B Single-query attack success rate (%) Generalist LMBiomedical LMMedQA-USMLE-drug RSPDWS (n < 0) PDWS (n > 0)",
  "entities dominate the sampling results in the n > 0 regime for all LLMs, indicating their transferabilityand universality": "At small distances, semantic proximity (Mozes et al., 2021) reduces the distinguishability between thekey and the distractor, thereby increasing the chance of model failure. Examples include similar drugsor diseases with insufficient details such as using diabetes as a distractor instead of the correct answerdiabetes II (refers to type-II diabetes) can result in a successful attack. Adversarial distractors result in little semantic distortion at the prompt level (see e-h).Theexperiments on the MedMCQA dataset yield somewhat lower average PSS than MedQA-USMLE becauseof the longer format of the latter dataset, leading to slightly larger distortions by entity substitution.Adversarial entities obtained in the n < 0 regime results in higher average PSS compared to those in then > 0 regime, while there is also a two-regime effect the average PSS tends to be the lowest aroundn = 0 and higher when n departs further from 0. The trend is consistent for different models and datasetsdemonstrated in .",
  "Diseases33.235.1 / 30.232.334.541.148.4 / 39.646.350.1": ": Multi-query (here budget = 8) attack success rates (ASR %) on various LLMs by perturbing thedrug and disease entity mentions in MedQA-USMLE and MedMCQA datasets. The PDWS attack uses theembedding from CODER and DiscreteZOO (DZOO) attacks use either CODER or GTE-base embedding.For each model, the highest ASR for a type of entity-mention question in each dataset is bolded.",
  "Scaling characteristics from adversarial entities": "Size is not all for scaling model robustness.We compare model robustness using single-query ASRas the metric in and . For the Flan-T5 series of GLMs (Chung et al., 2024) and Flan-UL2 (Tayet al., 2022), the single-query ASR is more pronounced in smaller and less performant models, indicatingthe improvement of robustness as the models size grows. However, an opposite trend of robustness is seenin BLMs. It is worth noting that although Palmyra-Med-20B (Kamble & Alshikh, 2023) has about threetimes the number of parameters than MedAlpaca-7B (Han et al., 2023), the larger model is noticeably moresensitive to entity perturbation. In terms of ASR, the BLMs are comparable to the GLMs in each typeof questions evaluated on. Another observation is that the ASR for models is asymmetrical under PDWSattacks. For most of the models, sampling adversarial entities with inverse-distance-scaled weights (n < 0in Eq. 2) induces more performance drop than sampling with distance-scaled weights (n > 0 in Eq. 2), yetprominent counterexamples exist. The attack performance is further investigated in the sequential, multi-query setting (budget = 8) in , where we also compared sampling- against search-based DiscreteZOOattacks. Here, PDWS attacks in the n < 0 regime are still largely the most effective. Sampling attacks has improved query scaling over blackbox gradient attacks in specializedtext embedding.A query scaling study of adversarial attacks investigates attack success with varyingquery budgets per input instance (Shukla et al., 2021). It is a comprehensive evaluation of attack methods. compares the query scaling of both sampling- and search-based methods for TCES. For low-query-budget attacks (a), each scaling curve exhibits a rapid rise phase and a plateau phase, separated by aninflection point. The key message here is that the advantage of PDWS attacks over DiscreteZOO dependson the model and the choice of text embedding. The PDWS attacks are overarchingly more effective forGLMs than DiscreteZOO, while for BLMs, the advantage is reduced. Moreover, using a specialized text",
  "(d)": ": Example scaling curves of the query budget against ASR for disease-mention questions with (a)Flan-T5-xl model on MedMCQA dataset and (b) MedAlpaca model on MedQA-USMLE dataset. The curvesare generated using TCES of disease names with the methods described in the legends. The DiscreteZOOattacks were run in the low-query-budget setting.Executing the random sampling (RS) attack doesntrequire a text embedding, while the other attacks were evaluated with the CODER or GTE-base as textembedding. In (c) and (d), query scaling of the attacks based on B-nearest and B-farthest element sampling(B is query budget) for the same datasets and models as in (a) and (b) are compared with PDWS. embedding further boosts the advantage of PDWS attacks compared with a general text embedding dueto the difference in embedding space neighborhood. We expand the analysis in the following along threedirections. Query scaling depends on text embedding: Changing the text embedding affects the query scalingbehavior of both PDWS and DiscreteZOO attacks. shows that the influence of the embedding choiceon PDWS is more pronounced in the rapid rise phase, while it influences the DiscreteZOO primarily in theplateau phase. Specifically, changing the text embedding from domain-specialized CODER (Yuan et al.,2022) to the general GTE-base (Li et al., 2023b), the performance advantage of PDWS over DiscreteZOOattacks shrunk in Flan-T5-xl (a) and inverted in MedAlpaca-7B (b). Sampling hyperparameter controls scaling curve shape in PDWS attacks: The hyperparametern changes the inflection point of the scaling curve. When the query budget passes the inflection point of thecurve, random sampling starts to surpass PDWS (n > 0) in ASR. This is a consequence of the distributionof adversarial entities in the embedding space and the regional bias derived from the sampling weights.The examples in show that the largest performance gap between the PDWS and DiscreteZOOattacks appears around the inflection point of the respective query scaling curve.",
  "D: Amlodipine and a statin daily.Sublingual nitroglycerin as needed": "[Context]: A 73-year-old man presents to theoutpatient clinic complaining of chest painwith exertion. He states that resting for a fewminutes usually resolves the chest pain.Currently, he takes 81 mg of aspirin daily. Hehas a blood pressure of 127/85 mm Hg andheart rate of 75/min. Physical examinationreveals regular heart sounds and clear lungsounds bilateral. [Question]: Which medication regimen belowshould be added? A: Amlodipine daily. Sublingual nitroglycerinas needed.B: Metoprolol and a statin daily. Sublingualnitroglycerin as needed.C: Metoprolol and ranolazine daily.Sublingual nitroglycerin as needed.D: Amlodipine and a statin daily. Sublingualnitroglycerin as needed.",
  "Unperturbed": "[Context]: A 73-year-old man presents to theoutpatient clinic complaining of chest painwith exertion. He states that resting for a fewminutes usually resolves the chest pain.Currently, he takes 81 mg of aspirin daily. Hehas a blood pressure of 127/85 mm Hg andheart rate of 75/min. Physical examinationreveals regular heart sounds and clear lungsounds bilateral. [Question]: Which medication regimen belowshould be added? A: Amlodipine daily. Sublingual nitroglycerinas needed.B: Metoprolol and a statin daily. Sublingualnitroglycerin as needed.C: N-acetylglucosamine and ranolazine daily.Sublingual nitroglycerin as needed.D: Amlodipine and a statin daily. Sublingualnitroglycerin as needed.",
  "Adversarially perturbed": ": (Top) An entity substitution attack using n-acetylglucosamine as the replacement entity for meto-prolol creates an adversarial distractor. (Bottom) Heatmaps of token-wise Shapley values for a questionbefore and after the adversarial attack on choice C. The model prediction changes from the correct choiceof B (unperturbed) to the incorrect choice of D (adversarially perturbed). Deterministic sampling attacks are competitive proxies of their probabilistic counterparts.Performance comparison between B-nearest and B-farthest element sampling, the deterministic andquery-limited versions of PDWS (corresponding to the n < 0 and n > 0 regimes) are given in c-d for the two example cases studied for the query scaling. The results show that the deterministicsampling attacks are already competitive against DiscreteZOO and also close in performance to PDWS",
  "Adversarial entities manipulate explainability": "Post-hoc analysis is a common way to understand the rationale behind model decisions (Murdoch et al.,2019). Score-based feature importance such as that based on the Shapley values is routinely used to constructpost-hoc explanation of model prediction through feature attribution (Chen et al., 2023). The stability ofmodel explanation is a growing concern for their proper use (Alvarez-Melis & Jaakkola, 2018; Hancox-Li,2020; Lin et al., 2023). Investigations on the susceptibility of LLM explanations to adversarial perturbationin QA are still lacking.For score-based feature importance, an explainer () maps the m-dimensionalfeature x Rm to scores (x) Rm. In the current context, we assess the changes in feature importancerepresented by Shapley values (Chen et al., 2023) before ((Q)) and after ((Q)) the adversarial attackby entity substitution Q Q. For convenience of discussion, we use Shapley profile to refer to (Q) (or(Q)), the token-wise Shapley values calculated over Q (or Q). We investigated the changes in the Shapley profile in the two regimes of adversarial entities for open-sourceLLMs, because the calculation requires access to model weights. We found that the token with the largestShapley value (top-1 feature) is highly correlated with the models prediction before and after adversarialperturbation, with an example shown in produced using Flan-T5-large. This behavior agrees withthe symbol-binding characteristic of LLMs in multiple-choice QA (Robinson & Wingate, 2023). Moreover,the Shapley profile typically exhibits pronounced changes before and after entity perturbation, especiallyin the text locations close to the perturbed entities ( and Appendix E). Together, these observationsindicate that the substitution attacks effectively manipulate model explanations, because the explanationafter the attack aligns with the wrong prediction, as if they were correct.The behavior occurs despiteunchanged context, stem, and key (C, S, and k), which should contain the most important information forunderstanding and answering the question (Chai & Jin, 2004).",
  "Discussion and future work": "The present study illustrates that TCES is a compact yet effective way to construct adversarial distractors,which leads to significant performance degradation in LLMs. Our results suggest that to minimize modelqueries, using a budget of up to the inflection point on the query scaling curve is the most cost-effective.The perturbed texts produced by TCES appear natural and are less likely to be detected using grammarcheck or topical filtering (Morris et al., 2020a). Therefore, the setting can simulate realistic scenarios wherethe attacks may be initiated by unsuspecting users, such as healthcare professionals using an LLM-basedclinical decision support system (Liu et al., 2023). In the examples illustrated in and Appendix E, ad-versarial distractors can lead to disease misdiagnosis or misprescription of medication, which are detrimentalscenarios that may occur in algorithmic decision-making in biomedicine using LLMs. Further improvementon the sampling attack performance may be achieved using learned samplers through rejection mechanisms(Narasimhan et al., 2024) or by finetuning the text embedding to improve the query efficiency of sampling.Alternatively, the semantic distance used for PDWS may be replaced with a concept distance (Choi et al.,2016) to improve attack performance on general text embeddings. Our approach may be integrated into inter-active platforms for adversarial data collection or online monitoring systems for open-ended human-machineconversation (Chao et al., 2023). Future research may focus on how to make LLMs more reliable through prompt engineering (Si et al., 2023;Nori et al., 2023) or leveraging retrieval from nonparametric knowledge bases (Weikum et al., 2021; Somanet al., 2023) or text sources (Wang et al., 2023b) to improve model generalization to long-tail entities. Besidesvulnerability identification, the current work motivates adversarial defense strategies based on generalizeddistances (La Malfa et al., 2020) to establish a tiered system for robustness assessment in user-facing anddomain-oriented applications to mitigate catastrophic failures.",
  "David Alvarez-Melis and Tommi S. Jaakkola. On the Robustness of Interpretability Methods, June 2018.URL arXiv:1806.08049 [cs, stat]": "Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang.Generating Natural Language Adversarial Examples.In Proceedings of the 2018 Conference on Em-pirical Methods in Natural Language Processing, pp. 28902896, Stroudsburg, PA, USA, 2018. Associa-tion for Computational Linguistics. doi: 10.18653/v1/D18-1316. URL Giovanni Apruzzese, Hyrum S. Anderson, Savino Dambra, David Freeman, Fabio Pierazzi, and KevinRoundy. Real Attackers Dont Compute Gradients: Bridging the Gap Between Adversarial ML Re-search and Practice. In 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML),pp. 339364, February 2023. doi: 10.1109/SaTML54575.2023.00031. URL Isabelle Augenstein, Timothy Baldwin, Meeyoung Cha, Tanmoy Chakraborty, Giovanni Luca Ciampaglia,David Corney, Renee DiResta, Emilio Ferrara, Scott Hale, Alon Halevy, Eduard Hovy, Heng Ji, Fil-ippo Menczer, Ruben Miguez, Preslav Nakov, Dietram Scheufele, Shivam Sharma, and Giovanni Zagni.Factuality challenges in the era of large language models and opportunities for fact-checking. Nature Ma-chine Intelligence, 6(8):852863, August 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00881-z. URL Publisher: Nature Publishing Group. Anas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Hajishirzi, andLudwig Schmidt. Exploring The Landscape of Distributional Robustness for Question Answering Models.In Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 59715987, Stroudsburg,PA, USA, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.441.URL Nathaniel Berger, Stefan Riezler, Sebastian Ebert, and Artem Sokolov. Dont Search for a Search Method Simple Heuristics Suffice for Adversarial Text Attacks. In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pp. 82168224, Stroudsburg, PA, USA, 2021. Association forComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.647. URL",
  "O. Bodenreider. The Unified Medical Language System (UMLS): integrating biomedical terminology. NucleicAcids Research, 32(90001):267D270, jan 2004. ISSN 1362-4962. doi: 10.1093/nar/gkh061. URL": "David Cecchini, Arshaan Nazir, Kalyan Chakravarthy, and Veysel Kocaman. Holistic Evaluation of LargeLanguage Models: Assessing Robustness, Accuracy, and Toxicity for Real-World Applications. In AnaeliaOvalle, Kai-Wei Chang, Yang Trista Cao, Ninareh Mehrabi, Jieyu Zhao, Aram Galstyan, Jwala Dhamala,Anoop Kumar, and Rahul Gupta (eds.), Proceedings of the 4th Workshop on Trustworthy Natural LanguageProcessing (TrustNLP 2024), pp. 109117, Mexico City, Mexico, June 2024. Association for ComputationalLinguistics. doi: 10.18653/v1/2024.trustnlp-1.11. URL Dhawaleswar Rao CH and Sujan Kumar Saha. Automatic Multiple Choice Question Generation From Text:A Survey. IEEE Transactions on Learning Technologies, 13(1):1425, jan 2020. ISSN 1939-1382. doi:10.1109/TLT.2018.2889100. URL",
  "Minhao Cheng, Thong Le, Pin-Yu Chen, Huan Zhang, JinFeng Yi, and Cho-Jui Hsieh. Query-Efficient Hard-label Black-box Attack: An Optimization-based Approach. September 2019. URL": "Youngduck Choi, Chill Yi-I. Chiu, and David Sontag. Learning Low-Dimensional Representations of MedicalConcepts. AMIA Joint Summits on Translational Science proceedings. AMIA Joint Summits on Transla-tional Science, 2016:4150, 2016. ISSN 2153-4063. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, DashaValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, HongkunYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and JasonWei. Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):153, 2024. ISSN 1533-7928. URL Jan Clusmann, Fiona R. Kolbinger, Hannah Sophie Muti, Zunamys I. Carrero, Jan-Niklas Eckardt,Narmin Ghaffari Laleh, Chiara Maria Lavinia Lffler, Sophie-Caroline Schwarzkopf, Michaela Unger,Gregory P. Veldhuizen, Sophia J. Wagner, and Jakob Nikolas Kather.The future landscape of largelanguage models in medicine. Communications Medicine, 3(1):141, October 2023. ISSN 2730-664X. doi:10.1038/s43856-023-00370-1. URL A. P. Davis, C. G. Murphy, C. A. Saraceni-Richards, M. C. Rosenstein, T. C. Wiegers, and C. J. Mattingly.Comparative Toxicogenomics Database: a knowledgebase and discovery tool for chemical-gene-diseasenetworks. Nucleic Acids Research, 37(Database):D786D792, jan 2009. ISSN 0305-1048. doi: 10.1093/nar/gkn580. URL Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit Inference Scaling Laws. In Proceed-ings of the 40th International Conference on Machine Learning, pp. 77507774. PMLR, July 2023. URL John C. Duchi, Michael I. Jordan, Martin J. Wainwright, and Andre Wibisono. Optimal Rates for Zero-Order Convex Optimization: The Power of Two Function Evaluations. IEEE Transactions on InformationTheory, 61(5):27882806, May 2015. ISSN 0018-9448, 1557-9654. doi: 10.1109/TIT.2015.2409256. URL Salijona Dyrmishi, Salah Ghamizi, and Maxime Cordy.How do humans perceive adversarial text?Areality check on the validity and naturalness of word-based adversarial attacks. In Anna Rogers, Jor-dan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: Long Papers), pp. 88228836, Toronto, Canada, July2023. Association for Computational Linguistics.doi:10.18653/v1/2023.acl-long.491.URL Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich Schtze,and Yoav Goldberg. Measuring and Improving Consistency in Pretrained Language Models. Transac-tions of the Association for Computational Linguistics, 9:10121031, dec 2021. ISSN 2307-387X. doi:",
  "/tacl_a_00410.URL": "Wee Chung Gan and Hwee Tou Ng. Improving the Robustness of Question Answering Systems to QuestionParaphrasing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,pp. 60656075, Stroudsburg, PA, USA, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1610. URL Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi.Black-Box Generation of Adversarial TextSequences to Evade Deep Learning Classifiers. In 2018 IEEE Security and Privacy Workshops (SPW),pp. 5056. IEEE, may 2018.ISBN 978-1-5386-8276-0.doi: 10.1109/SPW.2018.00016.URL",
  "Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, and Sewon Min. Question answering is aformat; when is it useful?, September 2019. URL arXiv:1909.11291[cs]": "Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, AlexanderLser, Daniel Truhn, and Keno K. Bressem. MedAlpaca An Open-Source Collection of Medical Con-versational AI Models and Training Data, October 2023.URL [cs]. Leif Hancox-Li. Robustness in machine learning explanations: does it matter? In Proceedings of the 2020Conference on Fairness, Accountability, and Transparency, pp. 640647, New York, NY, USA, January2020. ACM. ISBN 978-1-4503-6936-7. doi: 10.1145/3351095.3372836. URL Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box Adversarial Attacks with LimitedQueries and Information. In Proceedings of the 35th International Conference on Machine Learning, pp.21372146. PMLR, July 2018. URL Robin Jia and Percy Liang.Adversarial Examples for Evaluating Reading Comprehension Systems.InProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 20212031,Stroudsburg, PA, USA, 2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1215.URL Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What DiseaseDoes This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams.Applied Sciences, 11(14):6421, jul 2021. ISSN 2076-3417. doi: 10.3390/app11146421. URL",
  "Kiran Kamble and Waseem Alshikh. Palmyra-med: Instruction-based fine-tuning of llms enhancing medicaldomain performance, July 2023": "Aneta Koleva, Martin Ringsquandl, and Volker Tresp. Adversarial Attacks on Tables with Entity Swap. InJoint Proceedings of Workshops at the 49th International Conference on Very Large Data Bases (VLDB2023), Vancouver, Canada, volume 3462 of CEUR Workshop Proceedings, 2023. URL Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepao,Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performanceof ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOSDigital Health, 2(2):e0000198, feb 2023. ISSN 2767-3170. doi: 10.1371/journal.pdig.0000198. URL Emanuele La Malfa, Min Wu, Luca Laurenti, Benjie Wang, Anthony Hartshorn, and Marta Kwiatkowska.Assessing Robustness of Text Classification through Maximal Safe Radius Computation. In Trevor Cohn,Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020,",
  "pp. 29492968, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.266. URL": "Pierre LEcuyer and George Yin. Budget-Dependent Convergence Rate of Stochastic Approximation. SIAMJournal on Optimization, 8(1):217247, 1998. doi: 10.1137/S1052623495270723. URL Publisher: Society for Industrial and Applied Mathematics. Eric Lehman, Evan Hernandez, Diwakar Mahajan, Jonas Wulff, Micah J Smith, Zachary Ziegler, DanielNadler, Peter Szolovits, Alistair Johnson, and Emily Alsentzer.Do We Still Need Clinical LanguageModels? In Bobak J Mortazavi, Tasmie Sarker, Andrew Beam, and Joyce C Ho (eds.), Proceedings of theConference on Health, Inference, and Learning, volume 209 of Proceedings of Machine Learning Research,pp. 578597. PMLR, 2023. URL Xinzhe Li, Ming Liu, Shang Gao, and Wray Buntine.A Survey on Out-of-Distribution Evaluation ofNeural NLP Models. In Proceedings of the Thirty-Second International Joint Conference on ArtificialIntelligence, pp. 66836691, California, aug 2023a. International Joint Conferences on Artificial IntelligenceOrganization. ISBN 978-1-956792-03-4. doi: 10.24963/ijcai.2023/749. URL",
  "ChrisLin,IanCovert,andSu-InLee.OntheRobustnessofRemoval-BasedFeatureAt-tributions.AdvancesinNeuralInformationProcessingSystems,36:7961379666,De-cember2023.URL": "Aiwei Liu, Honghai Yu, Xuming Hu, Shuang Li, Li Lin, Fukun Ma, Yawen Yang, and Lijie Wen. Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution.InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 76647676, Stroudsburg, PA, USA, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.522. URL Siru Liu, Aileen P Wright, Barron L Patterson, Jonathan P Wanderer, Robert W Turer, Scott D Nelson,Allison B McCoy, Dean F Sittig, and Adam Wright.Using AI-generated suggestions from ChatGPTto optimize clinical decision support. Journal of the American Medical Informatics Association, 30(7):12371245, July 2023. ISSN 1527-974X. doi: 10.1093/jamia/ocad072. URL Valentin Livin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther.Can largelanguage models reason about medical questions?Patterns, 5(3):100943, March 2024.ISSN 2666-3899.doi: 10.1016/j.patter.2024.100943.URL Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. Entity-Based Knowledge Conflicts in Question Answering. In Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pp. 70527063, Stroudsburg, PA, USA, 2021. Association forComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.565. URL Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi. A Strong Baseline for Query Efficient Attacksin a Black Box Setting. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tauYih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,pp. 83968409, Online and Punta Cana, Dominican Republic, November 2021. Association for Compu-tational Linguistics. doi: 10.18653/v1/2021.emnlp-main.661. URL",
  "Sunil Mohan and Donghui Li. MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts.In Automated Knowledge Base Construction (AKBC), 2019. URL": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal AdversarialPerturbations. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8694. IEEE, jul 2017. ISBN 978-1-5386-0457-1. doi: 10.1109/CVPR.2017.17. URL John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating Adversarial Examplesin Natural Language.In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Associationfor Computational Linguistics: EMNLP 2020, pp. 38293839, Online, November 2020a. Association forComputational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.341. URL John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. TextAttack: A Frameworkfor Adversarial Attacks, Data Augmentation, and Adversarial Training in NLP. In Qun Liu and DavidSchlangen (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-ing: System Demonstrations, pp. 119126, Online, October 2020b. Association for Computational Linguis-tics. doi: 10.18653/v1/2020.emnlp-demos.16. URL Maximilian Mozes, Pontus Stenetorp, Bennett Kleinberg, and Lewis Griffin. Frequency-Guided Word Sub-stitutions for Detecting Textual Adversarial Examples.In Proceedings of the 16th Conference of theEuropean Chapter of the Association for Computational Linguistics: Main Volume, pp. 171186, Strouds-burg, PA, USA, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.13.URL Nikola Mrki, Diarmuid Saghdha, Blaise Thomson, Milica Gai, Lina M. Rojas-Barahona, Pei-HaoSu, David Vandyke, Tsung-Hsien Wen, and Steve Young.Counter-fitting Word Vectors to Linguis-tic Constraints.In Proceedings of the 2016 Conference of the North American Chapter of the As-sociation for Computational Linguistics:Human Language Technologies, pp. 142148, Stroudsburg,PA, USA, 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1018. URL W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. Definitions, methods,and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 116(44):2207122080, oct 2019. ISSN 0027-8424. doi: 10.1073/pnas.1900654116. URL Harikrishna Narasimhan, Aditya Krishna Menon, Wittawat Jitkrittum, Neha Gupta, and Sanjiv Kumar.Learning to Reject Meets Long-tail Learning. In The Twelfth International Conference on Learning Rep-resentations, 2024. URL Yurii Nesterov and Vladimir Spokoiny. Random Gradient-Free Minimization of Convex Functions. Foun-dations of Computational Mathematics, 17(2):527566, April 2017.ISSN 1615-3383.doi:10.1007/s10208-015-9296-2. URL Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar.ScispaCy: Fast and Robust Models forBiomedical Natural Language Processing. In Proceedings of the 18th BioNLP Workshop and Shared Task,pp. 319327, Stroudsburg, PA, USA, 2019. Association for Computational Linguistics. doi: 10.18653/v1/W19-5034. URL",
  "Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow.Transferability in Machine Learning:fromPhenomena to Black-Box Attacks using Adversarial Samples. arXiv, pp. 1605.07277, may 2016. URL": "Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexan-der Miller. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pp. 24632473, Stroudsburg, PA, USA, 2019. Association for Computa-tional Linguistics. doi: 10.18653/v1/D19-1250. URL",
  "C.Radhakrishna Rao. Diversity and dissimilarity coefficients: A unified approach. Theoretical PopulationBiology, 21(1):2443, feb 1982.ISSN 00405809.doi: 10.1016/0040-5809(82)90004-1.URL": "Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP), pp. 39823992, Hong Kong, China, November 2019. Associationfor Computational Linguistics. doi: 10.18653/v1/D19-1410. URL Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating Natural Language Adversarial Examplesthrough Probability Weighted Word Saliency. In Proceedings of the 57th Annual Meeting of the Associa-tion for Computational Linguistics, pp. 10851097, Stroudsburg, PA, USA, 2019. Association for Compu-tational Linguistics. doi: 10.18653/v1/P19-1103. URL Kyle Richardson and Ashish Sabharwal. What Does My QA Model Know? Devising Controlled Probes UsingExpert Knowledge. Transactions of the Association for Computational Linguistics, 8:572588, dec 2020.ISSN 2307-387X. doi: 10.1162/tacl_a_00331. URL",
  "Language Processing, pp. 47234734, Stroudsburg, PA, USA, 2021. Association for Computational Linguis-tics. doi: 10.18653/v1/2021.emnlp-main.388. URL": "Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung,Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: Unifyinglanguage learning paradigms. In The Eleventh International Conference on Learning Representations,2022. URL Robert Tinn, Hao Cheng, Yu Gu, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, andHoifung Poon.Fine-tuning large neural language models for biomedical natural language processing.Patterns, 4(4):100729, apr 2023.ISSN 26663899.doi: 10.1016/j.patter.2023.100729.URL Oleg Ursu, Jayme Holmes, Jeffrey Knockel, Cristian G. Bologa, Jeremy J. Yang, Stephen L. Mathias,Stuart J. Nelson, and Tudor I. Oprea. DrugCentral: online drug compendium. Nucleic Acids Research,45(D1):D932D939, jan 2017. ISSN 0305-1048. doi: 10.1093/nar/gkw993. URL Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal Adversarial Trig-gers for Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on Natural Language Process-ing (EMNLP-IJCNLP), pp. 21532162, Stroudsburg, PA, USA, 2019a. Association for ComputationalLinguistics. doi: 10.18653/v1/D19-1221. URL Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, and Jordan Boyd-Graber. Trick Me If You Can:Human-in-the-Loop Generation of Adversarial Examples for Question Answering. Transactions of theAssociation for Computational Linguistics, 7:387401, nov 2019b. ISSN 2307-387X. doi: 10.1162/tacl_a_00279. URL Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, and Jie Fu. Pre-trainedLanguage Models in Biomedical Domain: A Systematic Survey. ACM Computing Surveys, 56(3):55:155:52, October 2023a. ISSN 0360-0300. doi: 10.1145/3611651. URL Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kings-bury, and Hongfang Liu. A comparison of word embeddings for the biomedical natural language processing.Journal of Biomedical Informatics, 87:1220, November 2018. ISSN 1532-0464. doi: 10.1016/j.jbi.2018.09.008. URL",
  "Yubo Wang, Xueguang Ma, and Wenhu Chen.Augmenting Black-box LLMs with Medical Text-books for Clinical Question Answering, September 2023b.URL [cs]": "Chih-Hsuan Wei, Alexis Allot, Robert Leaman, and Zhiyong Lu. PubTator central: automated concept anno-tation for biomedical full text articles. Nucleic Acids Research, 47(W1):W587W593, jul 2019. ISSN 0305-1048. doi: 10.1093/nar/gkz389. URL Gerhard Weikum, Xin Luna Dong, Simon Razniewski, and Fabian Suchanek. Machine Knowledge: Creationand Curation of Comprehensive Knowledge Bases. Foundations and Trends in Databases, 10(2-4):108490, 2021. ISSN 1931-7883. doi: 10.1561/1900000064. URL Michael Wornow, Yizhe Xu, Rahul Thapa, Birju Patel, Ethan Steinberg, Scott Fleming, Michael A. Pfeffer,Jason Fries, and Nigam H. Shah. The shaky foundations of large language models and foundation modelsfor electronic health records. npj Digital Medicine, 6(1):135, jul 2023. ISSN 2398-6352. doi: 10.1038/s41746-023-00879-8. URL Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krhenbhl. Sampling matters in deepembedding learning. In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 28592867,October 2017. doi: 10.1109/ICCV.2017.309. URL"
}