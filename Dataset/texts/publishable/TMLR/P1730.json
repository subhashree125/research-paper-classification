{
  "Abstract": "We develop an empirical Bayes prior for probabilistic matrix factorization.Matrixfactorization models each cell of a matrix with two latent variables, one associated withthe cells row and one associated with the cells column. How to set the priors of thesetwo latent variables? Drawing from empirical Bayes principles, we consider estimating thepriors from data, to find those that best match the populations of row and column latentvectors. Thus we develop the twin population prior. We develop a variational inferencealgorithm to simultaneously learn the empirical priors and approximate the correspondingposterior. We evaluate this approach with both synthetic and real-world data on diverseapplications: movie ratings, book ratings, single-cell gene expression data, and musicalpreferences.Without needing to tune Bayesian hyperparameters, we find that the twinpopulation prior leads to high-quality predictions, outperforming manually tuned priors.",
  "Introduction": "This paper is about empirical Bayes methods for setting the priors in Bayesian matrix factorization (Mnih& Salakhutdinov, 2007; Gopalan et al., 2015). Matrix factorization models each cell of a matrix with twolatent variables, one associated with its row and one associated with its column.Matrix factorizationhas found broad applications across many fields, including studying consumer behavior, understandinglegislative patterns, assessing pharmaceutical impacts, and exploring social networks (Gopalan et al., 2015;Koren et al., 2009; Gerrish & Blei, 2011; Jamali & Ester, 2010).",
  "Here, Ui and Vj are per row and per column specific latent vectors, and row and col are the priorsdistributions": "This formulation encompasses many factorization models.In Gaussian matrix factorization(Mnih &Salakhutdinov, 2007), the priors are Gaussians and Xi,j is drawn from a Gaussian with mean U i Vj andvariance 2. In Poisson matrix factorization (Canny, 2004; Dunson & Herring, 2005; Gopalan et al., 2015),the priors are over the positive reals and Xi,j is drawn from a Poisson with rate U i Vj. An observed matrix of data X then defines a posterior distribution P(U, V | X) over the row variablesand the column variables. The posterior can provide interpretations of the data and an avenue to formpredictions about missing entries, for example, for a recommendation system. The prior distributions on the row variables and column variables significantly impact the quality of themodel posterior. How should we set them? Practitioners typically assume a simple parametric family forthe priors, such as a Gaussian or a Gamma, and then find the prior hyperparameters best suited to thedata, e.g., with cross-validation (Salakhutdinov & Mnih, 2008; Schmidt et al., 2009). This approach can beeffective, but it is expensive and only allows for priors from a simple parametric family. In this paper, we develop an empirical Bayes (EB) methodology for setting the priors (Robbins, 1992;Efron, 2012), learning them from data. The EB idea is to set the priors using the data, for example byfinding the one that maximizes the marginal log-likelihood of the data. The EB idea is applicable to anyprior distribution from which are repeatedly drawn multiple independent variables. For example, all theUi are independently drawn from the same row and the Vj are independently drawn from the samecol. EB has found applications in applied sciences in diverse fields such as astronomy (Bovy et al., 2011),actuarial sciences (Bhlmann & Gisler, 2005), genomics (Smyth, 2005; Love et al., 2014), economics (Frost& Savarino, 1986; Angrist et al., 2017), and survey sampling (Rao & Molina, 2015). EB priors have beensuccessfully employed in simple hierarchical models, such as in variational autoencoders (Kingma & Welling,2013; Tomczak & Welling, 2018; Kim & Mnih, 2018). Matrix factorization, however, provides a different type of application of EB. In matrix factorization, thereare two priors to set, one for the row variables and one for the column variables, and the same data containsinformation about them, namely the observed data Xi,j contains information about both Ui and Vj. Thus,we will find empirical Bayes priors for both row variables and column variables. The result is the twinempirical Bayes prior (TwinEB), a practical EB method for matrix factorization. Other methods for EB onmatrix factorization include Wang & Stephens (2021) and da Silva et al. (2023); we discuss these in section2. Specifically, we model the two priors with mixture distributions, one for each prior. We use mixtures sincethey are a flexible family of distributions that can approximate a wide range of distributions (Titteringtonet al., 1985; Nguyen et al., 2020). We use variational inference (Blei et al., 2017; Wainwright & Jordan, 2008)to simultaneously estimate the priors and approximate the corresponding posterior. We verify the efficiencyand the robustness of this approach with real-world data about recommendation systems and computationalbiology, and for both Poisson matrix factorization and Gaussian matrix factorization.",
  "Related work": "One approach to setting the hyperparameters of priors involves using hierarchical Bayesian models (Gelman,2006). In this line of work, the priors hyperparameters are treated as unknown and assigned a prior withhyperpriors to be determined. Gopalan et al. (2015) employs this method for introducing hierarchical Poissonfactorization, and Levitin et al. (2019) applies it to gene signature discovery from scRNAseq data. Our workinvolves learnable priors (mixtures), which leads to a more flexible class of models while keeping a simplermodel as no extra variables for the hyperparameters are introduced. A fruitful direction of research for setting priors for matrix factorization has also been the use of empiricalBayes, a methodology by which one tries to find the prior that matches, in some sense, the populationdistribution of the data.In Rukat et al. (2017), the authors set a single and fixed values for the priorhyperparameter based on the expected value of the observed matrix. In Wang & Stephens (2021), the authorsformulate prior elicitation for matrix factorization (MF) as steps in a variational expectation maximizationalgorithm, and in that context, find that it is equivalent to solving the EB normal means (EBNM) problem(Jiang & Zhang, 2009). Our proposed approach is closely related to Wang & Stephens (2021), but we proposeto update both priors simultaneously which bypasses potentially costly numerical integration required in theEBNM step. A related line of research makes the connection to EBNM via random-matrix theory, focusingspecifically on denoising principal components (Zhong et al., 2022). Closely related to TwinEB, da Silvaet al. (2023) optimizes hyperparameters to match the prior predictive distribution to statistics computedfrom the data, and derives closed-form solutions for PMF and its hierarchical extensions. In , wecompare our method to Wang & Stephens (2021) and da Silva et al. (2023). In Appendix E, we give a moredetailed review of these methods. Wang et al. (2024) uses auxiliary information to improve inference in matrix factorization. They assume theexistence of an auxiliary matrix G, comprising per user extra features. Their method is closest to that ofWang & Stephens (2021), except that they define the per row latent variables as a function of the auxiliarymatrix G. They use an ensemble of regression trees to model the functional dependence of the row-wiselatents on the auxiliary variables. They use a Gaussian likelihood, with a mean as a linear function of theuser and item representations. Van Linh et al. (2020) incorporate auxiliary information about the items (columns). They assume descrip-tions of each item exists. They use pre-computed word-embeddings to train a neural network to learn userrepresentations. They use a Poisson likelihood with a rate computed as the linear combination of the userand item representations. Other flexible distribution learning methods can be used to specify expressive priors. For instance, Zhouet al. (2020) extend variational autoencoders (VAEs) and use normalizing flows (Papamakarios et al., 2021)to learn flexible priors on the latent embeddings of users and items. They combine user and item auxiliaryinformation with these latent embeddings to learn user and item representation that are then passed throughmulti-layered perceptrons to generate the parameter of a Bernoulli distribution. Empirical Bayes (EB) priors have been explored in the context of variational autoencoders (VAEs) (Kingma& Welling, 2013) under the name aggregated posterior, average encoding distribution (Hoffman & Johnson,2016) or VampPrior (Tomczak & Welling, 2018). The VampPrior learns an amortized posterior and a priorover the latent variables using a shared neural network. It models a prior on the rows only and was usedto address posterior collapse (Tomczak & Welling, 2018) or to learn disentangled representations (Kim &Mnih, 2018). Our method focuses on matrix factorization, and we derive EB priors for both row and columnlatent variables.",
  "Empirical Bayes priors for probabilistic matrix factorization": "Our goal is to develop empirical Bayes (EB) priors for Bayesian matrix factorization models. We will focushere on Poisson matrix factorization (PMF). In Appendix B, we derive EB priors for Gaussian matrixfactorization (GMF). With matrix factorization, the presence of repeated and identically distributed latent variables for each rowand each column provides the opportunity to learn their prior distribution from data. This is a form ofempirical Bayes (Robbins, 1992; Efron, 2012) that prescribes a population prior (see .1). This population prior aims to align the models marginal distribution of observations with the observedpopulation distribution. In the special case of matrix factorization, there are two distinct populations: thepopulations of row vectors, and the population of column vectors. With TwinEB, we learn one prior for eachpopulation. This is a form of hierarchical modeling without introducing an extra layer of latent variables.",
  "Background: Population priors for simple hierarchical models": "In this paper, we assume to have data X that is drawn from an unkown distribution P , also called populationdistribution, that we would like to model with a probabilistic model consisting of latent variables Z, a priorover the latent variables , and a likelihood function P(X | Z). A crucial step in Bayesian statistics is the choice of the prior distribution; if done arbitrarily, it can lead tosuboptimal posterior inference (Wang et al., 2021). We choose to follow an empirical Bayes principle thatprescribes a population prior (Hoffman & Johnson, 2016; Tomczak & Welling, 2018). This prior, by design,aligns the models marginal distribution of observations with the population distribution P (X).",
  "Published in Transactions on Machine Learning Research (12/2024)": ": TwinEB is robust to the number of mixture components on real-world datasets. Test hold-outlikelihood as a function of Kr, the number of row components (x-axis) and Kc, the number of columncomponents (hue) for TwinEB with the Poisson likelihood. Each experiment is from 10 seeds. The testhold-out likelihood is centered at the result of (Kr, Kc) = (70, 100), highlighted by the gray dashed lines.Note that the range of the vertical axis is orders of magnitude smaller than the range of the real worldexperiments, suggesting that TwinEB is not sensitive to the choice of the number of mixture components.",
  "When the prior satisfies Equation (13), this property is called self-consistency (Laird, 1978)": "The structure of Equations (13) suggests to use families of mixtures of parametric distributions to approxi-mate the row and column population priors (Tomczak & Welling, 2018). Mixtures can approximate complexdistributions when their number of components increases while having the convenience of remaining para-metric (Titterington et al., 1985; Nguyen et al., 2020). We choose to model the priors by dropping theirdependence on the other variable and express them as,",
  "k=1kPk,k(Vj),(15)": "where Kr and Kc are the number of components in the mixtures, row = {, , } and col = {, , }.The locations RKrL and RKcL, the scales RKrL and RKcL, and the mixture weights Kr and Kc are the parameters of the mixtures priors.As Kr and Kc increase, the priorsbecome more and more expressive (see ). shows a graphical model representation of matrixfactorization with EB priors.",
  "Posterior inference in PMF with twin EB priors": "Given data X, our goal is to calculate the posterior P(U, V | X; row, col), which also depends on our choiceof priors row, col. The challenges are that this posterior is intractable (for any prior) and we simultaneouslywant to fit the priors to satisfy the EB criterion in Equation (16). Our strategy will be as follows. We will use variational inference (VI) (Blei et al., 2017) to approximate theposterior, taking gradient steps in the variational objective with respect to the posterior approximation (thevariational family). At the same time, however, the variational objective of VI is an approximation (lowerbound) of the log-marginal from Equation (16). So we also take gradient steps with respect to the EB priorsto maximize it. The result is an algorithm that simultaneously approximates the posterior and learns theEB prior.",
  "Eq[log q(U, V | X)].(24)": "Here, we use gradient ascent to maximize L(X; , ) with respect to (Ranganath et al., 2014).Wefurther use stochastic reparameterization gradients to take such steps (Kingma & Welling, 2013; Rezendeet al., 2014). Maximum marginal likelihood. At the same time, we would like to set the prior parameters to maximizethe marginal likelihood of the data (Equation (16)). The variational objective in Equation (24) convenientlyalso provides a lower-bound on the marginal likelihood (Blei et al., 2017):",
  "log P(X; row, col) L(X; , ).(25)": "So, we will also follow stochastic gradients of the ELBO with respect to the prior parameters row, colto maximize L(X; , ) with respect to row, col.This strategy has been used in the context of linearregression (Mukherjee et al., 2023). Twin EB. Putting these two pieces together, our algorithm is a stochastic gradient ascent of the ELBO withrespect to two sets of parameters. In optimizing with respect to , we minimize the KL divergence betweenq and the posterior; in optimizing with respect to row, col, we maximize the (approximate) marginallikelihood of the data. We use the Adam algorithm for stochastic optimization (Kingma & Ba, 2014) with a batch size of 128, andusing ten particles to obtain unbiased noisy estimates of the gradient of the ELBO via the reparameterizationtrick (the particles are samples from q to estimate the expectation Eq of the ELBO with Monte-Carlo).",
  "GoodBooks": ": Twin population priors induce robustness to prior selection The held-out likelihood issensitive to the choice of the prior hyper-parameters. GMF endowed with population priors on both row andcolumn latent variables, TwinEB (GMF), achieves comparable or better results than other methods. Eachsub-panel displays held-out log-likelihood from adjusting the column prior variance with a fixed row prior,while the right sub-panel does the opposite, varying the row prior variance with a constant column prior. Wedemonstrate four datasets, from the top to bottom, MovieLens 1M, Ru1322b-scRNAseq, UserArtists, andGoodBooks. In all datasets, we set L = 15. Similar results hold for other values of L (see Appendix D.3).",
  "Experiments": "We study Algorithm 1 in several real-world matrix factorization settings: book ratings, movie ratings, artistpreferences, and single-cell RNA sequence gene counts. For all datasets, we studied both Gaussian andPoisson matrix factorization. In all datasets, we set L = 15. Similar results hold for other values of L(see Appendix D.3).We found that TwinEB performs as well or better than manually searching for aparameterized prior, and performs as well or better than setting a simple parameterized prior by empiricalBayes.",
  "Datasets": "In this section we first review four real-world datasets, ranging from user-preferences to genomics, and thenexplore the impact of twin population priors on the performance of Gaussian and Poisson matrix factorization.In our experiments, we fix the number of row and column mixtures to Kr = 70 and Kc = 100 respectively.",
  "Evaluation Metric and Baselines": "We evaluate model performance using the likelihood of unseen interactions, that is, test holdout-likelihood(HOLL). To build intuition, we will use the example of user-item interactions where rows indicate users andcolumns items, and the entries of the input matrix their observed interactions. One approach to testingperformance is to hold-out a fraction of the entries of the input matrix, and to train the model on theheld-in portion, and test performance on the held-out entries.In this approach, all users are observedduring training. To test generalizability to cases when new users are observed, we can further hold-out aportion of users (rows) during training, and then test the performance on these hold-out users. Specifically,we will test the performance on masked entries of these hold-out users. This procedure measures stronggeneralization (Steck, 2019). At test time, we need to learn user-specific latents for the held-out rows. For this, we run the model in aspecial training step where the column latents are held fixed. Since these held-out users are neverthelesscoming from the same pool as our training users, we use only 30% of the observed entries for this specialtraining step. This may result in a more accurate measurement of out-of-distribution generalizability bybreaking spurious correlations between items and users due to biases present in the current dataset. Thesebiases may include the time-period of data collection, platforms used that may not exist in a new dataset.Finally, we report the model performance on 30% of the unseen entries of the held-out users.See thesupplement for the derivation of Equation (45) and more details on the experiments. Baseline. We evaluate the performance of the PMF (GMF) model with TwinEB against multiple baselines,namely (i) TwinEB-Single, (ii) PMF (GMF). TwinEB-Single is a simple form of TwinEB where all latentdimensions have an identical prior, which we learn. PMF is a prior of the same family as the TwinEB,but with fixed hyperparameters. We compare TwinEB against a large choice of fixed parameters, akin tohyperparameter selection.",
  "Experimental Procedure and Results: Gaussian Matrix Factorization": "We preprocess the data as follows. We standardize each column by subtracting the mean from non-zeroentries and dividing the result by their standard deviation. We study two scenarios: (i) maintaining a fixedprior on row-wise variables while varying the prior on column-wise variables, and (ii) holding the prior oncolumn-wise variables constant and adjusting the prior on row-wise variables. We set the variational familyas well as the mixture components for the population priors to be Gaussian. We set the fixed prior to N(0, 1)and vary the variance of the non-fixed one over {0.001, 0.01, 0.1, 1.0, 10}. For example, in , the leftcolumn of plots corresponds to fixing the prior on the column-wise variables to P(Vj) = N(0, 1) and varyingthe prior on the row-wise variables as P(Ui) = N(0, 2) with 2 {0.001, 0.01, 0.1, 1, 10}.",
  "We compared the GMF model to TwinEB (GMF) and TwinEB-Single (GMF). We treat zero entries asmissing values": "displays the outcomes for four real-world datasets. In GoodBooks and MovieLens-1M datasets,TwinEB achieves the highest test HOLL. For the UserArtists dataset, TwinEB does better than the fixedprior. Similar results were obtained by varying L to other values and are reported in Supplementary Figures 5,6 in Appendix D. We also compare to the method of Wang & Stephens (2021), we used the corresponding package flashr,that currently supports GMF. We designed an imputation experiment where we held-out 10% of entries ofa standardized matrix, and compare reconstruction accuracy on non-zero entries. We applied flashr to theabove four real-world datasets. Except in Ru1322b, the optimization objective in flashr encounters NaNvalues, which halt execution and terminate without results. For the Ru1322b dataset it achieves a meanabsolute error of 0.64 vs 0.51 for the TwinEB. For a comparison on simulated data, please see SupplementaryTable S1. We note that it is not straightforward to compare our method to that of Zhong et al. (2022); thesoftware implementation does not immediately support missing data and imputation.",
  "Experimental Procedure and Results: Poisson Matrix Factorization": "For the MovieLens 1M and GoodBooks datasets, we binarize the matrix, setting entries to one if a user hasrated a movie and to zero otherwise. For the Ru1322b dataset, we normalize the rows such that the sum ofall rows are equal; we then round each value to the nearest integer. This is to account for the effect of librarysize (Heumos et al., 2023). We treat zeros as missing data. In all models, we set the variational family to beLog-Normal, and the prior (or the mixture components for the population priors) to be Gamma. Similar toGMF, we vary the row or column prior parameters, while keeping the other one fixed. We parameterize theGamma prior by its mean and variance Gamma(, 2), where = / and 2 = /2. In each scenario, weset the fixed prior to Gamma(1, 10). For the varying prior, we set its mean to = 1 and change its variancealong {0.01, 0.1, 0.25, 1.0, 10.0, 100.0}. shows the results. In datasets except for GoodBooks, TwinEB outperforms TwinEBSingle. In the UserArtists dataset, TwinEBoutperforms other methods. TwinEB, on average, takes about 1.5X the runtime of PMF (ranging form 1.1Xto 2.1X). We point out that finding a good prior using grid-search incurs the sum of the cost of evaluationsof the individual points in the grid, over 6.5X the running time of the PMF with TwinEB prior. We notethat using a grid to find fixed priors (or alternatively, learning the scalar prior) does not guarantee reachinga best test HOLL. Indeed in the UserArtists dataset, TwinEB yields the best test HOLL. We also compared TwinEB to the method of da Silva et al. (2023); given the data, it estimates values forthe shape and rate parameters of the Gamma prior for both the row and column r.v.s. displays theestimated hyperparameters and the resulting test HOLL. PMF equipped with fixed priors values calculated from this methods yields Test HOLL that are on par orslightly worse than the TwinEB method. This method yielded negative hyperparameter estimates for theMovieLens-1M and GoodBooks datasets. These parameters are effectively not usable, since the parametersof a Gamma must be positive. Hence, we emphasize that unlike ours, this methods cannot be used on alldatasets. We explore additional baselines in Appendix D.2.",
  "Simulation: Complexity of the Prior": "Here, we examine the performance of the population priors as the number of mixture components are varied.To this end, we simulate a 1, 000 by 1, 500 dataset, with L = 64 dimensional row and column-wise r.v.s. Wesample the row- and column-wise r.v.s from a mixture of 15 and 20 Gamma distributions respectively, the",
  "Simulated Data - Study of K": ": Increasing the number of mixtures improves the model performance. Each line showsthe average test-HOLL over 10 seeds for a fixed value for Kc, the number of column mixtures, and varyingvalues of Kr, the number of row mixtures. Here, we set L = 64 to its true simulated value. The results aresimilar for L = 32 (see supplement).",
  "rate and shape parameters of which are sampled from a Gamma(1, 1). The mixture weights are sampledfrom a Dirichlet(e0, . . . , e0) where the concentration parameter is e0 = 10": "Performance Increase with Number of Mixture Components. The more mixture components, themore flexible the EB prior is. If learning a EB prior is beneficial, then we expect the performance to increasewith the number of mixture components. We run PMF with population priors with varying number of rowand column mixtures, setting the dimension of the latent variable to its true value, and, to avoid modelmisspecification, set the mixture prior family to Gamma. We then report the average of the test HOLLacross ten different seeds. , the log-likelihood of test held-out data increases with the number ofmixture components in the row prior.",
  "Discussion": "We introduced the twin population priors for probabilistic matrix factorization. We derived a method toestimate the corresponding posterior using Monte Carlo and variational inference. On real-world data, thismethod finds a prior as good as the best parametric prior chosen retrospectively. Our method uses the widely applicable black box variational inference (BBVI); the availability and ease of useof automatic differentiation, our implementation brings flexible priors in the context of matrix factorizationto a wider audience.For example, our proposed method works on multiple data types, both Gaussianand Poisson likelihoods, and it is easy to extend it to other reparameterizable likelihoods. In contrast, themethods of Wang et al. (2024) and da Silva et al. (2023) are tailored for only Gaussian and only count dataonly. With TwinEB, the latent dimensions L of the row-and column-wise variables is a hyperparameter.Inpractice, we found that setting L at about 15 was generally good enough (used across all our experiments)and ablation studies showed that the results were stable with the choice of L (GFM: Supplementary , PMF: Supplementary ). However, an attractive property of the method of da Silva et al is that itcan automatically set this value in closed form. Methods based on prior predictive statistics suggested byda Silva et al. (2023) could optimize the selection of L. A limitation of our model compared to others, is that it does not leverage auxiliary information. Auxiliaryinformation is information in addition to the observed interaction matrix. For instance, for movie ratings,the additional information could be user-specific attributes including age, race, and occupation, while item-",
  "Broader Impact Statement": "As a data-driven approach, our method is subject to potential pitfalls inherent in such methods, includinga tendency to inherit and amplify biases present in the data. If employed without proper supervision, thiscan lead to serious consequences including biased and/or discriminatory outcomes in real-world applications.These biases implicitly discriminate based on protected attributes (e.g., race, gender). One line of researchassumes specific protected features are known, and enforces fairness by decoupling these features from learntlatent variables (Togashi & Abe, 2022). If fairness criteria can be formulated in terms of a regularizationterm, it can be reinterpreted as learning fairness-aware priors and incorporated in our framework (Zhu et al.,2018).",
  "Acknowledgements": "This research was funded in part through the NIH/NCI Cancer Center Support Grant P30 CA008748 (listall MSK authors); NIH grant 5K99CA277562-02 (S.S.), the Eric and Wendy Schmidt Center at the BroadInstitute of MIT and Harvard (A.N.), the Africk Family Fund (A.N.), the National Science Foundation (NSF)grants IIS-2127869 and DMS-2311108 (D.B.), the Office of Naval Research grant N000142412243 (D.B). andthe Simons Foundation (D.B.). SPS was partially supported by the Halvorsen Center for ComputationalOncology and the MacMillan Center for the Non-Coding Cancer Genome.",
  "AIntroduction": "This is the supplement for the manuscript titled \"Population priors for matrix factorization\". The figures inthis document supplement Figures 2, 3, and 4 in the main text. We provide derivations for twin populationpriors for Gaussian matrix factorization, as well as computing the likelihood of the held-out data in section B.We then give more details about our experimental setup, including the parameters used in training (e.g.,batch-size) in section C. Finally, we provide results for additional experiments in section D: for additionalvalues of the latent dimension L for the datasets studied in the main text. Note that this document is accompanied by an archive file code.zip, that contains the source code, in-structions to install and run the code, and scripts to recreate the experiments and plot the figures in themanuscript. All scripts have been de-identified.",
  "BDerivations": "In this section, we give more details and derivations for the quantities defined in the main text. Specifically, insection B.1 we derive the twin population priors for the Gaussian matrix factorization (GMF). In section B.2,we derive the variational approximation to the twin population priors in GMF. Finally, in section B.3, wederive the expression for held-out likelihood in matrix factorization.",
  "CExperimental details": "In this section we give more details on our experimental studies in the main manuscript. In section C.1we describe preprocessing for the gene expression in the Ru1322-scRNAseq dataset.In section C.2, wespecify the parameters used during training. In section C.4 we give a brief description of the artifacts thataccompany this supplementary material.",
  "We set a batch size of 128 in all our experiments.We ran Poisson and Gaussian matrix factorizationexperiments for a maximum of 20, 000 iterations. By this step, all runs had converged": "We initialized the learning rate for the row and column variables, rlr and clr separately. We fix the initiallearning rate rlr {0.01} and clr {0.01}. In the experiments in the main text, we use 10 Monte Carlosamples to approximate the ELBO, while in the supplemental experiments, we use a single particle. For the PMF experiments in the supplement, we subsample zeros as is standard (Gopalan et al., 2015).We uniformly randomly subsample the same number of zeros as non-zero values to estimate the likelihood.Concretely, let L(Xout) denote the log-likelihood of the masked entries of the held-out rows, Xout = {Xi,j}.Then L(Xout) can be decomposed as the sum of non-zero and zero Xi,j:",
  "C.4Implementation": "Please refer to the code directory for the source code and instructions on how to run the model. The fileREADME.md contains instructions for installing the software, and running it. Under the data directory, weincluded preprocessed data for the MovieLens-100K dataset, a smaller version of the MovieLens-1M studiedin the main text. In the notebooks directory, we put notebooks used for preprocessing the data, and plottingthe figures in the manuscript. Finally, in the pipelines directory, we put nextflow scripts that recreateexperiments that we have run for the manuscript.",
  "DAdditional experiments": "In this section we present additional experimental results.We show results for additional values of thelatent dimension L. Concretely, section D.3 studies the effects of the twin population priors on Poisson andGaussian matrix factorization, on three real world datasets, while section D.4 examines the sensitivity of thetwin population priors to the choice of its hyper-parameters. We find that these results corroborate thosethat were presented in the main manuscript, that is, matrix factorization with traditional priors is sensitiveto the choice of the hyper-parameters of the prior, and twin population priors is a robust way to set theprior in this family of models.",
  "D.2Additional baselines": "Here, we introduce two additional baseline methods. One is based on normalizing flows (Papamakarios et al.,2021), and the other is based on learning the hyper-parameters of Gamma priors for our PMF experiments(close to Hierarchical Poisson Factorization of Gopalan et al. (2015)). We call both Twin, since we aresimultaneously learning the hyperparameters of the priors on the row and column latents. TwinHP In TwinHP, we infer the hyper-parameters of the two Gamma priors. This represents a moreexhaustive search in the hyper-parameter space of a model with uni-modal priors.Table S2 shows theresults (a positive value indicates that TwinEB performs better). We note that in the Ru1322b datasetthis method has the worst performance (as measured by Test Hold-Out likelihood), suggesting that theuni-modal prior is not well specified for that dataset. On the other hand, it achieves the best HOLL in theMovieLens-1M dataset, suggesting that a uni-modal prior is more appropriate in this dataset.",
  "det(fz) =1 + uT (1 tanh2(wT z + b)) w": "We do not claim that this implementation cannot be improved. Nevertheless, we find that TwinEB compareswell with this NF prior, out-performing it in 3 out of the 4 real-world datasets. We show the result in thefollowing table, where the second column shows the difference between Test-HOLL of TwinEB and TwinNF(a positive value indicates that TwinEB performs better). We have used the same training procedure as inthe main text, with 10 seeds.",
  "D.4Performance Increases, then Plateaus with the Number of Mixture Components,": "We show the performance of TwinEB on simulated data as we vary the number of row and column mixturecomponents . Here, we add results for additional values of L, and also study the MovieLens 100K dataset.The result in Figures 7 suggests that as the number of mixture components increases, the performance ofTwinEB improves. The gain is apparent when the number of row components Kr increases. This is expected,as our evaluation procedure, measures the generalizability for held-out rows, but not held-out columns.",
  "Aj for the input to the EBNM problem. That is where functions u(.)and s(.) come from": "In their experiments, they set Gu = Gv = G, and try two settings for the family of the prior, namely,G {SN, PN}, the scale mixture of normals, and a mixture of point mass at zero and a standard normaldistribution. G = SN = Bb bN(0, b) and G = PN = 00 + (1 0)N(0, b). For SN, Bb b = 1 andb are B values lying on a user specified grid. This paper is somewhat sparse in details. For instance, the scale mixture of normals is not explicitly definedanywhere in the paper. More details are found in the first authors PhD thesis Wang (2017). The frameworkis implemented in the flashr R package."
}