{
  "Abstract": "Hypergraphs are a powerful abstraction for modeling high-order interactions between a setof entities of interest and have been attracting a growing interest in the graph-learningliterature. In particular, directed hypegraphs are crucial in their capability of representingreal-world phenomena involving group relations where two sets of elements affect one anotherin an asymmetric way. Despite such a vast potential, an established, principled solutionto tackle graph-learning tasks on directed hypergraphs is still lacking. For this reason, inthis paper we introduce the Generalized Directed Hypergraph Neural Network (GeDi-HNN),the first spectral-based Hypergraph Neural Network (HNN) capable of seamlessly handlinghypergraphs featuring both directed and undirected hyperedges. GeDi-HNN relies on agraph-convolution operator which is built on top of a novel complex-valued Hermitian matrixwhich we introduce in this paper: the Generalized Directed Laplacian LN. We prove thatLN generalizes many previously-proposed Laplacian matrices to directed hypergraphs whileenjoying several desirable spectral properties. Extensive computational experiments againststate-of-the-art methods on real-world and synthetically-generated datasets demonstrate theefficacy of our proposed HNN. Thanks to effectively leveraging the directional informationcontained in these datasets, GeDi-HNN achieves a relative-percentage-difference improvementof 7% on average (with a maximum improvement of 23.19%) on the real-world datasets andof 65.3% on average on the synthetic ones.",
  "Introduction": "In recent years, ground-breaking research in the graph-learning literature has been prompted by seminalworks on Graph Neural Networks (GNNs) such as (Scarselli et al., 2009; Micheli, 2009; Li et al., 2016; Kipfand Welling, 2017; Velikovi et al., 2018). Since representing a set of complex relationships solely throughundirected or directed graphs can prove too restrictive in many real-world scenarios, generalizations tographs allowing for higher-order (group) relationships, i.e., hypergraphs, have been considered. Hypergraphsgeneralize the notion of a graph to the case where an edge (a hyperedge) can connect an arbitrary numberof nodes, thus allowing to capture not just pairwise (dyadic) relationships but also group-wise (polyadic)dynamics (Schaub et al., 2021). This has led to a new stream of research devoted to the investigation of",
  "Hypergraph Neural Networks (HNNs) (Feng et al., 2019; Chien et al., 2021; Huang and Yang, 2021; Wanget al., 2023a;b)": "While most of the literature on HNNs has focused on undirected hypergraphs, many real-world phenomenasuch as chemical reactions are naturally modeled on hypergraphs whose hyperedges have a notion of direction.Despite this, the directed case has been addressed only sporadically, and often only in application-specificscenarios such as traffic forecasting (Luo et al., 2022) and music recommendation (La Gatta et al., 2022). Tothe best of our knowledge, a general solution based on a convolution operator which is solidly grounded inspectral graph theory while not being problem-dependent is missing. We aim at bridging such a gap. In this paper, we introduce the Generalized-Directed Hypergraph Neural Network (GeDi-HNN), the firstspectral-based HNN capable of seamlessly handling hypergraphs featuring both directed and undirectedhyperedges. GeDi-HNN relies on a graph-convolution operator which is built on top of a novel HermitianLaplacian matrix which we introduce in this paper: the Generalized Directed Laplacian LN. LN generalizesvarious Laplacian matrices: the one proposed in Zhou et al. (2006) and used in Feng et al. (2019) forundirected hypergraphs, the Sign-Magnetic Laplacian proposed for directed graphs in (Fiorini et al., 2023),and the classical Laplacian matrix (Chung and Graham, 1997) used for undirected graphs.",
  "We extend the literature on spectral-based HNNs by introducing GeDi-HNN, the first spectral HNNcapable of handling hypergraphs with both directed and undirected hyperedges": "We introduce the Generalized Directed Laplacian matrix LN; we prove that it enjoys several desirableproperties, among which admitting an eigenvalue decomposition, and that it generalizes many existingLaplacian matrices. Compared to state-of-the-art methods, GeDi-HNN achieves a relative-percentage-difference improve-ment of 7% on average (with a maximum improvement of 23.19%) on the real-world datasets and of65.3% on average on the synthetic ones. This demonstrates its efficacy in extracting and utilizing theinformation encoded in the hyperedge directions. The paper is organized as follows. Preliminaries and previous works are summarized in . LN isintroduced in , along with its properties. provides an overview of GeDi-HNNs architecture,which is built upon LN. Experimental results are reported in . Conclusions are drawn in .The proofs of our theorems and additional details are provided in the Appendix.",
  "Undirected and Directed Hypergraphs": "A hypergraph is an ordered pair H = (V, E), with n := |V | and m := |E|, where V is the set of vertices(or nodes) and E 2V \\ {} is the (nonempty) set of hyperedges. The hyperedge weights are stored in thediagonal matrix W Rmm. The vertex and hyperedge degrees are defined as du = eE:ue |we| for u Vand e = |e| for e E and are stored in two diagonal matrices Dv Rnn and De Rmm. Hypergraphswhere (e) = k for some k N for all e E are called k-uniform. Graphs are 2-uniform hypergraphs.Following Gallo et al. (1993), we define a directed hypergraph as a hypergraph where each edge e E ispartitioned in a head set H(e) and a tail set T(e). If T(e) is empty, e is an undirected edge.",
  "Graph Fourier Transform and Graph Convolutions": "Let L be a generic Laplacian matrix of a given 2-uniform hypergraph H which embeds its topology. Weassume that L admits an eigenvalue decomposition L = UU , with U Cnn. U is the conjugate transposeof U, and Rnn is a diagonal matrix. Let x Cn be a graph signal, i.e., a function x : V C whosedomain coincides with the vertices of H. Following Shuman et al. (2013), we call x = F(x) = U x the graph",
  "Published in Transactions on Machine Learning Research (10/2024)": "As activation function, we adopt a complex extension of the ReLU function, in which, for a given z C,(z) = z if (z) 0 and (z) = 0 otherwise. A similar choice is followed in other GNNs/HNNs works suchas (Fiorini et al., 2023; 2024). We project the complex-valued output of the last convolutional layer into thereals via an unwind operation by which Z(X) Cnc is transformed into ((Z(X))||(Z(X))) Rn2c,where || is the concatenation operator. To obtain the final result, we add linear layers to GeDi-HNNs architecture and a residual connection forevery convolutional layer except for the first one. These connections have been proven to aid in trainingdeeper models by allowing them to retain information from the input of the previous layers (He et al., 2016;Kipf and Welling, 2017). GeDi-HNNs architecture is depicted in .",
  "v 2.(3)": "When restricting to undirected graphs (i.e., 2-uniform undirected hypergraphs), an alternative Laplacianmatrix, the so-called Signed Laplacian Matrix, can be obtained with a similar construction to equation 3.This involves applying an arbitrary orientation to the edges of the graph (i.e., arbitrarily multiplying by1 exactly one entry per column of B). Calling such a matrix B, the Signed Laplacian matrix L and itsnormalized counterpart LN are defined as follows:",
  "Q = Dv + AL = Dv AQN = I LNLN = I QN.(5)": "While the definition of L in equation 4 does not extend nicely to general (not 2-uniform) hypergraphs,the definition of LN in equation 5 does.2A generalization of the Signed Laplacian to general undirectedhypergraphs which follows LN = I QN from equation 5 is proposed by Zhou et al. (2006), and reads:3",
  "= I QN.(6)": "1Following w.l.o.g. Singh and Chen (2022), we employ the approximation G = Kk=0 kk, from which we deduce Y x =U GUx = U(Kk=0 kk)Ux = Kk=0 k(UkU)x = Kk=0 kLkx.2For instance, the choice of which entries of B should be multiplied by 1 would drastically affect L (rendering the orientationnot arbitrary anymore) and L may feature both positive and negative off-diagonal entries, thereby violating LN = I QN(notice that QN 0 holds by construction).3In Zhou et al. (2006), QN is called .",
  "Discrete Laplacian Matrices for Directed 2-Uniform Hypergraphs": "In directed 2-uniform hypergraphs, the presence of edge directions renders the graph asymmetric and noneof the previous definitions of the graph Laplacian apply. Indeed, those in equation 3, equation 4, andequation 6 would symmetrize the graph and destroy its directions, while the one in equation 5 would leadto an asymmetric matrix which does not admit an eigenvalue decomposition and, thus, would prevent theapplication of the graph Fourier transform. The Magnetic Laplacian L(q), proposed by Lieb and Loss (1993) in the context of electromagnetic fieldsand adopted within a spectral GNN by Zhang et al. (2021b;a), is a complex-valued, Hermitian Laplacianmatrix. It encodes the directional information of the graph while enjoying an eigenvalue decomposition with anonnegative, real spectrum. This Laplacian matrix generalizes the Laplacian L defined in equation 5. LettingAs := 1",
  "The Sign-Magnetic Laplacian L is a matrix proposed by Fiorini et al. (2023) which is well-defined also forgraphs with negative edge weights and enjoys some extra desirable properties. If q = 1": "4, L and L(q) coincideif the latter is first constructed for an unweighted version of the graph and then multiplied component-wiseby As. Thus, L is invariant to a positive weight scaling which could otherwise alter the sign pattern of L(q)and, thus, the edge direction. Letting Ds := diag(|As| e) and sign : R {1, 0, 1} be the component-wisesignum function, L and its normalized version are defined as follows:",
  "Theorem 4. QN is positive semidefinite": "To show that LN is positive semidefinite, we first derive the equation of ||x||2LN , i.e., the p-Dirichlet energyfunction with p = 2 induced by the Generalized Directed Laplacian for a signal x Cn. In line with the2-uniform case (Shuman et al., 2013), such a function provides a measure of the global smoothness of x acrossthe entire hypergraph.",
  "Generalized Directed Hypergraph Neural Network (GeDi-HNN)": "We embed the Generalized Directed Laplacian LN in GeDi-HNN, the first HNN capable of handlinghypergraphs with both undirected and directed hyperedges via a spectral-based convolution operator. For thispurpose, we rely on the localized filter of which led to equation 1. Letting L = LN, our convolutionoperator is Y x = 0I + 1LN.",
  "Proposition 1. The convolution operator obtained from equation 1 by letting L = LN with parameters 0, 1coincides with the one obtained by letting L = QN with parameters 0 = 0 + 1, 1 = 1": "Proof. Consider the two operators 0I + 1LN and 0I + 1 QN. Since LN = I QN, the first operator reads:0I + 1(I QN). This is rewritten as (0 + 1)I 1 QN. By operating the choice 0 = 0 + 1 and 1 = 1,the second operator is obtained.",
  ": GeDi-HNNs architecture: after two complex convolutional layers and a residual connection, weunwind the real and imaginary parts of the feature matrix and apply a fully connected layer": "Complexity of GeDi-HNN.Let us assume w.l.o.g. (as it does not affect the asymptotic analysis ofcomplexity) that the input and output data of each convolutional layer except for the last one have c channelsand that the output of the last convolutional layer and the number of channels of each linear layer are equalto c. With convolutional layers and S linear layers, GeDi-HNNs complexity is O((n2c + nc2) + nc + (S 1)(nc2) + ncd + nd), where d is the number of hidden node classes (the last channel of the last linear layer).Letting c := max{c, c, d} be the largest number of channels throughout the network, we have a complexity ofO((n2c) + ( + S)(nc2)). Such a quantity is quadratic w.r.t. the number of nodes n and the largest numberof channels c, which is in line with previous GNNs and HNNs architectures. For more details see Appendix C.",
  "Numerical Experiments": "We now illustrate the results of an extensive set of experiments carried out to evaluate the performance ofGeDi-HNN on directed hypergraphs. We compare our proposal against 11 state-of-the-art methods fromthe hypergraph-learning literature: HGNN (Feng et al., 2019), HCHA4 (Bai et al., 2021), HCHA withthe attention mechanism (Bai et al., 2021), HNHN (Dong et al., 2020), HyperGCN (Yadati et al., 2019),UniGCNII (Huang and Yang, 2021), HyperDN (Tudisco et al., 2021), AllDeepSets (Chien et al., 2021),AllSetTransformer (Chien et al., 2021), LEGCN Yang et al. (2022), ED-HNN (Wang et al., 2023a), andPhenomNN (Wang et al., 2023b). The hyperparameters of these baselines and of our proposed model areselected via grid search (see Appendix E). The experiments are carried out on the node classification task of predicting the class associated with eachnode, which is the same task that was consistently used throughout the papers where the 11 baselines wereproposed. The comparison is carried out on real-world datasets (Subsection 5.1) and on synthetic hypergraphs(Subsection 5.2).",
  "Node Classification Task on Real-World Datasets": "We test GeDi-HNN on 10 real-world datasets from the literature: Cora, Citeseer, and PubMed (Zhang et al.,2022); email-Enron and email-Eu (Benson et al., 2018); Texas, Wisconsin, and Cornell (Pei et al., 2020);WikiCS (Mernyei and Cangea, 2020); and Telegram (Bovet and Grindrod, 2020). We test the 11 baselines(which, we recall, are not designed to handle directed hypergraphs) on an undirected version of the 10 datasetswhich is compiled following the procedure of Feng et al. (2019). Differently, we test GeDi-HNN, which is theonly method designed to handle directed hypergraphs, on a directed version of these instances, which wecompile following a slight modification to the previous procedure. For every node p sharing a relationshipwith nodes a, b, c, d, we create the hyperedge e with H(e) = {p} and T(e) = {a, b, c, d}. Considering, e.g., acitation relationship in CiteSeer where paper p cites papers a, b, c, d, in the undirected case we follow Fenget al. (2019) and create the hyperedge {a, b, c, d} to semantically represent the paper p whereas, in thedirected case, we set {p} as head and {a, b, c, d} as tail. We adopt the split proposed by Zhang et al. (2021b)for Telegram, Texas, Wisconsin, and Cornell and the split of Chien et al. (2021) for the other ones. Allthe experiments are conducted using 10-fold cross-validation. More details on the datasets can be found inAppendix D.",
  "MethodCoraCiteseerPubmedemail-Euemail-Enron": "HGNN79.39 1.3672.45 1.1686.44 0.4439.80 2.7744.32 5.44HCHA/HGNN+79.14 1.0272.42 1.4286.41 0.3641.01 3.5544.59 6.77HCHA w/ Attention58.20 2.5368.44 1.2779.90 1.7028.75 2.8235.68 6.96HNHN76.36 1.9272.64 1.5786.90 0.3029.92 1.8830.01 12.56HyperGCN78.45 1.2671.28 0.8282.84 8.6730.81 2.8036.76 5.87UniGCNII78.81 1.0573.05 2.2188.25 0.4040.81 2.7641.62 5.28LEGCN74.74 1.2572.74 0.8688.12 0.7430.16 2.2835.41 5.76HyperND79.20 1.1472.62 1.4986.68 0.4329.23 1.8035.41 5.62AllDeepSets76.88 1.8070.83 1.6388.75 0.3329.92 1.8836.76 7.01AllSetTransformer78.58 1.4773.08 1.2088.72 0.3741.58 5.1345.41 8.43ED-HNN80.31 1.3573.70 1.3889.03 0.5330.85 2.8742.97 7.37PhenomNN82.29 1.4275.10 1.5988.07 0.4831.09 3.8337.03 7.21",
  "MethodTelegramTexasWisconsinCornellWikiCS": "HGNN59.42 6.0471.08 7.3275.69 4.6470.81 4.7377.95 5.69HCHA/HGNN+52.12 3.3271.35 6.7773.53 5.4170.81 5.0676.50 5.07HCHA w/ attention57.69 2.8672.97 6.5070.59 6.4073.51 4.7311.67 4.79HNHN50.77 8.2775.41 7.2681.18 3.7274.32 5.1426.47 18.1HyperGCN55.77 3.9565.95 9.0370.98 5.0568.39 6.8775.80 6.16UniGCNII55.58 5.0184.17 5.4486.47 5.0276.76 5.1383.24 1.07LEGCN45.19 5.1579.19 4.7884.51 5.3573.78 6.1278.73 1.19HyperND43.65 4.3581.62 6.6085.10 4.4574.87 4.6072.28 3.14AllDeepSets38.46 6.0882.97 5.8584.51 5.4378.11 3.7083.00 1.10AllSetTransformer57.12 5.2180.27 5.5681.96 6.2676.47 5.4183.37 3.77ED-HNN54.42 6.0183.78 7.6486.27 2.4577.84 5.6782.12 1.57PhenomNN54.61 4.7284.59 5.4186.28 4.6276.49 5.5680.07 0.61",
  "GeDi-HNN75.01 4.9684.59 4.7888.43 3.3180.54 2.7982.23 1.47GeDi-HNN w/o directions64.80 6.6083.51 4.5186.66 4.9677.83 4.6582.52 1.19": "The accuracy obtained across the different methods and datasets is reported in . The results showthat, across the whole dataset, GeDi-HNN achieves an average additive performance improvement overthe best-performing competitor of approximately 4.20 percentage points. In terms of Relative PercentageDifference (RPD)5, we have an average RPD improvement of 7.06%. The most significant improvement isobserved on Telegram, where GeDi-HNN achieves an average RPD improvement of approximately 23.19%and an average additive improvement of 15.59 percentage points w.r.t. the best competitor from the literature(HGNN). Overall, GeDi-HNN ranks first on 9 out of 10 datasets and fourth on the 10th dataset, where it",
  "achieves an accuracy of 82.23, which is only 1.14 percentage points less than the best one recorded in theexperiment": "also presents the results of an ablation study aimed at demonstrating that a significant portion ofthe superior performance of GeDi-HNN is attributable to the Generalized Directed Laplacian LN ratherthan to the networks architecture. In this study, we compare GeDi-HNN to GeDi-HNN w/o directions,a version which employs the undirected hypergraph Laplacian proposed in Zhou et al. (2006) (whichdisregards hyperedge directions) instead of LN. As shown in , GeDi-HNN outperforms GeDi-HNNw/o directions with an RPD improvement of 4.90% (an additive difference of 3.35 percentage points) onaverage across 9 out of 10 datasets and achieves nearly identical performance on the 10th dataset (WikiCS),where the difference between the two versions is negligible (of, additively, only 0.29 percentage points). Thelargest improvement, observed on Telegram, is of a RPD of 14.61% (an additive difference of 10.21 percentagepoints). These results underscore the importance of incorporating the directionality of the hyperedges and suggest that,thanks to the Generalized Directed Laplacian, GeDi-HNN effectively captures and utilizes this information.",
  ": Schematic representation ofthe direction of the flow induced by thehyperedges in a synthetic dataset with5 classes (c = 5)": "Drawing inspiration from the methodology proposed in Zhang et al.(2021b), we rely on a collection of datasets which are generated asfollows. First, the set V of vertices is partitioned into c equally-sizedclasses C1, . . . , Cc with uniform probability. Subsequently, for eachclass C, Ii intra-class undirected hyperedges are created, each witha cardinality uniformly sampled from {hmin, . . . , hmax}, containingvertices of the same class also sampled with uniform probability.Similarly, for each pair of classes Ci and Cj with i < j, Io inter-class directed hyperedges are created. The head and tail sets aresampled from Ci and Cj, respectively, with uniform probability, andboth have a cardinality uniformly sampled from {hmin, . . . , hmax}. portrays the directional flow the hyperedges induce amongthe classes of a synthetic dataset with 5 classes. Notice how theflow is directed from a class Ci to a class Cj only if i < j. Using this methodology, we generate three distinct datasets withparameters n = 500, c = 5, hmin = 3, hmax = 10, Ii = 30, andan increasing number of inter-class hyperedges Io = 10, 30, 50. Forthese datasets, we implement a 50%/25%/25% split for training,validation, and testing, respectively. The experiments are conductedusing 10-fold cross-validation. The experimental results, summarized in , indicate a substantial performance difference betweenGeDi-HNN and the other 11 baselines, which increases the higher the value of Io is. On average, GeDi-HNNachieves an accuracy that surpasses the best competitor from the literature with an RPD improvement of65.31% (an additive improvement of 35.47 percentage points). The additive difference compared to thesecond-best performer is of up to 39.19 percentage points. We conclude with the ablation study where GeDi-HNN is compared to GeDi-HNN w/o directions. Theresults reveal a substantial accuracy difference of 40.48 percentage points (on average) between GeDi-HNN andGeDi-HNN w/o directions Notably; as the number of inter-class hyperedges (Io) increases, the performanceof GeDi-HNN improves, while that of GeDi-HNN w/o directions declines. This finding underscores thesignificant contribution of our proposed Generalized Directed Laplacian to the superior performance ofGeDi-HNN.",
  "Conclusion": "We introduced GeDi-HNN, the first spectral HNN capable of handling hypergraphs with both undirectedand directed edges. GeDi-HNN is built upon a novel complex-valued Laplacian matrix, the GeneralizedDirected Laplacian, which is a Hermitian matrix that employs a complex-number representation of thehyperedge directions. This approach naturally generalizes several previously proposed Laplacians for bothgraphs and hypergraphs. Our proposal enables the seamless integration of directionality in HNNs, which iscrucial for accurately modeling various real-world phenomena involving asymmetric high-order interactions.Our proposed GeDi-HNN model utilizes this new Laplacian matrix to perform spectral convolutions onhypergraphs featuring both undirected and directed hyperedges. Extensive computational experiments on both real-world and synthetic datasets have demonstrated thesuperior performance of GeDi-HNN in 12 out of 13 experiments compared to a comprehensive representativegroup of state-of-the-art methods for the node classification task. These findings underscore the importanceof incorporating directional information within GeDi-HNNs convolution operator. Specifically, GeDi-HNNconsistently outperforms existing models across various datasets, achieving an average relative-percentage-difference improvement of 7% on real-world dataset (with a maximum improvement of 23.19%) and of 65.3%on synthetic datasets. The superiority of our method is particularly evident in the experiments on synthetichypergraphs. These results highlight the potential of adopting GeDi-HNN to significantly enhance themodeling of complex, directed interactions within a hypergraph to the benefit of the hypergraph-learningtask at hand.",
  "Mingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional neural networks on graphs with chebyshevapproximation, revisited. Advances in neural information processing systems, 35:72647276, 2022a": "Yixuan He, Xitong Zhang, Junjie Huang, Benedek Rozemberczki, Mihai Cucuringu, and Gesine Reinert.PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed andDirected Graphs. arXiv preprint arXiv:2202.10793, 2022b. Jing Huang and Jie Yang. Unignn: a unified framework for graph and hypergraph neural networks. InProceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, 2021. Thomas. N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5thInternational Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings, 2017.",
  "Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graphneural network model. IEEE Transactions on Neural Networks, 20(1):6180, 2009": "Michael T Schaub, Yu Zhu, Jean-Baptiste Seby, T Mitchell Roddenberry, and Santiago Segarra. Signalprocessing on higher-order networks: Livinon the edge... and beyond. Signal Processing, 187:108149, 2021. David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The emergingfield of signal processing on graphs: Extending high-dimensional data analysis to networks and otherirregular domains. IEEE signal processing magazine, 30(3):8398, 2013.",
  "Peihao Wang, Shenghao Yang, Yunyu Liu, Zhangyang Wang, and Pan Li. Equivariant hypergraph diffusionneural operators. In International Conference on Learning Representations (ICLR), 2023a": "Yuxin Wang, Quan Gan, Xipeng Qiu, Xuanjing Huang, and David Wipf. From hypergraph energy functionsto hypergraph neural networks. In Proceedings of the 40th International Conference on Machine Learning,pages 3560535623, 2023b. Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.Hypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in neuralinformation processing systems, 32, 2019.",
  "Theorem 1. If H is an undirected hypergraph, LN = and QN = QN": "Proof. Since H = (V, E) is an undirected hypergraph, B is binary and only takes values 0 and 1 (rather thanbeing ternary and taking values 0, 1, i, which is the case in general). In particular, for each edge e E wehave Bue = 1 if either u H(e) or u T(e) and Bue = 0 otherwise. Consequently, the directed incidencematrix B is identical to the non-directed incidence matrix B, i.e., B = B. Thus, by construction, LN = and QN = QN.",
  "Corollary 3. LN is positive semidefinite": "Proof. Since LN is Hermitian, it can be diagonalized as UU for some U Cnn and Rnn, where is diagonal and real. We have xLNx = xUU x = yy with y = U x. Since is diagonal, we haveyy = uV uy2u. Thanks to Theorem 5, the quadratic form xLNx associated with LN is a sum ofsquares and, hence, nonnegative. Combined with xLNx = uV uy2u, we deduce u 0 for all u V .",
  "The detailed calculations for the (inference) complexity of GeDi-HNN are as follows": "1. The Generalized Directed Laplacian LN is constructed following equation 7 in time O(n2m), wherethe factor m is due to the need for computing the product between two rows of B to calculate eachentry of LN. After LN has been computed, the convolution matrix Y Cnn is constructed in timeO(n2). Note that such a construction is carried out entirely in pre-processing and is not required atinference time. 2. Each of the convolutional layers of GeDi-HNN requires O(n2c+nc2 +nc) = O(n2c+nc2) elementaryoperations across 3 steps. Let Xl1 be the input matrix to layer l = 1, . . . , . The operations thatare carried out are the following ones.",
  "DFurther Details on the Datasets": "We test GeDi-HNN on ten real-world dataset. Cora, Citeseer, and PubMed (Zhang et al., 2022); email-Eu,and email-Enron (Benson et al., 2018); Texas, Wisconsin, and Cornell (Pei et al., 2020); WikiCS (Mernyeiand Cangea, 2020); and Telegram (Bovet and Grindrod, 2020). Cora, Citeseer, and PubMed are citation networks with node labels based on paper topics. In these citationnetworks, the nodes represent papers, their relationships denote citations of one paper by another, and thenode features are the bag-of-words representation of papers. Email-Enron and email-Eu are two email datasetsone from communications exchanged between Enronemployees (Klimt and Yang, 2004) and the other from a European research institution (Paranjape et al., 2017).The nodes are email addresses and their relationships are of sender-receiver type. Since no node labeling ispresent in these two datasets, we define the node labels (node classes) using the Spinglass algorithm (Reichardtand Bornholdt, 2006). Texas, Wisconsin, and Cornell are WebKB data sets extracted from the CMU World Wide KnowledgeBase (Web->KB) project.8 WebKB is a webpage data set collected from computer science departments ofvarious universities by Carnegie Mellon University. In these networks, the nodes represent web pages, andthe relationship are hyperlinks between them. The node features are the bag-of-words representation of theweb pages. The web pages are manually classified into the five categories: student, project, course, staff, andfaculty.",
  "Io = 1050025059.05Io = 30500450510.79Io = 50500650511.63": "Model Settings.We trained every learning model considered in this paper for up to 500 epochs. Weadopted a learning rate of 5 103 and employed the optimization algorithm Adam with weight decays equalto 5 104 (in order to avoid overfitting). For all the models that adopt the classification layer, we set it to 2.",
  "We adopted a hyperparameter optimization procedure to identify the best set of parameters for each model.In particular, the hyperparameter values are:": "For AllDeepSets and ED-HNN, the number of basic block is chosen in {2, 4, 8}, the number of MLPsper block in {1, 2}, the dimension of the hidden MLP (i.e., the number of filters) in {64, 128, 256, 512},and the classifier hidden dimension in {64, 128, 256}. For AllSetTransformer the number of basic block is chosen in {2, 4, 8}, the number of MLPs per blockin {1, 2}, the dimension of the hidden MLP in {64, 128, 256, 512}, the classifier hidden dimension in{64, 128, 256}, and the number of heads in {1, 4, 8}.",
  ". 0 = 0.1, 1 = 0.1 and prop step= 8,2. 0 = 0, 1 = 50 and prop step= 16,3. 0 = 1, 1 = 1 and prop step= 16,4. 0 = 0, 1 = 20 and prop step= 16": "For GeDi-HNN and GeDi-HNN w/o directionality, the number of convolutional layers is chosenin {1, 2, 3}, the number of filters in {64, 128, 256, 512}, and the classifier hidden dimension in{64, 128, 256}. We tested GeDi-HNN both with the input feature matrix X Cnc where (X) =(X) = 0 and with (X) = 0.",
  "FFrom a Directed Hypergraph to the Generalized Directed Laplacian": "To illustrate the representation of a directed hypergraph in our Generalized Directed Laplacian, consider adirected hypergraph H = (V, E) with V = {v1, v2, v3, v4, v5} and E = {e1, e2}. The incidence relationshipsare defined as follows: v1, v2 H(e1), v3 T(e1), v4, v5 H(e2), and v1, v2 T(e2). The hyperedges haveunit weights (i.e., W = I). The hyperedge cardinalities are e1 = 3 and e2 = 4. For this hypergraph, we construct our Generalized Directed Laplacian using the following matrices: theincidence matrix B, its conjugate transpose B, the vertex degree matrix Dv, and the hyperedge degreematrix De."
}