{
  "Abstract": "Most adversarial attacks and defenses focus on perturbations within small p-norm con-straints. However, p threat models cannot capture all relevant semantics-preserving per-turbations, and hence, the scope of robustness evaluations is limited.In this work, weintroduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leveragesthe advancements in score-based generative models to generate unrestricted adversarial ex-amples that overcome the limitations of p-norm constraints. Unlike traditional methods,ScoreAG maintains the core semantics of images while generating adversarial examples, ei-ther by transforming existing images or synthesizing new ones entirely from scratch. Wefurther exploit the generative capability of ScoreAG to purify images, empirically enhancingthe robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAGimproves upon the majority of state-of-the-art attacks and defenses across multiple bench-marks. This work highlights the importance of investigating adversarial examples boundedby semantics rather than p-norm constraints. ScoreAG represents an important step to-wards more encompassing robustness assessments.",
  "(d) APGD ()": ": Examples of various adversarial attacks on an image of the class tiger shark\" (a). The insetvisualizes a heatmap of the strength of the corresponding perturbation. Despite the fact that the perturbationgenerated by ScoreAG-GAT (b) lies outside of common p-norm constraints ( = 188/255, 2 = 18.47), itis aware of the semantics: removing a small fish to change the predicted label to hammer shark\". This isin stark contrast to APGD (Croce & Hein, 2020b) with matching norm constraints, which either (c) resultsin highly perceptible and unnatural changes, or (d) fails to preserve image semantics completely. This is anexample of Generative Adversarial Transformation (GAS), one of the three use-cases of ScoreAG.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Diffusion-Based Attacks. Two recent works by Chen et al. (2023a) and Xue et al. (2023) propose DiffAt-tack and Diff-PGD, respectively. Diff-PGD performs projected gradient descent in the latent diffusion spaceto obtain -bounded adversarial examples, whereas DiffAttack generates unrestricted adversarial examplesby leveraging a latent diffusion model. However, as both methods employ only the final denoising stagesof the diffusion process in a similar fashion to SDEdit (Meng et al., 2021), the adversarial perturbationsonly incorporate changes of high-level features. Finally, Chen et al. (2023c) implement PGD in the -normwithin the latent space of stable diffusion. In parallel, Chen et al. (2023b) apply PGD iteratively at eachstep of the diffusion process and combine it with adversarial inpainting. Unlike previous works, ScoreAGdoes not rely on PGD in the latent space for its attack and semantic preservation, but solely leverages thediffusion manifold in combination with a task-specific guidance. Adversarial Purification. In response to the introduction of adversarial attacks, a variety of adversarialpurification methods to defend machine learning models have emerged. Early works utilized GenerativeAdversarial Networks (GANs) Song et al. (2017; 2018); Samangouei et al. (2018) and Energy-Based Models(EBMs) (Hill et al., 2020) to remove adversarial perturbations from images. More recent methods haveshifted focus towards score-based generative models, like ADP (Yoon et al., 2021), and diffusion models,such as DiffPure (Nie et al., 2022). However, ADP and DiffPure only denoise with small noise magnitudesduring the purification process and are thereby limited to correcting high-level adversarial features, whereasScoreAG traverses the whole diffusion process, providing more flexibility in purifying perturbations. Kanget al. (2023) have recently shown that these purification methods decrease in effectiveness in a white-boxsetting by evasion attacks. However, as previously mentioned, we focus on preprocessor black-box attacks,which are more relevant in real-world problems.",
  "How can we generate semantics-preserving adversarial examples beyond p-norm constraints?": "We propose to leverage the significant progress in diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020)and score-based generative models (Song et al., 2020) in generating realistic images. Specifically, we introduceScore-Based Adversarial Generation (ScoreAG), a framework designed to synthesize adversarial examples,transform existing images into adversarial ones, and purify images. Using diffusion guidance (Dhariwal &Nichol, 2021), ScoreAG can generate semantics-preserving adversarial examples that are not captured bycommon p-norms (see ). Overall, ScoreAG represents a novel tool for assessing and enhancing theempirical robustness of image classifiers.",
  "Background": "Score-Based Generative Modelling. Score-based generative models (Song et al., 2020) are a class ofgenerative models based on a continuous-time diffusion process {xt}t accompanied by their correspond-ing probability densities pt(x). The diffusion process progressively perturbs a data distribution x0 p0 intoa prior distribution x1 p1. This transformation is formalized as a Stochastic Differential Equation (SDE),i.e.,",
  "Here, () : R>0 serves as a time-dependent weighting parameter, and t is uniformly sampled fromthe interval": "In this formulation, x0 p0 is sampled from the data distribution, and xt p0t(xt | x0) follows thediffusion process at time t. The goal is to train the network s to accurately match the true score functionxt log p0t(xt | x0), enabling data generation through the reverse diffusion process, which can be solvedusing numerical solvers.",
  "Diffusion Guidance.To enable conditional generation with unconditionally trained diffusion models,": "Dhariwal & Nichol (2021) introduce classifier guidance. The central idea is to generate samples from theconditional distribution p(x0 | c), where c represents a specific class, i.e., sampling images of class c. Toachieve this, the authors replace the gradient of the unconditional distribution pt(xt) in the reverse process(see equation 2) with its conditional counterpart.",
  "xt log p(xt | c) = xt log p(xt) + xt log p(c | xt),(4)": "where xt log p(xt) represents unconditional score function and xt log p(c | xt) represents the guidancescore. The unconditional score function is approximated using the neural network s, which is trained usingthe loss in equation 3. To compute the guidance score xt log p(c | xt), Dhariwal & Nichol (2021) utilize the gradients of a time-dependent classifier f(xt, t) with respect to xt. The guidance score steers the generation process towardssamples that are consistent with the desired class c. This method allows an unconditional diffusion model,i.e., a model trained without conditional information, to be adapted for conditional tasks, enabling thegeneration of class-specific samples. Classifier guidance has since been extended to handle arbitrary conditions c, such as guiding generationtowards CLIP embeddings (Nichol et al., 2021). This flexibility in choosing different conditions is essentialto ScoreAG and enables us to adapt the model for three distinct tasks by adjusting the guidance condition,as described in the next section.",
  "Score-Based Adversarial Generation": "In this section, we introduce Score-Based Adversarial Generation (ScoreAG), a framework employing gen-erative models to evaluate robustness beyond the p-norm constraints. ScoreAG is designed to perform thefollowing three tasks: (1) the generation of adversarial images (see Sec. 3.2), (2) the transformation of exist-ing images into adversarial examples (see Sec. 3.3), and (3) the purification of images to enhance empiricalrobustness of classifiers (see Sec. 3.4). ScoreAG consists of three steps: (1) select a guidance term for the corresponding task to model the con-ditional score function xt log p(xt | c), (2) adapt the reverse-time SDE with the task-specific conditionalscore function, and (3) solve the adapted reverse-time SDE for an initial noisy image x1 N(0, I) usingnumerical methods. Depending on the task, the result is either an adversarial or a purified image. Weprovide an overview of ScoreAG in .",
  "dt": ": An overview of ScoreAG and its three steps.ScoreAG starts from noise x1 and iterativelydenoises it into an image x0.It uses the task-specific guidance terms xt log pt(c | xt) and the scorefunction xt log pt(xt) to guide the process towards the task specific condition c. The network s is used forapproximating the score function xt log pt(xt) and for the one-step Euler prediction x0. where log pt(xt) is modeled by a score-based generative model. Solving the adapted reverse-time SDE yieldsa sample of the conditional distribution p(x0 | c), i.e., an adversarial or purified image. To simplify thepresentation, we will denote class-conditional functions as py(xt) rather than the more verbose p(xt | y).",
  "Problem Statement": "In the realm of adversarial robustness, traditional evaluation methods often constrain adversarial perturba-tions within an p-norm ball, providing a limited robustness assessment. These limitations are addressed byunrestricted attacks. In this work, we consider the following three key tasks: (1) Generating new adversarialimages that inherently belong to a specific class y but are misclassified by the classifier as y; (2) Transform-ing existing images x into adversarial examples, i.e., images that are misclassified as y (see adversary) whilemaintaining their core semantics and true class y; and (3) Purifying adversarial images xADV to recovercorrect classification and enhance empirical robustness. Adversary. Let y {1, . . . , K} denote the true class of a clean image x CHW , y = y be adifferent class, and f() : CHW {1, . . . , K} a classifier. An image xADV CHW is termedan adversarial example if it is misclassified by f, i.e., f(x) = y = y = f(xADV), while preserving thesemantics, i.e., (x) = (xADV) with denoting a semantics-describing oracle.Therefore, adversarialexamples do not change the true label of the image. To enforce this, conventional adversarial attacks restrictthe perturbation to lie in a certain p-norm, avoiding large differences to the original image. In contrast,ScoreAG is not limited by p-norm restrictions but preserves the semantics by employing a class-conditionalgenerative model. In the following, we introduce each task in detail.",
  "Generative Adversarial Synthesis": "Generative Adversarial Synthesis (GAS) aims to synthesize images that are adversarial by nature. Whilethese images maintain the semantics of a certain class y, they are misclassified by a classifier into a differentclass y. The formal objective of GAS is to sample from the distribution py(x0 | f(x0) = y), where f(x0) = ycorresponds to the guidance condition c.",
  "is the expected probability of classifying generated samples x0 as class y": "While a direct Monte Carlo approximation to equation 7 is theoretically feasible, drawing samples from theclass-conditional generative model pt,y(x0 | xt) would be expensive. Instead, we approximate pt,y(x0 | xt)as a Dirac distribution centered on the one-step Euler solution x0 to equation 1 from t to 0 x0 = xt t dxt",
  "pt,y(f(x0) = y | xt) p(f(x0) = y | xt).(8)": "Thus, we approximate xt log pt,y(f(x0) = y | xt) xt log p(f(x0) = y | xt), which, in practice, corre-sponds to maximizing the cross-entropy between the classification f(x0) of the generated sample and thetarget class y. In contrast to Dhariwal & Nichol (2021), our approximation allows us to work with the classifier f directlyinstead of fine-tuning a time-dependent variant. Moreover, this can be adapted to discrete-time diffusionmodels with the approach by Kollovieh et al. (2023).",
  "Generative Adversarial Transformation": "While in GAS we synthesize adversarial samples from scratch, Generative Adversarial Transformation (GAT)focuses on transforming existing images into adversarial examples. For a given image x and its correspondingtrue class label y, the objective is to sample a perturbed image misclassified as y while preserving the coresemantics of x. We denote the resulting distribution as py(x0 | f(x0) = y, x) for the guidance conditionc = {x, f(x0) = y} leading to the following conditional score (equation 5):",
  "xt log pt,y(xt | x, f(x0) = y) = xt log pt,y(xt) + xt log pt,y(x,f(x0) = y | xt).(9)": "By assuming independence between x and y given xt, we split the guidance term into sx xt log pt,y(x |xt) + sy xt log pt,y(f(x0) = y | xt), implying that y should not influence the core semantics of the givenimage. Note that we introduced the two scaling parameters sx and sy to control the possible deviationfrom the original image and the strength of the attack, respectively.While we treat the score functionxt log pt,y(xt) and the guidance term xt log pt,y(f(x0) = y | xt) as in the GAS setup, we model thedistribution pt,y(x | xt) as a Gaussian centered at the one-step Euler prediction x0 (equation 8),",
  "pt,y(x | xt) = N(x0, I).(10)": "It follows that our sampling process searches for an adversarial example while minimizing the squared errorbetween x and x0. Importantly, this lets us generate samples x0 close to x without imposing specific p-norm constraints. Furthermore, our framework is not limited to the squared error, but can also utilize otherdifferentiable similarity metrics as guidance such as the LPIPS (Zhang et al., 2018) score. Note that, whileadversarial examples generated by ScoreAG are unrestricted in the sense of p-balls, they are constrained tothe data manifold of the generative model through the construction of our generative process. This yieldsan unrestricted attack that preserves the core semantics using the class-conditional score network s. As a result, GAT provides a more comprehensive robustness assessment than traditional p-threat models.This enhanced assessment capability stems from the inherent properties of GAT, which (1) encompasses allsemantics-preserving adversarial examples within the p-balls as captured by the generative model, and (2)includes semantics-preserving adversarial examples that conventional p-threat models do not capture.",
  "Generative Adversarial Purification": "Generative Adversarial Purification (GAP) extends ScoreAG to counter adversarial attacks. It is designed topurify adversarial images, i.e., remove adversarial perturbations through its generative capability to enhancethe robustness of machine learning models. Given an adversarial image xADV that was perturbed to induce a misclassification, GAP aims to sample animage from the data distribution that resembles the semantics of xADV, which we denote as p(x0 | xADV)with xADV corresponding to the guidance condition c. We model its score function analogously to equation 9:",
  "xt log pt(xt | xADV) = xt log pt(xt) + sx xt log pt(xADV | xt),(11)": "where sx is a scaling parameter controlling the deviation from the input.Note that we omit y sincethere is no known ground-truth class label. As previously, we utilize a time-dependent score network s toapproximate the term xt log pt(xt). The term pt(xADV | xt) is modeled according to equation 10, as beforeassuming it follows a Gaussian distribution with a mean of the one-step Euler prediction x0. Note thatScoreAG, just as other purification methods, cannot detect adversarial images. Therefore, it also needs topreserve image semantics if there is no perturbation.",
  "Experimental Evaluation": "The primary objective of our experimental evaluation is to assess the capability of ScoreAG in generatingand purifying adversarial examples. More specifically, we investigate the following properties of ScoreAG:(1) the ability to synthesize adversarial examples from scratch (GAS), (2) the ability to transform existingimages into adversarial examples (GAT), and (3) the enhancement of classifier robustness by leveraging thegenerative capability of the model to purify images (GAP). This evaluation aims to provide comprehensiveinsights into the strengths and limitations of ScoreAG in the realm of adversarial example generation andclassifier robustness. Baselines. In our evaluation, we benchmark our adversarial attacks against a wide range of establishedmethods covering various threat models. Specifically, we consider the fast gradient sign-based approachesFGSM (Goodfellow et al., 2014), DI-FGSM (Xie et al., 2019), and SI-NI-FGSM (Lin et al., 2019). In addition,we include Projected Gradient Descent-based techniques, specifically Adaptive Projected Gradient Descent(APGD) and its targeted variant (APGDT) (Croce & Hein, 2020b). For a comprehensive assessment, wealso examine single pixel, black-box, and minimal perturbation methods, represented by OnePixel (Su et al.,2019), Square (Andriushchenko et al., 2020) and Fast Adaptive Boundary (FAB) (Croce & Hein, 2020a),respectively. Finally, we compare to the unrestricted attacks Composite Adversarial Attack (CAA) (Hsi-ung et al., 2023), PerceptualPGDAttack (PPGD), FastLagrangePerceptualAttack (LPA) (Laidlaw et al.,2020), and DiffAttack (Chen et al., 2023a), which is based on latent diffusion. Furthermore, we compare toAdversarial Content Attack (ACA) (Chen et al., 2023c) in App. B.3.1 To evaluate the efficacy of ScoreAG in purifying adversarial examples, we conduct several experiments in apreprocessor-blackbox setting. For the evaluation, we employ the targeted APGDT and untargeted APGDattacks (Croce & Hein, 2020b) and ScoreAG in the GAS setup.Our experiments also incorporate thepurifying methods ADP (Yoon et al., 2021) and DiffPure (Nie et al., 2022). Additionally, we compare withstate-of-the-art adversarial training techniques that partially utilize supplementary data from generativemodels (Cui et al., 2023; Wang et al., 2023; Peng et al., 2023). Experimental Setup. We employ three benchmark datasets for our experiments: CIFAR10, CIFAR100(Krizhevsky et al., 2009), and TinyImagenet. We utilize pre-trained Elucidating Diffusion Models (EDM)in the variance preserving (VP) setup (Karras et al., 2022; Wang et al., 2023) for image generation. As ourclassifier, we opt for the well-established WideResNet architecture WRN-28-10 (Zagoruyko & Komodakis,2016). The classifiers are trained for 400 epochs using SGD with Nesterov momentum of 0.9 and weightdecay of 5104. Additionally, we incorporate a cyclic learning rate scheduler with cosine annealing (Smith",
  "(a) FID (GAS).(b) Robust Accuracy (GAS).(c) Robust Accuracy (GAP)": ": FID (a) and accuracy (b) for increasing sy scales in the synthesis (GAS) setup, and robust accuracy(c) for increasing sx scales in the purification (GAP) setup under APGD attack. Classifier: WRN-28-10.The shaded area shows the 95% CI over four seeds. & Topin, 2019) with an initial learning rate of 0.2.To further stabilize the training process, we applyexponential moving average with a decay rate of 0.995. Each classifier is trained four times to ensure thereproducibility of our results, and we report standard deviations with (). For pretrained classifiers withonly one available model, we do not report standard deviations. For the restricted methods, we considerthe common norms in the literature 2 = 0.5 for CIFAR10 and CIFAR100, 2 = 2.5 for TinyImagenet, and = 8/255 for all three datasets. For DiffAttack, ACA, and DiffPure we take the implementation of theofficial repositories, while we use Torchattacks (Kim, 2020) for the remaining baselines. The runtimes for allmethods are shown in Tab. 11 in the appendix. Evaluation Metrics. To evaluate our results, we compute the robust accuracy, i.e., the accuracy after anattack. Furthermore, we use the clean accuracy, i.e., the accuracy of a (robust) model without any attack.For the GAS task, we use the FID (Heusel et al., 2017) to assess the similarity between the distributionof synthetic images and the test set, providing a distribution-level measure.Since FID is not suitablefor instance-based evaluation, we use the LPIPS score (Zhang et al., 2018) for the GAT task to measureperceptual similarity at the instance level.",
  "Quantitative Results": "Evaluating Generative Adversarial Synthesis. As explained in Sec. 3.2, ScoreAG is capable of synthe-sizing adversarial examples. (a) and (b) show the accuracy and the FID of a WRN-28-10 classifieras sy increases, respectively. Notably, the classifier yields nearly identical performance as on real data whensy = 0. However, even a minor increase of sy to 0.125 results in a substantial reduction in accuracy whilemaintaining a low FID. Setting sy to 1.0 causes the classifiers performance to drop below random guessinglevels for the CIFAR10 dataset. Additionally, (a) presents sample images generated at various scales.Notably, increasing sy leads to subtle modifications in the images. Rather than introducing random noise,these changes maintain image coherence up to a scale of sy = 0.5. Beyond this point, specifically at sy = 1.0,there is a noticeable decline in image quality, as reflected by the FID. Since our approach leverages a generative model, it enables the synthesis of an unlimited number of ad-versarial examples, thereby providing a more comprehensive robustness assessment. Moreover, in scenariosrequiring the generation of adversarial examples, our method allows for rejection sampling at low sy scales,ensuring the preservation of image quality. This is particularly important for adversarial training, wheresynthetic images can enhance robustness (Wang et al., 2023). Evaluating Generative Adversarial Transformation. Beyond the synthesis of new adversarial exam-ples, our framework allows converting pre-existing images into adversarial ones as described in Sec. 3.3. Weshow the accuracies and LPIPS scores of various attacks in Tab. 1. Notably, ScoreAG consistently achieves0% accuracy, lower than the 2 and 0 restricted methods across all three datasets, making it competitive toAPGDT and LPA. This demonstrates ScoreAGs capability of generating adversarial examples. Surprisingly,the other unrestricted diffusion-based method, DiffAttack, yields considerably lower attack success rates. Weattribute this discrepancy to the fact that it only leverages the last few iterations of the denoising diffusionprocess.Finally, we observe that the LPIPS scores of ScoreAG are comparable to the restricted meth-",
  "Originalsx = 32sx = 48sx = 64sx = 96(b) Transform (GAT)": ": Examples on the CIFAR10 dataset. (a) shows the synthesis (GAS) setup and generatesimages of the classes horse\", truck\", and deer\", which are classified as automobile\", ship\", and horse\",respectively, as sy increases. (b) shows the transformation (GAT) setup and transforms images ofthe classes ship\", horse\", and dog\", into adversarial examples classified as ship\", deer\", and cat\". Forsx = 32, the images are outside of common perturbation norms, i.e., 2 = 0.5 and = 8/255, but preserveimage semantics. We show examples of selected baselines in .",
  "Unrestricted": "CAA (Hsiung et al., 2023)25.104.0536.755.1633.7510.2043.167.43PPGD (Laidlaw et al., 2020)50.9329.7643.8832.5210.9523.3333.1433.53LPA (Laidlaw et al., 2020)0.000.000.000.000.020.300.010.00ScoreAG (Ours)0.000.000.000.000.020.160.000.00 Additionally, we evaluate ScoreAG on the high-resolution ImageNet-Compatible3 dataset, a commonly usedsubset of ImageNet. We selected two robust classifiers as most attacks achieved 0% accuracy on standardclassifiers. More specifically, we selected the RaWideResNet-101-2 by Peng et al. (2023) and WideResNet-50-2 by Salman et al. (2020). We show the results for ScoreAG and selected baselines, including AdversarialContent Attack (Chen et al., 2023c) (ACA), in Tab. 9. The restricted baselines have a perturbation distanceof p = 4/255. As we can observe, ScoreAG again achieves competitive performance, i.e., best and second-",
  "ADP (Yoon et al., 2021)93.09--85.45--WRN-28-10DiffPure (Nie et al., 2022)89.0287.7288.4688.3088.1888.57WRN-28-10ScoreAG-GAP (Ours)93.930.1291.340.4692.131.4190.250.4490.890.4090.740.67WRN-28-10": "adversarial examples. Therefore, the purification needs to be applied to all images. However, ScoreAG stillachieves a high clean accuracy. We demonstrate its applicability to common corruptions in App. B.4. Hyperparameter study. We explore the impact of the scale parameters sy and sx on accuracy and FID, asdepicted in . In (c), we examine the efficacy of purification against adversarial attacks of APGDunder both 2 and norms across different sx scales. At sx = 0, the generated images are unconditionalwithout guidance and independent of the input. Therefore, the robust accuracy equals random guessing. Assx increases, the accuracy improves, reaching a performance plateau at approximately sx = 10. Increasing sxfurther reduces the accuracy as the sampled images start to resemble adversarial perturbations. In practice,we scale s by t1.",
  "sx = 480.100.170.510.50sx = 640.110.210.440.40sx = 960.130.340.350.30": "Finally, Tab. 3 shows the robust accuracy and median2 distances across different scale configurations for theCIFAR10 and CIFAR100 datasets. We can observe thatan increase in sy leads to reduced classifier accuracy forCIFAR10, improving the efficacy of the adversarial at-tacks. A rise in sx, however, increases the accuracy asthe generated image closer resembles the original. Themedian 2 distance exhibits a similar behavior. While alower sy yields no difference for both datasets, increas-ing sx decreases the median distances for CIFAR10 andCIFAR100. In (b), we show examples across vari-ous sx scales on the CIFAR10 dataset. Notably, all scalespreserve the image semantics and do not display any ob-servable differences. In practice, we iteratively increasethe scale sy if the attack is not successful.",
  "Qualitative Analysis": "To investigate the quality of the adversarial attacks, we deploy ScoreAG on the ImageNet dataset (Denget al., 2009) with a resolution of 256256. We use the latent diffusion model DiT proposed by Peebles & Xie(2022), along with a pre-trained latent classifier from (Kim et al., 2022). The images are sampled using thedenoising procedure by Kollovieh et al. (2023) as explained in Sec. 3.2. Note that as the generative processis performed in the latent space, the model has more freedom in terms of reconstruction. We show an example image of a tiger shark in with corresponding adversarial attacks. While theclassifier correctly identifies the tiger shark in the baseline image, it fails to do so in the generated adversarialexamples.Notably, the p-bounded methods display noticeable noisy fragments.In contrast, ScoreAGproduces clean adversarial examples, altering only minor details while retaining the core semantics mostnotably, the removal of a small fish which prove to be important classification cues. We provide furtherexamples for GAS in Sec. B.6 and for GAT in Sec. B.7. The synthetic images display a high degree ofrealism, and the transformed images show visible differences while preserving the semantics of the originalimage.",
  "Human Study": "To evaluate whether ScoreAG generates semantics-preserving adversarial examples, we perform a humanstudy on adversarially modified (real) as well as synthetically generated images. For the study, we chooseCIFAR10 images as it (1) avoids any class-selection bias, whereas high-resolution datasets usually containmany classes only distinguishable by human experts; and (2) is the most commonly used dataset in relatedwork. Hyperparameters are set to produce an interesting regime, where the generated adversarial imagesare significantly outside common p-norm balls and constitute strong attacks for the classifier in question.In particular, we randomly sample five images from each class to generate 50 adversarial examples usingsx = 16 and sy = 48. These adversarial examples have an average 2-norm difference to their clean coun-terparts of 0.68 0.24, exceeding the common 2-norm ball constraint of 0.5 (Croce et al., 2020) by onaverage 36%. For the synthetic examples, we generate 50 images without (sy = 0) and 50 images withguidance (sy = 0.125), again in a class-balanced fashion. For the adversarial guided synthetic examples, weemploy rejection sampling to only consider images that lead to misclassification by the classifier. To ensurehigh data quality for the study, we used the Prolific platform (Eyal et al., 2021) to employ 60 randomlychosen human evaluators to label the 200 images. To avoid bias, we presented the adversarial examples (syn-thetic or modified) before the unperturbed examples and introduced the category \"Other / I dont know\".",
  "Real2%94%Synthetic0%70%": "We compute human accuracy by choosing the majority voteclass of all 60 human evaluators and compare it with theground truth class. We show the results of the human studyin Tab. 4. Notably, humans can still accurately classify 94%of the adversarial modified images despite significantly larger2 distances, establishing almost perfect semantic preserva-tion for GAT. For GAS, humans classify 70% of the (suc-cessful) synthetic adversarial images correctly. This is lowerthan for adversarial modification and shows that the genera-tion of completely synthetic semantics-preserving adversarialexamples is a harder task than adversarial modification. Still, GAS achieves good semantic preservation,significantly outperforming random guessing (10%). We believe it is critical that semantic preservation ofunrestricted attacks is evaluated through human studies as done in some early works (Song et al., 2018;Khoshpasand & Ghorbani, 2020). As this is missing in all related unrestricted attack works used as base-lines in this work, we hope to contribute to establishing this as an evaluation standard, and that our resultscan serve as interesting baselines for future works.",
  "Related Work": "Diffusion Models. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) and score-based gen-erative models (Song et al., 2020) received significant attention in recent years, owing to their remarkableperformance across various domains (Kong et al., 2020; Lienen et al., 2023; Kollovieh et al., 2023) and havesince emerged as the go-to methodology for many generative tasks. Dhariwal & Nichol (2021) proposeddiffusion guidance to perform conditional sampling using unconditional models. A recent study has shownthat classifiers can enhance their robust accuracy when training on images generated by diffusion models(Wang et al., 2023), demonstrating the usefulness and potential of diffusion models in the robustness domain. Adversarial Attacks. An important line of work are white-box approaches, which have full access to themodel parameters and gradients, such as the fast gradient sign method (FGSM) introduced by Goodfellowet al. (2014). While FGSM and its subsequent extensions (Xie et al., 2019; Dong et al., 2018; Lin et al.,2019; Wang, 2021) primarily focus on perturbations constrained by the norm, other white-box techniquesemploy projected gradient descent and explore a broader range of perturbation norms (Madry et al., 2017;Zhang et al., 2019). In contrast, black-box attacks are closer to real-world scenarios and do not have access tomodel parameters or gradients (Narodytska & Kasiviswanathan, 2016; Brendel et al., 2017; Andriushchenkoet al., 2020). As ScoreAG-GAT and ScoreAG-GAS rely on the gradients of the classifier to compute guidancescores, they are categorized as white-box attacks.",
  "Discussion": "Limitations and Future Work. Our work demonstrates the potential and capabilities of score-basedgenerative models in the realm of adversarial attacks and robustness. While ScoreAG is able to generate andpurify adversarial attacks, some drawbacks remain. Primarily, the evaluation of unrestricted attacks remainschallenging. We resolve this limitation by performing a human study and argue that this should becomestandard. Moreover, the proposed purification approach is only applicable to a preprocessor-blackbox setting,as computing the gradients of the generative process efficiently is an open problem. Conclusion. In this work, we address the question of how to generate unrestricted adversarial examples.We introduce ScoreAG, a novel framework that bridges the gap between adversarial attacks and score-based generative models.Utilizing diffusion guidance and pre-trained models, ScoreAG can synthesizenew adversarial attacks, transform existing images into adversarial examples, and purify images, therebyenhancing the empirical robust accuracy of classifiers. Our results indicate that ScoreAG can effectivelygenerate semantics-preserving adversarial images beyond the limitations of the p-norms. Our experimentalevaluation demonstrates that ScoreAG matches the performance of existing state-of-the-art attacks anddefenses. We see unrestricted adversarial examples - as generated by our work - as vital to achieve a holisticview of robustness and complementary to hand-picked common corruptions (Kar et al., 2022) or classical pthreat models. Broader ImpactThis work contributes to the domain of robustness, focusing on unrestricted adversarialattacks. Our framework, ScoreAG, is designed for the generation and purification of adversarial images.While there exists the potential for malicious misuse, we hope for our insights to enhance the understanding ofmachine learning models robustness. Moreover, despite the competitive empirical performance of ScoreAG,we advise against relying solely on the algorithm. This paper has been supported by the Munich Center for Machine Learning and by the DAAD programKonrad Zuse Schools of Excellence in Artificial Intelligence, sponsored by the German Federal Ministry ofEducation and Research, and the German Research Foundation, grant GU 1409/4-1.",
  "Peer Eyal, Rothschild David, Gordon Andrew, Evernden Zak, and Damer Ekaterina. Data quality of plat-forms and panels for online behavioral research. Behavior Research Methods, pp. 120, 2021": "Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification.In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16251634, 2018. doi:10.1109/CVPR.2018.00175. Ivan Fursov, Matvey Morozov, Nina Kaploukhaya, Elizaveta Kovtun, Rodrigo Rivera-Castro, Gleb Gusev,Dmitry Babaev, Ivan Kireev, Alexey Zaytsev, and Evgeny Burnaev. Adversarial attacks on deep modelsfor financial transaction records. In Proceedings of the 27th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining, pp. 28682878, 2021.",
  "William Peebles and Saining Xie.Scalable diffusion models with transformers.arXiv preprintarXiv:2212.09748, 2022": "ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, JasonMartin, and Duen Horng Chau. Robust principles: Architectural design principles for adversarially robustcnns. arXiv preprint arXiv:2308.16258, 2023. Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversariallyrobust imagenet models transfer better? Advances in Neural Information Processing Systems, 33:35333545, 2020.",
  "Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adver-sarial attacks using generative models. arXiv preprint arXiv:1805.06605, 2018": "Leslie N Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using largelearning rates. In Artificial intelligence and machine learning for multi-domain operations applications,volume 11006, pp. 369386. SPIE, 2019. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265.PMLR, 2015. Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman.Pixeldefend: Lever-aging generative models to understand and defend against adversarial examples.arXiv preprintarXiv:1710.10766, 2017.",
  "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016": "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-retically principled trade-off between robustness and accuracy. In International conference on machinelearning, pp. 74727482. PMLR, 2019. Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectivenessof deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 586595, 2018.",
  "A.1Reproducibility": "Our models are implemented using PyTorch with the pre-trained EDM models by Karras et al. (2022) andWang et al. (2023), and the guidance scores are computed using automatic differentiation. In Tab. 5 andTab. 6, we give an overview of the hyperparameters of ScoreAG. For the methods DiffAttack, DiffPure,CAA, PPGD, and LPA, we use the corresponding authors official implementations with the suggestedhyperparameters. For the remaining attacks, we use Adversarial-Attacks-PyTorch with its default parameters(Kim, 2020).",
  "A.2Hyperparameters": "To train the WRN-28-10 classifiers, we use the parameters shown in Tab. 5. In Tab. 6, we show the scaleparameters used to evaluate the attacks and purification of ScoreAG, i.e., the results shown in Tab. 1, Tab. 2and Tab. 8. The attacks on robust models do not sequentially increase the scale sy but use fixed scales ofsx = 48 and sy = 80. For the common corruptions we use a scale of sx = 40 on the robust models. Finally,for the EDM sampler we use the default sampling scheduler and parameters by Karras et al. (2022).",
  "Original ScoreAGFABSquareCAADiffAttack": ": Examples from the CIFAR10 dataset. The figure presents selected baseline images correspondingto the examples in (b). For ScoreAG-GAT, we used sx = 48. As baselines, we included FAB (2 = 0.5)and Square ( = 8/255) to represent restricted attacks, as they achieve the lowest and highest LPIPSscores, respectively. Additionally, we show the two unrestricted baselines, CAA and DiffAttack.",
  "B.2Qualitative Effect of the Scale Parameters": "To provide a more intuitive understanding of ScoreAG, we show the visual effect of the scale parameters sxand sy in and 7. These visualizations illustrate the effects of the scale parameters sx and sy. Whenboth scale parameters are set to zero, the model behaves as a standard diffusion model. Increasing sx guidesthe diffusion process toward a specific image, which is used in the GAP setup. Increasing sy introducesadversarial perturbations, allowing the synthesis of adversarial images. When both parameters are greaterthan zero, the GAT model transforms existing images into adversarial examples.",
  "(f) Original": ": Effect of the scale parameter sx. The images display adversarial images generated by ScoreAG-GAT across different scales sx on a robust WRN-50-2 (Salman et al., 2020) with sy = 8. For sx = 0, thesetup equals the GAS setup and synthesizes an image unrelated to the input. As the scale increases, theimage gets closer to the original.",
  "B.3Additional classifiers for adversarial attacks using GAT": "To verify the efficacy of ScoreAG and demonstrate its applicability across various architectures, we evaluatethe accuracy of GAT on four more pretrained classifiers via PyTorch Hub2 for the datasets CIFAR10 andCIFAR100 using the same hyperparameters, i.e., scale parameters, as for the WRN-28-10 classifier. We showthe adversarial accuracy in Tab. 8, including selected baselines. As we can observe, ScoreAG successfullygenerates adversarial attacks on various classifiers, reaching accuracies close to 0%. This demonstrates theflexibility of ScoreAG and applicability to arbitrary pre-trained classifiers.",
  "(f) sy = 48": ": Effect of the scale parameter sy. The images display adversarial images generated by ScoreAG-GAT across different scales sy on a robust WRN-50-2 (Salman et al., 2020) with sx = 0.25. For sy = 0,the setup equals the GAP setup and synthesizes an image without adversarial perturbations. As the scaleincreases, the adversarial content strengthens, causing the images to diverge further from the original.",
  "B.4Purification of Common Corruptions": "In addition to the purification of adversarial attacks, we test the applicability of ScoreAG (GAP) on commoncorruptions (Hendrycks & Dietterich, 2019). We show the robust accuracy of standard and robust classifiersbefore and after purification using DiffPure and GAP in Tab. 10.Our results show that ScoreAG consistentlyincreases the robust accuracy over the base model. In 5/7 settings, ScoreAG achieves a better accuracy than",
  "B.7Generative Adversarial Transformation": "In , we show additional examples of the GAT task. All original images are classified correctly into theImageNet classes golden retriever\", spider monkey\", football helmet\", jack-o-lantern\", pickup truck\",and broccoli\", while the adversarial images are classified as cocker spaniel\", gibbon\", crash helmet\", bar-rel\", convertible\", and custard apple\", respectively. While all adversarial images display subtle differencesthey do not alter the core semantics of the images and are not captured by common p-norms.",
  "B.8Runtime comparison of the attacks": "All experiments were conducted on A100s. In Tab. 11, we report the runtimes in seconds of various methods.The numbers display the average time to generate one adversarial example on the ImageNet-Compatibledataset. Note that sampling an image without guidance using the same generative model as ScoreAG takes15.00 seconds. The difference stems from the additional overhead induced by the gradient computations."
}