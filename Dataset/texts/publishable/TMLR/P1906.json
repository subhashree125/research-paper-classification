{
  "Abstract": "Inverse reinforcement learning (IRL) methods infer an agents reward function using demon-strations of expert behavior. A Bayesian IRL approach models a distribution over candidatereward functions, capturing a degree of uncertainty in the inferred reward function. This iscritical in some applications, such as those involving clinical data. Typically, Bayesian IRLalgorithms require large demonstration datasets, which may not be available in practice.In this work, we incorporate existing domain-specic data to achieve better posterior con-centration rates. We study a common setting in clinical and biological applications wherewe have access to expert demonstrations and known reward functions for a set of trainingtasks.Our aim is to learn the reward function of a new test task given limited expert demonstrations. Existing Bayesian IRL methods impose restrictions on the form of inputdata, thus limiting the incorporation of training task data. To better leverage informationfrom training tasks, we introduce kernel density Bayesian inverse reinforcement learning(KD-BIRL). Our approach employs a conditional kernel density estimator, which uses theknown reward functions of the training tasks to improve the likelihood estimation across arange of reward functions and demonstration samples. Our empirical results highlight KD-BIRLs faster concentration rate in comparison to baselines, particularly in low test taskexpert demonstration data regimes. Additionally, we are the rst to provide theoreticalguarantees of posterior concentration for a Bayesian IRL algorithm. Taken together, thiswork introduces a principled and theoretically grounded framework that enables BayesianIRL to be applied across a variety of domains.",
  "Introduction": "Inverse reinforcement learning (IRL) is a strategy within the domain of reinforcement learning (RL) thatinfers an agents reward function from demonstrations of the agents behavior. The learned reward func-tion helps identify the objectives driving the agents behavior. IRL methods have been successfully appliedacross several domains, including robotics (Ratli et al., 2006a; Kolter et al., 2008), swarm animal move-ment (Ashwood et al., 2020), and ICU treatment (Yu et al., 2019a). The earliest IRL algorithms learned pointestimates of the reward function (Ng & Russell, 2000; Abbeel & Ng, 2004), and were applied to problems inpath planning (Mombaur et al., 2010) and urban navigation (Ziebart et al., 2009). Despite the success of early IRL approaches, there are limitations to inferring a point estimate of the rewardfunction. First, the reward function is frequently non-identiable (Abbeel & Ng, 2004; Ratli et al., 2006b;Ziebart et al., 2009), meaning there are multiple reward functions that explain the expert demonstrationsequally well.Second, for nite demonstration data, the point estimate of the reward function fails to capture the noise in the data-generating process, and does not model uncertainty in the resulting rewardfunction estimate. A Bayesian approach learns a posterior distribution over reward functions after observingall samples. This distribution places mass on reward functions proportional to how well they explain theobserved behavior, using the information from the samples (Ramachandran & Amir, 2007; Michini & How,2012b; Choi & Kim, 2012; Balakrishnan et al., 2020; Chan & van der Schaar, 2021; Michini & How, 2012a),better highlights possible alternative explanations of agent objectives. Despite their ability to produce a set of candidate reward functions, Bayesian IRL algorithms typicallyrequire large amounts of high-quality expert demonstration data to ensure posterior contraction (Brownet al., 2020). A Bayesian posterior changes depending on the samples that are observed. Contraction isthe phenomenon of the posterior concentrating at the true value as more samples are observed. However, insome applications, such as those involving clinical or biological data, expert demonstrations are expensiveto obtain.For example, in a clinical application, collecting expert demonstrations may involve parsing electronic health records (EHRs) (Johnson et al., 2016), which contain information associated with patienttrajectories as well as clinical notes (Johnson et al., 2023). However, each clinician treats a limited numberof patients, so there are few patient trajectories.In a biological application, we might be interested in studying the factors that drive the hunting behavior of T-cells, which aects how fast they kill tumorcells. For example, certain conditions may induce pack hunting in which a T-cell works with other T-cells to kill tumor cells. One common approach to gather information about how cells move under dierentconditions is genetic perturbation studies (Feldman et al., 2022), which examine the cellular behavior changesfollowing an induced genetic perturbation. Previous work has studied how some genetic perturbations, suchas RASA2 (Carnevale et al., 2022) and CUL5 knockout Liao et al. (2024), aect the eciency of T cellskilling tumor cells. However, each study is expensive to run, and thus, a limited number of trajectories areavailable. In this work, we use supplementary data sources to improve the posterior contraction rate forBayesian IRL algorithms in applications with few expert demonstration trajectories. We assume that we have several training tasks and a single test task. We have access to expert demonstrationsas well as the associated reward function for each training task. This assumption is reasonable in manyapplication areas. Referring to the examples above, in clinical applications, the reward function representsthe clinical treatment objective. These treatment objectives can be obtained from EHR clinical notes, whichcontain clinician-transcribed goals of treatment. In the biophysical movement example, the reward functionrepresents how various factors, such as the density of the tumor cells and the proliferation rate of T-cells,induce a certain type of hunting behavior. In both examples, each training task is associated with a knownreward function. Our goal is to infer the reward function for the test task, i.e., a patient trajectories treatedby a new clinician or cell behavior trajectories from a new genetic perturbation, with a small set of expertdemonstrations. Integrating supplementary data poses challenges within the current landscape of Bayesian IRL algorithms.Existing methods for single-task IRL (Ramachandran & Amir, 2007; Michini & How, 2012b) are restrictedto a framework where the set of expert demonstrations arises from a single task. Alternative formulations,such as meta-IRL methods, can accommodate demonstrations from several distinct tasks. However, meta-IRL methods cannot incorporate information about the known reward functions from the training tasks.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": We study KD-BIRLs performance in a 5 5 Gridworld environment in which the reward ispositive in one state (upper right corner). The rst panel shows the data generating reward function R, thesecond panel shows the KD-BIRL posterior mean, and the third and fourth panels show the state occupancydistribution of the training task and expert task demonstrations respectively. The KD-BIRL posterior meanis within the equivalence class of R, because the darkest square is in the upper right hand corner, but thereare incorrect estimates of reward in other states.",
  "Related Work": "Single-task Bayesian IRL. Single-task Bayesian IRL methods receive as input a set of demonstration datathat originates from an expert policy optimizing for an unknown reward function. There have been manyfollow-up methods which improve upon the original Bayesian IRL algorithm (Ramachandran & Amir, 2007).To perform inference for more complex reward function structures, several approaches learn nonparametricreward functions, which use strategies such as Gaussian processes (Levine et al., 2011; Qiao & Beling, 2011)and Indian buet process (IBP) priors (Choi & Kim, 2013).oi et al. (2018) decompose a complex global IRL task into simpler sub-tasks. Other approaches reduce computational complexity by either usinginformative priors (Rothkopf & Ballard, 2013), dierent sampling procedures (e.g., Metropolis-Hastings (Choi& Kim, 2012), expectation maximization (Zheng et al., 2014)), variational inference (Chan & van der Schaar,2021)), or learning several reward functions that each describe a subset of the state space (Michini & How,2012b;a). In contrast, we assume access to demonstration data from experts optimizing for multiple tasks,and use the known reward functions of these tasks to facilitate reward function inference for a new task.",
  "Single-task Bayesian IRL": "IRL methods infer an agents reward function given expert demonstrations of its behavior. We can modelthe agents behavior using a Markov decision process (MDP). The MDP is dened by (S, A, P, ), where Sis the state space; A is a discrete set of actions; P(st+1|st, at) denes state transition probabilities from timet to t + 1; and is a constant discount factor.",
  "i=1, and each sample is a": "2-tuple that species the expert agents state and chosen action. The demonstration samples arise from anagent following the stationary policy : S A, which optimizes for a xed but unknown ground truthreward function R : S A R, where R R and R is the space of reward functions. Given these expert demonstrations, Bayesian IRL algorithms treat R as inherently random and aim tolearn a posterior distribution over R that can rationalize the expert demonstrations. By specifying a priordistribution over the reward space R, p(R), and a likelihood function for the observed data, rn",
  "j=1, where each state-action tuple (sj, aj) is a demonstration of": "an expert optimizing for the reward function Rj. We note that there will be several state-action pairs thatcorrespond to the same reward function since there are several samples from each training task; therefore,Rj is not unique. Our goal is to learn the unknown reward function R of a new test task given a limitedamount of expert demonstrations of the new test task, {(se",
  "Conditional kernel density estimation": "In this work, we propose approximating the likelihood in Equation 1 using conditional density estimation(CDE). In .1, we show that CDE allows us to incorporate information about both the rewardfunctions and demonstrations associated with training tasks. Given a set of empirical observations, CDEaims to capture the relationship between the responsive variable Y and the covariates X by modeling theprobability density function,",
  "p(x)": "With this perspective, any appropriate conditional density estimator can be used to approximate the like-lihood; examples include the conditional kernel density estimator (CKDE) (van der Vaart, 2000), Gaussianprocess models (Riihimki & Vehtari, 2012), or diusion models (Cai & Lee, 2023). In our work, we chooseCKDE because it is nonparametric, which allows us to model complex reward structures.Additionally,",
  "CKDE has a closed form and is straightforward to implement (Holmes et al., 2007; Izbicki & Lee, 2016)": "Specically, the CKDE estimates the conditional density p(y|x) by approximating the joint distributionp(x, y) and marginal distribution p(x) separately via kernel density estimation (KDE). Intuitively, KDEestimates the probability density function of a random variable by considering how close each observed datapoint is to a given location. The likelihood at that location is higher if there are more observations nearby,with closer observations contributing more to the estimate. Given pairs of vector observations {(xj, yj)}m",
  "(3)": "where K and K are kernel functions with bandwidths h, h > 0 respectively, and dx, dy are distance functionsthat measure the similarities between x, xj, y, yj, respectively. There are many choices for suitable kernels(e.g. uniform, normal, triangular) and distance metrics (e.g. Euclidean distance, cosine similarity, Manhattandistance) (Cawley & Talbot, 2010), which should be chosen depending on the application domain.To",
  "Methods": "There are several concerns with using Equation (2) as the likelihood function to learn the posterior dis-tribution of the reward function of the test task. First, Equation (2) uses the optimal Q-value function,which is dicult to learn given a limited amount of demonstration data from the test task. As a result, alarge demonstration dataset is required for posterior contraction. Furthermore, current literature often uses",
  "Kernel density Bayesian IRL": "To avoid the issues with prior approaches, we propose kernel density Bayesian inverse reinforcement learn-ing (KD-BIRL), which uses CKDE to approximate the likelihood.CKDE calculates the likelihood of a reward function sample by weighting the density estimate based on the distance to observations from thetraining data. The CKDE allows us to approximate the Bayesian IRL likelihood using the training dataset{(sj, aj, Rj)}m",
  ".(7)": "There are many possible options for the kernel K, including uniform and triangular kernels. In KD-BIRL,we use a Gaussian kernel because it can approximate bounded and continuous functions well.Several prior distributions can be appropriate for the prior distribution, p(R) depending on the characteristics ofthe MDP, including Gaussian (Qiao & Beling, 2011), Beta (Ramachandran & Amir, 2007), or Chineserestaurant process (CRP) (Michini & How, 2012a). To dene distance metrics on a vector space, one canchoose cosine similarity, dot product similarity, Manhattan distance, etc. In our work, we choose Euclideandistance for ds, dr and a uniform or Gaussian prior depending on the application domain. We choose thebandwidth hyperparameters h, h using rule-of-thumb procedures (Silverman, 1986). These procedures denethe optimal bandwidth hyperparameters as the variance of the pairwise distance between the training datademonstrations and the training data reward functions respectively. Intuitively, one wants to choose thesmallest bandwidth hyperparameter the data will allow, to avoid density estimates that are too smooth. There are many techniques to sample from the posterior distribution (Equation 6), such as MCMC sam-pling Van Ravenzwaaij et al. (2018) and variational inference Blei et al. (2017). We use a Hamiltonian MonteCarlo algorithm (Team, 2011) (details in Appendix F, and Algorithm 1) which is suited to large parameterspaces. However, note that KD-BIRL introduces a new likelihood estimation approach, and that any pos-terior sampling algorithm (i.e. Markov Chain Monte Carlo (MCMC), Metropolis Hastings, Gibbs Samplingetc.) can be used. A key computational gain of our approach over the original Bayesian IRL algorithm(also a sampling-based approach) is that we avoid the cost of re-estimating the Q-value function with everyiteration of sampling. This is possible because our posterior distribution (Equation 6) does not depend onQ. Thus, sampling from the posterior is less computationally intensive than sampling from the standardBayesian IRL algorithm posterior.",
  "Feature-based reward function": "Now, we investigate a feature-based reward function, which enables KD-BIRL to perform posterior inferencein environments with a high-dimensional state space. To build intuition for this approach, we rst investigatea 10 10 Gridworld environment, where the state space S is the series of vectors of length 100. Here, weselect (s, a) = [x, y] to be a function that maps the state vector to the spatial coordinates of the agent. Thatis, we treat the coordinates of the agent as a known feature vector. In this setting, the reward functionis R = wT (s, a) where w is a low-dimensional weight vector that we aim to learn. Similar to priordomains, KD-BIRL receives training task samples from two other reward functions each parameterized bycorresponding weight vectors. Our results indicate that a feature-based reward function allows KD-BIRL to perform reward inference ina 10 10 Gridworld under two dierent reward functions. First, we note that the posterior mean, whenprojected onto the Gridworld, is nearly identical to the ground truth weight vector w projected onto theGridworld. This indicates that the learned posterior is concentrated close to w, which is ideal. Next, wevisualize the learned posterior parameters for both weights, and nd that KD-BIRL is able to accuratelyrecover the relative magnitude and sign of the individual weights w in two settings with dierent w values(). Our results in the 10 10 Gridworld setting demonstrate that KD-BIRL is able to infer rewardfunctions in environments with high-dimensional (R R100) state spaces using a featurized reward function. Next, we investigate KD-BIRLs performance in the sepsis treatment environment, which presents additionalchallenges due to its continuous state space. Unlike the previous Gridworld environments, each state in thesepsis environment is a vector of length 46, where each element corresponds to a real-valued measurementof a given feature.As a result, the reward function parameterization needs to learn a low-dimensional representation of the state features. To do this, we learn using a variational auto-encoder (VAE). The useof a VAE is appropriate in this situation because we do not know in advance which features will be mostrelevant to reward function inference. The VAE takes as input vectors of length 47 (46 state features + 1treatment action), and projects them into a latent space that has 3 features. Now, we seek to understand how KD-BIRL performs when it receives few test task expert demonstrationsin comparison to the size of the training task dataset. KD-BIRL receives expert demonstrations from oneunknown reward function, which is parameterized using a set of weights w, and training task demonstrationsfrom four additional known reward functions. Our baselines include AVRIL, which only receives the expert",
  "Theoretical Guarantees": "Before we investigate our method empirically, we prove asymptotic posterior consistency. Posterior consis-tency ensures that as the number of samples increases, the posterior distribution centers around the truereward function parameter value. This is important to ensure that our method is reliable at incorporatingnew data, and given enough data, will accurately learn the parameters corresponding to the true rewardfunction. KD-BIRL uses a probability density function to approximate the likelihood; consequently, we can reasonabout its posteriors asymptotic behavior. In particular, we show that the posterior estimate contracts as itreceives more samples. This type of analysis also aligns with the behavior that we want in practice; moredemonstration samples should shrink the posterior distribution around the true reward function.",
  "where pm(s, a|R) is the approximation of the likelihood using m training dataset samples": "Lemma 5.1 veries that we can approximate the true likelihood function using CKDE, which opens the doorto Bayesian inference. Because the IRL problem is non-identiable, the correct reward function may notbe unique (Abbeel & Ng, 2004; Ratli et al., 2006b; Ziebart et al., 2009). In this work, we assume thatany two reward functions that lead an agent to behave in the same way are considered equivalent. Thatis, if a set of demonstration trajectories is equally likely under two distinct reward functions, the functionsare considered equal: R1 R2 if p(|R1) p(|R2)L1 = 0. Using this intuition, we can then dene theequivalence class [R] for R as [R] = {R R : R R}. Our objective is to learn a posterior distributionthat places higher mass on reward functions that are in the equivalence class [R]. We now show that as n, the size of expert demonstration dataset for the test task, and m, the size of thetraining demonstration dataset, approach , the posterior distribution contracts to the equivalence class ofthe reward function that best explains the expert demonstration dataset [R]. Theorem 5.2. Assume the prior for R, denoted by P, satises P({R : KL(R, R) < }) > 0 for any > 0,where KL is the KullbackLeibler divergence. That is, prior P denes a non-empty set of possible rewardfunctions such that each reward function is within a KL-divergence distance of from the data-generatingreward function R. Assume R Rd is a compact set. Let pn",
  ". How much quicker can KD-BIRL contract in comparison to baseline methods? (.4)": "To answer these questions, we investigate two environments. The rst is a Gridworld setting with a discretestate space. We use three grid sizes (2 2, 5 5 and 10 10) to investigate how KD-BIRLs performancescales. The second setting is a simulated sepsis treatment environment (Amirhossein Kiani, 2019), whichhas a continuous state space and is thus, more challenging. The objective of the sepsis environment is toadminister a series of treatment such that a patient is successfully discharged.",
  "Baselines": "We evaluate KD-BIRLs empirical performance in comparison to three Bayesian IRL baselines. We focus onBayesian IRL approaches because the inferred posterior distribution provides uncertainty estimates of thereward function, which frequentist approaches fail to provide. Our rst baseline is a single-task Bayesian IRL(BIRL) approach (Ramachandran & Amir, 2007), which we compare to in the environments with smaller statespaces. We avoid this comparison in the larger state-space environments because BIRL is computationallyprohibitive to run. Next, we compare to a more recent single-task Bayesian IRL method, ApproximateVariational Reward Imitation Learning (AVRIL) (Chan & van der Schaar, 2021). AVRIL uses variationalinference to approximate the posterior distribution on the reward function, which is more computationallyecient at the cost of approximate inference. As stated earlier, one goal of our evaluations is to verifythat incorporating data from training tasks improves the contraction rate of the posterior. Thus, we alsocompare to a baseline in which AVRIL is initialized using an informative prior learned from the trainingtasks. In this way, AVRIL can also take advantage of the information stored in the training task dataset.We initialize the mean and variance parameters of the AVRIL prior using the reward function samplesfrom the training dataset. Specically, given a training dataset, {(sj, aj, Rj)}m",
  "Evaluation Metrics": "To evaluate the learned posterior distributions of the reward functions, we follow earlier work and useexpected value dierence (EVD) (Choi & Kim, 2012; 2013; Brown & Niekum, 2018; Levine et al., 2011).EVD measures the dierence in total reward obtained by an agent whose policy is optimal for the true(data-generating) reward function and an agent whose policy is optimal for some estimated reward function.Dene the value function of a policy under the reward function R as,",
  "EV D = |V ,R V (rL),R|,": "where and (rL) are the optimal policies for the true reward function R and a reward function rL drawnfrom the posterior, respectively. EVD is a suitable metric for comparing the performance of methods withoutdirectly comparing the learned reward functions, which may have distinct functional forms. The lower theEVD, the better the estimated reward function rL captures the qualities of the true reward function R",
  "Original reward function parameterization": "In our rst set of evaluations, we represent R as a vector in which each cell contains a parameter correspondingto the scalar reward in one of the states. For example, in a 10 10 Gridworld environment, there are 100distinct states, so the reward function R R100 is represented as a vector of length 100. We rst demonstrate that KD-BIRLs marginalized posterior distribution is more concentrated than baselineapproaches in a 2 2 Gridworld. Here, the test task demonstrations arise from the reward function R =. In addition to these test demonstrations, KD-BIRL receives training task demonstrations fromthe reward functions R1 = and R2 = . To evaluate our method, we visualize the densityof the posterior samples for each of the baselines, marginalized at each state. The posterior samples from KD-BIRL are more concentrated around R than baseline approaches ().The BIRL posterior samples are also close to the ground truth reward, but not as concentrated as those of KD-BIRL. In contrast, neither AVRIL nor AVRIL with an informative prior has concentrated posteriors in all of",
  ":KD-BIRL learns more concen-": "trated posteriors towards R than base-line methods. Dashed vertical lines displaythe true reward. The x and y-axis repre-sents the sampled reward marginalized atthe given state and estimated density, re-spectively. A perfect marginalized posteriordistribution would have the highest densityat the black line (R) in each state. : Using known features to parameterize re-ward enables reward inference in two settings in a10x10 Gridworld. The rst column shows w projectedonto the Gridworld, the second shows the KD-BIRLposterior mean, and the third shows the joint densityplots of the two weights, with the rst on the x-axisand the second on the y-axis. KD-BIRL accurately in-fers the relative magnitude and sign of the individualweights. the states. Notably, in state , these distributions are mostly uniform, when they should be concentratedat zero.This result indicates that using the combination of the training- and test task demonstrations can improve concentration of the posterior, but an informative prior is insucient to make use of thisinformation. In contrast, incorporating this information using the CKDE-based likelihood formulation canlead to concentrated state-marginalized posterior distributions. While the 2 2 Gridworld experiment is promising, we would like to apply our method in environmentswith much higher-dimensional reward functions. As discussed in , kernel density estimation scalespoorly as the number of reward function parameters increase (Izbicki & Lee, 2017). To support this claimand motivate the use of a feature-based reward function, we show results under the original reward functionparameterization in a 5 5 Gridworld environment. We select a data-generating reward function R that has a nonzero reward in only one of the states (,Panel 1). The state occupancy of the test task expert demonstrations can be seen in , Panel 4. FromPanel 4, we see that the test task expert demonstrations all approach the upper right corner of the grid. KD-BIRL also receives training task demonstrations from three other reward functions. The state occupancydensity of the training demonstrations can be seen in , Panel 3. In contrast with the test taskexpert demonstrations, the training task demonstrations cover far more states in the environment. Thus,the training task demonstrations contain information about portions of the state-space that the expertdemonstration do not cover, which should ideally improve posterior distribution inference. In our results, we nd that our method is able to estimate a posterior whose mean is in the equivalence classof R (, Panel 2). In other words, an agent that follows the reward function identied by the posteriormean would still be directed to the terminal state in the upper right corner of the grid (which contains thehighest reward). However, despite having demonstrations from four separate reward functions across thetraining and test tasks, the posterior mean is slightly inaccurate. There are states (, in ,Panel 2) where the estimated scalar reward is incorrect, which suggests that the CKDE struggles to learn 25independent reward parameters successfully from these diverse demonstrations. This result motivates the",
  ": The KD-BIRL posterior empirically contracts faster than baselines. Here, we evaluatefour sepsis treatment settings, each with a distinct w.The x, y axes represent the number of test": "demonstrations and the EVD of the learned posterior respectively, and the error bars indicate standarderror. We compare to two baselines, AVRIL and AVRIL with an informative prior. Our results indicatethat KD-BIRL is able to produce lower or comparable EVD with fewer samples than either baseline in a lowexpert demonstration data regime (n < 1000). : The marginal posterior of KD-BIRL for a chosen w in the sepsis environment: The true parameteris displayed in the black dashed line, and the density of the posterior marginalized at each parameter is plottedon three plots.",
  "demonstrations, and AVRIL with an informative prior, which uses a prior that is calculated based on themean and variance of the demonstrations in the training task dataset": "In four settings, each with a dierent w, KD-BIRLs posterior samples generate consistently lower EVD thanAVRIL (), and notably lower standard error than AVRIL with an informative prior. In particular,KD-BIRL achieves lower EVD in the low-expert demonstration data regime where there are fewer than1000 samples in the test expert demonstration dataset.Lower EVD implies that the posterior is more concentrated at w. shows that as n increases, the EVD of KD-BIRL is lower than both baselines,which implies that the KD-BIRL posterior is more concentrated with the same number of samples. Thisindicates that KD-BIRL can learn a more accurate posterior with fewer test demonstration samples byleveraging information from the training tasks. Taken together, this suggests that the KD-BIRL posteriorrequires fewer samples to contract to w. We also note that the informative prior that is learned using thetraining task demonstrations has a large variance, which we believe leads to posterior samples that produceEVDs with high variance. Finally, as noted earlier, one of the benets of using a Bayesian method is uncertainty quantication. Wevisualize the learned posterior parameters for a fth setting with a new w (), and nd that theposterior is concentrated at the true parameter. Our experiments demonstrate that using a VAE to learn",
  "Discussion": "In this work, we present kernel density Bayesian inverse reinforcement learning (KD-BIRL), an IRL algo-rithm that uses CKDE to incorporate existing domain-specic datasets to improve the posterior contractionrate. We address a gap in the Bayesian IRL literature by leveraging a consistent likelihood estimator andtranslating this estimator into posterior consistency guarantees for this class of methods. Our experimentsdemonstrate that KD-BIRL can perform IRL in high-dimensional and continuous state spaces. Notably,KD-BIRL learns a more concentrated posterior distribution with fewer samples, outperforming a leadingsingle-task IRL method, even when this leading method is initialized with an informative prior. Our resultsshow that KD-BIRL can learn posterior distributions that capture key characteristics of objectives whileproviding uncertainty estimates, even with limited data. Limitations Our work assumes that the expert demonstrations are of high quality; we anticipate thattraining task demonstrations that are not optimal w.r.t the corresponding reward function will result in lessaccurate likelihood estimates. Furthermore, we assume that the features used in the feature-based rewardfunction fully capture the information stored within a state-action tuple. Further work is needed to analyzethe eect of less representative features. Our work also assumes that the L1 norm is a valid distance metricbetween posterior densities, and this may not be true depending on the application setting. Future Work Several future directions remain open. The particular choices of distance metrics and hyper-parameters used in the CKDE depend on the environment and reward function parameterization; additionalexperimentation is required to characterize the relationship between these metrics and the quality of thelearned posterior. Furthermore, it is of interest to explore other ways in which the CKDE can be used toinfer higher dimensional reward functions. This may include exploring how to speed up the CKDE (Holmeset al., 2007), or replacing it with another nonparametric conditional density estimator (Rojas et al., 2005;Hinder et al., 2021). Additionally, our work proposes using density estimation to approximate the likelihood,and there are several other choices for learning this density function including diusion models (Cai & Lee,2023), and Gaussian processes (DEmilio et al., 2021)."
}