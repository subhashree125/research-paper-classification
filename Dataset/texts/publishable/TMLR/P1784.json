{
  "Abstract": "Complex logical query answering (CLQA) is a recently emerged task of graph machine learn-ing that goes beyond simple one-hop link prediction and solves the far more complex task ofmulti-hop logical reasoning over massive, potentially incomplete graphs. The task receivedsignificant traction in the community; numerous works expanded the field along theoreticaland practical axes to tackle different types of complex queries and graph modalities withefficient systems. In this paper, we provide a holistic survey of CLQA with a detailed tax-onomy studying the field from multiple angles, including graph types (modality, reasoningdomain, background semantics), modeling aspects (encoder, processor, decoder), supportedqueries (operators, patterns, projected variables), datasets, evaluation metrics, and appli-cations. Finally, we point out promising directions, unsolved problems and applications ofCLQA for future research.",
  "Introduction": "Graph databases (graph DBs) are key architectures to capture, organize and navigate structured relationalinformation over real-world entities. Unlike traditional relational DBs storing information in tables witha rigid schema, graph DBs store information in the form of heterogeneous graphs, where nodes represententities and edges represent relationships between entities. In graph DBs, a relation (i.e. heterogeneousconnection between entities) is a first-class citizen. With the graph structure and a more flexible schema,graph DBs allow for an efficient and expressive way to handle higher-order relationships between distant",
  "a": ": A complex logical query a and its execution over an incomplete graph b . Symbolic engines likeSPARQL c perform edge traversal and retrieve an incomplete set of easy answers directly reachable in thegraph, i.e., {UofT}. Neural query execution d recovers missing ground truth edges (dashed) and returnsan additional set of hard answers {UdeM, NYU} unattainable by symbolic methods. entities, especially navigating through multi-hop hierarchies. While traditional DBs require expensive joinoperations to retrieve information, graph DBs can directly traverse the graph and navigate through linksmore efficiently. Due to its capabilities, graph databases serve as the backbone of many critical industrialapplications including question answering in virtual assistants (Flint, 2021; Ilyas et al., 2022), recommendersystems in marketplaces (Dong, 2018; Hamad et al., 2018), social networking in mobile applications (Bronsonet al., 2013), and fraud detection in financial industries (Tian et al., 2019; Pourhabibi et al., 2020). One of the most important tasks of graph DBs is to perform complex query answering. The goal is to retrievethe answers of a given input query from the graph database. Given the query, graph DBs first translate andoptimize the query into a more efficient graph traversal pattern with a query planner, and then execute thepattern on the graph database to retrieve the answers from the graph storage using the query executor. Thestorage compresses the graphs into symbolic indexes suitable for fast table lookups. Querying is thus fastand efficient under the assumption of completeness, i.e., stored graphs have no missing edges. However, most real-world graphs are notoriously incomplete, e.g., in Freebase, 93.8% of people have noplace of birth and 78.5% have no nationality (Mintz et al., 2009), and about 68% of people do not haveany profession (West et al., 2014), while in Wikidata, about 50% of artists have no date of birth (Zhanget al., 2022a), and only 0.4% of known buildings have information about height (Ho et al., 2022). In light ofincompleteness, navely traversing the graph to find answers leads to a significant miss of relevant results, andthe issue further exacerbates with an increasing query complexity. This inherently hinders the application ofgraph databases. Link prediction aims to predict missing information, but is a challenging task. Prior workspredict links by learning a latent representation of entities or links (Bordes et al., 2013; Yang et al., 2015;Trouillon et al., 2016; Sun et al., 2019) or mining rules (Galrraga et al., 2013; Xiong et al., 2017; Lin et al.,2018; Qu et al., 2021). While it is possible to use one-hop link predictors to materialize all predicted facts(above certain confidence threshold) and run deterministic query answering pipelines, the computationalcomplexity of this operation is quadratic in the number of entities and is prohibitively expensive for anyreal-world graph. Furthermore, these approaches rank possible candidates for completion, meaning that theydo not tell which of the completions could be traversed. Also reasoning can be used to complete specificinformation, but there is always a trade-off between possibly incomplete results and decidability with a",
  "denser graph, some SPARQL entailment regimes (Hawke et al., 2013) do not guarantee that query executionterminates in finite time": "On the other hand, recent advances in graph machine learning enabled expressive reasoning over large graphsin a latent space without facing decidability bottlenecks. The seminal work of Hamilton et al. (2018) onGraph Query Embedding (GQE) laid foundations of answering complex, database-like logical queries overincomplete KGs where inferring missing links during query execution is achieved via parameterization ofentities, relations, and logical operators with learnable vector representations and neural networks.Forthe incomplete knowledge graph in (), given a complex query At what universities do the TuringAward winners in the field of Deep Learning work?, traditional symbolic graph DBs (SPARQL- or Cypher-like) would return only one answer (UofT), reachable by edge traversal. In contrast, neural query embeddingparameterizes the graph and the query with learnable vectors in the embedding space. Neural query executionis akin to traversing the graph and executing logical operators in the embedding space that infers missing linksand enriches the answer set with two more relevant answers, UdeM and NYU, unattainable by symbolic DBs. Since then, the area has seen a surge of interest with numerous improvements of supported logical operators,query types, graph modalities, and modeling approaches. In our view, those improvements have been ratherscattered, without an overall aim. There still lacks a unifying framework to organize the existing works andguide future research. To this end, we present one of the first holistic studies about the field. Conceptually,we devise the taxonomy of CLQA methods that includes various aspects of query answering and is supposedto be a bridge between data management and ML communities. The taxonomy () classifies existingworks along three main axes, i.e., (i) Graphs ( logical formalisms behind the underlying graphand its schema) (ii) Modeling ( what are the neural approaches to answer queries) (iii) Queries( what queries can be answered). We then discuss Datasets and Metrics ( how wemeasure the performance of query answering). Each of these dimensions is further divided into fine-grainedaspects. Finally, we discuss CLQA applications () and summarize open challenges for future research(). Related Work. While there exist insightful surveys on general graph machine learning (Chami et al., 2022),simple link prediction in KGs (Ali et al., 2021; Chen et al., 2023), and logic-based link prediction (Zhanget al., 2022b; Delong et al., 2024), the complex query answering area remained uncovered so far. With ourwork, we close this gap and provide a holistic view on the state of affairs in this emerging field. We alsoelaborate on the similarities and differences between CLQA and KG-based Question Answering (KGQA) (adifferent subfield of NLP) in Appendix B.",
  "Preliminaries": "Here, we discuss the foundational terms and preliminaries often seen in the database and graph learningliterature.The rest of the preliminaries is deferred to the Appendix A elaborating on the mapping tostructured languages like SPARQL, formalizing approximate graph query answering, logical operators, andfuzzy logic. The full hierarchy of definitions is presented in .",
  "Definition 2.1 (Con) Con is an infinite set of constants.1": "Definition 2.2 ((Standard / Triple based) Knowledge Graph) We define a knowledge graph (KG)G = (E, R, S), where E Con represents a set of nodes (entities), R Con is a set of relations (edge types),and S (E R E) is a set of edges (triples) 2. Each edge s S on a KG G denotes a statement (or fact)(es, r, eo) in a triple format or r(es, eo) as a formula in first-order logic (FOL) form, where es, eo E denote 1In case the constants do not include representations for the real numbers, this set would be countably infinite.2There might be an overlap between E and R, i.e., some constants might be used as an edge and as a node.",
  "PhD": ": An example of KG modalities. Triple-only graphs have directed binary edges between nodes.Hyper-relational graphs allow key-value relation-entity attributes over edges. Hypergraphs consist of hyper-edges composed of multiple nodes. the subject and object entities, and r R denotes the relationship between the subject and the object. Often,the graph is restricted such that the set of nodes and edges must be finite, we will make this assumptionunless indicated otherwise. Forexample,onestatementontheKGin bis(Hinton, university, UofT)oruniversity(Hinton, UofT). Note all relations are binary on KGs, i.e., each relation involves exactly twoentities. This is also the choice made for the resource description framework (RDF) (Brickley et al., 2014)that defines a triple as a basic representation of a fact, yet RDF allows a broader definition of triples, wherethe object can be literals including numbers and timestamps (detailed in the following paragraphs).",
  "Basic Graph Query Answering": "We base our definition of basic graph queries on the one from (Hogan et al., 2021, section 2.2.1 basic graphpatterns), but adapt it to our graph formalization. Other definitions can be found in Appendix A.2. Definition 2.3 (Term and Var) Given a knowledge graph G = (E, R, S) (or equivalent G = (E, R, S, L)),we define the set of variables Var = {v1, v2, . . . } which take values from Con, but is strictly disjoint from it.We call the union of the variables and the constants the terms: Term = Con Var",
  "With these concepts in place a Basic Graph Queryis defined as follows:": "Definition 2.4 (Basic Graph Query) Aba-sic graph query is a 4-tuple Q = (E, R, S, S)(equivalently a 5-tuple Q=(E, R, S, S, L),with literals),with ETerm a set of nodeterms, R Term a set of relation terms, andS, S E R E, two sets of edges (or equiv-alent to how graph edges are defined with literals).The query looks like two (small) graphs; one formedby the edges in S, and another by the edges in S.The former set includes edges that must be matchedin the graph to obtain answers to the query, while the latter contains edges that must not be matched (i.e.,atomic negation).",
  "With VarQ = (E R) Var (all variables in the query), the answer to the query is defined as follows": "Definition 2.5 (Basic Graph Query Answering) Given a knowledge graph G and a query Q as definedabove, an answer to the query is any mapping : VarQ Con such that replacing each variable vi in theedges in S with (vi) results in an edge in S, and replacing each variable vj in the edges in S with (vj)results in an edge that is not in S. The set of all answers to the query is the set of all such mappings. For triple-based KGs, each edge (a, R, b) in the edge set S (respectively S) of a query can be seen as arelation projection R(a, b) (resp. R(a, b)), i.e., as binary functions. Now, because the conjunction (i.e., all)of these relation projections (resp. the negation) have to be true, we also call these conjunctive queries withatomic negation (CQneg). The SPARQL query language defines basic graph patterns (BGPs). These closelyresemble our basic graph query for RDF graphs, but with S = , i.e., they do not support atomic negationbut only conjunctive queries (CQ). We could analogously create CQ and CQneg classes for the other KGtypes.",
  "Note there that the variables are not always mapped to nodes which occur in the graph. It is well possiblethat the query contains an aggregation function which results in a literal value": "In the neural logical query answering literature (starting from Hamilton et al. (2018); Ren et al. (2020)), wecan find the concepts of easy and hard answers. This refers to whether the answers can be found by onlyhaving access to G or not. Definition 2.7 (Easy and Hard answers) Given a knowledge graph G, subgraph of a larger unobservablegraph G, and a query Q. Easy and hard answers are defined in terms of exact query answering (Defini-tion A.5). The set of easy answers is the intersection of the answers obtained from G, and those from G.The set of hard answers is the set difference between the answers from G and those from G.",
  "Published in Transactions on Machine Learning Research (11/2024)": "enables two main branches of KGQA models, i.e., (1) semantic parsing from a natural languagequestion to a SPARQL query (Keysers et al., 2020) where the main challenge is compositionalgeneralization to unseen combinations of relations; (2) retrieval-based LLM approaches (Yasunagaet al., 2021; Baek et al., 2023; He et al., 2024) where the main challenge is to maximize the recallof k extracted candidate subgraphs and re-rank those to predict the answers. Some approachesthat support predicting missing links on the fly rely on pre-trained shallow KG embeddings (Saxenaet al., 2020) which, essentially, perform a 1p relation projection step. As retrieval-based techniquesare better amenable to other modalities, extending KGQA to support images and scene graphs isalso possible (Wang et al., 2023c).",
  "A Taxonomy of Query Reasoning Methods": "In the following sections, we devise a taxonomy of query answering works.We categorize existing andenvisioned approaches along three main directions: (i) Graphs what is the underlying structure againstwhich we answer queries; (ii) Modeling how we answer queries and which inductive biases are employed;(iii) Queries what we answer, what are the query structures and what are the expected answers. Thetaxonomy is presented in . In the following sections, we describe each direction in detail and illustratethem with examples covering the currently existing CLQA literature (more than 50 papers).",
  "Graphs": "The Graphs category covers the underlying graph structure (G in Definition A.4) against which complexqueries are sent and the answers are produced.Understanding the graph, its contents and modelingparadigms is crucial for designing query answering models. To this end, we propose to analyze the un-derlying graph from three aspects: Modality, Reasoning Domain, and Background Semantics.",
  "Modality": "We highlight four modalities common for KGs and graph databases: standard triple-based KGs, hyper-relational KGs, and hypergraph KGs. The difference among the three modalities is illustrated in .Additionally, we outline multi-modal KGs that contain not just a graph of nodes and edges, but also text,images, audio, video, and other data formats linked to the underlying graph explicitly or implicitly. We categorize the literature along the Modality aspect in . To date, most query answering approachesoperate solely on triple-based graphs. Among approaches supporting hyper-relational graphs, we are onlyaware of StarQE (Alivanistos et al., 2022) that incorporates entity-relation qualifiers over labeled edges andits extension NQE (Luo et al., 2023). We posit that the hyper-relational model might serve as a theoreticalfoundation of temporal query answering approaches since temporal attributes are in fact continuous key-valueedge attributes. To date, we are not aware of complex query answering models supporting hypergraphs ormulti-modal graphs. We foresee them as possible area of future research in the area.",
  "Triple-onlyHyper-RelationalHypergraphMulti-modal": "GQE (Hamilton et al., 2018), GQE w hash (Wang et al., 2019), CGA (Maiet al., 2019), TractOR (Friedman & Van den Broeck, 2020), Query2Box (Renet al., 2020), BetaE (Ren & Leskovec, 2020), EmQL (Sun et al., 2020), MPQE(Daza & Cochez, 2020), Shv (Gebhart et al., 2023), Q2B Onto (Andresel et al.,2021), RotatE-Box (Adlakha et al., 2021), BiQE (Kotnis et al., 2021), HyPE (Choudhary et al., 2021b), NewLook (Liu et al., 2021), CQD (Arakelyan et al.,2021), PERM (Choudhary et al., 2021a), ConE (Zhang et al., 2021b), LogicE(Luus et al., 2021), MLPMix (Amayuelas et al., 2022), FuzzQE (Chen et al., 2022),GNN-QE (Zhu et al., 2022), GNNQ (Pflueger et al., 2022), SMORE (Ren et al.,2022), KGTrans (Liu et al., 2022), LinE (Huang et al., 2022b), Query2Particles (Bai et al., 2022), TAR (Tang et al., 2022), TeMP (Hu et al., 2022), FLEX (Linet al., 2022), TFLEX (Lin et al., 2023), NodePiece-QE (Galkin et al., 2022b),ENeSy (Xu et al., 2022), GammaE (Yang et al., 2022a), NMP-QEM (Longet al., 2022), QTO (Bai et al., 2023c), SignalE (Wang et al., 2022), LMPNN(Wang et al., 2023e), Var2Vec (Wang et al., 2023a), CQDA (Arakelyan et al.,2023), Query2Geom (Sardina et al., 2023), SQE (Bai et al., 2023b), RoConE (He et al., 2023), FIT (Yin et al., 2023b), LitCQD (Demir et al., 2023), CylE(Nguyen et al., 2023b), LARK (Choudhary & Reddy, 2023), WFRE (Wang et al.,2023d), BiDAG (Xu et al., 2023a), NRN (Bai et al., 2023a), Query2Triple (Xuet al., 2023b), SCoNe (Nguyen et al., 2023a), CQD Onto (Andresel et al., 2023),UnRavL (Cucumides et al., 2024), UltraQuery (Galkin et al., 2024)",
  "Reasoning Domain": "Following Definition 2.5, a query Q includes constants Con, variables Var, and returns answers as mappings : VarQ Con. By Reasoning Domain we understand the space of possible constants that query answeringmodels can reason about. We highlight three common domains (Discrete, Discrete + Time, Discrete +Continuous), illustrate them in , and categorize existing works in . Each subsequent domainis a superset of the previous domains, e.g., Discrete + Continuous includes the capabilities of Discrete andDiscrete + Time and expands the space to continuous inputs and outputs. In the Discrete domain, constants, variables, and answers can be entities E and (or) relation types R of theKG, Con E R, Var E R, E R. That is, queries may only contain entities (or relations) andtheir answers are only entities (or relations). For example, a query ?x : education(Hinton, x) in can",
  "year == 1973": ": Reasoning Domains. The Discrete domain only allows entities and relations as constants, variables,and answers.The Discrete + Time domain extends the space to discrete timestamps and time-specificoperators. The Discrete + Continuous domain allows continuous inputs (literals) and outputs. only return two entities {Edinburgh, Cambridge} as answers. Conceptually, the framework allows relationtypes r R to be variables as well. For example, a SPARQL graph pattern {Hinton ?r Cambridge} or?r : r(Hinton, Cambridge) in the functional form that returns all relation types between two nodes Hintonand Cambridge. However, to the best of our knowledge, all current approximate query answering literatureand datasets limit the queries such that they do never have variables in the relation position, i.e., R Con.(we discuss queries structure in more detail in ). To date, most of the literature in the field belongsto the Discrete reasoning domain (). Some nodes and edges might have timestamps from a set of discrete timestamps t T S indicating a validityperiod of a certain statement. In a more general case, certain subgraphs might be timestamped. We definethe Discrete + Time domain when queries include temporal data. In this domain, the set of constants isextended with the set of timestamps, i.e., T S Con, and relation projections may be instantiated with acertain timestamp Rt(a, b). With such a graph, one can extend set of queries and the set of possible answersto include time information. An example of such a work is this is the Temporal Feature-Logic EmbeddingFramework (TFLEX) by Lin et al. (2023) that defines additional operators before, after, between overedges with discrete timestamps. For instance (), given a timestamped graph and a query ?x : educationyear==1973(Hinton, x), theanswer set includes only Edinburgh as the timestamp 1973 falls into the validity period of only one edge. Finally, the most expressive domain is Discrete + Continuous that enables reasoning over continuousinputs (such as numbers, texts, continuous timestamps) often available as node and edge attributes orliterals. Formally, for numerical data, the space of constants is extended with real numbers R Con. Alsothe query formalism is extended to allow reasoning over the real numbers. An example query in ?x : education(Hinton, x) x.students < 30000 includes a conjunctive term x.students < 30000 thatrequires numerical reasoning over the students attribute of a variable x to produce the answer Cambridge.In a similar fashion, extending the answer set to continuous outputs can be framed as a regression task.To date, LitCQD (Demir et al., 2023) and NRN (Bai et al., 2023a) are the only approaches supportingnumerical literals (in R) as query constants and potential query answers. NRN, however, treats literals asdiscrete entities in the graph. LitCQD employs TransEA (Wu & Wang, 2018) (n addition to the ComplEx-based (Lacroix et al., 2018) link predictor) as a regressor to predict numerical values of entities attributes.Incorporating literals into queries, LitCQD defines filtering operators less than, greater than, equalimplemented as exponential functions of predicted and target values. For example, the term x.students <30000 is represented as a conjuction students(x, C) lt(C, 30000) of a relation projection and the less-than",
  "DiscreteDiscrete + TimeDiscrete + Continuous": "GQE (Hamilton et al., 2018), GQE w hash (Wang et al., 2019), CGA (Mai et al.,2019), TractOR (Friedman & Van den Broeck, 2020), Query2Box (Ren et al.,2020), BetaE (Ren & Leskovec, 2020), EmQL (Sun et al., 2020), MPQE (Daza& Cochez, 2020), Shv (Gebhart et al., 2023), Q2B Onto (Andresel et al., 2021),RotatE-Box (Adlakha et al., 2021), BiQE (Kotnis et al., 2021), HyPE (Choud- hary et al., 2021b), NewLook (Liu et al., 2021), CQD (Arakelyan et al., 2021),PERM (Choudhary et al., 2021a), ConE (Zhang et al., 2021b), LogicE (Luuset al., 2021), MLPMix (Amayuelas et al., 2022), FuzzQE (Chen et al., 2022),GNN-QE (Zhu et al., 2022), GNNQ (Pflueger et al., 2022), SMORE (Ren et al.,2022), KGTrans (Liu et al., 2022), LinE (Huang et al., 2022b), Query2Particles(Bai et al., 2022), TAR (Tang et al., 2022), TeMP (Hu et al., 2022), FLEX (Lin et al., 2022), NodePiece-QE (Galkin et al., 2022b), ENeSy (Xu et al., 2022),GammaE (Yang et al., 2022a), NMP-QEM (Long et al., 2022), StarQE (Ali-vanistos et al., 2022), QTO (Bai et al., 2023c), SignalE (Wang et al., 2022),LMPNN (Wang et al., 2023e), NQE (Luo et al., 2023), Var2Vec (Wang et al.,2023a), CQDA (Arakelyan et al., 2023), Query2Geom (Sardina et al., 2023),SQE (Bai et al., 2023b), RoConE (He et al., 2023), FIT (Yin et al., 2023b), CylE (Nguyen et al., 2023b), LARK (Choudhary & Reddy, 2023), WFRE (Wang et al.,2023d), BiDAG (Xu et al., 2023a), Query2Triple (Xu et al., 2023b), SCoNe(Nguyen et al., 2023a), CQD Onto (Andresel et al., 2023), UnRavL (Cucumideset al., 2024), UltraQuery (Galkin et al., 2024)",
  "works.University": ": Background Semantics. Facts-only are graphs that only have assertions (ABox) and have nohigher-level schema. Class Hierarchy introduces node types (classes) and hierarchical relationships betweenclasses. Finally, Complex Axioms add even more complex logical rules (TBox), for example, governed bycertain OWL profiles, e.g., A professor is someone who has one or more students and works at university. Relational databases often contain a schema, that is, a specification of how tables and columns are organizedthat gives a high-level overview of the database content. In graphs databases, schemas exist as well and,for instance, node types are common in the Labeled Property Graph (LPG) paradigm. RDF graphs employthe standards based on Description Logics (Baader et al., 2003). As incorporating schema is crucial for",
  "designing effective query answering ML models, we introduce Background Semantics () as the notionof additional schema information available on top of plain facts (statements)": "Facts-only.In the simplest case, there is no background schema such that a KG consists of statements(facts) only, that is, a KG follows Definition 2.2, G = (E, R, S). In terms of description logics, the graph onlyhas assertions (ABox). Queries, depending on the Reasoning Domain (.2), involve only entities,relations, and literals.The original GQE (Hamilton et al., 2018) focused on facts-only graphs and themajority of subsequent query answering approaches () operate exclusively on schema-less KGs. Class Hierarchy.Classes of entities, or node types, are a natural addition to a facts-only graph as a basicschema. Using Definition 2.2 with a set of types T , a graph G is defined as G = (E, R, S), where E = E T(with E T = ). In other words, the types can be used as normal entities. To indicate that an entityhas a type, a specially labeled edge can be used (e.g., rdf:type), or a labeling function can be defined;both options have equivalent expressive power. Because types are nodes themselves, it is possible to specifyhierarchical relationships between classes using RDF Schema (RDFS) (Brickley et al., 2014)3. In LPG, it ismore common to only have a labeling function, and not allow type hierarchies. Practically, edges involvingtypes might be present physically in a KG or be considered as an additional input to a particular model. To date, we are aware of three query answering approaches () that incorporate entity types. Con-textual Graph Attention (CGA, Mai et al. (2019)) only uses types for entity embedding initialization andrequires each entity to have only one type. A similar technique was used in the evaluation of mpqe, butonly to initialize the representation of variables (for which types were assumed given). The queries are notconditioned by entity types and the answer set still includes entities only. That is, constants in queries andthose bound to variables are never types. In Type-Aware Message Passing (Temp, Hu et al. (2022)), type embeddings are used to enrich entity andrelation representations that are later sent to a downstream query answering model. Each node might haveseveral types. In the inductive scenario (we elaborate on inference scenarios in .1) with unseennodes at inference time, type and relation embeddings are learned invariants that transfer across trainingand inference entities. Queries, and bound variables are still limited to non-types only (im() E). The TBox and ABox Neural Reasoner (TAR, Tang et al. (2022)) incorporates types and their hierarchy toimprove predictions over entities. They also introduce the task of predicting types of answer entities, thatis, (t) ( E T ), for one specific variable called t. Other variables cannot bind to types and constants inthe query cannot be types either. The class hierarchy in TAR is used in three auxiliary losses besides theoriginal entity prediction, that is, concept retrieval prediction of the answer set of types, subsumption predicting which type is a subclass of another type, and instantiation predicting a type for each entity.",
  "A natural next step for the Class Hierarchy family of approaches is to incorporate types in queries in theform of constants and variables": "Complex Axioms.Finally, a schema might contain not just a class hierarchy but a set of more complexaxioms involving, for example, a hierarchy of relations, restrictions on relations, or composite classes. Sucha complex schema can now be treated as an ontology O and we extend the definition of the graph to includeit: G = (E, R, S, O). In , the axiom Professor 1 hasStudent works.University describes thatA professor is someone who has one or more students and works at university. In terms of description logics,a graph has an additional terminology component (TBox). The expressiveness of the TBox directly affectsthe complexity of symbolic reasoning engines up to exponential (ExpTime) for most expressive fragments. In graph representation learning, incorporating complex ontological axioms is non-trivial even for simplelink prediction models (Zhang et al., 2022b). In the query answering literature, the only attempt to includecomplex axioms is taken by Andresel et al. (2021). In Q2B Onto (O2B), an extension of Query2Box (Q2B,Ren et al. (2020)), the set of considered complex axioms belongs to the DL-LiteR fragment and supportsthe hierarchy of classes (subclasses), the hierarchy of relations (subproperties), as well as range and domainof relations. The model architecture is not directly conditioned on the axioms and remains the original",
  "Facts-only (ABOX)+ Class Hierarchy+ Complex Axioms (TBOX)": "GQE (Hamilton et al., 2018), GQE w hash (Wang et al., 2019), TractOR(Friedman & Van den Broeck, 2020), Query2Box (Ren et al., 2020), BetaE(Ren & Leskovec, 2020), EmQL (Sun et al., 2020), Shv (Gebhart et al.,2023), RotatE-Box (Adlakha et al., 2021), MPQE (Daza & Cochez, 2020),BiQE (Kotnis et al., 2021), HyPE (Choudhary et al., 2021b), NewLook(Liu et al., 2021), CQD (Arakelyan et al., 2021), PERM (Choudhary et al.,2021a), ConE (Zhang et al., 2021b), LogicE (Luus et al., 2021), MLP-Mix (Amayuelas et al., 2022), FuzzQE (Chen et al., 2022), GNN-QE (Zhuet al., 2022), GNNQ (Pflueger et al., 2022), SMORE (Ren et al., 2022),KGTrans (Liu et al., 2022), LinE (Huang et al., 2022b), Query2Particles (Bai et al., 2022), FLEX (Lin et al., 2022), TFLEX (Lin et al., 2023),NodePiece-QE (Galkin et al., 2022b), ENeSy (Xu et al., 2022), Gam-maE (Yang et al., 2022a), NMP-QEM (Long et al., 2022), StarQE (Ali-vanistos et al., 2022), QTO (Bai et al., 2023c), SignalE (Wang et al., 2022),LMPNN (Wang et al., 2023e), NQE (Luo et al., 2023), Var2Vec (Wang et al., 2023a), CQDA (Arakelyan et al., 2023), Query2Geom (Sardinaet al., 2023), SQE (Bai et al., 2023b), RoConE (He et al., 2023), FIT(Yin et al., 2023b), LitCQD (Demir et al., 2023), CylE (Nguyen et al.,2023b), LARK (Choudhary & Reddy, 2023), WFRE (Wang et al., 2023d),BiDAG (Xu et al., 2023a), NRN (Bai et al., 2023a), Query2Triple (Xuet al., 2023b), SCoNe (Nguyen et al., 2023a), UnRavL (Cucumides et al.,",
  "Modeling": "In this section, we discuss the literature from the perspective of Modeling. Following the common methodol-ogy (Battaglia et al., 2018), we segment the Modeling methods through the lens of Encoder-Processor-Decodermodules (illustrated in ). (1) The Encoder Enc() takes an input query q, target graph G with its en-tities and relations, and auxiliary inputs (e.g., node, edge, graph features) to build their representations in",
  "Neural Query Executor": ": Neural Query Execution through the Encoder-Processor-Decoder modules. Encoder function fbuilds representations of inputs (query, target graph, auxiliary data) in the latent space. Processor P executesthe query with its logical operators against the graph conditioned on other inputs. Decoder function g buildsrequested outputs that might be discrete or continuous. the latent space. (2) The Processor P leverages the chosen inductive biases to process representations of thequery with its logical operators in the latent or symbolic space. (3) The Decoder Dec() takes the processedlatents and builds desired outputs such as a distribution over discrete entities or regression predictions incase of continuous tasks. Generally, encoder, processor, and decoder can be parameterized with a neuralnetwork or be non-parametric. Finally, we analyze computational complexity of existing processors.",
  "Encoder": "We start the modeling section with encoders, i.e., how different methods encode and represent entities andrelations from the KG. There are three different categories, Shallow Embedding, Transductive Encoder, andInductive Encoder each representing a different way of producing the neural representation of the enti-ties/relations. Different encoding methods are suitable in different inference setups (details in .4),and may further require different logical operator methods (details in .2). illustrates thethree common encoding approaches.",
  "c": ": Categorization of Encoders.a Shallow encoders perform entity and relation embedding lookupand send them to the processor.bTransductive encoders additionally enrich the representations withquery, graph, classes, or other latents.c Inductive encoders do not need learnable entity embeddings. Shallow Embeddings.The first line of approaches encodes each entity/relation on the graph as a low-dimensional vector, and thus we achieve an entity embedding matrix E and a relation embedding matrix R.The shape of the entity embedding matrix is |E| d (|R| d), where d is the dimension of the embedding.Shallow embedding methods assume independence of the representation of all the nodes on the graph.",
  "Shallow EmbeddingTransductive EncoderInductive Encoder": "GQE(Hamilton et al., 2018), GQE w hash(Wang et al., 2019),CGA (Mai et al., 2019), TractOR (Friedman & Van den Broeck, 2020),Query2Box (Ren et al., 2020), BetaE (Ren & Leskovec, 2020), EmQL(Sun et al., 2020), Shv (Gebhart et al., 2023), Q2B Onto (Andresel et al.,2021), RotatE-Box (Adlakha et al., 2021), HyPE (Choudhary et al., 2021b), NewLook (Liu et al., 2021), CQD (Arakelyan et al., 2021), PERM(Choudhary et al., 2021a), ConE (Zhang et al., 2021b), LogicE (Luus et al.,2021), FuzzQE (Chen et al., 2022), SMORE (Ren et al., 2022), LinE(Huang et al., 2022b), Query2Particles (Bai et al., 2022), TAR (Tanget al., 2022), FLEX (Lin et al., 2022), TFLEX (Lin et al., 2023), Gam-maE (Yang et al., 2022a), NMP-QEM (Long et al., 2022), QTO (Bai et al., 2023c), SignalE (Wang et al., 2022), Var2Vec (Wang et al., 2023a),CQDA (Arakelyan et al., 2023), Query2Geom (Sardina et al., 2023), Ro-ConE (He et al., 2023), FIT (Yin et al., 2023b), LitCQD (Demir et al.,2023), CylE (Nguyen et al., 2023b), WFRE (Wang et al., 2023d), NRN(Bai et al., 2023a), SCoNe (Nguyen et al., 2023a), CQD Onto (Andresel",
  "(Hu et al., 2022), UnRavL(Cucumidesetal.,2024),UltraQuery (Galkin et al.,2024)": "This independence assumption gives the model much freedom, free parameters to learn. Such modelingorigins from the KG completion literature, where the idea is to learn the entity and relation embeddingmatrices by optimizing a pre-defined distance/score function over all edges on the graph, e.g., a triplet factdist(es, r, eo). The majority of query answering literature follows the same paradigm with various differentembedding spaces and distance functions to learn the entity and relation embedding matrices. Multipleembedding spaces have been proposed. For example, GQE (Hamilton et al., 2018) and Query2Box (Renet al., 2020) embed into Rd (point vector in the Euclidean space); FuzzQE (Chen et al., 2022) embeds into thespace of real numbers in range (fuzzy logic score); BetaE (Ren & Leskovec, 2020) uses Beta distribution,a probabilistic embedding space; ConE Zhang et al. (2021b) on the other hand embeds entities as a pointon a unit circle. Each design choice motivates the inductive bias for executing logical operators (as we showin .2). Some approaches employ shallow entity and relation embeddings already pre-trained on asimple link prediction task and just apply on top of them a query answering decoder with non-parametriclogical operators. For example, CQD (Arakelyan et al., 2021), LMPNN (Wang et al., 2023e), Var2Vec (Wanget al., 2023a), and CQDA (Arakelyan et al., 2023) take pre-trained embeddings in the complex space Cd and apply non-parametric t-norms and t-conorms to model intersection and union, respectively. QTO (Baiet al., 2023c) goes even further and fully materializes scores of all possible triples in one |R||E||E|",
  "matrix given pre-trained entity and relation embeddings at preprocessing stage": "Despite being the mainstream design choice, the downside of shallow methods is that (1) shallow embeddingsdo not use any inductive bias and prior knowledge of the entity or its neighboring structure since theparameters of all entities/relations are free parameters learned from scratch; (2) they are not applicablein the inductive inference setting since these methods do not have a representation/embedding for thoseunseen novel entities by design. One possible solution is to randomly initialize one embedding vector for anovel entity and finetune the embedding vector by sampling queries involving the novel entity (detailed in.3). However, such a solution requires gradient steps during inference, rendering it not ideal. Transductive Encoder.Similar to shallow embedding methods, transductive encoder methods learn thesame entity embedding matrix E. Besides, they learn an additional encoder Enc(q, E, R, . . . ) (parameter-ized with ) on top of the query q, entity and relation embedding matrices (and, optionally, other availableinputs). The goal is to apply the encoder to the embeddings of entities in the query q in order to capturedependencies between neighboring entities in the graph. Specifically, the additional encoder may take severalrows of the feature matrix as input and further apply transformations. For example, BiQE (Kotnis et al.,2021) and kgTransformer (Liu et al., 2022) linearize a query graph Gq into a sequence and apply a Trans-former (Vaswani et al., 2017) encoder that attends to all other embeddings in the query and obtain the finalrepresentation of the [MASK] token as the target query. MPQE (Daza & Cochez, 2020) and StarQE (Ali-",
  "Processor": "Having encoded the query and other available inputs, the Processor P executes the query in the latent (orsymbolic) space against the input graph. Recall that a query q is defined as q(E, R, S, S) where E and R terms include constants Con and variables Var, statements in S and S include relation projections R(a, b),and logical operators ops over the variables. We define Processor P as a collection of modules that performrelation projections R(a, b) given constants Con and logical operators ops {, , , . . . } over variablesVar (we elaborate on the logical operators in .1). Depending on the chosen inductive biases andparameterization strategies behind those modules, we categorize Processors into Neural and Neuro-Symbolic(a). Furthermore, we break down the Neuro-Symbolic processors into Geometric, Probabilistic, andFuzzy Logic (b). Note that in this section we omit pure entity encoder approaches like TeMP (Huet al., 2022), NodePiece-QE (Galkin et al., 2022b), and NRN (Bai et al., 2023a) (for numerical literals only)that can be paired with any neural or neuro-symbolic processor. To describe processor models more formally,we denote e as an entity vector, r as a relation vector, and q as the query embedding that is often a functionof e and r. We use Gq as the query graph. : Categorization of Query Processors. a provides a general view on Neural, Symbolic, Neuro-Symbolic methods as well as encoders that can be paired with any processor. b further breaks downneuro-symbolic processors.",
  "GQE (Hamilton et al., 2018), GQE w/ hashing (Wang et al.,": "2019), CGA (Mai et al., 2019), BiQE (Kotnis et al., 2021),MPQE (Daza & Cochez, 2020), StarQE (Alivanistos et al., 2022),MLPMix (Amayuelas et al., 2022), Query2Particles (Bai et al.,2022), KGTrans (Liu et al., 2022), RotatE-m, DistMult-m,ComplEx-m (Ren et al., 2022), GNNQ (Pflueger et al., 2022),SignalE (Wang et al., 2022), LMPNN (Wang et al., 2023e), SQE(Bai et al., 2023b), LARK (Choudhary & Reddy, 2023), BiDAG(Xu et al., 2023a), Query2Triple (Xu et al., 2023b)",
  ")": "EmQL (Sun et al., 2020), TractOR (Friedman& Van den Broeck, 2020), CQD (Arakelyan et al.,2021), LogicE (Luus et al., 2021), FuzzQE (Chenet al., 2022), TAR (Tang et al., 2022), FLEX (Linet al., 2022), TFLEX (Lin et al., 2023), UnRavL (Cucumides et al., 2024), ENeSy (Xu et al., 2022),QTO (Bai et al., 2023c), NQE (Luo et al., 2023),Var2Vec (Wang et al., 2023a), CQDA (Arakelyanet al., 2023), FIT (Yin et al., 2023b), LitCQD(Demir et al., 2023), WFRE (Wang et al., 2023d),CQD Onto (Andresel et al., 2023), UnRavL (Cu-cumides et al., 2024), UltraQuery (Galkin et al.,2024) Neural Processors.Neural processors execute relation projections and logical operators directly in thelatent space Rd parameterizing them with neural networks. To date, most existing purely neural approaches",
  "GNN": ": Neural Processors.(a) Relation projections R(a, b) and logical operators (non-parametric orparameterized with ) are executed sequentially in the latent space; (b) a query is encoded to a graph orlinearized to a sequence and passed through the encoder (GNN or Transformer, respectively). A pooledrepresentation denotes the query embedding. operate exclusively on the query graph Gq only executing operators within a single query and do not conditionthe execution process on the full underlying graph structure G. Since the query processing is performed inthe latent space with neural networks where Union () and Negation () are not well-defined, the majority ofneural processors implement only relation projection (R(a, b)) and intersection () operators. We aggregatethe characteristics of neural processors as to their embedding space, the way of executing relation projection,logical operators, and the final decoding distance function in . We illustrate the difference betweentwo families of neural processors (sequential execution and joint query encoding) in . The original GQE (Hamilton et al., 2018) is the first example of the neural processor. That is, queries q,entities e, and relations r are vectors in Rd. Query embedding starts with embeddings of constants C (anchornodes e) and they get progressively refined through relation projection and intersection, i.e., it is common toassume that query embedding at the initial step 0 is equivalent to embedding(s) of anchor node(s), q(0) = e.Relation projection is executed in the latent space with the translation function q + r, and intersectionis modeled with the permutation-invariant DeepSet (Zaheer et al., 2017) neural network. Several follow-upworks improved GQE to work with hashed binary vectors {+1, 1}d (Wang et al., 2019) or replaced DeepSetwith self-attention and translation-based projection to a matrix-vector product (Mai et al., 2019). Recently,Ren et al. (2022) proposed DistMult-m, ComplEx-m, and RotatE-m, extensions of simple link predictionmodels for complex queries that, inspired by GQE, perform relation projection by the respective compositionfunction and model the intersection operator with DeepSet and, optionally, L2 norm. The other line of works apply neural encoders to whole query graphs Gq without explicit execution of logicaloperators.Depending on the query graph representation, such encoders are often GNNs, Transformers,or MLPs. It is assumed that neural encoders can implicitly capture logical operators in the latent spaceduring optimization. For instance, MPQE (Daza & Cochez, 2020), StarQE (Alivanistos et al., 2022), andLMPNN (Wang et al., 2023e) represent queries as relational graphs (optionally, hyper-relational graphs forStarQE) where each edge is a relation projection and intersection is modeled as two incoming projectionsto the same variable node. All constants C and known relation types are initialized from the respectiveembedding matrices. All variable nodes in all query graphs are initialized with the same learnable [VAR]feature vector while all target nodes are initialized with the same [TAR] vector. Then, the query graph ispassed through a GNN encoder (R-GCN (Schlichtkrull et al., 2018) for MPQE, StarE (Galkin et al., 2020)for StarQE, GIN (Xu et al., 2019) for LMPNN), and the final state of the [TAR] target node is consideredthe final query embedding ready for decoding. A recent LMPNN extends query graph encoding with anadditional edge feature indicating whether a given projection R(a, b) has a negation or not and derives aclosed-form solution for the merged projection and negation operator for the ComplEx composition function.A different approach is taken by GNNQ (Pflueger et al., 2022) that frames query answering as a subgraphclassification task. That is, an input query is not directly executed over a given graph G, but, instead,the task is to classify whether a given precomputed subgraph G G satisfies a given conjunctive query.",
  "Sheavesq, e Rd, Fr Rddf(q, Gq as cochain)--Frq Fre": "Neuro-Symbolic Processors.In contrast to purely neural and symbolic models, we define neuro-symbolicprocessors as those who (1) explicitly design logic modules (or neural logical operators) that simulate thereal logic/set operations, or rely on various kinds of fuzzy logic formulations to provide a probabilistic viewof the query execution process, and (2) execute relation traversal in the latent space. The key differencebetween neuro-symbolic processors and the previous two is that neuro-symbolic processors explicitly model",
  ": Geometric Processors and their inductive biases": "the logical operations with strong inductive bias so that the processing / execution is better aligned withthe symbolic operation (e.g., by imposing restrictions on the embedding space) and more interpretable. Wefurther segment these methods into the following categories. Geometric Processors.Geometric processors design an entity/query embedding space with differentgeometric intuitions and further customize neuro-symbolic operators that directly simulate their logicalcounterparts with similar properties (as illustrated in ). We aggregate the characteristics of geometricmodels as to their embedding space and inductive biases for logical operators in . Query2Box (Ren et al., 2020) embeds queries q as hyper-rectangles (high-dimensional boxes) in the Euclideanspace. To achieve that, entities e and relations r are embedded as points in the Euclidean space where eachrelation has an additional learnable offset vector ro (entities offsets are zeros). The projection operatoris modeled as an element-wise summation q + r of centers and offsets of the query and relation, that is,the initial box is obtained by projecting the original anchor node embedding e (with zero offset) with therelation embedding r and relation offset ro. Accordingly, an attention-based neuro-intersection operator isdesigned to simulate the set intersection of the query boxes in the Euclidean space. The operator is closed,permutation invariant and aligns well with the intuition that the size of the intersected set is smaller thanthat of all input sets. The union operator is achieved via DNF, that is, union is the final step of concatenatingresults of operand boxes. Several works extend Query2Box, i.e., Query2Onto (Andresel et al., 2021) attemptsto model complex ontological axioms by materializing entailed triples and enforcing hierarchical relationshipusing inclusion of the box embeddings; RotatE-Box (Adlakha et al., 2021) designs an additional rotation-based Kleene plus (+) operator denoting relational paths (we elaborate on the Kleene plus operator in.1); NewLook (Liu et al., 2021) adds symbolic lookup from the adjacency tensor4 to the operators,modifies projection with MLPs and models the difference operator as attention over centers and offsets(note that the difference operator is a particular case of the 2in negation query, we elaborate on that in.1); Query2Geom (Sardina et al., 2023) replaces an attention-based intersection operator with asimple non-parametric closed-form geometric intersection of boxes. HypE (Choudhary et al., 2021b) extends the idea of Query2Box and embeds a query as a hyperboloid (twoparallel pairs of arc-aligned horocycles) in a Poincar hyperball to better capture the hierarchical information.A similar attention-based neuro-intersection operator is designed for the hyperboloid embeddings with thegoal to shrink the limits with DeepSets. ConE (Zhang et al., 2021b), on the other hand, embeds queries on the surface of a set of unit circles. Eachquery is represented as a cone section and the benefit is that in most cases the intersection of cones is stilla cone, and the negation/complement of a cone is also a cone thanks to the angular space bounded by 2.Based on this intuition, they design geometric neuro-intersection and negation operators. RoConE (He et al.,",
  "To sum up, the geometric-based processors are often designed with a strong geometric prior such thatproperties of the logical/set operations can be better simulated or satisfied": "Probabilistic Processors.Instead of a geometric embedding space, probabilistic processors aim to modelthe query and the logic/set operations in a probabilistic space.Some examples of implementing logicaloperators in a probabilistic space are illustrated in . The aggregated characteristics are presented in. BetaE (Ren & Leskovec, 2020) builds upon the Beta distribution and embeds entities/queries as high-dimensional Beta distribution with learnable parameters. The benefit is that one can design a parameterizedneuro-intersection operator over two Beta embeddings where the output is still a Beta embedding with moreconcentrated density function. A neuro-negation operator can be designed by simply taking the reciprocalof the parameters in order to flip the density.PERM (Choudhary et al., 2021a) looks at the Gaussiandistribution space and embeds queries as a multivariate Gaussian distribution. Since the product of Gaussianprobability density functions (PDFs) is still a Gaussian PDF, the neuro-intersection operator accordinglycalculates the parameters of the Gaussian embedding of the intersected set. NMP-QEM (Long et al., 2022)",
  ": Fuzzy-Logic Processors and fuzzy logical operators": "develops this idea further and represents a query as a mixture of Gaussians where logical operators aremodeled with MLP or attention over distribution parameters. LinE (Huang et al., 2022b) transforms theBeta distribution into a discrete sequence of values. A similar neuro-negation operator is introduced bytaking the reciprocal as BetaE while designing a new neuro-intersection/union operator by taking element-wise min/max. GammaE (Yang et al., 2022a) replaces Beta distribution with Gamma distribution as entityand query embedding space. Parameterizing logical operators with operations over mixtures of Gammadistributions, union and negation become closed, do not need DNF or DM transformations, and can beexecuted sequentially along the query computation graph. Overall, probabilistic processors are similar togeometric processors since they are all inspired by certain properties of the probability and geometry usedfor embeddings and customize neuro-logic operators.",
  "NMP-QEMe RdMLPr(q)Attn({qi})DNFMLP(q)e Ki=1 qi qi q = Ki=1 iN(i, i)": "Fuzzy-Logic Processors.Unlike the methods above, fuzzy-logic processors directly model all logicaloperations using existing fuzzy logic theory (Klement et al., 2013; van Krieken et al., 2022) where intersectioncan be expressed via t-norms and union via corresponding t-conorms (Section A.4). In such a way, fuzzy-logic processors avoid the need to manually design or learn neural logical operators as in the previous twoprocessors but rather directly use established fuzzy operators commonly expressed as differentiable, element-wise algebraic operators over vectors (). While intersection, union, and negation are non-parametric,the projection operator might still be parameterized with a neural network. The aggregated characteristicsof fuzzy-logic processors are presented in . Generally, fuzzy processors aim to combine execution inembedding space (vectors) with entity space (symbols). The described methods are different in designingsuch a combination. One of the first fuzzy processors is EmQL (Sun et al., 2020) that imbues entity and relation embeddingswith a count-min sketch (Cormode & Muthukrishnan, 2005). There, projection, intersection, and union areperformed both in the embedding space and in the symbolic sketch space, e.g., intersection is modeled asan element-wise multiplication and union is an element-wise summation of two sketches. CQD (Arakelyanet al., 2021) scores each atomic formula in a query with a pretrained neural link predictor and uses t-normsand t-conorms to compute the final score of a query directly in the embedding space. CQD does not trainany neural logical operators and only requires pretraining the entity and relation embeddings with one-hoplinks such that the projection operator is equivalent to top-k results of the chosen scoring function, e.g.,ComplEx (Lacroix et al., 2018). The idea was then extended in several directions: Query Tree Optimization(QTO) (Bai et al., 2023c) added a look-up from the materialized tensor of scores of all possible triples",
  "Decoder": "The goal of decoding is to obtain the final set of answers or a ranking of all the entities. It is the finalstep of the query answering task after processing. Here we categorize the methods into two buckets: non-parametric and parametric.Parametric methods require a parameterized method to score an entity (orpredict a regression target from the processed latents) while non-parametric methods can directly measurethe similarity (or distance) between a pair of query and entity on the graph. Most of the methods belong tothe non-parametric category as shown in the Distance column of processor tables , , ,.For instance, geometric models (Ren et al., 2020; Andresel et al., 2021; Adlakha et al., 2021; Choudhary et al., 2021b; Zhang et al., 2021b) pre-define a distance function between the representation ofthe query and that of an entity. Commonly employed distance functions are L1 (Hamilton et al., 2018; Renet al., 2022; Gebhart et al., 2023; Amayuelas et al., 2022; Tang et al., 2022; Xu et al., 2022; Long et al.,2022), L2 (Huang et al., 2022b; Chen et al., 2022), or their variations (Luus et al., 2021; Lin et al., 2022;",
  "NQEq, e, r d(Trf(q, Gq))q1 q2q1 + q2 q1 q21 qdot(q, e)": "2023), cosine similarity (Wang et al., 2019; Mai et al., 2019; Daza & Cochez, 2020; Wang et al., 2023e), dotproduct (Sun et al., 2020; Ren et al., 2022; Alivanistos et al., 2022; Liu et al., 2022; Bai et al., 2022; Luo et al.,2023), or naturally model the likelihood of all the entities without the need of a distance function (Arakelyanet al., 2021; Kotnis et al., 2021; Zhu et al., 2022; Pflueger et al., 2022; Bai et al., 2023c; Arakelyan et al., 2023;Wang et al., 2023a). Probabilistic models often employ KL divergence (Ren & Leskovec, 2020; Yang et al.,2022a) or Mahalanobis distance (Choudhary et al., 2021a). A rather exotic approach is Wasserstein-Fisher-Rao metric (WFR) used in WFRE Wang et al. (2023d). WFR is an optimal transport (OT) metric betweendistributions. Due to its computational complexity, the metric is approximated by the 1D convolution-basedSinkhorn optimization routine. One important direction (orthogonal to the distance function) that current methods largely ignore is howto perform efficient answer entity retrieval over extremely large graphs with billions of entities. A scalableand approximate nearest neighbor (ANN) search algorithm is necessary.Existing frameworks includingFAISS (Johnson et al., 2019) or ScaNN (Guo et al., 2020) provide scalable implementations of ANN. However,ANN is limited to L1, L2 and cosine distance and mostly optimized for CPUs. It is still an open researchproblem how to design efficient scalable ANN search algorithms for more complex distance functions suchas KL divergences so that we can retrieve with much better efficiency for different CLQA methods withdifferent distance functions (preferably, using GPUs). We conjecture that parametric decoders are to gain more traction in numerical tasks on top of plain entityretrieval for query answering. Such tasks might involve numerical and categorical features on node-, edge-,and graph levels, e.g., training a regressor to predict numerical values for node attributes like age, length,etc. Besides, a parametric decoder gives new opportunities to generalize to inductive settings where we mayhave unseen entities during evaluation. SE-KGE (Mai et al., 2020) takes a step in this direction by predictinggeospatial coordinates of query targets. LitCQD (Demir et al., 2023) expands the set of query variables andanswers to continuous numerical values by joint training of a link prediction model between entities and aregression model on top of entities attributes with numerical values.",
  "Computation Complexity": "Here we analyze the time complexity of different query reasoning models categorized by different operations,including relation projection, intersection, union, negation and answer retrieval after obtaining the repre-sentation of the query. We list the asymptotic complexity in for methods that perform stepwiseencoding of the query graph, and for methods (mostly GNN-based) that encode the whole querygraph simultaneously including projection and other logic operations. : Time complexity of each operation on G = (E, R, S). Across all methods, we denote the embeddingdimension and hidden dimension of MLPs as d, number of layers of MLPs/GNNs as l, number of branchesin an intersection operation as i, number of branches in a union operation as u.",
  "Queries": "The third direction to segment the methods is from the queries point of view. Under the queries category, wehave three subcategories: Query Operators, Query Patterns, and Projected Variables. For query operators,methods have different operator expressiveness, which means the set of query operators one model is ableto handle including existential quantification (), conjunction (), disjunction (), negation (), Kleeneplus (+), filter and various aggregation operators. For query patterns, we refer to the structure/pattern ofthe (optimized) query plan, ranging from paths and trees to arbitrary directed acyclic graphs (DAGs) andcyclic patterns. As to projected variables (by projected we refer to target variables that have to be boundto particular graph elements like entity or relation), queries might have a different number (zero or more) oftarget variables. We are interested in the complexity of such projections as binding of two and more variablesinvolves relational algebra (Codd, 1970) and might result in a Cartesian product of all retrieved answers.",
  "Query Operators": "Different query processors have different expressiveness in terms of operators a method can handle. Through-out all the works, we compiled that classifies all methods based on the supported operators. : Query answering processors and supported query operators. Processors supporting unions alsosupport projection and intersection (). Models supporting negation () also support unions, projections,and intersections.",
  "Projection and Intersection ()Union ()Negation ()Kleene +Filter & Aggr": "GQE (Hamilton et al., 2018)GQE hashed (Wang et al., 2019)CGA (Mai et al., 2019)TractOR (Friedman & Van den Broeck, 2020)MPQE (Daza & Cochez, 2020)BiQE (Kotnis et al., 2021)Sheaves (Gebhart et al., 2023)StarQE (Alivanistos et al., 2022)RotatE-m, DistMult-m,ComplEx-m (Ren et al., 2022)GNNQ (Pflueger et al., 2022) Query2Box (Ren et al., 2020)EmQL (Sun et al., 2020)Query2Onto (Andresel et al., 2021)HypE (Choudhary et al., 2021b)NewLook (Liu et al., 2021)PERM (Choudhary et al., 2021a)CQD (Arakelyan et al., 2021)kgTransformer (Liu et al., 2022)Query2Geom (Sardina et al., 2023)LitCQD (Demir et al., 2023)BiDAG (Xu et al., 2023a)NRN (Bai et al., 2023a)CQD Onto (Andresel et al., 2023) BetaE (Ren & Leskovec, 2020)ConE (Zhang et al., 2021b)LogicE (Luus et al., 2021)MLPMix (Amayuelas et al., 2022)Query2Particles (Bai et al., 2022)LinE (Huang et al., 2022b)GammaE (Yang et al., 2022a)NMP-QEM (Long et al., 2022)FuzzQE (Chen et al., 2022)TAR (Tang et al., 2022)GNN-QE (Zhu et al., 2022)FLEX (Lin et al., 2022)TFLEX (Lin et al., 2023)ENeSy (Xu et al., 2022)QTO (Bai et al., 2023c)SignalE (Wang et al., 2022)LMPNN (Wang et al., 2023e)NQE (Luo et al., 2023)Var2Vec (Wang et al., 2023a)CQDA (Arakelyan et al., 2023)SQE (Bai et al., 2023b)RoConE (He et al., 2023)CylE (Nguyen et al., 2023b)FIT (Yin et al., 2023b)WFRE (Wang et al., 2023d)LARK (Choudhary & Reddy, 2023)SCoNe (Nguyen et al., 2023a)Query2Triple (Xu et al., 2023b)UnRavL (Cucumides et al., 2024)UltraQuery (Galkin et al., 2024)",
  "We start from simplest conjunctive queries that involve only existential quantification () and conjunction() and gradually increase the complexity of supported operators": "Existential Quantification ().When appears in a query, this means that there exists at least oneexistentially quantified variable. For example, given a query At what universities do the Turing Awardwinners work? and its logical form q = U? . V: win(TuringAward, V ) university(V, U?), here V isthe existentially quantified variable. Query processors model existential quantification by using a relationprojection operator. Mapping to query languages like SPARQL, relation projection is equivalent to a triplepattern with one variable, e.g., {TuringAward win ?v} (). Generally, as introduced in .2,query embedding methods embed a query in a bottom-up fashion, starting with the embedding of the anchors",
  "Query processors, as described in .2, employ different parameterizations of conjunctions aspermutation-invariant set functions.A family of neural processors (Hamilton et al., 2018; Wang et al.,": "2019; Ren et al., 2022) often resort to the DeepSet architecture (Zaheer et al., 2017) that first projects eachset element independently and then pools representations together with a permutation-invariant function(e.g., sum, mean) followed by an MLP. Alternatively, (Mai et al., 2019; Bai et al., 2022) self-attention, canserve as a replacement of the DeepSet where set elements are weighted with the attention operation. Theother family of neural processors combine projection and intersection by processing the whole query graphwith GNNs (Daza & Cochez, 2020; Alivanistos et al., 2022; Pflueger et al., 2022; Wang et al., 2023e) or withthe Transformer over linearized query sequences (Kotnis et al., 2021; Liu et al., 2022). Geometric proces-sors (Ren et al., 2020; Liu et al., 2021; Choudhary et al., 2021b; Zhang et al., 2021b; He et al., 2023; Nguyenet al., 2023b) implement conjunction as the attention-based average of centroids and offsets of respectivegeometric objects (boxes, hyperboloids, cones, or cylinders). Probabilistic processors (Ren & Leskovec, 2020;Choudhary et al., 2021a; Huang et al., 2022b; Yang et al., 2022a) implement intersection as a weighted sumof parametric distributions that represent queries and variables. Fuzzy-logic processors (Arakelyan et al., 2021; Luus et al., 2021; Chen et al., 2022; Zhu et al., 2022; Wanget al., 2023d; Yin et al., 2023b; Demir et al., 2023) commonly resort to t-norms, generalized versions ofconjunctions in the continuous space and corresponding t-conorms for modeling unions (Section A.4).Often, due to the absence of a principal study, the choice of the fuzzy logic is a hyperparameter. We positthat such a study is an important avenue for future works in fuzzy processors. More exotic neuro-symbolicmethods for modeling conjunctions include element-wise product of count-min sketches (Sun et al., 2020) or",
  ": Query operators (relation projection and intersection), corresponding SPARQL basic graph pat-terns (BGP), and their computation graphs. Relation Projection (left) corresponds to a triple pattern": "as a weighted sum in the feature logic (Lin et al., 2022; 2023). Finally, some processors (Tang et al., 2022;Xu et al., 2022) perform conjunctions both in the embedding and symbolic space with neural and fuzzyoperators. We note that certain neural query processors that embed queries directly (Hamilton et al., 2018; Wang et al.,2019; Mai et al., 2019; Friedman & Van den Broeck, 2020; Ren et al., 2022) or via GNN/Transformer encoderover a query graph (Daza & Cochez, 2020; Kotnis et al., 2021; Gebhart et al., 2023; Alivanistos et al., 2022;Das et al., 2022; Pflueger et al., 2022) support only projections and intersections, that is, their extensionsto more complex logical operators are non-trivial and might require changing the underlying assumptions ofmodeling entities, variables, and queries. In some cases, support of unions might be enabled when re-writinga query to the disjunctive normal form (discussed below). Disjunction ().Query processors implement the disjunction operator in several ways. However, model-ing disjunction is notoriously hard since it requires modeling any powerset of entities on the graph in a vectorspace. Before delving into details about different ways of modeling disjunction, we first refer the readers tothe Theorem 1 in Query2Box (Ren et al., 2020). The theorem proves that we need the VC dimension of thefunction class of the distance function to be around the number of entities on the graph. The theorem shows that in order to accurately model any EPFO query with the existing framework, thecomplexity of the distance function measured by the VC dimension needs to be as large as the number ofKG entities. This implies that if we use common distance functions based on hyper-plane, Euclidean sphere,or axis-aligned rectangle,5 their parameter dimensionality needs to be at least (|E|) for real KGs. In otherwords, the dimensionality of the logical query embeddings needs to be (|E|), which is not low-dimensional;thus not scalable to large KGs and not generalizable in the presence of unobserved KG edges. The first idea proposed in Query2Box (Ren et al., 2020) is that given a model has defined a distance functionbetween a query representation and the entity representation, then a query can be transformed (or re-written)into its equivalent disjunctive normal form (DNF), i.e., a disjunction of conjunctive queries. For example,we can safely convert a query (AB)(C D) to ((AC)(AD)(B C)(B D)), where A, B, C, Dare atomic formulas. In such a way, we only need to process disjunction at the very last step. For modelsthat have defined a distance function between the query representation and entity representation d(q, e)(such as geometric processors (Ren et al., 2020; Andresel et al., 2021; Adlakha et al., 2021; Choudharyet al., 2021b; Zhang et al., 2021b; Sardina et al., 2023; He et al., 2023; Nguyen et al., 2023b) and someneural processors (Liu et al., 2022; Amayuelas et al., 2022; Wang et al., 2023e), the idea of using DNF tohandle disjunction is to (1) embed each atomic formula / conjunctive query in the DNF into a vector qi, (2)",
  ": Query operators (union, negation, Kleene plus), corresponding SPARQL basic graph patterns(BGP), and their computation graphs": "calculate the distance between the representation/embedding of each atomic formula / conjunctive queryand the entity d(qi, e), (3) take the minimum of the distances mini(d(qi, e)). The intuition is that sincedisjunction models the union operation, as long as the node is close to one atomic formula / conjunctivequery, it should be close to the whole query. Potentially, many neural processors with the defined distancefunction and originally supporting only intersection and projection can be extended to supporting unionswith DNF. One notable downside of this modeling is that it is exponentially expensive (to the number ofdisjunctions) in the worst case when converting a query to its DNF. Another category of mostly probabilistic processors (Choudhary et al., 2021a; Yang et al., 2022a) proposesa neural disjunction operator implemented with the permutation-invariant attention over the input set withthe closure assumption that the result of attention weighting union remains in the same probabilistic spaceas its inputs. Such models design a more black-box framework to handle the disjunction operation underthe strong closure assumption that might not be true in all cases. The third way of modeling disjunction is based on the De Morgans laws (Ren & Leskovec, 2020). Accordingto the De Morgans laws (DM), the disjunction is equivalent to the negation of the conjunction of the negationof the statements making up the disjunction, i.e., A B = (A B). For methods that can handle thenegation operator (detailed in the following paragraph), they model disjunction by using three negationoperations and one conjunction operation. DM conversion was explicitly probed in probabilistic (Ren &Leskovec, 2020), geometric (Zhang et al., 2021b), and fuzzy (Luus et al., 2021) processors. Finally, most fuzzy-logic (Arakelyan et al., 2021; Chen et al., 2022; Tang et al., 2022; Zhu et al., 2022; Baiet al., 2023c; Arakelyan et al., 2023; Wang et al., 2023a;d; Yin et al., 2023b; Demir et al., 2023) processorsemploy t-conorms, generalized versions of disjunctions in the continuous space (Section A.4). Moreexotic versions of neuro-symbolic disjunctions include element-wise summation of count-min sketches (Sunet al., 2020), feature logic operations (Lin et al., 2022; 2023), as well as performing a union in both embeddingand symbolic spaces (Tang et al., 2022; Xu et al., 2022) with fuzzy operators. Negation ().For negation operation, the goal is to model the complement set, i.e., the answers Aqto a query q = V? : r(v, V?) are the exact complement of the answers Aq to query q = V? : r(v, V?):Aq = V/Aq.Correspondingly, negation in SPARQL can be implemented with FILTER NOT EXISTS orMINUS clauses. For example (), a logical formula with negation field(DeepLearning, V) is equivalentto the SPARQL BGP {?s ?p ?v.FILTER NOT EXISTS {DeepLearning field ?v}} where {?s ?p ?v}models the universe set (1) of all facts that gets filtered by the triple pattern. Modeling the universe set (1) and its complement is the key problem when designing a negation operatorin neural query processors, e.g., an arbitrary real R or complex C space is unbounded such that 1 is not",
  "defined. For that reason, many neural processors do not support the negation operator. Still, there existseveral approaches to handle negation": "The first line of works (Bai et al., 2022; Amayuelas et al., 2022; Long et al., 2022) designs a purely neuralMLP-based negation operator over the query representation avoiding the universe set altogether. Similarly,a token of the negation operator can be included into the linearized query representation (Bai et al., 2023b)to be encoded with Transformer or recurrent network. A step aside from purely neural operators is taken byGNN-based processors (Wang et al., 2023e) that treat a negation edge as a new edge type during messagepassing over the query computation graph. The second line is customized to different embedding spaces and aims to simulate the calculation of theuniverse and complement in the embedding space, e.g., using geometric cones (Zhang et al., 2021b; He et al.,2023) or cylinders (Nguyen et al., 2023b), parameters are angles such that the space (and, hence, 1) isbounded to 2 and the complement is straight 2 . Probabilistic methods (Ren & Leskovec, 2020; Huanget al., 2022b; Yang et al., 2022a) naturally represent negation as an inverse of distribution parameters. Thirdly, fuzzy logic processors explicitly model the universe set 1 and the complement over the same realvalued logic space. For instance, LogicE (Luus et al., 2021), FuzzQE (Chen et al., 2022), WFRE (Wanget al., 2023d) restrict the query embedding space to the range d where each query q d is a vector.This way, the universe 1 is represented with a vector of all ones (in the embedding space 1d) and negation issimply 1q. TAR (Tang et al., 2022), GNN-QE (Zhu et al., 2022), FIT (Yin et al., 2023b) operate over fuzzysets where each entity has a corresponding scalar q in the bounded range. Therefore, the universe1 can still be a vector of all ones (in the entity space 1|E|) and negation is 1 q. ENeSy (Xu et al., 2022)defines the universe as the uniform distribution over the entity space with each element weighting|E| ( is",
  "a hyperparameter). CQDA (Arakelyan et al., 2023) employs a strict cosine fuzzy negation 1": "2(1 + cos(q))over scalar scores q.More exotic processors (Lin et al., 2022; 2023) employ feature logic for modelingnegation. We also note that the difference operator introduced in Liu et al. (2021) is in fact a commonintersection-negation (2in) query pattern used in all standard benchmarks (). Kleene Plus (+) and Property Paths.Kleene Plus is an operator that applies compositionally andrecursively to any regular expression (RegEx) that denotes one or more occurrence of the specified pattern.Regular expressions exhibit a direct connection to property paths in SPARQL. We defined a very basic regulargraph query in Definition A.7, here we generalize that further to property paths. To define property pathsmore formally, given a set of relations R and operators {+, , ?, !,, /, |}, a property path p can be obtainedfrom the recursive grammar p ::= r | p+ | p | p? | !p | p | p1/p2 | p1|p2 Here, r is any element of R, + isa Kleene Plus denoting one or more occurrences, is a Kleene Star denoting zero or more occurrences, ?denotes zero or one occurences, ! denotes negation of the relation or path, p traverses an edge of type p in theopposite direction, p1/p2 is a sequence of relations (corresponds to relation projection), and p1|p2 denotes analternative path of p1 or p2 (corresponds to a union operation). For example (), an expression withKleene plus knows(JohnDoe, V )+ can be represented as a SPARQL property path {JohnDoe knows+ ?v.}. Property paths are non-trivial to model for neural query processors due to compositionalilty and recursivenature. To the best of our knowledge, RotatE-Box (Adlakha et al., 2021) is the only geometric processorthat handles the Kleene Plus operator implementing the subset of operators {+, /, |}. RotatE-Box providestwo ways to handle Kleene Plus. The first method is to define a r+ embedding for each relation r R,note this is independent and separate from the regular relation embedding for r; another way is to use atrainable matrix to transform the relation embedding r to the r+ embedding. Note the two methods donot support Kleene Plus over paths. RotatE-Box also implements relation projection (as a rotation in thecomplex space) and union (with DeepSets or DNF) but does not support the intersection operator. We hypothesize that better support of the property paths vocabulary might be one of main focuses in futureneural query processors. Particularly for Kleene Plus, some unresolved issues include supporting idempotence((r+)+ = r+) and infinite union of sets (r+ = r|(r/r)|(r/r/r) . . . ). Filter.Filter is an operation that can be inserted in a SPARQL query. It takes any expression of booleantype as input and aims to filter the results based on the boolean value, i.e., only the results rendered True",
  ": Query operators (Filter, Count Aggregation, Optional), corresponding SPARQL basic graphpatterns (BGP), and their computation graphs": "under the expression will be returned. The boolean expression can thus be seen as a condition that theanswers to the query should follow. For the filter operator, we can do filter on values/literals/attributes,e.g., Filter(Vdate 2000 01 01&&Vdate 2000 12 31) means we would like to filter dates notin the year 2000; Filter(LANG(Vbook) = en) means we would like to filter books not written in English,Filter(?pages > 100) means returning the books that have more than 100 pages (as illustrated in ).To the best of our knowledge, there does not exist a reasoning model that claims to handle all possible Filters,which leaves room for future work on this direction. However, the first attempt towards handling filters istaken by LitCQD (Demir et al., 2023) that allows greater than, less than, and equal filtering operatorsover numerical values and treats them as conjunctive terms to the main logical query, e.g., lt(?pages, 100).Such terms are processed by a jointly trained regression model. We envision several possibilities to support filtering in neural query engines: (1) the simplest option usedby Thorne et al. (2021a;b) in natural language engines is to defer filtering to the postprocessing stage whenthe set of candidate nodes is identified and their attributes can be extracted by a lookup. (2) Filtering oftenimplies reasoning over literal values and numerical node attributes, that is, processors supporting continuousvalues (as described in .1) might be able to perform filtering in the latent space by attaching, forinstance, a parametric regressor decoder (.3) when predicting ?pages > 100. Aggregation.Aggregation is a set of operators in SPARQL queries including COUNT (return the numberof elements), MIN, MAX, SUM, AVG (return the minimum / maximum / sum / average value of all elements),SAMPLE (return any sample from the set). For example (), given a triple pattern {StephenKing wrote?book.}, the clause COUNT (?book) as ?n returns the total number of books written by StephenKing. Most aggregation operators require reasoning over sets of numerical values/literals. Such symbolic operationshave long been considered a challenge for neural models (Hendrycks et al., 2021). How to design a betterrepresentation for numerical values / literals requires remains an open question. Some neural query proces-sors (Ren et al., 2020; Ren & Leskovec, 2020; Zhang et al., 2021b; Zhu et al., 2022), however, have the meansto estimate the cardinality of the answer set (including predicted hard answers) that directly correspondsto the COUNT aggregation over the target projected variable (assumed to be an entity, not a literal). Forexample, GNN-QE (Zhu et al., 2022) returns a fuzzy set, i.e., a scalar likelihood value for each entity, that,after thresholding, has low mean absolute percentage error (MAPE) of the number of ground truth answers.Answer cardinality estimation is thus obtained as a byproduct of the neural query processor without tailoredpredictors. Alternatively, when models cannot predict the exact count, Spearmans rank correlation is asurrogate metric to evaluate the correlation between model predictions and the exact count. Spearmansrank correlation and MAPE of the number of ground truth answers are common metrics to evaluate theperformance of neural query processors and we elaborate on the metrics in .5. LitCQD (Demir",
  "roommate": ": Query patterns: path, tree-like, DAG, and cyclic queries. A DAG query has two branches fromthe intermediate variable, a cyclic query contains a 3-cycle. Existing neural query processors support pathand tree-like patterns. et al., 2023) extends the answer set to numerical literals and by default implements the AVG aggregation ofpredicted values when a query has multiple correct target entities. The target metrics are typical regressionmetrics like mean absolute error (MAE) or mean squared error (MSE). Optional and Solution Modifiers.SPARQL offers many features yet to be incorporated into neuralquery engines to extend their expressiveness. Some of those common features include the OPTIONAL clausethat is essentially a LEFT JOIN operator. For example (), given a triple pattern {King wrote ?book.}that returns books, the optional clause {King wrote ?book.OPTIONAL {King award ?a.}} enriches theanswer set with any existing awards received by King. Importantly, if there are no bindings to the optionalclause, the query still returns the values of ?book. In the querys computation graph, the optional clausecorresponds to the optional branch of a relation projection. A particular challenge for neural query processorsoperating on incomplete graphs is that the absence of the queried edge in the graph does not mean thatthere are no bindings instead, the edge might be missing and might be predicted during query processing. Solution modifiers, e.g., GROUP BY, ORDER BY, LIMIT, apply further postprocessing of projected (returned)results and are particularly important when projecting several variables in the query. So far, all existingneural query processors are tailored for only one return variable. We elaborate on this matter in .3. A General Note on Incompleteness.Finally, we would like to stress out that neural query enginesperforming all the described operators (Projection, Intersection, Union, Negation, Property Paths,Filters, Aggregations, Optionals, and Modifiers) assume the underlying graph is incomplete and queriesmight have some missing answers to be predicted, hence, all the operators should incorporate predicted hardanswers in addition to easy answers reachable by graph traversal as in symbolic graph databases. Evaluationof query performance with those operators in light of incompleteness is still an open challenge (we elaborateon that in .5), e.g., having an Optional clause, it might be unclear when there is no true answer(even predicted ones are in fact false) or a model is not able to predict them.",
  "Query Patterns": "Here, we introduce several types of query patterns commonly used in practical tasks and sort them in theincreasing order of complexity.Starting with chain-like Path queries known in the literature for years,we move to Tree-Structured queries (the main supported pattern in modern CLQA systems). Then, weoverview DAG and cyclic patterns which currently are not supported by any neural query answering systemand represent a solid avenue for future work. Path Queries.As introduced in Section A.3, previous literature starts with path queries (aka multi-hopqueries), where the goal is simply to go beyond one-hop queries such as q = V?.r(v, V?), where r R, v Vand V? represents the answer variable. As shown in and , there is no logical operator such asbranch intersection or union involved. Therefore, in order to answer such a query, we simply find or infer theneighbors of the entity v with relation r. Path queries are a natural extension of one-hop queries. Formally,we denote a path query as follows. qpath = V?.V1, . . . , Vk1 : r1(v, V1)r2(V1, V2) rk(Vk1, V?), where",
  "D": ": Answers to example tree-like, DAG, and cyclic query patterns given a toy graph.Note thedifference in the answer set to the tree-like and DAG queries in the DAG query, a variable v must havetwo outgoing edges from the same node. ri R, i [1, k], v V, Vi are all existentially quantified variables. We denote a k-hop path query if it hask atomic formulas. The query plan of a k-hop path query is a chain of length k starting from the anchorentity. For example (), a 2-hop path query is V?.v : student(Stanford, v) roommate(v, V?) whereStanford is the starting anchor node, v is a tail variable of the first projection student and at the same timeis the head variable of the second projection roommate thus forming a chain. As shown in the definition, in order to handle path queries, it is necessary to develop a method to handleexistential quantification and conjunction operators. Several query reasoning methods (Guu et al., 2015;Das et al., 2017) aim to answer the path queries with sequence models using either chainable KG embeddings(e.g., TransE) in Guu et al. (2015) or LSTM in Das et al. (2017). These methods initiated one of the firstefforts that use embeddings and neural methods to answer multi-hop path queries. We acknowledge theefforts in this domain but emphasize their limitations in terms of query expressiveness and, therefore, focusour attention in this work on more expressive query answering methods that operate on tree-like and morecomplex patterns. Tree-Structured Queries.Path queries only have one anchor entity and one answer variable.Suchqueries have limited expressiveness and are far away from real-world query complexity seen in the logs (Maly-shev et al., 2018) of real-world KGs like Wikidata. One direct extension to increase the expressiveness andcomplexity is to support tree-structured (tree-like) queries. Tree-like queries may have multiple anchor enti-ties, and different branches (from different anchors) will merge at the final single answer node, thus forminga tree structured query plan. Such merge can be achieved by intersection, union, or negation operators. Forexample, as shown in , the query plan of At what universities do the Turing Award winners in thefield of Deep Learning work? is not a path but a tree. Alternatively, the example in depicts a queryq = V?, v1, v2 : student(Stanford, v1) roommate(v1, V?) student(Stanford, v2) classmate(v2, V?) thatconsists of two branches of 2-hop path queries joined by the intersection operator at the end. Tree-like queries pose more challenges to the previous models that are only able to handle path (multi-hop)queries since a sequence model no longer applies to tree-structured execution plans with logical operators. Inlight of the challenges, neural and neuro-symbolic query processors (described in ) are designed toexecute more complex query patterns. These processors design neural set/logic operators and do a bottom-uptraversal of the tree up to the single root node. Arbitrary DAGs.Based on tree-structured queries, one can further increase the complexity of the querypattern to arbitrary directed acyclic graphs (DAGs). The key difference between the two types of queriesis that for DAG-structured queries, one variable node in the query plan (that represents a set of enti-ties) may be split and routed to different reasoning paths, while the number of branches/reasoning pathsin the query plan always decreases from the anchor nodes to the answer node.We show one examplein and in .Consider the tree-like query from the previous paragraph q1 = V?, v1, v2 :student(Stanford, v1) roommate(v1, V?) student(Stanford, v2) classmate(v2, V?) and the DAG queryq2 = V?, v : student(Stanford, v) roommate(v, V?) classmate(v, V?). The two queries search for V? whoare roommate and classmate with Stanford students. However, the answer sets of the two queries are differ-ent (illustrated in ). That is, the answer to the DAG query Vq2 = {A} is the subset of the answers to",
  "Projected Variables": "By projected variables we understand target query variables that have to be bound to particular graphelements such as entity, relation, or literals. For example, a query in q = V?.v : win(TuringAward, v)field(DeepLearning, v) university(v, V?) has one projected variable V? that can be bound to three answernodes in the graph, V? = {UofT, UdeM, NYU}. In the SPARQL literature (Hawke et al., 2013), the SELECTquery specifies which existentially quantified variables to project as final answers. The pairs of projectedvariables and answers form bindings as the result of the SELECT query. Generally, queries might have zero,one, or multiple projected variables, and we align our categorization with this notion. Examples of suchqueries and their possible answers are provided in . Currently, most neural query processors focus onthe setting where queries have only one answer variable the leaf node of the computation graph, as shownin (center). Zero Projected Variables.Queries with zero projected variables (Boolean queries) do not returnany bindings but rather probe the graph on the presence of a certain subgraph or relational patternwhere the answer is Boolean True or False. In SPARQL, the equivalent of zero-variable queries is theASK clause.Zero-variable queries might have all entities and relations instantiated with constants, e.g.,q = student(S, D) roommate(D, E) as in (left) is equivalent to the SPARQL query ASK WHERE {Sstudent D. D roommate E.}. The query probes whether a graph contains a particular subgraph (path)induced by the constants. Such a path exists, so the answer is q = {True}. Alternatively, zero-variable queries might have existentially quantified variables that are never projected (upto the cases where all subjects, predicates, or objects are variables). For example, a query q1 = v1, v2, v3 :student(v1, v2) roommate(v2, v3) probes whether there exist any nodes forming a relational path v1studentv2roommate v3. In a general case, a query q2 = p, s, o : p(s, o) asks if a graph contains at least one edge. We note that in the main considered setting with incomplete graphs and missing edges zero-variable queriesare still non-trivial to answer. Particularly, a subfield of neural subgraph matching (Rex et al., 2020; Huang",
  "et al., 2022a) implies having incomplete graphs. We hypothesize such approaches might be found useful forneural query processors to support answering zero-variable queries": "One Projected Variable.Queries with one projected variable return bindings for one (of possibly many)existentially quantified variable. In SPARQL, the projected variable is specified in the SELECT clause, e.g.,SELECT DISTINCT ?v in (center). Although SPARQL allows projecting variables from any part of aquery, most neural query engines covered in follow the task formulation of GQE (Hamilton et al.,2018) and allow the projected target variable to be only the leaf node of the query computation graph.This limitation is illustrated in (center) where the target variable ?v is the leaf node of the querygraph and has two bindings v = {A, E}. It is worth noting that existing neural query processors are designed to return a unique set of answers to theinput query, i.e., it corresponds to the SELECT DISTINCT clause in SPARQL. In contrast, the default SELECTreturns multisets with possible duplicates. For example, the same query in (center) without DISTINCTwould have bindings v = {A, A, E} as there exist two matching graph patterns ending in A. Implementingnon-DISTINCT query answering remains an open challenge. Most neural query processors have a notion of intermediate variables and model their distribution in the entityspace. For instance, having a defined distance function, geometric processors (Ren et al., 2020; Choudharyet al., 2021b) can find nearest entities as intermediate variables. Similarly, fuzzy-logic processors operatingon fuzzy sets (Tang et al., 2022; Zhu et al., 2022) already maintain a scalar distribution over all entities aftereach execution step. Finally, GNN-based (Daza & Cochez, 2020; Alivanistos et al., 2022) and Transformer-based (Liu et al., 2022) processors explicitly include intermediate variables as nodes in the query graph (ortokens in the query sequence) and can therefore decode their representations to the entity space. The maindrawback of all those methods is the lack of filtering mechanisms for the sets of intermediate variables afterthe leaf node has been identified. That is, in order to filter and project only those intermediate variablesthat lead to the final answer, some notion of backward pass is required. The first step in this direction istaken by QTO (Bai et al., 2023c) that runs the pruning backward pass after reaching the answer leaf node. Multiple Projected Variables.The most general and complex case for queries is to have multipleprojected variables as illustrated in (right). In SPARQL, all projected variables are specified inthe SELECT clause (with the possibility to project all variables in the query via SELECT *). In the logicalform, a query has several target variables q =?v1, ?v2, ?v : student(Stanford, ?v1) roommate(?v1, ?v) student(Stanford, ?v2) classmate(?v2, ?v) such that the output bindings are organized in tuples. For ex-ample, one possible answer tuple is {?v1 : F, ?v2 : F, ?v : A} denotes particular nodes (variable bindings) thatsatisfy the query pattern. As shown in the previous paragraph about one-variable queries, some neural query processors have the meansto keep track of the intermediate variables. However, none of them have the means to construct answer tupleswith variables bindings and it remains an open challenge how to incorporate multiple projected variables",
  "u": ": Standard query patterns with names, where p is projection, i is intersection, u is union, n isnegation. In a pattern, blue node represents a non-variable entity, grey node represents a variable node, andthe green node represents the answer node. In a typical training protocol, models are trained on 10 patterns(first and third rows) and evaluated on all patterns. In the hardest generalization case, models are onlytrained on 1p queries. Some datasets further modify the patterns with additional features like qualifiers ortemporal timestamps. into such processors. Furthermore, some common caveats to be taken into account include (1) dealing withunbound variables that often emerge, for example, in OPTIONAL queries covered in .1, where answertuples might contain an empty value ( or NULL) for some variables; (2) the growing complexity issue wherethe answer set might potentially be polynomially large depending on the number of projected variables.",
  "Evaluation Setup": "Multiple datasets have been proposed for evaluation of query reasoning models.Here we introduce thecommon setup for CLQA task. Given a knowledge graph G = (E, R, S), the standard practice is to splitG into a training graph Gtrain, a validation graph Gval and a test graph Gtest (simulating the unobservedcomplete graph G from ). The standard experiment protocol is to train a query reasoning modelonly on the training graph Gtrain, and evaluate the model on answering queries over the validation graphGval and the test graph Gtest. Given a query q, denote the answers of this query on training, validationand test graph as qtrain, qval and qtest. During evaluation, queries may have missing answers, e.g.,a validation query q may have answers qval that are not in qtrain, a test query q may have answersqtest that are not in qval. The overall goal of CLQA task is to find these missing answers. The detailsof typical training queries, training protocol, inference and evaluation metrics are introduced in .2,.3, .4, and .5, respectively.",
  "Query Types": "Thestandardsetofgraphqueriesusedinmanydatasetsincludes14types:1p/2p/3p/2i/3i/ip/pi/2u/up/2in/3in/inp/pni/pin where p denotes relation projection,i is intersec-tion, u is union, n is negation, and a number denotes the number of hops for projection queries or number ofbranches to be merged by a logical operator. illustrates common query patterns. For example, 3p isa chain-like query of three consecutive relation projections, 2i is an intersection of two relation projections,3in is an intersection of three relation projections where one of the branches contains negation, up is a unionof two relation projections followed by another projection. The original GQE by Hamilton et al. (2018)",
  "Training": "Query reasoning methods are trained on the given Gtrain with different objectives/losses and dif-ferentdatasets.Followingthestandardprotocol,methodsaretrainedon10querypatterns1p/2p/3p/2i/3i/2in/3in/inp/pni/pin and evaluated on all 14 patterns including generalization to unseenip/pi/2u/up patterns. That is, the training protocol assumes that models trained on atomic logical opera-tors would learn to compositionally generalize to patterns using several operators such as ip and pi queriesthat use both intersection and projection. We summarize different training objectives in . Most methods that learn a representation of thequeries and entities on the graph optimize a contrastive loss, i.e., minimizing the distance between therepresentation of a query q and its positive answers e while maximizing that between the representation ofa query and negative answers e. Various objectives include: (1) max-margin loss (first column in )with the goal that the distance of negative answers should be larger than that of positive answers at leastby the margin . Such loss is often of the form as the equation below.",
  "k log (dist(q, e) ),": "where k is the number of negative answers. Other methods (third column in ) that directly modela logit vector over all the nodes on the graph may optimize a cross entropy loss instead of a contrastive loss.Besides, methods such as the two variants of CQD (Arakelyan et al., 2021; 2023), QTO (Bai et al., 2023c),FIT (Yin et al., 2023b), and LitCQD (Demir et al., 2023) only optimize the link prediction loss since theydo not learn a representation of the query. Almost all the datasets including GQE (Hamilton et al., 2018), Q2B (Ren et al., 2020), BetaE (Ren &Leskovec, 2020), RegEx (Adlakha et al., 2021), BiQE (Kotnis et al., 2021), Query2Onto (Andresel et al.,2021), TAR (Tang et al., 2022), StarQE (Alivanistos et al., 2022), GNNQ (Pflueger et al., 2022), TeMP (Hu",
  "), GQE w hash (Wangetal.,2019),CGA(Maiet al., 2019), MPQE (Daza&Cochez,2020),HyPE(Choudharyetal.,2021b),Shv (Gebhart et al., 2023)": "Query2Box (Ren et al., 2020), BetaE (Ren & Leskovec,2020), RotatE-Box(Adlakha et al., 2021), ConE(Zhang et al., 2021b), NewLook (Liu et al., 2021), Q2BOnto (Andresel et al., 2021), PERM (Choudhary et al.,2021a), LogicE (Luus et al., 2021), MLPMix (Amayue-las et al., 2022), FuzzQE (Chen et al., 2022), FLEX (Lin et al., 2022), TFLEX (Lin et al., 2023), SMORE(Ren et al., 2022), LinE (Huang et al., 2022b), Gam-maE (Yang et al., 2022a), NMP-QEM (Long et al.,2022), ENeSy (Xu et al., 2022), RoMA (Xi et al., 2022),SignalE (Wang et al., 2022), Query2Geom (Sardinaet al., 2023), RoConE (He et al., 2023), CylE (Nguyen",
  "),GNNQ(Pfluegeretal.,": "2022), KGTrans (Liu et al., 2022),Query2Particles (Bai et al., 2022),EmQL (Sun et al., 2020), LMPNN(Wang et al., 2023e), StarQE (Ali-vanistos et al., 2022), NQE(Luoetal.,2023),CQDA(Arakelyanet al., 2023), SQE (Bai et al., 2023b) et al., 2022), TFLEX (Lin et al., 2023), InductiveQE (Galkin et al., 2022b), SQE (Bai et al., 2023b) providea set of training queries of given structures sampled from the Gtrain. The benefit is that during training,methods do not need to sample queries online. However, it often means that only a portion of informationis utilized from the Gtrain since exponentially more multi-hop queries exist on Gtrain and the dataset cannever pre-generate all offline.SMORE (Ren et al., 2022) proposes a bidirectional online query samplersuch that methods can directly do online sampling efficiently without the need to pre-generate a trainingset offline. Alternatively, methods that do not have parameterized ways to handle logical operations, e.g.,CQD (Arakelyan et al., 2021), only require one-hop edges to train the overall system.",
  "missing link": ": Inference Scenarios. In the Transductive case, training and inference graphs are the same andshare the same nodes (Einf = Etrain). Inductive cases can be split into superset (or semi-inductive) wherethe inference graph extends the training one (Etrain Einf, missing links cover both seen and unseen nodes)and disjoint (or fully-inductive) where the inference graph is disconnected (Etrain Einf = , missing linksare among unseen nodes). By Inference we understand testing scenarios on which a trained query answering model will be deployedand evaluated. Following the literature, we distinguish Transductive and Inductive inference (). Inthe transductive case, inference is performed on the graph with the same set of nodes and relation types asin training but with different edges. Any other scenario when either the number of nodes or relation typesof an inference graph is different from that of the training is deemed inductive. The inference scenario playsa major role in designing query answering models, that is, transductive models can learn a shallow entity",
  "Galkin et al. (2024)WikiTopics-CLQA dataset": "embedding matrix thanks to the fixed entity set whereas inductive models have to rely on other invariancesavailable in the underlying graph in order to generalize to unseen entity/relation types. We discuss manytransductive and inductive models in .2. Below, we categorize existing datasets from the Inferenceperspective. The overview of existing CLQA datasets is presented in through the lens of supportedquery operators, inference scenario, graph domain, and other features like types or qualifiers. Transductive Inference.Formally, given a training graph Gtrain = (Etrain, Rtrain, Strain), the transductiveinference graph Ginf 6 contains the same set of entities and relation types, that is, Etrain = Einf and Rtrain =Rinf, while the edge set on Gtrain is a subset of that on the inference graph Ginf, i.e., Strain Sinf. In thissetup, query answering is performed on the same nodes and edges seen during training. From the entity setperspective, the prediction pattern is seen-to-seen missing links are predicted between known entities. Traditionally, KG link prediction focused more on the transductive task. In CLQA, therefore, the majorityof existing datasets () follow the transductive scenario. Starting from simple triple-based graphswith fixed query patterns in GQE datasets (Hamilton et al., 2018), Query2Box datasets (Ren et al., 2020),and BetaE datasets (Ren & Leskovec, 2020) that became de-facto standard benchmarks for query answeringapproaches, newer datasets include regex queries (Adlakha et al., 2021), wider set of query patterns (Kotniset al., 2021; Wang et al., 2021; Bai et al., 2023b), entity type information (Tang et al., 2022), ontologicalaxioms (Andresel et al., 2021), hyper-relational queries with qualifiers (Alivanistos et al., 2022; Luo et al.,2023), temporal queries (Lin et al., 2023), very large graphs up to 100M nodes (Ren et al., 2022), hierarchical",
  "graphs (Huang et al., 2022b), queries with numerical literals (Demir et al., 2023; Bai et al., 2023a), or querieswith cycles and multiedges (Yin et al., 2023b;a; Cucumides et al., 2024)": "Inductive Inference.Formally, given a training graph Gtrain = (Etrain, Rtrain, Strain), the inductive infer-ence graph Ginf = (Einf, Rinf, Sinf) is different from the training graph in either the entity set or the relationset or both. The nature of this difference explains several subtypes of inductive inference. First, the set ofrelations might or might not be shared at inference time, that is, Rinf Rtrain or |Rinf \\ Rtrain| > 0. Mostof the literature on inductive link prediction (Teru et al., 2020; Zhu et al., 2021; Galkin et al., 2022a) in KGsassumes the set of relations is shared whereas the setup where new relations appear at inference time is stillhighly non-trivial (Huang et al., 2022a; Gao et al., 2023; Chen et al., 2023). On the other hand, the inference graph might be either a superset of the training graph after adding newnodes and edges, Etrain Einf, or a disjoint graph with completely new entities as a disconnected component,Einf Etrain = as illustrated in . From the node set perspective, the superset inductive inferencecase might contain both unseen-to-seen and unseen-to-unseen missing links whereas in the disjoint inferencegraph only unseen-to-unseen links are naturally appearing. In CLQA, inductive reasoning is still an emerging area as it has a direct impact on the space of possiblevariables V, constants C, and answers A that might now include entities unseen at training time. Severalmost recent works started to explore inductive query answering (). InductiveQE datasets (Galkinet al., 2022b) focus on the inductive superset case where a training graph can be extended with up to 500%new unseen nodes. Test queries start from unseen constants and answering therefore requires reasoning overboth seen and unseen nodes. Similarly, training queries can have many new correct answers when answeredagainst the extended inference graph. GNNQ datasets (Pflueger et al., 2022) focus on the disjoint inductiveinference case where constants, variables, and answers all belong to a new entity set. TeMP datasets (Huet al., 2022) focus on the disjoint inductive inference as well but offer to leverage an additional class hierarchyas a learnable invariant. That is, the set of classes at training and inference time does not change. Theonly suite of datasets for fully-inductive inference on both unseen entities and relations was introduced inUltraQuery (Galkin et al., 2024). Inductive inference is crucial to enable running models over updatable graphs without retraining. We con-jecture that inductive datasets and models are likely to be the major contribution area in the future work.",
  "Several metrics have been proposed to evaluate the performance of query reasoning models that can bebroadly classified into generalization, entailment, and query representation quality metrics": "Generalization Metrics.Since the aim of query reasoning models is to perform reasoning over massiveincomplete graphs, most metrics are designed to evaluate models generalization capabilities in discoveringmissing answers, i.e., qtest\\qval for a given test query q. As one of the first works in the field, GQE (Hamil-ton et al., 2018) proposes ROC-AUC and average percentile rank (APR). The idea is that for a given testquery q, GQE calculates a score for all its missing answers e qtest\\qval and the negatives e / qtest.The models performance is the ROC-AUC score and APR, where they rank a missing answer against atmost 1000 randomly sampled negatives of the same entity type. Besides GQE, GQE+hashing (Wang et al.,2019), CGA (Mai et al., 2019) and TractOR (Friedman & Van den Broeck, 2020) use the same evaluationmetrics. However, the above metrics do not reflect the real world setting where we often have orders of magnitudemore negatives than the missing answers. Instead of ROC-AUC or APR, Query2Box (Ren et al., 2020)proposes ranking-based metrics, such as mean reciprocal rank (MRR) and hits@k. Given a test query q, foreach missing answer e qtest\\qval, we rank it against all the other negatives e / qtest. Given theranking r, MRR is calculated as 1 r and hits@k is 1[r k]. This has been the most used metrics for the task.Note that the final rankings are computed only for the hard answers that require predicting at least onemissing link. Rankings for easy answers reachable by edge traversal are usually discarded.",
  "Applications": "The framework of complex query answering is applied in a variety of graph-conditioned machine learningtasks. For example, SE-KGE (Mai et al., 2020) applies GQE to answer geospatial queries conditioned on nu-merical {x, y} coordinates. The coordinates encoder fuses numerical representations with entity embeddingssuch that the prediction task is still entity ranking. In case-based reasoning, CBR-SUBG (Das et al., 2022) is a method for question answering over KGs basedon subgraph extraction and encoding. As a byproduct, CBR-SUBG is capable of answering conjunctivequeries with projections and intersections. However, due to a non-standard evaluation protocol and customsynthetic dataset, its performance cannot be directly compared to CLQA models. Similarly, Wang et al.(2023b) merge a Query2Box-like model with a pre-trained language model to improve question answeringperformance. LEGO (Ren et al., 2021) also applies CLQA models for KG question answering. The idea isto simultaneously parse a natural language question as a query step and execute the step in the latent spacewith CLQA models. LogiRec (Wu et al., 2022) frames product recommendation as a complex logical query such that sourceproducts are root nodes, combinations of multiple products form intersections, and non-similar products tobe filtered out form negations. LogiRec employs BetaE as a query engine. Similarly, Syed et al. (2022) designan explainable recommender system based on Query2Box. Given a logical query, they first use Query2Boxto generate a set of candidates and rerank them using neural collaborate filtering (He et al., 2017).",
  "Summary and Future Opportunities": "We proposed a deep and detailed review of Complex logical query answering (CLQA) methods based on thenew taxonomy in categorizing the existing approaches along three main areas: Graphs, Modeling,and Queries with their respective sub-areas. Going forward, there is still much room to unlock the full powerof CLQA by addressing numerous open challenges. Adhering to the taxonomy, we summarize the challengesin three main areas: Graphs, Modeling, and Queries.",
  "Along the Graph branch:": "Modality: Future systems need to handle a richer variety of graph structures and data types. For ex-ample, beyond conventional knowledge graphs that contain only triples of subjectpredicateobject,we want to support hyper-relational graphs where edges can connect more than two nodes or haveadditional qualifiers. We also aim to include hypergraphs, which represent complex relationshipswith edges that join multiple entities simultaneously.In addition, we envision multimodal datasources where graph data is combined with textual descriptions, images, or other media. For in-stance, a system might incorporate a knowledge graph of people and places along with associatedimages and text captions, integrating all these modalities seamlessly. Reasoning Domain: Another goal is to facilitate logical reasoning and neural query answeringover dynamic and continuous information within graphs. Many real-world knowledge graphs includetemporal facts (such as events happening at specific times) and literal values (such as numericalmeasurements or textual strings). Because a large portion of a graphs information is stored as theseliteral valueslike a numerical temperature reading or a textual labela robust system should beable to answer queries that involve these literals. For example, the system might reason over a time-series of events to answer a query about when a particular relationship held, or interpret textualdata attached to nodes to respond to a query that references an entitys description. Background Semantics: We want to incorporate complex background knowledge encoded byformal semantics and axioms. This means supporting not just direct entity-to-entity relationshipsbut also higher-order relationships between classes of entities and their hierarchical structures. Forexample, the system should enable neural reasoning that respects the rules of description logics orsubsets of the Web Ontology Language (OWL). In practice, this might allow the model to deducethat if an entity belongs to a particular subclass (like Cat within Mammal), it should inheritrelationships and constraints from its superclass. A concrete example might be reasoning that ifa graph knows Fluffy is a Cat and that All Cats are Mammals, then the system can concludeFluffy is a Mammal. Supporting these complex axioms will allow more powerful and semanticallyrich reasoning over graph data.",
  "Encoder:": "We seek inductive encoders that can interpret new relationships in a knowledge graph even if theywere unseen during training. For example, if a graphs schema is updated with a new relation typelike collaboratesWith, the encoder should handle queries involving this relation without requiringretraining.This capability supports two key goals: (1) Updatability: The neural database canquickly adapt to changes in the graphs schema or content. For instance, if new nodes and edgesappear in a KG about scientific publications, the system can incorporate them on the fly.(2)Pretrain-Finetune Strategy: A model could be pre-trained on general graph structures and thenfine-tuned to answer queries on a new, custom graph with a unique set of entity and relation types.Ultimately, we aspire to create a single CLQA model that can respond to queries about any unseenKG, regardless of the vocabulary of entities and relations it uses.",
  "In the Queries branch:": "Operators: We aim to develop neural methods that can handle a wider variety of complex queryoperators found in declarative graph query languages. For example, a system should support theKleene plus and star operators, which let queries match repeated patterns in a path (like allancestors or paths of any length connecting two nodes). It should also manage property paths,where queries involve sequences of edges that fulfill certain property constraints (such as entitiesconnected by a path of type friendOf repeated one or more times). In addition, the system shouldhandle filters that restrict results based on conditions, for example retrieving people from a graphwho are over 30 years old and live in a specific city. Patterns: We want query answering systems that can handle patterns beyond simple tree struc-tures. For instance, a query might involve a directed acyclic graph (DAG) pattern such as Findtopics that are prerequisites for both X and Y courses or a cyclic graph pattern like Identify peoplewho follow each other on a social network. This complexity means the system must interpret andrespond to queries where relationships loop back or branch in multiple directions, replicating thecomplexity of many real-world graph queries. Projected Variables: Instead of only returning a single final answer entity (like the name of a city),queries may require returning intermediate variables or entire tuples of variables. For example, a usermight ask Which actors and their respective directors collaborated on a film in 2020? The answerrequires projecting multiple variablesboth the actor and the directorfor each case. Similarly, aquery might require returning a relationship as a variable, such as identifying not just the entitiesbut also the nature of their relationship. The formalism presented in this survey supports this, andsome papers present preliminary results, but none are evaluated on the quality of bindings to theseintermediate variables. Expressiveness: We want systems to answer queries that go beyond simple fragments like EPFO(existential positive first-order) and EFO (existential first-order). This means supporting the unionof conjunctive queries with negations (UCQ and UCQneg) and moving toward the expressivenessfound in full database query languages. For instance, the system should handle a query like Find allpersons who are not authors of any paper in 2021 but who co-authored a paper in 2020. This queryincludes negation (not authors) combined with conjunction (and who co-authored), illustratingadvanced expressiveness.",
  "In Datasets and Evaluation:": "We require larger, more varied benchmark datasets that reflect real-world complexity. For example,a new benchmark might include not just classic triple-based knowledge graphs but also hyper-relational graphs or graphs that combine textual and visual data. These benchmarks should presentqueries that use more expressive semanticssuch as temporal conditions (Which actors starred inmovies after 2010?) or numeric filters (Cities with population over one million?). They shouldalso incorporate a wider range of query operators (like union or intersection) and complex querypatterns (such as nested queries). For instance, a robust benchmark might ask the system to handlea query like: Find images of all institutions connected to entities that are both scientists andauthors requiring the system to interpret graph modalities, apply multiple operators, and returnresults from multiple data sources. Finally, we would need more insight into how difficult certainevaluation datasets really are. Currently, we cannot say whether the queries in one dataset areintrinsically harder to answer or whether it is just an artifact of how systems are designed thatscores are different.This would also lead to more insight into what query answering system isexpected to be better for what datasets and queries. Current evaluation methods often focus narrowly on whether a system predicts the correct hard an-swers. We need a more comprehensive evaluation framework that measures a systems performanceacross the entire query answering process. For example, this framework might include metrics thatassess the efficiency of the query plan, how well the system handles uncertainty in queries, how itranks potential answers by relevance, or how it deals with partial information. Concretely, beyondjust checking if the system finds the correct city for a query like What is the capital of France? wemight evaluate how it handles ambiguous queries (What is the capital of a big European countrynear Germany?), or how well it explains its reasoning steps. This means developing new met-rics that reflect accuracy in retrieving possible answers (hard answers), correctness in reasoningabout complex semantics, and clarity in explaining results, ensuring a more principled and nuancedevaluation of query answering systems. Overall CLQA has demonstrated promise across a wide range of domains.In this survey we provide adetailed taxonomy to study CLQA. We envision CLQA to be among the core tasks and capabilities of thenext generation graph databases. We thank Pavel Klinov (Stardog), Matthias Fey (Kumo.AI), Qian Li (Stanford) and Keshav Santhanam(Stanford) for providing valuable feedback on the draft. In addition, Michael wants to thank several peoplefrom the L&R group from the VU Amsterdam, including Daniel Daza, Dimitrios Alivanistos, Frank vanHarmelen, Ruud van Bakel, Yannick Brunink, as well as Teodor Aleksiev (BSc. VU), Max Zwager (MSc.VU), and Patrick Koopmann (University Dresden) for discussions on various aspects of the work. Michael waspartially funded by the Graph-Massivizer project, part of the Horizon Europe programme of the EuropeanUnion (grant 101093202); his research was further made possible by a gift from Accenture, LLP.",
  "Erik Arakelyan, Daniel Daza, Pasquale Minervini, and Michael Cochez. Complex query answering withneural link predictors. In International Conference on Learning Representations, 2021": "Erik Arakelyan, Pasquale Minervini, , Daniel Daza, and Michael Cochez Isabelle Augenstein. Adaptingneural link predictors for complex query answering. In Thirty-seventh Conference on Neural InformationProcessing Systems, 2023. URL Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. Learning toretrieve reasoning paths over wikipedia graph for question answering. In International Conference onLearning Representations, 2020. URL Franz Baader, Diego Calvanese, Deborah McGuinness, Peter Patel-Schneider, Daniele Nardi, et al. Thedescription logic handbook: Theory, implementation and applications. Cambridge university press, 2003.",
  "Samy Badreddine, Artur dAvila Garcez, Luciano Serafini, and Michael Spranger. Logic tensor networks.Artificial Intelligence, 303:103649, 2022": "Jinheon Baek, Alham Fikri Aji, and Amir Saffari. Knowledge-augmented language model prompting forzero-shot knowledge graph question answering. In Proceedings of the 1st Workshop on Natural LanguageReasoning and Structured Explanations (NLRSE), pp. 78106, Toronto, Canada, 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.nlrse-1.7. URL Jiaxin Bai, Zihao Wang, Hongming Zhang, and Yangqiu Song. Query2Particles: Knowledge graph reasoningwith particle embeddings. In Findings of the Association for Computational Linguistics: NAACL 2022.Association for Computational Linguistics, 2022.",
  "Pablo Barcel, Mikhail Galkin, Christopher Morris, and Miguel Romero Orth. Weisfeiler and leman gorelational. In The First Learning on Graphs Conference, 2022. URL": "Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, MateuszMalinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductivebiases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collaborativelycreated graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMODinternational conference on Management of data, pp. 12471250, 2008. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translatingembeddings for modeling multi-relational data. Advances in neural information processing systems, 26,2013.",
  "Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velikovi. Geometric deep learning: Grids,groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021": "Davide Buffelli, Pietro Li, and Fabio Vandin. SizeShiftReg: a regularization method for improving size-generalization in graph neural networks. In Advances in Neural Information Processing Systems, 2022. Nilesh Chakraborty, Denis Lukovnikov, Gaurav Maheshwari, Priyansh Trivedi, Jens Lehmann, and AsjaFischer. Introduction to neural network-based question answering over knowledge graphs. WIREs DataMining and Knowledge Discovery, 11(3):e1389, 2021.doi: Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher R, and Kevin Murphy. Machine learning ongraphs: A model and comprehensive taxonomy. Journal of Machine Learning Research, 23(89):164, 2022.URL Mingyang Chen, Wen Zhang, Yuxia Geng, Zezhong Xu, Jeff Z Pan, and Huajun Chen. Generalizing tounseen elements: A survey on knowledge extrapolation for knowledge graphs. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, 2023. doi: 10.24963/ijcai.2023/737. URL",
  "Graham Cormode and Shan Muthukrishnan. An improved data stream summary: the count-min sketch andits applications. Journal of Algorithms, 55(1):5875, 2005": "Tamara Cucumides, Daniel Daza, Pablo Barcel, Michael Cochez, Floris Geerts, Juan L Reutter, and MiguelRomero. UnRavL: A neuro-symbolic framework for answering graph pattern queries in knowledge graphs.The Third Learning on Graphs Conference, 2024. URL Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew McCallum.Chains of reasoning overentities, relations, and text using recurrent neural networks. In Proceedings of the 15th Conference of theEuropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp. 132141,Valencia, Spain, 2017. Association for Computational Linguistics.URL Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia,and Andrew McCallum. Knowledge base question answering by case-based reasoning over subgraphs. InInternational Conference on Machine Learning, ICML 2022, Proceedings of Machine Learning Research.PMLR, 2022.",
  "Emma Flint.Announcing alexa entities (beta):Create more intelligent and engaging skills witheasy access to alexas knowledge, 2021.URL": "Tal Friedman and Guy Van den Broeck. Symbolic querying of vector spaces: Probabilistic databases meetsrelational embeddings. In Conference on Uncertainty in Artificial Intelligence, pp. 12681277. PMLR,2020. Luis Galrraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. AMIE: Association rule miningunder incomplete evidence in ontological knowledge bases. In Proceedings of the International World WideWeb Conference (WWW), 2013. Mikhail Galkin, Priyansh Trivedi, Gaurav Maheshwari, Ricardo Usbeck, and Jens Lehmann. Message passingfor hyper-relational knowledge graphs. In Proceedings of the 2020 Conference on Empirical Methods inNatural Language Processing, EMNLP 2020, pp. 73467359. Association for Computational Linguistics,2020.",
  "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural messagepassing for quantum chemistry. In ICML, pp. 12631272, 2017": "Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 855864, 2016. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Acceler-ating large-scale inference with anisotropic vector quantization. In International Conference on MachineLearning, 2020. URL",
  "Vinh Thinh Ho, Daria Stepanova, Dragan Milchevski, Jannik Strtgen, and Gerhard Weikum. Enhancingknowledge bases with quantity facts. In WWW 22: The ACM Web Conference 2022, pp. 893901. ACM,2022": "Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia dAmato, Gerard de Melo, Claudio Gutierrez, SabrinaKirrane, Jos Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. Knowledge graphs. ACMComputing Surveys (CSUR), 54(4):137, 2021. Zhiwei Hu, Vctor Gutirrez-Basulto, Zhiliang Xiang, Xiaoli Li, Ru Li, and Jeff Z Pan.Type-awareembeddings for multi-hop reasoning over knowledge graphs.In Proceedings of the Thirty-First In-ternational Joint Conference on Artificial Intelligence, IJCAI-22, pp. 30783084, 2022.URL Qian Huang, Hongyu Ren, and Jure Leskovec. Few-shot relational reasoning via connection subgraph pre-training. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances inNeural Information Processing Systems, 2022a. URL Xingyue Huang, Miguel Romero Orth, smail lkan Ceylan, and Pablo Barcel. A theory of link predictionvia relational weisfeiler-leman. In Thirty-seventh Conference on Neural Information Processing Systems,2023. Zijian Huang, Meng-Fen Chiang, and Wang-Chien Lee.LinE: Logical query reasoning over hierarchicalknowledge graphs. In Aidong Zhang and Huzefa Rangwala (eds.), KDD 22: The 28th ACM SIGKDDConference on Knowledge Discovery and Data Mining, pp. 615625. ACM, 2022b. Ihab F Ilyas, Theodoros Rekatsinas, Vishnu Konda, Jeffrey Pound, Xiaoguang Qi, and Mohamed Soliman.Saga: A platform for continuous construction and serving of knowledge at scale. In Proceedings of the2022 International Conference on Management of Data, pp. 22592272, 2022.",
  "Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with rewardshaping. In Empirical Methods in Natural Language Processing (EMNLP), 2018": "Xueyuan Lin, Gengxian Zhou, Tianyi Hu, Li Ningyuan, Mingzhi Sun, Haoran Luo, et al. FLEX: Feature-logic embedding framework for CompleX knowledge graph reasoning. arXiv preprint arXiv:2205.11039,2022. Xueyuan Lin, Chengjin Xu, Fenglong Su, Gengxian Zhou, Tianyi Hu, Ningyuan Li, Mingzhi Sun, HaoranLuo, et al. TFLEX: Temporal feature-logic embedding framework for complex reasoning over temporalknowledge graph. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL Lihui Liu, Boxin Du, Heng Ji, ChengXiang Zhai, and Hanghang Tong. Neural-answering logical queries onknowledge graphs. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & DataMining, pp. 10871097, 2021. Xiao Liu, Shiyu Zhao, Kai Su, Yukuo Cen, Jiezhong Qiu, Mengdi Zhang, Wei Wu, Yuxiao Dong, andJie Tang. Mask and reason: Pre-training knowledge graph transformers for complex logical queries. InProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2022. Xiao Long, Liansheng Zhuang, Aodi Li, Shafei Wang, and Houqiang Li. Neural-based mixture probabilisticquery embedding for answering FOL queries on knowledge graphs. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Processing, EMNLP 2022, pp. 30013013. Association forComputational Linguistics, 2022. Haoran Luo, Haihong E, Yuhao Yang, Gengxian Zhou, Yikai Guo, Tianyu Yao, Zichen Tang, XueyuanLin, and Kaiyang Wan. NQE: N-ary query embedding for complex query answering over hyper-relationalknowledge graphs. In AAAI 2023, 2023.",
  "Zhezheng Luo, Jiayuan Mao, Joshua B. Tenenbaum, and Leslie Pack Kaelbling. On the expressiveness andlearning of relational neural networks on hypergraphs, 2022. URL": "Francois Luus, Prithviraj Sen, Pavan Kapanipathi, Ryan Riegel, Ndivhuwo Makondo, Thabang Lebese, andAlexander Gray. Logic embeddings for complex query answering. arXiv preprint arXiv:2103.00418, 2021. Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. Contextual graph attentionfor answering logical queries over incomplete knowledge graphs. In Proceedings of the 10th InternationalConference on Knowledge Capture, pp. 171178, 2019. Gengchen Mai, Krzysztof Janowicz, Ling Cai, Rui Zhu, Blake Regalia, Bo Yan, Meilin Shi, and Ni Lao. SE-KGE: A location-aware knowledge graph embedding model for geographic question answering and spatialsemantic lifting. Transactions in GIS, 24(3):623655, 2020. Stanislav Malyshev, Markus Krtzsch, Larry Gonzlez, Julius Gonsior, and Adrian Bielefeldt. Getting themost out of wikidata: Semantic technology usage in Wikipedias knowledge graph. In Proceedings of the17th International Semantic Web Conference (ISWC18), volume 11137 of LNCS, pp. 376394. Springer,2018. Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequentiallearning problem. In Psychology of learning and motivation, volume 24, pp. 109165. Elsevier, 1989.",
  "Eduardo Mizraji. Vector logic: a natural algebraic representation of the fundamental logical gates. Journalof Logic and Computation, 18(1):97121, 2008": "Boris Motik, Peter F Patel-Schneider, Bijan Parsia, Conrad Bock, Achille Fokoue, Peter Haase, RinkeHoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sattler, et al. OWL 2 web ontology language: Structuralspecification and functional-style syntax. W3C recommendation, 27(65):159, 2009. Chau Nguyen, Tim French, Wei Liu, and Michael Stewart. SConE: Simplified cone embeddings with symbolicoperators for complex logical queries. In Findings of the Association for Computational Linguistics: ACL2023, pp. 1193111946, 2023a. URL Chau Duc Minh Nguyen, Tim French, Wei Liu, and Michael Stewart. CylE: Cylinder embeddings for multi-hop reasoning over knowledge graphs. In Proceedings of the 17th Conference of the European Chapter ofthe Association for Computational Linguistics, pp. 17361751. Association for Computational Linguistics,2023b. URL",
  "Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector spaceusing box embeddings. In International Conference on Learning Representations, 2020": "Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, JureLeskovec, and Denny Zhou. Lego: Latent execution-guided reasoning for multi-hop question answering onknowledge graphs. In International Conference on Machine Learning, pp. 89598970. PMLR, 2021. Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec, and Dale Schuurmans.SMORE: Knowledge graph completion and multi-hop reasoning in massive knowledge graphs. Proceedingsof the 28th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 2022.",
  "Mark B Ring. CHILD: A first step towards continual learning. Learning to learn, pp. 261292, 1998": "Jeffrey Sardina, Callie Sardina, John D Kelleher, and Declan OSullivan. Analysis of attention mechanismsin box-embedding systems. In Artificial Intelligence and Cognitive Science: 30th Irish Conference, AICS2022, pp. 6880. Springer, 2023. Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowl-edge graphs using knowledge base embeddings. In Proceedings of the 58th Annual Meeting of the Associa-tion for Computational Linguistics, pp. 44984507, Online, 2020. Association for Computational Linguis-tics. doi: 10.18653/v1/2020.acl-main.412. URL",
  "Komal Teru, Etienne Denis, and Will Hamilton. Inductive relation prediction by subgraph reasoning. InInternational Conference on Machine Learning, pp. 94489457. PMLR, 2020": "James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy.Database reasoning over text. In Proceedings of the 59th Annual Meeting of the Association for Computa-tional Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume1: Long Papers), pp. 30913104. Association for Computational Linguistics, 2021a. James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, and Alon Halevy. Fromnatural language processing to neural databases. In Proceedings of the VLDB Endowment, volume 14, pp.10331039. VLDB Endowment, 2021b.",
  "Denny Vrandecic and Markus Krtzsch. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):7885, 2014": "Dingmin Wang, Yeyuan Chen, and Bernardo Cuenca Grau. Efficient embeddings of logical variables forquery answering over incomplete knowledge graphs. In Proceedings of the AAAI Conference on ArtificialIntelligence. AAAI Press, 2023a. Kai Wang, Chunhong Zhang, Jibin Yu, and Qi Sun. Signal embeddings for complex logical reasoning inknowledge graphs. In International Conference on Knowledge Science, Engineering and Management, pp.255267. Springer, 2022. Meng Wang, Haomin Shen, Sen Wang, Lina Yao, Yinlin Jiang, Guilin Qi, and Yang Chen. Learning to hashfor efficient search over incomplete knowledge graphs. In 2019 IEEE International Conference on DataMining (ICDM), 2019. Siyuan Wang, Zhongyu Wei, Jiarong Xu, Taishan Li, and Zhihao Fan. Unifying structure reasoning andlanguage model pre-training for complex reasoning. IEEE/ACM Trans. Audio, Speech and Lang. Proc.,pp. 15861595, 2023b. Yanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec. Vqa-gnn: Reasoningwith multimodal knowledge via graph neural networks for visual question answering. In 2023 IEEE/CVFInternational Conference on Computer Vision (ICCV), pp. 2152521535. IEEE Computer Society, 2023c.URL Zihao Wang, Hang Yin, and Yangqiu Song. Benchmarking the combinatorial generalizability of complexquery answering on knowledge graphs. In Thirty-fifth Conference on Neural Information Processing Sys-tems Datasets and Benchmarks Track (Round 2), 2021. Zihao Wang, Weizhi Fei, Hang Yin, Yangqiu Song, Ginny Y. Wong, and Simon See. Wasserstein-Fisher-Rao embedding: Logical query embeddings with local comparison and global transport.In Findingsof the Association for Computational Linguistics: ACL 2023, pp. 1367913696, 2023d.URL Zihao Wang, Yangqiu Song, Ginny Y. Wong, and Simon See. Logical message passing networks with one-hop inference on atomic formulas. In The Eleventh International Conference on Learning Representations,ICLR, 2023e. URL Boris Weisfeiler and Andrey Leman. The reduction of a graph to canonical form and the algebra whichappears therein. Nauchno-Technicheskaya Informatsia, 2(9):1216, 1968. English translation by G. Ryabovis available at Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. Knowledgebase completion via search-based question answering. In 23rd International World Wide Web Conference,WWW 14, pp. 515526. ACM, 2014.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? InInternational Conference on Learning Representations, 2019": "Yao Xu, Shizhu He, Li Cai, Kang Liu, and Jun Zhao. Prediction and calibration: Complex reasoning overknowledge graph with bi-directional directed acyclic graph neural network. In Findings of the Associationfor Computational Linguistics: ACL 2023, pp. 71897198, 2023a.URL Yao Xu, Shizhu He, Cunguang Wang, Li Cai, Kang Liu, and Jun Zhao.Query2Triple: Unified queryencoding for answering diverse complex queries over knowledge graphs. In Findings of the Association forComputational Linguistics: EMNLP 2023, pp. 1136911382, 2023b. URL",
  "Zezhong Xu, Wen Zhang, Peng Ye, Hui Chen, and Huajun Chen. Neural-symbolic entangled framework forcomplex query answering. In Advances in Neural Information Processing Systems, 2022": "Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and relationsfor learning and inference in knowledge bases. In International Conference on Learning Representations,2015. Dong Yang, Peijun Qing, Yang Li, Haonan Lu, and Xiaodong Lin. GammaE: Gamma embeddings for logicalqueries on knowledge graphs. In Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2022, pp. 745760. Association for Computational Linguistics, 2022a.",
  "Haotong Yang, Zhouchen Lin, and Muhan Zhang.Rethinking knowledge graph evaluation under theopen-world assumption. In Advances in Neural Information Processing Systems, 2022b. URL": "Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec.QA-GNN: Rea-soning with language models and knowledge graphs for question answering. In Proceedings of the 2021Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, pp. 535546, Online, 2021. Association for Computational Linguistics.doi:10.18653/v1/2021.naacl-main.45. URL Gilad Yehudai, Ethan Fetaya, Eli A. Meirom, Gal Chechik, and Haggai Maron. From local structures to sizegeneralization in graph neural networks. In Proceedings of the 38th International Conference on MachineLearning, 2021.",
  "Yangze Zhou, Gitta Kutyniok, and Bruno Ribeiro.OOD link prediction generalization capabilities ofmessage-passing GNNs in larger test graphs.In Advances in Neural Information Processing Systems,2022": "Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: Ageneral graph neural network framework for link prediction. Advances in Neural Information ProcessingSystems, 34:2947629490, 2021. Zhaocheng Zhu, Mikhail Galkin, Zuobai Zhang, and Jian Tang. Neural-symbolic models for logical querieson knowledge graphs. In International Conference on Machine Learning, ICML 2022, Proceedings ofMachine Learning Research. PMLR, 2022.",
  "Hyper-relational KGs generalize triple KGs by allowing edges to have relation-entity qualifiers. We definethem as follows:": "Definition A.1 (Hyper-relational Knowledge Graph - derived from Alivanistos et al. (2022))With E and R defined as in Definition 2.2, let Q = 2(RE),7 we define a hyper-relational knowledge graphG = (E, R, S). In this case, the edge set S (E R E Q) consists of qualified statements, where eachstatement s = (es, r, eo, qp) includes qp that represent contextual information for the main relation r. Thisset qp = {q1, . . .} = {(qr1, qe1), (qr2, qe2) . . .} R E is the set of qualifier pairs, where {qr1, qr2, . . .} arethe qualifier relations and {qe1, qe2, . . .} the qualifier entities. We note that if E and R are finite sets, then also Q is finite and, there are only a finite number of (r, qp)combinations possible. As a consequence, we find that with these conditions, and by defining a canonicalordering over Q, we can represent the hyper-relational graph using first order logic by coining a new predicaterqp for each combination. The statement from the definition can then be written as rqp(es, eo). For example, one statement on a hyper-relational KG in () is (Hinton, education, Cambridge,{(degree, Bachelor)}).The qualifier pair {(degree, Bachelor)} provides additional context to themain triple and helps to distinguish it from other education facts. If the conditions mentioned above aremet, we can write this statement as education{(degree:Bachelor)}(Hinton, Cambridge).",
  ":RDF-star andHyper-relationalKnowl-edge Graphs": "This concept is closely related (albeit with different theoretical foundations) tothe RDF-star format introduced in Hartig et al. (2022). RDF-star explicitlyseparates metadata (context) from data (statements). RDF-star allows us touse a triple as the subject or object of another triple, which gives the sameexpressive power than hyper-relational graphs. A core difference, affecting theway data is modeled, is that the identity of a triple is solely defined by its con-stituents. This means that if a triple is used in two places, it is the same triple,which makes making complex statements about the triple more complicated(). For example, assume we want to express that Max Welling did hisBSc thesis on flat universe in Utrecht and his PhD degree on gravityat the same university. With a hyper-relational graph we can make two edges.Each edge would state that Max Welling studied in Utrecht, but each wouldhave two qualifier pairs with the corresponding topic and institute information.In RDF-star, we cannot model it with the same triple as subject of anothertriples, because RDF-star has no way to pair-up the topic and degree level,and will instead think that the main triple has four pieces of information con-nected to it. The hyper-relational graphs introduced above do not have suchissues. RDF-star further extends the values allowed in the object and qualifiervalue position to include literals, which we will discuss below in Definition A.3. Definition A.2 (Hypergraph Knowledge Graph) With E and R defined as in Definition 2.2, a hy-pergraph KG is a graph G = (E, R, S) where statements S (R 2E) are hyperedges. Such a hyperedges = (r, e) = (r, {e1, . . . , ek}) has one relation type r and links the k entities together, where the order of theseentities is not significant. k, the size of e is called the arity of the hyperedge.",
  "An example triple with a literal object () is (Cambridge, established, 1209).In (Hinton,education, Cambridge),educationisarelation,whereasin(Cambridge, established, 1209),established is an attribute": "The knowledge graph definitions above are not exhaustive. It is possible to create graphs with other prop-erties. Examples include graphs with undirected edges, without edge labels, with time characteristics, withprobabilistic or fuzzy relations, etc. Besides, it is also possible to have graphs or edges with combined char-acteristics. One could, for instance, define a hyperedge with qualifying information. Because of this plethoraof options, we decided to limit ourselves to the options above. In the next section we introduce how to querythese graphs.",
  "A.2Basic Approximate Graph Query Answering": "Until now, we have assumed that our KG is complete, i.e., it is possible to exactly answer the queries.However, we are interested in the setting where we do not have the complete graph. The situation can bedescribed as follows: Given a knowledge graph G (subset of a complete, but not observable graph G) and abasic graph query Q. Basic Approximate Graph Query Answering is the task of answering Q, without havingaccess to G. Depending on the setting, an approach to this could have access to G, or to a set of example(query, answer) pairs, which can be used to produce the answers (see also .3). In the example from 8Both RDF and RDF-star also allow blank nodes used to indicate entities without a specified identity in the subject andobject position Brickley et al. (2014) . Besides, they also have support for named graphs (and in some cases for quadruples).We do not support these explicitly in our formalism, but all of these can be modeled using hyper-relational graphs.",
  "Approximate Query Answering Systems provide a score R for every possible mapping9. Hence, the answerprovided by these systems is a function from mappings (i.e., a ) to their corresponding score in R": "Definition A.4 (Basic Approximate Graph Query Answering) Given a knowledge graph G (sub-graph of a complete, but not observable knowledge graph G), a basic graph query Q, and the scoring domainR, a basic approximate graph query answer to the query Q is a function f which maps every possible mapping( : VarQ Con) to R.",
  "UdeMHinton33LeCunNYU32. . .. . .. .": "The objective is to make it such that the correct answers according to thegraph G get a better score (are ranked higher, have a higher probability,have a higher truth value) than those which are not correct answers.However, it is not guaranteed that answers which are in the observablegraph G will get a high score. For our example, a possible mapping is visualized in . Each rowin the table corresponds to one mapping . Ideally, all correct mappingsshould be ranked on top, and all others below. However, in this examplewe see several correct answers ranked high, but also two which are wrong.",
  "A.3Graph Query Answering": "In the previous sections, we introduced the basic graph query and how it can be answered either exactly,or in an approximate graph querying setting. However, there is variation in what types of queries methodscan answer; some only support a subset of our basic graph queries, while others support more complicatedqueries. In this section we will again focus on exact (non-approximate) query answering and look at somepossible restrictions and then at extensions. Definition A.5 (Graph Query Answering) Given a knowledge graph G, a query formalism, and a graphquery Q according to that formalism. An answer to the query is any mapping : VarQ Con such that therequirements of the query formalism are met. The set of all answers is the set of all such mappings. For our basic graph queries introduced in Definition 2.4, the query formalism sets requirements as to whatedges must and must not exist in the graph (Definition 2.5). In that context we already mentioned conjunctivequeries, which exist either with (CQneg) or without (CQ) negation. If the conditions for writing our graphsusing first order logic (FOL) hold, we can equivalently write our basic graph queries in first order logic.Each variable in the query becomes existentially quantified, and the formula becomes a conjunction of 1) theterms formed from the triples in S, and 2) the negation of the terms formed from the triples in S. If thereare variables on the edges of the query graph, then we can rewrite the second order formula in first orderlogic, by interpreting them as a disjunction over all possible predicates from the finite number of options.Our example query from above becomes the following FOL sentence.",
  "target": ": A space of query patterns and their relative expressiveness. Multi-hop reasoning systems tacklethe simplest Path queries. Existing complex query answering systems support Tree queries whereas DAGand Cyclic queries remain unanswerable by current neural models. RestrictionsThe first restriction one can introduce are multi-hop queries, known in the NLP and KGliterature for a long time (Guu et al., 2015; Das et al., 2017; Asai et al., 2020) mostly in the context ofquestion answering. Formally, multi-hop queries (or path queries) are CQ which form a chain, where the tailof one projection is a head of the following projection, i.e.,",
  "qpath = Vk, V1, . . . , Vk1 : r1(v, V1) r2(V1, V2) rk(Vk1, Vk)": "where v E, i [1, k] : ri R, Vi Var and all Vi are existentially quantified variables. In other words,path queries do not contain branch intersections and can be solved iteratively by fetching the neighbors ofthe nodes. One could also define multi-hop queries which allow negation. Other ways of restricting CQ and CQneg, resulting in more expressive queries than the multi-hop ones exist.One can define families of logical queries shaped as a Tree, a DAG, and allowing cyclic parts. Illustratedin , path (multi-hop) queries form the least expressive set of logical queries. Tree queries add morereasoning branches connected by intersection operators, DAG queries drop the query tree requirement andallow queries to be directed acyclic graphs, and, finally, cyclic queries further drop the acyclic requirement.Note that these queries do not allow variables in the predicate position. Besides, all entities (in this contextreferred to as anchors) must occur before all variables in the topological ordering. We elaborate more on these query types in .2 and note that the majority of surveyed neural CLQAmethods in are still limited to Tree-like queries, falling behind the expressiveness of many graphdatabase query languages. Bridging this gap is an important avenue for future work.",
  "ExtensionsThe first extension we introduce is the union": "Definition A.6 (Union of Sets of Mappings) Given two sets of mappings (like from Definition 2.5),we can create a new set of mappings by taking their union. This union operator is commutative and asso-ciative, we can hence also talk about the union of three or more mappings. It is permissible that the domainsof the mappings in the input sets are not the same. We can define a new type of query by applying the union operation on the outcomes of two or more underlyingqueries. If these underlying queries are basic graph queries, we will call this new type of queries Unionsof Conjunctive Queries with negation (UCQneg) and if the basic queries did not include negation, asUnion of Conjunctive Queries (UCQ). These classes are also familiar from FOL, and indeed correspondto a disjunction of conjunctions.",
  "rk(vn, v?)am": "Moreover, there are FOL fragments that are equivalent to these fragments. Specifically, all queries in EPFO,which are Existential Positive First Order sentences, have an equivalent query in UCQ, and all queries inEFO, Existential First Order sentences, have an equivalent query in UCQneg. The reason is that EPFO andEFO sentences can be written in the Disjunctive Normal Form (DNF) as a union of conjunctive terms. Some query languages, like SPARQL, allow an optional part to a query. In our formalism, we can definethe optional part using the union operator. Assuming there are n optional parts in the query, create 2n different queries, in which other combinations of optional parts are removed. The answer to the query is thenthe union of the answers to all those queries. If the query language already allowed unions, then optionalsdo not make it more expressive. Beyond these extensions, one could extend further to all queries one could express with FOL, whichrequires either universal quantification, or negation of conjunctions. These are, however, still not all possiblegraph queries. An example of interesting queries, which are not in FOL, are conjunctive regular pathqueries. These are akin to the path queries we discussed above, but without a specified length. Definition A.7 (Regular Path Query) A regular path query is a 3-tuple (s, R, t), where s Term thestarting term of the path, R Term the relation term of the path, and t Term the ending term of the path.The query represents a path starting in s, traversing an arbitrary number of edges of type R ending in t. Because this kind of query is a conjunction of an arbitrary length, it cannot be represented in FOL. If onewants to express paths with a fixed length, this would be a multi-hop path like the one described above. Ifone wants to express a maximum length, then this could be done using a union of all allowed lengths. Forthe two latter cases, the query can still be expressed in EPFO. Definition A.8 (Regular Path Query Answering) Given a knowledge graph G and a regular path queryQ = (s, R, t). An answer to the query is any mapping : VarQ Con , such that if we replace all variablesv in Q with (v), obtaining (s, R, t), there exists a path in the graph that starts at node s, then traverses oneor more edges of type R, and ends in t. The set of all answers to the query is the set of all such mappings. There exist several variations on regular path queries and they can also be combined with the above querytypes to form new ones. In we illustrate how the fragments relate to other query classes. Mostmethods fall strictly within the EFO fragment, i.e., they only support a subset with restrictions as wediscussed above. We will discuss further limitations of these methods in .",
  "Definition A.9 (Projection of a Query Answer) Given a query answer , and a set of variables V Var. The projection of the query answer on the variables V is {(var, val)|(var, val) , var V}": "In other words, it is the answer but restricted to a specific set of variables. The query forms introducedabove can all be augmented with a projection to only obtain values for specific variables. This correspondsto the SELECT ?var clause in SPARQL. Alternatively, it is possible to project all variables in V which isequivalent to the SELECT * clause. A query without any projected variable is a Boolean subgraph matchingproblem equivalent to the ASK query in SPARQL.",
  "A join is used to combine the results of two separate queries into one. Specifically,": "Definition A.10 (Join of Query Answers) Given two query answers A and B, and VarA, and VarBthe variables occurring in A and B, respectively. The join of these two answers only exists in case theyhave the same value for all variables they have in common, i.e., var VarA VarB : A(var) = b(var).In that case, join(A, B) = A B.",
  "Regular Path Queries": ": Current query answering models cover Existential Positive First Order (EPFO) and ExistentialFirst Order (EFO) logic fragments (marked in a red rectangle). EPFO and EFO are equivalent to unionsof conjunctions (UCQ), and those with atomic negation (UCQneg), respectively.These, in turn, are asubset of first order logic (FOL). FOL queries, in turn, are only a subset of queries answerable by graphdatabase languages. For example regular path queries cannot be expressed in FOL. Languages like SPARQL,SPARQL*, or Cypher, encompass all the query types and more.",
  "A.4Triangular Norms and Conorms": "Answering logical queries implies execution of logical operators. Approximate query answering, in turn,implies continuous vector inputs and output truth values that are not necessarily binary.Besides, themethods often require that the logical operators are smooth and differentiable. Triangular norms (T-norms)and triangular conorms (T-conorms) define functions that generalize logical conjunction and disjunction,respectively, to the continuous space of truth values and implement fuzzy logical operations. T-norm defines a continuous function : with the following properties (x, y) =(y, x) (commutativity), (x, (y, z)) = ((x, y), z) (associativity), and y z (x, y) (x, z)(monotonicity). Also, the identity element for is 1, i.e., (x, 1) = x. The goal of t-norms is to generalizelogical conjunction with a continuous function. The T-conorm can be seen as a duality of a t-norm thatsimilarly defines a function with the same domain and range : .T-conormsuse the continuous function to generalize disjunction to fuzzy logic. The function satisfies the samecommutativity, associativity, and monotonicity properties as , but with 0 as the identity element, i.e.,(x, 0) = x.",
  "A.5Graph Representation Learning": "Graph Representation Learning (GRL) is a subfield of machine learning aiming at learning low-dimensionalvector representations of graphs or their elements such as single nodes (Hamilton, 2020).For example,hv Rd denotes a d-dimensional vector associated with a node v. Conceptually, we want nodes that sharecertain structural or semantic features in the graph to have similar vector representations (where similarityis often measured by a distance function). Shallow EmbeddingsThe first GRL approaches focused on learning shallow node embeddings, that is,learning a unique vector per node directly used in the optimization task. For homogeneous (single-relation)graphs, DeepWalk (Perozzi et al., 2014) and node2vec (Grover & Leskovec, 2016) trained node embeddingson the task of predicting walks in the graph whereas in multi-relational graphs TransE (Bordes et al., 2013)trained node and edge type embeddings in the autoencoder fashion by reconstructing the adjacency matrix. Graph Neural NetworksThe idea of graph neural networks (GNNs) (Scarselli et al., 2008) implieslearning a network encoder with shared parameters on top of given (or learnable) node features by performingneighborhood aggregation. This framework can be generalized to message passing (Gilmer et al., 2017) whereat each layer t a node v receives messages from its neighbors (possibly adding edge and graph features),aggregates the messages in a permutation-invariant way, and updates the representation:",
  "h(t)v= Updateh(t1)v, AggregateuN(v)Message(h(t1)v, h(t1)u, euv))": "Here, hu is a feature of the neighboring node u, euv is the edge feature, the Message function builds amessage from node u to node v and can be parameterized with a neural network. As the set of neighborsN(v) is unordered, Aggregate is often a permutation-invariant function like sum or mean. The Updatefunction takes the previous node state and aggregated messages of the neighbors to produce the final stateof the node v at layer t and can be parameterized with a neural network as well. Classical GNN architectures like GCN (Kipf & Welling, 2017), GAT (Velickovic et al., 2018), and GIN (Xuet al., 2019) were designed to work with homogeneous, single-relation graphs. Later, several works havedeveloped GNN architectures that work on heterogeneous graphs with multiple relations (Schlichtkrull et al.,2018; Vashishth et al., 2020; Zhu et al., 2021). GNNs and message passing paved the way for Geometric DeepLearning (Bronstein et al., 2021) that leverages symmetries and invariances in the input data as inductivebias for building deep learning models.",
  "BCLQA vs KGQA": "KG-based question answering (KGQA) (Chakraborty et al., 2021) is the adjacent sub-field tackling naturallanguage questions and multi-hop reasoning over structured data. The input of a typical KGQA model is aquestion posed in natural language, and, given a background graph, the output is a set of possible answerentities. While CLQA and KGQA are similar in terms of tackling complex multi-hop questions (and perhapsspanning over several modalities), KGQA approaches are rather different for the following reasons:",
  "The majority of KGQA benchmarks include only projection (such as 2p) and intersection queries(2i / 3i) thus missing out on unions and negations": "Since the query is posed in natural language, KGQA pipelines often rely on entity linking modulestailored to a specific graph (to map a string to an entity ID in the graph). Besides being an externalcomponent with possible error propagation, it hampers generalization capabilities of the models,e.g., a KGQA pipeline with a tailored entity linker cannot generalize to unseen graphs. In contrast,the language-agnostic nature of CLQA allows to train graph reasoning models that can generalizeto complex queries over completely unseen KGs at inference time (Galkin et al., 2024). We believe that the synergy between CLQA and KGQA is achievable by combining the best from the bothworlds, e.g., inductive and generalizable reasoning over complex queries while predicting missing links (fromCLQA), and deriving logical forms from the questions along with efficient retrieval mechanisms for fasterscoring of relation projections (from KGQA)."
}