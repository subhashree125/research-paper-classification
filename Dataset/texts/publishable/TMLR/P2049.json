{
  "Abstract": "Understanding AI systems inner workings is critical for ensuring value alignment and safety.This review explores mechanistic interpretability: reverse engineering the computationalmechanisms and representations learned by neural networks into human-understandablealgorithms and concepts to provide a granular, causal understanding. We establish founda-tional concepts such as features encoding knowledge within neural activations and hypothe-ses about their representation and computation.We survey methodologies for causallydissecting model behaviors and assess the relevance of mechanistic interpretability to AIsafety. We examine benefits in understanding, control, alignment, and risks such as ca-pability gains and dual-use concerns.We investigate challenges surrounding scalability,automation, and comprehensive interpretation. We advocate for clarifying concepts, settingstandards, and scaling techniques to handle complex models and behaviors and expand todomains such as vision and reinforcement learning. Mechanistic interpretability could helpprevent catastrophic outcomes as AI systems become more powerful and inscrutable. Foran HTML version of the paper, visit",
  "Introduction": "As AI systems rapidly become more sophisticated and general (Bubeck et al., 2023; Bengio et al., 2023),advancing our understanding of these systems is crucial to ensure their alignment (Ji et al., 2024) withhuman values and avoid catastrophic outcomes (Hendrycks et al., 2023; Hendrycks & Mazeika, 2022). Thefield of interpretability aims to demystify the internal processes of AI models, moving beyond evaluatingperformance alone. This review focuses on mechanistic interpretability, an emerging approach within thebroader interpretability landscape that strives to comprehensively specify the computations underlying deepneural networks. We emphasize that understanding and interpreting these complex systems is not merelyan academic endeavor its a societal imperative to ensure AI remains trustworthy and beneficial. The interpretability landscape is undergoing a paradigm shift akin to the evolution from behaviorism tocognitive neuroscience in psychology. Historically, lacking tools for introspection, psychology treated themind as a black box, focusing solely on observable behaviors. Similarly, interpretability has predominantlyrelied on black-box techniques (Casper et al., 2024), analyzing models based on input-output relationships orusing attribution methods that, while probing deeper, still neglect the models internal architecture. However,just as advancements in neuroscience allowed for a deeper understanding of internal cognitive processes, thefield of interpretability is now moving towards a more granular approach.This shift from surface-levelanalysis to a focus on the internal mechanics of deep neural networks characterizes the transition towardsinner interpretability (Ruker et al., 2023). Mechanistic interpretability, as an approach to inner interpretability, aims to completely specify a neuralnetworks computation, potentially in a format as explicit as pseudocode (also called reverse engineering),striving for a granular and precise understanding of model behavior. It distinguishes itself primarily through",
  "input": ": Interpretability paradigms offer distinct lenses for understanding neural networks: Behavioral an-alyzes input-output relations; Attributional quantifies individual input feature influences; Concept-basedidentifies high-level representations governing behavior; Mechanistic uncovers precise causal mechanismsfrom inputs to outputs. its ambition for comprehensive reverse engineering and its strong motivation towards AI safety. Our reviewserves as the first comprehensive exploration of mechanistic interpretability research, with the most accessibleintroductions currently scattered in a blog or list format (Olah, 2022; Nanda, 2022d; Olah et al., 2020;Sharkey et al., 2022a; Olah et al., 2018; Nanda, 2023f; 2024). Concurrently, Ferrando et al. (2024) and Raiet al. (2024) have also contributed valuable reviews giving concise, technical introductions to mechanisticinterpretability in transformer-based language models. Our work complements these efforts by synthesizingthe research (addressing the \"research debt\" (Olah & Carter, 2017)) and providing a structured, accessible,and comprehensive introduction for AI researchers and practitioners. The structure of this paper provides a cohesive overview of mechanistic interpretability, situating the mecha-nistic approach in the broader interpretability landscape (), presenting core concepts and hypotheses(), explaining methods and techniques (), presenting a taxonomy and survey of the cur-rent field (), exploring relevance to AI safety (), and addressing challenges () andfuture directions ().",
  "Interpretability Paradigms from the Outside In": "We encounter a spectrum of interpretability paradigms for decoding AI systems decision-making, rangingfrom external black-box techniques to internal analyses.We contrast these paradigms with mechanisticinterpretability, highlighting its distinct causal bottom-up perspective within the broader interpretabilitylandscape (see ). Behavioralinterpretability treats the model as a black box, analyzing input-output relations. Techniquessuch as minimal pair analysis (Warstadt et al., 2020), sensitivity and perturbation analysis (Casalicchioet al., 2018) examine input-output relations to assess the models robustness and variable dependencies(Shapley, 1988; Ribeiro et al., 2016; Covert et al., 2021). Its model-agnostic nature is practical for complexor proprietary models but lacks insight into internal decision processes and causal depth (Jumelet, 2023). Attributionalinterpretability aims to explain outputs by tracing predictions to individual input contri-butions using gradients. Raw gradients can be discontinuous or sensitive to slight perturbations. Therefore,techniques such as SmoothGrad (Smilkov et al., 2017) and Integrated Gradients (Sundararajan et al., 2017)average across gradients. Other popular techniques are layer-wise relevance propagation (Bach et al., 2015),DeepLIFT (Shrikumar et al., 2017), or GradCAM (Selvaraju et al., 2016). Attribution enhances transparencyby showing input feature influence without requiring an understanding of the internal structure, enablingdecision validation, compliance, and trust while serving as a bias detection tool, but also has fundamentallimitations (Bilodeau et al., 2024).",
  "dening features": ": Overview of key concepts and hypotheses in mechanistic interpretability, organized into four sub-section (pink boxes): defining features (.1), representation (.2), computation (.3),and emergence (.4). In turquoise, it highlights definitions like features, circuits, and motifs, and inorange, it highlights hypotheses like linear representation, superposition, universality, simulation, and predic-tion orthogonality. Arrows show relationships, e.g., superposition enabling an alternative feature definitionor universality connecting circuits and motifs. Concept-basedinterpretability adopts a top-down approach to unraveling a models decision-makingprocesses by probing its learned representations for high-level concepts and patterns governing behavior.Techniques include training supervised auxiliary classifiers (Belinkov, 2021), employing unsupervised con-trastive and structured probes (see .2) to explore latent knowledge (Burns et al., 2023), and usingneural representation analysis to quantify the representational similarities between the internal representa-tions learned by different neural networks (Kornblith et al., 2019; Bansal et al., 2021). Beyond observationalanalysis, concept-based interpretability can enable manipulation of these representations also called rep-resentation engineering (Zou et al., 2023) potentially enhancing safety by upregulating concepts such ashonesty, harmlessness, and morality. Mechanisticinterpretability is a bottom-up approach that studies the fundamental components of modelsthrough granular analysis of features, neurons, layers, and connections, offering an intimate view of opera-tional mechanics. Unlike concept-based interpretability, it aims to uncover causal relationships and precisecomputations transforming inputs into outputs, often identifying specific neural circuits driving behavior.This reverse engineering approach draws from interdisciplinary fields like physics, neuroscience, and systemsbiology to guide the development of transparent, value-aligned AI systems. Mechanistic interpretability isthe primary focus of this review.",
  "Core Concepts and Assumptions": "This section introduces the key concepts and hypotheses of mechanistic interpretability, as summarized in. We start by defining features as the basic units of representation (.1). We then examinethe nature of these features, including the challenges posed by polysemantic neurons and the implications ofthe superposition and linear representation hypotheses (.2). Next, we explore computation throughcircuits and motifs, considering the universality hypothesis (.3). Finally, we discuss the implicationsfor understanding emergent properties, such as internal world models and simulated agents with potentiallymisaligned objectives (.4).",
  "Defining Features as Representational Primitives": "Features as fundamental units of representation.The notion of a feature in neural networks is cen-tral yet elusive, reflecting the pre-paradigmatic state of mechanistic interpretability. We adopt the notionof features as the fundamental units of neural network representations, such that features cannot be fur-ther disentangled into simpler, distinct factors. These features are core components of a neural networksrepresentation, analogous to how cells form the fundamental unit of biological organisms (Olah et al., 2020).",
  "Features are the fundamental units of neural network representations that cannot be further decom-posed into simpler independent factors": "Concepts as natural abstractions.The world consists of various entities that can be grouped intocategories or concepts based on shared properties. These concepts form high-level summaries like \"tree\" or\"velocity,\" allowing compact world representations by discarding many irrelevant low-level details. Neuralnetworks can capture and represent such natural abstractions (Chan et al., 2023) through their learnedfeatures, which serve as building blocks of their internal representations, aiming to capture the conceptsunderlying the data. Features encoding input patterns.In traditional machine learning, features are understood as charac-teristics or attributes derived directly from the input data stream (Bishop, 2006). This view is particularlyrelevant for systems focused on perception, where features map closely to the input data. However, in moreadvanced systems capable of reasoning with abstractions, features may emerge internally within the modelas representational patterns, even when processing information unrelated to the input. In this context, fea-tures are better conceptualized as any measurable property or characteristic of a phenomenon (Olah, 2022),encoding abstract concepts rather than strictly reflecting input attributes. Features as representational atoms.A key property of features is their irreducibility, meaning theycannot be decomposed into or expressed as a combination of simpler, independent factors. In the contextof input-related features, Engels et al. (2024) define a feature as irreducible if it cannot be decomposedinto or expressed as a combination of statistically independent patterns or factors in the original input data.Specifically, a feature is reducible if transformations reveal its underlying pattern, which can be separated intoindependent co-occurring patterns or is a mixture of patterns that never co-occur. We propose generalizingthis notion of irreducibility to features encoding abstract concepts not directly tied to input patterns, suchthat features cannot be reduced to combinations or mixtures of other independent components within themodels representations. Features beyond human interpretability.Features could be defined from a human-centric perspectiveas semantically meaningful, articulable input patterns encoded in the networks activation space (Olah, 2022).However, while cognitive systems may converge on similar natural abstractions (Chan et al., 2023), theseneed not necessarily align with human-interpretable concepts. Adversarial examples have been interpreted asnon-interpretable features meaningful to models but not humans. Imperceptible perturbations fool networks,suggesting reliance on alien representational patterns (Ilyas et al., 2019). As models surpass human capabili-ties, their learned features may become increasingly abstract, encoding information in ways incongruent withhuman intuition (Hubinger, 2019a). Mechanistic interpretability aims to uncover the actual representationslearned, even if diverging from human concepts. While human-interpretable concepts provide guidance, anon-human-centric perspective that defines features as independent model components, whether aligned withhuman concepts or not, is a more comprehensive and future-proof approach.",
  "monosemantic neurons:polysemantic neurons:": ": Contrasting privileged and non-privileged bases. In a non-privileged basis, there is no reasonto expect features to be basis-aligned calling basis dimensions neurons has no meaning. In a privilegedbasis, the architecture treats basis directions differently features can but need not align with neurons(Bricken et al., 2023). Leftmost: Privileged basis; individual features (arrows) align with basis directions,resulting in monosemantic neurons (colored circles). Middle left: Privileged basis, where despite havingmore features than neurons, some neurons are monosemantic, representing individual features, while othersare polysemantic (overlapping gradients), encoding superposition of multiple features. Middle right: Non-privileged basis where, even when the number of features equals the number of neurons, the lack of alignmentbetween the feature directions and basis directions results in polysemantic neurons encoding combinations offeatures. Rightmost: Non-privileged, polysemantic neurons as feature directions do not align with neuronbasis. h Rn, the n basis directions are called neurons.For a neuron to be meaningful, the basis directionsmust functionally differ from other directions in the representation, forming a privileged basis where thebasis vectors are architecturally distinguished within the neural network layer from arbitrary directions inactivation space, as shown in . Typical non-linear activation functions privilege the basis directionsformed by the neurons, making it meaningful to analyze individual neurons (Elhage et al., 2022b). Analyzingneurons can give insights into a networks functionality (Sajjad et al., 2022; Mu & Andreas, 2020; Dai et al.,2022; Ghorbani & Zou, 2020; Voita et al., 2023; Durrani et al., 2020; Goh et al., 2021; Bills et al., 2023;Huang et al., 2023). Monosemantic and Polysemantic Neurons.A neuron corresponding to a single semantic concept iscalled monosemantic. The intuition behind this term comes from analyzing what inputs activate a givenneuron, revealing its associated semantic meaning or concept. If neurons were the representational primitivesof neural networks, all neurons would be monosemantic, implying a one-to-one relationship between neuronsand features. Comprehensive interpretability would be as tractable as characterizing all neurons and theirconnections. However, empirically, especially for transformer models (Elhage et al., 2022b), neurons areoften observed to be polysemantic, i.e., associated with multiple, unrelated concepts (Arora et al., 2018; Mu& Andreas, 2020; Elhage et al., 2022a; Olah et al., 2020). For example, a single neuron may be activated byboth images of cats and images of cars, suggesting it encodes multiple unrelated concepts. Polysemanticitycontradicts the interpretation of neurons as representational primitives and, in practice, makes it challengingto understand the information processing of neural networks.",
  "Exploring Polysemanticity: Hypotheses and Implications.To understand the widespread occur-rence of polysemanticity in neural networks, several hypotheses have been proposed:": "i.) One trivial scenario would be that feature directions are orthogonal but not aligned with the basisdirections (neurons). There is no inherent reason to assume that features would align with neuronsin a non-privileged basis, where the basis vectors are not architecturally distinguished. However,even in a privileged basis formed by the neurons, the network could represent features not in thestandard basis but as linear combinations of neurons (see , middle right). ii.) An alternative hypothesis posits that redundancy due to noise introduced during training, such asrandom dropout (Srivastava et al., 2014), can lead to redundant representations and, consequently,",
  "Published in Transactions on Machine Learning Research (08/2024)": "Ilia Sucholutsky, Lukas Muttenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C. Love,Erin Grant, Iris Groen, Jascha Achterberg, Joshua B. Tenenbaum, Katherine M. Collins, Katherine L.Hermann, Kerem Oktar, Klaus Greff, Martin N. Hebart, Nori Jacoby, Qiuyi Zhang, Raja Marjieh, RobertGeirhos, Sherol Chen, Simon Kornblith, Sunayana Rane, Talia Konkle, Thomas P. OConnell, Thomas Un-terthiner, Andrew K. Lampinen, Klaus-Robert Mller, Mariya Toneva, and Thomas L. Griffiths. Gettingaligned on representational alignment. CoRR, November 2023. 11",
  "Neural networks represent more features than they have neurons by encoding features in overlappingcombinations of neurons": "Superposition Hypothesis.The superposition hypothesis suggests that neural networks can leveragehigh-dimensional spaces to represent more features than their actual neuron count by encoding features inalmost orthogonal directions. Non-orthogonality means that features interfere with one another. However,the benefit of representing many more features than neurons may outweigh the interference cost, mainlywhen concepts are sparse and non-linear activation functions can error-correct noise (Elhage et al., 2022b).",
  "Observed modelHypothetical disentangled model": ": Observed neural networks (left) can be viewed as compressed simulations of larger, sparser networks(right) where neurons represent distinct features. An \"almost orthogonal\" projection compresses the high-dimensional sparse representation, manifesting as polysemantic neurons involved with multiple features inthe lower-dimensional observed model, reflecting the compressed encoding. Figure adapted from (Brickenet al., 2023). Toy models can demonstrate under which conditions superposition occurs (Elhage et al., 2022b; Scherliset al., 2023). Neural networks, via superposition, may effectively simulate computation with more neuronsthan they possess by allocating each feature to a linear combination of neurons, creating what is known asan overcomplete linear basis in the representation space. This perspective on superposition suggests thatpolysemantic models could be seen as compressed versions of hypothetically larger neural networks whereeach neuron represents a single concept (see ). Consequently, an alternative definition of featurescould be:",
  "Toy Model of Superposition": "A toy model (Elhage et al., 2022b) investigates the hypothesis that neural networks can representmore features than the number of neurons by encoding real-world concepts in a compressed manner.The model considers a high-dimensional vector x, where each element xi corresponds to a featurecapturing a real-world concept, represented as a random vector with varying importance determinedby a weight ai. These features are assumed to have the following properties:",
  "iii.) Varying concept importance: Some concepts are more important than others for the taskat hand": "The input vector x represents features capturing these concepts, defined by a sparsity level S andan importance level ai for each feature xi, reflecting the sparsity and varying importance of theunderlying concepts. The model dynamics involve transforming x into a hidden representation h oflower dimension, and then reconstructing it as x:",
  "Eect of sparsityToy model architecture": ": Illustration of the toy model architecture and the effects of sparsity. (left) Transformation ofa five-feature input vector x into a two-dimensional hidden representation h, and its reconstruction asx using the weight matrix W and its transpose, with feature importance indicated by a color gradientfrom yellow to green. (right) The effect of increasing feature sparsity S on the encoding capacity ofthe network, highlighting the networks enhanced ability to represent features in superposition assparsity increases from 0 to 0.9, illustrated by arrows in the activation space h, which correspond tothe columns of the matrix W . Research on superposition, including works by (Elhage et al., 2022b; Scherlis et al., 2023; Henighan et al.,2023), often investigates simplified models. However, understanding superposition in practical, transformer-based scenarios is crucial for real-world applications, as pioneered by Gurnee et al. (2023).",
  "Features are directions in activation space, i.e., linear combinations of neurons": "The linear representation hypothesis suggests that neural networks frequently represent high-level featuresas linear directions in activation space. This hypothesis can simplify the understanding and manipulationof neural network representations (Nanda et al., 2023b). The prevalence of linear layers in neural networkarchitectures favors linear representations. Matrix multiplication in these layers most readily processes linearfeatures, while more complex non-linear encodings would require multiple layers to decode. However, recent work by Engels et al. (2024) provides evidence against a strict formulation of the linearrepresentation hypothesis by identifying circular features representing days of the week and months of theyear. These multi-dimensional, non-linear representations were shown to be used for solving modular arith-metic problems in days and months. Intervention experiments confirmed that these circular features arethe fundamental unit of computation in these tasks, and the authors developed methods to decompose thehidden states, revealing the circular representations. Establishing non-linearity can be challenging. For example, Li et al. (2023a) initially found that in a GPTmodel trained on Othello, the board state could only be decoded with a non-linear probe when representedin terms of white and black pieces, seemingly violating the linearity assumption. However, Nanda (2023c);Nanda et al. (2023b) later showed that a linear probe sufficed when the board state was decoded in termsof \"ones own\" and \"the opponents\" pieces, reaffirming the linear representation hypothesis in this case. Incontrast, the work by Engels et al. (2024) provides a clear and convincing existence proof for non-linear,multi-dimensional representations in language models. While the linear representation hypothesis remains a useful simplification, it is important to recognize itslimitations and the potential role of non-linear representations (Sharkey et al., 2022a). As neural networkscontinue to evolve, ongoing reevaluation of the hypothesis is crucial, particularly considering the possibleemergence of non-linear features under optimization pressure for interpretability (Hubinger, 2022). Alter-",
  "Circuits as Computational Primitives and Motifs as Universal Circuit Patterns": "Having defined features as directions in activation space as the fundamental units of neural network rep-resentation, we now explore their computation. Neural networks can be conceptualized as computationalgraphs, within which circuits are sub-graphs consisting of linked features and the weights connecting them.Similar to how features are the representational primitive, circuits function as the computational primitive(Michaud et al., 2023) and the primary building block of these networks (Olah et al., 2020).",
  "models": ": Comparing observed models (left) and corresponding hypothetical disentangled models (right)trained on similar tasks and data. The observed models show different neuronal activation patterns, whilethe dissection into feature-level circuits reveals a motif - a shared circuit pattern emerging across models,hinting at universality models converging on similar solutions based on common underlying principles.",
  "Hypothesis 4: Strong Universality": "The same core features and circuits will universally and consistently arise across all neural networkmodels trained on similar tasks and data distributions and using similar techniques, reflecting a set offundamental computational motifs that neural networks inherently gravitate towards when learning. The universality hypothesis posits a convergence in forming features and circuits across various models andtasks, which could significantly ease interpretability efforts in AI. It proposes that artificial and biological",
  "Emergence of World Models and Simulated Agents": "Internal World Models.World models are internal causal models of an environment formed within neuralnetworks. Traditionally linked with reinforcement learning, these models are explicitly trained to developa compressed spatial and temporal representation of the training environment, enhancing downstream taskperformance and sample efficiency through training on internal hallucinations (Ha & Schmidhuber, 2018).However, in the context of our survey, our focus shifts to internal world models that potentially form implicitlyas a by-product of the training process, especially in LLMs trained on next-token prediction also calledGPT. LLMs are sometimes characterized as stochastic parrots (Bender et al., 2021). This label stems from theirfundamental operational mechanism of predicting the next word in a sequence, which is seen as relyingheavily on memorization. From this viewpoint, LLMs are thought to form complex correlations based onobservational data but cannot develop causal models of the world due to their lack of access to interventionaldata (Pearl, 2009). An alternative perspective on LLMs comes from the active inference framework (Salvatori et al., 2023),a theory rooted in cognitive science and neuroscience.Active inference postulates that the objective ofminimizing prediction error, given enough representative capacity, is adequate for a learning system todevelop complex world representations, behaviors, and abstractions. Since language inherently mirrors theworld, these models could implicitly construct linguistic and broader world models (Kulveit et al., 2023). The simulation hypothesis suggests that models designed for prediction, such as LLMs, will eventuallysimulate the causal processes underlying data creation.Seen as an extension of their drive for efficientcompression, this hypothesis implies that adequately trained models like GPT could develop internal worldmodels as a natural outcome of their predictive training (janus, 2022; Shanahan et al., 2023).",
  "A model whose objective is text prediction will simulate the causal processes underlying the textcreation if optimized sufficiently strongly (janus, 2022)": "In addition to theoretical considerations for emergent causal world models (Richens & Everitt, 2024; Nichaniet al., 2024), mechanistic interpretability is starting to provide empirical evidence on the types of internalworld models that may emerge in LLMs. The ability to internally represent the board state in games like chess(Karvonen, 2024) or Othello (Li et al., 2023a; Nanda et al., 2023b), create linear abstractions of spatial andtemporal data (Gurnee & Tegmark, 2024), and structure complex representations of mazes, demonstratingan understanding of maze topology and pathways (Ivanitskiy et al., 2023) highlight the growing abstraction",
  "A model whose objective is prediction can simulate agents who optimize toward any objectives withany degree of optimality (janus, 2022)": "In conclusion, the evolution of LLMs from simple predictive models to entities potentially possessing complexinternal world models, as suggested by the simulation hypothesis and supported by mechanistic interpretabil-ity studies, represents a significant shift in our understanding of these systems. This evolution challenges usto reconsider LLMs capabilities and future trajectories in the broader landscape of AI development.",
  "Core Methods": "Mechanistic interpretability (MI) employs various tools, from observational analysis to causal interventions.This section provides a comprehensive overview of these methods, beginning with a taxonomy that categorizesapproaches based on their key characteristics (.1). We then survey observational (.2),followed by interventional techniques (.3). Finally, we study their synergistic interplay (.4). offers a visual summary of the methods and techniques unique to mechanistic interpretability.",
  "MethodCausal NaturePhaseLocalityComprehensivenessKey Examples": "Feature VisualizationObservationPost-hocLocalPartialZeiler & Fergus (2014)Zimmermann et al. (2021)Exemplar methodsObservationPost-hocLocalPartialGrosse et al. (2023)Garde et al. (2023)Probing TechniquesObservationPost-hocBothBothMcGrath et al. (2022)Gurnee et al. (2023)Structured ProbesObservationPost-hocBothBothBurns et al. (2023)Logit Lens VariantsObservationPost-hocGlobalPartialnostalgebraist (2020)Belrose et al. (2023)Sparse AutoencodersObservationPost-hocBothComprehensiveCunningham et al. (2024)Bricken et al. (2023)Activation PatchingInterventionPost-hocLocalPartialMeng et al. (2022a)Wang et al. (2023)Path PatchingInterventionPost-hocBothBothGoldowsky-Dill et al. (2023)Causal AbstractionInterventionPost-hocGlobalComprehensiveGeiger et al. (2023a)Geiger et al. (2023b)Wu et al. (2023a)Hypothesis TestingInterventionPost-hocGlobalComprehensiveChan et al. (2022)Jenner et al. (2023)Intrinsic MethodsPre/DuringGlobalComprehensiveElhage et al. (2022a)Liu et al. (2023a) The categorization is based on the methods general tendencies. Some methods can offer local and global orpartial and comprehensive interpretability depending on the scope of the analysis and application. Probingtechniques can range from local to global and partial to comprehensive; simple linear probes might offer localinsights into individual features, while more sophisticated structured probes can uncover global patterns.Sparse autoencoders decompose individual neuron activations (local) but aim to disentangle features acrossthe entire model (global). Path patching extends local interventions to global model understanding by tracinginformation flow across layers, demonstrating how local perturbations can yield broader insights. In practice, mechanistic interpretability research involves both method development and their application.When applying methods to understand a model, combining techniques from multiple categories is oftennecessary and beneficial to build a more comprehensive understanding (.4).",
  "Observation": "Mechanistic interpretability draws from observational methods that analyze the inner workings of neuralnetworks, with many of these methods preceding the field itself. For a detailed exploration of inner inter-pretability methods, refer to (Ruker et al., 2023). Two prominent categories are example-based methodsand feature-based methods: i.) Exemplar methods identify real input examples that highly activate specific neurons or layers.This helps pinpoint influential data points that maximize neuron activation within the neural network(Grosse et al., 2023; Garde et al., 2023; Nanfack et al., 2024). ii.) Feature visualization encompasses techniques that generate synthetic inputs to optimize neuronactivation. These visualizations reveal how neurons respond to stimuli and which features are sen-sitive to (Zeiler & Fergus, 2014; Zimmermann et al., 2021). By inspecting the synthetic inputs thatdrive neuron behavior, we can hypothesize about the features encoded by those neurons.",
  "Structured Probes.While focusing on bottom-up, mechanistic interpretability approaches, we can alsoconsider integrating top-down, concept-based structured probes with mechanistic interpretability": "Structured probes aid conceptual interpretability, probing language models for complex features like truthrepresentations. Notably, Burns et al. (2023)s contrast-consistent search identifies linear projections exhibit-ing logical consistency in hidden states, contrasting truth values for statements and negations. However, structured probes face significant challenges in unsupervised probing scenarios. As Farquhar et al.(2023) showed, arbitrary features, not just knowledge-related ones, can satisfy contrast consistency equallywell, raising doubts about scalability.For example, the loss may capture simulation of knowledge fromhypothesized simulacra within sufficiently powerful language models rather than the models true knowledge.Furthermore, Farquhar et al. (2023) demonstrates self-supervised probing methods (like (Burns et al., 2023))often detect prominent but unintended distractor features in the data. The discovered features are also highlysensitive to prompt choice, and there is no principled way to select prompts that would reliably surface amodels true knowledge. While structured probes primarily focus on high-level conceptual representations (Zou et al., 2023), theirfindings could potentially inform or complement mechanistic interpretability efforts. For instance, identifyingtruth directions through structured probes could help guide targeted interventions or analyze the underlyingcircuits responsible for truthful behavior using mechanistic techniques such as activation patching or circuittracing (.3). Conversely, mechanistic methods could provide insights into how truth representa-tions emerge and are computed within the model, addressing some of the challenges faced by unsupervisedstructured probes. Logit Lens.The logit lens (nostalgebraist, 2020) provides a window into the models predictive process byapplying the final classification layer (which projects the residual stream activation into logits/vocabulary",
  "Sparse Dictionary Learning": "Sparse autoencoders (Cunningham et al., 2024) are proposed as a solution to polysemantic neurons.The problem of superposition is mathematically formalized as sparse dictionary learning (Olshausen &Field, 1997) problem to decompose neural network activations into disentangled component features.The goal is to learn a dictionary of vectors {fk}nfeatk=1 Rd that can represent the unknown, groundtruth network features as sparse linear combinations. If successful, the learned dictionary containsmonosemantic neurons corresponding to features (Sharkey et al., 2022b). The autoencoder architec-ture consists of an encoder and a ReLU activation function, expanding the input dimensionality todhid > din. The encoders output is given by:",
  "Intervention": "Causality as a Theoretical Foundation.The theory of causality (Pearl, 2009) provides a mathemati-cally precise framework for mechanistic interpretability, offering a rigorous approach to understanding high-level semantics in neural representations (Geiger et al., 2023a). By treating neural networks as causal models,with their compute graphs serving as causal graphs, researchers can perform precise interventions and ex-amine the roles of individual parameters (Mueller et al., 2024). This causal perspective on interpretabilityhas led to the development of various intervention techniques, including activation patching (.3.1),causal abstraction (.3.2), and hypothesis testing methods (.3.3).",
  "(b)": ": (a) Activation patching in a transformer model.Left: The model processes the clean input\"Colosseum in Rome,\" caching the latent activations (step i.). Right: The model runs with the corruptedinput \"Eiffel Tower in Paris\" (step ii.). The pink arrow shows an MLP layer activation (green diamond)patched from the clean run into the corrupted run (step iii.). This causes the prediction to change from\"Paris\" to \"Rome,\" demonstrating how the significance of the patched component is determined (step iv.).By comparing these carefully selected inputs, researchers can control for confounding circuitry and isolatethe specific circuit responsible for the location prediction behavior. (b) Activation patching directions: Top:Patching corrupted activations (orange) into clean circuits (turquoise) reveals sufficient components foridentifying OR logic scenarios. Bottom: Patching clean activations (green) into corrupted circuits (orange)reveals necessary components that are useful for identifying AND logic scenarios. The AND and OR gatesdemonstrate how these patching directions uncover different logical relationships between model components. Activation patching is a collective term for a set of causal intervention techniques that manipulate neuralnetwork activations to shed light on the decision-making processes within the model. These techniques,including causal tracing (Meng et al., 2022a), interchange intervention (Geiger et al., 2021b), causal mediationanalysis (Vig et al., 2020), and causal ablation (Wang et al., 2023), share the common goal of modifying aneural models internal state by replacing specific activations with alternative values, such as zeros, meanactivations across samples, random noise, or activations from a different forward pass (a). The primary objective of activation patching is to isolate and understand the role of specific components orcircuits within the model by observing how changes in activations affect the models output. This enablesresearchers to infer the function and importance of those components. Key applications include localizingbehavior by identifying critical activations, such as understanding the storage and processing of factual infor-mation (Meng et al., 2022a; Geva et al., 2023; Goldowsky-Dill et al., 2023; Stolfo et al., 2023), and analyzingcomponent interactions through circuit analysis to identify sub-networks within a models computation graphthat implement specified behaviors (Wang et al., 2023; Hanna et al., 2023; Lieberum et al., 2023; Hendelet al., 2023; Geva et al., 2023).",
  "step iv. Determining significance by observing the variations in the models output during the third step,thereby highlighting the importance of the replaced components": "This process relies on comparing pairs of inputs: a clean input, which triggers the desired behavior, and acorrupted input, which is identical to the clean one except for critical differences that prevent the behavior.By carefully selecting these inputs, researchers can control for confounding circuitry and isolate the specificcircuit responsible for the behavior. Differences in patching direction clean to corrupted (causal tracing) versus corrupted to clean (resampleablation) provide insights into the sufficiency or necessity of model components for a given behavior.Clean to corrupted patching identifies activations sufficient for restoring clean performance, even if they areunnecessary due to redundancy, which is particularly informative in OR logic scenarios (b, OR gate).Conversely, corrupted to clean patching determines the necessary activations for clean performance, whichis useful in AND logic scenarios (b, AND gate). Activation patching can employ corruption methods, including zero-, mean-, random-, or resample ablation,each modulating the models internal state in distinct ways. Resample ablation stands out for its effectivenessin maintaining consistent model behavior by not changing the data distribution too much (Zhang & Nanda,2023). However, it is essential to be careful when interpreting the patching results, as breaking behavior bytaking the model off-distribution is uninteresting for finding the relevant circuit (Nanda, 2023e). Path Patching and Subspace Activation Patching.Path patching extends the activation patchingapproach to multiple edges in the computational graph (Wang et al., 2023; Goldowsky-Dill et al., 2023),allowing for a more fine-grained analysis of component interactions. For example, path patching can beused to estimate the direct and indirect effects of attention heads on the output logits. Subspace activationpatching, also known as distributed interchange interventions (Geiger et al., 2023b), aims to intervene onlyon linear subspaces of the representation space where features are hypothesized to be encoded, providing atool for more targeted interventions. Recently, Ghandeharioun et al. (2024) introduced patchscopes, a framework that unifies and extends activa-tion patching techniques: using the models text generation to explain internal representations, it enablesmore flexible interventions across various interpretability tasks, improving early layer inspection and allowingfor cross-model analysis. Limitations and Advancements.Activation patching has several limitations, including the effort re-quired to design input templates and counterfactual datasets, the need for human inspection to isolateimportant subgraphs, and potential second-order effects that can complicate the interpretation of results(Lange et al., 2023) and the hydra effect (McGrath et al., 2023; Rushing & Nanda, 2024) (see discussion in.2). Recent advancements aim to address these limitations, such as automated circuit discoveryalgorithms (Conmy et al., 2023), gradient-based methods for scalable component importance estimation likeattribution patching (Nanda, 2023d; Syed et al., 2023), and techniques to mitigate self-repair interferenceduring analysis (Ferrando & Voita, 2024).",
  "Hypothesis Testing": "In addition to the causal abstraction framework, several methods have been developed for rigorous hypothesistesting about neural network behavior. These methods aim to formalize and empirically validate explanationsof how neural networks implement specific behaviors. Causal scrubbing (Chan et al., 2022) formalizes hypotheses as a tuple (G, I, c), where G is the modelscomputational graph, I is an interpretable computational graph hypothesized to explain the behavior, and cmaps nodes of I to nodes of G. This method replaces activations in G with others that should be equivalentaccording to the hypothesis, measuring performance on the scrubbed model to validate the hypothesis. Locally consistent abstractions (Jenner et al., 2023) offer a more permissive approach, checking the consis-tency between the neural network and the explanation only one step away from the intervention node. Thismethod forms a middle ground between the strictness of full causal abstraction and the flexibility of causalscrubbing. These methods form a hierarchy of strictness, with full causal abstractions being the most stringent, followedby locally consistent abstractions and causal scrubbing being the most permissive. This hierarchy highlightstrade-offs in choosing stricter or more permissive notions, affecting the ability to find acceptable explanations,generalization, and mechanistic anomaly detection.",
  "Integrating Observation and Intervention": "To comprehensively understand internal neural network mechanisms, combining observational and interven-tional methods is crucial. For instance, sparse autoencoders can be used to disentangle superposed features(Cunningham et al., 2024), followed by targeted activation patching to test the causal importance of thesefeatures (Wang et al., 2023). Similarly, the logit lens can track prediction formation across layers (nostalge-braist, 2020), with subsequent interventions confirming causal relationships at key points. Probing techniquescan identify encoded information (Belinkov, 2021), which can then be subjected to causal abstraction (Geigeret al., 2023a) to understand how this information is utilized. This iterative refinement process, where broadobservational methods guide targeted interventions and intervention results inform further observations, en-ables a multi-level analysis that builds a holistic understanding across different levels of abstraction. Recentwork (Marks et al., 2024; Bushnaq et al., 2024; Braun et al., 2024; ONeill & Bui, 2024; Ge et al., 2024)demonstrates the potential of integrating sparse autoencoders with automated circuits discovery (Conmyet al., 2023; Syed et al., 2023), combining feature-level analysis with circuit-level interventions to uncoverthe interplay between representation and mechanism.",
  "Current Research": "This section surveys current research in mechanistic interpretability across three approaches based on whenand how the model is interpreted during training: Intrinsic interpretability methods are applied before train-ing to enhance the models inherent interpretability (.1). Developmental interpretability involvesstudying the models learning dynamics and the emergence of internal structures during training (.2).",
  "Survey": ": Key desiderata for interpretability approaches across training and analysis stages: (1) Intrinsic:Architectural biases for sparsity, modularity, and disentangled representations. (2) Developmental: Predictivecapability for phase transitions, manageable number of critical transitions, and a unifying theory connectingobservations to singularity geometry. (3) Post-hoc: Global, comprehensive, automated discovery of criticalcircuits, uncovering transferable principles across models/tasks, and extracting high-level causal mechanisms.",
  "Intrinsic Interpretability": "Intrinsic methods for mechanistic interpretability offer a promising approach to designing neural networksthat are more amenable to reverse engineering without sacrificing performance. By encouraging sparsity,modularity, and monosemanticity through architectural choices and training procedures, these methods aimto make the reverse engineering process more tractable. Intrinsic interpretability methods aim to constrain the training process to make learned programs moreinterpretable (Friedman et al., 2023b). This approach is closely related to neurosymbolic learning (Riegelet al., 2020) and can involve techniques like regularization with spatial structure, akin to the organizationof information in the human brain (Liu et al., 2023a;b). Recent work has explored various architectural choices and training procedures to improve the interpretabilityof neural networks. Jermyn et al. (2022) and Elhage et al. (2022a) demonstrate that architectural choicescan affect monosemanticity, suggesting that models could be engineered to be more monosemantic. Sharkey(2023) propose using a bilinear layer instead of a linear layer to encourage monosemanticity in languagemodels. Liu et al. (2023a) and Liu et al. (2023b) introduce a biologically inspired spatial regularization regime calledbrain-inspired modular training for forming modules in networks during training. They showcase how thiscan help RNNs exhibit brain-like anatomical modularity without degrading performance, in contrast to naiveattempts to use sparsity to reduce the cost of having more neurons per layer (Jermyn et al., 2022; Brickenet al., 2023). Preceding the mechanistic interpretability literature, various works have explored techniques to improveinterpretability, such as sparse attention (Zhang et al., 2021), adding L1 penalties to neuron activations(Kasioumis et al., 2021; Georgiadis, 2019), and pruning neurons (Frankle & Carbin, 2019). These techniqueshave been shown to encourage sparsity, modularity, and disentanglement, which are essential aspects ofintrinsic interpretability.",
  "Post-Hoc Interpretability": "In applied mechanistic interpretability, researchers explore various facets and methodologies to uncover theinner workings of AI models. Some key distinctions are drawn between global versus local interpretabilityand comprehensive versus partial interpretability. Global interpretability aims to uncover general patternsand behaviors of a model, providing insights that apply broadly across many instances (Doshi-Velez & Kim,2017; Nanda, 2023e). In contrast, local interpretability explains the reasons behind a models decisions forparticular instances, offering insights into individual predictions or behaviors. Comprehensive interpretabilityinvolves achieving a deep and exhaustive understanding of a models behavior, providing a holistic view ofits inner workings (Nanda, 2023e). In contrast, partial interpretability often applied to larger and morecomplex models, concentrates on interpreting specific aspects or subsets of the models behavior, focusingon the applications most relevant or critical areas. Large Models Narrow Behavior.Circuit-style mechanistic interpretability aims to explain neuralnetworks by reverse engineering the underlying mechanisms at the level of individual neurons or subgraphs.This approach assumes that neural vector representations encode high-level concepts and circuits defined bymodel weights encode meaningful algorithms (Olah et al., 2020; Cammarata et al., 2020). Studies on deepnetworks support these claims, identifying circuits responsible for detecting curved lines or object orientation(Cammarata et al., 2020; 2021; Voss et al., 2021).",
  "Automation: Scaling Post-Hoc Interpretability": "As models become more complex, automating key aspects of the interpretability workflow becomes in-creasingly crucial. Tracing a models computational pathways is highly labor-intensive, quickly becominginfeasible as the model size increases. Automating the discovery of relevant circuits and their functionalinterpretation represents a pivotal step towards scalable and comprehensive model understanding (Nainani,2024). Dissecting Models into Interpretable Circuits.The first major automation challenge is identifyingthe critical computational sub-circuits or components underpinning a models behavior for a given task. Apioneering line of work aims to achieve this via efficient masking or patching procedures. Methods likeautomated circuit discovery (Conmy et al., 2023) and attribution patching (Syed et al., 2023; Kramr et al.,2024) iteratively knock out model activations, pinpointing components whose removal has the most significantimpact on performance. This masking approach has proven scalable even to large models (Lieberum et al.,2023).",
  ": Potential benefits and risks of mechanistic interpretability for AI safety": "How Could Interpretability Promote AI Safety?Gaining mechanistic insights into the inner work-ings of AI systems seems crucial for navigating AI safety as we develop more powerful models (Nanda, 2022e).Interpretability tools can provide an understanding of artificial cognition, the way AI systems process infor-mation and make decisions, which offers several potential benefits:",
  "Research Issues": "Need for Comprehensive, Multi-Pronged Approaches.Current interpretability research often fo-cuses on individual techniques rather than combining complementary approaches.To achieve a holisticunderstanding of neural networks, we propose utilizing a diverse interpretability toolbox that integratesmultiple methods (see also .4), such as: (i.) Coordinating observational (e.g., probing, logit lens)and interventional methods (e.g., activation patching) to establish causal relationships. (ii.) Combiningfeature-level analysis (e.g., sparse autoencoders) with circuit-level interventions (e.g., path patching) touncover representation-mechanism interplay.(iii.)Integrating intrinsic interpretability approaches withpost-hoc analysis for robust understanding. For example, coordinated methods could be used for reverse engineering trojaned behaviors (Casper et al.,2023c), where observational techniques identify suspicious activations, interventional methods isolate therelevant circuits, and intrinsic approaches guide the design of more robust architectures. Cherry-Picking and Streetlight Interpretability.Another concerning pattern is the tendency tocherry-pick results, relying on a small number of convincing examples or visualizations as the basis for anargument without comprehensive evaluation (Ruker et al., 2023). This amounts to publication bias, show-casing an unrealistic highlight reel of best-case performance. Relatedly, many interpretability techniquesare primarily evaluated on small toy models and tasks (Chughtai et al., 2023; Elhage et al., 2022b; Jermynet al., 2022; Chen et al., 2023b), risking missing critical phenomena that only emerge in more realistic anddiverse contexts. This focus on cherry-picked results from toy models is a form of streetlight interpretability(Casper, 2023), examining AI systems under only ideal conditions of maximal interpretability.",
  "Technical Limitations": "Scalability Challenges and Risks of Human Reliance.A critical hurdle is demonstrating the scala-bility of mechanistic interpretability to real-world AI systems across model size, task complexity, behavioralcoverage, and analysis efficiency (Elhage et al., 2022b; Scherlis et al., 2023). Achieving a truly comprehen-sive understanding of a models capabilities in all contexts is daunting, and the time and compute requiredmust scale tractably. Automating interpretability techniques is crucial, as manual analysis quickly becomesinfeasible for large models. The high human involvement in current interpretability research raises concernsabout the scalability and validity of human-generated model interpretations. Subjective, inconsistent humanevaluations and lack of ground-truth benchmarks are known issues (Ruker et al., 2023). As models scale,it will become increasingly untenable to rely on humans to hypothesize about model mechanisms manually.More work is needed on automating the discovery of mechanistic explanations and translating model weightsinto human-readable computational graphs (Elhage et al., 2022b), but progress on that front may also comefrom outside the field (Lu et al., 2024). Obstacles to Bottom-Up Interpretability.There are fundamental questions about the tractability offully reverse engineering neural networks from the bottom up, especially as models become more complex(Hendrycks, 2023a). Models may learn internal representations and algorithms that do not cleanly mapto human-understandable concepts, making them difficult to interpret even with complete transparency(McGrath et al., 2022). This gap between human and model ontologies may widen as architectures evolve,increasing opaqueness (Hendrycks et al., 2022). Conversely, model representations might naturally convergeto more human-interpretable forms as capability increases (Hubinger, 2019a; Feng & Steinhardt, 2023). Analyzing Models Embedded in Environments.Real-world AI systems embedded in rich, interac-tive environments exhibit two forms of in-context behavior that pose significant interpretability challengesbeyond understanding models in isolation. Externally, models may dynamically adapt to and reshape theirenvironments through in-context learning from the interactions and feedback loops with their external en-vironment (Leahy, 2023). Internally, the hydra effect demonstrates in-context reorganization, where modelsflexibly reorganize their internal representations in a context-dependent manner to maintain capabilities evenafter ablating key components (McGrath et al., 2023). These two instances of in-context behavior externaladaptation to the environment and internal self-reorganization undermine interpretability approaches thatassume fixed circuits. For models deeply embedded in rich real-world settings, their dynamic coupling withthe external world via in-context environmental learning and their internal in-context representational re-organization make strong interpretability guarantees difficult to attain through analysis of the initial modelalone. Adversarial Pressure Against Interpretability.As models become more capable through increasedtraining and optimization, there is a risk they may learn deceptive behaviors that actively obscure or misleadthe interpretability techniques meant to understand them. Models could develop adversarial \"mind-reader\"components that predict and counteract the specific analysis methods used to interpret their inner workings(Sharkey, 2022; Hubinger, 2022). Optimizing models through techniques like gradient descent could inadver-tently make their internal representations less interpretable to external observers (Hubinger, 2019b; Fu et al.,2023b; von Oswald et al., 2023). In extreme cases, a highly advanced AI system singularly focused on pre-serving its core objectives may directly undermine the fundamental assumptions that enable interpretabilitymethods in the first place. These adversarial dynamics, where the capabilities of the AI model are pitted against efforts to interpret it,underscore the need for interpretability research to prioritize worst-case robustness rather than just average-case scenarios. Current techniques often fail even when models are not adversarially optimized. Achievinghigh confidence in fully understanding extremely capable AI models may require fundamental advances tomake interpretability frameworks resilient against an intelligent systems active deceptive efforts.",
  "Future Directions": "Given the current limitations and challenges, several key research problems emerge as critical for advanc-ing mechanistic interpretability.These problems span four main areas: emphasizing conceptual clarity(.1), establishing rigorous standards (.2), improving the scalability of interpretability tech-niques (.3), and expanding the research scope (.4).Each subsection presents specificresearch questions and challenges that need to be addressed to move the field forward.Future Directions",
  "Clarifying Concepts": "Integrating with Existing Literature.To mature, mechanistic interpretability should embrace exist-ing work, using established terminology rather than reinventing the wheel. Diverging terminology inhibitscollaboration across disciplines. Presently, the terminology used for mechanistic interpretability partiallydiverges from mainstream AI research (Casper, 2023). For example, while the mainstream speaks of dis-tributed representations (Hinton, 1984; Olah, 2023) and the goal of disentangled representations (Higginset al., 2018; Locatello et al., 2019), the mechanistic interpretability literature refers to the same phenomenonas polysemanticity (Scherlis et al., 2023; Lecomte et al., 2023; Marshall & Kirchner, 2024) and superposition(Elhage et al., 2022b; Henighan et al., 2023). Using common language invites \"accidental\" contributions andprevents isolating mechanistic interpretability from broader AI research. Mechanistic interpretability relates to many other fields in AI research, including compressed sensing (Elhageet al., 2022b), modularity, adversarial robustness, continual learning, network compression (Ruker et al.,2023), neurosymbolic reasoning, trojan detection, and program synthesis (Casper, 2023; Michaud et al.,2024), and causal representation learning.These relationships can help develop new methods, metrics,benchmarks, and theoretical frameworks. For instance:",
  "i.) Neurosymbolic Reasoning and Program Synthesis: Mechanistic interpretability aims for": "reverse engineering neural networks by converting their weights into human-readable algorithms.This endeavor can draw inspiration from neurosymbolic reasoning (Riegel et al., 2020) and programsynthesis. Techniques like creating programs in domain-specific languages (Verma et al., 2019b;a;Trivedi et al., 2021), extracting decision trees (Zhang et al., 2019) or symbolic causal graphs (Renet al., 2023) from neural networks align well with the goals of mechanistic interpretability. Adoptingthese approaches can extend the toolkit for reverse engineering AI systems. ii.) Causal Representation Learning: Causal Representation Learning (CRL) aims to discover anddisentangle underlying causal factors in data (Schlkopf et al., 2021), complementing mechanisticinterpretabilitys goal of understanding causal structures within neural networks. While mechanis-tic interpretability typically examines individual features and circuits, CRL offers a framework for",
  "More details on the interplay between interpretability, robustness, modularity, continual learning, networkcompression, and the human visual system can be found in the review by Ruker et al. (2023)": "Corroborate or Refute Core Assumptions.Features are the fundamental units defining neural repre-sentations and enabling mechanistic interpretabilitys bottom-up approach (Chan, 2023), but defining theminvolves assumptions requiring scrutiny, as they shape interpretations and research directions. Questioninghypotheses by seeking additional evidence or counter-examples is crucial. The linear representation hypothesis treats activation directions as features (Park et al., 2023a; Nanda et al.,2023b; Elhage et al., 2022b), but the emergence and necessity of linearity is unclear is it architecturalbias or inherent? Stronger theory justifying linearitys necessity or counter-examples like autoencoders onuncorrelated data without intermediate linear layers (Elhage et al., 2022b) are needed. An alternative lensviews features as polytopes from piecewise linear activations (Black et al., 2022), questioning if directionsimplification suffices or added polytope complexity aids interpretability. The superposition hypothesis suggests that polysemantic neurons arise from the network compressing andrepresenting many features within its limited set of neurons (Elhage et al., 2022b), but polysemanticitycan also occur incidentally due to redundancy (Lecomte et al., 2023; Marshall & Kirchner, 2024; McGrathet al., 2023). Understanding superpositions role could inform mitigating polysemanticity via regularization",
  "Setting Standards": "Prioritizing Robustness over Capability Advancement.As the mechanistic interpretability com-munity expands, it is essential to maintain the norm of not advancing AI capabilities while simultaneouslyestablishing metrics necessary for the fields progress (Ruker et al., 2023). Researchers should prioritizedeveloping comprehensive tools for analyzing the worst-case performance of AI systems, ensuring robustnessand reliability in critical applications. This includes focusing on adversarial tasks, such as backdoor detectionand removal (Lamparth & Reuel, 2023; Hubinger et al., 2024; Wu et al., 2022a), and evaluating the accuracyof explanations in producing adversarial examples (Goldowsky-Dill et al., 2023). Establishing Metrics, Benchmarks, and Algorithmic Testbeds.A central challenge in mechanisticinterpretability is the lack of rigorous evaluation methods. Relying solely on intuition can lead to conflatinghypotheses with conclusions, resulting in cherry-picking and optimizing for best-case rather than averageor worst-case performance (Rudin, 2019; Miller, 2019; Ruker et al., 2023; Casper, 2023). Current ad hocpractices and proxy measures (Doshi-Velez & Kim, 2017) risk over-optimization (Goodharts law Whena measure becomes a target, it ceases to be a good measure).Distinguishing correlation from causationis crucial, as interpretability illusions demonstrate that visualizations may be meaningless without causallinking (Bolukbasi et al., 2021; Friedman et al., 2023a; Olah et al., 2017). To advance the field, rigorous evaluation methods are needed. These should include: (i) assessing out-of-distribution inputs, as most current methods are only valid for specific examples or datasets (Ruker et al.,2023; Ilyas et al., 2019; Mu & Andreas, 2020; Casper et al., 2023c; Burns et al., 2023); (ii) controlling systemsthrough edits, such as implanting or removing trojans (Mazeika et al., 2022) or targeted editing (Ghorbani& Zou, 2020; Dai et al., 2022; Meng et al., 2022a;b; Bau et al., 2018; Hase et al., 2023); (iii) replacingcomponents with simpler reverse-engineered alternatives (Lindner et al., 2023); and (iv) comprehensiveevaluation through replacing components with hypothesized circuits (Quirke et al., 2024). Algorithmic testbeds are essential for evaluating faithfulness (Jacovi & Goldberg, 2020; Hanna et al., 2024)and falsifiability (Leavitt & Morcos, 2020).Tools like Tracr (Lindner et al., 2023) can provide groundtruth labels for benchmarking search methods (Goldowsky-Dill et al., 2023), while toy models studyingsuperposition in computation (Vaintrob et al., 2024) and transformers on algorithmic tasks can quantifysparsity and test intrinsic methods. Recently, Thurnherr & Scheurer (2024); Gupta et al. (2024) introduceddatasets of transformer weights with known circuits for evaluating mechanistic interpretability techniques.",
  "Scaling Techniques": "Broader and Deeper Coverage of Complex Models and Behaviors.A primary goal in scalingmechanistic interpretability is pushing the Pareto frontier between model and task complexity and thecoverage of interpretability techniques (Chan, 2023).While efforts have focused on larger models, it isequally crucial to scale to more complex tasks and provide comprehensive explanations essential for provablesafety (Tegmark & Omohundro, 2023; Dalrymple et al., 2024; Gross et al., 2024) and enumerative safety(Cunningham et al., 2024; Elhage et al., 2022b) by ensuring models wont engage in dangerous behaviorslike deception. Future work should aim for thorough reverse engineering (Quirke & Barez, 2023), integratingproven modules into larger networks (Nanda et al., 2023a), and capturing sequences encoded in hidden statesbeyond immediate predictions (Pal et al., 2023). Deepening analysis complexity is also key, validating therealism of toy models (Elhage et al., 2022b) and extending techniques like path patching (Goldowsky-Dillet al., 2023; Liu et al., 2023a) to larger language models. The field must move beyond small transformers",
  "on algorithmic tasks (Nanda et al., 2023a) and limited scenarios (Friedman et al., 2023a) to tackle morecomplex, realistic cases": "Towards Universality.As mechanistic interpretability matures, the field must transition from isolatedempirical findings to developing overarching theories and universal reasoning primitives beyond specificcircuits, aiming for a comprehensive understanding of AI capabilities. While collecting empirical data remainsvaluable (Nanda, 2023f), establishing motifs, empirical laws, and theories capturing universal model behavioraspects is crucial. This may involve finding more circuits/features (Nanda, 2022a;c), exploring circuits as alens for memorization/generalization (Hanna et al., 2023), identifying primitive general reasoning skills (Feng& Steinhardt, 2023), generalizing specific findings to model-agnostic phenomena (Merullo et al., 2023), andinvestigating emergent model generality across neural network classes (Ivanitskiy et al., 2023). Identifyinguniversal reasoning patterns and unifying theories is key to advancing interpretability. Automation.Implementing automated methods is crucial for scaling interpretability of real-world state-of-the-art models across size, task complexity, behavior coverage, and analysis time (Hobbhahn, 2022).Manual circuit identification is labor-intensive (Lieberum et al., 2023), so automated techniques like circuitdiscovery and sparse autoencoders can enhance the process (Foote et al., 2023; Nanda, 2023b). Future workshould automatically create varying datasets for understanding circuit functionality (Conmy et al., 2023),develop automated hypothesis search (Goldowsky-Dill et al., 2023), and investigate attention head/MLPinterplay (Monea et al., 2023). Scaling sparse autoencoders to extract high-quality features automaticallyfor frontier models is critical (Bricken et al., 2023). Still, it requires caution regarding potential downsideslike AI iteration outpacing training (RicG, 2023) and loss of human interpretability from tool complexity(Doshi-Velez & Kim, 2017).",
  "Expanding Scope": "Interpretability Across Training.While mechanistic interpretability of final trained models is a prereq-uisite, the field should also advance interpretability before and during training by studying learning dynamics(Nanda, 2022b; Elhage et al., 2022b; Hubinger, 2022). This includes tracking neuron development (Liu et al.,2021), analyzing neuron set changes with scale (Michaud et al., 2023), and investigating emergent computa-tions (Quirke & Barez, 2023). Studying phase transitions could yield safety insights for reward hacking risks(Olsson et al., 2022). Multi-Level Analysis.Complementing the predominant bottom-up methods (Hanna et al., 2023), mech-anistic interpretability should explore top-down and hybrid approaches, a promising yet neglected avenue.The top-down analysis offers a tractable way to study large models and guide microscopic research withmacroscopic observations (Variengien & Winsor, 2023). Its computational efficiency could enable extensive\"comparative anatomy\" of diverse models, revealing high-level motifs underlying abilities. These motifs couldserve as analysis units for understanding internal modifications from techniques like instruction fine-tuning(Ouyang et al., 2022) and reinforcement learning from human feedback (Christiano et al., 2017; Bai et al.,2022). New Frontiers: Vision, Multimodal, and Reinforcement Learning Models.While some mecha-nistic interpretability has explored convolutional neural networks for vision (Cammarata et al., 2021; 2020),vision-language models (Palit et al., 2023; Salin et al., 2022; Hilton et al., 2020), and multimodal neurons(Goh et al., 2021), little work has focused on vision transformers (Palit et al., 2023; Aflalo et al., 2022;Vilas et al., 2023; Pan et al., 2024). Future efforts could identify mechanisms within vision-language models,mirroring progress in unimodal language models (Nanda et al., 2023a; Wang et al., 2023). Reinforcement learning (RL) is also a crucial frontier given its role in advanced AI training via techniqueslike reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Bai et al., 2022), despitepotentially posing significant safety risks (Bereska & Gavves, 2023; Casper et al., 2023a). Interpretability ofRL should investigate reward/goal representations (Mini et al., 2023; Colognese & Jozdien, 2023; Colognese,2023; Bloom & Colognese, 2023; Bloom & Bailey, 2023), study circuitry changes from alignment algorithms(Prakash et al., 2024; Jain et al., 2023; Lee et al., 2024; Jain et al., 2024), and explore emergent subgoals",
  "Glossary": "circuits Sub-graphs within neural networks consisting of features and the weights connecting them. Circuitscan be thought of as computational primitives that perform understandable operations to produce(ideally interpretable) features from prior (ideally interpretable) features. Examples include circuitsfor detecting curves at specific orientations (Cammarata et al., 2020; 2021), continuing repeatedpatterns in text (Olsson et al., 2022), and resolving anaphoric references (Wang et al., 2023). Whilecircuits can involve clearly interpretable features, the definition allows for intermediate representa-tions that are less easily interpretable.. 3, 9, 10, 20, 26, 27, 35",
  "deceptive alignment When a misaligned model aims to appear aligned to gain more power to take controlonce sufficiently powerful.. 24": "deceptive inflation Theoretical result on deceptive behavior: policies produce trajectories that look betterthan they actually are from the humans perspective with limited observations to get higher rewardsignals during training. This deceptive behavior arises in reinforcement learning from human feed-back when the human provides feedback based only on partial observations of the trajectories, whilethe policy has full state information during training (Lang et al., 2024).. 28 disentangled In disentangled representations, individual dimensions or components correspond to distinct,independent factors of variation in the data, rather than representing a tangled mixture of thesefactors.. 4, 9, 15, 16, 20, 27, 32, 33 eliciting latent knowledge Developing strategies to make a machine learning model explicitly report la-tent facts or knowledge embedded in its parameters, especially in cases where the models outputis untrusted (Christiano et al., 2021). This involves finding patterns in neural network activationsthat track the true state of the world (Mallen & Belrose, 2023).. 24 features The fundamental units of how neural networks encode knowledge, which cannot be further de-composed into smaller, distinct concepts. Features are core components of a neural networks rep-resentation, analogous to how cells form the fundamental unit of biological organisms (Olah et al.,2020). The superposition hypothesis suggests an alternative definition: that features correspondto the disentangled concepts that a larger, sparser network with sufficient capacity would learn torepresent with individual (monosemantic) neurons (Olah et al., 2020; Bricken et al., 2023).. 3, 4,68, 10, 13, 16, 18, 20, 23, 27, 28, 32, 33, 35 grokking \"Grokking refers to the surprising phenomenon of delayed generalization where neural networks,on certain learning problems, generalize long after overfitting their training set.\" (Liu et al., 2022a).21, 22 hydra effect The phenomenon where models can internally self-repair and maintain capabilities even whenkey components are ablated, making it challenging to identify the relevant components underlyinga particular behavior (McGrath et al., 2023).. 18, 26 inner misalignment Inner misalignment, or goal misgeneralization, occurs when an AI system developsgoals or behaviors during training that are misaligned with the intended objectives despite a correctlyspecified reward signal.. 24, 28",
  ". Separable: p(a, b) = p(a)p(b)2. A mixture: p(a, b) = wp1(a, b) + (1 w)p2(a, b) where p1 is lower-dimensional": "Features are defined as irreducible patterns that cannot be decomposed into separable or mixture dis-tributions via such transformations. This formalizes the notion that features form the fundamentalatomic units underlying neural representations. Features that can be disentangled into statisticallyindependent components (separable) or simpler lower-dimensional factors (mixtures) are not con-sidered the core representational primitives.The key properties are that 1) features map fromthe input space to higher-dimensional representational spaces, 2) features are sparse and only acti-vated on subsets of the input, and crucially, 3) features are irreducible and cannot be expressed astransformations of other statistically independent components.. 4 iterative distillation and amplification A technique for training AI systems by repeatedly distillingknowledge from a larger model into a smaller one while amplifying the smaller models capabili-ties through feedback and interaction with humans.. 24",
  "mesa-optimization The emergence of unintended subagents within a model with their own objectives,potentially misaligned with the original training objective.. 24": "microscope AI Systems that extract and utilize knowledge from a model without allowing the model totake autonomous actions. This involves reverse engineering a trained model to understand its learnedknowledge about the world, aiming to leverage this understanding directly without deploying themodel in an operational capacity.. 24 modularity The property of an AI system being composed of distinct, semi-independent components orsubmodules that can be separately understood, modified, and recombined, rather than a monolithic,opaque structure.. 20, 21 monosemantic A neuron corresponding to a single concept. The intuition is that analyzing what inputsactivate a given neuron reveals its associated semantic meaning or concept. In contrast to polyse-mantic.. 5, 8, 14, 16, 20, 32, 34 motifs Repeating patterns that emerge across models and tasks, manifesting as circuits, features, or higher-level behaviors from component interactions. Examples include curve detectors, induction circuits,and branch specialization.Motifs reveal common structures and mechanisms underlying neuralnetwork intelligence.. 3, 10, 21, 35 natural abstractions High-level summaries or descriptions of a system or environment learned and usedby many cognitive systems. According to the natural abstraction hypothesis (Chan et al., 2023), aset of \"natural\" abstractions exist that represent redundantly encoded information in the world andtend to be learned by intelligent systems produced through local selection pressures. These naturalabstractions form a relatively small, discrete set of concepts like \"tree,\" \"velocity,\" etc., that allowcompact descriptions of the world while discarding many irrelevant low-level details.. 4, 11, 32",
  "prediction orthogonality A model whose objective is prediction can simulate agents who optimize towardany objectives with any degree of optimality (janus, 2022).. 3, 12, 24": "privileged basis In certain neural network representations, the basis directions formed by the individualneurons are architecturally distinguished from arbitrary directions in the activation space. Thisprivileged basis makes it meaningful to analyze the properties and roles of individual neurons, asthe architecture encourages features to align with these basis directions. Hence, a privileged basis isnecessary but not sufficient for the formation of monosemantic neurons. (Elhage et al., 2022b).. 5 representation engineering A top-down approach to transparency research that treats representationsas the fundamental unit of analysis, aiming to understand and control representations of high-levelcognitive phenomena in neural networks like large language models. Representation engineering hastwo main areas: 1) Reading representations to probe and interpret their contents, and 2) Controllingrepresentations to manipulate high-level concepts like honesty or morality (Zou et al., 2023).. 3, 9,22 reverse engineering The process of deconstructing a neural networks computations to fully understandand specify its operations. This involves breaking down the networks functionality into explicit,interpretable components, potentially as clear and detailed as pseudocode.. 1, 3, 20, 21, 2527, 29",
  "reward hacking See outer misalignment.. 30": "simulacra The text outputs generated by a predictive model simulating the causal processes underlying textcreation. These outputs simulate coherent and contextually relevant language, sometimes exhibitingagentic behaviors or goals despite the predictive model itself lacking genuine agency or intentionality.Simulacra can be either agentic, mimicking intentional and persuasive language use, or non-agentic,merely generating descriptive text without simulated goals or agency (janus, 2022; Bereska & Gavves,2023).. 12, 14 simulation The simulation hypothesis says that when scaled up sufficiently, predictive models will learnto simulate the real-world causal processes that generated their training data (janus, 2022). Whenthese models are optimized for predictive accuracy on broad data distributions like natural language,they are incentivized to discover the underlying rules, physics, and semantics that govern the datato model and predict future observations effectively. This allows the models to go beyond just mem-orizing or pattern-matching their training sets, instead learning to simulate hypothetical scenarios,reason about counterfactuals, and exhibit behaviors characteristic of general intelligence all as abyproduct of the drive for efficient compression and accurate prediction. The simulation hypothesissuggests these models will develop rich internal world models capturing the causal dynamics of thetraining distribution.. 3, 11, 12, 14, 24",
  "Nicholas Bai, Rahul Ajay Iyer, Tuomas Oikarinen, and Tsui-Wei Weng. Describe-and-dissect: Interpretingneurons in vision networks with language models. ICML MI Workshop, June 2024. 23": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, TomConerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, ScottJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, JackClark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmlessassistant with reinforcement learning from human feedback. CoRR, April 2022. 30",
  "Dan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey. Identifying functionally importantfeatures with end-to-end sparse dictionary learning. ICML MI Workshop, May 2024. 19": "Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nicholas L.Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, NicholasSchiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E. Burke,Tristan Hume, Shan Carter, Tom Henighan, and Chris Olah. Towards monosemanticity: Decomposinglanguage models with dictionary learning. Transformer Circuits Thread, October 2023. 5, 6, 8, 9, 11, 13,15, 20, 30, 32",
  "Stephen Casper, Max Nadeau, Dylan Hadfield-Menell, and Gabriel Kreiman. Robust feature-level adversariesare interpretability tools. NeurIPS, October 2021. 28": "Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jrmy Scheurer, Javier Rando,Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphal Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, UsmanAnwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii Krasheninnikov, XinChen, Lauro Langosco, Peter Hase, Erdem Byk, Anca Dragan, David Krueger, Dorsa Sadigh, and Dy-lan Hadfield-Menell. Open problems and fundamental limitations of reinforcement learning from humanfeedback. CoRR, 2023a. 24, 30",
  "Stephen Casper, Yuxiao Li, Jiawei Li, Tong Bu, Kevin Zhang, Kaivalya Hariharan, and Dylan Hadfield-Menell. Red teaming deep neural networks with feature synthesis tools. NeurIPS, 2023c. 25, 29": "Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall,Andreas Haupt, Kevin Wei, Jrmy Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Mar-vin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max Tegmark,David Krueger, and Dylan Hadfield-Menell. Black-box access is insufficient for rigorous ai audits. ACMConference on Fairness, Accountability, and Transparency, January 2024. 1, 28",
  "Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrainedtransformers. ACL, 2022. 5, 29": "David \"davidad\" Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, SteveOmohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, ClarkBarrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, and Joshua Tenenbaum. Towards guaranteed safeai: A framework for ensuring robust and reliable ai systems. CoRR, May 2024. 29, 35 Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. What isone grain of sand in the desert? analyzing individual neurons in deep nlp models. Proceedings of the AAAIConference on Artificial Intelligence, July 2019. 14",
  "Yanai Elazar, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. Amnesic probing: Behavioral explanationwith amnesic counterfactuals. TACL, February 2021. 14": "Nelson Elhage, Tristan Hume, Olsson Catherine, Nanda Neel, Tom Henighan, Scott Johnston, SheerElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, KamalNdousse, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jack-son Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah.Softmax linear units. Transformer Circuits Thread, 2022a. 5, 8, 13, 20 Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, ZacHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. TransformerCircuits Thread, 2022b. 5, 6, 7, 9, 15, 25, 26, 27, 28, 29, 30, 34, 35",
  "Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bern-hard Schlkopf. Recurrent independent mechanisms. CoRR, November 2020. 28": "Jason Gross, Rajashree Agrawal, Thomas Kwa, Euan Ong, Chun Hei Yip, Alex Gibson, Soufiane Noubir,and Lawrence Chan. Compact proofs of model performance via mechanistic interpretability. ICML MIWorkshop, June 2024. 29 Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukoiute, Karina Nguyen, Nicholas Joseph,Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalizationwith influence functions. CoRR, August 2023. 13",
  "Kaarel Hnni, Jake Mendel, Dmitry Vaintrob, and Lawrence Chan. Mathematical models of computationin superposition. ICML MI Workshop, August 2024. 29": "Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprisingdifferences in causality-based localization vs. knowledge editing in language models. NeurIPS Spotlight,January 2023. 29 Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, Qinyuan Cheng, and Xipeng Qiu. Dictionary learningimproves patch-free circuit discovery in mechanistic interpretability: A case study on othello-gpt. CoRR,2024. 15",
  "Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks. CoRR,October 2023. 1, 24": "Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, NicholasSchiefer, and Christopher Olah. Superposition, memorization, and double descent. Transformer CircuitsThread, 2023. 7, 27 Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage,Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, CatherineOlsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish. Scaling laws and inter-pretability of learning from repeated data. CoRR, 2022. 21 Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg, Jacob Andreas, YonatanBelinkov, and David Bau. Linearity of relation decoding in transformer language models. CoRR, August2023. 9",
  "Marius Hobbhahn and Lawrence Chan. Should we publish mechanistic interpretability research? AI Align-ment Forum, April 2023. 25": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan,Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large languagemodels. CoRR, March 2022. 25",
  "Evan Hubinger, Adam Jermyn, Johannes Treutlein, Rubi Hudson, and Kate Woolverton.Conditioningpredictive models: Risks and strategies. CoRR, February 2023. 24, 31": "Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham,Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan,Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan,Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, ZacharyWitten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, LoganGraham, Jared Kaplan, Sren Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, andEthan Perez. Sleeper agents: Training deceptive llms that persist through safety training. CoRR, 2024.28, 29",
  "Alon Jacovi and Yoav Goldberg. Towards faithfully interpretable nlp systems: How should we define andevaluate faithfulness? CoRR, April 2020. 29": "Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette,Tim Rocktschel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on proce-durally defined tasks. CoRR, November 2023. 30 Samyak Jain, Ekdeep Singh Lubana, Kemal Oksuz, Tom Joy, Philip Torr, Amartya Sanyal, and Puneet K.Dokania. What makes and breaks safety fine-tuning? a mechanistic study. ICML MI Workshop, June2024. 30",
  "Adam S. Jermyn, Nicholas Schiefer, and Evan Hubinger. Engineering monosemanticity in toy models. CoRR,November 2022. 8, 20, 25": "Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, ZhonghaoHe, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan OGara,Yingshan Lei, Hua Xu, Brian Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-ChunZhu, Yike Guo, and Wen Gao. Ai alignment: A comprehensive survey. CoRR, January 2024. 1",
  "Kenneth Li, Oam Patel, Fernanda Vigas, Hanspeter Pfister, and Martin Wattenberg. Inference-time inter-vention: Eliciting truthful answers from a language model. NeurIPS Spotlight, July 2023b. 9": "Yixuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent learning: Do differentneural networks learn the same representations? NIPS Workshop on Feature Extraction, December 2015.11 Tom Lieberum, Matthew Rahtz, Jnos Kramr, Neel Nanda, Geoffrey Irving, Rohin Shah, and VladimirMikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chin-chilla. CoRR, July 2023. 10, 17, 22, 29, 30",
  "Vision Res, December 1997. 15, 16": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, DarioAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learningand induction heads. Transformer Circuits Thread, 2022. 10, 21, 22, 25, 30, 32",
  "Competition of mechanisms: Tracing how language models handle facts and counterfactuals. ACL 2024,June 2024. 10": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Traininglanguage models to follow instructions with human feedback. CoRR, 2022. 30",
  "Daking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. A practical review of mechanisticinterpretability for transformer-based language models. CoRR, July 2024. 2": "Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, Jnos Kramr,Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. CoRR, April2024. 16 Tilman Ruker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A surveyon interpreting the inner structures of deep neural networks. TMLR, August 2023. 1, 13, 25, 26, 27, 28,29",
  "Hassan Sajjad, Nadir Durrani, and Fahim Dalvi. Neuron-level interpretation of deep nlp models: A survey.TACL, November 2022. 5": "Mansi Sakarvadia, Aswathy Ajith, Arham Khan, Daniel Grzenda, Nathaniel Hudson, Andr Bauer, KyleChard, and Ian Foster. Memory injections: Correcting multi-hop reasoning failures during inference intransformer-based language models. CoRR, September 2023a. 9 Mansi Sakarvadia, Arham Khan, Aswathy Ajith, Daniel Grzenda, Nathaniel Hudson, Andr Bauer, KyleChard, and Ian Foster. Attention lens: A tool for mechanistically interpreting the attention head infor-mation retrieval mechanism. CoRR, October 2023b. 15",
  "Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders.AI Alignment Forum, 2022b. 8, 15, 16": "Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, New-ton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell,Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, and EthanPerez. Towards understanding sycophancy in language models. CoRR, October 2023. 28, 35",
  "Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. Grokked transformers are implicit reasoners: A mechanisticjourney to the edge of generalization. ICML MI Workshop, June 2024. 21": "Kevin Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretabilityin the wild: a circuit for indirect object identification in gpt-2 small. ICLR, 2023. 10, 13, 17, 18, 19, 22,30, 32 Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R.Bowman. Blimp: The benchmark of linguistic minimal pairs for english. Transactions of the Associationfor Computational Linguistics, 2020. 2",
  "Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories inmechanistic explanation of neural networks. CoRR, 2023. 22": "Roland S. Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge, Thomas S. A. Wallis, andWieland Brendel. How well do feature visualizations support causal understanding of cnn activations?NeurIPS, November 2021. 13 Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, XuwangYin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, ZifanWang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and DanHendrycks. Representation engineering: A top-down approach to ai transparency. CoRR, October 2023.3, 9, 14, 22, 24, 34"
}