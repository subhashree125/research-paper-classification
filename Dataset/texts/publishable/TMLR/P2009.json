{
  "Abstract": "In this paper, we develop a theory about the relationship between invariant and equivariantmaps with regard to a group G. We then leverage this theory in the context of deep neuralnetworks with group symmetries in order to obtain novel insight into their mechanisms.More precisely, we establish a one-to-one relationship between equivariant maps and certaininvariant maps.This allows us to reduce arguments for equivariant maps to those forinvariant maps and vice versa. As an application, we propose a construction of universalequivariant architectures built from universal invariant networks. We, in turn, explain howthe universal architectures arising from our construction differ from standard equivariantarchitectures known to be universal. Furthermore, we explore the complexity, in terms ofthe number of free parameters, of our models, and discuss the relation between invariantand equivariant networks complexity. Finally, we also give an approximation rate for G-equivariant deep neural networks with ReLU activation functions for finite group G.",
  "Introduction": "Symmetries play a fundamental role in many machine learning tasks. Incorporating these symmetries intodeep learning models has proven to be a successful strategy in various contexts. Notable examples includethe use of convolutional neural networks (CNNs) to address translation symmetries (LeCun et al., 2015;Cohen & Welling, 2016), deep sets and graph neural networks for permutation symmetries (Zaheer et al.,2017; Scarselli et al., 2008; Kipf & Welling, 2017; Defferrard et al., 2016), and spherical CNNs for rotationsymmetries (Cohen et al., 2018; Esteves et al., 2020). The underlying common principle in all these casesis as follows: once the inherent symmetries of a target task are identified, the learning model is designed toencode these symmetries. In doing so, we aim to improve the quality of learning by building a model thatfits the characteristics of the task better. In mathematics, symmetries are represented by the concepts of groups and group actions. A group is aset of transformations, and the action of a group results in a transformation of a given set. The symmetriesof group actions are usually divided into two categories: invariant tasks and equivariant tasks. Invariant",
  "Published in Transactions on Machine Learning Research (09/2024)": "For a representation (, V ) of group G, we call a subspace W V G-invariant subspace if ()(W) W for any G.Then, (, W) is also a representation of G.This representation (, W) is called asubrepresentation of (, V ). When a representation (, V ) has no nontrivial subrepresentation, this is calledirreducible representation. This means that if (, W) is a subrepresentation of the irreducible representation(, V ), then (, W) is equal to (, {0}) or (, V ). By Maschkes theorem (Curtis & Reiner, 1966, (10.7),(10.8)),any finite-dimensional representation over R can be factorized to the direct sum of irreducible representationsup to isomorphic. In particular, irreducible representations are building blocks of representations. By Schurs Lemma (Curtis & Reiner, 1966, (27.3)), for two irreducible representations (, V ), (V ),HomG(V, V ) = 0 holds if and only if (, V ) is not isomorphic to (, V ). Thus, by this fact with Maschkestheorem, any intertwining operator can be factorized to the sum of intertwining operators between irreduciblerepresentations. For G = Sn, by the argument with Youngs diagrams, we can calculate how the irreducible representationschange by induction and restriction of them. By combining such argument and Theorem 5, we can calculatethe number of parameters of the weight matrix between the input layer and the first hidden layer as follows:",
  "In this paper, we explore the relation between invariant and equivariant maps. Our main result Theorem 1": "states that for a given group G acting on a set, there is a one-to-one correspondence between G-equivariantmaps and Hi-invariant maps, where the Hi are some stabilizer subgroups of G. This allows us to reduce anyequivariant tasks to some invariant tasks and vice versa. As a main application of Theorem 1, we study universal approximation for equivariant maps. The universalapproximation theorem (Pinkus, 1999), which is fundamental in deep learning, asserts that any reasonablysmooth function can be approximated by an artificial neural network with arbitrary accuracy. In the presenceof symmetries, we enforce the neural network architecture to match the symmetries of the target tasks, and,by doing so, we significantly reduce the hypothesis class of neural networks. Naturally, it is essential toensure that the universal approximation property is not compromised. It turns out that the universal approximation problem is more involved in the equivariant than in theinvariant setup. Indeed, for the symmetric group of permutations of n elements Sn, Zaheer et al. (2017)showed that the DeepSets invariant architecture is universal via a representation theorem which is famousas a solution for Hilberts 13th problem by Kolmogorov (1956) and Arnold (1957). However, they do notprovide such a theoretical guarantee for their equivariant architecture. Their result for invariance was thenextended to more general groups by Maron et al. (2019b) and Yarotsky (2022), but it is another series ofpapers which later solved the equivariant case, each with their own techniques (Keriven & Peyr, 2019; Segol& Lipman, 2019; Ravanbakhsh, 2020). Using our decomposition Theorem 1, we propose, in Theorem 2 an alternative way to build universalG-equivariant architectures via Hi-invariant maps for some suitable subgroups Hi G. As we explain inRemarks 1 and 2, the equivariant universal approximator that we obtain is different from others in theliterature. As a second application of our main theorem, we examine the number of parameters as well as the rate ofapproximation by invariant/equivariant neural networks with ReLU activation. In Theorem 3, we provideboth lower and upper bounds on the minimal number of parameters required to approximate an equivariantmap to a given accuracy with regard to the minimal number of parameters required to approximate aninvariant map in that same accuracy. Finally, in Theorem 4 and Corollary 1, we give an approximation ratefor G-equivariant neural networks among G-equivariant functions with Hlder smoothness condition. Thislast result is an extension of a result from Sannai et al. (2021) from Sn to any finite group G.",
  "We introduce a relation between invariant maps and equivariant maps. This allows us to reducesome problems for equivariant maps to those for invariant maps": "We apply the relation to constructing universal approximators by equivariant deep neural networkmodels for equivariant maps. Our models for universal approximators are different from the standardmodels, such as the ones from Zaheer et al. (2017) or Maron et al. (2019a). However, the numberof parameters in our models can be very small compared to the fully connected models.",
  "Related work": "Symmetries in machine learningSymmetries have been considered since the early days of machinelearning by Shawe-Taylor (1989; 1993); Wood & Shawe-Taylor (1996).In contemporary deep learning,they keep generating increasing interest following seminal works from Kondor (2008); Gens & Domingos(2014); Qi et al. (2017); Ravanbakhsh et al. (2017); Zaheer et al. (2017). The most commonly encounteredsymmetries involve translations, addressed by convolutional architectures (LeCun et al., 2015) and theirgeneralization to arbitrary compact groups (Cohen & Welling, 2016; Kondor, 2008); rotations (Cohen et al.,2018; Esteves et al., 2020); as well as permutations. The latter are particularly relevant for set data (Zaheeret al., 2017; Qi et al., 2017; Maron et al., 2020) as well as for graph data via most of the graph neuralnetwork architectures (Scarselli et al., 2008; Defferrard et al., 2016; Kipf & Welling, 2017; Bruna et al., 2014).Permutation symmetries are also present in modern Transformers of architectures, via self attention (Vaswaniet al., 2017) and positional encoding. Recent extension of Transformers for graph learning as proposed by Kimet al. (2021), utilize Laplacian eigenvectors for positional encoding features. Thus, to address ambiguity ineigenvector choices, Lim et al. (2023) recently proposed an architecture that is invariant to change of basis inthe eigenspaces. Villar et al. (2021; 2024) are interested in the symmetries arousing from classical physics andthe distinction between passive symmetries coming from physical law and being empirically observed, andpassive symmetries coming from arbitrary choice of design such as labeling of elements of a set or nodes ofa graph. Regarding the connection between invariant and equivariant maps, this work is a direct follow-upof Sannai et al. (2019). The methods and results from Theorems 1 and 2 extend the content of Sannaiet al. (2019) from the symmetric group Sn to an arbitrary group.More recently, Blum-Smith & Villar (2023) explain how to build equivariant maps from invariant polynomials using invariant theory. Finally,we shall mention that invariant/equivariant deep learning is now a fast growing field with a plethora ofvarious architectures. Some researchers have recently proposed to unify most of existing approaches througha general framework known as Geometric Deep Learning (Bronstein et al., 2021). Symmetries and universal approximationUniversal Approximation property is fundamental classicaldeep learning, extensively studied since the pioneer works by Cybenko (1989); Hornik et al. (1989); Funa-hashi (1989); Barron (1994); Krkov (1992); Pinkus (1999) and more and further explored by Sonoda &Murata (2017); Hanin & Sellke (2017); Hanin (2019). Concerning invariant architecture, Yarotsky (2022)observed that it is straightforward to build a universal invariant architecture from universal classic architec-ture just by group averaging when the group is finite. However, this is unfeasible for large groups such as Sn.Additionally, this issue was recently addressed by Sannai et al. (2024) who showed that, it can sometimesbe enough to average over a subset of the group rather that the entire group. By doing so, they are, forinstance, able to reduce the averaging operation complexity from O(n!) to O(n2) in the case of graph neuralnetworks. Zaheer et al. (2017) prove universality of DeepSets via a representation theorem from Kolmogorov(1956) and Arnold (1957) coupled with a sum decomposition. The effectiveness of this universal decompo-sition for continuous functions on sets is discussed by Wagstaff et al. (2022), who argue that the underlyinglatent dimension must be high-dimensional. Recently, Tabaghi & Wang (2023) improved the universalityresult from Zaheer et al. (2017). Some generalizations to other groups have been proposed by Maron et al.(2019b) and Yarotsky (2022), and universality of some invariant graph neural networks was proved by Maronet al. (2019b) and Keriven & Peyr (2019). In the case of equivariant networks, Keriven & Peyr (2019); Segol & Lipman (2019); Ravanbakhsh (2020)established universality results for finite groups using high order tensors. More recently, Dym & Maron (2021)suggested a universal architecture for rotation equivariance and Yarotsky (2022) for the semi-direct productof Rn and SOn(R) (translation plus rotation). In this paper, we propose circumventing the challenge ofdirectly addressing equivariant universality by constructing equivariant universal architectures directly frominvariant networks, known to be universal, via our decomposition theorem. The resulting architecture differsfrom others in the literature, as explained in more details in Remarks 1 and 2.",
  "In this section, we review some notions of group actions and introduce invariance/equivariance for maps. InAppendix A, we summarize some necessary notions and various examples for groups": "For sets X and V , we consider the set Map(X, V ) = V X = {f : X V } of maps from X to V . In manysituations, we regard X as an index set and V as a set of objects (such as channels or pictures). We showsome examples.",
  "Example 1.(i) If X = {1, 2, . . . , n} and V = R, then Map(X, V ) is idenfied with Rn by Map(X, V ) Rn : f (f(1), f(2), . . . , f(n))": "(ii) Let , m be positive integers, and X = {1, 2, . . . , } {1, 2, . . . , m} and V = {0, 1, . . . , 255}3. Then,Map(X, V ) can be regarded as the set of digital images of m pixels with the RGB color channels.For f Map(X, V ) and (i, j) X, f(i, j) = (rij, gij, bij) (rij, gij, bij {1, 2, . . . , 255}) representsRGB color at (i, j)-th pixel.",
  "(iii) Let V V X be a set of some digital images as in Example 1(ii) and X = {1, 2, . . . , n}. Then,V X is the set of n-tuples of the digital images in V": "(iv) For X = R2 and V = R3, Map(R2, R3) = (R3)R2 can be regarded as the space of ideal images.Here, the ideal means that the size of the image is infinitely extended and the pixels of theimage are infinitely detailed in the sense of Yarotsky (2022). This is a similar notion to the setL2(R, Rm) of the signals introduced in Yarotsky (2022, .2).",
  "for f V X and G. For f V X, let Of be the G-orbit G f = { f | G}. A few examples of suchtype of action are listed below": "Example 2.(i) The permutation group Sn of the set X = {1, 2, . . . , n} acts on X by permutation.Any unordered set {v1, v2, . . . , vn} of n elements of V can be regarded as an Sn-orbit of the orderedn-tuple (v1, v2, . . . , vn) V X for X = {1, 2, . . . , n}.",
  "(ii) The Sn-orbit of an element f in Map({1, 2, . . . , n}, R3) can be regarded as a point cloud consistingof n points in R3": "(iii) Let X = {1, 2, . . . , n}2 and V = R. We consider the diagonal Sn-action (i, j) = ((i), (j))( Sn) on X.Then, Sn also acts on V X = Rnn.Let Sym(n) be the subset of symmetricmatrices in Rnn. This Sym(n) is stable by the diagonal action of Sn. The Sn-orbit Sn A of anA Sym(n) Rnn can be regarded as an isomorphism class of undirected weighted graph. Indeed,Sym(n) is the set of adjacency matrices of undirected weighted graphs, and two graphs are isomorphicif and only if the corresponding adjacency matrices A1, A2 satisfy A1 = A2 for an element Sn.",
  "We define invariant and equivariant maps as follows:": "Definition 1. Let G be a group, X, Y be two G-sets, and V, W be two sets. Then, a map F from V X toW is G-invariant if F satisfies F( f) = F(f) for every G and f V X. A map F from V X to W Y isG-equivariant if F satisfies F( f) = F(f) for every G and f V X. We denote",
  "Some examples of invariant or equivariant tasks are the following": "Example 3.(i) Let X = Y = {1, 2, . . . , n} and V = R3.The classification task of point cloudscan be regarded as a task to find an appropriate Sn-invariant map from V X to a set of classesW = {c1, c2, . . . , cm}. (ii) A task of anomaly detection from n pictures is a permutation equivariant task as in Zaheeret al. (2017, Appendix I). This is to find an appropriate Sn-equivariant map V X to W Y forV = ({0, 1, . . . , 255}3)m, W = {0, 1}, and X = Y = {1, 2, . . . , n}. (iii) A task of classification of digital images of pixels is finding an appropriate 90-degree rotationinvariant map V X to a set of classes W = {c1, c2, . . . , cm}, where V = {0, 1, . . . , 255}3 and X ={1, 2, . . . , }2.An image segmentation task can be regarded as finding an appropriate 90-degreerotation equivariant map V X to W Y for the same V, W, X as Example 3 (ii) and Y = {1, 2, . . . , }2.",
  "Fi( f) = F1(i)(f) .(4)": "Let us focus on i = 1. Notice that if is a permutation such (1) = 1, it is straightforward from equation (4)that F1(f) = F1(f) for all f V X. Since it is fairly known that the set of permutations such that (1) = 1is a subgroup of Sn, denoted as StabSn(1), (this group is isomorphic to Sn1), we deduce from equation (4)thatF1 = F()(1) InvStabSn(1)(V X, W) .(5)",
  "Invariant-equivariant relation": "Our main theorem is a relation between a G-equivariant maps and some invariant maps for some subgroupsof G. Recall that, for any y in a G-set Y , its stabilizer, denoted as StabG(y), is the subgroup of all the Gsuch that y = y (see the recall en groups and group actions in Appendix A).",
  "iI InvStabG(yi)(V X, W)": "We first show the well-definedness of .That is, for (F) = (F()(yi))iI, we shall prove thatF()(yi): V X W is StabG(yi)-invariant for any i I.To do so, we check that for StabG(yi)and f V X, F( f)(yi) = F(f)(yi). Let F be a G-equivariant map from V X to W Y and be an elementin StabG(yi). This implies 1 yi = yi, hence the inverse 1 is also in StabG(yi). Then, by G-equivarianceof F, we have",
  "is well-defined": "Next, we prove that the map is well-defined. The well-definedness of can be rephrased that theimage (( Fi)iI)(f)(y) = Fi( f) for G such that y = 1 yi of ( Fi)iI iI InvStabG(yi)(V X, W)is independent of the choice of G and is G-equivariant. We first notice that such a exists since, fromthe G-orbit decomposition Y = iI Oyi, there is an orbit representative yi such that y Oyi, and thusy = 1yi for some G. If G is another choice so that y = 1yi, then we have 1yi = y = 1yi.This implies that 1 stabilizes yi. Thus, we can represent = for some StabG(yi). The value ofFi at f becomesFi( f) = Fi(() f)) = Fi( ( f)) = Fi( f).",
  "Here, the third equality also follows from the definition of the map of in equation (8). Therefore, Fsatisfies F(f)(y) = F( f)(y) for any G, any f V X, and any y Y . Hence, F is G-equivariant": "Finally, we show that the map is the inverse map of the map and vice versa, i.e., both and are identities. Let F be a map in EquivG(V X, W Y ). Then, the image of the map can be written as ( Fi)iIsuch that Fi(f) = F(f)(yi). Then, the map takes ( Fi)iI to F such that F (f)(y) = Fi( f) = F( f)(yi)for y Y and G such that y = 1 yi. Because F is G-equivariant, we have",
  "iI Map(V X, W)": "Next, we shall show that EquivG(V X, W Y ) is a linear subspace of Map(V X, W Y ). In order to do that,we first have to show that the action of F on Map(V X, W Y ) is itself linear. Let F, F Map(V X, W Y ) and",
  "iI InvStabG(yi)(V X, W) is alinear bijection between vector spaces. Thus, by a fairly known fact from linear algebra, its inverse mustbe linear too": "Theorem 1 implies that any G-equivariant map F is determined by the StabG(yi)-invariant maps F()(yi)for i I. In other words, the parts F()(y) for y {yi | i I} are redundant to construct G-equivariantmap F. This theorem is quite elementary, in the sense that its statement as well as its proof, only requiresbasic knowledge in group theory. However, the idea of linking G-equivariant maps to invariant maps onsome subgroups of G is not new and has more profound implications. For instance, it is central in grouprepresentation theory as it relates to the so-called Frobenius reciprocity theorem (Curtis & Reiner, 1966),about the relationships between the representation of G and the representations of its subgroups. In addition,the result from Theorem 1 can be recovered from some other more general theorems in more advanced topics,such as Theorem 1.1.4 in Cap & Slovk (2009), about homogeneous vector bundles.",
  "fV X = supxXf(x)V .(9)": "for f V X and similarly for g W Y . Let K V X be a compact subset. We assume that K is stable bythe action of G, i.e., f K for any G and f K. We denote EquivcontG(K, W Y ) (resp. InvcontH(K, W)for a subgroup H) the subset of continuous maps in EquivG(K, W Y ) (resp. InvH(K, W)):",
  ",(10)": "where is the map defined in Theorem 1. Then, the set HG-equiv is included in EquivcontG(K, W Y ). Indeed,by Theorem 1, this set is included in EquivG(K, W Y ) and we can show that F = (Fy)yY : K W Y iscontinuous if and only if Fy : K W is continuous for all y Y because we consider the supremum normon W Y .",
  "On the structural characteristics of universal equivariant networks arising from Theorem 2": "On the G-action between the hidden layers.Several works have studied the design of multi-layeruniversal equivariant architectures, including Zaheer et al. (2017), Maron et al. (2019b), Segol & Lipman(2019), and Ravanbakhsh (2020). How do a universal architecture arising from our Theorem 2, differ fromthe already existing aforementioned ones? The main difference resides in the nature of the G-action in the hidden layers.In fact the way ourarchitecture is built involves a new G-action, noted , which is different from the original action . Weprovide below an explanation of this phenomenon in the form of a remark. The rigorous demonstrations areleft to Appendix B and require some background in group Representation Theory. Remark 1. Say that we seek to design multi-layer architectures that are equivariant to some G-action. The usual strategy, as in the case of the previously mentioned works, is to focus on designing a singlelayer block that is equivariant for . Then by stacking these blocks, we obtain an equivariant multi-layerarchitecture, because equivariance is preserved by composition. Now, consider the scenario of our construction from Theorem 2. Assume that we are given some familiesof multi-layers Hi-invariant maps, for the action , where the Hi G are some subgroups that have beenidentified thanks to the decomposition from Theorem 1. Then, by aggregating some of these Hi-invariantmaps, and applying , as is equation (10), a new multi-layer architecture F is built. Moreover, by Theorem 1again, the F obtained by this procedure are guaranteed to be equivariant for the action . What can be said about the layer-wise structure of such F? It turns out that the G-equivariance of thehidden layers is no longer realized by on both the input and the first hidden space. Instead, it involvesanother G-action in the hidden space. If we look at the first layer 1, which consists in plugging-inthe aggregated invariant maps to the input space (see ), its equivariance, from the input layer tothe first hidden layer of F, will be written as 1( f) = 1(f), as seen in (b).",
  "Overall, using commutative diagrams, highlights the differences between the group actions involvedin these architectures": "This new action , which is still an action of the group G, can be described as the induced representationof the restricted representation action on the hidden layers. In Appendix B, a rigorous discussion about thisphenomenon and definition of this action are done using tools from Representation Theory. On the number of free parameters.Another notorious advantage of multi-layer equivariant architec-tures such as Zaheer et al. (2017); Maron et al. (2020), is that the design of equivariant blocks requires fewfree parameters thanks to a phenomenon of parameters sharing enabled by the symmetry. Hence, in terms ofthe total number of free parameters, there is a significant advantage in using layer-wise equivariant networks,rather than naive fully connected ones, in order to approximate an equivariant map. Is it also the case forour architecture? We clarify that point in the following remark. We claim that, indeed, our equivariant architecture is moreefficient than naive fully connected networks, in terms of a number of free parameters. Proofs and rigorousexplanations are left in appendix C. Remark 2. Although our equivariant model differs from the standard equivariant models, the number ofparameters of our models can also be less than that of a fully connected neural network. For example, letG = Sn act on X = Y = {1, . . . , n} by permutation and V, W be vector spaces over R. For Sn-equivariantmap F : V n W n, we can take G-equivariant deep neural network F approximating F by Theorem 2. Then,",
  "(b) Equivariant architecture arising from Theorem 2: the action on the input space isdifferent than the action , in red, on the hidden spaces": ": Commutative diagrams for comparing between a usual equivariant multi-layer architecture, andours arising from Theorem 2. For each diagram, the two lines both represent the same multi-layer equivariantarchitecture with the i being the equivariant layers between hidden spaces. Each square encapsulating a symbol is a commutative diagram that represents the equivariance of the layer i, i.e, the fact that icommutes with some action of the group. This means that, within these squares, from the top left to thebottom right the two possible paths are equal. the first hidden layer of F can be written as the form V n21for a vector space V1. The linear map V n V n21is equivariant for permutation action on V n and the direct sum of induced representations of restrictionrepresentations to the stabilizer subgroup on V n21 .By the argument on irreducible representations, thenumber of parameters of the linear map between the input layer V n and the first hidden layer V n21becomes5 dim V dim V1. Because the number of parameters of the linear map V n V n21for the fully connected modelis n3 dim V dim V1, our model can be constructed by much fewer parameters than the fully connected model.In Appendix C, precise statement and proof of this fact are concluded using Schurs Lemma and Youngsdiagrams.",
  "Permutation equivariant maps": "As a first example, we consider Sn-equivariant maps. Recall the context from .1 where X = Y ={1, 2, . . . , n} and V, W are generic sets. The set Y only has a single orbit by the Sn action: O1 = Sn 1,which can be written asO1 = {(1 i) 1 | i = 1, . . . , n} = {1, 2, . . . , n},",
  "In this subsection, G is any finite group. By Proposition 3 in Appendix A, G is a subgroup of Sn for somen. Hence, G acts on X = Y = {1, 2, . . . , n} via permutation action of Sn": "The G-orbit decomposition of Y is generally nontrivial. Let Y = ki=1 Oyi be the G-orbit decompositionof Y and y1, y2, . . . , yl be elements of G such that Oyi = Gyi for i = 1, 2, . . . , k.We represent Oyi asOyi = {yi1, yi2, . . . , yimi}, and choose ij G such that yij = 1ij yi. Then, we remark that ki=1 mi = n.",
  "The special orthogonal group SO2(R)": "As for Example 3 (iii), we consider the rotation actions on the set of the ideal images. The index sets areX = Y = R2. We consider Map(X, R3) = (R3)X as the set of ideal images with the values for ideal RGB colorchannels V = R3. Let W be a set of output labels. Then, the 2-dimensional special orthogonal group SO2(R)acts on X = Y = R2. We consider an SO2(R)-equivariant (i.e., rotation equivariant) map F : V R2 W R2.Then, the orbit Oyi = SO2(R) yi of yi = (i, 0) R2 for i I = [0, ) is Oyi = {(a, b) R2 | a2 + b2 = i2},",
  "(Tv f)(x) = f(T 1v (x)) = f(x v)": "We consider an T-equivariant (i.e., translation equivariant) map F : V R2 W R2. In this case, for (0, 0) Y = R2, the orbit O(0,0) = T (0, 0) is same as Y = R2 and StabT((0, 0)) = {Id}. Then, by Theorem 1, theT-equivariant map F can be written by",
  "In the case of finite group G, we investigate the number of parameters from the point of view of approxi-mations. Let G be a group and X, Y be two finite G-sets. Let Y =": "iI Oyi be the G-orbit decompositionof Y and K V X be a compact subset which is stable by the G-action. For F EquivcontG(K, W Y ), wedefine ceqG (; F) to be the minimum number of the parameters of G-equivariant deep neural networks thatcan approximate F in accuracy > 0. More precisely, there exists a G-equivariant deep neural network F ofwhich the number of parameters is equal to ceqG (; F) and supfK F(f)F(f)W Y , and there is no suchG-equivariant deep neural network of which the number of parameters is less than ceqG (; F). Similarly, forsubgroup H of G and F InvcontH(K, W), we define cinvH (; F) to be the minimum number of the parametersof H-invariant deep neural networks that can approximate F in accuracy > 0. The existences of suchcinvH (; F) and ceqG (; F) are guaranteed by the results for universal approximation theorems such as Maronet al. (2019b) or Theorem 2 in this volume.",
  "iIcinvStabG(yi)(; Fi).(14)": "Proof. We first show the left inequality.Let Fi0 InvStabG(yi0)(V X, W) for an i0 I.We can find aG-equivariant deep neural network F such that F(f) F(f)W Y for any f K and the number ofparameters of F is equal to ceqG (; F). The existence of F is guaranteed by Theorem 2. For this F, F()(yi0)satisfies F(f)(yi0) Fi0(f)W and the number of parameters of the deep neural network F()(yi0)is less than or equal to ceqG (; F). Thus, Fi0 can be approximated in accuracy by a StabG(yi0)-invariantdeep neural network of which the number of parameters is less than or equal to ceqG (; F). This impliescinvStabG(yi0)(; Fi0) ceqG (; F). Because i0 I is arbitrary, we have the first inequality of equation (14).",
  "Approximation rate for G-equivariant deep neural networks": "In the case of G = Sn, X = {1, 2, . . . , n}, V = W = R, and K = n V n, Sannai et al. (2021, Theorem 4)showed an approximation rate of Sn-invariant ReLU deep neural networks for elements in a Hlder space.In this subsection, we generalize this result to G-invariant maps for a finite group G Sn and show anapproximation rate for G-equivariant ReLU deep neural networks. Let be a positive constant. The Hlder space of order is the space of continuous functions f suchthat all the partial derivatives of f up to order exist and the partial derivatives of order are Hlder-smooth. This space is usually denoted as C,, but we denote it as C here for convenience. Forf : K R, the Hlder norm is defined by",
  "The following is a generalization of Sannai et al. (2021, Theorem 4) to finite group G Sn": "Theorem 4. Let G Sn be a finite group acting on the sets X = Y = {1, 2, . . . , n} and Y = ki=1 Oi bethe G-orbit decomposition, and K = n Rn. For any > 0, let HG-inv be a set of G-invariant ReLUdeep neural networks from K to R which has at most O(log(1/)) layers and O(n/ log(1/)) non-zero",
  "supfK|F(f) F(f)|": "Proof. To generalize Sannai et al. (2021, Theorem 4), it is sufficient to define the fundamental domain Gfor finite group G Sn and a sorting map Sort: n G, and to show that Sort can be realized byReLU deep neural networks. The remaining part of the proof is similar to the proofs of Sannai et al. (2021,Proposition 9, Theorem 4)",
  "By applying Sannai et al. (2021, Proposition 9 and Proof of Theorem 4) with this SortG, we can completethe proof": "As mentioned in Sannai et al. (2021) for G = Sn, also for finite group G Sn, the approximation rate inTheorem 4 is the optimal rate without invariance obtained by Yarotsky (2022, Theorem 1). Thus, we showthat G-invariant deep neural networks can also achieve the optimal approximation rate even with invariance.",
  "By combining Theorem 4 and similar argument in the proof of Theorem 3, we can show the approximationrate for G-equivariant maps:": "Corollary 1. Let G Sn be a finite group acting on the sets X = Y = {1, 2, . . . , n} and Y = ki=1 Oi be theG-orbit decomposition. Let V = W = R and K = n Rn. For any > 0, let HGequiv be a set of G-equivariant ReLU deep neural networks from K to Rn with at most O(log(1/)) layers and O(kn/ log(1/))non-zero parameters. Then, for any G-equivariant map F (CB)n, there exists F HGequiv such that",
  "Conclusion": "In this study, we explained a fundamental relationship between equivariant maps and invariant maps forstabilizer subgroups. The relation allows any equivariant tasks for a group to be reduced to some invarianttasks for some stabilizer subgroups. As an application of this relation, we proposed the construction ofuniversal approximators for equivariant continuous maps by using some invariant deep neural networks.",
  "Although our model is different from the other standard models introduced by Zaheer et al. (2017) or": "Maron et al. (2019b), the number of parameters of our models is fewer than fully connected models. Moreover,by the relation, we showed inequalities of the number of parameters of invariant or equivariant deep neuralnetworks to approximate any continuous invariant/equivariant maps. We also showed an approximation rateof G-equivariant ReLU deep neural networks for elements of a Hlder space for finite group G.",
  "Puoya Tabaghi and Yusu Wang. Universal representation of permutation-invariant functions on vectors andtensors, 2023": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,volume 30. Curran Associates, Inc., 2017. URL Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal:Equivariant machine learning, structured like classical physics. In A. Beygelzimer, Y. Dauphin, P. Liang,and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL",
  "AGroups, actions, and orbits": "To introduce a precise statement of the main theorem, we briefly review several notions of groups, actions, andorbits. In general, a group is a set of transformations and an action of the group on a set is a transformationrule. This is further explained in the following examples. For more details, we refer the readers to Rotman(2012).",
  ". (Associativity) For any , , G, () = ()": "We call G a finite group if the number of elements of G is finite. If the product of G is commutative, i.e.,for any , G, = holds, then G is called an abelian (or a commutative) group. Let G be a groupand H a subset of G. We call H a subgroup of G if H is a group with the same product as that of G.",
  "This group StabG(x) is called the stabilizer subgroup of G with respect to x": "Next, we introduce the notion of orbits by the group actions. An orbit is the set of elements obtained bytransformations of a fixed base element. Let G be a group acting on X from left. Then, for x X, we definethe G-orbit Ox of x asOx = G x = { x | G}.",
  "Let H be a subgroup of a finite group G. Then, H acts on G from left and right by the product: For H and G, acts on from left (resp. right) by (resp. ). For any G, we definethe setH = { G | H}": "This is nothing but the right H-orbit of for the above right action of H on G. This is called the left cosetof H with respect to . Because the left coset is identical to the H-orbit for the right action of H, we candecompose G into a disjoint union of the left cosets as a H-orbit decomposition:",
  "Proposition 3. Any finite group G can be realized as a subgroup of Sn for some positive integer n": "Proof. Let n := |G| and G = {1, 2, . . . , n}. For any G, i = j for some j {1, 2, . . . , n}, becausethe product i is in G. We set (i) = j. Then, we define a map : G Sn; . This map is injectiveand preserves the product, i.e., = . Through this map, G can be regarded as a subgroup of Sn.",
  "To explain the architecture of our models, we introduce some notions of representation theory": "Let G be a finite group, V be a vector space over R, and GL(V ) be the group of the automorphismsof V .Here, an automorphism of V is a linear bijection from V to V .Then, a group homomorphism: G GL(V ) is called a (linear) representation of G on V . Defining a representation : G GL(V ) isequivalent to defining an action of G on V by . Then, it is also known that considering a representation: G GL(V ) is equivalent to considering an R[G]-module V . Here, R[G] is the group algebra defined as",
  "Therefore, the dimension of R[G] R[H] W is m dim W = (G : H) dim W, where (G : H) is the index of H inG": "Let G Sn be a finite group, F : V n W n be a G-equivariant continuous map, and let Y={1, 2, . . . , n} = mi=1 Oyi be a G-orbit decomposition with Oyi = {1(yi) | G} of yi Y . By Theorem 1,we have (F) = ( Fi)mi=1 for the StabG(yi)-invariant continuous map Fi() = F()(yi). Then, we have an approximator F i : V n W as a StabG(yi)-invariant deep neural network of the StabG(yi)-invariant mapFi (cf. Segol & Lipman (2019), Ravanbakhsh (2020)) and, by Theorem 2, can reconstruct an approximatorF = ((F i)mi=1).",
  "CTheoretical explanation of Remark 2": "In the remaining part, we argue about the number of parameters of the affine maps between layers. If aweight matrix between two layers of neural networks is equivariant by some G-action, this can be regarded asan intertwining operator. To count the number of parameters of intertwining operators, we need the notionof irreducible representations. We here review these notions briefly.",
  "be the k-th hiddent layer of F = (F 1)": "Then, the number of free parameters of the affine map from the input layer V n to the first hidden layer((V (1)1)n)n is 5 dim V dim V (1)1+ 2 dim V (1)1.Moreover, the number of free parameters of the affine mapfrom the (k 1)-th hidden layer ((V (1)k1)n)n to the k-th hidden layer ((V (1)k)n)n for k = 2, . . . , d is at most",
  "dim V (1)k1 dim V (1)k+ 2 dim V (1)k": "Proof. For G = Sn, StabG(1) Sn1 holds and the left coset decomposition is Sn = ni=1 StabG(1)(1 i).Thus, the first hidden layer is isomorphic to R[Sn] R[Sn1] (V (1)1)n for a finite vector space V (1)1. By permutation, Sn acts on the part Rn part of V n Rn R V . By the argument of Young tableau(c.f. (James, 1987, 9.2)), the permutation representation on Rn is the sum of two irreducible representationscorresponding to the partitions 1 = (n) (trivial representation) and 2 = (n1, 1) of n. On the other hand,the representation on R[Sn]R[Sn1] (V (1)1)n R[Sn]R[Sn1] Rn R V (1)1is the induced representation of therestricted representation of the permutation representation on Rn. By the argument of Young tableau again(c.f. (James, 1987, 9.2)), the restricted representation is the sum of irreducible representations correspondingto the partitions 11 = (n1), 21 = (n1), and 22 = (n2, 1) of n1. Then, the induced representationof it is the sum of irreducible representations corresponding to the partitions 111 = (n), 112 = (n 1, 1),211 = (n), 212 = (n 1, 1), 221 = (n 1, 1), 222 = (n 2, 2), and 223 = (n 2, 1, 1). Let V bethe irreducible representation over R corresponding to the partition of n. Then, we have the irreduciblerepresentation decomposition",
  "Bj0 = B1": "holds for any j0 = 1, 2, . . . , n and StabSn(1). This implies that by = e, Bj = B1 holds for anyj = 1, 2, . . . , n. Furthermore, B1 = B1 holds for any StabSn(1). Because the orbit decomposition of{1, 2, . . . , n} by H1 = StabSn(1) is"
}