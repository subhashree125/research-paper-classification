{
  "Abstract": "Model-based offline reinforcement learning methods (RL) have achieved state-of-the-art per-formance in many decision-making problems thanks to their sample efficiency and gener-alizability. Despite these advancements, existing model-based offline RL approaches eitherfocus on theoretical studies without developing practical algorithms or rely on a restrictedparametric policy space, thus not fully leveraging the advantages of an unrestricted policyspace inherent to model-based methods. To address this limitation, we develop MoMA, amodel-based mirror ascent algorithm with general function approximations under partialcoverage of offline data. MoMA distinguishes itself from existing literature by employing anunrestricted policy class. In each iteration, MoMA conservatively estimates the value func-tion by a minimization procedure within a confidence set of transition models in the policyevaluation step, then updates the policy with general function approximations instead ofcommonly-used parametric policy classes in the policy improvement step. Under some mildassumptions, we establish theoretical guarantees for MoMA by proving an upper boundon the suboptimality of the returned policy. We also provide a practically implementable,approximate version of the algorithm.The effectiveness of MoMA is demonstrated vianumerical studies.",
  "Introduction": "Reinforcement Learning (RL) has emerged as an effective approach for optimizing sequential decision makingby maximizing the expected cumulative reward to learn an optimal policy through iterative online interactionswith the environment.RL algorithms have made significant advances in a wide range of areas such asautonomous driving (Shalev-Shwartz et al., 2016), video games (Torrado et al., 2018), and robotics (Koberet al., 2013). However, numerous real-world problems require methods to learn only from pre-collected andstatic (i.e., offline) datasets because interacting with the environment can be expensive or unethical, suchas assigning patients to inferior or toxic treatments in healthcare applications (Gottesman et al., 2019).",
  "Published in Transactions on Machine Learning Research (10/2024)": "scenarios with limited seeds. It emphasizes the use of distributional metrics, which are less sensitive tooutliers, providing a clearer and more robust picture of algorithm performance across different runs. Specifically, for our MoMA, model-based baseline MOPO, and model-free baseline CQL, we plot the aggre-gate metrics including interquartile mean (IQM), mean, and optimality gap together with 95% bootstrapconfidence intervals (CIs) for each one of the 9 tasks in . IQM has better statistical efficiency thanmedian, while optimality gap is a robust alternative to mean. Higher IQM and mean scores are better, andlower optimality gap score is better. We further supplement the aggregate metrics with performance profilesbased on score distributions (Agarwal et al., 2021), defined as the fraction of runs above a certain scorethreshold, and higher curve is better. We plot the performance profiles and bootstrap 95% confidence bandsin . For the HalfCheetah environment, MoMA consistently outperforms the two baselines across thethree data settings measured by all three metrics, and exhibit uniformly higher performance profiles. For themedium-expert data setting, MoMA achieves the best or competitive results across the three environments.",
  "Preliminaries": "Markov decision processes and offline RL: We consider an infinite-horizon Markov decision process(MDP) M = (S, A, P, r, , 0), with continuous state space S, discrete action space A = {A1, A2, ..., Am}, atransition dynamics P(s | s, a) with s, s S and a A, a reward function r : SA , a discount factor [0, 1), and an initial state distribution 0. We assume a model space P for the transition dynamic, i.e.P P. The reward function r is assumed to be known throughout this work. A stochastic policy maps fromstate space to a distribution over actions, representing a decision strategy to pick an action with probability(|s) given the current state s, i.e. ( | s) (A) := {p Rm : mGiven a policy and a transition dynamics P, the value function V i=1 pi = 1, pi 0, i} for all s S.P (s) := EP,[t=0 tr(st, at)|s0 = s]denotes the expected cumulative discounted reward of under the transition dynamics P with an initialstate s and a reward function r.We use V P := Es0V P (s) to denote the expected value integratedover S with an initial distribution 0.The action-value function (i.e., Q function) is defined similarly:QP (s, a) = EP,[t=0 tr(st, at)|s0 = s, a0 = a]. Let dP (s, a) := (1) t=0 tPr(st = s, at = a|s0 0) bethe occupancy measure of the policy under the dynamics P. Then V P can be expressed as E(s,a)dP [r(s, a)].Assuming that a static offline dataset Dn = {(si, ai, ri, si) : i = 1, ..., n} is generated by some behavior policyunder the ground truth transition dynamics P , model-based offline RL methods aim to learn an optimalpolicy that maximizes the value V P through learning the dynamics from the offline dataset without anyfurther interactions with the environment. Partial coverage: One fundamental challenge in offline RL is distribution shift (Levine et al., 2020):the visitation distribution of states and actions induced by the learned policy inevitably deviates from thedistribution of offline data. The concept of coverage has been introduced to measure the distribution shiftusing the density ratio (Chen & Jiang, 2019). Denote (s, a) to be the offline distribution that generates thestate-action pairs (si, ai)ni=1 in offline data. Full coverage means sups,a dP (s, a)/(s, a) < for all possiblepolicies , which may not hold in practice.In contrast, partial coverage only assumes that the offlinedistribution covers the visitation distribution induced by some comparator policy (Xie et al., 2021), suchthat sups,a dP (s, a)/(s, a) < . Our work aims to learn the optimal policy among all polices covered byoffline data, i.e., := { : sups,a dp(s, a)/(s, a) < }.",
  "Model-based mirror ascent for offline RL": "In this section, we present MoMA, which is summarized in Algorithm 1. MoMA can be separated intotwo steps in each iteration: 1) In the policy evaluation step, we conservatively evaluate the updated policythrough a minimization procedure within a confidence set of transition models; 2) In the policy improvementstep, we update the policy based on mirror ascent (MA) under the current transition model. The conservativepolicy optimization procedure is designed to mitigate distribution shift by penalizing the value of state-actionpairs that are rarely visited, thereby addressing the high uncertainty in their value estimation. This approachensures performance under a partial coverage assumption rather than a full coverage assumption. We providedetails for the policy evaluation and policy improvement in section 3.1 and section 3.2 respectively.",
  "P = arg minP PLn(P)": "is an estimator based on Ln. For example, if Ln(P) denotes the negative log-likelihood function of P, thenP is the maximum likelihood estimator (MLE) for P. n can be understood as the radius of the confidenceset. Then, we find Pt that minimizes the value function V tPwithin Pn,n, which is formulated as",
  "Pt =argminP Pn,nV tP .(1)": "If the radius of the confidence set n is set to zero, then Pt exactly corresponds to the MLE for P. Inthis scenario, no uncertainty penalization is considered. A positive n allows us to construct a perturba-tion around the MLE. Finding the most pessimistic Pt within the confidence region implicitly imposes anuncertainty penalization for rarely visited state-action pairs. The minimizer Pt, combined with the current policy t, can be used to evaluate QtPt through Monte Carlomethods. In section 4.3, we will provide an example using negative log-likelihood to illustrate the implemen-tation procedure. The conservative policy evaluation step is designed to provide a pessimistic estimate of the value functiongiven a policy. This idea has been employed in pessimistic model-free actor-critic algorithms (Khodadadianet al., 2021; Zanette et al., 2021; Xie et al., 2021), where the critic lower bounds the Q function. For example,Khodadadian et al. (2021) constructed the pessimism for Q under the tabular setting. Zanette et al. (2021)assumed a linear Q function and conservatively estimated Q by minimizing Q within a confidence set of thecoefficients for Q. Xie et al. (2021) conservatively estimated Q in a general function approximation setting,however, the size of the function class was limited by the sample size of offline data. Compared to thesemodel-free methods, MoMA has several advantages. First, unlike Xie et al. (2021), the size of the functionclass for approximation in MoMA can be arbitrarily large. Second, MoMA has no restriction on the policyclass, which is crucial when the optimal policy is not contained in a restricted parametric policy class. SeeAppendix 7 for a detailed discussion about the comparisons between our work and existing literature in thepolicy evaluation step.",
  "tD (t( | s), p)s.(2)": "Intuitively, the update rule aims to maximize the value function while ensuring minimal deviation from theprevious step. The distance D measures the divergence between the updated policy and the original policyfrom the previous step. The parameter t can be interpreted as the step size, indicating the weight placed ondistance control. This update rule is distinct from existing literature in that it does not require any explicitpolicy parameterization. This feature underscores the advantage of MoMA over existing model-based offlinealgorithms that rely on parametric policy classes, e.g., Rigter et al. (2022); Guo et al. (2022); Rashidinejadet al. (2022); Bhardwaj et al. (2023). As the Bregman distance defines a general class of distance measures, one can design corresponding algo-rithms based on specific distance measures if desired. Notably, natural policy gradient (NPG) (Kakade,2001) is a special case of policy mirror ascent when D(, ) is set to be KL divergence. We remark that for a continuous state space S, it is impossible to enumerate (2) for infinitely many states.To overcome this issue, in section 4.2 we provide a computationally efficient algorithm through functionapproximation, which is one of the key advantages of MoMA compared to existing literature, e.g., Algorithm1 in Xie et al. (2021).",
  "Function approximation in MA": "In the policy improvement step of algorithm 1, updating t+1( | s) for an infinite number of states s iscomputationally impossible, as QtPt can only be evaluated when t( | s) is known for all s. Although MonteCarlo estimation may be utilized in evaluating QtPt in eq. (2), the computational complexity would growexponentially with the number of iterations T, resulting in computational inefficiency. Notably, this issueis also present in Algorithm 1 of Xie et al. (2021). In contrast, our practical algorithm exhibits polynomialdependence on T, which is computationally efficient. A detailed justification for this claim is provided inAppendix 7.",
  "t(t( | s))i": "and Q,t(s, ) is a vector with its i-th element as Q,t(s, Ai). The augmented action-value function notonly incorporates information about the action-value function but also includes information about the policyitself. Following Lan (2022), we approximate Q,t(s, p) by a parametric function ft(s, p; t) Ft such thatft(s, p; t ) Q,t(s, p) for some t , which is sufficient to approximate Q,t(s, Ai) for each Ai according toQ,t(s, p) = Q,t(s, ), p. To this end, for each i = 1, ..., m, we introduce ft,i(s; t,i) Ft,i to approximateQ,t(s, Ai), and thus",
  ", s S.(7)": "Such a design of function approximation enjoys several benefits. 1) The objective function in (7) is concavewhich can be solved by standard first-order optimization methods. 2) Compared to commonly-used para-metric policy classes, our policy class is unrestricted, ensuring it includes the optimal policy. 3) The functionapproximation error in eq. (5) can be made arbitrarily small by enlarging the function classes Ft,i.",
  "i=1pi log pi": "and introduce a multi-layer neural network ft,i(s; t,i) for approximating Q,t(s, Ai). For each i = 1, ..., m, inorder to find the best t,i, we first sample {sj}Nj=1 i.i.d. from dtPt, then run any policy evaluation proceduresuch as Monte Carlo (algorithm 4 in Appendix E) for Q,t(sj, Ai) for each j = 1, ..., N. Using training data(sj, Q,t(sj, Ai))Nj=1, we can obtain t,i by standard neural network (NN) training procedure. In addition,thanks to the form of , we have a closed form solution to (7) which is the update rule",
  "exptft,is; t,ifor each i = 1, . . . , m. In the policy evaluation step t, given t1 which is the output": "from the (t 1)-th iteration, we can count the number of calls of t(s) from t to t + 1 as by realizing weneed t in the Monte Carlo evaluation of V tPk for k = 1, . . . , K and the sampling from dtP in the policyevaluation step. Specifically, for each sampling or Monte Carlo evaluation, the effective numbers of using tis1",
  "Extension to continuous action space": "We now extend MoMA to handle complex RL tasks with nonlinear dynamics and continuous action spaces.Instead of considering p (A) introduced in section 3.2 where A is assumed to be finite, we consider acontinuous action space A RdA in this section. Here dA is the dimension of the action space and A isassumed to be a compact convex set. In the continuous-action case, we consider the deterministic policy : S A, i.e. (s) A is a feasible action for each state s S.",
  "Theoretical analysis": "In this section, we present the upper bound on the suboptimality of the learned policy in Algorithm 2 interms of sample size, number of iterations and all key parameters. All proofs are presented in Appendix C.We first present the following assumptions. Assumption 1. The following conditions hold.(a) (Data generation). The dataset D = (si, ai, ri, si)ni=1 satisfies (si, ai)i.i.d. with si P ( | si, ai), where denotes the offline distribution induced by the behavior policy under P .",
  "est :=supP Pn,nE(s,a)[P( | s, a) P ( | s, a)1] n.(12)": "Assumption 1(a) is related to offline data generation, common in offline RL theoretical literature. Assumption1(b) essentially requires the partial coverage of the offline distribution. The concentrability coefficient Cmeasures the distribution mismatch between the offline distribution and the occupancy measure inducedby . Assumption 1(c) requires that the model class P is sufficiently large such that there is no modelmisspecification error.Assumption 1(d) is needed to provide a fast statistical rate uniformly over theconfidence set. We remark that assumption 1(d) is a mild condition. For example, commonly-used empiricalrisk functions (e.g. negative log-likelihood) satisfy it. See proposition 1, corollary 1 for more details inAppendix B. Now we provide the suboptimality upper bound for algorithm 2 in the following theorem, assuming anaccess to a computational oracle for solving the contained minimization problem (1) in the conservativepolicy evaluation step. Theoretical results for algorithm 1, in which we assume no function approximation,are summarized in Theorem 2 in Appendix B.",
  ".(13)": "Term (a) can be bounded by the distance between the true dynamic model P and Pt, which converges to 0as the sample size increases to . Term (b) is managed through the mirror ascent update rule, which tendsto 0 as the number of iterations k . Term (c) is negative thanks to the construction of the confidenceset and the definition of Pt. The proof is completed by averaging both sides of the above equation over T. The upper bound in Theorem 1 includes three terms: a model error (depending on fixed n) coming fromusing offline data for estimation of the transition model, an optimization error (depending on iteration T)from the policy improvement, and a function approximation error coming from using Monte Carlo samplesapproximating the augmented Q function. The model error is a finite-sample term that cannot be reducedunder the offline setting, while the optimization error can be reduced when the number of iterations Tincreases. Typically, we have est = OP (1/n). The function approximation error involves an approximationerror approx that decreases as the function class is enlarged maxt,i |Ft,i| , an estimation error thatscales with OP (1/ N), and a distribution mismatch sups dP (s)/0(s) between the initial distribution andthe occupancy measure induced by a single under P . Indeed, if maxt,i |Ft,i| , then approx 0 byDefinition A.3. Consequently, the function approximation error can converge to 0 as long as maxt,i |Ft,i| and N at the same speed based on its expression in Theorem 1. The function approximation error andthe model error share similar intuitive interpretations. In the model error, C measures the transfer of estchanging from the offline distribution to the target distribution dP . Analogously, sups dP (s)/0(s) in the",
  "Synthetic dataset: an illustration": "We design a test environment based on a modified random walk with terminal goal states to generate datawith partial coverage and understand how pessimism helps MoMA avoid common pitfalls faced by model-based offline RL methods. (p) is set to be mi=1 pi log(pi). Environment and offline datasetFor each episode that starts with an initial state s0 U(2, 2),at time n a particle undergoes a random walk and transits according to a mixture of Gaussian dynamics:sn+1sn =: s aN(1,a, 0.1)+(1a)N(2,a, 0.1), where the discrete action a {1, 0, 1} corresponds",
  "1.613.20 2.334.39 2.966.13 5.07": "Contribution from pessimismTo understand pessimism empirically, we zoom into the state s = 0.1,where the optimal policy is consecutive Right, the data-supported suboptimal policy is consecutive Left,and the faulty policy centers on Stay. Due to inaccurate MLE, a model-based algorithm without pessimismover exploits the model and converges to the faulty action Stay. In contrast, pessimism allows MoMA totrust the model on Left which has high coverage, while cautiously modifying the model such that Stay doesnot lead to substantial Right movement, i.e., 0 increases (see in Appendix F). This behavior isclearly captured during the training process: shown in the right plot of , while the weight of thefaulty action Stay monotonically increases for NPG, it decreases from the 10th epoch for MoMA. As a result,the suboptimal action weight (shown middle) eventually dominates for MoMA but vanishes for NPG, andNPGs value function V (0.1) under the true dynamics (shown left) decreases due to the over exploitation ofthe learned dynamics model.",
  "See section 4.5 for more implementation details. We adjust the implmentation accordingly, and evaluate onD4RL (Fu et al., 2020) MuJoCo benchmark datasets": "We consider the medium, medium-replay, and medium-expert datasets for the Hopper, HalfCheetah, andWalker2D tasks (all v0), respectively. We compare against SOTA model-based baseline algorithms MOPO(Yu et al., 2020) and RAMBO (Rigter et al., 2022), and model-free baseline algorithms CQL (Kumar et al.,2020) and IQL (Kostrikov et al., 2021).With the exception of RAMBO for which we cite the resultsreported in Rigter et al. (2022), we train all algorithms for 1E6 steps with early stopping and with 5 differentrandom seeds; additional experimental details are given in Appendix F.3. We summarize the scores (averagereturns of 10 evaluation episodes) in . MoMA consistently demonstrates performance that is at leastcomparable to state-of-the-art (SOTA) algorithms. Notably, in 4 out of 9 cases, our algorithm outperformsthe other two model-based RL algorithms, achieving the highest performance.",
  "As one of the anonymous reviewers suggested, the confidence intervals (CIs) reported in our study can bebiased due to being based on only a small number of seeds": "As a supplement to the standard mean of evaluation scores reported in , we have adopted the evalu-ation scheme proposed by Agarwal et al. (2021). Their method emphasizes the use of distributional metrics,which are less sensitive to outliers, providing a clearer and more robust picture of algorithm performanceacross different runs. Specifically, we now include the interquartile mean (IQM) and the optimality gap as aggregate metrics,along with 95% bootstrap CIs. The IQM provides a robust measure of central tendency by focusing onthe middle 50% of the data, reducing the influence of extreme values. The optimality gap measures howclose the performance is to an optimal policy, offering a meaningful interpretation of results. Additionally, wepresent performance profiles based on score distributions, which offer a comprehensive view of the algorithmsperformance across different scenarios. These results are detailed in Appendix F.2.",
  "The policy evaluation step": "In .1, we mentioned two fundamental advantages that can be attributed to the MoMAs design inthe policy evaluation phase: 1) better expressiveness of the policy class, and 2) more flexibility of functionapproximations. We add more explanations about these two points here. First, we elaborate on betterexpressiveness of the policy class and more flexibility of the value function class. Indeed, these two advantagesmainly stem from the construction of a confidence set as well as the separation of estimation and optimizationunder our model-based framework. Specifically, the offline dataset is only used to infer the transition modelrather than directly infer the value function (which also depends on a policy). Thanks to this model-based",
  "The policy improvement step": "For the policy improvement step, we have developed the first computationally efficient algorithm undergeneral function approximations (rather than linear approximations) with a theoretical guarantee. Existingliterature is only computationally efficient either under linear approximation settings (Zanette et al., 2021;Xie et al., 2021), or without a theoretical guarantee for the policy improvement step (Cheng et al., 2022).We give detailed comparisons below. Though Algorithm 1 of Xie et al. (2021) employs a mirror ascent method, it is not efficiently implementablewhen |S| = , since it is impossible to enumerate every s in a continuous state space to update the policywhen the ft in Algorithm 1 of Xie et al. (2021) actually needs the access to t( | s) for every s. Even iffinitely many t(s) are employed for obtaining ft via Monte Carlo methods, it still incurs an exponentialcomplexity of at least CT . To clearly show the difference, we exhibit our proposed algorithm and theone in Xie et al. (2021):",
  "for each i = 1, . . . , m": "In the update rule of Xie et al. (2021), t+1 (Ai | s) does not obtain a closed form without iteratively callingthe previous iteration. In the continuous state space, this procedure is computationally inefficient. Moreover,assuming that the number of calls of t() used to approximate ft; t,iis C, then at least CT number",
  "Acknowledgements": "We thank the Assigned Action Editor, Shixiang Gu, and the reviewers for their insightful comments andsuggestions that significantly improve this paper.Yanxun Xu is supported in part by National ScienceFoundation grants 1918854 and 1940107, and National Institute of Health grant R01MH128085. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deepreinforcement learning at the edge of the statistical precipice. Advances in neural information processingsystems, 34:2930429320, 2021.",
  "Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The Interna-tional Journal of Robotics Research, 32(11):12381274, 2013": "Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning with fisherdivergence critic regularization. In International Conference on Machine Learning, pp. 57745783. PMLR,2021. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learningvia bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.",
  "Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policygradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019": "Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcementlearning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems,34:1170211716, 2021. Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao. Optimal conservative offlinerl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716, 2022. Martin Riedmiller.Neural fitted q iterationfirst experiences with a data efficient neural reinforcementlearning method. In Machine Learning: ECML 2005: 16th European Conference on Machine Learning,Porto, Portugal, October 3-7, 2005. Proceedings 16, pp. 317328. Springer, 2005.",
  "Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement learning forautonomous driving. arXiv preprint arXiv:1610.03295, 2016": "Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for offline reinforcementlearning: Towards optimal sample complexity. In International Conference on Machine Learning, pp.1996720025. PMLR, 2022. Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, ThomasLampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller.Keep doing what worked: Behavioralmodelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.",
  "Ming Yin and Yu-Xiang Wang. Towards instance-optimal offline reinforcement learning with pessimism.Advances in neural information processing systems, 34:40654078, 2021": "Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, andTengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information ProcessingSystems, 33:1412914142, 2020. Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo:Conservative offline model-based policy optimization. Advances in neural information processing systems,34:2895428967, 2021.",
  "l(P1, s, s, a) l(P2, s, s, a) l0|P1(s|s, a) P2(s|s, a)|": "Assume further that l(P, s, s, a) is also strongly convex w.r.t. P(s, s, a) under the norm L2,P . SupposeH2(P ( | s, a), P( | s, a)) c3EsP (||s,a)l(P, s, s, a) c3EsP l(P , s, s, a). Let = c12n, then withhigh probabilities, we have P Pn,n and",
  "with high probability, where Unif(0, 1, ..., T 1)": "The upper bound in Theorem 2 includes two terms: a statistical error (depending on fixed n) coming fromusing offline data for estimation, and an optimization error (depending on iteration T) coming from thepolicy improvement. Different from Theorem 1, function approximation is not involved in this case. Thesacrifice is that the update rule is computationally infeasible.",
  "where the third line above is because D(p, p) 1": "2p p2 (see section 3.2). For simplicity, we assume thenorm is L1-norm here. Even in the general case, recall that this norm is defined on Rm, and by a wellknown result in functional analysis, all norms on a finite dimension linear space are equivalent. So we canstill establish a step similar to the third line in (22), replacing the second term by C",
  ".(25)": "The second term in (19) can be handled with simulation lemma: Let r(s, a) = AtPt(s, a). Consider twomodified MDPs, Mt = (S, A, Pt, r, ) and M = (S, A, P , r, ). We still focus on the policy and evaluateit under both modified MDPs. Since the visitation measure only depends on the transition probabilities andthe discounting factor, we can rewrite the expectation of r under visitation measure as the value function ofmodified MDP. Then directly apply simulation lemma:",
  "C.2Proofs for theorem 1": "Proof of theorem 1. We first focus on V Pt V tPt . Let A = {A1, A2, ..., Am}, and the goal is to find an optimalrandomized policy in the probability simplex m := {p Rm : mi=1 pi = 1, pi 0, i = 1, . . . , m}. Then, fora given t(s) m, we use",
  "F.1Synthetic dataset: an illustration": "Environment and behavioral policy detailsFor each episode that starts with an initial state s0 U(2, 2), at time n a particle undergoes a random walk and transits according to a mixture of Gaussiandynamics: sn+1 sn =: s aN(1,a, 0.1) + (1 a)N(2,a, 0.1), where the discrete action a {1, 0, 1}corresponds to Left, Stay, and Right, respectively. We choose the random walk steps 1,1 = 2, 2,1 = 0,1,0 = 1,1 = 0, 2,0 = 2,1 = 2 as known parameters, and 1 = 0 = 0.6, 1 = 0.4 as the ground truthunknown model parameters that we estimate with expectation maximization (EM). We generate a partiallycovered offline dataset collected by a biased (to the left) behavioral policy , and define a goal-reachingreward function, respectively given by:",
  "t (t(s))i": "for all s R. Thus, the particle is encouraged to reach either the positive or negative terminal state withthe shortest path possible, with a slight favor towards the positive end if the particle starts off near 0. Theoffline dataset contains 50 episodes, which are sufficient for an accurate estimation of 1 but may leadto misestimation of 0 and 1. Indeed, for our particular dataset, while the MLE 1 is accurate, 0 isunderestimated and 1 is overestimated, which could make over-exploitation of the MLE a problem.",
  "(a = 1 | s) = 1 (a = 0 | s)": "Based on these settings, we applied both a parametric policy gradient method and the proposed nonpara-metric method. The results indicate that the optimal parametric policy yields a sub-optimal policy with avalue of 23.50, whereas the proposed nonparametric method successfully identifies the optimal policy witha value of 27.08. This demonstrates the superiority of the nonparametric policy method over a pre-specifiedpolicy class."
}