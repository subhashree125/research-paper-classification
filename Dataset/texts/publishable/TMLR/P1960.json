{
  "Abstract": "We introduce a clipping strategy for Stochastic Gradient Descent (SGD) which uses quantilesof the gradient norm as clipping thresholds. We prove that this new strategy provides arobust and efficient optimization algorithm for smooth objectives (convex or non-convex),that tolerates heavy-tailed samples (including infinite variance) and a fraction of outliersin the data stream akin to Huber contamination.Our mathematical analysis leveragesthe connection between constant step size SGD and Markov chains and handles the biasintroduced by clipping in an original way. For strongly convex objectives, we prove thatthe iteration converges to a concentrated distribution and derive high probability boundson the final estimation error. In the non-convex case, we prove that the limit distributionis localized on a neighborhood with low gradient. We propose an implementation of thisalgorithm using rolling quantiles which leads to a highly efficient optimization procedurewith strong robustness properties, as confirmed by our numerical experiments.",
  "Introduction": "Stochastic gradient descent (SGD) (Robbins & Monro, 1951) is the core optimization algorithm at theorigin of most stochastic optimization procedures (Kingma & Ba, 2014; Defazio et al., 2014; Johnson &Zhang, 2013). SGD and its variants are ubiquitously employed in machine learning in order to train mostmodels (Kushner & Yin, 2003; Benveniste et al., 2012; Lan, 2020; Shalev-Shwartz et al., 2007; Bottou et al.,2018; Ma et al., 2018). The convergence properties of SGD are therefore subjects of major interest. Early studies of SGD convergence generally relied on strong assumptions such as bounded domain (Shalev-Shwartz et al., 2009) or uniformly bounded gradient variance (Rakhlin et al., 2011) and obtained errorbounds in expectation. With the recent resurgence of interest for robust statistics (Hsu & Sabato, 2016;Diakonikolas et al., 2019; Lecu & Lerasle, 2017; Prasad et al., 2018), variants of SGD based on clipping areshown to be robust to heavy-tailed gradients (Gorbunov et al., 2020; Tsai et al., 2022), where the gradientsamples are only required to have a finite variance. The latter requirement has been further weakened tothe existence of a q-th moment for some q > 1 in (Sadiev et al., 2023; Nguyen et al., 2023). In this paper,we go further and show that another variant of clipped SGD with proper thresholds is robust both to heavytails and outliers in the data stream. Robust statistics appeared in the 60s with the pioneering works of Huber, Tukey and others (Tukey, 1960;Huber, 1992; 1972; Rousseeuw & Hubert, 2011; Hampel, 1971). More recently, the field found new momentumthanks to a series of works about robust scalar mean estimation (Catoni, 2012; Alon et al., 1996; Jerrum",
  "Published in Transactions on Machine Learning Research (10/2024)": "In addition, we show similarly to Theorem 1 that the set C is small and, therefore, also petite accordingto the definitions of (Meyn & Tweedie, 1993, Chapter 5). Since V is everywhere finite and bounded onC (because the latter is compact), the conditions of (Meyn & Tweedie, 1993, Theorem 11.3.4) are fulfilledimplying that the chain is Harris recurrent. We have shown that the Markov chain verifies the fourth condition of (Meyn & Tweedie, 1993, Theorem13.0.1). This allows us to conclude that the Markov chain is ergodic i.e. we have for any initial measure that P t ,pTV 0 and the following sum is finite",
  "minRd L() := E[(, )](1)": "using observations G(, t) of the unknown gradient L(), based on samples (t)t0 received sequentiallythat include corruptions with probability < 1/2. Formulation (1) is common to numerous machine learningproblems where is a loss function evaluating the fit of a model with parameters on a sample , theexpectation E is w.r.t the unknown uncorrupted sample distribution.",
  ",(2)": "where > 0 is a constant step size and t is the clipping factor with threshold chosen as the p-th quantilet = Qp( G(t, t)) with G(t, t) an uncorrupted sample of L(t) and p (0, 1) (details will follow).Quantiles are a natural choice of clipping threshold which allows to handle heavy tails (Rothenberg et al.,1964; Bloch, 1966) and corrupted data. For instance, the trimmed mean offers a robust and computationallyefficient estimator of a scalar expectation (Lugosi & Mendelson, 2021). Since the quantile Qp( G(t, t)) isnon-observable, we introduce a method based on rolling quantiles in which keeps the procedure O(d)both memory and complexity-wise. The main benefit of QC-SGD 2 is to grant robustness to the presenceof a proportion < 1/2 of corruptions in the stream of gradient samples. This could not be achieved byprevious clipped SGD methods (Gorbunov et al., 2020; Tsai et al., 2022; Sadiev et al., 2023; Nguyen et al.,2023). We also show that iteration (2) is adaptive to heavy-tailed gradient variance and converges to a limitdistribution with strong concentration properties.",
  "Contributions.Our main contributions are as follows:": "For small enough and well-chosen p, we show that, whenever the optimization objective is smoothand strongly convex, QC-SGD converges geometrically to a limit distribution such that the deviationaround the optimum achieves the optimal dependence on . In the non-corrupted case = 0 and with a strongly convex objective, we prove that a coordinatedchoice of and p ensures that the limit distribution is sub-Gaussian with constant of order O().In the corrupted case > 0, the limit distribution is sub-exponential. For a smooth objective (non-convex) whose gradient satisfies an identifiability condition, we provethat the total variation distance between QC-SGD iterates and its limit distribution vanishes sub-linearly. In this case, the limit distribution is such that the deviation of the objective gradient isoptimally controlled in terms of . Finally, we provide experiments to demonstrate that QC-SGD can be easily and efficiently imple-mented by estimating Qp( G(t, t)) with rolling quantiles. In particular, we show that the iterationis indeed robust to heavy tails and corruption on multiple stochastic optimization tasks.",
  "Our theoretical results are derived thanks to a modelling through Markov chains and hold under an Lqassumption on the gradient distribution with q > 1": "Related works.Convergence in distribution of the Markov chain generated by constant step size SGD,relatively to the Wasserstein metric, was first established in (Dieuleveut et al., 2020). Another geometricconvergence result was derived in (Yu et al., 2021) for non-convex, non-smooth, but quadratically growingobjectives, where a convergence statement relatively to a weighted total variation distance is given and aCLT is established. These papers do not consider robustness to heavy tails or outliers. Early works proposedstochastic optimization and parameter estimation algorithms which are robust to a wide class of noise ofdistributions (Martin & Masreliez, 1975; Polyak & Tsypkin, 1979; 1981; Price & VandeLinde, 1979; Stankovi& Kovaevi, 1986; Chen et al., 1987; Chen & Gao, 1989; Nazin et al., 1992), where asymptotic convergenceguarantees are stated for large sample sizes. Initial evidence of the robustness of clipped SGD to heavytails was given by (Zhang et al., 2020) who obtained results in expectation. Subsequent works derived high-confidence sub-Gaussian performance bounds under a finite variance assumption (Gorbunov et al., 2020;Tsai et al., 2022) and later under an Lq assumption (Sadiev et al., 2023; Nguyen et al., 2023) with q > 1.A similar SGD clipping scheme to (2) is presented in (Seetharaman et al., 2020), however, in contrast toour work, they do not consider the robust setting and focus on experimental study while we also providetheoretical guarantees. Robust versions of Stochastic Mirror Descent (SMD) are introduced in (Nazin et al., 2019; Juditsky et al.,2023). For a proper choice of the mirror map, SMD is shown to handle infinite variance gradients withoutany explicit clipping (Nemirovskij & Yudin, 1983; Vural et al., 2022). Finally, (Diakonikolas et al., 2022)study heavy-tailed and outlier robust streaming estimation algorithms of the expectation and covariance.On this basis, robust algorithms for linear and logistic regression are derived. However, the involved filteringprocedure is hard to implement in practice and no numerical evaluation of the considered approach isproposed. Agenda.In we set notations, state the assumptions required by our theoretical results andprovide some necessary background on continuous state Markov chains. In , we state our resultsfor strongly convex objectives including geometric ergodicity of QC-SGD (Theorem 1), characterizations ofthe limit distribution and deviation bounds on the final estimate. In , we remove the convexityassumption and obtain a weaker ergodicity result (Theorem 2) and characterize the limit distribution in termsof the deviations of the objective gradient. Finally, we present a rolling quantile procedure in anddemonstrate its performance through a few numerical experiments on synthetic and real data.",
  "for all R > 0, where () is independent of": "In addition to the unbiased property, Assumption 4 imposes that the noise distribution be expressible asthe combination of two components, one of which must be diffuse with density satisfying a minorizationinequality. Note that this is a weak constraint since it is satisfied, for example, as soon as the noise admits a density w.r.t. Lebesgues measure which is positive everywhere. This condition is similar to (Yuet al., 2021, Assumption 2.3) since both find their origin in Markov chain minorization conditions (Meyn &Tweedie, 1993, .2). These ensure that a chain properly explores its state space and are a commonway to prove Markov chain convergence (Rosenthal, 1995b;a; Douc et al., 2004; Meyn & Tweedie, 1994;Baxendale, 2005). Our last assumption formalizes the requirement of a finite moment for the gradient error.",
  "for all Rd, where Aq, Bq > 0. When L is not strongly convex, we further assume that Aq = 0": "The bound (4) captures the case of arbitrarily high noise magnitude through the dependence on .This is consistent with convex optimization problems with L-Lipschitz-smooth objectives (Assumption 1)where the norm of the gradient L() is bounded by L . Assumption 5 improves upon theconditions used in (Gorbunov et al., 2020; Tsai et al., 2022; Gorbunov et al., 2023; Nguyen et al., 2023) sincethese either required a uniformly constant upperbound (independent of ) or only considered the case q = 2(finite variance). For non-strongly convex L, we require Aq = 0 since may not exist.",
  "for t 0 and A B(Rd). The transition kernel P,p acts on probability distributions M1(Rd) throughthe mapping P,p which is defined, for all A B(Rd), by P,p(A) =": "A P,p(, A)d() = P(t+1 A|t ). For n 1, we similarly define the multi-step transition kernel P n,p which is such that P n,p(t, A) =P(t+n A | t) and acts on probability distributions M1(Rd) through P n,p = (P,p)P n1,p . Finally,we define the total variation (TV) norm of a signed measure as",
  "(A) 2(A)": "The second equality reflects the fact that the TV distance between two probability measures corresponds tothe largest absolute difference between the probabilities they assign to the same event. The TV distance isa broadly used metric to quantify the convergence of Markov chains (Levin & Peres, 2017; Baxendale, 2005;Meyn & Tweedie, 1993; Rosenthal, 1995a) besides the Wasserstein distance (Dieuleveut et al., 2020). In the next section, we will prove that the Markov chain defined by iteration (2) converges to uniqueinvariant distribution in TV distance. This convergence mode will allow us to extrapolate the properties ofthe limit distribution on the iterates t and thus derive non-asymptotic concentration bounds for them, seeCorollaries 2 and 1 below.",
  "where 0 is the Dirac measure located at 0": "The proof of Theorem 1 is given in Appendix D.3 and relies on the geometric ergodicity result of (Meyn& Tweedie, 1993, Chapter 15) for Markov chains with a geometric drift property.A similar result forquadratically growing objectives was established by (Yu et al., 2021) and convergence w.r.t. Wassersteinsmetric was shown in (Dieuleveut et al., 2020) assuming gradient co-coercivity. However, robustness was notconsidered in these works. Theorem 1 establishes the iterations convergence to a unique invariant measure,p. The properties of this limit distribution will be explored in the sequel. The restriction p comesfrom the consideration that other quantiles are not estimable in the event of -corruption. Condition (8) is",
  "p": "The proof can be found in Appendix D.6. The strong concentration properties given by Proposition 2 forthe invariant distribution appear to be new. Still, the previous result remains asymptotic in nature. Highconfidence deviation bounds for an iterate t can be derived by leveraging the convergence in Total Variationdistance given by Theorem 1 leading to the following result.Corollary 1. In the setting of Proposition 2, in the absence of corruption = 0, after T iterations, for > 0, we have",
  "+ T M1 + 0 2": "Choosing a smaller step size in Corollary 1 allows to improve the deviation bound. However, this comesat the cost of weaker confidence because of slower convergence due to a greater . See Appendix C for adiscussion including a possible compromise. Corollary 1 may be compared to the results of (Gorbunov et al.,2020; Tsai et al., 2022; Sadiev et al., 2023; Nguyen et al., 2023) which correspond to 1/T and have asimilar dependence on the dimension through the gradient variance. Although their approach is also basedon gradient clipping, they use different thresholds and proof methods. In the presence of corruption, theinvariant distribution is not sub-Gaussian. This can be seen by considering the following toy Markov chain:",
  "Xt + w.p.1 Xt + w.p": "where < 1, > 0 are constants and is a positive random noise. Using similar methods to the proofof Theorem 1, one can show that (Xt)t0 converges (for any initial X0) to an invariant distribution whosemoments can be shown to grow linearly, indicating a sub-exponential distribution and excluding a sub-Gaussian one. We provide additional details for the underlying argument in Appendix D.7. For the corruptedcase, the sub-exponential property stated in Proposition 2 holds with a constant K of order /, which isnot satisfactory and leaves little room for improvement due to the inevitable bias introduced by corruption.Therefore, we propose the following procedure in order to obtain a high confidence estimate, similarly toCorollary 1.",
  "iN (only satisfying L2 bounds) into a strong one with": "sub-exponential deviation. This is done by picking (i)Twhich is such that the median of its distances toother estimators r(i)N/2 is minimal. The aggregated estimator satisfies the high probability bound given inthe next result.Corollary 2. Assume the same as in Theorem 1 and Proposition 1. Consider given by Algorithm 1, withthe assumption that the gradient sample sets used for each(n)T",
  ".(13)": "We obtain a high confidence version of the bound in expectation previously stated in Proposition 1. Asargued before, the above bound depends optimally on . Similar bounds to (13) are obtained for q = 2in (Diakonikolas et al., 2022) for streaming mean estimation, linear and logistic regression. Their resultsenjoy better dimension dependence but are less general than ours since we handle the case q (1, 2) andconsider strongly convex objectives more broadly. In addition, our results further extend to non-convexobjectives as detailed in the next section. Finally, the implementation of the algorithm in (Diakonikolaset al., 2022) is not straightforward whereas our method is quite easy to use (see ).",
  "Non-Convex Objectives": "In this section, we drop Assumption 2 and consider the optimization of possibly non-convex objectives.Consequently, the existence of a unique optimum and the quadratic growth of the objective are no longerguaranteed. This motivates us to use a uniform version of Assumption 5 with Aq = 0 since the gradient isno longer assumed coercive and its deviation moments can be taken as bounded. In this context, we obtainthe following weaker (compared to Theorem 1) ergodicity result for QC-SGD. Theorem 2 (Ergodicity). Let Assumptions 1, 3, 4 and 5 hold with Aq = 0 (uniformly bounded moments)and let L be an objective such that inf L() > is finite. Let (t)t0 be the Markov chain generated byQC-SGD with step size and quantile p . Assume that p and are such that 3p(1)/4 > L +and that the subset of Rd given by",
  "where ,p is a unique invariant measure and where 0 is the Dirac measure located at 0": "The proof is given in Appendix D.10 and uses ergodicity results from (Meyn & Tweedie, 1993, Chapter 13).Theorem 2 provides convergence conditions for an SGD Markov chain on a smooth objective in a robustsetting. We are unaware of anterior results of this kind in the literature. Condition (14) requires that the",
  "p(1 )3p(1 )/4 L .(16)": "The statement of Proposition 3 is clearly less informative than Propositions 1 and 2 since it only pertainsto the gradient rather than, for example, the excess risk. This is due to the weaker assumptions that donot allow to relate these quantities. Still, the purpose remains to find a critical point and is achieved upto O(11/q) precision according to this result. Due to corruption, the estimation error on the gradientcannot be reduced beyond (11/q) (Prasad et al., 2020; Hopkins & Li, 2018; Diakonikolas & Kane, 2019).Therefore, one may draw a parallel with a corrupted mean estimation task, in which case, the previous rateis, in fact, information-theoretically optimal.",
  "Implementation and Numerical Experiments": "The use of the generally unknown quantile Qp( G(t, t)) in QC-SGD constitutes the main obstacle toits implementation.For strongly convex objectives, one may use a proxy such as at ref + b withpositive a, b and ref Rd an approximation of serving as reference point.This choice is consistentwith Assumptions 1 and 5, see Lemma 2 in Appendix D. For instances of Problem (1) defined with an",
  "return T": "asymptotically linear function such as the logistic, hinge or Hubers loss, a constant threshold can be usedsince the gradient is a priori uniformly bounded, implying the same for the quantiles of its deviations. Inpractice, we propose a simpler and more direct approach: we use a rolling quantile procedure, described inAlgorithm 2. The latter stores the values (G(tj, tj))1jS in a buffer of size S N and replacesQp( G(t, t)) in QC-SGD by an estimate Qp which is the pS-th order statistic in the buffer. Note thatonly the norms of previous gradients are stored in the buffer, limiting the memory overhead to O(S). Thecomputational cost of Qp can also be kept to O(S) per iteration thanks to a bookkeeping procedure (seeAppendix B).",
  "Linear regression.We consider least-squares linear regression and compare RQC-SGD with Hubersestimator (Huber, 1973) and clipped SGD (designated as CClip()) with three clipping levels max": "d for {0.8, 1.0, 1.2} where max is a fixed data scaling factor. These thresholds provide a rough estimateof the gradient norm near the optimum . We generate covariates X and labels Y both heavy-tailed andcorrupted. Corruption in the data stream is generated according to Assumption 3 with outliers representedeither by aberrant values or fake samples Y = Xfake + using a false parameter fake, see Appendix Bfor further details on data generation and fine tuning of the Huber parameter. All methods are run withconstant step size and averaged results over 100 runs are displayed on (top row). As anticipated, Hubers loss function is not robust to corrupted covariates.In contrast, using gradientclipping allows convergence to meaningful estimates. Although this holds true for a constant threshold, shows it may considerably slow the convergence if started away from the optimum. In addition,the clipping level also affects the final estimation precision and requires tuning. Both of the previous issuesare well addressed by RQC-SGD whose adaptive clipping level allows fast progress of the optimization andaccurate convergence towards a small neighborhood of the optimum. Logistic regression.We test the same methods on logistic regression. Hubers baseline is representedby the modified Huber loss (also known as quadratic SVM (Zhang, 2004)). We generate data similarly tothe previous task except for the labels which follow Y Bernoulli((X)) with the sigmoid function.Corrupted labels are either uninformative, flipped or obtained with a fake fake (see details in Appendix B).Results are displayed on the bottom row of . As previously, Hubers estimator performs poorly with corruption. However, constant clipping appears to bebetter suited when the gradient is bounded, so that the optimization is less affected by its underestimation.We observe, nonetheless, that a higher clipping level may lead to poor convergence properties, even at alow corruption rate. Note also that the constant levels we use are based on prior knowledge about the datadistribution and would have to be fine tuned in practice. Meanwhile, the latter issue is well addressed byquantile clipping. Finally, notice that no algorithm truly approaches the true solution for this task. Thisreflects the difficulty of improving upon Proposition 3 which only states convergence to a neighborhoodwhere the objective gradient is comparable to the estimation error in magnitude. Classification with shallow networks.Finally, we evaluate the performance on the task of traininga single hidden layer neural network classifier on some real datasets which corresponds to a non-convexoptimization problem. To handle multiclass data, we use the cross entropy loss and replace Hubers baselinewith plain SGD for simplicity. We define constant clipping baselines using thresholds given by the quantilesof order p = 0.25, 0.5, and 0.75 of the norms of a batch of gradients at the beginning of the optimisation. Dueto the greater sensitivity to corruption observed in this case, we set = 0.02 and use p = 0.9 for RQC-SGD.",
  "Loss": "codrna RQC_SGDSGDCClip(p=0.25)CClip(p=0.5)CClip(p=0.75) 025000 50000 75000 100000 1.4 1.6 1.8 2.0 2.2 2.4 2.6sensorless 0.000.250.500.751.00 1e6 0.8 1.0 1.2 1.4 1.6covtype : Evolution of the test loss (y-axis) against iteration t (x-axis) for the training of a single hidden layernetwork on different real world classification datasets (average over 20 runs). We observe more consistentand stable objective decrease for RQC-SGD whereas constant clipping baselines are slower and may fail toconverge. We train all methods with one sample per iteration using equal step sizes and evaluate them through the testloss. We provide further results and experimental details in Appendix B. Results are displayed on . Unsurprisingly, standard SGD is not robust to corrupted samples and, while using a constant clipping levelhelps keep the optimisation on track, the experiments show that careful tuning may sometimes be necessaryto prevent divergence. On the other hand, the adaptive clipping levels used by RQC-SGD allow to make theiteration faster and more resilient to corruption. This leads to an optimization path with a more consistent",
  "Conclusion": "We introduced a new clipping strategy for SGD and proved that it defines a stochastic optimization procedurewhich is robust to both heavy tails and outliers in the data stream. We also provided an efficient rollingquantile procedure to implement it and demonstrated its performance through numerical experiments onsynthetic and real data.Future research directions include improving the dimension dependence in ourbounds, possibly by using sample rejection rules or by considering stochastic mirror descent (Nemirovskij &Yudin, 1983; Beck & Teboulle, 2003) clipped with respect to a non Euclidean norm. This may also procurerobustness to higher corruption rates. Another interesting research track is the precise quantification of thegeometric convergence speed of the Markov chain generated by constant step size SGD on a strongly convexobjective.",
  "Sbastien Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in MachineLearning, 8(3-4):231357, 2015": "Herv Cardot, Peggy Cnac, and Pierre-Andr Zitt. Efficient and fast estimation of the geometric medianin Hilbert spaces with an averaged stochastic gradient algorithm. Bernoulli, 19(1):18 43, 2013. doi:10.3150/11-BEJ390. URL Herv Cardot, Peggy Cnac, and Antoine Godichon-Baggioni. Online estimation of the geometric medianin Hilbert spaces: Nonasymptotic confidence balls. The Annals of Statistics, 45(2):591 614, 2017. doi:10.1214/16-AOS1460. URL Olivier Catoni. Challenging the empirical mean and empirical variance: A deviation study. Annales delInstitut Henri Poincar, Probabilits et Statistiques, 48(4):1148 1185, 2012. doi: 10.1214/11-AIHP454.URL",
  "Ilias Diakonikolas, Daniel M Kane, and Ankit Pensia. Outlier robust mean estimation with subgaussianrates via stability. Advances in Neural Information Processing Systems, 33:18301840, 2020": "Ilias Diakonikolas, Daniel M Kane, Ankit Pensia, and Thanasis Pittas.Streaming algorithms for high-dimensional robust statistics. In International Conference on Machine Learning, pp. 50615117. PMLR,2022. Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochasticgradient descent and Markov chains. The Annals of Statistics, 48(3):1348 1382, 2020. doi: 10.1214/19-AOS1850. URL",
  "Boris Teodorovich Polyak and Yakov Zalmanovich Tsypkin. Robust pseudogradient adaptation algorithms.Automation and Remote Control, 41(10):14041409, 1981": "Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation viarobust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology),82, 2018. Adarsh Prasad, Sivaraman Balakrishnan, and Pradeep Ravikumar. A robust univariate mean estimator isall you need. In International Conference on Artificial Intelligence and Statistics, pp. 40344044. PMLR,2020.",
  "Peter J Rousseeuw and Mia Hubert. Robust statistics for outlier detection. Wiley Interdisciplinary Reviews:Data Mining and Knowledge Discovery, 1(1):7379, 2011": "Abdurakhmon Sadiev, Marina Danilova, Eduard Gorbunov, Samuel Horvth, Gauthier Gidel, PavelDvurechensky, Alexander Gasnikov, and Peter Richtrik. High-probability bounds for stochastic opti-mization and variational inequalities: the case of unbounded variance. In International Conference onMachine Learning, pp. 2956329648. PMLR, 2023. Prem Seetharaman, Gordon Wichern, Bryan Pardo, and Jonathan Le Roux. Autoclip: Adaptive gradientclipping for source separation networks. In 2020 IEEE 30th International Workshop on Machine Learningfor Signal Processing (MLSP), pp. 16. IEEE, 2020.",
  "Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science, vol-ume 47. Cambridge university press, 2018": "Nuri Mert Vural, Lu Yu, Krishna Balasubramanian, Stanislav Volgushev, and Murat A Erdogdu. Mirrordescent strikes again: Optimal stochastic convex optimization under infinite noise variance. In Conferenceon Learning Theory, pp. 65102. PMLR, 2022. Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A Erdogdu. An analysis of constantstep size SGD in the non-convex regime: Asymptotic normality and bias. Advances in Neural InformationProcessing Systems, 34:42344248, 2021. Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar,and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural InformationProcessing Systems, 33:1538315393, 2020.",
  "Expectation estimation.We estimate the expectation of a random vector X by minimizing the objectiveL() = 1": "22 with = E[X] using a stream of both corrupted and heavy-tailed samples, see Appendix Bfor details. We run RQC-SGD (Algorithm 2) and compare it to an online version of geometric and coordinate-wise Median-Of-Means (GMOM and CMOM (Cardot et al., 2017; 2013)) which use block sample means tominimize an L1 objective (see Appendix B). Although these estimators are a priori not robust to -corruption,we ensure that their estimates are meaningful by limiting to 4% and using blocks of 10 samples. Thus,blocks are corrupted with probability < 1/2 so that the majority contains only true samples.displays the evolution of t for each method averaged over 100 runs for increasing and constant stepsize. We also display a single run for = 0.04. We observe that RQC-SGD is only weakly affected by theincreasing corruption whereas the performance of GMOM and CMOM quickly degrades with , leading tounstable estimates.",
  "BExperimental details": "As previously mentioned, the dimension is set to d = 128 in our experiments with synthetic data. We alsoset min = 1 and max = 5 as minimum and maximum scaling factors. For all tasks and algorithms, theoptimization starts from 0 = 0. Bookkeeping in RQC-SGDThe buffer in Algorithm 2 stores values in sorted order along with theirages. The most recent and oldest values have ages 0 and S 1 respectively. At each iteration, a newgradient is received, all ages are incremented and the oldest value is replaced by the new one with age 0.The latter is then sorted using one iteration of insertion sort. The estimate Qp is retrieved at each iterationas the value at position pS.",
  "Algorithm": "RQC-SGDCMOMGMOM : Evolution of t (y-axis) against iteration t (x-axis) for the expectation estimation task,averaged over 100 runs at different corruption levels (bands widths correspond to the standard deviationof the 100 runs). For = 0.04, the evolution on a single run is also displayed. We observe good performancefor RQC-SGD for increasing while CMOM and GMOM are more sensitive.",
  "B.1Mean estimation": "Data generationWe compute a matrix = (AA + AA)/2 where A Rdd is a random matrix withi.i.d centered Gaussian entries with variance 1/d sampled once and for all. We generate true samples asX = 1 +V where V is a vector of i.i.d symmetrized Pareto random variables with parameter 2 and 1 Rd",
  "E XNb2andE XNb1,": "where XNb is the average of Nb independent copies of X. The block size is set to Nb = 10 in the wholeexperiment. The above objectives are optimized by computing samples of XNb in a streaming fashion sothat one step is made for each Nb samples. In order to compensate for this inefficiency we multiply the stepsize by Nb for both GMOM and CMOM. For GMOM, we additionally multiply the step size by",
  "B.2Linear regression": "Data generationWe choose the true parameter by independently sampling its coordinate uniformlyin the interval [5, +5]. The true covariates are sampled as X = V where is a diagonal matrix withentries sampled uniformly in the interval [min, max] (once and for all) and V is a vector of i.i.d symmetrizedPareto random variables with parameter 2. The labels are sampled as Y = X+ where is a symmetrizedPareto random variable with parameter 2.",
  "We train a single hidden layer neural network classifier with 100 hidden neurones for all datasets. We useone sample per iteration and step size = 102 for all methods": "As previously, RQC-SGD is run with buffer size S = 100 and unif = 10.The quantile value was setto p = 0.9. We compute the gradient norms over a batch of samples of size S at the beginning of theoptimization and use the quantiles of order p = 0.25, 0.5 and 0.75 as the clipping level for the constantclipping baselines.",
  "DataWe used publicly available datasets for our experiments. We provide details about their character-istics and sources in": "We use a 10% share of each dataset as a test set in order to compute the test loss plotted in Figures 2 and 3.We also ensure the test set contains at least 5000 elements. Optimization is run using the remaining trainset which is corrupted as specified next. The results are averaged over 20 runs for each datasets.",
  "CGeometric convergence speed and relation to step size": "The geometric Markov chain convergence stated in Theorem 1 occurs at a speed determined by the con-traction factor which mainly depends on the step size and quantile p defining the iteration. Therefore,an explicit formulation of this dependency is necessary to precisely quantify the convergence speed. Thisquestion is lightly touched upon in (Yu et al., 2021) whose Proposition 2.1 is an analogous SGD ergodicityresult. Like Theorem 1, the latter relies on the Markov chain theory presented in (Meyn & Tweedie, 1993).It is argued in (Yu et al., 2021) that a vanishing step size 0 causes to be close to one, leading toslow convergence but with smaller bias. However, these considerations remain asymptotic and do not quiteaddress the convergence speed issue. More generally, the precise estimation of the factor goes back to the evaluation of the convergence speedof a Markov chain satisfying a geometric drift property. Near optimal results exist for chains with particularproperties such as stochastic order (Lund et al., 1996; Roberts & Tweedie, 2000), reversibility (Jerison,2019) or special assumptions on the renewal distribution (Berenhaut & Lund, 2001). Unfortunately, suchproperties do not hold for SGD. Let (t)t0 be a Markov chain satisfying the drift property:",
  "L() L(), ,(20)": "valid for all , .Inequality (19) is stated, for example, in (Nesterov, 2014, Theoerem 2.1.12) (seealso (Bubeck, 2015, Lemma 3.11) and (20) is just a characterization of strong convexity (see for instance (Nes-terov, 2014, Theorem 2.1.9)). Note that Lemma 1 is a simple generalization of the usual contraction property in strongly-convex optimiza-tion where is usually taken as . In that case, one has L() = 0. An example of such a result is givenby (Bubeck, 2015, Theorem 3.10). A similar inequality to Lemma 1 was previously obtained in (Dieuleveutet al., 2020, Proposition 2) where convergence of SGD as Markov chain was studied.",
  "First, we define = inf . Note that Assumption 4 excludes the existence of such that G() = 0 almostsurely, therefore, we have > 0": "Thanks to Assumption 4 and conditioning on t = Rd for t 0, the distribution of t+1 has a strictlypositive density at least on a ball of radius around t. This implies that the chain is aperiodic sinceP,p(t, Wt) > 0 for any neighborhood Wt contained in the previous ball. Moreover, by induction, thedistribution of t+m has positive density at least on a ball of radius m around t. Thus for m high enoughwe have P m,p(t, A) > 0 for any set A with non zero Lebesgue measure. It follows that the Markov chain isirreducible w.r.t. Lebesgues measure and is thus -irreducible (see (Meyn & Tweedie, 1993, Chapter 4)).",
  "Assuming that the distribution of the noise has a density, one can show that the chain is aperiodic andsatisfies a minorization property as in the proof of Theorem 1 (see Appendix D.3)": "Defining V (x) = 1 + x, we can show that V verifies a geometric drift property similar to (25). Consequently,Theorem 15.0.1 of (Meyn & Tweedie, 1993) applies to the chain (Xt)t0 and implies that it convergesgeometrically to a limit distribution analogously to the claim of Theorem 1. We denote M kk = E|X|k the absolute moments of X for k 1 and show that Mk = (k) (we merely providea sketch and do not attempt to explicitly compute the involved constants). For X following the invariantmeasure, using the recursion defining Xt and the positivity of , it is easy to establish the inequality fork 1",
  "D.10Proof of Theorem 2": "As previously done in the proof of Theorem 1, we show that the Markov chain is aperiodic. Note that sinceL has finite lower bound inf L, we can replace it with 1 + L() inf L and assume it is positive in the restof the proof without loss of generality. We will now show that it satisfies a drift property. Let befixed, using Assumptions 1 and 3, we have :",
  "D.11Proof of Proposition 3": "By Theorem 2, the assumptions imply that the Markov chain (t)t0 converges to an invariant distribution,p. For ,1, by invariance of ,1, we have that the variables L( G()) and L() areidentically distributed. Taking the expectation w.r.t. , this implies the identity E[L(G())] = E[L()].Plugging into Inequality (42) from the proof of Theorem 2, we find"
}