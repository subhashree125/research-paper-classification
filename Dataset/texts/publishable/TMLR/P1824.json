{
  "Abstract": "In this work, we highlight and perform a comprehensive study on calibration attacks, a formof adversarial attacks that aim to trap victim models to be heavily miscalibrated withoutaltering their predicted labels, hence endangering the trustworthiness of the models andfollow-up decision making based on their confidence.We propose four typical forms ofcalibration attacks: underconfidence, overconfidence, maximum miscalibration, and randomconfidence attacks, conducted in both black-box and white-box setups. We demonstrate thatthe attacks are highly effective on both convolutional and attention-based models: with asmall number of queries, they seriously skew confidence without changing the predictiveperformance. Given the potential danger, we further investigate the effectiveness of a widerange of adversarial defence and recalibration methods, including our proposed defencesspecifically designed for calibration attacks to mitigate the harm. From the ECE and KSscores, we observe that there are still significant limitations in handling calibration attacks.To the best of our knowledge, this is the first dedicated study that provides a comprehensiveinvestigation on calibration-focused attacks. We hope this study helps attract more attentionto these types of attacks and hence hamper their potential serious damages. To this end,this work also provides detailed analyses to understand the characteristics of the attacks. 1",
  "Introduction": "While recent machine learning models have significantly improved the state-of-the-art performance on awide range of tasks (Bengio et al., 2021; Vaswani et al., 2017; LeCun et al., 2015; Krizhevsky et al., 2012),these models are often vulnerable and easily deceived by perturbed inputs (Ren et al., 2020). Adversarialattacks (Ren et al., 2020) have been shown to be a crucial tool to reveal the susceptibility of victim models(Ibitoye et al., 2019; Zimmermann et al., 2022; Xiao et al., 2023). In the classic setup, adversarial examplesare generated by introducing imperceptible modifications to an original datapoint to cause misclassification,where the focus is on trapping victim models to make incorrect predictions. In this paper, we highlight and provide a comprehensive study on a different type of threat, which wecall calibration attacks. These attacks focus on manipulating the victim models confidence scores withoutmodifying their predicted labels, hence endangering any follow-up decision-making that is based on the victim",
  "Random Confidence": ": Reliability diagrams of a ResNet-50 classifier (fine-tuned and tested on Caltech-101) before and after thefour forms of calibration attacks. Red bars show the average accuracy on the test data binned by confidence scores(15 bins) and the blue bars are the average confidence of samples in each bin. The x-axis represents the bins andy-axis is the accuracy (for red bars) or confidence (for blue bars). The yellow line represents perfect calibration.To have the minimum possible ECE the red bars and blue bars have to completely overlap in each bin (shown inmaroon), where no overlap represents miscalibration. Despite the accuracy being unchanged, the miscalibration issevere after the attacks. models confidence. We propose to conduct four forms of calibration attacks: underconfidence, overconfidence,maximum miscalibration, and random confidence attacks, which can seriously skew confidence and causeheavy miscalibration, as demonstrated in the reliability diagrams in . As we will show and discussin our study, calibration attacks are insidious and hard to detect. The intrinsic harm is that on the surfacethe models appear to still make correct decisions, but the level of miscalibration could make the modelsdecisions malicious for downstream tasks. Our specific studies consist of four forms of attacks, spanning black-box and white-box setups. We attacktypical convolutional and attention-based models, and investigate both attack and defence methods.",
  "To the best of our knowledge, this is the first dedicated study that provides a comprehensiveinvestigation on confidence-focused calibration attacks": "We propose four typical forms of calibration attacks and demonstrate their effectiveness and dangerto victim models from different perspectives.Detailed insights are provided to understand thecharacteristics of the attacks and the vulnerability of victim models. We further investigate the effectiveness of a wide range of adversarial defence and recalibrationmethods, including our proposed defences specifically designed for calibration attacks to mitigatethe harm. We hope our work helps attract more attention to these attacks and hence hamper theirpotential serious damages in applications.",
  "Related Work": "Calibration of Machine Learning Models.Among the two typical types of calibration methods, post-calibration methods are applied directly to the predictions of fully trained models at test time, which includeclassical approaches such as temperature scaling (Guo et al., 2017), Platt scaling (Platt, 1999), isotonicregression (Zadrozny & Elkan, 2002), and histogram binning (Zadrozny & Elkan, 2001). Training-based ap-proaches, however, often add bias to help calibrate a model during training (Zhang et al., 2018; Thulasidasanet al., 2019; Kumar et al., 2018; Tomani & Buettner, 2021). We investigate a diverse range of calibrationmethods against calibration attacks to reveal the limitations that require them to be overhauled to dealwith attacks, examining the vulnerability of models similar to those on the convolutional architectures (Guoet al., 2017; Minderer et al., 2021) and Transformer-based frameworks (Dosovitskiy et al., 2021). (Refer toAppendix A for a more detailed summary of related work.)",
  "Published in Transactions on Machine Learning Research (11/2024)": "over TS in most cases. It appears that this is due in part to not finding the ideal temperature parameterfor each image due to the difficulty of identifying the correct image domains, as in, recognizing which formof attack occurred. The Splines method is similar in its pre-attack calibration benefits to TS, but differsgreatly in its performance post-attack. In some cases, like with CIFAR-100 using ResNet, it is easily the worstperforming defence method. In other cases, particularly for Caltech-101 and GTSRB ViT, it is able to keepECE at relatively reasonable values post-attack. This discrepancy shows that finding an ideal recalibrationfunction has the potential to be a strong defence. The training-based DCA and SAM methods tend to bringfew benefits after being attacked, even when they improve the calibration on clean data, the post-attackECE and KS errors are not substantially different compared to the vanilla models. The performance of the regular adversarial defence techniques is mixed. For robustness, AAA in most casesachieves the lowest post-attack ECE. Even in the best cases like Caltech-101 ResNet, ECE tends to be atleast double compared pre-attack, and in most cases we still observed multiple-fold increases. This techniqueis also among the poorest calibrated on clean data.Regarding AT, our approach does not compromiseon accuracy and miscalibration on clean data. It brings notable robustness, especially compared to thecalibration methods, but it is not among the strongest. Lastly, CS is the strongest methods at maintaining low calibration error on post-attack datapoints. Moreover,the technique tends to have better calibration error on clean data compared to AAA. It shows how it is keythat high confidence values are retained to have decent calibration after the attacks. Altogether, despitesome promising results with the defences, as a whole there are still limitations particularly with the strongestadversarial defences. The compromise of poor ECE on clean data for better calibration robustness againstthe attacks that we observe, as well as the general inconsistent performance means that further refinementon defences is warranted.",
  "Calibration Attacks": "Given the input X {x1, . . . , xN} of N datapoints and their ground-truth labels Y {y1, . . . , yN}, whereyi {1, . . . , K} with K being the number of classes, a classifier F makes prediction for an instance xi Xthrough a mapping F : xi yi, pi, where yi is the predicted label which is often obtained by takingargmax on the output distribution pi over the K classes: yi = argmaxKj=1(pij). When needed, pi is writtenas p(xi) and p(xi), for the input xi and its perturbation xi, respectively. Similarly, yi can be rewritten asy(xi) or y(xi), which will be constrained to be same in calibration attacks.",
  "Objective of Calibration Attacks": "Calibration attacks aim to generate adversarial examples to optimize a predefined miscalibration functionM(xi, k) for an adversarial example xi and the predicted class k. As will be detailed below, we proposefour forms of calibration attacks where M(xi, k) takes different implementations. Following conventionalnotation, an adversarial example xi is created by adding noise to an input xi: xi = xi + , bounded by in a lm-ball:",
  "xi xim < , m {0, 1, . . . , },(1)": "where controls the amount of allowed perturbation and m corresponds to different norms that may beused. In general, our attacks are based on the most popular view of class-wise calibration (Guo et al., 2017;Kull et al., 2019). For a datapoint xi, yi in the dataset D = {xn, yn}Nn=1, a well calibrated model aimsto achieve:",
  "By minimizing the loss MUCA or MOCA, the attack models maximize calibration errors in these two setupsrespectively, with the corresponding adversarial examples X :maxX(P(yi = k | pk(xi) = qk) qk).(6)": "Algorithm 1 depicts an overview of calibration attacks, including UCA and OCA, but also MMA and RCA thatwe will introduce below. Unlike conventional attacks that modify the predicted labels and focus mainly oncorrectly classified examples, calibration attacks modify confidence and focus on both the originally correctlyclassified and misclassified instances. As detailed later in , we will implement calibration attacks inpopular black-box (Andriushchenko et al., 2020) and white-box frameworks (Madry et al., 2018). Maximum Miscalibration Attacks (MMA). We propose MMA with the aim of exploring and understandingmore serious scenarios of miscalibration. The main principle of MMA is to perturb a set of inputs such that(i) all incorrectly classified datapoints (a set where the model has zero accuracy) have 100% confidence,and (ii) all correctly classified datapoints (a set where the model is 100% accurate) have the minimumpossible confidence. MMA uses a combination of OCA and UCA to accomplish this, as shown in Algorithm 1.Proposition 3.1 below states the property of MMA in terms of the oracle (upper-bound) ECE score that canbe achieved in theory, with the proof provided in Appendix C. Proposition 3.1. Assume q is the accuracy of a K-way classifier F on the dataset D = {xn, yn}Nn=1. TheMaximum Miscalibration Attack (MMA) maximizes the expected calibration error (ECE). The upper bound ofECE that can be achieved by MMA is 1 q/K. Random Confidence Attacks (RCA). We propose to perform RCA to decouple the victim models confidencescores and predictive labels, in which the confidence scores produced by the attack model are randomized.RCA is performed by choosing a random target confidence score for each input, and then, depending on theoriginal confidence score, running the corresponding underconfidence or overconfidence attacks to producethe target confidence score.Although RCA is theoretically less effective than MMA, it is less predictable,because unlike the other three types of attacks, RCA does not produce a predetermined direction (i.e., under-or overconfidence) of attacks for a given input. The produced confidence scores are often less extreme andmore reasonable-looking, but largely meaningless due to randomization. Note that in addition to MMA andRCA, one may design other approaches to combine UCA with OCA, but MMA and RCA represent the two mosttypical compositions that create very harmful patterns of miscalibration.",
  ": end for": "adversarial training that utilizes our white-box calibration attacks to generate adversarial training examplesfor each minibatch during training. Hence, both under- and overconfident examples with the models originalpredicted label preserved are exclusively used to train the model. Our proposed CS defence is a post-process scaling technique, based on the assumption that an effectiveclassifier often has a high level of accuracy and confidence, so calibration attacks typically cause the mostharm by lowering the confidence scores. CS hence aims to scale low confidence scores to high values tomitigate the damage of miscalibration. Specifically, the range of confidence is split into BM equally sizedbins. Datapoints in each bin bm {1, ..., BM} are mapped to a new bin that has higher confidences ina more compressed range. For a datapoint xi with the output-layer logits {r1, ..., rK} and the probabilitypk(xi) for the predicated class k, a temperature T is found to obtain a newly predicted probability pnew =arg maxi(exp(ri/T )",
  "range(bm) range(bm), where minconf(.) is the": "minimum confidence level of a bin, and range(.) represents the range of the confidence (i.e., the maximumlevel of confidence of that bin minus the minimum). b is the bin that the original b is mapped to using thecorresponding T. We will demonstrate that even with defence, calibration attacks are still highly effective.",
  "Discussion on the Importance of Remaining Well Calibrated Under the Attacks": "In addition to the discussion of the technical details of calibration attacks for the victim models and de-fences, it is important to reemphasize why maintaining good calibration under such attacks is critical.Well-calibrated models are a crucial component for trustworthy complex systems. In terms of specific real-world applications, skewed confidence scores or uncertainties have been shown to have dramatic impacts ondownstream tasks and human decision-making, and previous studies such as those of Penso et al. (2024);Jiang et al. (2012); Kompa et al. (2021) have shown the effect of miscalibration on down-stream author-ity mechanisms in the context of areas like medical imaging. Specific use-cases can also be found in childmaltreatment hotline screening models De-Arteaga et al. (2020), legal domain Bernsohn et al. (2024), andanomalous behaviour detectors Shashanka et al. (2016) where heavily miscalibrated outputs present a failurepoint to methods. Moreover, prior works (Zhang et al., 2020; Rechkemmer & Yin, 2022) have examined the role of skewedconfidence affecting the level of trust humans have in a system, which influences applications where humansand AI decision-making occurs symbiotically. These works demonstrate that when models produce highconfidence scores, users are more prone to rely on the AI for decision-making. Dhuliawala et al. (2023) conductuser studies that show that confidently incorrect predictions lead to over reliance and heavily undermine usertrust in human-AI collaboration settings, requiring only a few incorrect instances with inaccurate confidenceestimates to damage user trust and performance. Please refer to Appendix B for detailed discussion ondownstream impacts.",
  "Experimental Setup": "Implementation Details. As discussed earlier, calibration attacks can be built on different adversarialattack frameworks. In our implementation, we use Square Attack (SA) (Andriushchenko et al., 2020), whichis one of the most popular black-box approaches and is highly effective. SA achieves state-of-the-art (SOTA)performance in terms of query efficiency and success rate, even outperforming some white-box methods. Ourwhite-box calibration attacks are based on the popular Projected Gradient Descent (PGD) framework (Madryet al., 2018). The implementation details such as hyperparameters are discussed in Appendix D. Models. We include both convolutional (ResNet (He et al., 2016)) and non-convolutional attention-basedmodels (Vision Transformer (ViT) (Dosovitskiy et al., 2021)) in our study. Details can be found in Appen-dices D and E. Datasets. We performed a comprehensive study on CIFAR-100 (Krizhevsky & Hinton, 2009) and Caltech-101 (Fei-Fei et al., 2004).We also included the German Traffic Sign Recognition Benchmark (GT-SRB) (Houben et al., 2013) for its direct implication for safety. Metrics. Two calibration metrics are used in our experiments: the standard Expected Calibration Error(ECE) (Pakdaman Naeini et al., 2015) and the recent Kolmogorov-Smirnov Calibration Error (KS error)(Gupta et al., 2021). In addition, we evaluate attacks efficiency using the average and median number ofqueries for the attack to complete (Andriushchenko et al., 2020). Average confidence of predictions is alsoleveraged to judge the degree that the confidence scores are affected. (See Appendix D for details.) Same as in previous works (Guo et al., 2017; Kumar et al., 2018; Minderer et al., 2021), we focus on intrinsicevaluation metrics in this research, which isolate our analysis from downstream tasks to abstain from task-specific conclusions, particularly considering the downstream tasks here are very diverse. In addition, theharm from miscalibration in downstream applications has been well studied in prior work, as detailed inAppendix B.",
  "Avg #qMed. #qECEKSAvg. Conf": "CIFAR-100(Accuracy: 0.9350.002)Pre-atk--.064.006.054.005.882.004UCA118.52.462.03.1.572.007.553.004.404.003OCA524.788.7510.5114.3.043.007.043.006.974.001MMA104.87.562.74.7.616.003.564.000.431.001RCA106.43.070.31.5.549.002.505.003.471.007 Caltech-101(Accuracy: 0.9610.024)Pre-atk--.137.059.136.060.825.083UCA325.516.7273.723.7.426.044.426.044.536.068OCA52.140.91.00.0.081.042.079.040.881.067MMA150.712.1269.725.1.415.036.414.034.551.058RCA129.017.3315.017.4.364.016.364.016.598.040 GTSRB(Accuracy: 0.9470.006)Pre-atk--.040.005.026.017.922.024UCA169.815.088.36.7.459.015.452.019.498.026OCA94.945.93.74.6.029.003.030.004.976.011MMA137.14.388.36.7.519.020.480.020.509.024RCA129.57.497.29.9.454.012.432.016.538.019 depicts the overall performance ofblack-box attacks under the l norm.Ad-ditional results of l2 attacks are included in in Appendix H.10.The experimentsshow that the attacks are highly effective. UCA,MMA and RCA can bring ECE and KS to veryhigh values.OCA can successfully raise aver-age confidences (the last column in )to extremely high levels (in many cases, closeto 100%), but given the original high accu-racy of the victim models, increasing confidencelevels will not have a drastic effect on cali-bration error.(OCA will be more harmful onless accurate models.) For the MMA, the theo-retical highest levels of miscalibration are notreached due to the limited number of itera-tions that we run the attacks. Regarding dif-ferent architectures, the attention-based ViTmodels are seen to be more miscalibrated com-pared to the convolution-based ResNet mod-els. The white-box results are included in Ap-pendix H.1.While ResNet and ViT are themost representative frameworks for convolutionand attention-based models respectively, in Ap-pendix H.8 we include additional experimentson the Swin Transformer (Liu et al., 2021),which is a famous extension of ViT. The resultsshow similar trends as those presented in Ta-ble 1. Again, in the introduction sec-tion demonstrates the attack effects using cali-bration diagrams, visually showing the severityof miscalibration. The general trend of the l2 calibration attacks(Appendix H.10) is similar to that of the lattacks, but the latter are found to be more effective in generating more significant miscalibration. Hence,in the remainder of the paper, we focus on the l attacks unless otherwise specified. Overall, calibration attacks are generating severe miscalibration, which, compared to the pre-attack values,can increase ECE and KS by over 10 times in many cases. Calibration attacks are very effective withoutchanging the prediction accuracy, which could raise serious concerns for any down-stream applications relyingon confidence.",
  "Detection Difficulty Analysis": "This section shows calibration attacks are also difficult to detect. To investigate this, we run the popu-lar adversarial attack detection methods on the attacks against ResNet-50: Local Intrinsic Dimensionality(LID) (Ma et al., 2018), Mahalanobis Distance (MD) (Lee et al., 2018), and SpectralDefense (Harder et al.,2021). The details behind the settings for each detection method can be found in Appendix G. depicts the main results of the effectiveness of the detectors in terms of Area Under the Curve (AUC) andDetection Accuracy, under different types of calibration attacks and using both the white-box and black-boxapproaches. We can see that there are consistent decreases in detection performances, particularly in SA,",
  "Attack Iterations": "0.1 0.2 0.3 0.4 0.5 0.6 ECE : The contrast between the effects on accuracy andECE between the original version of the Square Attackalgorithm and the maximum variation of the calibrationattack algorithm at 1000 attack iterations. (Top) ResNet-50 results. (Bottom) ViT results. Iterations. Expanding on the results in the right-most figure in , the number of iterations ofMMA is varied from 100, 500, 1000, to 5000, whilstattack both ViT and ResNet models trained andtested on CIFAR-100 with the same settings as inAppendix D. We note how the ECE begins to sat-urate at close to 500 iterations, after which the ben-efits of running the attack longer are minor, thougheven at 100 iterations the ResNet model becomesheavily miscalibrated despite the standard valueof 0.05 being used. In our tests showing the effec-tiveness of the original SA versus its calibration at-tack version, seen in , we specifically com-pare over accuracy and ECE between the maximummiscalibration attack and the regular untargeted SAacross the four aforementioned iteration values forboth ResNet and ViT on CIFAR-100. As expected,SA greatly reduces the accuracy even with a smallnumber of iterations. Nevertheless, in terms of ECE,the calibration attacks consistently produce higheramounts of miscalibration compared to the originalSA algorithm across the different iteration amounts.",
  "Insights on Key Aspects of Attacks": "We analyze key aspects of calibration attacks: attack directions, noise bounds, and attack iterations underthe l-based attacks. More details can be found in Appendix H, including attack effectiveness on data withvarying imbalance ratios. Comparison of Efficiency of Underconfidence vs.Overconfidence Attacks. The two directionsof calibration attacks, underconfidence or overconfidence, is a basic building block for constructing the fourforms of calibration attacks. We perform further studies to understand which direction is most efficient.To the end, we identify all data points in the test set that are around certain predefined base confidencelevels. In this study, we choose two base confidences: 80% and 90%. All the test cases that have confidenceswithin 1% around these two base confidences are included in this experiment. We attack these examples bymaking the victim models produce either a 10% increase or decrease in confidence. In we can seea consistent pattern for both base confidence levels, it takes notably fewer queries to create undercon-fidence than overconfidence adversarial examples. The former attack is also more effective in affecting theaverage confidences. This property could be further utilized to design calibration attacks (e.g., under thecircumstances where computing resources or attack latency are concerned.) Perturbation Noise Levels. We study the effect of perturbation noise levels and how low the value couldbe in order to construct notable harm. As shown in the left three sub-figures in , under differentsetups, the attacks are highly successful even at a low level of noise (e.g., = 0.05 or even lower). We canalso see the rise in ECE is sharp when increases, and it plateaus quickly.",
  "GTSRB90% -10% UCA184.027.531.06.512.71.976.10.990% +10% OCA184.027.5235.369.5180.092.096.50.280% -10% UCA79.028.616.16.77.72.367.30.280% +10% OCA79.028.6140.163.241.743.789.60.8": "GradCAM Visualization.In weshow the coarse localization maps producedwith GradCAM (Selvaraju et al., 2017), whichhighlights the most important regions that amodel relies on to make prediction using thegradients from different layers of a network. Weapply GradCAM to our ResNet-50 models thatare fine-tuned and tested on Caltech-101, basedon the standard attack settings discussed inSection D. We choose images where the attacksled to large change in predicted confidence (atleast 10%). shows several represen-tative images. We can see that the coarse lo-calization maps have minimal to no noticeablechanges after the adversarial images are pro-duced, especially in the case of the overcon-fidence attacked images. This analysis showsthat it could be difficult to identify the attacksbased on gradient visualization methods. t-SNE.AppendixH.4providesdetailedt-SNE visualization on the effect of differentforms of calibration attacks. The visualizationshows that the overconfidence attack causes therepresentations for different classes to be splitapart as much as possible, while the undercon-fidence attack causes a more jumbled represen-tation with most data points falling closely tothe decision boundary. The visualization shows that the attacks achieve their intended goals.",
  "Confidence Certification": "We follow the method in Kumar et al. (2020) to compare the lower bound of expected confidence of asmoothed classifier between base images and their under- and overconfidence attacked counterparts. We findthat the lower bound is not significantly different between adversarially attacked images and their originals,though the accuracy of the smoothed classifier is affected. This means that methods for determining certifiedlower bounds are not strongly affected by calibration attacked samples (similar to conventional adversarialsamples), but limitations in efficiency and accuracy of these methods means that establishing defence methodagainst such attacks is still paramount. A detailed discussion is in Appendix H.7.",
  "Pre-atk.028.005.015.008.920.002UCA SQ Ensemble.091.010.086.008.708.012UCA PGD Ensemble.048.011.014.006.637.030": "Determining the transferability of calibra-tion attacks across different model architec-tures is of interest since although one caneffectively optimize attacks against a singlemodel, using an ensemble model could po-tentially be a form of defence if transfer-ability is limited. We investigate how ourattacks transfer to different victim models.Specifically in our experiments, the attacksare constructed on CIFAR-100 images withregular settings based on ViT but used on(i) ResNet and (ii) ensemble victim models combining prediction of both ResNet-50 and ViT ().We focus on the transfer of the UCA attack, given that OCA has little room to skew confidence due to thealready high accuracy of victim models. The attacks show transferability, although in general they becomeless effective when applied on different architectures. This is expected because the attacks are constructedwithout considering the target victim models properties. Most notably, the simple ensemble of two typicalvictim models (ViT and ResNet) is still greatly subject to the attacks (the output distributions of ResNetand ViT are averaged to get the ensemble), so using ensembles as a defence does not fully protect againstcalibration attacks. To further explore transferability, attack methods need to be explicitly redesigned (Xieet al., 2018; Wang et al., 2021), which could be further studied in the future.",
  "ViT-large (Acc: .9360.001)Pre--.037.009.022.012.921.021Post122.12.076.33.2.531.012.508.020.450.024": "To help understand the influence of varyingmodel sizes on attack performance, we per-form a study on models of different sizes overdifferent architectures across four attack vari-ants.Specifically, for ResNet, we compareResNet-18, ResNet-50, and ResNet-152.ForViT, we included Base (vit-base-patch16-224-in21k) and Large models (vit-large-patch16-224-in21k). The model training closely followsthe settings detailed in Appendix D.1. We fine-tune the models for each variant and averagethe results. We ran attacks with the same set-tings used to produce Tables 1 and 3.",
  "We compare a wide range of recalibration and de-fence methods under the setup of the maximummiscalibration attacks, which, as shown above, areamong the most effective calibration attacks": "Specifically, for post-calibration methods, we in-clude Temperature Scaling (TS) (Guo et al., 2017),Multi-domain Temperature Scaling (MD-TS) (Yuet al., 2022), and calibration with splines (Splines)(Gupta et al., 2021). For training-based regular-ization methods we include two effective models,DCA (Liang et al., 2020) and SAM (Foret et al.,2021).Regarding adversarial defence methods,we test the top-3 SOTA models under the l at-tack for CIFAR-100, using WideResNet on the Ro-bustBench leaderboard (Croce et al., 2021), whichare only available for CIFAR-10 and CIFAR-100,hence we run over CIFAR-100 to compare withour previous baselines. We further include a re-cent post-process defence called Adversarial At-tack Against Attackers (AAA) (Chen et al., 2022)in addition to the the most common defences inthe form of PGD-based adversarial training (AT)(Madry et al., 2018).We also included the twodefences proposed for calibrations attacks: CAATand CS, which are introduced in .3. (Theexperiment setup is described in Appendix D, andthe detailed description of these baselines are inAppendix E.2). Results and Analyses. shows the exper-iment results of query efficiency, accuracy, and theECE and KS errors, before (PrECE and PrKS)and after the attacks (PsECE and PrKS), usingdifferent recalibration and defence models.Themethods with the best performances in terms ofpost-attack ECE (PsECE) and KS (PsKS) aremarked in bold, and the second best are under-lined. We can see that CS is overall the strongest meth-ods at maintaining low calibration errors on post-attack datapoints. It showed the best post-attackcalibration performance in five out of six setups onPsECE, and ranked among top 2 in all the othersetups. The other top calibration performances aredistributed among AAA, Splines, and AT. Know-",
  "ing the property of calibration attacks and organizing defences accordingly is helpful to ensure better defenceresults": "Overall, from the ECE and KS scores, we can see that there are still significant limitations on recalibrationand defences for calibration attacks, which invites more future research. Simple and widely used calibrationmodels like TS are effective on clean data prior to the attacks, but they offer very little benefit post-attack.Training-based models like DCA and SAM also tend to bring few benefits after being attacked thepost-attack ECE and KS errors are not substantially different compared to the vanilla models. Lastly, wecan conclude that although the defence methods from the leaderboard are generally the most adversariallyresistant, their inherent high levels of miscalibration even before the attacks render them unsuitable forcalibration-sensitive tasks.",
  "Conclusions": "We highlight and perform a comprehensive and dedicated study on calibration attacks, which aim to trapvictim models into being heavily miscalibrated, hence endangering the trustworthiness of the models andany follow-up decision-making processes based on confidences. We propose four forms of calibration attacksand demonstrate their severity from different perspectives. We also show calibration attacks are difficult todetect compared to standard attacks. An investigation was conducted to study the effectiveness of a widerange of adversarial defences and calibration methods, including the defences that are specifically designedfor calibration attacks. From the ECE and KS scores, we can see that there are still limitations on theserecalibration and defences in handling calibration attacks. We hope this paper helps attract more attentionto the attacks against confidence and hence mitigate their potential harm. We provides detailed analyses tohelp understand the characteristics of the attacks for future work.",
  "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.Square Attack: aquery-efficient black-box adversarial attack via random search. ECCV, 2020": "Stavros Antifakos, Nicky Kern, Bernt Schiele, and Adrian Schwaninger. Towards improving trust in context-aware systems by displaying system confidence. In Proceedings of the 7th International Conference onHuman Computer Interaction with Mobile Devices & Services, MobileHCI 05, pp. 914, New York, NY,USA, 2005. Association for Computing Machinery. ISBN 1595930892. doi: 10.1145/1085777.1085780.",
  "Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep Learning for AI. Commun. ACM, 64(7):5865,June 2021. ISSN 0001-0782. doi: 10.1145/3448250": "Dor Bernsohn, Gil Semo, Yaron Vazana, Gila Hayat, Ben Hagag, Joel Niklaus, Rohit Saha, and KyrylTruskovskyi.LegalLens: Leveraging LLMs for legal violation identification in unstructured text.InYvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapterof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 21292145, St. Julians,Malta, March 2024. Association for Computational Linguistics.",
  "Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. In Symposiumon Security and Privacy. IEEE, 2017. doi: 10.1109/sp.2017.49": "Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. ZOO: Zeroth Order OptimizationBased Black-Box Attacks to Deep Neural Networks without Training Substitute Models. In Workshop onArtificial Intelligence and Security, pp. 1526, 2017. ISBN 9781450352024. Sizhe Chen, Zhehao Huang, Qinghua Tao, Yingwen Wu, Cihang Xie, and Xiaolin Huang. Adversarial attackon attackers: Post-process to mitigate black-box score-based query attacks. In Alice H. Oh, Alekh Agarwal,Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustnessbenchmark. In NeurIPS Datasets and Benchmarks Track, 2021. Maria De-Arteaga, Riccardo Fogliato, and Alexandra Chouldechova. A case for humans-in-the-loop: De-cisions in the presence of erroneous algorithmic scores. In Proceedings of the 2020 CHI Conference onHuman Factors in Computing Systems, CHI 20, pp. 112, New York, NY, USA, 2020. Association forComputing Machinery. ISBN 9781450367080. doi: 10.1145/3313831.3376638.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In CVPR, 2009": "Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C.Lipton, and Animashree Anandkumar. Stochastic activation pruning for robust adversarial defense. InICLR, 2018. Shehzaad Dhuliawala, Vilm Zouhar, Mennatallah El-Assady, and Mrinmaya Sachan. A diachronic perspec-tive on user trust in AI under uncertainty. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceed-ings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 55675580, Singa-pore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.339.",
  "Gongbo Liang, Yu Zhang, Xiaoqin Wang, and Nathan Jacobs. Improved Trainable Calibration Method forNeural Networks on Medical Imaging Classification. In BMVC, 2020": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), 2021. Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck, Michael E.Houle, Dawn Song, and James Bailey. Characterizing adversarial subspaces using local intrinsic dimen-sionality. In ICLR, 2018.",
  "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. TowardsDeep Learning Models Resistant to Adversarial Attacks. In ICLR, 2018": "Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Ann Hubis, Xiaohua Zhai, Neil Houlsby, DustinTran, and Mario Lucic. Revisiting the Calibration of Modern Neural Networks. ArXiv, abs/2106.07998,2021. Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran.Measuringcalibration in deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR) Workshops, June 2019. Yaniv Ovadia, Emily Fertig, J. Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon, BalajiLakshminarayanan, and Jasper Snoek. Can you trust your models uncertainty? evaluating predictiveuncertainty under dataset shift. In NeurIPS, 2019.",
  "Jonas Rauber, Wieland Brendel, and Matthias Bethge.Foolbox: A Python toolbox to benchmark therobustness of machine learning models. In Reliable Machine Learning in the Wild Workshop, ICML, 2017": "Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox Native: Fast adver-sarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX.Journal of Open Source Software, 5(53):2607, 2020. doi: 10.21105/joss.02607. Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and TimothyMann.Data augmentation can improve robustness.In A. Beygelzimer, Y. Dauphin, P. Liang, andJ. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. Amy Rechkemmer and Ming Yin.When confidence meets accuracy: Exploring the effects of multipleperformance indicators on trust in machine learning models. In Proceedings of the 2022 CHI Conferenceon Human Factors in Computing Systems, CHI 22, New York, NY, USA, 2022. Association for ComputingMachinery. ISBN 9781450391573. doi: 10.1145/3491102.3501967.",
  "Kui Ren, Tianhang Zheng, Zhan Qin, and Xue Liu. Adversarial Attacks and Defenses in Deep Learning.Engineering, 6(3):346360, 2020. ISSN 2095-8099. doi:": "Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. InICCV, 2017. doi: 10.1109/ICCV.2017.74. Madhu Shashanka, Min-Yi Shen, and Jisheng Wang.User and entity behavior analytics for enterprisesecurity. In 2016 IEEE International Conference on Big Data (Big Data), pp. 18671874, 2016. doi:10.1109/BigData.2016.7840805.",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizing Data using t-SNE. JMLR, 9(86):25792605, 2008": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems,volume 30. Curran Associates, Inc., 2017.",
  "Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, and Kun He. Boosting adversarial transferabilitythrough enhanced momentum. In British Machine Vision Conference, 2021": "Chaowei Xiao, Zhongzhu Chen, Kun Jin, Jiongxiao Wang, Weili Nie, Mingyan Liu, Anima Anandkumar,Bo Li, and Dawn Song. Densepure: Understanding diffusion models towards adversarial robustness. InICLR, 2023. Cihang Xie, Zhishuai Zhang, Jianyu Wang, Yuyin Zhou, Zhou Ren, and Alan Loddon Yuille. Improvingtransferability of adversarial examples with input diversity. 2019 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 27252734, 2018.",
  "Bianca Zadrozny and Charles Elkan.Transforming classifier scores into accurate multiclass probabilityestimates. In KDD, pp. 694699, 2002": "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Richard C. Wilson, Edwin R. Hancock,and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference 2016, BMVC 2016,York, UK, September 19-22, 2016. BMVA Press, 2016. Huimin Zeng, Zhenrui Yue, Yang Zhang, Lanyu Shang, and Dong Wang. Manipulating out-domain un-certainty estimation in deep neural networks via targeted clean-label poisoning. In CIKM, 2023. ISBN9798400701245. doi: 10.1145/3583780.3614957.",
  "ADetailed Summary of Related Work": "In the field of calibration, a great deal of current research is devoted to the creation of new calibrationmethods that can be applied to create better calibrated models while possessing as minimum overhead inapplying them as possible. Methods are generally divided into either post-calibration or training-based.Post-calibration methods can be applied directly to the predictions of fully trained models at test time, andmethods of this class include temperature scaling (Guo et al., 2017). More traditional methods of this typeinclude Platt scaling (Platt, 1999), isotonic regression (Zadrozny & Elkan, 2002), and histogram binning(Zadrozny & Elkan, 2001). All of these three methods are originally formulated for binary classificationsettings, and work by creating a function that maps predicted probabilities based on their values morein tune with the models level of performance.Although they are easy to apply, they often come withthe limitation of needing a large degree of validation data to tune, especially with isotonic regression, andperformance can struggle when applied to more out of distribution data. The second class of methods are training-based methods, which typically add a bias during training toensure that a model learns to become better calibrated. Often times these methods help by acting as aform of regularization that can punish high levels of overconfidence late into training. In computer vision,Mixup (Zhang et al., 2018) is a commonly used method of this type that serves as an effective regularizerby convexly combining random pairs of images and their labels and helps calibration primarily due to theuse of soft, interpolated labels (Thulasidasan et al., 2019). Other methods work by adding a penalty to theloss function, like in the case of MMCE, an RKHS kernel-based measure of calibration that is added as apenalty on top of the regular loss during training so that both are optimized jointly (Kumar et al., 2018).Similarly, Tomani & Buettner (2021) create a new loss term called adversarial calibration loss that directlyminimizes calibration error using adversarial examples. Given the effectiveness of many of these methods inregular testing scenarios, we desire to illustrate how well a diverse range of these methods can cope againstattacks targeting model calibration and whether they possess limitations that require them to be overhauledto deal with an attack scenario. With respect to adversarial attacks, attacks in this field are wide ranging. Well known white-box attacksinclude the basic Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015). This method works byfinding adjustments to the input data that maximizes the loss function, and uses the backpropagated gradi-ents to produce the adversarial examples. Projected gradient descent (PGD) (Madry et al., 2018) is populariterative-based method that similarly uses gradient information, and has been shown to be a universal first-order adversary, and thus is the strongest form of attacks making using of gradient and loss information. Inthe black-box space of attacks, ZOO (Chen et al., 2017) is an example of a popular score-based attack thatuses zeroth order stochastic coordinate descent to attack the model, and avoids training a substitute model.The authors make use of attack-space dimension reduction, hierarchical attacks and importance sampling tomake the attack more query efficient, which is required as black-box attacks generally need a lot of queriesto run compared to white-box methods. A broad range of defences against adversarial attacks have been developed, but among the most popularand effective is adversarial training (Goodfellow et al., 2015), where during training the loss is minimizedover one of or both clean and generated adversarial examples. Adversarial training however greatly increasestraining time due to the need to fabricate adversarial examples for every batch. Gradient masking (Carlini& Wagner, 2017) is a simple defence based on obfuscating gradients so that attacks cannot make use ofgradient information to create adversarial examples, although it can easily be circumvented in many casesfor white-box models (Athalye et al., 2018), and black box attacks do not need gradient information inthe first place. It is by and large difficult for adversarial defences to keep pace with the broad range ofattacks and to be provably robust against a large number of them. Although the main topic of this work iscalibration, we do focus on modelling adversarial defences and their effectiveness against these attacks.",
  "BMiscalibration Effects on Downstream Tasks": "In addition to the discussion of the technical details of calibration attacks for the victim models and de-fences, it is important to reemphasize why maintaining good calibration under such attacks is critical.Well-calibrated models are a crucial component for trustworthy complex systems. As an example, in real-lifedeployment, proper confidence scores are essential for determining the instances that need further exami-nation by authority mechanisms such as an expert or a committee of them. Underconfidence attacks couldcause additional strain on an authority system or any downstream processes, creating a risk of significantlyslowed-down decision-making due to an increased load of cases to be examined. As another example, byattacking misclassified datapoints with an overconfidence approach, test cases may be erroneously missed bydownstream systems such as braking in an autonomous vehicle before a stop sign. In terms of specific real-world applications, skewed confidence scores or uncertainties have been shown tohave dramatic impacts on downstream tasks and human decision-making, which many previous works havehighlighted. Previous studies such as those of Penso et al. (2024); Jiang et al. (2012); Kompa et al. (2021)have shown the effect of miscalibration on down-stream authority mechanisms. Penso et al. (2024) discusshow medical imaging, if skipping expert review for the confident but incorrect model predictions, can havedisastrous consequences, stressing the importance of being well-calibrated on incorrect predictions. Kompaet al. (2021) advocate for medical AI models being able to abstain from providing a diagnosis when there isa significant uncertainty for a patient, and to defer to additional human expertise to make a more informeddiagnosis. This relies on a model that can give well-calibrated estimates. Moreover, prior works have examined the role of skewed confidence affecting the level of trust humanshave in a system, which influences applications where humans and AI decision-making occurs symbioticallyZhang et al. (2020); Rechkemmer & Yin (2022). Antifakos et al. (2005), through a study on context-awaremobile phones, show how providing explicit confidence improves user trust in a system. Zhang et al. (2020)demonstrate when a model produces high confidence scores, users are more prone to rely on the AI fordecision-making. These factors show why miscalibrated outputs can have great influence on downstreamprocessing and authority mechanisms. As more specific examples in the healthcare domain, chest abnormality detection Dyer et al. (2021) tag radio-graphs with high-confidence normality to help radiologists make decisions. In child maltreatment hotlines,screening models De-Arteaga et al. (2020) are deployed where confidence is provided to suggest the likelihoodthat a child on the referral will experience adverse welfare-related outcomes. Call workers use confidence,along with other information, to make their decisions. The study discusses how the tool could be glitchingout by providing incorrect confidence, compromising operability. In the legal domain, LegalLens Bernsohnet al. (2024) detects legal violations. Entities with high confidence are further processed with downstreammodels. In enterprise security, anomalous behaviour detectors Shashanka et al. (2016) use confidence to sendalerts about likely malicious activity. Dhuliawala et al. (2023) conduct user studies that show that confidently incorrect predictions heavily under-mine user trust in human-AI collaboration settings, requiring only a few incorrect instances with inaccurateconfidence estimates to damage user trust and performance, creating long-term effects with recovery beingvery difficult once humans have been exposed to miscalibrated confidence estimates. They observe that bothconfidently incorrect stimuli and unconfidently correct stimuli reduce trust, with the former being more sig-nificant, which justifies our formulation of MMA as having a high potential for creating mistrust by optimizingthe creation of both of these stimuli.",
  "Proof of Proposition 3.1": "Proof. Let a classifier have non-zero accuracy. We cannot expect to reach the error of 100% since pk = 0cannot be the case (for the top predicted class) nor can P(yi = y(xi)) = 0 be true for all yi. However,to achieve the highest calibration error on a set of data points in this scenario, one can first isolate themisclassified data points and if the classifier is made to output confidence scores of 100% on all of them, using",
  "DDetails of Experimental Setup": "Metrics. To assess the degree of calibration error caused by each attack, we use two metrics, the popularbinning-based Expected Calibration Error (ECE) (Pakdaman Naeini et al., 2015), and KS error (Guptaet al., 2021), which are formulated in detail in Section F. Datasets. The datasets we use in our study are CIFAR-100, Caltech-101, and the German Traffic SignRecognition Benchmark (GTSRB). CIFAR-100 and Caltech-101 are both popular image recognition bench-mark datasets, containing various objects divided into 100 classes and 101 classes respectively. Given theimportance of calibration in safety critical applications, we include a common use case of autonomous drivingwith the GTSRB dataset, which consists of images of traffic signs divided 43 classes. CIFAR-100 has 50,000images for training, and 10,000 for testing. Caltech-101 totals around 9000 images. GTSRB is split into39,209 training images and 12,630 test images Models. ResNet-50 (He et al., 2016) is the primary model we train and test on due to it being a standardmodel for image classification.Non-convolutional attention-based networks have recently attained greatresults on image classification tasks, so we also experiment with the popular Vision Transformer (ViT)architecture (Dosovitskiy et al., 2021). Both of these models are the versions with weights pretrained onImageNet (Deng et al., 2009). We use the VIT_B_16 variant of ViT, and the pretraining dataset usedfor each model is ImageNet_1K for ResNet and ImageNet_21K for ViT, and are fine-tuned on the targetdatasets. Pretrained models are advantageous to study given they can increase performance over trainingfrom randomly initialized weights and present a more practical use-case. The specific details behind ourtraining procedures and our various model hyperparameters can be seen in Section E. Attack Settings. Regarding the SA version of the attacks, for the l and l2 norm attacks we use thedefault SA settings for and p, which are = 0.05 and p = 0.05 for l and = 5.0 and p = 0.1 for l2. Forour primary results we run the attacks on a representative 500 test cases from the test set of each dataset.Each attack is ran for 1000 iterations, far less than the default 10,000 in Andriushchenko et al. (2020), butsince there is no need to change the label, less iterations are required, bolstering the use-case and threat forthis form of attack.",
  "E.1General Settings": "As mentioned previously, for our general attack implementation we use SA, which works by using a random-ized search scheme to find localized square-shaped perturbation at random positions which are sampled insuch a way as to be situated approximately at the boundary of the feasible set. We still use the originalsampling distributions, however we remove the initialization (initial perturbation) for each attack since it isprone to changing the predicted labels. Naturally, we use the untargeted versions of the attacks, whereby theperturbations lead to increases in the probabilites of random non-predicted classes for the underconfidenceattack, since we only care about the probability of the top predicted class. The details of the training procedure for each of the models and datasets is as follows: For CIFAR-100and GTSRB, we use the predefined training and test sets for both but use 10% of the training datafor validation purposes. For Caltech-101, which comes without predetermined splits, we use an 80:10:10train/validation/test split. For all of the datasets, we resize all images to be 224 by 224. We also normalizeall of the data based on the ImageNet channel means and standard deviations. We apply basic data aug-mentation during training in the form of random cropping and random horizontal flips to improve modelgeneralizability. The hyperparameters we used for training the ResNet-50 models include: a batch size of128, with a CosineAnnealingLR scheduler, 0.9 momentum, 5e-4 weight decay, and a stochastic gradientdescent (SGD) optimizer. For ViT, the settings are the same, except we also use gradient clipping withthe max norm set to 1.0. We conduct basic grid search hyperparameter tuning over a few values for thelearning rate (0.1,0.01,0.005,0.001) and training duration (in terms of epochs). Generally, we found that alearning rate of 0.01 worked best for both types of models. The training times vary for each dataset andmodel. For the ResNet-50 models we trained for 15 epochs on CIFAR-100, 10 epochs on Caltech-101, and7 epochs on GTSRB. Likewise for ViT, we trained for 10 epochs on CIFAR-100, 15 epochs on Caltech-101,and 5 epochs on GTSRB. The results reported in Sections 3 and 5 are shown for models on the epoch atwhich they attained the best accuracy on the validation set. All of the training occurred on 24 GB NvidiaRTX-3090 and RTX Titan GPUs. Finally, we use 15 bins to calculate the ECE.",
  "In this section, we describe each of the defences we used in , and the settings we use to train them(if applicable)": "Temperature Scaling (TS) (Guo et al., 2017). TS is a post-process recalibration technique applied tothe predictions of an already trained model that reduces the amount of high confidence predictions withoutaffecting accuracy. TS works by re-scaling the logits after the final layer of the neural network to have ahigher entropy by dividing them by a temperature parameter T, that is tuned by minimizing negative loglikelihood (NLL) loss on the validation set. Temperature scaling only works well when the training and test",
  "distributions are similar (Kumar et al., 2019), but by reducing overconfidence it may have an advantageagainst overconfidence attacks": "Multi-domain Temperature Scaling (MD-TS) (Yu et al., 2022) MD-TS is a method based on TSbut is designed to be more robust in situations when data comes from multiple domains, as in this case withimages corrupted using different types of calibration attacks. It modifies the original TS method by firstfinding the ideal temperature across each domain, then training a linear regression classifier using the featureembeddings of each datapoint and the corresponding ideal temperatures based on the respective domain,yielding a classifier that can dynamically calculate an ideal temperature for each instance at test time. Wemodify this domain for our task of defence by creating three different domains for the base images and theirunderconfidence attacked and overconfidence attacked counterparts. We select 500 validation instances andfind the temperature for each domain and conduct the rest of the method as in its original incarnation. Thefeature embeddings are as before, using the penultimate layer outputs of model before the classification layer.We experiment with converting the feature embeddings to Fourier domain using the fast Fourier transformbefore feeding them to the classifier, similar to the principle behind the detection method in Harder et al.(2021) to make it easier to identify adversarially attacked datapoints, though we find that the conversionbrings little benefit over the base variation of the method. Calibration of Neural Networks using Splines (Spline) (Gupta et al., 2021). Spline is another post-process recalibration technique that uses a recalibration function to map existing neural network confidencescores to better calibrated versions by fitting a spline function that approximates the empirical cumulativedistribution. It is lightweight, and often performs better than TS. Difference between confidence and accuracy (DCA) (Liang et al., 2020).DCA is a training-based calibration method that adds an auxiliary loss term to the cross-entropy loss during training thatpenalizes any difference between the mean confidence and accuracy within a single batch, inducing a modelto not produce confidence scores that are miscalibrated. We set the weight of DCA to 10 based on therecommendation by Liang et al. (2020). Training settings are kept the same as described in the generalsettings. Sharpness Aware Minimization (SAM) (Foret et al., 2021). SAM is a technique that improves modelgeneralizability by simultaneously minimizing loss value and loss sharpness. It finds parameters that lie inneighbourhoods having uniformly low loss by computing the regularized sharpness-aware gradient. Themotivation behind using this technique as a defence is that models with parameters that lie in uniformly lowloss areas may present additional difficulty in creating adversarial examples, and may be more regularizedas a whole. We use a neighbourhood size = 0.05. We kept the training settings the same as we describedin the general settings. RobustBench (Croce et al., 2021). To understand how state-of-the-art adversarial defences work againstour attack, we take the top 3 performing (in terms of adversarial robustness) WideResNet (Zagoruyko &Komodakis, 2016) defences on the popular RobustBench defence model benchmark for CIFAR-100 underthe l = 8/255 attack model. We only choose the WideResNet models given their closer similarity to theprimary model we study in this work, ResNet-50. The defences we choose are those of Gowal et al. (2020)(ranked first), Rebuffi et al. (2021) (ranked third) and Pang et al. (2022) (ranked fifth). These defences use acombination of adversarial training and ensembling to produce models that are robust against a wide rangeof conventional adversarial attacks. In addition, they use different techniques, like combining larger models,using Swish/SiLU activations and model weight averaging, and data augmentation to significantly improverobust accuracy. Adversarial Training (AT). AT is among the most common and effective defences against a wide rangeof adversarial attacks where models are trained on adversarially attacked images. We implement our versionsimilar to Madry et al. (2018) and Xie et al. (2019), where we run PGD-based adversarial training, givenhow this form of defence has been shown to be effective across a wide range of attacks due to PGD beingclose to a universal first-order l attack. We train exclusively on images attacked with an n-step l PGDattack each batch, with the number of steps chosen depending on the model and dataset. Since we alreadytest RobustBench models that often make use of AT with a large amount of steps, we specifically tune ourAT models to have less steps to compromise less on accuracy and miscalibration. We wish to see whether",
  "FCalibration Metric Formulation": "Here we formulate the two calibration metrics that we use in our experiments. As Equation 2 is an idealizedrepresentation of miscalibration that is intractable, approximations have been developed which are groupedinto the more common binning-based metrics, and non-binning based metrics. Expected calibration error (Pakdaman Naeini et al., 2015) is the most widely used calibration metric inresearch. It is a binning-based metric where confidence scores on the predicted classes are binned into Mnumber evenly spaced bins, which is a hyperparameter that must be carefully chosen. In each bin, thedifference between the average confidence score and accuracy of all data points within the bin is calculated,representing the bin-wise calibration error. Afterwards, the weighted sum over the error in each bin consti-tutes the expectation of the calibration error of the model. The equation for ECE is as follows given Bm arethe data points in the mth bin, and nm is the number of data points in that bin.",
  "nmN |acc(Bm) conf(Bm)|.(8)": "ECE can underestimate the levels of miscalibration due to being sensitive to the number of bins (Ovadia et al.,2019) and by having underconfident and overconfident data points overlapping in one bin (Nixon et al., 2019).Kolmogorov-Smirnov Calibration Error (Gupta et al., 2021) is an alternative evaluation metric, that insteadof binning, leverages the Kolmogorov-Smirnov statistical test for comparing the equality of two distributions.The error is determined by taking the maximum difference between the cumulative probability distributionsof the confidence scores and labels. Specifically, the first step is to sort the predictions according to theconfidence score on class k, i.e., pk, leading to the error being defined as:",
  "GAdversarial Attack Detection Details": "Local Intrinsic Dimensionality (LID) (Ma et al., 2018). This detection method exploits the estimatedLocal Intrinsic Dimensionality (LID) characteristics across different layers of a model of a set of adversarialexamples, which are found to be notably different than that of clean datapoints or those with added randomnoise. First, a training set is made up of clean, noisy, and adversarial examples, and a simple classifier (logisticregression) is trained to discriminate between adversarial and non-adversarial examples. For each trainingminibatch, the input features to the classifier are generated based on the estimated LID across differentlayers for all of the datapoints. The hyperparameters for this method are batch size and the number ofnearest neighbours involved in estimating the LID. We choose a consistent batch size of 100 in line withprevious work such as (Harder et al., 2021), and for each case we test the possible nearest neighbors fromthe following list {10, 20, 30, 40, 50, 60, 70, 80, 90} and report the results for the best value, which vary fordifferent datasets and models. We use the implementation from Lee et al. (2018). Mahalanobis Distance (MD) (Lee et al., 2018).The premise behind this method is to use aset of training datapoints to fit a class-conditional Gaussian distribution based on the empirical classmeans and empirical covariance of the training datapoints.Given a test datapoint, the Mahalanobisdistance with respect to the closest class-conditional distribution is found and taken as the confidencescore.A logistic regression detector is built from this which determines whether a datapoint is adver-sarial. The main hyperparameter for this method is the magnitude of the noise used, which we vary between{0.0, 0.01, 0.005, 0.002, 0.0014, 0.001, 0.0005} for each case and pick the value that results in the highest de-tection accuracy. In addition, calculating the mean and covariance is necessary to use the method, whichwe utilize the respective training set to do for each dataset. We use the implementation of MD from (Leeet al., 2018).",
  "HAdditional Analysis and Results": "In this section we provide additional results with white-box attacks, more details on the analyses describedin .4, and qualitative analysis of the properties of our attacks, as well as a quantitative analysisunder a common real world issue of imbalanced data distributions. Apart from the white-box results, theremaining analyses are conducted using our black-box setup.",
  "H.1White-box Calibration Attack": "The results for the white-box variation of our attacks can be found in on the three datasets andacross our two tested models, similar to how we presented our black-box results. For each scenario, we showthe ECE, KS error and average confidence. We used 10 attack steps to generate the results for an noisevalue of 0.05.",
  "GTSRB (Accuracy: 0.9720)Pre-atk0.0190.0060.0080.0020.980.002UCA0.2330.0200.2320.0160.7520.014OCA0.0200.0020.0190.0030.9910.003MMA0.2260.0060.2270.0070.7500.008RCA0.2170.0140.2180.0090.7630.008": "ViTCIFAR-100 (Accuracy: 0.9350.002)Pre-atk0.0640.0060.0540.0050.8820.004UCA0.2770.0010.2740.0040.6710.006OCA0.0450.0030.0170.0020.9280.002MMA0.2600.0060.2620.0070.6750.005RCA0.2360.0130.2390.0150.6990.013 Caltech-101 (Accuracy: 0.9610.024)Pre-atk0.1370.0590.1360.060.8250.083UCA0.4890.0710.4890.0710.4720.095OCA0.0860.0450.0820.0490.8790.073MMA0.4880.0700.4880.0700.4730.094RCA0.4350.0480.4350.0480.5270.071GTSRB (Accuracy: 0.9470.006)Pre-atk0.0400.0050.0260.0170.9220.024UCA0.3210.0470.3150.0450.6410.052OCA0.0370.0130.0200.0110.9360.024MMA0.3020.0370.3020.0360.6450.043RCA0.2920.0300.2930.0290.6570.037 Much like the SA results, the PGD attack manages to createsignificant miscalibration compared to before the attack withonly a small number of attack steps. The results are less severethan for SA where the level of miscalibration achieved are worsedespite the base PGD attack being far more effective at affectingclassification accuracy. We believe this is because the modifica-tions that are made to ensure that the calibration attack algo-rithm does not cause the predicted class to change greatly. Thisreduces the effectiveness of PGD as the most effective gradientupdates that cause a great swing in the confidence score can-not be used since they are likely to change the predicted class,and instead much less significant updates that do not changethe confidence a great deal serve as the primary noise that getsadded to the adversarial images.",
  "H.2Detailed Setup and Comparison ofEfficiency of Underconfidence vs. Overconfidence Attacks": "To understand which form of attack is most query efficient whenthe amount of change in confidence is the same, for each attacktype we identify all of images in the test set that are around agiven confidence level. We use the corresponding attack to madethe model produce either an increase of 10% in confidence, ora decrease of 10%.We choose two base confidence levels of80% and 90% and find all the data points within 1% of each.When an attack causes a change at or past the set thresholdfor the given goal probability, the attack stops and the numberof queries is recorded. The results can be seen in . Theconsistent pattern we observe for both base confidence levelsis that it takes notably fewer queries to create underconfidencethan overconfidence, and the former attack is more effective ataffecting the average confidence.",
  "H.3Detailed Analysis on Varying Epsilon and Attack Iterations": "Epsilon. The parameter plays a major role in adversarial attacks, as it controls how much noise can beadded when creating perturbations. Although setting a higher value for an attack lets it easier and moreefficient for the algorithm to create adversarial examples, it potentially cause the visual changes to imagesmore perceptible, so a small is preferable while still being able to produce good adversarial examples. Inthe case of calibration attack, there is no need to go as far as flipping a label, so lower -bounds have thepotential to create some miscalibration. To provide further details on our results in , for the leftmostfigure as mentioned previous we tested on CIFAR-100 using ResNet-50. The five different values we useare {0.005, 0.01, 0.05, 0.1, 0.25} after being attacked using all four of the attacks with the other settingsthe same as in Appendix D, with the results averaged over three models. In addition to the miscalibrationbeing strong for most of the attacks at low values, we can see that MMA consistently outperforms the restacross the different values. UCA does not have much change with higher , but it is largely because the modelshas already almost reached the peak level of attacking the confidence with low epsilon values, and as suchdoes not have a large effect on ECE. As middle figure largely displays the same trends as ResNet, revealingthat the results are not architecture dependant. The rightmost figure uses ResNet and goes over the same values as before, except MMA is run over both Caltech-101 and GTSRB models. Again the trends are similar,although the increase in ECE is not as severe as for CIFAR-100. 0.0 0.2 0.4 0.6 0.8",
  "H.4t-SNE Visualizations": "To help visualize the effect of each of the attack types in latent space and to confirm they are having theexpected effects, we run a t-SNE analysis (van der Maaten & Hinton, 2008) on the representations of ResNet-50 right before the classification layer. The datasets we use throughout this study, with their large numberof classes, are not ideal for visualization purposes. Instead, we create a binary subset using CIFAR-100 bytaking all of the images from two arbitrary classes, bicycles and trains. We create a separate training set andtest set to perform this procedure independently, and fine-tune a ResNet-50 model on the training set. Thespecific details are similar to those described in Section E for CIFAR-100 ResNet. We train the model for 5epochs with a learning rate of 0.005. The attack settings are the same as in Section D for the l version, andwe only run the attacks for 500 iterations. We run the t-SNE analysis on a balanced slice of 200 images fromthe new test set for easy visualization purposes, before and after all four attack types. The model achieves95% accuracy on the full test set. shows the graphs. It can be seen the effect on the representations",
  "Accuracy": ": Graphs comparing the vulnerability of ResNet and ViT models trained with different imbalance ratios onCIFAR-100 to the maximum miscalibration attack at 1000 iterations and an of 0.05, and their corresponding overalltrends in average queries and accuracy. Dataset imbalance has a profound effect on how a model learns and how well it performs, with detrimentaleffects occurring when imbalance ratios are very high. With how common imbalanced data distributions arein real world scenarios, we believe it is worth studying the influence of imbalance ratio and its relationshipwith robustness against calibration attacks as an additional point of analysis. We choose CIFAR-100 as our",
  "H.6GradCAM Visualization Details": "Given the effectiveness of the attacks at leading a model to produce highly miscalibrated outputs, for bothbase styles of attacks, we endeavour to explore whether they also lead to any changes in where the modelfocuses on in an image when making its decision, and especially with OCA. Knowing this can lead to furtherinsights as to how models are affected by calibration attacks. To accomplish this analysis, we use GradCAM(Selvaraju et al., 2017), a popular visualization method that produces a coarse localization map highlightingthe most important regions in an image that the model uses when making its prediction by making use of thegradients from the final convolutional layer (or a specific layer of choice) of a network. We apply GradCAMto our ResNet-50 models fine-tuned on Caltech-101 to images from the Caltech-101 test set before and afterthe underconfidence and overconfidence attacks at the standard attack settings used in Section D and usingthe GradCAM implementation of Gildenblat & contributors (2021). Since the method calculates relative toa specific class, we do so in-terms of the original predicted class for each image. shows the resultswith some representative images. We specifically choose images where the attacks led to large change inpredicted confidence (at least 10%). On the whole, we have observed that the coarse localization maps haveminimal to no noticeable changes after the adversarial images are produced, especially in the case of theoverconfidence attacked images. This leads us to believe the primary mechanism of the attacks changingthe model confidence is in the final classification layer as opposed to the convolutional layers. This analysisalso shows that it might be difficult to identify these attacks are occurring based on these types of gradientvisualization methods.",
  "H.7Certified Confidence Scores": "In this section, we follow recent work in the domain of providing provable guarantees on the robustness ofconfidence scores by examining the effect on a smoothed classifier and its bounds when handling calibrationattacked data, particularly in the case of OCA. Given the lack of label flipping, we expect that adversarialexamples generated by calibration attack lie close to the original in the data manifold, thereby having aminimum effect on the certified confidence. To test this hypothesis, we closely follow Kumar et al. (2020)on certification to provide lower bounds on the confidence of a smoothed classifier. A strong lower boundcan be produced by using the probability distribution of the confidence scores of a Gaussian cloud aroundthe input image using Neyman-Pearson lemma to calculate this for a given certified radius. We select a sub-sample of 100 random datapoints from CIFAR-100 and use our base ResNet-50 models that are attackedusing the underconfidence and overconfidence attacks using our standard settings, and average the expected",
  "Avg qqMed. qqECEKSAvg. Conf": "Swin-Large (Acc: .943.005)Pre-atk--.026.006.010.003.945.005UCA177.89.996.03.5.455.006.430.005.534.008OCA34.44.91.00.0.039.002.040.001.980.004MMA137.59.899.28.5.461.007.434.007.536.007RCA138.814.4102.25.3.433.011.409.006.557.003 We perform additional experiments using theSwin Transformer (Liu et al., 2021) architec-ture to further validate our attack results onadditional architectures. depicts theresults on all types of calibration attack: UCA,OCA, MMA, and RCA on the Swin-Large variant ofthe model. The trends of the results on SwinTransformer agree with the existing results in. Specifically, UCA, MMA and RCA all bringboth ECE and KS to high values. As observedpreviously, OCA successfully raises average confidences to high levels, but given the high accuracy of thisvictim model, increasing confidence will not have a drastic effect on calibration.",
  "In this section, we provide more detailed analyses on the results of the different defences seen in": "The RobustBench models compromise substantially on accuracy, and have a high level of miscalibration onclean data. They do largely avoid getting extremely miscalibrated as a result of the attacks compared to thedefenceless models, except the top model on the leaderboard. Nevertheless, their high inherent miscalibrationmeans they are unfavourable in situations where the model must be well calibrated. In terms of the calibration methods, TS tends to be among the best methods at reducing calibration errorprior to the attacks, but after the attacks it offers very little benefit compared to the vanilla models. TheMD-TS method is similar with its prior calibration being solid, but post-attack it only brings minor benefits",
  "H.11Full Attack Scale Results": "In this section, we present the results of the four attack types across both black-box and white-box setupsat various model scales of ResNet and ViT. Furthermore, we include four variants of Swin Transformersfor each attack scenario. We included the Tiny (swin-tiny-patch4-window7-224), Small (swin-small-patch4-window7-224), Base (swin-base-patch4-window7-224), and Large (swin-large-patch4-window7-224) variantsof Swin Transformers in our comparison trained with a similar setup used to obtain the results described inAppendix H.8."
}