{
  "Abstract": "Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train aglobal model by aggregating the locally trained models without sharing any local training data.In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the localdata distributions observed by each of these clients. Under such non-iid label distributionsacross clients, FL suffers from the client-drift problem where every client drifts to its own localoptimum. This results in slower convergence and poor performance of the aggregated model.To address this limitation, we propose a novel regularization technique based on adaptiveself-distillation (ASD) for training models on the client side. Our regularization schemeadaptively adjusts to each clients training data based on the global models prediction entropyand the client-data label distribution. We show in this paper that our proposed regularization(ASD) can be easily integrated atop existing, state-of-the-art FL algorithms, leading to afurther boost in the performance of these off-the-shelf methods. We theoretically explainhow incorporation of ASD regularizer leads to reduction in client-drift and empirically justifythe generalization ability of the trained model. We demonstrate the efficacy of our approachthrough extensive experiments on multiple real-world benchmarks and show substantial gainsin performance when the proposed regularizer is combined with popular FL methods. Thelink to the code is",
  "Introduction": "Federated Learning (FL) is a machine learning paradigm where the clients collaboratively learn a shared modelunder the orchestration of the server without sharing any of their local training data with other clients or theserver. Due to the privacy-preserving nature of FL, it has found many applications in smartphones (Hardet al., 2018; Ramaswamy et al., 2019), the Internet of Things (IoT), healthcare organizations (Rieke et al.,2020; Xu et al., 2021), where training data is generated at edge devices or from privacy-sensitive domains. Asoriginally introduced in (McMahan et al., 2017), FL involves model training across an architecture consisting",
  "(b) client 2": ": Impact of one round of local training on the test accuracy of two clients with different labeldistribution sampled from CIFAR-10 dataset: The effect of local learning on test accuracy is analyzed bymeasuring the change in accuracy before and after local training, with positive values indicating improvedmodel performance. Interestingly, in scenarios where classes with low probability of occurrence or under-represented, models trained using FedAvg frequently exhibit a decline in accuracy post-training. In contrast,incorporating our proposed adaptive self-distillation regularizer (ASD) into FedAvg (FedAvg+ASD) not onlyeffectively captures knowledge from well-represented classes but also preserves information about under-represented classes. A similar pattern is observed with FedNTD and FedNTD+ASD. of one server and multiple clients. In traditional FL, each client securely holds its training data due to privacyconcerns as well as to avoid large communication overheads while transmitting the same. At the same time,these clients aim to collaboratively train a generalized model that can leverage the entirety of the trainingdata disjointly distributed across clients. Data ingested at the edge/client devices are often highly heterogeneous as a consequence of the data generationprocess. They can differ in terms of quantity imbalance (the number of samples at each client are different),label imbalance (empirical label distribution across the clients widely vary), and feature imbalance (featuresof the data across the clients are non-iid). When there exists a label or feature imbalance, the objectivefor every client becomes different as the local minimum for every client objective will be different. In suchsettings, during the local training, the clients model starts to drift towards its local minimum and fartheraway from the global objective. This is undesirable as the goal of FL is to converge to a global model thatgeneralizes well across all the clients. This phenomenon, known as client-drift, is introduced and exploredin earlier works (Karimireddy et al., 2020; Acar et al., 2021; Wang et al., 2021). In this work, we will beconsidering only the label heterogeneity. In any given FL round, the client initializes its model with globalmodel weights and then starts training its model using the local data. Due to this, the client training oftenleads to overfitting the local data and cannot retain the knowledge acquired from the global model in anearlier FL round. Recently (He et al., 2022b) introduced a class-wise adaptive weighting scheme (FedCAD) at the server side.The major drawback of FedCAD is that it assumes the presence of related auxiliary data and reliability on theserver to compute the weights for the clients. Dependency on the server for computing the adaptive class-wiseweights necessitates the availability of auxiliary data at the server. Another work (Lee et al., 2022) proposesFedNTD which poses the client-drift as a local forgetting problem. It cannot mitigate client-drift effectivelysince it assigns uniform weights to regularization loss for all samples, independent of label distribution.Consequently, it treats high and low probability samples similarly, biasing the client model towards thosewith higher probability of occurrence, thus degrading performance. To address these issues and motivated byclient model regularization in mitigating client drift and to remove the servers dependency on computingclient-side weights, we introduce a computationally efficient strategy known as Adaptive Self-Distillation(ASD) for Federated Learning. Importantly, ASD does not require any auxiliary data. We use the KLdivergence between the global and local models as the regularizer. For every sample, the weight assignedto the regularization loss is adaptively adjusted based on the global models prediction entropy and the",
  "Published in Transactions on Machine Learning Research (December/2024)": ": The table shows the impact of ASD on the algorithms on CIFAR-100 Dataset. We consistently seethat the top eigenvalue and the trace of the Hessian decrease and the Accuracy improves when ASD is used.This suggests that using ASD makes the global model reach to a flat minimum for better generalization.",
  "In summary, the key contributions of this work are:": "We introduced a novel computationally efficient regularization method ASD in the context of FederatedLearning that alleviates the client drift problem by adaptively weighting the regularization loss foreach sample based on the global models prediction entropy and the label distribution of client data. We demonstrate the efficiency of our method by extensive experiments on datasets such as CIFAR-10,CIFAR-100, and Tiny-ImageNet datasets by combining our proposed ASD regularizer with thepopular FL methods and improving their performance. We present a theoretical analysis of the client-drift and show that our regularizer minimizes theclient-drift. We also empirically show that ASD promotes better generalization by converging to aflat minimum.",
  "Federated Learning (FL)": "In recent times, addressing heterogeneity in Federated Learning has become an active area of research, andthe field is developing rapidly. For brevity, we discuss a few related works here. In FedAvg (McMahan et al.,2017), the two main challenges explored are reducing communication costs (Yadav & Yadav, 2016) andensuring privacy by avoiding having to share the data. There are some studies based on gradient inversion(Geiping et al., 2020) raising privacy concerns owing to gradient sharing while some studies have proposed indefense of sharing the gradients (Kairouz et al., 2021; Huang et al., 2021). FedAvg is the generalization oflocal SGD (Stich, 2018) by increasing the number of local updates, significantly reducing communicationcosts for an iid setting, but does not give similar improvements for non-iid data. Several works perform",
  "Client-Drift in FL": "Due to data heterogeneity, federated training suffers from client drift. To address this, momentum-basedserver aggregation was proposed in (Wang et al.; Hsu et al., 2019), which was later extended to handle anyclient and server updates in (Reddi et al.) FedProx (Li et al., 2020) introduced a proximal term to penalizedeviations of client weights from the globally initialized model. SCAFFOLD (Karimireddy et al., 2020)tackled the issue as one of objective inconsistency, introducing a gradient correction term as a regularizer.Subsequently, FedDyn (Acar et al., 2021) enhanced this with a dynamic regularization term. In (Kim et al.,2024), a proximal term was introduced in the clients optimization based on the accelerated global model,and momentum was applied on the server to track its updates.",
  "Federated Learning Using Knowledge Distillation": "Knowledge Distillation (KD) introduced by (Hinton et al., 2015) is a technique to transfer the knowledge froma pre-trained teacher model to the student model by matching the predicted probabilities. Self-distillationwas introduced in (Zhang et al., 2019) where the student distills from the same model to the sub-networksof the model. The teacher model predictions are updated every batch. In our method, distillation happenswith the full network, and the teachers predictions are updated after every communication round. Adaptivedistillation was used in (Tang et al., 2019). The server-side KD methods such as FedGen (Seo et al., 2022)use KD to train the generator at the server and the generator is broadcasted to the clients in the subsequentround. The clients use the generator to generate the data to provide the inductive bias. This method incursextra communication of generator parameters along the model and training of the generator in general isdifficult. In FedDF (Lin et al., 2020) KD is used at the server that relies on the external data. The KD isperformed on an ensemble of client models, especially client models acts as a separate teacher model andthen the knowledge is distilled into a single student model (global model). In FedNTD (Lee et al., 2022)the non-true class logits are used for distillation. This method gives uniform weights to all the samples. InFedCAD (He et al., 2022b) and FedSSD (He et al., 2022a), the client-drift problem is posed as a forgettingproblem, and a weighting scheme has been proposed. Importantly, the computation of adaptive weights of theclient samples is done with the help of the server with the assumption that the server has access to auxiliarydata. One shortcoming of this method is the assumption of the availability of auxiliary data on the server,which is impractical. In (Zhang et al., 2022) logits were calibrated based on the label distribution. This istotally different from our approach as we are adjusting the weights of the distillation loss. Unlike all of theseapproaches, we propose a novel ASD strategy that aims to mitigate the challenge of client drift due to non-iiddata without relying on the server and access to any form of auxiliary data to compute the adaptive weights.",
  "Problem Setup": "We assume there is a single server/cloud and m clients/edge devices. We further assume that client k hasits own training dataset Dk with nk training samples drawn iid from the data distribution Pk(x, y). Thedata distributions {Pk(x, y)}Kk=1 across the clients are assumed to be non-iid. In this setup, we perform the",
  "Local": "DatasetServer Step 2: Clients train their local models with the starting point, Step 1: Broadcast Step 3: Aggregate : Federated Learning with Adaptive Self-Distillation: The figure describes the overview of theproposed approach based on Adaptive distillation. In Step 1. The server broadcasts the model parameters,In Step 2. clients train their models by minimizing both the cross entropy loss and predicted probabilitydistribution over the classes between the global model and the client model by minimizing the KL divergence,the importance of each sample in the batch is decided by the proposed adaptive scheme as a function of labeldistribution and the KL term. The server model is fixed while training the client. In Step 3. The serveraggregates the client models based on FedAvg aggregation. The process repeats till convergence.",
  "Lk(w) is given below.Lk(w) =Ex,yPk(x,y)[lk(w; (x, y))](3)": "Here, lk is cross-entropy loss. The expectation is computed over training samples drawn from Pk(x, y) ofa client k. This is approximated as the empirical average of the losses corresponding to samples from theDataset Dk. LASDk(w) in Eq. 2 denotes our proposed Adaptive Self-Distillation loss (ASD) term whichconsiders label imbalance and quantifies how easily the predictions of the local model can drift from theglobal model. ASD loss is designed so that client models learn from the local data and at the same time notdrift too much from the global model. We define (ASD) Loss as follows.",
  "LASDk(w) E[k(x, y)DKL(qg(x, wt)||qk(x, w))](4)": "In the above Eq. 4 wt represents the global model parameters at FL round t and w represents the trainablemodel parameters of client k, initialized with wt at round t. k(x, y) denotes the weight for the sample xwith label ground truth label y. For simplicity, we denote the global model softmax predictions qg(x, wt) asqg(x) and client model softmax predictions qk(x, w) as qk(x). DKL is the KL divergence. The Eq. 4 can beapproximated by the following equation for a mini-batch.",
  "c=1qcg(xi)log(qcg(xi))(11)": "DKL in Eq. 8 captures how close the local models predictions are to the global model for any given sample xi.Our weighting scheme in Eq. 11 decides how much to learn from the global model for that sample based onthe entropy H(xi) of the server model predictions and the label distribution of the client data (pyi k ). H(xi)captures the confidence of global model predictions, higher value implies the server predictions are noisy sowe tend to reduce the weight, i.e, we give less importance to the global model if its entropy is high. The pyi k isthe probability that the sample belong to a particular class. We give more weight to the sample if it belongsto the minority class. This promotes learning from the local data for the classes where the representationis sufficient enough and for the minority classes we encourage them to stay closer to the global model. Insummary, the choice of alpha is designed to ensure that, when the global model encounters samples with highprediction entropy, we decrease the weighting on the regularization loss. Conversely, for samples with a lowprobability of occurrence, we prioritize learning from the global model. This adaptive approach enables localmodels to effectively learn from the cross-entropy loss for more frequent labels while leveraging the globalmodels guidance for less frequent labels. In , we highlight the importance of adaptive weights, wherewe clearly show that ASD with adaptive weights consistently improves performance when combined withoff-the-shelf FL methods. We approximate the label distribution pyi",
  "where Iyi=c denotes the indicator function and its value is 1 if the label of the ith training sample belongs to": "class c else it is 0. To simplify notation, we use pck for pyi=ckas it depends only on the class c for a client k.Finally we use Eq.12 and Eq. 11 to compute the LASDk(w) defined in Eq.5. The choice of KL divergence inthe Eq. 8 is motivated by the seminal work of Hinton et.al., which aims to match the temperature-raisedsoftmax values between the pre-trained teacher model and student model for effective knowledge transfer.We also analyzed the other statistical divergences such as reverse KL and Jenson-shannon divergence andempirically found that KL divergence is better. More details are presented in the Sec. A.13 of the Appendix.",
  "f(w)2(13)": "Gd(w, ) is function of both the w and . For convenience, we simply write Gd and mention argumentsexplicitly when required. fk(w) in the above Eq. 13 is same as Eq. 2.With this, we now establish a series of propositions to show that ASD regularization reduces the Gradientdissimilarity, which as a result, leads to lower client drift.",
  "Proposition 3.1. infwRd Gd(w, ) is 1,": "The above proposition implies that if all the clients gradients are progressing in the same direction, whichmeans there is no drift Gd = 1. The result follows from Jensens inequality. The lower value of Gd is desirableand ideally 1. To analyze the Gd, we need fk(w) which is given in the below proposition.",
  "c pck(gc + ckgc), where gc=E[l(w; x, y) | y = c],gc=E[exp(H(x))DKL(qg(x)||qk(x)) | y = c] and ck =1pck": "The result follows from the tower property of expectation and the assumption that class conditional distributionis the same for all the clients. From the above proposition, we can see that the gradients fk(w) only differdue to pck which captures the data heterogeneity due to label imbalance. The proof is given in Sec. A.15 ofthe appendix.",
  "The assumption on weakly correlated class-wise gradients intuitively implies that gradients of loss for aspecific class cannot give any significant information on the gradients of the other class": "Proposition 3.4. When the class-conditional distribution across the clients is the same, and the As-sumption 3.3 holds then a range of values for such that whenever c we havedGdd< 0 andGd(w, ) < Gd(w, 0). The proposition implies that there is a value of c such that the derivative of Gd w.r.t is negative.The proof is given in Sec. A.15 of the appendix. This indicates that by appropriately selecting the valueof we can make the Gd lower which in turn reduces the client drift. One of the key assumptions on theheterogeneity is the existence of the below quantity.",
  "supwRd Gd(w, ) < supwRd Gd(w, 0)(16)": "The above inequality 16 is true as proposition 3.4 guarantees that the value of Gd(w, ) < Gd(w, 0) for allw when c. If inequality 16 is not true, one can find a w that contradicts the proposition 3.4 which isimpossible. This means for some value of c we have B2() < B2(0) from Eq. 14 and Eq. 16. The key takeaway from the analysis is that by introducing the regularizer we can tightly bound the heterogeneitywhen compared to the case without the regularizer. Based on the works (Karimireddy et al., 2020; Li et al.,2020) we explain that lower B2() implies better convergence, which is also supported by empirical evidence.These details are provided in the Sec. A.16 of the Appendix.",
  "Discussison on the Generalization of ASD": ": The table shows the impact of ASD on the algorithms on CIFAR-100 Dataset using the non-iidpartition of = 0.3. We consistently see that the top eigenvalue and the trace of the Hessian of the loss ofthe global model decrease and the accuracy improves when ASD is used. This suggests that by using ASDwe can make global model reach a flat minimum towards better generalization.",
  ": Eigen spectrum with and without the ASD regularizer. It is evident that ASD regularizer not onlyminimizes the top eigenvalue but most of the eigenvalues and attains the flatness": "The key reason for better generalization is the adaptive self-distillation loss. It has been shown in (Mobahiet al., 2020) that self-distillation improves the generalization in centralized settings. Its been empiricallyshown in (Zhang et al., 2019) that self-distillation helps the model to converge to flat-minimum. Generally,converging to flat minima is indicative of improved generalization, a concept explored in prior studies suchas (Keskar et al., 2017) and (Yao et al., 2020). The top eigenvalue and the trace of the Hessian computed fromthe training loss are typical measures of flatness of the minimum to which the training converges, i.e., lowervalues of these measures indicate the presence of a flat minimum. To gain a deeper understanding of thisphenomenon in a federated learning setting, we analyzed the top eigenvalue and the trace of the Hessian of thecross-entropy loss for global models obtained with and without the ASD regularizer. The following argumentestablishes that if the client models converge to flat minima, it would also ensure convergence of the resultantglobal model to a flat minimum. We assume the Hessians of the functions fk (kth clients local objective),",
  "KKi=1 H(fi) (H(g) denotes the Hessian of function g). This implies 1(H(f)) 1": "KKi=11(H(fi))(1(A) denotes top eigenvalue of matrix A). Thus when the local models converge to a flat minimum, itwill ensure the convergence of the global model to a flat minimum. Following the method of (Yao et al.,2020), we computed the top eigenvalue and trace of the Hessian. In , we observe that FedAvg+ASDattains lower values for the top eigenvalue and trace compared to FedAvg, suggesting convergence to flatminimum. The Eigen density plot in the figure 3 also confirms the same. We use the CIFAR-100 datasetwith non-iid data partitioning of = 0.3 (refer to Sec. 4). In we have presented our analysis whenASD is combined with FedAvg, FedDyn and FedSpeed. The results for other algorithms are presented inSec. A.8 of appendix. A similar concept has been explored in FedSAM (Qu et al., 2022; Caldarola et al.,2022); the issue with SAM-based methods is they require an extra forward and backward pass, which doublesthe computational cost on the resource constrained edge devices. However, our method can be applied toSAM-based methods and further improve its performance. ASD consistently attains the flatness with theother FL algorithms and enhances their generalization.",
  "Experiments": "We perform the experiments on CIFAR-10, CIFAR-100 (Krizhevsky & Hinton, 2009), Tiny-ImageNet (Le &Yang, 2015) datasets with different degrees of heterogeneity in the balanced settings (i.e., the same numberof samples per client but the class label distribution of each varies). We set the total number of clients to 100in all our experiments. We set the client participation rate to 0.1, i.e., 10 percent of clients are sampled onan average per communication round, similar to the protocol followed in (Acar et al., 2021). We build ourexperiments using publicly available codebase by (Acar et al., 2021).For generating non-iid data, Dirichletdistribution is used. To simulate the effect of label imbalance, for every client we sample the probabilitydistribution over the classes from the aforementioned Dirichlet distribution pdirk= Dir(, C). Every sampleof pDirkis a vector of length C and all the elements of this vector are non-negative and sum to 1. This vectorrepresents the label distribution for the client. The parameter known as the concentration parameter,captures the degree of label heterogeneity. Lower values of capture high heterogeneity and as the valueof increases, the label distribution becomes more uniform. Another parameter of Dirichlet distribution(i.e., C), its value can be interpreted from the training dataset ( C = 100 for CIFAR-100). For notationalconvenience, we omit C from Dir(, C) by simply re-writing as Dir(). By configuring the concentrationparameter to 0.6 and 0.3, we sample the data using the Dirichlet distribution across the labels for eachclient from moderate to high heterogeneity by controlling . This is in line with the approach followed in(Acar et al., 2021) and (Yurochkin et al., 2019).",
  "Results and Discussion": "For evaluation, we report accuracy on the test dataset as our performance metric and the number ofcommunication rounds required to attain the desired accuracy as a metric to quantify the communication cost.Specifically, we evaluate the global model on the test set and report its accuracy after every communicationround. For comparison, we consider the popular methods for federated learning, such as FedAvg, FedProx,FedDyn, FedSpeed FedNTD, FedSAM and FedDisco. We augment each of these methods with our approach(ASD) and observe a significant boost in performance. For a fair comparison, we consider the same modelsused in Fedavg (McMahan et al., 2017), and FedDyn (Acar et al., 2021), for CIFAR-10 and CIFAR-100classification tasks. The model architecture used for CIFAR-100 contains 2 convolution layers followed by 3fully connected layers. For Tiny-ImageNet, we use 3 convolution followed by 3 fully connected layers. Thedetailed architectures are given in Sec A.2 of the appendix. Hyperparameters: SGD algorithm with a learningrate of 0.1 and decay the learning rate per round of 0.998 is used to train the client models. Temperature is set to 2.0. We only tune the hyper-parameter . More hyperparameter setting details and impact of , are provided in Sec. A.3 and A.6 of the appendix, respectively. The impact of client participation rate andthe number of clients on ASD are shown in Sec. A.7 of the appendix. Implementation of ASD with other FLmethods is discussed in Sec. A.12 of appendix. We compare the convergence of different schemes for 500communication rounds. Following the testing protocol of (Acar et al., 2021), we average across all the clientmodels and compute the test accuracy on the averaged model, which is reported in our results. In all thetables, we report the test accuracy of the global model in % at the end of 500 communication rounds. All theexperiments in the tables are performed over three different initializations, mean and standard deviations ofaccuracy over the three experiments are reported. We also demonstrate the efficacy of our proposed methodwith deeper architectures such as ResNet-20 and Vision Transformer (ViT) models in Sec A.4 and Sec A.5 ofthe appendix respectively.",
  "Performance of ASD on CIFAR-10/100 and Tiny-Imagenet": "In , we report the performance of CIFAR-100 and Tiny-ImageNet datasets with various algorithms fornon-iid (Dir( = 0.3) and Dir( = 0.6)) as well as the iid settings. Each experiment is performed over threedifferent initializations, and the mean and standard deviation of the accuracy are reported. On CIFAR-100,we observe that our proposed ASD applied on FedDyn improves its performance by 1.45% for Dir( = 0.3)and 1.6% Dir( = 0.6). Similarly, for Tiny-ImageNet we observe that FedDyn+ASD improves FedDyn by 2.4% for Dir( = 0.3) and by 1.4% for Dir( = 0.6). The test accuracy vs communication rounds plot onCIFAR-100 and Tiny-ImageNet datasets is shown in Figures 4, 5 6 across non-iid and iid partitions. We cansee that adding ASD gives consistent improvement across the rounds1. We obtain significant improvementsfor FedAvg+ASD against FedAvg, FedProx+ASD against FedProx, FedSpeed+ASD against FedSpeed, etc.In Sec A.9 of the appendix we present the CIFAR-10 results where we observe that adding ASD consistentlygives an improvement of 0.4% 1.99% improvement across the algorithms.",
  "FedSpeed34.08FedSpeed+ASD (ours)36.59": "In this section, we analyze the impact of our ASD regularizer to mimic the cross-device setting. We increasethe client participation to 500 clients and only 1% of the clients participate in every round. We considerthe CIFAR-100 dataset and the non-iid data partition of = 0.3. In the , we observe that ASDconsistently improves the performance of the algorithms. The accuracies are reported after averaging overthree different initializations at the end of 1000 communication rounds. Even in this challenging setting ASDconsistently improves the performance of the FL algorithms.",
  "Computation Cost": "The major computation for the distillation scheme comes from the teacher forward pass, student forward pass,and the student backward pass (Xu et al., 2020). We assume Cs as the total computational cost of servermodel forward pass and Ck be the total computation cost of client model k the forward pass per epoch. Wedo not need Cs computations every epoch, we only need to compute once and store the values of H(x) whilekeeping the same backward computation. Specifically, for the computation of distillation regularizer we onlyneed E Ck + Cs local computations compared to E Ck computations without regularizer. Here E denotesthe local epochs of the client. Since Cs = Ck, we have (E + 1) Ck local computations. Thus, our regularizerintroduces minimal forward computation on the edge devices, which typically have low computation. InSec. A.14 of appendix we discuss the computation vs accuracy of ASD.",
  "Conclusion": "In this work, we presented an efficient and effective method for addressing client data heterogeneity due tolabel imbalance in federated learning using our proposed Adaptive Self-Distillation (ASD), which does notrequire any auxiliary data and no extra communication cost. We also theoretically showed that ASD haslower client-drift leading to better convergence. Moreover, we performed analysis to show that ASD hasbetter generalization by analyzing the top eigenvalue and trace of the Hessian of the global models loss.The effectiveness of our approach is shown via extensive experiments across datasets such as CIFAR-10,CIFAR-100 and Tiny-ImageNet with different degrees of heterogeneity. Our proposed regularizer (ASD) canbe integrated easily atop any of the FL frameworks. We evaluated this efficacy by showing improvement inthe performance when combined with FedAvg, FedProx, FedDyn, FedSAM, FedDisco, FedNTD and FedSpeed.We have also shown that the computation required to implement ASD is simply an additional forward passon the client-side training, i.e, all the gains we obtain with ASD requires minimal compute. Our research caninspire the designing of the computationally efficient regularizers that concurrently reduce client-drift andimprove the generalization.",
  "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distributionfor federated visual classification. arXiv preprint arXiv:1909.06335, 2019": "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Federated visual classification with real-world datadistribution. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part X 16, pp. 7692. Springer, 2020. Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. Evaluating gradient inversion attacksand defenses in federated learning. Advances in Neural Information Processing Systems, 34:72327241,2021. Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and openproblems in federated learning. Foundations and Trends in Machine Learning, 14(12):1210, 2021. Sai Praneeth Karimireddy,Satyen Kale,Mehryar Mohri,Sashank Reddi,Sebastian Stich,andAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-national Conference on Machine Learning, pp. 51325143. PMLR, 2020. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. Onlarge-batch training for deep learning: Generalization gap and sharp minima. In International Conferenceon Learning Representations, 2017. URL Geeho Kim, Jinkyu Kim, and Bohyung Han. Communication-efficient federated learning with acceleratedclient gradient. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1238512394, 2024.",
  "Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Franoise Beaufays. Federated learning for emojiprediction in a mobile keyboard. arXiv preprint arXiv:1906.04329, 2019": "Sashank J Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konen`y, SanjivKumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International Conference onLearning Representations. Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albarqouni, Spyridon Bakas,Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health withfederated learning. NPJ digital medicine, 3(1):17, 2020.",
  "Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-efficient distributed sgd with slow momentum. In International Conference on Learning Representations": "Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat, GalenAndrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization.arXiv preprint arXiv:2107.06917, 2021. Kan Wu, Jinnian Zhang, Houwen Peng, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Tinyvit: Fastpretraining distillation for small vision transformers. In European conference on computer vision, pp. 6885.Springer, 2022.",
  "Rui Ye, Mingkai Xu, Jianyu Wang, Chenxin Xu, Siheng Chen, and Yanfeng Wang. Feddisco: Federatedlearning with discrepancy-aware collaboration. 2023": "Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and YasamanKhazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference onMachine Learning, pp. 72527261. PMLR, 2019. Zelin Zang, Siyuan Li, Di Wu, Ge Wang, Kai Wang, Lei Shang, Baigui Sun, Hao Li, and Stan Z Li. Dlme:Deep local-flatness manifold embedding. In European Conference on Computer Vision, pp. 576592.Springer, 2022. Jie Zhang, Zhiqi Li, Bo Li, Jianghe Xu, Shuang Wu, Shouhong Ding, and Chao Wu. Federated learningwith label distribution skew via logits calibration. In International Conference on Machine Learning, pp.2631126329. PMLR, 2022. Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your ownteacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings ofthe IEEE/CVF international conference on computer vision, pp. 37133722, 2019.",
  "A.2Model Architectures": "In , the model architecture is shown. We use PyTorch style representation. For example convlayer(3,64,5) means 3 input channels, 64 output channels and the kernel size is 5. Maxpool(2,2) representsthe kernel size of 2 and a stride of 2. FullyConnected(384,200) represents an input dimension of 384 and anoutput dimension of 200. The architecture for CIFAR-100 is exactly the same as used in (Acar et al., 2021).",
  "A.3Hyper-Parameter Settings": "The value of is specified in units of batch-size B. We chose from {10, 20, 30}. We set = 20 for all theTiny-ImageNet experiments. For CIFAR-10/100 we chose to be 10 and 30 respectively. The batch-size (B)of 50 and learning rate of 0.1 with decay of 0.998 is employed for all the experiments unless specified. All theexperiments are carried out with 100 clients and with 10% client participation.",
  "A.4Experiments with Deeper Models (CNNs)": "In this section, we perform experiments with the deep models such as ResNet-20 on CIFAR-100 dataset withDirichlet = 0.3. For this experiment we have used 300 communication rounds, the number of clients as 30,and the client participation rate is set to 20%. In the table 6, we report the numbers averaged over 3 differenttrials. We observe that the addition of our proposed regularizer ASD atop mutiple popular FL methods leadsto consistent improvements, thereby further justifying the efficacy of our proposed method.",
  "A.5Experiments with Deeper Models (ViT)": "We perform experiments with ViT architecture using the Tiny-ViT (Wu et al., 2022) as client models onImageNet-100 dataset (Zang et al., 2022) with non-iid data partitioning of Dirichlet = 0.3. The choice ofTiny-ViT is motivated by the fact that edge devices are traditionally computational resource-constrained andTiny-ViT is designed for such applications. For this experiment, the number of clients is set to 200, and theclient participation rate is set to 5%. We have used 300 communication rounds. In the , we report thenumbers averaged over 3 different trials. We observe that the addition of our proposed regularizer ASD atopFedAvg and FedDyn leads to consistent improvements, thereby further justifying the efficacy of our proposedmethod on the deeper architectures.",
  "FedDyn28.02FedDyn+ASD (ours)36.70": "In , we performed an experiment on ViT-Small architecture on CIFAR-100. We observe that addingour ASD regularizer improves the baseline FedAvg by 1.4% and 1.7% for = 0.3 and = 0.6, respectively.In this setup we consider 100 clients with 10% participation and the accuracy is reported at the end of 300rounds.",
  ": Impact of and on CIFAR-100 dataset with non-iid partitioning of = 0.3 with FedAvg+ASD": "We study the impact of changing the hyper-parameters and on the CIFAR-100 dataset with the Dirichletnon-iid partition of = 0.3. We report the accuracy at the end of 500 rounds. When using FedAvg+ASDalgorithm. In we see that the accuracy of the model increases with and then slightly drops after acritical point. This is expected as too less value of is similar to FedAvg and very high value of will ignorethe local learning. It can also be seen that for all the values of the Accuracy peaks at = 2. In all of ourexperiments we set the temperature parameter set to 2.0.",
  "A.7Impact of client participation / number of clients on ASD": "We fix the client participation to 2% and vary the number of clients from 100 to 500. We perform thisablation using the CIFAR-100 dataset with a non-iid Dirichlet data partitioning of = 0.3. We summarizeour observations in the Tables 9 and 10 below. It can be seen that ASD improves the performance of thebaselines FedAvg and FedDyn in all the settings. In particular, we would like to highlight the point herethat despite increasing the number of clients, the total number of training data samples across all the clientsremains constant (for CIFAR-100). Thus as the number of clients increases, the number of data samplesper client decreases. This further aggravates the adverse impact of label heterogeneity across clients, andhence accuracy degrades in general. However, we are happy to observe and report that, even under such achallenging setup, our proposed adaptive self-distillation-based strategy consistently improves the accuracywhen combined on top of the existing baseline algorithms.",
  "FedDyn37.9636.5634.7130.3726.45FedDyn+ASD (ours)39.4040.4937.4833.2328.60": "In , unlike the previous ablation, here we fix the number of clients to 100 and vary the clientparticipation rate from 5%, 10% and 15%. We consider the CIFAR-100 dataset with non-iid partitioningof ( = 0.3). As expected, the accuracy of the FL-trained models improve with an increase in the clientparticipation rate. We would also like to highlight here that, by adding our proposed ASD strategy consistentlyimproves the accuracy when combined on top of the existing baseline algorithms such as FedAvg and FedDyn.",
  "A.10Accuracy vs Communication rounds": "In the below figures 8 9 and 10, we present how the accuracy is evolving across the communication roundsfor the FL methods FedNTD, FedProx, FedDisco with and without the ASD regularizer. We present theseresults for non-iid ( = 0.3 and = 0.6) and with the iid data partitions for both the CIFAR-100 andTiny-ImageNet datasets. It can be seen that adding ASD to these off-the-shelf FL methods consistentlyimproves the performance.",
  "A.11Privacy of Proposed Method": "In our method, which is ASD regularizer, the adaptive weights are computed by the client without dependingon the server and it does not assume access to any auxiliary data at the server as assumed in methods suchas FedCAD (He et al., 2022b) and FedDF (Lin et al., 2020). In our method, only model parameters arecommunicated with the server similar to FedAvg (McMahan et al., 2017). Thus our privacy is similar to theFedAvg method at the same time obtaining significant improvements in the performance.",
  "A.12Implementation of ASD with the FL Methods": ".We now present the integration of ASD loss with the existing FL methods. For all the methods FedAvg,FedDyn, FedSpeed, FedProx, FedDisco and FedSAM, we augment the client loss of each of these methodswith our proposed ASD loss in the Eq 5. FedNTD (Lee et al., 2022) uses the non-true distillation loss, itdistills the knowledge only from the non-true classes.",
  "A.13On the choice of KL divergence": "The distillation loss introduced by the seminal work of (Hinton et al., 2015) matches the temperature-raisedsoftmax values between the pre-trained teacher model and student model for effective knowledge transfer.It is essentially cross entropy between two softmax vectors. KL divergence differs from cross entropy by aconstant and hence achieves the same optimization objective. In our context, we treat the server model asthe teacher model and the client model as the student model. Other divergence measures such as reverse KLand JS can also be considered, but we did not see any significant performance difference empirically. In factKL divergence performed better compared to reverse-KL and JS divergence as shown in Table below. Forthis experiment we used the CIFAR-100 dataset with 100 clients and 10% client participation rate.",
  "K + 2) > 0(54)": "for some > 0 and = f(w0) f(w), f(w is the local minimum.It can be seen that the convergence is inversely related to . High value of leads to faster convergence.From Eq. 54 we can see that can be increased by decreasing the value of B. Thus reducing the value of Bhelps in better convergence.",
  "RLS + B2F": "R). We can see that convergence has a direct dependence onB2. This is the only term that is linked to heterogeneity assumption. So the lower value of B implies fasterconvergence. This motivates to have a tighter bound on heterogeneity. ASD achieves this by introducing theregularizer and choosing the appropriate value of . In the figure 12 we empirically we verify the impact ofASD on the convergence. We plot the smoothed estimates of the norm of the difference of the global modelparameters between the successive communication rounds i.e wt wt1. communication round 0.2 0.4 0.6 0.8 1.0 1.2 ||wtwt1|| FedAvgFedAvg+ASD"
}