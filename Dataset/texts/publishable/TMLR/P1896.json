{
  "Abstract": "Personalized summarization models cater to individuals subjective understanding of saliency,as represented by their reading history and current topics of attention. Existing personalizedtext summarizers are primarily evaluated based on accuracy measures such as BLEU, ROUGE,and METEOR. However, a recent study argued that accuracy measures are inadequate forevaluating the degree of personalization of these models and proposed EGISES, the firstmetric to evaluate personalized text summaries. It was suggested that accuracy is a separateaspect and should be evaluated standalone. In this paper, we challenge the necessity of anaccuracy leaderboard, suggesting that relying on accuracy-based aggregated results mightlead to misleading conclusions. To support this, we delve deeper into EGISES, demonstratingboth theoretically and empirically that it measures the degree of responsiveness, a necessarybut not sufficient condition for degree-of-personalization. We subsequently propose PerSEval,a novel measure that satisfies the required sufficiency condition. Based on the benchmarkingof ten SOTA summarization models on the PENS dataset, we empirically establish that (i)PerSEval is reliable w.r.t human-judgment correlation (Pearsons r = 0.73; Spearmans = 0.62; Kendalls = 0.42), (ii) PerSEval has high rank-stability, (iii) PerSEval as a rank-measure is not entailed by EGISES-based ranking, and (iv) PerSEval can be a standalonerank-measure without the need of any aggregated ranking.",
  "Introduction": "With the incessant rise of information deluge, it has become even more imperative to develop efficient andaccurate models to summarize the salient information in long documents for faster consumption, eliminatingthe irrelevant and supporting faster decision-making Ter Hoeve et al. (2022). However, the notion of saliencycan be highly subjective in many use cases, particularly for documents containing multiple aspects and topics.This calls for summarizers that must be personalized to the users preferences as depicted by their readingbehavior and current topic(s) of attention (Ghodratnama et al., 2021; Ao et al., 2021). This calls for robustand reliable measures for evaluating the degree-of-personalization in them. Dearth of Personalization Evaluation. Major studies on evaluating text summaries focus on accuracymeasurement and include the proposal of a multitude of measures such as the ROUGE variants (e.g.,ROUGE-n/L/SU4 etc.) (Lin, 2004), BLEU (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005),BERTScore (Zhang et al., 2020), PYRAMID (Gao et al., 2019) and the more recently proposed ones such asSUPERT (Gao et al., 2020), WIDAR (Jain et al., 2022), and InfoLM (Colombo et al., 2022b). Other studiesacknowledged the need for more qualitative aspects such as consistency, coherence, and fluency (Yuan et al.,2021; Deng et al., 2021; Tam et al., 2023; Jain et al., 2023; Zhong et al., 2022). Recently, Vansh et al. (2023)established, both theoretically and empirically, that personalization is a different aspect than accuracy. Theauthors proposed a measure for degree-of-personalization for the first time and called it EGISES .",
  ": EGISES Personalization-Accuracy Paradox. The absurd case of high personalization (therebyhigh user-experience), yet low accuracy": "the strong notion of a models degree of insensitivity to users subjective expectations. However, we arguethat this degree of insensitivity does not truly measure personalization but rather a related and necessaryaspect, which can be understood as the responsiveness of a model. A high degree of personalization mustimply a very high-quality user experience (UX). However, one cannot expect high UX while having lowaccuracy. At the same time, we demonstrate that a model can have high accuracy but a poor EGISES score.We resolve this apparent paradox by establishing both theoretically and empirically that EGISES, in reality,accounts for only the degree of responsiveness of models. In other words, we mathematically prove that amodel can have a very good EGISES score but may still fall short of being personalized simply because oflow accuracy performance. P-Accuracy is not a solution. As an extension to standard accuracy measures, Vansh et al. (2023)proposed a measure called P-Accuracy (P-Acc) that penalizes accuracy by a factor that is a function of theEGISES score. Although P-Acc might apparently seem to address the EGISES paradox, in this paper, weprove that P-Acc fails to address the issue for two reasons. First, it has not been designed to evaluate thedegree-of-personalization of a model but rather to get a more realistic, human-evaluator adjustable accuracyscore. This makes P-Acc unable to distinguish the two models, where one exhibits low accuracy but highEGISES while the other exhibits high accuracy but low EGISES. Secondly, we prove that P-Acc is unstablefor models with relatively low accuracy and may lead to anomalous results if we measure personalizationusing it. This motivates us to propose a novel consolidated personalization measurement framework for textsummarizers, called PerSEval (Personalized Summarizer Evaluator) that builds on the design principles ofEGISES and bridges the UX-gap\". PerSEval Design Principle. The underlying design principle of PerSEval entails that higher accuracyperformance should not obfuscate the original EGISES score of a model. Otherwise, a model can be consideredhighly personalized simply because of its high accuracy, which is misleading, as was proven in (Vansh et al.,2023). However, the converse should not be true; hence, a lower accuracy score should penalize the originalEGISES score. Based on these design objectives, we propose a penalty factor, called EDP (Effective DEGRESSPenalty) that can be injected into EGISES to form PerSEval to measure the true degree of personalization.EDP incorporates the inconsistency of accuracy of model w.r.t its best accuracy performance and how muchthat is off from the maximum achievable accuracy (which is 0 for a normalized metric). In the best case, theEDP factor comes to 1 (i.e., the penalty is 0), while in the worst case, it tends to 0 (i.e., the penalty is 1). Observations and Insights. We first empirically establish that the EGISES-based rank does not simplyentail the PerSEval-based rank. In other words, models rank differently when EDP is applied. Therefore, wecan conclude that the EGISES-paradox not just theoretically exists but has real evidence. We then showthat PerSEval provides a much more reliable ranking of models with significantly higher human-judgmentcorrelation in terms of Pearsons r (0.73), Spearmans (0.62), and Kendalls (0.42). For fair comparisons,we consider the same top ten state-of-the-art news summarization models and the same PENS test dataset(Ao et al., 2021) that Vansh et al. (2023) considered to evaluate EGISES. We also take a step beyond (Vanshet al., 2023) and demonstrate that the accuracy leaderboard is not only insufficient but is at best redundantand at worst can be misleading for evaluating personalized summarizers. This is established by showing that",
  "EGISES is Not Enough: The Personalization-Accuracy Paradox": "As proposed in (Vansh et al., 2023), the degree-of-personalization is a quantitative measure of how mucha summarization model fine-tuned for personalization is adaptive to a users (i.e., readers) subjectiveexpectation. This also implies that it measures how accurately a model can capture the users \"evolving\"profile reflected through reading history (i.e., a temporal span of the reading and skipping actions of a useron a sequence of documents that is interleaved by the actions of generating and reading summaries). This isbecause the subjective expectation is a function of the reading history. A low degree of personalization,by definition, implies poor user experience. If a model does not efficiently capture the users profile,it may lead to summaries that contain (a) additional irrelevant information, (b) are not coherent with thereading history, and (c) do not cover all the topics of interest within the reading history. In this situation,poor UX would mean that the user would have to spend more time getting to the information he/she isinterested in or suffer from information overload and fatigue. To illustrate this, we borrow the example given by Vansh et al. (2023) where we have two readers, Aliceand Bob, having two very different reading histories over the same time span. Alice has been primarilyreading news articles mostly on the broad topic of \"civilian distress\" in the Hamas-Israeli conflict, whileBob is following up on articles related to \"war-front battles\". In a situation where both Bob and Alice comeup with an article covering both topics but rather wish to read the summary of the article instead, theirrespective expected summaries would be quite different. A non-personalized summarization model can beconsidered 100% accurate w.r.t recall and F-score-based measures (e.g., ROUGE-variants and METEOR,respectively) if it generates a summary that is a union of the expected summaries of Alice and Bob. Onthe other hand, a model can be considered 100% accurate w.r.t precision-based measures (eg., BLEU) if itcovers the intersection between the expected summaries of Alice and Bob, which will not be much since theyare quite different profiles. However, while the first situation would imply that Alice and Bob experiencesignificantly redundant content in the generated summary that would drop their overall UX, the second casewould entail that both Alice and Bob will have poor UX in terms of significant lack of content coverage.Hence, the utility of any summarization model as experienced by any user is directly associated with thesubjective UX and cannot be traded in for accuracy, which is a rather clinical scoring mechanism that doesnot truly reflect the UX of a user in terms of redundancy and coverage (among other subjective aspects suchas correctness and fluency). This phenomenon was established theoretically and empirically by Vansh et al. (2023) for the first time,thereby emphasizing the fact that accuracy measures can mislead an evaluator to select a model that haspoor UX. To address this, they proposed a novel measure, called EGISES, for personalization evaluationin summarizers. However, in this paper we establish both theoretically and empirically that if EGISES isused for personalization evaluation (i.e., a measure to understand a models capacity to engage readers interms of UX), then we can come to a rather paradoxical possibility where a model can have a high degree ofpersonalization (i.e., acceptable EGISES score) but low accuracy, and yet, by definition, that would entailhigh UX (see ). In other words, although high accuracy can lead to poor UX, the inverse(i.e., low accuracy leading to high UX) is absurd. We term this as the personalization-accuracyparadox and attribute it to the incorrect interpretation or usage of EGISES. In this paper, we proposePerSEval as a corrective measure of EGISES that resolves this paradox2. In the following section, weshow that EGISES measures responsiveness, a necessary yet distinct attribute to personalization.",
  "Personalization vs. Responsiveness": "In this section, we first distinguish responsiveness from personalization. Informally, responsiveness is thecapacity of a model to discern the differences in the profiles (i.e., reading histories) of two readers quantitativelyand accurately predict the dissimilarity in their corresponding expected summaries that is proportionate totheir profile difference. However, there can be scenarios where a model exhibits high responsiveness atthe cost of losing accuracy. To illustrate this, we continue with the example from the previous section.Suppose we observe an arbitrary model to generate two different summaries for a given news article, onefocusing on \"Israeli Prime Minister\" and the other on \"Jewish protests on war\", skipping the articles contenton civilian distress and war-front battle information. In that case, we have to conclude that the modelapparently discerned the difference between Alice and Bobs profiles, thereby predicting the proportionatedissimilarity in the expected summaries but not the expected summaries themselves. Thus, the model isinaccurate and yet responsive. Therefore, interpreting such responsiveness as personalization leads us to thepersonalization-accuracy paradox. We prove this formally in . We establish that EGISES measures how sensitive (or insensitive) a model is to the differences in thereaders subjective expectations (i.e., responsiveness) but not personalization. Therefore, EGISES can givea fairly good score to the model in the example. To elucidate this, we first define an Oracle personalizedsummarization model as follows: Definition 1. Personalized Summarization Oracle. A summarization model M,h (parameterized with) is an Oracle if for specific j-th reader profile hj (i.e., reading history) it generates an optimal summarys(di,hj) of the document di (i.e., M,h : (di, hj) s(di,hj)), where s(di,hj) suij uij; uij is the j-th readerssubjective expected summary of di and is determined by hj.",
  "We now recall the notion of insensitivity-to-subjectivity, the foundation of EGISES, as in Vansh et al. (2023):": "Definition 2. Weak Insensitivity-to-Subjectivity. A summarization model M,h is (weakly) Insensitive-to-Subjectivity w.r.t a given document di and corresponding readers j and k, if (uij, uik), ((uij, uik) Umax) ((suij, suik) > Smax), where is an arbitrary distance metric defined on the metric space M,where d, u and s are defined3, Umax is the maximum limit for ui, uj to be mutually indistinguishable, and Smax is the maximum limit for sui, suj to be mutually indistinguishable. Definition 3. Strong Insensitivity-to-Subjectivity.A summarization model M,h is (strongly)Insensitive-to-Subjectivity w.r.t a given document di and corresponding readers j and k, if (uij, uik), M,hsatisfies: (i) the condition of weak insensitivity, and (ii) ((uij, uik) > Umax) ((suij, suik) Smax). Based on this notion, Vansh et al. (2023) defined (summary-level) \"deviation\" of a model M,h. We generalizethis to our notion of summary-level Degree-of-Responsiveness (DEGRESS), the measure for responsiveness, asfollows: Definition 4. Summary-level DEGRESS. Given a document di and j-th readers expected summary uij, thesummary-level responsiveness of a personalized model M,h, (denoted by DEGRESS(suij|(di, uij))), is definedas the \" proportional divergence\" between model-generated summary suij of di for j-th user from all otheruser-specific summary versions w.r.t a corresponding divergence of uij from all other user-profiles.",
  ": Existence of EGISES Personalization-Accuracy Paradox: High (and same; (a)) DEGRESS,yet low accuracy (red line; (b))": "Here, |D| is the total number of documents in the evaluation dataset, |U| is the total number of userswho created gold-reference summaries that reflect their expected summaries (and thereby, their subjectivepreferences), and |Udi| (= |Sdi|) is the number of users who created gold-references for document di. w isthe divergence of the model-generated summary suij (and the corresponding expected summary uij) fromdocument di itself in comparison to all the other versions. It helps to determine how much percentage(therefore, the softmax function) of the divergence (i.e., (suij, suik) should be considered for the calculationof DEGRESS. If suij is farther than suik w.r.t di then DEGRESS(suij|(di, uij)) < DEGRESS(suik|(di, uik)), implyingthat M,h is more responsive to the k-th reader. A lower value of DEGRESS(suij|(di, uij)) indicates that whilereader-profiles are different, the generated summary suij is very similar to other reader-specific summaries(or vice versa), and hence, is not responsive at the summary-level. The system-level DEGRESS and EGISEShave been formulated as follows:",
  "|D|;EGISES(M,h) = 1 DEGRESS(M,h)(2)": "EGISES measures the degree of insensitivity-to-subjectivity for relative benchmarking of how much modelslack personalization (i.e., a lower score is better within the range ) instead of assigning an absolutegoodness score. As can be noted, the EGISES formalism does not enforce any penalty on accuracydrop. Here, accuracy would be an inverse function of (suij, uij)|di for the same metric distance thatDEGRESS uses. Hence, EGISES (and DEGRESS) should be interpreted as a measure of responsiveness (i.e.,proportionate divergence) and not personalization.",
  "Theorem 1. The accuracy f 1((su, u)) of a model M,h on the metric space M with distance function(, ) can vary keeping DEGRESS(su|(d, u)) constant": "Proof. We follow the same triangulation proof technique as in (Vansh et al., 2023). Let d, u and s betriangulated as per . Keeping d and u fixed, we can perform an arbitrary rotation operation with das center (rot(, d, ); : angle of rotation) on suij and suik s.t. rot(, d, ) is a closure operator in M. Now,(p, q) M, s.t.",
  "And similarly, maxq(rot(suik, di, q), uik) > minq(rot(suik, uik, q), uik)": "If p = q, then DEGRESS(sui|(di, ui)) (and therefore, EGISES) remains unchanged for a given di. However,due to the existence of a total ordering of accuracy f 1((s, u)), for any arbitrary , the accuracy, therefore,can be varied by changing from a minima to a maxima (f 1((s, u)) ). The proof establishes the theoretical existence of the personalization-accuracy paradox if we interpret EGISESas a measure of personalization instead of responsiveness. It sets the motivation to design a correctivemeasure for EGISES that truly measures personalization. As discussed in the introduction, Vansh et al.(2023) proposed a follow-up measure called P-Accuracy (P-Acc), that might seem to solve this problem.However, some major limitations of P-Acc prevent it from being used for evaluating degree-of-personalization.We elucidate that in the following section.",
  "(3)": "Here, (0, 1] calibrates the importance to be given to lack of personalization in a model while is the control that helps the human-evaluator to regulate the final penalty to accuracy due to lack ofpersonalization. According to P-Acc, it apparently seems that there cannot be a situation where if bothaccuracy and EGISES scores are low (i.e., DEGRESS is high), the final score will be high, thereby preventingthe EGISES Paradox. However, three limiting properties of P-Acc render it unusable for personalizationevaluation. We prove their existence as follows: Theorem 2 (Limitation 1: Lack of Interpretability of Score). There exists infinite possible model collections = {i | i = {Mi,h}} where (k, l) pair : P-Acc(Mik,h) = P-Acc(Mil,h) when (i) ScoreAcc(Mik,h) ScoreAcc(Mil,h), (ii) ScoreAcc(Mik,h) ScoreAcc(Mil,h), and (iii) ScoreAcc(Mik,h) = ScoreAcc(Mil,h).",
  ", , +, s.t. ScoreAcc(M,h) 0": "The proof implies that P-Acc can take a negative value even for a reasonably high accuracy model. Asan example, for = 0.5, = 1 (i.e., full importance to lack of personalization), EGISES(M,h) 0.9 ifScoreAcc(M,h) 0.35 then P-Acc < 0.Theorem 4 (Limitation 3: Anomalous Behavior). There exists anomalous condition when P-Acc(Mik,h) <P-Acc(Mil,h) even when EGISES(Mik,h) < EGISES(Mil,h) (NT: lower is better).",
  "(ScoreAcc(Mik,h), ScoreAcc(Mil,h)) which is a possible condition": "A bounding case of the proof condition is when for ScoreAcc(Mik,h)=ScoreAcc(Mil,h)=0, EGISES(Mik,h) = 0 (best case)=P-Acc(Mik,h) = 1 (worst case), and EGISES(Mil,h) =1 (worst case)=P-Acc(Mil,h) = 0.37 (better case).Hence, P-Acc is not suitable for modelsthat have relatively low accuracy scores. The fact that P-Acc is useful to regulate accuracy (and not measurepersonalization) is evident from in Appendix C.2 where we observe that, unlike PerSEval-scores asoutlined in section 8.1, P-Acc scores are heavily dominated by the evaluated PENS models accuracy scoresand therefore, do not differ significantly even though they lack responsiveness (i.e., low EGISES scores; see in Appendix C.2). As a remedy, we propose PerSEval (Personalized Summarizer Evaluator) in thenext section.",
  "PerSEval: Measure for Personalization": "The design objective of PerSEval is two-fold: (i) to ensure that a model is penalized for poor accuracyperformance, but at the same time, (ii) to ensure that the evaluation of responsiveness (i.e., DEGRESS) is notobfuscated by high accuracy (since high accuracy does not entail high responsiveness as proved in Vansh et al.(2023)). In other words, the underlying maxim behind the design of PerSEval is accuracy is not a reward,but lack of it is surely a penalty! We term this penalty as Effective DEGRESS Penalty Factor (EDP). As perthe design maxim, if accuracy is 100%, then there will be no EDP applied, and the PerSEval score will be thesame as the DEGRESS score. In the subsequent sections, we develop the motivation and formulation of EDP.",
  "Accuracy-drop Penalty (ADP)": "In this section, we introduce the first component of EDP - Accuracy-drop Penalty (ADP). ADP is a document-levelpenalty due to a drop in accuracy for the best-case scenario where a model-generated summary of documentdi (suij) is closest to the corresponding readers expected summary uij. In this case, we denoted suij as sui.We define ADP as follows: Definition 5. Accuracy-drop Penalty. Given document di and user-generated summaries Udi, thedocument-level ADP of a model M,h, denoted by ADP(sui*|(di, ui*)), is the relative deviation of the bestperformance ((sui, ui)|di) of M,h (sui, ui)|di from the best possible performance (i.e., 0) w.r.t itsproximity to the worst possible performance (i.e., 1).",
  "(4)": "Here, ADP is defined as a shifted sigmoid function where 4 helps to bring the minimum penalty to zero,while the factor of 10 in the exponentiation ensures that the maximum penalty reaches 1 when the ratio of thedifference in the best case to the worst case is around 1.5 before it starts exploding (i.e., over-penalization).ADP ensures that even if the DEGRESS score is acceptable, a penalty due to accuracy drop can still be imposedas a part of EDP. ADP, however, fails to address the scenario where the best-case scenario is acceptable (i.e.,accuracy is fairly high) but is rather an outlier case i.e., for most of the other model-generated summaryversions, there is a considerable accuracy drop. To address this issue, we introduce a second penalty componentwithin EDP called Accuracy-inconsistency Penalty (ACP).",
  "Accuracy-inconsistency Penalty (ACP)": "ACP accounts for outlier conditions of the best performance, as explained previously. It evaluates how amodel performs w.r.t accuracy for a specific generated summary compared to its average performance. Moreformally, it is defined as follows: Definition 6. Accuracy-inconsistency Penalty. Given document di and user-generated summaries Udi,the summary-level ACP of a model M,h, denoted by ACP(suij|(di, uij)), is defined as the relative deviation ofthe summary performance (suij, uij)|di of M,h from its best performance (sui*, ui*)|di as compared tothe deviation of its average performance (sui, ui)|di from (sui*, ui*)|di. It is to be noted that, unlike ADP, ACP is a summary-level measure. This penalty evaluates if the modelconsistently performs w.r.t accuracy and therefore, conversely, does not inject any additional penalty toDEGRESS when a model is consistent. The summary-level ACP is formulated as:",
  "PerSEval: Formulation": "We now lay the design of the PerSEval framework as an extension to DEGRESS (i.e., 1EGISES). A multiplicativeinjection of EDP (0, 1] should be such that the best accuracy (i.e., ADP = 0) with no inconsistency (i.e.,ACP = 0) would lead to an EDP value of 1, and thereby, DEGRESS remains unobfuscated (which is the desired",
  "|D|(7)": "We design EDP as an inverse sigmoid function of the overall DEGRESS Penalty (DGP), which sums up ADPand ACP. and are hyper-parameters that help to control the shape of EDP. 3 ensures that EDP is1 (i.e., 1 0) when there is no penalty, thereby making PerSEval equivalent to DEGRESS. 1 ensuresthat the function does not drop sharply, thereby over-penalizing (and hence, dampening) an otherwise fairlygood DEGRESS score (i.e., responsiveness). Since may significantly affect the overall human-judgmentcorrelation, we did an ablation study (Fig 3; .1) to find the optimal value (which was observed to be1.7). The system-level PerSEval and is bounded by the system-level DEGRESS score.",
  "Model Benchmarking Dataset": "Microsoft PENS Dataset (News Domain).Our study, as in (Vansh et al., 2023), assesses models usingtest data from the PENS dataset provided by Microsoft Research (Ao et al., 2021)4. This dataset pairs newsheadlines with articles, serving as concise summaries. The test set creation involved two phases: initially, 103English speakers selected 50 articles of their interest from a pool of 1000, sorted based on exposure time.In the second phase, participants generated preferred headlines (gold references) for 200 articles withoutknowledge of the originals. The assignment ensured an average of four gold-reference summaries per article.The PENS dataset was chosen because it is the only one that contains the users reading history, i.e., thetemporal sequence of interactions (clicking, reading, and user-generated gold summaries), making it ideal forevaluating five SOTA personalized summarization models that require user reading history as an input. OpenAI CNN/Daily Mail Dataset (News Domain).To understand the applicability of PerSEvalon mainstream gold-standard news datasets, we design an indirect evaluation methodology with theOpenAI CNN/DM dataset (validation and test) released by Stiennon et al. (2020). The test dataset contains639 articles processed by 19 policies, resulting in 5515 summaries rated by the same annotators on accuracy,coherence, coverage, and overall quality using a 7-point Likert scale. However, unlike the PENS dataset,this dataset lacks the temporal span of human evaluator interactions forming the reading historyrequired by the PENS summarization models. Additionally, we do not have gold-reference summaries (butonly ratings).5 Hence, to conduct our experiments, we evaluated the personalization of 4 (out of 19) OpenAIpolicies, focusing on RLHF-tuned policies, as base-model and SFT-based policies are non-personalized. Sincethe policy-generated summaries are not specific to any human annotator as the policies are nottrained on any reading history, we do not observe multiple user-specific summaries from the samepolicy - a necessary condition for evaluating personalization. To address this, we concatenate all summariesgenerated by different policies (say, k such summaries) that were rated above 6 (out of rmax = 7) by aspecific annotator (say, Alice) into one combined gold-reference summary as per Alice (uAlice). After this 4We comply with the Microsoft Research License Terms.5It is to be noted that human-written gold reference summaries were not always selected or rated as highest by the humanannotators, clearly indicating the subjectivity of saliency. step, the content of each of all the policy-generated summaries (s) that received some rating ([1, rmax]) fromAlice (rAlice(s) r(s,Alice)) is then concatenated (k + (rmax r(s,Alice)))-times to create a policy-generatedsurrogate personalized summary (sAlice) for Alice. Ideally, this surrogate should match Alices combinedgold-reference uAlice. If r(s,Alice) is much lower than rmax, then the surrogate should get penalized. Theinjection of redundant, unmatchable content helps to achieve this. We repeat this process for all annotatorsand generate the PerSEval scores of each policy across 27 (out of 639) articles. Since k and r(s,) can differbetween annotators, the penalty (and thereby length) of the surrogates will subjectively differ as well. OpenAI TL;DR (Reddit) Dataset (Open Domain).To understand the broader applicability ofPerSEval, we also appropriated the OpenAI TL;DR dataset Stiennon et al. (2020). This dataset is a collectionof 123,169 Reddit posts adopted from the dataset by Vlske et al. (2017) covering 29 subreddits (i.e.,domains) of varied types, their corresponding OpenAI policy-generated summaries, and the human-writtengold-reference summaries. A subset of the validation dataset comprises 1038 posts that were fed into 13policies to generate 7713 summaries. Like the OpenAI CNN/DM dataset, 32 human annotators rated thesummaries in the same format as in the previous OpenAI CNN/DM dataset. As a part of the appropriation,we carry the same method as that with CNN/DM leading to the evaluation of 4 RLHF-tuned policies (outof 13) and 57 (out of 1038) annotated articles. However, we would like to emphasize that this indirectbenchmarking is neither an equivalent evaluation methodology to the PENS dataset based one, noris a simulation of the required setup. There is so far no other dataset comparable to PENS for evaluatingpersonalization, and hence, all benchmarking results in the outlined method should be seen as baselinewithin the scope of the method.",
  "SOTA Summarization Models Evaluated": "Personalized Summarization Models (Microsoft PENS dataset). We study ten SOTA summarizationmodels for comparative benchmarking as in (Vansh et al., 2023). Five of them are specifically trainedpersonalized models and follow the PENS framework (Ao et al., 2021): PENS-NRMS Injection-Type-1(T1)/Type-2 (T2), PENS-NAML T1, PENS-EBNR T1 and PENS-EBNR T2.The others are genericSOTA models - BRIO (Liu et al., 2022), SimCLS (Liu & Liu, 2021), BigBird-Pegasus (Zaheer et al., 2020),ProphetNet (Qi et al., 2020), and T5-base (Orzhenovskii, 2021). The selections are based on their consistenttop-5 ranking over the preceding four years on the CNN/Daily Mail news dataset Hermann et al. (2015).Appendix A contains model descriptions. Non-personalized Summarization Models (Microsoft PENS dataset). For the non-personalizedmodels, we follow the evaluation setup used by Vansh et al. (2023) by augmenting the documents withthe reference summaries of each reader as document titles (i.e., cues). This results in subjective documentversions corresponding to each reader. The models ideally should pick up the cues and generate them back asan output, thereby inducing an apparent sense of personalization. This injection process provides robustbaselines for comparative evaluation. RLHF-tuned OpenAI Summarization Policies (OpenAI CNN/DM & TL;DR (Reddit) datasets).We analyze five OpenAI policies (parameter size 1.3B and 6.7B) that have been RLHF-tuned using TL;DR(Reddit) training datasets. The base pre-trained model architecture is similar to that of GPT-3. The basemodel is then further fine-tuned in a supervised setup on a filtered set of the Reddit posts to generateSFT-models (denoted \"sup\"). The Reward Model (denoted \"rm\") is a linear head that predicts the rating.The Reinforcement Learning of the policy model is done via the PPO algorithm Zheng et al. (2023b).",
  "Baseline Distance Metrics and Scores": "PerSEval is a generic measurement framework (like EGISES) where the specific metric space M on which isdefined should be appropriately selected such that we achieve the best human-judgment (HJ) correlation. Inthis paper, we choose seven summarization accuracy metrics that are defined on standard algebraic spaces andplug them in PerSEval in isolation as distance metrics (i.e., ) : (i) ROUGE (RG)-L, (ii) ROUGE (RG)-SU4,",
  "Models (i.e., Policies)PSE-RG-LPSE-RG-SU4PSE-METEORPSE-BLEUPSE-JSDPSE-BScorePSE-InfoLM-": "sup4/rm4 (6.7B)0.943/0.7190.941/0.7300.906/0.5250.887/0.3950.943/0.7320.998/0.7190.999/0.836sup4/rm4-t.7 (6.7B)NA/0.711NA/0.648NA/0.593NA/0.465NA/0.737NA/0.790NA/0.818sup4/rm4 (1.3B)0.617/0.5230.617/0.5020.413/0.3570.612/0.1630.617/0.5660.994/0.7950.999/0.605sup4/rm4-kl14 (6.7B)0.783/NA0.783/NA0.562/NA0.592/NA0.783/NA0.884/NA0.783/NAsup4/rm4-t.7 (1.3B)0.505/0.4810.5/0.4140.267/0.4170.266/0.3230.502/0.5230.92/0.7760.646/0.602 : SOTA model-benchmarking on Microsoft PENS and OpenAI CNN/DM & TL;DR (Reddit) datasetsw.r.t PerSEval (PSE) (NA: not applied on the dataset); PerSEval hyper-parameters: = 3, = 1.7(optimal ; see 3), and = 4. Observation 1: Disagreement between keyword-based (RG-L, RG-SU4,METEOR, BLEU) and non-keyword-based PSE variants (JSD, BScore, InfoLM) in terms of non-personalizedbaseline leaderboard from rank-3 onward; Observation 2: InfoLM- has more reliable discriminatoryperformance with less sharp changes for PENS dataset; Observation 3: Specialized personalized modelslack personalization w.r.t all variants of PerSEval; Observation 4: RLHF-tuned LLM-based policies exhibitmuch higher personalization baseline (i.e, baseline-II) compared to non-personalized specialized summarizationmodels (i.e., baseline-I) within the limited scope of the surrogate-based indirect evaluation; Observation 5:PSE-scores are very high for top-performing OpenAI policy due to most surrogate summaries being highlyrated and very similar to the combined gold-references. (iii) BLEU-1, (iv) METEOR,6 (v) BertScore (BScore) defined on embedding space, (vi) Jenson-ShannonDistance (Menndez et al., 1997) on probability space, and (vii) InfoLM- ( = 1; = 1) on probabilityspace generated from the embedding space of a masked-LM. RG is chosen because it has a very high HJ(Pearson & Kendall) correlation (> 0.7) in most standard datasets such as CNN/DM and TAC-2008 (forRG-L) as reported in (Bhandari et al., 2020; Zhang et al., 2024), and DUC-2001/2002 (for RG-L/SU-4) (Lin,2004). For the same reason, the variant of InfoLM is chosen. Comprehensive benchmark results w.r.toptimal PerSEval hyperparameters (i.e., = 3, = 1.7, = 4; see for ablation results) for each ofthe variants are given in . We observe that most non-personalized models, such as BigBird-Pegasus,produce significantly stronger baselines across most PerSEval variants. However, keyword-based PSE variants(RG-L, RG-SU4, METEOR, BLEU) disagree with non-keyword-based PSE variants (JSD, BScore, InfoLM)in terms of the non-personalized baseline leaderboard from rank-3 onward. One reason for this is that JSD,BScore, and InfoLM being embedding-based can handle out-of-distribution (OOD) situations leading tolesser penalty for keyword-level mismatch that happens in abstraction summarization. We also find that theInfoLM- variant shows \"smoother discrimination\" (i.e., no sharp jump in the performance) when comparedto RG-SU4, BLEU, and JSD. At the same time, the discriminatory performance of InfoLM- and JSDvariants is much better than BScore, METEOR, and RG variants, leading to a more reliable leaderboard.The results on the OpenAI datasets should be seen within the context that: (a) the models are essentiallypolicy variants that differs w.r.t size and specific PPO implementation, and (b) the indirect benchmarkingmethod is limited (as outlined in section 6.2). 6First four defined as similarity functions on the string space while BScore is defined as similarity function (as opposed todistance function) on high dimensional embedding space; see Appendix B.1 for details on each of the seven measures.",
  "Meta-evaluation of PerSEval: Experiment Design": "In this section, we lay down the experiment design that forms the foundation to establish (i) the reliability ofPerSEval w.r.t human-judgment correlation, (ii) the stability of PerSEval, (iii) PerSEval as a rank-measureis not entailed by DEGRESS-rank (i.e., the EGISES paradox is empirically existent), and (iv) PerSEval canbe a standalone rank-measure without the need of any aggregated ranking.",
  "Meta-evaluation of PerSEval Reliability: Survey-based Direct Method": "Meta-evaluation Objective. As pointed by Vansh et al. (2023), unlike the meta-evaluation of accuracymeasures, direct meta-evaluation of personalization is not a feasible task since that would require humanevaluators to work as a team and to understand how their subjective assessment of the PerSEval scores(i.e., to what extent they agree with the scores) corresponds with the differences in the model-generatedsummaries and their own subjective expected summaries. As an alternative, we propose a survey-basedevaluation methodology to simulate this scenario. We argue that the meta-evaluation of PerSEval shouldhave two objectives: (i) whether human evaluators would judge the responsiveness of models in the same\"ratio-way\" as what DEGRESS does, and (ii) whether human evaluators would apply the accuracy drop to theresponsiveness in the same \"factor-way\" as what PerSEval does. In other words, the central objective is tovalidate to what extent human evaluators agree with the design principles of PerSEval at a cognitivelevel. Participants. We opened the survey to a selected pool of graduate students demonstrating fairEnglish comprehension. The pool consisted of students from five different backgrounds: (i) computer science,(ii) electrical engineering, (iii) humanities & social sciences, (iv) mathematics, and (v) physics. The surveywas promoted in ongoing courses, student associations, and social media groups. 169 students (45% male,55% female) within the age group of 25-40 completed the survey. No other personal details were asked. Survey Procedure. Each participant was shown a pair of gold-reference summaries (corresponding to twouser profiles) for a specific news article from the PENS dataset. Along with this, five pairs of model-generatedsummaries were shown, each pair corresponding to five of the ten models studied in this paper (i.e., twoparticipant responses covered all the ten models for a given document). To eliminate response bias, theparticipants were unaware of which of the six pairs was a gold-reference, and the model names were also notrevealed. The same set (shuffled) was shown to two random participants to get an average. Each participantwas asked to provide similarity ratings between 1 (low) and 6 (very high) for the summary pairs. A samplesnapshot questionnaire is provided in Appendix D (figure 4). Modeling Human-judgment of Personalization (PerSEval-HJ). We model a \"human version\" ofDEGRESS (termed DEGRESS-HJ) using the normalized rating as the divergences (i.e., (uij, uik) and (suij, suik)for document di and gold-references uij and uik). As mentioned above, if PerSEval needs to be reliable, thenthe necessary condition is that DEGRESS should have a high correlation with DEGRESS-HJ, failing which it canbe concluded that human evaluators do not interpret divergences in the ratio-way of DEGRESS. We use thestandard Pearsons coefficient (r), Spearmans , and Kendalls rank coefficients for this. As a sufficientpart of the reliability test, we model the \"human version\" of EDP using standard accuracy measures (i.e.,RG-L, RG-SU4, METEOR, BLEU, and InfoLM-) as surrogates. The motivation behind this is that suchmeasures are known to have high HJ-correlation. The objective was to check whether such a surrogate, if usedas a factor (just as in PerSEval) with PerSEval-HJ, shows a high correlation with PerSEval, failing whichimplies that human evaluators do not cognitively resonate with this factor-styled discounting of DEGRESS",
  "Meta-evaluation w.r.t Stability": "PerSEval, being a rank measure, is said to be stable if, for any random sample of document (and correspondinggold-references) selected from the evaluation dataset, the rank of the evaluated models does not change.This meta-measure is important because it objectively checks if PerSEval can be relied on any arbitrarypersonalization evaluation dataset, unlike the PerSEval-HJ based reliability evaluation, which is subjectiveand indirect in nature. In order to establish stability, we need to define it w.r.t a specific sampling method. Sampling Method for Stability Meta-evaluation. To understand the stability of PerSEval, we createrandom sample collections Ck(D,S,U), where k = {80%, 60%, 40%, 20%} of the PENS dataset and N = |(D, S, U)|.",
  ": PSE-ILM Ablation: Effect of on HJ-Corr; Optimal performance at = 1.7 across all threestandard correlation measures (Pearson r, Spearman , Kendall )": "Each collection has ten random sample sets nki N; i = [1 : 10] (with replacement). We benchmark themodels on all 40 sample sets to obtain corresponding leaderboards. We compare that with the rank obtainedfrom the entire dataset. We formalize this notion of stability of a rank measure as follows: Definition 7. Weakly Stable Rank Measure. A rank measure is -weakly stable if the maximum rank-correlation (w.r.t stat. ) between the measure-generated model-ranking on each nki and model-ranking on theentire dataset N is . Definition 8. Strongly Stable Rank Measure. A rank measure is -strongly stable if (i) it is -weaklystable, (ii) the bias over C(D,S,U) w.r.t the mean score of each Ck(D,S,U) for each evaluated model is b, and(iii) the variance over C(D,S,U) w.r.t the expected variance of the scores of each Ck(D,S,U) for each model is var; = max(b, var).",
  "Meta-evaluation of PerSEval: Results": "Reliability of PerSEval.We compute the HJ-correlation of the seven variants of PerSEval w.r.t to eachof the five PerSEval-HJ variants (RG-L, RG-SU4, METEOR, BLEU, and InfoLM-; see for theresults7). An 11-point hyper-parameter ablation study shows that the optimal correlation is at = 1.7 (). We observe that PerSEval=1.7-InfoLM- has the overall best correlation across all five PerSEval-HJvariants. We further observed that: (a) PerSEval-HJInfoLM as a human-judgment estimate has thebest performance of each PerSEval-variants (Pearsons r = 0.79; Spearmans = 0.68; Kendalls = 0.47w.r.t PSE-HJInfoLM-), and (b) PerSEval-InfoLM- performs the best across. The high performance ofPerSEval=1.7-InfoLM- can attributed to InfoLM-, as an accuracy (and thereby, distance measure)having a very high system-level HJ-correlation (Pearsons 0.95; Kendalls 0.85) when compared tothe other measures on the CNN/DM dataset, as reported in Colombo et al. (2022b). Indirect Evaluation Method is Inadequate for Meta-evaluation.As discussed in section 6.2, themethod of evaluating personalization of RLHF-tuned policies is indirect and limited in scope, since thesepolicies are trained on annotator profile agnostic ratings and not on their reading history. Hence, althoughthe OpenAI dataset may be used for generating baselines in this limited setup, they are not suitable formeta-evaluation of any personalization measure. To validate this point, we design an OpenAI dataset-oriented PerSEval-HJ variant where we model the divergence between policy-generated surrogate summaries",
  "Kendalls 0.24/ 0.67/-0.330.51/ 0.67/-0.330.42/ 0.67/-0.330.47/ 0.33/-0.330.45/ 0.67/-0.670.38/ 0/-0.330.42/ 0.33/-0.33": ": PerSEval (PSE) Reliability: Human-judgment (HJ) corr. between PSE=1.7-X and PerSEval-HJ-X on Microsoft PENS/OpenAI CNN-DM/OpenAI TL;DR (Reddit) datasets. Observation 1: PerSEval-InfoLM- has maximum highest HJ correlation across all PerSEval-HJ variants on PENS dataset; Obser-vation 2: PerSEval-HJ-InfoLM- and PerSEval-HJ-BScore are optimal meta-evaluation measures sincethey are most closely aligned to all the seven PerSEval variants; Observation 3: Difference in humanannotator ratings does not serve as reliable surrogate for estimating DEGRESS-HJ (and thereby, PerSEval-HJ)as evident from contradictory results on structurally equivalent OpenAI CNN/DM and TL;DR (Reddit)datasets; Observation 4: As a follow-up, the indirect evaluation method can serve for baseline generationbut not meta-evaluation. ((suij, suik)) with the difference in the ratings given by the annotators (i.e., |r(suij) r(suik)|) and thatbetween combined gold-references ((uij, uik)) with the difference in the average ratings of all summariesincluded in the combined gold reference (i.e., |r(uij) r(uik)|). For calculating penalty due to EDP, we takethe distance between gold-reference and generated summary to be |r(suij) r(uij)). We then create sevenvariants of PerSEval-HJ-X as in the PENS dataset-based evaluation, where X stands for the measure usedfor estimating the DEGRESS normalization factor (i.e., the distance of sui and ui from the document di; seeequation 2). It is evident from that the HJ-correlation so computed is unstable since the results arecontradictory even though both the OpenAI datasets are structured in exactly the same manner with fourcommon policies. This shows that the difference in rating cannot be an alternative to the direct similarityjudgment that was collected via the survey.",
  ": Accuracy-leaderboards may mislead: HJ-Corr. of Borda-Kendall (BK) consensus-basedaggregated rank vs. PSE=1.7-InfoLM (ILM)-": "Evidence of the EGISES Paradox.We look at the inter-correlations between EGISES-rank andPerSEval-rank of the selected models on the PENS dataset. The values less than 1 across all the variants(see ) suggest that PerSEval is not just an offset of EGISES and is not entailed by it (which wouldhave otherwise been the case if the EGISES paradox was non-existent in reality). Stability of PerSEval.We compute --stability of the best performing PerSEval-InfoLM- on PENSover the ten models as per the sampling method and stability definitions in .2. We observe PerSEvalto be 0.00248-strongly-stable w.r.t Spearman- = 1 and Kendall- = 1 (see ). This establishes a veryhigh stability of PerSEval along with its reliability, making it robust. For detailed results, see Appendix C.1.",
  "Accuracy Leaderboards may Mislead": "We demonstrate that accuracy leaderboards, at best, are redundant and PerSEval-rank is sufficient to capturepersonalization. For this, we generate the Borda-Kendall consensus-based aggregated rank (Colombo et al.,2022a) and compare the HJ-correlation with that of PerSEval-InfoLM-. We observe that the stand-aloneHJ correlation has the same strength w.r.t the aggregated rank for accuracy measures like BLEU, therebyrendering them redundant in the context of personalization evaluation, while is significantly higher (Spearman: 0.51+; Kendall : 0.40+) than that of measures such as RG-L, InfoLM- (see ). This indicatesthat accuracy ranking can also inject noise.",
  "Related Work": "Evaluation of PersonalizationPersonalization evaluation has been well studied in recommendationsystems (recsys) (Zangerle & Bauer, 2022), such as metrics based on the Jaccard Index, rank-order editdistance (Hannak et al., 2013), MAE/RMSE/Hit-Ratio (Li et al., 2024), and nDCG (normalized DiscountedCumulative Gain) (Matthijs & Radlinski, 2011). A comprehensive compilation of all recsys-oriented metricsand their applications can be found in (Zangerle & Bauer, 2022; Kuanr & Mohapatra, 2021). While relevant torecsys, these metrics are not pertinent for text summarization since they rely on human feedback (such as clicksand likes) on a rank list of potentially preferable items\" a situation that does not exist for summarizers. Asurvey-based qualitative analysis of the usefulness of model-generated summaries was proposed by Ter Hoeveet al. (2022). Although this work establishes empirically that model-generated summary utility is subjective(as argued in this paper), yet to date, the only work on the formal quantitative evaluation of personalizationin summarization models is EGISES (Vansh et al., 2023) which, however, can only capture responsiveness.There is a growing interest in learned summarization evaluation metrics, as opposed to designed ones Peyrard& Eckle-Kohler (2017); Peyrard & Gurevych (2018); Bhm et al. (2019); Liu et al. (2023). LLM-basedautomated summarization evaluation has also been studied Shen et al. (2023); Zheng et al. (2023a). However,to the best of our knowledge, these works have not focused on evaluating personalized summarization modelswhere subjectivity in saliency needs to be respected (and not normalized). Any such automated evaluatorneeds to be fine-tuned on large volumes of user profiles having varied subjective expectations. This sort ofdataset is hard to design, and such a project only makes sense if designed metrics are not able to achieveminimum human-judgment correlation. 9In this paper, X: RG-L, RG-SU4, METEOR, BLEU, JSD, BScore (we assume pre-computed embeddings), InfoLM- whichuses BERT Base (uncased; 110M params) as pre-trained Masked Language Model.10System specifications: Machine architecture: x86_64; CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz; CPU Cores: 16;Thread(s) per core: 2. Personalized SummarizationAspect-based. An aspect-based summarization model generates sum-maries that are coherent with the aspects (i.e., themes/topics) therein (Narayan et al., 2018; Frermann &Klementiev, 2019; Tan et al., 2020; Hirsch et al., 2021; Hayashi et al., 2021; Meng et al., 2021; Soleimaniet al., 2022). While explicit aspects can be restrictive and rather broad (e.g., the MA news training datasetwhere six broad aspects are identified: sport\", health\", travel\", news\", science technology\", tv showbiz\"),implicit aspect-based summarization implies augmenting the aspect query with concepts that are related tothe predefined aspects. Although these models have specific use-cases, they are not trained to adapt to thereaders evolving profile (i.e., reading behavioral pattern) that constitutes discourse-level interest drift (andnot just topic-level static interests). Also, the evaluation was w.r.t accuracy using standard ROUGE-variants. Interactive Human-feedback-based. One of the earliest interactive interface-based iterative personalizedsummarization frameworks was proposed by Yan et al. (2011) where users could click on specific sentencesin the generated summary that are of their interest (implicit preference), and read the associated context(i.e., surrounding text) of the selected sentence before sending this preference as feedback to the model for arevised version. A similar interactive interface-based framework was proposed by PVS et al. (2018) whereusers can iteratively select sentences and the phrases therein that they prefer (and do not prefer as well),while the summarizer, an ILP-based model proposed by Boudin et al. (2015), updates the summary based onthis feedback until the user is satisfied. On similar lines, Ghodratnama et al. (2021) introduced a personalizedsummarization method for extractive summarization. Extracted summary concepts are presented to readersfor their feedback, which is used to iteratively fine-tune the summary until no further negative feedback isreceived. Bohn & Ling (2021) proposed a framework where the acceptance or rejection of summary sentenceswas made dynamic as the summary gets generated on-the-fly. However, all these works have been evaluatedbased on standard accuracy evaluations (such as ROUGE variants). User-preference Trained Reward Model-based. Another way of inducing personalization in models isto train a base model (pre-trained or supervised fine-tuning) as an agent within a reinforcement learningframework (usually policy-gradient based) using a reward model (RM) as the environment. The RM istrained on human preferences to predict human ratings for specific actions of the agent (i.e., selecting specificwords/sentences), thereby providing the rewards (Stiennon et al., 2020; Nguyen et al., 2022). However,whether these models can explicitly remember\" individual preferences (or even that of user groups withsimilar interests) is still to be probed and not quite clear. Nevertheless, the evaluations were on accuracy only. User-preference-history-based. There has been considerable work in user preference-history-based productreview summarization. The user profile is usually modeled in terms of discrete attributes such as rating, user-ID, and product-ID, and history text that represents user-written historical review-summaries. Some of theproposed models use user-specific vocabularies to predict user-preference words that in turn serve as a guidefor generating summaries Ma et al. (2018); Li et al. (2019); Chan et al. (2020). Others have proposed modelsthat learn user preference by jointly training these discrete attributes and the historical review-summariesLiu & Wan (2019); Xu et al. (2021; 2023), where the historical summaries provide information about thewriting-style, purchasing preferences, and aspect-of-interest of the users. These works, however, do not fitinto the general setup of personalized summarization because the summaries generated are not aligned to aprospective buyers (i.e., a consumer of review-summary or reader in our parlance) preference behavior, butrather tuned to a different set of buyers who are active reviewers and who have provided gold-reference reviewsummaries of their own reviews (i.e., the review-to-summary is a one-to-one mapping and not subjective).Nevertheless, the evaluation has been done using accuracy measures (ROUGE variants) only. So far, the onlypertinent work incorporating the readers history as preference is the proposed models that were designedusing the PENS framework (Ao et al., 2021), which we studied extensively (see .2). It is clear fromour study that they need significant improvement in terms of personalization (as measured by PerSEval).",
  "Conclusion": "In this paper, we presented PerSEval, a corrective measure for EGISES (proposed by Vansh et al. (2023)),which, to the best of our knowledge, is the only known personalization measure for summarizers. Wefirst introduced the concept of responsiveness, in contrast to personalization, as a measure to evaluatethe capacity of a model to discern the differences in reader profiles (i.e., reading histories) and generate reader-specific summaries that maintain this difference proportionately. We then showed that EGISESmeasures the former. We thereby proved theoretically and empirically on the real-world PENS dataset thatmeasuring responsiveness does not imply measuring personalization since there can be models that generatedistinctly different summaries for different reader profiles (i.e., high responsiveness) but are quite off from theexpected summaries (i.e., low accuracy and thereby low user-experience (UX)). We then formulated PerSEval(more specifically, DEGRESS) as a discounted EGISES where the discount factor is a penalty due to accuracydrop called EDP (Effective DEGRESS Penalty Factor). We analyzed the ten SOTA summarization modelsusing seven variants of PerSEval and observed that the model leaderboard reliability depends on the chosenvariant. We further observed that the variant PerSEval-InfoLM- performs best regarding rank-stability, ameta-evaluation measure we proposed in this paper. We also proposed a novel survey-based meta-evaluationprotocol for human-judgment (HJ) to analyze the extent to which human annotators agree with the designprinciples of PerSEval at a cognitive level. We found that PerSEval-InfoLM- has the highest overallHJ-correlation (Pearsons r = 0.79; Spearmans = 0.68; Kendalls = 0.47). We finally established thatseparate accuracy leaderboards for personalized summarizers can be misleading and PerSEval can serve as aunified measure, thereby emphasizing that personalization and accuracy are inseparable aspects of UX.",
  "Discussions & Future Directions": "User guideline: PerSEval as a measure is useful in summarization tasks where the user experience of theutility of the summaries generated by any model will be subjective and will depend upon the users currentpreferences (which itself is a reflection of past activities). For example, executive summaries and Minutes-of-meetings (MoMs) need to be personalized (as different participants might have different takeaways/todosfrom the meeting summary) without compromising on the accuracy of the proceedings. In the same way,news recommendation platforms are also likely to benefit from personalized summarization as it increases thelikelihood of the article being clicked and read in full if the title or summary is more personalized as per theuser profile. In such use cases, PerSEval should be adopted rather than accuracy measures or P-Accuracy. Itis important to note that we do not rule out existing conventional accuracy measures and also P-Accuracy (asproposed in Vansh et al. (2023)) in favor of PerSEval. These measures have their own important role to playfor different use cases. Specifically, summarization tasks that do not need personalization should be betterevaluated using standard accuracy measures. An example would be the research paper summarization taskwhere saliency, generally, should imply the core contributions of the paper, and hence, the user experience ofsuch summary utility does not depend on specific user preference history. Likewise, there are use cases whereP-Accuracy can be more useful, especially when accuracy is the primary objective, yet summarization modelsto be evaluated also claim to be personalized. An example would be the task of multi-document updatesummarization for very specific like-minded stakeholders (say, members of a government event-monitoringunit engaging in daily tweets) whose topics of preferences are already known. In this case, P-Accuracy willfactor in the lack of personalization (if any) and provide a more realistic accuracy score (and, thereby, theaccuracy leaderboard). Future directions: In this work, we analyze the effect of seven variants of PerSEval on SOTA summarizationmodels, out of which the ROUGE-variants, BLEU, and METEOR are defined on the string space, JSD isdefined on the probability space, BERTScore is defined on the embedding space, and InfoLM is definedon the probability space that is generated from the embedding space using a masked-LM. Although thesecover all the most common algebraic spaces on which PerSEval can be defined, it remains to be understoodhow other alternate measures on the same spaces, such as BaryScore (Colombo et al., 2021), MoversScore(Zhao et al., 2019), DepthScore (Staerman et al., 2022), and other variants of InfoLM using multiple Csiszarf-divergences, will behave w.r.t HJ-correlation. Finally, we have only explored one method of estimatingPerSEval-HJ (i.e., mimicking the human way of computing PerSEval using RG-L as the distance betweenmodel-generated summary and human-reference) for analyzing HJ-correlations. However, there can be otheralternative methods of estimating PerSEval-HJ, including incorporating inter-annotator-agreement statistics(such as Kappa statistic). We also consider it a promising future direction to study the effectiveness ofPerSEval as an objective function to steer the fine-tuning of the personalized summarization models under aReward Model-driven reinforcement learning setup. In general, the current work can potentially open updirections of systematic evaluation studies of personalization capabilities in models, particularly LLMs with their In-context Learning capabilities, that apply to all AI challenges where the quality of the model outputis subjective to the users past preferences. We, therefore, encourage the research community to criticallystudy our work and build on it.",
  "Ethics Statement": "We would like to declare that we used the PENS dataset prepared and released by Microsoft Research. Ourhuman-judgment survey was conducted according to the norms set by the Institutional Review Board (IRB)and respects participant anonymity as per guidelines. Xiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He, and Xing Xie. PENS: A dataset and genericframework for personalized news headline generation. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers), pp. 8292. Association for Computational Linguistics,August 2021. doi: 10.18653/v1/2021.acl-long.7. URL",
  "Evaluation Measures for Machine Translation and/or Summarization, pp. 6572, Ann Arbor, Michigan,June 2005. Association for Computational Linguistics. URL": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. Re-evaluatingevaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pp. 93479359. Association for Computational Linguistics, November2020. doi: 10.18653/v1/2020.emnlp-main.751. URL Florian Bhm, Yang Gao, Christian M. Meyer, Ori Shapira, Ido Dagan, and Iryna Gurevych. Betterrewards yield better summaries: Learning to summarise without references. In Kentaro Inui, Jing Jiang,Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP), pp. 31103120, Hong Kong, China, November 2019. Association for ComputationalLinguistics. doi: 10.18653/v1/D19-1307. URL",
  "Tanner Bohn and Charles X. Ling. Hone as you read: A practical type of interactive summarization, 2021.URL": "Florian Boudin, Hugo Mougard, and Benoit Favre. Concept-based summarization using integer linearprogramming: From concept pruning to multiple optimal solutions. In Conference on Empirical Methodsin Natural Language Processing (EMNLP) 2015, 2015. Hou Pong Chan, Wang Chen, and Irwin King. A unified dual-view model for review summarization andsentiment classification with inconsistency loss. In Proceedings of the 43rd International ACM SIGIRConference on Research and Development in Information Retrieval, SIGIR 20, pp. 11911200, New York,NY, USA, 2020. Association for Computing Machinery. ISBN 9781450380164. doi: 10.1145/3397271.3401039.URL",
  "Andrzej Cichocki, Sergio Cruces, and Shun-ichi Amari.Generalized alpha-beta divergences and theirapplication to robust nonnegative matrix factorization. Entropy, 13:134170, 2011. URL": "Pierre Colombo, Guillaume Staerman, Chlo Clavel, and Pablo Piantanida. Automatic text evaluationthrough the lens of Wasserstein barycenters. In Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Processing, pp. 1045010466, 2021. Pierre Colombo, Nathan Noiry, Ekhine Irurozki, and Stphan Clmenon. What are the best systems? newperspectives on nlp benchmarking. Advances in Neural Information Processing Systems, 35:2691526932,2022a. Pierre Jean A Colombo, Chlo Clavel, and Pablo Piantanida. Infolm: A new metric to evaluate summarization& data2text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp.1055410562, 2022b.",
  "Lea Frermann and Alexandre Klementiev. Inducing document structure for aspect-based summarization. In": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 62636273,Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1630. URL Yang Gao, Wei Zhao, and Steffen Eger. SUPERT: Towards new frontiers in unsupervised evaluation metricsfor multi-document summarization. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pp. 13471354. Association for Computational Linguistics, July 2020. doi:10.18653/v1/2020.acl-main.124. URL",
  "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), 2019": "Samira Ghodratnama, Mehrdad Zakershahrak, and Fariborz Sobhanmanesh.Adaptive summaries: Apersonalized concept-based summarization approach by learning from users feedback. In Service-OrientedComputing ICSOC 2020 Workshops, pp. 281293, Cham, 2021. Springer International Publishing. ISBN978-3-030-76352-7. Aniko Hannak, Piotr Sapiezynski, Arash Molavi Kakhki, Balachander Krishnamurthy, David Lazer, AlanMislove, and Christo Wilson. Measuring personalization of web search. In Proceedings of the 22ndinternational conference on World Wide Web, pp. 527538, 2013. Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris Ackerson, Raj Neervannan, and Graham Neubig.WikiAsp: A Dataset for Multi-domain Aspect-based Summarization. Transactions of the Associationfor Computational Linguistics, 9:211225, 03 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00362. URL Karl Moritz Hermann, Tom Koisk, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,and Phil Blunsom. Teaching machines to read and comprehend. In Proceedings of the 28th InternationalConference on Neural Information Processing Systems - Volume 1, NIPS15, pp. 16931701, Cambridge,MA, USA, 2015. MIT Press. Eran Hirsch, Alon Eirew, Ori Shapira, Avi Caciularu, Arie Cattan, Ori Ernst, Ramakanth Pasunuru, HadarRonen, Mohit Bansal, and Ido Dagan. iFacetSum: Coreference-based interactive faceted summarizationfor multi-document exploration. In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing: System Demonstrations. Association for Computational Linguistics, 2021. URL Raghav Jain, Vaibhav Mavi, Anubhav Jangra, and Sriparna Saha. Widar-weighted input document augmentedrouge. In Advances in Information Retrieval: 44th European Conference on IR Research, ECIR 2022,Stavanger, Norway, April 1014, 2022, Proceedings, Part I, pp. 304321. Springer, 2022. Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, GrahamNeubig, and Chunting Zhou. Multi-dimensional evaluation of text summarization with in-context learning.In Findings of the Association for Computational Linguistics: ACL 2023, pp. 84878495, Toronto, Canada,July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.537. URL",
  "Madhusree Kuanr and Puspanjali Mohapatra. Assessment methods for evaluation of recommender systems:A survey. Foundations of Computing and Decision Sciences, 46:393 421, 2021. URL": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, VeselinStoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural languagegeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pp. 78717880. Association for Computational Linguistics, July 2020. doi:10.18653/v1/2020.acl-main.703. URL Junjie Li, Haoran Li, and Chengqing Zong. Towards personalized review summarization via user-awaresequence network. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence andThirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium onEducational Advances in Artificial Intelligence, AAAI19/IAAI19/EAAI19. AAAI Press, 2019. ISBN 978-1-57735-809-1. doi: 10.1609/aaai.v33i01.33016690. URL Shu Li, Yuan Zhao, Longjiang Guo, Meirui Ren, Jin Li, Lichen Zhang, and Keqin Li. Quantification andprediction of engagement: Applied to personalized course recommendation to reduce dropout in moocs.Information Processing & Management, 61(1):103536, 2024.",
  "Hui Liu and Xiaojun Wan. Neural review summarization leveraging user and product information. In": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management,CIKM 19, pp. 23892392, New York, NY, USA, 2019. Association for Computing Machinery. ISBN9781450369763. doi: 10.1145/3357384.3358161. URL Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluationusing gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods inNatural Language Processing, pp. 25112522, Singapore, December 2023. Association for ComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.153. URL Yixin Liu and Pengfei Liu. SimCLS: A simple framework for contrastive learning of abstractive summarization.In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing (Volume 2: Short Papers), pp. 10651072.Association for Computational Linguistics, August 2021. doi: 10.18653/v1/2021.acl-short.135. URL Yixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. BRIO: Bringing order to abstractive sum-marization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 28902903, Dublin, Ireland, May 2022. Association for ComputationalLinguistics. doi: 10.18653/v1/2022.acl-long.207. URL Shuming Ma, Xu Sun, Junyang Lin, and Xuancheng Ren. A hierarchical end-to-end model for jointly improvingtext summarization and sentiment classification. In Proceedings of the Twenty-Seventh International JointConference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization,2018.",
  "Franklin Institute, 334(2):307318, 1997. ISSN 0016-0032. doi: URL": "Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conferenceon Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2018.URL Duy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-Sinh Nguyen, Dung Tien Tien Le, Shahab Sabahi, Minh-Tien Nguyen, and Hung Le. Make the most of prior data: A solution for interactive text summarizationwith preference feedback. In Findings of the Association for Computational Linguistics: NAACL 2022, pp.19191930, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-naacl.147. URL Shumpei Okura, Yukihiro Tagami, Shingo Ono, and Akira Tajima. Embedding-based news recommendationfor millions of users. In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD 17, pp. 19331942, New York, NY, USA, 2017. Association for ComputingMachinery. ISBN 9781450348874. doi: 10.1145/3097983.3098108. URL",
  "Processing Workshop, pp. 6769, 2021": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association for ComputationalLinguistics, pp. 311318, 2002. Maxime Peyrard and Judith Eckle-Kohler. Supervised learning of automatic pyramid for optimization-based multi-document summarization. In Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 10841094, Vancouver, Canada, July 2017.Association for Computational Linguistics. doi: 10.18653/v1/P17-1100. URL",
  "Maxime Peyrard and Iryna Gurevych.Objective function learning to match human judgements foroptimization-based summarization.In Proceedings of the 2018 Conference of the North American": "Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2(Short Papers), pp. 654660, New Orleans, Louisiana, June 2018. Association for Computational Linguis-tics. doi: 10.18653/v1/N18-2103. URL Avinesh PVS, Benjamin Httasch, Orkan zyurt, Carsten Binnig, and Christian M. Meyer. Sherlock: a systemfor interactive summarization of large text collections. Proc. VLDB Endow., 11(12):19021905, aug 2018.ISSN 2150-8097. doi: 10.14778/3229863.3236220. URL Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and MingZhou. ProphetNet: Predicting future n-gram for sequence-to-SequencePre-training. In Findings of theAssociation for Computational Linguistics: EMNLP 2020, pp. 24012410. Association for ComputationalLinguistics, November 2020. doi: 10.18653/v1/2020.findings-emnlp.217. URL GS Ramesh, Vamsi Manyam, Vijoosh Mandula, Pavan Myana, Sathvika Macha, and Suprith Reddy. Ab-stractive text summarization using t5 architecture. In Proceedings of Second International Conference onAdvances in Computer Engineering and Communication Systems: ICACECS 2021, pp. 535543. Springer,2022. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and Lidong Bing. Large language modelsare not yet human-level evaluators for abstractive summarization. In Findings of the Association forComputational Linguistics: EMNLP 2023, pp. 42154233, Singapore, December 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.278. URL Amir Soleimani, Vassilina Nikoulina, Benoit Favre, and Salah Ait Mokhtar. Zero-shot aspect-based scientificdocument summarization using self-supervised pre-training. In Proceedings of the 21st Workshop onBiomedical Language Processing, pp. 4962, Dublin, Ireland, May 2022. Association for ComputationalLinguistics. doi: 10.18653/v1/2022.bionlp-1.5. URL",
  "Guillaume Staerman, Pavlo Mozharovskyi, Pierre Colombo, Stphan Clmenon, and Florence dAlch Buc.A pseudo-metric between probability distributions based on depth-trimmed regions, 2022": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, DarioAmodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in NeuralInformation Processing Systems, 33:30083021, 2020. Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, and Colin Raffel. Evaluating thefactual consistency of large language models through news summarization. In Findings of the Associationfor Computational Linguistics: ACL 2023, pp. 52205255, Toronto, Canada, July 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.findings-acl.322. URL Bowen Tan, Lianhui Qin, Eric Xing, and Zhiting Hu. Summarizing text on any aspects: A knowledge-informedweakly-supervised approach. In Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pp. 63016309. Association for Computational Linguistics, November2020. doi: 10.18653/v1/2020.emnlp-main.510. URL T Tawmo, Mrinmoi Bohra, Pankaj Dadure, Partha Pakray, et al. Comparative analysis of t5 model forabstractive text summarization on different datasets. In Proceedings of the International Conference onInnovative Computing Communication (ICICC) 2022. SSRN, 2022. URL Maartje Ter Hoeve, Julia Kiseleva, and Maarten Rijke. What makes a good and useful summary? Incorporatingusers in automatic summarization research.In Marine Carpuat, Marie-Catherine de Marneffe, andIvan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies, pp. 4675, Seattle, UnitedStates, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.4. URL Rahul Vansh, Darsh Rank, Sourish Dasgupta, and Tanmoy Chakraborty. Accuracy is not enough: Evaluatingpersonalization in summarizers. In Findings of the Association for Computational Linguistics: EMNLP2023, pp. 25822595, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.169. URL Michael Vlske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learn automaticsummarization. In Lu Wang, Jackie Chi Kit Cheung, Giuseppe Carenini, and Fei Liu (eds.), Proceedingsof the Workshop on New Frontiers in Summarization, pp. 5963, Copenhagen, Denmark, September 2017.Association for Computational Linguistics. doi: 10.18653/v1/W17-4508. URL Chuhan Wu, Fangzhao Wu, Mingxiao An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. Neural news rec-ommendation with attentive multi-view learning. In Proceedings of the Twenty-Eighth International JointConference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence Organization,2019a. Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang, and Xing Xie. Neural news recommen-dation with multi-head self-attention. In Proceedings of the 2019 Conference on Empirical Methods inNatural Language Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP), pp. 63896394, Hong Kong, China, November 2019b. Association for ComputationalLinguistics. doi: 10.18653/v1/D19-1671. URL Hongyan Xu, Hongtao Liu, Pengfei Jiao, and Wenjun Wang. Transformer reasoning network for personalizedreview summarization. In Proceedings of the 44th International ACM SIGIR Conference on Research andDevelopment in Information Retrieval, SIGIR 21, pp. 14521461, New York, NY, USA, 2021. Associationfor Computing Machinery. ISBN 9781450380379. doi: 10.1145/3404835.3462854. URL",
  "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M. Meyer, and Steffen Eger.MoverScore:Text generation evaluating with contextualized embeddings and earth mover distance. In Proceedings": "of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th InternationalJoint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 563578, Hong Kong, China,November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1053. URL Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In Thirty-seventh Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track, 2023a. URL Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,Yuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi, Nuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,Zhangyue Yin, Rongxiang Weng, Wensen Cheng, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,Qi Zhang, Xipeng Qiu, and Xuanjing Huang. Secrets of rlhf in large language models part i: Ppo, 2023b. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han.Towards a unified multi-dimensional evaluator for text generation. In Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Processing, pp. 20232038, Abu Dhabi, United Arab Emirates,December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.131. URL",
  "We briefly introduce the SOTA summarization models that were analyzed to understand their degree-of-personalization below:": "1. PENS-NRMS Injection-Type 1: The PENS framework (Ao et al., 2021) takes user embeddingas input along with the news article to generate a personalized summary for that user. To generateuser embedding, NRMS (Neural News Recommendation with Multi-Head Self-Attention) (Wu et al.,2019b) is used. It includes a news encoder that utilizes multi-head self-attentions to understandnews titles. The user encoder learns user representations based on their browsing history and usesmulti-head self-attention to capture connections between news articles. Additive attention is added tolearning the news and user representations more effectively by selecting important words and articles.Here, Injection-Type 1 indicates that NRMS user embedding is injected into PENS by initializingthe decoders hidden state of the headline generator, which will influence the summary generation. 2. PENS-NRMS Injection-Type 2: To generate a personalized summary, NRMS user embedding isinjected into attention values (Injection-Type 2) of PENS that helps to personalize attentive valuesof words in the news body. 3. PENS-NAML Injection-Type 1: NAML (Neural News Recommendation with Attentive Multi-View Learning) Wu et al. (2019a) incorporates a news encoder that utilizes a multi-view (i.e., titles,bodies, and topic categories) attention model to generate comprehensive news representations. Theuser encoder is designed to learn user representations based on their interactions with browsed news.It also allows the selection of highly informative news during the user representation learning process.This user embedding is injected into the PENS model using Type-1 for personalization. 4. PENS-EBNR Injection-Type 1: EBNR (Embedding-based News Recommendation for Millionsof Users) Okura et al. (2017) proposes a method for user representations by using an RNN modelthat takes browsing histories as input sequences. This user embedding is injected using Type 1 intothe PENS model for personalization.",
  ". PENS-EBNR Injection-Type 2: This personalized model injects EBNR user embedding intoPENS using type-2": "6. BRIO: Instead of a traditional MLE-based training approach, BRIO Liu et al. (2022) assumes anon-deterministic training paradigm that assigns probability mass to different candidate summariesaccording to their quality, thereby helping it to better distinguish between high-quality and low-qualitysummaries. 7. SimCLS: SimCLS (A Simple Framework for Contrastive Learning of Abstractive Summarization)Liu & Liu (2021) uses a two-stage training procedure. In the first stage, a Seq2Seq model (BART(Lewis et al., 2020)) is trained to generate candidate summaries with MLE loss. Next, the evaluationmodel, initiated with RoBERTa is trained to rank the generated candidates with contrastive learning. 8. BigBird-Pegasus: BigBird Zaheer et al. (2020) is an extension of Transformer based modelsdesigned specifically for processing longer sequences. It utilizes sparse attention, global attention,and random attention mechanisms to approximate full attention. This enables BigBird to handlelonger contexts more efficiently and, therefore, can be suitable for summarization. 9. ProphetNet: ProphetNet Qi et al. (2020) is a sequence-to-sequence pre-trained model that employsn-gram prediction using the n-stream self-attention mechanism. ProphetNet optimizes n-step aheadprediction by simultaneously predicting the next n tokens based on previous context tokens, thuspreventing overfitting on local correlations. 10. T5: T5 (Text-To-Text Transfer Transformer) is based on the Transformer-based Encoder-Decoderarchitecture that operates on the principle of the unified text-to-text task for any NLP problem,including summarization. Some recent analyses on the performance of T5 on summarization taskscan be found in Tawmo et al. (2022); Ramesh et al. (2022); Etemad et al. (2021).",
  ". RG-SU4: ROUGE-SU4 (Lin, 2004) was designed to consider skip-bigram matches as well, whichallows for non-contiguous n-gram matches": "3. BLEU: BLEU (Bilingual Evaluation Understudy) (Papineni et al., 2002) is a popular evaluationmetric that measures the precision of n-gram matches between the model-generated summaries andthe reference summaries. BLEU computes a modified precision score for various n-gram lengths andthen combines them using a geometric mean. 4. METEOR: METEOR (Metric for Evaluation of Translation with Explicit ORdering) (Banerjee& Lavie, 2005) matches unigrams based on surface forms, stemmed forms, and meanings and thencalculates score using a combination of precision, recall, and the order-alignment of the matchedwords w.r.t reference summary. 5. Jensen-Shannon Distance: The Jensen-Shannon Distance (JSD) (Menndez et al., 1997) is ametric used in summarization evaluation to measure the dissimilarity between probability distributionsof words in a reference summary and a generated summary. It quantifies the information divergenceand similarity, providing a nuanced assessment of the semantic content overlap between the twosummaries. 6. BertScore: BertScore (BScore) (Zhang et al., 2020) is a metric for evaluating machine-generated sum-maries, emphasizing contextual embeddings from BERT to assess both word overlap and contextualrelationships. It overcomes the limitations of keyphrase-based measures like ROUGE. 7. InfoLM-: Given a user-generated reference summary u and a model-generated summary su,InfoLM (Colombo et al., 2022b) recursively masks each token position k of both u (denoted [u]k) andsu (denoted [su]k) to obtain individual masked contexts of length lu and lsu respectively. For eachmasked context, it uses a pre-trained masked-language model to estimate the corresponding probabilitydistribution over the vocabulary (i.e., p | []k; M,h), resulting in two bags of distributions of sizelu and lsu for u and su. The bags of distributions (for both masked u and masked su) are thenaveraged out, as follows11:",
  "B.2Model Rank Aggregation & Agreement": "1. Borda-Kendall Consensus based Rank Aggregation: The Borda-Kendall (BK) consensusentails aggregating a set of permutations, denoted as 1, . . . , L N, which represent the rankings ofN models across L 1 tasks or instances (in our case, the pair of accuracy rank measure and thePerSEval-variant to be aggregated). This aggregation involves summing the ranks of each model andsubsequently ranking the obtained sums. Formally:",
  "C.1PerSEval Stability Results": "In this section, we provide a detailed analysis of the stability performance of all the seven PerSEval variants(in .1, we discussed that of the best performing PerSEval-InfoLM variant only). We analyze the-bias and the -variance of each variant across all the ten SOTA models that have been studied. We observethat while the best-performing variant w.r.t bias is PerSEval-BertScore and w.r.t variance is PerSEval-RG-L,the worst performances w.r.t both are pretty low with an overall 0.0072 -stability across all the variants(see ). We also observed a consistent 100% rank-correlation (i.e., -stability) across all the variants,showing PerSEvalto be extremely stable.",
  "DSurvey Format: Human-Judgment Meta-evaluation of PerSEval": "In this section, we present the screenshot of the questionnaire designed for the survey for computing thehuman-judgment version of PerSEval (PerSEval-HJ). Two consecutive respondents evaluated the generatedsummary pairs of all ten benchmarked models. : Sample Questionnaire: Six pairs of summaries for a specific document; five pairs are model-generated summaries (each user evaluates five of the ten models) for a specific document, while one pair isuser-generated gold reference)."
}