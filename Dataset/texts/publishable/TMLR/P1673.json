{
  "Abstract": "This work proposes a scalable probabilistic latent variable model based on Gaussian processes(Lawrence, 2004) in the context of multiple observation spaces. We focus on an applicationin astrophysics where it is typical for data sets to contain both observed spectral featuresas well as scientific properties of astrophysical objects such as galaxies or exoplanets. Inour application, we study the spectra of very luminous galaxies known as quasars, and theirproperties, such as the mass of their central supermassive black hole, their accretion rateand their luminosity, and hence, there can be multiple observation spaces. A single datapoint is then characterised by different classes of observations, which may have differentlikelihoods.Our proposed model extends the baseline stochastic variational Gaussianprocess latent variable model (GPLVM) (Lalchand et al., 2022) to this setting, proposing aseamless generative model where the quasar spectra and scientific labels can be generatedsimultaneously when modelled with a shared latent space acting as input to different setsof Gaussian process decoders, one for each observation space. In addition, this frameworkallows training in the missing data setting where a large number of dimensions per datapoint may be unknown or unobserved. We demonstrate high-fidelity reconstructions of thespectra and the scientific labels during test-time inference and briefly discuss the scientificinterpretations of the results along with the significance of such a generative model. Thecode for this work is available at",
  "Introduction": "Many challenges in the contemporary physical sciences arise from the analysis of large-scale, noisy, het-eroscedastic, and high-dimensional datasets (Clarke et al., 2016). Hence, there is increasing consensus thataddressing the challenges posed by large-scale data in the experimental pipelines for discovery, forecasting,and prediction warrant scalable machine learning. Modern experiments in physics, chemistry, and astronomyare capable of producing extremely complex high-dimensional data, but it is not just the sheer volume of thedata, but also the velocity, referring to the rate of production (Carleo et al., 2019) of data, which poses anadditional challenge. Further, an additional axis is the variety or heterogeneity inherent in scientific datasetsin the form of multiple outputs or observation spaces. For instance, images (pixels) can be accompanied withattributes like illumination, pose, and resolution. In addition, some of these attributes or observed featuresare often unknown or unobserved, resulting in incomplete data sets with missing components. The multipleoutput setting is inadequately addressed in the probabilistic machine learning literature; in this work wetackle precisely this setting proposing a multi-output scalable probabilistic model for a scientific applicationin astronomy. A preponderance of literature in unsupervised learning focuses on the setting of a single large-scale homogeneousdataset, for instance, images, text, speech, or continuous numerical values. Further, most parametric models",
  "Published in Transactions on Machine Learning Research (Jan/2025)": "Another extension is related to the structure of the kernels, since the kernels factorise over dimensions and areclosed over the multiplicative operation, one approach is to select a composite product kernel over dimensionsof the latent space instead of two sets of GPs with their own respective kernels. The former approach (notstudied here) induces a partitioning of the latent space into dimensions which would cater to specific views ofthe data and can be modelled with individual kernel functions. In our context, we did not see the gain inthe mixture kernel formulation; note that this approach raises an additional question as to how many latentdimensions to allocate to each kernel function. Nevertheless, partitioning the latent space and modellingwith mixed or composite kernels could be beneficial in specific settings. More experiments are required tounderstand the exact settings in which this approach may outshine the alternative framework presented here.",
  "have been constrained on scalability and examine less than 50 astronomical objects. We demonstrate ourframework on datasets over 400 larger. We summarise our key contributions below:": "ContributionsWe propose a probabilistic generative framework called the Shared stochastic GPLVMwhich is designed for use cases with multiple outputs/observation spaces. We seamlessly account for missingdimensions both at training and test time due to the probabilistic nature of the model. We demonstratethrough astrophysical experiments that we can reconstruct previously unseen/test spectral pixels to a highdegree of fidelity, interpolate missing or unobserved spectral regions and predict scientific labels. Crucially,we demonstrate that it is possible to share predictive strength by learning a common latent variable spaceZ across multiple-outputs (X, Y ) where Y RNL is an additional view of the data, we refer to this asan additional observation space with L dimensions. In this way we indirectly model the relationships andcorrelation structure between the different observation spaces. We demonstrate this concretely with anexperiment where we generate/predict all the scientific attributes at test-time by using latent variables (Z)only informed by the quasar spectra (X), we can denote this cross-modal prediction as X Z Y . Tothe best of our knowledge, predicting multiple outputs with stochastic variational GPs for scalability ismethodologically novel. Further, this is the first demonstration of a scalable probabilistic latent variablemodel in astrophysical settings. In we discuss how the development of these unsupervised learningframeworks can instigate novel insights into some of the major open questions in astronomy today.",
  "Stochastic Variational GPLVM with a Shared latent space": "In this section, we first describe background on the stochastic variational GPLVM (Lalchand et al., 2022). Wethen develop the idea of a shared latent space and inducing points within the stochastic variational GPLVMframework. The fundamental contribution of this work is to develop an inference scheme to show that ashared latent space does not preclude scalable inference through SVI.",
  "SV-GPLVM: Stochastic Variational GPLVM": "In the traditional formulation underlying GPLVMs we have a training set comprising of N D-dimensionalreal-valued observations X {xn}Nn=1 RND. These data are associated with N Q-dimensional latentvariables, Z {zn}Nn=1 RNQ where Q D provides dimensionality reduction (Lawrence, 2004). Theforward mapping (Z X) is governed by GPs independently defined across dimensions D. The sparse GPformulation describing the data is as follows:",
  "d=1N(xn,d; fd(zn), 2x),": "where Qnn = KnnKnmK1mmKmn is N N, F {fd}Dd=1 where fd RN, U {ud}Dd=1 where ud RM andxd is the dth column of X. Knn is the N N covariance matrix corresponding to a user-chosen positive-definitekernel function k(z, z) evaluated at latent points {xn}Nn=1 and parametrised by shared hyperparameters .Inducing variables per dimension {ud}Dd=1 are distributed with a GP prior ud| Z N(ud; 0, Kmm) (whereKmm is M M) computed on inducing input locations [z1, . . . zM]T Z RMQ which live in latent spacewith Z and have dimensionality Q (matching zn). In addition, Knm is the N M cross-covariance computedon the latents zn and the inducing locations zm. The core idea of Stochastic Variational Inference (SVI) applied to sparse variational Gaussian processes (GPs),as introduced by Hensman et al. (2013), is that the inducing variables ud can be variationally marginalized.This is achieved by learning their variational distribution q(ud) N(md, Sd) using stochastic gradient",
  "nlog p(zn)(5)": "Latent point estimates {zn}Nn=1 can be learnt along with and variational parameters ( Z, md, Sd) by takinggradients of the ELBO in Eq. (5). However, an important constraint is that this formulation assumes asingle-kernel matrix (single set of kernel hyperparameters) underlying all the independent GPs D. In thenext section, we introduce the concept of an additional observation space with dimensions L, which can bemodelled using an independent stack of Gaussian processes (GPs), fl. These GPs are equipped to learn theirown set of hyperparameters for added flexibility while sharing the latent embedding Z and inducing inputs Zto capture correlations across the different output spaces.",
  "Shared joint variational lower bound": "In the astrophysical application we focus on in this work we have two observation spaces correspondingto N quasars.We denote the quasar spectra (pixels) with the matrix X RND and the scientificlabels corresponding to the N objects with Y RNL. The GPLVM construction models each column(pixel dimension and label dimension) with an independent GP, with the GPs corresponding to the pixeldimensions {fd}Dd=1 and label dimensions {fl}D+Ll=D+1 modelled with their own independent kernels and kernelhyperparameters, x and y. Within each observation space the kernel hyperparameters are shared, so welearn two sets of hyperparameters corresponding to two observation spaces.",
  "l=D+1log p(yl|y, Z)(14)": "In order to induce sparsity we introduce inducing variables ud and ul for each individual dimension in theobservation spaces; however, they are underpinned by shared inducing inputs Z which live in the shared latentspace Z and share the same dimensionality, Q. With sparse GPs each of the terms in the decompositionabove can be bounded by Lx, while the inducing points Z can be shared between the terms yielding the jointevidence lower bound.",
  "Predictions & Reconstructions": "High-dimensional points can arrive in different formats for the test data, either we observe both modalities{x, y} where x = [x1, . . . , xd]T and similarly for y or only one of the modalities with the other missing,i.e. {x} or {y} only. The prediction exercise then involves inferring the latent z corresponding to theunseen test point. Since the GPLVM is a decoder only model, we cannot obtain the latent embedding z deterministically;instead we re-optimize the ELBO with the additional test data point (x, y) while keeping all the globaland model hyperparameters frozen at their trained values. Note that since the ELBO factorises across data",
  "d=dp(xd, |fd, z, 2x)q(.)(16)": "Once we infer z we can compute the full reconstruction distributions (we may be interested in this if thetest data point y had any missing dimensions, for example y = [y1, y2, ?, ?, y5, . . . , yl ]T ) which are just theGP posterior predictive for each column or dimension, without loss of generality, for dimension yl :",
  "Schematic of the Model": "In we present a schematic of the model architecture with two observation spaces (X, Y ), the corre-sponding stacks of individual GPs {fd} and {fl} which model the individual columns of the spectra X andscientific attributes Y , respectively, and the low-dimensional latent space Z. The dimensionality of the latentand observation spaces is denoted by Q, D, L, respectively, and N denotes the number of objects / datapoints (quasars). Note that the correlation between the two observation spaces is not explicitly but implicitlymodelled through a shared latent space. Generating a single data point (xn, yn) (a row across X and Y )entails a forward pass through the GPs, where xn = [. . . , xnd, . . .] is generated as [f1(zn), f2(zn), . . . , fD(zn)]and yn = [. . . , ynl, . . .] is generated as [fD+1(zn), fD+2(zn), . . . , fD+L(zn)].",
  "{": ": Shared GPLVM with multiple observation spaces. The blocks on the right-hand side denote thedouble observation spaces (X, Y ) of quasar spectra and scientific labels respectively. In the center are twostacks of GPs, one for each observation space which control the data generation process through the sharedlatent space. In the figure above we assume Q = 2 (for ease of visualisation) since we denote the GPs are twodimensional surfaces, however, typically Q can be higher than 2 corresponding to higher dimensional GPs.",
  "Algorithm": "We enclose the pseudo-code in Algorithm 2 for stochastic variational inference in the context of the sharedmodel for clarity. Let Lx and Ly denote the ELBOs for each of the observation spaces and let L(B)xandL(B)ydenote the ELBOs formed with a randomly drawn mini-batch of the data (across all dimensions). For amini-batch (subset) of the data XB X, the mini-batch ELBO is given by,",
  "Computational Cost": "The training cost of the canonical stochastic variational GPLVM is dominated by the number of inducingpoints O(M 3D) (free of N) where M N and D is the data-dimensionality (we have D GP mappings fd, oneper output dimension). The practical algorithm is made further scalable with the use of mini-batched learning.In our shared model with two sets of GPs, the dynamics of the training cost are the same except that theyincrease linearly in the number of additional dimensions (L), making the cost O(M 3(D + L)). The number ofglobal variational parameters to be updated in each step (parameters of q(U)) is MQ+M(D+L)+M 2(D+L),where MQ are the M Q-dimensional inducing inputs Z (shared), M(D+L) is the size of the mean parametersof the inducing variables ud, ul and M 2(D + L) are the full-rank covariances of the inducing variables.The local variational parameters Z (the latent embedding shared across GPs) are of size NQ and modelhyperparameters (kernel hyperparameters) are of size 2Q + 4, which account for Q input lengthscales, a scalarsignal variance and noise variance per GP group {fd} and {fl}. We use the squared exponential kernel withautomatic relevance determination across both sets of GPs.",
  "(Note that the gradients of Lx and Ly with respect to Z are 0 and the only terms that are optimised are the additionalterms corresponding to the new data points.)": "(lvarez et al., 2010) where the targets {xd(z)}Dd=1 are multidimensional, continuous and Gaussian distributedcorresponding to inputs z Rp. The main focus is on defining a suitable cross-covariance function betweenthe outputs, this allows treatment of the outputs as a single GP with a suitable covariance function (Alvarezet al., 2012). The intrinsic and linear co-regionalization models (Goovaerts, 1993; Journel & Huijbregts, 1976)are a popular approach in which each output is modelled as a weighted sum of shared latent functions (GP)where typically the number of latent processes is smaller than the number of outputs enabling efficiencies. Tosome extent, multitask learning with GPs can be viewed as an instance of multi-output learning where wewant to avoid tabula rasa learning for each task and evolve a framework for sharing information betweenmultiple tasks (Bonilla et al., 2007). Further, there is the simplistic paradigm where each output is modelledwith its own independent single-output GP and no correlation between the outputs is assumed. This approach,while easy to implement, is severely limited in its ability to jointly model the outputs. In the unsupervised paradigm, the starting point is a high-dimensional data matrix N D. The conventionalGaussian process latent variable model (GPLVM) operates like a multi-output model by default where eachcolumn of the data is modelled by an independent GP on the same shared set of inputs, kernel function",
  "Models ()BaselineShared (ours)Specialised (ours)": "Attributes ()Spectra0.1103 0.00350.0972 0.00240.0969 0.0013Bolometric Luminosity0.2444 0.00430.2144 0.00330.2069 0.0014Black hole mass0.2712 0.00310.2319 0.00280.2362 0.0025Eddington Ratio0.2525 0.00700.2118 0.0060.2226 0.0015 : Summary of test-time reconstruction abilities. Mean absolute error on denormalised data ( standarderror of mean) evaluated on average of 5 splits with 75% of the data used for training in the 22k dataset. Theshared and specialised versions of the model outperform the baseline model in all the reconstruction tasks. Inthe shared vs. specialised comparison the performance difference is not statistically significant for the spectrareconstruction but overall favours the shared model with hyperparameter sharing across the scientific labelreconstruction. and hyperparameters. The GPLVM is a decoder-only model, and some of their prominent variants are theback-constrained GPLVM (Lawrence & Quionero Candela, 2006), the supervised GPLVM (Jiang et al.,2012), the discriminative GPLVM (Urtasun & Darrell, 2007) and the shared GPLVM (Ek, 2009). The latterconsiders the task of dealing with multiple views or observation spaces (each of which is high-dimensional).This work is built on the idea of a shared latent space underlying the multiple observation spaces similar toEk et al. (2007) but adapts it for scalable inference using stochastic variational inference (SVI). The stochasticvariational GPLVM was proposed in Lalchand et al. (2022), but only considered a single observation space asin a canonical GPLVM.",
  "Experiments": "In this section, we demonstrate a range of experiments aimed at assessing the reconstruction quality of test(unseen) quasar spectra and scientific attributes, as well as the robustness of uncertainty quantification bycomputing the negative log predictive density (NLPD) of test labels log p(Y |Z) under the predictivedistribution where Z has been informed by both modalities (spectra and labels) and only spectra. The data used in this work are quasar spectra observed as part of the Sloan Digital Sky Survey (SDSS)DR16 (Lyke et al., 2020). We chose all quasars with spectra that have a signal-to-noise ratio (SNR) perpixel > 10, which results in a total of 22844 quasar spectra. The observed spectra are shifted into the restframe wavelength space, re-binned onto a common wavelength grid, and flux normalized to unity at around2500 . We mask strong absorption lines that might arise in the spectra due to foreground galaxies along ourline-of-sight to the quasar, and are thus not intrinsic spectral features of the quasar. The four scientific labelsfor these quasars are (1) their SMBH mass, (2) their bolometric luminosity, i.e. the total power output acrossall electromagnetic wavelengths, (3) their redshift which denotes the factor by which the emitted wavelengthshave been stretched due to the expansion of the universe, and (4) their Eddington ratio, which is a measureof the accretion and growth rate of the SMBH. All measurements were previously uniformly determined byWu & Shen (2022). We conduct experiments across two data sets with 1k (see ) and 22k points (as anablation to assess performance with a smaller dataset). The \"Baseline\" model in the experiments refers to thecanonical stochastic variational GPLVM (Lalchand et al., 2022) which treats multiple observation spacesusing the same set of independent GPs learning a single set of kernel hyperparameters. The \"Specialised\"model refers to the shared model but with individual hyperparameters for each scientific label.",
  "Reconstructing Quasar spectra": "We assess the quality of our probabilistic generative model in reconstructing test quasar spectra. At testtime we deal with spectra and scientific labels from test quasars denoted by (Xgt, Y gt) (we use the indexgt to denote ground truth). The 2-step prediction learns the low-dimensional shared latent variablesZ (point estimate per data point) which acts as input to the GP decoder predicting the correspondingspectra Z Xest. Note that the ground truth spectra contains several missing pixels (dimensions) and",
  "Reconstructing missing spectra": "In this experiment, we test the generative models ability to learn from massively missing chunks of thespectra at test-time. We observe a partial window of the spectra in each plot (given by the shaded region in), hence the latent variables corresponding to these points are only informed by the observed region. Wethen reconstruct the whole spectrum from the latent variables informed by the partial spectra. We encloseour results in . Reconstruction entails the following inference steps: Xpartial Z Xfull. We notethat the quality of the mean prediction deteriorates compared to the fully observed test point predictions.However, the coverage of the prediction intervals is robust even as we move away from the shaded observedregions. Furthermore, if the model is given a spectral region with very little information (e.g., in the bottompanel of , the shaded region contains information only about the quasars continuum), the uncertaintiesincrease significantly, as one would expect.",
  "Predicting scientific labels only from spectra X": "The dimensions L corresponding to the scientific labels in the dataset governed by their own GP decoders{fl}D+Ll=D+1 are a critical prediction quantity. The ability to reconstruct these quantities from learned latentvariables is an important test of the generalizability of our model. Very often astronomers want to reasonabout the scientific attributes of quasars just by analysing their spectra. In this experiment, we demonstrateprecisely this use case in which latent variables Z are informed only by spectra X, computing the cross-modal prediction involves learning Z from the ground truth spectra and then using just Z to predict thescientific labels Y , succinctly, we can write these steps as: X Z Y . In we demonstrate the accuracy of our reconstructions by plotting each of the dimensions againstground truth held-out data. We show reconstructions for 200 test points randomly sampled from the full testset. Each point in the scatter denotes a quasar, and the x-axis denotes the ground truth measurement. Theorange vertical error bars denote the intervals 1 computed by extracting the diagonals from the predictiveposterior GP for each dimension. We can observe a high degree of prediction accuracy across the threescientific labels and, furthermore, the reconstruction quality is weakly correlated to the spectral signal-to-noiseratio (SNR) (at least beyond the data quality cut of SNR > 10 which we apply as a preprocessing step); this ismainly due to acute data imbalances where more than 90% of the objects appear at the lower SNRs affectingprediction quality and generalization for the quasars at higher SNRs (see Appendix D). Note that we did notattempt to reconstruct the redshift label from the spectra X. This is because the spectra have already beennormalized to account for their redshift. Specifically, the spectra were shifted to rest-frame wavelength spaceby dividing the observed wavelengths by 1 + z (where z is the redshift). As a result, normalized spectralshapes no longer contain significant information about their redshift (as shown in Yang et al. (2021) andOnorato et al. (2024)). However, we still include the redshift information in our multimodal GPLVM. This is because excluding itmarginally weakens the results, both for spectra reconstruction and label prediction. We hypothesize that,while the spectral shapes themselves do not carry redshift information, the patterns of missing pixels maystill encode some redshift-related information. From we can see that the variability of NLPD estimates is much higher under the cross-modal protocolof decoding scientific labels on the basis of the spectrum. In other words, reconstructing scientific labels fromspectra alone has a much higher uncertainty than reconstructing scientific labels from both modalities. As aresult, the shared model offers only a subtle improvement over the baseline model in terms of uncertaintyquantification. In the fully observed protocol, the shared model is more confident () and accurate() in its predictions relative to the baseline model. It is also important to note that under the muchharder cross-modal protocol, the performance of the shared model surpasses that of the baseline model.",
  "ExperimentLuminosityBlack hole massEddington Ratio": "Baseline (Fully observed)0.076 0.00120.148 0.00430.1178 0.0031Baseline (Spectra observed)0.1143 0.0420.1742 0.06210.1376 0.0646Shared (Fully observed)-0.029 0.00180.1009 0.00130.0702 0.018Shared (Spectra observed)0.0862 0.0510.1604 0.02450.1294 0.0461 : Summary of test-time uncertainty quantification under the full and partial reconstruction framework.Mean negative log predictive density (lower is better) on de-normalised data across 5 train/test splits. Ahigher NLPD indicates lower confidence in the predictions and the model indicates this when reconstructingthe labels of a quasar just on the basis of its spectrum. The fully observed model corresponds to predictionsbased on test data when the all modalities are observed and reconstructed, (Xgt, Y gt) Z Y est or whenonly the spectrum of the quasar is informing the latent, (Xgt Z Y est). The latents are in turn used todecode the scientific labels.",
  "Generating spectra corresponding to synthesized labels: an ablation study": "In this experiment, we demonstrate the ability of the model to generate spectra corresponding to synthesizedscientific labels. Concretely, we simulate artificial labels by systematically varying only one of the labelswithin a reasonable range in each plot in . The range of variation for each label is summarized bythe colorbar in each plot, for instance, in the black hole mass simulation we generate 100 spectra (X)corresponding to simulated labels (Y ) of black hole masses log10(M/M) in the range [7.9, 10.0]; in order toablate the influence of other labels (redshift, bolometric luminosity, and the Eddington ratio) on the spectrawe keep their values fixed to mean values computed from the training dataset. Lastly, we can summarizethe inference steps as the inverse of the previous section: Y Z X. The ability for prediction andgeneration of cross-modalities is an important strength of our design. Reassuringly, the model exhibits the expected behaviour. For instance, the emission lines are broader forquasars with higher black hole masses, and the spectral dependence with bolometric luminosity shows e.g.in the Mg, II line at 2800 the well-known Baldwin effect (Baldwin, 1977), which indicates that quasarspectra show a decreasing equivalent width of their UV and optical emission lines with increasing bolometricluminosity. In addition, the gray bars denote the 95% prediction intervals averaged across the 100 spectra at eachdimension. The uncertainty intervals are wider at higher wavelengths, as there is a high concentrationof missing pixels in the training data at those wavelengths; hence, there is greater uncertainty about thegenerated spectra.",
  "On benchmarking with a traditional regression approach": "The dependency between the spectra and labels is modelled through the shared latent space, which generatesboth views of the data. Without a generative model it would not be possible to generate spectra correspondingto scientific labels (generate X corresponding to Y ), we would only be able to predict labels based on fixedobserved spectra. With a joint generative model, it is possible to generate both spectra and labels or generateone conditioned only on observing the other view through a jointly learned compression. In other words, wecan extract the shared latent (Z) corresponding to X (spectra) to predict a reconstructed estimate of the spectra and the labels ( X, Y ), or the other way round Yoptimise Zdecode ( X, Y ) as shown in experiments.Further, the spectra are only partially observed with several missing pixels making canonical regression lessstraightforward as one would need to account for missing/uncertain inputs. In the case of training a GP topredict labels directly from fixed high-dimensional spectra, one would have to impute the missing pixels atthe outset, as the prior covariance matrix cannot be computed on missing inputs.",
  "Limitations": "The primary limitation of the model arises from the fact that GPLVMs are decoder-only models, theyconstitute a (potentially) smooth mapping from the latent space to the data space. This means that pointsclose in latent space will be close in data space but not the other way round. Data space similarities canbe preserved by including a back-constraint or an encoder that additionally maps the data to latent space(Lawrence & Quionero Candela, 2006; Bui & Turner, 2015). The encoder usually takes the form of aneural network, but other choices are possible. As the title of the manuscript emphasizes, the model wepropose is a decoder-only model. The absence of an encoder complicates the test-time inference as there isno deterministic way to access the latent points z corresponding to the new unseen observation (x, y).Inferring the latent point corresponding to the unseen test point entails freezing the model and variationalparameters post-training and re-optimizing the ELBO objective subject to (z) with the new data point(s)included (see Algorithm 2). This optimization procedure for test-time inference ends up being too inconvenient in contrast to encoder-decoder models such as VAEs (Kingma & Welling, 2013), where inferring a latent point corresponding to anunseen data point entails a forward pass through the trained encoder network (constant-time predictionsO(1)). The main reason an extension to an autoencoded shared stochastic GPLVM is not straightforward isdue to the presence of missing data. The encoder network must be capable of handling arbitrarily missingdimensions. Methods like the partial VAE (Ma et al., 2018a;b) address this challenge in the context ofVAEs where each observation is augmented with a column index indicating the observed dimension; theencoder network then processes these tuples as a set with a permutation invariant set encoding function. Asimilar approach can be straightforwardly adapted from our shared latent space setup with Gaussian processdecoders. The combination of a set encoder (to process missing dimensions) and a non-parametric decoderhas not appeared in the literature to the best of our knowledge. We are currently working on incorporatingthis feature into our framework. Note that the presence of an encoder acts as a constraint in the model,and there is an inherent trade-off in terms of faster test inference and marginally weaker reconstructions /predictions.",
  "Methodological extensions for future work": "A natural extension of the shared GPLVM proposed here is to incorporate an encoder, bringing the wholearchitecture more in line with modern deep-generative autoencoder models while simultaneously preservingthe advantage of uncertainty quantification in the outputs. Bui & Turner (2015) propose the canonicalGPLVM with an encoder and train it with SVI, therefore, a natural extension is to demonstrate it for theshared setting. The shared setting in the data context presented here opens up questions about the encodingof partially observed vectors along with fully observed ones, and masked encoders (He et al., 2022) might bea relevant architectural paradigm to explore.",
  "Scientific Interpretation and Significance": "Our new generative model allows us to simultaneously model the spectral properties of quasars as well as theirscientific labels, thus opening up novel possibilities to study the evolution of quasars and their dependencyon physical parameters governing the SMBH growth. In the following, we will highlight just two possible andexciting future applications of this work. Astronomers observe SMBHs with billions of solar masses in size in the center of very distant, high-redshiftquasars, at a time when the universe is still in its infancy and only a few hundred Myr (million years) old.This rapid growth of SMBHs in the very short amounts of available cosmic time has been an open puzzlefor decades, and it has been argued that very high accretion rates in excess of the theoretical upper limit,the so-called Eddington limit, are required to explain the rapid black hole growth. However, obtainingprecise black hole mass measurements of quasars is challenging and time-intensive as it requires multi-epochobservations of certain emission lines in the quasar spectra (Peterson, 1993; Barth et al., 2015). This procedurebecomes increasingly challenging for very distant, high-redshift quasars, as relativistic time-dilation effectsrequire longer timespans of these observations. Furthermore, the traditionally used rest-frame optical emissionlines to calibrate the black hole masses are unobservable with ground-based observatories, as these opticalwavelengths have been shifted to the infrared at larger distances, which require space-based telescopes, suchas NASAs recently launched James Webb Space Telescope. Using the new generative model provides uswith the possibility to determine the masses of SMBHs for quasars at all redshifts from single-epoch dataobserved with ground-based observatories, as it does not require wavelength coverage of specific emissionlines; further, it can handle missing data. Furthermore, we have shown that our model is able to predict other physical properties of quasars such astheir bolometric luminosities (see ). This suggests that we can obtain a measurement of the quasarsabsolute luminosities using their spectral information alone, which provides a new opportunity to use quasarsas the so-called standard candles. Standard candles are incredibly valuable for astronomy, as knowing theluminosity of an object allows one to determine its distance. Famously, supernovae have been used as standardcandles, which led to the Nobel Prize winning discovery of the expansion of our universe and the existenceof dark energy (Riess et al., 1998). The use of quasars as standard candles has previously been suggestedleveraging the relationship between the X-ray and UV luminosities of quasars to determine their distances(e.g. Lusso & Risaliti, 2017; Sacchi et al., 2022). Our generative model enables predictions of the quasarsbolometric luminosities leveraging these spectral dependencies on luminosity, enabling the use of quasars asstandard candles (or standardizable candles) to map the expansion of the universe to larger distances thanpossible with supernovae. Further testing of the required precision with which the quasars luminosities andthus distances can be predicted by the generative model will be necessary in order to determine whether theconstraints on the universes expansion rate obtained by the more numerous quasars can improve on thecurrent constraints by the fewer but very accurately measured distances from supernovae.",
  "Jack A. Baldwin. Luminosity Indicators in the Spectra of Quasi-Stellar Objects. The Astrophysical Journal,214:679684, June 1977. doi: 10.1086/155294": "Aaron J. Barth, Vardha N. Bennert, Gabriela Canalizo, Alexei V. Filippenko, Elinor L. Gates, Jenny E.Greene, Weidong Li, Matthew A. Malkan, Anna Pancoast, David J. Sand, Daniel Stern, Tommaso Treu,Jong-Hak Woo, Roberto J. Assef, Hyun-Jin Bae, Brendon J. Brewer, S. Bradley Cenko, Kelsey I. Clubb,Michael C. Cooper, Aleksandar M. Diamond-Stanic, Kyle D. Hiner, Sebastian F. Hnig, Eric Hsiao,Michael T. Kandrashoff, Mariana S. Lazarova, A. M. Nierenberg, Jacob Rex, Jeffrey M. Silverman, Erik J.Tollerud, and Jonelle L. Walsh. The Lick AGN Monitoring Project 2011: Spectroscopic Campaign andEmission-line Light Curves. The Astrophysical Journal Supplement Series, 217(2):26, April 2015. doi:10.1088/0067-0049/217/2/26.",
  "Thang D. Bui and Richard E. Turner. Stochastic variational inference for Gaussian process latent variablemodels using back constraints. In Black Box Learning and Inference NIPS workshop, 2015": "Giuseppe Carleo, Ignacio Cirac, Kyle Cranmer, Laurent Daudet, Maria Schuld, Naftali Tishby, Leslie Vogt-Maranto, and Lenka Zdeborov. Machine learning and the physical sciences. Reviews of Modern Physics,91(4):045002, 2019. P. Clarke, P. V. Coveney, A. F. Heavens, J. Jykk, B. Joachimi, A. Karastergiou, N. Konstantinidis, A. Korn,R. G. Mann, J. D. McEwen, S. de Ridder, S. Roberts, T. Scanlon, E. P. S. Shellard, and J. A. Yates. Bigdata in the physical sciences: challenges and opportunities. ATI Scoping Report, 2016.",
  "E. Lusso and G. Risaliti. Quasars as standard candles. I. The physical relation between disc and coronalemission. Astronomy and Astrophysics, 602:A79, June 2017. doi: 10.1051/0004-6361/201630079": "Brad W. Lyke, Alexandra N. Higley, J. N. McLane, Danielle P. Schurhammer, Adam D. Myers, Ashley J.Ross, Kyle Dawson, Solne Chabanier, Paul Martini, Nicols G. Busca, Hlion du Mas des Bourboux, MaraSalvato, Alina Streblyanska, Pauline Zarrouk, Etienne Burtin, Scott F. Anderson, Julian Bautista, DmitryBizyaev, W. N. Brandt, Jonathan Brinkmann, Joel R. Brownstein, Johan Comparat, Paul Green, Axel dela Macorra, Andrea Muoz Gutirrez, Jiamin Hou, Jeffrey A. Newman, Nathalie Palanque-Delabrouille,Isabelle Pris, Will J. Percival, Patrick Petitjean, James Rich, Graziano Rossi, Donald P. Schneider,Alexander Smith, M. Vivek, and Benjamin Alan Weaver. The Sloan Digital Sky Survey Quasar Catalog:Sixteenth Data Release. The Astrophysical Journal Supplement Series, 250(1):8, September 2020. doi:10.3847/1538-4365/aba623. Chao Ma, Wenbo Gong, Jos Miguel Hernndez-Lobato, Noam Koenigstein, Sebastian Nowozin, and ChengZhang. Partial vae for hybrid recommender system. In NIPS Workshop on Bayesian Deep Learning, volume2018, 2018a. Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jos Miguel Hernndez-Lobato, Sebastian Nowozin,and Cheng Zhang. Eddi: Efficient dynamic discovery of high-value information with partial vae. arXivpreprint arXiv:1809.11142, 2018b.",
  "Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012": "Silvia Onorato, Joseph F. Hennawi, Jan-Torge Schindler, Jinyi Yang, Feige Wang, Aaron J. Barth, EduardoBaados, Anna-Christina Eilers, Sarah E. I. Bosman, Frederick B. Davies, Bram P. Venemans, ChiaraMazzucchelli, Silvia Belladitta, Fabio Vito, Emanuele Paolo Farina, Irham T. Andika, Xiaohui Fan, FabianWalter, Roberto Decarli, Masafusa Onoue, and Riccardo Nanni. Optical and near-infrared spectroscopy ofquasars at z > 6.5: public data release and composite spectrum. arXiv e-prints, art. arXiv:2406.07612,June 2024. doi: 10.48550/arXiv.2406.07612.",
  "Bradley M. Peterson. Reverberation Mapping of Active Galactic Nuclei. Publications of the AstronomicalSociety of the Pacific, 105:247, March 1993. doi: 10.1086/133140": "Adam G. Riess, Alexei V. Filippenko, Peter Challis, Alejandro Clocchiatti, Alan Diercks, Peter M. Garnavich,Ron L. Gilliland, Craig J. Hogan, Saurabh Jha, Robert P. Kirshner, B. Leibundgut, M. M. Phillips,David Reiss, Brian P. Schmidt, Robert A. Schommer, R. Chris Smith, J. Spyromilio, Christopher Stubbs,Nicholas B. Suntzeff, and John Tonry. Observational Evidence from Supernovae for an AcceleratingUniverse and a Cosmological Constant. The astronomical journal, 116(3):10091038, September 1998. doi:10.1086/300499. A. Sacchi, G. Risaliti, M. Signorini, E. Lusso, E. Nardini, G. Bargiacchi, S. Bisogni, F. Civano, M. Elvis,G. Fabbiano, R. Gilli, B. Trefoloni, and C. Vignali. Quasars as high-redshift standard candles. Astronomyand Astrophysics, 663:L7, July 2022. doi: 10.1051/0004-6361/202243411.",
  "Raquel Urtasun and Trevor Darrell. Discriminative Gaussian process latent variable model for classification.In Proceedings of the 24th international conference on Machine learning, pp. 927934, 2007": "Laurens van der Maaten. Learning a parametric embedding by preserving local structure. In David vanDyk and Max Welling (eds.), Proceedings of the Twelth International Conference on Artificial Intelligenceand Statistics, volume 5 of Proceedings of Machine Learning Research, pp. 384391, Hilton ClearwaterBeach Resort, Clearwater Beach, Florida USA, 1618 Apr 2009. PMLR. URL",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine LearningResearch, 9(86):25792605, 2008. URL": "Qiaoya Wu and Yue Shen. A Catalog of Quasar Properties from Sloan Digital Sky Survey Data Release 16.The Astrophysical Journal Supplement Series, 263(2):42, December 2022. doi: 10.3847/1538-4365/ac9ead. Jinyi Yang, Feige Wang, Xiaohui Fan, Aaron J. Barth, Joseph F. Hennawi, Riccardo Nanni, Fuyan Bian,Frederick B. Davies, Emanuele P. Farina, Jan-Torge Schindler, Eduardo Baados, Roberto Decarli, Anna-Christina Eilers, Richard Green, Hengxiao Guo, Linhua Jiang, Jiang-Tao Li, Bram Venemans, Fabian Walter,Xue-Bing Wu, and Minghao Yue. Probing Early Supermassive Black Hole Growth and Quasar Evolutionwith Near-infrared Spectroscopy of 37 Reionization-era Quasars at 6.3 < z 7.64. The AstrophysicsJournal, 923(2):262, December 2021. doi: 10.3847/1538-4357/ac2b32.",
  "22xqn,n": "xn,d is a scalar (dth dimension of point xn), kTn is a matrix 1 M - the nth row of Knm, we know thatp(fd|ud, Z) = N(KnmK1mmud, Knn KnmK1mmKmn). Further, fd(zn) is a scalar, denoting the value atindex zn of the vector fd. qn,n is the nth entry in the diagonal of the matrix Qnn = Knn KnmK1mmKmn.Next, performing the integration with respect to q(ud) = N(ud; md, Sd) yields,",
  "nlog p(zn)(24)": "where all the data dependent terms factorise enabling mini-batching of gradients. The shared model uses asum of ELBOs Lx + Ly with a shared latent embedding Z, which we call the joint evidence lower bound(Eq. (15) in the main paper). The additive structure ensures that the joint ELBO is also factorisable acrossdata points N.",
  "BExperimental set-up": "In this section we detail the configuration of the experiments in of the main paper. For each of thedatasets, we repeat every experiment with five random seeds yielding different splits of the training data.The attributes of the data and sparse GP set-up are given in . We used a learning rate of 0.001 across",
  ": Experimental configuration to reproduce experiments in section 3 of the main paper": "all parameters and ran the mini-batch loop with a batch size of 128 for 15,000 iterations on an Intel Core i7processor with a GeForce RTX 3070 GPU with 8GB RAM memory. In order to give an estimate of the scaleof the model for the 22k dataset we enclose a summary snapshot of the number of trainable parameters inour shared model.",
  "Attributes ()Spectra0.0731 0.00200.0707 0.0033Black hole mass0.1882 0.00460.1765 0.0054B. Luminosity0.1731 0.00430.1658 0.0084Eddington Ratio0.1707 0.00240.1653 0.0026": ": Summary of test-time reconstruction abilities for a smaller 1k dataset sampled with the sameSNR data quality cut of SNR > 10. Mean absolute error on denormalised data ( standard error of mean)evaluated on average of 5 splits with 90% of the data used for training in the 1k dataset. The performanceimprovement is not statistically significant for the spectra reconstruction and the bolometric luminosityprediction but favours the shared model for black hole mass and Eddington ratio.",
  "DSensitivity to SNR": "In we essentially plot the SNR per data point (test quasars) vs. the absolute error. The triangularscatter denotes a density imbalance: more data points are clustered at lower SNR values ( 1015) comparedto higher SNR values (>20). This is just an artefact of the data quality cut we deploy. The points seemevenly scattered across the range of absolute errors and SNR values. There doesnt appear to be a consistenttrend where absolute error systematically increases or decreases with SNR. There is a very weak negativecorrelation (more discernable in the zoomed in plots in the bottom row) for the black hole mass and weakpositive correlation for the luminosity. The unusual weak positive correlation can be better understood in thecontext of the count and missing pixels analysis () in each SNR bin. There are very few high SNR (>40) quasars in the dataset and further, these quasars also have more missing pixels than quasars at lowerSNRs. Since the training dataset is biased towards lower SNR quasars, the model generalises poorly to higherSNR examples, for luminosity and Eddington ratio. It is highly likely that the pattern of missing pixels issimilar among brighter objects and in turn among less luminous objects, making it difficult for the model toadapt to this shift at test time due to the acute data imbalance.",
  "EError vs. Uncertainty Calibration": "In , we quantify uncertainty calibration by computing the mean predictive uncertainty for increasingerror levels. We bin test points into five evenly spaced intervals over the range of the mean absolute errorfor each label. The x-axis denotes the centers of the bins for each label and the predictive uncertainty iscomputed as the square root of the variance the diagonal of the GP posterior predictive distribution.In each label, we observe the trend of predictive uncertainty increasing with absolute error, although thiseffect is very subdued at lower error levels. Further, it is important to note that there is inherent noise inthis calculation emanating from the differing number of points in each bin; this might explain some of thevariability in the bolometric luminosity uncertainty estimates.",
  "FCross-validating M and Q": "The two main parameters of our shared framework which need to be fixed at the outset are: the numberof inducing points M and the latent space dimensionality Q. We set M at 250 in all 22k experiments aftercross-validation to M = 1000 and found that M = 250 gave the best possible trade-off in terms of speed andaccuracy. The reconstruction results with M = 1000 were only marginally better than with inducing pointsM = 250, but the compute was significantly increased due to the cubic scaling in inducing points.",
  ": Sensitivity to Q: The negative ELBO objective for varying latent space dimensionality (lower isbetter)": "In , we visualize the evolution of the ELBOs across varying latent dimensionality. We notice ameaningful improvement in increasing the dimensionality from Q = 2 but very marginal gains beyondQ = 10; we use this setting in experiments. It may be important to highlight that due to automatic relevancedetermination of the squared exponential kernel, setting a high latent dimensionality should not degrade resultsas the model automatically prunes redundant dimensions by driving the corresponding inverse lengthscalesto 0. However, they do increase the compute cost, hence, it is important to set Q at a reasonable valuewhich is flexible enough for structure discovery and not too constrained, while simultaneously minimising thecomputational burden."
}