{
  "Abstract": "We introduce a new private regression setting we call Private Regression in Multiple Outcomes(PRIMO), inspired by the common situation where a data analyst wants to perform a set of lregressions while preserving privacy, where the features X are shared across all l regressions,and each regression i [l] has a different vector of outcomes yi. Naively applying existingprivate linear regression techniques l times leads to a l multiplicative increase in error overthe standard linear regression setting. We apply a variety of techniques including sufficientstatistics perturbation (SSP) and geometric projection-based methods to develop scalablealgorithms that outperform this baseline across a range of parameter regimes. In particular,we obtain no dependence on l in the asympotic error when l is sufficiently large. We apply ouralgorithms to the task of private genomic risk prediction for multiple phenotypes using datafrom the 1000 Genomes project and the Database of Genotypes and Phenotypes (dbGaP).Empirically, we find that even for values of l far smaller than the theory would predict, ourprojection-based method improves the accuracy relative to the variant that doesnt use theprojection.",
  "Introduction": "Linear regression is one of the most fundamental statistical tools used across the applied sciences, for bothinference and prediction. In genetics, polygenic risk scores Krapohl et al. (2018); Pattee & Pan (2020) arecomputed by regressing phenotype (e.g. disease status) onto individual genomic data (SNPs) in order toidentify genetic risk factors. In the social sciences, observed societal outcomes like income or marital statusmight be regressed on a fixed set of demographic features Agresti & Barbara (2009). In many of these caseswhere the data records correspond to individuals, there are two aspects of the problem setting that co-occur:",
  "Aspect 1. The individuals may have a legal or moral right to privacy that has the potential to be compromisedby their participation in a study": "Aspect 2. Multiple regressions will be ran using the same set of individual characteristics across each regressionwith different outcomes, either within the same study or across many different studies. Aspect 1 has been established as a legitimate concern through both theoretical and applied work. Theseminal paper of Homer et al. (2008) showed that the presence of an individual in a genomic dataset could beidentified given simple summary statistics about the dataset, leading to widespread concern over the sharingof the results of genomic analyses. In the machine learning setting, where what is being released is a model wtrained on the underlying data, there is a long line of research into Membership Inference Attacks\" Hu et al.(2021); Shokri et al. (2016), which given access to w are able to identify which points are in the training set.Over the last decade, differential privacy Dwork & Roth (2014) has emerged as a rigorous solution to theprivacy risk posed by Aspect 1. In the particular case of linear regression the problem of how to privatelycompute the optimal regressor has been studied in great detail, which we summarise in Subsection 2.1. Aspect 2 has been studied extensively from the orthogonal perspective of multiple hypothesis testing, butuntil now has not been considered in the context of privacy. The problem of overfitting or p-hacking in",
  "Published in Transactions on Machine Learning Research (01/2025)": "Michael J. Kearns. Efficient noise-tolerant learning from statistical queries. In S. Rao Kosaraju, David S.Johnson, and Alok Aggarwal (eds.), Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory ofComputing, May 16-18, 1993, San Diego, CA, USA, pp. 392401. ACM, 1993. doi: 10.1145/167088.167200.URL Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization andhigh-dimensional regression. In Shie Mannor, Nathan Srebro, and Robert C. Williamson (eds.), Proceedingsof the 25th Annual Conference on Learning Theory, volume 23 of Proceedings of Machine Learning Research,pp. 25.125.40, Edinburgh, Scotland, 2527 Jun 2012. PMLR. URL Keegan Korthauer, Patrick K. Kimes, Claire Duvallet, Alejandro Reyes, Ayshwarya Subramanian, MingxiangTeng, Chinmay Shukla, Eric J. Alm, and Stephanie C. Hicks. A practical guide to methods controlling falsediscoveries in computational biology. Genome Biology, 20(1):118, 2019. doi: 10.1186/s13059-019-1716-1.URL E. Krapohl, H. Patel, S. Newhouse, C J Curtis, S. von Stumm, P S Dale, D. Zabaneh, G. Breen, P F OReilly,and R. Plomin. Multi-polygenic score approach to trait prediction. Molecular Psychiatry, 23(5):13681374,2018. doi: 10.1038/mp.2017.163. URL",
  "Preliminaries": "We start by defining the standard linear regression problem. Let X X n Rnd consist of d-dimensionalsamples from n individuals, yi Yn be a vector of n outcomes for a regression indexed by i, and parameterspace W Rd denote the subset of linear regression coefficients we will optimize over. Following conventionin prior work, we will let ||X||2, ||Y||2, ||W||2 denote supxX ||x||2, supyY ||y||2, supwW ||w||2 respectively.",
  "nXT yi": "The definition of data privacy we use throughout is the popular (, )-differential privacy introduced in Dworket al. (2006). We refer the reader the to Dwork & Roth (2014) for an overview of the basic properties of(, )DP including closure under post-processing and advanced composition. In order to define a differentiallyprivate mechanism, we first define what it means for two datasets to be adjacent. Definition 1. Two datasets (X, y), (X, y) X n Yn are adjacent if they differ in a single element, e.g.there exists i, i [n], such that X {xi} \\ {xi} = X, y {yi} \\ {yi} = y. We say that (X, y), (X, y) arefeature-adjacent if they share labels y = y, and i [n] such that X {xi} \\ {xi} = X. Similarly, they arelabel-adjacent if X = X, and i [n] such that y {yi} \\ {yi} = y.",
  "Pr[M(X, Y ) O] e Pr[M(X, Y ) O] + (1)": "When Definition 1 holds for adjacency defined over (xi, yi) pairs, this is the standard setting of differentialprivacy, which we call Full DP in order to differentiate it from its relaxations. In the less restrictive casewhere Equation 1 holds only over pairs of label-adjacent datasets, we are in the well-studied setting of label",
  "nl||XW Y ||2F <": "We will use yj to denote the vector of n outcomes for the jth outcome, and yi Yl for the vector of l outcomescorresponding to individual i [n]. We note that in PRIMO under full DP, the adjacency condition holdsover pairs (xi, yi). The (, )-DP algorithm we will use most throughout is the Gaussian Mechanism. Westate a version of the Gaussian mechanism with constant c(, ) that is valid for all > 0, which follows fromanalyzing the mechanism using Renyi DP Mironov (2017) and converting back to (, )-DP.Lemma 1 (Dwork & Roth (2014)). Let f : X n Rd an arbitrary d-dimensional function, and define itssensitivity 2(f) = supXX ||f(X) f(X)||2, where X X are datasets that differ in exactly one element.Then the Gaussian mechanism GaussMech(, , ) releases f(X)+N(0, 2), and is (, )-differentially private",
  "Private Linear Regression": "Private linear regression is well-studied under a variety of different assumptions on the data generatingdistributions and parameter regimes. Typically analysis of private linear regression is done either under thefully agnostic setting where only parameter bounds ||X||2, |Y|, ||W||2 are assumed, under the assumption ofa fixed design matrix and y generated by a linear Gaussian model (the so-called realizable case), or underthe assumption of a random design matrix Milionis et al. (2022). In this paper we focus on the first fullyagnostic setting, because in our intended applications within the social and biomedical sciences in generalwe neither have realizability (when y is actually a linear function of x) or Gaussian features. In the fullyagnostic (not realizable) setting Wang (2018) provides a comprehensive survey of existing private regressionapproaches and bounds, including a discussion of the impact of different parameter norms, and proposing anew adaptive technique. Broadly speaking, techniques for private linear regression fall into 4 classes, sufficient statistics perturbation(SSP) Vu & Slavkovic (2009); Foulds et al. (2016), Objective Perturbation (ObjPert) Kifer et al. (2012),Posterior sampling Dimitrakakis et al. (2016), and privatized (stochastic) gradient descent (NoisySGD)Chaudhuri et al. (2011). The methods in this paper are a sub-class of SSP-based methods, which correspondto Algorithm 1 where l = 1. Methods based on SSP rely on perturbing XT X and XT y Xy separately,and then using these noisy estimates to compute the (regularized) least squares estimator:",
  "n +d2": "n22 )).Neither of these bounds are exactly applicable to our setting; the bound of Bassily et al. (2014) is notneccessarily tight for linear regression, and the bound of Cai et al. (2020) makes realizability assumptionsthat clearly fail to hold for the type of genomic applications that motivate our work. We also note that allexisting lower bounds for private linear regression are under full DP, and so do not immediately apply to thealgorithms in which operate in the weaker feature-DP or label-DP settings. In general, the focus ofthis work is on upper bounds, and so we highlight this curious lack of relevant lower bounds in the literatureas a potential future direction, given the long line of work on upper bounds for private linear regression.Acknowledging the above issues with existing bounds, throughout the paper well use the lower bound ofBassily et al. (2014) as a benchmark for the cost to accuracy of taking l > 1, and in settings in which thebounds for PRIMO match this lower bound we will say we have PRIMO for Free. Given any (, )-DP algorithm for computing wj privately, we can use it as a sub-routine to solve PRIMO bysimply running it l times to compute each column of W. Hence by running any of the optimal algorithms (forexample SSP Vu & Slavkovic (2009)) l times with parameters /",
  "Results": "The primary contribution of this work is to introduce the novel Private Regression in Multiple Outcomes(PRIMO) problem, and to provide a class of algorithms that trade off accuracy, privacy, and computation. Inaddition to introducing the PRIMO problem, to our knowledge we are the first to apply private query releasemethods to linear regression (). We also provide a compelling practical application of our algorithmsto the problem of genomic risk prediction with multiple phenotypes using data from The 1000 GenomesProject Fairley et al. (2019) and The Database of Genotypes and Phenotypes Mailman et al. (2007). Algorithm 1 is our meta-algorithm for the PRIMO problem, and has two variants, ReuseCovGauss whichcorresponds to M = GaussMech, and ReuseCovProj, which corresponds to M = Algorithm 2. Algorithm 2itself has two variants, which correspond to the label private setting (GaussProjX), and to the featureprivate setting (GaussProjY ) respectively. In we adapt the previously proposed sufficient statisticsperturbation (SSP) algorithm of Vu & Slavkovic (2009); Foulds et al. (2016) into the ReuseCovGauss Algorithm.Via a novel accuracy analysis of SSP for the case when the privacy levels for E1, E2i differ in Equation 2(Theorem 1), we show that since the noisy covariance matrix is reused across the regressions (as it onlydepends on X), by allocating the majority of our privacy budget to computing this term, we are able toobtain PRIMO for Free when l is sufficiently small, because the error term is dominated by the error incomputing the noisy covariance matrix, which does not depend on l, rather than the error from computingthe noisy association term (row 1 of ). When l > ||X||22||W||22",
  "nXT Y + N(0, 2) returned by the Gaussian Mechanism onto the space of feasiblevalues that the non-private quantity 1": "nXT Y could take (Line 5 of Algorithm 2), can reduce the error forsufficiently large l. This set of feasible values is the image of the domain of Y under XT when X isconsidered public, and it is the image of the domain of X under Y T when Y is considered public. Theassumption that either X or Y are public are critical to these algorithms, as computing the projection dependson the space we are projecting into in a way that would not be easily privatized. The setting where X ispublic and Y is private corresponds to feature DP, and the setting where X is public and Y is public is labeldifferential privacy. In rows 3 5 in we summarise the guarantees of Theorems 3,4 that characterizethe accuracy in these settings. In both the Feature DP and Label DP settings, the projection improves thedependence in the error on l, d, at the cost of a factor of n in the denominator. As a result, until l, d aresufficiently large relative to n wed expect the projection to actually increase error. We discuss these boundsfurther in . In , we implement our ReuseCovGauss and ReuseCovProj algorithms using SNP data from two ofthe most common genomic databases Mailman et al. (2007); Fairley et al. (2019). We compare our algorithmsto each other, and to the non-private baseline. In both settings we reach the surprising conclusion thateven for relatively small values of l = 11, 100, much smaller than the theory would predict, ReuseCovProjoutperforms ReuseCovGauss. Moreover, ReuseCovProj is able to achieve non-trivial MSE (as measured byR2) for very large values of l, as predicted by Theorem 3.",
  "Full DP: The ReuseCovGauss Algorithm": "We start by presenting our first algorithm for PRIMO, ReuseCovGauss, which corresponds to Algorithm 1with M = GaussMech. To understand the intuition behind ReuseCovGauss, we start from the analysis of theridge regression variant of SSP in Wang (2018). Equation 13 in Wang (2018) shows that if wi is the noisyridge regressor output by SSP, and wi is the (non-private) OLS estimator, then w.p. 1 :",
  ", log(1/), log(1/)": "Proof. The privacy proof follows from a straightforward application of the Gaussian mechanism. We notethat releasing each vi privately, is equivalent to computing XT Y + E2, where E2 Ndl(0, 22). Nowit is easy to compute l-sensitivity (f) of f(X) = XT Y . Fix an individual i, and an adjacent datasetX = X/{xi, yi} {xi, yi}. Then f(X) f(X) = V = [yi1xi yi1xi1, . . . yilxi yilxil]. Then:",
  "Hence setting 2 = c(, )2": "l||X||2||Y||2/ by the Gaussian mechanism Dwork & Roth (2014) publishing Vsatisfies (/2, /2) DP. Similarly if g = XT X, (g) ||X||22, and so setting 1 = c(, )||X||22/, meanspublishing I is (/2, /2)-DP. By basic composition for DP, the entire mechanism is (, )-DP. To prove the accuracy bound we follow the general proof technique developed in Wang (2018) analysingthe accuracy guarantees of the ridge regression variant of SSP in the case min(XT X) = 0, adding somemathematical detail to their exposition, and doing the appropriate book-keeping to handle our setting wherethe privacy level (as a function of the noise level) guaranteed by E1 and E2 differ. The reader less interestedin these details can skip to Equation 11 below for the punchline.",
  "Improved Algorithms for Large l": "The improvement of ReuseCovGauss over the naive PRIMO baseline in the previous section only applies inthe parameter regime where the asymptotic error in Equation 5 is dominated by the covariance error term. Inthe regime where the association error term dominates, ReuseCovGauss still incurs a l multiplicative factorin the error term, which does not improve over the baseline. In this section we show that via a reduction fromprivately computing the association term to computing the image of a private vector under a public matrix,that under the weaker feature differential privacy we can obtain improved bounds for PRIMO over both thenaive baseline and ReuseCovGauss when l >n",
  "Let us reconsider the problem of privately computing the association term 1": "nXT Y where Y Ynl, X X n Rnd, and Y is considered public, so M is only constrained to be feature DP. Let vec(X) =(x11, . . . , x1n, x21, . . . xd1, . . . xdn) Rnd. Then since1nXT Y is linear in every entry of X, there exists amatrix C(Y ) 1nYdldn such that1nXT Y = C vec(X).We explicitly derive an expression for C in",
  "Algorithm 2 is our projection-based subroutine for privately computing 1": "nXT Y , which we call GaussProjY .We first state the squared error of GaussProjY in Theorem 2, and then translate this error into the PRIMOerror of ReuseCovProjY (Algorithm 1 with M = Algorithm 2) in Theorem 3. The crux of the proof isLemma 8 from Nikolov et al. (2013), which uses a geometric argument to bound the error after the projection.",
  "n2), which for l": "d n is strictly larger than the error of the projection mechanism. We also notethat the bound in Theorem 3 is strictly better than the error given by applying the Median Mechanismalgorithm of Blum et al. (2011) for low-sensitivity queries, which is tailored for l error, and which alsorequires discrete X. For example, when Y = {0, 1}, X = {0, 1}d, then |Y| = 1, ||X||2 =",
  "which is a factor ofn": "d1/4 worse than the lower bound. We also note that the bound in Theorem 3 is strictlybetter than the error given by applying the Median Mechanism algorithm of Blum et al. (2011) for low-sensitivity queries, which is tailored for l error, and which also requires discrete X. For example, whenY = {0, 1}, X = {0, 1}d, then |Y| = 1, ||X||2 =",
  "PRIMO Under Label Differential Privacy": "The setting where X is public and Y is considered private is strictly easier to satisfy than setting in whichthey are both considered private, and in particular in order for Algorithm 1 to satisfy label DP, we onlyneed to add noise to terms that involve Y . Thus we can compute I in Line 2 of Algorithm 1 exactly withoutadding any noise E1. We still need to compute the association term 1",
  "n ) < O( dl2": "n2 ) =l > n23 , which we summarise in row 5 of . Interestingly, in contrast to the public label setting, thiscondition has no dependence on the dimension d. Substituting Equation 16 into Equation 7 with E1 = 0since we are in the label-private setting and dont have to add noise to the covariance matrix, we can optimizeover to get:Theorem4.LetAdenotethelabel-privatevariantofAlgorithm1withE1=0, M=GaussProjX(X, Y, /2, /2). Then A is an (, , , ) solution to the label DP PRIMO problem with",
  "Computational Efficiency": "In this section we first analyze the computational complexity of Algorithm 1, which we implement moreefficiently in Algorithm 3. Theorem 5 shows this is O(max(min(nl2, n2l), nld, nd2, ld2, d3)), which for n > d > lis O(nd2). Subsection 6.2 shows how sketching the covariance matrix using s points can maintain accuracywhile reducing this to O(sd2).",
  "Step 3: In the case where M is the projection algorithm, computing the projection argminvC(X n)||vvi||22, (nl min(n, l) + nd + nld via diagonalization)": "Forming the covariance matrix XT X is a matrix multiplication of two d n matrices, which can be done viathe naive matrix multiplication in time O(nd2), and via a long-line of fast matrix multiplication algorithmsin time O(d2+(n)); For example if n < d.3 it can be done in time that is essentially O(d2) Gall (2012). Step 2can be completed by solving the equation I wi = vi, i = 1 . . . l via the conjugate gradient method, which takestime O((I)d2 log(1/)) to compute an -approximate solution Mahoney (2011) where (I) is the conditionnumber. We note that this has to be done separately for each i = 1 . . . l giving total time O(l (I)d2 log(1/)).Alternatively, an exact solution I1vi can be computed directly using the QR decomposition of the matrixI. The decomposition I = QR can be computed in time O(d3) Mahoney (2011) and does not depend onthe vi, after which using R wi = QT vi, wi can be computed in time O(d2) via backward substitution. Thisgives a total time complexity of O(d3 + ld2). So if d and1",
  "s.t. ||x||2 n||X||2(18)": "Now, given the spectral decomposition of A = UU T , and the coordinates of b in the eigenbasis U T b, Lemma2.2 in Hager (2001) gives a simple closed form for x that computes each coordinate in constant time. Sincethere are nd coordinates of x, this incurs an additional additive factor of O(nd) in the complexity, which isdominated by the cost of diagonalizing A. So the complexity of this step is the complexity of diagonalizingA = CT C, or equivalently finding the right singular vectors of C, plus the complexity of computing U T b. Thisis seemingly bad news, as C Rdndl is a very high-dimensional matrix, and the complexity for computingthe SVD of C without any assumptions about its structure is O(d3ln min(l, n)) Golub & Van Loan (1996).However, it is evident from the construction of C in Subsection 5 that",
  "Sketching ReuseCov": "The discussion in the previous section shows that when n > d > l, the complexity of Algorithm 3 is O(nd2) orthe cost of forming the covariance matrix. In this section we show how sub-sampling s < n points can improvethis to O(sd2) by giving an analysis of sub-sampled SSP. The key ingredient is marrying the convergence ofthe sub-sampled covariance matrix to XT X with the accuracy analysis of SSP we saw in .",
  "(22)": "So it suffices to bound each term with high probability. The second term, || ws ws||XT X+I can be boundedusing the same arguments as in Theorem 1, with small differences due to scaling. Crucially though, as weneed to bound this in the norm induced by XT X rather than XTS XS, we will need to utilize the convergenceof XTS XS XT X via Matrix-Chernoff bounds.",
  "Experiments": "Datasets. We evaluate the accuracy of our algorithms ReuseCovGauss and ReuseCovProjX on the task ofgenomic risk prediction using real X data from two of the largest publicly available databases, and simulatedoutcomes Y so that we can easily vary the number of outcomes l. In addition, in Appendix 8.6 we include",
  "1nl SStot . We use R2 because it is easier to interpret given that achieving": "error better than the constant predictor implies R2 (0, 1]. In Figures 3d,4 we also visualize the resultsusing a different metric, the log of the ratio of the MSE of the private predictor to the MSE of the optimalnon-private predictor, where a larger ratio indicates a greater multiplicative error. Realistic n, varying l. We find that for values of l starting as small as 11 for 1KG and 101 for dbGaP,ReuseCovProjX achieves higher R2 than ReuseCovGauss, and is able to achieve non-trival R2 for large l,which is consistent with our Theorem 3. For l < 11 on 1KG and l < 101 on dbGaP, ReuseCovGauss achieveshigher R2. In addition to being consistent with the theory, this is intuitive; when l is small the bias inducedby the projection outweighs the reduction in the overall noise. Figures 2a, 2b show that for l > 101, 801 on1KG and dbGaP respectively ReuseCovGauss has negative R2 by contrast, zooming in on the R2 curvefor ReuseCovProj in Figures 2c, 2d we are able to achieve non-trivial R2 at all values of l, and see minimalincrease in error as l increases. Large n, varying l. Our upper bounds suggest that for large values of n, ReuseCovGauss will outperformReuseCovProjX, and that the MSE should increase polynomially with increasing l, or equivalently linearlyin log l when we take the log ratio. Both of these trends can be seen in (a). Given that the relativeperformance of ReuseCovProjX is improving as l increases, we expect that for larger values of l that werecomputationally prohibitive to run at such large n, ReuseCovProjX would again be the superior algorithm.",
  "Additional Ablations. In the setting where l <n": "d (Row 2 of ), theory suggests our algorithmsshould not outperform naive composition of private regressions. Since private regression methods like DP-SGDBassily et al. (2019) are known to equal or outperform SSP, our ReuseCov algorithms which are variants ofSSP might actually perform worse than composing DP-SGD across l regressions. We construct such a datasetand show this is in fact the case in in the Appendix. The experiments above on genomic data generate outcomes Y from a noisy linear model; in in theAppendix we show that generating outcomes from a 2-layer MLP with noise doesnt change the relativeordering of the algorithms by performance.",
  "(f)": ": Each R2 value is averaged over 10 iterations.The shaded area around the lines indicatesthe error bars for the R2 value at a given value of (l, d). In Figures (a)-(d) we plot the average R2 ford = 25, l = (1, 11, 101, 201, 401, 601, 801, 1001), fixing (, ) = (5, 1",
  "(d)": ": Comparing the log of the ratio of the squared loss of the private estimator to the square lossof the OLS estimator.Each value is averaged over 10 iterations.The shaded area around the linesindicates the error bars at the given value of l.(a) and (b) range l up to 100, 000, (c) and (d) showl = (1, 11, 101, 201, 401, 601, 801, 1001) while we fixed d = 25 and (, ) = (5, 1",
  "Limitations": "There are 3 main criticisms that can be levied at the results in this paper. The first is that the reductions inerror in the feature DP and label DP settings due to projection in Lines 3 5 of rely on sufficientlylarge l : l >n d, l > n2/3 respectively. As a result, there are practical settings where n is particularly largeor l is small where the theory does not suggest ReuseCovProj will outperform ReuseCovGauss. The secondcritique is that this improvement requires us relaxing our notion of DP to label or feature DP. In settingswhere both X and Y are highly sensitive, if we want to guarantee full DP ReuseCovGauss would be is ouronly option. Finally, all of our algorithms are variants of SSP Vu & Slavkovic (2009), which has complexitythat scales super-linearly in d due to forming the covariance matrix XT X. For high-dimensional regressionsettings algorithms like private SGD are preferred due to their scalability Chaudhuri et al. (2011). Withrespect to the first critique, we note that in common settings, like in genomics, d can be very large relative ton, and son",
  "(b)": ": Both (a) and (b) show the log of the ratio of the squared loss of the private estimator to the squareloss of the OLS estimator, but in (a) the outcomes are synthetically generated from a linear model, whereas in(b) the outcomes are generated from a 2-layer neural network, to test how the algorithms perform when theoutcomes are not generated by a linear model. All values are averaged over 5 iterations, over l = (1, 11, 101)with (, ) = (5, 1",
  "n2 ) and n = 5 1e5 on a synthetic dbGaP dataset": "that l > n2/3. We also note that our experimental results show that the projection improves error at valuesof l much smaller than the theory would suggest, making this algorithm the practical choice for values of l assmall as 11! Finally, the ReuseCovGuass Algorithm is itself also a contribution of this work. On the issue ofscalability in Subsections 6.1, 6.2 in the Appendix we consider the computational complexity of our methods.When n > d > l, the computational complexity of the ReuseCovGauss algorithm is the cost of computing thecovariance matrix XT X, which is O(nd2). In Subsection 6.2 in the Appendix we show this can be reduced toO(sd2), where s < n is the number of rows sub-sampled without replacement from X, and we quantify theresulting error from sampling (Theorem 6). Despite these improvements, the computational complexity ofour methods is still super-linear in d.",
  "Alan Agresti and Finlay Barbara. Statistical methods for the social sciences. Pearson Prentice Hall, 2009": "Sergl Aydre, William Brown, Michael Kearns, Krishnaram Kenthapadi, Luca Melis, Aaron Roth, andAmaresh Ankit Siva. Differentially private query release through adaptive projection. In Marina Meila andTong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021,18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 457467.PMLR, 2021. URL",
  "Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex optimizationwith optimal rates. CoRR, abs/1908.09970, 2019. URL": "Raef Bassily, Kobbi Nissim, Adam D. Smith, Thomas Steinke, Uri Stemmer, and Jonathan R. Ullman.Algorithmic stability for adaptive data analysis. SIAM J. Comput., 50(3), 2021. doi: 10.1137/16M1103646.URL Brett Beaulieu-Jones, Zhiwei Wu, Chris Williams, Ran Lee, Sanjeev Bhavnani, James Byrd, and CaseyGreene. Privacy-preserving generative deep neural networks support clinical data sharing. Circulation:Cardiovascular Quality and Outcomes, 12, 07 2019. doi: 10.1161/CIRCOUTCOMES.118.005122.",
  "Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foundations and Trendsin Theoretical Computer Science, 9(3-4):211407, 2014. URL #DworkR14": "Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves:Privacy via distributed noise generation. In Serge Vaudenay (ed.), Advances in Cryptology - EUROCRYPT2006, 25th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St.Petersburg, Russia, May 28 - June 1, 2006, Proceedings, volume 4004 of Lecture Notes in Computer Science,pp. 486503. Springer, 2006. doi: 10.1007/11761679\\_29. URL",
  "William W. Hager. Minimizing a quadratic over a sphere. SIAM J. Optim., 12:188208, 2001": "Moritz Hardt and Guy N. Rothblum. A multiplicative weights mechanism for privacy-preserving dataanalysis. In 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, pp. 6170, 2010.doi: 10.1109/FOCS.2010.85. Nils Homer, Szabolcs Szelinger, Margot Redman, David Duggan, Waibhav Tembe, Jill Muehling, John V.Pearson, Dietrich A. Stephan, Stanley F. Nelson, and David W. Craig. Resolving individuals contributingtrace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays. PLOSGenetics, 4(8):19, 08 2008. doi: 10.1371/journal.pgen.1000167. URL",
  "Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu, and Xuyun Zhang. Membershipinference attacks on machine learning: A survey. ACM Computing Surveys (CSUR), 2021": "Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam D. Smith, and Jonathan R. Ullman. The priceof privately releasing contingency tables and the spectra of random matrices with correlated rows. InLeonard J. Schulman (ed.), Proceedings of the 42nd ACM Symposium on Theory of Computing, STOC 2010,Cambridge, Massachusetts, USA, 5-8 June 2010, pp. 775784. ACM, 2010. doi: 10.1145/1806689.1806795.URL",
  "Michael W. Mahoney. Randomized algorithms for matrices and data. CoRR, abs/1104.5557, 2011. URL": "Matthew D Mailman, Michael Feolo, Yan Jin, Michi Kimura, Kimberly Tryka, Rinat Bagoutdinov, Li Hao,Alex Kiang, Justin Paschall, Lon Phan, Natalia Popova, Sherri Pretel, Luda Ziyabari, Ming Lee, YongzhaoShao, Zhining Y Wang, Karl Sirotkin, Michael Ward, Maxim Kholodov, Karen Zbicz, Judith Beck, MichaelKimelman, Sergey Shevelev, Don Preuss, Elena Yaschenko, Alexander Graeff, James Ostell, and Stephen TSherry. The ncbi dbgap database of genotypes and phenotypes. Nature genetics, 39(10):11811186, 2007.",
  "Ilya Mironov. Renyi differential privacy. CoRR, abs/1702.07476, 2017. URL": "S. Muthukrishnan and Aleksandar Nikolov. Optimal private halfspace counting via discrepancy. In Howard J.Karloff and Toniann Pitassi (eds.), Proceedings of the 44th Symposium on Theory of Computing Conference,STOC 2012, New York, NY, USA, May 19 - 22, 2012, pp. 12851292. ACM, 2012. doi: 10.1145/2213977.2214090. URL Seth V. Neel, Aaron L. Roth, and Zhiwei Steven Wu. How to use heuristics for differential privacy. In2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS), pp. 7293, 2019. doi:10.1109/FOCS.2019.00014.",
  "Nicolas Papernot, Martn Abadi, lfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervisedknowledge transfer for deep learning from private training data, 2016. URL": "Jack Pattee and Wei Pan. Penalized regression and model selection methods for polygenic scores on summarystatistics. PLOS Computational Biology, 16(10):127, 10 2020. doi: 10.1371/journal.pcbi.1008271. URL Florian Priv.Using the UK biobank as a global reference of worldwide populations: application tomeasuring ancestry diversity from GWAS summary statistics. Bioinform., 38(13):34773480, 2022. doi:10.1093/bioinformatics/btac348. URL",
  "Reza Shokri, Marco Stronati, and Vitaly Shmatikov. Membership inference attacks against machine learningmodels. CoRR, abs/1610.05820, 2016. URL": "Justin Thaler, Jonathan R. Ullman, and Salil P. Vadhan. Faster algorithms for privately releasing marginals. InArtur Czumaj, Kurt Mehlhorn, Andrew M. Pitts, and Roger Wattenhofer (eds.), Automata, Languages, andProgramming - 39th International Colloquium, ICALP 2012, Warwick, UK, July 9-13, 2012, Proceedings,Part I, volume 7391 of Lecture Notes in Computer Science, pp. 810821. Springer, 2012. doi: 10.1007/978-3-642-31594-7\\_68. URL",
  "Joel A. Tropp. Improved analysis of the subsampled randomized hadamard transform. CoRR, abs/1011.1595,2010. URL": "Jonathan R. Ullman and Salil P. Vadhan. Pcps and the hardness of generating private synthetic data.In Yuval Ishai (ed.), Theory of Cryptography - 8th Theory of Cryptography Conference, TCC 2011,Providence, RI, USA, March 28-30, 2011. Proceedings, volume 6597 of Lecture Notes in Computer Science,pp. 400416. Springer, 2011. doi: 10.1007/978-3-642-19571-6\\_24. URL",
  "Additional Related Work": "Query Release.In Subsection 5 we show that privately computing the association term1nXT Y isequivalent to the problem of differentially privately releasing a set of l d low-sensitivity queries Bassily et al.(2021). While less is known about the optimal l2 error for arbitrary low-sensitivity queries, it is clear thatgeometric techniques based on factorization and projection do not (at least obviously) apply, since there isno corresponding notion of the query matrix AQ Edmonds et al. (2019). In the case where X, Y are bothprivate, this corresponds to releasing a subset of l d 2-way marginal queries over d + l dimensions, which iswell-studied Dwork et al. (2015b); Thaler et al. (2012); Ullman & Vadhan (2011). In the related work sectionof the Appendix we discuss why applying existing algorithms for private release of marginals seems unlikelybe able to both improve over the Gaussian Mechanism and achieve non-trivial error. However, in the lessrestrictive but still practically relevant setting where the labels Y are public, computing the association term1nXT Y is equivalent to the problem of differentially privately releasing a special class of dl low-sensitivity\"queries we term inner product queries\" (Definition 6). For this special class of queries we can adapt theprojected Gaussian Mechanism of Aydre et al. (2021) that is optimal for linear queries under l2 error inthe sparse regime. We are able to obtain greatly improved results over the naive Gaussian Mechanism inthe regime where l >n d, as summarised in and presented in Subsection 5. This is a key regime ofinterest, particularly for genomic data, where d the number of SNPs could be in the hundreds of millions, nthe population size in the hundreds of thousands ( 400K for UK Biobank Priv (2022) one of the mostpopular databases), and l the number of recorded phenotypes could be in the thousands. Projection mechanisms and Algorithm 2.Since we make heavy use of projection mechanisms inSubsection 5 in the public label setting, we elaborate on the difference between the existing methods andour Algorithm 2. The most technically related work to our Algorithm 2 is Aydre et al. (2021), which isitself a variant of the projection mechanism of Nikolov et al. (2013). There are several key differences in ouranalysis and application.The projection in Nikolov et al. (2013) is designed for linear queries over a discretedomain, and runs in time polynomial in the domain size. Algorithm 2 allows continuous X, Y and runs intime polynomial in n, d, l. Like the relaxed projection mechanism Aydre et al. (2021), when our data isdiscrete we relax our data domain to be continuous in order to compute the projection more efficiently. Unlikein their setting which attempts to handle general linear queries, due to the linear structure of inner productqueries our projection can be computed in polynomial time via linear regression over the l2 ball, as opposedto solving a possibly non-convex optimization problem. Moreover, due to this special structure we can use thesame geometric techniques as in Nikolov et al. (2013) to obtain theoretical accuracy guarantees (Theorem 2). Private release of 2-way marginalsConsider the problem of privately releasing all 2-way marginalsover n points in {0, 1}d+l. Theorem 5.7 in Dwork et al. (2015b) gives a polynomial time algorithm based onrelaxed projections that achieves mean squared error O(n d + l), which matches the best known informationtheoretic upper bound Dwork et al. (2015b), although there is a small gap to the existing lower boundmin(n, (d + l)2). This relaxed projection algorithm outperforms the Gaussian Mechanism when n",
  "log |Q| log |X|1/4 , the high accuracy regime, is achieved by thesimple and efficient Gaussian Mechanism Dwork & Roth (2014), which is also optimal over worst-case sets ofqueries Q Bun et al. (2013)": "Linear Queries under l2-loss.For the l2 error, in the high accuracy n |Q| regime the factorizationmechanism achieves error that is exactly tight for any workload of linear queries Q up to a factor of log(1/),although it is not efficient (Theorem Edmonds et al. (2019)). In the low-accuracy regime, the algorithm ofNikolov et al. (2013) that couples careful addition of correlated Gaussian noise (akin to the factorizationmechanism) with an l1-ball projection step achieves error within log factors of what is (a slight variant) of aquantity known as the hereditary discrepancy opt,(A, n) (Theorem Nikolov et al. (2013)). This quantityis a known lower bound on the error of any (, ) mechanism for answering linear queries Muthukrishnan& Nikolov (2012), and so the upper bound is tight up to log factors in |Q|, |X|. Theorem 21 in Nikolovet al. (2013) analyzes the simple projection mechanism that adds independent Gaussian noise and projectsrather than first performing the decomposition step that utilizes correlated Gaussian noise, achieving errorO(nd log(1/) log |X|/), which matches the best known (worst case over Q) upper bound for the sparsen < d case Gupta et al. (2011). In our Theorem 2 we give such a universal upper bound, rather than onethat depends on the hereditary discrepancy of the matrix Y . While the bound can of course be improved fora specific set of outcomes Y by the addition of the decomposition step to the projection algorithm, we omitthis step in favor of a simpler algorithm with more directly comparable bounds to existing private regressionalgorithms. While the information-theoretic l or l2 error achievable for linear queries is well-understood Edmonds et al.(2019); Nikolov et al. (2013); Bun et al. (2013), as synthetic data algorithms like PrivateMultiplicativeWeightsand MedianMechanism, or the factorization or projection mechanisms are in general inefficient, there aremany open problems pertaining to developing efficient algorithms for specific query classes, or heuristicapproaches that work better in practice. Examples of these approaches along the lines of the factorizationmechanism McKenna et al. (2018; 2019), efficient approximations of the projection mechanism Aydre et al.(2021); Dwork et al. (2015b), and using heuristic techniques from distribution learning in the framework ofiterative synthetic data algorithms Yoon et al. (2019); Torkzadehmahani et al. (2020); Beaulieu-Jones et al.(2019); Neel et al. (2019). Sub-sampled Linear Regression.In Subsection 6.2 we analyze SSP where we first sub-sample a randomset of s points without replacement, and use this sub-sample to compute the noisy covariance matrix.Sub-sampled linear regression has been studied extensively absent privacy, where it is known that uniformsub-sampling is sub-optimal in that it produces biased estimates of the OLS estimator, and performs poorlyin the presence of high-leverage points Derezinski et al. (2018). To address these shortcomings, techniquesbased on leverage score sampling Drineas et al. (2006), volume-based sampling Derezinski & Warmuth(2017)Derezinski et al. (2018), and spectral sparsification Lee & Sun (2015) have been developed. Crucially, inthese methods the probability of a point being sub-sampled is data-dependent, and so they are (not obviously)compatible with differential privacy.",
  "i=1Zi) > 1": "Lemma 11 (folklore e.g. Wang et al. (2018)). Given a dataset X n of n points and an (, )DP mechanismM. Let the procedure subsample take a random subset of s points from X n without replacement. Then if = s/n, the procedure M subsample is (O(), )DP for sufficiently small .",
  "Proof of Theorem 6:": "Proof. Our analysis will hinge on the case where l = 1 e.g. that of standard private linear regression, which wewill extend to the PRIMO case by our choice of as in the proof of Theorem 1. The fact that the Algorithmis (O(), ) private follows immediately from the Gaussian mechanism, and the secrecy of the sub-samplelemma (Lemma 11), which is why we can set 1 = n",
  "DP-SGD outperforms ReuseCov for l <n": "d. We construct our dataset by sub-sampling n = 5000 imagesof dimension d = 784 from MNIST Deng (2012), and generating l = 1, 11, 101 outcomes from a noisy linearmodel with unit norm. We implement DP-SGD using the Opacus Yousefpour et al. (2021) library fromMeta, and do privacy accounting across regressions by composing in zCDP and then converting back to(, )DP. shows that DP-SGD outperforms our ReuseCov algorithms when l (1, 11, 101), although,in agreement with the theory, this gap shrinks as l increases."
}