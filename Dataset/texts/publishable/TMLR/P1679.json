{
  "Abstract": "We obtain asymptotic minimax optimal posterior contraction rates for estimation of proba-bility distributions on d under the Wasserstein-p metrics using Bayesian Histograms. Tothe best of our knowledge, our analysis is the first to provide minimax posterior contractionrates for every p 1 and problem dimension d 1. Our proof technique takes advantageof the conjugacy of the Bayesian Histogram.",
  "x yp2 d(x, y)1/p,(1)": "where M(1, 2) is the set of couplings of 1 and 2; specifically the joint probability measures on E Ewith marginals 1 and 2 respectively. Some benefits of using the Wasserstein metric include its sensitivityto distance in the underlying space, ability to compare distributions regardless of continuity level, and its 1-dimension equivalent representation as the Lp distance between quantile functions, which facilitates quantilefunction inference (Zhang et al., 2020). In this paper we study the problem of nonparametrically estimating a distribution P0 on Ed (where E =) under the Wasserstein metric from n independent and identically distributed (i.i.d) random variablesY1, . . . , Yn drawn from P0.Our focus is on the unconstrained problem; that is, we place no additionalassumptions on P0.From the viewpoint of analyzing only frequentist estimators, this is a well studiedproblem. The frequentist convergence rates of the empirical measure under the expected Wasserstein distanceare studied in (Fournier & Guillin, 2015; Singh & Pczos, 2018; Bobkov & Ledoux, 2019; Weed & Bach,2019) to varying degrees of generality. A consequence of the work of (Singh & Pczos, 2018) is that on themetric space (d, 2), for d N, for the class of Borel probability measures, the empirical measure",
  "is minimax optimal (at least up to logarithmic terms) for every p 1. Further, the minimax rate is lowerbounded by n1/2p for d 2p, and n1/d for d > 2p": "Far less has been done in providing frequentist guarantees for Bayesian statistical procedures when theinferential goal is to estimate a non-parametric distribution underneath a Wasserstein distance. In a non-parametric Bayesian model aimed at inferring a probability distribution on Ed, for each sample size n, a prior0n is placed on the space of Borel probability measures on Ed. We denote this space Pd(E). The samplesize n posterior distribution, which we denote n(|Y1, Y2, . . . , Yn), is a regular conditional distribution overPd(E) induced from the likelihood and the prior 0n. Given a distance function d between probabilitymeasures on E (e.g Kullback-Leibler, Hellinger, Wasserstein1, Total Variation, etc.) we say the sequenceof posterior distributions contracts almost surely at the rate n under P0 if n(P Pd(E) : d(P0, P) n)",
  "converges almost surely to 0 as n when Y1, Y2, . . . , Yniid P0 (Ghosal & Van der Vaart, 2017)": "There are other useful but weaker notions of posterior contraction, such as the in probability variant whichseeks only to show that for every Mn , n(P Pd(E) : d(P0, P) Mnn) 0 in probability underP0. We focus on the stronger almost sure version in this work because an almost sure PCR of n holdinguniformly over every P0 in a finite class Pd(E) implies the existence of an estimator Pn derived from theposterior achieving EP0 d(P0, Pn) n uniformly over P0 Pd(E). Specifically, an asymptotically minimax,almost sure PCR holding uniformly over the finite class Pd(E) cannot decay at a faster rate than the minimaxrate 1. It is for this reason that if for every P0 Pd(E) the almost sure Posterior Contraction Rate (PCR)n is achieved, and n is the frequentist minimax rate over Pd(E), we say the Bayesian method is agnosticto the prior choice in the presence of an infinite amount of data under class Pd(E).In particular, thepractitioner can specify prior knowledge that may be beneficial for inference at small sample sizes when thatprior knowledge is correct, but if this prior knowledge is inaccurate, the method will still be competitive(from the minimax perspective) with the best possible estimator when the sample size is sufficiently large.In our work Pd(E) is not finite, and we introduce in Definition 1 a slightly stronger property of contractionrate across a distribution class which cannot outperform the minimax rate regardless of the size of Pd(E);the main result of this work proves this stronger property. Ghosal et al. (2000) provides a general three condition verification strategy for proving these PCRs. Thisstrategy or minor variants have been the catalyst for a myriad of minimax PCR results for the problem ofestimating a distribution via i.i.d samples. For example, Scricciolo (2007) uses a histogram model with aprior on the bin weights and number of bins to prove minimax PCRs for the distribution class possessing Holder smooth densities on d, where (0, 1], d 1 and d is the Hellinger distance. Kruijer & Van derVaart (2008) also achieves minimax PCRs for the estimation of Holder density class when d {1, 2} andd is the Hellinger distance using a weighted mixture of Beta density functions with a prior on the mixtureweights and number of mixed densities. A notable achievement of these works is that the value is notused in the construction of the posterior, yet minimax optimality is still achieved for the Holder densityclass.That is, the methods adapt to the smoothness parameter.Shen et al. (2013) provides minimaxPCRs for the estimation of smooth distributions on Rd using a weighted mixture of Gaussian distributionswith a prior on the covariance matrix of the mixed Gaussians when d is the Hellinger distance or TotalVariation distance; smoothness adaptive guarantees are provided. However, note that rather than studyingthe unconstrained distribution estimation setting, these works focus on estimation of distributions known topossess a smoothness property, and in any case do not study estimation underneath Wasserstein distance. Unfortunately the Ghosal et al. (2000) approach is more difficult to use when d is a Wasserstein metric.Challenges include Wp, p 2 not being dominated by Total Variation or Hellinger distances, causing theneed for explicit test construction. Also, the Kullback-Leibler neighborhood condition, which ensures suchneighborhoods of P0 have sufficient prior mass, may make it more difficult to achieve the minimax rateunder Wp, p 1 because depending on the model under consideration, approximation of distributions underthe Kullback-Leibler divergence may not be achievable at the square of minimax rate under Wp 2. In lightof these challenges, there have been far fewer theoretical advances in proving minimax optimal PCRs fordistribution estimation under Wp, p 1 than under Total Variation and Hellinger distances.",
  "Published in Transactions on Machine Learning Research (01/2025)": "Chae et al. (2021) successfully derive a 2 condition verification strategy for proving PCRs under Wassersteindistance when E = R. This work appears to be the first providing a set of general conditions for provingWasserstein PCRs for the unconstrained, non-parametric distribution estimation problem, and their methodis applicable for distributions with unbounded domain. Their results are restricted to the study of 1 di-mensional distributions. Also, the Ghosal et al. (2000) and Chae et al. (2021) frameworks depend on theposterior distribution being available through Bayes formula. Camerlenghi et al. (2022) observes that thiscan be limiting and develops a method to derive PCRs when the posterior is not available via Bayes formula.They apply their technique to derive Wasserstein PCRs for each d N, v 1 for the model placing a Dirich-let process prior on the data generating distribution. The PCR derived for P0 Pd() is n 1 (d+p)which via the discussion earlier in this section is slower decaying than the minimax rate by a polynomialfactor for every d N, p 1. In the Deconvolution problem, where for i {1, 2, . . . , n}, Yi = Xi + i and X1, . . . , Xniid P0 is independent of 1, . . . , niid 0 and 0 is known and the goal is to infer P0, Wassersteindistance PCRs have been derived in Rousseau & Scricciolo (2023),Gao & van der Vaart (2016), and Scricciolo(2018). P0 is called the mixing distribution. The distribution of Y1, however, is the convolution P0 0.",
  "Contributions": "Our main contribution is Theorem 1. In it we obtain PCRs for every dimension d 1 and for every distanceWp, p 1 and the PCRs achieved are minimax optimal at least up to logarithmic terms. To the best of ourknowledge, our result is the first to provide a minimax optimal PCR across each (d N, p 1) setting forestimating an unconstrained P0 Pd(). These rates are achieved using a Bayesian Histogram modelthat partitions d into bdn equal area squares where bn := 2log2(kn) for a sequence kn growing as afunction of the sample size n at the appropriate rate, uses the Multinomial likelihood to weight the constantdensity within each square, and places a sample size dependent Dirichlet prior distribution on the weightvector with prior concentration vector bn (of dimension bdn). This model induces a sequence of posteriordistributions n,kn,bn over Pd(). In Theorem 1, we show that",
  "d": "where log terms in n which are specified in the theorem are ignored here. In the problem of providingoptimal PCRs for estimating distributions on d under Wasserstein-p distance, our results close the gapbetween the minimax rates for this problem and the Wasserstein PCRs provided by Camerlenghi et al. (2022). The remainder of this paper is organized as follows. In we formally introduce the Bayesian His-togram model. In we introduce the strong notion of posterior contraction under which our theoremsare proved and show connections between this notion of posterior contraction with minimax lower boundrates and traditional PCR statements. In we state the main theorem and the three fundamentallemmas upon which the main theorem depends. We then prove the main theorem. In we provideinstruction on how to use the prior distribution to express prior beliefs when using this model in practice.In we provide the proofs of the lemmas and in we provide concluding remarks.",
  "Excluding the right end points are a notational convenience but extension of the arguments that follow toinclude the right endpoint is trivial": "For sequences of numbers an, bn defined for sufficiently large n N, an bn means there exists a C > 0 andN0 N such that for n N0, an Cbn. For b, d N, we denote [b] := {1, 2, . . . , b} and [b]d := dj=1[b].For B Rd, B(B) denotes the Borel measurable subsets of B.For j N, Sj1 refers to the (j 1)dimensional probability simplex. That is Sj1 := {(x1, . . . , xj) Rj : jt=1 xt = 1, xt 0 for t [j]}.Also note that R+ := {x R : x > 0} and for z N and Rz+, the Dirichlet probability measureDirichlet : B(Sz1) is given by",
  "Bayesian Histogram Model Definition": "We suppose Y1, Y2, . . . , Yniid P0 where P0 Pd. For b N, let b := {jjj,b}jjj[b]d Rbd+ . For an increasingsequence kn, let bn := 2Kn, where Kn := log2(kn), n := {n,jjj}jjj[bn]d Sbnd1. For n N, the BayesianHistogram model likelihood and prior are given by",
  "Y1, . . . , Yn|ni.i.d Histogram(|n, bn),n|bn Dirichlet(|bn).(5)": "Also, let zn(|Y1, . . . , Yn) refer to the posterior probability measure over Sbnd1 derived from Equation 5. Asiii,bn > 0 for every iii [bn]d and for every n N, Equation 5 induces a sequence of posterior distributionsover Pd. Specifically let b : Sbd1 Pd be the map that takes a given = {jjj}jjj[b]d and produces itscorresponding Histogram probability measure. That is",
  "where the second equality above holds if iii,bn > 0 for iii [bn]d": "We note that posterior distributions derived from improper prior distributions are not considered in thiswork, and therefore to consider the posterior measure sequence n we require that iii,bn > 0 for iii [bn]d.However, we allow Pn to be defined regardless of whether or not the prior distribution over the simplex isproper. In particular, it is still defined in the event that some or all of the iii,bn parameters are zero. Whenthe prior distribution is proper, Pn has an additional interpretation: it is the Posterior Mean Histogram.In the lemmas and theorems that follow that involve analysis of the posterior distribution sequence n, wemake clear that we require iii,bn > 0 for iii [bn]d and n N. Pn,kn,bn (and n,kn,bn) are indexed by the choice of kn (which determines the total number of bins) andbn, which gives the prior concentrations on those bins. In the subsequent subsection we establish constraintson kn and bn that ensure Pn,kn,bn and n,kn,bn are minimax statistical procedures.",
  "Notions of Minimax Posterior Contraction": "First we introduce a strong notion of posterior contraction across an entire distribution class which cannotdecay faster than the minimax rate in general (regardless of the size of the space). It is for this reason wecall this stronger notion of posterior contraction a minimax-conscious PCR. Definition 1. (minimax-conscious PCR) Let d be a distance over Pd and n some sequence of posteriordistributions over Pd. The sequence n is called a minimax-conscious PCR for the sequence of posteriordistributions n on the space (Pd, d) if there exists sequence zn such that zn 0 and sequence n suchthat n n, j=1 n < and for some N sufficiently large, and n N, for every P0 Pd, whenever",
  "where the inf is taken over all function of n iid samples, then n mn. In particular, a minimax-consciousPCR can never decay faster than the minimax rate": "Note that a minimax-conscious PCR is a statement about the behavior of a posterior distribution sequenceacross an entire distribution class. Next we make the connection between a minimax-conscious PCR andthe traditional, almost sure PCR definition discussed in the introduction. Specifically, a minimax-consciousPCR implies an almost sure PCR at the same rate for every distribution in the class.",
  "(10)": "Now assuming that for each n N and jjj [bn]d, jjj,bn > 0, and that bn satisfies Assumption 2, we havethat for 1 p < and d N and C0(d, p) sufficiently large, n(d, p) is a minimax-conscious PCR for thesequence of posterior distribution n,kn,bn on the space (Pd, Wp) where bn = 2log2(kn).",
  "dd > 2p,(11)": "where the inf is taken over all estimators P from n observations. Thus the PCRs of Theorem 1 are up tologarithmic terms attaining the minimax rates. The assumption on the prior concentrations, Assumption 2,is flexible enough to support a vague prior. Specifically, the mean of a Dirichlet distribution with commonconcentration on all categories is a discrete uniform distribution, so the practitioner wishing to encodevagueness by asserting that under the prior on average all bin probabilities are equal will want to set allprior bin concentrations to a common value. When d 2p, by Assumption 1, the number of bins is nd2p ,thus Assumption 2 is satisfied when each concentration is set to Cn( d",
  ") for some C > 0. Likewise whend > 2p, by Assumption 1, there are n bins and Assumption 2 is satisfied when all concentrations are Cn p": "d .Also note that while Assumption 2 places an upper bound on the total volume of the prior concentrationsto ensure the prior does not overwhelm the empirical Histogram at large sample sizes, it in general doesnot place any shape restrictions on the prior; in particular other prior shapes besides the uniform can beconstructed. The proof of Theorem 1 is composed from the following three auxiliary lemmas. The first auxiliary lemmaupper bounds the rate of convergence of the posterior mean histogram, Pn,kn,bn, towards P0 in mean W ppdistance. The second lemma establishes an exponentially decaying upper bound on the probability the W ppdistance between the posterior mean histogram and P0 deviates from its mean by more than > 0. Thethird lemma establishes a PCR around Pn,kn,bn, rather than P0. It is the third lemma that leverages theconjugacy of this model.",
  "almost surely under P0 whenever n N1(d, v)": "The main technical challenges appear in proving the auxiliary lemmas. Given Lemmas 3, 4 and 5, Theorem1 follows easily and we show this now. For ease in notation, through the remainder of the paper we drop thekn and bn subscripts from the notation for the posterior, thus n,kn,bn is referred to as n (and Pn,kn,bnis referred to as Pn). This does not cause ambiguity in what follows because the values of kn and bn aregiven in Assumptions 1 and 2.",
  "Using Prior Information": "As discussed in the introduction, minimax optimal posterior contraction results are an indicator that aBayesian method is (in the worst case risk sense) robust to the selection of an inaccurate prior when thesample size is large. The general benefit of the prior is in scenarios where the practitioner has a belief about P0 before datacollection, is able to encode this information through the prior, and the belief ends up being correct, inwhich case the small sample size performance can be better than under a purely frequentist estimationapproach. The type of belief that is representable through the prior in this model is a hypothesis about the probabilitydistribution of a partition of d. Specifically, suppose the practitioner believes, but is not certain, that P0satisfies that on a size M partition of d, denoted {Rj}Mj=1, the probability in region Rj is pj for j [m].Further suppose there exists a k0 {1, 2, . . . } sufficiently large such that there exists a size M partition of{iii}iii[2k0]d, denoted {Ij,k0}Mj=1 and for each j [M]",
  "iiiIj,k0Aiii,2k0,": "where recall Aiii,2k0 is defined in Equation 3. In words, each region Rj is expressible as unions of membersof the level k0 dyadic partition. Note that because Rj is expressible as a union of members of the partition{Aiii,2k0}iii[2k0]d and the partitions {Aiii,2k}iii[2k]d for k k0 are nested in the {Aiii,2k0}iii[2k0]d partition, Rj isalso expressible as a union of members of {Aiii,2k}iii[2k]d for k k0. In particular for k k0, there is a sizeM partition of {iii}iii[2k0]d, denoted {Ij,k}Mj=1 and for each j [M], Rj =",
  "We now prove Lemma 3": "Proof of Lemma 3. By the definition of the partitions {Sk}kN in Lemma 6, the partitions S1, S2, . . . , SKnare nested in the sense that for k [Kn 1] and S Sk, there exists a collections of sets in Sk+1 such thatS is exactly the union of these sets. In particular the sets S Sk are expressible as unions of sets from SKn.Also using that bn = 2Kn, we have for k {1, 2, . . . , Kn} and S Sk a set of indices IS,k,n [bn]d such that",
  "W pp (, ) = supfCbEf Ef c.(26)": "We now prove the bounded difference inequality nescessary to apply Mcdiarmids inequality. So let xxxn =(x1, x2, . . . , xn1, xn) [0, 1)d and xxxn = (x1, x2, . . . , xn1, xn) [0, 1)d. Additionally we use the notationWp(P0, Pn(xxx)) to indicate that Pn is constructed from xxx [0, 1)d, and let jjjx [bn]d and jjjx [bn]d satisfythat xn Ajjjx,bn and xn Ajjjx,bn and let Cjjj,bn = ni=1 I(xi Ajjj,bn) for jjj [bn]d. Via equation 26 andexpressing expectations as sums over the partition members Ajjj,bn, we thus have that",
  ".(35)": "Using Equation 34, the definition in Equation 35, the preimage form of n (Equation 7), the definition of Pn(Equation 9), the definition of zn (the posterior measure over the simplex Sbnd1), and that by Assumption1 and the definition of n(d, p) in Equation 12, 2Knp = o( pn(d, p)) (as n ) we have that almost surelyunder P0 and eventually in n and for each d N, p 1",
  "(36)": "We will now derive an upper bound on sup1Sbnd1 maxj[Kn] Vn(1, k) which, if it holds, ensures the eventinside the probability of the last line of Equation 36 must be false. We will use this upper bound and theunion bound to control from above the probability in the last line of Equation 36. Regarding the upperbound on sup1Sbnd1 maxj[Kn] Vn(1, k), note that",
  ") pn(d, p).(37)": "To see the in Equation 37, observe that by Assumption 2, the total prior concentration is dominatedby n and therefore the term in front of the summand on LHS of is log(n)n. Thus by definition of pn(d, p) (Equation 12), this is sufficient to conclude LHS pn(d, p) in the d < 2p case. In the d = 2pcase the sum contributes a factor log(n) to LHS and so again LHS pn(d, p). In the d > 2p case, the sum",
  "jjj[bn]d jjj,bna.s= n +": "jjj[bn]d jjj,bn.Finally note that by definition of Sk, |Sk| = 2dk. So for n N and k {1, 2, . . . , Kn} applying Dirichlet con-centration of measure Lemma 8 with := log(n) , we have that for C1(d, p) sufficiently large, eventuallyin n and almost surely under P0",
  "Kn log(n).(40)": "Finally note Kn = log(kn) log(kn) + 1 2 log(n) and the asymptotic arguments of this proof do notdepend on P0 either through constants or eventuality. Thus by Equations 36, 39 and 40, we have that foreach d N and p 1 and C1(d, p) sufficiently large, and an N(d, p) (depending only on d, p but not on P0),and n Nn(P Pd : Wp(P, Pn) n(d, p)) 2 log1(n)(41)",
  "Conclusions": "In this work we obtained minimax optimal PCRs for unconstrained distribution estimation on d under-neath the Wasserstein-p distances for every data dimension d. To the best of our knowledge these are the firstPCRs achieving minimaxity for every problem dimension d under Wp, p 1 distance. Our proof techniqueavoids verifying a Kullback-Liebler prior support condition by using conjugacy and a direct analysis of theposterior distribution. These results may be useful to practitioners needing to estimate a distribution underneath a Wassersteindistance when they have some knowledge prior to data collection about the shape of the distribution theyare estimating, intend to encode this through a prior distribution to potentially achieve increased accuracyat low sample sizes, and yet simultaneously require a guarantee of precision at large sample sizes that isrobust to inaccurate prior selection. An important area for future work is to determine whether for high dimensional data, Bayesian models canadaptively achieve minimax optimal PCRs underneath Wasserstein-p distances in constrained distributionestimation settings where it is safe to assume that the distribution to be estimated is of low entropy or hasa smooth density.",
  "n(P Pd : d(P, P0) n) 0": "In the introduction we claim that an almost sure PCR holding uniformily over a finite distribution classcannot decay faster than the minimax rate for that class. While we do not actually use this claim in ourwork, for completeness, a proof of this claim is provided below. This lemma is also inspired by Ghosal et al.(2000) Lemma 2.5.",
  "Acknowledgements": "This work was supported by contracts DOE DE-SC0021015, NSF CCF-2115677, and the Laboratory DirectedResearch and Development program at Sandia National Laboratories, a multimission laboratory managedand operated by National Technology and Engineering Solutions of Sandia, LLC, a wholly-owned subsidiaryof Honeywell International, Inc., for both the U.S. Department of Energys National Nuclear Security Ad-ministration under contract DE-NA0003525. This paper describes objective technical results and analysis.Any subjective views or opinions that might be expressed in the paper do not necessarily represent the viewsof the U.S. Department of Energy or the United States Government. The authors further acknowledge andthank both Jeff Phillips and the reviewers for helpful conversations."
}