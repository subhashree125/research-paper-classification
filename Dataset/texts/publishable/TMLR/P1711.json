{
  "Abstract": "Graph Machine Learning often involves the clustering of nodes based on similarity structureencoded in the graphs topology and the nodes attributes. On homophilous graphs, theintegration of pooling layers has been shown to enhance the performance of Graph NeuralNetworks by accounting for inherent multi-scale structure. Here, similar nodes are groupedtogether to coarsen the graph and reduce the input size in subsequent layers in deeperarchitectures. The underlying clustering approach can be implemented via graph poolingoperators, which often rely on classical tools from Graph Theory. In this work, we introducea graph pooling operator (ORC-Pool), which utilizes a characterization of the graphsgeometry via Olliviers discrete Ricci curvature and an associated geometric flow. PreviousRicci flow based clustering approaches have shown great promise across several domains,but are by construction unable to account for similarity structure encoded in the nodeattributes. However, in many ML applications, such information is vital for downstreamtasks. ORC-Pool extends such clustering approaches to attributed graphs, allowing for theintegration of geometric coarsening into Graph Neural Networks as a pooling layer.",
  "Introduction": "Multi-scale structure is ubiquitous in relational data across domains; examples include complex molecules incomputational biology, systems of interacting particles in physics, as well as complex financial and socialsystems. Many graph learning tasks, such as clustering and coarsening, rely on such structure. Graph NeuralNetworks (GNNs) account for multi-scale structure via pooling layers, which form crucial building blocks inmany state of the art architectures.",
  ": Clustering based onconnectivity vs. node attributesonly": "Node clustering often utilizes an implicit or explicit pooling operation,which decomposes a given graph into densely connected subgraphs bygrouping similar nodes. Applied at different scales, aggregating informationacross subgraphs allows for uncovering multi-scale structure (coarsening).A number of classical algorithms implement such operations, includingspectral clustering (22; 51), the Louvain algorithm (11) and Graclus (14).Recently, clustering approaches based on graph curvature (41; 50; 59; 54;19) have received interest. They utilize discrete Ricci curvature (43; 23)to characterize the local geometry of a graph by aggregating geometricinformation across 2-hop node neighborhoods. An associated curvatureflow (42) characterizes graph geometry at a more global scale, uncoveringits coarse geometry. A crucial shortcoming of classical node clustering approaches in the case of attributed graphs is their inabilityto capture structural information encoded in the node attributes, as they evaluate only similarity structureencoded in the graphs topology. However, such information is often vital for downstream tasks. Applying a",
  "Published in Transactions on Machine Learning Research (12/2024)": "has characterized oversquashing via discrete Ricci curvature (40; 20): Edges that connect nodes in differentclusters (bridges), have low Ricci curvature. This effect is emphasized under ORC flow, as the weight ofthe bridges increases. With that, our proposed curvature adjustment may amplify oversquashing effects.Recent literature has proposed graph rewiring as a mitigation for oversquashing effects. During rewiring,edges are re-sampled proportional to a relevance score (edge weight), which may be assigned utilizing theLovasz bound (3), random edge dropping (46) or discrete curvature (40; 20), among others. Since we alreadycompute discrete curvature during pooling, we could add a rewiring step with little overhead. We leave aninvestigation of this approach for future work.",
  "Summary of contributions.The main contributions of the paper are as follows:": "1. We introduce ORC-Pool, a trainable pooling operator, which utilizes discrete Ricci curvature and anassociated geometric flow to identify salient multi-scale structure in graphs. ORC-Pool groups nodesaccording to similarity structure encoded both in the graphs topology, as well as its nodes attributes,presenting the first extension of Ricci flow based clustering to attributed graphs.",
  "Background and Notation": "Let G = {V, E} denote a graph, V (|V | = N) the set of nodes and E = V V the set of edges; G is endowedwith the usual shortest-path metric dG. We assume that G is attributed and denote node attributes asX R|V |m. We further assume that G is weighted; i.e., its edges are endowed with scalar attributes, givenby a weight function w : E R+. Below, we recall basic Graph Neural Networks concepts and introduce thegraph curvature notions and associated curvature flow utilized in this work.",
  "Graph Neural Networks with Pooling": "Message-passing Graph Neural Networks.The blueprint of many state of the art architectures areMessage-Passing GNNs (MPGNNs) (25; 30), which learn node embeddings via an iterative message-passing(MP) scheme: Node representations are iteratively updated as a function of their neighbors representations.Node attributes in the input graph determine the node representations at initialization. Formally, MPGNNsconsist of a message m(l+1)vand an update function fUp (l = 0, . . . , L 1 denoting the layer):",
  "maps Pool: G GP = (XP , EP ) composed of three functions that act on the nodes, node attributes andedges of G, respectively ():": "1. A selection function, which identifies a set of nodes that are merged to supernodes (Vk {1, . . . , N}):Sel : {1, . . . , N} (V1, . . . , VK). The selection function lies at the heart of the pooling operator. Below,we propose a geometric selection function, which utilizes the graphs geometry for node-level clustering.",
  "Graph Curvature": "Classically, Ricci curvature establishes a connection between the local dispersion of geodesics and the localcurvature of a manifold, deriving from a crucial connection between volume growth rates and curvature:Negative curvature characterizes exponential fast volume growth, while positive curvature indicates contraction.Several analogous notions have been proposed for discrete spaces, including by Ollivier (43), Forman (23)and Erbar-Maas (18). Common among all notions is the observation that edges, which encode long-rangedependencies, have low (negative) curvature, while edges that form local connections have high curvature,allowing for a characterization of local and global connectivity patterns in a graph. Here, we focus on Ollivierscurvature, which we introduce below.",
  "neighborhood. Then Olliviers Ricci curvature (ORC) is given by M(x, y) = 1 W (Mx ,My )": "dM(x,y). Analogous tothe continuous case, ORC can be defined in discrete spaces: Let B denote a matrix with entries bij = ewijinverse proportional to the edge weights and DB denote a matrix with entries dii = j bij. Consider adiffusion process on the graph G, i.e., we place a point mass v at a node v and consider the diffusionpv(t) = vetD1B (DBB) (12). The distribution generated by pv(t) defines a measure on the t-neighborhoodof v. We define a discrete analog of ORC (43; 26) for an edge e = (u, v) via the transportation distancesbetween neighborhood measures defined by diffusion processes starting at its adjacent vertices, i.e.,",
  "dG(u, v).(2)": "How does this quantity relate to the coarse structure of the graph? In most graphs, similar nodes form denselyconnected subgraphs (homophily principle). As a result, if u, v are similar, then the diffusion processes willstay nearby with high probability, exploring the densely connected subgraph. In contrast, if u, v are dissimilar,e.g., belong to separate clusters, then the diffusion processes are likely to explore separate clusters, drawingapart quickly. The transportation distance W1(pv(t), pu(t)) is much higher in the second case than in the first.Thus, edges that connect dissimilar nodes have low (usually negative) curvature, whereas edges that connectsimilar nodes have high curvature. Hence, bridges between communities may be identified via low curvature",
  "Curvature-based Clustering": "Ricci Flow.In continuous spaces, an associated geometric flow (Ricci flow) is of great importance, as itreveals deep connections between the geometry and topology of the manifold (geometrization conjecture).As a loose analogy, a geometric flow associated with discrete Ricci curvature can be defined, which hasbeen related to the community structure (41) and coarse geometry (61) of graphs. Ollivier (42) proposes acurvature flowddtdG(u, v)(t) = uv(t) dG(u, v)(t)((u, v) E) , associated with discrete ORC along edges (u, v). A discretization of this flow gives a combinatorial evolutionequation, which evolves edge weights according to the local geometry of the graph: Consider a family ofweighted graphs Gt = {V, E, W t}, which is constructed from an input graph G = G0 by evolving its edgeweights as (setting dt = 1)wtu,v (1 uv)dG(u, v)((u, v) E) ,(3)",
  "where uv, dG(u, v) are computed on Gt. Eq. 3 may be viewed as a discrete analogue of continuous Ricciflow. To control the scale of the edge weights, we normalize after each iteration": "Geometric clustering algorithms.Geometric clustering approaches based on ORC for non-attributedgraphs have previously been considered, e.g. (50; 41; 54). These algorithms are partition-based, i.e., removeedges to decompose the graph into subgraphs, and come in two flavors: The first takes a local perspective,cutting edges with ORC below a given threshold (e.g., (50)). The second evolves edge weights under Ricciflow (Eq. 3), encoding local and more global geometric information into the edge weights, before removingedges with weight below a certain threshold (e.g., (41)). Both approaches exploit the observation that ORCflow highlights\" the multi-scale structure of the graph (e.g., community structure). We emphasize thatcurvature, by construction, evaluates only the graphs connectivity, but cannot account for node attributes(apart from carefully chosen initialization of edge weights, see above).",
  "Curvature Approximation": "Computing ORC as defined above involves solving an optimal transport problem (i.e., computing theWasserstein-1 distances between measures on the 1-hop neighborhoods of the adjacent vertices). In thediscrete setting, this corresponds to computing the earth movers distance, which, in the worst case, hascomplexity O(|E|m3) (where m denotes the maximum degree of nodes in G). A commonly used approximationvia Sinkhorns algorithm has a reduced complexity of O(|E|m2), which can still be prohibitively expensiveon large-scale graphs. Recently, Tian et al. (54) introduced a combinatorial approximation of ORC of theform uv := 1 2upuv + lowuv, where up and low are combinatorial upper and lower bounds, which can becomputed in O(|E|m). More details, as well as the formal statement of the bounds can be found in Apx. A. Inlarge or dense graphs, implementing the above introduced Ricci flow via this approximation can significantlyreduce runtime (54).",
  "Geometric Coarsening and Pooling in attributed graphs": "Characterizing the coarse geometry of a graph is invaluable for graph learning tasks. In this paper, weintroduce a geometric pooling operator (ORC-Pool, ), which may serve as a standalone graph coarseningapproach, as well as a pooling layer that can be integrated into GNN architectures. ORC-Pool allows forevaluating similarity structure encoded in the graphs geometry via ORC flow, while also accounting forsimilarity structure encoded in the node attributes. We follow the SRC framework (28) discussed above toformalize our approach.",
  "Geometric Graph Coarsening": "Geometric selection function.To define a pooling operator that effectively coarsens a graph, theselection function needs to identify a cluster assignment that preserves the overall graph topology (communitystructure, etc.). We propose a geometric selection function, which computes the cluster assignment guidedby the relation between curvature and graph topology (sec. 2.2). We encode curvature information intoa matrix CT = (cij) with entries cij = wij given by the evolved edge weights after T iterations of ORCflow. The matrix C can be seen as a curvature-adjusted adjacency matrix, which assigns an importancescore to each edge reflecting its structural role: Under ORC flow (Eq. 3), structurally important edges areassigned a high score, whereas edges between similar nodes receive a low score. This encodes global similaritystructure, uncovering the coarse geometry of the graph (see discussion above and ). Edges whose weightdecreases under ORC flow contract (positive curvature), moving the adjacent, similar nodes closer together.On the other hand, edges whose weight increases under ORC flow expand (negative curvature), moving theadjacent dissimilar nodes further apart. This naturally induces a coarsening of the graph. Curvature-basedcoarsening utilizes this emerging structure by cutting edges with a high weight (wTij > ) and then mergingthe remaining connected components into supernodes. The threshold is a crucial hyperparameter, which isoften difficult to choose in practice. Ni et al. (41) learn the threshold by optimizing the modularity of theresulting decomposition. Modularity optimization is NP-hard and hence an exact solution is intractable.They approximate the solution via an expensive parameter search. Since this subroutine is both expensiveand non-differentiable, it is not suitable for integration into a trainable architecture. Hence, we need toemploy a different auxiliary loss for identifying edges to cut, which we now describe. Graph Cuts.Let S {0, 1}NK denote the assignment matrix computed by the selection function, itsentries specify the assignment of N vertices to K supernodes, i.e., sik = 1, if vertex i is Vk and sik = 0otherwise. The problem of computing S by removing edges relates to the mincut problem, a classical problemin Combinatorics. In a seminal paper, Shi and Malik (49) show that the min-cut problem can be written asan optimization problem with respect to the number of edges within and between putative clusters (here,supernodes):",
  "Using classical results from spectral graph theory, one can show that the optimizer takes the form Q = UKV ,where UK RNK is formed by the top K eigenvectors of the normalized adjacency A = D1/2AD1/2": "and V O(K) is an orthogonal transform. To recover the cluster assignment S, one can apply k-meansclustering to the rows of Q (57). Approaches for efficiently computing minimal graph cuts have beenadapted for the design of effective graph pooling operators (9; 31). We have argued above that computing acurvature-adjustment emphasizes the underlying community structure (e.g., Fig 2), which should make iteasier to learn graph cuts. Note that after curvature-adjustment, the graph is weighted, i.e., we now optimizethe sum of edge weights within and between putative clusters. We will provide theoretical evidence for thisobservation in the next section. Weighted graph cuts have been considered previously, e.g., in metis (14),which underlies the Graclus pooling layer. We can adapt Eq. 6 to a loss function for computing a clusterassignment based on the (normalized) curvature-adjusted adjacency matrix, i.e.,",
  ".(7)": "Note that the entries of the degree matrix D are the weighted node degrees after adjustment. For T = 0,we recover the classical minimum graph cut objective. Notably, the resulting objective is differentiable andcan be optimized with standard techniques (see above), which is preferable over the parameter search for asuitable edge weight threshold performed in (50; 41). Geometric pooling operator (ORC-Pool).The reduction and connection functions of ORC-Pool closely resemble canonical choices that are common among many pooling operators (e.g., Min-CutPool (8), DiffPool (65), NMF (4), among others): Given a selection S {0, 1}NK, which assigns Nnodes to K supernodes, we set Red(X) = ST X =: X and Con(E) = ST ES := E. Naturally, if G is notattributed, then there is no need for a reduction function. In the coarsened graph, edge weights are againinitialized either as 1 or according to differences in supernode attributes (implemented in the Con function).This coarsening scheme may be applied multiple times, i.e., we can learn a sequence S1, S2, . . . of selectionmatrices, which compute cluster assignments with varying degree of coarseness. With that, ORC-Pool mayserve as a standalone coarsening or clustering approach.",
  "ORC-Pool layer for Graph Neural Networks": "While the geometric pooling operator described above applies to attributed graphs, its selection function doesnot evaluate similarity structure encoded in node attributes, aside from a specific edge weight initializationdiscussed earlier. In this section, we will describe an alternative coarsening approach, where ORC-Pool isintegrated into GNNs as a pooling layer grounded in graph geometry. Stacked on top of MP base layers,which learn node representation that reflect similarity structure in both graph topology and node attributes,ORC-Pool integrates both types of structural information to coarsen the graph. Pooling operators can be integrated into GNNs by stacking pooling layers, which implement the operator, ontop of blocks of base layers (usually MP layers). The base layer learns node embeddings X = GNN(X, A, ),where are trainable parameters. If the input graph is attributed, its node attributes are used to initializethe base layer. We implement the ORC-Pool layer as an MLP, where we learn the cluster assignment matrixS with an MLP with softmax activation (enforcing the constraint S1k = 1N) and trainable parameters (S = MLP( X, )), which are trained using Eq. 7 as an auxiliary loss. The GNN is trained end-to-end. Inparticular, we optimize the training objective",
  ",(8)": "where the first term of the objective corresponds to the auxiliary loss and the second encourages mutuallyorthogonal clusters of similar size, preventing degenerate clusters (F denotes the Frobenius norm). Noticethat the training objective closely resembles that of MinCutPool (8), but utilizes the curvature-adjustedadjacency matrix CT . In particular, we recover MinCutPool as a special case (T = 0). We obtain the adjacency matrix of the coarsened graph by setting AP = ST AS and recomputing curvature-adjusted weights via Ricci flow on the superedges. We initialize superedge weights as a measure of similarityof the supernode attributes, which are obtained by setting XP = ST X. Stacking blocks of base layers andpooling layers on top of each other yields a successive coarsening of the graph, which reflects its multi-scalestructure. Notice that curvature-based importance scores influence the message functions on each scale. Thetrainable parameters , are trained end-to-end as part of the GNN architecture and the respective lossfor the downstream task, allowing for a seamless integration of geometric pooling into GNN architectures.As a final remark, notice that the curvature-adjustment employed in ORC-Pool is flexible and could beincorporated into other (dense) pooling operators too (especially those that build on graph cuts).",
  "Basic properties": "Permutation-invariance.Graphs and sets are permutation-invariant, as there is no natural order relationon the nodes. Standard MP layers are permutation-equivariant and pooling layers permutation-invariantto encode this structure as inductive bias. The curvature-adjustment added in ORC-Pool preserves thisproperty (for details see Apx. G). Impact on Expressivity.A classical measure of the representational power of GNNs is expressivity, whichevaluates a GNNs ability to distinguish pairs of non-isomorphic graphs. It is well-known that standardMPGNNs are as powerful as the Weiserfeiler-Lehmann test, a classical heuristic for graph isomorphismtesting (63; 38). To reap the benefits of pooling, pooling operators should preserve the expressivity ofthe MP base layer. Recently, (10) established conditions for this property. A simple corollary shows thatORC-Pool fulfills these conditions (see Apx. G for details): Corollary 1. Consider a simple architecture with a block of MP base layers, followed by a ORC-Pool layer.Let G1, G2 denote two 1-WL-distinguishable graphs with node attributes X1, X2. Further let X1 = X2 denotethe node representations learned by the block of MP layers. Then the coarsened graphs GP1 , GP2 learned by theORC-Pool layer are 1-WL distinguishable.",
  "Definition 1. Consider a class of model graphs Ga,b (a b 2), which areconstructed by taking a complete graph with b nodes and replacing each by acomplete graph with a + 1 nodes": "Ga,b-graphs can be seen as instances of stochastic block models with b blocks of size a + 1 and exhibit a clearcommunity structure (see for an example). Due to the regularity of the graph, we can categorize itsedges into three types: (1) bridges, which connect dissimilar nodes in different clusters; (2) internal edges,which connect similar nodes in the same cluster that are at most one hop removed from a dissimilar node; and(3) all other internal edges, which connect similar nodes within the same cluster. (41) derived an evolutionequation for edge weights under ORC flow for Ga,b graphs: Lemma 1 (informal, (41)). Let wt = [wt1, wt2, wt3] denote the weights of edges of types (1)-(3). Assumingw0 = , the edge weights evolve under ORC flow (Eq. 3) as wt+1 = F(a, b)wt, where F(a, b) is a fixed(3 3)-matrix, depending on a, b only.",
  "(Cu, Cv)": "Here, W = (wij) denotes a weighted adjacency matrix, dv weighted node degrees and (Cu, Cv) = 1, if u, vare in the same cluster and zero otherwise. High modularity is often used as an indicator of a strongseparation of the graph into subgraphs, under which communities can be easier to detect. We show thatmodularity increases as edge weights evolve under ORC flow:",
  "LayerCoraCiteSeerPubMed": "EMD0.47 0.04 (19.97 0.35)0.35 0.04 (16.77 0.21)0.24 0.04 (571.11 2.94)Sinkhorn0.45 0.03 (56.36 1.43)0.35 0.03 (42.16 0.71)0.22 0.05 (904.38 3.08)ORC-approx0.45 0.03 (16.88 0.43)0.35 0.03 (14.62 0.10)0.21 0.03 (548.43 1.79) Curvature computation.ORC-Pool defines a dense pooling layer; i.e., E [Vk/|N|] = O(N). Thisproperty is inherited from the MinCutPool objective and preserved by the curvature adjustment. Hence,the complexity of the pooling operator is O(NK) for storage and O(K(|E|+NK)) for minimizing the relaxedmin-cut loss (9). As noted in sec. 2.2.3, the computation of the curvature-adjustment can be costly on large,dense graphs. We test the performance of the two introduced curvature approximations (Sinkhorn distances,combinatorial ORC) on the accuracy and average runtime per epoch. We focus on node-level tasks, whereinput graphs are of larger size. Our results (Tab. 5) indicate that computing Sinkhorn distances is actuallyslower than EMD on large, dense input graphs. This is consistent with previous results for curvature-basedcommunity detection on graphs with similar topology (54). In contrast, the combinatorial ORC approximationdelivers a faster method, which retains accuracy similar to exact ORC. We note that our implementation canlikely be further optimized, which could lead to additional speedups, e.g., via more effective parallelization.",
  "LayerMUTAGENZYMESPROTEINSIMDB-BINARYREDDIT-BINARYCOLLABPEPTIDES": "No Pool0.81 0.050.29 0.040.74 0.030.58 0.020.85 0.030.67 0.030.63 0.03Diff0.83 0.090.31 0.030.75 0.040.64 0.060.86 0.020.70 0.020.66 0.02Mincut0.83 0.070.37 0.050.76 0.050.65 0.050.85 0.020.67 0.020.67 0.02DMoN0.84 0.080.40 0.040.76 0.050.65 0.040.85 0.020.68 0.020.67 0.02TV0.83 0.080.37 0.050.75 0.040.54 0.030.85 0.030.70 0.020.67 0.02Graclus0.84 0.060.42 0.030.75 0.040.61 0.040.84 0.040.68 0.020.66 0.03ORC (us)0.90 0.060.38 0.060.78 0.040.71 0.040.88 0.020.71 0.020.69 0.01",
  "Experimental Analysis of Geometric Pooling": "In this section we present experiments to demonstrate the advantage of our proposed pooling layer ORC-Pool.We test our hypothesis that encoding local and global geometric information into the pooling layers canincrease the accuracy of the GNN in downstream tasks. Experimental setup.We implement a simple GNN architecture, consisting of blocks of GCN base layers,followed by a pooling layer. When comparing ORC-Pool with other state of the art pooling layers, we keepthe architecture fixed (only altering pooling layers) to enable a fair comparison. Details on the architectureused in our node- and graph-level experiments can be found in Apx. D.1. We utilize the popular benchmarksPlanetoid (64) for node clustering, and TUDataset (37) and LRGBDataset (17) for graph classification.Experiments are performed using PyTorch Geometric. We compare ORC-Pool with four state of theart pooling layers, which were reported as best-performing across domains and graph learning tasks (28):MinCutPool (8), DiffPool (65), TVPool (32), and Deep Modularity Networks (DMoN) (55). Weimplement ORC-Pool using exact ORC, i.e., W1(, ) is computed via the earth movers distance. Node Clustering.We compare the performance of ORC-Pool and other pooling layers for nodeclustering, where we evaluate the Normalized Mutual Information (short NMI, defined in sec. D.2) of thecluster assignments computed by the GNN, as well as the average runtime per epoch. The number of desiredclusters is known to the model beforehand. Results for Planetoid are reported in Tab. F. We see thatORC-Pool performed best overall with the highest average NMI on all three graphs. The second best modelin all cases was MinCutPool, which the ORC-Pool layer is based on. In terms of runtime, we observedthat for the larger PubMed graph, DiffPool, DMoN, and TVPool are about one order of magnitudefaster per epoch compared to MinCutPool and ORC-Pool. The difference in time per epoch is muchsmaller for Cora and CiteSeer. Graph classification.We further compare the performance of ORC-Pool with that of other poolinglayers for graph classification, where we report the accuracy of label assignments. ORC-Pool performed bestoverall on TUDataset, achieving the best accuracy on all datasets but ENZYMES (see Tab. 5). We furthertested ORC-Pool on Peptides, a large-scale graph classification tasks from the long-range graph benchmarkLRGB (17). Here, again, ORC-Pool showed superior performance. Run times for all experiments can befound in Tab. D.3.",
  "Discussion": "We introduced ORC-Pool, a geometric pooling operator that leverages coarse geometry, characterized byOllivier-Ricci curvature and an associated geometric flow. ORC-Pool extends a class of Ricci flow basedclustering algorithms to attributed graphs and can be incorporated into GNN architectures as a pooling layer.While curvature adjustment leads to improved performance in downstream tasks, it also adds computationaloverhead, specifically in node-level tasks with large, dense input graphs. While our experiments indicate thatthe runtime impact is modest in most cases, we show that a combinatorial ORC approximation can reduceruntime impact while retaining accuracy on node-level tasks. Nevertheless, an important direction for futurework is the study of alternative curvature notions or approximations (see Apx. A.2), which could result inmore scalable implementations of ORC-Pool. In addition, it would be interesting to investigate the impactof ORC-Pool layers in other graph learning tasks, such as graph regression or reconstruction, as well as on awider range of graph domains. Lastly, it would be interesting to consider extensions to directed input graphs.",
  "I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts without eigenvectors a multilevel approach.IEEE transactions on pattern analysis and machine intelligence, 29(11):19441957, 2007": "A. Duval and F. Malliaros. Higher-order clustering and pooling for graph neural networks. In Proceedingsof the 31st ACM international conference on information & knowledge management, pages 426435,2022. V. P. Dwivedi, L. Rampek, M. Galkin, A. Parviz, G. Wolf, A. T. Luu, and D. Beaini. Long rangegraph benchmark. In Thirty-sixth Conference on Neural Information Processing Systems Datasets andBenchmarks Track, 2022.",
  "Y. Lin, L. Lu, and S.-T. Yau. Ricci curvature of graphs. Tohoku Mathematical Journal, 63(4):605 627,2011": "C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and M. Neumann. TUDataset: A collectionof benchmark datasets for learning with graphs. In ICML 2020 Workshop on Graph RepresentationLearning and Beyond (GRL+ 2020), 2020. C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grohe. Weisfeilerand Leman go neural: Higher-order graph neural networks. In Proceedings of the AAAI conference onartificial intelligence, volume 33, pages 46024609, 2019.",
  "For completeness, we restate the upper and lower bounds on ORC derived by Tian et al. (54) in our notation.We compute the combinatorial ORC approximation defined in sec. 2.2.3 using these bounds": "Recall the definition of ORC given in Eq.2, and note that we fix t = 1 and = 0.Let N(x) bethe 1-hop neighborhood of node x. For vertices u, v we have := N(u) \\ N(v), r := N(v) \\ N(u), andc := N(u) N(v). Mass distributions on node neighborhoods are denoted as pu(x). We further define theshorthands Lu :=",
  "A.2Formans Curvature": "Formans curvature (23) gives a combinatorial notion of Ricci curvature, which aggregates local informationin the 2-hop neighborhood of an edge. That is, we consider the endpoints of a certain edge and all edges thatshare an endpoint with the edge. One formalization of this curvature, which considers the contributions ofconnectivity and triangles for the curvature computation, resembles Olliviers curvature qualitatively and hasfound applications in community detection. Its computation is easily parallelizable, allowing for a fast andscalable implementation. This has given rise to a range of applications of Formans curvature in NetworkAnalysis (60; 53; 48; 58; 62). While in general less suitable for identifying community structure (54), it couldpresent a viable alternative to ORC in large-scale graphs, where computing ORC is infeasible. We can adaptthe ORC-Pool operator proposed in this paper to the case of Formans curvature by simply exchanging thecurvature notion .",
  "BIllustration of Ricci Flow": "In the main text, we illustrated the evolution of edge weights under Ricci flow on a dumbbell graph. In ,we plot networks sampled from the stochastic block model, with two, three, four and five communities (leftto right). We show the network with original weights (all equal to one) on the top and curvature-adjustededge weights (after 5 iterations of Ricci flow, i.e., T = 5) at the bottom, where again darker colors correspondto smaller curvature. We see that curvature reliable uncovers bridges between communities due to their lowcurvature, whereas edges within communities have a higher weight. We further illustrate the effect of Ricci flow on molecular graphs, which are representative of graph topologiesoften encountered in graph-level tasks. shows the evolution of edge weights under Ricci flow (T = 4)for two graphs from the MUTAG data set, again initializing edge weights uniformely with one. We see thatthe curvature-adjustment highlights bridges (dark color) between functional substructures, e.g., rings. Thisallows the selection function to perform a pooling that aligns with multi-scale structure in the graph.",
  "CAdditional Discussion of Related Work": "We provide a more detailed discussion of related work by (47), which also proposes a curvature-based graphpooling approach, albeit with several key differences to our approach. We provide a detailed conceptualcomparison of the two approaches below; an experimental comparison can be found in sec. E.3. Setting.Classically, graph pooling layers are designed to jointly evaluate similarity structure encoded in thenode features and graph connectivity, instead of connectivity only (see, e.g., DiffPool, MincutPool, as well as",
  ": Evolution of edge weights under Ricci flow in MUTAG": "discussions in references such as (28)). illustrates that in attributed graphs both types of informationneed to be evaluated to identify meaningful sets of nodes to be pooled. However, the pooling layer introducedin (47) only evaluates similarity structure encoded in the connectivity. This is also corroborated by theexperimental comparison of both approaches below, in which ORC-Pool achieves higher performance for alltasks on attributed graphs. Motivation.The motivation for our proposed pooling layer (and that of related pooling layers) is tocapture salient multi-scale structure. In sec. 2.2, we give a geometric motivation for how curvature capturessuch structure utilizing a connection of discrete Ricci curvature and random walks (see also ). On theother hand, the approach in Sanders et al. is motivated by addressing over-smoothing and over-squashing.While we agree that pooling (in general) mitigates over-smoothing due to the induced scale separation (as wediscuss in sec. 4.1), over-squashing effects may actually be amplified as we discuss in the appendix. Different use of curvature and different notion.We note that while both the approach in Sanders etal. and our proposed approach leverages different notions of curvature, there are two fundamental differences:(1) Our approach relies on a curvature-adjusted adjacency matrix, which is computed using Ricci flow, ageometric flow associated with discrete Ricci flow. The approach in Sanders et al. uses a discrete notion of",
  "D.1Details on Experimental Setup": "We implement a GNN architecture consisting of GCN base layers and a single pooling layer, which are jointlytrained. For node-level tasks, one GCN layer is used to compute a graph embedding, and another GCNlayer is used to compute node clusters from the embedding. Then the pooling layer computes the loss ofthe current node assignments, which is used to update the GCN parameters. For the graph-level tasks, ourarchitecture alternates between blocks of GCN base layers and pooling layers, which are jointly trained. Wetrain a global pooling layer on top, which computes the readout by aggregating node representations acrossthe graph. To reduce the computational overhead of the ORC computation, we use the curvature-adjustmentonly in the first pooling layer, i.e., the second pooling layer is a standard MinCutPool layer. We note thatthe choice of using MinCutPool in subsequent layers was driven by a desire to maximize scalability, as thecurvature-adjustment would require recomputing curvature in the coarsened graph. However, we note thatcomputing the curvature-adjustment in subsequent layers has reduced computational cost due to the smallersize of the coarsened graph, which suggests that this is a viable extension of the present approach that maybe considered in future work. To compare against Graclus, we incorporated the Graclus pooling layer, based on METIS, to PyTorchGeometric. We note that our implementation is not optimized for runtime. Hence, we expect that it ispossible to obtain further speedups in Graclus.",
  "D.2Node Clustering": "Architecture.The first GCN layer embeds the nodes of the graph into an m-dimensional feature space.That is, if the graph initially had a node feature tensor of dimension n k, where n is the number of nodes,in the output we have a node feature tensor of dimension n m. The second GCN layer takes this embedded",
  "graph and computes an assignment tensor of dimension n c, where c is the number of clusters. By takingthe softmax of this n c tensor, we obtain the final assignment of each node to a single cluster": "We note that in the original implementation of TVPool, custom MP layers are used before pooling. Sincethe goal of our experiments is a fair comparison of the effectiveness of the pooling layers, we use standardGCN layers as base layers for all pooling approaches. We also performed the experiments with a GCN without pooling using the same architecture, but omittingthe pooling layer. The output is computed by a softmax function, where the predicted class corresponds tothe element with the highest value.",
  "Cora27081056614337CiteSeer3327910437036PubMed19717886485003": "Hyperparameters.Parameters for the experiments are as follows: For Planetoid, one GCN layer withoutput dimension 8 and an ELU activation function is used to embed the graph. The optimizer is an Adamoptimizer with a learning rate of 1e2. The models are trained for at most 10000 epochs, or until the bestNMI is found under a patience constraint. That is, if we achieve the current best NMI on epoch n andpatience is p, we stop training if the model does not have a better NMI at any epoch up to n + p. For thePlanetoid graphs, patience is set to 100, and for Amazon-ratings (from Heterophilous), patience is setto 250. When applying ORC-Pool to the Planetoid graphs, four Ricci flow iterations are used, and forAmazon-ratings, two Ricci flow iterations are used. NMI.We measure accuracy as the Normalized Mutual Information (short: NMI) if the cluster assignmentscomputed by the GNN. NMI is a classical evaluation metric for community detection. In our experimentswe employ NMI to measure the accuracy in the node clustering task. We give a brief definition of NMI forcompleteness. Let S RNk denote a vector that encodes the label assignment to k clusters, i.e., we set sil = 1 if node ibelongs to cluster Cl and sil = 0 otherwise. The entries of S can be viewed as random varibles drawn fromthe distribution P(sl = 1) = Nl/N and P(sl = 0) = 1 P(sl = 1), where Nl := |Cl|. Using the marginalprobability distribution Psl and the joint probability distribution P(sl, sl), we further define the entropiesH(sl) and H(sl, sl), as well as the conditional entropy of sl given sl as H(sl|sl) = H(sl, sl) H(sl). TheNMI for two cluster assignments S, S is given by",
  "D.3Graph Classification": "Architecture.The main part of the model consists of two blocks of GCN layers followed by a poolinglayer. The first set of GCN layer(s) embeds the nodes of the graph into an m-dimensional feature space.That is, if the graph initially had a node feature tensor of dimension n k, where n is the number of nodes,in the output we have a node feature tensor of dimension n m. The second GCN layer takes this embeddedgraph and computes an assignment tensor of dimension n c, where c is the number of clusters. By takingthe softmax of this n c tensor, we obtain the final assignment of each node to a single cluster. Here, theclustering simply gives us a new graph in which groups of nodes have been aggregated, which we pass intothe second block. After the second block of GCN layer(s) and pooling, we pass the resulting graph into athird block of GCN layers to get the final node embeddings. These are passed into a global pooling layer,which aggregates node embeddings. In our case, we use global mean pooling, which simply computes theaverage of all node embeddings. The resulting vector is then passed into a linear classifier.",
  "As an additional baseline, we also run a GCN without pooling on all data sets. The output is obtained usinga global mean pooling followed by a linear classifier": "Data.Statistics for TUDataset graphs are shown below (37). MUTAG is a common dataset of chemicalswhere the task is to predict the mutagenic effect. For ENZYMES, we classify enzymes into one of six classesdepending on which type of chemical reaction they catalyze. PROTEINS has a binary classification task forwhether a protein is an enzyme. REDDIT-BINARY, COLLAB, and IMDB-BINARY are social networkswhere we predict the subreddit, research field, and genre of people in the network, respectively. For thegraphs that did not have any node features (COLLAB, IMDB-BINARY, REDDIT-BINARY), we add adummy node feature that is 1.0 for all nodes.",
  "Peptides-func15535910": "Hyperparameters.Parameters for the experiments are as follows: For all graphs, the GCN blocks depictedabove consist of a GCN layer with output dimension 8 and an ELU activation function. The optimizer is anAdam optimizer with a learning rate of 5e4 and a weight decay of 1e4. The models are trained for atmost 10000 epochs, or until the best accuracy on a validation is found under a patience constraint. That is,if we achieve the current best validation accuracy on epoch n and patience is p, we stop training if the model",
  "REDDIT-BINARYCOLLABPEPTIDES-FUNC": "No Pool1.68 0.061.02 0.022.85 0.05Diff5.28 0.181.74 0.033.79 0.02Mincut5.39 0.172.05 0.014.64 0.07DMoN2.95 0.052.11 0.014.68 0.07TV5.34 0.092.10 0.024.35 0.05Graclus4.83 0.111.90 0.034.01 0.04ORC (us)12.90 0.176.05 0.0136.27 0.10 Results for Heterophilous Graphs.The Heterophilous graph Amazon-ratings is a network of Amazonproducts where edges connect products that are frequently bought together (45). The five classes correspondto product ratings. Unlike the Planetoid graphs, we do not expect nodes of the same class to consistentlycluster together. The natural clusters of the graph generally correspond to categories of items, rather thanitem ratings. Pooling layers combine clusters of nodes in order to coarsen a graph, a procedure whose utility",
  "Amazon-ratings24492930503005": "depends on an implicit homophily assumption. Hence, we do not expect GNNs with pooling layers to performwell on data sets such as the Amazon graph. This is confirmed by our experimental results, which show poorperformance (in terms of NMI) for all models. In contrast, in the original Heterophilous study, GCNswithout pooling layers were able to attain good performance (45).",
  "E.1Number of Ricci Flow iterations": "We investigate the impact of using a varying number of Ricci flow iterations on the performance of ORC-Pool.In the case of node clustering, we re-run ORC-Pool on the Cora and CiteSeer graphs, using 2, 4, and 6iterations of Ricci flow. In the results presented in the main text, 4 iterations were used.",
  "0.04 (0.033 0.001)0.34 0.03 (0.028 0.000)40.47 0.04 (0.035 0.000)0.35 0.04 (0.029 0.001)60.44 0.05 (0.034 0.001)0.33 0.02 (0.029 0.001)": "We find that the NMI is comparable for different numbers of Ricci flow iterations. This indicates thatthe performance ofORC-Pool is not very sensitive to the number of iterations used, and that thecurvature-adjustment captures crucial multi-scale structure with only a few iterations. We also investigate the impact of different numbers of iterations of Ricci flow on a graph classification task.We compared different numbers of iterations for the REDDIT-BINARY data set, testing 2, 3, and 4 iterations.In the main results, 3 iterations were used. Our results show comparable results across all experiments,implying that the observations in node-level tasks above extend to this setting. Based on our results, weexpect that the performance of ORC-Pool is not very sensitive to the number of iterations of Ricci flowand computing the curvature-adjustment based on a small number of iterations already leads to significantperformance increases. This is consistent with previous findings for curvature-based methods, see, e.g., (54).",
  "For all of the experiments above, we used standard GCN layers as base layers for each experiment. Here, weinvestigate the performance of our model architecture using GAT and GIN instead of GCN": "For node clustering, we re-run our CiteSeer experiment, again comparing ORC-Pool against four baselines.The experimental setup follows the description in Apx. D.2 apart from GIN and GAT replacing GCN. Ourresults show a high NMI and overall similar performance gains using ORC-Pool. For graph classification,we make analogous observations when re-running our PROTEINS experiment with GAT and GIN replacingGCN base layers in the model architecture described in Apx. D.3.",
  "GATGIN": "Diff0.28 0.031 (0.062 0.004)0.29 0.025 (0.097 0.003)Mincut0.32 0.018 (0.083 0.002)0.33 0.023 (0.118 0.004)DMoN0.31 0.063 (0.077 0.008)0.30 0.070 (0.094 0.010)TV0.32 0.016 (0.086 0.007)0.30 0.026 (0.095 0.008)ORC (us)0.34 0.040 (0.156 0.016)0.35 0.040 (0.219 0.019) : We report average classification accuracy for PROTEINS with GAT and GIN replacing GCN baselayers. We use a 80/10/10 train/val/test split. Averages are determined based on 10 trials. Highest accuracyin bold.",
  "Curv0.350.320.15ORC (us)0.47 0.040.35 0.040.24 0.04": "For node clustering, we compare the two models on three Planetoid data sets. The CurvPool clustering isdeterministic, as it is based only on the edge curvatures of the original graph (hence no variance is reported).We used HighCurvPool with a threshold of 0.2. We see that ORC-Pool outperforms CurvPool onall three Planetoid graphs, by an especially large margin for Cora and PubMed. This is expected, sinceORC-Pool utilizes information encoded in both node attribute and connectivity, whereas CurvPool onlyutilizes connectivity. For graph classification, we compared ORC-Pool and CurvPool on one attributed (PROTEINS) and oneunattributed data set (IMDB-BINARY). We observe that ORC-Pool achieved higher performance thanCurvPool on PROTEINS and equal performance on IMDB-BINARY. This illustrates that ORC-Pool isable to utilize crucial information encoded in the node attributes and that this is crucial for the performancegains observed across data sets. We note that achieving high performance with CurvPool requires careful tuning of the curvature thresholdfor pooling nodes. As shown in (47), the choice of this hyperparameter can cause accuracy to change by asmuch as 5-10 percentage points, depending on the dataset. Therefore, hyperparameter tuning, e.g., gridsearch needs to be performed to determine the optimal threshold. In contrast, the key hyperparameterin ORC-Poolis the choice of the number of Ricci flow iterations to which downstream performance is fairlyinsensitive, as the experiments above indicate.",
  "We include additional experiments on large-scale data sets from the Open Graph Benchmark (33). For nodeclustering, we use OGBN-ARXIV, and for graph classification, we use OGBG-MOLHIV": "Like Cora and CiteSeer, OGBN-ARXIV is a citation network. It has 169,343 nodes and 1,166,243 edges; thegoal is the classification of papers by subject area. OGBG-MOLHIV is a molecular property prediction dataset.It has 41,127 graphs with an average of 25.5 nodes. We follow the preferred evaluation metrics for OGB,reporting accuracy for OGBN-ARXIV and ROC-AUC for OGBG-MOLHIV. When running ORC-Pool onOGBN-ARXIV, 4 iterations of Ricci flow were used. For OGBG-MOLHIV, 2 iterations of Ricci flow wereused.",
  "Expressivity.In the main text we stated the following result on the impact of ORC-Pool layers on theexpressivity of the GNN:": "Corollary 2 (Expressivity of ORC-Pool). Consider a simple architecture with a block of MP base layers,following by a ORC-Pool layer. Let G1, G2 denote two 1-WL-distinguishable graphs with node attributesX1, X2. Further let X1 = X2 denote the node representations learned by the block of MP layers. Then thecoarsened graphs GP1 , GP2 learned by the ORC-Pool layer are 1-WL distinguishable.",
  "Proof. This result is a simple adaptation of a recent result by (10, Thm.1), which establishes conditions underwhich pooling layers preserve expressivity:": "Theorem 4 ( (10), Thm. 1). Let G = {X, E} (X RNm) denote the input graph and G the graphobtained after applying a block of MP base layers to G; X denoting the new multiset of node features. LetPool : G GP denote an SRC pooling layer after the MP layers, which produces a pooled graph GP withmulti-sets XP . Pool preserves the expressivity of the MP layers, provided that the following conditions hold:",
  ". The reduction function assigns supernode representations as xPj = Ni=1 xisji": "The authors show that conditions (2),(3) hold for dense pooling layers, such as MinCutPool and, hence, alsoORC-Pool. In particular, by construction, graph cuts or partition-based community detection algorithmsproduce non-overlapping communities; hence the selection function assigns nodes to a unique cluster, whichbecomes a supernode. The reduction function computes supernode attributes as XP = ST X, which alignswith condition (3). Condition (1) is independent of the choice of pooling operator and is fulfilled for any MPlayer that is as powerful as the 1-WL test, e.g., GIN (63). Remark 1. We note that the inherent limitations in representational power in the MP layers may affect thequality of the coarsening learned by the pooling layer for certain classes of graphs. However, the empiricalresults presented in this work and the related literature indicate that, in practice, pooling layers coarsen graphseffectively.",
  "This property is inherited from MinCutPool. For completeness, we provide a the argument below": "Proof. Again, we utilize the SRC framework to describe the ORC-Pool operator. The seletion functionSel computes a cluster assignment matrix S. It is easy to see that the graph cut objective (Eq. (3.5)) ispermutation-equivariant. The permutation-equivariance is not impacted by the curvature-adjustment, asit simply performs a re-weighting of the edges. It is further easy to see that the reduction and connectionfunctions (Red and Con) are permutation-invariant. With that, the composition (Sel Red) Con ispermutation-invariant, as desired. Pooling and Over-squashing.As the number of layers in an MPGNN increases, one observes the formationof bottlenecks (2): Information from far distant nodes is encoded into fixed-length node representationsduring message passing, leading to information loss. This effect is particularly strong for bridges betweenclusters, which connect dissimilar nodes whose neighborhoods have a small intersection. Previous literature",
  ": Ga,b (a =b = 3)": "In the main text, we have given a brief argument for how coarsening approachesbased on ORC-Pool preserve the structural integrity of the graph. In this section,we expand on these results. To corroborate our claim that ORC flow reveals coarsegeometry and emphasizes the community structure, we employ a graph model, whichexhibits community structure as found in real data:Definition 2. Consider a class of model graphs Ga,b (a > b > 2), which areconstructed by taking a complete graph with b nodes and replacing each by a completegraph with a + 1 nodes. Graphs of the form Ga,b are special cases of stochastic block models (1) with b blocks(communities) of size a + 1 (see for an example). Due to the regularity of the graph, we can categorizeits edges into three types: (1) bridges, which connect dissimilar nodes in different clusters; (2) internal edges,which connect similar nodes in the same cluster that are at most one hop removed from a dissimilar node; and(3) all other internal edges, which connect similar nodes within the same cluster. (41) derived an evolutionequation for edge weights under ORC flow for Ga,b graphs:Lemma 3 ((41)). Let wt = [wt1, wt2, wt3] denote the weights of edges of types (1)-(3). Assuming initializationw0 = , the edge weights evolve under ORC flow wt+1u,v (1 uvdG(u, v))wtu,v as",
  "(Cu, Cv) .(10)": "Here, W = (wij) denotes a weighted adjacency matrix, dv weighted node degrees and (Cu, Cv) = 1, if u, vare in the same cluster and zero otherwise. It can be shown that a higher modularity corresponds to astronger separation of the graph into subgraphs, which is easier to detect; e.g., the corresponding min-cutproblem is easier to solve. We show that modularity increases, as edge weights evolve under ORC flow:"
}