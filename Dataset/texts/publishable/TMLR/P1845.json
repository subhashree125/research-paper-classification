{
  "Abstract": "Gaussian smoothed sliced Wasserstein distance has been recently introduced for comparingprobability distributions, while preserving privacy on the data. It has been shown that itprovides performances similar to its non-smoothed (non-private) counterpart. However, thecomputational and statistical properties of such a metric have not yet been well-established.This work investigates the theoretical properties of this distance as well as those of generalizedversions denoted as Gaussian-smoothed sliced divergences GSDp.We rst show thatsmoothing and slicing preserve the metric property and the weak topology.To studythe sample complexity of such divergences, we then introduce n thedouble empiricaldistribution for the smoothed-projected . The distribution n is a result of a doublesampling process: one from sampling according to the origin distribution and the secondaccording to the convolution of the projection of on the unit sphere and the Gaussiansmoothing. We particularly focus on the Gaussian smoothed sliced Wasserstein distanceGSWp and prove that it converges with a rate O(n1/2p). We also derive other properties,including continuity, of dierent divergences with respect to the smoothing parameter. Wesupport our theoretical ndings with empirical studies in the context of privacy-preservingdomain adaptation.",
  "Introduction": "Divergences for comparing two distributions have been shown to be important for achieving good performancein the contexts of generative modeling (Arjovsky et al., 2017; Salimans et al., 2018), domain adaptation (Longet al., 2015; Courty et al., 2016; Lee et al., 2019), and in computer vision (Bonneel et al., 2011; Solomonet al., 2015) among many more applications (Kolouri et al., 2017; Peyr & Cuturi, 2019; Nguyen et al., 2023).Examples of divergences that have proved useful for these tasks are the Maximum Mean Discrepancy (Grettonet al., 2012; Long et al., 2015; Sutherland et al., 2017), the Wasserstein distance (Monge, 1781; Kantorovich,1942; Villani, 2009) or its variant the sliced Wasserstein distance (SW) (Kolouri et al., 2016; Bonneel &Coeurjolly, 2019; Kolouri et al., 2019b; Nguyen et al., 2021; 2022; 2024). The SW distance has the advantage of being computationally ecient, since it uses a closed-form solution fordistributions with support on R, by computing the expectation of one-dimensional (1D) random projectionsof distributions in Rd. Owing to this eciency and the resulting scalability, this distance has been successfullyapplied in several applications ranging from generative models to domain adaptation (Kolouri et al., 2019a;",
  "Deshpande et al., 2019; Wu et al., 2019; Lee et al., 2019) and its statistical properties have been well-studiedin Nadjahi et al. (2020)": "Recently, Gaussian smoothed variants of the Wasserstein distance and the sliced Wasserstein distance havebeen introduced respectively in (Nietert et al., 2021) and in Rakotomamonjy & Ralaivola (2021). One mainmotivation behind these variants is to provide a privacy guarantee for the distribution comparison task asGaussian smoothing is known to be a mechanism for achieving dierential privacy (Dwork et al., 2014). Whilethe properties of the Gaussian smoothed Wasserstein distance have been extensively studied by Nietert et al.(2021), the properties of the Gaussian smoothed sliced Wasserstein distance have not been fully investigatedyet although they are known to be more computationally ecient. In this work, we focus on the slicing of Gaussian-smoothed measure discrepancies by providing theoreticalproperties of more general divergences induced by some base distances or divergences for distributions denedin Rd. These base distances/divergences encompass Wasserstein, maximum mean discrepancy, Sinkhorndivergence. As for a main contribution, we rst establish the topological properties of these divergences interm of a metrization of the weak topology and a semi-lower continuous property. Then we focus on the samplecomplexity of such divergences by introducing the double empirical distribution n for the smoothed-projectedorigin distribution . The new empirical distribution is a result of double sampling process: one from samplingaccording to the origin distribution and the second according to the convolution of the projection of onthe unit sphere and the Gaussian smoothing. The introducing of n is inspired from the implementationpart: we sample X1, . . . , Xn from the raw distribution to dene n then project it on the unit sphere andsmooth this projection with a Gaussian distribution. This smoothing is a continuous measure that needs tobe sampled. For that reason, we add a double sampling and then provide n. We particularly focus on theGaussian smoothed sliced Wasserstein distance. Given the importance of the noise level in the privacy/utility trade-o achieved by the divergence, weinvestigate an order relation and a continuity result with respect to the noise level. These properties are ofhigh impact as it supports a computationally cheap warm-start/ne-tuning procedure when looking for aprivacy/utility compromise of the divergence. Our theoretical study is backed by some numerical experimentson toy problems and on domain adaptation illustrating how owing to the topology induced by our metric andits continuity, dierential privacy comes almost for free (without loss of performance) and multiple modelswith dierent level of privacy can be cheaply computed. Comparison with previous works.Here we highlight the position of this work compared to the mostlinked previous ones, in particular Nadjahi et al. (2020) and Rakotomamonjy & Ralaivola (2021). The workof Nadjahi et al. (2020) is focused on sliced Wasserstein distance and its statistical properties, however ourwork is based on the properties of the Gaussian smoothed with general divergences (e.g. Wasserstein, MMD,Sinkhorn divergence). We argue that the properties cannot be directly derived from (Nadjahi et al., 2020),especially the sample complexity result. In Rakotomamonjy & Ralaivola (2021), the authors investigated thesmoothed Wasserstein distance and their theoretical nding was principally on proving the metric property,whereas we further investigate sample and projection complexities and the continuity properties w.r.t. thesmoothing noise level. We emphasize that the novelty of the present paper consists in the theoreticalproperties derived from the denition of the empirical measure n. The smoothing of the raw measures, froma theoretical point view, is a continuous measure (see Lemma 3.5) that needs to be sampled. This entails todene the second sampling step and construct n, an empirical version for the smoothing projection of . Tothe best of our knowledge, this work is the rst introducing the double randomness in the case of smoothingoptimal transport discrepancies. Recent works (Goldfeld et al., 2020; Nietert et al., 2021) addressed thesmoothing Wasserstein an their theoretical results relied only on n. Layout of the paper.The paper is organized as follows: after introducing the notation and somebackground in , we detail the topological properties of Gaussian-smoothed sliced divergence in.1 while the double sampling process and its statistical properties are established in .2. Thenoise analyses are provided in .3. Experimental analyses for supporting the theory and showcasing therelevance of our divergences in domain adaptation are depicted in . Discussions on the perspectivesand limitations are in . All the proofs of the theoretical results and some additional experiments arepostponed to the appendices in the supplementary.",
  "x": "y 1A(x + y)d(x)d(y), where 1A() is the indicator functionover A. Given two independent random variables X and Y , we remind that X + Y . Thed-dimensional unit-sphere is noted as Sd1 { Rd : = 1}. We denote by ud the uniform distributionon Sd1 and we use () to denote the Kronecker delta function. We note as Ef the expectation of thefunction f with respect to . Let : R R be the Gamma function expressed as (v) = 0tv1etdt for v > 0. For k N, ()k denotedthe Pochhammer symbol, also known in the literature as a rising factorial, namely ()0 = 1, ()1 = , and()k = (+k)",
  "RdRd x xp(x, x)dxdx1/p": "where (, ) { P(Rd Rd)|1# = , 2# = } and 1, 2 are the marginal projectors of on eachof its coordinates. When d = 1, the Wasserstein distance can be calculated in closed-form owing to thecumulative distributions of and (Rachev & Rschendorf, 1998). In practice for empirical distributions,the closed-form solution requires only the sorting of the samples, which makes it very ecient. Becauseof this eciency, eorts have been devoted to derive a metric for high-dimensional distributions basedon 1D Wasserstein distance. The main idea is to project high-dimensional probability distributions ontoa random one-dimensional space and then to compute the Wasserstein distance. This operation can betheoretically formalized through the use of the Radon transform, leading to the so-called sliced Wassersteindistance (Kolouri et al., 2016; Bonneel & Coeurjolly, 2019; Kolouri et al., 2019b; Nguyen et al., 2021).",
  "Published in Transactions on Machine Learning Research (11/2024)": "and for each dataset, we used the same neural network architecture for representation mapping and forclassication. Approaches dier only on how distance between distributions have been computed. Here foreach noise value , we have trained the model from scratch for 100 epochs. Results are depicted in .For the two problems, we can see that performances obtained with the Gaussian-smoothed sliced Wassersteinor MMD divergences are similar to those obtained with DANN or SWD across all ranges of noise. Thesmoothed version of Sinkhorn is less stable and induces a slight loss of performance. Owing to the metricproperty and the induced weak topology, the privacy preservation comes almost without loss of performancein this domain adaptation context. In the second analysis, we have studied the privacy/utility trade-o when ne-tuning models, using only oneepoch, for decreasing values of . Results are shown in . They highlight that depending on the dataand the used smoothed divergence, performance varies between one percent for Oce 31 to four percent forUSPS to MNIST. Note that except for the largest value of , we are training a model using only one epochinstead of a hundred. A very large gain in complexity is thus achieved for swiping the full range of noise level.Hence depending on the importance this slight drop in performance will have, it is worth using a large valueof and preserving strong privacy or go through a validation procedure of several (cheaply obtained) models.",
  ". if D(, ) satises the triangle inequality then GSDp(, ) satises the triangle inequality": "The above theorem shows that under mild hypotheses over the base divergence D, as being a metric forinstance, the metric property of its Gaussian-smoothed sliced version naturally derives. As exposed in theappendix, the more involved property to prove is the identity of indiscernibles. We further postponed to the appendix the proofs of the two other topological properties: (i) GSD metrizesthe weak topology on Pp(Rd) and (ii) GSD is lower semi-continuous with respect to the weak topology inPp(Rd). Now, we establish under which conditions on the divergence D, the convergence of a sequence in GSDimplies weak convergence in Pp(Rd). We say that {k}kN converges weakly to and write, k , iff(x)dk(x) f(x)d(x), as k , for every f in the space of all bounded continuous real functions. Theorem 3.2. Let > 0, p 1, Pp(Rd), and {k Pp(Rd)}kN a sequence of distributions. Assumethat the divergence D is bounded and metrizes the weak topology on P(R). Then, limk GSDp(k, ) = 0if and only if k .",
  "Statistical properties": "The next theoretical question we are interested in is about the incurred error when the true distribution isapproximated by its empirical distribution n. Such a case is common in practical applications where only (high-dimensional) empirical samples are at disposal. Specically, we are interested in quantifying two key propertiesof empirical Gaussian-smoothed divergence: (i) the convergence of the double empirical GSDp(n, n)(see Denition 3.6) to GSDp(, ) (ii) the convergence of GSDp(, ) (see (1)) to GSDp(, ), whenapproximating the expectation over the random projection with sample mean.",
  "Double empirical divergence of GSD": "Let T x1 , . . . , T xn and T y1 , . . . , T yn be i.i.d. observations of Run N and Run N, respectively. Sampling i.i.d.{T xi }i=1,...,n is given by the following scheme: for i = 1, . . . , n, we rst choose the component N(uXi, 2)from the mixture 1",
  "Sample complexity of GSWp": "Herein, our goal is to quantify the error made when approximating GSWp(, ) with GSWp(n, n).More precisely, we are interested in establishing an order of the convergence rate of GSDp(n, n) towardsGSDp(, ), according to the sample size n. This rate stands for the so-called sample complexity. The convergence results in the sequel are given in expectation. Recall that the empirical distributionsare derived from a double sampling process, which leads to consider a double expectations, wrt the origindistribution En and wrt the sampling from the Gaussian smoothing EN nwhere n and N nare then-fold product extensions of and N, respectively. We rst consider the conditional expectation given thesamples X1, . . . , Xn, i.e. EN n[|X1, . . . , Xn], and then apply En. We denote by",
  "nni=1 N(uXi, 2), ). For (i) we get a standard order of O( log n": "n ), which comes from aby-product of Fournier & Guillin (2015). For (ii), through a coupling via the maximal coupling using the totalvariation distance (Theorem 6.15 in Villani (2009)), we obtain the order O(n1/2). The control technique for(ii) was inspired from Goldfeld et al. (2020) and Nietert et al. (2021).",
  "< 2/22. This can be satised when X is a -sub-gausssian ( 0). Namely, E[e(XE[X])] e2": "2for all Rd. If the parameter veries < /2, then the latter condition holds.Remark 3.10. Note that the sample complexity depends on the amount of smoothing through the moment ofthe Gaussian noise : the larger the amount of smoothing (and thus the privacy), the worse is the constantof the complexity. Hence, a trade-o on privacy and statistical estimation appears here as a reasonableguarantee on the dierential privacy usually requires a large Gaussian variance.",
  "Sd1 Dp(Ru N, Ru N)ud(u)du": "The term A2(p, ) corresponds to the variance of Dp(RuN, Ru N) with respect to u ud. It is worthto note that the precision of the Monte Carlo scheme approximation depends on the number of projections Land the variance of the evaluations of the divergence Dp . The estimation error decreases at the rate L1/2",
  "L": "However we dont have a proper control for p 2 of the second term in the RHS, | GSWpp(n, n) GSWp(, )|, as it can be seen from Proposition 3.11. For that reason, we derive an overall complexity inthe case of p = 1.Corollary 3.13. The sample and projection complexities of GSW(, ) reads as complexity(GSW) =O(n1/2 + L1/2). If we consider the number of projections as L = n for some (0, 1) then the overallcomplexity complexity(GSW(, )) = O(n/2).",
  "Order relation": "We rst show that the noise level tends to reduce the dierence between two distributions as measured usingGSDp(, ) provided the base divergence D satises some mild assumptions.Proposition 3.14. Let , Pp(Rd) and consider the noise levels 1, 2 such that 0 1 2 < .Assume that the base divergence D satises D( N2, N2) D( N1, N1), for any , P(R).Then, G2SDp(, ) G1SDp(, ). Note that the assumption for the base divergence inequality holds for the Gaussian-smoothed Wassersteindistance Nietert et al. (2021). While we conjecture that it holds also for smoothed Sinkhorn and MMD, weleave the proofs for future works. Based on the property in Proposition 3.14, we show some specic propertiesof the metric with respect to the noise level .",
  "Supporting the theoretical results": "Sample complexity.The rst experiment (see ) analyzes the sample complexity of dierentbase divergences. It shows that the sample complexity stays similar to the one of their original and slicedcounterparts up to a constant (see Proposition 3.8). For this purpose, we have considered samples in Rd randomly drawn from a Normal distribution N(0, I). For the Sinkhorn divergence, the entropy regularizationhas been set to 0.1 and for MMD, we used a Gaussian kernel for which the bandwidth has been set to themean of all pairwise distances between samples. The number of projections has been xed to L = 50 and weperform 20 runs per experiment. For the rst study, the convergence rate has been evaluated by increasing thesamples number up to 25,000 with xed dimension d = 50. For the second one, we vary both the dimensionand the number of samples. shows the sample complexity of some sliced divergences, respectively noted as SWD, SKD andMMD for Sliced Wasserstein distance, Sinkhorn divergence and Maximum Mean discrepancy and theirGaussian-smoothed sliced versions, named as GS SWD, GS SKD and GS MMD. On the top plot, we can seethat all Gaussian-smoothed sliced divergences preserve the complexity rate with just a slight to moderate",
  "Two other experiments on the sample complexity and identity of indiscernibles are also reported in thesupplementary material": "Projection complexity.We have also investigated the impact of the number of projections when estimatingthe distance between two sets of 500 samples drawn from the same distribution, N(0, I). plotsthe approximation error between the true expectation of the sliced divergences (computed for a number ofL = 10, 000 projections) and its approximated versions. We remark that, for all methods, the error rangeswithin 10-fold when approximating with 50 projections and decreases with the number of projections. Performance path on the impact of the noise parameter.Since the Gaussian smoothing parameter is key in a privacy preserving context, as it impacts on the level of privacy of the Gaussian mechanism, wehave analyzed its impact on the smoothed sliced divergence. We have reproduced the experiment for thesample complexity but with dierent values of . The number of projections has been set to 50. shows these sample complexities. The rst very interesting point to note is that the smoothing parameter hasalmost no eect on the GS MMD sample complexity. For the GS SWD and GS SKD divergences, instead,the smoothing tends to increase the divergence at xed number of samples. Another interpretation is thatto achieve a given value of divergence, one needs more far samples when the smoothing is larger (i.e. forgetting a given divergence value at = 5, one needs almost 10-fold more samples for = 15). This overheadof samples needed when smoothing increases is properly described, for the Gaussian-smoothed sliced SWD inour Proposition 3.8, as the sample complexity depends on the moments of the Gaussian. As for conclusion from these analyses, we highlight that the Gaussian-smoothed sliced MMD seems to presentseveral strong benets: its sample complexity does not depend on the dimension and seems to be the bestone among the divergence we considered. More interestingly, it is not impacted by the amount of Gaussiansmoothing and thus not impacted by a desired privacy level.",
  "Domain adaptation with GSW": "As an application, we have considered the problem of unsupervised domain adaptation for a classication task.In this context, given source examples Xs and their label ys and unlabeled target examples Xt, our goal is todesign a classier h() learned from the source examples that generalizes well on the target ones. A classicalapproach consists in learning a representation mapping g() that leads to invariant latent representations,invariance being measured as a distance between empirical distributions of mapped source and target samples.",
  "ming,hLc(h(g(Xs)), ys) + D(g(Xs), g(Xt))": "where Lc can be the cross-entropy loss or a quadratic loss and D a divergence between empirical distributions,in our case, D will be any Gaussian-smoothed sliced divergence. We solve this problem through stochasticgradient descent, similarly to many approaches that use sliced Wasserstein distance as a distributiondistance Lee et al. (2019). Note that, in practice, using a smoothed divergence preserves the privacy of thetarget samples as shown by (Rakotomamonjy & Ralaivola, 2021). When performing such model adaptation, a privacy/utility trade-o that has to be handled. In practice, onewould prefer the most private model while not hurting its performance. Hence, one would seek the largestnoise level > 0 to use while preserving accuracy on target domain. Hence, it is useful to evaluate how themodel performs on a range of noise level (hence, privacy level). This can be computationally expensive at itrequires to fully train several models on hundreds of epochs. Instead, we leverage on the continuity of ourGSD to employ a ne-tuning strategy: we train a domain adaptation model for the largest desired value of (over the full number of epochs) and when is decreased, we just ne-tune the lasted model by training ononly one epoch. Our experiments evaluate the studied Gaussian-smoothed sliced divergences in classical unsupervised domainadaptation. We have considered two datasets: a handwritten digit recognition (USPS/MNIST) and Oce 31datasets. In our rst analysis, we have compared our GSD performances with non-smoothed divergences. The rstone is the sliced Wasserstein distance (SWD) Lee et al. (2019) and the second one is the Jenssen-Shannonapproximation based on adversarial approach, known as DANN Ganin & Lempitsky (2015). For all methods",
  "Conclusion": "This work provided the properties of Gaussian-smoothed sliced divergences for comparing distributions. Wederived several theoretical results related to their topological and statistical properties and showed, undermild conditions on their base divergences, the smoothing and slicing operations preserves the metric property.From a statistical point of view, we introduced the double empirical distribution and focused on the samplecomplexity of the smoothed sliced Wasserstein distance and we proved that it converges with a rate O(n1/2p).We furhter analyzed the behavior of these divergences on domain adaptation problems and conrm the factthat using those divergences yields only to slight loss of performances while preserving privacy. Note that inthe obtained bound we use upper bound of higher moments of the smoothing distribution. An importantdirection for future research is considering non Gaussian smoothing distribution enjoying this property.",
  "Soheil Kolouri, Phillip E. Pope , Charles E. Martin, and Gustavo K. Rohde. Sliced Wasserstein auto-encoders.In International Conference on Learning Representations, 2019a": "Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized slicedWasserstein distances. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 261272. Curran Associates,Inc., 2019b. Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancyfor unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 1028510295, 2019. Tianyi Lin, Zeyu Zheng, Elynn Chen, Marco Cuturi, and Michael Jordan. On projection robust optimaltransport: Sample complexity and model misspecication. In International Conference on ArticialIntelligence and Statistics, pp. 262270. PMLR, 2021.",
  "Gaspard Monge. Mmoire sur la thotie des dblais et des remblais. Histoire de lAcadmie Royale desSciences, pp. 666704, 1781": "Kimia Nadjahi, Alain Durmus, Lnac Chizat, Soheil Kolouri, Shahin Shahrampour, and Umut imekli.Statistical and topological properties of sliced probability divergences. In Hugo Larochelle, MarcAurelioRanzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural InformationProcessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,December 6-12, 2020, virtual, 2020.",
  "Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs using optimal transport.In International Conference on Learning Representations, 2018": "Justin Solomon, Fernando d de Goes, Gabriel Peyr, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du,and Leonidas Guibas. Convolutional Wasserstein distances: Ecient optimal transportation on geometricdomains. ACM Trans. Graph., 34(4):66:166:11, 2015. Dougal J Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De, Aaditya Ramdas, Alexander J Smola,and Arthur Gretton. Generative models and model criticism via optimized maximum mean discrepancy.In ICLR (Poster), 2017.",
  "A.1Proof of Theorem 3.1: GSDp is a proper metric on Pp(Rd) Pp(Rd)": "Before starting the proof, we add this notation: the characteristic function of a probability distribution P(Rd) is (t) = E[eiXt]. Given this denition, similarly to the Fourier transform, the characteristicfunction of the convolution of two probability distributions readsas (t) = (t) (t). Non-negativity (or symmetry). The non-negativity (or symmetry) follows directly from the non-negativity(or symmetry) of Dp, see Denition 2.3. Identity property. If the base divergence Dp satises the identity property in one dimensional measures,then for any Pp(Rd) and u Sd1, one has that Dp(Ru N, Ru N) = 0, hence, by Denition2.3, GSDp(, ) = 0. Let us now prove the fact that for any , Pp(Rd), GSDp(, ) = 0 entails = a.s. On one hand, GSDp(, ) = 0 gives the fact that Dp(Ru N, Ru N) = 0 for ud-almost everyu Sd1, hence Ru N = Ru N for ud-almost every u Sd1. Following the techniques in proofof Proposition 5.1.2 in Bonnotte (2013), for any measure P(Rm) (with m 1), F[]() stands for theFourier transform of and is given as F[](v) =",
  "= F[N](v)F[](vu)": "Since for ud-almost every u Sd1, Ru N = Ru N, and hence F[Ru N] = F[Ru N] F[N]F[] = F[N]F[] (by the Fourier transform of the convolution) F[] = F[]. Since the Fouriertransform is injective, we conclude that = .Triangle inequality. Assume that D is a metric and let , , Pp(Rd). We then have",
  "A.2Proof of Theorem 3.2: GSDp metrizes the weak topology": "The proof is done by double implications and the technical material relies on the continuous mappingtheorem (Athreya & Lahiri, 2006) and bounded convergence theorem for the rst direct implication . Thesecond one, , is based on the fact that weak convergence is equivalent to the convergence correspondingto Lvy-Prokhorov distance (Huber, 2011)",
  "Sd1 limk Dp(Ruk N, Ru N)ud(u)du1/p= 0": "(Bycontrapositive).Supposethatkdoesntconvergeweaklytoandassumethatlimk GSDp(k, ) = 0.On one hand, since Rd is a complete separable space then the weak con-vergence is equivalent to the convergence corresponding to Lvy-Prokhorov distance dened as: TheLvy-Prokhorov distance (, ) between , P((E, ), T ) (space of probability measures on a measurablemetric space) is given by:",
  "(, ) = inf>0{(A) < (A) + ,(A) < (A) + ,for all A T }, where A = {x E : (x, A) < }": "Hence there exists > 0 and a subsequence {s(k)}kN such that (s(k), ) > . One the otherhand, we have limk GSDp(s(k), ) = 0, that is equivalent to {D(Rus(k) N, Ru N)}k con-verges to 0 in Lp(Sd1) = {f : Sd1 R| Sd1 f(u)ud(u)du < }. Since the Lp-convergence en-tails the point-wise convergence (Khoshnevisan, 2007), there exists a subsequence {s(t(k))}k such thatlimk D(Rus(t(k)) N, Ru N) = 0 almost everywhere for all u Sd1. Recall that the divergence D metrizes the weak convergence in P(R) then Rus(t(k)) N Ru N almost everywhere for all u Sd1.Therefore, Rus(t(k)) Ru almost everywhere for all u Sd1. Using Cramr-Wold device (Huber, 2011),we get s(t(k)) . Since the Lvy-Prokhorov distance metrizes the weak convergence, it entails thatlimk (s(t(k)), k) = 0, that contradicts the fact that (s(k), ) > . We then conclude by contrapositive",
  "A.3Proof of Proposition 3.3: GSDp is lower semi-continuous": "Recall that the base divergence D is lower semi-continuous w.r.t. the weak topology in P(R), namelyfor every sequence of measures {k}kN and {k}kN in P(R) such that k and k , one hasD(, ) lim infk D(k, k). Now, let {k}kN and {k}kN are two sequences of measure in Pp(Rd) such that k and k .By continuous mapping theorem (Bowers & Kalton, 2014) and Levys continuity theorem, we obtainRuk N Ru N and Ruk N Ru N for all u Sd1. Since the base divergence D is a lowersemi-continuous with respect to weak topology in P(R), then",
  "Let us give rst the overall structure of the proof. We we use frequently the triangle inequality for Wassersteindistances between the quantities n, 1": "nN(uXi, 2) and Ru N. We then obtain two quantities, I andII (see below for explicit), bounding En|N n[GSWp(n, )]. To control I bound, we use a well knownconverging bound in Fournier & Guillin (2015) of Wasserstein distance between empirical and true measure.For II bound, we consider maximal TV-coupling in Villani (2009)] and use result of the 2p-moment of absoluteGaussian random variable founded in Winkelbauer (2014).",
  "Step 2: Control of II": "We follow the lines of proofs of Proposition 1 in Goldfeld et al. (2020) and Theorem 2 in Nietert et al. (2021).Using a coupling n and Ru N) via the maximal TV-coupling (see Theorem 6.15 in Villani (2009)]), thecontrol of the total variation of the Wasserstein distance, we get for any xed u Sd1",
  "A.9Proof of Proposition 3.17: continuity of the smoothed Gaussian sliced Wasserstein w.r.t": "From Lemma 1 in (Nietert et al., 2021), we know that the Gaussian-smoothed Wasserstein is continuous withrespect to , for any distribution Ru and Ru. In addition, for any u, we have Wp(Ru N, Ru N) Wp(Ru, Ru). Then by applying Lebesgues dominated convergence theorem (Bowers & Kalton, 2014) tothe above inequality with Wp(Ru, Ru) as a dominating function, that is ud-almost everywhere integrablebecause both measures are in Pp(Rd), we then conclude that the Gaussian-smoothed SWD is continuousw.r.t. .",
  "A.10Proof of Proposition 3.18: continuity of the smoothed sliced squared-MMD w.r.t": "Let us rst recall the denition of the MMD divergence. Let k : R R R be a measurable boundedkernel on R and consider the reproducing kernel Hilbert space (RKHS) Hk associated with k and equippedwith inner product < , >Hk and norm Hk. Let PHk(R) be the set of probability measures suchthat",
  "B.2Identity of indiscernibles": "The second experiment aims at checking whether our divergences converge towards a small value when thedistributions to be compared are the same. For this, we consider samples from distributions and chosenas normal distributions with respectively mean 2 1d and s1d with varying s (noted as the displacement).Results are depicted in . We can see that all methods are able to attain their minimum when s = 2.Interestingly, the gap between the Gaussian smoothed and non-smoothed divergences for Wasserstein andSinkhorn is almost indiscernible as the distance between distribution increases."
}