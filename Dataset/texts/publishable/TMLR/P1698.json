{
  "Abstract": "Multi-Agent Reinforcement Learning (MARL) algorithms face the challenge of efficientexploration due to the exponential increase in the size of the joint state-action space.While demonstration-guided learning has proven beneficial in single-agent settings, its directapplicability to MARL is hindered by the practical difficulty of obtaining joint expert demon-strations. In this work, we introduce a novel concept of personalized expert demonstrations,tailored for each individual agent or, more broadly, each individual type of agent withina heterogeneous team. These demonstrations solely pertain to single-agent behaviors andhow each agent can achieve personal goals without encompassing any cooperative elements,thus naively imitating them will not achieve cooperation due to potential conflicts. To thisend, we propose an approach that selectively utilizes personalized expert demonstrationsas guidance and allows agents to learn to cooperate, namely personalized expert-guidedMARL (PegMARL). This algorithm utilizes two discriminators: the first provides incentivesbased on the alignment of individual agent behavior with demonstrations, and the secondregulates incentives based on whether the behaviors lead to the desired outcome. We evaluatePegMARL using personalized demonstrations in both discrete and continuous environments.The experimental results demonstrate that PegMARL outperforms state-of-the-art MARLalgorithms in solving coordinated tasks, achieving strong performance even when providedwith suboptimal personalized demonstrations. We also showcase PegMARLs capability ofleveraging joint demonstrations in the StarCraft scenario and converging effectively evenwith demonstrations from non-co-trained policies.",
  "Introduction": "The use of expert demonstrations1 has been proven effective in accelerating learning in single-agent reinforce-ment learning, as evidenced by studies such as Kang et al. (2018); Chen & Xu (2022); Rengarajan et al. (2022).This approach has since been extended to Multi-Agent Reinforcement Learning (MARL) (Lee & Lee, 2019;Qiu et al., 2022), which typically assumes the availability of high-quality collaborative joint demonstrations.However, from a practical standpoint, collecting joint demonstrations can be labor-intensive, demanding oneuser per agent in cooperative scenarios. Furthermore, these demonstrations are not scalable. If we change thenumber of agents or introduce new types of agents, we will need to gather a new set of demonstrations tolearn from. In contrast, it is much easier to obtain demonstrations for individual agents, or even better, for each type ofagents in a heterogeneous setting. We thus ask the following research question: could we leverage individual-wise task demonstrations instead?In this work, we refer to such expert demonstrations that addresssingle-agent behaviors for personal objectives as personalized demonstrations (see ). Since thepersonalized demonstrations will not necessarily reflect how the agents can collaborate and may even conflictwith each other in the joint setting, naively mimicking the demonstrations will not achieve cooperation.Therefore, purely imitation learning-based approaches would not be effective. We need an approach that",
  "selectively utilizes suitable personalized expert demonstrations as guidance and allows agents to learn tocooperate via collecting reward signals from the environments": "OursutilizesPersonalizedSingleAgentDemonstrations forJointPolicyLearningExistingMethodsutilizeJointDemonstrations forJointPolicyLearning DemonstrationsneedtoberecollectediftheagentconfigurationschangesAgentsofthesametypecanutilizethesamesetofdemonstrations Toprow:demonstrations;Bottomrow:learningtasksAgentsofdifferenttypesGoals : Joint demonstrations are costly to collect but offer richinformation on collaborative behaviors. Personalized demonstrationsare easier to collect, but solely focus on individual agent goals, so theylack cooperative elements. To this end, we present our algo-rithm, Personalized Expert-GuidedMARL (PegMARL), which carriesout personalized occupancy matchingas a form of guidance through reward-shaping. We implement this via twodiscriminators. The first, a personal-ized behavior discriminator, evaluateslocal state-action pairs, providing pos-itive incentives for actions that alignwith the demonstration and negativeincentives for divergent ones.Thesecond, a personalized transition dis-criminator, assesses whether a localstate-action pair induces a desiredchange in dynamics similar to thatobserved in the demonstration, ad-justing the incentive weight accord-ingly. We demonstrate the effective-ness of PegMARL on both discretegridworld and continuous multi-agent particle environments (Lowe et al., 2017; Mordatch & Abbeel, 2017).The main contributions of this paper are summarized as follows: (1) We propose PegMARL, the first approach that enables utilizing personalized demonstrations forpolicy learning in heterogeneous MARL environments, which (i) avoids demonstration recollection regardlessof the number and type of agents involved, and (ii) is compatible with most MARL policy gradient methods. (2) We demonstrate PegMARLs effectiveness with personalized demonstrations in both discrete andcontinuous environments. Our algorithm outperforms state-of-the-art decentralized MARL algorithms, puremulti-agent imitation learning, and reward-shaping techniques in terms of scalability and convergence speed,and achieves robust performance even when provided with suboptimal demonstrations. (3) We showcase PegMARLs capability to also leverage joint demonstrations, regardless of whether theyare sampled from co-trained or non-co-trained policies2. Experimental results on the StarCraft environment(Samvelyan et al., 2019) demonstrate that PegMARL converges effectively even with demonstrations fromnon-co-trained policies, which include potentially conflicting behaviors.",
  "Related Works": "Imitation Learning (IL). IL methods seek to replicate an expert policy from demonstration data generatedby that policy. Behavior Cloning (BC) is a commonly employed IL technique (Pomerleau, 1991; Bojarski et al.,2016), where the expert policy is estimated through supervised learning on demonstration data. However, BCis susceptible to compounding errors resulting from distribution shifts (Ross et al., 2011). Another threadof IL research is Inverse Reinforcement Learning (IRL) (Ng et al., 2000; Ziebart et al., 2008), in which theunderlying reward function is estimated from the demonstration data and then used for policy learning.To alleviate the computational overhead of IRL, Generative Adversarial Imitation Learning (GAIL) (Ho &Ermon, 2016) was introduced, allowing direct policy learning from observed data without intermediate IRLsteps. Song et al. (2018) extended this approach to introduce Multi-Agent GAIL (MAGAIL), which adaptsGAIL to high-dimensional environments featuring multiple cooperative or competing agents. In general,IL approaches can rarely perform better than demonstrations. Therefore, they are not directly suitable for 2Co-trained policies refers to policies that have been trained together in the same environment with shared experiences. Werefer the readers to for an illustration and .2 of Wang et al. (2023) for more information.",
  "Redagentnavigatestothegoalafterthegreenagentstepsaside": "fortheredagent: Navigatingtothekeylocation(thedoorisopen) forthegreenagent: Navigatingtothegreensquaretoopenthedoor(notethedoorshuts ifthegreenagentmovesaway) MAPPOfailedtolearnmeaningfulpolicies Objective:Thegreenagentopensthedoor,allowingtheredagenttoretrievethekey. duetothesparseness oftheenvironmentalrewardsignals : An example of utilizing personalized demonstrations to learn cooperative multi-agent policies.To learn successful cooperation, the agents are required not only to imitate the demonstrations to achievepersonal goals but also to learn how to avoid conflicts and collaborate. We visualize the state visitationfrequency of the personalized demonstrations and the joint policies learned by our algorithm and MAPPO,where a darker color means a higher value. We observe that the demonstrations guide the agents in exploringthe state space more efficiently than in MAPPO.",
  "scenarios where only personalized expert demonstrations that do not demonstrate how to collaborate areavailable": "Learning from Demonstration (LfD). In contrast to IL methods, LfD aims to leverage demonstrationdata to facilitate learning rather than simply mimicking expert behavior. Existing LfD works incorporatedemonstration data into a replay buffer with a prioritized replay mechanism to accelerate the learning process.Due to the off-policy nature of the demonstration data, most methods are value-based (Hester et al., 2018;Vecerik et al., 2017). There has been some recent work where the demonstrations are used to aid exploration,especially in environments with large state-action spaces (Kang et al., 2018; Chen & Xu, 2022; Rengarajanet al., 2022). For instance, POfD (Kang et al., 2018) learns an implicit reward from the demonstration datausing a discriminator and incorporates it into the original sparse reward. LOGO (Rengarajan et al., 2022)uses the demonstration data to directly guide the policy update: during each update iteration, the algorithmseeks a policy that closely resembles the behavior policy within a trust region. However, these approaches areprimarily focused on single-agent settings. In multi-agent settings, Qiu et al. (2022) suggest using demonstrations to pretrain agents through imitationlearning as a warm start, followed by optimization of the pretrained policies using standard MARL algorithms.Lee & Lee (2019) augment the experience buffer with demonstration trajectories and gradually decrease themixing of demonstration samples during training to prevent the learned policy from being overly influencedby demonstrations. Similar to POfD, DM2 (Wang et al., 2023) enables agents to enhance their task-specificrewards by training discriminators as well. Each agent matches toward a target distribution of concurrentlysampled trajectories from a joint expert policy to facilitate coordination. These approaches, however, requirejoint demonstrations of the same team configurations sampled from cohesively trained policies. While effective,this requirement can be cumbersome and limiting, especially in real-world scenarios where such coordinateddemonstrations are difficult to obtain (as mentioned in ). Our approach differs in that we leverage only personalized expert demonstrations to learn a cooperativepolicy. Notably, PegMARL can also utilize joint demonstrations when they are available, including thosefrom non-co-trained policies. By accommodating these diverse and potentially conflicting demonstrations,PegMARL can leverage a wider range of available data, offering greater flexibility and robustness acrossvarious multi-agent applications.",
  "Under review as submission to TMLR": "N local spaces Si as S = S1 S2 SN. By noting the local state of agent i as si Si, the global stateis s = (s1, s2, , sN). Similarly, we define the global action space A as A = A1 A2 AN, meaningthat for any a A, we may write a = (a1, a2, , aN) with ai Ai. The transition probability from states to s after taking a joint action a is denoted by P(s|s, a) = i Pi(si|s, a). All agents share a commonreward function r = R(s, a), and is the discount factor. In this work, we focus on decentralizedlearning and define the global policy as (a|s), where are the policy parameters. Specifically, we have = (1, 2, , N) as the factorized global policy parameters, and we can write (a|s) =",
  "i i(ai|s) usingpolicy factorization": "Personalized Tasks and MDPs. To introduce the notion of personalized demonstration, we need toextract the individual tasks of each agent from the collective tasks of multiple agents. For example, in , the green agents personalized task is to open the door and the red agents personalized task is to reachthe key without the others presence. We then define Personalized Markov Decision Processes (PerMDPs)(Si, Ai, Qi, ri, ) for each agent or, more generally, for each type of agents that share the same objectivewithin a heterogeneous team. Here, a type refers to behaviorally identical agents who share the same stateand action spaces, as well as the same reward function, using the formalism from Bettini et al. (2023), and weprovide the PerMDP definition for each agent for simplicity. We assume that the state space Si and actionspace Ai of the personalized task Ti are the same as the local state and action spaces of agent i from the jointtask. The transition probability from state si to si after taking a action ai is represented as Qi(si|si, ai).By following an arbitrary policy i(ai|si) for this personalized task, we can collect a set of personalizeddemonstrations Bi = {(sti, ati)}Ht=0, where H is the episode horizon. Wed like to emphasize that while weassume the personalized tasks and the joint task are conducted in the same environment map, the underlyingtransition dynamics are different. Additionally, the joint reward r may not necessarily equal the summationof rewards ri for each personalized task.",
  "MARL with Personalized Expert Demonstrations": "Now, we are ready to present the main problem we are interested in solving in this work. Assume each agenti is associated with a personalized task Ti. We collect one set of expert demonstrations for each agent ior, equivalently, each personalized task Ti. By letting an expert user perform each personalized task in therespective personalized MDP, we obtain a collection of expert demonstrations denoted by {BE1, BE2, ..., BEN }.We assume that the underlying expert policy associated with BEi is Ei(ai|si), and Ei is the occupancymeasure following the experts policy Ei for agent i. Note that while we establish the problem formulationand develop our algorithm in a fully observable setting, the experiments are conducted in both fully andpartially observable settings, encompassing both discrete and continuous environments (details in ).",
  "In standard Multi-Agent Reinforcement Learning, agents aim to discover optimal joint policies that maximizethe long-term return R() := , r = 1": "NNi=1i , r. Typically, the learning process commences withrandom exploration, which is often inefficient due to MARLs exponentially growing exploration spaces,especially when rewards are sparse. Beginning with the intuition of leveraging personalized demonstrations asguidance for how each agent should accomplish their personalized tasks to promote more effective exploration,akin to Kang et al. (2018) for single-agent cases, we can define the objective for learning from personalizeddemonstrations in multi-agent settings as follows:",
  "agent Env": ": A motivating example illustratingthe imitation of personalized demonstrationsin a multi-agent environment. The primarytechnical challenge lies in the discrepancybetween the transition dynamics in the per-sonalized MDP and the local transition dy-namics for each agent in the multi-agentenvironment. where is a weighting term balancing the long-term rewardand the personalized policy similarity. The Jensen-Shannon(JS) Divergence terms enable individual agents to align theiractions with their respective personalized demonstrations andfacilitate the achievement of their specific objectives. However, only imitating the personalized demonstration maynot always yield favorable outcomes and can even impede thelearning process. Previous works in MARL have predominantlyutilized joint demonstrations as guidance. As demonstrated byDM2 (Wang et al. (2023)), trajectories must be sampled from aco-trained joint expert policy for their joint action-matching ob-jective to converge. The crucial aspect that jointly coordinateddemonstrations offer, which personalized demonstrations lack,is compatibility. To illustrate, consider the example depictedin , where two agents aim to swap positions withoutentering the danger region. Two sets of optimal joint policiesexist for this task: agent a takes path 1 and agent b takes path2, or vice versa. When sampling joint demonstrations from aset of co-trained policies, the agents behaviors will be naturallycompatible. In contrast, if personalized demonstrations wereprovided, agents could indifferently choose between both pathsto reach their goals, potentially resulting in conflicting strategiesthrough naive imitation.",
  "exploration efficiency while circumventing the imitation of conflicting behaviors that might hinder joint-policytraining?": "Consider the scenario depicted in , where the blue agent seeks to replicate its personalized demon-stration within a multi-agent environment. The blue agent can successfully transition to the neighboringgrid on the right if its unoccupied; otherwise, the transition fails. Viewing the blue agent as the focal agentand the green agent as part of the environment, successful imitation hinges on the agents local transitionmatching those of the environment where the personalized demonstrations originated. We define P(si|si, ai)as an approximation for the i-th agents local transition probability in the multi-agent environment, which isgoverned by the true transition dynamics P(s|s, a) and the policy . With this insight, we further refinethe objective function as follows:",
  "domi= domEi = {(si, ai) Si Ai|DJSPi(si|si, ai) || Qi(si|si, ai) }.(6)": "where Q(si|si, ai) represents the true transition dynamics of the personalized MDP from which demonstrationsare collected, and is a threshold parameter that theoretically controls transition mismatch tolerance. Byadjusting the learning objective in this manner, the occupancy matching only happens on those localstate-action pairs that guide us toward the desired next local state under the current policy.",
  "E[log(1 Di(si, ai))] + EEi[log(Di(si, ai))],(7)": "where D(si, ai) : Si Ai (0, 1) is a discriminative classifier to discern if the (si, ai) pair is from thedemonstration or the current policy. However, this doesnt account for the constraint in equation 6. Therefore,by introducing an indicator function 1(si, ai) for whether (si, ai) belongs to the domain of iand Ei , wecan obtain",
  "E1(si, ai) log(1 Di(si, ai))+ EEi1(si, ai) log(Di(si, ai)).(8)": "Since direct access to the transition distributions is unavailable, verifying whether each (si, ai) pair is withinthe domain is not directly feasible. To address this, we further approximate the indicator function usinganother discriminative classifier Di(si, ai, si) : Si Ai Si (0, 1), estimating the likelihood of a (si, ai, si)tuple being from the demonstrations. In essence, it quantifies the likelihood that the corresponding (si, ai)induces a desired change in local state transition as observed in the demonstrations, effectively serving as alearned proxy for the -constrained domain in Equation 6. After approximating the inner JS divergence term in equation 5 with two discriminators, we integrate thediscriminative rewards derived from these discriminators into the environmental reward for the outer problem.Specifically, we estimate the reshaped reward ri as ri = r D i(si, ai, si) log(1 Di(si, ai)), where i andi are parameters for the two discriminators. The personalized behavior discriminator Di evaluates localstate-action pairs, providing positive incentives for actions that align with the demonstration and negativeincentives for divergent ones, while the personalized transition discriminator D i assesses if a local state-actionpair induces a desired transition in local state akin to that observed in the demonstration, adjusting the",
  "incentive weight accordingly. Subsequently, policy optimization is conducted by maximizing the long-termreturn with the reshaped reward": "Our PegMARL algorithm, detailed in Algorithm 1, is compatible with any policy gradient methods, withMAPPO (Yu et al. (2021)) adopted in our implementation. While presented in a fully observable setting,PegMARL can be adapted for partially observable scenarios. In such cases, we process observations fromjoint environment rollouts by removing dimensions not observable in the Personalized MDP for discriminatorsinputs. Notably, PegMARL can also utilize joint demonstrations; in this case, the observation processingstep is unnecessary as both demonstrations and environment rollouts are derived from the same joint MDP.Compared to DM2 (Wang et al., 2023), which also uses a discriminator-based approach for multi-agentlearning from demonstrations, PegMARL introduces the personalized transition discriminator D i as a keyinnovation. This additional discriminator helps address the challenge illustrated in by evaluatingwhether local state-action pairs induce desired transitions similar to those in the demonstrations, effectivelyfiltering out less desirable actions while retaining beneficial ones. This component enables PegMARL tohandle both personalized demonstrations and joint demonstrations from various sources, including thosesampled from non-co-trained policies, while DM2 requires demonstrations from co-trained policies to achieveconvergence.",
  "Experiments": "In this section, we empirically evaluate the performance of PegMARL, focusing on the following questions:(1) How does PegMARL, which leverages personalized demonstrations, compare to state-of-the-art MARLtechniques? (2) How does PegMARL scale with an increasing number of agents and in the case of continuousstate-action spaces? (3) How does the sub-optimality of personalized expert demonstrations affect theperformance of PegMARL? and (4) How does PegMARL perform when trained with joint demonstrationssampled from co-trained or non-co-trained expert policies?",
  "Main Results with Personalized Demonstrations": "We evaluate PegMARL with personalized demonstrations on discrete gridworld environments ()and a continuous multi-agent particle environment (MPE, a). The gridworld environment is fullyobservable with discrete state and action spaces, while the MPE is partially observable with a continuousstate space and discrete action space. Both environments feature sparse reward signals, which adds to thechallenge for standard MARL algorithms to learn effectively. Our comparison includes four strong baselines: MAPPO (Yu et al. (2021)), a leading decentralized MARLalgorithm; MAGAIL (Song et al. (2018)), a state-of-the-art multi-agent imitation learning algorithm; DM2 (Wang et al. (2023)), which combines distribution matching reward with environmental reward; and ATA(She et al. (2022)), one of the best multi-agent reward-shaping methods. Notably, MAGAIL and DM2 werenot originally designed for personalized demonstrations. Hence, we have adapted them for use in personalizeddemonstration settings through necessary modifications. Specifically, we processed the observations from jointenvironment rollouts by removing dimensions not observable in the Personalized MDP, similar to our approachin PegMARL. This adjustment ensures that the inputs to their respective discriminators are compatible withthe personalized demonstration data. In this context, MAGAIL serves as an ablation of PegMARL lackingthe environmental signal and transition discriminator, while DM2 represents an ablation lacking only thetransition discriminator. Details on algorithm implementation and hyperparameter choices can be found inAppendix C. For each scenario, we collect two sets of personalized demonstrations for each agent: optimal and suboptimal,by training single-agent RL in simplified environments that focus on each agents specific tasks. The averageepisodic rewards of suboptimal demonstrations are approximately half of their optimal counterparts (moredetails can be found in Appendix B). We execute each method in every environment with 10 distinct randominitializations and plot the mean and variance across all runs. Oracle denotes the best possible returnachievable with optimal policies. How does PegMARL scale with an increasing number of agents? The lava scenario (a)contains three variations, each involving different quantities of agents and escalating levels of complexity.The agents receive a positive reward only upon reaching the goal, incur penalties for collisions, and instantlydie and terminate the episodes if they step into the lava, making learning in this environment challenging. Aswe observe from , MAPPO struggles to develop meaningful behavior across all scenarios due to thesparse reward structure. MAGAIL performs worse, primarily due to the absence of environmental rewardsignals. The naive imitation of personalized agent demonstrations leads to frequent collisions among agents.While ATA can learn suboptimal policies in the 2-agent setting, its learning efficacy declines drastically as thenumber of agents increases. This underscores the challenges of training as the complexity of the multi-agentenvironment increases. In contrast, PegMARL demonstrates superior generalizability across scenarios with more agents, maintainingstable performance despite suboptimal demonstrations. As DM2 has been adapted to accommodate person-",
  ": Learning curves of PegMARL versus other baseline methods under the door scenario. PegMARLshows better robustness in terms of convergence and generalizability": "alized demonstrations, it closely resembles PegMARL but lacks the personalized transition discriminatorcomponent. As the number of agents increases, we would expect the space of possible interactions amongagents to expand, and therefore for the personal demonstrations to be potentially misleading. Consequently,DM2 experiences a decrease in convergence speed as the number of agents increases. This highlights theimportance of the personalized transition discriminator in PegMARL, enabling effective handling of inter-agentinteractions in personalized demonstration scenarios, thereby ensuring its efficacy across varying agent counts. How does PegMARL perform under the heterogeneous setting? The door scenario (b)contains two variants with varying levels of difficulty. The easy case is fairly straightforward: success shouldbe achievable by adhering to personalized demonstrations, which illustrate how each agent navigates to theirrespective goal locations. As shown in , most algorithms, except for MAGAIL, showcase proficientperformance. Notably, PegMARL exhibits the swiftest convergence. We attribute MAGAILs failure in thiscase to its inability to direct the green agent to remain positioned at the green square a behavior notexplicitly demonstrated. This underscores the importance of environmental reward signals when integratingpersonalized demonstrations into multi-agent learning paradigms. The hard case necessitates a higher degreeof agent cooperation: once the red agent gains entry to the middle room, the green agent must move asidefrom the green square to enable the red agents passage into the right room. In this complex setting, onlyPegMARL and DM2 demonstrate commendable convergence. PegMARL, equipped with the personalizedtransition discriminator, maintains faster convergence compared to DM2. How does PegMARL perform in the continuous setting? We modified the cooperative navigationtask from the multi-agent particle environment (Lowe et al. (2017); Mordatch & Abbeel (2017)) to evaluatethe performance of our algorithm in a continuous environment (more details can be found in Appendix A.2).b demonstrates the learning curves of PegMARL versus MAPPO and DM2. This validates the abilityof PegMARL in continuous environments.",
  "Results with Joint Demonstrations": "While PegMARL is primarily designed to leverage personalized demonstrations, it can also be extended toutilize joint demonstrations. Unlike other MARL algorithms like DM2 (Wang et al. (2023)) that requirecompatible joint demonstrations sampled from co-trained policies to achieve convergence, PegMARL hasthe flexibility to utilize joint demonstrations even from non-co-trained policies, which potentially containconflicting behaviors. To demonstrate, we conduct comparative experiments within the StarCraft Multi-AgentChallenge (SMAC) environment (Samvelyan et al. (2019)). We adopted DM2s original implementation andparameters for accurate results replication. We employed the same two tasks that were used in DM2: 5mv6m,which involves 5 Marines (allies) against 6 Marines (enemies), and 3sv4z, which features 3 Stalkers (allies)versus 4 Zealots (enemies). The win rates of the demonstrations in both cases are approximately 30%. 0.00.20.40.60.81.0 Timesteps1e7 0.0 0.2 0.4 0.6 0.8 1.0",
  "Mean Battle Won Rate": "3s_vs_4z PegMARLPegMARL(diff) DM2DM2(diff) IPPOdemo : Learning curves of PegMARL versus DM2 in two tasks under the SMAC scenarios. The suffixdiff in the legend indicates that the joint demonstrations used are sampled from non-co-trained policies.Otherwise, the demonstrations are sampled from co-trained policies. How does PegMARL scale with joint demonstrations? presents the learning curves ofPegMARL alongside the DM2 baseline in both the 5mv6m and 3sv4z configurations within the SMACenvironment. When the joint demonstrations are sampled from co-trained policies, PegMARL achieves acomparable, and in some cases even higher, success rate compared to DM2 in both tasks. When the joint demonstrations are sampled from policies that are non-co-trained, the success rate ofDM2 exhibits a significant decline in both tasks. In the 5mv6m task, DM2s success rate drops to nearly0, indicating a failure to learn. In contrast, PegMARL maintains a similar level of performance comparedto results from co-trained demonstrations, with only a slight decrease in convergence speed. These findingsunderscore the versatility and effectiveness of PegMARL in scenarios requiring collaboration among diverse",
  "Conclusions": "In this work, we introduce PegMARL, a novel approach for Multi-Agent Reinforcement Learning, which adoptspersonalized expert demonstrations as guidance and allows agents to learn to cooperate. Specifically, thealgorithm utilizes two discriminators to dynamically reshape the reward function: one provides incentives toencourage the alignment between the policy behavior with provided demonstrations, and the other regulatesincentives based on whether the behavior leads to the desired objective. We demonstrate PegMARLseffectiveness, scalability, and robustness in both discrete and continuous environments, where it outperformsstate-of-the-art decentralized MARL algorithms, pure imitation learning, and reward-shaping techniques. Weobserve that PegMARL can achieve near-optimal policies even with suboptimal demonstrations. Furthermore,We showcase PegMARLs capability to leverage joint demonstrations and converge successfully, regardless ofwhether they are sampled from co-trained or non-co-trained policies. Applicability and Limitations: PegMARL is most applicable when: a) The task allows for some degreeof independent decision-making, allowing individual agent behaviors to be isolated and demonstrated,even if only partially. b) The environment provides feedback on cooperative behaviors, enabling agentsto learn coordination beyond individual demonstrations. For example, in the door scenario, agents gainexploration guidance for door operation and navigation from personalized demonstrations. At the same time,environmental feedback on joint success drives them to learn coordination about timing and positioning(e.g., the green agent learning to make way for the red agent). Similarly, in the lava scenario, personalizeddemonstrations provide guidance for lava-avoiding paths. Still, environmental penalties for collisions andrewards for collective goal achievement encourage the emergence of cooperative navigation strategies. However, PegMARL may face challenges in tasks requiring continuous, seamless collaboration among agents,such as cooperative object lifting and relocation by multiple agents. In these scenarios, where constant,integrated teamwork is crucial, the reliance on personalized demonstrations may not provide sufficient guidancefor complex, ongoing coordination. Despite this, PegMARLs capability to utilize joint demonstrations fromdiverse sources, including non-co-trained policies, suggests the potential for addressing this limitation. Itwould be interesting future work to explore PegMARLs capabilities to handle more intricate and continuouslycollaborative tasks. Another limitation of our work is the absence of formal convergence guarantees. While DM2 establishesconvergence for individual agents optimizing distribution matching objectives, our approach introduces addi-tional complexity through the transition discriminators dynamic adjustment of demonstration influence. Theinterplay between the dual discriminators and multi-agent dynamics poses challenges for theoretical analysis,particularly since the joint distribution cannot be directly recovered from individual agent demonstrations inour setting. While our extensive empirical results demonstrate robust convergence across diverse scenarios,establishing theoretical convergence guarantees remains an interesting direction for future work.",
  "Yunbo Qiu, Yuzhu Zhan, Yue Jin, Jian Wang, and Xudong Zhang. Sample-efficient multi-agent reinforcementlearning with demonstrations for flocking control. arXiv preprint arXiv:2209.08351, 2022": "Desik Rengarajan, Gargi Vaidya, Akshay Sarvesh, Dileep Kalathil, and Srinivas Shakkottai. Reinforcementlearning with sparse rewards using guidance from offline demonstration. In International Conference onLearning Representations, 2022. URL Stphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structuredprediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificialintelligence and statistics, pp. 627635. JMLR Workshop and Conference Proceedings, 2011. Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas Nardelli, Tim GJRudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agentchallenge. arXiv preprint arXiv:1902.04043, 2019.",
  "A.1Discrete Gridworld Environments": "We use two gridworld environments with discrete state and action space: the lava scenario and the doorscenario (). The agents in the lava scenario are homogeneous because they have the same objective:navigating to their corresponding goal location. The door scenario has heterogeneous agents: the assistantagent (green) must open the door while the other agent (red) must reach the goal. The lava scenario: This environment has a 6-by-6 lava pond in the center (a). We provide threevariations of this scenario, each with varying numbers of agents and increasing levels of complexity. Themain goal for the agents is to efficiently navigate to their assigned goal locations while avoiding steppinginto the lava. An episode terminates when all agents reach their respective goals (succeed), or if any agentsstep into the lava or the maximum episode length is reached (fail). The door scenario: This environment is adapted from Franzmeyer et al. (2022) (see b). In thisscenario, the green agent must navigate to a designated green square and maintain its presence there tosustain the open state of the green door, thereby enabling the entry of a red agent into the right side room.An episode ends when the red agent reaches the red goal location (succeed) or the maximum episodelength is reached (fail). Each agents local state space is its {x, y} coordinates in the map. We concatenate all the local states togetherto form the global state and assume all agents have access to the global state, which has a dimension of Rn2 (n is the agent number). The local action space includes five actions: left, right, up, down, and stay. A sparsereward is granted when an episode succeeds, while a small penalty will be subtracted according to the stepstaken (10 step_count/max_step). Agents will receive a penalty of 1 if they collide with each other.",
  "A.2Cooperative Navigation": "We modified the cooperative navigation task of multi-agent particle environment Lowe et al. (2017); Mordatch& Abbeel (2017) to evaluate the performance of our algorithm in a continuous environment. The modifiedenvironment consists of 2 agents, 2 goal landmarks and 1 wall between the agents and the goals. The agentsneed to navigate around the wall to occupy both landmarks. Each agent has a partial observation of theenvironment, including its position and velocity as well as the relative positions of other agents and landmarks.The action space is discrete, consisting of moving left, right, up, down, and staying in place. We sparsifytheir original reward as follows:",
  "A.3The SMAC Environment": "The StarCraft Multi-Agent Challenge (SMAC) (Samvelyan et al. (2019)) is an environment specificallydesigned for evaluating multi-agent reinforcement learning (MARL) algorithms. Built on the StarCraft IIgame, SMAC presents a variety of challenging scenarios where multiple agents must collaborate to controlindividual units in combat. Each agent has partial observation of its surroundings within a certain sightrange. The action space is discrete, including move[direction], attack[enemy_id], stop, and no-op. A globalreward is given at each timestep based on the total damage dealt to the enemy units, with an additionalbonus of 10 or 200 awarded for killing each enemy unit or for winning a combat respectively. We consider thesame two tasks as used in DM2 (Wang et al. (2023)):",
  "B.1Personalized Demonstrations": "We let each individual agent perform their designated task in the same environment map without the otheragents presence to collect personalized demonstrations. This can be implemented through two main methods:a) recording expert human demonstrations for each agent type, and b) training single-agent RL in simplifiedenvironments that focus on their specific tasks. In our experiments, we adopted the latter option. It takesabout 2 hours to train a single-agent PPO for each personalized task. Discrete Gridworld Environments: shows an example of personalized demonstrations for thelava scenario, and visualizes the personalized demonstrations for the door scenario. We summarizethe details of the suboptimal demonstrations for the gridworld environment in , where the averageepisodic rewards are approximately 4.5, about half of their optimal counterparts.",
  ": The details of suboptimal demonstrations for the gridworldenvironment": "We provide visual representations of the policy and state occupancy measure corresponding to suboptimaldemonstrations in for the lava scenarios and in for the door scenario. The red squaresymbolizes the agents initial position in these visualizations, while the green square designates its respectivegoal location. Arrows within the figures denote available actions at each state, with arrow length indicatingthe probability associated with each action.",
  ": An illustration of person-alized demonstrations for the coopera-tive navigation task. There is only oneagent in the environment. The agentsobjective is to reach one of the twogoal locations": "Cooperative Navigation: For the cooperative navigation task fromthe multi-agent particle environment (Lowe et al. (2017); Mordatch& Abbeel (2017)), the collaborative goal is for the two agents to coverboth goals, regardless of which agent covers which goal. Therefore,we design the personalized task such that a single agent covers eitherof the two goals without the other agents presence. An illustrationof personalized demonstration for this task is shown in .",
  "B.2Joint Demonstrations": "The SMAC Environment: Regarding the joint demonstrationsutilized in the SMAC environment (Samvelyan et al. (2019)), weadopted the demonstrations provided by the authors of DM2 (Wanget al. (2023)). For joint demonstrations sampled from co-trainedpolicies, they are derived from jointly trained expert policies thatachieve approximately a 30% win rate. For joint demonstrationssampled from non-co-trained policies, they are obtained from expertpolicies that were trained independently in separate teams but executed together in the same environment.",
  "C.1Training with Personalized Demonstrations": "The algorithms are implemented based on MAPPO (Yu et al. (2021)), with each agent having separatepolicy and critic networks, discriminators, and optimizers. Both the policy and critic networks are two-layerMLPs with a hidden dimension of 64 and Tanh activation. The discriminators, including the personalizedbehavior discriminator and the personalized transition discriminator, are three-layer MLPs with a hiddendimension of 64 and Tanh activation. The personalized behavior discriminator takes (si, ai) as input, whilethe personalized transition discriminator takes (si, ai, si) as input. In partially observable settings, lets assume that the agents observation in the multi-agent MDP is denotedas oi Oi, while in the PerMDP, it is denoted as oi Oi . Since the PerMDP involves a single agent and themulti-agent MDP involves multiple agents, the observation space for each agent cannot be the same in thesetwo settings. For experiments on cooperative navigation, we use a heuristic function to convert oi by removingthe dimensions that is not within the observation space Oi before passing them to the discriminators. All baseline algorithms, except ATA, use the same codebase we implemented. MAGAIL is trained withthe personalized behavior discriminator but does not use the environmental reward. DM2 uses both theenvironmental reward and the personalized behavior discriminator. PegMARL utilizes the environmentalreward, the personalized behavior discriminator, and the personalized transition discriminator. ATAs originalimplementation is adopted for comparison. We summarized the common hyperparameters used for PegMARL,DM2 and MAGAIL in . The gail rew coef in DM2 is set to be 0.02 for all the gridworld scenarios and0.1 for the cooperative navigation task. The coefficient in PegMARL is 0.05 for all the gridworld scenariosand 0.2 for the cooperative navigation task."
}