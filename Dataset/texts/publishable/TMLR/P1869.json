{
  "Abstract": "Recent work on permutation-based model merging has shown impressive low- or zero-barrier modeconnectivity between models from completely different initializations. However, this line of workhas not yet extended to the Transformer architecture, despite its dominant popularity in the languagedomain. Therefore, in this work, we investigate the extent to which separate Transformer minimalearn similar features, and propose a model merging technique to investigate the relationship betweenthese minima in the loss landscape. The specifics of the architecture, like its residual connections,multi-headed attention, and discrete, sequential input, require specific interventions in order tocompute model permutations that remain within the same functional equivalence class. In mergingthese models with our method, we consistently find lower loss barriers between minima comparedto model averaging, across models trained on a masked-language modeling task or fine-tuned on alanguage understanding benchmark. Our results show that the minima of these models are less sharpand isolated than previously understood, and provide a basis for future work on merging separatelytrained Transformer models.",
  "Introduction": "The geometry of loss landscapes is the subject of extensive prior work attempting to understand the behavior andproperties of different minima (Kawaguchi, 2016; Li et al., 2018; Du et al., 2019). Prior work on understanding the losslandscapes of deep neural networks has found different types of geometric paths of low loss between converged models,demonstrating a degree of connectedness between these separately trained minima (Freeman & Bruna, 2017; Garipovet al., 2018; Tatro et al., 2020). These findings have restructured our understanding of the relationship between differentminima in loss space by uncovering entire low-loss regions in which several minima may be co-located. A related line of work finds specifically linear paths between differently initialized models, where the linear inter-polations of these minima have low loss like either endpoints (Entezari et al., 2022; Ainsworth et al., 2023). Thisconnectivity is feasible by transforming the weights of the models, without changing their function, in order to comparethem within a more similar loss space. The types of transformations exploit inherent model symmetries that areachievable via permuting model features, where the same underlying function may have numerous valid parameterconfigurations. So far, this work has mostly covered simple architectures like multi-layer perceptrons or VGGNets(Simonyan & Zisserman, 2015). Some work has included ResNets as well (He et al., 2016), and show that only widerResNets can be merged with much smaller loss than standard versions (Ainsworth et al., 2023; Stoica et al., 2024). While this research has led to important conclusions about loss landscapes between separately trained models, theTransformer architecture has been largely unexplored in terms of understanding its permutation symmetries and losslandscape geometry. Prior work has emphasized the importance of understanding loss landscape geometry, as a betterunderstanding of loss geometry can lead to improvements in optimization techniques, ensembling, and model mergingtechniques (Garipov et al., 2018; Ainsworth et al., 2023). There has been some prior work in merging Transformerarchitectures (Jin et al., 2023; Imfeld et al., 2023), but these either do not account for models from separate initializations,",
  "or do not create valid permutation mappings that transform a model into another member of its symmetric equivalencegroup (Chen et al., 1993)": "Therefore, in this work, we explore the extent to which separately trained Transformer models learn similar representa-tions, and then propose permutation-based merging method to align representations from these separate models. Withthis, we can study the extent of the connectivity between their minima in loss space in order to extend our understandingof loss landscape geometry to this important and popular architecture. We specifically investigate their connectivitythrough the lens of permutation-invariant linear mode connectivity (Entezari et al., 2022). Our permutation-basedmerging method builds upon prior work from Ainsworth et al. (2023), but focuses on necessary interventions neededfor the Transformer architecture, such as merging Multi-Headed Attention, and the inputs/outputs of each residualconnection.",
  ". We extend our approach to fine-tuned models and show consistently smaller loss barriers between modelscompared to vanilla merging": "In , we discuss related work to place our contributions in context of the existing literature. In , wedescribe our method for computing the permutations needed to align two Transformer minima in order to comparethem in loss space. We outline the models, data, and evaluation used to test our approach in . Finally, wediscuss our findings and analysis in .1",
  "Related Work": "Loss Landscape & Mode Connectivity.Deep neural networks are typically trained by optimizing a loss functionwith an SGD variant. Loss landscapes of these networks have been shown to contain infinitely many global minimizersthat are equally reachable via SGD (Kawaguchi, 2016; Du et al., 2019). Overparameterization is one of the reasonsbehind the abundance of minima leading to different functions that behave similarly on the training data (Nguyen et al.,2019; Simsek et al., 2021; Liu et al., 2022). Permutation and scaling invariances also lead to functionally identicalminima that differ in the weight space (Tatro et al., 2020; Entezari et al., 2022). Prior work has established that the optima of loss functions are in fact connected by simple curves over which trainingand test accuracy are nearly constant (no loss barrier) (Freeman & Bruna, 2017; Garipov et al., 2018; Draxler et al.,2018). This phenomenon is referred to as mode connectivity. Entezari et al. (2022) conjectured that if the permutationinvariances of neural networks are taken into account, these optima are linearly mode connected, i.e. the linear pathconnecting these two models has no loss barrier. This is linear mode connectivity. Interpolating ModelsEmpirically, linear interpolation between neural network weights has become an importanttool. In the context of fine-tuning the same large pre-trained model, averaging models enabled state-of the art accuracyon ImageNet (Wortsman et al., 2022). Wortsman et al. (2022); Rame et al. (2022) established that if fine-tuned modelslie in a single low error basin, then weight averaging performs similarly to ensembling. It is however not guaranteedthat finetuned models (starting from the same initialization) will reside in the same loss basin. Prior work on linear interpolation-based model merging has focused on improving the algorithms used to bring thehidden units of two networks into alignment, in order to reduce the barrier to interpolation between them. Singh & Jaggi(2020) develop a strong optimal transport-based method which allows linear interpolation between a pair of ResNetnetworks (He et al., 2016). Entezari et al. (2022) use an approach based on simulated annealing (Zhan et al., 2016) inorder to find permutations such that wide MLPs trained on MNIST can be linearly interpolated with a barrier of nearlyzero. Ainsworth et al. (2023) develop several permutation-based algorithms for MLPs and ResNets, and demonstrate",
  "PMHA": ": Example of a Transformer layer with its parameters outlined in blue boxes, specific hidden states as circles,and the flow of operations indicated with arrows. | indicates the dot-product operation, and indicates addition.LN refers to LayerNorm modules. We include permutation and inverse permutation matrices at each weight matrixto indicate the proposed operations from our method. Pres refers to the residual permutation, PMHA refers to themulti-headed attention (MHA) permutation, and PFF aligns the feed-forward layers. zero-barrier linear mode connectivity on widened ResNets. Stoica et al. (2024) extend this work and allow for featuremerges to happen within each model as well, removing internal feature redundancies and improving model mergingoutcomes.",
  "Proposed Transformer Merging Method": "In this section, we describe the components of our method that address how to permute specific portions of a Transformermodel B in order to bring them into alignment with a separately trained model, A. Extending permutation-based modelmerging to Transformers is non-trivial as they have more complicated connections than simpler MLP-like architectures.We discuss how we find permutation matrices computed from feature correlations, and then we specifically address theparameters involved in multi-headed attention, residual connections, and pre- and post-net parameters. We diagrama Transformer layer in , and the proposed permutation operations that we describe in detail throughout thissection.",
  "Computing Correlation and Permutation Matrices": "Given two models trained on the same data but from from separate initializations, namely A and B, we computepost-activation features for each layer or sublayer parameter W in order to determine which features mightcorrespond between models (Ainsworth et al., 2023; Stoica et al., 2024). In our setting, features are computed at thetoken level, and all special non-vocabulary and non-padding tokens are always included (such as [SEP], [CLS]). At agiven layer or sublayer, we compute d-dimensional activations across n tokens from both models XA, XB Rnd,and then determine feature relatedness via cross-correlation computed as the following:",
  "i=1Ci,(i)(2)": "where : {1, 2, ..., d} {1, 2, ..., d} is a permutation mapping. This optimization problem finds the featurecorrespondences between the two models that lead to the highest total correlation captured. This problem an instanceof the assignment problem and can be solved using the Jonker-Volgenant algorithm (Crouse, 2016; Tatro et al., 2020;Ainsworth et al., 2023).",
  "Multi-Headed Attention": "In finding corresponding neurons in the Multi-Headed Attention (MHA) parameters, namely the key (WK), query(WQ), value (WV ), and linear layer (WO) weights, we propose several methods to compute potential permutationmatrices. For each of the key, query, and value weights, the full parameter W Rdmodeldmodel is logically partitioned into hattention heads each of output dimension dk = dmodel/h (Vaswani et al., 2017). In order to apply a permutation tothese full weight matrices and maintain the functional equivalence of the overall model, permutations must operateon each attention head separately, and not permute features between attention heads. This is because the final hiddenvector from MHA reflects a concatenation of the result from each head, which are computed separately with weightsWKi, WQi, WVi for head i.",
  "IgnoringHeads": ": Example permutation matrices resulting from different strategies for attention head alignment. Each Pireflects permutations for features within attention heads.Since our models are trained from distinct initializations, the correspondence of their attention heads may differ inaddition to the correspondence of features within each head. This distinction is diagrammed in the first two permutationsof . We collect features from just after the attention computation and before the linear layer following attention.These features are the aforementioned concatenations. We first compute C = corr(XA, XB) and then partition thiscorrelation matrix by heads into dk dk correlation matrices, for each potential attention head pair. Let Cjk be theblock of the correlation matrix corresponding to head pair (j, k). We then compute the optimal permutation for eachunique head pair, and store its head-internal permutation and cost from the following:",
  "in Algorithm 1. We note that LinearSumAssignment refers to the specific assignment problem we encounter in ourmethod": "The resulting permutation matrix PMHA applies as following: P TMHA can apply to WO as described previously, butwe apply PMHA to each of WV , WK, and WQ. We note that the WQix, WKix dot-product attention operationmultiplies away the head dimension dk, meaning we do not necessarily have to use the same permutation for WV and{WQ, WK}. However, we still have to use the same outer permutation for all three matrices, as the attention weightsresulting from WQix, WKix are still head-specific. In experimentation, we do not find a notable difference betweenusing a separate permutation for {W Q, W K}2, and using the same permutation for WQ, WK and WV . Therefore,we consider only the latter case for simplicity. We show an example of an output from our proposed algorithm in , as well as some alternative approachesto permuting MHA weights. The first permutation matrix shows the resulting block-diagonal structure of assumingthat heads are aligned between different minima. The second matrix shows an example from our method, and the thirdmatrix disregards head structure and allows permuting features across heads. We note that ignoring heads will not leadto a valid permutation where f(x; ) = f(x; ()), but we still include it for experimental comparisons.",
  "Residual Connections": "Each Transformer layer, assuming no cross-attention, contains two residual connections. This means all layers, from theembedding layer to the output layer, is linked via consecutive residual connections. This diverges even from ResNets,which generally contain skip-connections every 2 or 3 layers (He et al., 2016). We diagram the residual connections of a Transformer layer and their relationships to model parameters in . Thefirst connection skips past the multi-headed attention sublayer, and the second connection skips past the feed-forwardsublayers, as diagrammed. The connections can be formulated as the following, where LN refers to LayerNorm:",
  "xrf = LN(W2ReLU(W1xra) + xra).(7)": "As seen in the equations, the input and output of both sublayers are added to create a new output, which implies that if apermutation operation is applied to the output state, the permutation needs to be the same for both addends. We note that the addends are normalized via the LayerNorm module, and any permutation to the output would need topermute the features of the LayerNorm module as well. We apply the permutation to the weights of LN. Because LN isnot a full weight matrix, and maintains the same feature ordering as its input, we must apply the permutation to theaddends of the residual connections as well (Stoica et al., 2024).",
  "= P W2ReLU(W1xra) + P xra= P W2ReLU(W1xra) + P (WOMHA(x) + x).(8)": "We see that due to the residual structure, any permutation applied to second feed-forward weight parameter, W2, mustalso be applied to MHA, or more specifically the WO matrix. To unpermute these features, we apply P T to where thepermuted xra and xrf states become inputs, which are W1 and {WQ, WV , WK}, respectively. Because the input to each layer must be permuted P x, and the output of each layer is also permuted P xrf, wecan see that the entire Transformer architecture uses the same {P , P T} matrices for all the weights involved inresidual connections. This is unlike our other proposed permutations for multi-headed attention which are specificto each Transformer layer. Requiring the same permutation throughout the model reduces the degrees of freedomavailable in finding an optimal permutation, which is an important result demonstrating a potential difficulty of aligningTransformers. At the ends of the models, namely the embedding layer(s) and output layer(s), we also apply these transformations, asthe input to the first Transformer block and the output of the last Transformer block are permuted. We apply this P tothe embedding weights, including positional and any special token embeddings, and at the final layer, we apply P T tothe weight matrix immediately following the last Transformer block LayerNorm. This is usually a pooling or denselayer, depending on the model task. Because of the multiple potential features that could contribute to the computation of the residual permutations, namelyboth xra and xrf across all layers, we consider several strategies for learning these mappings. We test 4 approaches:First refers to only obtaining features from the features immediately following the embedding layer(s). Last refers toonly obtaining features from just after the final Transformer layers LayerNorm. All refers to concatenating all featuresxrf, and xra from all Transformer layers, and separate refers to computing 2 {P , P T} pairs per Transformer layer, onefor xrf, and the other for xra. The separate approach also does not lead to a valid permutation like Ignore Heads in.2, but we also include it for experimental comparisons.",
  "Feed-Forward and Output Layers": "Unlike the residual stream or Multi-Headed attention, Feed-Forward sub-layers require no special attention in order topermute them into a new space. We simply compute correlations from the features after the computation of the firstFeed-Forward layer (W1), and compute P , P T separately for each Transformer layer. We apply these permutations asdescribed in .1:",
  "W 2 W2P T(9)": "As many models have task-specific output layers like classification heads or masked-language modeling heads, there isalso another permutation able to be computed between a pooling/dense layer and the actual model head, which tends toalso be a linear layer. This permutation mapping would proceed as normal, like the feed-forward example, but we donot include it in our experimentation as we see no notable differences in its inclusion.",
  "Models": "We investigate Transformer encoder-based masked language models in this work. Specifically, we consider 5 differentBERT models, seeds 1 through 5, from the MultiBERTs reproductions (Devlin et al., 2019; Sellam et al., 2021). Eachof these models is a bert-base-uncased checkpoint, but trained with a different random initialization and randombatch ordering. These properties are the type of SGD-related variation we seek to study between different minima. All",
  "Tasks and Datasets": "We focus on two different tasks to test our method. We use the masked language modeling task to test our base method,as this is the main task that BERT and many other pretrained Transformer encoded models are trained with. We alsoconsider fine-tuning these models for classification tasks, and then comparing them in their fine-tuned state. We use theGeneral Language Understaning Evaluation (GLUE) benchmark (Wang et al., 2018) for our classification tasks, andexclude WNLI as in Devlin et al. (2019). GLUE is a set of 8 diverse natural language understanding classification tasks.We report scores on GLUE test sets from our reproductions in in Appendix A. For our experiments on masked language modeling, we use the validation set of the Wikitext-103 benchmark as ourevaluation data Merity et al. (2016)3. For computing model activations, we extract a random sample of just over 1million sentences of the Books corpus Zhu et al. (2015). For a majority of our experiments, we sample 100k sentencesand use this subset to compute features, unless stated otherwise. We take a diverse sample across different genres amongthe books available. We choose the Books corpus as it is part of the original pre-training data from BERT (Devlin et al.,2019). For GLUE experiments, we use the full training data for each of the tasks to compute features, and the full validationsets to compute losses. The amount of data available for each task varies, and statistics are also reported in inAppendix A.",
  "Evaluation": "To compute loss barriers, we compute several interpolations of A and B, as A + (1 )B. Specifically, we use21 samples evenly spaced between = 0 and = 1, inclusive. We use the definition of loss-barrier as Frankle &Carbin (2018)4, defined as the maximum difference between the loss of an interpolation and the average loss of the basemodels:maxL(A + (1 )B) 1",
  "By component": "We report results on the 10 MultiBERTs merges after merging different sets of Transformer components described in. We show pseudo-perplexity results across the range of interpolations, displayed . We report vanillaaveraging with no permutations, merging all feed-forward sublayers, merging all multi-headed attention sublayers,and merging all feed-forward sublayers and all multi-headed attention sublayers. Aligning and merging either thefeed-forward sublayers or the attention sublayers clearly leads to a perplexity reduction over the baseline, and theircombination leads to a stark reduction, of almost 7 the original perplexity at = 0.5. We do not include permuting",
  "Published in Transactions on Machine Learning Research (11/2024)": "Kenji Kawaguchi.Deep learning without poor local minima.In D. Lee, M. Sugiyama, U. Luxburg,I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Cur-ran Associates, Inc., 2016.URL Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. Onlarge-batch training for deep learning: Generalization gap and sharp minima. In International Conference onLearning Representations, 2016.",
  "Multi-headed attention": ": Loss Barriers of merged MultiBERTs with feed-forward and attention components merged. Methodsdescribe which MHA algorithm was applied. Loss barriers are the largest difference between an interpolation and theaverage of the individual model losses. Maintaining head structure in the permutation while allowing different headcorrespondences between models is the most optimal permutation.",
  "Vanilla Attention Avg.4.310.21Monotonic Head Alignment4.130.20Ignore-Heads3.970.25Head-Perm3.710.23": ": Visualization of correlation matrices between features before and after permuting. These features are from theseventh multi-headed attention layer from 2 different MultiBERTs models. On the left, 12 attention head boundariesare clearly visible, and highly correlated regions do not necessarily correspond to the same attention head indices,supporting our two-stage permutation method. On the right, the two-stage permutation method outcome can be seen viathe dark diagonal line, and its surrounding block diagonal pattern. We report loss barriers for our Head-Permutation multi-headed attention approach as compared to some alternatives alsodescribed in .2 in . These results reflect permuting both attention parameters as well as feed-forwardparameters. We see that our proposed Head-Permutation approach for the attention sub-layer outperforms simpleattention averaging, as well as approaches ignoring the multi-headed structure of the weight parameters (Ignore-Heads),and not allowing for different head correspondences across different models (Monotonic). We also show an examplecorrelation matrix between the first multi-headed attention layer from 2 different MultiBERTs models in . Thecorrelation matrix shows clear attention head boundaries, as well as a scattered pattern that supports our proposedtechnique that does not assume any monotonically ordered head correspondence.",
  "Amount of Data": "We report loss barriers on the masked language modeling task while varying the amount of sentences used to computecorrelations. All previous experiments are reported on 100k sentences. We combine feed-forward and attentionlayers in this setting. Results are shown in . Values are fairly similar across data amounts, suggesting thatthere is no strong relationship between the amount of data used and the overall loss barrier. We do see, however, smallvariations between different data amounts, which are likely due to the content of the different sets of sentences usedto create the correlation matrices. We believe that more than the quantity of tokens, the type and quality of text usedfor computing correlations likely matters more. We leave further investigation into specific data sources for aligningtext-based Transformer models for future work. : Loss barriers on the masked language modeling task with different data amounts used for computing features.Standard error regions are shaded. The amount of data does not have a strong directional relationship with the size ofthe loss barrier in this setting.",
  "GLUE Results": "We investigate loss barriers between fine-tuned BERT models across 8 different GLUE tasks. We compare loss barriersfrom vanilla averaging to those obtained after applying our method to combine the models. Different from our result onmasked language modeling, we include residual permutations as we find it has a slight decrease in loss barriers over",
  "MNLI-mm0.610.030.720.08QQP1.370.091.200.11QNLI0.640.040.770.06SST-20.420.040.360.07CoLA1.310.141.110.13STS-B5.150.444.240.35MRPC2.740.081.930.11RTE0.530.040.410.05": "While we are able to observe lower loss barriers between minima, the trends of loss reduction across interpolations isinconsistent, especially compared to the masked language modeling setting. For example, while the maximum loss ofvanilla averaging is higher than the maximum loss of our approach, there are still other interpolation weights where theloss of our approach is instead higher than vanilla averaging. We also note an interesting pattern of lower loss thaneither parent model for several tasks around = 0.15 and again at = 0.85 . Additionally, the loss barrier curves ofsome vanilla merges between these fine-tuned models have different behavior than their pretrained alternatives, as seenin with the presence of M like loss curve shapes between minima. While our method can extend to thissetting to find a lower loss path between fine-tuned minima, it is unclear which kind of data is necessary to observe thelowest loss path. We leave further investigation into connecting fine-tuned models, and further understanding of theirpre-merging connectivity in loss space, for future work.",
  "Discussion and Conclusion": "By considering the set of functionally equivalent Transformers reachable using permutation mappings, we can con-sistently find linear paths between models with lower loss than the path obtained via vanilla interpolation. Thisconclusion about the connectedness between these models has implications on our understanding of the smoothnessof Transformer loss space and the sharpness of their minima; in comparing minima using our method, they are far lessisolated than previously understood. This understanding of the geometric properties of minima can have implicationsin how we design optimization methods, ensembles of models, and additional merging techniques. For example, itis widely contested whether sharp minima can generalize as well as flat minima across many deep learning modelsKeskar et al. (2016); Dinh et al. (2017). In our work, we show that it is necessary to consider permutation invariances ofTransformer models when characterizing the geometric properties of their minima. As we take only a first attempt at connecting separately trained Transformers along a lower loss path, there is muchroom for future work in understanding Transformer loss landscapes. A deeper understanding of the relationshipsbetween fine-tuned models, Transformer width and loss barriers, and the data needed to compute more informativecorrelations is needed in order to further characterize the relationship between Transformer minima.",
  "Broader Impact Statement": "While it is already difficult to meaningfully characterize the capabilities of a trained model, it is even more difficultto know what an interpolated models capabilities are, with respect to their parent models. Our work focuses on thelosses of these interpolated models rather of their performance in other aspects. Outside of loss, it is unclear how theperformance of these models changes when combined. Thorough testing of interpolated models should be investigatedbefore actual use.",
  "David F. Crouse. On implementing 2d rectangular assignment algorithms. IEEE Transactions on Aerospace andElectronic Systems, 52(4):16791696, 2016. doi: 10.1109/TAES.2016.140952": "Fahim Dalvi, Hassan Sajjad, Nadir Durrani, and Yonatan Belinkov. Analyzing redundancy in pretrained transformermodels. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP), pp. 49084926, Online, November 2020. Associationfor Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.398. URL Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectionaltransformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedingsof the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019.Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL",
  "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In InternationalConference on Learning Representations, 2016": "Quynh Nguyen, Mahesh Chandra Mukkamala, and Matthias Hein. On the loss landscape of a class of deep neuralnetworks with no bad local valleys.In International Conference on Learning Representations, 2019.URL Giovanni Puccetti, Anna Rogers, Aleksandr Drozd, and Felice DellOrletta. Outlier dimensions that disrupt transformersare driven by frequency. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Associationfor Computational Linguistics: EMNLP 2022, pp. 12861304, Abu Dhabi, United Arab Emirates, December2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.93. URL Alexandre Rame, Matthieu Kirchmeyer, Thibaud Rahier, Alain Rakotomamonjy, patrick gallinari, and MatthieuCord. Diverse weight averaging for out-of-distribution generalization. In Alice H. Oh, Alekh Agarwal, DanielleBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL Yuxin Ren, Qipeng Guo, Zhijing Jin, Shauli Ravfogel, Mrinmaya Sachan, Bernhard Schlkopf, and Ryan Cotterell. Allroads lead to rome? exploring the invariance of transformers representations. arXiv preprint arXiv:2305.14555,2023. Thibault Sellam, Steve Yadlowsky, Ian Tenney, Jason Wei, Naomi Saphra, Alexander DAmour, Tal Linzen, JasmijnBastings, Iulia Raluca Turc, Jacob Eisenstein, et al. The multiberts: Bert reproductions for robustness analysis. InInternational Conference on Learning Representations, 2021.",
  "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. InInternational Conference on Learning Representations, 2015": "Berfin Simsek, Franois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and JohanniBrea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. InMarina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,volume 139 of Proceedings of Machine Learning Research, pp. 97229732. PMLR, 1824 Jul 2021. URL Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. In H. Larochelle, M. Ranzato, R. Hadsell,M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2204522055. Curran Associates, Inc., 2020. URL",
  "Norman Tatro, Pin-Yu Chen, Payel Das, Igor Melnyk, Prasanna Sattigeri, and Rongjie Lai. Optimizing mode connectivityvia neuron alignment. Advances in Neural Information Processing Systems, 33, 2020": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and IlliaPolosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-taskbenchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupaa, and AfraAlishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP, pp. 353355, Brussels, Belgium, November 2018. Association for Computational Linguistics.doi: 10.18653/v1/W18-5446. URL Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos,Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiplefine-tuned models improves accuracy without increasing inference time. In International Conference on MachineLearning, pp. 2396523998. PMLR, 2022.",
  "AGLUE Reproduction": "We report the average values of our GLUE reproductions across the MultiBERTs models here. We train MNLI-mismatched, QQP, QNLI, SST-2, CoLA, STS-B, and RTE tasks for 3 epochs, and we train MRPC for 5 epochs. Wefollow all other hyperparameters of the reproduction implemented in Ren et al. (2023). : Results on GLUE for both the original BERT model Devlin et al. (2019), and our reproduction acrossMultiBERTs models 1-5. We report the average and the standard deviation across 5 models. Accuracy is reported forMNLI-mm, QNLI, SST-2, and RTE. F1 scores are reported for QQP and MNLI. Matthews correleation is reported forCoLA, and Spearman-r correlation is reported for STS-B."
}