{
  "Abstract": "In self-supervised learning (SSL), representations are learned via an auxiliary task withoutannotated labels. A common task is to classify augmentations or different modalities of thedata, which share semantic content (e.g. an object in an image) but differ in style (e.g. theobjects location). Many approaches to self-supervised learning have been proposed, e.g.SimCLR, CLIP, and DINO, which have recently gained much attention for their represen-tations achieving downstream performance comparable to supervised learning. However, atheoretical understanding of self-supervised methods eludes. Addressing this, we present agenerative latent variable model for self-supervised learning and show that several families ofdiscriminative SSL, including contrastive methods, induce a comparable distribution overrepresentations, providing a unifying theoretical framework for these methods. The proposedmodel also justifies connections drawn to mutual information and the use of a projectionhead. Learning representations by fitting the model generatively (termed SimVAE) improvesperformance over discriminative and other VAE-based methods on simple image benchmarksand significantly narrows the gap between generative and discriminative representation learn-ing in more complex settings. Importantly, as our analysis predicts, SimVAE outperformsself-supervised learning where style information is required, taking an important step towardunderstanding self-supervised methods and achieving task-agnostic representations.1",
  ": SSL Modelfor J semantically re-lated samples (termsexplained in 3)": "In self-supervised learning (SSL), a model is trained to perform an auxiliary taskwithout class labels and, in the process, learns representations of the data that areuseful in downstream tasks. Of the many approaches to SSL (e.g. see Ericsson et al.,2022), contrastive methods, such as InfoNCE (Oord et al., 2018), SimCLR (Chenet al., 2020a), DINO (Caron et al., 2021) and CLIP (Radford et al., 2021), have gainedattention for their representations achieving downstream performance approachingthat of supervised learning. These methods exploit semantically related observations,such as different parts (Mikolov et al., 2013; Oord et al., 2018), augmentations(Chen et al., 2020a; Misra & Maaten, 2020), or modalities/views (Baevski et al.,2020; Radford et al., 2021) of the data, considered to share latent semantic content(e.g. the object in a scene) and differ in style (e.g. the objects position). SSLmethods are observed to pull together representations of semantically relatedsamples relative to those chosen at random and there has been growing interestin formalising this to explain why SSL methods learn useful representations, buta mathematical mechanism to justify their performance remains unclear.",
  "Published in Transactions on Machine Learning Research (09/2024)": ": Ablation experiment across the number of augmentations considered during training of the SimVAEmodel using the MNIST (left) and CIFAR10 (right) datasets. Two, four, six, eight, and 10 augmentationswere considered. The average and standard deviation of the downstream classification accuracy using Linear,MLP probes, and a KNN & GMM estimators are reported across three seeds. Batch size of 128 for allreported methods and number of augmentations. Means and standard errors are reported for three runs. : Ablation experiment across the likelihood p(x|z) variance considered during training of the SimVAEmodel using the MNIST (left) and CIFAR10 (right) datasets. The average and standard deviation of thedownstream classification accuracy using Linear, MLP probes and a KNN & GMM estimators are reportedacross three seeds. Means and standard errors are reported for three runs.",
  "Background and Related Work": "Representation Learning aims to learn a mapping f : X Z (an encoder) from data xX to representationsz =f(x)Z (typically |Z|<|X|) that perform well on downstream tasks. Representation learning is not welldefined in that downstream tasks are arbitrary and good performance on one may not mean good performanceon another (Zhang et al., 2022). For instance, image representations are commonly evaluated by predictingsemantic class labels, but the downstream task could instead be to detect lighting, position or orientation,which representations useful for predicting class may not capture. This suggests that general-purpose ortask-agnostic representations should capture as much information about the data as possible, as supported byrecent works that evaluate on a range of downstream tasks (e.g. Balaevi et al., 2023). Self-Supervised Learning includes many approaches that can be categorised in several ways (e.g. Balestrieroet al., 2023; Garrido et al., 2022). The SSL methods that we focus on (predictive SSL) are defined below: Instance Discrimination (Dosovitskiy et al., 2014; Wu et al., 2018) treats each data point xi and any of itsaugmentations as samples of a distinct class labelled by the index i. A softmax classifier (encoder + softmaxlayer) is trained to predict the class label (i.e. index) and encoder outputs are taken as representations. Latent Clustering performs clustering on representations. Song et al. (2013); Xie et al. (2016); Yang et al.(2017) apply K-means, or similar, to the hidden layer of a standard auto-encoder. DeepCluster (Caron et al.,2020) iteratively clusters ResNet encoder outputs by K-means and uses cluster assignments as pseudo-labelsto train a classifier. DINO (Caron et al., 2021), a transformer-based model, can be interpreted similarly asclustering representations (Balestriero et al., 2023). Contrastive Learning encourages representations of semantically related data (positive samples) to be closerelative to those sampled at random (negative samples). Early SSL approaches include energy-based models(Chopra et al., 2005; Hadsell et al., 2006); and word2vec (Mikolov et al., 2013) that predicts co-occurringwords and encodes their pointwise mutual information (PMI) in its embeddings (Levy & Goldberg, 2014; Allen& Hospedales, 2019). InfoNCE (Oord et al., 2018; Sohn, 2016) extends word2vec to other data domains. Fora positive pair of semantically related samples (x, x+) and randomly selected negative samples X={xk }Kk=1,the InfoNCE objective is defined by:",
  ",(1)": "where sim(, ) is a similarity function, e.g. dot product. The InfoNCE objective (Eq. 1) is optimised ifsim(z, z) = PMI(x, x) + c, for some constant c (Oord et al., 2018). Many works build on InfoNCE, e.g.SimCLR (Chen et al., 2020a) uses synthetic augmentations and CLIP (Radford et al., 2021) uses differentmodalities as positive samples; DIM (Hjelm et al., 2019) takes other encoder parameters as representations;and MoCo (He et al., 2020), BYOL (Grill et al., 2020) and VicREG (Bardes et al., 2022) find alternativestrategies to negative sampling to prevent representations from collapsing. Due to their wide variety, we do not address all SSL algorithms, in particular those with regression-styleauxiliary tasks, e.g. reconstructing data from perturbed versions (He et al., 2022; Xie et al., 2022); orpredicting perturbations, e.g. rotation angle (Gidaris et al., 2018).",
  "z p(x|z)p(z). Latent variables z can be": "used as representations (4.1). A VAE (Kingma & Welling, 2014) maximises the ELBO, with p, q modelledas Gaussians parameterised by neural networks. A -VAE (Higgins et al., 2017) weights ELBO terms toincrease disentanglement of latent factors. A CR-VAE (Sinha & Dieng, 2021) considers semantically relatedsamples through an added regularisation term. Further VAE variants are summarised in Appendix A.1.",
  "Variational Classification (VC).Dhuliawala et al. (2023) define a latent variable model for classifyinglabels y, p(y|x)=": "z q(z|x)p(y|z), that generalises softmax neural network classifiers. The encoder is interpretedas parameterising q(z|x); and the softmax layer encodes p(y|z) by Bayes rule (VC-A). For continuous datadomains X, e.g. images, q(z|x) of a softmax classifier (parametersised by the encoder) is shown to overfit to adelta-distribution for all samples of each class, and representations of a class collapse together (recentlytermed neural collapse, Papyan et al., 2020), losing semantic and probabilistic information that distinguishesclass samples and so harming properties such as calibration and robustness (VC-B).2 Prior theoretical analysis of SSL.There has been considerable interest in understanding the mathemat-ical mechanism behind self-supervised learning (Arora et al., 2019; Tsai et al., 2020; Wang & Isola, 2020;Zimmermann et al., 2021; Lee et al., 2021; Von Kgelgen et al., 2021; HaoChen et al., 2021; Wang et al.,2021; Saunshi et al., 2022; Tian, 2022; Sansone & Manhaeve, 2022; Nakamura et al., 2023; Shwartz-Ziv et al.,2023; Ben-Shaul et al., 2023), as summarised by HaoChen et al. (2021) and Saunshi et al. (2022). A threadof works (Arora et al., 2019; Tosh et al., 2021; Lee et al., 2021; HaoChen et al., 2021; Saunshi et al., 2022)aims to prove that auxiliary task performance translates to downstream classification accuracy, but Saunshiet al. (2022) prove that to be impossible for typical datasets without also considering model architecture.Several works propose an information theoretic basis for SSL (Hjelm et al., 2019; Bachman et al., 2019; Tsaiet al., 2020; Shwartz-Ziv et al., 2023), e.g. maximising mutual information between representations, butTschannen et al. (2020); McAllester & Stratos (2020); Tosh et al. (2021) raise doubts about this. We showthat this apparent connection to mutual information is justified more fundamentally by our model for SSL.The previous works most similar to our own are those that take a probabilistic view of self-supervised learning,in which the encoder approximates the posterior of a generative model and so can be interpreted to reversethe generative process (e.g. Zimmermann et al., 2021; Von Kgelgen et al., 2021; Daunhawer et al., 2023). : Assessing the information in representations. Original images (left cols) and reconstructionsfrom representations learned by generative unsupervised learning (VAE, -VAE, CR-VAE), generative SSL(our SimVAE) and discriminative SSL (SimCLR, VicREG) on MNIST (l), Fashion MNIST (r). Discriminativemethods lose style information (e.g. orientation).",
  "Here, we propose a latent variable model for self-supervised learning, referred to as the SSL Model, and itsevidence lower bound, ELBOSSL, as the rationale behind predictive SSL methods": "We consider the data used for predictive SSL, X =Ni=1 xi, as a collection of subsets xi ={xji}j of semanticallyrelated data, where xji X may, for example, be different augmentations, modalities or snippets (indexed j) ofdata observations xi.3 All xji xi are considered to have some semantic information in common, referred toas content, while varying in what we refer to as style, e.g. mirror images of an object reflect the same object(content) in different orientations (style). A hierarchical generative process for this data, the SSL Model, isshown in . Under this model, a subset x of J semantically related data samples is generated by sampling:(i) yp(y), which determines the common semantic content, (ii) zj p(z|y), conditionally independently forj [1, J], which determine the style of each xj (given the same y), and (iii) each xj p(x|zj); hence",
  ",(ELBOSSL)": "where the approximate posterior q(z|x) q(zj|xj) is assumed to factorise.4 A derivation of ELBOSSL isgiven in Appendix A.2. Similarly to the standard ELBO, ELBOSSL comprises: a reconstruction term, theapproximate posterior entropy and the (now conditional) log prior over all zj z. By assuming that p(z|y) are unimodal and concentrated relative to p(z) (i.e. Var[z|y]Var[z], y), latentvariables of semantically related data zji zi form clusters and are closer on average than random pairs justas SSL representations are described. Thus, fitting the SSL Model to the data by maximising ELBOSSL,induces a mixture prior distribution over latent variables, pSSL(z) = y p(z|y)p(y) (, right), comparableto the distribution over representations induced by predictive SSL methods (, left). This connectiongoes to the heart of our main claim: that the SSL Model and ELBOSSL underpin predictive SSL algorithms(2). To formalise this, we first establish how discriminative and generative methods relate so that the lossfunctions of discriminative predictive SSL methods can be compared to the generative ELBOSSL objective.",
  "A Probabilistic Model behind Predictive SSL": "So far, we have assumed that latent variables make useful representations without justification. We nowjustify this by considering the relationship between discriminative and generative representation learning(4.1) in order to understand how the generative ELBOSSL objective can be emulated discriminatively (4.2).We then review predictive SSL methods and show how they each emulate ELBOSSL (4.3).",
  "Discriminative vs Generative Representation Learning": "Standard classification tasks can be approached discriminatively or generatively, defined by whether ornot a distribution over the data space X is learned in the process. We make an analogous distinction forrepresentation learning where, whatever the approach, the aim is to train an encoder f : X Z so thatrepresentations z =f(x) follow a distribution useful for downstream tasks, e.g. clustered by classes of interest. Under a generative model zx, latent variables z are assumed to determine semantic properties of the data x,which are predicted by the posterior p(z|x). Thus, learning an approximate posterior q(z|x) (parameterised by 3|xi| may vary and domains X j xji can differ with modality.4Expected to be reasonable for zj that carry high information about xj, such that observing related xk or its representationzk provides negligible extra information, i.e. p(zj|xj, xk)p(zj|xj).",
  "Emulating a Generative Objective Discriminatively": "Having seen in 4.1 that, in principle, an encoder can be trained generatively or discriminatively so thatrepresentations follow a target distribution p(z), we now consider how this can be achieved in practice forthe prior of the SSL Model, p=pSSL. While ELBOSSL gives a principled generative objective to learn representations that fit pSSL(z), a discriminativeloss that is minimised when pf(z)=pSSL(z) is unclear. Therefore, using ELBOSSL as a template, we considerthe individual effect each term has on the optimal posterior q(z|x) (parameterised by encoder f) to understandhow a discriminative (or p(x|z)-free) objective might induce a similar distribution over representations. Entropy: as noted in 4.1, discriminative methods can be considered to have a posterior with very lowfixed variance, q(z|x) zf(x), for which H[q] is constant and is omitted from a discriminative objective. Prior: Eq[log p(z|y)] is optimal w.r.t. q iff all related samples xj x map to modes of p(z|y). For uni-modal p(z|y), this means representations zj z collapse to a point, losing style information that distinguishesxj x.5 Since the prior term is not generative it is included directly in a discriminative objective. Reconstruction: Eq[log p(xj|zj)] terms are maximised w.r.t. q iff each xj maps to a distinct representationzj, which p(x|zj) maps back to xj; countering the prior to prevent collapse. In a discriminative objective,this generative term is excluded, and is considered emulated by a term that requires zj to be distinct. We see that ELBOSSL includes a term acting to collapse clusters p(z|y) and another preventing such collapse,which counterbalance one another when the full ELBOSSL objective is maximised. Since the reconstructionterm, p(x|z), must be excluded from a discriminative objective, a discriminative objective SSL is consideredto emulate ELBOSSL if it combines the prior term with a (p(x|z)-free) substitute for the reconstruction term,R(), that prevents representation collapse (both between and within clusters), i.e.",
  "zq(z|x)(log p(z|y) + R(z, x)).(3)": "We can now restate our claim as: predictive SSL objectives have the form of Eq. 3 and so emulate ELBOSSLto induce a distribution over representations comparable to the prior pSSL(z) of the SSL Model (). The analysis of ELBO terms shows that objectives of the form of Eq. 3 fit the view that SSL pulls togetherrepresentations of related data and pushes apart others (e.g. Wang & Isola, 2020). That is, the prior pullsrepresentations to modes and the R() term pushes to avoid collapse. Thus ELBOSSL, which underpins Eq.3, provides the underlying rationale for this pull/push perspective of SSL, depicted in . Lastly, we note that the true reconstruction in ELBOSSL not only avoids collapse but is a necessary componentin order to approximate the posterior and learn meaningful representations. We will see, as may be expected,that substituting this term by R() can impact representations and their downstream performance.",
  "Emulating the SSL Model with Predictive Self-Supervised Learning": "Having defined ELBOSSL as a generative objective for self-supervised representation learning and SSL as adiscriminate approximation to it, we now consider classes of predictive SSL, specifically instance discrimination,latent clustering and contrastive methods, and their relationship to SSL, and thus the SSL Model. Instance Discrimination (ID) trains a softmax classifier on data samples labelled by their index {(xji, yi =i)}i,j. From VC-A (2), this softmax cross-entropy objective can be interpreted under the SSL Model asmaximising a variational lower bound given by",
  "zq(z|x)(log p(z|y) log p(z))+ c .(4)": "Eq. 4 (RHS) matches Eq. 3 with J =1 and R()=H[p(z)], the entropy of p(z), where z = f(x) are outputs ofthe classifier encoder. Intuitively, maximising entropy (in lieu of the reconstruction term) might be expectedto avoid collapse, but although representations (z) of distinct classes are spread apart, those within the sameclass, zji zi, collapse together (VC-B, 2). Deep Clustering (DC) iteratively assigns temporary labels y = c, c [1, C], to data x by clusteringrepresentations z output by a ResNet encoder; and trains the encoder together with a softmax head to predictthose labels. While subsets x of data with the same label are now defined according to a ResNets inductivebias, the same loss is used as Eq. 4, having the form of Eq. 3 with J =1. Each cluster of representationszj z again collapses together. Contrastive Learning based on the InfoNCE objective (Eq. 1), contrasts the representations of positivepairs x ={x, x+} with those sampled at random. The InfoNCE objective is known to be optimised whensim(z, z) = PMI(x, x) + c (2). From this, it can be shown that for X ={x+, x1 , ..., xk }, X =X{x},",
  "jlog p(zj)) c(5)": "(See Appendix A.3 for a full derivation.) Here, p(z, z+) is the probability of sampling z, z+ assuming thatx, x+ are semantically related. Under the SSL Model this is denoted p(z, z+|y) .= p(z|y), hence the change innotation for ease of comparison to Eq. 3. Thus, the InfoNCE loss lower bounds an objective of the form ofEq. 3 with R()=H[p(z)] as in Eq. 4, but with J =2 samples. As a brief aside, we note that ID and DC consider J =1 sample xji at a time, computing p(zi|yi; i) fromparameters i stored for each class (weights of the softmax layer), which could be memory intensive. However,for J 2 samples, one can estimate p(zi|yi) without stored parameters as p(zi|yi)=",
  "InfoNCE is known to optimise a lower bound on mutual information (MI), I(x, x) = E[logp(x,x)": "p(x)p(x)], (Oordet al., 2018) and Eq. 5 shows it lower bounds I(z, z). It has been argued that maximising MI is in fact theunderlying rationale that explains contrastive learning (Hjelm et al., 2019; Ozsoy et al., 2022). Previousworks have challenged this showing that better MI estimators do not give better representations (Tschannenet al., 2020), that MI approximation is inherently noisy (McAllester & Stratos, 2020) and that the InfoNCEestimator is arbitrarily upper-bounded (Poole et al., 2019). Note also that for disjoint xi (e.g. non-overlappingaugmentation sets), the range of pointwise mutual information values is unbounded, [, k], k>1, yet usingthe bounded cosine similarity function, sim(z, z)=zz zz (e.g. Chen et al., 2020a) to model PMIvalues outperforms use of the unbounded dot-product (see A.5).6 Our analysis supports the idea that MI isnot the fundamental mechanism behind SSL and explains the apparent connection: MI arises by substitutingentropy H[p(x)] for the reconstruction term in ELBOSSL as in predictive SSL methods. Rather than aiderepresentation learning, this is shown to collapse representations together and lose information.",
  "Rationale for a Projection Head": "Downstream performance is often found to improve by adding several layers to the encoder, termed aprojection head, and using encoder outputs as representations and projection head outputs in the lossfunction. Our analysis has shown that the ELBOSSL objective clusters representations of semantically relateddata while ensuring that all representations are distinct, whereas predictive SSL objectives collapse thoseclusters. However, near-final layers are found to exhibit similar clustering, but with higher intra-clustervariance (Gupta et al., 2022), as also observed in supervised classification (Wang et al., 2022). We conjecturethat representations from near-final layers outperform those used in the loss function because their higherintra-cluster variance, or lower collapse, gives a distribution closer to pSSL of the SSL Model.",
  "Generative Self-Supervised Learning (SimVAE)": "The proposed SSL Model () has been shown to justify: (i) the training objectives of predictive SSLmethods; (ii) the more general notion that SSL pulls together/pushes apart representations; (iii) theconnection to mutual information; and (iv) the use of a projection head. As a proposed basis for self-supervised learning, we look to provide empirical validation by maximisingELBOSSL to (generatively) learn representations that perform comparably to predictive SSL. MaximisingELBOSSL can be viewed as training a VAE with mixture prior pSSL(z)=",
  "Experimental Setup": "Datasets and Evaluation Metrics.We evaluate SimVAE representations on four datasets including twowith natural images: MNIST (LeCun, 1998), FashionMNIST (Xiao et al., 2017), CelebA (Liu et al., 2015) andCIFAR10 (Krizhevsky et al., 2009). We augment images following the SimCLR protocol (Chen et al., 2020a)which includes cropping and flipping, and colour jitter for natural images. Frozen pre-trained representationsare evaluated by a k-Nearest Neighbors (k-NN; Cover & Hart, 1967)(k-NN) and a non-linear MLP probeon classification tasks (Chen et al., 2020a; Caron et al., 2020). Downstream performance is measured interms of classification accuracy (Acc). Generative quality is evaluated by FID score (Heusel et al., 2017) andreconstruction error. For further experimental details and additional results for clustering under a gaussianmixture model and a linear probe see Appendices A.7 and A.8. Baselines methods.We compare SimVAE to other VAE-based models including the vanilla VAE (Kingma& Welling, 2014), -VAE (Higgins et al., 2017) and CR-VAE (Sinha & Dieng, 2021), as well as to state-of-the-art self-supervised discriminative methods including SimCLR (Chen et al., 2020a), VicREG (Bardeset al., 2022), MoCo (He et al., 2020) and its extension MoCo v2 (Chen et al., 2020b). As a lower bound,",
  "SimVAE98.4 0.082.1 0.095.6 0.451.8 0.098.5 0.086.5 0.093.2 0.147.1 0.0": ": Content retrieval. Top-1% classification accuracy() for MNIST, FashionMNIST, CIFAR10,and CelebA (gender classification) using a MLP probe (MP) and k-Nearest Neighbors (k-NN) classificationmethods; We report mean and standard errors over three runs; Bold indicates best scores in each methodclass: generative (teal), discriminative methods (red). we also provide results for a randomly initialized encoder. For fair comparison, the augmentation strategy,representation dimensionality, batch size, and encoder-decoder architectures are invariant across methods. Toenable a qualitative comparison of representations, decoder networks were trained for each discriminativebaseline on top of frozen representations using the reconstruction error. See Appendix A.7 for further detailson training baselines and decoder training. Implementation Details.We use MLP and Resnet18 (He et al., 2016) network architectures for simpleand natural image datasets respectively. For all generative approaches, we adopt Gaussian posteriors, q(z|x),priors, p(z), and likelihoods, p(x|z), with diagonal covariance matrices (Kingma & Welling, 2014). ForSimVAE, we adopt Gaussian p(z|y) as described in 5 and, we fix the number of augmentations to J =10(see for an ablation). Ablations were performed for all sensitive hyperparameters for each method andparameter values were selected based on the best average MLP Acc across datasets. Further details regardinghyperparameters and computational resources can be found in Appendix A.7.",
  "Results": "We assess content and style information retrieval by predicting attributes that are preserved (e.g. object class,face gender) or disrupted (e.g. colour, position and orientation) under augmentation, respectively. Content Retrieval. reports downstream classification performance across datasets using class labels.SimVAE outperforms predictive SSL for simple datasets (MNIST, FashionMNIST, CelebA), which empiricallysupports H1, demonstrating that SimVAE achieves self-supervised learning where data distributions can bewell-modelled. SimVAE also materially reduces the performance gap () between generative and discriminativeSSL, by approximately half ( = 32.8% 17.6%) for more complex settings (CIFAR10). The performanceimprovement by SimVAE over other generative methods supports H3. Style Retrieval.We reconstruct MNIST, FashionMNIST, CIFAR-10, and CelebA images from represen-tations to gain qualitative insights into the style information captured. Figures 3 and 4 show a significantloss of orientation and colour in reconstructions from predictive SSL representations across datasets. CelebAhas 40 mixed labeled attributes, some of which clearly pertain to style (e.g. hair color). Removing thosewith high-class imbalance, we evaluate classification of 20 attributes and report results in . SimVAEoutperforms both generative and predictive SSL baselines at classifying hair color (left, middle). Acrossall attributes (right), SimVAE outperforms or performs comparably to both generative and predictive SSLbaselines. These results support H2 qualitatively and quantitatively, confirming that discriminative methodslose more style information relative to generative methods (, middle). Performance for each CelebAattribute is reported in in Appendix A.8.",
  "SimVAE67.5 0.3": ": Style retrieval on CelebA. (Acc-MP ) (left) hair colour prediction (mean and standard error over3 runs, best results in bold); (middle) content vs. style prediction (gender vs hair colour), best performancein top-right; (right) Performance gain of SimVAE vs baselines (M) across all 20 CelebA attributes. From leftto right, lowest to highest mean score; Image Generation.While generative quality is not relevant to our main hypotheses, out of interest weshow randomly generated SimVAE images and quality metrics in Appendix A.8. We observe small butsignificant improvements in FID score and reconstruction error relative to previous VAE methods.",
  "Conclusion": "The impressive performance of self-supervised learning over recent years has spurred considerable interest inunderstanding the theoretical mechanism underlying SSL. In this work, we propose a latent variable model(SSL Model, ) as the theoretical basis to explain several families of self-supervised learning methods,termed predictive SSL, including contrastive learning. We show that the ELBO of the SSL Model relatesto the loss functions of predictive SSL methods and justifies the general notion that SSL pulls togetherrepresentations of semantically related data and pushes apart others. Predictive SSL methods are found to maximise entropy H[p(z)], in lieu of the ELBOs reconstruction term.Although this creates a seemingly intuitive link to the mutual information between representations thatwas thought to explain contrastive methods, it in fact causes representations of semantically related datato collapse together, losing style information about the data, and reducing their generality. Our analysisalso justifies the use of a projection head, the existence of which suggests caution in (over-)analysingrepresentations that are ultimately not used. We provide empirical validation for the proposed SSL Model by showing that fitting it by variational inference,termed SimVAE, learns representations generatively that perform comparably, or significantly reduce theperformance gap, to discriminative methods at content prediction. Meanwhile, SimVAE outperforms wherestyle information is required, taking a step towards task-agnostic representations. SimVAE also outperformsprevious generative VAE-based approaches, including CR-VAE tailored to SSL. Learning representations of complex data distributions generatively, rather than discriminatively, remainsa challenging and actively researched area (Tschannen et al., 2024). Balestriero & LeCun (2024) recentlyhighlighted the scale of the challenge for higher-variance datasets such as ImageNet (Deng et al., 2009). Wehope that the theoretical connection to popular SSL methods and the notable performance improvementsof SimVAE over other VAE-based methods encourages further investigation into generative representationlearning. This seems particularly justified given the potential for uncertainty estimation from the posterior, theability to generate novel data samples, and the potential for fully task-agnostic representations learned under aprincipled rather than heuristic approach. The transparency of the SSL Model may also allow representationsto be learned that disentangle rather than discard style information (Higgins et al., 2017) through carefulchoice of model parameters. The SSL Model thus serves as a basis for understanding self-supervised learningand offers guidance for the design and interpretation of future representation learning methods.",
  "Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predictingimage rotations. arXiv preprint arXiv:1803.07728, 2018. 3": "Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. In NeurIPS, 2020. 3, 8, 19 Kartik Gupta, Thalaiyasingam Ajanthan, Anton van den Hengel, and Stephen Gould. Understanding andimproving the role of projection head in self-supervised learning. arXiv preprint arXiv:2212.11491, 2022. 8",
  "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trainedby a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. 9": "Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, ShakirMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variationalframework. In ICLR, 2017. 4, 8, 9, 11 R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.In ICLR, 2019. 3, 4, 8",
  "The proposed hierarchical latent variable model for self-supervised learning () is fitted to the datadistribution by maximising the ELBOSSL and can be viewed as a VAE with a hierarchical prior": "VAEs have been extended to model hierarchical latent structure (e.g. Valpola, 2015; Ranganath et al., 2016;Rolfe, 2017; He et al., 2018; Snderby et al., 2016; Edwards & Storkey, 2016), which our work relates to.Notably, Edwards & Storkey (2016) propose the same graphical model as , but methods differ in howposteriors are factorised, which is a key aspect for learning informative representations that depend only onthe sample they represent. Wu et al. (2023) and Sinha et al. (2021) combine aspects of VAEs and contrastivelearning but do not propose a latent variable model for SSL. Nakamura et al. (2023) look to explain SSLmethods via the ELBO, but with a second posterior approximation not a generative model.",
  "A.2Derivation of ELBOSSL and SimVAE Objective": "Let x = {x1, ..., xJ} be a set of J semantically related samples with common label y (e.g. the index i ofa set of augmentations of an image xi). Let = {, , } be parameters of the generative model for SSL() and be the parameter of the approximate posterior q(z|x). We derive the Evidence Lower Bound(ELBOSSL) that underpins the training objectives of (projective) discriminative SSL methods and is used totrain SimVAE (5).",
  "(= ELBOSSL)": "Terms of ELBOSSL are analogous to those of the standard ELBO: reconstruction error, entropy of theapproximate posterior H(q(z|x)) and the (conditional) prior. Algorithm 1 provides an overview of thecomputational steps required to maximise ELBOSSL under Gaussian assumptions described in 5, referred toas SimVAE. As our experimental setting considers augmentations as semantically related samples, Algorithm 1incorporates a preliminary step to augment data samples.",
  "A.3Detailed derivation of InfoNCE Objective": "For data sample xi0 xi0, let xi0 xi0 be a semantically related positive sample and {xir}kr=1 be randomnegative samples. Denote by x = {xi0, xi0} the positive pair, by X ={xi1, ..., xik} all negative samples, andby X = x X all samples. The InfoNCE objective is derived as follows (by analogy to Oord et al. (2018)).",
  "A.4Derivation of parameter-free p(zi|yi) = s(zi)": "Instance Discrimination methods consider J =1 sample xi at a time, labelled by its index yi =i, and computesp(xi|y = i; i) from stored instance-specific parameters i. This requires parameters proportional to thedataset size, which could be prohibitive, whereas parameter number is often independent of the datasetsize, or grows slowly. We show that contrastive methods (approximately) optimise the same objective,but without parameters, and here explain how that is possible. Recall that the label i is semanticallymeaningless and simply identifies samples of a common distribution p(x|y=i) .=p(x|yi). For J 2 semanticallyrelated samples xi = {xji}Jj=1, xji p(x|yi), their latent variables are conditionally independent, hencep(zi|yi) =",
  "j=kzji zki )}(if z2 = 1)": "The result can be rearranged into a Gaussian form (a well known result when all distributions are Gaussian),but the last line also shows that, under the common practice of setting embeddings to unit length (z2 =1),s() can be calculated directly from dot products, or cosine similarities (up to a proportionality constant,which does not affect optimisation).",
  "/p(x)...if x =x (i.e. x is an augmentation of x)0...otherwise;(12)": "and PMI(x, x) = log p(x) k > 0 or PMI(x, x) = , respectively. Here k = log arg maxx p(x) is afinite value based on the most likely sample. For typical datasets, this can be approximated empirically by1N where N is the size of the original dataset (since that is how often the algorithm observes each sample),hence k = log N, often of the order 5 10 (depending on the dataset). If the main objective were to accurately approximate PMI (subject to a constant c) in Eq. 10, e.g. toapproximate mutual information, or if representation learning depended on it, then, at the very least, thedomain of sim(, ) must span its range of values, seen above as from for negative samples to a smallpositive value (e.g. 5-10) for positive samples. Despite this, the popular bounded cosine similarity function(cossim(z, z) =zT z ||z||2||z||2 ) is found to outperform the unbounded dot product, even though thecosine similarity function necessarily cannot span the range required to reflect true PMI values, while thedot product can. This strongly suggests that representation learning does not require representations tospecifically learn PMI, or for the overall loss function to approximate mutual information. Instead, with the cosine similarity constraint, the InfoNCE objective is as optimised as possible if represen-tations of a data sample and its augmentations are fully aligned (cossim(z, z) = 1) and representations ofdissimilar data are maximally misaligned cossim(z, z) = 1, since these minimise the error from the truePMI values for positive and negative samples (described above). Constraints, such as the dimensionality ofthe representation space vs the number of samples, may prevent these revised theoretical optima being fullyachieved, but the loss function is optimised by clustering representations of a sample and its augmentationsand spreading apart those clusters. Note that this is the same geometric structure as induced under softmaxcross-entropy loss (Dhuliawala et al., 2023). We note that our theoretical justification for representations not capturing PMI is supported by the empiricalobservation that closer approximations of mutual information do not appear to improve representations(Tschannen et al., 2020). Also, more recent contrastive self-supervised methods increase the cosine similaritybetween semantically related data but spread apart representation the without negative sampling of InfoNCE,yet outperform the InfoNCE objective despite having no obvious relationship to PMI (Grill et al., 2020;Bardes et al., 2022).",
  "A.7.2Datasets": "MNIST.The MNIST dataset (LeCun, 1998) gathers 60000 training and 10000 testing images representingdigits from 0 to 9 in various caligraphic styles. Images were kept to their original 28x28 pixel resolution andwere binarized. The 10-class digit classification task was used for evaluation. FashionMNIST.The FashionMNIST dataset (Xiao et al., 2017) is a collection of 60000 training and10000 test images depicting Zalando clothing items (i.e., t-shirts, trousers, pullovers, dresses, coats, sandals,shirts, sneakers, bags and ankle boots). Images were kept to their original 28x28 pixel resolution. The 10-classclothing type classification task was used for evaluation. CIFAR10.The CIFAR10 dataset (Krizhevsky et al., 2009) offers a compact dataset of 60,000 (50,000training and 10,000 testing images) small, colorful images distributed across ten categories including objectslike airplanes, cats, and ships, with various lighting conditions. Images were kept to their original 32x32 pixelresolution. CelebA.The CelebA dataset (Liu et al., 2015) comprises a vast collection of celebrity facial images. Itencompasses a diverse set of 183000 high-resolution images (i.e., 163000 training and 20000 test images),",
  "A.7.3Data augmentation strategy": "Taking inspiration from SimCLRs (Chen et al., 2020a) augmentation strategy which highlights the importanceof random image cropping and colour jitter on downstream performance, our augmentation strategy includesrandom image cropping, random image flipping and random colour jitter. The colour augmentations are onlyapplied to the non gray-scale datasets (i.e., CIFAR10 & CelebA datasets). Due to the varying complexityof the datasets we explored, hyperparameters such as the cropping strength were adapted to each datasetto ensure that semantically meaningful features remained after augmentation. The augmentation strategyhyperparameters used for each dataset are detailed in table 2.",
  "This section contains all details regarding the architectural and optimization design choices used to trainSimVAE and all baselines. Method-specific hyperparameters are also reported below": "Network Architectures.The encoder network architectures used for SimCLR, MoCo, VicREG, andVAE-based approaches including SimVAE for simple (i.e., MNIST, FashionMNIST ) and complex datasets(i.e., CIFAR10, CelebA) are detailed in a, a respectively. Generative models which include allVAE-based methods also require decoder networks for which the architectures are detailed in b andb. The latent dimensionality for MNIST and FashionMNIST is fixed at 10 and increased to 64 for theCelebA and CIFAR10 datasets. The encoder and decoder architecture networks are kept constant acrossmethods including the latent dimensionality to ensure a fair comparison.",
  ": Resnet18 network architectures used for CIFAR10 & CelebA datasets": "on the downstream MLP classification accuracy across datasets. The final values of hyperparameters wereselected to reach the best average downstream performance across datasets. While we observed stableperformances across datasets for the VAE family of models, VicREG and MoCo, SimCLR is more sensitive,leading to difficulties when having to define a unique set of parameters across datasets. For VAEs, thelearning rate was set to 8e5, and the likelihood probability, p(x|z), variance parameter was set to 0.02 for-VAE, CR-VAE and SimVAE. CR-VAEs parameter was set to 0.1. SimVAEs prior probability, p(z|y),variance was set to 0.15 and the number of augmentations to 10. VicREGs parameter was set to 25 andthe learning rate to 1e-4. SimCLRs temperature parameter, , was set to 0.7 and learning rates were adaptedfor each dataset due to significant performance variations across datasets ranging from 8e5 to 1e3. ForMoCo, the temperature parameter was fixed to 0.7, for MoCov2 to 0.2. To ensure a more fair comparisonbetween methods, we kept the augmentation strategy identical across methods and did not add blurring forMoCov2 training.",
  "A.7.5Evaluation Implementation Details": "Following common practices (Chen et al., 2020a), downstream performance is assessed using a linear probe, amulti-layer perceptron probe, a k-nearest neighbors (kNN) algorithm, and a Gaussian mixture model (GMM).The linear probe consists of a fully connected layer whilst the mlp probe consists of two fully connected layerswith a relu activation for the intermediate layer. Both probes were trained using an Adam optimizer witha learning rate of 3e4 for 200 epochs with batch size fixed to 128. Scikit-learns Gaussian Mixture modelwith a full covariance matrix and 200 initialization was fitted to the representations using the ground truthcluster number. The k-NN algorithm from Pythons Scikit-learn library was used with k spanning from 1 to15 neighbors. The best performance was chosen as the final performance measurement. No augmentationstrategy was used at evaluation. Computational Resources.Models for MNIST, FashionMNIST and CIFAR10 were trained on aRTX2080ti GPU with 12G RAM. Models for CelebA were trained on an RTX3090 GPU with 24G RAM. Weobserve that while the family of generative models requires more time per iteration, the loss converges fasterwhile discriminative methods converge at a slower rate when considering the optimal set of hyperparameters.As a consequence, generative baselines and SimVAE were trained for 400 epochs while discriminative methodswere trained for 600 to 800 epochs.",
  "SimVAE98.0 0.080.0 0.087.1 0.340.1 0.096.6 0.071.1 0.058.4 0.639.3 0.0": ": Top-1% self-supervised Acc () for MNIST, FashionMNIST, CIFAR10, and CelebA (genderclassification) using a linear probe (LP) and Gaussian Mixture Model (GMM) classification methods; Wereport mean and standard errors over three runs; Bold indicate best scores in each method class: generative(teal), discriminative methods (red). Ad-hoc decoder training VAE-based approaches, including SimVAE, are fundamentally generative methodsaimed at approximating the logarithm of the marginal likelihood distribution, denoted as log p(x). In contrast,most traditional self-supervised methods adopt a discriminative framework without a primary focus onaccurately modeling p(x). However, for the purpose of comparing representations, and assessing the spectrumof features present in z, we intend to train a decoder model for SimCLR & VicREG models. This decodermodel is designed to reconstruct images from the fixed representations initially trained with these approaches.To achieve this goal, we train decoder networks using the parameter configurations specified in Tables 3b and4b, utilizing the mean squared reconstruction error as the loss function. The encoder parameters remainconstant, while we update the decoder parameters using an Adam optimizer with a learning rate of 1e4",
  "until a minimal validation loss is achieved (i.e. 10-80 epochs)": "Conditional Image Generation.To allow for a fair comparison, all images across all methods are generatedby sampling z from a multivariate Gaussian distribution fitted to the training samples representations. Moreprecisely, each Gaussian distribution is fitted to z conditioned on a label y. Scikit-Learn Python libraryGaussian Mixture model function (with full covariance matrix) is used.",
  "A.8Additional Results & Ablations": "Content retrieval with linear & gaussian mixture model prediction heads.?? reports the top-1%self-supervised classification accuracy using a linear prediction head and a gaussian mixture model. From ??,we draw similar conclusion as with : SimVAE significantly bridges the gap between discriminativeand generative self-supervised learning methods when considering a supervised linear predictor and fullyunsupervised methods for downstream prediction. report the normalized mutual information (NMI)and adjusted rank index (ARI) for the fitting of the GMM prediction head. Content & Style retrieval for all CelebA attributes. reports average classification accuracyusing a MLP probe (over 3 runs) for the prediction of 20 CelebA facial attributes for SimVAE, generativeand discriminative baselines. Augmentation protocol strength ablation. reports the downstream classification accuracyacross methods for various augmentations strategies. More precisely, we progressively increase the croppingscale and color jitter amplitude. Unsurprisingly (Chen et al., 2020a), discriminative methods exhibit highsensitivity to the augmentation strategy with stronger disruption leading to improved content prediction.The opposite trend is observed with vanilla generative methods where reduced variability amongst the dataleads to increased downstream performance. Interestingly, SimVAE is robust to augmentation protocol andperforms comparably across settings.",
  ": Ablation experiment across the augmentation strength for (left) image cropping and (right) colorjitter strength considered during training of the SimVAE and baseline models using the CIFAR dataset": "# of augmentation ablation. reports the downstream classification accuracy for increasingnumbers of augmentations considered simultaneously during the training of SimVAE for MNIST and CIFAR10datasets. On average, a larger number of augmentations result in a performance increase. Further explorationis needed to understand how larger sets of augmentations can be effectively leveraged potentially by allowingfor batch size increase. From , we fix our number of augmentations to 10 across datasets. Likelihood p(x|z) variance ablation.We explore the impact of the likelihood, p(x|z), variance, 2, acrosseach pixel dimension on the downstream performance using the MNIST and CIFAR10 datasets. highlights how the predictive performance is inversely correlated with the 2 on the variance range consideredfor the CIFAR10 dataset. A similar ablation was performed on all VAE-based models and led to a similarconclusion. We therefore fixed 2 to 0.02 for -VAE, CR-VAE and SimVAE across datasets."
}