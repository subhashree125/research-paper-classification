{
  "Abstract": "A Schrdinger bridge establishes a dynamic transport map between two target distributionsvia a reference process, simultaneously solving an associated entropic optimal transportproblem. We consider the setting where samples from the target distributions are available,and the reference diffusion process admits tractable dynamics. We thus introduce CoupledBridge Matching (BM2), a simple non-iterative approach for learning Schrdinger bridgeswith neural networks. A preliminary theoretical analysis of the convergence properties ofBM2 is carried out, supported by numerical experiments that demonstrate the effectivenessof our proposal.",
  "Introduction": "The Schrdinger bridge problem seeks a process, the Schrdinger bridge, with prescribed initial and terminaldistributions, such that the distribution of the Schrdinger bridge minimizes the Kullback-Leibler (KL)divergence to the distribution of a reference process. Schrdinger bridges play a central role in measuretransport theory (Marzouk et al., 2016). Notably, it is known that the initial-terminal distribution of aSchrdinger bridge provides a solution to a corresponding entropic optimal transport problem (Peyr & Cuturi,2020). Schrdinger bridges thus provide an effective framework for finding an alignment between samplesfrom two target distributions. Furthermore, diffusion-based generative models (Ho et al., 2020; Song et al.,2021) can be interpreted as solving trivial instances of the Schrdinger bridge problem (Peluchetti, 2023).Consequently, Schrdinger bridges offer a more general approach to contemporary generative applications. We consider the setting where samples are readily available from both target distributions, and where thereference process is a diffusion process solution to a stochastic differential equation (SDE). We thus introduceCoupled Bridge Matching (BM2), a novel methodology aimed at computing the Schrdinger bridge giventhe reference SDE and samples from the two marginal distributions of interest. BM2 builds upon BridgeMatching (BM), introduced1 by Peluchetti (2021). Our approach advances recent contributions by Peluchetti(2023); Shi et al. (2023) by removing the need to solve a sequence of optimization problems. A neural networkis employed to jointly learn a forward drift function and a backward drift function corresponding to theforward and backward dynamics of a Schrdinger bridge. BM2 achieves several key desiderata:",
  "Published in Transactions on Machine Learning Research (12/2024)": "Forward-Backward SB SDE: Chen et al. (2022) propose two training algorithms addressing the dynamicSB problem. Both approaches employ loss functions that require divergence computations (violating desiderata(iv)) and the use of two distinct neural networks. The first method is iterative, resembling DIPF (violatingdesiderata (i)), while the second method involves differentiating through entire discretized paths, resulting inhigh memory consumption (violating desiderata (iii)). The subsequent works concentrate on solving the static Schrdinger bridge (2), or EOT (3), problem. Oncethis is achieved, solutions to the dynamic problem are trivially obtained through the standard decompositionS = S0,1R|0,1. Although these works differ in nature and objectives, we include them here due to their sharedcharacteristic with BM2: the non-iterative nature of the algorithm. Light SB: in two notable works, Korotin et al. (2023) and Gushchin et al. (2024) propose non-iterative,sample-based EOT solvers for the Euclidean cost function, i.e., for the specific choice of reference dynamics(R). Korotin et al. (2023) introduces an approximation to (an adjusted version of) the Schrdinger potentialfor 1 via a mixture of Gaussian distributions, resulting in a mixture of Gaussian distributions approximationto S1|0. Gushchin et al. (2024) builds upon this approximation and introduces an additional sample-basedtraining objective that takes as input any coupling C0,1 C(0, 1), whereas Korotin et al. (2023) requiresthe independent coupling 00. While also non-iterative, the proposals of Korotin et al. (2023); Gushchinet al. (2024) differ from BM2 in two key aspects: (a) they learn a solution in the static setting instead of thedynamic one, and (b) they employ mixture of Gaussian distributions approximations, rather than neuralnetwork approximators for the drift functions. Consequently, these methods may face challenges in scaling tomodern generative ML applications. Light SB, in both variants, demonstrates strong performance in thebenchmark presented in : the results of (Gushchin et al., 2024, ) and (Korotin et al., 2023,) are directly comparable with the results of . However, it is worth noting that this benchmarkis particularly well-suited for Light SB, as acknowledged by its authors, since each benchmark target S0,1 isconstructed such that S1|0 is itself a mixture of 5 Gaussian distributions.",
  "S0,1,R :=arg minP P(0,1)KL(P R),(1)": "where KL( ) is the KL divergence and P(0, 1) is the class of distributions of stochastic processeshaving initial distribution 0 and terminal distribution 1. We narrow down (1) to the case where R is thedistribution of a diffusion process. In this case, under suitable conditions (Lonard, 2014b), (1) admits aunique solution which is also a diffusion process. From this point forward, 0, 1 and R are considered fixed.For brevity, we will thus denote the Schrdinger bridge S0,1,R simply as S, and apply the same notationconvention to any distribution dependent on these variables.",
  "X0 0,dXt = dWt,t ,(R)": "with > 0. Our approach is not limited to the choice of SDE (R), BM2 readily extends to the broader classof reference SDEs examined in Peluchetti (2023). The main requirement for the applicability of BM2 is theanalytical availability of (4, 5) for the chosen reference SDE. We address the case, commonly employed ingenerative applications, of dXt = tdWt for a schedule t explicitly in Appendix A, and refer the readerto Peluchetti (2021; 2023) for the general setting. As our developments are orthogonal to the specific choiceof reference process, we focus on the simplest case for explanatory reasons.",
  "Bridge Matching (BM)": "We succinctly review Bridge Matching, and refer to Peluchetti (2021; 2023); Shi et al. (2023) for more details.BM takes as input a joint distribution Q0,1 with marginal distributions Q0, Q1 and a SDE, (R). Firstly, astochastic process Q0,1 is constructed as a mixture of diffusion bridges (R|0,1), such that the endpoints(X0, X1) of X Q0,1 are distributed according to Q0,1. This process, which is a mixture of diffusionprocesses, is not itself a diffusion process in general (Jamison, 1974). However, we can obtain a marginal-matching diffusion process with distribution M Q0,1 for which M Q0,1t= Q0,1t, 0 t 1. Consequently,X M Q0,1 is a diffusion process for which X0 Q0 and X1 Q1, i.e. it defines a dynamic transport fromQ0 to Q1.",
  "(Xt, t, X0) (Xt, t)2dt,(10)": "by replacing each integral with an expectation over uniform time t U(0, 1), and then approximating bothexpectations with Monte Carlo estimators. While we will rely exclusively on (9, 10) in the experiments of, Q0,1mand Q0,1mcan be inferred from paths X Q0,1 also by performing maximum likelihoodestimation or by employing a drift matching estimator (Liu et al., 2022; Peluchetti, 2023).",
  "S := {P P | P is a Schrdinger bridge for some target marginal distributions} = R M,": "where the equivalence is established by Jamison (1975) under appropriate assumptions. We additionallydefine the following restrictions: P(0, ) := {P P | P0 = 0}, P(, 1) := {P P | P1 = 1},P(0, 1) := {P P | P0 = 0 and P1 = 1}. Restrictions to R, M, S and C employ the same notation.",
  "For Q P, it is instructive to view BM as a map between distributions defined by the composition of two": "projections: QRp Q0,1 Mp M Q0,1. Here, the reciprocal projection Rp : P R projects Q onto the reciprocalclass R, while the Markovian projection Mp : R M projects Q0,1 onto the class of diffusion processes, seeShi et al. (2023). It follows that if P R, then P = Rp(P), and if P M, then P = Mp(P). Consequently,if P S, P = (Mp Rp)(P) for the BM map (Mp Rp), and conversely if P = (Mp Rp)(P) then P S.",
  "All of m, m, M are defined in . System (11) defines an update step (H, K)(11) (HK0,1, KH0,1)": "We are interested in the fixed points of such updates, i.e. (H, K) such that (H, K)(11) (H, K). It holdsthat H = K = S is a fixed point to (11). As S S(0, 1), S = S0,1 = M S0,1, see the review at the endof . Consequently, 0M S0,1|0= 0S|0 = S and 1M S0,1|1= 1S|1 = S. In this case, the SB-optimal drifts s and s of (S, S ) respectively replace Km0,1and Hm0,1in (HK0,1, K H0,1). Under the additionalassumption that H = K, or equivalently that (HK0,1, K H0,1) are the time reversal of each other, thisfixed point is unique. Let G = H = K, we have G = 0M G0,1|0= G0M G0,1|0= M G0,10M G0,1|0= M G0,1 andG0 = 0, G1 = 1, thus G = S(0, 1) = S. We have shown the following: Lemma 1 (Fixed points of (11)). Under suitable conditions (Lonard, 2014a), the updates (H, K)(11)(HK0,1, KH0,1), parametrized by diffusion process distributions, admit H = K = S as fixed point. If H = K,this fixed point is unique.",
  "When Km0,1= s and Hm0,1= s, (11) has reached an equilibrium. The updates (H, K)(11) (HK0,1, KH0,1)": "are realized through the computation of the drifts Km0,1and Hm0,1, i.e. by minimizing the losses (9, 10),where Q0,1 is respectively equal to K0,1 and H0,1. Our proposal, BM2, follows from replacing the completeminimization of (9, 10) with partial and stochastic minimization of (9, 10) through stochastic gradient descent.More precisely, consider the forward and backward SDEs with distributions F() and B():",
  "(12)": "At each optimization step, BM2 attempts to minimize L(; ) in via a step of stochastic gradient descent,starting from = and keeping fixed, resulting in . The subsequent optimization step employs .The complete training objective is presented in Algorithm 1, where sg() refers to the stop-gradient operator L(; ) is minimized in the first arguments only and discretize() represents a generic SDE discretizationscheme. For completeness, we outline the standard SGD training loop in Algorithm 2, where sgdstep() refersto an update step via a generic gradient descent optimizer. It should be noted that merely performing coupled drift matching of F and B, wherein F learns the driftconsistent with paths from B and vice versa, does not yield the Schrdinger bridge as a fixed point (Bortoliet al., 2021). The introduction of the mixing process is crucial in ensuring this property. Moreover, L(; )must be minimized only with respect to its first argument: the application of the stop-gradient operator sg()is not an efficiency consideration but a necessary component.",
  "Convergence Properties": "At each training step, BM2 performs a partial and stochastic minimization of the loss L(; ) from (12) withrespect to , where L(; ) is defined by an expectation over a distribution dependent on , yielding .Subsequently, is updated to match , and the process advances to the next training step. The alternationbetween expectation and maximization steps bears resemblance to the classical Expectation-Maximization(EM) algorithm (Dempster et al., 1977).",
  "Complete Minimization": "We start by establishing in Theorem 1 that the version of BM2 where L(; ) is fully minimized at eachtraining step recovers the I-BM and DIPF iterations for two specific initialization choices of (F(), B()).The prior convergence results of Bortoli et al. (2021); Shi et al. (2023); Peluchetti (2023) (see the review of.1) toward S thus apply.",
  "for C1(B), C2(B) independent of F, D1(F), D2(F) independent of B, with 0 C1(B) C2(B) and 0 D1(F) D2(F)": "The losses Lf(f; b) and Lb(b; f) are easily amenable to optimization in their first arguments, as seen inAlgorithm 1. Lemma 2 relates these losses to more interpretable KL divergences between distributions. By(15), a decrease of Lf(f; b) due to a change in f corresponds to equivalent decreases of KL(M B0,1 F) fora fixed b, or B. Thus, partial minimization of Lf(f; b) brings F closer to M B0,1, the BM transport basedon B0,1, and the result of a complete minimization step, by means of reverse KL minimization. Symmetricconsiderations apply to Lb(b; f) as function of its first argument. Putting this result and Lemma 1 togetheryields Theorem 2. Theorem 2 (Partial BM2 Iterations). At each optimization step, decreases of Lf(; ) and Lb(; ) in correspond to equivalent decreases of KL(M B0,1() F()) and KL(M F0,1() B()). If the losses Lf(; )and Lb(; ) cannot be decreased in , i.e., at optimality, and if F() = B(), then F() = B() = S.",
  "Infinitesimal Minimization": "We conclude our theoretical investigation by relating our proposal to the work of Karimi et al. (2023),which introduces a continuous variant of the IPF procedure. In IPF, the two target marginal distributionsare replaced sequentially, one at a time. Each step corresponds to solving a static Schrdinger half-bridgeproblem (Lonard, 2014a), where in (2), C(0, 1) is replaced by either C(0, ) or C(, 1). The approachproposed by Karimi et al. (2023) retains either the even or odd steps of the IPF scheme while substitutingthe alternate steps with partial minimizations of the corresponding half-bridge problems. In the limit ofinfinitesimally small improvements, this yields a dynamical system for the evolution of the iterates overcontinuous algorithmic time. We demonstrate that a similar result can be obtained for a modified version of BM2, where forward KLdivergences are minimized instead of reverse KL divergences. The resulting dynamical system is a symmetrizedversion of the one obtained by Karimi et al. (2023). Let F , B represent the current state in the optimizationprocess. We consider a partial minimization of KL(F M B0,1), instead of KL(M B0,1 F), in F and apartial minimization of KL(B M F 0,1), instead of KL(M F 0,1 B), in B. As in Karimi et al. (2023), partialminimization is formulated as",
  "(19)": "In (19), KL( ) denotes the generalized KL divergence between unnormalized densities, as is the casehere for the second arguments, and the initial conditions f (0)1|0(x1|x0) and b(0)0|1(x0|x1) are determined by (F(), B()) with null drift terms. (19) can be contrasted with Karimi et al. (2023, Equation (13)). InAppendix C.1 we report a simple numerical application of (19) to the Gaussian setting, which recovers S.",
  "Numerical Experiments": "To evaluate the performance of BM2 on EOT problems, we utilize the benchmark developed by Gushchinet al. (2023). For the reference process (R), this benchmark provides pairs of target distributions 0, 1with analytical EOT solution S0,1 and analytical SB-optimal drift function s. We focus on the mixturesbenchmark, which consist of a centered Gaussian distribution as S0 = 0 and a mixture of 5 Gaussiandistributions for S1|0.S1 = 1 is not a mixture of Gaussian distributions, but has 5 distinct modes.The benchmark is constructed for dimensions d {2, 16, 64, 128} and entropic regularization parameters {0.1, 1, 10}.",
  "BW22(S1|0(X1|X0), P1|0(X1|X0))S0(dX0),(21)": "BW22(, ) is the squared Bures-Wasserstein distance, i.e. the squared Wasserstein-2 distance between(assumed) multivariate Gaussian distributions (Dowson & Landau, 1982), and VS[X1] is the varianceof X1 S1. We focus on the divergence KL(S P), rather than KL(P S), as a low KL(S P) more accurately indicatesthat P approximates S effectively across the entire support of S. The data-processing inequality impliesthat KL(S0,1 P0,1) KL(S P). The cBW22-UVP(, ) metric, introduced by Gushchin et al. (2023), is anormalized and conditional extension of the standard BW22(, ) distance. Results for evaluation metrics (20)and (21) are summarized in and , respectively. In our benchmarking, we compare BM2 against the I-BM and DIPF methods (.1). Each experiment isrepeated five times, including both model training and metric evaluation, to obtain uncertainty quantification.We use 1, 000 Monte Carlo samples to estimate (20, 21). For simplicity, we employ the EulerMaruyamascheme (Kloeden & Platen, 1992) with 200 discretization steps (t = 0.005) in all path sampling procedures.Each method undergoes 50, 000 SGD training steps with a batch size of 1, 000, settings similar to those usedby Gushchin et al. (2023), enabling qualitative comparison of our results with theirs. We use the AdamWoptimizer with a learning rate of 104 and hyperparameters: = (0.9, 0.999), = 108, wd = 0.01, where wddenotes weight decay. Time is sampled as t U(, 1 ) for = 0.0025.",
  ": Monte Carlo estimate of KL(S P) as function of and d, standard deviation in gray": "For BM2, we employ a single feedforward neural network with 3 layers of width 768 and ReLU activation,resulting in approximately 1 million parameters. We initialize the neural network parameters such that theresulting initial forward and backward drift functions evaluate to zero everywhere, a choice that has proveneffective in our experiments. As mentioned in .1, we implement path caching and an exponentialmoving average for parameters used in path sampling. The cache contains 5, 000 initial-terminal valuesfrom both (F()) and (B()), refreshed every 200 training steps. We omit reporting the results for boththe two-stage training procedure and the consistency loss described in .1, as neither approachdemonstrated consistent performance improvements across the considered benchmark. For I-BM and DIPF, each outer loop iteration comprises 5, 000 SGD steps, totaling 10 outer loop (algorithmic)iterations. Following best practices (Bortoli et al., 2021; Shi et al., 2023), we alternate time directions overiterations for both algorithms. Each method employs two separate neural networks for forward and backwardtime directions, maintaining a total parameter count close to 1 million, matching BM2s model size. As withBM2, we implement path caching (for DIPF, entire discretized paths are cached) and EMA for sampling,with an EMA decay rate of 0.99. We also consider BM2, a variant of BM2 that learns Schrdinger bridges for 0, 1 across multiple values.This amortized version leverages BM2s non-iterative nature. At each optimization step, is sampled fromU(0.1, 4) and utilized in discretizing SDEs (F(), B()) (lines 3 and 5 of Algorithm 1) and in bridge sampling(lines 7 and 8 of Algorithm 1). The neural network implementing drift functions f(x, t, ) and b(x, t, )is modified to accept as an additional input, resulting in conditional drift functions f(x, t, , ) andb(x, t, , ). Path caching is adjusted to store values corresponding to cached paths. In , we additionally include three baselines. EOT: sampling from the EOT solution, accounting forthe bias due to Monte Carlo estimation. SB(discr): sampling from the SB solution via the SB-optimal drifts, additionally accounting for EulerMaruyama scheme discretization error. 01: sampling from theindependent coupling. Results for additional discretization intervals are reported in Appendix C.2, and weillustrate in the evolution of (21) during training for a representative benchmark setting.",
  ": Monte Carlo estimate of cBW22-UVP(S0,1, P0,1) as function of and d, standard deviation in gray": "We now discuss the results presented in Tables 1 and 2. BM2 demonstrates superior overall performanceacross dimensions and entropic regularization settings in both metrics. I-BM also shows good performance,particularly in comparison to the DIPF procedure, which aligns with the findings of Shi et al. (2023). As expected, the performance of all methods deteriorates as the number of dimensions increases. This isbecause the metric(20) scales linearly with the number of dimensions, assuming a constant error rate inestimating each component of the true drift s. Similar considerations apply to the metric (21). While BM2 exhibits a performance gap compared to BM2, it yields reasonable results in low-dimensionalsettings (d = 2, 16). This gap may be due to increased pressure on model capacity or the need to normalizeloss levels across values. All methods perform poorly in the high regularization setting ( = 10), especiallyin high dimensions (d = 64, 128), which we include for completeness. It should be noted that, in such cases,sampling from the independent coupling (a trivial solution) is preferable to sampling from the SB-optimalSDE for the chosen discretization interval.",
  "Relevant works that, like BM2, address the dynamic Schrdinger bridge problem (1) include:": "Schrdinger Bridge Flow: in a concurrent work, De Bortoli et al. (2024) propose a non-iterativemethodology called -DSBM (Diffusion Schrdinger Bridge Matching). -DSBM is optimally implementedthrough a forward-backward SDE approach, in which case -DSBMs formulation and training objective alignwith our proposal (12, F(), B()), see De Bortoli et al. (2024, Equations (10) and (11)). The formulationof -DSBM begins by establishing a probability flow in the space of path probability measures, whosediscretization yields -IMF (Iterative Markovian Fitting). Under mild conditions, two theoretical resultsare established: (i) the -IMF iterates converge to the SB solution, and (ii) the non-parametric updates ofthe functional loss (14), obtained through functional gradient descent with respect to the drift functions,recover the -IMF iterates. The practical implementation, -DSBM, thus replaces non-parametric driftfunctions with parametric neural network approximators, and employs standard stochastic gradient descenton the parametric loss (12). This theoretical framework provides strong convergence guarantees for -DSBMand paves the way to further theoretical developments. Furthermore, De Bortoli et al. (2024) demonstratethe methods effectiveness through extensive numerical experiments on high-dimensional computer visionproblems, complementing our synthetic benchmark results. I-BM and DIPF: closely related to BM2 are the iterative, sample-based DIPF (Bortoli et al., 2021; Vargaset al., 2021) and I-BM (Shi et al., 2023; Peluchetti, 2023) procedures, which do not satisfy desiderata (i).Built on similar bridge matching principles, BM2 can be viewed as a modification of I-BM that employs asingle optimization loop, resulting in a simpler algorithm that we have empirically shown to be competitive.",
  "Conclusions": "In this work we introduced Coupled Bridge Matching (BM2), a novel approach for learning Schrdingerbridges from samples. BM2 builds on the principles of Bridge Matching while addressing key limitations ofexisting iterative methods. Our approach offers several advantages, including a simple single-loop optimizationprocedure, exactness in the idealized setting, modest memory requirements, and a straightforward loss function.The numerical experiments demonstrate that BM2 is competitive with and often outperforms existing iterativediffusion-based methods like I-BM and DIPF across various dimensions and entropic regularization settings. On the theoretical front, there is substantial room for improvement. Firstly, while bearing some resemblance tothe standard convergence result for the EM algorithm, Theorem 2 lacks a quantity analogous to the likelihoodbeing maximized in the EM algorithm. It remains unclear whether decreases in KL(M B0,1() F()) andKL(M F0,1() B()) can be linked to decreases in KL(F()S) and KL(B()S). Secondly, the requirementthat F() = B(), equivalently that (F()) and (B()) are time-reversals of each other, appears unnecessary.Notably, all numerical simulations conducted do not explicitly enforce this condition, which emerges naturallyduring the training process. Thirdly, it would be valuable to study problem (16) where reverse KL divergencesare partially minimized, aligning more closely with the BM2 algorithm. In this scenario, Lemma 4 nolonger holds, and it may be necessary to impose a corresponding additional constraint to maintain tractableanalytical computations. The attractors of (19), and of a corresponding dynamical system arising fromreverse KL minimization, can be investigated to assess further convergence properties of BM2. On the empirical front, the applications of BM2 in contemporary generative machine learning tasks remainunexplored. Given the promising results from previous studies employing Bridge Matching, such as those byLiu et al. (2023) and Somnath et al. (2023), it is anticipated that BM2 could be effectively applied to variousdomains, including image generation, audio synthesis, and molecular design. Future work could investigatethe scalability and performance of BM2 in these domains.",
  "Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion Schrdinger BridgeMatching. In Thirty-Seventh Conference on Neural Information Processing Systems, 2023": "Vignesh Ram Somnath, Matteo Pariset, Ya-Ping Hsieh, Maria Rodriguez Martinez, Andreas Krause, andCharlotte Bunne. Aligned diffusion Schrdinger bridges. In Robin J. Evans and Ilya Shpitser (eds.),Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 ofProceedings of Machine Learning Research, pp. 19851995. PMLR, 31 Jul04 Aug 2023. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-Based Generative Modeling through Stochastic Differential Equations. In International Conferenceon Learning Representations, 2021.",
  "tdWt,t ,(22)": "with 0, t : R>0 strictly positive and continuous. With bs:t := ts udu, 0 s t 1, t ischosen such that b0:1 = 1, to disentangle the contribution of t from the contribution of . Indeed, underthese conditions, t defines a time-warping: if Xt is the solution to (R), then Xb0:t has the same distributionas the solution to (22). Consequently, the solutions to (2) and (3) are independent of t.",
  "C.1Infinitesimal Minimization, Gaussian Case": "Consider the one-dimensional case d = 1, with target Gaussian marginal distributions 0 = N(0, 20) and1 = N(1, 21), and a reference diffusion distribution R associated with (R). In this setting, the solutionto the static Schrdinger bridge problem (2) is known analytically and is given by a bivariate Gaussiandistribution (Mallasto et al., 2022).",
  "parameters. Subsequently, we verify that the proposed functional forms for f (l)1|0(x1|x0) and b(l)0|1(x0|x1) indeedsolve (19)": "We examine the scenario where 0 = 2, 1 = 2, 0 = 1 = = 1. illustrates the evolution ofEF (l)[X1], VF (l)[X1], and CF (l)[X0, X1] over algorithmic time l. These quantities represent the mean andvariance of X1 and the covariance between X0 and X1 according to F (l), respectively. The correspondingvalues ES[X1], VS[X1], and CS[X0, X1] for the static Schrdinger bridge solution S0,1 from Mallasto et al.(2022) are depicted as dashed gray lines, demonstrating convergence.",
  "C.2Results for Additional Discretization Intervals": "In we report the results for metric (21) obtained by considering the BM2, I-BM and DIPF methodologiesfor different discretization intervals t = 1/T where T is the number of time-steps. For all methods weemploy the same number of time-steps at training and inference (testing) time. We recall that in time-steps have been employed to produce the results of Tables 1 and 2, and that we rely exclusively onthe EulerMaruyama discretization scheme."
}