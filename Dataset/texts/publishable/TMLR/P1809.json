{
  "Abstract": "Deep Reinforcement Learning proved efficient at learning universal control policies whenthe goal state is close enough to the starting state, or when the value function features fewdiscontinuities. But reaching goals that require long action sequences in complex environ-ments remains difficult. Drawing inspiration from the cognitive process which reuses learnedatomic skills in a global planning procedure, we propose an algorithm which encodes reach-ability between abstract goals as a graph, and produces plans in this goal space. Transitionsbetween goals rely on the exploitation of a learned policy which enjoys a property we calltranslation invariant local optimality, which encodes the intuition that goal-reaching skillscan be reused throughout the state space. Overall, our contribution permits solving largeand difficult navigation tasks, outperforming related methods from the literature.",
  "Introduction": "Model-free Reinforcement Learning (RL) has demonstrated an outstanding ability to learn complex optimalpolicies from raw interaction data, for well-defined atomic tasks involving relatively short time and state spaceoutreach, such as balancing a pendulum (Barto et al., 1983), learning to walk for a quadruped (Kimura et al.,2002), or learning to balance a bicycle (Randlv & Alstrm, 1998). But when it comes to solving more structured, long-term tasks, such as navigating through a building or amaze, baking a cake, or assembling furniture, hierarchical methods that separate global planning and locallearned controllers may appear more appropriate (Eysenbach et al., 2019; Levy et al., 2019; Ichter et al.,2020). It is notable that often (although not always), atomic tasks enjoy a property which we call translationalinvariance, ie. learned policies can be re-used in unexplored states, without further computation, to reachlocal goals. Balancing a bicycle, for instance, implies in practice an optimal policy that recommends thesame sequences of actions regardless of the geographical position, mostly because gravity does not changetoo much across the globe and that we ride bicycles on surfaces that have close enough friction properties.Similarly, when navigating in a reasonably homogeneous environment, reaching position B from position A,can be achieved by applying the same policy than reaching position B + from position A + , providedthere are no obstacles in the way. The optimal policies might somehow differ, but are close enough in manypractical cases. In this paper, we consider environments which enjoy this translational invariance propertyfor local, atomic goal-reaching tasks. In such tasks, this invariance property enables pre-training actionprimitives under the form of local goal-reaching policies, and then transferring and composing them to solve",
  "Published in Transactions on Machine Learning Research (/)": "from the first 100 houses actually reasonably covers the set of possible images and that the pre-trained valuefunction is a good estimator of can I reach g from s in less than 3 actions?, even for images from differenthouses, that were not seen during training. From this perspective, SoRBs value function in this precise case captures the notion of translation invariancewithout stating it explicitly. Once this is made clear, then what SoRB really does is sample states froman oracle (the reset function in the environment) and connects them using this value function, while SGMbecomes almost equivalent to PO-RGL: it builds the same graph as SoRB and then prunes it. What enablesSoRB and SGM to perform this graph building is precisely because the value function was trained on richenough data to be able to evaluate distances between images sampled from new houses.Without thisimplicitly-TILO property, they wouldnt be able to build this graph reliably. In conclusion, the exploitation of translation invariance in RGL can be interpreted as a principled general-ization of the previous example: it permits pre-training the lower-level policy in a playground (such as the100 first houses of the SUNCG dataset) and transferring the pre-trained value function to unseen states inanother MDP, where the TILO property ensures to retain local optimality of policies. In turn, this decou-ples, in a principled way, learning lower-level skills (like walking in ant-maze, regardless of locally specificsurrounding obstacles) from chaining skills and navigating.",
  "We formalize the notion of re-usability of a goal-reaching policy throughout the state space as oneof translation invariance": "We propose a complete graph-based model learning method, which relies on planning in the goalspace, and chains local application of translation invariant goal-reaching policies. By combiningplanning and RL, this method permits solving tasks over long horizons, a common pitfall for classicalRL methods. As such, the proposed algorithm owes to several different inspirations. First, it belongs to the family ofgoal-based RL methods. Since it couples planning and RL, it also connects with hierarchical RL. It alsoabstracts navigation problems into a more generic class of sequential decision problems. Finally, it presentsmany similarities with the Search on the Replay Buffer (SoRB) algorithm (Eysenbach et al., 2019) andsubsequent works, with several key differences which can be seen as a generalization of SoRB and permitbetter applicability. sets the necessary background and puts our contribution in perspective of thecurrent related literature. introduces key ingredients, namely a formal definition of goals as stateabstractions, a characterization of policy translation invariance, and finally the reachability graph learning(RGL) procedure. Sections 4, 5, and 6 evaluate RGL empirically, assess the contributions of its differentcomponents, discusses its properties and its ability to scale up to large and difficult domains. Finally, in we draw some conclusions and perspectives on the presented work.",
  "Background and related work": "Goals in Reinforcement Learning (RL). RL (Sutton & Barto, 2018) considers the problem of learning anoptimal decision making policy for an agent interacting over multiple time steps with a dynamic environment,modeled as a Markov Decision Process (Puterman, 2014) of unknown transition and reward models. At eachtime step, the agent and the environment are described through a state s S. When an action a A isperformed, the system transitions to a new state s, while receiving a reward r(s, a). Stochastic ShortestPath problems are a particular class of MDPs which aim at reaching a terminal goal state as quickly aspossible. Such problems can be encoded as MDPs featuring 1 rewards for all transitions but those to aterminal goal state. One can quantify the efficiency of a policy : S A in every state s S via its valuefunction V (s) = t=0 tr (st, (st)), with [0, 1) a discount factor on future rewards (which can also beinterpreted as a stepwise probability of non-termination).1 Training an RL agent consists of finding a policywith the highest possible value function. A long-standing goal in RL is to design multi-purpose agents, ableto achieve various goals through a single goal-conditioned policy (s, g) (Kaelbling, 1993), where the goalg is either a single state in S or an abstraction for a set of states. The ability of deep neural networks toapproximate complex functions has triggered a renewal of interest in learning universal value function andpolicy approximators (Schaul et al., 2015), V (s, g) and (s, g) respectively. Among the many approachesdeveloped to learn goal-based policies and value functions, Hindsight Experience Replay (Andrychowiczet al., 2017, HER) proposes a seminal method which defines goal-based reward functions by re-labellingstates collected in past trajectories as goals. Hierarchical RL (HRL). Combining local goal-reaching sequences of actions in order to achieve a moregeneral goal is the core idea of HRL (Sutton et al., 1999; Precup, 2000; Konidaris & Barto, 2009). Notably,among recent works, Kulkarni et al. (2016) define a bi-level hierarchical policy, using a DQN (Mnih et al.,",
  "hence providing an abstraction of navigation features into a more general framework. Note however thatnone of these methods provide a generic and formal definition of policy re-useablity": "Originality of the present work. With respect to this general body of work, our contribution has severalkey features. We formalize a set of necessary conditions defining a context which alleviates the need totrain the lower-level goal-conditioned policy on all states and goals, and enables policy re-use. Similarlyto SoRB, we exploit the policys value function as a local reachability measure, while introducing a level ofabstraction since we clearly distinguish between goals and states. Similarly to DSG, we incrementally explorethe environment and build a planning graph for chaining local skills, but the pre-training, exploration andgraph growth strategies are different (expanded discussion in Appendix I). As developed in the next sections,this provides a sparser, abstract planning graph, closer to a hierarchy of options. Also in contrast to SoRBand SGM, we do not rely on a pre-existing replay buffer and avoid defining nodes over an arbitrary subsetof sampled states; instead we incrementally grow a reachability graph to cover the attainable goal space.",
  "Learning a reachability graph to chain translation invariant local policies": "We aim to solve large goal-based MDPs, requiring long sequences of actions, and where locally trainedpolicies can be re-used throughout the state space. The rationale for the method we propose below goesas follows. Because they are continuous universal approximators, neural networks are intrinsically unsuitedto approximate abrupt environment of skill changes, and discontinuous functions such as the value func-tions arising in some difficult RL environments (e.g. mazes, non-holonomous robots, etc.). For instance, inmazes featuring thin walls, the value function is discontinuous and approximating it with a neural networkpropagates values through the walls which can lead to catastrophically bad greedy policies. Because neuralnetwork optimization assumes that samples are obtained independently and identically from a stationarydistribution, they are also unsuited to retain local information: either because the distribution (and hencethe training set) is unbalanced or because of distributional shift which causes catastrophic forgetting (French,1999; Kirkpatrick et al., 2017). The sequential nature of RL decisions makes it crucial to make good deci-sions in infrequently-visited states, to retain local information even when facing distributional shift, and toapproximate some functions that can easily be discontinuous. Neural networks are a viable policy class forlow-level policies and that it may be advantageous to add additional known structure to the problem setting(which can be learned) rather than attempting to learn a complete world model with no additional assump-tions which may require high sample-complexity. Elaborating on this statement, we turn to a hierarchy ofapproximators, coupling planning in a graph of goal space waypoints, with local goal-reaching skills learnedwith deep neural networks. When we would like to reach a goal g G from a state s0 S, we link g and s0 to their closest graph nodes. Specifically, we find vertices v and v0 whose waypoints gv and gv0minimize some measure of proximity d(g, gv) and d(P(s0), gv0) respectively, with P(s0) an abstraction ofs0 in the goal space. Then we find a shortest path between them in the graph, which defines an executioncurriculum of waypoints, and the local policy is used to reach each waypoints vicinity in sequence. The core of our contribution lies hence in the graph expansion and pruning method, its ability to accuratelyrepresent an abstraction of the environment dynamics despite unbalanced samples and discontinuous prop-erties, and finally its use to design goal-conditioned policies over large and complex state spaces. To presentthe method in a well-defined framework, we restrict the set of MDPs we consider to those enjoying a prop-erty we call translation invariance of local optimal policies which we discuss in section 3.2. We also discusstherein to what extent this assumption is a strong constraint and how it can be related to more generaltransformations than translations. Then, given such a generalist goal-reaching policy , we grow and prunea graph G which encodes an abstract notion of reachability and distance over the state space (.3).The pair (, G) can then be used jointly to encode a policy that benefits from the best of both worlds andallows one to exploit planning algorithms over G in order to define an execution curriculum of waypoints for; resulting in a global agent that can reliably learn to reach distant goals in complex environments.",
  "Translation invariance of local optimal policies: re-using macro-actions across all states": "Intuition indicates that a four-legged robot should not have to learn to walk again when it is moved from aroom of the lab to another. We formalize this notion of re-usability of learned policies as one of translationalinvariance.3 We say an MDP admits translation invariant local optimal policies (TILO policies) if thereexists a goal-conditioned optimal policy such that",
  "(s, ) S S, R, such that g B (P(s), ) , (s, g) = (s + , g + P()) ,(1)": "where B(P(s), ) is a ball, centered in P(s) and of radius . In plain words, such a policy guaranteesthat whichever close enough starting states s and s + we consider, we can always find local goals g andg + P() within a distance of P(s), for which the first action recommended by the policy will be thesame. A corollary of this property is that in deterministic MDPs, all actions taken to reach g from s arethe same as those necessary to reach g + P() from s + , for goals that are close enough to P(s). TILOpolicies can be trained to reach goals from any starting state, and the TILO property enables their re-usability throughout the state space to reach local goals (which marks a notable difference with the relativegoal policies introduced by Nachum et al. (2018) among others). Note that considering only translationinvariance is somehow restrictive as one could wish to identify invariances to other transformations (e.g.deformations on images, rotations, etc.) or the more general case of equivariance (Van der Pol et al., 2020;Mondal et al., 2022; Wang et al., 2022) which is a challenge in itself (Konidaris & Barto, 2007; Faust et al.,2018; Chiang et al., 2019) and which we reserve for future work. Appendix E discusses such a generalization. Arguably, MDPs that admit TILO policies do not represent the full span of MDPs. However we argue thatwith an appropriate choice of the metric on G, defining B(P(s), ), this property actually applies to manycommon control problems where the goal space supports an addition operation. Moreover, one can extendthe reasoning to -optimal policies, hence defining -TILO policies. An MDP admits -TILO policies if thereexists a policy that is -optimal and obeys equation 1. When one takes the four-legged robot that has onlybeen trained on the labs concrete grounds, to some other surface, it is reasonable to assume its translatedgoal-conditioned policy will not be optimal anymore. However, this policy is still likely to perform betterthan most other policies and hence to be -TILO. The method we develop herein applies to MDPs which admit -TILO policies that are pre-trained. Practi-cally, given a starting state s, we directly train a translation invariant goal-conditioned policy (s, g). Toenforce the TILO property, we actually train the policy as a function of the difference gs (or P(g)s whenG = S). Training of this policy is done before directed exploration and graph learning takes place. We alsodefine a goal-proximity quasi-metric d(g, g) = (Vmax V ( P(g), g))/(Vmax Vmin), indicating how closetwo goals are under policy , with Vmax and Vmin chosen so that, on the training domain, d(g, g)",
  "return V, E": "To ease the presentation of ideas, we present the proposed Reachability Graph Learning algorithm (RGL,Algorithm 1) in the context of deterministic MDPs, and defer the discussion of the stochastic case to theend of this section. Given a pre-trained -TILO policy , we wish to construct an oriented graph G = (V, E)which will represent the reachability between sub-goals, using . Each vertex v V of such a graph isassociated with a given goal gv, and directed edges e E indicate reachability of the successor nodes goalfrom the states corresponding to the source nodes goal. In other words, if an edge exists between v andw, then successfully reaches gw from states in Kv = Kgv. The edge linking v and w is weighted witha traversal cost of d(gv, gw). Knowledge of this weighted graph permits running a planning algorithm to",
  "Experimental setup": "To highlight the behavior of RGL, and provide a fair and interpretable benchmark against comparablemethods, we consider a set of navigation tasks in mazes, including a high-dimensional state space one. Environments. In each maze, an agent should be able to reach any goal position from its starting point.Hence the goal space G is always the 2-dimensional set of positions in the maze. We consider mazes ofdifferent complexities, with various map sizes and heterogeneous corridor widths, as illustrated in .Namely, four-rooms is a 41 41-size maze resembling the classical four-rooms benchmark, medium isa more challenging maze of the same size, hard is an even more challenging 5757-size maze and mixedhas the same size as hard and mixes corridors and rooms of different sizes. Note that compared to mazesused in the literature (e.g. those of Eysenbach et al. (2019)), here the walls are thin, inducing sharperdiscontinuities in the value function across a wall. For each map, we consider three different dynamics andstate spaces for the navigating agent, which we refer to as grid-maze, point-maze and ant-maze ().Grid-maze features a discrete {N, S, E, W} action space and deterministic transitions which perform unit-length moves, hence emulating navigation on a grid. Point-maze emulates a point mass moving freely in themaze. It has a continuous, two-dimensional action space of position increments in on the x and yaxes. The transitions are stochastic due to an added Gaussian noise N(0, 1). Contrarily to grid-maze whichhas a fixed starting state, point-maze randomly draws the starting point at every episode. In grid-maze andpoint-maze environments, the state space is simply described by the geographical position of the agent, asin the benchmarks of SoRB (Eysenbach et al., 2019) or SGM (Emmons et al., 2020), and S = G. Ant-mazeenvironments build upon the MuJoCo Ant simulator (Todorov et al., 2012) and sets the ant in one of thenavigation maps. Actions belong to the standard 8-dimensional action space of the Ant simulator, and thestate space is the 29-dimensional space whose first two coordinates are the position of the ants torso, asin the benchmarks of HAC (Levy et al., 2019) or Distop (Aubret et al., 2021). The transitions follow the",
  ": Mazes and starting points. From left to right: four-rooms, medium (size 41 41), hard, mixed(size 57 57)": "dynamics of the Ant simulator. In ant-maze environments, the goal space G describes only the torsos x andy coordinates. In all environments, agents receive a 1 reward at each time step, unless they reach the goalwhich terminates the episode. In all evaluations, every agent is independently trained 10 times. For the sakeof reproducibility, the hyperparameters for all algorithms are summarized in Appendix A, and our code andresults are available at [anonymous URL].",
  "grid-maze22discrete (number = 4)deterministicfixed single statepoint-maze22continuous (dim(A) = 2)stochasticuniform distribution on Sant-maze292continuous (dim(A) = 8)deterministicfixed single state": "Baselines. To illustrate the behavior of RGL, we compare against a plain DQN agent (Mnih et al., 2015)with HER in grid-maze environments, and SAC (Haarnoja et al., 2018) with HER in point-maze and ant-maze environments.As illustrated by previous works (Nachum et al., 2018; Levy et al., 2019), such acombination can efficiently learn a goal-reaching policy for goals lying a few actions away from the startingstate, but struggles to reach goals that require turning around walls. These DQN+HER and SAC+HERagents provide a baseline for performance. Another baseline consists in passing the policy learned by thesebase agents, along with their final replay buffer, to SoRB, to extend their outreach throughout the goal spacevia planning in a random subset of size Ninit of the replay buffer. Since SGM is more efficient than SoRB(due to their pruning method), we directly compare with SGM.6 Ablations.We also implement three variants of RGL. The first one is called TC-RGL, inspired by theSTC method (Ruan et al., 2022), where we replace the d pseudo-metric by a so-called temporal correlationnetwork, which is an additional network trained to measure reachability between states, based on theirtemporal proximity during training trajectories. This variant permits evaluating the core feature of STC asan alternative to using the value function as a reachability metric between goals. The second one is calledPrune-only RGL (PO-RGL). This version samples n states from S using an oracle, and sets them as graphnodes. Edges are computed in the same way than RGL. This algorithm does not need graph extension, andsimply prunes the graph as it successively tries to reach random goals. Finally, NoPG-RGL (no playground)is an RGL agent whose lower-level policy has not been pre-trained in an obstacle-free playground but in thetrue maze, to illustrate the influence of playground environments on the pre-trained policys quality. Pretraining. First of all, it is important to note that only RGL can, by design, exploit a goal-reaching policywhich has not been trained on the actual full maze (further discussion in Appendix J). This enables trainingRGLs goal-reaching policy in a 40 40 playground environment with no walls which facilitates pre-training(training in the full maze, as done with NoPG-RGL, is still possible but totally unnecessary since RGLsgoal-reaching policy needs only navigate to local goals). Such a pre-training is not compatible with SGM or 6Note that SoRB and SGM were introduced with tailor-made, goal-based, distributional DQN and DDPG agents. We foundthis was not necessary for finite-length trajectories and we retain the names even though we slightly change the base agents.",
  "This section illustrates how RGL dynamically grows and prunes its graph, illustrates the comparative im-portance of each part of the algorithm, and reports on its ability to reach far-standing goals": "Visualizing graph growth and pruning. We start by assessing separately the influence of the growthand pruning procedures on the properties of the final reachability graph. To isolate the effect of pruning, weartificially generate waypoints by using a generative model to draw states from the full state space, whichyields a graph with the same number of nodes Ninit as the SGM agents (edges weights are also initializedwith d), but with better state space coverage since the drawn states are not constrained by the explorationof the pre-trained DQN+HER agent. This graph is illustrated in Figures 2a and 2e for the four-rooms andhard grid-mazes (full results in Appendix B). In , red edges are those whose weights have beenset to + by the pruning procedure. We observe that (as anticipated in ) only erroneous edgeswhich were selected in a shortest path are pruned, and some remain in the graph, especially in grid-maze,which features a fixed unique starting state. This bears little consequences in terms of goal reachability sincethese are never selected in shortest paths from the initial state, but still result in a rather dense reachabilitygraph. Conversely, Figures 2b-2d and 2f-2h illustrate the graph growth and the pruning for RGL in the sameenvironment after respectively 1,000, 40,000 and 270,000 interaction steps. To avoid misinterpretations, it isimportant to note that since the graph is oriented, each green edge in these figures actually stands for twoedges in the graph. If only one has been pruned and rendering of the other happens afterwards, the segmentappears green while only one edge in the graph has non-infinite weight. Overall, the incremental growth ofRGLs graph yields an efficient coverage of the state space, avoiding the clusters of unnecessary nodes ofFigures 2a and 2e, and reducing the need for pruning.",
  ": Final average score for each method (standard deviation in parenthesis). Take-away: RGL statisticallyoutperforms non-dynamic graph methods": "across mazes. Interestingly, RGL produces graphs with less connectivity, which can be interpreted as a betterability to create meaningful connections between goal waypoints for navigation, and hence sparser abstractmodels for planning while retaining the useful information for plan efficiency. Additionally, TC-RGL featuresa large variance in the number of nodes and edges developed in the graph. This appears to stem from thetraining of the temporal consistency network which is very sensitive to the distribution of trajectories duringpre-training. In turn, this strongly affects the estimation of reachability when learning the graph and inducesthis variance in graph density. Appendix F provides further discussion on the impact of the graph densityshyperparameters (node and edge) on RGLs behavior.",
  "Scaling up RGL to more difficult environments": "The previous section demonstrated the overall behavior of RGL and its ability to build a more relevantplanning graph than similar methods. In this section, we consider additional challenges and evaluate RGLin other contexts. Namely, we explore how RGL compares to other approaches when the environment isnot constrained to start in the same states every time. We also evaluate how RGL can cope with stochastictransition dynamics. Finally, we scale up RGL to the large state space of MuJoCos ant simulator and",
  "demonstrate its behavior when the state space is much larger than the goal space, while comparing itsbehavior to state-of-the-art competitors HAC (Levy et al., 2019) and DSG (Bagaria et al., 2021)": "Easier exploration in environments with the ability to reset anywhere. Point-maze environmentspermit resetting the environment anywhere in the state space at the beginning of each episode, as in thebenchmarks of Eysenbach et al. (2019); Emmons et al. (2020). This feature induces diversity in the replaybuffers by triggering easier exploration of the state space, simply by enabling these random resets, andsomehow departs from the more constrained RL framework with a fixed (or a limited set of) starting state(s).Consequently, these environments are more favourable to SGM since their replay buffer covers a larger portionof the state space, and SGM performs better in these environments than in grid-maze ones. comparesthe behavior of all methods in the point-maze environments and illustrates how, even in this case, the graphgrowth of RGL eventually outperforms competing methods, as it dynamically and progressively discoversnew goal waypoints to better map the state space. : Accuracy for all agents on point-maze environments, versus interaction steps. Take-away: the ability toreset anywhere during training strongly helps SGMs coverage by design, but the dynamic graph growth of RGL andits variants still enables more efficient overall goal-reaching behaviors.",
  "Four roomsMediumHardMixed": "RGL (ours)0.91(0.1)0.85(0.24)0.77(0.2)0.67(0.23)NoPG-RGL0.94(0.05)0.87(0.05)0.7(0.15)0.61(0.15)TC-RGL0.82(0.17)0.68(0.27)0.54(0.31)0.64(0.19)PO-RGL0.87(0.18)0.7(0.18)0.48(0.2)0.64(0.17)SGM0.88(0.09)0.79(0.13)0.43(0.24)0.34(0.13)SAC + HER0.54(0.1)0.23(0.06)0.11(0.05)0.11(0.05) : Final average score for each method (standard deviation in parenthesis). Take-away: RGL statisticallyoutperforms non-dynamic graph methods, and methods that are unable to expand their graph after pre-training. Accommodating stochastic transition models. As mentioned in .3, RGL in its presentedversion is designed for deterministic dynamics and would require some adaptations to account for transitionuncertainty. Point-maze environments feature a rather high level of action noise ( = 1 for action values in). This makes the pruning procedure stochastic, as it will prune out edges depending of a single trialssuccess. Despite this rather naive behavior, RGL still manages to find paths (possibly sub-optimal) to goalsand reaches a high level of accuracy () demonstrating a reasonable level of robustness to transitionstochasticity. Scaling-up to high-dimensional and large state spaces, where G = S: ant-maze tasks. A keyachievement of deep RL is its ability to scale up to high-dimensional state spaces, either in continuouscontrol (e.g. MuJoCo robotic environments) or in image-based tasks (e.g. video games). In this last part, weconsider such environments, where RGL retains the same overall behavior, as its planning graph is definedover the goal state G and not the state space. The key challenge in these environments is therefore to derivea good lower-level goal-reaching control policy (s, g), which is independent of the contribution of this paperand may require a large engineering effort, unrelated to RGL itself. In other words, the lower level policy inthe Ant environment should learn to walk to any nearby position around its starting position, while RGLwill enable it to navigate to distant positions in the maze.",
  ": Final average score for each method (standard deviation in parenthesis). Take-away: RGL statisticallyoutperforms all baselines": "In comparison to the ant-maze environments used in Bagaria et al. (2021), the maps used here are larger.In turn, this makes reaching specific areas (goals) of the maze more difficult to reach. Besides, even whentaking into account the pre-training time of RGLs lower-level policy, (and the original DSG paper)shows that DSG requires many more learning steps to correctly learn the individual options correspondingto the vertices in its planning graph. In contrast, RGL leverages the possibility to re-use the same TILOpolicy throughout the state space, and quickly grows the planning graph while leaving the lower-level policyunchanged after the first 20, 000 time steps.Additionally, DSG relies quite strongly on dense reward modelsfor efficient training of options. The present environments avoid the (dense) reward shaping mechanism usedin Bagaria et al. (2021)s experiments, to avoid pre-training biases, which strongly penalizes DSG. Appendix Itest DSG in various Ant-Maze settings to analyse the reasons of DSG under-performances in the currentcontext. Finally, ant-maze tasks, on top of being highly challenging for the baseline agent, also violate the assumptionof .1 that any two states within Kg are reachable from each other for a negligible cost.Forinstance, some ant orientations, velocities and leg configurations are rather complex to reach from others.Consequently, an edge between g and g in RGLs graph only represents reachability of g from a subsetof states in Kg, which can lead to plan failure (extended discussion in Appendix B). Despite this, RGLmanages to achieve large values of accuracy almost as high as those obtained on point-maze tasks on thefour-rooms, medium and mixed mazes. The most challenging setting remains the hard maze, whichrequires fine motor skills to efficiently navigate through narrow corridors and requires turning around manycorners to navigate to far goals. High-dimensional goal spaces. The question of scaling up RGL to large goal spaces arises naturally as afollow-up to the previous paragraphs. For instance, when extending RGL from navigation to manipulationtasks, goal space dimension might increase and RGLs graph growth will suffer from the curse of dimension-ality. This issue arises for all methods which design a graph in goal space; for instance, SGM or SoRB will",
  "Conclusion": "Efficient coupling of planning and learning in complex MDPs with temporally extended goals is an activefield of research. In this work we defend the idea that efficient mechanisms rely on two implicit hypotheses:planning agents should plan in the goal space and learned policies are often re-usable throughout the statespace. We propose a formal framework accounting for these two notions, defining goals as state abstractions,and casting re-usability as translation invariance. This permits deriving an algorithm which performs plan-ning over a graph of goal waypoints, reachable by a lower level goal-reaching policy. This agent is namedRGL (reachability graph learning) after its training mechanism, which incrementally explores its environ-ment using a pre-trained lower level translation invariant goal-reaching policy, expands and prunes a graphencoding goal reachability. This approach can be seen either as a more grounded version of STC (Ruan et al.,2022), or a generalization of SoRB (Eysenbach et al., 2019) or SGM (Emmons et al., 2020) to a hierarchicalsetting with translation invariance. Empirical evaluation confirms the relevance of RGL agents and their keyfeatures. This contribution also forms a basis for future research directions. As is, RGL agents build a somewhatuniformly dense graph, as illustrated in h. This might not be necessary and further sparsity can beachieved in the open rooms of the four-rooms or the mixed environments. Similarly, weight learning inthe graph is currently rather naive and could better exploit interaction data during exploration, in particularin stochastic environments.The influence of playground environments on the final policies is also an openquestion, which connects to lifelong RL and curriculum learning: can one design good playgrounds for pre-training, the same way we let kids learn generic skills from simple (playground) tasks before we encouragethem to compose these solutions together? Finally, RGL requires an -TILO policy for agent control.Appendix D and E propose a discussion onwhether such policies are easy to obtain in the general case, beyond navigation tasks. We conjecture suchpolicies also exist in more complex contexts, like vision-based navigation (PO)MDPs, such as those proposedby Kempka et al. (2016) or for real-life robots, since humans seem to exploit such invariances in daily life.Formalizing how these policies can be discovered and how their definition affects the properties derived inthe present work is an exciting avenue for research. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew,Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Advances inneural information processing systems, 30, 2017. Giorgio Angelotti, Nicolas Drougard, and Caroline PC Chanel. Expert-guided symmetry detection in markovdecision processes. In Proceedings of the 14th International Conference on Agents and Artificial Intelli-gence, 2022.",
  "Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation.In International Conference on Learning Representations, 2019": "Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav Gupta, and Saurabh Gupta. Neural topologicalslam for visual navigation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1287512884, 2020. Hao-Tien Lewis Chiang, Jasmine Hsu, Marek Fiser, Lydia Tapia, and Aleksandra Faust. Rl-rrt: Kinodynamicmotion planning via learning reachability estimators from rl policies. IEEE Robotics and AutomationLetters, 4(4):42984305, 2019.",
  "Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.Search on the replay buffer: Bridgingplanning and reinforcement learning. arXiv preprint arXiv:1906.05253, 2019": "Aleksandra Faust, Kenneth Oslund, Oscar Ramirez, Anthony Francis, Lydia Tapia, Marek Fiser, and JamesDavidson. Prm-rl: Long-range robotic navigation tasks by combining reinforcement learning and sampling-based planning. In 2018 IEEE international conference on robotics and automation (ICRA), pp. 51135120.IEEE, 2018. Sbastien Forestier, Rmy Portelas, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goalexploration processes with automatic curriculum learning. The Journal of Machine Learning Research, 23(1):68186858, 2022.",
  "Leslie Pack Kaelbling. Learning to achieve goals. In IJCAI, volume 2, pp. 10948. Citeseer, 1993": "Lydia E Kavraki, Petr Svestka, J-C Latombe, and Mark H Overmars. Probabilistic roadmaps for pathplanning in high-dimensional configuration spaces. IEEE transactions on Robotics and Automation, 12(4):566580, 1996. Micha Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Jakowski. Vizdoom: Adoom-based ai research platform for visual reinforcement learning. In 2016 IEEE conference on computa-tional intelligence and games (CIG), pp. 18. IEEE, 2016. Hajime Kimura, Toru Yamashita, and Shigenobu Kobayashi. Reinforcement learning of walking behavior fora four-legged robot. IEEJ Transactions on Electronics, Information and Systems, 122(3):330337, 2002.",
  "Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko.Learning multi-level hierarchies withhindsight. In International Conference on Learning Representations, 2019": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. nature, 518(7540):529533, 2015. Arnab Kumar Mondal, Vineet Jain, Kaleem Siddiqi, and Siamak Ravanbakhsh. EqR: Equivariant represen-tations for data-efficient reinforcement learning. In International Conference on Machine Learning, pp.1590815926. PMLR, 2022.",
  "Soroush Nasiriany, Vitchyr H Pong, Steven Lin, and Sergey Levine. Planning with goal-conditioned policies.arXiv preprint arXiv:1911.08453, 2019": "Giambattista Parascandolo, Lars Buesing, Josh Merel, Leonard Hasenclever, John Aslanides, Jessica BHamrick, Nicolas Heess, Alexander Neitz, and Theophane Weber. Divide-and-conquer monte carlo treesearch for goal-directed planning. arXiv preprint arXiv:2004.11410, 2020. Alexandre Pr, Sbastien Forestier, Olivier Sigaud, and Pierre-Yves Oudeyer. Unsupervised learning of goalspaces for intrinsically motivated goal exploration. In International Conference on Learning Representa-tions, 2018.",
  "AHyperparameters and computational setup": "Tables 5 to 12 summarize the hyperparameters used when training the different algorithms.The actornetwork used for the lower level goal-reaching policy takes a state and a goal as input (the dimension variesdepending on the task) and processes them through a 2 hidden layer MLP. The output layer depends onthe algorithm. Training follows the procedure of DQN and HER with discount factor and exponentialsmoothing on the target network (factor ), and an Adam (Kingma & Ba, 2015) optimizer with defaultparameters. These parameters are the same for pretraining lower level policies for all algorithms. All RGLagents share the same Te, node, edge, and reach when applicable (for instance, PO-RGL uses edge but notnode since it does not create new nodes). TC-RGL uses specific values node and edge for thresholds onnode and edge creation, since it uses STCs temporal consistency network to measure node distance insteadof our d pseudo-metric; the scale of this networks output is unrelated to that of d (hence the differentvalues of node and edge). The results on grid-maze and point-maze were run on a desktop machine (Intel i9, 10th generation processor,64GB RAM) with no GPU usage. The results on ant-maze were obtained with single node computations.Each of these nodes was composed of 2 12-core Skylake Intel(R) Xeon(R) Gold 6126 2.6 GHz CPUs with 96Go of RAM (no GPU hardware).",
  "BComplete results on reachability graphs": "Figures 8, 9 and 10 present an extended version of the reachability graphs of figure 2 for all mazes in,respectively, grid-maze, point-maze and ant-maze environments. Blue dots in some figures correspond to thecurrent selected goal at the time the graph was printed and should be discarded. PO-RGL was created purely for didactic reasons in order to illustrate the pruning process independentlyof the incremental graph growth. Besides this illustration itself, these figures underline two features. First,the fact that RGL creates the graph incrementally makes it much sparser and avoids clusters of really close,redundant nodes. In turn, this sparse graph is much easier to prune than that of PO-RGL. Secondly, inenvironments with a fixed initial state (grid-maze, ant-maze), some edges never participate in the shortestpath to any goal and hence are never pruned.Even if the sparse growth of the RGL graph limits thisphenomenon, some impassable edges remain; e.g.some edges at the far right of figure 8p.Randomlyresetting the starting state at each episode permits a more complete and easier exploration of all shortestpaths, and hence results in a slightly more accurate pruning; e.g. the unpruned edges in grid-maze are betterpruned on figure 9p. (ant-maze environments) deserves a few additional comments. On this graph, to ease the readabilityand account for directed edges, whenever a directed edge exists between v and w, we plot the edges segmentclosest to v in green. Orange then means the reverse edge has not been created. Red means the edge has beenpruned. Some pruned edges appear in areas which seem passable. To explain this phenomenon, one needs torecall that the state space is 29-dimensional and a waypoint in the goal space (a geographical position of theants center of mass) can stand for a wide variety of configurations, as discussed in . For any twonodes g and g, it is possible that g was reachable fromP(g) but is not reachable from some other states inKg, since ant-maze environments violate the hypothesis that all states in Kg are reachable from each otherfor a negligible cost given the pre-trained policy. This leads to some edges being legitimately pruned whilea naive eye laid on the reachability graph might conclude there was a mistake. Finally, the graphs grown by RGL in ant-maze environments feature very few edges crossing walls. This is aside effect of the default values of node and edge (kept the same throughout all environments and mazes),and the fact that the ants geometry prevents its center of mass to get close to the wall. This sometimeshappens nonetheless when the ant randomly \"tries\" to climb over the wall (and systematically fails), whichalso places a few nodes that appear to be inside the walls.",
  "four rooms: 400medium: 500hard: 700mixed: 900": ": TC-RGL hyper-parameters. Hyper-parameters that are not reported here are the same that the ones in for RGL. targeted edge length is the minimum number of interactions that must separate two states of thesame trajectory, so that they can form a positive pair (distant states) in the TC-network training data.",
  "CComplete results on graph sizes": "Figures 12 and 15 complement Figures 4 and 5 by reporting the evolution of nodes/edges count acrossinteraction time steps in point-maze environments. Note that graphs on point-maze environments requireda log-scale on the y-axis for readability since TC-RGL spanned an order of magnitude more nodes than RGL(and two to three orders of magnitude more edges).",
  "In the present work, an important assumption is the existence of an -TILO policy. Thus it seems importantto discuss how restrictive this assumption is, and how commonly such policies might occur": "In position-based navigation tasks where S = G, generalization by translation invariance seems intuitiveand easily justified by the translation invariance of the MDPs transition model properties throughout thestate space. In navigation tasks where the state space is the agents full configuration, but with abstractgoal spaces (e.g. agent overall position), such as the ant-maze benchmarks, finding TILO policies is closelylinked to defining the goal space, and hence the P : S G projection. In this specific example, P is definedby simply keeping some variables of S and discarding the others. Here again, the TILO property is intuitiveand translation invariance permits generalizing learned policies to unexplored parts of the state and goalspaces. However, when it comes to state spaces with confounding variables, such as visual navigation tasks, thendefining P for abstract goal spaces might become more difficult as it links the input image pixels to positionson the navigation map. In a way, P encodes expert knowledge about what abstractions of the state definea useful goal space, as discussed for instance by Forestier et al. (2022). Such abstractions might be learned(Pr et al., 2018) but since they are a pre-requisite for training a goal-based policy, they are generallyconsidered to be provided by some expert. Such a description of goals is sometimes accessible for a minimalcost (as in navigation tasks), but a perspective for future work implies learning relevant goal descriptorsfrom data. One one can draw a parallel with recent work in expressing goals with natural language andexploiting (large) language models to embed the goal description. Note, however, that in the general case,even if the corresponding P encoding is given, there is no guarantee that a TILO policy exists.",
  ": Reachability graphs, grid-mazes": "Besides the considerations above, we argue that the existence of TILO policies is intrinsically linked more tothe nature of the task at hand than the definition of the goal space. Navigation is implicitly about findinga (potentially convoluted) path through a terrain with somewhat homogeneous properties. Hence, at leastfor this family of tasks, the existence of -TILO policies is a plausible assumption.",
  ": Reachability graphs, point-mazes": "Suppose (s, ) is a transformation of s, with a parameter .In .2, the transformation was atranslation of vector S, which defined (s, ) = s + . But as another example, when s stands forgeographic coordinates, (s, ) can be a rotation of angle around a central point. For genericity,we shall write . Then, to define the invariance of the policy to this transformation, we need the analogous transformationin the goal space, which we write (g, ). If S = G this transformation is obviously itself. For translationinvariance in the general S = G case of .2, we defined (g, ) = g + P().",
  "navigation tasks. Depending on the task at hand and the transformation considered, it might be verydifficult to prove the existence of ILO policies (Angelotti et al., 2022)": "A special case of ILO policies appears in visual navigation tasks, as those presented in the work of Eysenbachet al. (2019) or Emmons et al. (2020). Appendix J further discusses this specific case and the technicalitiesof training visual goal-reaching policies; in the present paragraph, we restrict ourselves to discussing whythe corresponding policies are ILO. In such tasks, the policys input is a first-person view of a robotssurroundings, the goal space is the state space, the action space are relative movements, and previous worksdiscard the fact that the stochastic process defined by the visual observations is likely not an MDP (rather aPOMDP). Now, let us consider two different states s and s of the robot in the maze, with the correspondingimage observations o(s) and o(s). Suppose also that the relative positions of the walls around s and s arethe same, that is for instance, if the agent was facing a wall from a certain distance and with a certain anglein s, it is also facing a wall with the same distance and angle in s. Then o(s) and o(s) differ only throughgraphical features like texture or lighting for example. If the policy (or value function) has been properlytrained to detect the wall in the picture, regardless of its texture or lighting, then it will be insensitive tothe texture or lighting change between o(s) and o(s), and hence will be able to generalize a goal-reachingpolicy trained in s, to s. Formally, let be the transformation that turns s into s, then a locally optimalpolicy (s, g) will output the same action in ((s), (g)). These policies are ILO by construction (becausethey rely on first-person views and because their neural network embeddings are supposedly trained to beinsensitive to irrelevant image features) and respect equation 2.",
  "FInfluence of graph density hyperparameters": "The thresholds node and edge on node and edge creation condition how coarse the graph is in the goal space.Consequently, they impact the density of the graph, hence the ability to accurately represent transitiondynamics. As such, they encode a notion of minimal required granularity to efficiently generate efficientgoal-reaching plans in the goal space. Despite RGLs ability to build sparse representative graphs, a poorchoice of node and edge parameters can be detrimental to RGLs goal reaching accuracy. reportshow sensitive PO-RGL and RGL agents are to these parameters, in the medium grid-maze. aillustrates how increasing the values of node to 0.2 (then 0.3) and edge to 0.4 (then 0.5) results in a graphwhich does not enable reaching distant goals anymore. A similar effect happens for PO-RGL, as choices forNinit will have a direct impact on the accuracy of the algorithm. b reports how varying Ninit from100 to 600 affects the goal reaching accuracy of PO-RGL. With only 100 nodes, the reachability graph ofPO-RGL features subgoals which are very distant from each other and rarely reachable between each-other,resulting in a graph with almost no edges (c). Hence, no goals besides those reachable by thelower-level policy can be reached. With 200 nodes (d), the final goal reaching accuracy of PO-RGLimproves to about 50% and keeps improving until Ninit = 400 nodes. For Ninit = 500 and 600, the numberof edges to prune in the graph becomes so large that it slows the learning down, resulting in less reachablenodes after 100,000 interaction steps because the graph contains too many misleading edges which have notbeen pruned yet. Overall, this illustrates how the directed, exploration-driven node and edge creation ofRGL yields graphs which are both much sparser and much more representative of reachability, than building",
  "GPre-training a goal-conditioned TILO policy in ant-maze": "Learning goal-based policies for ant-maze environments is challenging, even in the obstacle-free playground.To let SAC+HER converge efficiently, we build a process inspired from curriculum learning (Bengio et al.,2009). We sample goals uniformly in a disc around the agent, starting with a radius of 0. Every time theagent reaches a goal, we increment the radius of 0.1, and decrease it when it fails. If the radius reachesa value of 6, we stop incrementing and let the agent reach an accuracy close to 100% in this pre-trainingplayground. Note that this value of 6 is much larger than that of node and edge (see Appendix A). While navigating in the graph, following a sequence of sub-goals, the agent will change its direction manytimes in an episode. This may lead to more diversity in the states encountered while navigating the mazethan those seen during the pre-training. To mitigate this aspect and improve state diversity during pre-training, every 5 episodes, instead of a full agent reset, we reset only the agents position but retain theorientation, legs configuration and velocities from the last state of the previous episode.",
  "HHierarchical actor critic on various tasks": "Ant-maze tasks have been tackled in previous work, notably in the important HAC (Levy et al.,2019) contribution, on similar tasks to those reported here, in particular the four-rooms maze.Inorder to provide a fair comparison with RGL, we used the reference implementation of HAC pro-vided by the authors at section discusses why this implementation (withoutmodifications) fails on the tasks reported here. Goal and initial state sampling in HAC to promotes diversity. In the original HAC contribution,during training, goals are sampled uniformly in the center of each room (red areas in figure 18), then initialstates are sampled uniformly in the center of another room. This induces a variety of starting states andinsures that starting states and goals are always at least one room away from each other. In turn, thispromotes diversity in the replay buffers, which facilitate policy training. In the experiments reported in, we argued that this reset anywhere feature was a particularly favourable case for exploration.",
  ". The same 1717 maze map, but with uniformly sampled goals while keeping the starting state fixed(labelled Uniform goals / small four-rooms in )": "3. A larger 41 41 four-rooms map which is the one used in the RGL experiments of ,with the HAC goal / initial state sampling strategy (labelled HAC sampling / large four-rooms in). This map features slightly narrower passages between each room (proportionally to thesize of the room). Actions remain the same: the ant is not scaled up. Goals are sampled uniformly.HACs states and goals are scaled to the size of the map.",
  ": HAC average accuracy on variations of the four-rooms ant-maze task, versus number of episodes (episodelength is capped at 700 time steps but can be smaller if the goal is reached before)": ": HAC average accuracy on the medium, hard, and mixed mazes for the ant-maze task, versus numberof episodes (episode length is capped at 700 time steps but can be smaller if the goal is reached before). Green curve:uniform initial state sampling. Blue curve: fixed initial state. evaluated by the proportion of reached goals when goals and initial states are drawn according to HACssampling strategy. Similarly, agents that were trained with a fixed starting state are evaluated on the samesetting. Consequently, the only fair comparison with the RGL results of is when the starting stateis fixed and the goals are sampled uniformly. Recall that RGL reaches an accuracy of 89% on the largefour-rooms environment with fixed initial state and uniform goal sampling (). It appears that the goal / initial state sampling strategy is a crucial feature of HAC in ant-maze. Removingthis feature, and sampling goals uniformly, reduces the accuracy of HACs optimized policy from 76% to28% in the small four-rooms environment, and from 29% to 5% in the large one. It also appears HAC is rather sensitive to the scale of the map (despite appropriate state scaling in theinputs of the neural networks): even with the HAC initial state / goal sampling strategy, the accuracy ofthe optimized policy does not exceed 28% (versus the 89% of RGL). More steps are required to cross a roombetween passages and we hypothesize HAC suffers from this difficulty to span long trajectories between goalsand hence struggles to reach good accuracy in larger mazes. Note that the HAC sampling strategy is tailor-made for the four-rooms maze and is undefined for othermazes, so the comparison above cannot be reproduced for the medium, hard, and mixed mazes. Instead(and this goes beyond what was proposed by the HAC authors), in an attempt to have a comparison baseline,",
  "This section discusses the similarities and differences between DSG and RGL. While there are similarities intheir design principles, the two approaches remain quite different": "The main difference lies in the fact that DSG trains a different option for every vertex in the graph, followingDSCs procedure (Bagaria & Konidaris, 2019).In contrast, RGL does not interleave lower-level policytraining (which is a local goal reaching policy instead of a set of options) with graph expansion. This hasthe drawback (for RGL) of preventing local adaptation of behaviors, but the advantage of sample efficiencyand a full re-use of pre-trained policies (which is the key interest of the TILO abstraction).",
  "Aside from these elements, one should note that RGL generalizes from the S = G context to contexts whereS = G": "Also, and quite importantly, DSG uses the Euclidean distance between states as a reachability heuristic ingraph construction, while RGL uses the value function V (s, g) which is somehow more principled. Thisdistance is also used to define a dense reward model when training options. This last point is particularly important in explaining the difference in empirical scores between DSG andRGL (see ). As indicated by Bagaria et al. (2021, ), DSG makes use of dense rewards andthis is where it really shines ( of the DSG paper actually discusses shortly the need to extend tosparse rewards). In the experiments reported in the present paper, the rewards are sparse, as we are in apurely goal-reaching policy search, where we did not introduce any reward shaping.",
  ": Accuracy of various methods in the Ant-Maze, using the same maps used in figure 7. Every methods aretrained using sparse rewards, except the experiments labelled \"DSG (Dense rwd.)": "reports the difference in performance of DSG in the 4-rooms environment, both with dense andsparse rewards. In the case of sparse rewards, using DSCs procedure to re-train local options for every graphvertex might quickly become inefficient and, in contrast, the TILO policies of RGL appear as a better optionin this context. In comparison, performing the same experiment with sparse rewards leads to an accuracygap, and shows the reliance of this method for dense reward. However, the accuracy observed for DSG withdense reward is still far from the accuracy presented in the original paper. According to the authors, thismethod can fail to reach some goals while using dense rewards, especially when the goal to reach is at theother side of a wall. However, the learned value function is less influenced by this reward the further away the goal is from theagent. Such problems are more likely to occur when both the goal and the agent are close to the wall. In",
  "JTraining in playgrounds and generalizing to unseen states": "In we claimed that only RGL can, by design, exploit a goal-reaching policy which has not beentrained on the actual full maze. This contrasts with, for instance, .5 of the SoRB paper (Eysenbachet al., 2019) where a visual policy is first trained to navigate in 100 houses from the SUNCG dataset fromfirst-person images, then this policys value function is transferred to new houses where it enables building anaviagtion graph. We argue the claim of still holds and introduce nuance and justification below. It is important to note that (i) this visual policy is very close to being TILO (see Appendix E) and, (ii) moreimportantly, is trained on data that covers reasonably well the set of images the agent might encounter. Thissecond property makes the policy likely to generalize well to new houses in the first place. Consequently,although SoRB is indeed tested on new mazes, the combination of a first-person visual value function and thefact that training covers a representative set of images creates a favourable setting which implicitly capturestranslation invariance. We expand on this aspect in the following paragraphs. SoRB and SGM are designed to build a graph on states from the pre-training replay buffer or on statesthat are close enough to those of the pre-training replay buffer, when such states can be drawn from anoracle (which is what happens in the experiment of .5 in the SoRB paper). This constraint is liftedwith RGL thanks to the TILO property which nicely decouples local TILO skills learning from macro-actiongraph building. Specifically, for the transfer to new houses in the SoRB paper, it is important to examinehow the value function is trained. In a nutshell, the authors of SoRB take (s, g) pairs that are about 4 stepsaway in many different houses, and train a value function that indicates how many consecutive actionsdoes it take to transform this s image into this g image?. We conjecture that the combination of a resetanywhere possibility, plus training on 100 different houses, permits seeing a diversity of (s, g) pairs thatenables generalization to similar images in new houses (unfortunately the SUNCG dataset is not availableanymore to verify this conjecture, due to legal issues). Then, SoRBs MAXDIST parameter is set to 3, whichmeans SoRB creates an edge between two nodes whenever the value function estimates one needs at most3 actions to connect the two corresponding states. Our point is that the diversity of (s, g) pairs sampled"
}