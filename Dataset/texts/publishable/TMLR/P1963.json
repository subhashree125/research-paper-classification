{
  "Abstract": "Bayesian Neural Networks (BNNs) offer a principled and natural framework for proper un-certainty quantification in the context of deep learning. They address the typical challengesassociated with conventional deep learning methods, such as data insatiability, ad-hoc na-ture, and susceptibility to overfitting. However, their implementation typically either relieson Markov chain Monte Carlo (MCMC) methods, which are characterized by their com-putational intensity and inefficiency in a high-dimensional space, or variational inferencemethods, which tend to underestimate uncertainty. To address this issue, we propose anovel Calibration-Emulation-Sampling (CES) strategy to significantly enhance the compu-tational efficiency of BNN. In this framework, during the initial calibration stage, we collecta small set of samples from the parameter space. These samples serve as training datafor the emulator, which approximates the map between parameters and posterior proba-bility. The trained emulator is then used for sampling from the posterior distribution atsubstantially higher speed compared to the standard BNN. Using simulated and real data,we demonstrate that our proposed method improves computational efficiency of BNN, whilemaintaining similar performance in terms of prediction accuracy and uncertainty quantifi-cation.",
  "Introduction": "In recent years, Deep Neural Networks (DNN) have emerged as the predominant driving force in the fieldof machine learning and are regarded as fundamental tools for many intelligent systems (Cheng et al.,2018; LeCun et al., 1998; Sze et al., 2017). While DNN have demonstrated significant success in predictiontasks, they often struggle with accurately quantifying uncertainty. Additionally, due to their vulnerabilityto overfitting, they can generate highly confident yet erroneous predictions (Su et al., 2019; Kwon et al.,2022). In recent years, there have been some attempts to address this issue. For example, the Ensemble",
  "Published in Transactions on Machine Learning Research (09/2024)": "Duane L Beekly, Erin M Ramos, Gerald van Belle, Woodrow Deitrich, Amber D Clark, Mary E Jacka,Walter A Kukull, et al. The national Alzheimers coordinating center (NACC) database: an Alzheimerdisease database. Alzheimer Disease & Associated Disorders, 18(4):270277, 2004. Duane L Beekly, Erin M Ramos, William W Lee, Woodrow D Deitrich, Mary E Jacka, Joylee Wu, Janene LHubbard, Thomas D Koepsell, John C Morris, Walter A Kukull, et al. The National Alzheimers Coor-dinating Center (NACC) database: the uniform data set. Alzheimer Disease & Associated Disorders, 21(3):249258, 2007.",
  "This framework allows for reusing expensive forward evaluations from parameters to posterior probabilityand offers a computationally efficient alternative to existing MCMC procedures": "The standard CES method (Cleary et al., 2021) focuses on UQ in inverse problems and uses GaussianProcess (GP) models for the emulation component. GP models have a well-established history of applicationin emulating computer models (Currin et al., 1988), conducting uncertainty analyses (Oakley & OHagan,2002), sensitivity assessments (Oakley & OHagan, 2004), and calibrating computer codes (Kennedy &OHagan, 2002; Higdon et al., 2004). Despite their versatility, GP-based emulators are computationallyintensive, with a complexity of O(N 3) using the squared-exponential kernel, where N is the sample size.Lower computational complexity can be achieved using alternative kernels (Lan et al., 2015) or variouscomputational techniques (Liu et al., 2020; Bonilla et al., 2007; Gardner et al., 2018; Seeger et al., 2003).Nevertheless, scaling up GP emulators to high-dimensional problems remains a limiting factor. Furthermore,the prediction accuracy of GP emulators highly depends on the quality of the training data, emphasizing theimportance of rigorous experimental design. To address these issues, Lan et al. (2022) proposed an alternativeCES scheme called Dimension-Reduced Emulative Autoencoder Monte Carlo (DREAMC) method, whichuses Convolutional Neural Networks (CNN) as emulator. DREAMC improves and scales up the applicationof the CES framework for Bayesian UQ in inverse problems from hundreds of dimensions (with GP emulation)to thousands of dimensions (with CNN emulation). Here, we adopt a similar approach and propose a newmethod, called Fast BNN (FBNN), for Bayesian inference in neural networks. We use DNN for the emulationcomponent of our CES scheme. DNN has proven to be a powerful tool in a variety of applications andoffers several advantages over GP emulation (Lan et al., 2022; Dargan et al., 2020). It is computationallymore efficient and suitable for high-dimensional problems. The choice of DNN as an emulator enhancescomputational efficiency and flexibility. Besides the computational challenges of building emulators, efficiently sampling from posterior distributionsusing these emulators also presents a significant challenge due to the high dimensionality of the targetdistribution.Traditional Metropolis-Hastings algorithms, typically defined on finite-dimensional spaces,encounter diminishing mixing efficiency as the dimensions increase (Gelman et al., 1997; Roberts & Rosenthal,",
  "Related Methods": "Various MCMC methods have been employed to explore complex probability distributions for Bayesian in-ference. In this section, we discuss some of the main MCMC algorithms related to our work. Additionally,we discuss a variety of state-of-the-art methods utilized in our numerical experiments, which extend be-yond MCMC frameworks. These include Ensemble Deep Learning for Neural Networks (Perrone & Cooper,1995), BNNs with Variational Inference (Jaakkola & Jordan, 2000), BNNs leveraging Lasso Approxima-tion (MacKay, 1992), Monte Carlo Dropout (MC-Dropout) (Gal & Ghahramani, 2016), Stochastic WeightAveraging-Gaussian (SWAG) (Maddox et al., 2019), and Accelerated Hamiltonian Monte Carlo (HMC)(Zhang et al., 2017). These techniques offer a comprehensive spectrum for evaluating our FBNN model.",
  "Hamiltonian Monte Carlo (HMC)": "MCMC methods are designed for sampling from intractable probability distributions.The fundamentalprinciple involves constructing a Markov chain whose equilibrium distribution coincides with the targetdistribution. Various algorithms exist for constructing such Markov chains, the simplest of which is theMetropolis-Hastings (MH) algorithm (Metropolis et al., 1953; Hastings, 1970), which relies on a random walkto explore the parameter space. Hamiltonian Monte Carlo (HMC) is a special case of the MH algorithm thatincorporates Hamiltonian dynamics evolution and auxiliary momentum variables (Neal, 2011). Comparedto using a Gaussian random walk proposal distribution in the MH algorithm, HMC reduces the correlationbetween successive sampled states by proposing moves to distant states that maintain a high probability ofacceptance due to the approximate energy conserving properties of the simulated Hamiltonian dynamic. Thereduced correlation means fewer Markov chain samples are needed to approximate integrals with respect tothe target probability distribution for a given Monte Carlo error.",
  "Stochastic Gradient Hamiltonian Monte Carlo (SGHMC)": "As discussed earlier, HMC sampling methods provide a mechanism for defining distant proposals with highacceptance probabilities in a Metropolis-Hastings framework, enabling more efficient exploration of the statespace than standard random-walk proposals. However, a limitation of HMC methods is the required gradientcomputation for simulation of the Hamiltonian dynamical system; such computation is infeasible in problemsinvolving a large sample size. Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) (Chen et al., 2014)addresses computational inefficiency by using a noisy but unbiased estimate of the gradient computed froma mini-batch of the data. SGHMC is an effective method for Bayesian inference, particularly when dealingwith large datasets, as it leverages stochastic gradients and hyperparameter adaptation to efficiently explorehigh-dimensional target distributions.",
  "Random Network SurrogateHMC (RNSHMC)": "Alternatively, we can reduce the computational cost of HMC by constructing surrogate Hamiltonians. Asan example, the random network surrogateHMC (RNSHMC) method (Zhang et al., 2017) uses randomnon-linear bases to approximate posterior distributions. The goal is to explore and exploit the structure andregularity in parameter space for the underlying probabilistic model and construct an effective approximationof its geometric properties. To achieve this, RNSHMC starts by identifying suitable bases that can capturethe complex geometric properties of the parameter space. Through an optimization process, these basesare then used to form a surrogate function to approximate the expensive Hamiltonian dynamics. Unliketraditional HMC, which requires repeated evaluation of the model and its derivatives, RNS-HMC leveragesthe surrogate function to perform leapfrog integration steps, leading to a substantially lower computationalcost. Later, Li et al. (2019) extended this idea by using a neural network to directly approximate the gradientof the Hamiltonian.",
  "Preconditioned Crank-Nicolson (pCN)": "Preconditioned Crank-Nicolson (Da Prato & Zabczyk, 2014) is a variant of MCMC that incorporates apreconditioning matrix for adaptive scaling (Cotter et al., 2013). It involves re-scaling and random pertur-bation of the current state, incorporating prior information. Despite the Gaussian prior assumption, theapproach adapts to cases where the posterior distribution may not be Gaussian but is absolutely continuouswith respect to an appropriate Gaussian density. This adaptation is achieved through the Radon-Nikodymderivative, connecting the posterior distribution with the dominating Gaussian measure, often chosen as theprior. The algorithmic foundation of pCN lies in using stochastic processes that preserve either the posterioror prior distribution. These processes serve as proposals for Metropolis-Hastings methods with specific dis-cretizations, ensuring preservation of the Gaussian reference measure. The key steps of the pCN algorithmare outlined in Algorithm 1.",
  "Variational Inference": "The concept of variational inference has been applied in various forms to probabilistic models. The techniqueoffers a way to approximate posterior distributions in Bayesian models (Jordan et al., 1999). The approximatedistribution allows for a more feasible inference, especially for complex models like neural networks. In thecontext of BNNs, variational inference was brought into focus by Hinton & Van Camp (1993), which, whilenot explicitly termed as such in the modern sense, laid the groundwork for later developments. A moredirect application of variational inference to BNNs was detailed in later works (e.g., Graves, 2011). Morerecently, Kingma & Welling (2014), Rezende et al. (2014) and Blundell et al. (2015) significantly contributedto popularizing and advancing the use of variational inference in deep learning and Bayesian neural networksthrough the introduction of efficient gradient-based optimization techniques. Algorithm 2 succinctly shows the iterative process of optimizing the parameters of a variational distribution toapproximate the posterior distribution of a BNNs weights. Through the alternation of expectation (E-Step)and maximization (M-Step) phases, it aims to minimize the difference between the variational distributionand the true posterior, leveraging the Evidence Lower Bound (ELBO) (Jordan et al., 1999) as a tractablesurrogate objective function. This approach enables the practical application of Bayesian inference to neuralnetworks, facilitating the quantification of uncertainty in predictions and model parameters.",
  "Laplace Approximation": "Previous studies have shown that in the context of BNNs, the Laplace approximation serves as an efficientmethod for approximating the posterior distribution over the networks weights (Arbel et al., 2023; Blundellet al., 2015). At the core of the Laplace approximation is the assumption that, around the loss functionsminimum, the posterior distribution of the networks weights can be approximated by a Gaussian distribution.This is achieved by finding the mode of the posterior, called the Maximum A Posteriori (MAP; equivalent tothe minimum of the loss function in the Bayesian framework), and then approximating the curvature of theloss surface at this point using the Hessian matrix (Liang et al., 2018). The inverse of this Hessian is usedto define the covariance of the Gaussian posterior, thus simplifying the representation of uncertainty in thepredictions. More specifically in BNNs, this approach can be used to approximate the posterior distributionof the weights given the observed data.",
  "Monte Carlo Dropout": "Monte Carlo (MC) Dropout (Gal & Ghahramani, 2016) was introduced as a Bayesian approximation methodto quantify model uncertainty in deep learning. The core idea behind this method is to interpret dropout,a technique commonly used to prevent overfitting in neural networks, from a Bayesian perspective. Nor-mally, dropout randomly disables a fraction of neurons during the training phase to improve generalization.However, when viewed through the Bayesian lens, dropout can be seen as a practical way to approximateBayesian inference in deep networks. This approximation allows the network to estimate not just a single setof weights, but a distribution over them, enabling the model to express uncertainty in its predictions. TheMC Dropout technique involves running multiple forward passes through the network with dropout enabled.Each forward pass generates a different set of predictions due to the random omission of neurons, leading toa distribution of outputs for a given input.",
  "Stochastic Weight Averaging-Gaussian (SWAG)": "Building on the idea of Stochastic Weight Averaging (SWA) (Izmailov et al., 2018; Maddox et al., 2019),SWAG approximates the distribution of model weights by a Gaussian distribution, leveraging the empiricalweight samples collected from training.This approach allows for a more nuanced understanding of themodels uncertainty compared to SWA, which simply averages weights over the latter part of the trainingprocess. SWAG involves collecting a set of weights {Wi}Ni=1 over the last N epochs of training, where Wirepresents the weight vector at epoch i. The mean of the Gaussian distribution is then computed as thesimple average of these weights: = 1",
  "Bayesian UQ for Neural Networks: Calibration-Emulation-Sampling": "Standard neural networks (NN) typically consist of multiple layers, starting with an input layer, denotedas l0, followed by a series of hidden layers ll for l = 1, . . . , m 1, and ending with an output layer lm. Inthis architectural framework, comprising a total of m + 1 layers, each layer l is characterized by a lineartransformation, which is subsequently subjected to a nonlinear operation g, commonly referred to as anactivation function (Jospin et al., 2022):",
  "lm=Y": "Here, = (W , b) are the parameters of the network, where W are the weights of the network connectionsand b are the biases. A given NN architecture represents a set of functions isomorphic to the set of possibleparameters . Deep learning is the process of estimating the parameters from the training set (X, Y ) :={(xn, yn)}Nn=1 composed of a series of input X and their corresponding labels Y . Based on the training set,a neural network is trained to optimize network parameters in order to map X Y with the objective ofobtaining the maximal accuracy (under certain loss function L()). Considering the error, we can write NNas a forward mapping, denoted as G, that maps each parameter vector to a function that further connectsX to Y with small errors n :",
  "(; X, Y )=12Y G(X; )2(4)": "The point estimate approach, which is the traditional approach in deep learning, is relatively straightfor-ward to implement with modern algorithms and software packages, but tends to lack proper uncertaintyquantification (Guo et al., 2017; Nixon et al., 2019). To address this issue, stochastic neural networks, whichincorporate stochastic components in the network, have emerged as a standard solution. This is performedby giving the network either stochastic activation functions or stochastic weights to simulate random samplesfor . The integration of stochastic components into neural networks allows for an extensive exploration ofmodel uncertainty, which can be approached through Bayesian methods among others. It should be notedthat not all neural networks that represent uncertainty are Bayesian or even stochastic; some employ deter-ministic methods to estimate uncertainty without relying on stochastic components or Bayesian inference(Lakshminarayanan et al., 2017; Sensoy et al., 2018). Bayesian neural networks (BNN) represent a subsetof stochastic neural networks where Bayesian inference is specifically used for training, offering a rigorousprobabilistic interpretation of model parameters (MacKay, 1992; Neal, 2012). The primary objective is togain a deeper understanding of the uncertainty that underlies the specific process the network is modeling.",
  "p (Y | X, ) p().(6)": "BNN is usually trained using MCMC algorithms. Because we typically have big amount of data, the likelihoodevaluation tends to be expensive. One common approach to address this issue is subsampling, which restrictsthe computation to a subset of the data (see for example, Hoffman et al., 2010; Welling & Teh, 2011; Chenet al., 2014). The assumption is that there is redundancy in the data and an appropriate subset of the datacan provide a good enough approximation of the information provided by the full data set. In practice, it is achallenge to find good criteria and strategies for an effective subsampling in many applications. Additionally,subsampling could lead to a significant loss of accuracy (Betancourt, 2015).",
  "Fast Bayesian Neural Network (FBNN)": "We propose an alternative approach that explores smoothness or regularity in parameter space, a charac-teristic common to most statistical models. Therefore, one would expect to find good and compact formsof approximation of functions (e.g., likelihood function) in parameter space. Sampling algorithms can usethese approximate functions, also known as surrogate functions, to reduce their computational cost. Morespecifically, we propose using the CES scheme for high-dimensional BNN problems in order to bypass theexpensive evaluation of original forward models and reduce the cost of sampling to a small computationaloverhead. Compared with MCMC methods, which require repeatedly evaluating the original (large) NN forthe likelihood given the data, the proposed method builds a smaller NN emulator that bypasses the data(i.e., cuts out the middleman) by mapping the parameters directly to the likelihood function, thus avoid-ing costly evaluations. That is, the emulator is trained based on the parameter-likelihood pairs, which arecollected through few iterations of the original BNN. In contrast to subsampling methods, this approach",
  "from .end procedure": "can handle computationally intensive likelihood functions, whether the computational cost is due to high-dimensional data or complex likelihood function (e.g., models based on differential equations). Additionally,the calibration process increases the efficiency of MCMC algorithms by providing a robust initial point inthe high-density region. Algorithm 3 shows how our proposed method, called Fast Bayesian Neural Net-wor (FBNN), combines the strengths of BNN in uncertainty quantification, SGHMC for efficient parametercalibration, and the pCN method for sampling. More details are provided in the following sections.",
  "Calibration Early stopping in Bayesian Neural Network": "By calibration we mean collecting an optimal sample of parameters to build an emulator with a reasonablelevel of accuracy. This is aligned with traditional calibration goals of balancing accuracy and reliability, butwithin a new context. Here, the calibration step involves an early stopping strategy, aimed at collecting atargeted set of posterior samples without fully converging to the target distribution. More specifically, weuse the Stochastic Gradient Hamilton Monte Carlo (SGHMC) algorithm for a limited number of iterationsto collect a small set of samples. These samples include both the model parameters ((j)) and the outputspredicted by the model (G(X; (j))) for each sample j out of a total of J samples. The key focus of thistraining phase is not to obtain a precise approximation of the target posterior distribution, but rathercollecting a small number of posterior samples as the training data for the subsequent emulation step. TheSGHMC algorithm plays a crucial role in efficiently handling large datasets and collecting essential samplesduring the calibration step of the FBNN. Its ability to introduce controlled stochasticity in updates provesinstrumental in preventing local minima entrapment, thereby providing a comprehensive set of posteriorsamples that capture the variability in the parameter space.",
  "Emulation Deep Neural Network (DNN)": "The original forward mapping in BNN involves mapping input dataset X to the response variable Y . For thelikelihood evaluation using original forward mapping, it is necessary to calculate the likelihood L(; X, Y ) foreach sample of model parameters. This means that with each iteration, when a new set of model parametersis introduced, the original forward mapping needs to be applied to generate output predictions, followed bythe calculation of the likelihood. In general, this process can be very time-consuming. If, however, we have asmall set of estimated model parameters along with their corresponding predicted outputs collected duringthe calibration step, an emulator can be trained to eliminate the intermediary step (passing through each datapoint), allowing us to map the parameters directly to the likelihood function. This leads to a computationallyefficient likelihood evaluation. Therefore, to address the computational challenges of evaluating the likelihoodwith large datasets, we build an emulator Ge using the recorded pairs {(j), y(j) = G(X; (j))}Jj=1 obtainedduring the calibration step. More specifically, these input-output pairs are used to train a DNN model as",
  "(8)": "Given a DNN model where represents the input and G(X; ) denotes the output, we set the dimensionsas d0 = d and dK = D, where d represents the dimension of the input parameter vector , and D representsthe dimension of the output G(X; ). Here, the matrices Wk are defined in the space Rdk+1dk and thevectors bk in Rdk+1. The functions gk act as (continuous) activation mechanisms. In the context of ournumerical examples, the activation functions for the DNN emulator are selected to ensure that both thefunction approximations and their derived gradients have minimized errors.This involves a grid searchover a predefined set of activation functions to ensure that the network efficiently approximates the targetfunctions and their gradients.",
  "Y Ge(X; )2(10)": "Building upon the foundational concepts of using a DNN emulator Ge for approximating the forward mappingfunction G, we further elaborate on the implications and advantages of this approach for Bayesian inference,particularly in the context of handling large datasets and/or complex likelihood functions. The emulationstep, which involves training the DNN emulator with input-output pairs {(j), G(X; (j))}, serves as acritical phase where the emulator learns to mimic the behavior of the original model with high accuracy.The utilization of DNN emulator to approximate the likelihood function in Bayesian inference presents asignificant computational advantage over the direct use of the original BNN likelihood. This advantage stemsprimarily from the inherent differences in computational complexity between evaluating the the likelihoodwith a DNN emulator which takes a set of model parameters as input and yields predicted responsesandthe original BNN model which processes X as input to produce the response variable. In the sampling stage, the computational complexity could be significantly reduced if we use e instead of in the accept/reject step of MCMC. If the emulator is a good representation of the forward mapping,the difference between e and would be small and negligible.Then, the samples by such emulativeMCMC have the stationary distribution that closely follows the true posterior distribution. This approachnot only ensures that the sampling process is computationally feasible, but also maintains the integrity ofthe stationary distribution, closely approximating the true posterior distribution with minimal discrepancy.The integration of DNN emulators into the Bayesian inference workflow thus presents a compelling solutionto the computational challenges associated with evaluating likelihood functions in complex models.",
  "Sampling Preconditioned Crank-Nicolson (pCN)": "In the context of the FBNN method, the sampling step is crucial for approximating the posterior distributionefficiently. The method employs MCMC algorithms based on a trained emulator to achieve full explorationand exploitation. However, challenges arise, especially in high-dimensional parameter spaces, where classicalMCMC algorithms often exhibit increasing correlations among samples. To address this issue, the pCNmethod presented in Algorithm 1 has been used in our proposed framework as a potential solution. Un-like classical methods, pCN avoids dimensional dependence challenges, making it particularly suitable forscenarios like BNN models with a high number of weights to be inferred (Hairer et al., 2009). As explained in section 2.4, the pCN approach minimizes correlations between successive samples, a criticalfeature for ensuring the representativeness of the samples collected. This characteristic is vital for FBNNs, as",
  ": Sampling from a mixture of 25 Gaussians shown in (a) with 200k samples. SGHMC in (b) broadlyexplores the space, while pCN in (c) hones in on the high-density regions for precise mode capture": "it directly impacts the networks ability to learn from data and make robust predictions. The pCN methodexcels in traversing the parameter space with controlled perturbations, enhancing the algorithms ability tocapture the most probable configurations of model parameters. This focus on effective exploration around themode contributes to a more accurate representation of the underlying neural network, ultimately improvingmodel performance. In other words, the choice of pCN as the sampling method in FBNN is motivated byits tailored capacity to navigate and characterize the most probable regions of the parameter space. Thischoice reinforces the methodologys robustness and reliability, as pCN facilitates efficient sampling, leadingto a more accurate and representative approximation of the posterior distribution. To illustrate this, displays a simulation that contrasts the sampling mechanisms of SGHMC andpCN within a multimodal probability distribution. The task is to sample from a mixture of 25 Gaussiandistributions, represented in panel (a), using a total of 200,000 samples.Here, the target distributionis multimodal with several distinct peaks (modes).Middle figure shows that SGHMC has explored theparameter space, although with a less concentrated sampling around the modes compared to the targetdistribution. This indicates that while SGHMC is effective at exploring the space, it may not capture themodes as tightly as the target distribution. In the right panel related to pCN sampler, the concentration ofsamples around the modes is much higher compared to SGHMC, which indicates that pCN is more effectiveat exploring around the modes of the distribution. Thus, we believe the combination of SGHMC and pCN inour proposed framework can complement each other for a more effective exploration of the parameter space.",
  "Theoretical Foundations": "In this section, we aim to quantify the error between the true potential function and its emulation inthe context of the FBNN method.Let = (0, 1)d and consider forward mappings in the Sobolevspace W n,p() := {f Lp() : Df Lp() for all (N {0})d with || n} with fn,p :=",
  "[G(X; ) Ge(X; ), y G(X; ) + y Ge(X; ), G(X; ) Ge(X; )]": "Because Gj(X; ) L((0, 1)d), there exists a constant M > 0 such that max1jD Gj(X; ), y M.For /(MD) > 0, by Theorem 4.1 of Ghring et al. (2020), there exists a standard NN with ReLU activationfunctions and the depth K and the number of weights and units as in the condition such that",
  "We demonstrate the effectiveness of our method on eleven synthetic and real-world datasets, comparing itagainst a comprehensive selection of baseline approaches": "Datasets.We experiment on a series of regression and classification problems. Detailed information regard-ing these datasets, including the number of features and datapoints in each, and the number of parametersused in the main FBNN model for each dataset, is outlined in . We have also included the details ofthe DNN Emulator architecture for each dataset in . Baseline Methods.We present empirical evidence comparing our CES method against a broad array ofbaseline approaches including two baseline BNN methods equipped with the SGHMC and pCN samplers(shown as BNN-SGHMC and BNN-pCN), and BNN architectures incorporating Variational Inference, LassoApproximation, MC-Dropout, SWAG, and RNS-HMC. Detailed information about these methods was pro-vided in . We also include the results from DNN, which does not provide uncertainty quantification,but serves as a reference point. Moreover, we provide the results of Deep Ensembles, which consist of multipleDNNs, each initialized with different random seeds. We refer to this method as Ensemble-DNN. Althoughthe Ensemble-DNN approach allows for parallelization, it falls short in providing a probabilistic frameworkfor analysis, a significant advantage offered by our CES method. As discussed earlier, one of the distinctive features of our main FBNN model, more specifically shown asFBNN (SGHMC-pCN), lies in its strategic integration of the SGHMC sampler during the calibration stepand the pCN algorithm during the sampling step.This combination is carefully chosen to harness thecomplementary strengths of these two sampling methods. Further expanding our exploration, we introducethree additional FBNN models: FBNN (pCN-SGHMC), where pCN is employed in the calibration step andSGHMC in the sampling step; FBNN (pCN-pCN), where pCN is used in both steps; and FBNN (SGHMC-SGHMC), where SGHMC is used in both calibration and sampling steps. Throughout these experiments, we collect 2000 posterior samples for the BNN-SGHMC and BNN-pCN, withsamples being collected at each iteration. In contrast, for the FBNN methods, we use a small number (200)of samples from either BNN-SGHMC or BNN-pCN (depending on the specific FBNN model) along withthe corresponding predicted outputs during the calibration step. These 200 samples serve as the trainingdata for the emulator. Moreover, we evaluate the efficacy of utilizing only the initial 200 samples from theBNN-SGHMC model across all the datasets. This was done to demonstrate the necessity of collecting moresamples, either using the original BNN or employing the FBNN method, rather than relying our inferenceon a limited number of initial samples.",
  "Simulation38,64,32ReLU10000.5": "It is also crucial to highlight that the BNN-SGHMC and BNN-pCN models are trained from a randomlychosen initial point for the MCMC sampling process. On the other hand, in the FBNN methods, we employthe set of posterior samples collected during the last iteration of the calibration step as the starting pointfor the subsequent MCMC sampling. Metrics.To thoroughly assess the performance and effectiveness of each method, we use a range of keymetrics. These include Mean Squared Error (MSE) for regression tasks () and Accuracy for clas-sification tasks (). We also evaluate the models based on their computational cost, and variousstatistics related to the Effective Sample Size (ESS) of model parameters. These statistics include the mini-mum, maximum, and median ESS, as well as the minimum ESS per second. We also quantify the amountof speedup, denoted as spdup, a metric that compares the minimum ESS per second of each model withthat of BNN-SGHMC as the benchmark (). Analysing spdup is crucial as it provides a comparative",
  "measure of efficiency, highlighting the models capability to achieve high-quality parameter sampling withlower computational resource utilization relative to the benchmark BNN-SGHMC": "The effective sample size takes the autocorrelation among the consecutive samples into account. While wecan reduce autocorrelation using the thinning strategy, this leads to a higher computational time for thesame number of samples. Our spdup metric allows for a fair comparison of sampling algorithms (regardlessof what thinning strategy used) by taking both autocorrelation and computational cost into account. For UQ in regression cases, we evaluate the Coverage Probability (CP) set at 95%. In addition, we construct95% Credible Intervals (CI) by the prediction results of Bayesian models, along with the average true output,to illustrate UQ in regression problems. For classification problems, we use Expected Calibration Error (ECE)and Reliability Diagrams to evaluate UQ. ECE addresses model calibration, aiming for accurate uncertaintyestimates, while reliability diagrams offer a visual summary of probabilistic forecasts.",
  "We first evaluate our proposed method using a set of simulated and real regression problems": "Simulated Data.We begin our empirical evaluation by using simulated data. To this end, we utilizethe make_regression function from the sklearn.datasets package to generate a dataset consisting of5,000,000 observations and 1,000 predictors. compares the true and emulated log likelihood functions associated with posterior samples collectedusing BNN-SGHMC. The emulated values are based on the FBNN (SGHMC-pCN) model. As we can see,the two functions are similar, indicating that the emulator provides a reasonable approximation of the truetarget distribution. compares the MSE among all models, showing that while the DNN method achieves the lowest MSEat 0.71, the FBNN (SGHMC-pCN) model provides a similar performance. Notably, among all the FBNNvariants, FBNN (SGHMC-pCN) provides the highest CP at 92.2%, demonstrating a level of calibrationcomparable to that of the BNN model.The Ensemble-DNN demonstrates comparable performance toFBNN (SGHMC-pCN) in terms of CP, yet it operates at a pace three times slower. Examining the efficiency of sample generation, all FBNN variants have relatively higher ESS per secondcompared to all the other BNN models, except for BNN-RNS-HMC. Among all the models, FBNN (SGHMC-pCN) has the highest min ESS per second at 0.043. indicates our model provides the highestspeedup (16.33) compared to BNN-SGHMC as the baseline model, highlighting our methods computationalefficiency.Considering these results, FBNN (SGHMC-pCN) emerges as a strong approach with a goodbalance between predictive accuracy, computational efficiency, and uncertainty quantification, making it theoverall best option for Bayesian deep learning. a shows the estimated mean and prediction uncertainty for both BNN-SGHMC and FBNN (SGHMC-pCN) models, alongside the smoothed average and 95% interval for the true output. For clarity and concise-ness within our figures, we have employed Principal Component Analysis (PCA) and used the first principalcomponent to transform the original data into a one-dimensional representative feature (x-axis in ).As we can see, BNN and FBNN have very similar credible intervals. This consistency in credible intervalbounds is significant for UQ, indicating that both models effectively and almost equally quantify uncertaintyin their predictions. Wine Quality Data.As the first real dataset for the regression task, we use the Wine Quality data(Cortez et al., 2009). This dataset contains various physicochemical properties of different wines, while thetarget variable is the quality rating. The performance of FBNN (SGHMC-pCN) indicates a well-balancedapproach, making it superior to the other models for several reasons. Firstly, it achieves a competitively lowMSE of 0.52, comparable to other high-performing models like BNN-SGHMC and SWAG, but it surpassesthem in terms of speedup.Moreover, FBNN (SGHMC-pCN) exhibits a robust predictive performance,",
  "with a competitively high CP. b shows the prediction mean and 95% CI BNN-SGHMC and FBNN(SGHMC-pCN), as well as the smoothed average and 95% interval for the true output": "Boston Housing Data.The Boston housing dataset was collected in 1978 (Harrison Jr & Rubinfeld,1978). Each of the entries present aggregated data for homes from various suburbs in Boston. For thisdataset, FBNN (SGHMC-pCN) stands out with a notable balance between MSE (3.82), CP (81.1%), andcomputational efficiency, completing the task in just 91 seconds. This model significantly outperforms all theother models in terms of speedup (11.94), showcasing its effectiveness in sampling. c shows the 95%CIs and mean predictions of both BNN-SGHMC and FBNN (SGHMC-pCN). The FBNN (SGHMC-pCN)",
  "model, in particular, displays well-calibrated uncertainty quantification, mirroring the performance of theBNN models, implying that its probabilistic predictions capture the model uncertainty": "Alzheimer Data.Next, we analyze the data from the National Alzheimers Coordinating Center (NACC),which is responsible for developing and maintaining a database of patient information collected from theAlzheimer disease centers (ADCs) funded by the National Institute on Aging (NIA) (Beekly et al., 2004). TheNIA appointed the ADC Clinical Task Force to determine and define an expanded, standardized clinical dataset, called the Uniform Data Set (UDS). The goal of the UDS is to provide ADC researchers a standard setof assessment procedures to identify Alzheimers disease (Beekly et al., 2007). We have used 56 key featuresfor our analysis. These features were carefully selected to represent a wide spectrum of variables relevantto Alzheimers disease diagnosis, including functional abilities, brain morphometrics, living situations, anddemographic information (Ren et al., 2022). For the regression case, the goal is to predict Left HippocampusVolume, a critical marker in the progression of the disease (van der Flier & Scheltens, 2009), as a functionof other variables. For this dataset, shows that the FBNN (SGHMC-pCN) model stands out forits balanced performance, recording the second lowest MSE at 0.48 and a relatively high CP at 91.6%. Itshows a considerable improvement in computational efficiency, evidenced by a speedup factor of 22 timescompared to BNN-SGHMC as the baseline BNN model. Year Prediction MSD Data.For this data, the goal is to predict the release year of a song from audiofeatures. Songs are mostly western, commercial tracks ranging from 1922 to 2011, with a peak in the year2000s (Bertin-Mahieux, 2011). In the context of the YearPredictionMSD dataset, FBNN (SGHMC-pCN)showcases its superiority over other models by achieving a good balance between accuracy, computationalefficiency, and effective uncertainty quantification. With an MSE of 73.41, close to that of Ensemble-DNN,and CP of 92.23% it outperforms most other models.Moreover, the computational efficiency of FBNN(SGHMC-pCN) is highlighted by its speedup factor of 11.98 () over the baseline model BNN-SGHMC.",
  "(c) Boston Housing data": ": Comparative analysis of predictive credible intervals and mean predictions for regression tasks.For each dataset, the 95% CI for BNN predictions and FBNN predictions are shown as shaded areas. Theaverage predictions from BNN and FBNN are represented with dashed lines. Additionally, the 95% CI forthe true output as ground truth and the smoothed average true output are plotted as solid lines. The x-axisshows the first principal component of the predictors.",
  "Conclusion": "In this paper, we have proposed an innovative CES framework called FBNN, specifically designed to enhancethe computational efficiency and scalability of BNN for high-dimensional data. Our primary goal is to providea robust solution for uncertainty quantification in high-dimensional spaces, leveraging the power of Bayesianinference while mitigating the computational bottlenecks traditionally associated with BNN. In our numerical experiments, we have successfully applied several variants of FBNN, including differentconfigurations with BNN, to regression and classification tasks on both synthetic and real datasets. Remark-ably, the FBNN variant incorporating SGHMC for calibration and pCN for sampling, denoted as FBNN(SGHMC-pCN), not only matches the predictive accuracy of traditional BNN but also offers substantialcomputational advantages. More specifically, our numerical experiments across various regression and classi-fication tasks consistently demonstrate the superiority of the FBNN (SGHMC-pCN) method over traditionalBNNs and DNNs. In regression tasks, FBNN (SGHMC-pCN) demonstrates a competitive MSE while signif-icantly enhancing computational efficiency, achieving notable speedups compared to baseline models. Thisefficiency does not come at the expense of accuracy, as evidenced by the competitive MSE values and robustuncertainty quantification metrics. In classification tasks, FBNN (SGHMC-pCN) stands out by achievinghigh accuracy rates and low ECE values, which indicate reliable uncertainty quantification. The superior performance of FBNN (SGHMC-pCN) can be attributed to the complementary strengths ofSGHMC and pCN. SGHMC excels at broad exploration of the parameter space, providing an effectivemeans for understanding the global structure during the calibration step. On the other hand, pCN is adeptat efficient sampling around modes, offering a valuable tool for capturing local intricacies in the distributionduring the final sampling step. By combining these samplers within the FBNN framework, we achieve abalanced approach between exploration (calibration with SGHMC) and exploitation (final sampling withpCN). Future work could involve extending our method to more complex problems (e.g., spatiotemporal data)and complex network structures (e.g., graph neural networks). Additionally, future research could focus onimproving the emulation step by optimizing the DNN architecture. Finally, our method could be furtherimproved by embedding the sampling algorithm in an adaptive framework similar to the method of Zhanget al. (2018).",
  "Wilfred K. Hastings. Monte Carlo sampling methods using Markov chains and their applications. Biometrika,57(1):97109, 1970": "Dave Higdon, Marc Kennedy, James C. Cavendish, John A. Cafeo, and Robert D. Ryne. Combining fielddata and computer simulations for calibration and prediction. SIAM Journal on Scientific Computing, 26(2):448466, 2004. doi: 10.1137/S1064827503426693. Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the descriptionlength of the weights. In Proceedings of the sixth annual conference on Computational learning theory,pp. 513, 1993.",
  "Matthew D. Hoffman and Andrew Gelman. The no-u-turn sampler: Adaptively setting path lengths inhamiltonian monte carlo. Journal of Machine Learning Research, 15(47):15931623, 2014": "Matthew D. Hoffman, David M. Blei, and Francis R. Bach. Online learning for latent Dirichlet allocation.In John D. Lafferty, Christopher K. I. Williams, John Shawe-Taylor, Richard S. Zemel, and Aron Culotta(eds.), Advances in Neural Information Processing Systems (NurIPS), pp. 856864. Curran Associates,Inc., 2010. Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Av-eraging weights leads to wider optima and better generalization. In Amir Globerson and Ricardo Silva(eds.), Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018,Monterey, California, USA, August 6-10, 2018, pp. 876885. AUAI Press, 2018.",
  "Statistical Society Series B: Statistical Methodology, 63(3):425464, 2002. doi: 10.1111/1467-9868.00294": "Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In Yoshua Bengio and Yann LeCun(eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April14-16, 2014, Conference Track Proceedings, 2014. Yongchan Kwon, Joong-Ho Won, Beom Joon Kim, and Myunghee Cho Paik. Uncertainty quantificationusing Bayesian neural networks in classification: Application to ischemic stroke lesion segmentation. InMedical Imaging with Deep Learning, 2022. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-tainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Shiwei Lan, Julia A Palacios, Michael Karcher, Vladimir N Minin, and Babak Shahbaba.An efficientBayesian inference framework for coalescent-based nonparametric phylodynamics. Bioinformatics, 31(20):32823289, 2015. Shiwei Lan, Shuyi Li, and Babak Shahbaba.Scaling up Bayesian uncertainty quantification for inverseproblems using deep neural networks. SIAM/ASA Journal on Uncertainty Quantification, 10(4):16841713, 2022. doi: 10.1137/21M1439456.",
  "David JC MacKay. A practical bayesian framework for backpropagation networks. Neural Computation, 4(3):448472, 1992": "Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simplebaseline for Bayesian uncertainty in deep learning. Advances in neural information processing systems,32, 2019. Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller, and Edward Teller.Equation of state calculations by fast computing machines.The Journal of Chemical Physics, 21(6):10871092, 1953.",
  "Yueqi Ren, Babak Shahbaba, and Craig E Stark. Hierarchical, multiclass prediction of Alzheimers clinicaldiagnosis using imputed, multimodal NACC data. Alzheimers & Dementia, 18:e066698, 2022": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximateinference in deep generative models.In International conference on machine learning, pp. 12781286.PMLR, 2014. Gareth O. Roberts and Jeffrey S. Rosenthal. Optimal scaling of discrete approximations to Langevin diffu-sions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 60(1):255268, 1998. Matthias W. Seeger, Christopher K. I. Williams, and Neil D. Lawrence. Fast forward selection to speed upsparse gaussian process regression. In International Workshop on Artificial Intelligence and Statistics, pp.254261. PMLR, 2003.",
  "DatasetMethodMSECPTime (s)ESSminESS/sspdup": "SimulatedDNN0.71-3531---DatasetEnsemble-DNN0.7491.3%14633---BNN-VI1.1488.8%8408---BNN-Lasso0.9887.4%6941---BNN-MC-Dropout0.8393.4%2861---BNN-SGHMC0.7991.6%41281(109, 829, 1642)0.0021.00BNN-SGHMC(first 200)4.4892.8%3970(21, 66, 162)0.0052.01SWAG0.7390.0%13488(93, 1163, 1542)0.0062.61BNN-RNS-HMC1.6287.5%6046(126, 948, 1510)0.0217.89BNN-pCN0.8189.3%42523(107, 844, 1533)0.0020.95FBNN (pCN-SGHMC)0.9679.6%4512(132, 1241, 1615)0.02911.08FBNN (pCN-pCN)0.8685.2%4631(94, 975, 1621)0.0207.69FBNN (SGHMC-SGHMC)0.8290.1%4497(137, 1236, 1638)0.03011.53FBNN (SGHMC-pCN)0.7492.2%4312(186, 912, 1606)0.04316.33 WineDNN0.43-26---QualityEnsemble-DNN0.4147.4%137---BNN-VI0.6639.5%28---BNN-Lasso0.6340.9%42---BNN-MC-Dropout0.6732.3%12---BNN-SGHMC0.5351.3%505(111, 837, 1538)0.2191.00BNN-SGHMC(first 200)2.7554.6%61(13, 111, 150)0.2130.91SWAG0.5348.2%97(98, 1021, 1489)1.0104.39BNN-RNS-HMC0.6244.7%38(107, 925, 1520)2.81512.24BNN-pCN0.6551.1%620(99, 1003, 1532)0.1590.69FBNN (pCN-SGHMC)0.5232.2%68(91, 912, 1533)1.3385.95FBNN (pCN-pCN)0.6524.5%67(105, 1087, 1540)1.5676.86FBNN (SGHMC-SGHMC)0.5040.0%70(77, 806, 1536)1.1004.78FBNN (SGHMC-pCN)0.5248.1%57(92, 897, 1536)1.6147.33 BostonDNN3.21-14---HousingEnsemble-DNN3.1772.1%74---BNN-VI7.6083.7%85---BNN-Lasso6.2079.2%68---BNN-MC-Dropout10.1281.2%91---BNN-SGHMC3.8375.3%888(76, 649, 1536)0.0851.00BNN-SGHMC(first 200)7.4066.9%86(9, 87, 150)0.1041.22SWAG5.8171.9%104(68, 724, 1532)0.6537.22BNN-RNS-HMC9.4273.4%76(73, 1032, 1504)0.96010.6BNN-pCN3.2579.3%901(76, 649, 1536)0.0840.88FBNN (pCN-SGHMC)4.1641.7%186(71, 965, 1543)0.3814.22FBNN (pCN-pCN)3.8147.1%186(80, 966, 1541)0.4304.78FBNN (SGHMC-SGHMC)4.1548.9%94(69, 979, 1542)0.7348.22FBNN (SGHMC-pCN)3.8281.1%91(93, 938, 1543)1.02111.94 AlzheimerDNN0.49-326---DatasetEnsemble-DNN0.4289.3%1597---BNN-VI0.5387.6%341---BNN-Lasso0.5283.5%561---BNN-MC-Dropout0.6092.8%268---BNN-SGHMC0.4991.6%6524(102, 973, 1376)0.0151.00BNN-SGHMC(first 200)3.1772.7%641(7, 82, 150)0.0110.69SWAG0.7489.3%1214(106, 1002, 1542)0.0875.58BNN-RNS-HMC0.6192.4%7324(96, 892, 1531)0.0130.83BNN-pCN0.5189.9%6212(120, 1092, 1448)0.0191.23FBNN (pCN-SGHMC)0.5090.2%643(116, 994, 1504)0.18011.53FBNN (pCN-pCN)0.5691.4%682(108, 998, 1498)0.17110.93FBNN (SGHMC-SGHMC)0.5588.4%671(118, 1012, 1541)0.17611.24FBNN (SGHMC-pCN)0.4891.6%632(149, 984, 1497)0.21813.97 YearPredictionMSDDNN74.54-1569---DatasetEnsemble-DNN72.8990.47%7929---BNN-VI78.2188.43%2146---BNN-Lasso79.4489.04%3243---BNN-MC-Dropout83.0887.89%1287---BNN-SGHMC81.6783.81%25533(122, 1005, 1540)0.0041.00BNN-SGHMC(first 200)105.9294.13%2613(7, 84, 149)0.0020.56SWAG93.8786.33%3841(113, 987, 1537)0.0297.35BNN-RNS-HMC92.8280.19%17289(109, 925, 1563)0.0061.57BNN-pCN82.4585.74%25655(124, 873, 1631)0.0041.20FBNN (pCN-SGHMC)74.9288.73%2815(143, 1049, 1676)0.05010.63FBNN (pCN-pCN)74.0390.45%3046(138, 992, 1618)0.0459.48FBNN (SGHMC-SGHMC)76.6990.40%2974(146, 973, 1599)0.04910.27FBNN (SGHMC-pCN)73.4192.23%2724(156, 934, 1608)0.05711.98",
  "DatasetMethodAccTime(s)ESS(min,med,max)minESS/sspdupECE": "SimulatedDNN96%4257----DatasetEnsemble-DNN97%17415---0.382BNN-VI90%4275---0.399BNN-Lasso92%3189---0.363BNN-MC-Dropout87%2912---0.277BNN-SGHMC95%43841(47, 212, 1459)0.0011.000.471BNN-SGHMC(first 200)83%4218(21, 59, 156)0.0044.640.498SWAG81%4731(81, 773, 1368)0.01715.970.482BNN-RNS-HMC69%1309(135, 1190, 1493)0.10396.200.513BNN-pCN91%49774(36, 207, 1417)0.0010.670.475FBNN (pCN-SGHMC)93%5179(134, 959, 1419)0.05124.130.409FBNN (pCN-pCN)91%4858(146, 921, 1412)0.05828.030.423FBNN (SGHMC-SGHMC)95%4517(149, 891, 1540)0.03230.760.406FBNN (SGHMC-pCN)96%4489(154, 911, 1602)0.07032.000.396 AdultDNN85%426----DatasetEnsemble-DNN84%2153---0.556BNN-VI80%562---0.642BNN-Lasso83%256---0.631BNN-MC-Dropout82%187---0.540BNN-SGHMC83%5979(16, 202, 1520)0.0021.000.574BNN-SGHMC(first 200)78%581(1, 41, 148)0.0020.950.594SWAG79%1641(47, 912, 1532)0.02810.700.668BNN-RNS-HMC72%6110(89, 960, 1530)0.0145.440.658BNN-pCN83%6227(9, 117, 1518)0.0010.540.616FBNN (pCN-SGHMC)82%642(87, 892, 1539)0.13550.630.580FBNN (pCN-pCN)82%639(88, 890, 1540)0.13751.460.592FBNN (SGHMC-SGHMC)83%612(68, 941, 1541)0.11141.520.583FBNN (SGHMC-pCN)84%609(89, 875, 1539)0.14654.910.576 AlzheimerDNN82%51----DatasetEnsemble-DNN83%262---0.542BNN-VI72%61---0.546BNN-Lasso76%256---0.524BNN-MC-Dropout76%12---0.429BNN-SGHMC81%2736(81, 588, 1526)0.0291.000.499BNN-SGHMC(first 200)69%282(8, 84, 149)0.0280.960.523SWAG81%312(72, 913, 1562)0.2317.690.508BNN-RNS-HMC58%293(84, 915, 1540)0.2869.550.521BNN-pCN73%2660(71, 424, 1534)0.0260.900.469FBNN (pCN-SGHMC)76%277(76, 947, 1542)0.2749.260.568FBNN (pCN-pCN)77%274(70, 931, 1542)0.2558.330.377FBNN (SGHMC-SGHMC)80%278(81, 973, 1538)0.2918.630.448FBNN (SGHMC-pCN)84%280(92, 914, 1535)0.32811.090.376 MnistDNN92%231----DatasetEnsemble-DNN94%1129---0.312BNN-VI86%273---0.417BNN-Lasso87%184---0.445BNN-MC-Dropout89%212---0.328BNN-SGHMC90%8641(15, 364, 1456)0.0011.000.280BNN-SGHMC(first 200)78%916(1, 34, 149)0.0010.620.271SWAG83%1294(15, 431, 1376)0.0117.720.327BNN-RNS-HMC69%4541(14, 372, 1394)0.0032.050.349BNN-pCN88%8912(17, 398, 1471)0.0011.260.321FBNN (pCN-SGHMC)88%927(15, 412, 1383)0.01610.780.352FBNN (pCN-pCN)90%931(16, 393, 1421)0.01711.450.312FBNN (SGHMC-SGHMC)91%923(18, 409, 1461)0.01913.010.283FBNN (SGHMC-pCN)94%914(23, 474, 1521)0.02516.770.241 celebADNN80%3689----DatasetEnsemble-DNN82%15445---0.569BNN-VI80%1132---0.622BNN-Lasso79%2159---0.561BNN-MC-Dropout78%1641---0.512BNN-SGHMC81%17234(19, 383, 1537)0.0011.000.567BNN-SGHMC(first 200)69%1849(1, 85, 149)0.0010.630.642SWAG70%8913(85, 1014, 1467)0.0098.650.534BNN-RNS-HMC72%1835(72, 951, 1494)0.03935.590.612BNN-pCN76%19676(19, 331, 1538)0.0010.950.529FBNN (pCN-SGHMC)80%1972(99, 1155, 1542)0.05045.530.565FBNN (pCN-pCN)79%1951(116, 1155, 1542)0.05953.930.542FBNN (SGHMC-SGHMC)81%1904(93, 978, 1544)0.04844.300.568FBNN (SGHMC-pCN)85%1892(129, 785, 1517)0.06861.840.493 SVHNDNN96%3748----DatasetEnsemble-DNN97%17415---0.382BNN-VI90%4275---0.399BNN-Lasso92%3189---0.363BNN-MC-Dropout87%2912---0.277BNN-SGHMC91%18639(31, 379, 1528)0.0011.000.221BNN-SGHMC(first 200)74%2515(5, 67, 262)0.0011.190.246SWAG83%6294(78, 499, 1383)0.0127.450.282BNN-RNS-HMC71%9083(19, 410, 1384)0.0021.250.299BNN-pCN93%17812(27, 398, 1317)0.0010.910.275FBNN (pCN-SGHMC)93%1931(36, 391, 1403)0.01811.210.263FBNN (pCN-pCN)91%1927(29, 372, 1280)0.0159.040.318FBNN (SGHMC-SGHMC)96%1891(34, 412, 1464)0.01710.810.248FBNN (SGHMC-pCN)96%1884(47, 486, 1533)0.02414.990.223"
}