{
  "Abstract": "Pretraining for partial differential equation (PDE) modeling has recently shown promise inscaling neural operators across datasets to improve generalizability and performance. De-spite these advances, our understanding of how pretraining affects neural operators is stilllimited; studies generally propose tailored architectures and datasets that make it challeng-ing to compare or examine different pretraining frameworks. To address this, we comparevarious pretraining methods without optimizing architecture choices to characterize pre-training dynamics on different models and datasets as well as to understand its scaling andgeneralization behavior. We find that pretraining is highly dependent on model and datasetchoices, but in general transfer learning or physics-based pretraining strategies work best.In addition, pretraining performance can be further improved by using data augmentations.Lastly, pretraining can be additionally beneficial when fine-tuning in scarce data regimes orwhen generalizing to downstream data similar to the pretraining distribution. Through pro-viding insights into pretraining neural operators for physics prediction, we hope to motivatefuture work in developing and evaluating pretraining methods for PDEs.",
  "Introduction": "Pretraining is an immensely popular technique in deep learning in which models learn meaningful contextfrom a large dataset and apply this knowledge to downstream tasks (Devlin et al., 2019; Chen et al., 2023;Schiappa et al., 2022). In particular, recent work has highlighted the importance of self-supervised learning,which can leverage the inherent structure of unlabeled data and learn meaningful latent representations(Bardes et al., 2022; Chen et al., 2020; Leyva-Vallina et al., 2023; He et al., 2021). The success of these self-supervised pretraining strategies has motivated their application to broad scientific and engineering problems(Wang et al., 2022a;b; Cao et al., 2023; Zhou & Farimani, 2024a; Meidani et al., 2024; Li et al., 2024; Nguyenet al., 2023). In particular, pretraining has been used in partial differential equation (PDE) modeling toimprove neural operators and evaluate their scalability and generalizability (McCabe et al., 2023; Hao et al.,2024).",
  "Published in Transactions on Machine Learning Research (09/2024)": "Noise Gaussian noise is added to data samples and targets through sampling a Gaussian at zero mean anda prescribed variance: Xnoise = X +2N(0, I). Empirically, we set the variance to 107; when noiselevels are too high, model performance can significantly deteriorate. Shift Using the Fourier shift theorem, samples can be shifted in space and resampled in the spectral domain(Brandstetter et al., 2022).Shifting PDE solutions in space preserves physics, since the PDEsconsidered in this work are invariant across space. Mathematically, this can be verified by derivingor looking up the Lie groups for the 2D Advection, Heat, and Burgers equations, for which there aremany, and noting that the solutions can be shifted along the X or Y axes (Ibragimov, 1993). Weuniformly sample the magnitude of the shift between [0.5, 0.5]. Scale Scaling PDE solutions respects physics for the Heat and Advection equations, but not the Burgersequation. However, we still choose to include this augmentation to evaluate the effect of physicallyinconsistent augmentations; in practice, scaling PDE solutions still improves model performance.The implementation is done by multiplying PDE solutions by a constant, which we uniformly samplebetween [0.5, 0.5].",
  "Related Works": "The field of neural operators has grown rapidly in recent years, with many architectures developed toaccurately approximate solutions to PDEs (Li et al., 2021; 2023a; Gupta & Brandstetter, 2022; Brandstetteret al., 2023; Lu et al., 2021a). Many works expanded on this to propose architectures to solve PDEs morequickly, with less compute, or on irregular grids (Li et al., 2023b; Hemmasian & Barati Farimani, 2023; Liet al., 2023c), and as a result, within a range of test problems, neural operators can solve PDEs quickly andaccurately. However, neural operators still struggle to generalize across diverse physics, and as a result manyapproaches have been developed to pretrain neural operators. We summarize these past works in ,and briefly describe the main approaches here. Furthermore, we review additional work from the broaderfield of pretraining and transfer learning due to its relevance to PDE problems.",
  "Transfer": "Darcy, ElasticityFine-tuning task layers to transfer between domainsGoswami et al. (2022)Poisson, INSDirect transfer across PDE domainsChakraborty et al. (2022)Poisson, INS, Wave, FPDesiging a transferrable model by modifying neurons Zhang et al. (2023c)Heat, Adv, Nag, Burg, NS, ACCombining operator modules with gatingTripura & Chakraborty (2023)NS, ElasticityPos. encoding and masked pretraining across PDEsRahman et al. (2024)",
  "Large Models": "Poisson, HelmScaling model and datasets to characterize transferSubramanian et al. (2023)CNS, SWE, DiffReactEmbedding PDEs to a common space, with Axial ViT McCabe et al. (2023)INS, CNS, SWE, DiffReactDenoising & Fourier attention with large models/data Hao et al. (2024)Adv, Burg, Diff-Sorp, SWE, NS Aligning LLM guidance across diverse PDEsShen et al. (2024)INS, CNS, SWE, DiffReactConditional transformer across large PDE datasetsHang et al. (2024)Poisson, Helm, NS, Wave, ACScaling operator transformers to large datasetsHerde et al. (2024) ContrastiveKdV, Burg, KS, INSLie Symmetries for contrastive learningMialon et al. (2023)Heat, Advection, BurgPhysics-informed distance in a contrastive framework Lorsung & Farimani (2024)Burg, Adv-Diff, NSPhysical invariances to contrastively learn an encoder Zhang et al. (2023b) Meta-LearningHGO, Elasticity, TissueModel-agnostic meta-learning loss across tasksZhang et al. (2023a)LV, GS, NSNovel loss term to maximize learning between PDEs Yin et al. (2021)LV, GS, GO, NSHyper-network to adapt operators for specific tasksKirchmeyer et al. (2022)",
  ": A review of past works on pretraining neural operators for PDEs. We organize works by approximatecategories and describe their data and methods": "with different domains of 2D Darcy Flow and Elasticity problems. Another approach proposed by Tripura &Chakraborty (2023) is to design different operators that learn specific PDE dynamics and combine these ina mixture of experts approach, motivated by the observation that PDEs can often be compositions of eachother. To address the issue of transferring between physical domains that can have different numbers of vari-ables, Rahman et al. (2024) extend positional encodings and self-attention to different codomains/channels.",
  "Large PDE Modeling": "An extension of transfer learning is to train large models on diverse physics datasets, with the intention oflearning transferable representations through scaling behavior (Wei et al., 2022; Kaplan et al., 2020; Brownet al., 2020). 61 initially explores this scaling behavior by training large neural operator models on largePDE datasets to evaluate its ability to adapt to different coefficients. McCabe et al. (2023) propose a tailoredarchitecture for solving problems across different physics, and Hao et al. (2024) expand on this by makingarchitectural advancements and training on more diverse physics. Despite different approaches and datasets,these works generally rely on tailored, scalable architectures for large PDE datasets; pretraining is framed asphysics prediction across diverse physics and fine-tuning is done on the pretraining distribution or on unseencoefficients/PDEs.",
  "PDE Contrastive Learning": "Following the success of contrastive learning in the vision domain (Chen et al., 2020; Bardes et al., 2022;Zbontar et al., 2021), various methods for PDE contrastive learning have been proposed. Mialon et al. (2023)propose a contrastive learning framework in which augmented PDE samples are represented in a similar wayin latent space; notably augmentations are done with physics-preserving Lie augmentations (Brandstetteret al., 2022).Zhang et al. (2023b) follow a similar approach in which physically invariant samples areclustered together in latent space, while Lorsung & Farimani (2024) rely on PDE coefficients to define acontrastive loss. In general, contrastive methods have extensive literature and theory, however they tend tobe challenging to pretrain and may have incremental gains in the PDE domain.",
  "Pretraining and Transfer Learning": "While PDE problems contain unique structure and challenges, works from the broader field of pretrainingand transfer learning can still be leveraged to inform hypotheses and inspire novel work.In particular,many previous works seek to understand how models can learn representations that transfer across tasks ordomains, as well as design architectures and pretraining tasks to optimize this (Jiang et al., 2022; Liu et al.,2023a; Zhuang et al., 2020). Under this framework, most previous work in the PDE domain is consideredsupervised pretraining, with the notable exception of contrastive methods. Additionally, novel works tend tofocus on improving PDE-specific architectures, rather than designing specific objectives for task or domaintransfer. We hypothesize that this is a reflection of the need for numerical accuracy in physics problems; priorwork generally focuses on empirically improving prediction accuracy on different benchmark PDEs ratherthan leveraging theoretical justifications. This could highlight potential future opportunities to examinemore complex or principled pretraining learning methods such as adversarial learning or causal learning.",
  "Lie Point Symmetry Data Augmentations": "We consider a recent body of work proposing Lie Point Symmetry Data Augmentations (Brandstetter et al.,2022; Mialon et al., 2023), a set of PDE-specific data augmentations that preserve the underlying dynamics.Mathematically, given a PDE, one can derive a set of transformations {g1, g2, ..., gn}, each with a parameter{1, 2, ..., n} that can be randomly sampled to modulate the strength of the transformation. Since somePDEs may exhibit more Lie symmetries than others, we consider only shifting the PDE solution in space(Shift), which is valid for all PDEs considered, to ensure a fair comparison between datasets. For furtherdetails on mathematical theory and its implementation in augmenting PDEs, we refer the reader to Mialonet al. (2023) and Olver (1986).",
  "Physics-Agnostic Data Augmentations": "In computer vision literature, many successful data augmentations heavily modify inputs (Chen et al., 2020);in particular, cropping and cutting out portions of an image would not respect physics if adapted to thePDE domain. Following this, we investigate the effect of data augmentations that are physics-agnostic, inthat they can be applied to any PDE since the augmentation does not preserve the underlying dynamics.Following recent work on denoising neural operator architectures (Hao et al., 2024), we consider addingGaussian noise during pretraining (Noise). Furthermore, we consider scaling the PDE solution (Scale), inwhich the PDE solution values are scaled by a random constant, an approach similar to a color distortion,whereby the hue, contrast, saturation, or lightness of an image is scaled. Although these visual characteristicsdo not have direct counterparts in the PDE domain, their scaling can interpreted as scaling the intensity ofa PDE solution. For certain simple PDEs, scaling can preserve physics, but this is not generally true due tononlinearities in more complex PDEs, such as the Burgers equation or Navier-Stokes equations. Additionaldetails on hyperparameters and the implementation of data augmentations can be found in Appendix D.4.",
  "Computer Vision Strategies": "Inspired by diverse pretraining strategies to learn image representations, we adapt many pretraining strategiesfrom the computer vision (CV) domain to the PDE domain. In general, these strategies aim to train modelsthrough predicting visual attributes or sorting spatio-temporal sequences to learn visual representationswithout labels. Firstly, we consider an early work that pretrains a model to verify if a video is in the correct temporal order(Misra et al., 2016). This problem is formulated as a binary classification task in which a shuffled videoand the original video are assigned separate labels; within this work, we refer to this as Binary pretraining.Subsequent work proposed methods that not only verify temporal order, but can also sort temporally shuffledvideo frames (Lee et al., 2017). This is generally formulated as a nway classification task, where n denotesthe number of permutations in which a sequence of frames can be sorted. In the context of physics data,we can opt to shuffle the data spatially or temporally, as such we refer to these two pretraining strategiesas TimeSort or SpaceSort. Empirically, SpaceSort does not perform well, so we omit this strategy fromour results and analysis. For completeness, we include a preliminary set of results demonstrating this inAppendix E. An extension of sorting samples that have been shuffled along a single dimension (e.g., time, space) is to sortsamples shuffled across all dimensions. For images, sorting images shuffled along both the x and y axes isimplemented by solving jigsaw puzzles, a challenging task that reassembles an image from its shuffled patches(Noroozi & Favaro, 2016). This work has been extended to the video domain by solving spatio-temporalpuzzles (Kim et al., 2018). The extension to PDE data requires sorting data that have been partitionedinto discrete patches and shuffled along the space and time axes; we refer to this strategy as Jigsaw. Oneissue is that the number of possible classes scales with the factorial of the number of patches, and manyshuffled sequences are not significantly different from each other. To mitigate this, we sample the top kshuffled permutations that maximize the Hamming distance between the shuffled and the original sequence(Noroozi & Favaro, 2016); this ensures that models can see diverse samples during pretraining while limitingthe number of classes in the pretraining task.",
  "Contrastive Strategies": "A common strategy for pretraining in computer vision domains is to exploit similarities in the data to alignsamples in latent space. A proposed strategy to do this for PDEs is Physics Informed Contrastive Learning(PICL), which uses a Generalized Contrastive Loss (Leyva-Vallina et al., 2023) to cluster PDE data based ontheir coefficients in latent space (Lorsung & Farimani, 2024). Another strategy for self-supervised learningof PDE dynamics is using an encoder to align Lie augmented or physically invariant latent PDE samples(Mialon et al., 2023; Zhang et al., 2023b). Both works require the use of a specific encoder along withthe neural operator backbone, since a separate model is contrastively pretrained to provide conditioninginformation to a backbone for physics prediction. To adapt these strategies to our experimental setup, weconsider directly pretraining the neural operator contrastively with these strategies. However, these methodsdid not seem to show significant improvements over no pretraining, as such, they are omitted from our mainresults and analysis. Interested readers are directed to Appendix E for preliminary results regarding thesemethods.",
  "Experiments": "To evaluate the effectiveness of the proposed pretraining strategies and data augmentations, we consider adiverse set of experiments and neural operator architectures to train on. In particular, we hope to understandwhether different architectures or datasets influence pretraining performance and construct a holistic view ofpretraining for diverse PDE applications. We provide an overview of the setup and the different experimentspossible in .",
  "Data": "We consider predicting physics for the 2D Heat, Advection, Burgers, and incompressible Navier-Stokesequations. These equations describe a diverse range of fluid phenomena and form tasks of varying difficulties.For our experiments, we consider pretraining on a combined set of 2D Heat, Advection, and Burgers data,which contain 9216 data samples (3072 for each equation), as well as fine-tuning on a smaller set of 1024unseen samples for each PDE. We only pretrain on the Heat, Advection, and Burgers equations since thenumerical data for these PDEs are easier to generate, and as a result, transferring pretrained knowledge tomore challenging PDEs can be evaluated as a potentially useful method.",
  "tu + u(c u) 2u = 0,Burgers(3)": "To ensure a diverse set of physics data, the equation coefficients are randomly sampled according to Zhou& Farimani (2024b).In particular, for the Heat equation, we sample , for theAdvection equation, we sample c = [cx, cy] [0.1, 2.5]2, and for the Burgers equation, we sample [7.5 103, 1.5 102], and c = [cx, cy] [0.5, 1.0]2; we refer to this dataset as in-distribution (In). Sincethese equations also comprise the pretraining set, we additionally consider a case where the downstreamdataset comes from a separate distribution; in this case, we sample for the Heatequation, c = [cx, cy] [2.5, 3.0]2 for the Advection equation, and [5.0 103, 7.5 103], and c =[cx, cy] [1.0, 1.25]2 for the Burgers equation. We refer to this dataset as out-of-distribution (Out).",
  "In all cases, periodic boundary conditions are enforced and the solution is solved in a domain (x, y) = 2": "from t = 0 to t = 2. Furthermore, initial conditions are randomly from a summation of sine functions;the parameters are uniformly sampled from from Aj [0.5, 0.5], j [0.4, 0.4], lxj {1, 2, 3}, lyj {1, 2, 3}, j [0, 2) while fixing J = 5, L = 2:",
  "f(x, y) = A(sin(2(x + y)) + cos(2(x + y)))(6)": "We formulate this problem with periodic boundary conditions, variable viscosity , and variable forcingfunction amplitude A. Specifically, the viscosity is sampled uniformly from {{1, 2, 3, 4, 5, 6, 7, 8, 9} 10{6,7,8,9}} and the amplitude is uniformly sampled from A {{1, 2, 3, 4, 5, 6, 7, 8, 9, 10} 103}. The datais generated in a domain (x, y) = 2 and from t = 0 to t = 7.75, following the setup from Lorsung et al.(2024); furthermore, the initial conditions 0 are generated from a Gaussian random field according to Liet al. (2021).",
  "Neural Operators": "To compare different pretraining and data augmentation strategies, we consider their effects on improvingthe PDE prediction performance of different neural operators. Specifically, we consider the neural operators:Fourier Neural Operator (FNO) (Li et al., 2021), DeepONet (Lu et al., 2021a) and OFormer (Li et al., 2023a).Additionally, we consider the Unet model; while it is not explicitly a neural operator, it is commonly used inliterature and has shown good performance (Ronneberger et al., 2015; Gupta & Brandstetter, 2022). Theseneural operators are first trained using a pretraining strategy before being fine-tuned on a PDE predictiontask; this could either be fixed-future prediction to model a static solution or autoregressive prediction to",
  "Pretraining Strategies": "We compare models pretrained with different strategies with a baseline model that has not been pretrained(None) as well as a model trained with the same physics prediction objective on the pretraining dataset,more commonly known as transfer learning (Transfer). Furthermore, we vary the size of the fine-tuningdataset to study the effects of pretraining when given scarce downstream data. The fine-tuning dataset isalso varied between data samples that are within the pretraining distribution (In), outside the pretrainingdistribution with respect to the PDE coefficients (Out), or on samples from an unseen PDE (NS). Lastly,we study the effects of adding data augmentations during pretraining and fine-tuning.",
  "Fixed Future and Auto-regressive Prediction": "To model physics problems with static solutions, we consider predicting a PDE solution field at a fixedtimestep after an initial snapshot of the PDE data. In particular, given the PDE data from t = 1 to t = 8,models are trained to predict the PDE solution at t = 32. Alternatively, to model physics problems with time-dependent solutions, we consider auto-regressively pre-dicting PDE solutions directly after a current snapshot of PDE data. This is implemented using PDE dataon the interval [t, t + 8) as an input to predict future PDE solutions on the interval [t + 8, t + 16). In addi-tion, we use the pushforward trick (Brandstetter et al., 2023) to stabilize training. This introduces modelnoise during training by first predicting a future time window from ground-truth data and then using thisnoisy prediction as a model input; importantly, no gradients are propagated through the first forward pass.Additional details on training parameters can be found in D.5.",
  "Comparison of Pretraining Strategies": "We benchmark our proposed PDE pretraining strategies on different neural operators and datasets, andshow the condensed results for auto-regressive prediction in . For a detailed comparison, we presentresults of different PDE pretraining strategies for fixed-future and auto-regressive tasks on all datasets inAppendix A. Additionally, we consider cases where the fine-tuning dataset contains coefficients unseen duringpretraining, and present these out-of-distribution results in Appendix A as well. Through these experiments, we find multiple insights. Firstly, we observe that the pretraining performancevaries with the choice of model and dataset. Specifically, different models benefit differently from pretraining,as well as based on the predicted PDE and task (i.e. fixed-future vs. auto-regressive). However, transferlearning generally performs well across different tasks, models, and datasets, suggesting that it is a goodchoice for a pretraining task. This is also reflected in the literature, where previous work generally focuseson transferring knowledge between datasets (Chen et al., 2021; Goswami et al., 2022; Chakraborty et al.,2022; Tripura & Chakraborty, 2023) or pretrain by predicting physics of large datasets (Hao et al., 2024;McCabe et al., 2023; Subramanian et al., 2023). We hypothesize that transfer learning is effective sincePDE data is inherently unlabeled; physics prediction uses future timesteps as a label, similar to next-tokenprediction for GPT models, which is cast as self-supervised learning. When the data is sufficient, usingsurrogate objectives such as derivatives or sorting sequences may not be as effective as the true objective offixed-future of auto-regressive prediction. Another observation is that pretraining frameworks are generallydependent on specific architectures; for example, many CV pretraining strategies shuffle patches of data,which can introduce arbitrary discontinuities and high-frequency modes in FNO models, yet are not aschallenging for convolutional models such as Unet. Furthermore, pretraining strategies are also dependenton the downstream task; for example, Derivative pretraining works well for auto-regressive prediction butnot fixed future prediction, as the solution at a distant timestep is very different from the current derivatives,but the solution at the next timestep is highly dependent on the current derivatives. Secondly, we observe that directly adapting computer vision methods to the physics domain generally resultsin poor performance. In many experiments, using a CV pretraining method would often hurt performancecompared to not pretraining. This points to a general difference between CV and physics tasks. In thevision domain many downstream tasks are classification-based (i.e.ImageNet, Object Detection, etc.),which results in many pretraining tasks modeled around classification, whereas physics prediction is a high-dimensional regression task. Beyond this, physics predictions not only need to be visually consistent, but alsonumerically accurate, which can be difficult to learn from a classification task. In fact, using physics-based",
  "pretraining methods, such as transferring between prediction tasks, regressing derivatives, or a physics-informed contrastive loss, generally results in better performance": "Lastly, we observe that different models have different capacities for pretraining. For example, the OFormerarchitecture, which is based on transformers, benefits greatly from pretraining in many scenarios; this couldbe because transformers lack inductive bias and can model arbitrary relationships. Furthermore, Unet ar-chitectures also benefit consistently from pretraining; this is reflected in common convolutional architecturesused for pretraining in the CV domain, such as ResNet (He et al., 2015). DeepONet and FNO show smallerimprovements with pretraining, suggesting that the architectures are less tailored for pretraining. This isespecially true for FNO; we hypothesize that the learned Fourier modes may be very different between tasks,resulting in challenges when transferring weights to new tasks.",
  "Comparison of Data Augmentations": "To study the effects of augmenting data during pretraining and finetuning, we conduct experiments in whichdata augmentations are added to three pretraining strategies (None, Transfer, PICL). These experimentsare run to compare data augmentations to a baseline model that is not pretrained, as well as its effectson the most effective pretraining strategies (i.e.Transfer, PICL). The results are summarized in for auto-regressive prediction, and the complete results can be found in Appendix B. We find the bestaugmentation by considering the pretraining strategy and augmentation pairing with the lowest error. Tocalculate its improvement, this error is compared to a model that is not pretrained, however, to isolate theeffect of pretraining, this can be compared to a corresponding pretrained model without data augmentationin the Appendix. We find that different models benefit from different augmentations; for example, DeepONet performs wellwith shifted data, but OFormer performs well with noised data. However, across models, datasets, anddownstream tasks, one can generally find a data augmentation that improves performance. This suggeststhat the most effective pretraining frameworks should incorporate a data augmentation strategy, and indeedthe best-performing models considered in this study often make use of data augmentations. Furthermore,when an augmented model is compared to its baseline, the performance tends to increase even when thebaseline is pretrained, albeit modestly. Transfer learning performs best in nine of our 12 cases, and shiftaugmentation performs best in eight of our 12 cases, with their combination performing best in six, suggestingthat this combination improves performance best across different data sets and models. We believe that dataaugmentations can help due to the fact that PDE data remains scarce; numerical simulation is needed forhigh quality data, and as a result emulating a larger dataset with augmentations is beneficial.",
  "Scaling Behavior": "We compare the effect of pretraining for different numbers of downstream samples in . We measurethis effect by finding the best pretraining method for a given model, PDE, and dataset size, then calculatingits improvement over no pretraining; after calculating the improvement, we average this metric across theHeat, Advection, and Burgers PDEs for auto-regressive prediction. In general, we observe a trend in whichthe improvement of pretrained models diminishes as the number of fine-tuning samples increases, which isexpected as fine-tuning data approaches the pretraining dataset size. It follows that if the downstream datais abundant, directly training on this would be optimal. Additionally, despite these trends, the relative im-provement of different pretraining strategies remains approximately constant between different downstreamdataset sizes. An exception to these trends is the FNO model; we hypothesize that learned Fourier modes maybe more challenging to fine-tune than other learning mechanisms such as attention matrices or convolutionalkernels. For a detailed comparison of the scaling behavior in individual datasets and models, we refer readers toAppendix C. Empirically, we observe a higher variance between random seeds when using a smaller datasetfor fine-tuning. Furthermore, the advection equation can generally be learned with fewer samples and theperformance is approximately constant with increasing dataset size. Additionally, different models and pre-training strategies display different scaling behaviors, with some models and pretraining strategies displayinggreater increases in performance when fine-tuning to scarce data. This further underscores the importanceof proper architecture choices that scale well, such as using transformer-based neural operators. Lastly,scaling behavior is more pronounced in fixed-future experiments; this could be because there is less datain fixed-future experiments due to only predicting a single target per data sample as opposed to predictingmultiple targets across a longer auto-regressive rollout.",
  "Generalization Behavior": "We compare the effect of varying the distribution of the downstream dataset on the performance of pretrainedmodels. In particular, we compare fine-tuning to unseen coefficients of the same equation (Out) as wellas fine-tuning to an unseen PDE with novel initial conditions and forcing terms (NS); these results areshown in with 500 fine-tuning samples for auto-regressive prediction. In general, we observe reducedperformance when fine-tuning to the Navier-Stokes equations, compared to fine-tuning to samples within thepretraining distribution (In). For certain models, this also holds when fine-tuning to a dataset with unseencoefficients (Out). These generalization behaviors are also approximately consistent between different samplesizes of the fine-tuning dataset. It is important to note that certain pretraining frameworks generalize betterthan others; for example, Coefficient pretraining largely hurts performance, since the fine-tuning distributioncontains different coefficients by construction.",
  "Theoretical Insights": "There are many insights from the broader deep learning community that can help inform our results. In ourstudy, an important factor in pretraining performance is the choice of model architecture, which is reflectedboth in both PDE and deep learning literature. Many recent PDE works have chosen to include attentionmodules in their architectures (Hao et al., 2024; Herde et al., 2024; Li et al., 2023a; McCabe et al., 2023;Hang et al., 2024), which is supported by the observation that transformer architectures can be expressiveby making the least assumptions on the structure of data (Jiang et al., 2022).We similarly find thatOFormer tends to display larger improvements compared to other architectures. Furthermore, Kolesnikovet al. (2020) find that training on larger datasets is vital for transferability, which we also observe by limitingthe fine-tuning data. Additionally, the use of data augmentations can be understood as assembling a more comprehensive trainingset such that the distance between the training set and validation/test sets is reduced (Shorten & Khoshgof-taar, 2019). In the context of PDEs, physics data can contain different coefficients, initial conditions, andboundary conditions; augmentations can help to mimic invariances to these conditions (e.g., spatially shiftingdata to model different initial conditions, scaling data to mimic different coefficients, etc.) such that modelscan perform well in spite of these challenges. The utility of data augmentation is further exacerbated by thedifficulty in generating high-quality PDE data from numerical simulation, which could motivate future workin exploring more complex data augmentation approaches such as generating synthetic samples from GANs(Sandfort et al., 2019) or neural style transfer (Gatys et al., 2016). Lastly, we consider the connection of different PDE pretraining strategies to broader deep learning pretraininginsights.In general, it is understood that unsupervised learning is beneficial due to the abundance ofunlabeled data and the human labor required to label samples (Bengio et al., 2021). However, in the contextof PDE data, the goal is usually to predict a future frame of a current solution or to upsample low-resolutionphysics, both of which do not require human intervention to label data. In addition, numerical simulationis required to generate PDE data, which is the limiting factor in assembling large datasets. As a result, wehypothesize that this limitation is the main factor as to why unsupervised PDE methods relying on surrogateobjectives do not perform as well as supervised pretraining, also known as transfer learning. Currently,unsupervised methods rely on the same data as supervised methods, both because of the computationalcost of numerical simulation and because of the lack of human labor needed to label supervised samples.Addressing this limitation could be a promising direction for future work on unsupervised PDE learning.",
  "Best Practices": "Our experiments have demonstrated that transfer learning combined with the shift augmentation performsbest in the majority of our experiments. For pretraining strategies, we believe that transfer learning performsbest because additional pretraining strategies are not able to extract additional, task-specific informationbetter than transfer learning. Having identical training tasks is simply more effective in this case. For dataaugmentation, the shift augmentation is the only augmentation that preserves the underlying dynamics ofour data since all of our systems are spatially invariant. We believe that this improves performance overthe other augmentation strategies, which do not preserve the underlying dynamics. For further applications,transfer learning + shift augmentation is a good initial pretraining strategy, but practitioners will likelyneed to tune and adapt additional strategies to achieve maximal performance. Furthermore, previous work(Brandstetter et al., 2022) has demonstrated improved performance by combining multiple augmentations,which may help across many systems. However, due to computational cost, we leave this to future work for",
  "Conclusion": "In this work, we compare pretraining strategies for PDEs by examining pretraining frameworks that can beused across different models and datasets. In particular, we consider adapting CV pretraining to the PDEdomain through sorting spatio-temporal data to learn underlying dynamics without labels. Furthermore, wederive several PDE characteristics that can be predicted, such as its coefficients, derivatives, or reconstructedinput. Lastly, we implement existing contrastive as well as transfer learning strategies to construct a diverseset of pretraining strategies. Notably, these strategies can be applied to any model and PDE problem andare flexible to future advances in architectures or datasets. Through pretraining with different frameworks and data augmentations, we compare their effects on dif-ferent PDEs, models, downstream datasets, and fine-tuning tasks. We find that pretraining can be highlydependent on model and dataset choices, but in general transfer learning or physics-based strategies do well.Furthermore, we find that directly adapting pretraining strategies from other domains often fails, motivat-ing the need to design PDE-specific pretraining frameworks. Lastly, we observe that different models havedifferent capacities for pretraining, with transformer and CNN based architectures benefiting the most frompretraining and highlighting the need for architectures that have high capacity and transferability. To further understand PDE pretraining, we investigate the effect of adding data augmentations and varyingthe fine-tuning dataset. We find that data augmentations consistently benefit performance, with the shiftaugmentation showing best performance most often. Combining transfer learning with shift augmentationshows the best performance in the majority of test cases. Additionally, pretraining performance is accentu-ated when the fine-tuning dataset is scarce or similar to the pretraining distribution. Through establishinga deeper understanding of pretraining for PDEs, we hope that future work can leverage these insights topropose new pretraining strategies and expand on current architectures.",
  "Johannes Brandstetter, Daniel Worrall, and Max Welling. Message passing neural pde solvers, 2023": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners, 2020. Zhonglin Cao, Rishikesh Magar, Yuyang Wang, and Amir Barati Farimani.Moformer: Self-supervisedtransformer model for metalorganic framework property prediction. Journal of the American ChemicalSociety, 145(5):29582967, 2023.doi: 10.1021/jacs.2c11420.URL PMID: 36706365.",
  "Wuyang Chen, Jialin Song, Pu Ren, Shashank Subramanian, Dmitriy Morozov, and Michael W. Mahoney.Data-efficient operator learning via unsupervised pretraining and in-context learning, 2024": "Xinhai Chen, Chunye Gong, Qian Wan, Liang Deng, Yunbo Wan, Yang Liu, Bo Chen, and Jie Liu. Transferlearning for deep neural network-based partial differential equations solving. Advances in Aerodynamics,3(1):36, December 2021. ISSN 2524-6992. doi: 10.1186/s42774-021-00094-7. URL Ze Cheng, Zhongkai Hao, Xiaoqiang Wang, Jianing Huang, Youjia Wu, Xudan Liu, Yiru Zhao, SongmingLiu, and Hang Su. Reference neural operators: Learning the smooth dependence of solutions of pdes ongeometric deformations, 2024.",
  "Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deepnetworks, 2017": "Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neuralnetworks. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24142423,2016. doi: 10.1109/CVPR.2016.265. Somdatta Goswami, Katiana Kontolati, Michael D. Shields, and George Em Karniadakis. Deep transferoperator learning for partial differential equations under conditional shift. Nature Machine Intelligence, 4(12):11551164, December 2022. ISSN 2522-5839. doi: 10.1038/s42256-022-00569-2. URL Number: 12 Publisher: Nature Publishing Group.",
  "Zijie Li, Anthony Zhou, Saurabh Patil, and Amir Barati Farimani. Cafa: Global weather forecasting withfactorized attention on sphere, 2024": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier neural operator for parametric partial differential equations, 2021. Zongyi Li, Nikola Borislavov Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Prakash Otta, Moham-mad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, and Anima Anandku-mar. Geometry-informed neural operator for large-scale 3d pdes, 2023c.",
  "Lu Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. DeepXDE: A deep learning library forsolving differential equations. SIAM Review, 63(1):208228, 2021b. doi: 10.1137/19M1274067": "Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George EmKarniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions)based on fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, April 2022.ISSN 0045-7825.doi:10.1016/j.cma.2022.114778.URL Michael McCabe, Bruno Rgaldo-Saint Blancard, Liam Holden Parker, Ruben Ohana, Miles Cranmer, Al-berto Bietti, Michael Eickenberg, Siavash Golkar, Geraud Krawezik, Francois Lanusse, Mariel Pettee,Tiberiu Tesileanu, Kyunghyun Cho, and Shirley Ho. Multiple Physics Pretraining for Physical SurrogateModels, October 2023. URL arXiv:2310.02994 [cs, stat].",
  "Tapas Tripura and Souvik Chakraborty. A foundational neural operator that continuously learns withoutforgetting, October 2023. URL arXiv:2310.18885 [cs]": "Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-supervised spatio-temporal representation learning for videos by predicting motion and appearance statistics. 4 2019. URL Yuyang Wang, Rishikesh Magar, Chen Liang, and Amir Barati Farimani.Improving molecular con-trastive learning via faulty negative mitigation and decomposed fragment contrast. Journal of Chem-ical Information and Modeling, 62(11):27132725, 2022a. doi: 10.1021/acs.jcim.2c00495. URL PMID: 35638560. Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning ofrepresentations via graph neural networks. Nature Machine Intelligence, 4(3):279287, Mar 2022b. ISSN2522-5839. doi: 10.1038/s42256-022-00447-x. URL Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.",
  "A.1Fixed Future Experiments": ": Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 500 samples for each PDE. Normalized L2 errors (101) are calculated on 256 validation samples andaveraged over five seeds. The lowest errors are given in dark grey , and second lowest errors are given in",
  "A.2Auto-regressive Experiments": ": Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 500 samples for each PDE. Normalized L2 errors (101) are calculated on 256 validation samples andaveraged over five seeds. The lowest errors are given in dark grey , and second lowest errors are given in",
  "B.1Fixed Future Experiments": ": Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 500 samples for each PDE. Each baseline (e.g, None, PICL, Transfer) is followed by its variants withdifferent augmentations (e.g, Noise, P-Shift, T-Scale). Normalized L2 errors (101) are calculated on 256validation samples and averaged over five seeds. The lowest errors are given in dark grey , and second lowest",
  "B.2Auto-regressive Results": ": Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 500 samples for each PDE. Each baseline (e.g, None, PICL, Transfer) is followed by its variants withdifferent augmentations (e.g, Noise, P-Shift, T-Scale). Normalized L2 errors (101) are calculated on 256validation samples and averaged over five seeds. The lowest errors are given in dark grey , and second lowest",
  "We generate data according to the equations outlined in 4.1. We provide additional details here:": "Pretraining During pretraining, 9216 total samples are generated, with 3072 samples of the 2D Heat,Advection, and Burgers equations respectively.The samples are generated with a resolution of(nt, nx, ny) = (32, 64, 64) or (nt, nx, ny) = (32, 32, 32) on the domain (x, y) = 2 from t = 0 tot = 2; the discretization depends on the downstream resolution of the data. We sample equationcoefficients from a defined pretraining distribution. Heat, Advection, and Burgers equation samplesare generated with a finite-differences scheme; a first-order central difference is used to discretize thediffusive term, a first-order upwinding scheme is used to discretize the nonlinear convection term,and time is discretized with a forward Euler scheme. In addition, the advection equation is solvedwith its analytical solution. Training/Finetuning During training/fine-tuning, we generate equations using a procedure similar to pre-training and sample coefficients either in the pretraining distribution or from a disjoint distributionto test generalization to unseen coefficients.For fine-tuning on the Navier-Stokes equations, weuse a higher resolution of (nt, nx, ny) = (32, 64, 64), otherwise experiments are run with a resolu-tion of (nt, nx, ny) = (32, 32, 32). We generate 1024 samples for the Heat, Advection, Burgers, andNavier-Stokes equations to train with. An additional 1024 out-of-distribution samples for the Heat,Advection, and Burgers equations is also generated. Additionally, the Burgers equation, initial con-ditions are unchanged to evaluate fine-tuning to a reference problem undergoing different dynamics,such as in design optimization problems (Cheng et al., 2024). Validation Validation samples are generated similarly to fine-tuning samples, also with equation coefficientssampled from either the pretraining or disjoint distribution. We generate 256 samples for the Heat,Advection, Burgers, and Navier-Stokes equations.",
  "Hidden channels16# Blocks8Dim Scaling(1,2,4)# Params1M": "We implement modern FNO and Unet architectures according to Gupta & Brandstetter (2022). Furthermore,we implement DeepONet architectures according to DeepXDE (Lu et al., 2021b), and use the originalimplementation for OFormer (Li et al., 2023a). The hyperparameters used for the models are described in. For FNO, we use the standard spectral convolution and residual layers, as well as additional inputchannels for the x, y, and time coordinates and multiple timesteps. The Unet implementation uses multipleConvNext blocks followed by an upsample/downsample block at each layer.Additionally, an attentionblock is used at the middle layer between upsampling and downsampling. When implementing OFormer,an attention module is used to aggregate information from the inputs as well as output query coordinates;this is aggregated into a latent encoding which is propagated with an MLP and decoded at future timesteps.Lastly, DeepONet uses an implementation that bundles multiple timesteps into both the branch and trunknet MLPs, these predictions are combined to ouput multiple future timesteps.",
  "D.3Pretraining Details": "During pretraining, different strategies require different implementations and hyperparameters. A considera-tion is that many models need a linear head during pretraining to project model outputs to the classificationor regression dimension. Since models will be used for physics prediction, their outputs will be in the shapeof the solution field, rather than cross entropy probabilities or regressed values. We use a lightweight CNNprojector to downsample and flatten model outputs to the desired dimension. Models are generally trainedfor 200 epochs with a batch size of 32 using Adam with a learning rate of 1e-3, a weight decay of 1e-6, and aOneCycle scheduler for five seeds. A note is that for binary pretraining, the task is much easier and thereforethe pretraining can be done with 100 epochs. Binary: Binary pretraining is implemented by shuffling a sample in time and randomly choosing a shuffledor original input with corresponding labels of 0 or 1 to be used for classification. We use a CNN headto project model outputs to a single logit for a binary cross-entropy loss. Within this frameworkthere are a few design decisions. The difficulty of the task can be modulated by the Hammingdistance between the shuffled sample and the original sample. For example, if the shuffled sample isnot changed much (e.g. only two frames are swapped), the difference between a sorted and shuffledsample is small and thus more challenging to distinguish. We can leverage this to gradually decreasethe Hamming distance of shuffled samples to incrementally increase the difficulty of the task overpretraining. Empirically, this does not make a large difference during training so we choose to omitthis curriculum learning for simplicity. An additional consideration is the probability of sampling a shuffled or sorted sample. In theory,there are many more shuffled samples than sorted samples (i.e. more labels with 0 vs. 1); therefore,it may be beneficial to sample more shuffled samples and use a weighted binary cross-entropy loss.In practice this does not significantly affect training, so we uniformly sample sorted or shuffledsamples. A final consideration is that PDE solutions generally do not exhibit large changes in time,therefore, we patchify the time dimension when shuffling to create larger changes in shuffled patches.In general, models are able to learn to distinguish between shuffled and original inputs very well,and we display t-SNE embeddings of a pretrained FNO model on a validation set of shuffled andunshuffled samples in a. TimeSort/SpaceSort: Sorting along a single dimension is implemented by patchifying the solution fieldalong the desired dimension and shuffling these patches. This is done to create more distinct dif-ferences in the shuffled solution, with the patch size controlling the number of permutations of the",
  "max(i,j). The distance between samples is calculated in two parts for a given time": "t: dsystem(ui, uj) = ut+1i utj, and dupdate = F(G(ui)) G(uj), where G is our parameterizedmodel, and F() is our numerical update. dupdate is anchored to dsystem to account for mode collapse,giving us the loss function: dphysics(ui, uj) = dsystem(ui, uj) dupdate(ui, uj)2. is a hyperpa-rameter that defines a margin, above which samples are considered to be from different classes. Forpretraining, we construct the operator coefficient vector as = [cBurgers , , cAdvection]",
  "D.5Fine-tuning Details": "During fine-tuning, models trained until convergence for fixed-future or auto-regressive prediction and re-peated for five seeds. In fixed-future prediction, models are given the solution field at t = [0, 8) and thetarget is at t = 32. For auto-regressive prediction, models are given the solution field at t = [0, 8) and thetarget is at t = [8, 16). After this prediction, the models use their own output to predict the next stept = [16, 24) until the time horizon of t = 32. To stabilize auto-regressive rollout, we implement temporalbundling and the pushforward trick (Brandstetter et al., 2023). Losses are calculated using a relative L2norm (Li et al., 2021); validation losses are averaged across batch size and accumulated over timesteps or, inthe case of fixed-future prediction, at only one timestep. For experiments with different fine-tuning samplesizes, samples are randomly chosen from 1024 possible samples to reach the desired number of samples foreach seed. These fine-tuning datasets are independently resampled from the same pool for each seed, andlarger datasets are not required to be strict supersets of the smaller datasets, which could increase the vari-ance of results. We use an Adam optimizer with a learning rate of 1e-3, with weight decay of 1e-6, and aCosineAnnealing scheduler. All experiments are run on a NVIDIA GeForce RTX 2080Ti GPU.",
  "E.1Sorting Spatially Shuffled Sequences": "In this section, we compare SpaceSort with a baseline without pretraining. Both models are fine-tuned on 500samples that are within the pretraining distribution from the respective PDEs on a fixed-future predictionobjective. We hypothesize that spatially shuffling PDEs may confuse models during pretraining since itdestroys important information, as physical systems need to be continuous in space. : Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 500 samples for each PDE. L2 errors (103) are calculated on 256 validation samples and averaged overfive seeds.",
  "E.2Lie Contrastive Self-Supervised Learning": "In this section, we compare VICRegBardes et al. (2022) against baseline FNO training for a fixed-futureprediction task. We use values of = 1, = 25, = 25, matching values from the original paper. In thiscase, we use the first 8 frames to predict the system state at frame 32. : Models are pretrained on 9216 combined 2D Heat, Advection, and Burgers samples and finetunedon 100 samples from the Heat equation. L2 errores (103) are calculated on 256 validation samples andaveraged over five seeds."
}