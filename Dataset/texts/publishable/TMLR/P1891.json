{
  "Abstract": "Importance estimators are explainability methods that quantify feature importance for deepneural networks (DNN). In vision transformers (ViT), the self-attention mechanism natu-rally leads to attention maps, which are sometimes interpreted as importance scores thatindicate which input features ViT models are focusing on. However, attention maps donot account for signals from downstream tasks. To generate explanations that are sensitiveto downstream tasks, we have developed class-discriminative attention maps (CDAM), agradient-based extension that estimates feature importance with respect to a known classor a latent concept. CDAM scales attention scores by how relevant the corresponding to-kens are for the predictions of a classifier head. In addition to targeting the supervisedclassifier, CDAM can explain an arbitrary concept shared by selected samples by measur-ing similarity in the latent space of ViT. Additionally, we introduce Smooth CDAM andIntegrated CDAM, which average a series of CDAMs with slightly altered tokens. Our quan-titative benchmarks include correctness, compactness, and class sensitivity, in comparisonto 7 other importance estimators. Vanilla, Smooth, and Integrated CDAM excel across allthree benchmarks. In particular, our results suggest that existing importance estimatorsmay not provide sufficient class-sensitivity. We demonstrate the utility of CDAM in med-ical images by training and explaining malignancy and biomarker prediction models basedon lung Computed Tomography (CT) scans. Overall, CDAM is shown to be highly class-discriminative and semantically relevant, while providing compact explanations.Code available: : Extending attention maps (AM), the proposed class-discriminative attention maps (CDAM) quan-tify and visualize input features that are relevant for the target class in Vision Transformers (ViT). Wevisualize importance scores obtained with CDAM for a linear classifier with ViT-S/8, trained with DINO(Caron et al., 2021). Orange and blue colors correspond to positive and negative values, respectively.",
  "Introduction": "Vision transformers (ViT) are a family of computer vision models based on the transformer architecture(Vaswani et al., 2017), that demonstrate comparable or even better performance than convolutional neuralnetworks (CNN) on many object detection and classification tasks (Dosovitskiy et al., 2020; Wu et al., 2020).A powerful feature of the transformers is that the attention mechanism provides a built-in explanation ofthe models inner workings, if attention is understood as a weighing of the relevance of individual tokens(Mullenbach et al., 2018; Bahdanau et al., 2014; Serrano & Smith, 2019; Thorne et al., 2019). While attentionmaps provide intuitive and inherent interpretability, they fail to be discriminative with respect to a targetclass. Recognizing the potential of attention maps for explaining a ViTs inner workings, we have developedclass-discriminative attention maps (CDAM), a gradient-based extension that makes them sensitive to class-specific signals based on a classifier or a latent concept. Attention maps are the visualization of the self-attention of the class token query [CLS] in the final attentionlayer. Attention heads can be visualized separately, and in certain instances, may be related to semanticallymeaningful concepts (e.g., in language (Clark et al., 2019)).Recently, ViT trained with self-supervisedlearning (SSL) methods (Caron et al., 2021; Touvron et al., 2021; Oquab et al., 2024) have shown excellentperformance leveraging a greater amount of unlabeled data.Attention maps from such self-supervisedViT models provide high-quality semantic segmentations of input images. These attention maps clearlydistinguish regions of interest (ROI) from the background with very little noise, with different attention headsoften focusing on semantically distinct regions (Caron et al., 2021). There have been many improvements toViT including the introduction of convolution (Wu et al., 2021; Yuan et al., 2021) and spatial sparsity (Liet al., 2021). Attention maps, as well as our proposed CDAMs, are applicable to all ViT architectures withself-attention. The major shortcoming of attention maps, when considering them as an explainable AI (XAI) method, isthat they purely operate on the level of the last attention layer and therefore do not take into account anyinformation coming from a downstream task, such as a classifier head. The proposed CDAM overcomes thisshortcoming by calculating the gradients of the class score, i.e. the logit value of a target class, with respectto the token activations in the final transformer block (visualized in (a)). Essentially, CDAM scalesthe attention scores by how relevant the corresponding tokens are for the models decision. CDAM thereforeinherits the desirable properties of attention maps, but additionally indicates the evidence or counter-evidencefor a target class that the ViT relies on for its predictions (examples in ). We demonstrate that it isclass-discriminative in the sense that the heat maps clearly distinguish the object representing the targetclass from the background and, importantly, from objects that belong to other classes. Therefore, CDAMis a useful tool for explaining the decision-making process of classifiers with a ViT backbone. Note that thegoal of CDAM is to explain the classifier head and the final transformer block. In addition to the classifier-based explanations, we present a concept-based approach that replaces theclass score with the similarity measure in the latent space of the ViT, where a concept is defined throughselected images. Averaging the latent representations of the example images yields a concept embedding,which is analogous to a concept vector in variational autoencoders (VAE) and related generative models(Brocki & Chung, 2019). This allows obtaining a heat map for the target concept the model has neverbeen explicitly trained on, which is of particular interest for self-supervised models. We further demonstratewhen computing gradients of a class (.1) or concept similarity (.2) score, with respect tothe token activations in the final transformer block, smoothing or averaging could be applied to improveperformance. Specifically, we introduce two additional variants of CDAM; (1) Smooth CDAM averages overmultiple gradients that are computed with additional noise to tokens (inspired by SmoothGrad (Smilkovet al., 2017)) and (2) Integrated CDAM takes the integral of the gradients along the path from the baselineto the token in the final transformer block (inspired by IntGrad (Sundararajan et al., 2017)). We conduct several quantitative evaluations focusing on correctness, class sensitivity, and compactness.Correctness is measured by perturbing an increasing amount of input features and measuring the modeloutput (Brocki & Chung, 2023c;b).By using the ImageNet samples (Deng et al., 2009) with multipleobjects (Beyer et al., 2020) and applying importance estimators for different classes, we quantify the levelof class-discrimination. Sparsity and shrinkage of explanations are measured by the number of near-zero",
  "Published in Transactions on Machine Learning Research (11/2024)": "Figure S3: Correctness of importance estimators as measured by box sensitivity evaluation. The change inthe model output (logit) is measured as pixels within a ss box are perturbed. The sum of their importancescores would correlate with the change in the model output, if importance estimators are accurate. This isrepeated 100 times per sample and the Pearson correlation between the sum of importance scores and thechange in the model output is calculated, based on the Xmulti dataset. Figure S4: MIF/LIF perturbation curves for the evaluation of class-discrimination. Different classes aretargeted for obtaining importance scores and measuring model output (.5), a large Abox, therefore,indicates importance scores that are class-discriminative. See",
  "Related Works": "Interpretability of ViT, and more broadly explainable artificial intelligence (XAI), is an active area of re-search, since understanding their decision-making process would help improve the model, identify weaknessesand biases, and ultimately increase human trust in them. The proposed class-discriminative attention map(CDAM) is related to both attention and gradient-based explanation methods for deep learning models. Gradient-based methods operate by backpropagating gradients from the prediction score to the input featuresto produce feature attribution maps, which is also called saliency maps. Many variants have been proposed,see (Simonyan et al., 2013; Sundararajan et al., 2017; Selvaraju et al., 2017; Smilkov et al., 2017; Shrikumaret al., 2016) to just name a few. In particular, CDAM is similar to and inspired by the inputgradient method(Shrikumar et al., 2016), because it involves calculating the gradients with respect to tokens multiplied bytheir activation (Eqs. (2) and (4)). The main difference to most gradient-based methods is that we dontbackpropagate the gradients all the way to the input tokens. CDAM stops backpropagation at the tokensthat enter the final attention layer. In that regard, our method is related to GradCam (Selvaraju et al., 2017) which backpropagates the gradientsto the final convolutional feature map in a CNN. GradCam and CDAM therefore share the property thatthey operate on high-level features which is presumably the reason why they are both sensitive to thetargeted class, a feature that many explanation methods do not have (Rudin, 2019). Several methods haveattempted to apply gradient-based methods or Layer-Wise Relevance Propagation (LRP) (Bach et al., 2015)to Transformer architectures (Atanasova et al., 2020; Ali et al., 2022; Voita et al., 2019; Abnar & Zuidema,2020; Chefer et al., 2021b), mostly in the context of NLP tasks. GradCam for ViT backpropages the gradientsto the outputs of the final attention layer (Gildenblat, 2021). Attention has often been used to gain insight into a models inner workings (Mullenbach et al., 2018; Bah-danau et al., 2014; Serrano & Smith, 2019; Thorne et al., 2019) since it seems intuitive that this weighing ofindividual tokens correlates with how important they are for the performed task. There is also an ongoingdiscussion about whether attention provides meaningful explanations (Serrano & Smith, 2019; Wiegreffe &Pinter, 2019; Jain & Wallace, 2019). Furthermore, methods such as attention rollout (Abnar & Zuidema,2020) have been proposed that aim to quantify how the attention flows through the model to the input tokens.(Chefer et al., 2021a) defines the relevancy matrices, as the Hadamard product of the attention map andthe gradient of fc w.r.t. to the attention layer, accumulated at each layer. This relevance propagation (RP)method (Chefer et al., 2021a), to the best of our knowledge, shows the highest degree of class-discriminationin ViT models. This is used in our study as one of the baseline importance estimators. Many papers have been published on the problem of evaluating post-hoc explanation methods, leadingeven to the proposal of meta-evaluations (Hedstrm et al., 2023).In a recent overview of quantitativeevaluation methods (Nauta et al., 2023b) 12 desirable properties have been identified. We primarily focuson three of them, namely correctness, contrastivity, and compactness.In particular, carefully designedocclusion/perturbation studies can help us calculate the fidelity statistics (Brocki & Chung, 2023c), which isalso called the symmetric relevance gain (SRG) measure (Bluecher et al., 2024). Adebayo et al. (2018) firstdemonstrated that many importance estimators, which purport to be sensitive to the internals of a classifier,produce similar importance scores regardless of the relation of inputs and classes in the training data, whichthey called the data randomization test. Generally, if two importance scores with respect to two differentclasses produce very similar heat maps, the importance estimator is not class-sensitive. We quantify this",
  "Methods": "Adaptation of the transformer architecture to computer vision hinges on creating tokens from small patchesof the input image (Dosovitskiy et al., 2020). In addition to those image patch tokens, the classificationtoken [CLS] is added, which is utilized for downstream tasks such as classification. Attention maps are thetwo-dimensional visualization of the self-attention in the final attention layer with [CLS] as query token,where the attention scores are averaged over all heads. In our notation, [CLS] refers to the classificationtoken before and [CLS] after the final transformer block.",
  "to infer a class score fc or similarity score g(l, lc) (see Eq. (2) or Eq. (4)). (b) Detailed view of the finaltransformer block, adapted from (Dosovitskiy et al., 2020)": "Before fine-tuning, attention maps are not class-discriminative since they do not take into account any signalcoming from a downstream task. Therefore, we have developed class-discriminative attention maps (CDAM)that can be computed based on a known class or latent concept. In order to estimate a tokens relevance,CDAM computes gradients of a class score (.1) or concept similarity score (.2), obtaineddownstream using the [CLS] token, with respect to the token activations in the final transformer block(). Note that CDAM may not explain the full ViT architecture with multiple transformer blocks.Instead, CDAM focuses on the downstream task and the last transformer block. In the first scenario, the class score is obtained from the corresponding activation in the prediction vectorof a classifier trained on top of the ViT in a supervised fashion. In the second scenario, we define a conceptthrough a small set of examples (e.g. ten images of a dog) and obtain a concept vector by averaging theirlatent representations. Then, without training a classifier, a similarity measure between the concept vectorand the latent representation of a targeted sample image is computed and used as the target for the gradients.We call the resulting maps of feature relevance class-discriminative attention maps because they scale theattention scores",
  "and key matrices which pack together all the query and key vectors (Vaswani et al., 2017) and the indices,e.g. Ki, select single query or key vectors": "Self-supervised learning (SSL) methods such as DINO can train ViT models whose attention maps are high-quality semantic segmentations of the input, without using labels (Caron et al., 2021). Therefore, we utilizea ViT model (Dosovitskiy et al., 2020) pre-trained with DINO as a backbone for our main experimentsand applications. Furthermore, we demonstrate general applicability of CDAM by using other ViT models,namely pre-trained with Supervised Weakly through Hashtags (SWAG) (Singh et al., 2022), Data-efficientimage Transformers (DeIT) (Touvron et al., 2021) and DINOv2 (Oquab et al., 2024). We denote detailsabout the ViT architecture, training schemes, and model performance in the and Appendix A.5.Note that the heat maps shown in this article are scaled independently.",
  "CDAM for a known class": "To demonstrate our method, we have trained a linear classifier on the ImageNet (Deng et al., 2009) thatachieved an accuracy of 77.0%. We use random resized cropping and horizontal flipping with PyTorch defaultarguments as augmentation, Adam optimizer with learning rate 3 104, batch size of 128, and train for10 epochs. The parameters of the ViT backbone are frozen during training, so only the classifier head is",
  "jTijfcTij,(2)": "where Tij is the j-th component of the i-th token that is fed to the last attention layer (). We found inour experiments that calculating the gradients with respect to the layer-normalized tokens yields far betterresults than using the not normalized ones. This definition is a first-order estimation of the contribution of Ti to fc in the sense that it would be exactif fc depended on Tij only linearly. It is inspired by gradient-based explanation methods with the differencethat we dont backpropagate the gradients all the way to the input layer but only to the input tokens ofthe final attention layer. This allows us to gain insight into the decision-making process of the combinationof ViT and classifier based on high-level features. This approach leads to attention maps that are class-discriminative and well-aligned with the semantic relationships of the class and different parts of an image().",
  "CDAM for a latent concept": "Concept embeddings lc have been obtained by averaging latent representations of 30 images. Those imageswere randomly selected from a class in the ImageNet validation set (Deng et al., 2009). Then, CDAMs in were obtained using Eq. (4). We observe that CDAM reveals that the model clearly distinguishesthe parts of the images that are relevant for the latent concepts shared by selected images from the rest ofthe image. For example, when having selected images containing hammer (bottom right), CDAM primarilyhighlights the hammer. While the annotated classes were used for this experiment, the concepts sharedby those images are likely more nuanced, specific, and complex than the class itself. For example, CDAMalso provides positive importance scores for the wooden handles of the screwdrivers and the socket attachedthe ratchet, which imply relevance for the concepts shared by 30 images selected from the class hammer.Semantically, 30 images that are selected from a class hammer have a wooden handle (i.e., concept), whichis shared by multiple tools. Similarly, the socket at the top of the ratchet shares similarities with typicalmetal heads at the top of the hammer. Unrelated tools or parts appear in blue indicating that the modeldeems them to be unrelated to the concept hammer. CDAMs are discriminative with respect to the target concepts shared by selected samples (, right). Thismakes the proposed method particularly valuable for understanding the representations the ViT has learned,",
  "Tij.(4)": "Possible choices for g are for example the L2 distance, cosine similarity or dot product, where we use thelatter in this article. The definition Eq. (4) estimates the contribution of Ti to g, such that it allows us togauge how much each token is driving the similarity between l and lc.",
  "Tij(6)": "where T i = Ti + N0, 2with N0, 2representing Gaussian noise with standard deviation . For ourexperiments, we used n = 50. For the baseline Ti we choose the null vector of 0. We implement thesetwo variants by first propagating an image through the ViT up to the final transformer block, where theactivations of the tokens are obtained. Next, the activations are manipulated by either applying the noise orinterpolation step and the manipulated tokens are then further propagated to obtain the final ViT output.",
  "Quantitative Evaluation": "We perform quantitative evaluations of the correctness, class sensitivity, and compactness of importanceestimators. In addition to our proposed methods (vanilla, Smooth, and Integrated CDAM), we performevaluations on attention maps, Input Gradients, relevance propagation (RP) (Chefer et al., 2021a), partialLayer-wise Relevance Propagation (LRP) (Voita et al., 2019), SmoothGrad (Smilkov et al., 2017), IntGrad(Sundararajan et al., 2017), and GradCam for ViT (Gildenblat, 2021). Note that based on classic LRP(Bach et al., 2015), partial LRP (Voita et al., 2019) provides improvement for multi-head self-attentionmechanisms in modern transformer architecture (Voita et al., 2019). Relevance propagation is a state-of-the-art importance estimator for explaining a prediction (e.g., class discriminatory) based on ViT (Cheferet al., 2021a). For IntGrad, SmoothGrad, Integrated CDAM, and Smooth CDAM, n = 50 is used. The noise() used in SmoothGrad and Smooth CDAM is selected based on the best performance on a validation setof 200 images. To evaluate the correctness (also known as faithfulness) of importance scores, we employ two versions ofinput perturbation-based approaches (Samek et al., 2016; Petsiuk et al., 2018; Kindermans et al., 2017).Generally, an input feature (e.g., a pixel) is masked and a resulting model performance is observed. In allcases, input features are perturbed by being replaced with their blurred versions. A Gaussian blur with = 14 turns pixels uninformative while minimizing perturbation artifacts (Brocki & Chung, 2023c). Fidelity. The first evaluation method uses accuracy curves obtained from the Most Important First (MIF)and the Least Important First (LIF) perturbations, which are abbreviated as MIF/LIF perturbation (Brocki& Chung, 2023c;b). Pixels are ranked by importance scores from a specific explainability method (e.g.,CDAM) and are perturbed in the MIF or LIF order. In the MIF perturbation, an accurate importancescore should lead to a relatively rapid decrease of the model output (logit) for the target class. For the LIFperturbation, we expect an inverse relationship. Note that some importance scores are unsigned (i.e., only",
  "Data": "We have used two subsets of the ImageNet (denoted as Xval and Xmulti) (Deng et al., 2009), as well aslung computed tomography (CT) scans from the Lung Image Database Consortium image collection (LIDC)(Armato III et al., 2011). The first one, Xval, is a randomly selected subset of the ImageNet validationset consisting of 1000 images (Deng et al., 2009). The second one, Xmulti, selects 1000 images from thevalidation set that contain multiple objects (Beyer et al., 2020). We filtered out instances in which theclass labels provided by (Beyer et al., 2020) actually refer to the same object (e.g. notebook and laptop) bycalculating and thresholding the cosine similarity of the embedded class names. Embeddings were obtainedusing OpenAIs text-embedding-3-large model. The Xmulti dataset is used to evaluate the degree of class-discrimination exhibited by the explanations. Tothis end, we obtain explanations w.r.t to two annotated classes, perform the MIF/LIF perturbation and boxsensitivity, and record the model output for the chosen target class. For example, for the top right samplein , we would obtain the explanations for the classes Tench and Golden Retriever (not shown) andrecord the model output for the class Golden Retriever. For the right target (Tench), we obtain a fidelitystatistics ALIFMIF. For the wrong target (Golden Retriever), ALIFMIF. In the worst case scenario, whereexplanations for those two distinct classes are the same, ALIFMIF = ALIFMIF and the resulting ALIF-MIFis zero. Equivalent for Abox. The Lung Image Database Consortium image collection (LIDC) contains 1018 records of lung CT scans,that were collected from and validated by seven academic centers and eight medical imaging companies(Armato III et al., 2011). Presence of nodules (nodule 3 mm; nodule < 3 mm; non-nodule 3 mm) isannotated by up to 4 human annotators. Segmentation of nodules (i.e., ROI) is the average contour given byannotators. Eight biomarkers that are informative in clinical practices are also collected in LIDC: subtlety,calcification, sphericity, margin, lobulation, spiculation, texture, and diameter. For clinical descriptions ofbiomarkers, refer to (Opulencia et al., 2011). After preprocessing, 854 nodule crops of size, correspondingbiomarkers, and labels (benign or malignant) were saved and used for downstream procedures. Overall, theLIDC dataset used in this study contains 443 benign examples and 411 malignant examples.",
  "Results": "For our experiments using the ImageNet (Deng et al., 2009), we use the ViT-S/8 architecture (with a patchsize of 88) trained with DINO as a backbone (Caron et al., 2021). Instead of using a ViT model that is pre-trained in a self-supervised manner, we demonstrate CDAMs directly from an alternative ViT architecturetrained with supervised weakly through hashtags (SWAG) (Singh et al., 2022) in Appendix A.5. We furtherdemonstrate applicability of CDAM on computed tomography (CT) scans, using DINO (Caron et al., 2021)in , as well as using Data-efficient image Transformers (DeIT) (Touvron et al., 2021) and DINOv2(Oquab et al., 2024) in Appendix A.5.2.",
  "trainable. The predictions of this classifier were then used to create the heat maps shown in viaEq. (2)": "CDAM clearly distinguishes the region of interest (ROI) semantically related to the targeted class withpositive values (yellow) from the rest of the image. Most backgrounds and other objects are assigned eitherzero (irrelevant) or negative scores (counter-evidence). Note that CDAM does not assign non-zero Si,c totokens that have zero attention in attention maps, see also the scatter plot between CDAM and attentionmap Fig. S1. CDAM therefore appears to compute Si,c by multiplying the attention with the relevance ofthe corresponding token for the targeted class. We discuss the proportionality of CDAM to the attentionscores in Appendix A.1. It is a nice feature since CDAM inherits the high-quality object segmentationspresent in attention maps. This qualitative evaluation of shrinkage and sparsity of CDAM is reflected in thequantitative measures for compactness (.2.1). One of the most useful features of CDAM is that it is highly sensitive to the chosen class.In (left), when either the zuccini or bell pepper class is chosen, CDAM clearly highlights that vegetable. Mostof the other vegetables are assigned either near-zero or negative values (indicated in blue). Many otherimportance estimators do not distinguish the target class or only to a lesser degree. In , targetingthe burger or the hotdog does not seem to change importance scores from partial LRP, Input Grad, andIntGrad. Relevance propagation demonstrates some selective focus on the ROI, but is not as discriminativeas CDAM (). This visual impression of class-sensitivity is supported by the quantitative evaluation in.2.2. Additional examples of CDAM and other interpretability methods are shown in Appendix A.3.",
  "FidelityBox SensitivityAMIFALIFALIF-MIFAbox": "CDAM404103863212.2Integrated CDAM375104566913.0Smooth CDAM282111082712.4Relevance Propagation31595764310.0Partial LRP4898373484.5Attention map4528463934.8SmoothGrad534529-40.4IntGrad2268025747.6GradCAM3009446438.6Input Grad2737134393.5 : Evaluation of correctness by fidelity and box sensitivity. In Xmulti, importance scores for the targetclass were used to perturb pixels. The changes in the model output w.r.t. the perturbation percentage(fLIF-MIF; Fig. S2) or the box size (fbox; Fig. S3) are quantified by the area under the curves. CDAM performs favorably in terms of the correctness of estimated feature importance compared to othermethods. For the dataset containing multiple objects (), the Smooth CDAM outperforms all othermethods on the fidelity evaluation using MIF/LIF perturbation (Fig. S2), with ALIF-MIF = 827 ().On the box sensitivity benchmark, Integrated CDAM (Abox = 13.0) performs the best, closely followed bySmooth CDAM (Abox = 12.4) (Fig. S3). Some methods, such as IntGrad, perform very well on the MIFbenchmark but poorly for LIF (). We consider the difference (LIF-MIF) to be more meaningful andintuitive as ideally, one would want a large area under the LIF perturbation curve and a small area under theMIF perturbation curve. Thus, larger ALIF-MIF indicates higher correctness. Particularly, a good score inMIF could stem from simply triggering perturbation artifacts of the model (Brocki & Chung, 2023b; Hookeret al., 2019). For the random subset of ImageNet, the Integrated CDAM demonstrates the best performance (Abox =12.9), closely followed by Relevance Propagation (Abox = 12.7) on the box sensitivity benchmark (Fig. S6,Table S1). Relevance Propagation outperforms all other methods on the MIF/LIF perturbation, followed bythe Smooth CDAM (Fig. S7).",
  "Evaluation of class discrimination": "Class discrimination (or sensitivity) is measured by the difference in fidelity statistics A(LIF-MIF) whenperturbing according to importance scores obtained for the correct or wrong target class. Larger A(LIF-MIF)is considered to indicate better performance. CDAM (A(LIF-MIF)=739), Smooth CDAM (A(LIF-MIF)=823), and Integrated CDAM (A(LIF-MIF)=708)clearly outperform other methods in terms of class discrimination (). IntGrad (540), Input Grad(433), and Relevance Propagation (293) follow. Surprisingly, Partial LRP and SmoothGrad perform defi-ciently. Whereas SmoothGrad has low class-discrimination because its importance scores have low accuracyto begin with, Partial LRPs scores have mediocre accuracy but it appears to be insensitive to the choiceof the target class ().By design, attention maps do not consider the target class and therefore,A(LIF-MIF)=0. The importance scores for CDAM become anti-correlated with the model output when the wrong importancescores are used for the perturbation (Fig. S5). This anti-correlation probably results from the fact thatpixels that are evidence for the correct target class are counter-evidence for the wrong class. This resultcorroborates the visual impression that CDAM correctly assigns importance scores with opposite signs to theobjects corresponding to the targeted class and other objects that are present (). This is also reflectedin the negative values of fLIF fMIF for CDAM (Fig. S4), indicating that the ranking from least to mostimportant has been, at least partially, reversed.",
  "Evaluation of compactness": "Sparsity and shrinkage are evaluated by our compactness evaluation, which is one of the major desiredproperties in the explainability of deep learning (Nauta et al., 2023a). While sparsity may not always implythe most accurate estimation at a single data point, a bias-variance trade-off is well-known in machine learning(Lahlou Kitane, 2022). Ideally, compact explanations (e.g., sparse and shrunken importance estimators) arepreferred, when other properties such as correctness and class sensitivity are constant. CDAM and Integrated CDAM resulted in the highest degree of compactness, together with Input Grad. Forthose three importance estimators, on average, 88% of importance scores were less than 5% of the maximumscore . IntGrad and SmoothGrad show relatively high sparsity 0.84 and 80%, respectively. But bothare lacking in correctness (.2.1) and class-discrimination (.2.2). Relevance Propagationshows the lowest sparsity of all considered methods 48%, but a high level of correctness.These resultssuggest that these correctness and compactness metrics are orthogonal.",
  "Application to Medical Images": "To demonstrate CDAM for another use case we apply it to nodule malignancy and biomarker prediction usingthe Lung CT scans in the LIDC dataset (Armato III et al., 2011). Here, we show CDAMs from DINO-basedmodels, we also showcase use of DeIT (Dosovitskiy et al., 2020; Touvron et al., 2021) and DINOv2 (Oquabet al., 2024; Darcet et al., 2024) backbones that are fine-tuned on the LIDC dataset in Appendix A.5.2.",
  "Malignancy Prediction": "After preprocessing, the LIDC data consists of 443 benign and 411 malignant lung CT scans. Training,validation, and test sets (in the ratios of 0.7225, 0.1275, 0.15) were stratified by and balanced according tothese labels, e.g., benign and malignant. We fine-tuned a ViT model, with a DINO backbone pre-trained onthe ImageNet, for 50 epochs. In a parameter sweep, we varied the number of trainable layers (10 50) anddropout rates (0.0 0.09), where the learning rate was exponentially decaying ( = 0.0003 and = 0.95).The best accuracy on the test set of 0.85 was obtained with 50 trainable layers and dropout rate of 0.0031.Attention maps and CDAM are obtained based on this model. The obtained attention maps suggest that the model focuses on patches with nodule fragments (Fig. S11(b)).However, we note that attention maps almost always focus on pulmonary nodules without taking into accountthe downstream classification into benign and malignant samples. CDAM provides detailed structures wherepositive and negative values are indicated by orange and blue, respectively. In this binary classification where0 is for benign and 1 for malignancy, input pixels with positive values in CDAMs are driving the classificationtowards malignancy.",
  "Biomarker prediction": "We turn our attention to clinical biomarkers in LIDC that are routinely used by medical practitioners.Inspired by the concept bottleneck model (CBM) (Koh et al., 2020), we fine-tuned a ViT model pre-trainedwith DINO on 8 biomarkers (subtlety, calcification, sphericity, margin, lobulation, spiculation, texture, anddiameter). Essentially, a regression model was built on a pre-trained ViT model with the mean squared error(MSE) as loss function. For investigation of LIDC and incorporation of CBM, see (Brocki & Chung, 2023a). CDAMs were obtained for 8 biomarkers (). In the context of the margin, patches including the edgeof a nodule get positive CDAM scores, whereas patches corresponding to the nodule body get negativeCDAM scores. For the diameter, patches corresponding to the nodule body get a positive score, whereas thebackground pixels exhibit neutral (near zero) or slightly negative scores. When it comes to sphericity, patchesincluding fragments of nodule curvature get a positive CDAM score. Spiculation refers to the presence of",
  "Discussion": "Class-Discriminative Attention Map (CDAM) is a gradient-based extension of attention maps, that pro-vides strong discrimination with respect to a chosen class or concept, while exhibiting excellent fidelity andcompactness. Our proposed methods retain appealing characteristics of attention maps, namely their high-quality semantic segmentation. The proposed importance estimation for CDAM scales the attention scoresby how relevant they are for a target class or concept (Appendix A.1). Therefore, zero attention scoresremain zero in CDAM and it produces compact explanations with high sparsity and shrinkage. Besidesexplaining predictions of the classifier on the top of ViT, CDAM can also provide importance scores specificto the user-defined concept. In the context of self-supervised models, where class labels are absent, thismakes it a valuable technique to investigate the latent representations learned by the model. Post-hoc explanation methods for DNNs aim to make the full decision-making process of the model moretransparent by providing an approximation with certain desirable properties, such as correctness, sensitivity,and compactness. Particularly, many explanation methods that estimate feature importance are not veryuseful because they give almost identical results when targeting different classes in the model output (Rudin,2019; Adebayo et al., 2018). Even randomization in the model weights is often shown to have minimalimpact on explanations (i.e., saliency maps (Adebayo et al., 2018)). We find, both qualitatively and quanti-tatively, that CDAM is highly sensitive to the targeted class, assigning positive importance scores to objectscorresponding to the target class and negative ones to other (semantically distinct) objects in the image. We introduce Smooth and Integrated CDAM, which essentially average a series of (vanilla) CDAMs. Notably,instead of adding noise to or modifying the input images as in SmoothGrad (Smilkov et al., 2017) or IntGrad(Sundararajan et al., 2017), respectively, our methods act on tokens in the final transformer block. Whilewe conducted a number of quantitative and qualitative evaluations, there is no single XAI method thattriumph in all scenarios. However, considering that Smooth and Integrated CDAM require computing nvanilla CDAMs (e.g., n = 50), we recommend first trying vanilla CDAMs.If high fidelity is singularlydesirable, we suggest utilizing Smooth CDAM. Our application on medical images exemplifies the need for fine-grained class-sensitive explanations offered byCDAM. Other explainability methods including attention maps often highlight whole nodules and tumors.However, they are uninformative about the inner workings of the ViT model predicting malignancy or",
  "Broader Impact Statement": "The introduction of our importance estimators would enhance the transparency, trustworthiness and account-ability of vision transformer models. By providing clearer insights into model decision-making processes,accurate explainability methods help foster broader adoption of AI technologies. However, the explanationscould be manipulated by adversarial input perturbation or data poisoning. In turn, enhanced explanationsmay inadvertently expose vulnerabilities, making models more susceptible to adversarial attacks or maliciousexploitation. Therefore, it is imperative to continuously monitor and ensure the robustness of explanationmethods while implementing safeguards to mitigate the risks associated with adversarial attacks. This work was partially funded by the SONATA BIS grant [2023/50/E/ST6/00694] from the NationalScience Centre of Poland (Narodowe Centrum Nauki). This work was partially funded by the ERA-NetCHIST-ERA grant [CHIST-ERA-19-XAI-007] long term challenges in ICT project INFORM (ID: 93603), bythe National Science Centre (NCN) of Poland [2020/02/Y/ST6/00071]. This research was carried out withthe support of the Interdisciplinary Centre for Mathematical and Computational Modelling University ofWarsaw (ICM UW) under computational allocation no GDM-3540; the IDUB program (Excellence Initiative -Research University), the NVIDIA Corporations Academic Hardware Grant; and the Google Cloud ResearchInnovators program. Samira Abnar and Willem Zuidema.Quantifying attention flow in transformers.In Proceedings of the58th Annual Meeting of the Association for Computational Linguistics, pp. 41904197. Association forComputational Linguistics, 2020.",
  "Marco Ancona, Enea Ceolini, Cengiz ztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104, 2017": "Samuel G Armato III, Geoffrey McLennan, Luc Bidaut, Michael F McNitt-Gray, Charles R Meyer, Anthony PReeves, Binsheng Zhao, Denise R Aberle, Claudia I Henschke, Eric A Hoffman, et al. The lung imagedatabase consortium (lidc) and image database resource initiative (idri): a completed reference databaseof lung nodules on ct scans. Medical physics, 38(2):915931, 2011.",
  "Lennart Brocki and Neo Christopher Chung. Feature perturbation augmentation for reliable evaluation ofimportance estimators in neural networks. Pattern Recognition Letters, 176:131139, 2023b": "Lennart Brocki and Neo Christopher Chung. Fidelity of interpretability methods and perturbation artifactsin neural networks. In Krystal Maughan, Rosanne Liu, and Thomas F. Burns (eds.), The First TinyPapers Track at ICLR 2023, Tiny Papers @ ICLR 2023, Kigali, Rwanda, May 5, 2023. OpenReview.net,2023c. URL Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal andencoder-decoder transformers. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 397406, 2021a. Hila Chefer, Shir Gur, and Lior Wolf.Transformer interpretability beyond attention visualization.InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 782791, 2021b. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at?an analysis of BERTs attention. In Tal Linzen, Grzegorz Chrupaa, Yonatan Belinkov, and DieuwkeHupkes (eds.), Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP, pp. 276286, Florence, Italy, August 2019. Association for Computational Linguistics.doi: 10.18653/v1/W19-4828. URL",
  "Timothe Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers.International Conference on Learning Representations (ICLR), 2024": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.",
  "James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. Explainable predictionof medical codes from clinical text. arXiv preprint arXiv:1802.05695, 2018": "Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jrg Schltterer,Maurice van Keulen, and Christin Seifert. From anecdotal evidence to quantitative evaluation methods:A systematic review on evaluating explainable ai. ACM Computing Surveys, 55(13s):142, 2023a. Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jrg Schltterer,Maurice van Keulen, and Christin Seifert. From anecdotal evidence to quantitative evaluation methods:A systematic review on evaluating explainable ai. ACM Computing Surveys, 55(13s):142, 2023b.",
  "Pia Opulencia, David S Channin, Daniela S Raicu, and Jacob D Furst. Mapping lidc, radlex, and lungnodule image features. Journal of digital imaging, 24:256270, 2011": "Maxime Oquab, Timothe Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-naeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research,2024.",
  "Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use inter-pretable models instead. Nature machine intelligence, 1(5):206215, 2019": "Wojciech Samek, Alexander Binder, Grgoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Mller.Evaluating the visualization of what a deep neural network has learned. IEEE transactions on neuralnetworks and learning systems, 28(11):26602673, 2016. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. InProceedings of the IEEE international conference on computer vision, pp. 618626, 2017.",
  "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Generating token-levelexplanations for natural language inference. arXiv preprint arXiv:1904.10717, 2019": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv Jgou.Training data-efficient image transformers & distillation through attention. International Conference onMachine Learning, 2021. URL Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. arXiv preprint arXiv:1905.09418,2019.",
  "Sarah Wiegreffe and Yuval Pinter. Attention is not not explanation. arXiv preprint arXiv:1908.04626, 2019": "Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka,Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual Transformers: Token-based Image Representationand Processing for Computer Vision. arXiv e-prints, June 2020. doi: 10.48550/arXiv.2006.03677. URL Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: IntroducingConvolutions to Vision Transformers.In Proceedings of the IEEE/CVF International Conference onComputer Vision (ICCV), pp. 2231. IEEE Computer Society, October 2021. ISBN 9781665428125. doi:10.1109/ICCV48922.2021.00009. URL Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolutiondesigns into visual transformers. In Proceedings of the IEEE/CVF international conference on computervision, pp. 579588, 2021.",
  "k,nAiVinnhklc,k,(14)": "where we assume the architecture of (Dosovitskiy et al., 2020) and use the projection of the tokens onto thekey vector Vi = TiW V with W V Rdmodel dv. We have also used the fact that the latent representation l(i.e. [CLS]) is the sum of value vectors weighted by the corresponding attention, l =",
  "plus [CLS] due to the residual connection, see (b). The function h : Rdv Rdmodel": "consists of the layer normalization, MLP, and residual connection and performs the final processing of the[CLS] token before it enters the classifier ((b)). n is the n-th component of the gradient and Q[CLS],Vi and Ki are the rows of matrices Q, V and K that correspond to the [CLS] and the i-th tokens.",
  "which does not have such a straightforward interpretation as the single-headed case due to the mixing of theattention heads": "If we think of W O as acting first on (h lc), creating a column vector of dimension hdv, we can understandEq. (17) as the sum of h directional derivatives of the dv-long segments of this columns vector in the directionsof Y 1 to Y h. Each of the summands is proportional to the corresponding An, and the interpretation ofCDAM from the single-head case therefore extends to the multi-head one. Although our observation Ai = 0 Si,c = 0 implies that all terms that are not proportional to Ai in Eq. (10)should vanish, we dont have a theoretical argument why that would be the case. It seems unlikely that thevarious terms in the first sum cancel out, which leaves Am",
  "Figure S1: Scatter plot of raw attention scores and token importance scores obtained from 200 randomlyselected samples of the ImageNet validation set Xval": "Figure S2: Fidelity of importance estimators based on MIF/LIF perturbation curves obtained from theXmulti dataset. The change in the model output is measured as a proportion of input features are perturbedaccording to the Least Important First (LIF) or the Most Important First (MIF) orders.Importanceestimators are evaluated by the area between LIF and MIF perturbation curves, which is equivalent tothe area underneath fLIF-fMIF (right). See .",
  "A.5Additional ViT architectures and training strategies": "While the main manuscript has used ViT models pre-trained using DINO (Caron et al., 2021), other archi-tectures and pre-trained weights can be used for CDAM. In this section, we show examples of alternativeViT models that are trained in alternative manners. As expected, pre-trained ViT models that producehigh-quality attention maps result in high-quality CDAMs. In particular, note that the patch size of DINO(Caron et al., 2021) is 8 8 compared to much larger patch sizes used in other backbones.",
  "Figure S14: Attention maps and CDAMs from the ImageNet-1K dataset, based on the pre-trained ViTmodel backbone using SWAG Singh et al. (2022)": "In general, ViT models require a massive amount of training data and therefore often trained in self-supervised manner. As shown in DINO (Caron et al., 2021), self-supervised learning can not only improveperformance of fine-tuned models, but also improve attention maps. CDAM relies on attention maps, there-fore the quality of attention maps strongly influence that of CDAM. Note that the SWAG model has a patchsize of 16 16, which results in lower-resolution attention maps and CDAMs.",
  "A.5.2Alternative ViT models for the LIDC": "We demonstrate the use of CDAM with ViT models trained using Data-efficient image Transformers (DeIT)(Touvron et al., 2021) and DINOv2 (Oquab et al., 2024). Note that despite the use of the same acronym,DINOv2 is substantially different from DINO (Caron et al., 2021) due to using a different training strategyintroduced in iBOT (Zhou et al., 2022) on a much larger scale (Oquab et al., 2024). The original DINO(Caron et al., 2021) was pre-trained on the ImageNet, whereas DINOv2 (Oquab et al., 2024) used a newlycurated dataset LVD-142M. Additional tokens called registers were introduced to improve attention mapsof DINOv2 (Darcet et al., 2024). The LIDC dataset was split into 5 folds and stratified according to malignancy status. The best-performingmodels were chosen based on means of Accuracy (ACC) and Mean Squared Error (MSE) over 5 folds, inthe validation set. In a parameter sweep, we varied the number of trainable layers (10 - all), dropout ratesin the backbone (0.0 - 0.12), batch size (8 - 32) and learning rate scheduler parameters. The learning ratewas scheduled with CyclicLR scheduler. Binary Cross Entropy Loss and Huber Loss were used to trainclassification and regression models, respectively. Weights optimization was performed with Adam optimizerand random rotation was applied as data augmentation. For malignancy classification, the best-performingmodels based on DeIT and DINOv2 achieved a mean ACC of 0.896 and 0.904, respectively. For biomarkerregression, the best-performing models based on DeIT and DINOv2 archived the mean MSE of 0.409 and0.35, respectively."
}