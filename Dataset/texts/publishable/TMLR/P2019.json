{
  "Abstract": "Training generative models that capture rich semantics of the data and interpreting thelatent representations encoded by such models are very important problems in un-/self-supervised learning. In this work, we provide a simple algorithm that relies on perturbationexperiments on latent codes of a pre-trained generative autoencoder to uncover an attributegraph that is implied by the generative model. We perform perturbation experiments tocheck for inuence of a given latent variable on a subset of attributes.Given this, we show that one can t an eective graphical model that models a structural equation modelbetween latent codes taken as exogenous variables and attributes taken as observed variables.One interesting aspect is that a single latent variable controls multiple overlapping subsetsof attributes unlike conventional approaches that try to impose full independence. Usinga pre-trained generative autoencoder trained on a large dataset of small molecules, wedemonstrate that the graphical model between various molecular attributes and latent codeslearned by our algorithm can be used to predict a specic property for molecules which aredrawn from a dierent distribution.We compare prediction models trained on various feature subsets chosen by simple baselines, as well as existing causal discovery and sparselearning/feature selection methods, with the ones in the derived Markov blanket from ourmethod.Results show empirically that the predictor that relies on our Markov blanket",
  "Introduction": "While deep learning models demonstrate impressive performance in dierent prediction tasks, they oftenleverage correlations to learn a model of the data. As a result, accounting for the spurious ones among thosecorrelations present in the data, which can correspond to biases, can weaken the robustness of a predictivemodel on unseen domains with distributional shifts. Such failure can lead to serious consequences whendeploying machine learning models in the real world.",
  "Published in Transactions on Machine Learning Research (08/2024)": "perturbation. Then, we apply a peeling algorithm that would actually nd the attribute that is associatedwith a specic latent. In other words, we would nd that attribute that occurs last in the ancestral orderthat is inuenced by that latent variable. We use this algorithm to nd a DAG with minimal edges that isconsistent with the attribute sets for every latent variable. These steps are described in more detail below.",
  "Disentangled/Invariant generative models": "Several prior works (Higgins et al., 2017; Makhzani et al., 2015; Kumar et al., 2017; Kim & Mnih, 2018)have considered learning generative models with a disentangled latent space. Many of them dene disentan-glement as the independence of the latent representation, i.e., changing one dimension/factor of the latentrepresentation does not aect others. This involves additional constraints on the latent distribution duringtraining as opposed to our method. More generally, disentanglement requires that a primitive transformation in the observed data space (suchas translation or rotation of images) results in a sparse change in the latent representation. In practice, thesparse changes can need not be atomic, i.e., a change in the observed space can lead to small but correlatedlatent factors. Additionally, it has been shown that it maybe impossible to achieve disentanglement withoutproper inductive biases or a form of supervision (Locatello et al., 2019). In Truble et al. (2021), the authorsuse small number of ground truth latent factor labels and impose desired correlational structure to the latentspace. Similarly, we extract a graphical model between latent representations of the generative model andthe attributes that are available in the form of metadata. Discovering the causal structure of the latent factors is an even more challenging but worthwhile pursuitcompared to discovering only the correlated latent factors as it allows us to ask causal questions such asones arising from interventions or counterfactuals. Causal GAN (Kocaoglu et al., 2017) assumes that thethe causal graph of the generative factors is known a priori and learns the functional relations by learninga good generative model. CausalVAE (Yang et al., 2021) shows that under certain assumptions, a linearstructural causal model (SCM) on the latent space can be identied while training the generative model.In contrast, here, we learn the graphical structure hidden in the pre-trained generative model in a post-hocmanner which does not aect the training of the generative model. Further, while the derived model showsdomain generalization, indicating learning of invariant structure across distributions, it does not focus oncapturing true causality. Therefore, our work is dierent from Besserve et al. (2019), as our goal is to learna graphical model in the latent space instead of the causal structure hidden in the decoder of the generativemodel. Leeb et al. (2023) proposes a latent response framework, where the intervention in the latent variablesreveals causal structure of the learned generative process as well as the relations between latent variables.The present work is distinct, as it aims to capture the response in the meta attribute space upon interventionon a latent dimension.",
  "Invariant representation learning": "Among conceptually related works, Arjovsky et al. (2019) focuses on learning invariant correlations acrossmultiple training distributions to explore causal structures underlying the data and allow OOD generaliza-tion. To achieve this, they propose to nd a representation such that the optimal classier given isinvariant across training environments. We leverage the fact that the pre-training encompasses data froma wide set of subdomains however our method does not require multiple explicit training environments.Dubois et al. (2020) propose a decodable information bottleneck objective to nd an optimal set of predic-tors that are maximally informative of the label of interest, while containing minimal additional informationabout the dataset under consideration to avoid overtting. Our work instead aims to derive an interpretablerepresentation of the data using xed attributes and guided by a learned graphical model.",
  "Generative autoencoders for molecular representation learning": "In recent years, generative autoencoders have emerged as a popular and successful approach for modeling bothsmall and macromolecules (e.g., peptides) (Gmez-Bombarelli et al., 2018; Das et al., 2021; Chenthamarak-shan et al., 2020; Homan et al., 2021). Often, those generative models are coupled with search or samplingmethods to enforce design of molecules with certain attributes. Inspired by the advances in text generation(Hu et al., 2017) and the widely used text annotations of molecules, many of those frameworks imposedstructure in the latent space by semi-supervised training or discriminator learning with metadata/labels. A",
  "Interpretability in molecular learning": "While machine learning models, including deep generative models, have been successful in deriving an in-formative representation of dierent classes of molecules, it remains non-trivial and largely unexplored toinfer the relationship between dierent physicochemical and functional attributes of the data. Though lawsof chemistry and physics oer broad knowledge on that relationship, those are not enough to establish the(causal) mechanisms active in the system. For that reason, most experimental studies deal with partiallyknown causal relationships, while confounding and observational bias factors (e.g., dierent experimentalconditions such as temperature, assays, solution buer) are abundant. In this direction, recently Ziatdinovet al. (2020) have established the pairwise causal directions between a set of data descriptors of the micro-scopic images of molecular surfaces. Interestingly, they found that the causal relationships are consistentacross a range of molecular composition. To our knowledge, this is the rst work that infers a graphical model between data attributes from the latentsof a generative model of molecules and shows the generalizability of the inferred model across dierent datadistributions.",
  "Problem setup": "Consider a generative model consisting of a decoder Dec that takes input latent code z Rd1 and generatesa data point x = Dec(z) Xl1 and an encoder Enc that takes a data point and embeds it in the latentspace z = Enc(x). We assume that it has been pre-trained using some training method (e.g., Kingma &Welling (2014); Makhzani et al. (2015)) on the data distribution P(x) sampled from the domain X. Wefurther have access to a vector of attributes a(x) R|A|1 as metadata along with datapoints x. In mostcases, these metadata attributes can be directly computed on x, i.e., ai(x) : Xl1 R. For those attributeswe cannot directly compute on x, we assume that we also have access to an attribute estimator trained onthe data z from pai() : Rd1 R producing a regression estimate for attribute ai(x) = pai(Enc(x)). The key problem we would like to solve is to nd the structure of the graphical model implied by thegenerative model and attribute classier combination, where latent codes z act as the exogenous variableswhile a(x) act as the observed variables. In other words, we hypothesize a structural equation model asfollows:",
  "ai(x) = fi(aPa(i)(x), zki), i [1 : |A|](1)": "Here, aPa(i) is a subset of the attributes a(x) = a1(x) . . . a|A|(x) that form the parents of ai(x). Also, notethat the exogenous variables (i.e. zki {z1 . . . zd}) are actually the latent representations/codes at theinput of the generator. We call zki the latent variable associated with attribute ai(). Together, the mapfrom the space of latents z to a(x) can be called the probabilistic mechanism a(x) = M(z) as representedby the graphical model given by {f1() . . . f|A|()}. We can dene a DAG G(A, E) where (i, j) E if i Pa(j) in the structural equation model. Because it isdependent on the latent representation z, we call this DAG the structure of the graphical model impliedby the generative model, or simply, the attribute graph. This means it may actually improve as state-of-the-art generative models improve. Our principal aim is to learn an attribute graph that is consistent withperturbation experiments on the latent codes zi. The attribute graph should thus be viewed as a summaryof how changes to the learnt latent components translate to changes in the predicted/estimated attributes,",
  "(d)": ": PerturbLearn overview illustrating the: (a) perturbation procedure involving pre-trained generativemodel Enc(), Dec() and attributes a(x) resulting in a(x, x), (b) weight matrix heatmap derivation fromperturbations (sparsity threshold s = 0.1), (c) DAG building from sparse weights (red indicates the attributeof interest and blue indicates the Markov blanket), (d) application of function f learned on Markov blanketfeatures to OOD samples from X .",
  "assuming these changes are mostly mediated through inuences among the attributes, rather than througha direct inuence from latents on attributes": "We then investigate how the attribute graph implied by the generative autoencoder can help train a predictorfor a given attribute from other attributes, which can generalize on OOD data. Suppose we want to predictthe property ai(x), then, using the DAG, we take the Markov blanket of ai, i.e., the set of attributesMB(ai) A Z which makes it independent of the rest of the variables.This consists of all parents, children, and co-parents in a DAG. Since this subset contains only the features relevant to predicting ai,we should end up with a robust model of minimal size. Furthermore, this feature selection is important forexplainability since it is justied by Bayesian reasoning. We assume the structure of the graph is valid acrossboth domains only the edge weights might change which we can easily ne-tune with minimal samplesnecessary from the new domain. We visualize PerturbLearn, our solution to this problem, in .",
  "PerturbLearn algorithm overview": "Our key idea is the following observation, if we take zki associated with attribute ai(x), perturbing it tozki would actually aect ai and all its descendants in the attribute graph. So, we rst obtain the sparseperturbation map between each latent zj and the subset of attributes Aj A that it inuences upon",
  "mean, unit variance) to scale the inuences since attributes may have vastly dierent ranges. Thisin turn leads to weights which can be compared against each other": "For the purposes of this work, we assume a linear relationship between latents and attributes although thestrength need not be precise, as explained later. Therefore, we then learn a linear model (OLS) to predictthe change in attributes a(x, x) from the latent perturbations zj zj for all data points x. This is repeatedfor every latent dimension j. We obtain a weight matrix W R|A||Z| relating attributes in the rows tolatent variables in the columns. If we pick the elements whose absolute weights are above a specic thresholds, we obtain a sparse matrix. In theory, any sparse linear coecient learning method would work here (e.g.,LASSO (Tibshirani, 1996)) however, in practice, due to the number of samples necessary, the computationalload is intractable without the ability to parallelize the computation per latent dimension as in OLS. At thispoint, the values are simply binary inuenced or not. This would represent the attribute subsets eachlatent dimension inuences. This is summarized in Algorithm 1. Now, we need to associate a latent to oneor more attributes such that all attributes inuenced by this latent appear later in the ancestral order.",
  "Building the DAG": "PerturbLearn constructs the attribute graph iteratively starting from sink nodes (Algorithm 2). Supposeone could nd a latent variable that inuences only one attribute and suppose that the graph is a DAG, thenthat attribute should have no children. Therefore that node is added to the graph (and the correspondinglatent is associated with it) (Line 10 in Algorithm 2) and removed from the weight matrix (Lines 1718 inAlgorithm 2). Sometimes, during this recursion, there may be no latent variable that may be found to aecta single remaining attribute. We nd a subset with smallest inuence (Lines 67) and add a confoundingarrow between those attributes (Lines 1113) and add it to the graph. Now, if this latent aects any otherdownstream nodes previously removed, then we draw an edge from the current set of attributes to them(Lines 1415). In other words, we assume inuences can be explained via mediation through attributes ratherthan direct inuence from the latents. After the recursive procedure is performed, we obtain the transitivereduction (Aho et al., 1972) of the resulting DAG (Line 20). It is the minimal unique edge subgraph thatpreserves all ancestral relations. This would be the minimal graphical model that would still preserve alllatent-attribute inuence relationships. See for a visualization of this process on a toy example.",
  "Solubility9982MAEScaoldAbsorptionSorkun et al. (2019)": "the system, which demand time, computational resources, and domain experts. Using existing data moreeectively from similar domains is one way to lower barrier costs to solving problems in these areas. Therapeutics Data Commons (TDC) is a platform for AI-powered drug discovery which contains a numberof datasets with prediction tasks for drug-relevant properties (Huang et al., 2021). We use all of the regres-sion tasks in the pharmacokinetics domain i.e., drug absorption, distribution, metabolism, and excretion(ADME). These are summarized in . All datasets were divided into training, validation, and testingsplits according to a ratio of 70%, 10%, and 20%, respectively, based on scaold (the core structure of themolecule) in order to separate structurally distinct compounds. Because the structures dier greatly, unlikein a random split, scaold splitting approximates a realistic distribution shift and is widely used to measurerobust generalization in ADME prediction (Huang et al., 2021). All datasets were also ltered to includeonly organic molecules (atoms {B, C, N, O, F, P, S, Cl, Br, I}) with additional salt ions removed andpre-processed to use a canonical form of SMILES without isomeric information.",
  "Generative autoencoder details": "For small molecule representation, we use the VAE from Chenthamarakshan et al. (2020). This model wastrained primarily on the MOSES dataset (Polykovskiy et al., 2020) which is a subset of the ZINC Clean Leadsdataset (Irwin et al., 2012). These are 1.6M molecules which are considered lead-like for drug developmentwith benign functionality. The encoder uses a gated recurrent unit (GRU) with a linear output layer whilethe decoder is a 3-layer GRU with dropout.The encoded latent distribution is constrained to be close",
  "Methods": "In order to validate the attribute graphs learned on the latent features of the generative autoencoder, wedevise a series of experiments for learning with limited data. If the graph is valid, the Markov blanket ofa given node should include all the features needed to predict that attribute and only those features. Wehypothesize that this will lead to improved generalization and robustness when learning on only a few datapoints. We assume that the structure of the attribute graph will not change when the domain shifts, althoughthe relative strength of the links may change. However, these functions should be easy to learn given theminimal set of independent variables and therefore only a small dataset is needed. To this end, we consider a scenario in which we wish to train a regressor to predict a certain value of interestin a niche domain. There is limited data available for training in this domain, however, data from similardomains are more plentiful. This is a realistic scenario for many real-world applications where the cost ofacquiring more data is high and throughput is low due to the involvement of physical experimentation, slowvirtual simulation, or laborous user feedback. Therefore, we opt to leverage the larger dataset for pre-trainingthe regressor and the smaller dataset for ne-tuning. With our proposed method, we also use the largerdataset to learn an attribute graph from which we can extract the Markov blanket features. For every experiment in this work, we take 10 perturbations of 2500 samples (sampled with replacementfrom the training data) for each dimension in the latent space. We also train a target attribute estimatorusing the latent representations of the data which is used to estimate the inuence of perturbed samples.We then apply PerturbLearn using these inputs along with the pre-trained generative autoencoder to obtainour attribute graph modeling the original domain. We test our method against ve baseline sets of features. Each baseline uses the same model described inthe next paragraph. The rst baseline uses the full latent vector, z, as input features to the predictor model.The second baseline uses all available attributes (i.e., all possible nodes in the attribute graph). The thirdbaseline is simply a concatenation of the rst two (all attributes +z). We also compare our model trained onthe features from the Markov blanket of the attribute of interest to the blankets from causal graphs derivedusing Greedy Equivalence Search (GES) (Chickering, 2002) and LiNGAM (Shimizu et al., 2006) as well asthe feature sets chosen by Sparse Group Lasso (Simon et al., 2013) and Predictive Permutation FeatureSelection (PPFS) (Hassan et al., 2021) learned on the training data split. GES and LiNGAM are causaldiscovery methods which learn based on observational (labeled) data. Group Lasso is a form of structuredsparsity regularization which aims to learn a reduced set of input features used for prediction while PPFS isa Markov blanket-based feature-selection algorithm. Finally, we demonstrate that these methods outperforma nave baseline which always predicts the training mean (dummy). For initial (base model) training, we use a multilayer perceptron (MLP) model architecture for each featureset for comparison. The MLP hyperparameters are tuned using 5-fold cross-validation on the training setwhere the search space is a grid of combinations of: hidden layer size 64, 128, or 512 (2 hidden layers chosenindependently); dropout rate 0.25 or 0.5; and training duration 100 or 500 epochs. All models use a meansquared error (MSE) loss with a batch size of 256 (or the size of the dataset, if smaller), rectied linearunit (ReLU) activations, and Adam optimization with a learning rate of 0.001. Data is also scaled to zeromean and unit variance independently for each feature. For the Half-life and VDss datasets, we scale thetargets in the training set by taking the natural logarithm before learning the outputs are exponentiated",
  "PL blanket1740.990.071.330.071.160.060.950.040.850.030.750.02": "before measuring performance metrics. Note, the base model for the z-baseline is eectively equivalent tothe target attribute estimator used in the PerturbLearn step to make predictions for generated samples. Fine-tuning on the second domain uses the MLP base model as a feature extractor by freezing the weightsand using the outputs from the last hidden layer. These weights are then fed into a Gaussian Process (GP)regressor with a kernel consisting of a sum of two radial basis function (RBF) kernels and a white noise kernel.We optimize the kernel parameters with the BroydenFletcherGoldfarbShanno (L-BFGS-B) method with50 restarts. In order to get a robust estimate of the performance of the model, we run each ne-tuningexperiment 10 times on randomly drawn subsets and take the mean. We also repeat the entire procedure 8times (retraining the base model) to obtain the mean and standard deviation values in Tables 211. For all the experiments, we utilize the provided scaold splits from TDC. For each of the tasks, we performgraph learning with PerturbLearn and initial regressor training using the train split and report the ne-tuned results on the test set. We treat the validation and test sets as eectively two independentshifted domains. In all cases we use the full split when training (and graph learning) but restrict the testingsamples to set sizes. When applying our method, PerturbLearn, after learning the linear weights from the perturbation data, wemust choose a sparsity threshold before converting the data to a DAG. We tune this hyperparameter bychoosing the best performing threshold on the validation set for the smallest ne-tuning size, n. The sparsitythreshold is chosen from the set, {0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.2, 0.25}.",
  "Results": "shows the results of experiments using the FreeSolv dataset. Each row shows mean absolute error(MAE) results (mean standard deviation ()) averaged over 10 runs of ne-tuning the MLP base model(learned from the train split) with a GP regressor using either the PerturbLearn Markov blanket (alsolearned from the train split) or various baselines while each column shows a dierent number of ne-tuningsamples, n, from the test split. The n = 0 column shows the performance of the base model, withoutne-tuning, on the test split. The best performing model on the validation split with n = 7 is used tochoose PerturbLearn hyperparameters. The performance of our method (PL blanket) is best for small ne-tuning sizes (n = 0, 7, 10) and second-best for larger ones (n = 25, 50, 100) for the FreeSolv dataset.The performance of our method on the Half-life dataset () shows improvement over all baselines other than PPFS in all scenarios. However,the PPFS algorithm involves aggregating k Markov blankets independently derived from separate folds of the",
  "data to handle intrinsic sample ineciency, whereas PertubLearn operates on a single MB, and is thereforeis less complex": "These are also the smallest of all the datasets used in our experiments, each under 700 total samples. For therest of the datasets (Tables 511), our method is competitive with the best baselines, often outperforming ornearly indistinguishable, especially with few ne-tuning samples, however, in general, there is little dierencebetween the performance of the feature sets with sucient training split size. In some cases, the performance at n = 0 is better than when we include additional ne-tuning samples(n > 0). In these cases, the models may benet from a dierent method of ne-tuning which causes lessforgetting than learning a new model on top of a pre-trained feature encoder, however, in the interest ofconsistency across datasets we keep the ne-tuning procedure the same for all experiments. Since the baselinemethods also experience a proportional drop in performance in these cases, the performance of our methodunder the ne-tuning procedure presented here still follows the same trends noted above. Other methods ofne-tuning should be explored in future work. Given the impressive performance of our attribute graph from the FreeSolv experiment, we can also tryapplying this graph to other tasks. In this case, since the tasks are distinct but within the same family(ADME), we can think of this as a more extreme distribution shift problem. These results can be seen in thePL blanket (FS) row of Tables 311. In a few cases, such as VDss and PPBR, this leads to a noticeable",
  "improvement, surpassing the best baselines. In the rest, there is little dierence compared to the task-specicPerturbLearn blanket": "shows ne-tuning results at n = 25 where we observe in every case, except Hepatocyte clearance,either the task-specic or transferred PL blanket performs best or second-best. We do not witness a signicantdecrease in performance margin until data size increases past this range (> 1800 total samples). For largerdatasets, such as Lipophilicity and Solubility (Tables 1011), most predictors achieve equivalent performance.",
  "Discussion": "In this study, we proposed a simple framework to extract a graphical model relating dierent data attributesfrom the latents of a pre-trained generative model. The inuence of a latent on attributes is used to constructthe graph. In this way, our method leverages the unsupervised learning techniques which produce powerfulmodels trained on vastly more data than is available and labeled for any individual task. This also means ourmethod can be extended to use better models in the future, given they encode information in a continuouslatent space. We have provided here a domain generalization method that utilizes latent-mediated inuences among molec-ular attributes and have empirically shown its ecacy to predict molecular attributes by learning from limitedout-of-distribution data. We showcased the performance of the proposed method, along with other exist-ing feature selection methods, in predicting molecular properties in a low-data regime. Thus, this studyprovides a comprehensive evaluation of feature selection methods for molecular learning tasks, as well asoers insights into the derived statistical relations between molecular attributes. Taken together, the resultsshow the use of derived Bayesian information improves over the baseline feature sets, especially when datafrom the training and/or test domains is limited, demonstrating that extracting the dependency informationunderlying a pre-trained generative model leads to more robust and generalizable models. These models alsouse only the meaningful and relevant properties as features meaning the models will be both smaller andmore interpretable than their baseline counterparts. One potential limitation of our framework is the dependency on a pre-determined set of data attributes,which may not be comprehensive (or may already involve experts applying implicit causal models learnedfrom experience to choose attributes). Further investigation on the accordance of the derived attribute graphwith domain priors, using only low-level and cheaply computable attributes, and/or with expert knowledgewould be addressed in future."
}