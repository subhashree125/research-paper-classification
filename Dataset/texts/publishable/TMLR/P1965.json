{
  "Abstract": "Latent Consistency Distillation (LCD) has emerged as a promising paradigm for efficienttext-to-image synthesis. By distilling a latent consistency model (LCM) from a pre-trainedteacher latent diffusion model (LDM), LCD facilitates the generation of high-fidelity imageswithin merely 2 to 4 inference steps. However, the LCMs efficient inference is obtained atthe cost of the sample quality. In this paper, we propose compensating the quality loss byaligning LCMs output with human preference during training. Specifically, we introduceReward Guided LCD (RG-LCD), which integrates feedback from a reward model (RM) intothe LCD process by augmenting the original LCD loss with the objective of maximizingthe reward associated with LCMs single-step generation.As validated through humanevaluation, when trained with the feedback of a good RM, the 2-step generations from ourRG-LCM are favored by humans over the 50-step DDIM (Song et al., 2020a) samples fromthe teacher LDM, representing a 25-time inference acceleration without quality loss. As directly optimizing towards differentiable RMs can suffer from over-optimization, wetake the initial step to overcome this difficulty by proposing the use of a latent proxy RM(LRM). This novel component serves as an intermediary, connecting our LCM with the RM.Empirically, we demonstrate that incorporating the LRM into our RG-LCD successfullyavoids high-frequency noise in the generated images, contributing to both improved FrchetInception Distance (FID) on MS-COCO (Lin et al., 2014) and a higher HPSv2.1 score onHPSv2 (Wu et al., 2023a)s test set, surpassing those achieved by the baseline LCM.",
  "Introduction": "In the realm of modern generative AI (GenAI) models, computational resources are typically allocated acrossthree key areas: pretraining (Brown et al., 2020; Achiam et al., 2023a; Li et al., 2022b; Radford et al., 2021;Rombach et al., 2022; Saharia et al., 2022; Betker et al., 2023a), alignment (Ziegler et al., 2019; Stiennon et al.,2020; Ouyang et al., 2022; Hu et al., 2024; Clark et al., 2023; Rafailov et al., 2024), and inference (Zhang& Chen, 2022; Feng et al., 2023; Vijayakumar et al., 2016; Shih et al., 2024). Normally, increasing thecomputational budget across these areas leads to improvements in sample quality. For instance, the mostadvanced text-to-image (T2I) models, such as DALLE-3 (Betker et al., 2023a), Imagen (Saharia et al.,2022), and Stable Diffusion (Rombach et al., 2022) are built from diffusion models (DMs) (Sohl-Dicksteinet al., 2015; Ho et al., 2020; Song & Ermon, 2019).These models are pretrained on massive web-scale",
  "Published in Transactions on Machine Learning Research (10/2024)": "Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, StefanoErmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preferenceoptimization. arXiv preprint arXiv:2311.12908, 2023b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Humanpreference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXivpreprint arXiv:2306.09341, 2023a. Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score: Better aligningtext-to-image models with human preference. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 20962105, 2023b. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.Imagereward: Learning and evaluating human preferences for text-to-image generation.Advances inNeural Information Processing Systems, 36, 2024.",
  "Diffusion Model": "Diffusion models (DMs) (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Nichol & Dhariwal,2021) progressively inject Gaussian noise into data in the forward process and sequentially denoise the data tocreate samples in the reverse denoising process. The forward process perturbs the original data distributionpdata(x) p0(x0) to the marginal distributional pt(xt). From a continuous-time perspective, we can representthe forward process with a stochastic differential equation (SDE) (Song et al., 2020b; Karras et al., 2022)",
  "(t)2(xt, t)dt,xT N(0, 2I).(3)": "In this paper, we focus on conditional LDM that operates on the image latent space Z and includes a textprompt c passed to the denoising model (zt, c, t), where zt = E(xt) Z is encoded by a VAE (Kingma et al.,2021) encoder E. Moreover, we utilize Classifier-Free Guidance (CFG) (Ho & Salimans, 2022) to improvethe quality of conditional sampling by replacing the noise prediction with a linear combination of conditionaland unconditional noise prediction for denoising, i.e., (zt, , c, t) = (1 + ) (zt, c, t) (z, , t), where is the CFG scale.",
  "Consistency Model": "Consistency model (CM) (Song et al., 2023) is proposed to facilitate efficient generation. At its core, CMlearns a consistency function f : (xt, t) x that can map any point xt on the same PF-ODE trajectory tothe trajectorys origin, where is a fixed small positive number. Learning the consistency function involvesenforcing the self-consistency property",
  "Reward Guided Latent Consistency Distillation": "In this section, we start by presenting the core components of our RG-LCD framework, which augmentsthe standard LCD loss equation 8 with an objective towards maximizing a differentiable RM, as shown in (Sec. 4.1). We then motivate the development of a latent proxy RM (LRM) to support indirectRM optimization by illustrating the risk of suffering from reward over-optimization when directly optimizingtowards the RM with a gradient-based method. Following this, we then detail the procedure to pretrain andfinetune the LRM to match the preference of the RGB-based RM during RG-LCD (Sec. 4.2).",
  "RG-LCD with Differentiable RMs": "Recall that each LCD iteration samples a timestep tn+k, and construct the noisy latent ztn+k by perturb theimage latent z = E(x) with a Gaussian noise, given a sampled CFG scale and text prompt c. As the LCMf maps the ztn+k to the PF-ODE origin z0 = fztn+k, , c, tn+k, we construct the following objective tomaximize the reward associated with D( z0)",
  "RG-LCD with a Latent Proxy RM": "When training the LCM f towards J() with a gradient-based method, we may suffer from the issue ofreward over-optimization. As shown in the top row of , performing RG-LCD with ImageReward (Xuet al., 2024) causes high-frequency noise in the generated images. To mitigate this issue, we propose learninga latent proxy RM RL to serve as an intermediary to connect f and the expert RGB-based RM RE, wherethe E stands for Expert. Specifically, we train f to optimize the reward given by RL while simultaneouslyfinetuning the RL to matches the preference given by the expert RM RE that process RGB images. Ideally, the LRM RL should be capable of accessing the text-image pair even at the beginning of RG-LCD. Wethus initialize RL with a pretrained CLIP (Radford et al., 2021) text encoder, complemented by pretrainingits latent encoder from scratch. This latent encoder is pretrained following the same methodology used forCLIP visual encoders, ensuring it aligns effectively with the text encoders representation.",
  ": Human evaluation results on the PartiPrompt (1632 prompts) across three evaluation questions.Top row evaluates the RG-LCM (CLIP). Bottom row evaluates the RG-LCM (HPS)": "positive number and only give a non-zero positive reward to the sample favored by the expert. Moreover,since z0 = z corresponds to the latent of a real image, we can increase likelihood for Q0,j to prefer k = 0. While calculating RE(D(z), c) still requires decoding the latent, the application of the stop_grad operationeliminates the need for gradient transmission through D, leading to a substantial reduction in memory usage.Moreover, optimizing RL with LRM() is independent from optimizing f with LRG-LCD. Therefore, we canuse a smaller batch size to optimize RL without affecting the batch size used to optimize f. In essence, our LRM acts as a proxy connecting the LCM f and the expert RM RE. As we will show Sec.5.2, using this indirect feedback from the expert mitigates the issue of reward over-optimization, avoidinghigh-frequency noise in the generated images.",
  "Experiment": "We perform thorough experiments to demonstrate the effectiveness of our RG-LCD. Sec.5.1 conductshuman evaluation to compare the performance of our methods with baselines. Sec. 5.2 further increases theexperiment scales to experiment with a wider array of RMs with automatic metrics. By connecting bothevaluation results, we identify problems with the current RMs. Finally, Sec. 5.3 conducts ablation studieson critical design choices. Settings Our training are conducted on the CC12M datasets (Changpinyo et al., 2021), as the LAION-Aesthetics datasets (Schuhmann et al., 2022) used by the original LCM (Luo et al., 2023a) are no longeraccessible1.We distill our LCM from the Stable Diffusion-v2.1 (Rombach et al., 2022) by training for 10Kiterations on 8 NVIDIA A100 GPUs without gradient accumulation and set the batch size to reach themaximum capacity of our GPUs. We follow the hyperparameter settings listed in the diffusers (von Platenet al., 2022) library by setting learning rate 1e 6, EMA rate = 0.95 and the guidance scale range[min, max] = . As mentioned in Sec. 3.3, we use DDIM (Song et al., 2020a) as our ODE solver with a skipping step k = 20. We include more training details in Appendix A. Appendix D further includesexperiment results with diverse teacher T2I models, including Stable Diffusion 1.5 and Stable Diffusion XL.",
  ": Human evaluation results on the HPSv2 test set (3200 prompts) across three evaluation questions.Top row evaluates the RG-LCM (CLIP). Bottom row evaluates the RG-LCM (HPS)": "human preferences more accurately. We choose the teacher LDM (Stable Diffusion v2.1) and a standardLCM distilled from the same teacher LDM as the baseline methods. To demonstrate the efficacy of ourmethods, we compare the performance of our RG-LCMs over 2-step and 4-step generations against the 50-step generations from the teacher LDM and evaluate the 4-step generation quality of our RG-LCMs againstthe standard LCM. We follow a similar evaluation protocol as in (Wallace et al., 2023a) to generate images by conditioning onprompts from Partiprompt (Yu et al., 2022) (1632 prompts) and of HPSv2s test set (Wu et al., 2023a) (3200prompts). We hire labelers from Amazon Mechanical Turk for a head-to-head comparison of images basedon three criteria: Q1 General Preference (Which image do you prefer given the prompt?), Q2 Visual Appeal(Which image is more visually appealing, irrespective of the prompt?), and Q3 Prompt Alignment (Whichimage better matches the text description?). The full human evaluation results in and 5 show that the 2-step generations from RG-LCM (CLIP) aregenerally preferred (Q1) over the 50-step generations of the teacher LDM in both prompt sets, representinga 25-fold acceleration in inference speed. Even with CLIPScore feedback, the 4-step generations from ourRG-LCM are generally preferred (Q1) over the baseline methods. This indicates a noteworthy achievement,given that CLIPScore does not train on human preference data. Surprisingly, on the HPSv2 prompt set, the4-step generations from the RG-LCM (CLIP) are more preferred (59.4% against 50-step DDIM samples fromSD and 81.7% against 4-step LCM samples) compared to the 4-step generations of the RG-LCM (CLIP)(57.1% against 50-step DDIM samples from SD, and 69.0% against 4-step LCM samples). To investigate this phenomenon, we observe that both RG-LCMs score similarly in General Preference (Q1)and Prompt Alignment (Q3). However, the RG-LCM (CLIP) is rated slightly lower in Visual Appeal (Q2)than in the other criteria, whereas the RG-LCM (HPS) is rated significantly higher for Q2 compared toQ1 and Q3.This distinction highlights that CLIPScores primary contribution is enhancing text-imagealignment, whereas an RM like HPSv2.1 particularly focuses on improving visual quality. Thus, when over-optimizing towards HPSv2.1, the RG-LCM (HPS) can be biased in generating visually appealing samplesby sacrificing prompt alignment.",
  "Evaluating RG-LCD with Automatic Metrics": "In this section, we further train RG-LCD (ImgRwd) and RG-LCD (Pick) by leveraging feedback fromImageReward (Xu et al., 2024) and PickScore (Kirstain et al., 2024). Both of these RMs are trained onhuman preference data. We will use automatic metrics to perform a large-scale evaluation of the performanceof different models. As we have human evaluation results for RG-LCD (HPS) and RG-LCD (CLIP), we can",
  "RG-LCM (HPS)230.8533.6633.3533.6624.04RG-LCM (HPS)431.8334.8434.4334.7525.11RG-LCM (HPS) + LRM227.5825.9426.7727.2416.71RG-LCM (HPS) + LRM428.5327.4927.9428.8717.52": ": Evaluation of our RG-LCMs on the HPSv2 test prompts and MS-COCO datasets. NFEs denotethe number of function evaluations during inference. We train RG-LCMs with CLIPScore, PickScore, Im-ageReward (ImgRwd) and HPSv2.1. We employ the HPSv2.1 to evaluate the generations on the HPSv2Benchmarks test set. We calculate the FID of the generations on the MS-COCO. Except trained withCLIPScore, our RG-LCMs achieve better HPSv2.1 scores on HPSv2 test prompts at the expense of higherFIDs on MS-COCO. Integrating a LRM into our RG-LCD process allows for simultaneous improvement onHPSv2.1 scores on HPSV2 test prompts and FID on MS-COCO against the baseline LCM. also evaluate the quality of the automatic metrics. For each RG-LCD, we collect their 2-step and 4-stepgenerations by conditioning on prompts from HPSv2s test set and measuring the HPSv2.1 score associatedwith the samples. To comprehensively understand the sample quality from different models, we furthergenerate images conditioned on the prompts of MS-COCO (Lin et al., 2014) and measure their FrchetInception Distance (FID) to the ground truth images. presents the full evaluation results with the automatic metrics. Except for RG-LCM (CLIP), all theother RG-LCMs achieve higher HPSv2.1 scores than the baseline LCM but at the expense of higher FIDvalues on the MS-COCO dataset. Specifically, the RG-LCM (ImgRwd) model exhibits a notably high FIDvalue, yet it still secures an impressive HPSv2.1 score when evaluated on HPSv2 test prompts. The elevatedFID value aligns with expectations, as illustrates that optimization directed towards ImageRewardtends to introduce a significant amount of high-frequency noise into the generated images. Surprisingly,these high-frequency noises do not adversely affect the HPSv2.1 scores. Furthermore, the HPSv2.1 scores donot capture the human preference for the 4-step samples from RG-LCM (CLIP) by giving the highest scoreto RG-LCM (HPS)s 4-step samples, contrary to what is depicted in human evaluation shown in . These observations suggest that the HPSv2.1 score, as a metric, has limitations and requires further refine-ment. We conjecture that the Resize operation, which happens during the preprocessing phase, causes theHPSv2.1 model to overlook the high-frequency noise during reward calculation. As illustrated in , thehigh-frequency noise becomes less perceptible when images are reduced in size. Although resizing operationsenhance efficiency in tasks such as image classification (Lu & Weng, 2007; Deng et al., 2009; He et al., 2016)and facilitate high-level text-image understanding (Radford et al., 2021), they prevent the model from cap-turing critical visual nuances that are vital for accurately reflecting human preferences. Consequently, weadvocate for future RMs to exclude the Resize operation. One potential approach could involve trainingan LRM, as in our paper, to learn human preferences in the latent space without resizing input images.",
  "Ablation Study": "Ablation on the reward scale . We use the hyperparameter to determine the optimization strengthtowards the RM. We are especially interested in the impact of an extremely large , which can lead toreward over-optimization (Kim et al., 2023b). We already know that over-optimizing the ImageReward canlead to the introduction of high-frequency noise in the generated images. To expand our understanding,we conduct experiments a wider array of RMs including HPSv2.1, PickScore and CLIPScore and evaluatewhether over-optimizing these RMs will also leads to similar high-frequency noise. The results in reveal that an extremely large value does not introduce the high-frequency noisewhen using HPSv2.1, PickScore, and CLIPScore, even though all these metrics resize input images to 224x224pixels as in ImageReward. Notably, over-optimization of HPSv2.1 leads to generating images with repetitiveobjects as described in the text prompts and increases color saturation. Conversely, over-optimization ofPickScore tends to result in images with more muted colors. On the other hand, excessive optimizationof CLIPScore results in images where the text prompts are visibly incorporated into the imagery. Thesefindings align with the discussions in Sec. 5.1, suggesting that optimizing towards a preference-trained RMgenerally prioritizes visual appeal over text alignment. In contrast, over-optimizing CLIPScore compromisesvisual attractiveness in favor of text alignment. We include additional image samples in Appendix C. Ablation on the training iterations. In total, we train each model for 10K iterations. We take check-points from 1K, 2K, 4K, and 10K iterations and sample images with the same prompts and seeds. We canobserve performing RG-LCD with RMs that facilitate the visual appeal of the generated images also results",
  "Conclusion": "In this paper, we introduce RG-LCD, a novel strategy that integrates feedback from an RM into the LCDprocess. The RG-LCM learned via our method enjoys better sample quality while facilitating fast inference,benefiting from additional computational resources allocated to align with human preferences. By evaluatingusing prompts from the HPSv2 (Wu et al., 2023a) test set and PartiPrompt (Yu et al., 2022), we empiricallyshow that humans favor the 2-step generations of our RG-LCD (HPS) over the 50-step DDIM generationsof the teacher LDM. This represents a 25-fold increase in terms of inference speed without a loss in quality.Moreover, even when using CLIPScorea model not fine-tuned on human preferencesour methods 4-stepgenerations still surpass the 50-step DDIM generations from the teacher LDM. We also identify that directly optimizing towards an imperfect RM, e.g., ImageReward, can cause high-frequency noise in generated images. To reconcile the issue, we propose integrating an LRM into the RG-LCD framework. Notably, our methods not only prevents reward over-optimization but also avoids passinggradients through the VAE decoder and facilitates learning from non-differentiable RMs.",
  "Limitation and Impact Statement": "While our RG-LCD marks a critical advancement in the realm of efficient text-to-image synthesis, introducingan acceleration in the generation process without compromising on image quality, it is important to recognizecertain limitations. The approach relies on employing a reward model that reflects human preference, which,while effective in improving image quality metrics, may introduce additional costs in the training pipelineand necessitate fine-tuning to adapt to various domains or datasets. Despite these challenges, the impact ofRG-LCD is profound, offering a scalable solution that significantly enhances the accessibility and practicalityof generating high-fidelity images at a remarkable speed. This innovation not only broadens the potentialapplications in fields ranging from digital art to visual content creation but also sets a new benchmarkfor future research in text-to-image synthesis, emphasizing the importance of human-centric design in thedevelopment of generative AI technologies.",
  "Acknowledgement": "The work was funded by an unrestricted gift from Google. We would like to thank Google for their generoussponsorship. The views and conclusions contained in this document are those of the authors and should notbe interpreted as representing the sponsors official policy, expressed or inferred. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023a. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023b. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,Joyce Lee, Yufei Guo, et al.Improving image generation with better captions.Computer Science. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023a. James Betker, Gabriel Goh, Li Jing, TimBrooks, Jianfeng Wang, Linjie Li, LongOuyang, JuntangZhuang,JoyceLee, YufeiGuo, WesamManassra, PrafullaDhariwal, CaseyChu, YunxinJiao, and Aditya Ramesh.Improving image generation with better captions. 2023b. URL Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scaleimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 35583568, 2021.",
  "Kevin Clark, Paul Vicol, Kevin Swersky, and David J Fleet. Directly fine-tuning diffusion models on differ-entiable rewards. arXiv preprint arXiv:2309.17400, 2023": "Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Van-denhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models usingphotogenic needles in a haystack. arXiv preprint arXiv:2309.15807, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers. Advancesin Neural Information Processing Systems, 35:3015030166, 2022": "Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and TongZhang.Raft: Reward ranked finetuning for generative foundation model alignment.arXiv preprintarXiv:2304.06767, 2023. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, MohammadGhavamzadeh, Kangwook Lee, and Kimin Lee.Reinforcement learning for fine-tuning text-to-imagediffusion models. Advances in Neural Information Processing Systems, 36, 2024.",
  "Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-basedgenerative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, YutongHe, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow odetrajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023a. Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh, Krishnamurthy Dj Dvijotham,Jinwoo Shin, and Kimin Lee. Confidence-aware reward optimization for fine-tuning text-to-image models.In The Twelfth International Conference on Learning Representations, 2023b.",
  "Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Pretrained language models fortext generation: A survey. arXiv preprint arXiv:2201.05273, 2022b": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solverfor diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information ProcessingSystems, 35:57755787, 2022a.",
  "Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizinghigh-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023a": "Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinrio Passos, Longbo Huang,Jian Li, and Hang Zhao.Lcm-lora: A universal stable-diffusion acceleration module.arXiv preprintarXiv:2311.05556, 2023b. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and TimSalimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1429714306, 2023.",
  "Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki.Aligning text-to-imagediffusion models with reward backpropagation. arXiv preprint arXiv:2310.03739, 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.Direct preference optimization: Your language model is secretly a reward model. Advances in NeuralInformation Processing Systems, 36, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022.",
  "Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. International conferenceon machine learning, 2023": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, DarioAmodei, and Paul F Christiano.Learning to summarize with human feedback.Advances in NeuralInformation Processing Systems, 33:30083021, 2020. Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann,Sjoerd van Steenkiste, Ranjay Krishna, et al. Dreamsync: Aligning text-to-image generation with imageunderstanding feedback. arXiv preprint arXiv:2311.17946, 2023. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Sori-cut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodalmodels. arXiv preprint arXiv:2312.11805, 2023. Ashwin K Vijayakumar, Michael Cogswell, Ramprasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall,and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXivpreprint arXiv:1610.02424, 2016."
}