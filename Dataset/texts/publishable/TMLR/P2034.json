{
  "Abstract": "Neural networks (NNs) are known to exhibit simplicity bias where they tend to prefer learningsimple features over more complex ones, even when the latter may be more informative.Simplicity bias can lead to the model making biased predictions which have poor out-of-distribution (OOD) generalization and subgroup robustness. To address this, we propose ahypothesis about spurious features that directly connects to simplicity bias: we hypothesizethat spurious features on many datasets are simple features that are still predictive of thelabel. We empirically validate this hypothesis, and subsequently develop a framework whichleverages this hypothesis to learn more robust models. In our proposed framework, we firsttrain a simple model, and then regularize the conditional mutual information with respect toit to obtain the final model. We theoretically study the effect of this regularization and showthat it provably reduces reliance on spurious features in certain settings. We also empiricallydemonstrate the effectiveness of this framework in various problem settings and real-worldapplications, showing that it effectively addresses simplicity bias and leads to more featuresbeing used, enhances OOD generalization, and improves subgroup robustness and fairness.",
  "Introduction": "Motivated by considerations of understanding generalization in deep learning, there has been a series ofinteresting studies (Zhang et al., 2017a; Frankle & Carbin, 2019; Nakkiran et al., 2020) on understandingfunction classes favored by current techniques for training large neural networks. An emerging hypothesis isthat deep learning techniques prefer to learn simple functions over the data. While this inductive bias hasbenefits in terms of preventing overfitting and improving (in-distribution) generalization in several cases, it isnot effective in all scenarios. Specifically, it has been found that in the presence of multiple predictive featuresof varying complexity, neural networks tend to be overly reliant on simpler features while ignoring morecomplex features that may be equally or more informative of the target (Shah et al., 2020; Nakkiran et al.,2019; Morwani et al., 2023). This phenomenon has been termed simplicity bias and has several undesirableimplications for robustness and out-of-distribution (OOD) generalization. As an illustrative example, consider the Waterbirds dataset (Sagawa* et al., 2020). The objective here isto predict a birds type (landbird vs. waterbird) based on its image (see for an example). Whilefeatures such as the background (land vs. water) are easier to learn, and can have a significant correlationwith the birds type, more complex features like the birds shape are more predictive of its type. However,simplicity bias can cause the model to be highly dependent on simpler yet predictive features, such asthe background in this case. A model which puts high emphasis on the background for this task is notdesirable, since its performance may not transfer across different environments. A similar story arises in",
  ": Summary of the datasets we consider. Spurious features seem simpler than invariant features": "many different tasks summarizes various datasets, where the target or task-relevant features (alsoknown as invariant features), are more complex than surrogate features that are superficially correlated withthe label (also known as spurious features). Simplicity bias causes NNs to heavily rely on these surrogatefeatures. Several methods (Arjovsky et al., 2020; Creager et al., 2021; Bae et al., 2022; Gao et al., 2022) havebeen proposed to address this problem of OOD generalization. However, some knowledge of the differentenvironments of interests or the underlying causal graph is in general necessary to decide whether a feature isspurious or invariant (due to the No Free Lunch Theorem, see Appendix D for more discussion). Therefore,most techniques in the literature require this information about whether a feature is spurious or invariantto be available beforehand. To develop a principled framework for OOD generalization that does not usesuch prior knowledge, we believe it is useful to first start with a suitable assumption of what features will beregarded as being spurious. In this work, we use simplicity bias and observations from to propose sucha candidate hypothesis about spurious features: Features typically regarded as being spurious are relativelysimple while still being predictive of the label. If this hypothesis is true, alleviating simplicity bias can lead tobetter robustness and OOD generalization. We empirically evaluate this hypothesis and find it to be valid on considered datasets. In light of these results,we develop a new framework to mitigate simplicity bias and encourage the model to utilize a more diverse setof features for its predictions, and hence improve robustness and OOD generalization. The high-level goal ofour framework is to ensure that the predictions of the trained model M have minimal conditional mutualinformation I(M; S|Y )1 with any simple, predictive feature S, conditioned on the label Y . To achieve this, wefirst train a simple model M s on the task, with the idea that this model captures the information in simple,predictive features. Subsequently, to train the final model M we add its conditional mutual informationI(M; M s |Y ) with the model M s as a regularizer to the usual empirical risk minimization (ERM) objective.With this regularization term, we incentivize the model to leverage additional, task-relevant features that",
  "may be more complex. We refer to our approach as Conditional Mutual Information Debiasing or CMID (see)": "We show that our approach leads to models that use more diverse features on tasks that have beenpreviously proposed to measure simplicity bias. The method achieves improved subgroup robustness andOOD generalization on several benchmark tasks, including in the presence of multiple spurious features. Italso leads to improved predictions from the perspective of fairness since the predictions are less dependenton protected attributes such as race or gender. We also prove theoretical results to better understand andcharacterize our approach. Our contributions are: We empirically evaluate and validate the hypothesis that spurious features are simpler than invariantfeatures, with respect to representative datasets from (). Based on our findings, wepropose a novel framework (CMID) to mitigate a models simplicity bias and use a diverse set of featuresfor prediction (). We theoretically analyze the effect of our regularization and demonstrate its capability to reduce relianceon spurious features using a Gaussian mixture model setup (). Additionally, we establish anOOD generalization guarantee in a causal learning framework (). Most approaches developed forsubgroup robustness or OOD generalization either lack theoretical guarantees or show results tailored toone of these two settings (discussed further in Appendix C). Empirically, we demonstrate that our framework effectively mitigates simplicity bias, and achieves improvedOOD generalization, sub-group robustness and fairness across 10 benchmark datasets (). Theseinclude different modalities such as image, text, and tabular data and application domains such ashealthcare and fairness. While several approaches have been proposed for each of the goals we consider,prior work typically focuses on one or two of these applications, while our approach proves effective acrossall cases.",
  "Related Work": "We discuss related work on simplicity bias, OOD generalization and subgroup robustness in this section, andrelated work on fairness, feature diversification and debiasing methods in Appendix B. A detailed comparisonwith prior work appears in Appendix C, here we summarize some key differences. A conceptual differencefrom prior algorithms for these tasks is that we take a more data-driven approach to define spurious features.Hence our approach is more explicit in terms of its assumptions on the data which could make it easier toevaluate and use, and also allows us to prove theoretical guarantees. Various datasets seem to satisfy ourassumptions and hence our approach proves effective across varied applications. Moreover, in contrast toother methods, it does not involve training multiple complex models, access to group labels, or unlabelleddata from the target distribution. Simplicity Bias in NNs.Several works (Arpit et al., 2017; Valle-Perez et al., 2019; Geirhos et al., 2020;Pezeshki et al., 2021) show that NNs trained with gradient-based methods prefer learning solutions whichare simple. Nakkiran et al. (2019) show that the predictions of NNs trained by SGD can be approximatedwell by linear models during the early stages of training. Morwani et al. (2023) show that 1-hidden layerNNs are biased towards using low-dimensional projections of the data for predictions. Geirhos et al. (2022)show that CNNs make predictions that depend more on image texture than image shape. Shah et al. (2020)create synthetic datasets and show that in the presence of simple and complex features, NNs rely heavily onsimple features even when both have equal predictive power. In our work, we experiment with datasets inboth these papers (in addition to the datasets in ), to investigate the effectiveness of our method tomitigate simplicity bias. OOD Generalization.Towards developing models that perform better in the real world, OOD generaliza-tion requires generalization to data from new environments. Environments are usually defined based on thecorrelation between some spurious feature and the label. Various methods aim to recover a predictor that isinvariant across a set of environments. Arjovsky et al. (2020) develop the invariant risk minimization (IRM)framework where environments are known, while Creager et al. (2021) propose environment inference forinvariant learning (EIIL), to recover the invariant predictor, when the environments are not known. Predict",
  "Published in Transactions on Machine Learning Research (08/2024)": "worth noting here that it is impossible for a single algorithm to perform well in all cases. This is establishedby the No Free Lunch theorem: there is no inductive bias that is suitable for all tasks (Shalev-Shwartz &Ben-David, 2014) and relatedly, it is impossible to generalize without certain assumptions on the train andtest distributions (Wolpert, 1996). In most cases, we assume that the test data is i.i.d. and indeed, simplicity bias is useful in explainingin-distribution generalization of NNs in these cases. However, these observations may not necessarily extendto situations where the i.i.d. assumption on the test set is violated. Generalization under distribution shift issignificantly different and in such cases, simplicity bias can prevent the model from learning more complex,task-relevant features that may generalize better. Consequently, alleviating simplicity bias can be a usefulinductive bias in such cases. This is consistent with our experimental evaluations on several datasets fromvarious modalities and domains. Even in situations where mitigating simplicity bias is useful, there can be cases where the separation betweenthe feature complexity of spurious and invariant features may not be very large. This can make the selectionof an appropriate simple model class for our approach challenging. Indeed, in our experiments, we foundthat for the CelebA dataset, the spurious and task-relevant features are not significantly different in termsof complexity (Appendix 5.5). Consequently, our approach does not lead to much improvement in theworst-group accuracy on this dataset. We also note that in general, the use of methods designed to improve OOD generalization and subgrouprobustness can lead to a drop in the accuracy on the training set or the i.i.d. test set, compared to ERM.This is seen in some of our experimental results as well (see and related discussion for more details).This is because a model trained with the ERM objective relies strongly on the spurious feature(s), which ispredictive of the train set as well as the i.i.d. test set. In contrast, improvement on OOD test sets is achievedby leveraging other features, which may be less predictive on the train set but generalize better. Some waysto control this trade-off are to change the regularization strength for our approach, or the model selectionrule to take both train and validation set accuracies into account. This can allow the user to ensure goodtraining performance, while also attaining improvements in OOD generalization.",
  "Spurious Features are Simple and Predictive": "In this section, we conduct an experiment to validate our hypothesis that surrogate features are generallysimpler than invariant features (see for examples). First, we define simple models/features for a taskas follows:Definition 1 (Simple models and features). Consider a task on which benchmark models that can attainnear-zero training loss have a certain complexity (in terms of the number of parameters, layers, etc.). Weconsider models that have significantly lower complexity than benchmark models and cannot attain small losson the training data as simple models. Similarly, features that can be effectively learned using simple modelsare considered as simple features. Def. 1 proposes a metric of simplicity that is task-dependent since it depends on the complexity of the modelswhich get high accuracy on the task. We choose to not define quantitative measures in Def. 1 since thosewould be problem-dependent. As an example, for colored-MNIST (CMNIST), NNs with non-linearities arenecessary to achieve high accuracy, and for Waterbirds, deeper networks such as ResNet-50 (He et al., 2016)achieve the best results. Therefore, a linear model and a shallow CNN can be considered as simple modelsfor these two datasets, respectively. We discuss simple model selection in more detail in . Importantly, we observe that these simple models can still achieve good performance when the task is todistinguish between surrogate features even though they are not as accurate in predicting the invariant feature.Specifically, for CMNIST, we compare the performance of a linear model on color classification and digitclassification on clean MNIST data. Similarly, for Waterbirds, we consider a shallow CNN (specifically, the2DConvNet1 architecture in Appendix ) and compare its performance on background classification(using images from the Places dataset (Zhou et al., 2017)) and bird type classification (using segmentedimages of birds from the CUB dataset (Wah et al., 2011)). The results in corroborate the hypothesisthat surrogate features for these datasets are simple features, as they can be predicted much more accuratelyby simpler models than invariant features. Unsurprisingly, the respective benchmark models can capture thetask-relevant features much more accurately as they are more complex than the simple models used in eachcase. Motivated by these observations, we define spurious features as follows. Operationally, our assumption onspurious features has the advantage that it does not require knowledge of some underlying causal graph ordata from multiple environments to determine if a feature is spurious.",
  ": Train and test perfor-mance of the simple model onthe target task": "In the presence of such features, simplicity bias leads the model to prefer such features over invariant featuresthat are more complex. We also verify that when simple models are trained on the target tasks on thesedatasets, they tend to rely on these spurious features to make accurate predictions. Specifically, we considerthe digit classification task using CMNIST data, where the correlation between the color and the label in thetest set is 10%, and bird type classification using Waterbirds data, where the test set consists of balancedgroups (50% correlation). shows that the test accuracy is close to the correlation between the spuriousfeature-based group label and the target label. This indicates that simple models trained on the target taskutilize spurious features to make predictions. What if Assumption 1 is not true?We briefly note here that spurious features as defined by us maynot always be irrelevant for the task, and mitigating simplicity bias may not always be desirable (furtherdiscussed with other limitations in Appendix D). Indeed, through similar experiments, we found that for theCelebA dataset, the spurious and task-relevant features are not significantly different in terms of complexity(details in .5). As a result, our approach does not lead to much improvement in the worst-groupaccuracy on this dataset (.3). However, it is impossible for any learning rule to generalize acrossall types of distribution shifts (see Appendix D for more discussion); one can only aim to generalize underspecific types of distribution shifts by employing the appropriate inductive bias. Reducing reliance on simple,predictive features represents one such assumption, which seems reasonable based on the results presentedabove and proves effective in several cases, as shown in .",
  "CMID: Learning in the Presence of Spurious Features": "In this section, we outline our approach to mitigate simplicity bias. Our approach leverages the fact thatsimple models can capture surrogate features much better than invariant features (), and rely on thespurious features to make predictions, even when trained on the target task (). Thus, by ensuring thatthe final model has low conditional mutual information with respect to such a simple model, we encourage itto utilize a more diverse set of features. Let Z = (X, Y ) denote an input-label pair, where X X, Y {0, 1}, sampled from some distribution D,D denote a dataset with n samples, M() : X denote a model, parameterized by (shorthandM).Subscripts ()s and ()c denote simple and complex, respectively.Let M denote the class of allmodels, M(Z) : X {0, 1} R denote a loss function. I(; ) measures Shannon mutual informationbetween two random variables.With slight abuse of notation, let M and Y also denote the (binary)random variables associated with the predictions of model M on datapoints Xis and the labels Yis,across all i [n], respectively. The conditional mutual information (CMI) between the outputs of twomodels given the label is denoted by I(M1; M2|Y ). The empirical risk minimizer for class M is denoted as",
  "where ID(M; M s |Y ) denotes the estimated CMI over D, and is the regularization parameter": "A few remarks are in order about the choice of our regularizer and the methodology to select the simplemodel class. We penalize the CMI instead of MI. This is because both M s and M are expected to haveinformation about Y (for e.g. in the Waterbirds dataset both bird type and background are correlated withthe label). Hence they will not be independent of each other, but they are closer to being independent whenconditioned on the label. We note that we use CMI to measure dependence, instead of other measures suchas enforcing orthogonality of the predictions. This is for the simple reason that MI measures allpotentiallynon-lineardependencies between the random variables.",
  ": Performance of CMID using dif-ferent simple model architectures for theWaterbirds dataset": "We note that while several models can be considered simplebased on Def. 1, the choice of the simple model class Ms usedfor our approach is task dependent. Intuitively, we want to usethe simplest model that we expect to do reasonably well onthe given task. Models that are too simple may not be able tocapture surrogate features effectively, whereas models that arevery complex may rely on task-relevant features, even thoughsuch reliance may be weak due to simplicity bias. As an example, considers the problem of subgroup robustness on theWaterbirds dataset (where typically the hardest groups areimages of waterbirds on land backgrounds and vice versa) andshows a comparison of the average and worst-group accuracyof the model learned by CMID using various simple model architectures (see .3 for more detailsabout the task). We consider four models: a linear model, the shallow CNN (2DConvNet1) used in , a ResNet-18 pre-trained on ImageNet, and the ResNet-50 pre-trained on ImageNet, which is also thearchitecture of the benchmark model. We observe that using a shallow CNN as the simple model is mosteffective for CMID. This is because a linear model is too simple to capture the surrogate feature, i.e. thebackground in this case, whereas deep models like ResNet are complex enough to learn both types of featureswhile relying more strongly on the surrogate feature. As a result, regularizing the CMI with respect to suchmodels may not be as effective in reducing reliance on spurious features. However, we note that doing so stillleads to significantly better worst-group accuracy values compared to ERM. Although the choice of Ms may impact the performance of CMID, we use a fairly simple selection rulethroughout our experiments, which works well. In general, for datasets where the final model is a shallowNN, we used a linear model as the simple model. For image datasets where the final model is a ResNetor DenseNet-based model, we consider shallow 2D CNNs as simple models. For language datasets whereBERT-based models are the final models, we consider shallow MLPs or 1D CNNs as simple models. Detailsabout the model architectures can be found in Appendix G.3.2. We also note that the CMI is not differentiable in its original form since Y is discrete. Therefore, weemploy an estimator that uses the predicted probabilities instead of the thresholded predictions to makethe regularization differentiable and hence suitable for gradient-based methods. We include details for thisestimator and also an empirical comparison with the original CMI in Appendix A.",
  "where a yR() is a spurious attribute, with an unstable correlation with y, and 1, 2 > 0, > 0.5. Let2 := (2 1)2, 22 := 22 + 22 22": "We consider linear models to predict y from the features X1, . . . , Xd+1. Let M = {w = (w1, . . . , wd+1) : w Rd+1} be all possible linear models and Ms = {w = (w1, . . . , wd+1) : w Rd+1, w0 = 1} be a simplermodel class which only uses one of the d + 1 features. We consider the mean squared error (MSE) loss, andthe ERM solution is given by:",
  "d21": "Theorem 1 suggests that for an appropriately small c, regularizing CMI with the simple model leads to apredictor which mainly uses the invariant features. This is supported by experimental results on data drawnaccording to Assumption 2 with d = 1, shown in . We note that a lower value of c promotes conditionalindependence with Xd+1 and upweighs wi more strongly. When c 0, wd+1 0. We visualize this in in the Appendix for d = 1. In Appendix E.1, we show a similar theoretical result when multiple spuriousfeatures are present.",
  "OOD Generalization in a Causal Learning Framework": "Following the setting in Arjovsky et al. (2020); Liu et al. (2021b), we consider a dataset D = {De}eEtr,which is composed of data De Dneegathered from different training environments e Etr, where e denotesan environment label, ne represents the number of samples in e. Etr denotes the set of training environments.",
  "The corresponding maximal invariant predictor (MIP) of IE(M) is = arg maxIE(M)I(Y ; )": "The MIP is an invariant predictor that captures the most information about Y . Invariant predictors guaranteeOOD generalization, making MIP the optimal invariant predictor (Theorem 2.1 in Liu et al. (2021b)). As discussed in , most current work assumes that the environment labels e for the datapoints areknown. However, environment labels typically are not provided in real-world scenarios. In this work, wedont assume access to environment labels and instead, we rely on another aspect of these features: are theysimple or complex? We formalize this below: Assumption 3 (Simple and Complex Predictors). The invariant feature comprises of complex features, i.e. = [c], where c Mc. The variant feature comprises of simple and complex features, i.e., = [c, s],where c Mc, s Ms and I(Y ; s) > 0.",
  "Experiments": "We show that CMID reduces simplicity bias, and yields improvements across various robustness, OODgeneralization and fairness metrics. We first show that CMID mitigates simplicity bias in Slab and ImageNet-9 datasets. We then evaluate CMID on various OOD generalization tasks. These include data with multiplespurious features, real-world medical data, and a real-world fairness application. Finally, we test CMID onsome benchmark datasets for subgroup robustness and a fairness application. 2. We note that past approachesusually target one or two of these problem settings. Thus, in each section, we generally choose the mosttask-relevant methods to compare with, as established in prior work on that task. For consistency, in additionto ERM, we compare with JTT on most of the tasks, since it is the most similar method to CMID in terms ofits requirements. Throughout, we observe that CMID improves considerably on ERM and compares favorablywith JTT and domain-specific approaches on most data.",
  ": Results on the slab dataset, whentraining a linear model and a 1-hidden-layerNN using ERM, JTT, and CMID": "Slab Data.Slab data was proposed in Shah et al. (2020)to model simplicity bias. Each feature is composed of k datablocks or slabs. We consider two configurations of the slab data,namely 3-Slab and 5-Slab, as shown in . In both cases,the first feature is linearly separable. The second feature has3 slabs in the 3-Slab data and 5 slabs in the 5-Slab data. Thefirst feature is simple since it is linearly separable, while thefeatures with more slabs involve a piece-wise linear model andare thus complex. The linear model is perfectly predictive, butthe predictor using both types of features attains a much bettermargin, and generalizes better under fixed 1-norm perturba-tions to the features. shows the decision boundary usingERM/JTT and the proposed approach. We see that CMID encourages the model to use both features andattain better margin. We note that since both the features are fully predictive in this setting, approaches likeJTT are ineffective as they rely on incorrect predictions of one model to train another model by upweightingsuch samples. Texture vs Shape Bias on ImageNet-9.Geirhos et al. (2022) showed that CNNs trained on ImageNettend to make predictions based on image texture rather than image shape. To quantify this phenomenon, theauthors designed the GST dataset, which contains synthetic images with conflicting shapes and textures (e.g.,an image of a cat modified with elephant skin texture as a conflicting cue). The shape bias of a model on theGST dataset is defined as the number of samples for which the model correctly identifies shape compared tothe total number of samples for which the model correctly identifies either shape or texture. A shape bias of100 indicates that the model always uses shape when shape conflicts with texture, whereas 0 indicates thatthe model always uses texture instead of shape.",
  "Optimal-7575075750": ": Comparison of average test accuracies on i.i.d and OOD data and their difference (gap) on ColoredMNIST and Color+Patch MNIST datasets. Bold and underlined numbers indicate the best and second-bestOOD performance among the methods that dont use group labels. We consider ImageNet-9, a subset of ImageNet organized into nine classes by Xiao et al. (2020) (details inAppendix G.1.2) and train a ResNet18 model. We test OOD model performance on ImageNet-A (Hendryckset al., 2021), a set of natural adversarial examples on which existing ImageNet classifiers perform poorly. shows that CMID mitigates texture bias, resulting in improved performance on adversarial examples.",
  "Better OOD Generalization": "Synthetic: CMNIST and Color+Patch MNIST.We present results on two variants of the MNISTdataset (Deng, 2012), which contains images of handwritten digits, using a binary digit classification task(< 5 or not). The colored-MNIST data was proposed in Arjovsky et al. (2020), where color (red/green) isinjected as a spurious feature, with unstable correlation 1 pe with the label across environments. The traindata has two environments with pe = 0.1, 0.2 while the test data has pe = 0.9, to test OOD performance.Further, it contains 25% label noise to reduce the predictive power of the task-relevant feature: digit shape.We also consider the color+patch MNIST data proposed in Bae et al. (2022), where an additional spuriousfeature is injected into the data, in the form of a 3 3 patch. The position of the patch (top left/bottomright) is correlated with the label, with the same pe, but independent of the color. Following Bae et al. (2022), we evaluate on i.i.d test data with pe = 0.1 and OOD data with pe = 0.9 forboth the cases (details in Appendix G.2.1). shows that CMID gets competitive OOD performancewith methods that require group knowledge, and has the lowest gap gap between test performance on i.i.dand OOD samples, even in the presence of multiple spurious features. Medical: Camelyon17-WILDS.Camelyon17-WILDS is a real-world medical image dataset of datacollected from five hospitals (Bndi et al., 2019; Koh et al., 2021). Three hospitals comprise the training set,one is the validation set and the third is the OOD test set. Images from different hospitals vary visually. Thetask is to predict whether or not the image contains tumor tissue, and the dataset is a well-known OODgeneralization benchmark (Bae et al., 2022; Koh et al., 2021). shows that CMID leads to higher",
  "GDROYes93.591.492.988.981.477.788.969.9": ": Average and worst-group test accuracies on benchmark datasets for subgroup robustness. Bold andunderlined numbers indicate the best and second-best worst-group accuracy among the methods that dontuse group labels. average accuracies than existing group-based methods when evaluated on images from the test hospital.While JTT attains a comparable test accuracy, it exhibits a much larger gap between ID and OOD accuracycompared to CMID. Fairness: Adult-Confounded.The Adult-Confounded dataset is a semi-synthetic variant of the UCIAdult dataset (Newman et al., 1998; Leisch & Dimitriadou, 2021), developed by Creager et al. (2021). Itconsists of four sensitive subgroups based on binarized race and sex labels, and has confounded data wheresubgroup membership is predictive of the label on the training data but the correlation is reversed at testtime. Therefore, it tests whether the classifier makes biased predictions based on subgroup membership.",
  "Subgroup Robustness": "We evaluate our approach on four benchmark classification tasks for robustness to spurious correlations,namely on Waterbirds (Sagawa* et al., 2020), CelebA (Liu et al., 2015), MultiNLI (Williams et al., 2018) andCivilComments-WILDS (Borkan et al., 2019) datasets (, details in Appendix G.3). Although CMID does not require group knowledge for training, following Liu et al. (2021a), we use a validationset with group labels for model selection. shows the average and worst-group accuracies for CMIDand comparison with other methods (Duchi et al., 2019; Nam et al., 2020; Liu et al., 2021a) which do not usegroup information. GDRO (Sagawa* et al., 2020), which uses group information, acts as a benchmark. Wesee that on three of these datasets, CMID competes with state-of-the-art algorithms that improve subgrouprobustness. Interestingly, CMID seems particularly effective on the two language-based datasets. We alsonote that CMID is not very effective on CelebA images. We believe that this is because both the spuriousfeature (gender) and the invariant feature (hair color) for CelebA are of similar complexity. In .5,we explore this further with a similar experiment as in for CelebA.",
  "Fairness Application: Bias in Occupation Prediction": "The Bios dataset (De-Arteaga et al., 2019; Cheng et al., 2023) is a large-scale dataset of more than 300kbiographies scrapped from the internet. The goal is to predict a persons occupation based on their bio.Based on this task, Cheng et al. (2023) formalizes a notion of social norm bias (SNoB). SNoB captures theextent to which predictions align with gender norms associated with specific occupations. In addition togender-specific pronouns, these norms encompass other characteristics mentioned in the bios. They representimplicit expectations of how specific groups are expected to behave. Cheng et al. (2023) characterizes SNoB",
  ": Comparison between performance for pre-dicting the simple feature and the complex feature onCelebA dataset": "as a form of algorithmic unfairness arising from the associations between an algorithms predictions andindividuals adherence to inferred social norms. They also show that adherence to or deviations from socialnorms can result in harm in many contexts and that SNoB can persist even after the application of somefairness interventions. We note that SNoB is a distinct type of bias, and existing de-biasing methods havenot been evaluated on this task. To quantify SNoB, the authors utilize the Spearman rank correlation coefficient (pc, rc), where pc representsthe fraction of bios associated with occupation c that mention the pronoun she, and rc measures thecorrelation between occupation predictions and gender predictions. The authors employ separate one-vs-allclassifiers for each occupation and obtain the occupation prediction for a given bio using these classifiers. Forgender predictions, they train occupation-specific models to determine the gender-based group membership(female or not) based on a persons bio, and use the predictions from these models. A higher value of (pc, rc)represents a larger social norm bias, which indicates that in male-dominated occupations, the algorithmachieves higher accuracy on bios that align with inferred masculine norms, and vice-versa. showsresults on the Bios data. We compare with a group fairness approach, Decoupled (Dwork et al., 2018) thattrains separate models for each gender, in order to mitigate gender bias. We see that CMID addresses SNoBbias better than ERM and Decoupled, achieving a lower (pc, rc) and improved accuracy.",
  "Additional Experiment: Invariant and Spurious Features have Similar Complexity for CelebA": "In our subgroup robustness experiment for CelebA (), we found that our method did not yield asignificant improvement in worst-group accuracy. We investigate this further in this section. We show thatfor the CelebA dataset, the complexity of the invariant and surrogate features are actually quite similar.The experiment is similar to the experiments we did for CMNIST and Waterbirds in . We create asubset of the CelebA dataset by sampling an equal number of samples from all four subgroups. presents a comparison of the results when predicting the invariant feature (hair color) and the surrogatefeature (gender) using the simple model (2DConvNet2). We observe that the performance for both tasks iscomparable, suggesting that the features exhibit similar complexity. We contrast these results with those for CMNIST and Waterbirds in . For CMNIST and Waterbirds,there was a significant difference in the accuracy with which the simple model could predict the invariant andsurrogate features. For CelebA, the difference is much smaller which suggests that spurious features are notsimpler than invariant features for this datasetexplaining why our method is not as effective for it.",
  "Conclusion": "Our work presented a hypothesis about spurious features connected to simplicity bias: spurious features aresimple features that are still predictive of the label, which proved to be valid across several datasets. Weleveraged this hypothesis to propose a new framework (CMID) to mitigate simplicity bias and showed that ityields improvements over ERM and many other previous approaches across a number of OOD generalization,robustness and fairness benchmarks. It would be interesting to consider other natural definitions of spuriousfeatures (some of which may apply even when our definition is not suitable), and to build a comprehensiveand principled framework for distributional robustness under suitable assumptions. On the algorithmic side,our work suggests that using simple models to audit and refine much larger models can lead to improvedproperties of the larger model along certain robustness and fairness axes. It would be interesting to explorethe power of similar approaches for other desiderata and requirements that arise in trustworthy machinelearning, and to understand its capabilities for fine-tuning large pre-trained models.",
  "Carlo Alberto Barbano, Enzo Tartaglione, and Marco Grangetto. Unsupervised learning of unbiased visualrepresentations, 2022. 24": "Nourhan Bayasi, Ghassan Hamarneh, and Rafeef Garbi. Boosternet: Improving domain generalization ofdeep neural nets using culpability-ranked features. In 2022 IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pp. 528538, 2022. doi: 10.1109/CVPR52688.2022.00062. 24 Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics formeasuring unintended bias with real data for text classification. In Companion Proceedings of The 2019World Wide Web Conference, 2019. 11, 34 Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings ofthe 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 06,pp. 535541, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933395. doi:10.1145/1150402.1150464. URL 23 Pter Bndi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, MeykeHermsen, Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, QuanzhengLi, Farhad Ghazvinian Zanjani, Svitlana Zinger, Keisuke Fukuta, Daisuke Komura, Vlado Ovtcharov,Shenghua Cheng, Shaoqun Zeng, Jeppe Thagaard, Anders B. Dahl, Huangjing Lin, Hao Chen, LudwigJacobsson, Martin Hedlund, Melih etin, Eren Halc, Hunter Jackson, Richard Chen, Fabian Both, JrgFranke, Heidi Ksters-Vandevelde, Willem Vreuls, Peter Bult, Bram van Ginneken, Jeroen van der Laak,and Geert Litjens. From detection of individual metastases to classification of lymph node status at thepatient level: The camelyon17 challenge. IEEE Transactions on Medical Imaging, 38(2):550560, 2019. doi:10.1109/TMI.2018.2867350. 10",
  "Myra Cheng, Maria De-Arteaga, Lester Mackey, and Adam Tauman Kalai. Social norm bias: residual harmsof fairness-aware algorithms. Data Mining and Knowledge Discovery, pp. 127, 2023. 11, 35": "Elliot Creager, Joern-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning.In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on MachineLearning, volume 139 of Proceedings of Machine Learning Research, pp. 21892200. PMLR, 1824 Jul 2021.URL 2, 3, 8, 10, 11, 23, 32, 33 Nikolay Dagaev, Brett D. Roads, Xiaoliang Luo, Daniel N. Barry, Kaustubh R. Patil, and Bradley C. Love. Atoo-good-to-be-true prior to reduce shortcut reliance. Pattern Recogn. Lett., 166(C):164171, feb 2023. ISSN0167-8655. doi: 10.1016/j.patrec.2022.12.010. URL Maria De-Arteaga, Alexey Romanov, Hanna Wallach, Jennifer Chayes, Christian Borgs, Alexandra Choulde-chova, Sahin Geyik, Krishnaram Kenthapadi, and Adam Tauman Kalai. Bias in bios: A case study of seman-tic representation bias in a high-stakes setting. In Proceedings of the Conference on Fairness, Accountability,and Transparency, FAT* 19, pp. 120128, New York, NY, USA, 2019. Association for Computing Machinery.ISBN 9781450361255. doi: 10.1145/3287560.3287572. URL 35",
  "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE SignalProcessing Magazine, 29(6):141142, 2012. 10": "Yihe Deng, Yu Yang, Baharan Mirzasoleiman, and Quanquan Gu. Robust learning with progressive dataexpansion against spurious correlation. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. URL 4, 24 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectionaltransformers for language understanding. In Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Longand Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for ComputationalLinguistics. doi: 10.18653/v1/N19-1423. URL 35",
  "Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on Learning Representations, 2019. URL 1": "Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori Hashimoto, and Percy Liang. Out-of-distributionrobustness via targeted augmentations. In NeurIPS 2022 Workshop on Distribution Shifts: ConnectingMethods and Applications, 2022. URL 2 Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. 3 Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and WielandBrendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy androbustness, 2022. 3, 9, 22, 31 Jianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. Knowledge distillation: A survey.International Journal of Computer Vision, 129(6):17891819, March 2021. ISSN 1573-1405. doi: 10.1007/s11263-021-01453-z. URL 23 Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith.Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,Volume 2 (Short Papers), pp. 107112, New Orleans, Louisiana, June 2018. Association for ComputationalLinguistics. doi: 10.18653/v1/N18-2017. URL 22 Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning. In Proceedingsof the 30th International Conference on Neural Information Processing Systems, NIPS16, pp. 33233331,Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN 9781510838819. 22",
  "Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.Fairness withoutdemographics in repeated loss minimization. In International Conference on Machine Learning, 2018. 22": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016. doi:10.1109/CVPR.2016.90. 4, 34 Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration forthe (Computationally-identifiable) masses. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35thInternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp.19391948. PMLR, 1015 Jul 2018. URL 22",
  "Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient forrobustness to spurious correlations. In International Conference on Machine Learning, 2022. 4": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: A benchmark ofin-the-wild distribution shifts. In International Conference on Machine Learning, pp. 56375664. PMLR,2021. 10, 32, 33, 34 Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and Ed H.Chi. Fairness without demographics through adversarially reweighted learning. In Proceedings of the 34thInternational Conference on Neural Information Processing Systems, NIPS20, Red Hook, NY, USA, 2020.Curran Associates Inc. ISBN 9781713829546. 22, 33 Jungsoo Lee, Eungyeup Kim, Juyoung Lee, Jihyeon Lee, and Jaegul Choo. Learning debiased representationvia disentangled feature augmentation. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan(eds.), Advances in Neural Information Processing Systems, 2021. URL 26",
  "Friedrich Leisch and Evgenia Dimitriadou. mlbench: Machine Learning Benchmark Problems, 2021. Rpackage version 2.1-3.1. 11, 33": "Zhiheng Li, Anthony Hoogs, and Chenliang Xu. Discover and mitigate unknown biases with debiasing alternatenetworks. In Computer Vision ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327,2022, Proceedings, Part XIII, pp. 270288, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-19777-2. doi: 10.1007/978-3-031-19778-9_16. URL 24 Jongin Lim, Youngdong Kim, Byungjai Kim, Chanho Ahn, Jinwoo Shin, Eunho Yang, and Seungju Han.Biasadv: Bias-adversarial augmentation for model debiasing. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition (CVPR), pp. 38323841, June 2023. 24 Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, PercyLiang, and Chelsea Finn. Just train twice: Improving group robustness without training group information.In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on MachineLearning, volume 139 of Proceedings of Machine Learning Research, pp. 67816792. PMLR, 1824 Jul2021a. URL 4, 10, 11, 23, 24, 31, 32, 33, 34, 35",
  "Depen Morwani, Jatin Batra, Prateek Jain, and Praneeth Netrapalli. Simplicity bias in 1-hidden layer neuralnetworks, 2023. 1, 3": "Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L. Edelman, Fred Zhang, andBoaz Barak. SGD on Neural Networks Learns Functions of Increasing Complexity. Curran Associates Inc.,Red Hook, NY, USA, 2019. 1, 3 Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deepdouble descent: Where bigger models and more data hurt. In International Conference on LearningRepresentations, 2020. URL 1 Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Trainingdebiased classifier from biased classifier. In Advances in Neural Information Processing Systems, 2020. 4,11, 24, 26",
  "D.J. Newman, S. Hettich, C.L. Blake, and C.J. Merz. Uci repository of machine learning databases, 1998.URL ~mlearn/MLRepository.html. 11, 33": "Hongjing Niu, Hanting Li, Feng Zhao, and Bin Li.Roadblocks for temporarily disabling shortcutsand learning new knowledge.In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, andA. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2906429075. Cur-ran Associates, Inc., 2022. URL 24 Matteo Pagliardini, Martin Jaggi, Franois Fleuret, and Sai Praneeth Karimireddy. Agree to disagree:Diversity through disagreement for better transferability. In The Eleventh International Conference onLearning Representations, 2023. URL 23, 24",
  "Ji Ho Park, Jamin Shin, and Pascale Fung. Reducing gender bias in abusive language detection, 2018. 34": "Mohammad Pezeshki, Skou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and GuillaumeLajoie. Gradient starvation: A learning proclivity in neural networks. In A. Beygelzimer, Y. Dauphin,P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL 3 Shikai Qiu, Andres Potapczynski, Pavel Izmailov, and Andrew Gordon Wilson. Simple and Fast GroupRobustness by Automatic Feature Reweighting. International Conference on Machine Learning (ICML),2023. 4",
  "Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minimization.In International Conference on Learning Representations, 2021. URL 6, 8, 24": "Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neuralnetworks. In International Conference on Learning Representations, 2020. URL 1, 4, 10, 11, 23, 24, 33, 34, 35 Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparameteri-zation exacerbates spurious correlations. In Proceedings of the 37th International Conference on MachineLearning, ICML20. JMLR.org, 2020. 6",
  "Saeid Asgari Taghanaki, Aliasghar Khani, Fereshte Khani, Ali Gholami, Linh Tran, Ali Mahdavi-Amiri, andGhassan Hamarneh. Masktune: Mitigating spurious correlations by forcing to explore, 2022. 24": "Enzo Tartaglione, Carlo Alberto Barbano, and Marco Grangetto. End: Entangling and disentangling deeprepresentations for bias correction. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 1350813517, June 2021. 26 Damien Teney, Ehsan Abbasnejad, Simon Lucey, and Anton van den Hengel. Evading the simplicity bias:Training a diverse set of models discovers solutions with superior ood generalization. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1676116772, June2022. 23, 24 Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Mind the trade-off: Debiasing NLUmodels without degrading the in-distribution performance. In Proceedings of the 58th Annual Meetingof the Association for Computational Linguistics, pp. 87178729, Online, July 2020a. Association forComputational Linguistics. doi: 10.18653/v1/2020.acl-main.770. URL 23 Prasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. Towards debiasing NLU models fromunknown biases. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 75977610, Online,November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.613. URL 24 Guillermo Valle-Perez, Chico Q Camargo, and Ard A Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. International Conference on Learning Representations,2019. 3",
  "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset.Technical Report CNS-TR-2011-001, California Institute of Technology, 2011. 4": "Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing.Learning robust representations byprojecting superficial statistics out. In International Conference on Learning Representations, 2019a. URL 26 Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets are notenough: Estimating and mitigating gender bias in deep image representations. In International Conferenceon Computer Vision (ICCV), October 2019b. 23",
  "Wanqian Yang, Polina Kirichenko, Micah Goldblum, and Andrew Gordon Wilson. Chroma-vae: Mitigatingshortcut learning with generative classifiers, 2022. 24": "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deeplearning requires rethinking generalization. In International Conference on Learning Representations, 2017a.URL 1 Jianyu Zhang, David Lopez-Paz, and Leon Bottou.Rich feature construction for the optimization-generalization dilemma. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, GangNiu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning,volume 162 of Proceedings of Machine Learning Research, pp. 2639726411. PMLR, 1723 Jul 2022a. URL 23, 24 Michael Zhang, Nimit S Sohoni, Hongyang R Zhang, Chelsea Finn, and Christopher Re. Correct-n-contrast:a contrastive approach for improving robustness to spurious correlations. In Kamalika Chaudhuri, StefanieJegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th InternationalConference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 2648426516.PMLR, 1723 Jul 2022b. URL 4",
  "In this section, we describe our technique to compute the CMI regularization for our approach, as mentionedin . We begin with the relevant notation": "Let Z = (X, Y ) denote an input-label pair, where X X, Y {0, 1}, sampled from some distribution D, Ddenote a dataset with n samples, M() : X denote a model, parameterized by (shorthand M). Thepredictions of the model are given by 1[M(X) > 0.5]. Subscripts ()s and ()c denote simple and complex,respectively. Let M denote the class of all models M, M(Z) : X {0, 1} R denote a loss function.Let H() denote the Shannon entropy of a random variable. I(; ) measures Shannon mutual informationbetween two random variables. With slight abuse of notation, let M and Y also denote the (binary) randomvariables associated with the predictions of model M on datapoints Xis and the labels Yis, across all i [n],",
  "where m, m, y {0, 1}, (M(Xi), 1)=1[M(Xi)>0.5], and (M(Xi), 0)=11[M(Xi)>0.5]": "Note that the CMI computed using these would not be differentiable as these densities are computed bythresholding the outputs of the model. Since we want to add a CMI penalty as a regularizer to the ERMobjective and optimize the proposed objective using standard gradient-based methods, we need a differentiableversion of CMI. Thus, for practical purposes, we use an approximation of the indicator function 1[x > 0.5],given by (x 0.5, T), where T determines the degree of smoothness or sharpness in the approximation. This can be easily generalized for multi-class classification with C classes. In that case, m, m, y {0, , C 1}, M(Xi) is a C-dimensional vector with the mth entry indicating the probability of predicting class m,and (M(Xi), m)= 1[arg maxj[C]Mj(Xi) = m]. To make this differentiable, we use the softmax function with",
  ": Comparison of computationtimes (in milliseconds) for the originalCMI and the estimated CMI in (3)": "In this section, we evaluate the reliability and scalability of theCMI estimate in (3) compared to the original CMI, which iscomputed with discretized model outputs. We consider CMNISTdata with the hyperparameter values as mentioned in SectionG.2.1 for the results in this section. compares the times(in milliseconds) to compute the original CMI and the estimatedCMI, using batch size 64, for 10 classes and 200 classes. We seethat the computation time for the estimated CMI does not increase significantly as the number of classesincreases.",
  ": Performance of CMIDusing different simple model archi-tectures for CMNIST dataset": "In (), we compared the effect of using various simplemodel architectures on the performance of the model learned by CMID onthe Waterbirds dataset. In this section, we conduct a similar experimentto analyze the effect of using various simple model architectures on theCMNIST dataset. compares the performance of CMID whenusing a linear model and an MLP as the simple model. We observe thatusing a linear model is more effective in this case since the benchmarkmodel for this task is an MLP. As a result, the MLP may capture bothfeatures, and regularizing the CMI with respect to this model may notbe very effective in reducing the reliance on just the spurious features.",
  ": Training details for the simple models": "We note that in these experiments (for both Waterbirds and CMNIST datasets), we only tune the learningrate (LR) and weight decay (2) for training the simple model (as listed in ). The rest of thehyperparameter values for training the final model are kept consistent across each dataset (as listed in for CMNIST and for Waterbirds).",
  "BFurther Related Work": "Fairness.Several works aim to ensure fair predictions of models across subgroups in the data, whichare defined based on sensitive demographic information. Commonly used notions of group fairness includedemographic parity (Dwork et al., 2012), which aims to ensure that the representation of different demographicgroups in the outcomes of a model is similar to their overall representation, and equalized odds (Hardt et al.,2016), which aims for equal predictive performance across groups using metrics such as true positive rate.Fairness interventions used when group information is available include data reweighting to balance groupsbefore training (Kamiran & Calders, 2012), learning separate models for different subgroups (Dwork et al.,2018), and post-processing of trained models, such as adjusting prediction thresholds based on fairness-basedmetrics (Hardt et al., 2016). Various approaches have been proposed to achieve fairness when demographic information may not be available.Multicalibration (Hebert-Johnson et al., 2018) aims to learn a model whose predictions are calibrated for allsubpopulations that can be identified in a computationally efficient way. Hashimoto et al. (2018) proposes adistributionally robust optimization (DRO)-based approach to minimize the worst-case risk over distributionsclose to the empirical distribution to ensure fairness. Lahoti et al. (2020) proposes adversarial reweightedlearning (ARL), where an auxiliary model identifies subgroups with inferior performance and the model ofinterest is retrained by reweighting these subgroups to reduce bias. Feature Diversification and De-biasing Methods.Deep neural networks are known to exhibit unwantedbiases. For instance, CNNs trained on image data may exhibit texture bias (Geirhos et al., 2022), andlanguage models trained on certain datasets may exhibit annotation bias (Gururangan et al., 2018). Severalmethods have been proposed to mitigate these biases. Bahng et al. (2020) introduce a framework (ReBias) to",
  "In general, prior work focuses on one or two of the applications that we consider, while our approach proveseffective across several datasets for all cases": "Several methods that aim to improve subgroup robustness or OOD generalization, such as IRM (Arjovsky et al.,2020), GDRO (Sagawa* et al., 2020), PI (Bao et al., 2021) and BLOOD (Bae et al., 2022), require knowledgeof group or environment labels. This additional information is not always available and consequently, thesemethods have limited applicability in such cases. Approaches like JTT (Liu et al., 2021a) and EIIL (Creageret al., 2021) overcome this requirement. However, these methods rely on incorrect predictions (Liu et al.,2021a; Creager et al., 2021) of one model to train another model by upweighting such samples (Liu et al.,2021a) or learning features that generalize across such samples (Creager et al., 2021). As a result, they maynot be effective in settings where the spurious feature is highly predictive. Such settings can be relevant whenour goal is to mitigate simplicity bias. For example, in the slab dataset (), the simple model achievesperfect accuracy but has a much worse margin compared to the global max margin predictor. Our approachis still effective in such cases because it relies on prediction probabilities rather than thresholded predictions,",
  "which enables the differentiation between features that result in smaller margin or low confidence predictionsand those that lead to better margin or high confidence predictions": "We note that debiasing/feature diversification methods (Nam et al., 2020; Bahng et al., 2020; Li et al., 2022;Zhang et al., 2022a; Teney et al., 2022) can often be computationally expensive. Some of these involvetraining two complex models simultaneously (Nam et al., 2020; Bahng et al., 2020) or alternatively (Li et al.,2022). Zhang et al. (2022a); Teney et al. (2022) involves training multiple models and returning the averageor selecting the best one among them, respectively. In contrast, our results show that the simple approach totrain a single model with the CMI regularization can prove effective across several settings. While methodslike Pagliardini et al. (2023); Lee et al. (2022b) are simpler, they require access to additional data from atarget domain to encourage disagreement between two models for feature diversification. Our approach doesnot have such a requirement and allows for diversification directly on the training data. Several methods use a similar framework of first training a biased model and then training another modeldebiased or regularized with respect to the first one. We discuss the main differences with these methodshere. Many of these methods are designed (or evaluated) specifically for either vision (Jeon et al., 2022; Leeet al., 2022a; Bayasi et al., 2022; Yang et al., 2022; Kim et al., 2022; Taghanaki et al., 2022; Niu et al., 2022;Park et al., 2023; Barbano et al., 2022; Lim et al., 2023) or language (Utama et al., 2020b) datasets, whereasour approach is more broadly applicable to several modalities including vision, language and tabular data.Utama et al. (2020b) is similar to JTT and can suffer from similar drawbacks. Jeon et al. (2022); Bayasi et al.(2022) are designed specifically for CNN-based architectures. Several of these methods are computationallyintensive: Lee et al. (2022a); Kim et al. (2022) involve training several biased models before training the finaldebiased model, Yang et al. (2022) involves training a VAE in the first stage, Taghanaki et al. (2022) involvesusing an explainability approach to create a masked version of the dataset based on the first stage model totrain the final model, Bansal et al. (2022) proposes a model selection strategy that requires training multiplemodels before selecting the best one, Niu et al. (2022) involves using three models: a pre-trained model,and an autoencoder and the final model that are trained simultaneously, Barbano et al. (2022) involves athree-stage approach, Lim et al. (2023) involves creating adversarial samples for a biased model which areused to train the final model. In addition, our approach is theoretically grounded and explicit in its assumptions, which could make iteasier for a practitioner to evaluate and use. In particular, our definition of spurious features gives anexplicit characterization of features/biases that we regard as undesirable and our approach seeks to reduce,which can allow the user to understand the situations when the approach might be effective. We note thatmost approaches developed to improve subgroup robustness or OOD generalization either lack theoreticalguarantees (Bao et al., 2021; Bae et al., 2022; Liu et al., 2021a) or show results tailored to one of these twosettings. Sagawa* et al. (2020); Duchi et al. (2019) seek to optimize the worst-case risk over a small set ofgroups and hence, guarantees on worst-group risk developed in prior work on DRO which considers all subsetsof the data as the groups of interest, apply in these cases. Recent work by Deng et al. (2023); Chen et al.(2023) analyzes the learning dynamics of a two-layer CNN in the presence of spurious features. Deng et al.(2023) show that imbalanced data groups and predictive spurious features can cause stronger reliance on thesefeatures. Similarly, Chen et al. (2023) show that while the model learns both features, the spurious featuresare learned faster. However, these methods may not generalize to unseen environments, as is often times thecase in the OOD generalization problem. On the other hand, Arjovsky et al. (2020) prove that IRM canrecover the optimal invariant predictor that can generalize to any environment, under certain assumptionson the training data. However, follow-up work (Rosenfeld et al., 2021) shows that IRM may fail to recoversuch a predictor if the training set does not contain samples from several environments. In contrast, we showthat our approach reduces reliance on the spurious feature in a Gaussian mixture-based data setting, andalso obtain an OOD generalization guarantee in a causal learning framework, without requiring group orenvironment labels.",
  "DLimitations of our Approach": "We note that like any other regularization or inductive bias, CMID may not be effective for every task. Forcertain tasks, spurious features as defined by us in may not actually be spurious, and we may notalways want to reduce reliance on features that are simple and highly predictive of the label. However, it is",
  "In this section, we consider the same setup as in and present some additional results": ": Effect of CMI regularization (wrt X2) for different values of c (corresponding to regularizationstrength). Left: Lower values of c indicate stronger CMI regularization, resulting in more upweighting of w1wrt w2. Inset shows a zoomed-in region and markers compare the three solutions when 22/21 = 1/6. Right:Decision boundaries for predictors corresponding to the markers.",
  "E.2Results on the 10-class Colored MNIST Dataset": "In this section, we consider the 10-class colored MNIST dataset (following the setup in (Lee et al., 2021)),which is a commonly used dataset for evaluating debiasing methods. In the train set, the samples in everyclass are injected with a color with 95% correlation, while the colors in the test set are uncorrelated with thelabel.",
  ": Comparison of OOD test accuracy on10-class colored MNIST dataset": "In , we compare the test accuracy of our approachwith several debiasing methods, namely LfF (Nam et al.,2020), HEX (Wang et al., 2019a), and recent methodssuch as EnD (Tartaglione et al., 2021), DFA (Lee et al.,2021) and ReBias (Bahng et al., 2020). We observe thatCMID significantly improves over HEX, LfF and EnD, and is comparable to DFA, while ReBias significantlyoutperforms all other methods. We note that ReBias focuses particularly on debiasing with respect to textureor color bias, which is why it is more effective on this dataset.",
  "E.3Results using ViT": "In this section, we seek to verify that our method remains effective when the complex model is a ViT insteadof a CNN. Specifically, we compare the performance of ERM and CMID on the Waterbirds dataset, whentraining a ViT-B/16 model (Dosovitskiy et al., 2021). As shown in , our method attains better testperformance compared to ERM.",
  ": Comparison of average and worst-group accuracy on Waterbirds dataset whenthe complex model is a ViT": "In this experiment, for our approach, we use the same shallowNN as the simple model class, as used in the subgroup robustnessexperiments. Note that when considering our definition of modelcomplexity (number of trainable parameters, layers, etc.), deepViTs and CNNs are of the same scale. As a result, the choice ofthe simple model class can remain the same when consideringthe two architectures.",
  ": Comparison of average and worst-group ac-curacy on Waterbirds and CelebA datasets with CLIP": "We note that these datasets are challenging due toimbalances in training data. For example, predictingblonde vs. not-blonde on the CelebA dataset isdifficult because there are very few blonde males inthe training data. The imbalances in CLIP trainingdata, on the other hand, are unclear. While CLIPmodels may perform well on some benchmarks, itremains uncertain if they are effective in all cases.The results above demonstrate that CLIP models can be susceptible to natural spurious correlations,highlighting the need for grounded and consistent robustness techniques, such as the proposed method CMID.",
  "Proof. Using Assumption 3, the class of simple models only contains variant predictors, so ERM(Ms) = s.Consequently, the constraint in (2) can be written as I(M; s|Y ) = 0": "Considering the set of candidate predictors for M, namely {0, c, s, c}, we examine the CMI constraintfor each. Using the definition of mutual information, we have I(0, s|Y ) = 0 and I(s, s|Y ) = H(s|Y ).According to the definition of variant predictor, H(s|Y ) > H(s|Y, E) 0.",
  "GExperimental Settings": "We begin by describing some common details and notation that we use throughout this section. As in themain text, we use to represent the regularization strength for CMI. To ensure effective regularization, weadopt an epoch-dependent approach by scaling the regularization strength using the parameter S. Specifically,we set = c (1 + t/S) at epoch t. The temperature parameter T is set as 12.5 throughout the experiments.Additionally, we use LR to denote the learning rate, BS to denote the batch size, and 2 to denote the weightdecay parameter, which represents the strength of 2-regularization. When using the Adam optimizer, weemploy the default values for momentum. The experiments on Slab data, CMNIST and CPMNIST data and Adult-Confounded data were implementedon Google Colab. The ImageNet-9 experiments were run on an AWS G4dn instance with one NVIDIA T4GPU. For experiments on the subgroup robustness datasets and the Camelyon17-WILDS data, we used twoNVIDIA V100 GPUs with 32 GB memory each. We only used CPU cores for the Bios data experiments.",
  "G.1.1Slab Data": "Dataset.All the features in the 3-Slab and 5-Slab data are in the range . The features are generatedby defining the range of the slabs along each direction and then sampling points in that range uniformlyat random. The base code for data generation came from the official implementation of Shah et al. (2020)available at We consider 105 training samplesand 5 104 test samples. In both cases, the linear margin is set as 0.05. The 3-Slab data is 10-dimensional,where the remaining 8 coordinates are standard Gaussians and are not predictive of the label. The slabmargin is set as 0.075. The 5-slab data is only 2-dimensional, and the slab margin is set as 0.14. Training.We consider a linear model for the simple model and following Shah et al. (2020), a 1-hiddenlayer NN with 100 hidden units as the final model (for both ERM and CMID). Throughout, we use SGDwith BS = 500, 2 = 5 104 for training. The linear model is trained with LR = 0.05, while the NN istrained with LR = 0.005. For the 3-Slab data, the models are trained for 300 epochs. We consider c {100, 150, 200} and choosec = 150 for the final result. For the 5-Slab data, the models are trained for 200 epochs and we use a 0.99momentum in this case. We consider c {1000, 2000, 2500, 3000} and choose c = 3000 for the final result.Note that we consider significantly high values of c for this dataset compared to the rest because the simplemodel is perfectly predictive of the label in this case. This implies that its CMI with the final model is verysmall, and the regularization strength needs to be large in order for this term to contribute to the loss.",
  "G.1.2Texture vs Shape Bias on ImageNet-9": "DatasetImageNet-9 (Xiao et al., 2020) is a subset of ImageNet with nine condensed classes that eachconsist of images from multiple ImageNet classes. These include dog, bird, wheeled vehicle, reptile, carnivore,insect, musical instrument, primate, and fish. ImageNet-A (Hendrycks et al., 2021) is a set of handpickedimages that ResNet50 models trained on ImageNet classify incorrectly. We organize these images into thesame nine classes as ImageNet-9.",
  ":ImageNet-9classesmapped to corresponding GSTdataset classes": "Calculating shape biasThe GST dataset (Geirhos et al., 2022)consists of 16 shape classes. To interpret a prediction from a modeltrained on ImageNet-9 as a GST dataset prediction, we consider a subsetof classes from both and use the mapping listed in . Specifically,to determine whether a model trained on ImageNet-9 predicts correctlyon a GST image, we first determine which of the 5 ImageNet-9 classesfrom the Table has the highest probability based on the models output,and then use the mapping to obtain the predicted GST class label.",
  "number of correct shape predictions + number of correct texture predictions": "TrainingWe use ResNet18 pre-trained on ImageNet data as the simple model and train it on ImageNet-9using Adam with LR = 0.001, BS = 32, 2 = 104 for 2 epochs. We do not consider a simpler architecture andtraining from scratch since this pre-trained model already exhibits texture bias. For the final model, we considerthe same model and batch size, but we use SGD with 0.9 momentum as the optimizer, LR = 104, 2 = 0.001,and train for 10 epochs. We report the mean and standard deviation over 3 runs, where for each run we selectthe model with the highest shape bias. Values of CMID-specific parameters were c = 30, S = 10. For tuning,we consider LR {105, 104, 103} for both the models and BS {16, 32}, 2 {0.0001, 0.001, 0.01} andc {0.5, 15, 30, 50}. We implement JTT to obtain the results. We consider the upsampling parameter (Liuet al., 2021a) up {5, 20, 50, 100} for tuning. The other hyperparameters are the same as our approach.",
  "G.2.1CMNIST and CPMNIST": "Dataset.Following Bae et al. (2022), we use 25, 000 MNIST images (from the official train split) for eachof the training environments, and the remaining 10, 000 images to construct a validation set. For both testsets, we use the 10, 000 images from the official test split. Training.The details about the model architecture and parameters for training the simple model withERM and the final model with CMID, for both datasets, are listed in . Following Bae et al. (2022),the MLP has one hidden layer with 390 units and ReLU activation function. In both cases, the simple modelis trained for 4 epochs, while the final model is trained for 20 epochs. We choose the model with the smallestaccuracy gap between the training and validation sets. For tuning, we consider the following values for eachparameter: for the simple model, LR {0.005, 0.01, 0.05}, 2 {0.001, 0.005, 0.01}, and for the final model,LR {0.001, 0.005}, c and S , where lower values of S were tried for higher values of c andvice-versa. For the final results, we report the mean and standard deviation by averaging over 4 runs. For comparison, weconsider the results reported by Bae et al. (2022) for all methods, except EIIL (Creager et al., 2021) and JTT(Liu et al., 2021a). Results for EIIL are obtained by using their publicly available implementation for CMNISTdata (available at and incorporating the CPMNIST data into theirimplementation. The hyperparameter values in their implementation are kept the same. We implement JTTto obtain the results. We consider LR {0.001, 0.005, 0.01} and the parameter for upweighting minoritygroups (Liu et al., 2021a) up {5, 10, 15, 20, 25} for tuning.",
  "G.2.2Camelyon17-WILDS": "Dataset.Camelyon17-WILDS (Koh et al., 2021) contains 96 96 image patches which may or maynot display tumor tissue in the central region.We use the same dataset as Bae et al. (2022), whichincludes 302, 436 training patches, 34, 904 OOD validation patches, and 85, 054 OOD test patches, whereno two data splits contain images from overlapping hospitals. We use the WILDS package, available at for data-loading. Training.For the simple model, we train a 2DConvNet1 model (see Section G.3.2 for details) for 10epochs. We use the Adam optimizer with LR = 104, BS = 32, 2 = 104. For the final model, we traina DenseNet121 (randomly initialized, no pretraining) for 5 epochs using SGD with 0.9 momentum withLR = 104, BS = 32, 2 = 0.01 and c = 0.5, S = 10. We use the same BS and 2 values as Bae et al. (2022)for consistency. For tuning, we consider LR {105, 104, 103} for both the models and c {0.5, 2, 5, 15}for CMID. While Bae et al. (2022) and Koh et al. (2021) use learning rates 105 and 0.001, respectively, wefound a learning rate of 104 was most suited for our approach. We select the model with the highest averageaccuracy on the validation set to report the final results and report the mean and standard deviation byaveraging over 3 runs. For comparison, we use the results reported by Bae et al. (2022) for all the methods,except JTT. For the results for JTT, we implement the method and tune up {20, 50, 100} and the numberof epochs to train the first-stage model as {1, 2}. The rest of the hyperparameters are kept the same as ourapproach. We noticed that JTT (Liu et al., 2021a) had a high variance in test accuracy across multiple runsfor different random weight initialization. To account for this, we ran JTT and CMID over the same 8 seedsand randomly chose 3 of them to report the average accuracies over.",
  "G.2.3Adult-Confounded": "Dataset.The UCI Adult dataset (Newman et al., 1998; Leisch & Dimitriadou, 2021), comprises 48, 842census records collected from the USA in 1994. It contains attributes based on demographics and employmentinformation and the target label is a binarized income indicator (thresholded at $50, 000). The task iscommonly used as an algorithmic fairness benchmark. Lahoti et al. (2020); Creager et al. (2021) define foursensitive subgroups based on binarized sex (Male/Female) and race (Black/non-Black) labels: Non-BlackMales (G1), Non-Black Females (G2), Black Males (G3), and Black Females (G4). They observe that eachsubgroup has a different correlation strength with the target label (p(y = 1|G)), and thus, in some cases,subgroup membership alone can be used to achieve a low error rate in prediction. Based on this observation, Creager et al. (2021) create a semi-synthetic variant of this data, known asAdult-Confounded, where they exaggerate the spurious correlations in the original data. As G1 and G3have higher values of p(y = 1|G) across both the splits, compared to the other subgroups (see Creager et al.(2021) for exact values), these values are increased to 0.94, while they are set to 0.06 for the remainingtwo subgroups, to generate the Adult-Confounded dataset. In the test set, these are reversed, so that itserves as a worst-case audit to ensure that the model is not relying on subgroup membership alone in itspredictions. Following Creager et al. (2021), we generate the samples for the Adult-Confounded dataset byusing importance sampling. We use the original train/test splits from UCI Adult as well as the same subgroupsizes, but individual examples are under/over-sampled using importance weights based on the correlationbetween the original data and the desired correlation. Training.We use a linear model (with a bias term) as the simple model, and following Creager et al.(2021) use an Adagrad optimizer throughout. We use BS = 50. The simple model is trained for 50 epochs,with LR = 0.05, 2 = 0.001. Following Creager et al. (2021); Lahoti et al. (2020), we use a two-hidden-layer MLP architecture for the final model, with 64 and 32 hidden units, respectively. It is trained withLR = 0.04, c = 4, S = 4 for 10 epochs. We also construct a small validation set from the train split byrandomly selecting a small fraction of samples (5 50) from each subgroup (depending on its size) andthen upsampling these samples to get balanced subgroups of size 50. We choose the model with the lowestaccuracy gap between the train and validation sets. For tuning, we consider LR {0.01, 0.02, 0.03, 0.04, 0.05}and c, S {3, 4, 5}. For the final results, we report the mean and accuracy by averaging over 4 runs. Forcomparison, we reproduced the results for ERM from Creager et al. (2021), and thus, consider the valuesreported in Creager et al. (2021) for ERM, ARL and EIIL. We implement JTT to obtain the results, usingLR = 0.05 and 2 = 0.001 to train the final model. We tune the parameter for upweighting minority groups(Liu et al., 2021a) up over {10, 20, 25, 30}. The remaining parameters are kept the same as for our approach.",
  "We consider the following datasets for this task. We follow the setup in Sagawa* et al. (2020) for the firstthree and Koh et al. (2021) for CivilComments-WILDS": "WaterbirdsWaterbirds is a synthetic dataset created by Sagawa* et al. (2020) consisting of bird imagesover backgrounds. The task is to classify whether a bird is a landbird or a waterbird. The background of theimage land background or water background, acts a spurious correlation. CelebACelebA is a synthetic dataset created by Liu et al. (2015) containing images of celebrity faces. Weclassify the hair color as blonde or not blonde, which is spuriously correlated with the gender of the celebritymale or female, as done in Sagawa* et al. (2020); Liu et al. (2021a). MultiNLIMultiNLI (Williams et al., 2018) is a dataset of sentence pairs consisting of three classes:entailment, neutral, and contradiction. Pairs are labeled based on whether the second sentence entails, is",
  "neutral with, or contradicts the first sentence, which is correlated with the presence of negation words in thesecond sentence (Sagawa* et al., 2020; Liu et al., 2021a)": "CivilComments-WILDSCivilComments-WILDS is a dataset of online comments proposed by Borkanet al. (2019). The goal is to classify whether a comment is toxic or non-toxic, which is spuriously correlatedwith the mention of one or more of the following demographic attributes: male, female, White, Black,LGBTQ, Muslim, Christian, and other religion (Park et al., 2018; Dixon et al., 2018). Similar to previouswork (Koh et al., 2021; Liu et al., 2021a), we evaluate over 16 overlapping groups, one for each potentiallabel-demographic pair.",
  "G.3.2Model Architectures": "In this section, we discuss the architectures we consider for the simple models for this task. For the two imagedatasets, a shallow 2D CNN is a natural choice for the simple model as 2D CNNs can capture local patternsand spatial dependencies in grid-like data. On the other hand, for the two text datasets with tokenizedrepresentations, we consider a shallow MLP or 1D CNN for the simple model. MLPs can capture high-levelrelationships between tokens by treating each token as a separate feature, while 1D CNNs can capture localpatterns and dependencies in sequential data.",
  ": Left: 2MLP and Right: 1DConvNet architectures": "Next, we describe the details for the model architectures. Let F denote the filter size and C denote thenumber of output channels (for convolutional layers) or the output dimension (for linear/fully connected(FC) layers). Throughout, we use F =2 for the average pooling layers. shows the 2DConvNet1 andthe 2DConvNet2 architecture, which were used as simple models for Waterbirds and CelebA, respectively.These were the only two architectures we considered for the 2DCNN on these datasets. In 2DConvNet1, the2D convolutional layers use F =7, C =10 and F =4, C =20, respectively, while C =2000 for the FC layer.In the 2DConvNet2 architecture, F =5, C =10 for the 2D convolutional layer and C =500 for the FC layer. shows the 2MLP and the 1DConvNet architecture, which were used as simple models for MultiNLI andCivilComments-WILDS, respectively. For tuning, we considered these models as well as a 1DCNN with oneless 1D convolutional layer than 1DConvNet for both datasets. In 2MLP, the FC layers use C =100 and C =25,respectively. In the 1DConvNet architecture, the 1D convolutional layers use F =7, C =10, F =5, C =32 andF =5, C =64, respectively, while C =500 for the FC layer.",
  "G.3.3Training Details": "We utilize the official implementation of Sagawa* et al. (2020) available at as baseline code and integrate our approach into it.Most hyperparameter values are keptunchanged, and we list the important parameters along with model architectures for all the datasetsin . For the simple models, we consider shallow 2D CNNs for the image datasets, and MLPand 1D CNN for the text data, as discussed in the previous section.In all cases, the simple modelis trained for 20 epochs.For the final model, following Sagawa* et al. (2020), we use the Pytorchtorchvision implementation of ResNet50 (He et al., 2016) with pre-trained weights on ImageNet data",
  ": Values considered for tuning the hyperparameters for trainingthe final model for the four subgroup robustness datasets": "shows the values of LR, cand S we consider for tuning forthe final model. Following Sagawa*et al. (2020), we keep 2 = 0 forMultiNLI. For the rest, we consider2 {0.0001, 0.0005, 0.001}. For re-sults, we choose the model with the best worst-group accuracy on the validation set. For comparison, weconsider the values reported in Liu et al. (2021a).",
  "G.3.4Further Discussion on Experimental Results": ": Scatter plots comparing worst-group and average accuracy of various methods on four benchmarkdatasets for subgroup robustness. The dotted line is the x=y line. The dashed line represents the trade-offbetween average and worst-group accuracy. shows scatter plots comparing the worst-group and average accuracy of various methods on fourbenchmark datasets for subgroup robustness. The dashed line represents the trade-off between average andworst-group accuracy. Points on or close to this line correspond to methods that shift the balance between theinvariant/spurious features so that the gains in the worst-group accuracy match the decrease in the averageaccuracy. We observe that GDRO in all cases as well as CMID and JTT in several cases (except Waterbirdsand MultiNLI, respectively) lie far from this line. This suggests that in most cases, these methods allow formore gains in the worst-group accuracy compared to the decrease in the average accuracy.",
  "G.4Bias in Occupation Prediction Experiment": "We use a version of the Bios data shared by the authors of (De-Arteaga et al., 2019). We used the officialimplementation of (Cheng et al., 2023), available at toobtain results and for comparison purposes. In this implementation, they consider 25 occupations and trainseparate one-vs-all linear classifiers for each occupation based on word embeddings to make predictions. Wedirectly used their implementation to obtain results for ERM and Decoupled (Dwork et al., 2018) on the data. For our approach, we employed linear models for both the simple model and the final model. We directlyregularized the CMI with respect to the ERM from their implementation. The final model was trained for 5epochs using SGD with LR = 0.1, BS = 128, c = 5, S = 5. We only tuned the LR for this case, consideringvalues of 0.05 and 0.1."
}