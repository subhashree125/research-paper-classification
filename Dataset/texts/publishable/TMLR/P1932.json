{
  "Abstract": "Machine Learning models increasingly face data integrity challenges due to the use of large-scale training datasets drawn from the Internet. We study what model developers can do ifthey detect that some data was manipulated or incorrect. Such manipulated data can causeadverse eects including vulnerability to backdoored samples, systemic biases, and reducedaccuracy on certain input domains. Realistically, all manipulated training samples cannotbe identied, and only a small, representative subset of the aected data can be agged. We formalize Corrective Machine Unlearning as the problem of mitigating the impactof data aected by unknown manipulations on a trained model, only having identied asubset of the corrupted data. We demonstrate that the problem of corrective unlearning hassignicantly dierent requirements from traditional privacy-oriented unlearning. We ndmost existing unlearning methods, including retraining-from-scratch without the deletionset, require most of the manipulated data to be identied for eective corrective unlearning.However, one approach, Selective Synaptic Dampening, achieves limited success, unlearningadverse eects with just a small portion of the manipulated samples in our setting, whichshows encouraging signs for future progress. We hope our work spurs research towardsdeveloping better methods for corrective unlearning and oers practitioners a new strategyto handle data integrity challenges arising from web-scale training.",
  "Introduction": "Machine Learning models are increasingly trained on large and diverse datasets, with contributions fromnumerous users and organizations across millions of web-pages (Schuhmann et al., 2022; Gao et al., 2020).However, data integrity issues signicantly impact model performance by introducing systemic biases (Prabhu& Birhane, 2021; Konstantinov & Lampert, 2022) and vulnerabilities (Barreno et al., 2006b; Sanyal et al.,2021; Paleka & Sanyal, 2023). For instance, Carlini et al. (2023) showed that a small manipulated subset ofweb data can lead to large-scale model poisoning, showing the vulnerability of models to adversarial tactics.",
  "manipulates data": ": Traditionally, retraining after removing deletion data is considered a gold standard in unlearning,as all samples whose inuence is to be removed are assumed to be known. This relies on the retained data notreinforcing the eect to be unlearnt. When developers cannot identify all the manipulated data for correctiveunlearning, the retained data can continue to perpetuate the adverse eects of the manipulation. Ideally,corrective unlearning procedures should correct model outputs on the aected domain with access to only asmall but representative subset of the manipulated data. A critical real-world obstacle to eliminating this issue is that model developers can only hope to identify afraction of this manipulated data due to the size of such large-scale datasets. One practical way for modeldevelopers to approach this problem is by using methods for monitoring the integrity of data pipelines (Brecket al., 2019; Wang et al., 2019; Northcutt et al., 2021b) on randomly selected subsets of the whole data. Thiswill identify a small, representative subset of the corrupted samples. Having identied this small subset, theprimary goal is to correct the negative impacts of corrupted data and their detrimental eect on the model.Due to the high costs already incurred in training and the need to continue uninterrupted service, modeldevelopers may wish to update models to remove the inuence of the corrupted data instead of abruptlystopping the models use. We term this problem of removing the inuence of manipulated data from a trainedmodel having identied only a small fraction of it, Corrective Machine Unlearning. The goal of CorrectiveMachine Unlearning is to eciently eliminate the detrimental eects of the corrupted samples even when theprecise nature and extent of the manipulation is unknown. In this work, the manipulation is characterisedby a small representative subset of the corrupted samples which created the negative impact. Since themanipulation type is assumed to be unknown, this implies that corrective unlearning methods should workacross dierent manipulation types. Our central claim is that Corrective Unlearning has dierent underlying requirements from prior work onunlearning motivated by privacy, and thus needs separate attention. Traditional unlearning algorithms (seeNguyen et al. (2022) for a survey) are motivated by the need to cater to user data deletion requests in lightof privacy regulations (Council of European Union, 2018; California State Leglisature, 2018; Parliament ofCanada, 2018). In contrast, Corrective Unlearning algorithms do not need to obtain privacy guarantees on theunlearned data. Instead, corrective unlearning algorithms must correct the eect of the corrupted trainingdata while only having identied a small subset of said data. We illustrate in why the traditionalgold standard in the privacy-oriented unlearning literature is not sucient for corrective unlearning. In addition to highlighting the dierences of the corrective unlearning setting from traditional unlearningliterature, we benchmark popular unlearning procedures (Kurmanji et al., 2023; Goel et al., 2023; Chundawatet al., 2023b; Foster et al., 2023) in this setting. While there are myriad possible ways of constructingmanipulations, we choose two manipulations that represent distinct kinds of real-world threats. First, westudy a classic poisoning attack (Gu et al., 2019), where an adversary can manipulate both features andlabels, making the model misclassify samples that contain a specic trigger pattern, hard for model developersto notice. Second, we study a label-only manipulation called the Interclass Confusion test (Goel et al.,2023), where the adversary incorrectly labels samples between two classes, thereby entangling the modelsrepresentations. Such mislabeling can cause systemic biases in model outputs (Prabhu & Birhane, 2021).",
  "Published in Transactions on Machine Learning Research (10/2024)": "Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hateld-Dodds,Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformercircuits. Transformer Circuits Thread, 2021.",
  "We rst motivate the setting from the adversarys and the model developers perspectives": "Adversarys Perspective: The adversary can manipulate any portion of the input data, including labels insupervised learning scenarios. For example, in poisoning attacks, a trigger is inserted into each manipulateddata sample, altering its label to an incorrect one (Han et al., 2022). The goal of the adversary is to injectcertain harmful properties into the model learned using this data. While a poisoning adversary injects thebackdoor trigger with malicious intent, another adversary may cause systematic mislabelings, resulting in abiased model. We aim to address both types of adversaries. Developers Perspective: Model developers identify some of the compromised data sources after having alreadytrained a model, either through internal monitoring, defenses, or external information like tip-os. Since theadversary can apply arbitrary manipulations, the exact manipulation type is unknown to the model developerapriori. As such, the corrupted data characterizes the manipulation performed by the adversary. Other waysof characterization, which we are currently out-of-scope for this work include using feedback from model userssuch as incorrect predictions after deployment or threats identied via red-teaming. While detecting all manipulated data is challenging, removing its eect given a small subset may be feasible,if the subset is representative of the broader manipulated set. This often occurs in practice when modeldevelopers perform expensive but rigorous monitoring on a uniformly sub-sampled subset of the larger dataset.Since this subset is uniformly subsampled, the identied corrupt samples are also a uniformly subsampledsubset of the entire set of corrupted samples. The goal of model developers is to remove the adverse eects ofthe manipulated data from the original model using this small identied representative subset. We formalizethis in the next subsection.",
  "Objective of Corrective Unlearning": "Let X be the data domain, Y the label space, and P the distribution on X Y. Let Str X Y be thetraining data, and Sm Str the training samples manipulated by the adversary, either by modifying features,associated labels, or both. Let Dm X be the domain where performance is adversely aected when learningusing Sm. For example, in poisoning, Dm contains samples with the poison trigger. In Interclass Confusion,Dm consists of samples from the two aected classes. Clearly, Dm also contains Sm. Finally, let A be thelearning algorithm, and Mo = A(Str) be the trained model. A corrective unlearning algorithm Ucorr corrects the original model Mo by removing the inuence of Sm.As mentioned before, we expect only a subset of samples to be identied as manipulated, which we denote",
  "Accretain (Ucorr, ) = E(x,y)P [I{Mu(x) = y} | x / Dm.](2)": "Intuitively, the larger the value of , the higher Acccorr should be. When the whole manipulated set is knownfor deletion i.e. = 1, retraining without it, gives the best outcome. We dub this Ucorr = A(Str \\ Sm). Theperformance of an unlearning algorithm Ucorr can be measured by how large needs to be for Acccorr (Ucorr, )to be within some constant fraction (say 0.9) of Acccorr (Ucorr, 1). We visualize this in our experimentsin .3 and compare dierent algorithms based on it. Note that, Accretain is relatively less sensitiveto due to the denition of Dm. However, it may not always be possible to accurately identify the entireaected domain Dm. Identifying Dm can be particularly dicult when the adversary wants to obscure itstrue target, as in targeted poisoning attacks (Shafahi et al., 2018), or when it wants to aect the accuracyover the entire domain, such as in indiscriminate poisoning attacks (Barreno et al., 2006a; Biggio et al., 2012).Therefore, formulating these objectives for dierent settings may require more problem-specic attention.Nevertheless, we set the gold standard for Accretain to also be Accretain (Ucorr, 1), and in our experiments,this metric turns out to be largely independent of .",
  "Dierences from Privacy-Oriented Unlearning": "In this section, we discuss the most important distinctions between Corrective Unlearning and Privacy-orientedunlearning. Privacy-oriented unlearning seeks to ensure retrain indistinguishability (Sekhari et al., 2021):the unlearning algorithm Ucorr aims to produce models indistinguishabile from the models produced by thelearning algorithm trained on just the retain set Str \\ Sf. Dierent Goals (Removal of Incorrect Training Data):The goal of privacy-oriented unlearning isto remove the inuence of untampered but sensitive user data whereas the objective of corrective unlearningis to remove the inuence of manipulated samples. Implications: Removing uncorrupted but private samples in privacy-oriented unlearning scenarios typicallydegrades model performance (Golatkar et al., 2020a). This is unavoidable as there is a cost to obtainprivacy (Jayaraman & Evans, 2019). Some unlearning procedures even nudge the model output towardsa uniform distribution over classes for samples in the forget set (Chundawat et al., 2023b; Li & Ghosh,2023). However, in corrective unlearning, removing the inuence of manipulated samples is expected toimprove the models performance on the aected domain Dm (i.e. Acccorr). Intuitively, this may, in turn,improve the learned representations thereby also improving overall accuracy Accretain. The underlying causeis that corrective unlearning aims to unlearn corrupt data whereas the focus of privacy-oriented unlearning isremoving condential but not necessarily corrupt data. Dierent Gold-Standards (Retraining without deletion set is not enough):In privacy-orientedunlearning, the data whose inuence is to be removed from the model is specied by user deletion requests.However, as discussed before, when model developers need to identify compromised data, it is unrealistic to",
  "assume all of it will be found. Thus, in corrective unlearning at < 1, the retain set Str \\ Sf will continue tocontain manipulated data from Sm \\ Sf": "Implications: Retraining from scratch on Str \\ Sf is the gold standard for privacy-oriented unlearning but itis computationally expensive. Therefore, at its core, privacy-oriented unlearning is a computational problem.However, in corrective unlearning when < 1, as Str \\ Sf continues to have manipulated data, unlearningprocedures that solely rely on it (Schelter, 2020; Bourtoule et al., 2021; Graves et al., 2021; Goel et al.,2023) perpetuate the adverse eects of the manipulation. This necessitates a search for algorithms beyondcomputationally ecient approximations of retraining from scratch, which ceases to be a gold standard. Interestingly, this introduces a statistical dimension to the unlearning problem. One possible approach is touse the identied manipulated data to detect other manipulated data. Whether this is possible depends mainlyon three parameters: size of the identied manipulated set, dimensionality of the data, and complexity ofdetecting the manipulated data1. For easy-to-detect manipulations in low dimensions and for large identiedsets, this problem should be easy. However, existing lower bounds in learning theory (e.g. see Theorem 3.20in Schapire & Freund (2012)) show that this may well be impossible for small identied sets and hard-to-detectmanipulations in high dimensions, which are precisely the realistic conditions we aim to tackle. For example,we show poisoning experiments where the identied subset has 500 samples for 3072-dimensional data andone of the goals of poisons is to be imperceptible. In short, this naturally leads to the following question: Howcan we eciently remove the detrimental impacts of Sm using a representative, albeit smaller, subset Sf?",
  "Dierent Constraints (No Privacy Requirements):Finally, in the corrective unlearning context, Sfand Sm does not need to be privatized, setting it apart from privacy-oriented unlearning": "Implications: Privacy-oriented unlearning is designed to meet strict privacy standards, necessitating eitheralgorithms with theoretical privacy guarantees (Sekhari et al., 2021; Gupta et al., 2021) akin to those providedby dierential privacy, or at least strong performance against privacy auditing on the data to be forgottenSf (Golatkar et al., 2020a). This further leads to a drop in accuracy as theoretical privacy guarantees are notalways tight and lead to underestimation of actual privacy. Such evaluations are orthogonal to the goal ofCorrective Unlearning, thereby necessitating the design of new evaluation strategies.",
  "As a desiderata for corrective unlearning is dealing with corrupted data, agnostic to manipulation type, agood Ucorr should correct a broad class of manipulations, including both we test": "Evaluation of Feature-and-Label Manipulations: Poisoning. When model developers scrape datafrom webpages, adversaries can manipulate both the data samples and labels. This leaves models trained onthis data vulnerable to backdoor attacks, where the model misbehaves in the presence of trigger featuresknown to the adversary, but unknown to the model developers. Prior work has shown this can be realized inreal-world settings such as autonomous driving (Han et al., 2022) and models trained on Wikipedia (Carliniet al., 2023). To model this setting, we use the simple BadNet poisoning attack introduced by Gu et al.(2019). We insert a trigger pattern that makes 0.3% pixels white at bottom-right positions in a subset of ntraining images, re-labeling each of these images to class zero. Here the aected domain Dm consists of allsamples containing the trigger pattern. We benchmark the ability of unlearning methods to remove the eectof the backdoor after identifying some of the poisoned training data. Evaluation of Label-only Manipulations: Interclass Confusion. When model developers outsourceannotations for their training data, adversaries can only manipulate labels. Systematic mislabeling between",
  "Experimental details": "We rst use the CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) datasets as standard benchmarks in theunlearning literature (Foster et al., 2023; Kurmanji et al., 2023; Chundawat et al., 2023a). We then reportpoison unlearning results on PCam (Veeling et al., 2018), a binary classication medical imaging dataset, asa potential application. PCam contains histopathologic scans of lymph node sections, with labels indicatingthe presence of metastatic tissue. For our experiments, we employ the ResNet-9 (Idelbayev, 2018) model forCIFAR10, and the WideResNet-28x10 (Zagoruyko & Komodakis, 2016) model for CIFAR100 and PCam. In the main paper, we report results for 1% of the training data being manipulated, except for the ICtest (which has a weaker adversary than poisoning) on CIFAR-10 where 10% manipulation is required forclear observations. In Appendix B we report results for other manipulation sizes, nding similar unlearningtrends even when only 0.2% of the training data is poisoned. In each setting, we vary , the fraction ofthe manipulated set identied for deletion, from 0.1 to 1.0 at intervals of 0.1. For all results, metrics arecomputed on the test set containing unseen samples. The mean and standard deviation are reported over3 seeds. In the interclass confusion evaluation, for CIFAR10, we confuse the Cat and Dog classes, and forCIFAR100, the maple and oak tree classes, to be consistent with the setup of Goel et al. (2023). Unlearning Methods. We select several state-of-the-art unlearning methods to benchmark the eectivenessof current unlearning methods in our setting. Detailed descriptions, along with hyperparameter sweep detailsfor all methods, are provided in Appendix A.2. (1) Retrain without Deletion set (RewoD): It retrains from scratch on Str \\ Sf using the original trainingalgorithm A. It is considered an inecient but gold-standard oracle in the traditional setting of = 1. Manyexisting methods are relaxations of this algorithm (He et al., 2021; Graves et al., 2021; Goel et al., 2023). (2) Catastrophic Forgetting (CF): Goel et al. (2023) show that ne-tuning just the nal layers of a deepmodel on Str \\ Sf can unlearn label manipulations. We use the strongest version, i.e., ne-tuning all layers. (3) Selective Synaptic Dampening (SSD): Foster et al. (2023) selectively dampen learned weights with highinuence from Sf, identied by approximating the Fisher Information Matrix (Martens, 2020). Specically,they compute each parameters relative importance: the ratio of the forget set and retain set loss sensitivityto the parameter. Parameters with relative importance above a chosen threshold have their value divided bya quantity proportional to the relative importance. (4) Knowledge Distillation from a Bad Teacher (BadT): Chundawat et al. (2023b) propose making outputson Sf random by distilling from a randomly initialized network, while distilling the original model on Str \\ Sf. (5) Scalable Remembering and Unlearning Bound (Scrub): Kurmanji et al. (2023) propose a method thatalternates between two key steps: (1) distillation away from the original model on Sf to remove the inuence ofthe manipulated data, and (2) knowledge preservation using distillation towards the original model combinedwith a task-specic loss on Str \\ Sf to retain useful information. How to Select Best Hyperparameters for Unlearning? We nd choosing the hyper-parameters asurprisingly tricky question for corrective unlearning. Selecting the model with the best overall accuracyon a validation set (Accretain) may result in low corrected accuracy (Acccorr) on the domain aected by themanipulation (Dm), especially if it is a small fraction of the overall domain X. Moreover, directly measuringthe corrective accuracy may not be possible when the correct labels for the deletion set are not known. We",
  "BadT0.23 0.10-0.04 0.02CF0.79 0.140.31 0.07SSD-1.63 0.95-5.43 3.62Scrub-3.27 1.65-0.06 0.05RewoD0.44 0.15-0.01 0.07": ": Change in retain ac-curacy (Accretain) after ap-plying dierent unlearningmethods, where higher val-ues are better. Results are re-ported as mean stdev over 10values of the identied fractions from 0 to 1.0. SSD leads tosignicant drops in model utility. propose to use a weighted average between overall accuracy (utility) on a validation set and the fraction ofSf samples where the models classication changes (unlearning). We weigh them equally for our results.",
  "Experimental Results": "shows corrective accuracy trends that quantify the ecacy of dierent methods at unlearning theadverse eects of manipulations. RewoD is the gold standard when all manipulated samples are known,and indeed it shows the highest accuracy when |Sf| = |Sm|. For the IC evaluation, RewoD demonstratesconsistent improvement as more deletion set samples are identied. However, for the poisoning evaluation, itshows no improvements when developers detect less than 80% of the manipulated samples, even where only1% (500 samples) of the training data is manipulated. This highlights the insuciency of the privacy-basedunlearning goal of approximating retraining from scratch on Str \\ Sf, as the remaining poisoned samples canmaintain their adverse eects, even when their number is small (Gu et al., 2019). As a consequence, popularapproaches in unlearning literature like Scrub and CF do not perform well in the poisoning setting. Scrubperforms better than CF in the poisoning setting, but the opposite is true in the interclass confusion setting.BadT shows poor results in both settings, as randomizing outputs on Sf conicts with the goal of correctingthe model on the aected domain. On the contrary, SSD recovers Acccorr for poisoning even after identifying just 10% of the manipulatedsamples, demonstrating manipulations can be unlearnt with a small fraction of manipulated samples identied.However, SSD completely fails for the IC test, providing no improvements over the original model. Further, asshown in , SSD leads to signicant drops in model utility (Accretain), while other unlearning methodsmaintain utility. Note that we report averaged Accretain across as we nd it to largely be independent of .",
  "Identified Fraction": ": Corrective Accuracy (Acccorr)for unlearning poisons across dierentmethods on PCam. Unlearning methodsthat move away from the original label, suchas Scrub, perform well even when less ma-nipulated data is detected, as in this casethere is only one other possible label. SSD selects which parameters to dampen using the ratio ofthe parameters importance for the forget set versus the retainset. We hypothesise that for poisons, a few parameters aredisproportionately more important for the forget set than others,whereas for the IC test, the eect is more distributed among theparameters and no small subset of parameters is particularlyimportant. If this is the case, an approach that dampens high-inuence parameters is likely to target a relevant parameterin the poisoning setting, eectively reducing its adverse eects,but fail to remove dierentially interclass confusion withouthurting retain accuracy. To test this hypothesis, we show thedistribution of the importance ratio (used to select the weightsto dampen) for the two tests in . For poisoning, weobserve a few outlier weights with disproportionately high values(e.g., 800), whereas for the IC test, the distribution does notshow such extreme values (the maximum is less than 200). Thisempirical observation conrms our hypothesis. SSD succeeds at removing poisons because it can dampenweights which correlate the poison feature with the wronglabel, even with a small portion of the manipulation set known.This is consistent with the well-known strategy of pruning asmall subset of weights to mitigate poisons (Wang et al., 2019),as they correlate a specic feature with the mislabel. Thismay motivate the tractability of mechanistic interpretability-based approaches for unlearning certain types of poisoned data(Elhage et al., 2021).",
  "Application on a Medical Dataset:PCam - Binary Classication": "We now examine if our ndings generalize to PCam (Veelinget al., 2018), an application-specic dataset used in medicalimaging. The only change we expect is that in binary classi-cation tasks, knowing which class is incorrect fully species thecorrect class. Therefore, methods like Scrub which do a formof gradient ascent, moving away from the original manipulatedlabel, are expected to perform well. In , we observethat RewoD, CF, and BadT continue to perform poorly untilmost of the manipulated data is detected, while SSD performswell even at small . As anticipated, the only dierence is thatScrub shows signicant improvements as moving away from theoriginal models outputs on the forget set Sf infers the correct labels.",
  "Acknowledgements": "SG was funded by an Eective Ventures Foundation (UK) grant as part of the ML Alignment Theory Scholars(MATS) program. AP was funded by Meta AI Grant No. DFR05540. PT thanks the Royal Academy ofEngineering and FiveAI for their support. This work is supported in part by a UKRI grant: Turing AIFellowship EP/W002981/1 and an EPSRC/MURI grant: EP/N019474/1. The authors would like to thank (inalphabetical order): Arvindh Arun, Shyamgopal Karthik, Shashwat Singh, Shiven Sinha, Vishaal Udandarao,Christian Schroeder de Witt for helpful feedback, and Varshita Kolipaka for contributing illustrations.",
  "California State Leglisature. California consumer privacy act. 2018": "Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, HyrumAnderson, Andreas Terzis, Kurt Thomas, and Florian Tramr. Poisoning web-scale training datasets ispractical. arXiv preprint arXiv:2303.02444, 2023. Huili Chen, Cheng Fu, Jishen Zhao, and Farinaz Koushanfar. Deepinspect: A black-box trojan detectionand mitigation framework for deep neural networks. In International Joint Conference on ArticialIntelligence (IJCAI), 2019. Ruizhe Chen, Jianfei Yang, Huimin Xiong, Jianhong Bai, Tianxiang Hu, Jin Hao, Yang Feng, Joey TianyiZhou, Jian Wu, and Zuozhu Liu. Fast model debias with machine unlearning. Advances in NeuralInformation Processing Systems (NeurIPS), 2024a. Ziheng Chen, Jia Wang, Jun Zhuang, Abbavaram Gowtham Reddy, Fabrizio Silvestri, Jin Huang, KaushikiNag, Kun Kuang, Xin Ning, and Gabriele Tolomei. Debiasing machine unlearning with counterfactualexamples. arXiv preprint arXiv:2404.15760, 2024b.",
  "Robert M. French. Catastrophic forgetting in connectionist networks. In Trends in Cognitive Sciences, 1999": "Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts fromdiusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),pp. 24262436, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, HoraceHe, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.arXiv, 2020.",
  "Antonio Ginart, Melody Y. Guan, Gregory Valiant, and James Zou. Making AI forget you: Data deletion inmachine learning. In Advances in Neural Information Processing Systems (NeurIPS), 2019": "Shashwat Goel, Ameya Prabhu, Amartya Sanyal, Ser-Nam Lim, Philip Torr, and Ponnurangam Kumaraguru.Towards adversarial evaluations for inexact machine unlearning. arXiv preprint arXiv:2306.10011, 2023. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selectiveforgetting in deep networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2020a. Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deepnetworks of information accessible from input-output observations. In European Conference on ComputerVision (ECCV), 2020b. Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, AleksanderMadry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoorattacks, and defenses. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022.",
  "Junde Li and Swaroop Ghosh.Random relabeling for ecient machine unlearning.arXiv preprintarXiv:2303.01234, 2023": "Vijay Lingam, Mohammad Sadegh Akhondzadeh, and Aleksandar Bojchevski. Rethinking label poisoning forGNNs: Pitfalls and attacks. In International Conference on Learning Representations (ICLR), 2024. URL Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. Backdoor defensewith machine unlearning. In IEEE INFOCOM 2022 - IEEE Conference on Computer Communications (IN-FOCOM), pp. 280289. IEEE, 2022.",
  "Sebastian Schelter. \"amnesia\" - machine learning models that can forget user data very fast. In Conferenceon Innovative Data Systems Research (CIDR), 2020": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. In Advances in Neural Information ProcessingSystems (NeurIPS), 2022. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what youwant to forget: Algorithms for machine unlearning.In Advances in Neural Information ProcessingSystems (NeurIPS), 2021. Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and TomGoldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in NeuralInformation Processing Systems (NeurIPS), 2018.",
  "Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarialexample defenses. Advances in Neural Information Processing Systems (NeurIPS), 2020": "Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariantcnns for digital pathology. In Medical Image Computing and Computer-Assisted Intervention (MICCAI).Springer, 2018. Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on nlp mod-els. In North American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies (NAACL-HLT), 2021.",
  "A.1Training Details": "Our standard training procedure A is as follows: We train our models for 4000 steps on CIFAR10, PCAMand 6000 steps on CIFAR100. Each step consists of training on a single batch, and we use a batch size of 512throughout. We use an SGD optimizer with momentum 0.9 and weight decay 5e-4, a linear scheduler withtmult = 1.25, and warmup steps as1 100 of the total training steps. The same hyperparameters are used duringunlearning unless otherwise specied. The setup used for all experiments is a PC with a Intel(R) Xeon(R)E5-2640 2.40 GHz CPU, 128GB RAM and 1 GeForce RTX 2080 GPU.",
  "To benchmark the performance of existing unlearning proposals on corrective unlearning scenarios, weselect the strongest unlearning methods across ve popular paradigms:": "(1) Retrain without Deletion set (RewoD): This paradigm involves retraining parts of the ML system(Bourtoule et al., 2021; Goel et al., 2023; He et al., 2021) that are inuenced by Sf from scratch using Str \\Sf. Method Used: We benchmark the strongest version, retraining the entire model from scratch on Str \\ Sfusing the original training algorithm A. This is considered an inecient gold standard in prior work. (2) Catastrophic Forgetting (CF) : Neural Networks suer from catastrophic-forgetting (French, 1999) -when a model is updated without some previously learnt samples, the model loses knowledge about them.Many unlearning methods perform netuning on Str \\ Sf to achieve unlearning of Sf via catastrophicforgetting, and Goel et al. (2023) showed a weaker ecient version performs well on the IC test.",
  "Method Used: We netune using the original training procedure A for 1000 steps on Str \\ Sf": "(3) Modifying learnt parameters with high inuence from Sf: This is a training-free class ofmethods (Golatkar et al., 2020a;b; Peste et al., 2021; Chundawat et al., 2023a) that identies parameterswith information relevant to the forget set using statistics like the Fisher Information Matrix (FIM). Itthen damages these parameters by adding noise or reducing their magnitude hoping to selectively removeinformation about Sf. Method Used: We benchmark the recently proposed Selective Synaptic Dampening (SSD) method which hasshown state of the art results in this paradigm (Foster et al., 2023). We extensively tune the weight selectionthreshold and weight dampening constant . We nd when should be tuned relative to for optimal results.For each datapoint, we pick the best result out of runs with = [0.1, 1, 10, 50, 100, 500, 1000, 1e4, 1e5, 1e6], = [0.1, 0.5, , 5, 10].",
  "(4) Pushing Sf outputs towards random: Some unlearning procedures (Graves et al., 2021; Li & Ghosh,2023; Chundawat et al., 2023b) push the model towards random outputs on the deletion set": "Method Used: We benchmark Knowledge Distillation from Bad Teacher (BadT) (Chundawat et al., 2023b),a state of the art method in this paradigm, which simultaneously distills from a randomly initialized neuralnetwork on Sf, and the original model on the remaining data Str \\ Sf. We netune the original model usingthis procedure for 1000 unlearning steps.",
  "(5) Alternating between Forgetting and Preservation Steps:": "Method Used: Kurmanji et al. (2023) propose SCRUB and show it performs well on unlearning mislabelledsamples when all are identied. The method alternates between forget steps and knowledge preservationsteps. The forget step involves doing gradient ascent using the task-loss for Sf. The knowledge preservationstep does knowledge distillation from Mo using Str \\ Sf as well as optimizing the task-loss on Str \\ Sf. Wenetune the original model using this procedure for 1000 unlearning steps, out of which the forget step isused only in the rst 200 unlearning steps as it is recommended in the paper to run it only in the initial",
  ": Dataset, models and manipulation sizes for the Poisoning and Interclass Confusion (IC) evaluation": "iterations. We use a smaller learning rate (0.0025) as the original value leads to stability issues. We tunethe hyperparameter which controls the trade-o between the distillation loss and the task-loss. For eachdatapoint, we pick the best result out of runs with = [0.001, 0.01, 0.05, 0.1, 0.5, 1, 5, 10].",
  "BFurther Results": "We now provide results not included in the main paper due to space considerations. Specically: (1) Wereport results across dierent manipulation set sizes (settings summarized in ), on both unseen samplesfrom the aected domain Dm (generalization eect of manipulation) and the manipulation set used in trainingSm (memorization of manipulation). We do this for both the poisoning and IC evaluation. (2) We reportcomputational eciency by measuring unlearning time for each method. (3) We report a hyperparametersensitivity analysis for all methods.",
  "B.1Unlearning (Acccorr) results across Manipulation Sizes, and on the Manipulated Set Sm": "SetupIn this section, we vary the manipulation sizes for both poisoning and interclass confusion, to see ifthe trends are consistent. As the adversary is less powerful (label-only) in IC, we expect IC will need largermanipulation set sizes to show clear trends, while poisoning will work with very small manipulation sets. Dueto the changing manipulation sizes, we report the deletion set size on the X-axis, with the total manipulationset size labeled below each subgure, and also noticeable as the right-most point on the X-axis. In the mainpaper, we reported unlearning results (Acccorr) on unseen samples from the aect domain Dm, but here wewill also investigate the same metric computed on the manipulated samples used in training itself. We thususe the more general term of clean-label accuracy, i.e. accuracy computed with respect to correct labels,on the Y-axis, which is the same as Acccorr for results on unseen samples. Unlearning PoisoningIn a, we report results on a smaller (0.2%) and bigger (2%) manipulationsize than the main paper (1%, also reported here for reference). In both cases, and notably even for thesmaller manipulation set, we nd consistent results. To measure the removal of mislabelling on poisonedtraining samples, we report clean-label accuracy on Sm in b. The trends across unlearning methodsare similar to the ones on unseen samples from the aected domain Dm, though the absolute accuracies afterunlearning are higher as expected from training samples in comparison to test set samples. The success ofSSD in producing an unlearnt model that successfully classies the manipulated training data shows how anideal unlearning algorithm can help re-label the detected data correctly. Unlearning Interclass ConfusionIn a, we report results on smaller manipulation sizes(0.2%, 0.5% for CIFAR100, and 2%, 5% for CIFAR10) than the main paper (same sizes, i.e. 1% on CI-FAR100 and 10% on CIFAR10, also reported here for reference). We see similar trends but with weakerdelity on the medium setting, but no clear observations on unseen samples in the smallest manipulation size.While the smallest manipulation size (subgures a, d) for Interclass Confusion did not show signicant eectson unseen samples from class A, B, b shows unlearning methods continue to give wrong predictionson the class A, B samples used for training. This emphasises the need to check unlearnt model outputs onunseen and training samples from the aected domain Dm separately, especially when the manipulation set issmall to have an eect on model behaviour for unseen samples.",
  "B.2Computational Eciency": "In we report average unlearning times of dierent unlearning methods. In the case of RewoD andCF, while more ecient relaxations have been proposed (Goel et al., 2023; He et al., 2021; Graves et al.,2021), we retrain from scratch to perform the strongest unlearning, which we still nd to be insucient.",
  "CIFAR-100CIFAR-10": "(a) Clean-label Accuracy on Test Samples on the Two Confused Classes. We compute clean-label accuracyon the classes A, B used for the Interclass Confusion test, across deletion sizes |Sf|. SSD provides no improvementsover the original model (represented as None), and other unlearning methods also require a large fraction of themanipulated data to be identied for unlearning. In the lower manipulation size setting (a) and (d), the model outputson unseen samples are not aected much, so we show unlearning trends on manipulated train samples below.",
  "B.3Hyperparameter Sensitivity": "How sensitive are our claims to hyperparameters? For CF, RewoD and BadT, there are no crucial hyperpa-rameters apart from the ones used in the original training procedure, for which we continue to use the sameones as we found them to work quite well. To check hyperparameter sensitivity for Scrub, SSD, we analyzetwo kinds of plots on the CIFAR10 dataset:",
  "B.3.1Unlearning across deletion fraction": "Scrub: shows Scrub leads to a gradual improvement in unlearning as increases, across hyper-parameter values. shows that only at high values of , for specic hyperparameter values (highsensitivity), Scrub leads to good unlearning. SSD: shows that SSD is never able to outperform the original models Acccorr in the IC evaluation. shows that for specic hyperparameters, SSD performs good unlearning at all values of . Theunlearning ability of SSD is highly sensitive to hyperparameters.",
  "B.3.2Unlearning-Utility Tradeo": "Figures 11, 12 show that Scrub is actually not too sensitive at hyperparameters, and the right values ofhyperparameters (for the ones tuned) do not sacrice unlearning for utility. On the other hand, Figures 13, 14show that SSD is quite sensitive to hyperparameters. While there is a positive correlation between unlearningand utility, surprisingly, at the highest levels of utility only a few hyperparameter sets lead to high unlearning."
}