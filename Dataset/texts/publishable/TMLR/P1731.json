{
  "Abstract": "Existing test-time adaptation (TTA) approaches often adapt models with the unlabeledtesting data stream. A recent attempt relaxed the assumption by introducing limited hu-man annotation, referred to as Human-In-the-Loop Test-Time Adaptation (HILTTA) in thisstudy. The focus of existing HILTTA studies lies in selecting the most informative samplesto label, a.k.a. active learning. In this work, we are motivated by a pitfall of TTA, i.e.sensitivity to hyper-parameters, and propose to approach HILTTA by synergizing activelearning and model selection. Specifically, we first select samples for human annotation (ac-tive learning) and then use the labeled data to select optimal hyper-parameters (modelselection). To prevent the model selection process from overfitting to local distributions,multiple regularization techniques are employed to complement the validation objective. Asample selection strategy is further tailored by considering the balance between active learn-ing and model selection purposes. We demonstrate on 5 TTA datasets that the proposedHILTTA approach is compatible with off-the-shelf TTA methods and such combinationssubstantially outperform the state-of-the-art HILTTA methods. Importantly, our proposedmethod can always prevent choosing the worst hyper-parameters on all off-the-shelf TTAmethods. The source code is available at",
  "Introduction": "Deep learning has demonstrated remarkable success in numerous application scenarios. Nevertheless, thedisparity in distribution between training and testing data leads to poor generalization of deep neural net-works. In many real-world scenarios, the data distribution of testing data is often unknown until the inferencestage (Sun et al., 2020; Wang et al., 2020) and the distribution may experience constant shift over the courseof inference (Wang et al., 2022; Yuan et al., 2023; Press et al., 2023; Lee & Chang, 2024; Su et al., 2024a).These practical challenges render traditional unsupervised domain adaptation and source-free domain adap-tation paradigms ineffective. To overcome the above challenges, test-time adaptation (TTA) emerges byadapting model weights upon observing testing data at inference stage (Wang et al., 2020; Sun et al., 2020;Wang et al., 2022; Su et al., 2022; 2024a). The success of TTA is often attributed to the practice of aligningtesting data distribution with source domain distribution (Su et al., 2022; Kang et al., 2023; Wang et al.,2023a) and/or self-training on pseudo labels (Jang et al., 2023; Su et al., 2024b; Marsden et al., 2024a).Despite achieving impressive results on a wide range of tasks, e.g. classification (Sun et al., 2020), imagesegmentation (Wang et al., 2023b) and object detection (Chen et al., 2023), the fully unsupervised TTAparadigm is still overwhelmed by remaining challenges including hyper-parameter tuning (Zhao et al., 2023),continual distribution shift (Wang et al., 2022), non-i.i.d. (Gong et al., 2022; Niu et al., 2023; Yuan et al.,",
  "Active Learning": "Active learning (AL) aims to select the most informative samples for Oracle to label for supervised training.Prevailing approaches often prioritize uncertainty (Gal & Kendall, 2017), feature diversity (Sener & Savarese,2018) or the combination of both (Ash et al., 2020). The traditional AL approaches often adopt a batch-wise paradigm. When data arrive in a sequential manner stream-based AL (Saran et al., 2023) proposed toquantify the increment of the determinant as the criteria for incremental sample selection. Active learningwas integrated into test-time adaptation to tackle the forgetting issue (Gui et al., 2024). In contrast totackling HILTTA from a pure active learning perspective, we propose to synergize active learning and modelselection. Selecting samples for model selection often relies on an orthogonal objective to active learning. Inthe related works, active testing (Kossen et al., 2021; 2022; Matsuura & Hara, 2023; Ochiai et al., 2023) wasdeveloped to select a subset of testing data that is representative of the whole testing data. Active testing hasbeen demonstrated to help label-efficient model validation. Nevertheless, active learning and active testingwill prioritize supervised training and model validation respectively. We believe either is not enough for thepurpose of synergizing active learning and model selection for test-time adaptation.",
  "Human-in-the-Loop TTA(HILTTA)": "Despite the theoretical guarantee and superior empiri-cal observations, the underlying practice of utilizing la-bel budget by existing active HILTTA methods followsthe traditional line of research on active learning (Sener& Savarese, 2018; Saran et al., 2023). In this study, weaim to explore the question: How can human-in-the-loop test-time adaptation be realized beyond anactive learning perspective?To answer this ques-tion, we first revisit the pitfalls of state-of-the-art test-time adaptation (Zhao et al., 2023). Among the multipleremaining pitfalls, we pinpoint hyper-parameter selectionas the most nasty one due to the sensitivity to the choice of hyper-parameters by many TTA methods.Hyper-parameter selection, also referred to as model selection, aims to choose the optimal hyper-parametersfor model training. Established paradigms towards model selection often rely on a held-out validation setand the model selection task is defined as discovering the hyper-parameters trained upon which the model",
  "Published in Transactions on Machine Learning Research (12/2024)": "EATA (Niu et al., 2022). It adapts batch normalization layers using entropy minimization but intro-duces an additional fisher regularization term to prevent drastic parameter changes. We follow the officialimplementation3 of EATA to update BN parameters. We hold 2000 fisher training samples and keep theentropy threshold E0 = 0.4 log(#class). SAR (Niu et al., 2023).It updates batch normalization layers with a sharpness-aware optimizationapproach. We mainly follow its official code4, but set e0 = 0 for CIFAR10-C and CIFAR100-C, preventingfailure cases of frequently resetting the models weight.",
  "To the best of our knowledge, we are the first to approach Human-In-the-Loop Test-Time Adaptation(HILTTA) by synergizing active learning and model selection and propose an off-the-shelf HILTTAmethod": "To prevent the model selection from overfitting to a small validation set and local distribution, weintroduce multiple regularization techniques. We further identify an effective selection strategy forHILTTA by considering the feature diversity and model uncertainty. We demonstrate that the proposed method is compatible with state-of-the-art TTA methods. Thecombined methods outperform dedicated active TTA methods and stream-based active learningmethods on five test-time adaptation datasets.",
  "Test-Time Adaptation": "The generalization of deep learning model is hindered by the distribution gap between source and targetdomain. Traditional approaches towards improving the generalization of deep learning models follow un-supervised domain adaptation (UDA) (Ganin & Lempitsky, 2015) which adapts model weights to learnindiscriminative features across source and target domains. Simultaneously training on both source andtarget domain data is infeasible in many real-world scenarios where access to source training data is pro-hibited due to privacy or storage overhead. Source-free domain adaptation (SFDA) targets this issue andadapts model weights to target domain data only (Liang et al., 2020; Liu et al., 2021; Yang et al., 2021;Liang et al., 2021). In contrast to the assumptions made in UDA and SFDA, test-time adaptation (TTA)emerges in response to the need for adapting pre-trained model to unknown target domain at inferencestage (Su et al., 2022; Sun et al., 2020; Wang et al., 2020). TTA has been demonstrated to improve modelsgeneralization to out-of-distribution testing data and the success owns to self-training on unlabeled testing",
  "Human-in-the-Loop Test-Time Adaptation Protocol": "We first provide an overview of the existing test-time adaptation (TTA) evaluation protocol. TTA aimsto adapt a pre-trained model h(x; ) to a target testing data stream D = {xi, yi}i=1Nt where xi andyi {1 C} denote the testing sample and the associated unknown label, respectively.Existing test-time adaptation approaches often make an instant prediction y = h(x; ) on each testing sample and thenupdate model weights upon observing a cumulative batch of samples Bt = {xi}i=1Nb where t refers tothe t-th minibatch. Prevailing methods enable adaptation by self-training (Wang et al., 2020; Su et al.,2024b) or distribution alignment (Su et al., 2022). Unlike the existing assumption of totally unsupervisedadaptation, additional human annotation Blt = {xj, yj}j=1Nlb within each batch could be introduced (Guiet al., 2024) and TTA is implemented on the combine of large unlabeled data But = Bt \\ Blt and sparselylabeled data Blt. We refer to the novel evaluation protocol as Human-in-the-Loop Test-Time Adaptation(HILTTA) throughout the study.",
  "Model Selection with Sparse Annotation": "We present the details for model selection with sparse human annotation on the testing data stream. Thechoice of hyper-parameters is known to have a significant impact on the performance of model trainingand generalization. W.l.o.g., we define the task of model selection by introducing a fixed candidate poolof hyper-parameters = {m}m=1Nm with Nm options. The choice of hyper-parameters hinges on thesensitivity of respective TTA methods, e.g. pseudo label threshold, learning rate, and loss coefficient arecommonly chosen hyper-parameters for selection. We further denote the discriminative model trained upon",
  "xiDLtr(h(xi; ), )(1)": "We interpret the above bi-level optimization problem as discovering the optimal hyper-parameter trainedupon which the model weights achieves the best performance on the labeled dataset. Solving the aboveproblem through gradient-based method (Liu et al., 2018) is infeasible for TTA tasks where adaptation speedis a major concern while gradient-based methods require iteratively updating between meta gradient descentLval and task gradient descent Ltr. Considering the task gradient descent only takes a few steps ina typical TTA setting, we opt for an exhaustive search for the hyper-parameters in the discretized hyper-parameter space. In specific, we enumerate the candidate models adapted with different hyper-parametersas {m}m=1||. The candidate with the best validation loss is chosen as the best model , as follows.",
  "Design of Validation Objective": "In this section, we take into consideration two principles for the construction of validation objectives. First,the validation loss should mimic the behavior of the model on the testing data stream. Assuming the limitedlabeled data takes a snapshot of the whole testing data stream and classification is the task to be addressed,an intuitive choice is the cross-entropy loss as follows.",
  "c=11(yi = c) log hc(xi; m)(3)": "Alternative to the continuous cross-entropy loss, accuracy also characterizes the performance on the labeledvalidation set. However, the discretized nature of accuracy renders this option less suitable due to many tiedoptimal hyper-parameters. Regularization for Stable Model Selection: The stability of model selection heavily depends on theselection of the validation set and the variation of target domain distribution. Under a more realistic TTAscenario, the target domain distribution could shift over time, e.g. the continual test-time adaptation (Wanget al., 2022). Such a realistic challenge may render cross-entropy loss less effective due to over-adapting tolocal distribution. To stabilize the model selection procedure, we propose the following two measures. Anchor Deviation for Regularizing Model Selection: We first introduce an anchor deviation toregularize the model selection procedure. This regularization is similar to the anchor network proposed inSu et al. (2024a) except that we use the anchor deviation to regularize model selection which is orthogonalto the purpose in Su et al. (2024a). Specifically, we keep a frozen source domain model, denoted as 0. Theanchor loss is defined as the L2 distance between the posteriors of the frozen source model and the m-thcandidate model m. A big deviation from the source model prediction suggests the model has been adaptedto a very specific target domain and further adapting towards one direction would increase the risk of failingto adapt to the continually changing distribution.",
  "(xi,yi)Dlh(xi; 0) h(xi; m)(4)": "Smoothed Scoring for Model Selection: To determine the optimal model to select, we combine thecross-entropy loss and the anchor deviation as the final score. Direct summing the two metrics as the modelselection score is sub-optimal due to the relative scale between the two metrics.Therefore, we proposeto first normalize the two metrics as Norm(x) =xmin(x) max(x)min(x). After the normalization, the relative sizerelationships of different candidate model scores are preserved, and a comparable validation loss can beconveniently obtained by addition, as below.",
  "s.t.m = 1, . . . , ||,Sce(xi; m) = Norm(Hce(xi; m)),Sanc(xi; m) = Norm(Ranc(xi; m)),(5)": "Considering that the testing data stream is often highly correlated, i.e. temporally adjacent samples arelikely subject to the same distribution shift, the optimal hyper-parameter/model should change smoothlyas well. To encourage a smooth transition of the optimal model, we select the optimal model followingan exponential moving average fashion as in Eq. 6 where we denote the moving average validation loss atthe t-th batch as Ltval and the chosen best model is obtained as the one that minimizes the smoothedvalidation loss.",
  "Sample Selection for HILTTA": "The sample selection strategy should balance the objectives of active learning and model selection for moreeffective HILTTA. We achieve this target from two perspectives. First, the selected samples should prioritizelow-confidence samples for more effective active learning, as adopted by SimATTA (Gui et al., 2024) throughincremental K-means. However, selecting redundant low-confidence samples may harm the effectiveness ofmodel validation. This motivates us to select samples approximating the testing data distribution, similarto the objective of active testing (Kossen et al., 2021). Biasing towards either criterion would result in asuboptimal HILTTA. Our design mainly aims to balance the preference over selecting low-confidence anddiverse samples. A similar objective as above was achieved by a recent active learning approach, BADGE (Ash et al., 2020),which takes the gradient of loss w.r.t. the penultimate layer feature as the uncertainty weighted featureas gx =",
  "out Lce(h(x; ), y), where out RDC is the classifier weights. Hence, the gradient embedding": "gx RDC is of dimension D C.When the semantic space is substantially high, e.g.C = 1000 forImageNet (Deng et al., 2009), the above gradient embedding results in extremely high dimension features.For instance, a typical penultimate layer feature is 2048 channels and the gradient embedding could reach2,048,000 channels. Such a high dimension prevents efficient clustering operations, e.g. due to memoryconstraints, for sampling. To mitigate the ultra-high dimension challenge, we integrate uncertainty intofeature representation by multiplying the uncertainty, measured by the margin confidence, to the featurerepresentation as in Eq. 7, where sortd refers to the descending sorting.",
  "minBlt{gi}Nbi=1maxgi{gi}Nbi=1minckBlt||gi ck||,s.t. |Blt| K(8)": "Our K-Margin approach with the K-Center clustering approach offers a computationally efficient alternativeto K-Means++ while similarly balancing uncertainty and diversity in sample selection. Unlike K-Means++,which requires iterative center updates, K-Center clustering selects points that maximize coverage by mini-mizing the maximum distance from each sample to its nearest center. This method provides a representativesubset with less computational overhead, which is more suitable in TTA settings. By focusing on high-uncertainty, diverse samples, K-Center clustering ensures a labeled subset that accurately reflects the testdistribution. It guarantees that the labeled subset spans the feature space with fewer computations, therebymaintaining a consistent approximation of the test distribution. This enhances model selection and super-vised training by creating a robust validation set that captures broad feature variance without redundantlow-confidence samples.",
  "Overall Algorithm for HILTTA": "We propose two types of training losses for model updates and a validation loss for model selection. Duringthe model selection stage, we generate multiple candidate models by training with an unsupervised TTAloss, denoted as Lutr, following the off-the-shelf TTA methods. The optimal candidate model t is selectedusing arg minmLtval(m). After selecting the best model, we further refine it through supervised training,",
  "Experimental setting": "Datasets:We select a total of five datasets for evaluation.The CIFAR10-C and CIFAR100-C (Hendrycks & Dietterich, 2018) are small-scale corruption datasets, with 15 different common corruptions,each containing 10,000 corrupt images with 10/100 categories. For our evaluation in large-scale datasets, weopt for ImageNet-C (Hendrycks & Dietterich, 2018), which also contains 15 different corruptions, each with50,000 corrupt images in 1000 categories. Additionally, ImageNet-D (Rusak et al., 2022) is a style-transferdataset, offering 6 domain shifts, each consisting of 10,000 images selected in 109 classes. Additionally, weevaluate our method on the ModelNet40-C dataset (Sun et al., 2022), which includes 3,180 3D point cloudsaffected by 15 common types of corruption.",
  "return Predictions Y;": "CIFAR100-C datasets, we use a batch size of 200 and an annotation percentage of 3%, with pre-trainedWideResNet-28 (Zagoruyko & Komodakis, 2016) and ResNeXt-29 (Xie et al., 2017) models, respectively.For ImageNet-C and ImageNet-D, we use a batch size of 64 for adaptation and an annotation percentage of3.2% of the total testing samples, employing the ResNet-50 (He et al., 2016) pre-trained model. We use the optimizer recommended in the original papers for each methods unsupervised training. We setthe momentum for 0.5. For supervised training, we use the Adam optimizer with a learning rate of 1e-5.We consider seven distinct values for each hyper-parameter category regarding model selection, detailed inTab. 8. Competing Methods: We evaluate 6 state-of-the-art off-the-shelf test-time adaptation approaches underboth traditional unsupervised TTA and Human-in-the-Loop TTA protocols, including TENT (Wang et al.,2020), PL (Lee et al., 2013), SHOT (Liang et al., 2020), EATA (Niu et al., 2022), SAR (Niu et al., 2023),RMT (Dbler et al., 2023). For each of the above TTA method under HILTTA protocol, we embed theTTA loss into the Unsupervised Model Adaptation step of the HILTTA algorithm in Alg. 1. We furtherbenchmark against existing active TTA methods, including SimATTA (Gui et al., 2024) and VeSSAL (Saranet al., 2023). Specifically, we adapt VeSSAL for TTA by using the selected samples for supervised training.More details of the benchmarked methods are deferred to the Appendix.",
  "Evaluations on HILTTA": "We report the classification error rates for continual TTA in Tab. 2.For unsupervised TTA methods(Unsup. TTA), we report three results: worst(Worst), average (Avg.), and best (Best). The \"Worst\"and \"Best\" results are derived from fixed hyper-parameters that yield the highest and lowest overall errorrate, respectively. The \"Average\" results are calculated by averaging the accuracy across models trained withall candidate hyper-parameters. For the competing active TTA methods (Active TTA), we maintain thesame annotation rate as our method and use manually tuned hyper-parameters to ensure a fair comparison.These methods, lacking model selection capabilities, result in a single error rate per dataset. In contrast, ourmethod (Human-in-the-Loop TTA) automatically selects hyper-parameters using our proposed modelselection strategy, also resulting in a single error rate per dataset.",
  "From the results, we make the following observations:": "i) Existing unsupervised TTA methods, when applied without additional annotation, exhibit significantsensitivity to hyper-parameters. This sensitivity can lead to large variations in performance (e.g., TENTsbest accuracy at 62.96% versus its worst at 95.23% on ImageNet-C). In the worst case, performance caneven fall below the source model without any adaptation. In contrast, the \"Best\" performance across allmethods are relatively close to each other. Nevertheless, selecting the best model without oracle knowledgeis impractical in real-world scenarios. These findings highlight the persistent challenge of hyper-parametersensitivity in TTA. ii) Our proposed human-in-the-loop TTA consistently improves the performance of all off-the-shelf TTAmethods against their unsupervised counterparts even with the best hyper-parameters. This demonstratesthe robustness of our approach, mitigating the catastrophic failures that can arise from hyper-parametersensitivity. iii) Compared to existing active TTA approaches, even when manually tuning hyper-parameters to addressthe model selection issue, our proposed HILTTA consistently outperforms them across all four datasets.Notably, even a basic baseline like TENT achieves performance comparable to or better than existing activeTTA methods when combined with our approach. These results highlight the effectiveness of integrating asmall amount of labeled data through synergistic active learning and model selection, rather than relyingsolely on active learning.",
  "Ablation & Additional Study": "Unveiling the Impact of Individual Components: We conduct a comprehensive ablation analysis usingTENT as the off-the-shelf TTA method in Tab. 3. Compared with not adapting to the testing data stream,fine-tuning with only the unsupervised training loss (Unsup.Train) could improve the performance, asexpected. When human annotation is introduced, using the cross-entropy loss Hce as validation objective (CEValid.)does not guarantee a consistent improvement.As discussed in Zhao et al. (2023), this can beattributed to the challenges posed by the min-max equilibrium optimization problem across time, resultingin a notable performance decline. Incorporating additional anchor regularization for model selection (referredto as Anchor Reg.) consistently improves performance beyond the average error rate of the original TENT.",
  "w/ HIL---90.5262.6667.8221.36w/ HIL--65.9552.8747.5517.79w/ HIL-62.6152.6134.1817.79w/ HIL58.3548.7430.5315.87": "Evaluation of Alternative Active Learning Strategies: To optimize annotation resource allocation,we systematically evaluate various sample selection strategies within the HILTTA protocol, as illustratedin . We explore multiple approaches, including random sampling (Random), entropy-based selection(Entropy(Shannon, 1948)), feature diversity sampling (CoreSet (Sener & Savarese, 2018)), stream-basedactive learning (VeSSAL (Saran et al., 2023)), the incremental clustering method introduced by Gui et al.(2024), and also our approach, K-Margin, uniquely combines uncertainty and feature diversity. Given thatother active learning methods lack model selection capabilities, we employ cross-entropy loss Hce as thevalidation objective. The results clearly demonstrate that K-Margin significantly outperforms other selectionstrategies in terms of average error rate across four datasets.",
  "Entropy": "CoreSet VeSSAL ASE IC OURS CIFAR10-C : Comparison of different active learning strategies under HILTTA with TENT (Wang et al., 2020)as TTA unsupervised model adaptation strategy. Average classification error is reported, where a lower valueindicates better performance. Evaluation of Alternative Model Selection Strategies: We further compare our approach against sev-eral state-of-the-art model selection methods, including Entropy (Morerio et al., 2018), InfoMax (Musgraveet al., 2022), and MixVal (Hu et al., 2023), which are widely recognized in Unsupervised Domain Adapta-tion. Additionally, we consider the Oracle model selection strategy for online TTA, known as PitTTA (Zhaoet al., 2023), which utilizes all labels within the streaming test dataset Dt to perform model selection basedon accuracy comparison. The results presented in Tab.4 demonstrate that our proposed methods consis-tently surpass these alternative strategies, delivering comparable or superior performance to the best resultsobtained with fixed hyper-parameter settings. Notably, our HILTTA approach outperforms PitTTA(Zhaoet al., 2023) while requiring significantly fewer annotations, making it a more practical and resource-efficientsolution.",
  ": Performance on the 3D point cloudclassification task in ModelNet40-C dataset.Average classification error is reported": "Evaluations beyond 2D Image Recognition: To fur-ther evaluate the effectiveness of our method beyond 2D im-age classification task, we conduct experiments on 3D pointcloud classification using DGCNN (Wang et al., 2019) asthe backbone, adapting it continuously across 15 domainswithin the ModelNet40-C (Sun et al., 2022) dataset.Wemaintained the same hyper-parameter candidate set as out-lined in Tab. 8 and followed the experimental setup detailedin Tab. 2, with a batch size of 32 and an annotation per-centage of 3.2%. The results, presented in , show thatunsupervised TTA still consistently outperforms the base-line without any adaptation (Source). However, due to thesensitivity to hyper-parameters, all unsupervised TTA meth-ods exhibit a substantial variation in performance with theBest and Worst hyper-parameters. All off-the-shelf TTAmethods are consistently improved when integrated with ourHILTTA method. Five out of the six methods witness a more significant boost in performance, surpassingtheir unsupervised counterpart with Best hyper-parameters. Computation Efficiency: A major concern for test-time adaptation methods is the computation efficiency.We conduct empirical studies to measure the wall-clock time of both the inference and adaptation steps inHILTTA, utilizing a single RTX 3090 GPU, an Intel Xeon Gold 5320 CPU, and 30 GB of RAM. The resultsare presented in Tab. 5. In this experiment, we only perform human-in-the-loop annotation for every Nbatch on the testing data stream. The inference time required is fixed for all methods and the adaptationtime varies. We demonstrate that the additional model selection procedure significantly decreases the errorrate (27.77% 15.87%) with 9 times the overall time required (1ms v.s. 9.1ms). More importantly, theoverall time required can be further reduced by conducting sparser human annotation (larger N). WithN=10, we witness a 10% absolute error rate decrease (27.77% 17.13%) with only 2.8 times computationtime. Considering that annotating a single image would cost 3000 ms to 5000 ms, the adaptation time isnegligible.",
  "(d) CIFAR10-C(c) CIFAR100-C(a) ImageNet-C(b) ImageNet-D": ": Performance of TENT (Wang et al., 2020) with and without HIL. Bold lines represent the per-formance with different hyper-parameter values, while dashed lines indicate the performance with modelselection. annotation and the average error rate respectively. TENT w/HIL Sup.Train (Random/Entropy/K-Margin)and TENT w/HIL Model Sel. (Random/Entropy/K-Margin) refer to augmenting TENT with supervisedtraining on Random/Entropy/K-Margin labeled testing data and our proposed model selection respectively.The accuracy w/ HIL consistently surpasses all w/o HIL. Furthermore, random and entropy selection areconsistently less effective than our proposed K-Margin selection for model selection purposes. Crucially,our final HILTTA approach consistently achieves significantly better performance than the worst hyper-parameter, and on ImageNet-C, we even surpass the model using the best-fixed hyper-parameter.",
  ": (Left) Average selected learning rate per corruption for TENT on ImageNet-C. (Right) Averageerror rate per corruption for TENT on ImageNet-C": "Temporal Evolution Analysis of HILTTA: We present an analysis of the evolution of selected hyper-parameters over time. As shown in , the hyper-parameters selected by our HILTTA model selectionmethod always hover around the upper bound Fixed Best while competing methods deviate further awayfrom the upper bound. The advantage is also reflected in the accumulated error plot. In particular, state-of-the-art model selection methods such as MixVal (Hu et al., 2023), are susceptible to catastrophic failureas they struggle to adapt to continuously changing domains.",
  "1e-122.4181.8988.6488.8988.78": "Selecting Two hyper-parameters:In certain cases, more than one hyper-parameter needs to be selected.We evaluated HILTTA combined with PL (Lee et al., 2013) on CIFAR10-C by selecting two hyper-parameters:the self-training threshold and the learning rate. As shown in Tab. 6, our HILTTA achieved the 3rd bestresult among 25 candidate hyper-parameter sets. These results suggest that our method is still effective withmore than one hyper-parameters.",
  ": Performance of different TTA methods combined with our proposed HILTTA under differentlabeling budgets on CIFAR10-C, average error rates are reported (lower is better)": "Performance Under Adjustable Human Intervention Frequency: We investigate the effectiveness ofHILTTA under varying human intervention frequencies (N), as shown in Tab. 7. Instead of requesting humanintervention for every batch of test streaming data, we reduce the frequency by only requesting interventionfor every N-th batch. Consequently, the label rate is reduced to 1/N. Model selection is also performed inevery N batch, while for batches without human intervention, the previously selected hyperparameters areused for test-time adaptation. Remarkably, even when N = 10, corresponding to a mere 0.3% label rate,HILTTA combined with TENT significantly outperforms the original TENT without human intervention,achieving an error rate of 18.61% compared to 27.77%. This demonstrates that even with a very limitedlabeling budget and sparse human intervention, notable performance gains can be achieved. By employingan adjustable human intervention frequency, there is no need to annotate every batch, further enhancingHILTTAs practical impact.",
  "Conclusion": "In this study, we introduce a novel approach to human-in-the-loop test-time adaptation (HILTTA) by inte-grating active learning with model selection. Beyond utilizing active learning, we leverage the labeled data asa validation set, facilitating the smooth selection of hyper-parameters and enabling supervised training on asubset of the data. We also developed regularization techniques to enhance the validation loss and proposeda new sample selection strategy tailored for HILTTA. By integrating HILTTA with multiple existing TTAmethods, we observed consistent improvements due to the synergistic effects of active learning and modelselection, outperforming both the worst-case hyper-parameter choices and average performance, as well asexisting active TTA methods. Our findings offer new insights into optimizing limited annotation budgets inTTA tasks. Acknowledgement: This research is supported by the Agency for Science, Technology and Research(A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021), the National Natural ScienceFoundation of China (NSFC) under Grant 62106078, and Sichuan Science and Technology Program (ProjectNo. 2023NSFSC1421).",
  "Yijin Chen, Xun Xu, Yongyi Su, and Kui Jia. Stfar: Improving object detection robustness at test-time byself-training with feature alignment regularization. arXiv preprint arXiv:2303.17937, 2023": "Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustnessbenchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Bench-marks Track (Round 2), 2021. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In Proceedings of IEEE conference on Computer Vision and Pattern Recognition, 2009. Mario Dbler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-timeadaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,2023.",
  "Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Proceedingsof International Conference on Machine Learning, 2015": "Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon Kim, Jinwoo Shin, and Sung-Ju Lee.NOTE: Ro-bust continual test-time adaptation against temporal correlation. In Proceedings of Advances in NeuralInformation Processing Systems, 2022. Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska Meier,Douwe Kiela, Kyunghyun Cho, and Soumith Chintala.Generalized inner loop meta-learning.arXivpreprint arXiv:1910.01727, 2019.",
  "Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on Pattern Analysis andMachine Intelligence, 2017": "Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesistransfer for unsupervised domain adaptation.In Proceedings of International Conference on MachineLearning, 2020. Jian Liang, Dapeng Hu, Yunbo Wang, Ran He, and Jiashi Feng. Source data-absent unsupervised domainadaptation through hypothesis transfer and labeling transfer. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2021.",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In Proceedingsof International Conference on Learning Representations, 2018": "Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and AlexandreAlahi. Ttt++: When does self-supervised test-time training fail or thrive? In Proceedings of Advances inNeural Information Processing Systems, 2021. Robert A. Marsden, Mario Dbler, and Bin Yang. Universal test-time adaptation through weight ensem-bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, 2024a. Robert A Marsden, Mario Dbler, and Bin Yang. Universal test-time adaptation through weight ensem-bling, diversity weighting, and prior correction. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, 2024b.",
  "Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal,1948": "Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin DogusCubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with con-sistency and confidence. Advances in neural information processing systems, 33:596608, 2020. Yongyi Su, Xun Xu, and Kui Jia. Revisiting realistic test-time training: Sequential inference and adaptationby anchored clustering. Proceedings of Advances in Neural Information Processing Systems, 2022.",
  "Yongyi Su, Xun Xu, and Kui Jia. Towards real-world test-time adaptation: Tri-net self-training with balancednormalization. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024a": "Yongyi Su, Xun Xu, Tianrui Li, and Kui Jia. Revisiting realistic test-time training: Sequential inferenceand adaptation by anchored clustering regularized self-training. IEEE Transactions on Pattern Analysisand Machine Intelligence, 2024b. Jiachen Sun, Qingzhao Zhang, Bhavya Kailkhura, Zhiding Yu, Chaowei Xiao, and Z Morley Mao.Benchmarking robustness of 3d point cloud recognition against common corruptions.arXiv preprintarXiv:2201.12296, 2022. Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training withself-supervision for generalization under distribution shifts. In Proceedings of International Conference onMachine Learning, 2020. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results. Advances in neural information processing systems,30, 2017. Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-timeadaptation by entropy minimization. In Proceedings of International Conference on Learning Representa-tions, 2020.",
  "Qin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022": "Shuai Wang, Daoan Zhang, Zipei Yan, Jianguo Zhang, and Rui Li. Feature alignment and uniformity fortest time adaptation.In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2023a. Wei Wang, Zhun Zhong, Weijie Wang, Xi Chen, Charles Ling, Boyu Wang, and Nicu Sebe. Dynamicallyinstance-guided adaptation: A backward-free approach for test-time domain adaptive semantic segmenta-tion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023b.",
  "Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E Sarma, Michael M Bronstein, and Justin M Solomon. Dynamicgraph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 2019": "Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE conference onComputer Vision and Pattern Recognition, 2015. Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-tions for deep neural networks. In Proceedings of the IEEE conference on Computer Vision and PatternRecognition, 2017.",
  "Following prior works (Dbler et al., 2023; Wang et al., 2022; Niu et al., 2022), we perform continual testtime adaptation in the following datasets, detailed in follows": "CIFAR10-C (Hendrycks & Dietterich, 2018) is a small-scale corruption dataset, with 15 different commoncorruptions, each containing 10,000 corrupt images of dimension (3, 32, 32) with 10 categories. We evaluateseverity level 5 images, with \"Gaussian shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixelate jpeg\" sequence. CIFAR100-C (Hendrycks & Dietterich, 2018) is a small-scale corruption dataset, with 15 different commoncorruptions, each containing 10,000 corrupt images of dimension (3, 32, 32) with 100 categories. We evaluateseverity level 5 images, with \"Gaussian shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixelate jpeg\" sequence. ImageNet-C (Hendrycks & Dietterich, 2018) is a large-scale corruption dataset, with 15 different commoncorruptions, each containing 50,000 corrupt images of dimension (3, 224, 224) with 1000 categories. Weevaluate the first 5,000 samples with severity level 5 following Croce et al. (2021). The domain sequence is\"Gaussian shot impulse defocus glass motion zoom snow frost fog brightness contrast elastic pixelate jpeg\". ImageNet-D (Rusak et al., 2022), as a large-scale style-transfer dataset, is built upon DomainNet (Penget al., 2019), which encompasses 6 domain shifts (clipart, infograph, painting, quickdraw, real, and sketch).The dataset focuses on samples belonging to the 164 classes that overlap with ImageNet. Following Marsdenet al. (2024b), we specifically select all classes with a one-to-one mapping from DomainNet to ImageNet,resulting in a total of 109 classes and excluding the quickdraw domain due to the challenge of attributingmany examples to a specific class. After processing, it contains 5 domain shifts with each 10,000 imagesselected in 109 classes. The domain sequence is \"clipart infograph painting real sketch\". ModelNet40-C (Sun et al., 2022), as a 3D point cloud corrupted dataset for benchmarking point cloudrecognition performance under corruptions, is built upon ModelNet40 (Wu et al., 2015). We use the highestcorrupted level 5 to evaluate the performance. The domain sequence is \"background cutout density density inc distortion distortion rbf distortion rbf inv gaussian impulse lidar occlusion rotation shear uniform upsampling\".",
  "TENT (Wang et al., 2020). It focuses on updating batch normalization layers through entropy mini-mization. We follow the official implementation1 of TENT to update BN parameters": "PL (Lee et al., 2013). We implement self-supervised training on unlabeled data updating all parametersby cross-entropy with an entropy threshold E = log(#class). For CIFAR10-C and CIFAR100-C, we usean SGD optimizer with a 1e-3 learning rate, while an SGD optimizer with a 2.5e-5 learning rate for the rest. SHOT (Liang et al., 2020). It freezes the linear classifier and trains the feature extractor by balancingprediction category distribution, coupled with pseudo-label-based self-training. We follow its official code2",
  "ASE (Kossen et al., 2022). It uses surrogate estimators to sample in a uncertainty distribution to samplewith low bias. We follow its official implementation6": "SIMATTA (Gui et al., 2024). As an active test-time adaptation method, it selects labeled samplesthrough incremental clustering and updates the model by minimizing cross-entropy. To reduce the compu-tational and memory cost of incremental clustering, we keep the maximum length of anchors to 50 in allexperiments. For CIFAR10-C and CIFAR100-C, we set the lower entropy bound el to 0.005 and 0.01 for thehigher entropy bound eu. While for ImageNet-C and ImageNet-D, we set the lower entropy bound el to 0.2and 0.4 for the higher entropy bound eu. VeSSAL (Saran et al., 2023). As a stream-based active learning method, it uses volume sampling forstreaming active learning. We also follow its official implementation7, using feature embedding for stream-based data selection.",
  "A.3Hyper-parameter Candidate set for Model Selection": "Following Musgrave et al. (2022); Hu et al. (2023), in order to evaluate the performance of our methodon different types of hyper-parameters, we construct candidate sets with seven distinct values for eachhyper-parameter category related to model selection. We assess different hyper-parameters for each TTAmethod: for PL and EATA, the self-training threshold; for TENT and SAR, the learning rate; and for SHOTand RMT, the loss coefficient factor. To ensure generality, we use the same across diverse datasets, assummarized in Tab. 8.",
  "B.1Ablation Study on the Impact of": "We evaluate the impact of the moving average momentum parameter, , in Eq. 6 when applying TENT (Wanget al., 2020) combined with our proposed HILTTA. The experiments were conducted across four datasets:ImageNet-C, ImageNet-D, CIFAR100-C, and CIFAR10-C. We varied over a wide range, from 0.1 to 0.9,and observed in that the performance remained robust throughout.This demonstrates that ourproposed HILTTA method is not sensitive to the choice of values, maintaining stability across differentsettings. 0.10.20.30.40.50.60.70.80.9 Error Rate(%)",
  "B.2Comparison to Bi-level Optimization": "In some scenarios, the cost of collecting annotated data can outweigh the computational expense, makinghyperparameter optimization via bi-level optimization (Liu et al., 2018) a viable approach. However, bi-leveloptimization has limitations, particularly its sensitivity to the meta-learning rate (the learning rate in theouter loop). We compare our method, HILTTA, with the approach proposed in (Liu et al., 2018), using Adamas the meta-optimizer. The Higher package (Grefenstette et al., 2019) is adopted for meta optimization. Asshown in Fig 8, while the best performance achieved by bi-level optimization is comparable to ours (59.34%vs. 58.35% error rate), bi-level methods can be prone to collapse if the meta-learning rate is not carefullytuned. Additionally, bi-level optimization is unable to optimize hyperparameters that are non-differentiable. 1e-65e-61e-55e-51e-45e-41e-3",
  "B.3Comparison to Semi-supervised Learning Methods": "To evaluate the effectiveness of our proposed HILTTA, we compare it against semi-supervised methods, suchas FixMatch (Sohn et al., 2020) and MeanTeacher (Tarvainen & Valpola, 2017), both using the same label rateof 3%. Additionally, we include ATTA methods like SimATTA (Gui et al., 2024) and VeSSAL (Saran et al.,2023), adapted to the ATTA setting by integrating them with TENT (Wang et al., 2020), also with label rateof 3%. As shown in Tab. 9, our HILTTA (HIL+TENT) surpasses all other TTA, semi-supervised learning,and ATTA approaches. Semi-supervised methods typically require longer training times and multiple epochsto achieve convergence, making them less effective in stream-based tasks. It is worth noting that while bothsemi-supervised and ATTA methods rely on manually tuned hyperparameters, HILTTA achieves superiorperformance with model selection.",
  "Error Rate(%)27.7717.0316.5917.6617.4715.87": "Although semi-supervised learning and ATTA methods can achieve performance comparable to HILTTA,they suffer from significant sensitivity to hyperparameters. As shown in , we compare the performanceof TENT, HIL+TENT, and FixMatch on the CIFAR10-C dataset with learning rate as the variable hyper-parameter. The results indicate that semi-supervised methods such as FixMatch are particularly sensitiveto changes in learning rate, highlighting the challenges of tuning these methods effectively. Our proposedHILTTA is not only aimed at improving performance but, more importantly, at addressing hyperparametersensitivity issues by synergizing active learning with model selection. 5e-51e-42.5e-45e-41e-32.5e-35e-3 Learning Rate (LR) Error Rate (%) TENTFixMatchHIL+TENT",
  "B.4Studies on the Role of Regularization in HILTTA": "We evaluate the impact of the proposed anchor regularization and EMA smoothing techniques for modelselection within the HILTTA framework. These methods are designed to prevent the model from overfittingto the local data distribution in the testing data stream. While removing these regularizations may improveadaptation to a single domain, it negatively impacts performance across future domains. As shown in Tab. 10,in the continual TTA setting, model selection without regularization results in significantly worse performancecompared to when regularization is applied (17.25% vs.15.87%).This indicates that regularization iscrucial for maintaining robust performance in continual TTA. , model selection without regularization leadsto significantly worse results (17.25% vs. 15.87%), highlighting the necessity of regularization for robustperformance in continual TTA.",
  "B.5Comparing with Knowledge Distillation and Ensemble Learning": "We carried out additional experiments by comparing with ensemble learning and multi-teacher knowledgedistillation methods. Specifically, for ensemble learning, we average the posterior predicted by each candidatemodel and make a prediction with the average posterior. For multi-teacher knowledge distillation, we usethe pseudo label predicted by the ensemble model for self-training. As observed from Tab. 11, both ensemblelearning and multi-teacher knowledge distillation yields inferior results than HILTTA. This is most likelycaused by incorporating poor candidate models (ensemble learning) and using poor pseudo labels for self-training, i.e. learning from noisy labeled data (multi-teacher knowledge distillation).",
  "B.6Comparing Adaptation Time & Annotation Time": "We carried out empirical studies by measuring the wall-clock time lapses of the inference step (InferenceTime), adaptation step (Adaptation Time), and human annotation (Annotation Time) per sample.Inspecific, the \"Inference Time\" refers to the time-lapse of pure forward pass, the \"Adaption Time\" refers tothe time-lapse of backpropagation and weights update, and \"Annotation Time\" refers to the estimated timelapse required for human to annotate images at a 3% labeling rate. We evaluate under three categoriesof annotation cost, i.e. 2 sec/sample, 5 sec/sample, and 10 sec/sample. As seen from Tab. 12, despite theadaptation time being higher than the inference time (8.7 ms v.s. 0.4 ms), the annotation time is the mostexpensive step. These results suggest model selection and adaptation do not slow down the throughputunder the HILTTA protocol.",
  "B.7Computation Complexity of Competing Active TTA Methods": "We find that existing stream-based active learning (VeSSAL (Kossen et al., 2022)) and active TTA (SimATTA(Gui et al., 2024)) are inherently more computationally expensive than HILTTA due to the following reasons.First, VeSSAL adopts gradient embedding, i.e. g(xi) = l(f(xi; ), yi)/L, where L RKD refers to theclassifier weights. This results in a very high dimensional gradient embedding g(xi) RDK. The probabilityof labeling a sample pi involves calculating the inverse of covariance matrix, i.e. pi g(xi) 1g(xi), s.t. = xiB g(xi)g(xi) RDKDK. Hence, VeSSAL can hardly deal with classification tasks with a large numberof classes, e.g. ImageNet, where K is large and calculating a large matrix inverse is inherently time-consuming.Second, SimATTA employs a K-Means clustering algorithm for the selection of samples to annotate. SinceK-Means is an iterative approach the computation cost of SimATTA is relatively high as well. In contrast,our proposed sample selection is built upon K-Center clustering which sequentially selects the sample withthe highest distance to cluster centers (greedy algorithm), the computation cost is significantly lower. An",
  "B.8Studies on Random Batchsizes": "We recognize that in certain scenarios, test data may arrive inconsistently and intermittently. Results fromTab. 14 demonstrate that our proposed HILTTA framework is adaptable to such varying batchsize. At eachiteration, we vary the batchsize randomly among the choices . This versatility leads toeven greater improvements compared to fixed batch size. This enhanced performance stems from the factthat the optimal hyper-parameter values vary with different batch sizes, thus benefiting significantly fromour adaptive model selection approach."
}