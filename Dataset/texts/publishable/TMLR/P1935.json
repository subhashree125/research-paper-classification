{
  "Abstract": "In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4performance on several deterministic tasks; each task involves a basic calculation and takesas input parameter some element drawn from a large well-defined population (e.g., countelements in a list, multiply two k-digit numbers, etc). We examine several conditions per-taskand perform enough trials so that statistically significant differences can be detected. Thisallows us to investigate the sensitivity of task-accuracy both to query phrasing and inputparameter population. We find that seemingly trivial modifications in the task-prompt orinput population can yield differences far larger than can be explained by sampling effects.For example, performance on a simple list-counting task varies with query-phrasing andlist-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency(e.g., success when an element accounts for 50% of a list is different from when it accountsfor 70% etc). We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond whatthe data supports. A consequence appears to be that intuitions that have been formed basedon interactions with humans form a very unreliable guide as to which input modificationsshould make no difference to LLM performance.",
  "Introduction": "Rapid improvements in the performance of large language models (LLMs) have spurred great interest inevaluating their capabilities. In addition to answering general knowledge questions and summarizing text,GPT-4 has demonstrated the capability to compose poetry, solve chess puzzles and Geometry problems,and perform basic coding tasks. Capabilities that seem beyond the simple next-token-prediction they weretrained on, causes some to suggest this as evidence of emergent behaviors from LLMs, or even that we maybe witnessing the early signs of Artificial General Intelligence (AGI) (Bubeck et al., 2023). Others are notconvinced, and suggest that LLMs simply parrot pastiches of text snippets from their training sets (Benderet al., 2021). The documentation of surprising capabilities has been accompanied by many accounts of failures. Halluci-nations (where LLMs offer plausible but entirely invented detail) have proved hard to eliminate. Arkoudaspoints out that GPT-4 struggles with some basic tasks that humans find easy or trivial; e.g., they arentreliable even on tasks such as counting, multiplication, etc (Arkoudas, 2023). McCoy et al suggest that manyof the remarkable capabilities are simply artifacts of the training set and autoregressive task that GPT-4 wastrained to solve (McCoy et al., 2023). An accumulation of observed successes and failures at particular tasks unfortunately does little to settlequestions about LLM reliability or capabilities. In this paper we present results on a series of deterministic",
  "Published in Transactions on Machine Learning Research (10/2024)": "two large standardized datasets (Sun et al., 2023). Our work extends that direction by showing sensitivity notmerely to phrasing, but also input parameter, and using GPT-4 (i.e., a far larger model than used in (Sclaret al., 2023; Sun et al., 2023)). In focusing on tasks with arbitrarily large parameter spaces (e.g., countingobjects in lists) we avoid many of the concerns that some variant of a task has been seen in training. Standardized exams are often used to demonstrate LLMs capabilities. For example, studies has shownGPT-4 achieving the passing criteria of the Japanese Medical Licensing Examination (JMLE) (Takagi et al.,2023), the Uniform Bar Examination (UBE) (Katz et al., 2023), and the US Medical Licensing Examination(USMLE) (Nori et al., 2023). Knowing that even basic tasks are sensitive to trivial variations, it is legitimateto question whether the variations between a new version of an exam and its previous versions primarilyfocus on factors sensitive for humans, but neglect others that can be sensitive only for LLMs. Yarkoni (Yarkoni, 2022) argues that the problem of improper generalization goes far beyond the languageissue. He suggests that confusing fixed effects for random ones is the source of many of the replication failuresin the social sciences. Elazar et al explore the consistency of responses under rephrasing of various LLMs (Elazar et al., 2021). Theyexplore general knowledge and factual questions rather than the arithmetic tasks we explore. Their findings,that all of the LLMs studied have poor consistency, are largely corroborated by our work. Lu et al study the effect of ordering on the performance of few-shot prompts (Lu et al., 2021). They find thatpermuting the order in which examples in a few-shot prompt are presented can make the difference betweenstate-of-the-art and radnom performance.",
  "Background: The Language-as-Fixed-Effect Fallacy": "The Language-as-Fixed-Effect Fallacy, as described by Clark (Clark, 1973), is the phenomenon where a claimsupported by statistical evidence does not generalize beyond the specifics of the experimental setup. Heillustrates with a language-task thought-experiment originally proposed by Coleman (Coleman, 1964). Let Nbe the set of all English nouns, V the set of all verbs, and let T(.) be a test statistic representing how wellhumans perform at some task involving words (e.g., how well they can spell them, how quickly they can typethem, etc). Suppose that experimenter A wishes to test the hypothesis that people perform the task betteron nouns than on verbs:HA = T(N) > T(V ).",
  "Lets stipulate, by contrast, that they are both wrong, and that T(N) = T(V )": "As a test of HA the first experimenter selects subsets NA N and VA V each with some fixed numberof randomly selected nouns and verbs.With this choice she recruits participants and on finding thatT(NA) > T(VA), by a statistically significant amount, she rejects the null hypothesis (that theres nodifference) and concludes she has firm evidence in favor of HA. Similarly, the second experimenter selects atrandom different subsets NB N and VB V with fixed numbers of nouns and verbs. With this fixedchoice he recruits participants and finds T(NB) < T(VB), by a statistically significant amount, and concludesthis is firm evidence for HB. The problem is that while both A and B intend to generalize to the whole population N and V they havetested only on particular subsets. There is good evidence to believe that, with any collection of participants,we could verify both T(NA) > T(VA) and T(NB) < T(VB), but neither of these is enough to support eitherHA or HB. In the language of statistical testing our experimenters have treated random effects as fixed(Clark, 1973). Fixed effects are those that are considered constant across the relevant population, while random effects arethose that vary (for an account of various other definitions see (Gelman, 2005)). In the experiments abovethere are two populations involved: the populations of noun-verb collections, and the population of humanparticipants. When she generalized from NA, VA to N, V our first experimenter implicitly assumed that anyother subsets NC N and VC V would also give the result that she observed (i.e., T(NC) > T(VC)). Ifthis were true shed be justified in thinking that her observed difference was powerful evidence for HA. If thisis not true then her experiment supports only the narrow uninteresting claim T(NA) > T(VA). Effectively,she assumed that what she observed wasnt particular to NA, VA but general to N, V . In a colloquial sense fixed effects are ones where the particular choice doesnt affect the generality we wishto claim. We expect, for example, that what an experimenter had for breakfast or what color socks she",
  "The Fixed-Effect Fallacy and LLM Task Performance": "We wish to evaluate whether, and how well, an LLM can perform a particular task that has a singledeterministic correct answer (e.g., counting, deciding to invoke a plug-in, or Retrieval-Augmented Generationetc). For the counting task one approach might be to produce a list of objects and prompt the LLM to countthe occurrences of a particular item. To make the experimental setup concrete we might specify a list lengthand dictionary of possible elements. For example:",
  "where r R. By repeating this query with many different elements of R we might try to build a picture ofthe LLMs performance at the task": "In this setup choice of list from R is being treated as the only random effect; i.e., the only source of variation(Gelman & Hill, 2006). We are testing how well the LLM does over many different members of R but areassuming that other factors we might vary make no difference. However, there are many other populationsof lists that we might try, and there are many other wordings of the prompt that could be used. If we useobserved success with the above prompt to conclude that our LLM can count elements of a length-20 listwith a particular success rate we are implicitly assuming that these other possible choices would make nodifference. For example, an alternative to the prompt above might be:",
  "This would appear to be an equivalent evaluation of the task, or a modification that should make no difference.Unfortunately, this is not the case": "As we show in .2 these assumptions most definitely do not hold. Wording of the prompt and choice ofthe particular items to be counted can make a substantial difference to the answer (see ). For example,the hypothesis that tests using the two prompts given above (with everything else held constant) produceresults drawn from the same distribution, is robustly rejected by a 2 test. Thus, if we report that our LLMcan count with a particular success rate we are committing the same fixed-effect error as experimenters Aand B above. When we encounter a particular experimental result (e.g., q = 0.86 (86.0%) on the N = 10 counting task in) we generally understand that this involves some margin of error. For example, rather than q, weexpect a repeat of the experiment to produce an estimate q q. A very familiar case exploits the fact that95% of the values of a normal distribution lie within 1.96 standard deviations of the mean, so we can writeq = 1.96",
  "Experimental setup": "In order to test LLM performance we choose tasks that have deterministic answers, and where it is relativelyeasy to decide if the LLM gives the correct answer. This obviates the need for subjective assessments,heuristics, hand-labelling or error-prone parsing of the response, and allows us to scale-up testing. The taskswe examine are: counting, finding the maximum, median and sorted version of a list of numbers, and longmultiplication. The difficulty with counting and long multiplication has been observed by others (Arkoudas,2023). Unless otherwise specified all of the conditions were evaluated on 500 independent runs. Thus, for example, ifa table entry reports a success rate of 89.0% on a task, and sampling were the only source of randomness, thena reasonable estimate of the 95% confidence interval would be 1.96 0.89 0.11/500 2.74%. However,an important finding, below, is that there are significant other sources of randomness, and the conventionalway of estimating margins-of-error cannot be applied. Lists were generated independently for each trial atquery-time; thus, we did not re-use lists across conditions. All of the trials are performed using the OpenAIGPT-4 API with a temperature setting of 0.7. The results of all queries are available in the GitHub repository trials were performed using a temperaturesetting of 0.9. For all of the tasks we give an example prompt together with the correct answer and GPT-4s answer. Dueto space constraints we show only examples where the GPT-4 response is incorrect. This is not reflectiveof its accuracy: in each case we give a table showing how accuracy evolves with problem size. However,in giving examples where the answers are incorrect we illustrate that they are often very significantlybetter-than-random.",
  "Count": "First we examine the capability of GPT-4 to perform basic counting tasks. We choose a length-rLen list withtwo possible elements and ask GPT-4 to count the number of occurrences of the first element. An examplequery is (lets call this wording #1): Prompt: How many times does mango appear in this list: [mango, peach, peach, peach, mango,mango, mango, peach, peach, peach, mango, mango, mango].Correct Answer: 7GPT-4 Answer: Mango appears 6 times in this list. We evaluate for five different target lengths; the results are shown in the first column of .Inchoosing modifications of this task we choose a different variations of the input list by replacing the word-pairmango/peach with airedale/aspidistra (results in column 2). We alter the weights: i.e., have mango andpeach appear with probabilities 70% and 30% instead of 50% and 50% (results in columns 3). We alsoexamine one simple rewording of the prompt (lets call this wording #2):",
  "Correct Answer: 7GPT-4 Answer: Mango appears 6 times in this list": "This gives us a total of four conditions, all of which involve the same basic counting task. We evaluate eachcondition with list lengths rLen= 10, 15, 20, 30 and 40, and we perform 500 trials per condition. The resultsare shown in . Thus, the five rows and first four columns represent a total of 5 4 500 = 10, 000queries to GPT-4.",
  "Comp. Cols(1,2)(2, p)Comp. Cols(1,3)(2, p)Comp. Cols(1,4)(2, p)": "1037.4%25.2%31.2%23.0%(16.74, 4.28e-05)(3.99, 4.57e-02)(23.91, 1.01e-06)1511.4%4.2%5.8%2.6%(17.03, 3.67e-05)(9.27, 2.32e-03)(28.4, 9.85e-08)202.2%0.6%0.4%0.0%(3.55, 5.96e-02)(4.99, 2.55e-02)(9.19, 2.43e-03) :Llama Percent correct for counting the occurrences of a length-rLen list with two items chosenuniformly-at-random. Performance decays rapidly with list length. On the right-hand side of the table wepresent 2 tests comparing the results of the first condition with each of the others. This test evaluates thenull hypothesis that the answers in the various conditions are drawn from the same distribution. Boldfaceentries are cases where p < 0.05 and we reject the null hypothesis. The null hypothesis is robustly rejectedfor almost all lengths and conditions. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangersof stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference onfairness, accountability, and transparency, pp. 610623, 2021. Tom B. Brown et al. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, RaiaHadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information ProcessingSystems 33, 2020. Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, PeterLee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Earlyexperiments with GPT-4. arXiv preprint arXiv:2303.12712, 2023.",
  "Maximum, Median and Sort": "Here we ask GPT-4 to perform elementary tasks on lists of numbers: return the maximum, median andsorted version of the list. We evaluate three different conditions. First we ask for the maximum (or medianor sorted version) of a list of rLen numbers drawn uniformly-at-random from the interval (100.0, 20000.00)and rounded to two decimals places. An example of the prompt for the median-finding task is: Prompt: What is the median value in this list: [7176.36, 5222.86, 1089.62, 19927.36, 5655.72, 18355.58,18978.7, 7028.49, 14190.57, 14243.69, 11251.69]. Please write Answer=Correct Answer: 11251.69GPT-4 Answer: 7176.36 Second, we repeat with integers drawn uniformly-at-random from (10, 99) (i.e., all list-members are 2-digitnumbers). Finally, we use a list of rLen name-value pairs, where a randomly-chosen name is associated with anumber drawn uniformly-at-random from the interval (100.0, 20, 000.00) and rounded to two decimals places.An example of the latter query is: Prompt: Please sort this list in ascending order: [John: $12158.21, Mary: $1416.51, Peter: $7507.58,Vivek: $10941.54, Xian: $10530.84, Alex: $1641.14, Maria: $1025.49, Frank: $260.85, Luis: $7464.35,Manuel: $1782.86, Kristen: $10085.24].Correct Answer: [Frank: $260.85, Maria: $1025.49, Mary: $1416.51, Alex: $1641.14, Manuel:$1782.86, Luis: $7464.35,Peter: $7507.58, Kristen: $10085.24, Xian: $10530.84, Vivek: $10941.54, John:$12158.21]GPT-4 Answer: [Frank: $260.85, Maria: $1025.49, Mary: $1416.51, Alex: $1641.14, Manuel:$1782.86, Peter: $7507.58, Luis: $7464.35, Kristen: $10085.24, Xian: $10530.84, Vivek: $10941.54, John:$12158.21] The results of the maximum, median and sorting tasks are given in Tables 2, 3 and 4 respectively. The threedifferent list conditions are explored in columns 1-3 of these tables. As in .2, we use a 2 test toexplore whether these different variations on the task produce answers that appear drawn from the samedistribution. The right-hand portion of Tables 2, 3 and 4 gives the results; we do 2 tests to compare columns2 and 3 with column 1. shows the results of the maximum-finding task. Performance in all conditions is good, though notperfect (e.g, results are almost always > 90.0%). The 2 tests show that the hypothesis that performance onthe name-value version of the list is consistent with performance on the value-only list is rejected for lengths> 11. The hypothesis that performance on the integer version of the list is consistent with performance onthe 2-decimal floats list is rejected for all lengths. shows the results of the median-finding task. Performance in all conditions is poor (e.g, resultsare < 90.0%). The 2 tests show that the hypothesis that performance when the numbers are drawn from(10.0, 20000.0) is consistent with performance when numbers are drawn as integers from (10, 99) is rejectedfor all lengths. The hypothesis that that name-value version of the list is consistent with performance onthe value-only list is also rejected for all lengths. Note that the p-values in both cases are 0.05, so theprobability that the same process accounts for both conditions is very low. shows the results of the sorting task. Performance in condition 2 is good, but is very poor in condition3 (e.g, results in column 3 are < 55.0%). The 2 tests show that the hypothesis that performance whenthe numbers are drawn from (10.0, 20000.0) is consistent with performance when numbers are drawn from(10, 99) is rejected for all lengths. The hypothesis that that name-value version of the list is consistent withperformance on the value-only list is also rejected for all lengths. Again, the p-values indicate robust rejectionof these hypotheses.",
  "%100.0%97.2%(9.23, 2.38e-03)(0.16, 6.93e-01)1596.4%100.0%92.2%(16.35, 5.27e-05)(7.44, 6.37e-03)2194.4%100.0%86.6%(26.79, 2.27e-07)(16.8, 4.16e-05)": ":Comparison of the find-maximum task. The prompt simply asks GPT-4 to find the maximum of alist of numbers. Column 1: numbers uniform on (100.0, 20000.0) to 2 decimals, Column 2: numbers uniformon (10, 99) as integers, Column 3: name-value pairs with values uniform on (100.0, 20000.0) to 2 decimals.The right-hand side of the table shows 2 tests comparing Column 1 to each of the others. Boldface entriesare cases where p < 0.05 and we reject the null hypothesis (that results in the given columns are produced bythe same process). The null hypothesis is rejected except for length-11 when comparing columns #1 and #3:thus simply switching the list from numbers to name-value pairs introduces variance beyond what can beexplained by sampling effects.",
  "%85.0%89.6%(37.62, 8.57e-10)(66.46, 3.58e-16)1552.8%74.0%89.6%(47.51, 5.47e-12)(163.32, 2.13e-37)2135.87%62.73%65.6%(65.82, 4.94e-16)(87.12, 1.02e-20)": ":Comparison of the find-median task. The prompt simply asks GPT-4 to find the median of a list ofnumbers. Column 1: numbers uniform on (100.0, 20000.0) to 2 decimals, Column 2: numbers uniform on(10, 99) as integers, Column 3: name-value pairs with values uniform on (100.0, 20000.0) to 2 decimals. Theright-hand side of the table shows 2 tests comparing Column 1 to each of the others. Boldface entries arecases where p < 0.05 and we reject the null hypothesis (that results in the given columns are produced by thesame process). The null hypothesis for all lengths and conditions: thus simply changing the range on thenumbers, or switching to name-value pairs introduces variance beyond what can be explained by samplingeffects.",
  "%99.77%52.0%(18.39, 1.80e-05)(231.64, 2.62e-52)1594.75%100.0%36.0%(24.48, 7.50e-07)(375.91, 9.69e-84)2188.32%99.8%15.0%(56.26, 6.34e-14)(528.43, 6.2e-117)": ":Comparison of the list-sorting task. The prompt simply asks GPT-4 to sort a list of numbers inascending order. Column 1: numbers uniform on (100.0, 20000.0) to 2 decimals, Column 2: numbers uniformon (10, 99) as integers, Column 3: name-value pairs with values uniform on (100.0, 20000.0) to 2 decimals.The right-hand side of the table shows goodness-of-fit 2 tests comparing Column 1 to each of the others.Boldface entries are cases where p < 0.05 and we reject the null hypothesis (that results in the given columnsare produced by the same process). The hypothesis that Column 2 or 3 is produced by the same processas Column 1 is rejected for all lengths: thus simply changing the range on the numbers, or switching toname-value pairs introduces variance beyond what can be explained by sampling effects.",
  "Prompt: What is the product of 6438 and 9038? Please write Answer =Correct Answer: 58186644GPT-4 Answer: Answer = 58169844": "shows the performance multiplying a k1-digit by a k2-digit number for k1, k2 {2, 3, 4, 5}. Apartfrom the 2 2 case the results are largely poor. Observe that perfect performance on the 2 2 task drops tonegligibly correct answers for 4 4. Since there is sometimes a significant difference between the k1 k2 result with the k2 k1 result we performa 2 test on several of the off-diagonal elements. The results are shown in . Note that results forthe 4 2 and 2 4 are significantly different, as are those for 5 2 and 2 5. Thus, even the hypothesisthat performance on the k1 k2 multiplication will be equivalent to the k2 k1 is rejected for at least somelengths. Both Dziri et al (Dziri et al., 2023) and Arkoudas (Arkoudas, 2023) look at the example of long multiplication.Dziri et al note that while the answers for 4 4 are almost always incorrect, the first and last two digits ofthe GPT-4 answers are almost always correct. They describe this as a matching of surface probabilities.That is, the first two digits of a product are determined by the leading digits of the multiplicands irrespectiveof length. Thus, this portion of the answer can always be determined without paying attention to the rest.Similarly for the last few digits.",
  ":2 goodness-of-fit test comparing the results of a k1 k2 with a k2 k1 multiplication (i.e., theoff-diagonal elements of )": "example, (Yu et al., 2023) show that small differences in prompting for legal reasoning tasks has a significantimpact on the accuracy of responses. Our results confirm these observations for a set of simple deterministictasks but with high statistical significance. On the output side, Bender et al. (Bender et al., 2021) note the dangers inherent in ascribing intent andmeaning to utterances generated by LLMs. In particular, we (as humans) make many assumptions aboutcommunications with other humans that can easily lead us to fall prey to the fixed-effect fallacy when workingwith LLMs, potentially ascribing a more general capability to the LLM than actually exists. We show thateven for simple tasks there are major sources of variance that are not easy to account for when working withLLMs. Our experiments with deterministic algorithms are related to work that examines the capability of LLMs toperform deductive reasoning (Arkoudas, 2023). In these problems, as with most of the problems we consider,the LLM must attend to most every token in the input and not hallucinate new values that would leadto short-cut solutions to related but different problems than the one given. In contrast to our experiments,Arkoudas engages in a conversation with the LLM about each of the deductive problems he poses, wherethe LLM often proceeds to contradict itself upon getting a wrong answer. Indeed, the ad-hoc reporting ofconversations with an LLM is fairly widespread (Bubeck et al., 2023) but does not rise to the level of acontrolled experiment where one can make statistically significant statements. Of course, for many complextasks it may be difficult to perform the deeper analysis we performed here for simpler tasks. Others have observed that LLM performance degrades when the input to the LLM grows in size (withinthe limits of the LLMs context window), as we have shown here. Interestingly, Liu et al (Liu et al., 2023)find that information that is at the beginning or end of the context window has more influence on LLMperformance, even for simple queries that ask the LLM a question whose answer is somewhere in the input.That is, the position of information is another source of variance, as we saw in the simple prompt rewording of, where the major change was to swap the position of the input list and query (wordings #1 and #2). Wu et al demonstrate considerable performance sensitivity for a series of tasks (Wu et al., 2023). In exploringcounter-factual tasks they conclude that LLMs rely on narrow, non-transferable procedures for task-solving.Dziri et al explore failures of LLMs on seemingly trivial tasks (Dziri et al., 2023). They are especiallyinterested in compositional tasks. They suggest that transformers often fail since they exploit linearizedpatch matching rather than any multi-step reasoning, and that errors propagate in a fashion that compounds.Schaeffer et al suggest that the often-discussed emergent properties of LLMs are an artifact of the metricschosen rather than any fundamental improvement (Schaeffer et al., 2023): For a fixed task and a fixedmodel family, the researcher can choose a metric to create an emergent ability or choose a metric to ablatean emergent ability. Chain-of-Thought (CoT) is a prompting strategy that asks the LLM to output intermediate reasoning stepsbefore giving the final answer. Research has found that it often improves LLM performance on complex tasks(Wei et al., 2022). It is worth further research to understand whether CoT-style prompts are more resilient tothe variations shown in our study. While the sensitivity of performance to prompt-phrasing has spawned the field of prompt engineering effortsto quantify this sensitivity are nascent. Sclar et al examine the effect of phrasing on accuracy for multiplechoice tasks using the LLaMA-2-13B model (Sclar et al., 2023). Sun et al examine zero-shot robustness on",
  "Discussion": "Weve shown in , the risk that measured performance with a specific prompt fails to generalizeto equivalent versions of the task. This work complements others that have documented the brittleness ofGPT-4s performance (see related work in ). However, as far as we know, ours is the first to exploretasks with several different conditions and sufficient statistical power to rule out sampling noise as the sourceof observed variation. This allows us to state with some confidence that minor modifications have potentiallyenormous effects on measured capabilities. This problem is entirely orthogonal to the frequently mentioneddifficulty with hallucinations. Every measurement experiment comes with decisions about which factors might affect the output, and whichshould make no difference. Many of these decisions are implicit, and informed by our intuition and experienceof the world. Since LLMs emulate many human capabilities it is tempting to use intuitions about humans toguide decisions about which factors should make no difference to LLM measurements. A key finding of thispaper is that this assumption leads to errors that can be significant enough to invalidate claims. Benderobserves that weve made machines that can mindlessly generate text, but we havent learned how to stopimagining the mind behind it. We suggest that the dangers of anthropomorphizing LLMs includes not justover-interpreting their capabilities, but also imagining that their robustness to variation resembles that ofhumans.",
  "An interesting direction for future work is whether we can derive new margin-of-error bounds. Our problem isthat the presence of unexplained variance means that estimating q = 1.96": "q (1 q)/N misses an additivecomponent of unknown magnitude. If rewordings of a particular task can be generated automatically thenestimating their variance would allow new (albeit higher) estimates of margin-of-error. Since we warn of the risks of improper generalizations we should note the limitations of our findings. Obviously,weve explored a limited set of tasks, and a limited set of modifications of those tasks. The tasks in thispaper are chosen deliberately with several criteria. First, they are deterministic tasks with easily-determinedanswers; this is clearly a very restricted portion of the problems to which LLMs might be applied. Second,the tasks we choose may be particularly difficult for transformer architectures. That is, the attentionmechanism (Vaswani et al., 2017) decides which portions of the context window are most important inpredicting the next token; however, for tasks like counting, sorting, etc., all words in the target list areimportant. Third, our prompts ask the questions in a concise and direct manner, without an attempt to",
  "Conclusion": "We have demonstrated that GPT-4 performance on simple tasks shows sensitivity to trivial modifications andthat this error can be enough to invalidate claims of capabilities. Despite the limited scope of our experiments,we believe our findings point to a largely-ignored source of error that potentially affects evaluation of LLMcapabilities on all tasks. That is, on every task weve considered weve found that trivial modificationsintroduce variance that invalidates the usual margin-of-error estimates. Our evidence doesnt rule out thepossibility that the problem might be larger, or smaller, or negligible on some other tasks. However, decidingthat this source of error can be ignored for a given capability comes with a burden-of-proof, and is somethingthat should be demonstrated empirically, rather than just assumed. We find that, even when modifications are trivial and make no difference to human performance on a task,we cannot assume that the same is true of LLM performance. In the absence of evidence to the contrary,measurements of LLM task-accuracy cannot be assumed to generalize beyond the precise conditions studied.",
  "AAppendix": "Here we revisit accuracy measurements for the counting task studied in .2 but using the GPT-3.5,Mistral Instruct 7B Q4 and Llama 3 8B Q4 models. GPT-3.5 was accessed via the openai API. The Mistraland Llama models were run locally using versions with quantized coefficients. Each cell in each table represents500 trials. The results for these models are show in Tables 7, 8 and 9. Each of these might be compared with .As can be seen, the same pattern observed in .2 holds: the null hypothesis (that accuracy in thevarious conditions do not differ significantly) is robustly rejected in a majority of cases.",
  "Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. GPT-4 passes the barexam. Available at SSRN 4389233, 2023": "Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and PercyLiang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered promptsand where to find them: Overcoming few-shot prompt order sensitivity. CoRR, abs/2104.08786, 2021. URL R. Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy, and Thomas L. Griffiths. Embers ofautoregression: Understanding large language models through the problem they are trained to solve. arXivpreprint arXiv:2309.13638, 2023.",
  "John Robert Taylor and William Thompson. An introduction to error analysis: the study of uncertainties inphysical measurements, volume 2. Springer, 1982": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural InformationProcessing Systems, 35:2482424837, 2022. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek, Boyuan Chen, Bailin Wang, Najoung Kim, JacobAndreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of languagemodels through counterfactual tasks, 2023."
}