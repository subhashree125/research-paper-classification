{
  "Abstract": "Deep active learning (DAL) studies the optimal selection of labeled data for training deepneural networks (DNNs). While data selection in traditional active learning is mostly op-timized for given features, in DNN these features are learned and change with the learningprocess as well as the choices of DNN architectures. How is the optimal selection of dataaffected by this change is not well understood in DAL. To shed light on this question, wepresent the first systematic investigation on: 1) the relative performance of representativemodern DAL data selection strategies, as the architecture types and sizes change in the un-derlying DNN architecture (Focus 1), and 2) the effect of optimizing the DNN architectureof a DNN on DAL (Focus 2). The results suggest that the change in the DNN architecturesignificantly influences and outweighs the benefits of data selection in DAL. These resultscautions the community in generalizing DAL findings obtained on specific architectures,while suggesting the importance to optimize the DNN architecture in order to maximize theeffect of active data selection in DAL.",
  "Introduction": "Active learning (AL) is a long standing research area that studies how to carefully select the most informativesamples to label in order to best improve learning given a limited labeling budget (Cohn et al., 1996; Settles,2009). As the success of deep neural networks (DNNs) continues, there have seen substantial developmentsin bringing the success of AL to DNNs, creating a fast-growing subarea known as deep active learning (DAL)(Ren et al., 2021). In its essence, DAL starts with training a DNN on an initial pool of labelled data, followedby optimizing an acquisition function to select new data to be labelled to improve the DNN. This iterationcontinues until a labeling budget or desired accuracy is achieved (Ren et al., 2021). The main progress in DAL has been revolving around the design and developments of acquisition functionsto improve the selection of training data. Most data acquisition strategies can be categorized into effortsthat exploit DNN uncertainties (Ranganathan et al., 2017; Wang et al., 2016; Li et al., 2017; He et al., 2019;Ostapuk et al., 2019; Gal et al., 2017b; Freytag et al., 2014; Kding et al., 2016; Yoo & Kweon, 2019; Huanget al., 2022), explore the diversity of unlabeled data (Wang et al., 2017; Sener & Savarese, 2017; Geifman& El-Yaniv, 2017; Shui et al., 2020; Zhang et al., 2020b; Sinha et al., 2019; Kim et al., 2021), or combinethe advantages of the above two in a hybrid fashion (Liu et al., 2016; Coletta et al., 2019; Zhdanov, 2019;Ash et al., 2020; Shui et al., 2020; Kong et al., 2022; Wang et al., 2022; Settles et al., 2007; Shukla, 2022).",
  "Published in Transactions on Machine Learning Research (12/2024)": "acquisition functions vary depending on the nature of the dataset (balanced / imbalanced). Similarly, thescalability of acquisition functions has also been found to change with dataset (Ji et al., 2023). Additionally,many existing works Mittal et al. (2019); Mayer & Timofte (2020); Beck et al. (2021); Zhang et al. (2024)in the active learning community have implicitly shown in their experiment that the performance of differ-ent acquisition functions differ for different datasets. Our results presented in to andAppendix C.1.1 confirmed this effect. Because the optimal DNN architecture depends on the underlyingdataset, this may also contribute to the observed dependence between the DAL performance and choices ofDNN architecture.",
  "Related Works": "Deep active learning (DAL):DAL research has flourished over the years with the design of variousstrategies to select data from the unlabelled pool.The strategies could be broadly divided intro threecategories. Uncertainty-based strategies seek examples that a DNN is most uncertain about. A variety ofmeasures has been proposed to represent this broadly-defined uncertainty, including entropy (Joshi et al.,2009), BALD (Houlsby et al., 2011; Shelmanov et al., 2021), least confidence based on softmax outputs(Settles, 2009), margin sampling (Scheffer et al., 2001), expected gradient length (Settles et al., 2007; Huanget al., 2016; Zhang et al., 2017; Shukla, 2022), changes in outputs in response to input perturbation(Freytaget al., 2014; Kding et al., 2016), and estimation of DNN loss (Yoo & Kweon, 2019; Huang et al., 2022).Diversity-based strategies seek samples that are representative of the unlabelled data using approaches suchas density clustering (Wang et al., 2017), coreset optimization (Sener & Savarese, 2017; Geifman & El-Yaniv,2017), and leveraging adversarial networks (Zhang et al., 2020b; Sinha et al., 2019; Kim et al., 2021). Hybridstrategies combine these two approaches to sample diverse data which the DNN is most uncertain about (Ashet al., 2020; Shui et al., 2020; Wang et al., 2022; Kong et al., 2022), such as by considering the magnitudeas well as diversity of DNN gradients (Settles et al., 2007; Shukla, 2022). In addition to these pool-based active learning where new training data is obtained by querying an unlabelledpool, there are generative approaches (Zhang et al., 2020b) that generate examples informative to thecurrent model. These approaches leverage generative adversarial network (GAN) to generate informativedata examples that has high entropy (Mayer & Timofte, 2020) or are closer to the decision boundary (Zhu &Bento, 2017) (Mahapatra et al., 2018; Mayer & Timofte, 2020; Zhu & Bento, 2017). Additional approachesinclude utilizing the unlabeled data to pretrain the DNN feature before DAL (Simoni et al., 2021) or usingit in a semi-supervised fashion during DAL (Simoni et al., 2021; Gao et al., 2020), as well as exploring DNNtraining dynamics (Wang et al., 2022) as measured by the derivative of training loss with respect to thenumber of iterations assuming that models training faster generalize better. These developments are seen inboth image as well as text data domains. Most of these existing works were conducted on specific choices of DNN architectures, such as MLPs (Ashet al., 2020), LeNet (Geifman & El-Yaniv, 2017; Hu et al., 2021), CNNs (Gal et al., 2017b), and differentversions of VGG and RESNET (Ash et al., 2020; Shui et al., 2020) on image data, or BERT (Zhang et al.,2020a; Schrder et al., 2021; Wertz et al., 2022) and its two variants DistilBERT (Schrder et al., 2021;Kirk et al., 2022) and RoBERTa (Lu & MacNamee, 2020) on text data. A lack of consistency regardingthe choices of DNN architectures exist across existing studies, and it is not clear how the reported DALevaluations may be dependent on (or agnostic to) the choices of DNN feature extractors, a critical questionthat will be systematically investigated in this paper. Systematic evaluation of DAL methods:An observation emerging in recent works (Mittal et al., 2019;Munjal et al., 2020; Beck et al., 2021) is the inconsistency and reproducibility of the relative performanceof DAL methods across experimental settings. The lack of unified experimental setting, such as size of theinitial labeled pool, acquisition size, total labeling budget, random seeds, batch size, and optimizers havebeen credited for the inconsistencies of results reported (Munjal et al., 2020; Beck et al., 2021). It was furthershown that the gain of DAL over random acquisition is in general marginal compared to other strategies,such as network regularization, data augmentation, and semi-supervised techniques (Munjal et al., 2020;Beck et al., 2021). This paper will add to these findings focusing on the effect of optimizing the architectureof DNN feature extractor on DAL. DNN architecture optimization in DAL:There is a large body of literature in deterministic optimiza-tion or Bayesian inference of DNN architectures (Zoph & Le, 2016; Zoph et al., 2018; Kasim et al., 2020;Feng & Darrell, 2015; Lee et al., 2018; Dikov & Bayer, 2019; KC et al., 2021), supporting the notion thatthe complexity of DNN feature extractors has substantial impact when passively learning from given data.To date, only one work investigated the effect of optimizing DNN architecture in the context of active dataselection as DAL proceeds (Geifman & El-Yaniv, 2018). Specifically, an incremental architectural searchmethod was formulated over a modularly reduced search space customized for RESNET-18, integrated andevaluated with three existing DAL data acquisition strategies. This paper will substantially expand the scope",
  "Methodology": "In this Focus, we investigate how DAL may be affected by the optimization of the architecture of DNNfeature extractors.We consider three approaches to optimize the DNN architecture, namely supervisedjoint-training, supervised pre-training, and unsupervised pre-training. In supervised joint-training, we utilizethe labeled data that are increasingly acquired during DAL. This translates to a setting that is similar to(Geifman & El-Yaniv, 2018), where DNN architecture and weight parameters are simultaneously optimizedas DAL proceeds.In supervised pre-training, we utilize the initial labelled data to optimize the DNNarchitecture prior to active learning. In unsupervised pre-training, we utilize the unlabeled data to optimizeDNN architecture prior to active learning.The pre-trained DNN is then used to initialize DAL duringwhich the DNNs weight parameters are updated while the optimized architecture is kept fixed. This wasmotivated by the recent DAL work that advocated for unsupervised DNN pre-training (Simoni et al., 2021)but on fixed architectures. In all three settings, we consider three representative architecture optimizationapproaches. DNN architecture optimization approach during DAL:We consider three approaches to optimizethe DNN architecture during DAL. In supervised joint-optimization of DNN architectures and data acquisi-tion, at each acquisition round within DAL, we iterated between data selection given the choice of acquisitionfunction, and the optimization of the CNN architectural and weight parameters given the new data. In un-supervised pre-optimization (UPO) of DNN architectures, we adopt the idea from a recent work (Simoniet al., 2021; Caron et al., 2018) that pre-trains a DNN by iteratively clustering all unlabeled data and us-ing the obtained clusters as pseudo-labels to train the DNN. In supervised pre-optimization (SPO) of DNNarchitectures, we use a classification task on initial labelled data to pre-train the DNN. While the originalwork (Simoni et al., 2021; Caron et al., 2018) utilized this to pre-train the weight parameters of the DNN,we use this to simultaneously optimize the architecture and weight parameters of the DNN.",
  ". BADGE sampling (Ash et al., 2020) selects instances that generates diverse but also high gradientmagnitudes in the penultimate layer of the DNN": "7. Unsupervised Pretraining: Based on Simeoni et al. (2021), we adopt a two-step pretraining strategyfor both image and text datasets. This pretraining involves a combination of alternate unsupervisedclustering task and classification task supervised by the clustering labels. We begin with randominitialization of the network parameters and the features from the penultimate layers are clusteredusing k-means clustering.These generated pseudo- labels are then utilized as the ground truthfor a subsequent supervised classification task which in turn updates the network parameters. Thenetworks, once fully trained, serve as the initial models for all subsequent active learning experiments.This strategy is used in combination with all acquisition functions described above. These acquisition functions are the commonly used benchmarks in DAL research. Among them, the firstfour are representative of uncertainty-based acquisition strategies, with Entropy and BALD calculated basedon Bayesian drop-out strategies (Gal et al., 2017b). Coreset is representative of diversity-based strategies,and BADGE is a representative hybrid strategy. Each of these acquisition functions are tested without andwith unsupervised pretraining as the seventh DAL strategy considered. Network architectures:On image data, we consider two convolutional DNN architecture types that aremostly used in DAL literature, each with three different sizes: VGG-11, -16, -19; (Simonyan & Zisserman,2014), and RESNET-18, -34, -50 (He et al., 2016). On text data, we consider three transformer architecturesnamely BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and DistilBERT (Sanh et al., 2019) due totheir prominent use in text based active learning tasks. Details of DNNs are described in the Appendix B. Evaluation metrics:We consider two quantitative metrics: 1) labeling efficiency as described in (Becket al., 2021), which measures the amount of data required in comparison to random acquisition (as a ratio)to achieve the same test accuracy; and 2) a new metric that measures the percentage of gain in test-accuracyover random acquisition at each acquisition round averaged over all acquisition rounds. We use these twometrics to compare the relative performance of the considered acquisition functions across DNN architecturetypes and sizes.",
  "Experiments and Results": "Experiments in Focus 2 were performed on four image datasets including MNIST (Deng, 2012), FashionMNIST (Xiao et al., 2017), CIFAR10 (Krizhevsky, 2009b), and SVHN (Netzer et al., 2011). For Depth-Dropout and BBDropout method, we used CNN with truncation K 20 and 64 filters in each layer. Auniform distribution Up0, 1.1q and Up0, 1.0q was used to initialize the prior of architecture parameters a andb in equation 11 for Depth-Dropout method. The parameters a and b were initialized to 1.3133 and 1.000,respectively, in equation 9 for BBDropout. For PDARTS, we define a total of eight operations (Max pool3x3, Average Pool 3x3, Skip-connect, Identity, Separable Convolution 3x3 and 5x5, Dilated Convolution3x3 and 5x5) which are dropped off with dropout rate of 0.1, 0.4 and 0.7 in the subsequent training steps",
  "size at the same number of labeled data while that of BALD, and Coreset were unchanged. The change inacquisition size did not produce noticeable differences on the results (see Appendix C.4)": "Effect of data augmentation: shows the test accuracy gain over random acquisition achievedby the same six acquisition functions when we applied data augmentation, including horizontal flips andrandom crops, to all experiments. Compared to the results without augmentation, two main differences canbe observed: with data augmentation, 1) most acquisition functions tested exhibited either a substantiallylarger gain over the random acquisition (e.g., MNIST for RESNETs and VGG11), or a reduced error in the",
  "The optimized DNN architecture is then kept fixed while the pre-trained weight parameters are used toinitialize the DNN at each DAL acquisition": "DNN architectures:The computational cost associated with architecture optimization is high, especiallyfor complex architectures such as RESNET and transformers. Existing works in architecture inference oroptimization on transformers are also limited. Therefore we consider CNN as the choice of architecture inFocus 2. DNN architecture optimization:We consider three methods for architecture optimizations, includingtwo Bayesian inference methods and one NAS method, considering mainly the computational feasibility inincluding these methods in DAL. For NAS, we follow the PDARTS method described in (Chen et al., 2019) that defines an over-completenetwork with L cells each with N nodes. Each node signifies a feature layer and two nodes are connectedby operations o P O. We define each subsequent node xj as the linear combination of operations on node xidefined by architecture parameter i,j.:",
  "hl ppWl d hl1q zlq ` hl1(7)": "where hl is the feature map of the lth hidden layer, Wl is the weight matrix for CNN filters in the lth layer,zl is the activation mask for lth layer, d is a convolution operation, and p.q is an activation function. Wedefine a beta process as a prior over the number of hidden layers. A beta process sample can be denoted asphl, lq, where l P r0, 1s denotes the activation probability of a hidden layer function hl. A stick-breakingconstruction of the beta process can be represented as:",
  "m1qpkqqpzm|mq(12)": "where we use Kumaraswamy distribution (Kumaraswamy, 1980) for qpmq and continuous relaxation ofBernoulli distribution for qpzm|mq (Lee et al., 2018; Maddison et al., 2016; Jang et al., 2016; Gal et al.,2017a) . The distinction lies in the utilization of a beta process and its corresponding Bernoulli process independentlyby DepthDropout, enabling the inference of both the number of layers and nodes per layer. On the otherhand, BBDropout marginalizes the beta process, inferring the number of nodes in each layer but not thedepth. For both DepthDropout and BBDropout, the evidence lower bound (ELBO) of the marginal likelihoodof observed data D can be derived and optimized via structured stochastic variational inference (SSVI) asdescribed in (KC et al., 2021).",
  ": Visualization of optimized CNN architecture as DAL proceeds using Depth-Dropout. A: CIFAR10;B: FashionMNIST": "as used in Chen et al. (2019). All the models were trained for 500 epochs. Experiments were performedon workstations with RTX 2080Ti GPU and 32 GB of RAM as well as P8 and V100 GPU provided byResearch Computing at Rochester Institute of Technology (Rochester Institute of Technology, 2024). Theactive learning experiments are run on four image dataset in same setting as described in 3.2. The effect of optimized DNN architectures: summarizes the DAL performance across differentacquisition functions when applied to architecture optimization Depth-Dropout (A), BBDropout (B), andPDARTs(C) and fixed DNN architectures with 1, 5, and 9 convolutional layers (between input and outputlayers of the network) on CIFAR10 and FashionMNIST. The fixed architectures are used as baselines tocompare with the networks with optimized architectures. The DNN architectures were optimized eitherjointly or during pre-training using both supervised and unsupervised methods. Complete results on the restof the datasets can be found in Appendix D.1. The figure depict that the optimized networks, particularlythose optimized jointly or through supervised pre-optimization (SPO) approach, consistently outperformfixed pre-defined networks. SPO and joint optimization approaches, in general, showed consistent gains in allarchitecture optimization methods (i.e. A, B and C) in comparison to unsupervised pre-optimization(UPO) approach which performed least favorably. This may be attributed to the fact that the networkoptimization with unlabelled data was based on a clustering task different for primary DAL task. In case of joint optimization approach, the continuous change in the size of labeled data during DAL resultedin changes in optimal DNN architecture. This, in turn, induces continuous modifications to the optimalarchitecture, complicating the joint optimization with data selection. To corroborate this, we experimentedwith more extreme changes of data size by considering an initial size of 200 and acquisition size of 100 in thesame experiments. (A) and (B) shows the optimized architecture using Depth Dropout as DALproceeds for CIFAR10 and FashionMNIST respectively, where the bar graph on top indicates the probabilityof activation of a layer and the column below indicates the activation of filters in each layer. As shown,both the depth and width of the CNN increased as DAL proceeded with adding labeled data. These resultssuggested that the joint optimization approach via small and growing labeled data in addition to weightoptimization and active learning may complicate optimal data acquisition due to the continued change inthe architecture, whereas leveraging initial labeled data to pre-optimize the feature space may offer a simplerand yet more competitive solution. The limited performance with unsupervised pre-training using PDARTS may be attributed to the sensitivityof the unsupervised labeling task to the number of clusters used for generating pseudo-labels. Throughour experiments, we observed that employing a larger cluster size with PDARTS resulted in an optimizednetwork with minimal test accuracy p 30%q, which improved as the number of clusters approached theactual number of labels in the dataset. In contrast, supervised pre-training of PDARTS with an initially",
  "labeled dataset yielded a significantly superior optimized network compared to the unsupervised approach.This suggests that unsupervised pre-optimization of DNN architectures may be difficult": "Relative contribution of architecture versus data optimization:The shade or spread in describe the spread of performance between different acquisition functions for different architecture opti-mizations. A closer look into suggest that, with the exception of UPO on PDARTS, the performancespread across different acquisition functions is reduced using an optimized DNN architecture compared tofixed pre-defined networks. further summarizes the spread of performance across all optimizedor fixed DNN architectures, for each given acquisition function on CIFAR10 and FashionMNIST. Completeresults on the rest of the datasets can be found in Appendix D.2. As shown, different choices of architectureparameters induced a large performance gap of the DNN at any given data size for any acquisition functionused. This performance spread changed as DAL proceeded, although the trend of change was not consistentamong datasets: on FashionMNIST, the gap among different CNN architectures appeared to be larger atthe earlier stages of DAL when the data size was smaller, whereas on CIFAR this gap appeared to increaseas DAL proceeded Contrasting with , it is evident that the impact of the DNN architecturessubstantially outweighed and even reduced the impact of acquisition strategies. This further suggests that,when the labeling budget is small, the effort to identify optimal architecture of DNN feature extractor maybe critical in order to maximize the efficacy of active data selection.",
  "Relation between DAL acquisition, DNN architecture, and decision boundaries": "What may explain the observed interdependence between the DNN architecture and data acquisition?(Kolossov et al., 2023) showed in their work that a better performing model in general is not always anideal choice during active learning.Recent studies (Mickisch et al., 2020; Lei et al., 2023) showed thatthe task decision boundary of a network changes continuously during training to generalize to the availabledata. (Lei et al., 2023) showed that the variability in the decision boundary of network inversely affects thegeneralization and reproducibility of the results. Furthermore, when we consider different network archi-tectures to train on the same data, the decision boundary appears to visibly vary (Somepalli et al., 2022).Illustration of this is shown in where the class decision boundary of six architectures is plottedon the plane spanning three randomly selected images (Plane, Frog and Bird) of CIFAR-10. We furtherextended the visualization in to include different acquisition functions shown for FashionMNISTin . Complete results on remaining image datasets can be found in Appendix C.2. The decisionboundary appeared to change with both acquisition function as well as the underlying network architec-ture considered. The change in decision boundary appeared more prominent with change in the networkarchitecture in comparison to the change in acquisition function. To further understand the effect of change in decision boundary on data acquisition, we added a simpleexperiment on half moon dataset with MLP (Multi Layer Perceptron) architectures of varying sizes (1,2",
  "Effect of the dataset on data acquisitions": "Additionally, though not explicitly mentioned, different dataset differ from each other owing to the complex-ity of the dataset which includes underlying structure of dataset, dimensionality, noise, redundancy in data,decision boundary complexity, etc. These affect several factors that are important to the design of acquisitionfunctions, including the uncertainty of a DNN that will affect uncertainty-based acquisition functions, thediversity of the data samples that will affect diversity-based acquisition functions, and the decision boundarywhich will affect all acquisition functions. Recent work (Kim et al., 2021) has shown that the ranking of",
  "Limitations and Future work": "Investigations in Focus 2 of the current study is focused on relatively small CNN based architecture op-timization. To further generalize the findings, future studies need to extend to larger overparameterizedarchitectures such as RESNET, VGG, and transformers. Such extension will provide a more comprehensiveunderstanding of how optimization of different network architectures influence data optimization, provid-ing insights into the applicability of joint architecture and data optimization strategies across a variety ofDNN types. Additionally, current work is focused on relatively smaller datasets like MNIST, FashionM-NIST, SVHN and CIFAR10. Broadening the scope of datasets to include larger datasets like CIFAR100(Krizhevsky, 2009a), Imagenet (Deng et al., 2009), CelebA (Liu et al., 2015), etc will help generalizing theobservations across more complex datasets space. There is also room for incorporating additional data acquisition strategies in the presented study. The currentstudy examined a range of acquisition functions, covering prevalent strategies representative of uncertainty-based, diversity-based, and hybrid strategies found in the existing literature of DAL. Future work canbroaden the spectrum of acquisition strategies to more recent strategies, such as those incorporating neuraltangent kernels to assess DNN training dynamics (Wang et al., 2022), and those incorporating the conceptof semi-supervised learning in DAL. In terms of approaches for optimizing DNN architectures, we considered a cell-based (NAS) method, specifi-cally PDARTs, along with two Bayesian DNN architecture inference methods. For CNN networks consideredin the Focus 2 of this study, the connections between operations within a search cell exhibit a large influ-ence on the architectures performance. However, cell-based NAS methods are not directly transferrable totransformers that operate on attention mechanism rather than convolution. Future work will incorporateNAS methods for transformers, such as AdaBERT and NAS-BERTs, to further test the generalizability ofthe findings obtained in Focus 2 regarding the effect of DNN architecture optimization on DAL. Future workcan also include additional NAS methods for CNN-based architecture, such as dynamic-exploration DARTswhere the dynamic architecture varies the kernel size or network path of CNN according to input data. Finally, to investigate potential strategies for simultaneously optimizing DNN architecture and data selection,we considered unsupervised pre-optimizaiton of DNN artechitecture prior to DAL, versus supervised joint-optimization of DNN during DAL. Both approaches, however, essentially considered DNN architecture anddata optimization as two separate optimization problems with their respective objective functions.Aninteresting yet much more challenging future research direction may be the developments of joint DNNarchitecture and data optimization theories, methods, and algorithms that integrate these two optimizationobjectives in a more coherent formulation.",
  "Conclusion": "In this work, we examine the influence of DNN architectures on optimal data selection in DAL. We showthat the choices of DNN architecture substantially influence and outweigh data optimization in DAL, andthat its optimization helps increase the benefits of active data selection, with supervised pre-optimizationbeing most beneficial followed by joint optimization. We hope that the findings help inform the researchcommunity in improving the reproducibility of DAL evaluations by taking into account the important roleof DNN architecture choices in DAL, and in opening up new research avenues that better integrate DNN",
  "This work is supported by the National Science Foundation funding NSF OAC-2212548 and the NSF awardno. 2045804": "Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batchactive learning by diverse, uncertain gradient lower bounds. In International Conference on LearningRepresentations, 2020. URL Sren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. Dbpe-dia: A nucleus for a web of open data. In international semantic web conference, pp. 722735. Springer,2007. Nathan Beck, Durga Sivasubramanian, Apurva Dani, Ganesh Ramakrishnan, and Rishabh Iyer. Effectiveevaluation of deep active learning on image classification tasks. arXiv preprint arXiv:2106.15324, 2021. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervisedlearning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp.132149, 2018.",
  "Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv preprintarXiv:1611.01144, 2016": "Yilin Ji, Daniel Kaestner, Oliver Wirth, and Christian Wressnegger. Randomness is the root of all evil:more reliable evaluation of deep active learning. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pp. 39433952, 2023. Ajay J Joshi, Fatih Porikli, and Nikolaos Papanikolopoulos. Multi-class active learning for image classifi-cation. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 23722379. IEEE,2009. Christoph Kding, Erik Rodner, Alexander Freytag, and Joachim Denzler. Active and continuous explorationwith deep neural networks and expected model output changes. arXiv preprint arXiv:1612.06129, 2016.",
  "Kishan KC, Rui Li, and MohammadMahdi Gilany. Joint inference for neural network depth and dropoutregularization. Advances in Neural Information Processing Systems, 34, 2021": "Kwanyoung Kim, Dongwon Park, Kwang In Kim, and Se Young Chun. Task-aware variational adversarialactive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 81668175, 2021. Hannah Rose Kirk, Bertie Vidgen, and Scott A Hale.Is more data better?re-thinking the impor-tance of efficiency in abusive language detection with transformers-based active learning. arXiv preprintarXiv:2209.10193, 2022.",
  "Ya Li, Keze Wang, Lin Nie, and Qing Wang. Face recognition via heuristic deep active learning. In ChineseConference on Biometric Recognition, pp. 97107. Springer, 2017": "Peng Liu, Hui Zhang, and Kie B Eom. Active deep learning for classification of hyperspectral images. IEEEJournal of Selected Topics in Applied Earth Observations and Remote Sensing, 10(2):712724, 2016. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019.",
  "Burr Settles, Mark Craven, and Soumya Ray. Multiple-instance active learning. Advances in neural infor-mation processing systems, 20:12891296, 2007": "Artem Shelmanov, Dmitri Puzyrev, Lyubov Kupriyanova, Denis Belyakov, Daniil Larionov, Nikita Khro-mov, Olga Kozlova, Ekaterina Artemova, Dmitry V Dylov, and Alexander Panchenko. Active learningfor sequence tagging with deep pre-trained models and bayesian uncertainty estimates. arXiv preprintarXiv:2101.08133, 2021. Changjian Shui, Fan Zhou, Christian Gagn, and Boyu Wang. Deep active learning: Unified and principledmethod for query and training. In International Conference on Artificial Intelligence and Statistics, pp.13081318. PMLR, 2020. Megh Shukla. Bayesian uncertainty and expected gradient length-regression: Two sides of the same coin?In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 23672376,2022.",
  "Alaa Tharwat and Wolfram Schenck. A survey on active learning: State-of-the-art, practical challenges andresearch directions. Mathematics, 11(4):820, 2023": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.Glue:A multi-task benchmark and analysis platform for natural language understanding.arXiv preprintarXiv:1804.07461, 2018. Haonan Wang, Wei Huang, Ziwei Wu, Hanghang Tong, Andrew J Margenot, and Jingrui He. Deep activelearning by leveraging training dynamics. Advances in Neural Information Processing Systems, 35:2517125184, 2022. Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for deepimage classification. IEEE Transactions on Circuits and Systems for Video Technology, 27(12):25912600,2016.",
  "C.1.1Ranking of Acquisition function": ": Average ranking plot for acquisition functions for image datasets averaged across six networks(RESNET 18/34/50 and VGG 11/16/19). The ranking of different acquisition functions vary with datasetconsidered : Average ranking plot for acquisition functions for text datasets averaged across three networks(BERT, ROBERTA, DISTILBERT). The ranking of different acquisition functions vary with dataset con-sidered",
  "C.5.1Image Dataset": ": Labeling efficiency over random acquisition, for all six acquisition function with and withoutunsupervised pretraining (represented with suffix \"pre\" after the name) along the columns across four datasets(A - CIFAR, B - FashionMNIST, C - MNIST, D - SVHN) and six DNN architectures along each row.Numerical values for labeling efficiency comparison for image dataset in",
  "C.5.2Text Dataset": ": Labeling efficiency over random acquisition, for all six acquisition function with and withoutunsupervised pretraining (represented with suffix \"pre\" after the name) along the columns across four datasets(A - AGNEWS, B - BANKS77, C - DBPEDIA, D - QNLI) and three DNN architectures along each row.Numerical values for labeling efficiency comparison for text dataset in",
  "D.1Spread of performance across different acquisition function": ": Spread of performance across different acquisition functions given pre-defined (fixed) CNN ar-chitecture, or jointly-optimized, supervised pre-optimized (SPO), unsupervised pre-optimized (UPO) usingDepth-Dropout (A), BBDropout(B) and PDARTs(C) for MNIST (top row) and SVHN (bottom row) . Opti-mization of the DNN architecture, either jointly or during pre-training, in general improved over pre-definedCNN performance."
}