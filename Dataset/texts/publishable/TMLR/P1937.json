{
  "Abstract": "In unsupervised representation learning, models aim to distill essential features from high-dimensional data into lower-dimensional learned representations, guided by inductive biases.Understanding the characteristics that make a good representation remains a topic of ongoingresearch.Disentanglement of independent generative processes has long been creditedwith producing high-quality representations. However, focusing solely on representationsthat adhere to the stringent requirements of most disentanglement metrics, may result inoverlooking many high-quality representations, well suited for various downstream tasks.These metrics often demand that generative factors be encoded in distinct, single dimensionsaligned with the canonical basis of the representation space. Motivated by these observations, we propose two novel metrics: Importance-Weighted Orthog-onality (IWO) and Importance-Weighted Rank (IWR). These metrics evaluate the mutualorthogonality and rank of generative factor subspaces. Throughout extensive experiments oncommon downstream tasks, over several benchmark datasets and models, IWO and IWRconsistently show stronger correlations with downstream task performance than traditionaldisentanglement metrics. Our findings suggest that representation quality is closer relatedto the orthogonality of independent generative processes rather than their disentanglement,offering a new direction for evaluating and improving unsupervised learning models.",
  "Introduction": "Humans are able to process rich data such as high-resolution images to distill and memorize key informationabout potentially complex concepts. Similarly, representation learning aims to devise a procedure, oftenunsupervised, to encode potentially high-dimensional data into a lower-dimensional learned embedding space,such that classifiers or other predictors can easily extract information from the learned representations (Bengioet al., 2013). Understanding how to generate these convenient representations, requires the definition ofdesirable properties that representation learning models should enforce. In the domain of generative models,",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Individual correlations between metrics and downstream task performance. Each model is trainedon six different datasets. For each dataset, six different hyperparameters on 10 random seeds are trained fora total of 2160 learned representations. Correlations between downstream task performance and the threecommonly used metrics DCI-D, DCI-C and MIG together with IWO and IWR are calculated. Correlationsare calculated between averaged task performance and averaged metrics, where the average is taken over theseeds and grouped by hyperparameter. As such, we assess the capability of the metrics to point us to goodmodels and hyperparameters.",
  "the ability to disentangle the explanatory factors underlying the data has long been credited to be such adesirable property": "In the disentangled representation learning framework, data x is often assumed to be generated by anunderlying function g driven by ground truth, generative factors {zj}Kj=1 and other variability factors t, thatis x = g(z, t). A model then learns a mapping c = r(x) RL from the data to a latent representation space.A common characterization of disentanglement posits that the generative factors are represented by singledistinct components of c, implying that they manifest as orthogonal 1-d latent subspaces aligned with thecanonical basis within the latent representation, up to a scaling factor and irrelevant latent dimensions (i.e.,when L > K). Disentangled representations have played a pivotal role in improving the performance of tasks such asclassification (Zhou et al., 2022), segmentation (Kalkhof et al., 2022), registration Han et al. (2020), image-to-image translation (Fei et al., 2021), artifact reduction (Tang et al., 2022), domain adaption (Li et al.,2021), controllable synthesis (Kelkar & Anastasio, 2022) and disease decomposition (Couronn et al., 2021).However, while disentanglement represents an intuitive and useful notion to characterize good representations,its general applicability is limited. Indeed, Locatello et al. (2019) prove, by using orthogonal transformationsand probability (inverse) integral transforms, that unsupervised disentanglement learning is fundamentallyimpossible, without inductive biases on both models and datasets. In addition, the authors show thatdisentanglement exhibits weak correlation with the suitability of representations for common downstreamtasks. We believe that this weak correlation can be attributed to the stringent nature of disentanglementmeasures, which penalize many high-quality representations otherwise suitable for downstream tasks. To get an intuition of the problem, visualizes 2-d representation spaces encoding circles varying insize and color. While in space (i) and (ii) the dimensions encoding the generative factors are orthogonal toone another, in space (iii) they exhibit strong correlation. For the downstream task of regressing size andcolor, most models find the orthogonal spaces (i) and (ii) better suited than (iii), as they properly encode allsize and color combinations, while space (iii) fails to encode the largest, lightest and the smallest, darkestcircles. However, according to standard disentanglement metrics, space (ii) is completely entangled (worstcase), while space (iii) exhibits a better disentanglement, due to the requirement that the generative factorsmust align with the canonical basis of the representation space. The disentanglement properties of theserepresentations therefore stands in contrast to their downstream task utility. While disentangled representations can be well-suited for many downstream tasks, entangled but orthogonalrepresentations can be equally effective. In line with Wang & Isola (2020), we believe that correlation with",
  "relevant downstream tasks is a necessary condition for any metric measuring representation utility. Theabsence of this correlation is a significant weakness of common disentanglement metrics": "We therefore propose two new metrics: Importance-Weighted Orthogonality (IWO) and Importance-WeightedRank (IWR). To compute these metrics, we devise a novel methodology, Generative Component Analysis(GCA), that identifies the weighted vector subspaces where generative factors vary. IWO then measures themutual weighted orthogonality between the subspaces found through GCA, while IWR assesses their weightedrank. Intuitively, IWO can be thought of as an expansion of cosine similarity to general vector subspaces. We empirically assess the validity of the proposed metric in various synthetic experiments and by analysing theperformance of three common downstream tasks across six benchmark datasets and six widely used models,showing that IWO and IWR consistently display stronger correlations with downstream task performancethan popular disentanglement metrics such as MIG (Chen et al., 2018) or DCI (Eastwood & Williams,2018). Our research suggests that the utility of a representation may be closer related to its orthogonalitythan its disentanglement.",
  "Related Work": "Alongside the task of disentanglement, gauging a models performance in disentangling a representationhas emerged as a non-trivial problem. Beyond visual inspection of the results, a variety of quantitativemethodologies have been developed to tackle this issue. Higgins et al. (2017) propose to measure the accuracyof a classifier predicting the position of a fixed generative factor. Kim & Mnih (2018) further robustify themetric by proposing a majority voting scheme related to the least-variance factors in the representations.Chen et al. (2018) introduce the Mutual Information Gap estimating the normalized difference of the mutualinformation between the two highest factors of the representation vector. Eastwood & Williams (2018)propose the DCI metrics to evaluate the correlation between the representation and the generative factors.For each of them, one linear regressor is trained and the entropy over the rows (Disentanglement) and thecolumns (Completeness) is computed, along with the error (Informativeness) achieved by each regressor.The Modularity metric introduced by Ridgeway & Mozer (2018) computes the mutual information of eachcomponent of the representation to estimate its dependency with at most one factor of variation. SAP score(Kumar et al., 2018) estimates the difference, on average, of the two most predictive latent components foreach factor. The use of metrics such as the aforementioned ones contributed to shaping several definitions of disentangle-ment, each encoding a somewhat different aspect of disentangled representations, which led to a fragmentationof definitions (Locatello et al., 2019). Higgins et al. (2018) attempt instead to propose a unified view ofthe disentanglement problem, by defining a principled symmetry-based disentanglement framework, drawnfrom group representation theory. The authors established disentanglement in terms of a morphism fromworld states to decomposable latent representations, equivariant with respect to decomposable symmetriesacting on the states/representations. For a representation to be disentangled, each symmetry group must actonly on a corresponding (multidimensional) subspace of the representation. Following this conceptualization,Caselles-Dupr et al. (2019) demonstrate the learnability of such representations, provided the actions andthe transitions between the states. Painter et al. (2020) extend the work by proposing a reinforcementlearning pipeline to learn without the need for supervision. Noteworthy are the two proposed metrics: (i) anindependence score that, similarly to our work, estimates the orthogonality between the generative factors inthe fashion of a canonical correlation analysis; (ii) a factor leakage score, extended from the MIG metric toaccount for all the factors. Tonnaer et al. (2022) formalize the evaluation in the symmetry-based setting andproposed a principled metric that quantifies the disentanglement by estimating and applying the inverse groupelements to retrieve an untransformed reference representation. A dispersion measure of such representationsis then computed. Note that while most works focus on the linear manipulation of the latent subspace,the symmetry-based framework can also be used in non-linear cases. However, it requires modelling thesymmetries and the group actions, a challenging task in scenarios with no clear underlying group structure(Tonnaer et al., 2022). We aim to develop a metric which does not require such a group structure and worksin more general cases.",
  "Methodology": "Our goal is to establish a metric that quantifies the total orthogonality of a representation. This involvesestimating the orthogonality between the latent subspaces corresponding to each generative factor. However,this raises two challenges. First, the identification of the latent subspaces is not straightforward since the generative factor can exhibitnon-linear behaviour with respect to the learned representation. We propose Generative Component Analysis(GCA), a procedure where, for each generative factor, multiple non-linear regressors are used to identifyprogressively smaller subspaces where most of the generative factors information resides. These subspacesare then used to identify the generative components, that is, the set of orthonormal vectors spanning thelatent subspace. In a similar fashion to procedures such as linear principal component regression, we weighteach vector by a corresponding importance score to obtain an importance-ordered orthogonal (i.o.o.) basis,for each generative factor. Second, prominent methods for measuring orthogonality between subspaces do not provide sufficientlydiscriminative results for our use case. The conventional definition of orthogonal complement is binaryand too restrictive for nuanced applications. Conversely, methods like canonical correlation analysis, whichdetermines the similarity between vector spaces, typically operate under optimal conditions and may assignhigh scores even to spaces sharing only a single dimension. These approaches, while useful, tend to be overlypermissive for our specific objectives. We therefore propose Importance-Weighted Orthogonality (IWO) tocharacterize the orthogonality between generative factors. This is done by computing an average weightedprojection of each generative factors latent subspace onto all the others. To further discriminate betweenrepresentations with similar orthogonality properties, we also compute Importance-Weighted Rank (IWR),which estimates the spread of the importance weights, awarding representations that give more importanceto a few dimensions.",
  "Generative Component Analysis (GCA)": "Consider a latent representation or code, c RL, encoding the generative factors (z1, . . . , zK) RK, withL K. The generative factors are inducded through zj = f j (c), where f j is a potentially non-linearfunction. However, not all changes in c imply a change in zj. In particular, we define the invariant latentsubspace of zj to be the largest linear subspace Ij RL, such that f j (c + v) = f j (c), v Ij. Accordingly,the variant latent subspace (simply latent subspace) of zj is defined to be the orthogonal complement ofIj and will be denoted as Sj, with dimensionality Rj. In the next paragraph, we describe how to find animportance-weighted basis for Sj.",
  "Ll = Ec [(fjl(wl(c)), zj(c))] ,(1)": "where is a specific loss term. In particular, note that Ll1 Ll because of the potential information lossdue to dimensionality reduction. Let us now quantify the loss increment by each of the LNN projections asLl = Ll1 Ll. We define Rj as the smallest dimensionality at which the lowest achievable loss is reached,that is, Ll = 0 for l > Rj. For l = 1, we compute L1 as the difference between L1 and a baseline lossL0 = Ezj(Ezj [zj] , zj). Basis generationUsing the trained projection matrices WL1, . . . , W1, along with the layer-specific lossdifferences Ll, we now describe how to construct an i.o.o. basis spanning zjs latent subspace Sj. Formally,we want to devise a set of orthonormal vectors Bj = {b(j)l RL | l = 1, . . . , Rj}, along with their respectiveimportance weights {(j)l R>0 | l = 1, . . . , Rj}. In the following, we drop the superscript (j) to ease thenotation. Each projection matrix Wl, l = 1, . . . , L 1 eliminates a dimension from the data representation. Theadopted training methodology ensures that at each step the dimension discarded is the least important forregressing zj among the remaining ones. This is in turn based on which removed dimension results in the",
  "minimum increase of Ll+1. The removed dimension represents the 1-d null space of its correspondingprojection matrix": "We begin by finding the basis vector bL, corresponding to the least important dimension, by identifyingthe null space of matrix WL1 R(L1)L. For each subsequent dimension bl+1 RL, l = 1, . . . , L 2, weiteratively compute the composed projection matrix Wl = Wl WL1 RlL. The null space ker( Wl)contains the target dimension bl+1 along with the previously found basis vectors bl+2, . . . , bL. We retrievebl+1 by finding the dimension in the null space of Wl that is also orthogonal to the previously identified basisvectors, for example through QR decomposition. Finally, the remaining dimension b1, corresponding to themost significant basis vector, can be directly retrieved by computing W1 R1L and normalizing it. Thisprocess is depicted in , and a pseudocode implementation can be found in Appendix E.",
  "corresponding importance weights (j)1 , . . . , (j)Rj": "GCA allocates an i.o.o. basis for each generative factor of a learned representation. However, when comparingrepresentations, we also have to account for differences in LRj, as this loss corresponds to the best possibleregression of zj from the representation. In the DCI framework, this aspect is captured by the Informativenessmetric. In order not to favour representations with low Rj and high LRj over those with low LRj and higherRj, we adjust the importance weights of any factor zj whose LRj > 0. To do that, we first complete thefactors basis Bj to span the whole latent space, then we distribute the loss LRj among the importance ofthe basis vectors equally:",
  "Importance Weighted Orthogonality (IWO)": "Orthogonality is commonly treated as a dichotomous attribute; that is, vectors are classified as eitherorthogonal or non-orthogonal, and similarly, a subspace is considered to either reside in the orthogonalcomplement of another or not. The concept of cosine similarity provides a continuous measure of the degreeof orthogonality between two vectors. Analogously, we aim at a continuous measure that evaluates thedegree of orthogonality between two subspaces. Consider two orthonormal bases Bj = {b(j)1 , . . . , b(j)Rj} and",
  "l=1r(jk)l.(5)": "O(Sj, Sk) with its maximum reached if Sj is a subspace of Sk or vice-versa, and its minimum reachedwhen Sk lies in the orthogonal complement of Sj. This definition of orthogonality can be interpreted as theaverage squared cosine similarity between any vector pair from Sj and Sk. This formulation of orthogonality ignores the importance of dimensions within subspaces Sj and Sk. Toillustrate why this is problematic, consider two generative factors, zj and zk, sharing the same subspace S. If",
  "l=1r(jk)l.(7)": "Note that, with respect to Equation 5, IWO does not require a normalization factor as the square rootin Equation 6 guarantees that IWO . In addition, we subtract from 1 to simplify the comparisonwith standard disentanglement metrics. Therefore, the maximum of 1 is reached if zj lies in the orthogonalcomplement of zk and vice versa. The minimum of 0 is reached when zj and zk, in addition to lying in thesame subspace, also share the same importance along the same dimensions (cf. Appendix B for the proofs).Both Orthogonality and IWO can be efficiently calculated using matrix operations (cf. Appendix D).",
  "IWR(zj) = 1 H(j),(8)": "where H(j) = Rjl=1 (j)llogL((j)l ) and we always assume L > 1. IWR thus measures how the importanceis distributed among the L dimensions of the representation space. Note that, IWR(zj) = 0 if the importanceis distributed equally along all L dimensions spanning the representation space, while IWR(zj) = 1 if theimportance is concentrated in a single dimension only (cf. Appendix B for the proofs). We denote the meanover all generative factors of IWO and IWR as IWO and IWR.",
  "Experiments": "To evaluate the effectiveness of our IWO and IWR implementations, we first test whether we can (i) recoverthe true latent subspaces using GCA and (ii) correctly assess their orthogonality and importance spread. Forthat purpose, we set up a synthetic data generation scheme, providing us with the ground truth IWO andIWR values. For comparison, we also test how other metrics capture different aspects of the representation.In particular, Disentanglement, Completeness, Informativeness and Explicitness, as measured by the DCI-ESframework using its official implementation. We further evaluate IWO/IWR through the disentanglement_lib framework (Locatello et al., 2019) andmeasure how strong they correlate with downstream task performance for three different tasks deployedon the learned representations of six widely used variational autoencoder models trained on six benchmarkdisentanglement datasets and over a wide range of seeds and hyperparameters. More details are listed in theappendix and in our open-source code implementation1. Training of all neural networks in the LNN is performed in parallel. In principle, the gradient flow from eachindividual regressor fjl can be stopped after the corresponding Wl, however, we attested a faster convergence",
  "(5) High dimensional latent space + poly./trig. mapping": "50 5 Poly.0.980.580.980.750.990.5750 5 Poly.0.110.050.980.750.990.56100 5 Poly.0.980.640.970.740.980.63100 5 Poly.0.090.040.980.730.980.63250 5 Poly.0.970.670.970.720.980.68250 5 Poly.0.090.040.970.720.980.68 when letting the gradient of each fjl flow back up to c. The aim of simultaneously exploring all nestedsubspaces is to facilitate the identification of the smallest subspaces by guidance through the larger ones.Additionally, we assume a reduction factor of 1, so that Wl Rl(l+1) for l = 1, . . . , L, however, higher valuescan also be considered for larger representations. Further speed-up techniques are presented in Appendix I.",
  "Synthetic Experiments": "We introduce a synthetic data generating scheme, which generates vectors of i.i.d Gaussian distributed latentrepresentations c RL. Then, on the basis of the latent representations, we synthesize K generative factors{z1, . . . , zK}. For simplicity, we choose L as a multiple of K. For simulating a disentangled latent space, we define each zj to be linearly dependent on a single, distinctelement of c. To assess higher dimensional cases with non-linear relationships, we consider a non-linearcommutative mapping f : RRj R.In particular, we experiment with a polynomial (Poly.)and atrigonometric (Trig.) f. Notice that the commutativity enforces that the distribution is spread evenly acrossall dimensions, such that we can easily assess the performance of IWR. For simplicity, we always set Rj = Rfor all j = 1, . . . , K. Together, L (latent space dimension), K (number of factors) and R (latent subspacedimension) determine how many dimensions each generative factor shares with the others. We consider theshared dimensions to be contiguous. For more details about the synthetic data generation scheme, refer to and the pseudocode in Appendix E. To test representations that are not aligned with the canonical basis, we apply Random Orthogonal Projections(ROP) R RLL to c. In line with commonly used datasets such as Cars3D or dSprites, we rescale andquantize zj to the range . All experiments are run four times with differing random seeds. The standarddeviation was smaller than 0.02 for all reported values. The results are displayed in . (1) Permutation and additive Gaussian noiseWe define two setups, both with L = K = 5 and R = 1.First, we let z be a mere permutation of c, second we let z be c + , with N(0, 0.01) being Gaussiannoise. As expected, under conditions of low-complexity and (quasi-)perfect disentanglement we obtain highscores for all the metrics.",
  "Downstream Experiments": "For the systematic evaluation of IWOs and IWRs correlation with downstream task performance, we usethe disentanglement_lib framework (Locatello et al., 2019). We consider six benchmark datasets, namelydSprites, Color dSprites and Scream dSprites (Matthey et al., 2017), Cars3D (Reed et al., 2015),smallNORB (LeCun et al., 2004) and Shapes3D (Burgess & Kim, 2018). These datasets cover a widerange of complexities and variations, for more information on the individual datasets refer to Appendix F.2. On each of these datasets, six different commonly used Variational Auto Encoder (VAE) models are trained:-VAE Higgins et al. (2017), Annealed VAE (Burgess et al., 2018), -TCVAE (Chen et al., 2018) Factor-VAE (Kim & Mnih, 2018), DIP-VAE-I and DIP-VAE-II (Kumar et al., 2018). These models represent adiverse set of approaches to disentanglement, each introducing unique mechanisms to encourage factorizedrepresentations. For each model, we consider six different regularization strengths (cf. Appendix F.1), eachwith ten different random seeds, resulting in a total of 2160 learned representations. The representations are evaluated for their utility in regressing the generative factors. To this end, we considerthree distinct downstream models trained on the learned representations: (i) random forest, (ii) logisticregression, and (iii) multi-layer perceptron. Correlations between downstream task performance and thecommonly used metrics, DCI-D, DCI-C, and MIG, together with IWOs and IWR, are calculated throughoutthe considered regularization strengths. Our main objective is to investigate whether the orthogonality of a representation is indicative of theperformance on a variety of downstream tasks and thus a good metric for the utility or quality of therepresentation, as elucidated by Wang & Isola (2020). The primary distinction between Orthogonality andDisentanglement, as measured by the considered metrics, is that the former does not require alignment withthe canonical basis of the representation space. The three downstream models are chosen to distill this key",
  "Cars3d-0.12-0.36-0.390.560.63Color dSprites-0.25-0.48-0.26-0.25-0.77dSprites0.180.170.130.350.08Scream dSprites0.180.010.230.590.78Shapes3D-0.38-0.51-0.660.600.05smallNORB0.840.520.620.770.92": "difference. While we expect alignment with the canonical basis to benefit downstream task (i) Random forest,we do not expect such a benefit for tasks (ii) logistic regression or (iii) multi-layer perceptron. In a-2b, we present the results aggregated by model and dataset respectively. We refer the reader tothe complete results in in Appendix F for a detailed breakdown of all experiments and their outcomes. serves as an illustrative example, visualizing the comparison between the generative componentsas identified by GCA and the dimensions found through the DCI framework for one of the 2160 consideredrepresentation spaces. (1) Random forestHigh correlations are present for all models and datasets except for Cars3D, wherethe DCI and MIG metrics fail to correlate entirely. However, DCI-D correlates more reliably for the otherdatasets and models with the downstream task than IWO does. This can be attributed to the nature ofrandom forests in defining axis-aligned discriminative rules. Therefore, the downstream task benefits fromthe alignment of the generative factors with the canonical basis. Indeed a random forest might have a hardertime fitting latent space (ii) from , than fitting latent space (iii), as in the latter, one generative factoraligns with the canonical basis. However, we also notice that DCI-C and IWR correlate even stronger thanDCI-D and IWO do. This suggests that random forests benefit even more significantly from low-dimensionalrepresentations of generative factors than from their alignment with the canonical basis. (2) Logistic regressionFor logistic regression, where 2-regularization is applied to the weights, there isno reason to assume that alignment between generative factors and canonical basis should lead to higherdownstream task performance. Indeed, we can see that IWO and IWR correlate higher and more reliablythan DCI or MIG do. This leads us to assume that measuring the orthogonality detached from canonicalbasis alignment provides a better metric for the evaluation of downstream logistic regression.",
  "Discussion and Conclusions": "In our investigation, we pivoted from the conventional focus on the disentanglement of generative factors to anovel examination of their orthogonality. Our approach, geared towards accommodating non-linear behaviorswithin linear subspaces tied to generative factors, fills a gap in existing literature, as no prior method aptlyaddressed this perspective. The proposed Generative Component Analysis (GCA) efficiently identifies thegenerative factor subspaces and their importance. Leveraging GCA, we formulated Importance-WeightedOrthogonality (IWO) and Importance-Weighted Rank (IWR), two novel metrics offering unique insights intosubspace orthogonality and importance ranking. Throughout experiments, our implementation emerged as arobust mechanism for assessing orthogonality, exhibiting resilience across varying latent shapes, non-linearencoding functions, and degrees of orthogonality. Disentanglement has long been credited for fostering fairness, interpretability, and explainability in generatedrepresentations. However, as pointed out by Locatello et al. (2019), the utility of disentangled representationsinvariably hinges on at least partial access to generative factors. With such access, an orthogonal subspace couldbe rendered as useful as a disentangled one. Through orthonormal projection, any orthogonal representationdiscovered can be aligned with the canonical basis, achieving good disentanglement. In conjunction with IWOs stronger downstream task correlation across datasets, models and tasks, thisunderscores our assertion that latent representations, which successfully decouple generative factors, arecrucial for a wide range of downstream applications, regardless of their alignment with the canonical basis. In conclusion, our work lays the groundwork for a fresh perspective on evaluating generative models. Wehope GCA and IWO may help identify models crafting useful orthogonal subspaces, which might have beenoverlooked under the prevailing disentanglement paradigm. We hope that IWO extends its applicabilityacross a broader spectrum of scenarios compared to traditional disentanglement measures.",
  "Christopher P. Burgess, Irina Higgins, Arka Pal, Loc Matthey, Nick Watters, Guillaume Desjardins, andAlexander Lerchner. Understanding disentangling in beta-vae. CoRR, abs/1804.03599, 2018": "Hugo Caselles-Dupr, Michal Garcia Ortiz, and David Filliat. Symmetry-based disentangled representationlearning requires interaction with environments. In Advances in Neural Information Processing Systems 32:Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,Vancouver, BC, Canada, pp. 46084617, 2019. Tian Qi Chen, Xuechen Li, Roger B. Grosse, and David Duvenaud. Isolating sources of disentanglement invariational autoencoders. In Advances in Neural Information Processing Systems 31: Annual Conferenceon Neural Information Processing Systems, NeurIPS, 2018. Raphal Couronn, Paul Vernhet, and Stanley Durrleman. Longitudinal self-supervision to disentangleinter-patient variability from disease progression. In Proc. International Conference on Medical ImageComputing and Computer-Assisted Intervention (MICCAI), pp. 231241. Springer, 2021. Andrea Dittadi, Frederik Truble, Francesco Locatello, Manuel Wuthrich, Vaibhav Agrawal, Ole Winther,Stefan Bauer, and Bernhard Schlkopf. On the transfer of disentangled representations in realistic settings.In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May3-7, 2021. OpenReview.net, 2021. Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangledrepresentations. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. Cian Eastwood, Andrei Liviu Nicolicioiu, Julius von Kgelgen, Armin Kekic, Frederik Truble, AndreaDittadi, and Bernhard Schlkopf. DCI-ES: an extended disentanglement framework with connections toidentifiability. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali,Rwanda, May 1-5, 2023. OpenReview.net, 2023.",
  "Yuchen Fei, Bo Zhan, Mei Hong, Xi Wu, Jiliu Zhou, and Yan Wang. Deep learning-based multi-modalcomputing with feature disentanglement for MRI image synthesis. Medical Physics, 48(7):37783789, 2021": "Xu Han, Zhengyang Shen, Zhenlin Xu, Spyridon Bakas, Hamed Akbari, Michel Bilello, Christos Davatzikos,and Marc Niethammer. A deep network for joint registration and reconstruction of images with pathologies.In Machine Learning in Medical Imaging: 11th International Workshop (MLMI), pp. 342352, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference oncomputer vision, pp. 10261034, 2015. Irina Higgins, Loc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, ShakirMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variationalframework. In 5th International Conference on Learning Representations, ICLR, 2017.",
  "Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Proceedings of the 35th InternationalConference on Machine Learning, ICML, 2018": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio andYann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disentangled latentconcepts from unlabeled observations. In 6th International Conference on Learning Representations, ICLR2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,2018. Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invarianceto pose and lighting. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Visionand Pattern Recognition, 2004. CVPR 2004., volume 2, pp. II104. IEEE, 2004. Hongwei Li, Sunita Gopal, Anjany Sekuboyina, Jianguo Zhang, Chen Niu, Carolin Pirkl, Jan Kirschke,Benedikt Wiestler, and Bjoern Menze. Unpaired MR image homogenisation by disentangled representationsand its uncertainty. In Uncertainty for Safe Utilization of Machine Learning in Medical Imaging, andPerinatal Imaging, Placental and Preterm Image Analysis. Springer International Publishing, 2021. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schlkopf,and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangledrepresentations. In international conference on machine learning, pp. 41144124. PMLR, 2019.",
  "Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testingsprites dataset. 2017": "Milton Llera Montero, Casimir J. H. Ludwig, Rui Ponte Costa, Gaurav Malhotra, and Jeffrey S. Bowers. Therole of disentanglement in generalisation. In 9th International Conference on Learning Representations,ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. Matthew Painter, Adam Prgel-Bennett, and Jonathon S. Hare. Linear disentangled representations andunsupervised action estimation. In Advances in Neural Information Processing Systems 33: AnnualConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,2020. Abbavaram Gowtham Reddy, Benin Godfrey L, and Vineeth N. Balasubramanian. On causally disentangledrepresentations. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-FourthConference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium onEducational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022,pp. 80898097. AAAI Press, 2022.",
  "Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances in neuralinformation processing systems, 28, 2015": "Karl Ridgeway and Michael C. Mozer. Learning deep disentangled embeddings with the f-statistic loss.In Advances in Neural Information Processing Systems 31: Annual Conference on Neural InformationProcessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montral, Canada, pp. 185194, 2018. Raphael Suter, Djordje Miladinovic, Bernhard Schlkopf, and Stefan Bauer. Robustly disentangled causalmechanisms: Validating deep representations for interventional robustness. In Proceedings of the 36thInternational Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California,USA, volume 97 of Proceedings of Machine Learning Research, pp. 60566065. PMLR, 2019.",
  "Fan Tang, Xinyu Zhu, Jinrong Hu, Juhong Tie, Jiliu Zhou, and Ying Fu. Generative adversarial unsupervisedimage restoration in hybrid degradation scenes. 2022": "Loek Tonnaer, Luis Armando Prez Rey, Vlado Menkovski, Mike Holenderski, and Jim Portegies. Quantifyingand learning linear symmetry-based disentanglement. In International Conference on Machine Learning,ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine LearningResearch, pp. 2158421608. PMLR, 2022. Frederik Truble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal,Bernhard Schlkopf, and Stefan Bauer. On disentangled representations learned from correlated data.In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 1040110412. PMLR, 2021. Andrea Valenti and Davide Bacciu. Leveraging relational information for learning weakly disentangledrepresentations. In International Joint Conference on Neural Networks, IJCNN 2022, Padua, Italy, July18-23, 2022, pp. 18. IEEE, 2022. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment anduniformity on the hypersphere. In International conference on machine learning, pp. 99299939. PMLR,2020. Lei Zhou, Joseph Bae, Huidong Liu, Gagandeep Singh, Jeremy Green, Amit Gupta, Dimitris Samaras, andPrateek Prasanna. Lung swapping autoencoder: Learning a disentangled structure-texture representationof chest radiographs. preprint arXiv:2201.07344, 2022.",
  "Z(zj; F),(9)": "where zj and c represent a generative factor and the latent space respectively, and F a class of regressors(e.g., multilayer perceptrons or random forests). AULCC is the Area Under the Loss Curve, computed byrecording the minimum losses achievable by regressing zj from c with models in F of increasing capacity.The denominator, easily computable, acts as a normalizing constant so that E . As an example, E = 1suggests that a linear regressor is sufficient to reach zero error, proving that the representation is efficient. Toaccount for a bias toward large representations, the explicitness is paired with the Size (S), computed as theratio between the number of generative factors and the size of the latent representation. In , we depict four situations of a 3-dimensional latent space where two generative factors z1, z2 lie ina separate 2-dimensional plane each, and the relationship between each generative factor and its correspondinglatent subspace is non-linear, as hinted by the coloring. In particular, z(n)j= f (n)(c) = f (n)(P (n)jc), for j {1, 2} and n {i, ii, iii, iv}, with P (n)j R23 being a projection matrix. For cases (i) and (iii), theprojections span orthogonal planes, contrarily to cases (ii) & (iv) where the planes have a different inclination.Case (i) & (ii) are characterized by a quadratic relationship, i.e., f (n) = A(c1)2 + B(c2)2 (with A, B beingparameters), while case (iii) and (iv) encode a trigonometric relationship, i.e., f (n) = cos(2c1) + cos(2c2)(with being a parameter). Explicitness evaluates the capacity required by a model to regress the generative factors z1, z2, startingfrom the representation c. Given that the effect of the linear transformation is present in all four situations,the differences are determined only by the non-linearity f (n). Therefore the metric is able to discriminatecases (i) & (ii) from (iii) & (iv). Instead, IWO quantifies the orthogonality of the planes, regardless ofthe non-linearities in the generative factors, so it discriminates the orthogonal cases (i) & (iii) from thenon-orthogonal ones (ii) & (iv). Finally, note that the DCI-Disentanglement metric would penalize all fourconfigurations as they are not disentangled.",
  "Theorem B.1. Given a representation c RL, IWO = 1 if and only if the latent subspaces S1, . . . , SK ofthe generative factors z1, . . . , zK all lie in each others orthogonal complement": "Proof. Let Bj and Bk represent basis for the latent subspaces Sj and Sk. If the subspaces lie in each othersorthogonal complement, this implies that b(j)l b(k)m = 0, l = 1, . . . , Rj, m = 1, . . . , Rk. The computation ofIWO is therefore",
  "From proving that IWO(zj, zk) = 1 for any j = k, it follows that IWO = 1": "Conversely, if at least one subspace does not lie in the orthogonal complement of at least one other subspace,this implies that b(j)l b(k)m = 0 for at least one combination of l, m, j, k. Because (j)l> 0 for l Rj for anyj, this implies that IWO(zj, zk) < 1.",
  "Theorem B.2. Given a representation c RL, IWO = 0 if and only if the latent subspaces S1, . . . , SK ofthe generative factors z1, . . . , zK all share the same importance along the same dimensions": "Proof. Let Bj and Bk represent basis for the latent subspaces Sj and Sk. From .1 we have that()l ()l+1 for any l and any {j, k}. Let us first consider the special case ()l> ()l+1 for any l, i.e. thebasis are ordered from most important to least important basis vector. Assuming all subspaces share thesame importance along the same dimensions, it must follow that Bj = Bk. Then b(j)l= b(k)land (j)l= (k)l,l. Also, b(j)l b(k)l= 1 implies that b(j)l b(k)m = 0 for l = m. Then,",
  "= 1 1 = 0.(11)": "Let us now consider the special case where Sj = Sk = RR and (j)l= (k)m = 1/R, l, m. Note that, sinceall importance weights are equal, there is no order in the basis. Let Bj and Bk be two orthogonal basesspanning Sj and Sk respectively. Because Sj = Sk, each basis vector b(j)lin Bj can be expressed as a linearcombination of the basis vectors in Bk. We can then use Parsevals identity (Johnson & Riess, 1982) for the",
  "The general case for (j)l (j)l+1 then follows from the two special cases described above. From proving thatIWO(zj, zk) = 0 for any j = k, it follows that IWO = 0": "Conversely, if two generative factors do not share the same importance along the same dimensions, this caneither mean that their subspaces are different or it means their subspaces are the same, but the importanceis spread differently, i.e. (j)l= (k)lfor at least two different l. For both cases it follows that IWO(zj, zk)cannot be 0.",
  "CComplexity and Differentiability": "For GCA, the subspace learning step complexity is dependent on the backpropagation algorithm used, whilethe basis generation step complexity is dependent on the reduced QR decomposition algorithm used. Theworst-case complexity of IWO is O(KL3), for IWR it is O(KL). GCA and the metrics are composed ofdifferentiable operations and are therefore differentiable. While the metrics are optimizable, GCA requiressupervision from the generative factors, information that may not always be available.",
  "l,m(BjBk )2ml =": "l,m(bjl bkm)2, where bjl and bkm are the l-th and m-th rows of Bj and Bk respectively. The maximumof the trace is therefore min(Rj, Rk), reached if Sj is a subspace of Sk or vice-versa. This definition oforthogonality can be interpreted as the average absolute cosine similarity between any vector pair from Sjand Sk. To efficiently calculate the importance-weighted projection of zjs subspace onto zks subspace, we first scalethe corresponding bases vectors in Bj, Bk with their respective importance before projecting them onto oneanother. IWO is the sum of all individual projections. Using Uj = DjBj, where Dj RRjRj is diagonalwith the l-th diagonal entry corresponding to the square root of importance,",
  "E.2Synthetic Data Generation": "We report the pseudocode for generating the synthetic representations discussed in .1. For simplicity,we have assumed L as a multiple of K. describes two examples of the used schema.Require: Representation size L, # of generative factors K, # of shared dimensions R, mapping fEnsure: A representation c RL and its related generative factors z RK",
  "FExperimental Details": "In this section, we provide a detailed description of the correlation analysis of our orthogonality metricwith downstream tasks on six different datasets and six different models listed in . For each model,learned representations for six different regularization strengths are considered (ten different random seedsfor each reg. strength). All these representations are directly retrieved from or trained with the code ofdisentanglement_lib2. For the ES metric, we utilized the official codebase provided by the authors ofDCI-ES 3. Each dataset we investigate has independent generative factors associated with it.",
  "F.2Datasets": "dSprites DatasetThe dSprites dataset is a collection of 2D shape images procedurally generated from sixindependent latent factors. These factors are color (white), shape (square, ellipse, heart), scale, rotation, andx and y positions of a sprite. Each possible combination of these latents is present exactly once, resulting ina total of 737280 unique images. Color dSprites DatasetThis dataset retains the fundamental characteristics of the original dSpritesdataset, with the distinct variation that each sprite, in the observation sampling process, is rendered in acolor determined by random selection. Scream dSprites DatasetThe dataset mirrors the original dSprites dataset but introduces a uniquemodification in the observation sampling process: A random segment from the Scream image is selected asthe background, and the sprite is integrated into this image by inverting the color of the chosen segmentspecifically at the sprites pixel locations. Cars3D DatasetThe Cars3D dataset is generated from 3D computer-aided design (CAD) models of cars.It consists of color renderings of 183 car models from 24 rotation angles, each offset by 15 degrees, and from4 different camera elevations. The images are rendered at a resolution of 64 64. smallNORB DatasetThe smallNORB dataset is designed for 3D object recognition from shape, featuringimages of 50 toys categorized into five types: four-legged animals, human figures, airplanes, trucks, and cars.The images were captured under six lighting conditions, nine elevations, and 18 different angles. The datasetis split into a training set with five instances of each category and a test set with the remaining five instances. Shapes3D DatasetThe Shapes3D dataset, initially presented in Kim & Mnih (2018) is a specializedcollection designed for the study of factorized variations in 3D object representation.This dataset ischaracterized by its systematic variation across six ground-truth factors, making it particularly suitablefor experiments in disentanglement and representation learning. These factors include floor color with 10variations, wall color also with 10 variations, object color featuring 10 distinct options, object size representedin 8 different scales, object type with 4 unique categories, and azimuth with 15 varied positions. Such adiverse range of factors allows for comprehensive analysis and experimentation in 3D object recognition anddisentanglement tasks. The dataset is thoughtfully split into a training set and a test set, each containing abalanced mix of these variations to facilitate robust model training and evaluation.",
  "F.3IWO on Limited Data": "To assess the impact of smaller sample sizes on IWO and IWRs correlation with downstream task performance,we repeat the experiments detailed in .2 for the smallNORB dataset, but with only 50% and 10% ofthe data. The results are illustrated in . We observe that IWO and IWR are resilient to changes indataset size.",
  "F.4Segmentation Task": "The task involves segmenting the sprite from the Scream dSprites dataset using the learned representations, utilizinga decoder neural network that generates segmentation masks based on these representations. In , we list theresults of these experiments. We observe that both IWO and IWR correlate stronger with downstream segmentationperformance than classic disentanglement metrics.",
  "F.5.1Implementation Details": "We use the PyTorch Lightning framework4 for the implementation of the models required to discern IWO and IWR.In particular, we use the implementations of the Linear and Batch-Normalization layers. Whereas the setup of theLNN is equal for all models and datasets, the NN-heads vary in their complexity for different datasets and factors. As",
  "all considered models operate with a 10-dimensional latent space, each LNN has 10 layers. The output of each LNNlayer is fed to the next layer and also to the corresponding NN-head": "holds the NN head configuration per dataset and factor. These were found using a simple grid search on onerandomly selected learned representation. This is necessary as factors vary in complexity and so does the requiredcapacity to regress them. It is worth mentioning, that the Explicitness pipeline, as proposed by Eastwood et al. (2023),could actually be employed on top of the NN-heads, integrating both metrics. For the initialization of the LNN layers and the NN heads, we use Kaiming uniform initialization as proposed in Heet al. (2015). We further use the Adam optimization scheme as proposed by Kingma & Ba (2015) with a learning rateof 5 104 and a batch size of 128 for all optimizations. Data is split into a training (80%) and a test set (20%).During training, part of the training set is used for validation, which is in turn used as an early stopping criterion. Theimportance scores used for IWO should be allocated using the test set. In our experiments, the difference between theimportance scores computed on the training set and the test set was small. For further details on the implementation,please refer to our official code5.",
  "GImportance Analysis": "In order to test the effectiveness of IWO and IWRs importance weighing, we compare their downstream taskcorrelation with the downstream task correlation of pure orthogonality (without any weighing). holds theresults of the correlation analysis. We can see, perhaps unsurprisingly, that the importance of weighing plays animportant role in why IWO and IWR can serve as representation quality metrics, whereas orthogonality might be aquestionable contender at best.",
  "HLoss Analysis": "In , we depict the loss of neural network heads at different projection steps for a single run of syntheticexperiment 3 (L = 10, K = R = 5). L6 to L10 are omitted, as they are almost zero (similar to L5). Each generativefactor is analysed using GCA, which means for each generative factor we train an LNN spine with nine matricesW9 R910, . . . , W1 R12 and 10 NN heads acting on the projections. Because of the symmetry of the syntheticexperiments, all five generative factors are similarly encoded in the latent space. In , we are thereforedepicting the mean and standard deviation over the generative factors. An entire pass through each LNN projects therepresentation to the most informative dimension for the respective generative factor. When recovering the generativefactor from that projection, we incur a loss of L1. The fraction L1/L0 tells us that this is 20% better than naiveguessing (assuming the expectation value) of the factor. We see that we can almost perfectly recover the generative",
  "L5/L0L4/L0L3/L0L2/L0L1/L0": ": Loss of neural network heads at different projection steps for a single run of synthetic experiment 3(L = 10, K = 5 and Rj = 5). We only depict L5 to L1, as L6 to L10 are almost zero (similar to L5). Left:Magnitude of relative loss after convergence for different projection depths. Right: Loss evolution overtraining iterations. factors from projections to 5 and more dimensions. This is expected as the experiment is set up with Rj = 5. We seethat each subsequently removed dimension increases the loss by 20%. It follows that Ll/L0 0.2, i.e. l 0.2for 1 l 5. The right-hand side of depicts the relative validation loss of the NN heads during training.The relative loss at the 72nd iteration corresponds to the relative loss depicted in the left plot."
}