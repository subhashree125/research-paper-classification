{
  "Abstract": "In this paper, we explore the idea of training large language models (LLMs) over highlycompressed text. While standard subword tokenizers compress text by a small factor, neuraltext compressors can achieve much higher rates of compression. If it were possible to trainLLMs directly over neurally compressed text, this would confer advantages in training andserving eciency, as well as easier handling of long text spans. The main obstacle to thisgoal is that strong compression tends to produce opaque outputs that are not well-suitedfor learning. In particular, we nd that text navely compressed via Arithmetic Coding isnot readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novelcompression technique whereby text is segmented into blocks that each compress to the samebit length. Using this method, we demonstrate eective learning over neurally compressedtext that improves with scale, and outperforms byte-level baselines by a wide margin onperplexity and inference speed benchmarks. While our method delivers worse perplexitythan subword tokenizers for models trained with the same parameter count, it has the benetof shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generationsteps, often reducing latency. Finally, we provide extensive analysis of the properties thatcontribute to learnability, and oer concrete suggestions for how to further improve theperformance of high-compression tokenizers.",
  "Introduction": "Todays large language models (LLMs) are almost exclusively trained over subword tokens. The tokenizersused to produce these tokensoften BPE (Gage, 1994; Sennrich et al., 2016) or Unigram (Kudo, 2018),as implemented by the SentencePiece library (Kudo & Richardson, 2018)are compressors that typicallyachieve ~4 compression over natural language text (Xue et al., 2022).1 While these tokenizers hide thecharacter-level makeup of each token from the LLM (Xue et al., 2022; Liu et al., 2023), this downside iswidely seen as outweighed by the signicant benets of compression. Compared to raw byte-level models,an LLM trained over subword tokens sees ~4 more text per token, allowing it to model longer-distancedependencies, ingest more pretraining data, and predict more text at inference time, all without increasingcompute.2 Given these advantages, it raises the question, could we compress text further to achieve even greater gains?It is well known that autoregressive language models can be turned into lossless text compressors, and recent Work done while at Google DeepMind.1We refer here to token-level compression rate, i.e., the length reduction between a raw UTF-8 byte sequence and thecorresponding sequence of subword tokens. If instead we measure the number of bits required to encode the two sequences,subword compression typically delivers ~2 or less compression, depending on vocabulary size, which typically ranges from 32kto 256k. See .4 for discussion.2The increased cost of the input embedding and nal softmax layers due to increased vocabulary size is negligible for all butthe smallest models.",
  "as tokens": ": An overview of our approach for training an LLM (M2) over neurally compressed text. First, M1is trained as a standard byte-level language modelgiven a leftward context, M1 assigns a probability toeach possible following byte. Next, corpus text is compressed into a bitstream using M1 as a compressor.Specically, the probabilities that M1 assigns at each text position are fed into a compression algorithm likeArithmetic Coding that supports using dynamic symbol probabilities. Finally, this bitstream is chunked intotokens (e.g., 8-bit chunks), and M2 is trained as a language model over compressed text.",
  "Can we simply train an LLM over this neurally compressed text?": "In this paper we explore various options for doing so, focusing primarily on the idea of using ArithmeticCoding (AC) (Witten et al., 1987), which is known to reach the near-optimal compression rate for a particularmodel that assigns probabilities to text continuations. presents our high-level approach. First, asmall language model M1 is trained over raw byte sequences. Next, this frozen model is used to compresspretraining corpus text by applying a standard compression algorithm like AC. The resulting compressedbitstream is then chunked into tokens, which are used to train M2, a language model that directly readsand writes neural-compressed text. Given a perfect probabilistic model of the raw byte sequence, the compression step would output a fully-compressed bitstream that would be indistinguishable from random noise, and hence unlearnable by M2. Inreality, M1 can never be perfect (Zvonkin & Levin, 2007), so the M1-compressed output will still containlearnable patterns. We explore whether using compression powered by a relatively small M1 is able to removethe simple structure that M1 understands from the inpute.g., patterns of spelling, word frequency, and basicgrammarwhile retaining any higher-level structure that M1 fails to modele.g., patterns requiring deeperreasoning and long range coherence. A larger M2 would then learn to model this higher-level structure,",
  "without needing to relearn the low-level structure removed by M1.4 In theory, this process could be repeatedby training an even-larger M3 model on text compressed by M2, and so on": "In practice, we nd that text compressed via Arithmetic Coding is not readily learnable by a standardtransformer-based LLM, with resulting models predicting tokens at chance. Interestingly, this result holdseven when M1 is reduced to a context-free unigram model, suggesting that the challenge of modeling AC-compressed text stems from the diculty of learning the AC compression and decompression process itself.We verify this hypothesis by showing that even the sub-tasks of AC-compressing and AC-decompressing textare not learned well beyond a few initial tokens. To aid learnability, we propose compression via Equal-Info Windows, a simple technique that breaks textinto contiguous windows and compresses them via Arithmetic Coding independently. Rather than splittingtext into windows of equal text length, we track the number of bits output by the compressor, and closeeach window just before it exceeds a set information threshold (e.g., 32 bits of information). This has theadvantage that when chunking the subsequent bitstream into M2 tokens, there is a stable mapping from Ntokens to one window (e.g., four 8-bit tokens one 32-bit window). At each window boundary, we reset bothAC algorithm and the M1 model context. This ensures that each window may be mapped back onto raw textwithout any additional information. Through ablations on window size and M2 vocabulary size, we nd that Equal-Info Windows make learningof AC-compressed text possible across a range of settings. However, we also observe that learning progressesgradually, starting with tokens at the left edge of each window, and for longer windows, the model learnslittle about the tokens near the right edge. Our best-performing setting uses short 16-bit windows that eachcorrespond to a single 16-bit M2 token. Despite resetting the compression algorithm every 16 bits, we stillachieve ~5.3 token-level compression overall, which exceeds standard subword tokenizers. Remarkably, ourbest M2 models outperform byte-level baselines on perplexity benchmarks (bits/byte) for xed computationbudget (FLOPs/byte). This shows that learning over neural-compressed text can be eective. At the same time, our best M2 models underperform subword baselines. We suspect this is due at least inpart to the relatively unstable mappings our neural tokenizers induce between words and tokens. By contrast,standard subword tokenizers induce essentially stable word-to-token mappings, which likely makes the tokensequences they output well-suited for LLM training. We illustrate this contrast through qualitative examples.Whether a neural tokenizer can reach a high level of compression while maintaining high learnability for LLMtraining is an interesting question for future research. Our main contributions are as follows: (1) Outline advantages and challenges of training over neurallycompressed text.(2) Compare LLMs trained over dierent tokenizers along two axes: bits/byte andFLOPs/byte. (3) Show that standard LLMs cant learn to model vanilla AC-compressed text. (4) Show thatGZip-compressed text is learnable by standard LLMs, but not competitive. (5) Propose compression viaEqual-Info Windows, and show that it enables learning over neurally compressed text.",
  "Training LLMs over compressed text is appealing for many reasons. We discuss three advantages in detailbelow": "EciencyThe most straightforward advantage is eciency. By compressing the same text into a shortertoken sequence, the model can process more text for the same computational cost. In particular, a modeltrained over C compressed text will see C more text during training compared to a model trained over rawtext, given an equal compute budget. Increasing the amount of data seen in pretraining is often an eectivemeans of improving performance (Kaplan et al., 2020; Homann et al., 2022). Processing text more ecientlyalso confers benets at inference time, reducing the serving cost for handling a request of a given prompt andcontinuation length. In addition to reducing the raw compute needed for inference, compression can also",
  "improve inference latency, since generating better-compressed output requires fewer sequential autoregressivesteps": "Longer ContextA second advantage is that working with compressed text allows modeling longercontextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scalesquadratically with the sequence length, O(n2d). This has limited the sequence lengths used by such modelsin practical settings to ~10k tokens.5 If, via compression, each token represents (on average) C bytes of rawtext, then the resulting LLM can model dependencies across C longer distances compared to a raw textmodel operating over the same token sequence length. While the benets of modeling longer context (beyond~1,000 bytes) are modest when viewed merely as perplexity gains (Press et al., 2022), the ability to conditionon long context is critical for many applications, such as retrieving content from a document, or answering acoding question provided documentation. Distribution of ComputeA third potential advantage of training over compressed text is that informationwill be spread more uniformly across the sequence. By the nature of compression, a text span that is relativelypredictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g.,a unique product serial number). When an LLM is trained over well-compressed text, each token willrepresent roughly an equal amount of information. Since the LLM allocates equal compute to each token,this amounts to allocating more compute for harder text spans. This adaptivity is similar in spirit toAdaptive Computation Time (ACT) (Graves, 2017), which learns to allocate additional compute at somesequence positions in an end-to-end manner, but with the advantage that in our case the computation remainsdenseidentical operations are applied at each position.6",
  "Challenges of Training over Compressed Text": "LearnabilityIt is not at all obvious what types of compression are transparent enough to be learnablethrough a standard LLM training process. Strong compression can be seen as removing as much redundantor predictable information from a sequence as possible. Consequently, the bitstream output by a goodcompressor is inherently hard to distinguish from random noise. In this work, we explore the setting whereM2the model trained over compressed texthas a larger capacity than M1, the model used for compression.In principle, this setup should allow M2 to extract additional information from the signal even after M1 hascompressed it. However, for strong enough M1 compression, the resulting bitstream may be too noisy todetect any signal. As a prerequisite for M2 to eectively predict continuations of compressed text, we anticipate that it isnecessary for M2 to have the ability to decompress bits text and compress text bits. These sub-tasks arechallenging in their own right. First, M2 needs to accurately simulate M1 in order to know the probabilitiesit assigns to the text, which determine the output of compression.7 Training models to mimic other modelscan be dicult (Lester et al., 2022), and even in settings where models do learn to copy the behavior ofanother network (Hinton et al., 2015), this is often only when looking at which symbol was assigned thehighest probabilitythe actual probabilities assigned often dier (Stanton et al., 2021). Second, M2 needs tolearn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm,which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasksin detail in .2. A further learnability challenge is the high level of context sensitivity needed to interpret a bitstream ofcompressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the 5Exploring sub-quadratic attention mechanisms is an area of active research (Ainslie et al., 2020; Wang et al., 2020; Kitaevet al., 2020; Zaheer et al., 2020; Beltagy et al., 2020; Child et al., 2019, et alia). However, regardless of the cost of attention,compressing the input increases the eective context for free.6It should be noted that ACT learns to allocate more compute where it is useful, as opposed to merely where the predictionsare hard. For example, ACT learns to not waste compute on inherently unpredictable text spans. We expect that as a heuristic,allocating more compute to higher-perplexity text spans is valuable, but leave this to future work to verify.7For Arithmetic Coding, not only would M2 need to know the probabilities M1 assigns to the observed text, but it wouldalso need to know the probabilities assigned to many unobserved symbols. This is because Arithmetic Coding operates overcumulative probabilities, i.e., the probability that the next symbol is e or any alphabetically preceding symbol.",
  "Published in Transactions on Machine Learning Research (12/2024)": "As such, we achieve lossless compression within each window by allowing the AC decoder to be run multipletimes, incrementing the sequence length until we nd the sequence that, when compressed, no longer matchesthe compressed output and backtracking. As we do not include an AC decoder end-of-input symbol, when wereach the end of the input, there is some ambiguity in the number of characters that are included in thatwindow. However, as we compress extremely long sequences, over 10,000 characters, the nal window ina, trimmed, M2 example rarely correlates to the nal input characters. Thus most sequences (99.4%) aredecompressable using the approach above. In our validation data, just 0.0011% of windows represent the endof an input sequence. Additionally, the standard deviations of the loss across validation tokens is similar formodels trained over compressed input and models trained directly on SentencePiece. Thus we believe thatthese ambiguous tokens do not eect our results.",
  "Compression": "In this work, we focus on lossless compression, which aims to encode a sequence of input symbols, x0:N ={x0, x1, . . . , xN} X|V |, into a bitstream while minimizing the expected length of the bitstream. Compressionmethods are often factored into a modeling component and a coding component (Mahoney, 2013). Theinput sequence can be viewed as a sample from a true distribution p, x0:N p, with a standard autoregressivedecomposition, p(x0:N) = Ni=1 p(xi|x0, . . . , xi1). The modeling component aims to approximate p withp. While some compression algorithms assume static probabilities for each symbol, stronger algorithmsare adaptive, meaning that symbol probabilities may change based on context. In this work, we usecontext-aware transformer-based language models to represent p. The coding component of a compression algorithm converts the input sequence to a bitstream of length(x0:N). To maximize compression, we want a coding algorithm that minimizes the expected number of bitsin the bitstream, L := Ex0:Np[(x0:N)]. This is done by assigning shorter bit sequences to common symbolsand longer sequences to less common ones.8 The expected length is lower bounded by L H(p) whereH(p) := Ex0:Np[ log2 p(x)] (Shannon, 1948). This means that, given a near-optimal coding algorithm, theachievable level of compression derives from how well the model p approximates p.",
  "Arithmetic Coding": "Arithmetic Coding (Rissanen, 1976; Pasco, 1977) uses a model p to compresses a sequence x0:N to a bitstream,which is the binary expansion of a oat f [0, 1). The oat f is found by assigning successively smallersub-intervals to each symbol xi x0:N, with the nal interval enclosing f. An interval is made of an upperand lower bound, Ii = [li, ui) and its size is given by ui li. Starting with I0 = [0, 1), at each step of encoding,the interval for the symbol xi is created by partitioning the interval Ii1 based on the cumulative distributionof p given the previous context, pcdf(xi|x<i). The size of this interval is given by size(Ii1) p(xi|x<i). Thus:",
  "Ii(xi) :=li1 + size(Ii1) pcdf(w|x<i), li1 + size(Ii1) pcdf(xi|x<i),": "where w X is the symbol before xi in a strict ordering of X, i.e., w is the previous token in the vocabulary.Finally, the bitstream of minimal length that represents the binary expansion of a number inside the nalinterval f IN(x0:N) is used as the compressed representation. 8This process can result in extremely uncommon sequences becoming longer under compression, as no algorithm can compressall possible input strings (Mahoney, 2013). In practice, natural language inputs are highly compressible and these edge cases areinputs that one would not recognize as natural language.",
  "Bj(b, 1) :=blj1 + size(Bj1) 0.5, buj1": "Once the nal interval IN is computed, smaller and smaller bit intervals are created until reaching a bitinterval BT (b) that is fully enclosed by IN. At this point, the corresponding bitstream b is the nal compressedrepresentation. The coding component of Arithmetic Coding is nearly optimal: the output bitstream will have a length oflog p(x0:N) + 1 bits when using innite precision. In the nite precision setting using bits, an extraO(N2) bits are added (Howard & Vitter, 1992). See Witten et al. (1987) for an example implementation.In our experiments, we use precision = 14. The practical eect of using a nite precision implementation ofArithmetic Coding is that the models cumulative distribution gets quantized to integers using bits. Thisresults in a minimum probability of 2 being assigned to all tokens.",
  "Related Work": "Recent work has looked at using large language models for compression, but has not to our knowledgeattempted to train subsequent models over the resulting compressed output. Works like Deltang et al. (2024)use a transformer language model as the modeling component of Arithmetic Coding, but they do not trainover compressed output nor do they make modications to the compression algorithm to facilitate learnabilityby downstream models. Additionally, they focus on the setting of compressing xed-size sequences of bytes.By contrast, our models operate over input sequences of xed token length. This allows for models withhigher compression rates to leverage longer contexts, as more bytes are included in the input. Valmeekam et al. (2023) proposes changes to Arithmetic Coding to make it more amenable to use withLLMsnamely, they rank sort the logits from the model before creating text intervals, Ii(x0:N). This couldhelp alleviate issues stemming from errors in M2s simulation of M1. However, they do not train models ontop of their compressed output. Some approaches to token-free (i.e., purely character- or byte-level) language modeling down-sample theinput sequence via convolutions (Clark et al., 2022; Tay et al., 2022), which could be seen as a form ofend-to-end neural tokenization. However one important distinction is that the resulting tokenization issoftoutputting high-dimensional vectors and not implying a discrete segmentationin contrast to ourtokenization that outputs discrete tokens. Methods for learning discrete tokenization end-to-end have also been proposed (Chung et al., 2017; Godeyet al., 2022). In the case of MANTa (Godey et al., 2022), the learned segmentation appears to be fairlysemantic (i.e., respecting word and morpheme boundaries), which could be an advantage over our approach.However, they lack our bias towards encoding an equal amount of information per token. In modeling audio, it is common practice to use learned tokenizers that compress the raw input signal todiscrete tokens from a xed-size codebook (van den Oord et al., 2017; Baevski et al., 2020; Chung et al., 2021;Borsos et al., 2023). However, this compression is lossy, whereas we focus on lossless compression. Other recent work focuses on using the modeling component from well-known compressors to do othertasks. Jiang et al. (2022) uses the model from GZip to perform text classication. Vilnis et al. (2023) usesthe Arithmetic Decoding algorithm with an LLM as the model to do diverse parallel sampling from thatLLM. One could imagine that the model of our compressors (M1) is a teacher for M2, but unlike theseother applications, the M1 values are not used outside of compression. Klekci (2011) also explores learning over compressed text, but with several key dierences. First, theyuse n-gram language models (Shannon, 1948) while we use LLMs. Second, their model is conditioned on",
  "Training Data": "All training data used is English web text from C4 (en 3.1.0) (Rael et al., 2020). After tokenization, eachdocument in C4 has an <EOS> token appended to it. We concatenate 128 documents together to generate along sequence of text. Using UTF-8 byte-level tokenization, the average document length is 2,170 bytes, thusthese long sequences have an average length of 277,760 bytes. Despite the document breaks, we considerthese long sequences continguous for the training of language models. These sequences are then split intoindividual examples, which are shued using the deterministic dataset functionality from SeqIO (Robertset al., 2023).",
  "Training M1": "The model used for compression is a decoder-only Transformer model (Vaswani et al., 2017). It uses the 3msize seen in and a context length of 1,024. We use a batch size of 128, an rsqrt decay learning rateschedule (1/steps) starting at 1.0 with 10,000 warmup steps, and a z-loss of 0.0001. The model is trainedfor 2,500,000 steps using the Adafactor (Shazeer & Stern, 2018) optimizer. The feed-forward layers useReLU activations (Nair & Hinton, 2010; Fukushima, 1975), and we use distinct learnable relative attention",
  "Compression Methods": "When compressing C4 training data, we use an example length of 10,240 bytes and apply one of the followingcompression techniques (see Appendix B for more methods we considered). This results in compressedexamples that are, on average, much longer than our target sequence length of 512 M2 tokens. Thus, eachexample lls or nearly lls the models context window with a compressed sequence made from contiguousraw bytes. We compress 51,200,000 examples using each method, allowing us to train each M2 model for200,000 steps without repeating data. Arithmetic Coding: In this setting, we use a decoder-only transformer language model to model p, that is,when creating the interval Ii(x0:N), the partitions for each possible character, p(xi|x<i), are calculated usingthe probabilities for the next token output by the transformer. The compressor model is run over contiguous text sequences of 10,240 bytes. The generated logits are used asthe model distribution for Arithmetic Coding. We use the Range Encoding (a nite-precision implementationof Arithmetic Coding) implementation from TensorFlow Compression (Ball et al., 2024) with a precision of14. The range encoding implementation uses integers with precision + 2 bits. This is commonly used whenencoding 16-bit oat logits, so we do not expect it to cause numerical issues as our models are trained usingboat16. While the compressor model is only trained on sequences of length 1,024, it uses relative positionembeddings in its attention layers. Thus, it can be applied to longer sequences. Some works observe decreasedperformance as inputs are scaled to lengths beyond those seen in training (Varis & Bojar, 2021; Press et al.,2022), but we nd that compression performance is similar in the two settings. Compressing sequences oflength 1,024 yields a compression ratio of 5.46 while compressing sequences of length 10,240 yields a ratio of5.49. This suggests the performance drop from long sequences has minimal eect on compression, or that theincreased contextual information makes up this dierence. We will see that text compressed in this straightforward manner is not readily learnable by M2. Thus, weexplore alternative compression methods that modify the modeling and coding components for betterlearnability. shows how our dierent approaches aect the compression ratio. Static Logits Arithmetic Coding: One potential diculty of learning over compressed text is that themodeling component of the compression algorithm is hard to learnthat is, the second language model(M2) has trouble learning to simulate the probabilities the compressor model (M1) assigns to bytes. To weaken the compressor model, we replace the context-sensitive LM model with a static byte unigram modelthat is, the models distribution over bytes is the same for each token in the input, i.e., p(xi|x0, . . . , xi1) =p(xi). This distribution is estimated using the byte unigram statistics from the C4 training data. Equal Information Windows: The diculty in modeling compressed text could also be because thecoding component of the compression algorithm is hard to learn. That is, the language model is not able totrack the state variables used in Arithmetic Coding. Our proposed method of weakening the coding component of Arithmetic Coding compression is to reset theAC encoder once it has output a set number of bits, creating windows of xed size where each window isan independently AC-compressed sequence. This process is illustrated in . Windows will represent avariable amount of text, but as each window is created via compression, we expect roughly the same amountof information per window. In addition to resetting the AC encoder, we also reset the M1 models context.9 This means that each Wbits of output can be decoded independently, at the cost of a weaker M1 model due to the lack of context.",
  "As each window is fully self-contained, the model no longer has to learn to track Arithmetic Coding statevariables over long distances": "In cases where spare bits are available at the end of a window (but not enough to add an additional symbolof text), we pad with zeros. This complicates the decoding algorithm, but the compression scheme can remainlossless. See Appendix D.5 for further discussion, handling the end of the input sequence, and an alternativepadding approach that gives similar results. When compressing an additional character would result in a bitstream that is greater than W bits long,i.e., more than W binary expansions are needed to create an interval that is enclosed by Ii+1(x0:i+1), thebitstream (padded to W bits as necessary) representing the input up to and including character i is emitted.Then both the AC encoder and M1 model are reset. That is, Ii+1(xi+1:N) is calculated as if Ii(x0:i) = [0, 1);the bit interval is also reset to Bj(b = ) := [0, 1). Similarly, M1 is only conditioned on inputs that are partof the current window, the inputs after i. That is, p(xj|x<j) p(xj|xi...j). We use b to denote the bits per window, and v for the vocabulary size of M2.For example,EqualInfoAC[(b)its=16, (v)ocab=256] represents AC encoding with 16-bit Equal Info Windows and 8-bit M2tokens (vocabulary 256). GZip: As a baseline, we also explore training over text compressed using GZip (Deutsch, 1996) as implementedin the Python (Van Rossum & Drake, 2009) zlib library using the default compression level. GZip usesthe DEFLATE algorithma combination of Human Trees (Human, 1952) and LZ77 (Ziv & Lempel, 1977).First LZ77 is used to replace repeated substrings in the text with pointers back to the original substring.Then a Human Tree is built for the currentLZ77 compressedexample and used to compress it. Notethat this setting is dynamic, as the Human tree, and hence the binary codes for each character, are unique",
  "Tokenization of Compressed Text": "Most compression methods output a bitstream, but training M2 directly over bits would not be ideal. AsM1 was trained over UTF-8 bytes, the bit-level output of compression would result in M2 being applied tomuch longer sequences. Additionally, models are generally trained with vocabulary sizes much larger thantwo. Thus, we need a method to segment the bitstream into tokens, creating a more standard sequence fortraining language models. We convert the bitstream into a token sequence by grouping every N bits into a tokenresulting in avocabulary size of 2N. We explore settings of N {8, 16}, resulting in vocabulary sizes of v=256 andv=65,536. As the tokens are created from the compressed bitstream, we expect the distribution of tokens tobe more uniform than the usual Zipan (Zipf, 1935) distribution of word or subword tokens, allowing us touse larger vocabularies without encountering issues of rare or unattested tokens. Following Rajaraman et al. (2024), our tokenization scheme can be described as the tuple T=(Dict, DS, enc(), dec()). Dict is the space of tokens created by segmenting the compressed bitstreamin our case, a vocabulary of 256 or 65k. DS is the entire M1 model. The functions enc() and dec() performencoding (compression) and decoding (decompression). In our case, these functions vary with (i) the M1model, (ii) the compression algorithm (AC, EqualInfoAC, etc.), and (iii) the bitstream segmentation strategy. Throughout this work, we focus on the token compression ratio LiT /LoT the ratio between the inputand output token sequence lengths. It is important to note that the meaning of token can dier betweenthe input and output sequences. Generally, the input sequence is one byte per token, while output tokensrepresent multiple bytes. This is in contrast to the more standard bit compression ratio Lib/Lobthe ratioof input bits to output bits. As we aim to reduce the computational overhead of running LLMs by trainingthem on compressed input, we are more concerned with reducing the number of tokens that M2 consumes.This dierence is elucidated in . While SentencePiece results in a sequence length reduction of 4.28,the larger vocabulary means that 15 bits are required to represent each token. As such, the bit compressionratio is only 2.28, which is much lower than our AC-based compressors. Similarly, creating 16-bit tokensfrom the output of Arithmetic Coding does not change the bit compression ratiothe total number of bits isunchangedbut it does reduce the number of tokens in the sequence, and thus the number of tokens theLLM must process. We compute compression ratios over the C4 dev set, which is unseen during M1 training. To highlight the dierences between the tokenization methods above, we measure the performance (asbits/byte on a sample of the C4 validation set) of two trivial models for each tokenizer in . Theuniform model navely assigns equal probability to each token, regardless of context. The unigram modelalso ignores context, but assigns probabilities based on the global token frequencies observed in the trainingdata. With byte-level tokenization, each UTF-8 byte encodes to a single 8-bit token, so the uniform modelachieves 8 bits/byte. For more powerful tokenizers, the uniform model is stronger, indicating that thetokenizer itself has some language modeling ability. We observe that our compression-based tokenizers (AC,EqualInfoAC and GZip) output a near-uniform distribution of tokens across their vocabulary. This is reectedin the near-zero gain over uniform achieved by modeling unigram statistics.",
  "AC[v=256]5.49StaticAC[v=256]1.73EqualInfoAC[b=16, v=256]2.66EqualInfoAC[b=32, v=256]3.49EqualInfoAC[b=64, v=256]4.16EqualInfoAC[b=128, v=256]4.61": ": Bits/byte () performance of two trivial models across tokenizers. Uniform assigns equal probabilityto each token. Unigram assigns probabilities based on the empirical token frequencies. As the compression-based tokenizers output near-uniform distributions over tokens, there is little gain in modeling unigramstatistics. Thus, learning over this data requires modeling longer contexts.",
  "Training M2 on Compressed Data": "Each M2 model is trained for 200,000 steps with a batch size of 256 and a sequence length of 512. Thus eachmodel trains on 26.2 billion tokens. Of these, the vast majority (over 98.9%) are non-padding tokens; seeAppendix D.2 for details and for the exact size of each dataset. As methods with higher compressionratios cover more raw text per token, we also include the total number of bytes in each dataset. Shuing oftraining sets is seeded, and dataset state is checkpointed during training, so each training run results in themodel seeing each example exactly once. Models are trained at four sizes, as shown in , with 25m, 113m, 403m, and 2b parameters, excludingembedding parameters. When the compressed bitstream is chunked into 8-bit tokens, the M2 model has avocabulary size of 256. With 16-bit tokens the vocabulary increases to 65,536. All M2 models have a sequencelength of 512 tokens. Thus, when training on 16-bit tokens, twice as many bytes are seen per example and intraining overall, as compared to 8-bit tokens. All other hyperparameters match those used in M1.",
  "Baselines": "We compare our M2 models against baseline models trained with two standard tokenization methods, describedbelow. All hyperparameters, including sequence length (512), match those used for our M2 training above. Bytes: These baselines train directly over UTF-8 bytes, using the byte tokenizer from ByT5 (Xue et al.,2022), which simply encodes the input string as UTF-8 using Python s.encode(\"utf-8\"). The models see26.2 billion bytes total (see ).",
  "3m2564364102425m51286642048113m7681212643072403m102416246440962b20483224648192": "SentencePiece: These baselines train on text tokenized using the T5 (Rael et al., 2020) vocabulary,which has 32,000 tokens, and was trained using the Unigram algorithm (Kudo, 2018) as implemented by theSentencePiece library (Kudo & Richardson, 2018). Unigram is one of several popular subword tokenizationalgorithms, with others including BPE (Sennrich et al., 2016) and WordPiece (Schuster & Nakajima, 2012;Wu et al., 2016), and each of these is implemented by multiple libraries, sometimes with minor dierences. We opt to use the SentencePiece Unigram tokenizer as our subword baseline for a few reasons. First, theperformance of these competing subword tokenization algorithms is known to be very similar, with severalworks nding no statistically signicant dierence between them in many settings (Kudo, 2018; Ali et al.,2024; Schmidt et al., 2024). Second, where dierences are reported (marginal or otherwise), Unigram hasbeen found to be preferred, at least for English-only models (Kudo, 2018; Bostrom & Durrett, 2020; Aliet al., 2024; Schmidt et al., 2024). Finally, previous work indicates that the SentencePiece implementation ofUnigram outperforms the HuggingFace implementation (Ali et al., 2024; Schmidt et al., 2024).",
  "Numerical Stability": "Arithmetic Coding depends on the creation of intervals that cover each symbol in the vocabulary based onthe quantized cumulative distribution of a models logits when predicting the next token. As such, a smallchange in the logits due to numerical noise can result in vastly dierent output bitstreams. This can make thepractical use of neural language models in compression dicult. Common sources of noise include changesin batch size, parallel computation, changes to compute infrastructure (CPU vs. GPU vs. TPU, dierentTPU topology, etc.), changes to inference (computing the logits for the whole sequence at once vs. computinglogits for a single token at a time using KV caches), and changes to the longest sequence length in the batch. Methods like the rank-sorted algorithm used in LLMZip (Valmeekam et al., 2023) may help alleviate theseissues as only the order of tokens needs to match between settings. The development of alternate methodsof LLM-based compression should keep numerical stability issues in mind and ideally alleviate these issuesin the design of the algorithm. Increasing the level of quantization could also help reduce numerical noiseissues, as dierences would mostly be lost in quantization, but this would have a negative impact on thecompression ratio.",
  "Evaluation": "As the tokenization scheme varies across the approaches we consider, models cannot be directly comparedon per-token metrics such as negative log likelihood loss . Rather, following previous work (Dai et al.,2019; Al-Rfou et al., 2019; Choe et al., 2019; Gao et al., 2020, et alia), we report perplexity in terms ofbits-per-byte, [bits/byte] = (LoT /LiT )/ ln(2), which scales the models loss by the token-level compressionrate. We also compare models on how much computation (FLOPs) is required to perform inference over a givenlength of raw text (bytes). More specically, we calculate M2s expected FLOPs/byte by scaling FLOPs/tokenapproximated by 2 params (excluding embedding parameters) following Kaplan et al. (2020)by the",
  "token-level compression rate (as tokens/byte). For methods using an M1 model during compression, theFLOPs/byte cost of M1 is added.10 For more details on the evaluation metrics see Appendix D.4": "We evaluate models on a sample of the C4 validation set. During evaluation, the model is run over 20 batchesor ~2.6 million tokens. These tokens represent dierent amounts of text based on the compression method,making it impractical to run evaluation on the same sequence of bytes for all methods. To conrm that ourvalidation samples are large enough to be representative, for each method, we train ve 25m parameter modelswith dierent seeds. We nd the nal performance to be extremely stable, with the largest standard deviationin bits/byte being 0.0061. Thus, the variance introduced from sampling the validation set is negligible. SeeAppendix D.1 for more information about variance.",
  ") is an example Pareto frontier, showinghow a practitioner might value the trade-o between bits/byte and bytes/step. Our 2 billion parameterEqualInfoAC[b=16, v=65k] model is on this frontier": "It is apparent from that if FLOPs/byte were held constant, SentencePiece would achieve slightly betterbits/byte than EqualInfoAC. However there is another axis along which EqualInfoAC may still be preferred.Setting aside inference FLOPs, on average SentencePiece tokenization requires 23% longer sequences toencode the same text when compared to our best EqualInfoAC setting (b=16, v=65k). This means that,regardless of FLOPs used, the SentencePiece models will take more decoder steps at inference time. It isup to the practitioner whether it is worth it to trade o some bits/byte performance in order to achieveshorter sequences. In many serving scenarios, decoder steps are a practical bottleneck for determining systemlatency, as other aspects of model scale, such as width, can be mitigated with enough parallelism. There arecases where one may be willing to incur even more (parallelizable) inference costs to reduce latency, as inspeculative decoding (Leviathan et al., 2023). To this end, it may be advantageous to use more computeresources to scale up an EqualInfoAC[b=16, v=65k] model (recovering bits/byte performance) while retainingthe reduced latency due to the shorter sequence length. This can be seen visually in . GZip is Not CompetitiveTraining over GZip-compressed text is relatively ineective. M2s performancewhen trained over GZip highlights a counter-intuitive trend. While the GZip M2 models actually learn, itwould still be preferable to train over AC-compressed texteven though those models do not learn. Thisis due to the weak compression oered by GZip. The poor compression rate, coupled with weak learning,means that the GZip M2 models bits/byte performance lags behind even the 3m parameter M1 model. Short Windows are the BestWe see a similar eect in , which ablates the EqualInfoAC windowsize. In terms of bits/byte, the shortest 16-bit windows perform the best. However, the next-best setting isthe longest 128-bit windows, despite the fact that these M2 models fail to learn almost anything beyond theuniform distribution. This unintuitive trend stems from the fact that longer windows translate to bettercompression rates (see ). If we remove the eect of compression rate by looking at bits-per-token",
  "(b) Ignoring Compression Ratio": ": Performance of EqualInfoAC across various window sizes, b {16, 32, 64, 128}. When evaluatingbits/byte (left) to control for compression ratio, we see an unintuitive trend where for most model sizes b = 16is best but b = 128 is second-best. This is due to the higher compression rate achieved by longer Equal InfoWindows. When evaluating tokens/byte (right), a monotonic trend emerges, showing that shorter windowsare easier to learn. (b), we see a clearer monotonic trendincreasing window length makes it harder to learn, as wemove closer to simply running Arithmetic Coding over the whole sequence. For 64 and 128-bit windows,performance improvements with scale are small, but present; see for exact numbers. Larger M2 Vocabulary is HelpfulTokenizing compressed text using a larger 16-bit vocabulary (v=65k)results in a 2 higher token compression rate, seen in the leftward shift of each curve in .11 ForArithmetic Coding methods, larger vocabulary also improves bits/byte, seen as a downward shift in the curves.However, for GZip, we see the opposite trend. Arithmetic Coding and GZip dier the most in their codingcomponent, which suggests that the reason for this dierence could lie there. Note that the header and footerpresent in GZip-compressed data do not explain this dierence, see Appendix A.4. For EqualInfoAC[b=16],moving from v=256 to v=65k results in each window corresponding to a single token, which increases thestability of the token text mapping. This could be one reason for the performance gain; see .1for more discussion of stability. Emergence with Scale is UnlikelyGiven the recent ndings of Schaeer et al. (2023), we anticipatethat continuing to scale models beyond 2 billion parameters is unlikely to deliver an emergent ability tolearn over AC-compressed text, since the bits/byte metric we use is smooth. Results Persist Under Scaling Laws ParadigmWhen scaling models, Homann et al. (2022)recommend that training tokens should be scaled linearly with model size. However, in our experimentsabove, all models see the same number of tokens, regardless of model size. Consequently, our largest modelsmay be somewhat undertrained.12 To test whether following the scaling laws recommendation inuencesour results, we reevaluate our models at earlier checkpoints selected to maintain a constant ratio of trainingdata to model size. We nd that all core trends are unchanged in this setting. See Appendix D.3 for details. 11The same trend holds for larger 64 and 128-bit windows, but the performance increase with scale is so slight that we omitthem from the graph. See for the exact values.12The undertraining of our 2b models is also visible in their validation loss curves, which still have a signicant decreasingslope at 200,000 steps, showing the models have not yet converged.",
  "Bitstream Tokenization is Not the Main Source of Diculty": "The compression algorithms we consider output a bitstream, which we later chunk into tokens of a xed bitdepth (e.g., 8-bit tokens). As such, it is common for the bits representing a single character or UTF-8 byte tobe split across multiple tokens. Compounding this issue is that the value of these tokens are contextuallydetermined and may dier depending on the surrounding bytes. The fact that both 8-bit and 16-bit token chunking strategies work suggests that this is not too much ofan issue for the model. To further investigate this, we train two modelsone 25m and one 403mon theraw bitstream output by Arithmetic Compression, i.e., each token is either a 1 or a 0 and the vocabularyhas a size of 2. We use the same hyperparameters as in . Working at the bit level means that theoutput sequence is now longer than the input sequence, which was UTF-8 bytes. As such, this setting is notpractical in the real world. When trained to convergence, the two models have cross entropy losses of 0.693 for the 25m parameter modeland 0.6928 for the 403m modelnot meaningfully better than the nave uniform distribution, which yields aloss of 0.693. This failure mode is the same as in , which suggests that AC encoding itself is the mainsource of diculty, as opposed to any issue around tokenization or vocabulary size.",
  "TargetsTask": ": Arithmetic compression and decompression cast as sequence-to-sequence tasks. To account for thestrong performance that is possible by modeling only the output bytes in the decompression task, we alsotrain a Byte LM on just the targets. : Transformers struggle to learn Arithmetic Coding. In the sequence-to-sequence setting, a modelthat learns AC compression/decompression should have an accuracy of 100. Our models perform much worse.When tasked with decompression in a sequence-to-sequence format, our transformers improvement over purelanguage modeling of the targets was not statistically signicant (p = 0.07). Thus, the model is not able toleverage the compressed input. Similarly, AC compression is only learned to 1.7% accuracy.",
  "Transformers Struggle to Learn Arithmetic Coding": "Arithmetic Coding is a sequential algorithm that involves tracking multiple state variables as the input(byte) sequence is consumed. Each token in the output sequence represents multiple transformations of thesevariables, e.g., 8 transformations when using 8-bit token chunking. Theoretically, only 10 transformer layersare needed to have a computational path through the model layers that can process a sequence of 1,024tokens as a chain, where each token conditions on the previous one. While most of our transformers havethe capacity to model these sequencesonly our 25m model has fewer layerswe see in practice that theArithmetic Coding algorithm is still dicult to learn. To directly diagnose the ability to track Arithmetic Coding, we format AC compression and decompression assequence-to-sequence tasks, as shown in . The input provides the model with the true text, so we expecta model that is able to learn Arithmetic Coding should achieve an accuracy of 100. We compress sequencesof 1,024 bytes using M1 and Arithmetic Coding.13 We concatenate the bytes and AC output tokens to createthe compression task. For the decompression task, we simply ip the orderAC output tokens rst and thenbytes. The target token IDs (bytes or tokens) are shifted by the input vocabulary size, ensuring that they",
  "It clearly leverages the input as a byte-level language model trained over the same target tokens only reachesan accuracy of 76": "The model trained for compression actually shows some signs of learning. Training a language model directlyon the compressed output results in the model learning a uniform distribution over tokens, see . Whenthe model is able to attend to the input text, we see that the performance in is better than theuniform distribution (which would have a cross entropy loss of 5.545). While this method shows some hopefor the learnability of Arithmetic Coding, the need to include the input sequence negates the main advantageof compression, i.e., applying the model to a shorter sequence. Additionally, the compressors performance isfar from the 100 it should be able to achieve. We also nd training on these sequence-to-sequence datasets to be less stable than training on the languagemodeling datasets. In our experiments, large performance swings and divergence were relatively common.",
  "Larger Vocabulary Helps Beyond Increasing the Compression Ratio": "Our best results training over compressed text use EqualInfoAC with 16-bit windows and vocabulary size ateither 65k (best) or 256 (second-best). One clear advantage of the v=65k model is that it has a 2 bettertoken compression rate, so sees twice as much raw text during training. To assess whether its performancegain is due entirely to this advantage, we train a 25m parameter M2 model over the same dataset, but reduceits sequence length from 512 256. This model trains on half as many tokens, but sees the same amount ofunderlying text as the v=256 model.16 shows that even in this setting, the model with larger vocabulary is stronger.17 In fact, most of thebits/byte gain (84% absolute) is due to the structural change in tokenization, as opposed to the additionaltext seen. One possible explanation for its strong performance is that the v=65k model uses exactly onetoken to represent each equal-info window. Well see in the next section that in EqualInfoAC settings withmultiple tokens per window, any non-initial tokens are highly context-dependent, and learning proceeds on acurriculum from the easy window-initial tokens to the harder window-nal tokens.",
  "Analysis": "In this section we examine how neural compression based tokenizers dier from standard tokenizers, andconduct additional analysis on training dynamics and learnability of compressed data. This analysis leads us 14The slight gain is statistically insignicant (p = 0.07).15This could also be explained by how the end of sequence is handled, see Appendix D.516To compensate for the smaller number of tokens in a sample of 20 batches from validation set when each example is 256tokens, we compute our evaluation metrics over 40 batches.17It may be possible to achieve further gains by increasing the token bit depth further. However, most deep learning frameworksdo not support using unsigned data types for inputs, and the resulting large vocabulary size can cause a computational bottleneckin the nal softmax layer.",
  "Sentence Pairs": "SentencePieceArithmeticCoding[v=256] EqualInfoAC[b=16, v=256] : Edit distance between tokenized sentence pairs that dier only by a single prex word. WithSentencePiece, changing the prex has a minimal eect on tokenization, limited to the prex itself. AC has thelargest edit distances, as the prex aects the probabilities M1 assigns to all subsequent text. EqualInfoACfalls in the middle, as many distinct prexes lead to the same windowing of subsequent text, and each windowis encoded in isolation.",
  "AC-Based Tokenizations are Less Stable and Less Semantic than SentencePiece": "We observe that text tokenized by our AC-based methods is not stablethat is, the tokenization of similarinput data often leads to vastly dierent tokens sequences. To quantify this stability, we measure the eect ofadding dierent prexes to a xed test sentence. We sample 500 words from a list of 3,000 common Englishwords18 and prepend each to the example sentence in . We then tokenize these new sentences usingSentencePiece, AC[v=256], and EqualInfoAC[b=16, v=256]. Finally, for each tokenizer, we calculate theLevenshtein edit distance (Levenshtein, 1965) between all pairs of tokenized sentences. shows a histogram of these distances. We observe that SentencePiece tokenized sentence pairs alwayshave a low edit distance, as the eect of changing the prex is limited to the prex tokens. In contrast, theedit distance between AC-tokenized sentence pairs is consistently high. The presence of the prex aectsthe probabilities M1 assigns throughout the entire sequence, resulting in widely varying token outputs. Editdistance with EqualInfoAC falls in between these two extremes. The degree to which the tokens after theprex match depends on how the prex aligns with EqualInfo window boundaries, which in turn aectsthe windowing of the rest of the sentence. Since there are a relatively small number of possible window",
  "[The th] [ree c] [urrently l] [iving ] [species] [ are] [: A] [frica][n sav] [anna] [ ele] [pha] [nts, ] [Afr] [ican ] [forest ] [eleph][ants, ] [and the ] [Asi] [an e] [lep] [hant] [s.]": "alignments, many sentence pairs have highly overlapping token sequences. As AC tokenization is so muchless stable than the other settings, we restrict further analysis to SentencePiece and EqualInfoAC. While the performance of our EqualInfoAC[b=16, v=65k] model approaches that of the SentencePiece baseline,our edit distances analysis suggests that the two tokenization schemes dier in many regards. To betterunderstand these dierences in qualitative terms, we compare the tokenizations of a single sentence in . First, we observe that SentencePiece produces a stable text token mapping. For example, elephantsappears three times in the sentence, and stably maps to the same two-token sequence in all cases:[ elephant] [s].Similarly, both occurrences of African map to the same token: [ African].Incontrast, the EqualInfoAC tokenization is relatively unstable, with each occurrence of these words beingsegmented in a dierent way, and yielding dierent token sequences. Second, we nd that the SentencePiece tokenization is more semantic, by which we mean that thesegmentation it induces aligns better with meaningful linguistic unitswords and morphemes. While thereare some exceptions, e.g. savanna being tokenized as [s] [a] [v] [anna], the more common case is thatwhole words are parsed as single tokens (e.g., currently), or into meaningful morphemes (e.g., elephant-s).By comparison, EqualInfoAC tokenization appears to almost entirely disregard word and morpheme boundaries.As one example, we see Asian elephants. tokenized as [Asi] [an e] [lep] [hant] [s.]. Despitethesedierences,thereisanimportantsimilaritybetweenSentencePieceandEqualInfoAC[b=16, v=65k]: they are both stable in the token text direction.That is, a given to-ken ID, e.g., token #500, will always map to the same output text. This transparent decoding propertylikely makes it easier for a downstream model to learn over these tokens.19 When we move to versions of EqualInfoAC that contain multiple tokens per window,such asEqualInfoAC[b=16, v=256], this transparency is destroyed for all non-initial tokens within a window. This isillustrated in . When the same token appears window-initially in dierent contexts, we see the windowtext has a stable prexe.g., token #151 always maps to the prex le-. However, when occurring as thesecond token within a two-token window, there are no apparent correspondences between window text.20 AsEqualInfoAC window length increases, the proportion of tokens that are stable decreases. This may explainthe observed diculty of learning over longer windows. The window text for all instances of these tokens canbe seen in Appendix D.8.",
  "Note that examines window text, as opposed to token text correspondences. This is becausefor multi-token windows, the mapping from tokens to text is not well dened. More specically, each": "19Padding to reach a specic window size can require extra computation to discern between padding and characters thatcompress to all zeros, however we nd in Appendix D.5 that it is not an issue for M2 models.20A repeated text substring that happens to be aligned with a window multiple times is one of the few cases where the secondtoken will represent the same text.",
  "AC Decoding is Learned Step-by-Step": "As Arithmetic Coding is a sequential (left-to-right) and contextual algorithm, the text represented by a giventoken will dier based on the previous token. As such, a model should perform better on a token if it has astrong understanding of the token before it. When using EqualInfoAC compression, each window representsan independent Arithmetic Coding document. As we move deeper into the window, more and more ACdecompression must be done to understand the token.",
  "(b) Increase over trivial accuracy per token position": ": Earlier tokens within the 8-token window of an EqualInfoAC[b=64, v=256] model are learnedearlier in training. As training progresses, the model unlocks the ability to model tokens deeper and deeperinto the window. The plot on the right shows the increase over trivial accuracywhich we dene as themaximum accuracy achieved in the rst 2,000 steps of training. (Note, window-nal padding makes trivialaccuracy higher for later positions.) For tokens #13, later tokens reach higher accuracy (3 > 2 > 1), likelydue to the benet of local context. For tokens #48, accuracy deteriorates, indicating that the model hastrouble tracking the AC algorithm for more than ~32 bits. To understand how a tokens position within a window aects learning, we track the average accuracy at eachposition within the 8-token windows of a 403m parameter EqualInfoAC[b=64, v=256] model22 during training. shows both raw accuracy (left) as well as the increase over trivial accuracy (right), which we deneas the maximum accuracy achieved in the rst 2,000 steps of training. By that point in training, the modelpredictions no longer change drastically between updates, and trivial patterns, such as how end-of-windowpadding generally reduces the number of possible tokens at the ends of windows, have already been learned.Looking at accuracy increase highlights the sequential learning trend by discounting any part of accuracythat is text independent. In particular, we note that window-nal tokens have a non-uniform distributiondue to the use of window-nal padding bits (see our EqualInfoAC formulation in .3), which can belearned without any understanding of the text. We observe two interesting trends. First, there is a clear ordering as to when the model starts to makemeaningful (non-trivial) progress on a given position. The initial token (#1) is learned rst, followed fairlyquickly by #2 and then #3. Later tokens are only unlocked after 10,000 training steps, suggesting thatthe ability to model these tokens builds on a foundation of understanding the preceding tokens within thewindow. The second trend concerns the accuracy reached at each position. Here, we observe an increase in accuracyfrom #1 < #2 < #3, followed by a decrease from #3 < #4 < #5 and so on.23 We interpret the increaseacross the rst three positions as due to the benet of extra leftward context.24 This is akin to the initialbyte in a word being harder to predict than the following bytes. The decreasing performance at tokens #4and beyond suggests the model is unable to track AC decompression indenitely. While the model clearlylearns to decompress longer sequences as training progresses, reliably decoding past 32 bits of AC outputappears to be a challenge.",
  "Learnable Distributions are Less Uniform": "A well-known result in the compression literature is that there can be no recursive compression (Mahoney,2013). The compression algorithm removes information captured by its model, resulting in a uniform output 22The absolute accuracy of the EqualInfoAC[b=64, v=256] model is relatively poor, but its relatively long window providesthe clearest illustration of these positional trends. We observe similar trends for EqualInfoAC[b=16, v=256] which has smallerwindows but much stronger performance.23The nal token #8 also ts this trend when looking at the increase over non-trivial accuracy. The raw accuracy is higherthan previous tokens, #47, due to the skewed distribution induced by window-nal padding.24While suggests that 24 bit windows could be eective, but in Appendix A.1 we found it is not.",
  "(b) n-bit tokens following our M2 tokenization": ": The amount of noise in the entropy estimate grows as the length of bit segments grow. Largersegmentations of the bitstream result in larger vocabularies and therefore require larger sample sizes foraccurate entropy estimates. For each setting, we plot the 5%, 50%, and 95% percentile intervals for theentropy, normalized by the average entropy across partitions. We see that the noise grows with n and thatsettings like EqualInfoAC[b=16] are noisier than AC, despite this not being apparent in . To account for noise in the entropy estimation, we partition the data into 100 disjoint samples. This resultsin each partition being a sample of ~2 billion symbols for n-grams and ~130 million for tokens. We thencalculate the entropy for each partition and the KL divergence between the entropy of the 0.5, 0.50, and 0.95quantile points and a uniform distribution. These quantiles are then plotted on to illustrate samplingnoise90% of sampled entropies fall within these bounds. The log scaling of hides some of the noisetrends, namely that the noise grows with n and that settings like GZip and EqualInfoAC are noisier thanAC and RNG. These trends are seen in where the entropy has been normalized based on the meanentropy calculated across the partitions.",
  "Conclusion": "We have shown there is promise in the idea of training LLMs over neurally compressed text. In the best case,this will allow training over text that is better compressed than standard subword token sequences, whilemaintaining learnability. This an appealing prospect, as models that read and write more text per token aremore ecient to train and serve, and can model longer dependencies. While the very simplest approach does not work (training directly over a tokenized AC-encoded bitstream),we showed that a relatively simple modicationcompression via Equal Info Windowsalready bringsus within striking distance of popular tokenizers. When measured in terms of perplexity achievable atxed inference cost (FLOPs/byte), we nd that our method outperforms raw byte-level models, and comesincreasingly close to the performance of SentencePiece tokenization as scale increases to 2 billion parameters. While bespoke compression methods have developed around dierent modalities (e.g., text, audio, images,video) and dierent applications (e.g., delta-of-delta for regular repeating timestamps (Pelkonen et al., 2015)),to our knowledge, no ecient compression methods have been designed specically for use as LLM tokenizers.We are optimistic that future work will create such methods. Compared to todays subword tokenizers, weexpect these methods (i) will deliver higher compression rates, (ii) will come closer to equal information pertoken, thus allocating compute more eectively, and (iii) will give models a more direct view of the underlyingraw text, thus helping on spelling and pronunciation tasks. As a tradeo, we expect these neural tokenizerswill be somewhat less stable in their text token mapping, but perhaps not so unstable as our approachhere. In particular, we think it is worth exploring methods under which a given word typically maps to arelatively small number (tens not thousands) of relatable token sequences. One direction we left unexplored is the idea of passing information between the compressing model (M1) andthe LLM trained over compressed text (M2). Some additional signal of M1s internal state or output maybe helpful for M2 to accurately simulate M1, which is a prerequisite to awlessly encoding and decodingM1-compressed text. For hill-climbing in this space, we found it useful to iterate on the sequence-to-sequence sub-tasks ofcompression and decompression, which should, in theory, be learnable with high accuracy. Specically, iffuture work can devise a strong (~10) compressor that a transformer can be trained to accurately encodeand decode, we expect that this will be an ideal candidate for tokenizing text for LLMs.",
  "Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The Long-Document Transformer, December2020. URL": "Zaln Borsos, Raphal Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shari, DominikRoblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. AudioLM: A LanguageModeling Approach to Audio Generation. IEEE/ACM Transactions on Audio, Speech, and LanguageProcessing, 31:25232533, 2023. doi: 10.1109/TASLP.2023.3288409. URL Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. InTrevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics:EMNLP 2020, pp. 46174624, Online, November 2020. Association for Computational Linguistics. doi:10.18653/v1/2020.ndings-emnlp.414. URL James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composabletransformations of Python+NumPy programs, 2018. URL",
  "Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical Multiscale Recurrent Neural Networks. InInternational Conference on Learning Representations, 2017. URL": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu.w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised SpeechPre-Training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp.244250, 2021. doi: 10.1109/ASRU51503.2021.9688253. URL Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an Ecient Tokenization-Free Encoder for Language Representation. Transactions of the Association for Computational Linguistics,10:7391, 2022. doi: 10.1162/tacl_a_00448. URL Gautier Dagan, Gabriel Synnaeve, and Baptiste Rozire. Getting the Most Out of Your Tokenizer forPre-training and Domain Adaptation. In Forty-rst International Conference on Machine Learning, ICML,2024. URL Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics, pp. 29782988, Florence, Italy, July 2019. Association forComputational Linguistics. doi: 10.18653/v1/P19-1285. URL Simon DeDeo, Robert X. D. Hawkins, Sara Klingenstein, and Tim Hitchcock. Bootstrap Methods for theEmpirical Study of Decision-Making and Information Flows in Social Systems. Entropy. An Internationaland Interdisciplinary Journal of Entropy and Information Studies, 15(6):22462276, 2013. ISSN 1099-4300.doi: 10.3390/e15062246. URL Grgoire Deltang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern,Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, Marcus Hutter, and Joel Veness.Language Modeling Is Compression. In ICLR, 2024. URL",
  "Philip Gage. A New Algorithm for Data Compression. C Users Journal, 12(2):2338, 1994. URL": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, HoraceHe, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB Dataset ofDiverse Text for Language Modeling, December 2020. URL Nathan Godey, Roman Castagn, ric de la Clergerie, and Benot Sagot. MANTa: Ecient Gradient-Based Tokenization for End-to-End Robust Language Modeling.In Findings of the Association forComputational Linguistics: EMNLP 2022, pp. 28592870, Abu Dhabi, United Arab Emirates, December2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.ndings-emnlp.207. URL",
  "Georey Hinton, Oriol Vinyals, and Je Dean. Distilling the Knowledge in a Neural Network, March 2015.URL": "Jordan Homann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland,Katherine Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, KarenSimonyan, Erich Elsen, Oriol Vinyals, Jack William Rae, and Laurent Sifre. Training Compute-OptimalLarge Language Models. In Advances in Neural Information Processing Systems, 2022. URL Paul G. Howard and Jerey Scott Vitter. Analysis of Arithmetic Coding for Data Compression. InformationProcessing & Management, 28(6):749763, 1992. ISSN 0306-4573. doi: 10.1016/0306-4573(92)90066-9.URL",
  "Vladimir I. Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics.Doklady, 10:707710, 1965. URL": "Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast Inference from Transformers via SpeculativeDecoding. In Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pp. 1927419286. PMLR, 2329 Jul 2023.URL Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok,Rj Mical, Mohammad Norouzi, and Noah Constant.Character-Aware Models Improve Visual TextRendering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 1627016297, Toronto, Canada, July 2023. Association for ComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.900. URL",
  "J. J. Rissanen.Generalized Kraft Inequality and Arithmetic Coding.IBM Journal of Research andDevelopment, 20(3):198203, 1976. doi: 10.1147/rd.203.0198. URL": "Adam Roberts, Hyung Won Chung, Gaurav Mishra, Anselm Levskaya, James Bradbury, Daniel Andor,Sharan Narang, Brian Lester, Colin Ganey, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, AlexSalcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, SashaTsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, AndrewChen, Kathleen Kenealy, Kehang Han, Michelle Casbon, Jonathan H. Clark, Stephan Lee, Dan Garrette,James Lee-Thorp, Colin Rael, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, JeremyMaitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, JoshuaNewlan, and Andrea Gesmundo. Scaling Up Models and Data with t5x and seqio. Journal of MachineLearning Research, 24(377):18, 2023. URL",
  "Claude Elwood Shannon. A Mathematical Theory of Communication. The Bell System Technical Journal,27:379423, 1948. URL": "Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-Attention with Relative Position Representations. InProceedings of the 2018 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464468, New Orleans, Louisiana,June 2018. Association for Computational Linguistics.doi: 10.18653/v1/N18-2074.URL Noam Shazeer and Mitchell Stern. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost.In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings ofMachine Learning Research, pp. 45964604. PMLR, July 2018. URL",
  "Guido Van Rossum and Fred L. Drake. Python 3 Reference Manual. CreateSpace, Scotts Valley, CA, 2009.ISBN 1441412697": "Dusan Varis and Ondej Bojar. Sequence Length is a Domain: Length-based Overtting in Transformer Models.In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 82468257,Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.doi: 10.18653/v1/2021.emnlp-main.650. URL Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. Attention Is All You Need. In Advances in Neural Information Process-ing Systems 30, pp. 59986008. Curran Associates, Inc., 2017. URL Luke Vilnis, Yury Zemlyanskiy, Patrick Murray, Alexandre Tachard Passos, and Sumit Sanghai. ArithmeticSampling: Parallel Diverse Decoding for Large Language Models. In Proceedings of the 40th InternationalConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3512035136.PMLR, 2329 Jul 2023. URL Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, EvgeniBurovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stfan J. van der Walt, Matthew Brett,Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, EricLarson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold,Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antnio H. Ribeiro,Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms forScientic Computing in Python. Nature Methods, 17:261272, 2020. doi: 10.1038/s41592-019-0686-2. URL",
  "Ian H. Witten, Radford M. Neal, and John G. Cleary. Arithmetic Coding for Data Compression. Commu-nications of The Acm, 30(6):520540, June 1987. ISSN 0001-0782. doi: 10.1145/214762.214771. URL": "Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, MaximKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Je Klingner, Apurva Shah, Melvin Johnson, XiaobingLiu, ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, GeorgeKurian, Nishant Patil, Wei Wang, Cli Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,Greg Corrado, Macdu Hughes, and Jerey Dean. Googles neural machine translation system: Bridgingthe gap between human and machine translation, 2016. URL Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, andColin Rael. ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models. Transactionsof the Association for Computational Linguistics, 10:291306, 2022. doi: 10.1162/tacl_a_00461. URL Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Are TransformersUniversal Approximators of Sequence-to-Sequence Functions? In International Conference on LearningRepresentations, ICLR, 2020. URL Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for Longer Sequences.Advances in Neural Information Processing Systems, 33, 2020.",
  "A.1Equal-Info Windows with 24 bits": "In , we saw that when trained on 64-bit Equal-Info windows, our model achieves the highest accuracyon the third 8-bit token. All else being equal, we expect to see higher accuracy on later tokens, since themodel should benet from additional leftward context. Thus, the stark accuracy decrease from token #3(>8%) to token #4 (<4%) likely indicates a diculty in tracking the AC algorithm beyond 24 bits. In this section, we explore training over 24-bit windows.First, we train a 403m M2 model overEqualInfoAC[b=24, v=256] compressed data, following the procedure from .We nd thismodel achieves 1.394 bits/byte.This ts cleanly in the trend seen in while the model out-performs EqualInfoAC[b=32, v=256] (which includes the problematic token #4), it still underperformsEqualInfoAC[b=16, v=256]. We also train a model using EqualInfoAC[b=24, v=65k], and nd it achieves 1.249 bits/byte, again ttingcleanly between the 16-bit and 32-bit settings. It is noteworthy that the model performs well despite the factthat the token bit-depth (16) is not a divisor of the window size (24). This results in M2 tokens that crosswindow boundaries.",
  "A.2Resetting M1 Every Window is Benecial": "We saw in .1 that the initial characters within each equal-info window are especially costly to encode.This is due to our procedure of resetting the M1 models context at every window boundary. In this section,we experiment with maintaining M1 context over multiple windows. We expect this to improve the overallcompression rate, but to negatively impact learnability, as a window can no longer be decoded in isolation. As one extreme case, we consider never resetting M1. That is, we rst use M1 to assign probabilities to alltokens in the input, and then use Equal-Info AC to compress the sequence into multiple xed sized windows.In this setting we nd that M2 does not learn beyond outputting a uniform distribution over tokens.27 Inference FLOPs/byte 1.5 bits/byte EqualInfoAC[b=16, v=256] EqualInfoAC[b=16, v=65k] Reset Every WindowReset Every Other WindowNever Reset : Resetting M1 with each Equal-Info window is benecial. While reducing how often M1 is resetimproves the compression ratio (see ), bits/byte performance is much worse, as seen by these pointsbeing far above the scaling curves for our default settings. At the other extreme, we consider resetting M1 every other time an Equal-Info window is emitted. We train403m parameter M2 models in each of these settings, and show the results in and . While reset-ting M1 every other window does not yield as strong performance as our standard EqualInfoAC[b=16, v=65k]setting, it is notable that it is still learning at all. This suggests that M2 is able to leverage context fromcompressed windows earlier in the sequence. While the improved compression ratio places this compres-sion scheme further to the right in than EqualInfoAC[b=16, v=65k] or SentencePiece, the reducedperformance means M2 must be extremely large to reach the Pareto frontier. Exploring this compression scheme in a more traditional pre-training setting, where M2 is trained on trillionsof tokens, would be interesting as this model seems particularly under-t. Similarly, exploring how manywindows can be emitted before resetting M1 while remaining learnable for M2 would be of interest as futurework. 27Consequently, models trained on compressed data that used 8-bit or 16-bit tokenization have the same performance interms of bits/byte. 16-bit tokenization squares the number of possible symbols in the vocabulary. This results in a doublingof the negative log likelihood loss when predicting the uniform distribution. This eectively cancels out the doubling of thecompression rate 16-bit tokenization yields.",
  "EqualInfoAC[b=16, v=256]every window2.661.092every other window3.401.748never5.331.501EqualInfoAC[b=16, v=65k]every window5.311.015every other window6.801.697never10.661.501": ": Resetting M1 less often yields greater compression rate, but bits/byte performance degrades. Wesee that resetting M1 every other window results in more compressed, but still learnable data. However, M2models trained on that data do not perform as well as when M1 is reset every Equal-Info window. As seen in never resetting M1 seems to have strong performance, but this is an artifact of its high compressionrate; it does not actually learn anything beyond a uniform distribution.",
  "A.3Character Awareness and Spelling": "Language models trained over subword tokens are not character-awarethat is, they lack direct access tothe character-level makeup of their input text. As a result, these models tend to perform poorly on tasksrelating to spelling (Xue et al., 2022; Liu et al., 2023). In this section, we explore whether our M2 modelstrained over neurally compressed text can oer an improvement in this regard. We investigate spelling ability using an adapted version of the WikiSpell task (Liu et al., 2023). When fed aword as input, the model is required to spell it by outputing the component characters as separate tokens. Wecompare two of the 403m parameter models trained in the EqualInfoAC[b=16, v=256] M2 modeland the SentencePiece model. In each case, we ne-tune the model (which has been pre-trained for 200,000steps on language modeling) on the WikiSpell task for 200 steps5 epochs over the data. We continue thesame learning rate schedule as pre-training. For each model, we tokenize the WikiSpell input words with the same tokenizer used during pre-training.The choice of which token IDs to use for the output character sequence is less straightforward. For the M2model, we use the ByT5 vocabulary (Xue et al., 2022), which overlaps in an arbitrary way with the 256IDs used during pre-training. For the SentencePiece model, we consider two options. For a more directcomparison, we map each output character to an arbitrary ID shared with a non-character subword tokenin the vocabularySentencePiece (Shared). As an alternative, we also consider mapping each outputcharacter to the ID of that character within the SentencePiece vocabularySentencePiece (Characters).We expect this setting to perform better, as the model should already have some understanding of thesecharacters and their relation to larger subwords from pre-training. shows that a SentencePiece model leveraging its pre-trained character representations has the bestspelling performance on held-out evaluation samples. It is also the rst that reaches 100% training accuracy.The EqualInfoAC[b=16, v=256] model, which must share token meanings between the compressed inputand the character output, outperforms a SentencePiece model with the same limitation on the evaluationset. Additionally, the model pre-trained over compressed text reaches 100% training accuracy before theanalogous SentencePiece model. Interestingly, M2s inability to reach 100% accuracy suggests that the modelmay not actually be performing decompression internally while doing language modeling over compressedtext.28",
  "A.4GZip Headers and Footers": "GZip compressed documents have both a headertwo bytes that identify the le typeand a footertwobytes representing the Adler-32 checksum (Deutsch & Gailly, 1996) of the input. We train 25m M2 modelson versions of the dataset where the header/footer are removed, as well as versions where it is kept. In 28We note that only 0.18% of WikiSpell dev examples (9 of 5,000) contain a character that is unseen during training. Thus,even if the output character IDs are chosen arbitrarily, a character-aware model should in theory be able to get nearly perfectperformance.",
  "(b) Spelling performance on the training data": ":Reusing SentencePiece character representations yields the strongest speller (and thefastest to train), but when forced to learn the output representations from scratch, a pre-trainedEqualInfoAC[b=16, v=256] model is a better speller than a SentencePiece based model. , we see that including the header/footer oers a marginal improvement in bits/byte, at the costof a marginal decrease in compression ratio. These dierences are not large enough to overcome GZipspoor performance compared to other compression methods. We opt to use versions of GZip including theheader/footer throughout.",
  "A.5Avoiding End-of-Window Zeros": "Our Equal-Info Windows algorithm described in .3 involves compressing text until exceeding aspecied number of bits (e.g., 16-bits per window), then backtracking one character, and padding withzero bits. As a character may encode to all zero bits, we need to ensure there is no ambiguity betweenpadding bits and all-zero characters in order to achieve lossless compression. Our default solution, used inall experiments, is to greedily include the most characters possible in each window. Given the knowledgeof greedy encoding, we can design a decoder that decodes windows unambiguously by using look-ahead, asdetailed in Appendix D.5. An alternative resolution to the problem of window-nal all-zero characters is to simply avoid emitting thesecharacters during compressioninstead, delaying their encoding until the next window. While this degradesthe compression rate, it results in a much simpler decoding algorithm. Specically, with the knowledge that awindow includes the least number of characters that can result in the given bitstream, we can decode windowswithout any look-ahead. To test whether this alternative scheme (Delay) improves learnability, we retrain EqualInfo M2 modelsfollowing the procedure from , with the only dierence being that in generating M2s trainingdata, we delay emission of window-nal all-zero characters. Applying this delayed version of windowedcompression, we observe a reduction in compression rate from 2.66 to 2.20.",
  "B.1Equal-Text Windows": "Equal-Text windows are a simpler alternative to Equal-Info windows. Rather than consuming a variableamount of text and outputting a xed number of bits, Equal-Text windows feed a xed amount of text intothe Arithmetic Coder, which compresses to a variable number of bits. We anticipate a downside to Equal-Text windows is that it would be dicult for M2 to decipher where onewindow ends and another begins, as this no longer aligns with a xed number of tokens, and the boundarymay appear token-internally. We considered adding delimiter tokens between windows to overcome thisdiculty. However, we expect this would degrade the compression rate too much, especially for the short ACcompressed windows that we found most eective in . Further exploration of Equal-Text windows, especially to see if the delimiters are actually required, would beinteresting future work as the Equal-Text Windows algorithm is much simpler than Equal-Info Windows.",
  "B.2Human Coding": "We also considered using Human Coding (Human, 1952) as a baseline compression implementation.However, as most implementations use static probabilities for characters, the resulting compression ratewould likely be too low to be competitive. With static Human Coding, there is a xed mapping betweenbitstream subsequences and characters, which may improve learnability by M2 models. However, because the",
  "coding component assigns each character a whole number of bits, the coding is less less optimal compared toArithmetic Coding": "Human Coding can be made adaptive by updating the induced codebook periodically, based on newerdata. When considering bit-level compression, adaptive Human Coding performs similar to static HumanCoding (Mahoney, 2013). However, when considering token-level compression, and the fact that the adaptivedistribution will come from M1, not unigrams of the recent data, training M2 models on adaptive HumanCoding could be interesting future work. As Human coding is part of the GZip algorithm, we opted not toexplore Human Coding on its own.",
  "B.3Asymmetric Numeral Systems": "Another compression algorithm we considered was Asymmetric Numeral Systems (ANS) (Duda, 2014). ANShas strong coding performance and is amenable to adaptive probabilities. The internal state is only a singlenatural number, which may be easier for an LLM to track than the two real numbers used in ArithmeticCoding (AC). However, unlike AC, the encoding and decoding algorithm are stack-like, where the encoderruns left-to-right and the decoder runs right-to-left. We thought this would make streaming inference, wherea single M2 token is generated and then decoded by M1 before another M2 token is generates, dicult. Thuswe opted to explore AC over ANS in this work. However, the simpler state is appealing and using ANS forcompression would be of interest as future work.",
  "CInstability in the Initial Token of Multi-Token Windows": "There are cases where the token text mapping for the initial token in a multi-token EqualInfoAC window canbe unstable. When a characters bitstream crosses the token boundarythe purple characters in onlysome prex of the bitstream contributes to the value of the initial token. It is possible that another charactermay produce a dierent bitstream with a shared prex. If the token boundary comes before the dierence inthe bitstreams, then the two tokens will have the same value but represent dierent text. When this occursthe text prex will remain stable, i.e., any characters whose bitstreams are entirely contained within the initialtoken will match, but the nal character may dier. Thus the notion of mapping a compressed token to exactcharacters is not well dened, as there are often cases there a character is spread across two tokens. Note,this only occurs at token boundaries; EqualInfoAC[b=16, v=65k] is stable as no characters cross windows.This is most likely a reason that EqualInfoAC[b=16, v=65k] outperforms EqualInfoAC[b=16, v=256] inour byte-controlled ablations (.3). Therefore, we consider EqualInfoAC stable enough to enablelearnability by M2. Interestingly, Klekci (2011) point out this same issue, where a xed size view of a variable length streamcan cause false equivalencies when prexes match. Similar to our ndings, they nd the models do have somelimited ability to deal with these situations.",
  "D.1Variance": "Sampling from the validation set was seeded. For a given seed, the same batches are sampled at eachevaluation step within a training run. Similarly, when models of a dierent size are trained on the samecompressed data, the same evaluation batches are sampled, allowing for fair comparison. As the Bytes andSentencePiece baselines use deterministic datasets, the validation seed is not used. Instead the start_stepis incremented by 20 to get a new sample of 20 batches. Model initialization and the order of the training data is controlled by the training seed. This seed was alsochanged during variance testing. During training, the dataset is checkpointed and therefore each exampleis seen exactly once. The exact order of the training data is determined by the seed. As the Bytes andSentencePiece baselines use deterministic datasets, the training order is xed.",
  "Methodbits/byte": "Bytes1.2899 0.0020SentencePiece1.1171 0.0006AC[v=256]1.4573 0.0001StaticAC[v=256]4.6936 0.0005EqualInfoAC[b=16, v=256]1.4724 0.0044EqualInfoAC[b=32, v=256]2.0457 0.0058EqualInfoAC[b=64, v=256]1.8494 0.0052EqualInfoAC[b=128, v=256]1.7121 0.0003GZip[v=256]2.3374 0.0061 5 models with 25m parameters were trained with dierent seeds (both validation and training) for eachcompression method and the two baselines. The mean and standard deviation can be found in .The variance is so low that we only report single values for most other experimental settings, such as largermodels. Training models of size 403m and 2b over data compressed with EqualInfoAC[b=64, v=256] andEqualInfoAC[b=128, v=256], as well as a 2b model with EqualInfoAC[b=128, v=65k], occasionally diverged,collapsing to a simple model that just output the uniform distribution. The numbers for these settingsexclude these divergent runs. This resulted in 7 re-runs in the most problematic case.",
  "D.2The Amount of Raw Text Bytes Seen by M2": "shows the number of tokens and bytes found in the training dataset for each compression method.During the data generation process, sequences of 10,240generated by concatenating 128 C4 byte-tokenizeddocuments togetherare compressed. Some of these sequences, namely the nal sequence created fromthe tail of the concatenated docs, are too short to be compressed to the target length of 512. Thus, theexact number of tokens in the dataset can vary slightly. With no padding, each dataset would have beentrained on 26,214,400,000 tokens, we see all settings are close to this value, with the maximum deviationbeing EqualInfoAC[b=128, v=65k] with 1.06% fewer tokens. All compression datasets are created from thesame source sequences, thus the underlying byte sequences compressed by weaker methods are prexes of theunderlying sequences compressed by stronger methods.",
  "D.3Scaling Curves with Scaled Training Data": "When scaling models, Homann et al. (2022) argue that training data should be scaled linearly with modelsize. As such, when comparing settings with constant training FLOPs, a large part of the FLOPs budgetshould be used by adding more training data. We apply this technique to compensate for our 2b modelsbeing under-trained by plotting the scaling curves in , where the smaller models are trained with lessdata, proportional to their size. Models with 25m parameters only train for 3k steps, 113m for 11k, 403mfor 40k, and 2b for 200k steps. Otherwise, the settings match those in . Numerical values used in thegraph can be found in . Scaling the training data adjusts the absolute slopes of the lines for all models that learn. Models that donot learn still only predict a uniform distribution. The trends between settings are unchanged. Thus we optto plot the versions where training data is held constant across model sizes.",
  "D.4Evaluation Details": "In our experiments, dierent settings have dierent vocabulary size, tokenization, and has a dierent amountof underlying text due to variations in compression rate. Thus, they are not directly comparable using per-token versions metrics like the cross-entropy, negative log likelihood loss, or perplexity. To address this, weconvert our token-level negative log likelihood loss, , to byte-level negative log likelihood loss by dividing theloss by that compression methods specic token-level compression rate, byte = /(LiT /LoT ) = (LoT /LiT ).Note that we use per byte metrics over per character metrics as there is ambiguity as to what counts asa character when working with UTF-8 Unicode. As is common in evaluation of work related to compression, instead of the negative log likelihood loss byte(in the unit of nats) per byte, we use bits/byte. This would require using log base two instead of thenatural log during the negative log likelihood calculation, but this conversion can be done after the fact,bits/byte = log2(ebyte) = byte/ ln(2). Note that this results in the same conversion used in Gao et al. (2020),bits/byte = byte/ ln(2) = (LoT /LiT )/ ln(2), when the input tokens represent bytes. As one of the main advantages of an M2 model that processes compressed text is that it needs to be run overfewer tokens, we also compare models based on the amount of FLOPs required during inference. Dierentcompression methods result in dierent sequence lengths for the M2 model to process. Therefore, we need tostandardize our FLOPs measurement to the byte-level so that it is comparable across methods. We start withFLOPs/tokenapproximated by 2 num_params (not including embedding parameters) following Kaplanet al. (2020)and divide it by that methods token-level compression rate to get the FLOPs/byte, just likethe bits/byte conversion. For methods that require running an M1 model over each byte, the FLOPs/bytecost of the M1 model is added. Note, while there is a computational cost to running GZip over the inputtext, we ignore it as it is insubstantial compared to the cost of running model inference. Evaluation of language models is often done by running the model on the entire validation set, moving thesliding window formed by the models context window by a single token at each step. This yields strongermodels by providing the most context possible when making predictions for a token. As we care aboutrelative performances between methods, opposed to absolute performance, we opt to evaluate the model on asample of the C4 validation set. During evaluation, the model is run over 20 batches, resulting in predictions",
  "D.5End of Window and End of Input Sequence Handling": "In the implementation of EqualInfoAC[b=W], each output window must end up being W bits. Therefore,when the compression of an additional character would result in a bitstream of more than W bits, padding ofthe compressed bitstream without that additional character must be done. In cases where the nal character in the window only adds zeros to the bitstream, it is unclear at rst glanceif that nal character was included in the window, or if it was omitted and the trailing zeros are all padding.However, the compression scheme is still lossless if we are consistent in our encoding. By always includingthe most input characters possible in each window, we know that, during decoding, if the addition of a nalcharacter (which is compressed to all zeros) still results in the same compressed bitstream, then that nalcharacter is part of that window. The decoding algorithm also knows when to stop adding characters toinputwhen the addition of a new character would generate more than W bits when compressed.29 29The only exception is at the end of the sequence, when the amount of padding is dictated by running out of input charactersinstead of running out of room in the window. This can be solved by including an end-of-input symbol.",
  "DatasetSizeStepbits/byte": "Bytes25m3k1.62113m11k1.36403m40k1.182b200k1.03SentencePiece25m3k1.35113m11k1.15403m40k1.002b200k0.87AC[v=256]25m3k1.46113m11k1.46403m40k1.462b200k1.46AC[v=65k]25m3k1.46113m11k1.46403m40k1.462b200k1.46StaticAC[v=256]25m3k4.62113m11k4.62403m40k4.622b200k4.62StaticAC[v=65k]25m3k4.62113m11k4.62403m40k4.612b200k4.61EqualInfoAC[b=16, v=256]25m3k1.86113m11k1.50403m40k1.212b200k0.99EqualInfoAC[b=16, v=65k]25m3k1.62113m11k1.31403m40k1.102b200k0.94GZip[v=256]25m3k2.95113m11k2.48403m40k1.972b200k1.56GZip[v=65k]25m3k3.30113m11k3.08403m40k2.722b200k2.26 This kind of padding is present in many Arithmetic Coding implementations and is generally solved by eithergiving the AC decoder the original input sequence length and the compressed message, or by the AC decoderusing a special termination character. These xes are hard to apply in our setting. Passing the number oftokens present in a window to M2 would be possible during training, but it would make inference much morecomplex (requiring a solution such as M2 generating fertility scores that specify how many characters thegenerated tokens represent (Brown et al., 1993)). In order to include an end-of-input symbol for the ACdecoder, the M1 model must be able to assign reasonable probabilities to that symbol. Therefore it wouldneed to appear at the end of each training example, hindering M1s ability to be applied to longer sequences.",
  "(b) Tokens": ": Bias corrected KL divergence between the observed and uniform distributions for dierentsegmentations of the bitstream. This plot is similar to , however, the KL divergence calculations usethe entropy of the observed distribution after applying the Miller-Madow bias correction. After applying biascorrection, we see that the expected 0 KL divergence for the RNG baseline is now within the 90th percentilebounds. However, this can results in an, incorrect, negative KL divergence which is removed from the graph.Thus the RNG 50th percentile is shown as a scatter plot rather than a broken line. In this setting it is clearthat the 50th percentile for AC[v=65k]s above the 50th percentile for RNG, however, it is hard to disentanglethe two as their 5th percentile lines are similar.",
  "example, to account for new words that are added to languagesbut in this case our vocabulary size isalways 2n where n is the size of the current segmentation": "When we plot the KL divergence between the Miller-Madow estimated entropy and the uniform distribution,we see that the percentile interval for the RNG baseline now includes 0, the KL divergence we expect giventhe data was generated from random and independent bits. As bias correction is approximate, it is possiblethat, for a given sample, the correction will result in an entropy greater than the maximum entropy possiblefor a given vocabulary size. Given that KL divergence between a distribution P and the uniform distributionU simplies to the entropy of U minus the entropy of P, KL(P||U) = H[U] H[P] = log2 |V | H[p], thisresults in a negative KL divergence, which is not allowed. These points get removed from the graph duringlog scaling and the resulting 50% percentile line for RNG data looks strange. Therefore, we only plot pointswith positive KL divergence in . The Miller-Madow estimation of entropy makes it clear that the 0.5entropy quantile for AC compressed data is much higher than the 50% percentile for RNG data. Additionally,for n > 2, the AC entropy is statistically signicantly less than the RNG entropy; however, dierences in themean entropy only start to appear after ~8 decimal places. This slight dierence in mean, coupled with thefact that the 5% percentiles are similar, means we cannot condently assert the model will be able to easilydistinguish the AC compressed data from random data. Given that we care about the dierences between theentropy of data compressed with dierent methodswhich is invariant to biasand the strange plots whenvalues are less than 0, we opt to plot the plug-in estimator in instead of the Miller-Madow estimator.",
  "D.8Window Text Patterns and Token Positions": "We tokenize 20 documents of length 1,024 with EqualInfoAC[b=16, v=256] and nd that all 256 possibletoken values occur multiple times, both as the rst and as the second token within the window. Whentokenized with EqualInfoAC[b=16, v=65k], 34.5% of attested tokens appear more than once. showsall the window text for repeated tokens.",
  "TokenWindowPositionWindow Text": "1851[or ] / [or a ] / [or ac] / [or al] / [or cr] / [or d] / [or f] / [or h][or hi] / [or i] / [or k] / [or ma] / [or pr] / [or r] / [or s] / [or se][or su] / [or t] / [or to] / [or v] / [or wha] / [or y] / [or yo] / [or, t][or-] / [or.] / [ora] / [orc] / [orce ] / [ord] / [ord a] / [order][ore a] / [ore e] / [ore ev] / [ore g] / [ore i]2 / [ of F] / [ records ] / [. Lo] / [Alt] / [OI] / [ase ] / [at y][cian] / [cri] / [d. I] / [ery] / [h de] / [hen s] / [ides] / [n ne][oft] / [om i] / [onte] / [opp] / [pir] / [rev] / [reve] / [s may][tion a] / [y do] / [y t] 1511[le] / [le s] / [le t] / [le. ] / [lea] / [lec] / [led] / [led ][led t] / [leg] / [lege] / [leh] / [lem ] / [leme] / [lems] / [len][ler] / [les] / [less] / [let] / [lett] / [level] / [lew ] / [ley] / [lf ]2[ all ] / [ nut] / [ this] / [ un] / [. I w] / [Ni] / [as t] / [ceed ][choos] / [e Mi] / [e-li] / [etti] / [imag] / [ion a] / [k a] / [ne a][ng up] / [niversi] / [npo] / [nt pr] / [pi] / [rvices] / [s T] / [s your][s?] / [so c] / [stag] / [thou] / [thoug] / [ust] / [ust ]"
}