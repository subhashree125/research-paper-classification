{
  "Abstract": "Current state-of-the-art diffusion models employ U-Net architectures containing convolutionaland (qkv) self-attention layers. The U-Net processes images while being conditioned onthe time embedding input for each sampling step and the class or caption embedding inputcorresponding to the desired conditional generation. Such conditioning involves scale-and-shiftoperations to the convolutional layers but does not directly affect the attention layers. Whilethese standard architectural choices are certainly effective, not conditioning the attentionlayers feels arbitrary and potentially suboptimal. In this work, we show that simply addingLoRA conditioning to the attention layers without changing or tuning the other parts of theU-Net architecture improves the image generation quality. For example, a drop-in addition ofLoRA conditioning to EDM diffusion model yields FID scores of 1.91/1.75 for unconditionaland class-conditional CIFAR-10 generation, improving upon the baseline of 1.97/1.79. Codeis available at",
  "FFHQ64 (uncond.)EDM (vp)baseline79--2.3961805571only LoRA792042.4658935795with LoRA792042.37/2.2863941631": ": Image generation results. We compare three different conditionings for each setting: (baseline)conditioning only convolutional layers in residual blocks with scale-and-shift; (only LoRA) conditioningonly attention layers using LoRA conditioning and not conditioning convolutional layers; (with LoRA)conditioning both convolutional layers and attention layers with scale-and-shift and LoRA conditioning,respectively. For unconditional CIFAR-10 and FFHQ64 sampling using EDM with LoRA, we also report theFID score obtained by initializing the base model with pre-trained weights.",
  "Introduction": "In recent years, diffusion models have led to phenomenal advancements in image generation. Many cutting-edge diffusion models leverage U-Net architectures as their backbone, consisting of convolutional and (qkv)self-attention layers (Dhariwal & Nichol, 2021; Kim et al., 2023; Saharia et al., 2022; Rombach et al., 2022;Podell et al., 2024). In these models, the U-Net architecture-based score network is conditioned on thetime, and/or, class, text embedding (Ho & Salimans, 2021) using scale-and-shift operations applied to theconvolutional layers in the so-called residual blocks. Notably, however, the attention layers are not directlyaffected by the conditioning, and the rationale behind not extending conditioning to attention layers remainsunclear. This gap suggests a need for in-depth studies searching for effective conditioning methods forattention layers and assessing their impact on performance. Meanwhile, low-rank adaptation (LoRA) has become the standard approach for parameter-efficient fine-tuningof large language models (LLM) (Hu et al., 2022). With LoRA, one trains low-rank updates that are addedto frozen pre-trained dense weights in the attention layers of LLMs. The consistent effectiveness of LoRA forLLMs suggests that LoRA may be generally compatible with attention layers used in different architecturesand for different tasks (Chen et al., 2022; Pan et al., 2022; Lin et al., 2023; Gong et al., 2024). In this work, we introduce a novel method for effectively conditioning the attention layers in the U-Netarchitectures of diffusion models by jointly training multiple LoRA adapters along with the base model. Wecall these LoRA adapters TimeLoRA and ClassLoRA for discrete-time settings, and Unified CompositionalLoRA (UC-LoRA) for continuous signal-to-ratio (SNR) settings. Simply adding these LoRA adapters in adrop-in fashion without modifying or tuning the configurations of original model brings consistent enhancementin FID scores across several popular models applied to CIFAR-10, FFHQ 64x64, and ImageNet datasets. Inparticular, adding LoRA-conditioning to the EDM model (Karras et al., 2022) yields improved FID scoresof 1.75, 1.91, 2.28 for class-conditional CIFAR-10, unconditional CIFAR-10, and FFHQ 64x64 datasets,respectively, outperforming the baseline scores of 1.79, 1.97, 2.39. Moreover, we find that LoRA conditioning",
  "U-Net blockLoRA conditioning of attention block": ": Conditioning of U-Net Block: (left) scale-and-shift conditioning on the convolutional block (middle)LoRA conditioning on the attention block (right) top: TimeLoRA and ClassLoRA for the discrete-timesetting, bottom: unified composition LoRA for the continuous-SNR setting. by itself is powerful enough to perform effectively. Our experiments show that only conditioning the attentionlayers using LoRA adapters (without the conditioning convolutional layers with scale-and-shift) achievescomparable FID scores compared to the baseline scale-and-shift conditioning (without LoRA). Contribution.Our experiments show that using LoRA to condition time and class information on attentionlayers is effective across various models and datasets, including nano diffusion (Lelarge et al., 2024), IDDPM(Nichol & Dhariwal, 2021), and EDM (Karras et al., 2022) architectures using the MNIST (Deng, 2012),CIFAR-10 (Krizhevsky et al., 2009), and FFHQ (Karras et al., 2019) datasets. Our main contributions are as follows. (i) We show that simple drop-in LoRA conditioning on the attentionlayers improves the image generation quality, as measured by lower FID scores, while incurring minimal(10%) added memory and compute costs. (ii) We identify the problem of whether to and how to conditionattention layers in diffusion models and provide the positive answer that attention layers should be conditionedand LoRA is an effective approach that outperforms the prior approaches of no conditioning or conditioningwith adaLN (Peebles & Xie, 2023).",
  "Diffusion models": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b)generate images by iteratively removing noise from a noisy image. This denoising process is defined by thereverse process of the forward diffusion process: given data x0 q0, progressively inject noise to x0 by",
  "q(xt1 | xt)": "To sample from a trained diffusion model, one starts with Gaussian noise xT N (0, (1 T )I), wheret = ts=1(1s), and progressively denoise the image by sampling from p(xt1|xt) with t = T, T 1, . . . , 2, 1sequentially to obtain a clean image x0. The above discrete-time description of diffusion models has a continuous-time counterpart based on thetheory of stochastic differential equation (SDE) for the forward-corruption process and reversing it based onAndersons reverse-time SDE (Anderson, 1982) or a reverse-time ordinary differential equation (ODE) withequivalent marginal probabilities (Song et al., 2021a). Higher-order integrators have been used to reduce thediscretization errors in solving the differential equations (Karras et al., 2022). Architecture for diffusion models.The initial work of Song & Ermon (2019) first utilized the CNN-basedU-Net architecture (Ronneberger et al., 2015) as the architecture for the score network. Several improvementshave been made by later works (Ho et al., 2020; Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021; Hoogeboomet al., 2023) incorporating multi-head self-attention (Vaswani et al., 2017), group normalization (Wu &He, 2018), and adaptive layer normalization (adaLN) (Perez et al., 2018). Recently, several alternativearchitectures have been proposed. Jabri et al. (2023) proposed Recurrent Interface Network (RIN), whichdecouples the core computation and the dimension of the data for more scalable image generation. Peebles &Xie (2023); Bao et al. (2023); Gao et al. (2023); Hatamizadeh et al. (2023) investigated the effectiveness oftransformer-based architectures (Dosovitskiy et al., 2021) for diffusion models. Yan et al. (2023) utilizedstate space models (Gu et al., 2022) in DiffuSSM to present an attention-free diffusion model architecture. Inthis work, we propose a conditioning method for attention layers and test it on several CNN-based U-Netarchitectures. Note that our proposed method is applicable to all diffusion models utilizing attention layers.",
  "Low-rank adaptation": "Using trainable adapters for specific tasks has been an effective approach for fine-tuning models in the realmof natural language processing (NLP) (Houlsby et al., 2019; Pfeiffer et al., 2020). Low-rank adpatation(LoRA, Hu et al. (2022)) is a parameter-efficient fine-tuning method that updates a low-rank adapter: tofine-tune a pre-trained dense weight matrix W Rdoutdin, LoRA parameterizes the fine-tuning update Wwith a low-rank factorization",
  "where B Rdoutr, A Rrdin, and r min{din, dout}": "LoRA and diffusion.Although initially proposed for fine-tuning LLMs, LoRA is generally applicable to awide range of other deep-learning modalities. Recent works used LoRA with diffusion models for varioustasks including image generation (Ryu, 2023; Gu et al., 2023; Go et al., 2023), image editing (Shi et al.,2023), continual learning (Smith et al., 2023), and distillation (Golnari, 2023; Wang et al., 2023b). While allthese works demonstrate the flexibility and efficacy of the LoRA architecture used for fine-tuning diffusionmodels, to the best of our knowledge, our work is the first attempt to use LoRA as part of the core U-Net fordiffusion models for full training, not fine-tuning.",
  "Conditioning the score network": "For diffusion models to work properly, it is crucial that the score network s is conditioned on appropriateside information. In the base formulation, the score function xpt(x), which the score network s learns,depends on the time t, so this t-dependence must be incorporated into the model via time conditioning. Whenclass-labeled training data is available, class-conditional sampling requires class conditioning of the scorenetwork (Ho & Salimans, 2021). To take advantage of data augmentation and thereby avoid overfitting, EDM(Karras et al., 2022) utilizes augmentation conditioning (Jun et al., 2020), where the model is conditioned onthe data augmentation information such as the degree of image rotation or blurring. Similarly, SDXL (Podellet al., 2024) uses micro-conditioning, where the network is conditioned on image resolution or croppinginformation. Finally, text-to-image diffusion models (Saharia et al., 2022; Ramesh et al., 2022; Rombach et al.,2022; Podell et al., 2024) use text conditioning, which conditions the score network with caption embeddingsso that the model generates images aligned with the text description. Conditioning attention layers.Prior diffusion models using CNN-based U-Net architectures conditiononly convolutional layers in the residual blocks by applying scale-and-shift or adaLN (see (left) of ).In particular, attention blocks are not directly conditioned in such models. This includes the state-of-the-artdiffusion models such as Imagen (Saharia et al., 2022), DALLE 2 (Ramesh et al., 2022), Stable Diffusion(Rombach et al., 2022), and SDXL (Podell et al., 2024). To clarify, Latent Diffusion Model (Rombachet al., 2022) based models use cross-attention method for class and text conditioning, but they still utilizescale-and-shift for time conditioning. There is a line of research proposing transformer-based architectures (without convolutions) for diffusionmodels, and these work do propose methods for conditioning attention layers. For instance, DiT (Peebles& Xie, 2023) conditioned attention layers using adaLN and DiffiT (Hatamizadeh et al., 2023) introducedtime-dependent multi-head self-attention (TMSA), which can be viewed as scale-and-shift conditioningapplied to attention layers. Although such transformer-based architectures have shown to be effective,whether conditioning the attention layers with adaLN or scale-and-shift is optimal was not investigated. In.5 of this work, we compare our proposed LoRA conditioning on attention layers with the prioradaLN conditioning on attention layers, and show that LoRA is the more effective mechanism for conditioningattention layers. Diffusion models as multi-task learners.Multi-task learning (Caruana, 1997) is a framework where asingle model is trained on multiple related tasks simultaneously, leveraging shared representations betweenthe tasks. If one views the denoising tasks for different timesteps (or SNR) of diffusion models as related butdifferent tasks, the training of diffusion models can be interpreted as an instance of the multi-task learning.Following the use of trainable lightweight adapters for Mixture-of-Expert (MoE) (Jacobs et al., 1991; Maet al., 2018), several works have utilized LoRA as the expert adapter for the multi-task learning (Cacciaet al., 2023; Wang et al., 2023a; 2024; Zadouri et al., 2024). Similarly, MORRIS (Audibert et al., 2023) andLoRAHub (Huang et al., 2023) proposed using the weighted sum of multiple LoRA adapters to effectivelytackle general tasks. In this work, we took inspiration from theses works by using a composition of LoRAadapters to condition diffusion models.",
  "Discrete-time LoRA conditioning": "Diffusion models such as DDPM (Ho et al., 2020) and IDDPM (Nichol & Dhariwal, 2021) have a predeterminednumber of discrete timesteps t = 1, 2, . . . , T used for both training and sampling. We refer to this setting asthe discrete-time setting. We first propose a method to condition the attention layers with LoRA in the discrete-time setting. Inparticular, we implement LoRA conditioning on IDDPM by conditioning the score network with (discrete)time and (discrete) class information.",
  "Wt = W + W(t) = W + BtAt": "for t = 1, . . . , T. To clarify, the trainable parameters for each linear layer are W, A1, A2, . . . , AT , andB1, B2, . . . , BT . In particular, W is trained concurrently with A1, A2, . . . , AT , and B1, B2, . . . , BT . However, this approach has two drawbacks. First, since T is typically large (up to 4000), instantiating Tindependent LoRAs can occupy significant memory. Second, since each LoRA (At, Bt) is trained independently,it disregards the fact that LoRAs of nearby time steps should likely be correlated/similar. It would bepreferable for the architecture to incorporate the inductive bias that the behavior at nearby timesteps aresimilar. Compositional LoRA.Compositional LoRA composes m LoRA bases, A1, . . . , Am and B1, . . . , Bm, wherem T. Each LoRA basis (Ai, Bi) corresponds to time ti for 1 t1 < < tm T. The dense layer at timet becomes",
  "i=1(t)i BiAi,": "where t = ((t)1 , . . . , (t)m) is the time-dependent trainable weights composing the LoRA bases. To clarify,the trainable parameters for each linear layer are W, A1, A1, . . . , Am, B1, B1, . . . , Bm, and t. Since the score network is a continuous function of t, we expect t t if t t. Therefore, to exploitthe task similarity between nearby timesteps, we initialize (t)i with a linear interpolation scheme: fortj t < tj+1,",
  "0otherwise": "In short, at initialization, W(t) uses a linear combination of the two closest LoRA bases. During training,t can learn to utilize more than two LoRA bases, i.e., t can learn to have more than two non-zeros throughtraining. Specifically, (1, . . . , T ) RmT is represented as an m T trainable table implemented asnn.Embedding in Pytorch.",
  "where s(x; ) is the score network conditioned on the SNR level . We refer to this setting as the continuous-SNR setting": "The main distinction between Sections 3 and 4 is in the discrete vs. continuous parameterization, sincecontinuous-time and continuous-SNR parameterizations of score functions are equivalent. We choose toconsider continuous-SNR (instead of continuous-time) parameterizations for the sake of consistency with theEDM model (Karras et al., 2022). Two additional issues arise in the present setup compared to the setting of . First, by consideringa continuum of SNR levels, there is no intuitive way to assign a single basis LoRA to a specific noise level.Second, to accommodate additional conditioning elements such as augmentations or even captions, allocatingindependent LoRA for each conditioning element could lead to memory inefficiency.",
  "Unified compositional LoRA (UC-LoRA)": "Consider the general setting where the diffusion model is conditioned with N attributes cond1, . . . , condN,which can be a mixture of continuous and discrete information. In our EDM experiments, we condition thescore network with N = 3 attributes: SNR level (time), class, and augmentation information. Unified compositional LoRA (UC-LoRA) composes m LoRA bases A1, . . . , Am and B1, . . . , Bm to simulta-neously condition the information of cond1, . . . condN into the attention layer. The compositional weight = (1, . . . , m) of the UC-LoRA is obtained by passing cond1, . . . condN through an MLP. Prior diffusion models typically process cond1, . . . , condN with an MLP to obtain a condition embedding v,which is then shared by all residual blocks for conditioning. For the j-th residual block, v is further processedby an MLP to get scale and shift parameters j and j:",
  "Experiments": "In this section, we present our experimental findings. .1 describes the experimental setup. .2first presents a toy, proof-of-concept experiment to validate the proposed LoRA conditioning. .3evaluates the effectiveness of LoRA conditioning on attention layers with a quantitative comparison betweendiffusion models with (baseline) conventional scale-and-shift conditioning on convolutional layers; (onlyLoRA) LoRA conditioning on attention layers without conditioning convolutional layers; and (with LoRA)conditioning both convolutional layers and attention layers with scale-and-shift and LoRA conditioning,respectively. .4 investigates the effect of tuning the LoRA rank and the number of LoRA bases..5 compares our proposed LoRA conditioning with the adaLN conditioning on attention layers..6 explores the robustness of ClassLoRA conditioning compared to conventional scale-and-shiftconditioning in extrapolating conditioning information.",
  "Experimental Setup": "Diffusion models.We implement LoRA conditioning on three different diffusion models: nano diffusion(Lelarge et al., 2024), IDDPM (Nichol & Dhariwal, 2021), and EDM-vp (Karras et al., 2022). With nanodiffusion, we conduct a proof-of-concept experiment. With IDDPM, we test TimeLoRA and ClassLoRA forthe discrete-time setting, and with EDM, we test UC-LoRA for the continuous-SNR setting. Datasets.For nano diffusion, we use MNIST. For IDDPM, we use CIFAR-10 for both unconditional andclass-conditional sampling, and ImageNet64, a downsampled version of the ImageNet1k, for unconditionalsampling. For EDM-vp, we also use CIFAR-10 for both unconditional and class-conditional sampling andFFHQ64 for unconditional sampling. Configurations.We follow the training and architecture configurations proposed by the baseline worksand only tune the LoRA adapters. For IDDPM, we train the model for 500K iterations for CIFAR-10 withbatch size of 128 and learning rate of 1 104, and 1.5M iterations for ImageNet64 with batch size of 128and learning rate of 1 104. For EDM, we train the model with batch size of 512 and learning rate of1 103 for CIFAR-10, and with batch size of 256 and learning rate of 2 104 for FFHQ64. For sampling,in IDDPM, we use 4000 and 4001 timesteps for the baseline and LoRA conditioning respectively, and inEDM, we use the proposed Heuns method and sample images with 18 timesteps (35 NFE) for CIFAR-10 and40 timesteps (79 NFE) for FFHQ64. Here, NFE is the number of forward evaluation of the score networkand it differs from the number of timesteps by a factor of 2 because Heuns method is a 2-stage RungeKuttamethod. Appendix A provides further details of the experiment configurations. Note that the baseline works heavily optimized the hyperparameters such as learning rate, dropout probability,and augmentations. Although we do not modify any configurations of the baseline and simply add LoRAconditioning in a drop-in fashion, we expect further improvements from further optimizing the configurationfor the entire architecture and training procedure. LoRA.We use the standard LoRA initialization as in the original LoRA paper (Hu et al., 2022): for theLoRA matrices (A, B) with rank r, A is initialized as Aij N(0, 1/r) and B as the zero matrix. FollowingRyu (2023), we set the rank of each basis LoRA to 4. For TimeLoRA and ClassLoRA, we use 11 and 10LoRA bases, and for UC-LoRA we use 18 and 20 LoRA bases for CIFAR-10 and FFHQ. Due to our constrained computational budget, we were not able to conduct a full investigation on the optimalLoRA rank or the number LoRA bases. However, we experiment with the effect of rank and number of LoRAbases to limited extent and report the result in .4.",
  "Published in Transactions on Machine Learning Research (09/2024)": "where i corresponds to the interpolation or scaling of the class inputs. From this perspective, the weightscould be interpreted as a natural latent vector capturing the semantic information of the image, in asimilar sense that was highlighted in Kwon et al. (2023). While our current focus was exclusively on classinformation, we hypothesize that this method could be extended to train style or even text conditioning,especially considering the effectiveness of LoRA for fine-tuning.",
  "Main quantitative results": "Simply adding LoRA conditioning yields improvements.To evaluate the effectiveness of the drop-inaddition of LoRA conditioning to the attention layers, we implement TimeLoRA and ClassLoRA to IDDPMand UC-LoRA to EDM, both with the conventional scale-and-shift conditioning on the convolutional layersunchanged. We train IDDPM with CIFAR-10, ImageNet64 and EDM with CIFAR-10, FFHQ64. As reportedin , the addition of LoRA conditioning to the attention layers consistently improves the imagegeneration quality as measured by FID scores (Heusel et al., 2017) across different diffusion models anddatasets with only (10%) addition of the parameter counts. Note these improvements are achieved withouttuning any hyperparameters of the base model components.",
  "Effect of LoRA rank and number of LoRA bases": "We investigate the effect of tuning the LoRA rank and the number of LoRA bases on the EDM model forunconditional CIFAR-10 generation and report the results in . Our findings indicate that using moreLoRA bases consistently improves the quality of image generations. On the other hand, increasing LoRArank does not guarantee better performance. These findings suggest an avenue of further optimizing andimproving our main quantitative results of .3 and , which we have not yet been able topursue due to our constrained computational budget.",
  "Comparison with adaLN": "We compare the effectiveness of our proposed LoRA conditioning with adaLN conditioning applied to attentionlayers. Specifically, we conduct an experiment on EDM with scale-and-shift conditioning on convolutionallayers removed and with (i) adaLN conditioning attention layers or (ii) LoRA conditioning attention layers.We compare the sample quality of unconditional and class-conditional CIFAR-10 generation and report theresults in . We find that LoRA conditioning significantly outperforms adaLN conditioning for bothunconditional and conditional CIFAR-10 generation. This indicates that our proposed LoRA conditioning isthe more effective mechanism for conditioning attention layers in the U-Net architectures for diffusion models.",
  "Extrapolating conditioning information": "We conduct an experiment comparing two class-conditional EDM models each conditioned by scale-and-shiftand ClassLoRA, for the CIFAR-10 dataset. During training, both models receive size-10 one-hot vectors(ci)j = ij representing the class information. First, we input the linear interpolation ci +(1)cj (0 1) of two class inputs ci and cj (correspondingto airplane and horse, respectively) to observe the continuous transition between classes. As shown in thetop of , both the scale-and-shift EDM and ClassLoRA EDM models effectively interpolate semanticinformation across different classes. However, when a scaled input ci is received, with ranging from -1to 1, scale-and-shift EDM generates unrecognizable images when < 0, while ClassLoRA EDM generatesplausible images throughout the whole range, as shown in the bottom of . This toy experiment showsthat LoRA-based conditioning may be more robust to extrapolating conditioning information beyond therange encountered during training. Appendix D provides further details. : Results of (Top) interpolation of class labels in class-conditional EDM with (row1) ClassLoRA;(row2) scale-and-shift; (bottom) extrapolation of class labels in class-conditional EDM with (row1) Class-LoRA; (row2) scale-and-shift",
  "Conclusion": "In this work, we show that simply adding Low-Rank Adaptation (LoRA) conditioning to the attention layersin the U-Net architectures improves the performance of the diffusion models. Our work shows that we shouldcondition the attention layers in diffusion models and provides a prescription for effectively doing so. Someprior works have conditioned attention layers in diffusion models with adaLN or scale-and-shift operations,but we find that LoRA conditioning is much more effective as discussed in .5. Implementing LoRA conditioning on different and larger diffusion model architectures is a natural andinteresting direction of future work. Since almost all state-of-the-art (SOTA) or near-SOTA diffusion modelsutilize attention layers, LoRA conditioning is broadly and immediately applicable to all such architectures.In particular, incorporating LoRA conditioning into large-scale diffusion models such as Imagen (Sahariaet al., 2022), DALLE 2 (Ramesh et al., 2022), Stable Diffusion (Rombach et al., 2022), and SDXL (Podellet al., 2024), or transformer-based diffusion models such as U-ViT (Bao et al., 2023), DiT (Peebles & Xie,2023), and DiffiT (Hatamizadeh et al., 2023) are interesting directions. Finally, using LoRA for the textconditioning of text-to-image diffusion models is another direction with much potential impact.",
  "Acknowledgements": "This research was supported by a grant from KRAFTON AI. AN was supported in part by the Ministry ofScience and ICT (MSIT), South Korea, under the Information Technology Research Center (ITRC) SupportProgram #IITP-2024-RS-2022-00156295. EKR was supported by the National Research Foundation ofKorea(NRF) grant funded by the Korean government (No.RS-2024-00421203). We thank Sehyun Kwon andDongwhan Rho for their careful reviews and valuable feedback. We also thank the anonymous reviewers fortheir insightful comments.",
  "SimoRyu.Low-rankadaptationforfasttext-to-imagediffusionfine-tuning. 2023": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar SeyedGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho,David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep languageunderstanding. NeurIPS, 2022. Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vincent YF Tan, and Song Bai. DragDiffusion:Harnessing diffusion models for interactive point-based image editing. arXiv preprint arXiv:2306.14435,2023. James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin.Continual diffusion: Continual customization of text-to-image diffusion with c-lora.arXiv preprintarXiv:2304.06027, 2023.",
  "A.1Nano Diffusion": "We mostly followed the base code provided by Dataflowr with 3 exceptions. First, the sinusoidal embeddingimplemented in the original code was not correctly implemented. Although it did not have visible impact onTimeLoRA and UC-LoRA, it significantly deteriorated the sample quality of the scale-and-shift conditioning(see ). Second, during training process, the input image is normalized with mean = 0.5 and std = 0.5.However, it is not considered in the visualization process. Lastly, we extended the number of training epochsfrom 50 to 100 for better convergence.",
  "A.2.1CIFAR-10": "For CIFAR-10 training, we construct U-Net with model channel of 128 channels, and 3 residual blocks per eachU-Net blocks. We use Adam optimizer with learning rate of 1 104, momentum of (1, 2) = (0.9, 0.999),no weight decay, and dropout rate of 0.3. We train the model with Lhybrid proposed in the original paper for500k iterations with batch size of 128. The noise is scheduled with the cosine scheduler and the timestep issampled with uniform sampler at training. For sampling, we use checkpoints saved every 50k iterations withexponential moving average of rate 0.9999, and sample image for 4000 steps and 4001 steps for the baselineand LoRA conditioning, respectively.",
  "A.2.2ImageNet64": "For ImageNet64 training, we use the same U-Net construction used in CIFAR-10 experiment. The modelchannel is set as 128 channels, and each U-Net block contains 3 residual blocks. We use Adam optimizer withlearning rate of 1 104, momentum of (1, 2) = (0.9, 0.999), no weight decay and no dropout. We trainthe model with Lhybrid proposed in the original paper for 1.5M iterations with batch size of 128. The noise",
  "A.3.1CIFAR-10": "For CIFAR-10 training, we use the DDPM++ (Song et al., 2021b) with channel per resolution as 2, 2, 2. Weuse Adam optmizer with learning rate of 1 103, momentum of (1, 2) = (0.9, 0.999), no weight decay anddropout rate of 0.13. We train the model with EDM preconditioning and EDM loss proposed in the paper for200M training images (counting repetition) with batch size of 512 and augmentation probability of 0.13. Forsampling, we used checkpoints saved every 50 iterations with exponential moving average of 0.99929, andsample image using Heuns method for 18 steps (35 NFE).",
  "A.3.2FFHQ64": "For FFHQ training, we use the DDPM++ (Song et al., 2021b) with channel per resolution as 1, 2, 2, 2. Weuse Adam optmizer with learning rate of 2 104, momentum of (1, 2) = (0.9, 0.999), no weight decay anddropout rate of 0.05. We train the model with EDM preconditioning and EDM loss proposed in the paper for200M training images (counting repetition) with batch size of 256 and augmentation probability of 0.15. Forsampling, we used checkpoints saved every 50 iterations with exponential moving average of about 0.99965,and sample image using Heuns method for 40 steps (79 NFE).",
  "A.5MLP for composition weights of UC-LoRA": "For the composition weights of UC-LoRA, we used 3-layer MLP with group normalization and SiLU activation.Specifically, each LoRA module contains a MLP consisting of two linear layers with by group normalizationand SiLU activation followed by a output linear layer. MLP takes the shared condition embedding v Rdembas the input and outputs the composition weight (v) Rm, where demb is the embedding dimension (512for EDM) and m is the number of LoRA bases. This is implemented in Pytorch as self.comp_weights = nn.Sequential(Linear(in_features=embed_dim, out_features=128),GroupNorm(num_channels=N1, eps=1e-6),torch.nn.SiLU(),Linear(in_features=N1, out_features=N2),GroupNorm(num_channels N2, eps=1e-6),torch.nn.SiLU(),Linear(in_features=N2, out_features=num_basis),) In our experiment, we set (N1, N2) = (50, 50) for nano diffusion and (N1, N2) = (128, 64) for EDM. Notethe choice of depth, and the width of the MLP is somewhat arbitrary and can be further optimzied.",
  "BCompositional ClassLoRA": "To verify the effectiveness of a compositional version of ClassLoRA, we conducted a proof-of-conceptexperiment with EDM on CIFAR-10. We finetuned a pretrained unconditional EDM for class-conditionalCIFAR-10 sampling using a single LoRA basis and randomly initialized compositional weights. Note in thenon-compositional ClassLoRA, we used 10 LoRA bases, one for each class. As a result, with the addition ofonly 61560 parameter counts, we were able to convert (fine-tune) an unconditional EDM to a conditionalEDM and improve the FID score from 1.97 to 1.81.",
  "CCosine similarity between ts in nano diffusion": "We present cosine similarity between t and 500 for all LoRA bases in UC-LoRA for nano diffusion in . As observed in .2 and , cosine similarity is consistently high for t close to 500, provingthat there is a task similarity between nearby timesteps for all layers of the network. However, the patternsvaried depending on the depth of the layer within the network. This could be an interesting point for futureresearch to further understand the learning dynamics of the diffusion models.",
  "DExtrapolating conditioning information with Class LoRA": "We present a more detailed analysis along with comprehensive results of the experiments introduced in.6. The class-conditional EDM model used for comparison was trained with the default trainingconfigurations explained in Appendix A.3. For ClassLoRA-conditioned EDM, we used the unconditionalEDM as the base model and applied ClassLoRA introduced in .2 for the discrete-time setting. Wedid not use UC-LoRA for this ablation study, with the intent of focusing on the effect of LoRA conditioningon class information. We provide interpolated and extrapolated images as introduced in 5.6 for various classes in and, corroborating the consistent difference between the two class conditioning schemes across classes.We also experiment with strengthening the class input, where we use scaled class information input ci with > 1. shows that LoRA conditioning shows more robustness in this range as well. Considering the formulation of ClassLoRA, interpolating or scaling the class input is equivalent to interpolatingor scaling the class LoRA adapters, resulting in a formulation similar to Compositional LoRA or UC-LoRA:"
}