{
  "Abstract": "Graph privacy is crucial in systems that present a graph structure where the confidentialityand privacy of participants play a significant role in the integrity of the system itself. Forinstance, it is necessary to ensure the integrity of banking systems and transaction net-works, protecting the privacy of customers financial information and transaction details.We propose a method called GraphPrivatizer that privatizes the structure of a graph andprotects it under Differential Privacy. GraphPrivatizer performs a controlled perturbationof the graph structure by randomly replacing the neighbors of a node with other similarneighbors, according to some similarity metric. With regard to neighbor perturbation, wefind that aggregating features to compute similarities and imposing a minimum similarityscore between the original and the replaced nodes provides the best privacy-utility trade-off.We use our method to train a Graph Neural Network server-side without disclosing usersprivate information to the server. We conduct experiments on real-world graph datasetsand empirically evaluate the privacy of our models against privacy attacks.",
  "Introduction": "In recent years, many research efforts have been made to effectively learn from graph-structured data. Graph-based approaches have been successful in a variety of tasks such as fake news detection in social networks(Benamira et al., 2019) and drug discovery (Gaudelet et al., 2021). Graphs can incorporate both informationabout individual data points and about their interactions: Graph Neural Networks (GNNs, Scarselli et al.,2008) have been in capturing this information and learning over graph-structured data. Both the informationabout the individual data points and the relational information can be, however, of a sensitive nature andmust, therefore, be protected. Large scale machine learning models may require sending information to aserver where the training is performed, which poses a privacy risk. Efforts have thus been recently made toaddress privacy attacks on graphs (Zhang et al., 2021; 2022). One possibility to protect private informationon graphs is to use the formal privacy guarantees offered by Differential Privacy (DP, Dwork, 2006), whoserange of applications on graph-structured data has been recently expanding (Mueller et al., 2022b). DP hasbeen used both in centralized settings where a server has graph-wide access to information (Olatunji et al.,2023; Wu et al., 2022; Sajadmanesh et al., 2023) and in local settings (Sajadmanesh & Gatica-Perez, 2021).",
  "Published in Transactions on Machine Learning Research (09/2024)": "We would like to thank Prof. Thomas Grtner (TU Wien) for his helpful comments on the manuscript. Thiswork was funded in part by the TU Wien DK SecInt and by the Austrian Science Fund (FWF), projectNanOX-ML (6728).The computational results presented have been achieved in part using the ViennaScientific Cluster (VSC) and NISER:RIN4001. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computerand communications security, pp. 308318, 2016. Morgane Ayle, Jan Schuchardt, Lukas Gosch, Daniel Zgner, and Stephan Gnnemann. Training differ-entially private graph neural networks with random walk sampling. In Workshop on Trustworthy andSocially Responsible Machine Learning, NeurIPS 2022, 2022. Adrien Benamira, Benjamin Devillers, Etienne Lesot, Ayush K Ray, Manal Saadi, and Fragkiskos D Malliaros.Semi-supervised learning and graph neural networks for fake news detection. In Proceedings of the 2019IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, pp. 568569,2019.",
  "Related Work": "GNNs have gained increasing popularity as the framework of choice to solve graph-based learning tasks inrecent years. The efficacy of GNNs in graph representation learning has motivated the proposal of severalGNN variants such as Graph Convolutional Networks (Zhang et al., 2019), Graph Attention Networks(Velikovi et al., 2018), and GraphSAGE (Hamilton et al., 2017), as well as architectures for large multi-relation graphs (Iyer et al., 2021; Wang et al., 2019). Recent research efforts have also been made to addressprivacy attacks on graphs (Zhang et al., 2021; 2022). Privacy attacks can be categorized as graph propertiesattacks (inferring, e.g., the number of nodes), membership attacks (inferring, e.g., whether a subgraph ispart of a graph), and graph reconstruction attacks (Zhang et al., 2022). Specifically, the existence of an edgebetween two nodes is often sensitive in nature (Mueller et al., 2022b) and should be kept private. Differential Privacy (DP, Dwork, 2006) offers formal privacy guarantees to protect information about in-dividual training points, and has been used to provide privacy guarantees in GNNs as well. DP has beenutilized in centralized settings where a server has access to information on the entire graph (Olatunji et al.,2023; Wu et al., 2022; Sajadmanesh et al., 2023), and in local settings (Sajadmanesh & Gatica-Perez, 2021;Joshi & Mishra, 2022; 2023). Different formulations of DP on graphs aim at protecting the relationshipbetween nodes (edge-level DP) (Raskhodnikova & Smith, 2016; Hidano & Murakami, 2022), the individualnodes themselves (node-level DP) (Raskhodnikova & Smith, 2016; Ayle et al., 2022; Kasiviswanathan et al.,2013; Olatunji et al., 2023), or the entire graph as a single entity (graph-level DP) (Mueller et al., 2022a).For a survey on recent advances in DP approaches on structured data, refer to Mueller et al. (2022b). In this work, we focus on protecting the structure of the graph, i.e., hiding its edges. Previous work hasaddressed structural privacy using central (Sajadmanesh et al., 2023; Olatunji et al., 2023) or local (Joshi &Mishra, 2022; 2023; Hidano & Murakami, 2022) DP; these approaches require however some entity to haveaccess to the entire noiseless adjacency matrix of the graph (Sajadmanesh et al., 2023; Joshi & Mishra, 2022)or to part of it (Joshi & Mishra, 2023; Hidano & Murakami, 2022) in order to privatize the graph structure.Such approaches may thus pose privacy concerns or depend on the availability of public data (Olatunji et al.,2023). Additionally, approaches such as Sajadmanesh et al. (2023) require the introduction of a custom",
  "Preliminaries and Problem Statement": "In this section we recall the definitions of Graph Neural Network (GNN, Scarselli et al., 2008) and LocalDifferential Privacy (LDP, Dwork, 2006; Yang et al., 2020). Then, we briefly discuss randomized response(RR, Warner, 1965) and edge privacy in graphs, as well as LinkTeller (Wu et al., 2022) as the privacy attackwe use to validate our approach. Finally, we describe our local privacy setting and problem statement.",
  "Graph Neural Networks": "Consider an unweighted graph defined as a tuple G = (V, E, X, Y ), where V = VL VU is the set union oflabeled nodes VL and unlabelled nodes VU, E is the set of edges, X R|V |d is a feature matrix consistingof d-dimensional feature vectors, one for each node v V , and Y is the set of labels. Let N(v) denote theneighborhood of v, that is, the set of nodes which are adjacent to v. Let deg(v) denote the degree of v, that is,the size of its neighborhood. Graph Neural Networks (GNNs, Scarselli et al., 2008) are a class of models thathave been effective in learning over graph-structured data. A typical GNN consists of L layers where theembeddings of the nodes in a certain layer are obtained from the previous layer by means of an aggregationand an update function. Specifically, the embedding hlv for a node v in layer l is obtained by aggregating theembeddings of its neighbors N(v) from layer l 1 and passing the resulting aggregated message mlv throughthe update function. The aggregation function is a permutation invariant and differentiable function, whilethe update function is a trainable and non-linear function:",
  "Differential Privacy": "Differential Privacy (DP, Dwork, 2006) is a formal definition of privacy that protects individual trainingpoints.As originally introduced by Dwork (2006), central or global DP, simply referred to as DP, wasdesigned for a centralized setting where a trusted entity gathers all user data and guarantees to process itand produce an output while preserving the privacy of users. More formally, DP guarantees that an attackercannot confidently infer whether the output of a DP mechanism M was obtained from a database D orfrom a database D, where D and D differ in a single record and are thus said to be adjacent datasets. Ina local privacy setting no trusted entity can process the private data of users. In this more restrictive caseeach user perturbs its data locally and provides the central entity with a noisy version of its data only. In alocal privacy setting the datasets D thus consist of data from individual users which is then protected underLocal Differential Privacy (LDP, Yang et al., 2020).",
  "Pr[M(x) S] e Pr[M(x) S]": "We refer to as the privacy budget of the algorithm. In particular, = 0 indicates that the randomizedmechanism is perfectly private and implies that the output of the mechanism is independent of the input. Onthe other hand, = provides no privacy guarantee. The choice of is both problem and data dependent(Lee & Clifton, 2011), with common ranges often considering values (0, 10] (Wu et al., 2022; Sajadmanesh& Gatica-Perez, 2021). For a given deterministic function, DP can be achieved by adding random noise tothe output of the function to hide the contribution of individual training points, where the amount of noiseadded depends on the choice of privacy budget (Dwork et al., 2014). As no single entity is trusted withall the users private data, LDP is generally a stronger privacy model than DP. However, the lack of suchtrusted central entity and thus the need to add noise to the local data of each user before communicating itto the central entity entails a greater total noise which can negatively affect model performance (Cormodeet al., 2018). This, in turn, enhances the importance of novel approaches that can provide LDP with goodaccuracy which are thus our focus.",
  "DP and privacy attacks in GNNs": "DP can be applied to graph context by defining a notion of adjacency for graphs. Considering the (central-ized) DP setting first, we say two graphs G and G are edge adjacent if they differ exactly in one edge, thatis, if G can be obtained from G by adding or removing a single edge. Similarly, G and G are node adjacentif they differ in exactly one node, that is, if G can be obtained from G by adding or removing a single nodeand its edges. In our local privacy setting we consider LDP which can guarantee privacy on graphs in thesense of Definition 3.1 on any pair of adjacent user inputs. Focusing here on edge privacy, one can define anotion of edge adjacency to protect a users edges. A common definition of edge LDP considers a users vneighbor list as represented by |V |-dimensional bit vector (b1, . . . , b|V |), where bi,i=1,|V | = 1 if and only ifthere is an edge between node v and node vi, and bi = 0 otherwise. Definition 3.2 (-edge LDP, (Qin et al., 2017)). Let > 0. A randomized mechanism M : D R satisfies-edge local differential privacy if, for any possible pairs of users neighbor lists b, b differing by one bit, andfor any possible outputs S R, it holds that:",
  "Pr[M(b) S] e Pr[M(b) S]": "-edge set LDP can be seen as a relaxation of -edge LDP Definition 3.2, as it practically entails a morecontrolled perturbation of the edges of a node. To be more explicit on the relation between -edge set LDP(Definition 4.2) and the -edge LDP (Definition 3.2) notion used by related work such as Joshi & Mishra(2023), note that one can obtain any set perturbation as described in Definition 4.1 by means of two bitflips on the neighbor list of a node. A 2-edge LDP mechanism is thus also -edge set LDP. The conversedoes not hold as not all two bit flips on the neighbor list of a node correspond to a set perturbation as",
  "Problem statement": "We address private learning on graphs with LDP where eachnode can is considered as an individual user: for a graphG = (V, E, X, Y ) we consider V , E, X, and Y to be privateto individual nodes/users, which only share noisy versions ofthem with a server. The server uses the information to traina GNN for node classification, learning to predict labels Yin a semi-supervised learning setting. More explicitly, no en-tity (neither users/nodes, nor the server) has complete andnoise-free information about the graph. The only noiseless in-formation a node can have access to is its own features, label,and edges (). We elect this as our setting of choicebecause of its similarities with real-world scenarios where indi-vidual nodes/users may not have knowledge about the entiregraph beyond the nodes/users they directly interact with.",
  "Approach": "In this section, we introduce our approach, called GraphPrivatizer, which provides edge, feature, and labelprivacy. Specifically, we focus on investigating the trade-off between edge privacy and GNN performance inthe local privacy setting described in .4. A global notion of differential privacy is not allowed byour local privacy setting, as no central entity is trusted with the entire graph. -edge LDP (Definition 3.2)too has limitations in our privacy setting, as we will discuss in the next section. Therefore, we provide anew definition of edge privacy for our setting and propose a novel edge private algorithm based on it. Toguarantee label and/or feature privacy, GraphPrivatizer uses existing techniques presented in .3.",
  "Adjacent neighborhoods": "The definition of -edge LDP provided in .3 entails the perturbation of the neighbor list of a nodev using RR, where two neighbor lists are adjacent if they differ by one bit. As previously mentioned, thisperturbation leads to a great increase in the connectivity of the graph for small, thus desirable, values of. Moreover, in the local privacy setting described in .4 a nodes neighbor list include only itsimmediate neighbors and there are therefore no graph-wide neighbor lists to perturb with the standard RRapproach. To define a notion of privacy which is appropriate for our setting we choose to consider the set ofneighbors of node v: two neighbor sets are said to be adjacent if they differ by a single node. Definition 4.1 (Adjacent neighborhoods). Consider a node v.Let b = {v1, . . . , vd} = N(v), b ={v1, . . . , vd} = N (v) be two neighbor sets, with d = deg(v).We say N(v) and N (v) are adjacent ifthey differ in only one element; that is, they are adjacent neighborhoods if, without loss of generality, v1 = v1and vi = vi for i = 2, . . . , d.",
  "Edge privacy": "Equipped with the notion of adjacency in Definition 4.1, we develop a perturbation technique that acts onthe neighbor set of a node which is edge set LDP in the sense of Definition 4.2. Our approach informallyseeks to replace nodes in a neighbor set with other nodes that are similar with respect to some similaritymeasure. The perturbed neighborhood can in this way retain more of the original information content ofthe neighborhood and provide good performance. Specifically, our proposal randomly replaces a neighbor uof a node v with one of the neighbors of u itself. That is, we perform perturbations considering nodes in thetwo-hop extended local view (Sun et al., 2019) of v. Two neighborhoods of v are then adjacent according toDefinition 4.1 if one can be obtained from the other by means of an edge perturbation within the two-hopexpended local view of v which preserves the degree of v. The replacement itself then occurs via RR, whichensures the privacy of the procedure. Conceptually, we can describe our method as consisting of two steps.The neighbor set N(v) of a node v is perturbed by: (i) selecting a set of candidate replacement nodes forthe neighbors of v and (ii) randomly picking of the replacement nodes using RR. Algorithm 1 describes theprocedure. We provide a summary of the main notation for ease of read.",
  "s(v, u)cosine similarity betweenxv, and xu,, eq. (4)": "Consider a node v and assume we want to perturb its neighbor set N(v) by replacing some of its nodes.Nodes u N(v) are randomly replaced with nodes picked in a set of candidates, where the candidates areselected from nodes in the two-hop extended local view of v according to a similarity measure s. That is,the set of nodes which are considered as candidates to replace a node u is constituted of nodes u that aresimilar to u. Given our local privacy setting, non-parametric and non-learnable similarity measures are anatural choice as no prior information on the data is available. We provide a comparison of the performanceof our approach using the Euclidean distance and the cosine similarity in Appendix B, and find that thecosine similarity is preferable.",
  "We therefore measure the similarity between u and a candidate u using the cosine similarity s of theirfeature vectors xu and xu, s =xuxu": "xuxu. In this regard, we devise two strategies to obtain the set ofcandidates based on similarity, which are described in Algorithm 3 and Algorithm 4. For both strategies,only the nodes u which have a similarity score exceeding a threshold are selected as the set of candidates.As GNNs perform aggregations of the features of neighbors produce embeddings, we additionally propose touse such aggregated features to compute similarity scores. We therefore evaluate the similarity of a node uusing Aggu = Aggregate({xn : n N(u)}) instead of xu. We denote with the hyper-parameter whichparameterizes the contribution of aggregated features in the similarity computation. That is, for each nodeu we compute the aggregated feature vector xu, of node u and N(u) as",
  "xu,xu,.(4)": "To summarise, > 0 can be used to filter out dissimilar replacement candidates, while > 0 is usedto introduce aggregate information in the similarity computation.The case = = 0 introduces nothresholds for the application of RR and no aggregation, and will thus be used as our reference to in-vestigate the impact of such thresholds and aggregations. When computing similarities/aggregations be-tween nodes, noisy feature vectors obtained according to .3 are used, ensuring that feature pri-vacy is not violated.Our method is thus consistent with the setting described in .4 as weassume that the only noiseless information a node has access to consists of its own features, labels,and edges to 1-hop neighbors.Algorithm 2 describes the procedure to select replacement candidates.",
  ": end for7: return T (v)": "For each node v, the number of similarity values which need to be computed to perturb its neighborhood de-pends on the number of nodes in the two-hop extended local view of v and is upper bounded uN(v) deg(u)which may thus be computationally expensive for very dense graphs. However, in a practical setting wherenodes are distributed among different computing units, the neighbor perturbation is performed locally bythe individual nodes. Moreover, the nodes only require the (perturbed, possibly aggregated) features of theirneighbor to compute their similarity with them. In terms of communication cost, this amounts to two appli-cations of the Aggregate (.1) function and is done as a pre-processing step before training. Withthese considerations and the observation that real-world datasets have a small average degree (), weexpect an efficient implementation of our approach to scale well to sparse large graphs. Once a set of candidate nodes has been obtained, the neighbor perturbation is performed in Algorithm 1with RR. The probabilities of replacement associated with RR differ whether we consider one candidatereplacement, (strategy in Algorithm 3) or a set of candidate replacements (strategy in Algorithm 4). Denotewith Pr[u u] the probability that node u gets replaced with node u. If we consider the most similarreplacement candidate only, RR is applied as follows.",
  "Theorem 4.3. Algorithm 1 is -edge set LDP": "Proof. Algorithm 1 with the neighbor selection described in Algorithm 3 is a randomized mechanism basedon RR: we denote it as M. The proof follows from the DP of RR (see, e.g., Qin et al., 2017). Denotewith Pr[v u] the probability that a node v gets replaced by a node u, let p =1 e+1 be the probability ofnode replacement according to RR, and q = 1 p. Note that > 0 implies q > p. Let b = {v1, . . . , vd},b = {v1, . . . , vd} be two neighbor sets which differ in only one element; assume, without loss of generality,that v1 = v1. Then, given any output s = {s1, . . . , sd} of M, it holds that:",
  "Feature and label privacy": "In addition to edge privacy, GraphPrivatizer also ensures feature and label LDP. That is, for each node,GraphPrivatizer ensures that an attacker cannot confidently infer the feature vector or the label. Specifically,we make use of the Drop algorithm introduced in Sajadmanesh & Gatica-Perez (2021), which enables efficientLDP GNN training with both private labels and node features. Features are privatized with a multi-bitmechanism, which allows individual nodes to perturb their features before communicating them. Labelsare, instead, privatized using RR: a nodes class is randomly replaced with one of the other available classeswith the same approach described in Equation (6). We refer the reader to the original publication for moredetails. We assign a privacy budget x for feature privacy and y for label privacy.",
  "Theorem 4.4. GraphPrivatizer is + x + y LDP": "Proof. The private feature vectors are only used by the multi-bit mechanism, the private labels are only usedby the RR mechanism for labels, and the private edge information is only used by Algorithm 1. In particular,Algorithm 1 only post-processes the privatized feature vectors and, due to the composition and robustnessto post-processing properties of DP (Dwork et al., 2014), GraphPrivatizer is thus + x + y LDP.",
  ": Scheme of GraphPrivatizer and how it acts on the features, edges, and labels of a node v. Theshaded area highlights our contribution and where the algorithms we propose are utilized": "parameters and on the privacy-accuracy trade-off for GraphPrivatizer. With respect to this trade-off, asdiscussed in .2, we use the setting = = 0 as our reference and perform comparisons with , > 0to evaluate the benefits of higher thresholds and aggregation coefficients. We empirically evaluate theedge privacy of our approach against attacks that try to recover the private edges. We use a variety ofGNN architectures that include traditional convolutional GNNs, graph attention networks, and transformernetworks, and perform experiments on the most commonly used benchmark datasets for node classificationthat include citation, co-purchase, and social networks. We experiment with GCN (Kipf & Welling, 2017),GraphSAGE (Hamilton et al., 2017), GAT (Velikovi et al., 2018), GT (a graph transformer adapted fromShi et al. (2021)), GATv2 (Brody et al., 2022) and GraphConv (the graph convolution operator introducedin Morris et al. (2019)) on the Cora (Yang et al., 2016), Pubmed (Yang et al., 2016), LastFM (Rozemberczki& Sarkar, 2020), Facebook (Rozemberczki et al., 2021), and Amazon Photo (Shchur et al., 2018) datasets.In what follows, we will take the feature privacy budget x and label privacy budgets y to be fixed: theresults and discussion will therefore focus on the edge privacy parameter which is simply referred to asprivacy budget. We leave additional details on hyper-parameters and on the datasets used to Appendix A. With regard to the privacy attack, we assume the trained GNN exposes an inference API that an attackercan query. We assume the attacker possesses feature information on pairs of nodes and wishes to determinewhether an edge connects each pair of nodes. It should be noted that, according to the data model weadopt and describe in .4, no entity, either users or server, possesses noise-free information aboutother nodes features.For this reason, we assume that the attacker itself may only have access to thenoisy feature vector that a node sends to the server for, e.g., training. With these assumptions, we attackall models with LinkTeller (Wu et al., 2022) except for the GraphSAGE, which we attack with LSA2 (Heet al., 2021; Wu et al., 2022). For LinkTeller, we use the default influence and graph density parameters of0.001 and 1 (Wu et al., 2022). In all cases, we randomly sample 500 pairs of nodes that are connected in theoriginal, unperturbed graph, as well as 500 pairs of nodes that are not connected in the original, unperturbedgraph. The task of the attacker is to decide which of these nodes are connected or not connected, in abinary classification problem. We evaluate the performance of the attacker using the AUC, which we reportmultiplied by a factor 102, that is, AUC . LinkTeller is a threshold-based binary classifier, so we usethe AUC as a performance measure to capture all threshold values. A higher AUC denotes a higher abilityof the attack to correctly identify the edges of the graph, and thus lower edge privacy, where an increase of1 AUC point can be interpreted as a 1% increase in the likelihood of correctly identifying edges. It shouldbe noted that the attacker has access to the API of a GNN which was trained on perturbed data, while weevaluate the attack AUC with respect to the original unperturbed data, which is what should be protected.",
  "Comparison against baseline approaches": "We compare GraphPrivatizer against a baseline approach adapted from existing literature on private nodeclassification. Specifically the baseline approach is adapted from Sajadmanesh & Gatica-Perez (2021) andWu et al. (2022) to our setting. Specifically, for the baseline we consider a slightly relaxation of the settingdescribed in and assume that each node has access to the list of nodes in its 2-hop neighborhood. We",
  "Cora6.3 2.212.4 6.3LastFM6.3 3.221.7 13.8PubMed1.9 0.22.7 1.7Facebook5.0 0.613.2 8.5Amazon Photo3.4 2.78.0 3.3": "Across all models and datasets, GraphPrivatizer provides an average 6.6 AUC points improvement in privacywith respect to the non-edge-private LPGNN (Sajadmanesh & Gatica-Perez, 2021) setting while suffering a4.9% decrease in accuracy, while the RR baseline provides a similar 6.4 AUC points improvement in privacybut with a much less desirable 12.5% decrease in accuracy. Overall, GraphPrivatizer achieves thus a betterprivacy-utility trade-off than RR. Additional results in Appendix C.",
  "Results for GraphPrivatizer and discussion": "While in .1 we show that GraphPrivatizer outperforms the RR baseline approach and improves uponthe privacy-utility trade-off, here we investigate our approach in more detail and determine to which degreethe use of thresholds and aggregations described in is beneficial. We are interested in establishingif positive threshold and aggregation parameters (i.e., , > 0) consistently provide a better privacy-utilitytrade-off than the reference case where no aggregation or threshold are considered (i.e., = = 0). : Aggregate results for GraphPrivatizer. We denote as AccGP and AUCGP the average accuracyand AUC results for GraphPrivatizer with , > 0, where the average is taken across all positive testedvalues of and . We denote with Acc the average accuracy difference with the = = 0 case acrossall tested values of and larger than zero, where positive values of Acc indicate that GraphPrivatizerperforms better when some thresholds are applied and/or aggregation is performed during the neighborperturbation. Analogously, we denote with AUC the average AUC102 difference, where values close tozero indicate that GraphPrivatizer offers the same protection against privacy attack when introducing apositive threshold and/or feature aggregation during the neighbor perturbation. We denote the datasets weuse as Cora (Cr), LastFM (FM), PubMed (PM), Facebook (Fb), and Amazon Photo (Ph). Results reportedas (average values standard deviation), for = 0.1.",
  "Cr80.4 1.077 34 23 2FM84.2 1.384 33 11 1PM81.6 0.355 31.5 0.70 1Fb90.9 0.274 22 11 2Ph86.1 3.590 20.8 0.68 5": "We present an aggregate overview of our results in . We focus here on the case = 0.1 and onthe GCN, GraphSAGE, and GAT models, with additional results available in Appendix C. GraphPrivatizerconsistently performs better with , > 0 in terms of accuracy, as it provides a positive accuracy improvementAcc in almost the totality of cases. In more than half of the cases, it also provides performance in defendingagainst privacy attacks which is equivalent to the = = 0 setting, having AUC which overlaps with zero.It should moreover be noted that the AUC results reported in are, as mentioned, multiplied bya factor 102. For this reason, most of the cases where AUC > 0 correspond to only a small decrease inprivacy, with the majority of cases reporting AUC 2 which indicates an increase in the likelihood that theattacker is able to correctly identify edges of at most 2%. That is, it is on average preferable to introducefeature aggregation and/or to a threshold to select similar neighbors during the neighbor perturbation.If we compare the different models tested, GCN appears to be more prone to worse performance againstprivacy attacks. This behavior is consistent with the observations reported in Wu et al. (2022) and maybe explained by highlighting how the influence computation that underlies the LinkTeller attack is bettersuited for graph convolution aggregations, and thus for GCNs (more details on Wu et al. (2022)). Whileinformation propagation between nodes can be exploited for other GNN architecture the introduction of,e.g., the attention mechanism in GAT/GATv2 can negatively impact the attack performance. Nevertheless, and show that GraphPrivatizer performs well for different GNN models and datasets.",
  ": Results for GP-t with GAT, for = 0.1. Average values across 10 runs. The colormap is normalizedto interpret all Acc > 0 and AUC 2 as desirable": "exemplifies the trade-off between improvements in model accuracy, and a decrease in edge privacyfor all combinations of and , with the combination = = 0 representing the reference base case. Modelaccuracy tends to increase with larger values of and , while privacy tends to decrease. In fact, a higherthreshold implies that fewer nodes may be selected as candidate replacements during perturbation, with = 1 requiring that only nodes with similarity of 1 may be considered. With respect to the aggregationparameter , values larger than zero tend to provide increased accuracy for small . This suggests that,when computing similarities, it is beneficial to do so on aggregate features: in this way, nodes which behavesimilarly with respect to the output of the Aggregate function (.1), which is then used to trainthe GNN, will be favoured as candidates for replacement. Small values of and provide therefore a good trade-off between model accuracy and privacy, with somecombinations remarkably offering both improved accuracy and better privacy over the = = 0 case.For instance, the combination = 0.5, = 0.1 for Cora offers a 5.4% improvement in accuracy and a0.8% improvement in edge privacy. Considering the additional visualizations provided on Appendix C, thisbehavior generally holds across datasets, for both GP-t and GP-m. Even for GCN (see for instance b),where average results show an unfavorable AUC > 0, specific combinations of and can provide accuracyimprovements over the = = 0 case at a small or null privacy cost. Finally, we consider what the benefits of GraphPrivatizer are across the range of privacy budgets we tested.We focus here on GP-t on LastFM for GCN: shows for this case a decrease in protection againstprivacy attacks corresponding to a relatively large AUC = 10 5 when introducing a positive or . Analyzing this case more in detail, a shows that, as previously observed, on average GraphPrivatizerentails a10% increase in the attack performance for this specific experiment for positive thresholds oraggregation coefficients.Nevertheless, AccGP for = 0.1 is close to that of the non private model for = , despite having a smaller AUC. More in detail, when compared to GraphPrivatizer for = 0.1, theno aggregations and no thresholds achieve a better accuracy only for = 8, but with a worse AUC. Thatis, on average, GraphPrivatizer with threshold and feature aggregation provides a better trade-off between",
  "Conclusions": "Motivated by real-world applications, we investigated LDP GNNs. Considering a local privacy setting wherethe individual nodes of the graph can only have noise-free access to their own features, labels, and edges,we (i) introduced a new definition of LDP for our local privacy setting, (ii) proposed our private algorithmGraphPrivatizer, and (iii) empirically validated its performance on real-world datasets and against privacyattacks. GraphPrivatizer is a fully private algorithm that protects edges, features, and labels. We introduceda new methodology to protect edges by means of controlled perturbations which replace the neighbors ofa node with other similar nodes according to some similarity measure.We furthermore evaluated theimpact of thresholds on the similarity between the original and the perturbed neighbor nodes and of featureaggregation in computing the similarity scores, finding that a positive threshold and aggregation coefficientprovide the best privacy-utility trade-off.Compared to existing approaches which do not provide edge-privacy (Sajadmanesh & Gatica-Perez, 2021) or do so while requiring a central entity to have completeinformation about the edges of the graph (Sajadmanesh et al., 2023; Joshi & Mishra, 2022), GraphPrivatizerprovides LDP without requiring a trusted entity to have access to the adjacency matrix of the graph and isapplicable to a variety of GNN models. Future work could focus on addressing some of the limitations of GraphPrivatizer. First, GraphPrivatizerdoes not consider datasets containing edge features. While it could be possible to, e.g., privatize categoricaledge features using randomized response, our local privacy setting would need to be adapted to determinewhich entity can have noise-free access to the edge features. Additionally, better personalized user privacyrequirements could be considered. In particular, the introduction of a notion of trust between nodes couldallow two neighboring nodes that trust each other to exchange less noisy information, thus improving theGNN performance. Finally, while GraphPrivatizer is generally applicable to various types of GNN models,edge perturbations which are tailored to a specific GNN architecture can possibly provide a better privacy-utility trade-off and could therefore be explored in future work.",
  "Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In InternationalConference on Learning Representations, 2022. URL": "Graham Cormode, Somesh Jha, Tejas Kulkarni, Ninghui Li, Divesh Srivastava, and Tianhao Wang. Privacyat scale: Local differential privacy in practice. In Proceedings of the 2018 International Conference onManagement of Data, pp. 16551658, 2018. Cynthia Dwork. Differential privacy. In Automata, Languages and Programming: 33rd International Collo-quium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II 33, pp. 112. Springer, 2006.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InInternational Conference on Learning Representations, 2017": "Jaewoo Lee and Chris Clifton. How Much Is Enough? Choosing for Differential Privacy. In Xuejia Lai,Jianying Zhou, and Hui Li (eds.), Information Security, Lecture Notes in Computer Science, pp. 325340,Berlin, Heidelberg, 2011. Springer. ISBN 978-3-642-24861-0. doi: 10.1007/978-3-642-24861-0_22. Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan,and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In Proceedingsof the AAAI conference on artificial intelligence, volume 33, pp. 46024609, 2019. Tamara T Mueller, Johannes C Paetzold, Chinmay Prabhakar, Dmitrii Usynin, Daniel Rueckert, and Geor-gios Kaissis. Differentially private graph classification with gnns. arXiv preprint arXiv:2202.02575, 2022a.",
  "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gnnemann. Pitfalls of graphneural network evaluation. arXiv preprint arXiv:1811.05868, 2018": "Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label predic-tion: Unified message passing model for semi-supervised classification. In Zhi-Hua Zhou (ed.), Proceedingsof the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI 2021, Virtual Event /Montreal, Canada, 19-27 August 2021, pp. 15481554. ijcai.org, 2021. doi: 10.24963/IJCAI.2021/214.URL Haipei Sun, Xiaokui Xiao, Issa Khalil, Yin Yang, Zhan Qin, Hui Wang, and Ting Yu. Analyzing subgraphstatistics from extended local views with decentralized differential privacy. In Proceedings of the 2019ACM SIGSAC Conference on Computer and Communications Security, pp. 703717, 2019.",
  "Si Zhang, Hanghang Tong, Jiejun Xu, and Ross Maciejewski. Graph convolutional networks: a comprehensivereview. Computational Social Networks, 6(1):123, 2019": "Zaixi Zhang, Qi Liu, Zhenya Huang, Hao Wang, Chengqiang Lu, Chuanren Liu, and Enhong Chen. Graphmi:Extracting private graph data from graph neural networks. In Zhi-Hua Zhou (ed.), Proceedings of theThirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 37493755. InternationalJoint Conferences on Artificial Intelligence Organization, 8 2021.doi: 10.24963/ijcai.2021/516.URL Main Track.",
  "AExperimental details": "For all experiments, we divide our dataset with 50:25:25 train:validation:test set ratios, similarly to Sajad-manesh & Gatica-Perez (2021). We train all the models for 100 epochs, with learning rate 102, weightdecay 103, and dropout rate 0.5.We run experiments with edge privacy budget = {0.1, 1, 2, 8, }, = {0, 0.25, 0.5, 0.75, 1.0}, and = {0, 0.1, 0.25, 0.5, 1}. We perform 10 runs for each value of , and with different seeds and consider average results. We perform label and feature perturbations as describedin .3, with privacy budgets fixed at x = 3 and y = 3. Additionally, we set the KProp hyper-parameters in the Drop algorithm to the best values described in Sajadmanesh & Gatica-Perez (2021): weuse Kx = 16, Ky = 2 for Cora, Kx = 4, Ky = 2 for Facebook, and Kx = 16, Ky = 0 for LastFM andPubMed. After a grid-search tuning, we use Kx = 4, Ky = 2 for Amazon Photo. : Statistics of the datasets. Cora and PubMed Yang et al. (2016) are citation networks where anedge (i, j) exists between two documents if document i cites document j, and features consist of bag-of-words representations of the documents. Classes consist of document categories. Facebook Rozemberczkiet al. (2021) is a page-page graph of verified Facebook pages, where nodes correspond to official Facebookpages and edges correspond to mutual likes between pages.Node features are extracted from the sitedescriptions and the classes correspond to various page categories. LastFM Rozemberczki & Sarkar (2020)is a friendship graph of LastFM users where nodes represent users and edges friendships between users. Theclasses correspond to the home countries of the users. Amazon Photo (Shchur et al., 2018) is a co-purchasenetwork where nodes represent goods and edges correspond to two goods which have been frequently boughttogether on Amazon.The features consist of bag-of-words representations of the goods and the classescorrespond to product categories.",
  "BSimilarity metric": "The performance of GraphPrivatizer is affected by the choice of similarity measure in Algorithm 2. Non-parametric and non-learnable similarity measures are desirable in our local privacy setting, as they do notrequire access to data. We compare the cosine similarity with the Euclidean distance, and find that thecosine similarity is preferable. Refer to for details on how privacy attacks are performed.",
  "C.2Additional results for the GT, GATv2, and GConv models": ": Aggregate results for GraphPrivatizer for all models. We denote as AccGP and AUCGP the averageaccuracy and AUC results for GraphPrivatizer with , > 0, where the average is taken across all positivetested values of and . We denote with Acc the average accuracy difference with the = = 0 case acrossall tested values of and larger than zero, where positive values of Acc indicate that GraphPrivatizerperforms better when some thresholds are applied and/or aggregation is performed during the neighborperturbation. Analogously, we denote with AUC the average AUC102 difference, where values close tozero indicate that GraphPrivatizer offers the same protection against privacy attack when introducing apositive threshold and/or feature aggregation during the neighbor perturbation. We denote the datasets weuse as Cora (Cr), LastFM (FM), PubMed (PM), Facebook (Fb), and Amazon Photo (Ph). Results reportedas (average values standard deviation), for = 0.1."
}