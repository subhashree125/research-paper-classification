{
  "Abstract": "In deep reinforcement learning (RL) systems, abnormal states pose significant risks by poten-tially triggering unpredictable behaviors and unsafe actions, thus impeding the deploymentof RL systems in real-world scenarios. It is crucial for reliable decision-making systems tohave the capability to cast an alert whenever they encounter unfamiliar observations thatthey are not equipped to handle. In this paper, we propose a novel Mahalanobis distance-based (MD) anomaly detection framework, called MDX, for deep RL algorithms. MDXsimultaneously addresses random, adversarial, and out-of-distribution (OOD) state outliersin both offline and online settings. It utilizes Mahalanobis distance within class-conditionaldistributions for each action and operates within a statistical hypothesis testing frameworkunder the Gaussian assumption. We further extend it to robust and distribution-free ver-sions by incorporating Robust MD and conformal inference techniques. Through extensiveexperiments on classical control environments, Atari games, and autonomous driving scenar-ios, we demonstrate the effectiveness of our MD-based detection framework. MDX offers asimple, unified, and practical anomaly detection tool for enhancing the safety and reliabilityof RL systems in real-world applications.",
  "Introduction": "Deep reinforcement learning (RL) algorithms vary considerably in their performance and are highly sensitiveto a wide range of factors, including the environment, state observations, and hyper-parameters (Jordanet al., 2020; Patterson et al., 2020). The lack of robustness in RL algorithms and raised safety concernssurrounding learned policies hinder their deployment in real-world scenarios, particularly in safety-criticalapplications such as autonomous driving (Kiran et al., 2021; Liu et al., 2022; Hu et al., 2023). Recently,",
  "Published in Transactions on Machine Learning Research (10/2024)": "Stephanie C.Y. Chan, Samuel Fishman, Anoop Korattikara, John Canny, and Sergio Guadarrama. Measuringthe reliability of reinforcement learning algorithms. In International Conference on Learning Representa-tions, 2020. Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Chau. ShapeShifter: Robust physicaladversarial attack on faster R-CNN object detector. In Machine Learning and Knowledge Discovery inDatabases: European Conference, ECML PKDD 2018, Dublin, Ireland, September 1014, 2018, Proceed-ings, Part I 18, pp. 5268. Springer, 2019.",
  "(c)Performancedegradationwhennoises injected during policy learning": ": (a) An autonomous car navigates using location data observed from sensors such as GPS. Withoutan effective anomaly detection mechanism, inaccuracies or malfunctions in these sensors can cause the carto prematurely turn right, leading to a collision. (b) and (c): Performance degradation occurs when noisystates are observed in the Breakout environment. Gaussian noises with increasing standard deviations areinjected into the state observations during policy deployment (b) and policy learning (c).",
  "Related Work": "Anomaly Detection in Reinforcement Learning. Anomaly detection has yet to be extensively exploredin RL. The connection between anomaly detection and RL was first established in (Mller et al., 2022);however, their work is mainly conceptual and does not propose practical detection algorithms.Changepoint detection has been investigated in the tabular setting of RL, particularly in environments described asdoubly inhomogeneous under temporal non-stationarity and subject heterogeneity (Hu et al., 2022). Theyfocus on identifying best data chunks within the environment that exhibit similar dynamics for policylearning, while our detection focuses on anomaly detection in deep RL scenarios. Prior studies have alsoprobed anomaly detection in specific RL contexts, such as the offline imitation learning with a transformer-based policy network (Wang et al., 2024) and detecting adversarial attacks within cooperative multi-agentRL (Kazari et al., 2023). However, these studies are limited to specific scenarios that do not address generalanomaly detection, even in single-agent RL. Sedlmeier et al. (2020b) introduced a simple policy entropy based 1Compared with the classical tasks of policy evaluation and learning in offline RL, our offline setting also utilizes a fixeddataset but specifically focuses on developing detection methods given a fixed policy.",
  "Background": "Markov Decision Process. The interaction of an agent with its environment can be modeled as a MarkovDecision Process (MDP), a 5-tuple (S, A, R, P, ). S and A are the state and action spaces, P : S AS is the environment transition dynamics, R : S A S R is the reward function and (0, 1) is",
  "old (st, at), 1 , 1 + Aold (st, at),(2)": "where is a hyperparameter. We use PPO as the algorithm testbed to examine the efficacy of our anomalydetection framework. However, our detection methods are general and can be easily applied to other RLalgorithms (Zhang & Yu, 2020) such as DQN (Mnih et al., 2015; Hessel et al., 2018), A3C (Mnih et al.,2016), and DDPG (Lillicrap et al., 2016; Haarnoja et al., 2018; Fujimoto et al., 2018; Bai et al., 2023). Conformal Prediction. Conformal anomaly detection (Laxhammar & Falkman, 2011; Ishimtsev et al.,2017) is grounded in conformal prediction (Shafer & Vovk, 2008; Angelopoulos et al., 2021), which aims toconstruct a confidence band C1(X) for Y given a random data pair (X, Y ) P and a confidence level1 . Suppose we have a pre-trained model and a calibration dataset (X1, Y1), ..., (Xn, Yn) for conformalprediction. We can then compute a predictive interval for the new sample Xn+1 to cover the unseen responseYn+1 by leveraging the empirical quantiles of the residuals |Yi(Xi)| on the calibration dataset. This furtherleads to valid prediction intervals such that:",
  "Mahalanobis Distance-based (MDX) Detection Framework": "For a deep RL agent, acting on anomalous inputs could result in hazardous situations. Therefore, developingsuitable anomaly detectors for deep RL agents is particularly important in safety-critical scenarios. illustrates the operational flow of our MDX framework. Description of Detection Framework. Our detection framework is structured around two core com-ponents: feature extraction and detector estimation. The process begins by assessing whether a state isanomalous, which is crucially dependent on the associated policy. A state that prompts the policy to initiatea potentially unsafe action is labeled as an outlier. Specifically, we input the state into the policy networkand extract the feature vector from the penultimate layer of this network. We categorize states accordingto the actions determined by the policy, based on the intuition that states associated with the same actionshare similar features. For each action class, we estimate the mean value () and covariance matrix () ofthe feature vectors as the class centroid. A threshold is set as the class boundary that partitions the featurespace into inliers and outliers.",
  "Gaussian Assumption. The given pre-trained parameterized RL policy is a discriminative softmaxclassifier, (at = c|st) = expwc f(st) + bc/": "c expwcf(st) + bc, where wc and bc are the weight andbias of the policy classifier for action class c. The function f() represents the output of the penultimate layerof the policy network , serving as the state feature vector. Here, C = |A| is the size of the action space, andc is the mean vector of f(s) corresponding to the action class c 2. If we assume that the class-conditionaldistribution follows a multivariate Gaussian distribution sharing a single covariance (tied covariance) in agenerative classifier, i.e., (f(s) | a = c) = N (f(s) | c, ), then the posterior distribution of f(s) matchesthe form of a discriminative softmax classifier (Lee et al., 2018). This equivalence implies that f(s) fit aGaussian distribution under . We approximate state feature vectors with a class-conditional Gaussian 2Our MDX detection framework currently focuses on environments with the discrete action spaces. For continuous actionspaces, a natural solution is to discretize the actions into several bins and then follow the same detection pipeline, which deservesfurther validation.",
  "distribution with c and c for each action class, rather than using a single \"tied\" covariance across allaction classes (Kamoi & Kobayashi, 2020)": "Vanilla MD-based Detection. An MD-based detection based on Gaussian assumption can be immediatelydeveloped based on the mean vectors c and the covariance matrix c calculated from f(s) for each actionclass c. We first collect Nc state action pairs {(si, ai)}, separately for each action class c, and compute theempirical class mean and covariance of c:",
  "i:ai=c(f (si) c) (f (si) c) .(4)": "In distance-based detection, a straightforward metric is Euclidean distance (ED). However, MD generallyoutperforms ED in many tasks (Lee et al., 2018; Kamoi & Kobayashi, 2020; Ren et al., 2021), as it incorpo-rates the additional data covariance information to normalize the distance scales. Following the estimationin Eq. (4), we derive the class-conditional Gaussian distribution to characterize the data structure withinthe state representation space for each action class. For each state s observed by the agent, we compute itsDetection Mahalanobis Distance M(s) between s and the nearest class-conditional Gaussian distribution by:",
  "M(s) = minc(f(s) c) 1c(f(s) c) .(5)": "Unlike the previous work Lee et al. (2018), which defined a Mahalanobis confidence score based on a binaryclassifier in a validation dataset, we utilize M(s) as the detection metric within a statistical hypothesis testframework. Proposition 1 demonstrates that M(s) follows a Chi-squared distribution under the Gaussianassumption. Proposition 1. (Test Distribution of Detection Mahalanobis distance M(s)) Let f(s) be the p-dimensionalstate random vector for action class c. Under the Gaussian assumption P(f(s)|a = c) = N (f(s) | c, c),the Detection Mahalanobis Distance M(s) in Eq. (5) is Chi-Square distributed: M(s) 2p. Please refer to Appendix A for the proof. Based on Proposition 1, we can define a threshold = 2p(1 )by selecting a value from the specified Chi-Squared distribution to distinguish normal states from outliers.Given a new state observation s and a confidence level 1 , if M(s) > , s is detected as an outlier. Robust MD-based Detection. The estimation of c and c in Eq. (4) relies on Maximum LikelihoodEstimate (MLE), which is sensitive to the presence of outliers in the dataset (Rousseeuw & Van Zomeren,1990). As the offline data collected from the environment tends to be noisy, directly introducing MD foroutlier detection in RL easily results in a less statistically effective estimation of c and c, thus underminingthe detection accuracy for outliers. This vulnerability of the MD-based detector against noisy states promptsus to instantiate MDX with a more robust estimator (Huber, 2004). To this end, we apply the Minimum Covariance Determinant (MCD) estimator (Hubert & Debruyne, 2010)to estimate c and c by only using a subset of all collected samples. It only uses the observations wherethe determinant of the covariance matrix is as small as possible. Concretely, MCD determines the subset Jof observations with a size h, while minimizing the determinant of the sample covariance matrix calculated MDRobust MDInliersRandom Outliers",
  "i:iJ,ai=cf (si) ,J =set of h points :J K for all subsets K,(6)": "where we set h as (number_of_samples + number_of_features + 1)/2 (Rousseeuw, 1984). K representsthe total number of subsets that contain h points. In practice, the MCD estimator can be efficiently solvedby the FAST-MCD algorithm (Hubert & Debruyne, 2010) instead of performing a brute-force search overall possible subsets. Akin to Mahalanobis Distance, we define the Detection Robust Mahalanobis DistanceMrob(s) as robust detection metric:",
  "Mrob(s) = mincf(s) robc rob1cf(s) robc.(7)": "Since the robust Mahalanobis distance can still approximate the true Chi-squared distribution (Hardin &Rocke, 2005), we still employ the threshold value = 2p(1 ) for detecting outliers as in the MD case. As a motivating example, displays contours computed by both MD and Robust MD detection methodsfor state feature vectors in the Breakout game from the popular Atari benchmark (Bellemare et al., 2013;Brockman et al., 2016) with different types of outliers. These results demonstrate that estimation based onRobust MD is less vulnerable to outlying states (red points) and better fits inliers (black points) than MD.This robust parameter estimation highlights the potential advantage of Robust MD for RL outlier detection,where the data used for estimation tends to be noisy.",
  "Distribution-free Detection by Conformal Inference": "Although robust MD-based detection is less vulnerable to noise in RL environments, both MD and robustMD strategies heavily rely on the Gaussian assumption to construct the detection thresholds based onProposition 1. This distribution assumption is often violated in practice, diminishing the effectiveness ofMD and robust MD. In contrast, conformal prediction offers a mathematical framework that provides validand rigorous prediction distribution without assuming a specific underlying data distribution. The resultingconformal anomaly detection circumvents the limitation of the distribution assumption, potentially improvingthe detection efficacy. In the context of RL, conformal anomaly detection evaluates how a state conforms to a models currentprediction distribution, thereby discriminating abnormal states. As a distribution-free detection approach,conformal anomaly detection can enhance the distance-based detectors by additionally tuning the anomalythreshold in the calibration dataset. To design the conformal anomaly detection method, we leverage theDetection Mahalanoibis Distance M(s) as the non-conformity score, which measures how dissimilar a stateis from the instances in the calibration set. Following split conformal inference (Papadopoulos et al., 2002;Shafer & Vovk, 2008), we split the the previously collected offline dataset into the the calibration set Dcaland the evaluation set. A simple way is to evaluate the quantiles of the resulting empirical distribution tocreate the corresponding confidence band. Using the calibration set Dcal, we define the fitted quantiles Qc1of the conformity scores for the action class c as follows:",
  ",(8)": "where each (si, ai) is drawn from the calibration set Dcal and c is calculated by c = arg min M c(si) in M c(si)among all action classes.Finally, we use the class-dependent and well-calibrated detection thresholding = Qc1 in conformal MD-based detection instead of 2p(1 ) used in MD and Robust MD strategies.",
  "Algorithm 1 MDX Detection Framework in the Offline Setting": "1: Input: The given policy , the dimension of state feature vectors p, and a confidence level 1 .2: Output: Detection labels {ys} for each s in the evaluation trajectory.3: / * Step 1: Detection Design by Estimating Mean and Covariance * /4: Given state action pairs {(si, ai)} where ai (|si).5: for each action class c do",
  ": end for": "the environments we consider have finite horizons with restarts, catastrophic forgetting is not a concern. Weset a small window size of 5120 to balance past and recent data used for detection estimation. Based on theconstantly updated state feature vectors, c and c are continually estimated. This continuous updatingallows us to accurately track the state feature distribution, ensuring that our detector remains sensitive torecent and historical data shifts. Double Self-Supervised Detectors. Our current detector is continually refined using self-detected inliers,while any detected outliers are promptly discarded. However, a more practical approach is to leverage theseoutliers to create a complementary detector for outliers. This secondary self-supervised detector validatesthe detection results from the primary detector.For example, if the primary detector classifies a stateas an inlier and the secondary detector agrees that it is not an outlier, the state is confidently classifiedas such. Conversely, if there is a difference between the discrimination of the two detectors, the state israndomly classified as either an outlier or an inlier. In the event of disagreement, this random classificationis motivated by the need to avoid systematic bias that could arise from consistently favoring one detectorsoutput over the other. By introducing randomness, we ensure the system remains fair and does not overlyrely on potentially flawed outputs from either detector. This approach also preserves the systems abilityto learn and adapt over time, preventing the reinforcement of incorrect classifications. The double-detectorsystem thus enhances the robustness and reliability of the detection process, ensuring more accurate andconsistent identification of abnormal states. MD-based Detection Algorithm in the Online Setting. Algorithm 2 outlines our MD-based detectionprocedure for online RL, incorporating both moving window estimation and double self-supervised detectors.To update our double detectors, inliers and outliers are stored in buffers BI and BO, respectively. For eachclass, a window size m is specified. Within each class, the state-action pairs in the window are used toestimate c and c. These parameters are updated after every Nc newly collected data points in the windowfor action class c. This adaptive updating mechanism ensures that the detectors remain responsive to evolvingdata distributions. Online Anomaly Detection Procedure. Since our detection method relies on features extracted fromthe penultimate layer of the policy network, instead of training the policy from scratch, we pretrain thepolicy to ensure these features capture meaningful information about the environment. This pre-trainedpolicy results in more meaningful state features and enhances the detection procedure, contributing to arapid assessment among distinct detection algorithms through their learning curves. Moreover, deployinga randomly initialized policy in a real-world scenario is unreliable. Instead, it is common practice to use apre-trained policy as a warm start and then further improve it. For example, in recommendation systems, apre-trained policy is deployed initially to provide recommendations, and user feedback, such as click-throughrates (CTR), is used to iteratively update the online policy. Similarly, within our online detection algorithm,",
  "Anomaly Detection in the Online RL Setting": "In the online RL setting (Sutton & Barto, 2018; Dong et al., 2020), a policy is updated continuously, unlikethe fixed pre-trained policy used in our offline setting. Robust policy training with noisy states is crucialin safe RL, as the agents are more likely to encounter state outliers during training. In this section, weextend MDX to the online RL training scenario. Unlike the offline setting, the challenge here stems fromthe dynamic nature of policy updates, requiring our detector to adapt to the evolving distribution of featurevector outputs. The complexity increases when the improved policy starts gathering new samples throughexploration, posing a fundamental challenge in an online RL framework. An effective detection system mustdifferentiate between actual noisy observations and newly collected data through exploration. Training theRL agent and estimating the detector are interleaved in a noisy online environment. Various options formanaging detected outliers during training include removing or denoising the outlier states. In our detectionframework, we focus on direct removal and assess the resulting learning curves in the presence of noisy statesduring the training process. To address the challenges in detecting abnormal states in the online trainingsetting, we propose Moving Window Estimation and Double Self-supervised Detectors, both of which arepivotal for the empirical success of our anomaly detection approach. Moving Window Estimation. In the online setting, improving the policy causes a shift in the datadistribution within the replay buffer as the agent interacts with the environment (Rolnick et al., 2019; Xiaoet al., 2019). To effectively utilize information from the updated data distribution, we maintain a movingwindow to store experiences throughout the interaction steps. The moving window operates like a first-in-first-out buffer, storing the most recent samples and discarding the oldest ones. Data falling outside thewindow is cast away. The moving window can be adjusted to either prioritize a long historical context witha larger window size or consider only more recent experiences with a smaller size. In our experiments, since",
  "Experiments": "We first conduct experiments on both feature-input and image-input tasks to verify the effectiveness of ourMDX framework in both offline and online settings. For feature-input tasks, we choose two classical con-trol environments in OpenAI gym (Brockman et al., 2016), including Mountain Car (Barto et al., 1983)and Cart Pole (Moore, 1990). For image-input tasks, we choose six Atari games (Bellemare et al., 2013).We divide the six Atari games into two different groups. The first group includes Breakout, Asterix, andSpaceInvaders, which feature nearly static backgrounds. Enduro, FishingDerby, and Tutankham in the sec-ond group have time-changing or dramatically different backgrounds, presenting more challenging scenarios.We further conduct experiments on autonomous driving environments (Dosovitskiy et al., 2017) as one po-tential application. We select Proximal Policy Optimization (PPO) (Schulman et al., 2017) as our baselineRL algorithm. For feature-input classical control tasks, we use a policy network with two fully connectedlayers, each containing 128 units with ReLU activation functions. For image-input tasks, we use the samenetwork architecture as described in the PPO paper (Schulman et al., 2017). Three Types of Outliers. (1) Random Outliers. We generate random outliers by adding Gaussiannoise with zero mean and different standard deviations on state observations, simulating natural measurementerrors. (2) Adversarial Outliers. We perform white-box adversarial perturbations (Szegedy et al., 2013;Goodfellow et al., 2014b; Cao et al., 2020) on state observations for the current policy, following the strategyproposed in (Huang et al., 2017; Pattanaik et al., 2017). Particularly, we denote atw as the \"worst\" action, withthe lowest probability from the current policy t(a|s). The optimal adversarial perturbation t, constrained inan -ball, can be derived by minimizing the objective function J: min J(st+, t) = ni=1 pti log t(ai|st+), s.t. , where ptw = 1 and pti = 0 for i = w. We solve this minimization problem with the FastGradient Sign Method (FGSM) (Goodfellow et al., 2014b), a typical adversarial attack method in the deeplearning literature. The resulting adversarial outliers st + t force the policy to choose atw. (3) Out-of-Distribution (OOD) outliers. OOD outliers arise from the disparity in data distribution across differentenvironments. To simulate them, we randomly select states from other environments and introduce themto the current environment. In our experiments, we select images from other Atari games to serve as Out-of-Distribution (OOD) outliers within the considered environment. In the autonomous driving scenario, wedesignate rainy and nighttime observations as OOD outliers for the primary daytime setting on a sunnyday. This deliberate selection of diverse outlier examples enables comprehensive testing of our methodsrobustness across varied environments. Baseline Methods. A fundamental obstacle in assessing the anomaly detection strategies in RL lies inthe scarcity of suitable baselines in deep RL settings as introduced in . To rigorously substantiatethe effectiveness of MDX, we initiate our evaluation by comparing them with the foundational baselines wehave developed ourselves and implement two non-distance-based methods. (1) Euclidean distance (ED)assumes that all features are independent under the Gaussian assumption with one standard deviation, whichcan be considered as a simplified version of our MD method with an identity covariance matrix. (2) MD withTied covariance (TMD) follows the tied covariance assumption in (Lee et al., 2018), where features amongall action classes share a single covariance matrix estimation. (3) PEOC is a policy entropy-based detectionmethod proposed in Sedlmeier et al. (2020b). The authors assume that a successful training process reducesentropy for states encountered during training, which can then be used as a classification score to detectOOD states. (4) EnvModel follows the model-based detection algorithm utilizing learned dynamics modelsand bootstrapped ensembles (Haider et al., 2023). We train five autoencoders as environment transitionmodels in each environment. For the offline setting, the autoencoders are trained based on the dataset, whilefor the online setting, they are continuously updated. Each autoencoder predicts the next state given thecurrent state and action, and the minimum prediction error among the five models serves as the anomalydetection signal. (5) MD is our first proposed method with class-conditional Gaussian assumption. (6)",
  "Random78.890.150.050.093.078.594.9Adversarial57.687.150.049.889.976.590.9OOD89.190.450.071.393.578.294.6Average75.189.250.057.092.177.793.5": ": Average detection accuracy of MD, RMD, and MD+C compared with baselines across differentoutlier types in two feature-input classical control environments in the offline setting. The averages arecomputed across environments and outlier types. Accuracy is determined by applying detection techniquesto the balanced data composed equally of clean and noisy states. Robust MD (RMD) is the robust variant of MD under the Gaussian assumption. (7) MD+C uses well-calibrated conformality scores to construct a valid empirical distance distribution instead of relying on theChi-Squared distribution established upon the Gaussian assumption.",
  "Anomaly Detection in the Offline Setting": "In the offline setting, we randomly split the states from the given dataset into calibration and evaluation sets,each containing 50% of the data. The calibration set is used to construct our detectors, and the evaluationset is for testing. We first use PCA to reduce the state feature vectors into a 50-dimensional space. We thenapply (robust) MD to estimate mean vectors and covariances and calibrate the conformality score based on",
  "(e) Adversarial": ": Performance on Tutankham across various state outliers in online learning. The first row shows thepolicy performance during learning. The second row shows the relationship between the averaged detectionaccuracy and achieved final performance. . For OOD outliers, due to the potential differences in action spaces between the original environmentand the OOD environment, we select OOD states from the OOD environment by taking random actionswithin its own action space. For the Robust MD method, we use PCA to reduce state feature vectors into a50-dimensional space due to the expensive computation of the robust MD method. For the other methods,we use the original feature vectors output from the penultimate layer of . Results are averaged over threeseeds with hyperparameters given in of Appendix C.1. When our detectors identify an outlier, it isremoved from training. We compare the resulting learning curves for different detection methods. Additional Baselines. We add another two baselines as performance upper bound and lower bound. (1)For an ideal baseline, the method Auto automatically deletes true state outliers, showing the optimal trainingperformance of algorithms without the interruption from outliers. (2) At the other extreme, Random usesa totally random detector that detects a state as an inlier or outlier with a probability of 0.5. Main Results. Figs. 4 and 5 present the online performance on feature-input task MountainCar and image-input Atari game Tutankham. The first row shows the learning curves of cumulative rewards based on thePPO algorithm. To better highlight their differences, we omit the confidence bands in , while providingfull results with confidence bands in Appendix C.1 Figs. 14 to 21 for reference. The second row illustrates therelationship between detection accuracy and policy performance. For Atari games, the x-axis represents theaverage F1 score during learning, while the y-axis represents the final performance. For feature-input taskslike MountainCar and CartPole, some methods can achieve the maximum score, leading to no significantdifference in final performance. We exhibit the relationship between the average F1 score and the average",
  "Autonomous Driving Environment": "To verify the broader applicability of our method, we perform experiments on autonomous driving environ-ments based on CARLA (Dosovitskiy et al., 2017) and introduce practical scenarios in which all three typesof anomalies commonly occur. Since anomaly detection in autonomous driving is more practical in the offlinesetting and CARLA is a complex environment that exceeds our computational capacity for online training,we focus on the offline setting in this section. Random Noise. Malfunctioning sensors or cameras can introduce random noise into signal observations.For instance, a faulty camera lens may produce distorted images, while a malfunctioning LiDAR sensormight generate erroneous depth measurements. Such random noise can impair the reliability of perceptionsystems in autonomous vehicles. Adversarial Attacks. Adversarial attacks involve intentionally manipulating input signals to disrupt thefunctioning of RL systems (Bai et al., 2024a). In the context of autonomous driving, an attacker mighttamper with sensor data or traffic signs, resulting in misleading observations and potentially hazardousdriving behavior. Adversarial states thus pose a significant threat to the robustness and safety of autonomousdriving systems. Out-of-Distribution (OOD) States. Consider a scenario where an RL policy is trained exclusively undersunny weather. Encountering rainy weather poses a challenge, as the observations captured under theseconditions deviate from the training data distribution. Such observations are therefore considered Out-of-Distribution (OOD) states.",
  ":Detection accuracy on the CARLA town environment over three types of outliers": "Experimental Setup. We conduct experiments using the CARLA environment (Dosovitskiy et al., 2017).CARLA is an open-source simulator for autonomous driving research known for its high-quality renderingand realistic physics. The environment includes 3D models of static objects, such as buildings, vegetation,traffic signs, and infrastructure, as well as dynamic objects, such as vehicles and pedestrians. The task is todrive safely through the town. In each episode, the vehicle must reach a given goal without collision. Theepisode ends when the vehicle reaches the goal, collides with an obstacle, or exceeds the time limit. Noisy State Observations. Following the approach used in Atari game settings, we introduce Gaussiannoise to simulate random outliers and generate adversarial outliers using adversarial perturbations.ForOOD outliers, we leverage CycleGAN-Turbo (Zhu et al., 2017; Parmar et al., 2024), a technique designedfor adapting a single-step diffusion model (Ho et al., 2020) to new tasks and domains through adversariallearning (Goodfellow et al., 2014a). This method can perform various image-to-image translation tasks andoutperforms existing GAN-based and diffusion-based methods for various scene translation tasks, such asday-to-night conversion and adding/removing weather effects like fog, snow, and rain (Parmar et al., 2024).Specifically, we use CycleGAN-Turbo to create rainy and nighttime outliers. Examples of different anomalystates are presented in . Main Results. Given a fixed dataset and a pre-trained policy, we assess our detection methods across thethree types of outliers. shows the average accuracy, with MD+C achieving the highest performancein most scenarios, while RMD performs best in the presence of small random noises. Similar to the results inclassical control environments and Atari games, the entropy-based PEOC is still ineffective across all outliersettings. While EnvModel is competitive in identifying nighttime outliers, it is inferior to the other methodsagainst the other considered outliers. These results suggest that our proposed method effectively detectsoutliers for realistic problems, such as autonomous driving.",
  "Discussions and Conclusion": "In this paper, we present the first detailed study of a distance-based anomaly detection framework in deepRL, considering random, adversarial, and OOD state outliers in both offline and online settings.Theprimary detection backbone is based on Mahalanobis distance, and we extend it to robust and distribution-free versions by leveraging robust estimation and conformal prediction techniques. Experiments on classicalcontrol environments, Atari games, and the autonomous driving environment, demonstrate the effectivenessof our proposed methods in detecting the three types of outliers. The conformal MD method achieves thebest detection performance in most scenarios, especially in the online setting. Our research contributes todeveloping safe and trustworthy RL systems in real-world applications. Limitations and Future Work. In the online setting, especially with a high proportion of outliers, itmay be preferable to denoise the detected state outliers via some neighboring smoothing techniques, e.g.,mixup (Zhang et al., 2018; Wang et al., 2020), rather than deleting them directly as performed in thispaper. To relax the Gaussian assumption in the hypothesis test of our detection, we can consider othernon-parametric methods, such as one-class support vector machines (Choi, 2009) or isolation forests (Liuet al., 2008). A substantial challenge that remains for future work is to devise a more informed detectorto distinguish between real bad outliers that can cause truly misleading actions and good new sam-ples collected through exploration, which can potentially benefit the policy learning, especially for imageinputs (Zhang & Ranganath, 2023).",
  "Acknowledgements": "Ke Sun was supported by the State Scholarship Fund from China Scholarship Council (No:202006010082).Linglong Kong was partially supported by grants from the Canada CIFAR AI Chairs program, the AlbertaMachine Intelligence Institute (AMII), and Natural Sciences and Engineering Council of Canada (NSERC),and the Canada Research Chair program from NSERC. Hongming Zhang and Martin Mller were supportedby UAHJIC, the Natural Sciences and Engineering Research Council of Canada (NSERC), and an AmiiCanada CIFAR AI Chair. David Abel, Andr Barreto, Benjamin Van Roy, Doina Precup, Hado P van Hasselt, and Satinder Singh.A definition of continual reinforcement learning. Advances in Neural Information Processing Systems, 36,2024. Anurag Ajay, Abhishek Gupta, Dibya Ghosh, Sergey Levine, and Pulkit Agrawal. Distributionally adaptivemeta reinforcement learning. Advances in Neural Information Processing Systems, 35:2585625869, 2022.",
  "Nishanth Anand and Doina Precup. Prediction and control in continual reinforcement learning. Advancesin Neural Information Processing Systems, 36, 2024": "Anastasios Nikolas Angelopoulos, Stephen Bates, Michael Jordan, and Jitendra Malik. Uncertainty sets forimage classifiers using conformal prediction. In International Conference on Learning Representations,2021. Fengshuo Bai, Hongming Zhang, Tianyang Tao, Zhiheng Wu, Yanna Wang, and Bo Xu. Picor: Multi-taskdeep reinforcement learning with policy correction. Proceedings of the AAAI Conference on ArtificialIntelligence, 37(6):67286736, Jun. 2023. doi: 10.1609/aaai.v37i6.25825. URL",
  "Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. A reviewof safe reinforcement learning: Methods, theory and applications, 2024. URL": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor.In International conference on machinelearning, pp. 18611870. PMLR, 2018. Tom Haider, Karsten Roscher, Felippe Schmoeller da Roza, and Stephan Gnnemann. Out-of-distributiondetection for reinforcement learning agents with probabilistic dynamics models. In Proceedings of the 2023International Conference on Autonomous Agents and Multiagent Systems, pp. 851859, 2023.",
  "Mia Hubert and Michiel Debruyne.Minimum covariance determinant.Wiley interdisciplinary reviews:Computational statistics, 2(1):3643, 2010": "Vladislav Ishimtsev, Alexander Bernstein, Evgeny Burnaev, and Ivan Nazarov. Conformal k-nn anomalydetector for univariate data streams. In Conformal and Probabilistic Prediction and Applications, pp.213227. PMLR, 2017. Scott Jordan, Yash Chandak, Daniel Cohen, Mengxue Zhang, and Philip Thomas.Evaluating the per-formance of reinforcement learning algorithms. In International Conference on Machine Learning, pp.49624973. PMLR, 2020.",
  "Ricardo A Maronna and Vctor J Yohai. Robust estimation of multivariate location and scatter. WileyStatsRef: Statistics Reference Online, pp. 112, 2014": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. Nature, 518(7540):529533, 2015. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley,David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Inter-national conference on machine learning, pp. 19281937. PMLR, 2016.",
  "Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University ofCambridge, 1990": "Robert Mller, Steffen Illium, Thomy Phan, Tom Haider, and Claudia Linnhoff-Popien. Towards anomalydetection in reinforcement learning. In Proceedings of the 21st International Conference on AutonomousAgents and Multiagent Systems, pp. 17991803, 2022. Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S Fearing, Pieter Abbeel, Sergey Levine, and ChelseaFinn. Learning to adapt in dynamic, real-world environments through meta-reinforcement learning. arXivpreprint arXiv:1803.11347, 2018.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-tion algorithms, 2017. URL": "John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional con-tinuous control using generalized advantage estimation. International Conference on Learning Represen-tations, 2018. Andreas Sedlmeier, Thomas Gabor, Thomy Phan, Lenz Belzner, and Claudia Linnhoff-Popien. Uncertainty-based out-of-distribution classification in deep reinforcement learning.In Proceedings of the 12th In-ternational Conference on Agents and Artificial Intelligence. SCITEPRESS - Science and TechnologyPublications, 2020a. doi: 10.5220/0008949905220529. Andreas Sedlmeier, Robert Mller, Steffen Illium, and Claudia Linnhoff-Popien. Policy entropy for out-of-distribution classification. In Artificial Neural Networks and Machine LearningICANN 2020: 29thInternational Conference on Artificial Neural Networks, Bratislava, Slovakia, September 1518, 2020,Proceedings, Part II 29, pp. 420431. Springer, 2020b.",
  "Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. Journalof Machine Learning Research, 10(7), 2009": "Jiaye Teng, Chuan Wen, Dinghuai Zhang, Yoshua Bengio, Yang Gao, and Yang Yuan. Predictive inferencewith feature conformal prediction. In The Eleventh International Conference on Learning Representations,2023. Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal predictionunder covariate shift. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 50265033, 2012. doi:10.1109/IROS.2012.6386109.",
  "Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical riskminimization. In International Conference on Learning Representations, 2018": "Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Duane Boning, and Cho-Jui Hsieh. Robust deep rein-forcement learning against adversarial perturbations on observations. Advances in Neural InformationProcessing Systems, 2020. Lily H Zhang and Rajesh Ranganath.Robustness to spurious correlations improves semantic out-of-distribution detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp.1530515312, 2023.",
  "AProof of Proposition 1": "Proof. We show that for each action class c, the square of Mahalanobis distance d is identically independentChi-squared distributed under the Gaussian assumption. Without loss of generality, we denote and as the mean and variance matrix of the closest class-conditional Gaussian distribution. We need to showd = (f(s) )1(f(s) ) is Chi-squared distributed. Firstly, by eigenvalue decomposition, we have",
  "2k = 1k kuk ukuk uk = uk|2uk|2 = 1.(12)": "Each Xk is a standard Gaussian distribution. Then we have d, the square of Mahalanobis distance, Chi-squared distributed, i.e., d 2(p), independent of the action class c. Without loss of generality, the smallestd among all action classes, i.e., M(s), is also a Chi-squared distribution. That is to say, M(s) 2(p).",
  "B.3Effectiveness of Robust MD": "We take the cubic root of the Mahalanobis distances, yielding approximately normal distributions (Wilson &Hilferty, 1931). In this experiment, 250 clean states are drawn from the replay buffer, and 50 abnormal statesare drawn from each of the three types of outliers. We reduce the state feature dimension to 2 via t-SNEand compute Mahalanobis distances of these two kinds of states to their centrality within each action classunder the estimation based on MD or Robust MD, respectively. suggests that Robust MD separatesinliers and outliers better than MD on Breakout within a random action class, indicating its effectiveness indetecting RL evaluation. Similar results are also given in other games. We plot the distributions of inliers and three types of outliers on SpaceInvaders and Asterix games in Figs. 10and 11, respectively.It is worth noting that Robust MD is also capable of enlarging the separation ofdistributions between inliers and both random and adversarial outliers on SpaceInvaders game, while itsbenefit seems to be negligible on OOD outliers (Breakout) on SpaceInvaders games as well as in Asterixgame. We speculate that it is determined by the games difficulty. Specifically, the PPO algorithm can InliersRandom 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8 3 (Mahal. dist. ) Using non-robust estimates (MD) InliersRandom 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 Using robust estimates (Robust MD)",
  "B.4Sensitivity Analysis": "We provide the sensitivity analysis of Robust MD in terms of the PCA dimension in . The impactof the number of principal components on the detection performance for robust MD detection is shown in. The detection accuracy over all considered outliers improves as the number of principal componentsincreases, except for a slight decline for random and adversarial outliers (red and blue lines) on the Breakoutgame. The increase implies that the subspace spanned by principal components with small explained variancealso contains valuable information for detecting anomalous states from in-distribution states, which coincideswith the conclusion in (Kamoi & Kobayashi, 2020).",
  "C.1Setup and Full Main Results": "As a supplement to the results on the main pages, we provide the whole results on two feature-input tasksand six Atari games from to . The \"Mean Score\" in the first row indicates the accumulatedrewards of PPO, and the \"F1 Score\" in the second row shows the detection performance during RL training.The F1 score is computed based on precision and recall. The third row shows the relationship between theaverage F1 score and policy performance during training. We can find that higher detection accuracy isgenerally associated with better policy performance. We also find that the cumulative reward is not stronglycorrelated with detection ability in some games. A high detection accuracy may only improve the cumulativereward to a small degree. This suggests that we need more metrics to measure the effect of our detectionperformance more effectively. Hyperparameters in our methods are shown in .",
  "C.2Ablation Study on Double Anomaly Detectors": "reveals that double self-supervised detectors can help adjust the detection errors and improve thedetection accuracy compared with the single detector. MD with double detectors outperforms MD witha single detector significantly, although RMD with double detectors is comparable to RMD with a singledetector. iteration 0.0 0.2 0.4 0.6 0.8 1.0 1.2 accuracy Robust MD DoubleMD DoubleRobust MD SingleMD Single",
  "C.3Ablation Study on Number of Noisy Environments": "We train PPO in two, four, or six noisy environments with random and OOD outliers among all eight parallelenvironments. We use PCA to reduce the feature vectors to 50 dimensions and estimate the detector usingRobust MD. illustrates that compared with the Auto baseline, our RMD method is robust whenencountering different ratios of outliers, especially with a higher contamination ratio. The dashed lines indifferent colors represent Auto baselines that correspond to the different number of noisy environments.The training performance with our detection method gradually approaches the ideal baselines, i.e., Auto. iteration mean score"
}