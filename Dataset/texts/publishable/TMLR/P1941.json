{
  "Abstract": "We introduce a method for flexible and efficient continual learning in open-vocabulary imageclassification, drawing inspiration from the complementary learning systems observed inhuman cognition. Specifically, we propose to combine predictions from a CLIP zero-shotmodel and the exemplar-based model, using the zero-shot estimated probability that asamples class is within the exemplar classes. We also propose a tree probe method, anadaption of lazy learning principles, which enables fast learning from new examples withcompetitive accuracy to batch-trained linear models. We test in data incremental, classincremental, and task incremental settings, as well as ability to perform flexible inferenceon varying subsets of zero-shot and learned categories. Our proposed method achieves agood balance of learning speed, target task effectiveness, and zero-shot effectiveness. Code isavailable at",
  "Efficient incremental learning: can efficiently update the model with new training examples": "Flexible inference and continual improvement lead to robust and widely usable systems. Efficient incrementallearning reduces cost of training and facilitates responsive and interactive learning approaches. Imagineinteracting with a nature classification app: it begins with open-vocabulary classifiers, providing reasonablepredictions for plant and animal categories. Whenever an expert user correct mistakes, the app learns fromtheir input to improve on the labeled species without detriment to its existing capabilities, as shown in .",
  "Published in Transactions on Machine Learning Research (10/2024)": "Flowers102 The 102 Category Flower Dataset (Nilsback & Zisserman, 2008) is a compilation of flowerimages. It includes 8,189 images across 102 flower categories, with each category containing between 40 and258 images. The datasets images vary in size and aspect ratio, captured using different cameras, lightingconditions, and backgrounds. OxfordIIITPets The OxfordIIITPets dataset (Parkhi et al., 2012) is a collection of pet images, featuring7,349 images from 37 different cat and dog breeds. Each breed has between 100 and 200 images. The datasetis challenging because the appearance of the same breed can vary significantly, and different breeds may havesimilar-looking features. EuroSAT The EuroSAT dataset (Helber et al., 2019) is a remote sensing image dataset comprising Sentinel-2satellite data. It contains 27,000 images that cover 13 spectral bands and consist of 10 different land use andland cover categories, including forests, urban areas, and water bodies. This dataset is commonly employedfor remote sensing and land cover classification tasks. Since there is no official dataset split for this dataset,we randomly select 70% of images as training data and the rest as validation data. We use NumPy randompermutation to perform splitting with the seed set to 0. UCF101 The UCF101 dataset (Soomro et al., 2012) is a commonly used benchmark for action recognition.It consists of 13,320 videos from 101 action categories, with each category containing at least 100 videos. Theactions include a wide range of human activities such as basketball shooting, horse riding, and juggling. Thedataset is unique in its focus on complex, naturalistic action sequences, with videos varying in length from afew seconds to a minute. Since there is no official dataset split for this dataset, we randomly select 70% ofimages as training data and the rest as validation data. We use NumPy random permutation to performsplitting with the seed set to 0.",
  "Open-vocabulary image classification": "Open-vocabulary image classification aims to categorize images without constraints of predefined labels.CLIP (Radford et al., 2021) achieves this by learning embedding spaces of images and text, trained via acontrastive objective that corresponding image/text pairs will be more similar than non-corresponding pairs.A new image can be classified into an arbitrary set of labels based on the similarity of the image embeddingfeatures to the text embedding features corresponding to each label. An alternative approach is to jointlyencode the image and text information and decode into text. Following this approach, vision-language models,such as GPV-1 (Gupta et al., 2022), GPV-2 (Kamath et al., 2022), VL-BERT (Su et al., 2020), VL-T5 (Choet al., 2021), Unified-IO (Lu et al., 2022), Gato (Reed et al., 2022), and Flamingo (Alayrac et al., 2022),can solve a broad range of tasks that includes open-vocabulary image classification, but typically are largerand more complex than CLIP. We use CLIP as our consolidated model, due to its simplicity and verifiedeffectiveness in a broad range of tasks (Wortsman et al., 2022b; Gu et al., 2021; Ghiasi et al., 2021; Lin et al.,2022). According to the data overlap analysis in (Radford et al., 2021), though trained on Internet-scaledata, CLIP under-performs models trained for particular benchmarks. Our method continually and efficientlyimproves CLIPs capability for new concepts, enabling users to add customized expertise while maintaininggeneral capability.",
  "Continual learning": "Wang et al. (2023) provides a comprehensive survey of continual learning techniques, which aim to acquire newknowledge over time while preserving previously learned knowledge (McCloskey & Cohen, 1989). Approachesto continual learning can be broadly categorized into regularization (Li & Hoiem, 2016; Kirkpatrick et al.,2016; Zenke et al., 2017), parameter isolation (Aljundi et al., 2017; Rusu et al., 2016; Serr et al., 2018;",
  "Classier": "CLIP Text Encoder[apple, bear,bee, chair, ...] : Method overview. (a) Our model integrates exemplar-based and consolidated systems. Finalresults are made by fusing the predictions from both systems using a weighting method such as AIM. (b)Illustration of TreeProbe, which incrementally adds and hierarchically clusters examples. TreeProbe trainslogistic regression classifiers using examples in updated leaf nodes. Colors of exemplars indicate differentcategories. Given an image I and label set Y , our systems inference task is to assign the correct label y Y . Eachtraining example (or exemplar) consists of an image, and a ground-truth label. Our problem setting iscalled open-vocabulary or zero-shot, as the labels can be represented and related through text, and thetask at inference time may contain labels or candidate label sets not observed during training. The traininggoal is to efficiently update the model with each new example, such that accuracy improves for related labelswhile maintaining zero-shot performance. Analogous to complimentary learning systems (CLS) theory (OReilly et al., 2014), our approach () usestwo models: a CLIP image/text encoder as the consolidated system (Sec. 3.1), and an exemplar-based systemthat stores and encodes image and label embeddings (Sec. 3.2). The CLIP model can assign confidence toany label from an arbitrary set. The exemplar-based model can acquire expertise that complements the CLIPconsolidated model, but its scope is limited to exemplar labels. In Sec. 3.3, we propose simple mechanismsto combine the predictions of each model to retain zero-shot capability while improving in tasks related toreceived examples.",
  "Consolidated system": "Deep networks, such as CLIP (Radford et al., 2021), relate to consolidated systems in humans in that theyrepeatedly process examples to iteratively consolidate experience and improve predictive ability in denseweight connections. We use CLIP as the consolidated system, taking advantage of its representations andopen-vocabulary predictive ability learned from batch training on massive datasets. We wish to enablefast learning and retain zero-shot ability; therefore, we do not retrain or fine-tune the CLIP encoders.Adapters and prompt-based methods cannot meet the goal of efficient training because they require relativelylarge computation to run through a part or the whole CLIP encoders. Instead, we train complementaryexemplar-based models to be used in combination with CLIP. CLIP encodes an input image I using an image encoder fimg to an image embedding vI = fimg(I), and encodesan input collection of text labels T = t1, t2, . . . , tm using a text encoder ftxt to text embeddings vti = ftxt(ti).Following (Radford et al., 2021), a text label ti is framed as a sentence or caption contextualizing the imagelabel yi, such as a photo of a F-16A/B, a type of aircraft to represent the image label F-16A/B in anairplane classification task. The model computes logits for each label as cosine similarity between the imageand text label, weighted by temperature (=100 in CLIP):",
  "Exemplar-based system": "For the exemplar-based system, given one or more exemplars, our goal is to maximize classification performancewith minimal training time and acceptable inference time. We consider two approaches: instance-basedand model-based prediction. Instance-based prediction leverages the exemplar set directly by retrieving andcomparing samples. Model-based prediction seeks to capture the underlying structure of the data through aparameterized model. In both cases, our approach leverages the embeddings from CLIP encoders to reducestorage and improve sample efficiency of learning. Our exemplar memory module M stores encoded image embeddings and their labels. Each entry in thememory can be denoted by Mj = {vIj, yj} where j represents the entry index, vIj represents the imageembedding of Ij, and yj is the corresponding label. Given a set of target labels, the exemplar-based model can produce a probability for each label pe(y = yi|vI).Alternatively, the prediction can be represented as an embedding vector ve of composition of one or severaltext embeddings. Prediction in terms of ve enables the exemplar model to support zero-shot prediction forlabels similar to exemplar labels. KNN: Given vI, the KNN memory module finds its most similar k entries in the memory through cosinesimilarities between vI and all vIj in the memory. Let Nk(vI) be the set of indices of the k highest cosinesimilarity scores to vI. KNN classification for vI can be performed by majority voting from the values:y = arg maxy",
  "exemplar labels. The predicted embedding is the text embedding of the label y with maximum probability:ve = ftxt(ty)": "Compared to KNN, the LinProbe model is much slower to train O(n) for n training samples assuming aconstant number of epochs, which may be prohibitive when the model needs to be updated quickly based onfew examples. However, classification accuracy tends to be higher. Tree probe: In a continual learning setting, we would ideally have fast training time of KNN with therelatively good accuracy of LinProbe.We take inspiration from the instance-based and lazy learningliterature (Aggarwal, 2014), particularly locally linear models (Domeniconi & Gunopulos, 2002; Atkesonet al., 1997). These methods classify a test sample by finding its k-nearest neighbors and applying a linearclassifier trained on the neighbors. This achieves O(1) training time but may be impractical for inference,since a new classifier may need to be trained for each test sample. Instead, we propose an approximation, building a clustering tree from the training data and training a linearclassifier in each leaf node, as shown in . Starting from a root node, we search for the nearest leafnode for a new data point and insert it if the node has not reached the predefined node capacity . If a leafnode reaches , it splits into two child nodes and becomes a non-leaf node. The attached data points aredistributed to their children by KMeans clustering. In experiments, when receiving new data, samples areadded into the cluster tree one by one. Only classifiers in affected leaf nodes need to be retrained. Whenfixing the number of linear model training epochs and KMeans iterations, the complexity to incorporate anew exemplar in training is O( + log n) with >> log n; the training time stays limited even when thetotal number of exemplars is very large. The simplest inference method would be to assign a test sample to a leaf node in the cluster tree and classifyit within the corresponding linear model, but this may lead to non-smooth predictions for samples near thecluster boundaries. In our experiments, we ensemble the classifiers from leaf nodes corresponding to the knearest neighbors, such that the final output probability pe(y = yi|vI) is the average of the probabilitiespredicted by each neighbors classifier. Similar to KNN, the exemplar embedding ve can be computed as the text embedding of the most likely labelfrom all neighbor classifiers. However, we obtain the best performance using a similarity-weighted averageembedding from the most likely label yj of each classifier in the retrieval set: ve = j j ftxt(tyj), where jis similarly defined as in the KNN classification. The tree-probe method, denoted TreeProbe, achieves similaraccuracy to LinProbe in our continual learning experiments with sufficiently large , but with much fastertraining time.",
  "be more accurate; otherwise, the zero-shot model will almost certainly outperform. The test label is unknown,but an educated guess could enable a better prediction": "Addressing this issue, we devise an adaptive weighting mechanism, named Adaptive Instance Marginalization(AIM), that estimates the likelihood of a test samples label being in the exemplar set and balances thepredictions from both models accordingly.The target label set is divided into exemplar y Ye andnon-exemplar y / Ye subsets, where Ye is the union of all exemplar labels.",
  "Hamburger": "Toyota Camry Sedan 2012 Volvo XC90 SUV 2007Toyota Corolla Sedan 2012 : The upper row illustrates several randomly selected samples in target tasks and zero-shot tasks.In the middle and the lower row, we illustrate how data samples are organized in task, class and dataincremental learning scenarios. Borders of images with different colors indicates the source of the data. Redclass names in data and class incremental learning are used to highlight differences of the two settings. We evaluate our system on target and zero-shot classification tasks. A task is an assignment of an image intoa label from particular label set. The exemplar model has received examples in the context of target tasksbut not zero-shot tasks. We utilize general tasks such as ImageNet (Russakovsky et al., 2015), SUN397 (Xiaoet al., 2010), CIFAR100 (Krizhevsky & Hinton, 2009), and fine-grained tasks like EuroSAT (Helber et al.,",
  "Evaluation scenarios": "We consider several continual learning scenarios for receiving data: 1) Data incremental: A fraction ofthe training data, randomly sampled without enforcing class balance, is added in each stage; 2) Classincremental: All training data for a randomly sampled subset of classes are added in each stage; 3) Taskincremental: All data for a single task, i.e. a dataset of examples assigned to a set of target labels, areadded in each stage. Data incremental learning includes seven stages, comprising 2%, 4%, 8%, 16%, 32%,64%, and 100% of task data, respectively. Class incremental learning divides a task into five stages, eachcontaining 20% of classes. In task incremental learning, each task is considered a stage. In data and classincremental experiments, models are built separately for each target task. A target task is fully evaluated ifthere is at least one training sample for that task, even if there are no training samples for some classes. Intask incremental, one model is built spanning all accumulated labels in each stage. In all cases, results arereported as the average accuracy of target and zero-shot tasks at each stage. We additionally compute theaveraged accuracy on seen classes and unseen classes of each target task for class incremental learning to givemore detailed result analysis. An intuitive demonstration of these continual learning scenarios are shown in.",
  "Test imagesCandidate labels Eval metric": "All DTDlabels Acc. forDTD DTDUCF101 All UCF101labels Acc. forUCF101 ImageNet All ImageNetlabels Acc. forImageNet Avg. acc.Evaluate on each zero- shot task and averagetheir accuracies Flexible Inference: Zero- shotFlexible Inference: Union + Zero- shot Union target labels:FGVCAircraft labels +OxfordIIIPets labels +Food101 labels + ... Union zero- shot labels:DTD labels +UCF101labels +ImageNet labels Acc. forAircraft FGVCAircraft OxfordIIIPet Acc. forPets Food101 ... Acc. forFood Avg. acc. Union task Evaluate on each target task and average theiraccuracies",
  "Val. data (100 zero- shot labels)Val. labelVal. acc": "Acc. forthe split Sampled zero- shot task Union target labels +100 zero- shot labels Avg. acc. Flexible Inference: Mix + Zero- shot Acc. forthe split Acc. forthe split ... Acc. forthe split Avg. acc.Evaluate on each of ve splits and average theiraccuracies 1/5 union targetlabels +100 zero- shotlabels Union target labels 1/5 split1/5 split1/5 split1/5 split... Union target labels +100 zero- shot labels Union target labels +100 zero- shot labels 1/5 union targetlabels +100 zero- shotlabels 1/5 union targetlabels +100 zero- shotlabels Union target labels +100 zero- shot labels",
  "Flexible inference: In the task incremental setting, after all training data for target tasks is received, wewill evaluate each methods performance in three inference scenarios:": "Zero-shot: evaluate on each zero-shot task and then average the accuracy on all tasks. Under thisscenario, higher performance of a model than CLIP would indicate positive transfer from learnedtasks to zero-shot tasks, while lower performance would suggest forgetting has occurred. Union + Zero-shot: we first create a union of target task labels that is considered as candidatelabels for each target task. Then, we randomly sample 100 labels from the union of zero-shot tasklabels. We separately compute the average accuracies on the target and zero-shot tasks. The finalperformance is the average of the target and zero-shot accuracy. Mix + Zero-shot: we randomly split the union of target labels into five splits. For each split, weadditionally add a random sample of 100 labels from the zero-shot task. To balance evaluation acrossclasses, we draw 100 test samples per class. Performance is the average across all splits.",
  "(d) Flexible inference": ": (a) Results comparing CLIP zero-shot, LinProbe and CLIP+LinProbe with Avg-Emb and AIM-Embon target tasks under the data incremental learning scenario. (b) Results of corresponding models on targettasks under class incremental learning. We visualize curves for seen and unseen classes including the overallperformance identified with different markers. (c) Results of corresponding models on target tasks under taskincremental learning, along with the performance of fine-tuning the whole CLIP network (CLIP Fine-tune)and ZSCL (Zheng et al., 2023). (d) Flexible inference results after task incremental learning on all tasks.Note LinProbe is hard to see in (a) and (b) because it has similar results as CLIP+LinProbe (Avg-Emb). We first investigate the importance of complementary learning systems and the effectiveness of AIM. We designexperiments comparing CLIP Zero-shot model, exemplar model using LinProbe and then complementarymodels that incorporate CLIP Zero-shot and LinProbe using the Avg-Emb and AIM-Emb fusing operations,denoted by CLIP+LinProb (Avg-Emb) and CLIP+LinProbe (AIM-Emb). The results on target tasks under data, class, and task incremental learning scenario are shown in (a-c).For data incremental, except for CLIP zero-shot, all other methods constantly improve on target tasks, withCLIP+LinProbe (AIM-Emb) being the best on all stages. When the percentage of available data is largerthan 20%, all other models surpass the zero-shot model. LinProbe and CLIP+LinProbe (Avg-Emb) havesimilar performances on target tasks, which can also be observed in the class incremental learning results in (b). Results under class incremental learning are crucial as this setup simulates scenarios where exemplar labels donot encompass all candidate labels. In such cases, we cannot simply decide between using the exemplar modelor zero-shot model for prediction by checking if exemplar labels fully overlap with candidate labels. In (b), we plot the accuracies on seen and unseen classes along with the overall accuracy on all classes. Thezero-shot model underperforms other methods on seen classes but excels in unseen classes. LinProbe achieveshigh accuracy on seen classes but performs poorly on unseen ones, with accuracy close to 0. CLIP+LinProbe(Avg-Emb) shows similar results. The AIM-Emb version has slightly lower accuracy than the Avg-Embversion on seen classes in early stages (<60% classes) but maintains reasonable performance on unseen classes,particularly in earlier stages. Its unseen accuracy decreases as more classes are learned because AIM graduallyfavors the exemplar model. Overall, CLIP+LinProbe (AIM-Emb) demonstrates the highest overall accuracyamong all methods, highlighting the effectiveness of AIM. In (c) & (d) under the task-incremental learning and flexible inference scenario, in addition to themodels mentioned above, we also compare with ZSCL (Zheng et al., 2023). For the implementation, we tryto maintain identical training techniques and hyperparameters as in the original codes of ZSCL, and we willrelease codes for sanity check with written details in the supplemental material. Furthermore, we train CLIPby fine-tuning all its parameters except for the logit scale on target datasets, resulting in a stronger exemplar",
  "modelCLIP Fine-tune. As a side-to-side comparison with ZSCL, we only use the data of the current stageto train this model but without additional reference datasets used in ZSCL": "From the results in (c), the AIM approach steadily shows better performance than the zero-shot modelat all stages. CLIP Fine-tune, LinProbe and CLIP+LinProbe (Avg-Emb) can only beat the zero-shot modelafter the 6-th stage (FGVCAircraft). However, CLIP Fine-tune is better than LinProbe and CLIP+LinProbe(Avg-Emb) in early stages, implying that the loss of generalization to other tasks is less severe. ZSCL achievesrelatively close performance as CLIP+LinProbe (AIM-Emb) in early stages but lags behind quickly in latter( 6% off in the final stage). Overall, CLIP+LinProbe (AIM-Emb) has the best performance across allstages and show better performance than exemplar-only models, demonstrating the benefits of complementarysystems. As shown in (d), exemplar-only models, being biased towards learned classes, are generally incompetenton zero-shot tasks. LinProbe is extremely bad on zero-shot tasks, leading to poor performance on bothZero-shot and Union+Zero-shot scenarios where zero-shot tasks are evaluated. However, CLIP+LinProbe(Avg-Emb) shows considerable improvement over LinProbe in Zero-shot and Mix+Zero-shot scenarios whileremaining comparable in Union+Zero-shot. CLIP Fine-tune received less severe impact of biased learningto target classes but still has a decent decrease under Zero shot. When replacing the fusing operation withAIM-Emb, performances across all scenarios significantly improve. Note that ZSCL also has good Zero-shotperformance, as mentioned in its paper, and obtains the best Union+Zero-shot performance in our evaluation.Our method is inferior to ZSCL under the Union+Zero-sho scenario mainly because AIM likely assignsmore weights to the exemplar model due to the large portion of in-exemplar labels (around 91%) of allcandidate labels. Overall, our system shows a decent flexible inference ability with the effectiveness of thecomplementary systems well validated.",
  "(a) Task-incremental learning results on target tasks(b) Time to add one example of TreeProbe and LinProbe(c) Accuracy vs. training time for exemplar models": ": (a) Results of different complementary models under the task-incremental learning scenario.Numbers included in a bracket are the node capacity of corresponding TreeProbe models. (b) We performan efficiency analysis of TreeProbe with different node capacities and LinProbe by estimating how much timeneeded to incorporate one example into the exemplar model with different number of instances carried in theexemplar set. To minimize randomness, each method is run five times. At each data collection point, wesample an example from the exemplar set and fit it to memory five times. Thus, each data point averages 25unique simulations. x and y-axes are visualized in log scale. (c) The accuracies of different complementarymodels on target tasks after learning the final stage (y-axis) vs. the total training time in log scale (x-axis). In this part, we perform a thorough analysis of different complementary models using AIM on their perfor-mances and efficiency: CLIP+KNN, CLIP+LinProbe and CLIP+TreeProbe. We can also vary the nodecapacities of TreeProbe to create different versions of CLIP+TreeProbe models. We compare their performance under the task incremental learning scenario. Results in (a) show thatnode capacity can be used to control the performance of CLIP+TreeProbe. When node capacity exceeds5k, TreeProbe steadily outperforms KNN, and when it reaches 100k, TreeProbe slightly surpasses LinProbe.",
  "ZSCL5409.322.674.3-5.9": ": Efficiency comparison of different methods. Training Time is measured in seconds, and Avg. Acc. isthe averaged accuracy over all target tasks evaluated at the end of task-incremental learning. shows thedifference from the baseline method TreeProbe (50k) ( means times). For equal comparison using thesame GPU, the linear layers in TreeProbe and LinProbe are implemented using PyTorchs linear layer (i.e,torch.nn.Linear) to support GPU accelerated training.",
  "TreeProbe (50k)69.3-0.175.9+10.685.5+20.2": ": Comparison of different methods on MTIL in Order I from ZSCL (Zheng et al., 2023). All resultsare taken from other ZSCLs paper except for TreeProbe. Transfer evaluates the models performance onzero-shot tasks; Last is the averaged accuracy on all target tasks after finishing the final task while Avg.computes the average task performance on all training stages. As introduced in Sec. 2.2, ZSCL (Zheng et al., 2023) explores a similar setting to open-vocabulary continuallearning and is mainly evaluated under the task-incremental learning scenario. To ensure a fair comparison,we evaluate our approach using their evaluation protocols, employing the same prompt ensemble technique,data split ratio and seed, backbone network (ViT-B/16), and other relevant parameters. The results are",
  ": Accuracies of CLIP Zero-shot and TreeProbe on target and zero-shot tasks with pretrained imageencoders of different capacities": "We use the CLIP ViT/B-32 model as our default zero-shot model but can easily adapt to other models. Weexperiment with the pre-trained CLIP model ViT-L/14@336px, which boasts approximately 4x the capacityof ViT-B/32, and ViT-H/14, which is double the size of ViT-L/14@336px. We present our findings in Tab. 3,where both methods are evaluated under the task incremental learning scenario. Performance improvesfor larger models, and TreeProbe consistently outperforms CLIP Zero-shot on the target tasks with nearlyidentical performance on zero-shot tasks. This demonstrates that our approach is beneficial across a widerange of zero-shot model sizes.",
  "Conclusion and limitations": "In this work, we present an efficient and performant tree probe exemplar-based model and AdaptiveInstance Marginalization to combine zero-shot and exemplar models for open-vocabulary continual learning.Our method is able to efficiently incorporate new samples, improving performance for related tasks withoutnegatively impacting zero-shot task performance, providing a simple way to continually improve large-scalepretrained image-text classification models. We believe this work is a step towards more flexible, efficient,and effective strategies in open-vocabulary continual learning. Our work has several limitations. First, we do not consider constraints in how many exemplars can bestored. Since each exemplar requires storing up to 4KB for the image and text encodings (using the baseCLIP ViT-B/32 model without any compression or limiting precision), roughly one million exemplars can bestored per 4GB of memory. Second, we do not investigate how to improve the consolidated zero-shot model.This is a promising direction for future work. Finally, we do not explore structured prediction problems likesemantic segmentation or visual question answering, which likely require more complex image representationsthan a single embedding vector. This work is supported in part by ONR award N00014-21-1-2705, ONR award N00014-23-1-2383, andU.S. DARPA ECOLE Program No. #HR00112390060. The views and conclusions contained herein are thoseof the authors and should not be interpreted as necessarily representing the official policies, either expressedor implied, of DARPA, ONR, or the U.S. Government. The U.S. Government is authorized to reproduce anddistribute reprints for governmental purposes notwithstanding any copyright annotation therein.",
  "Charu C. Aggarwal. Instance-based learning : A survey. In Data Classification: Algorithms and Applications,chapter 6. CRC Press, 2014": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, ArthurMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, AidaNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman,and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198,2022.",
  "C. Domeniconi and D. Gunopulos. Adaptive nearest neighbor classification using support vector machines.In NeurIPS, 2002": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR. OpenReview.net,2021. Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing objects by their attributes. In ComputerVision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 17781785. IEEE, 2009.",
  "Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, and Derek Hoiem. Towards general purpose visionsystems: An end-to-end task-agnostic vision-language architecture. In CVPR, 2022": "Jie Hao, Kaiyi Ji, and Mingrui Liu. Bilevel coreset selection in continual learning: A new formulation andalgorithm. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine(eds.), NeurIPS, 2023. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deeplearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in AppliedEarth Observations and Remote Sensing, 2019.",
  "Hikmat Khan, Nidhal Carla Bouaynaya, and Ghulam Rasool. Remembering transformer for continual learning.arXiv, 2024": "Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, andFahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. InICCV, pp. 1514415154, 2023. James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A.Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, ClaudiaClopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks.CoRR, abs/1612.00796, 2016. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.3d object representations for fine-grainedcategorization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13),2013.",
  "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H. Lampert. icarl: Incrementalclassifier and representation learning. In Proc. CVPR, 2017": "Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, AliRazavi, Ashley D. Edwards, Nicolas Manfred Otto Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, MahyarBordbar, and Nando de Freitas. A generalist agent. ArXiv, abs/2205.06175, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scalevisual recognition challenge. IJCV, 115(3):211252, 2015.",
  "Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, KorayKavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv:1606.04671, 2016": "Jameel Abdul Samadh, Hanan Gani, Noor Hussein, Muhammad Uzair Khattak, Muzammal Naseer, Fa-had Shahbaz Khan, and Salman H. Khan. Align your prompts: Test-time prompting with distributionalignment for zero-shot generalization. In NeurIPS, 2023. Fahad Sarfraz, Elahe Arani, and Bahram Zonooz. Sparse coding in a dual memory system for lifelong learning.In Brian Williams, Yiling Chen, and Jennifer Neville (eds.), AAAI, pp. 97149722. AAAI Press, 2023.",
  "S-2Implementation details": "We conduct our experiments on a setup featuring an RTX 3090 GPU and an AMD Ryzen 9 5950X CPU, usingPyTorch as our primary framework. We adhere to the CLIP code example, using sklearn LogisticRegressionto implement linear classifiers and setting the sklearn regularization strength to 0.316. The maximum iterationis set to 5k. Our tree probes node capacity is set at 50k. For efficient retrieval from large-scale exemplar sets,we use FAISS (Johnson et al., 2019), specifically using the IndexFlatIP class for its precision and performance.Model performances are gauged via Top-1 accuracy, with the officially released ViT-B/32 CLIP checkpointserving as our memory or zero-shot model. We select k = 9 based on a hyperparameter sweep. Our approachis not sensitive to k, with very similar performance in a range from 6 to 30. Additional implementation details of CLIP Fine-tune and ZSCL. Since we do not assume to haveadditional knowledge about data distributions, we choose the learning rate for both methods as the mostfrequent used learning rate in ZSCL experiments, i.e., 1e-5. For each task, following ZSCL, we warmuptraining for 100 iterations and proceed to train 1K iterations. For the weight ensemble technique used inZSCL, we also use an update interval of 100 iterations. Batch size is kept to 64 for both methods, withAdamW (Loshchilov & Hutter, 2019) optimizer and the beta set to 0.9. The base network backbone we usefor both methods is CLIP ViT-B/32, identical to our approach. Additional implementation details of GPU-version TreeProbe/LinProbe. After sweeping a good setof hyperparameters that lead to smaller number of training epochs while maintaining good performance onheld-out classification datasets (Caltech101), we set learning rate to 0.005, optimizer to SGD, weight decayto 0.1, and use 20 epochs for training each stage. We use the cross-entropy loss for training the probes.",
  "Learning": "Proportion of classes Dataset 1Dataset 2Dataset 3Number of datasets ... Figure S1: Illustration of continual learning scenarios. Data incremental learning includes seven stages, eachcomprising 2%, 4%, 8%, 16%, 32%, 64%, and 100% of task data respectively. Class incremental learningdivides a task into five stages, each containing 20% of classes. In task incremental learning, each task isconsidered a stage. We present an illustration of data, class, and task incremental learning scenarios in Fig. S1. When evaluatingdifferent methods in data and class incremental learning scenarios, we ensure fairness by randomly selecting anidentical portion of data/class for all methods, achieved by setting the same seed. Models are separately builtfor each target task. The performance of each stage is averaged across all target tasks. In task-incremental",
  "S-4.1General tasks": "ImageNet ImageNet (Russakovsky et al., 2015) contains 1,281,167 training images, 50,000 validation imagesand 100,000 test images. The categories represent a wide variety of objects, animals, scenes, and even abstractconcepts. This dataset has served as a fundamental dataset to evaluate performances of classification models,or as a pretraining dataset. CIFAR100 The CIFAR100 dataset (Krizhevsky & Hinton, 2009) consists of object images and is a subset ofthe 80 million tiny images dataset. It contains 60,000 3232 color images from 100 object categories, with600 images per category. The dataset has 100 fine-grained classes, grouped into 20 coarse-grained classes. SUN397 The SUN397 dataset (Xiao et al., 2010) consists of scene images, containing 108,754 images across397 scene categories, with each category having between 100 and 500 images. This dataset is commonly usedfor scene understanding tasks. Since there is no official dataset split for this dataset, we randomly select60% of images as training data, 20% as validation data, and the rest as test data. We use NumPy randompermutation to split with the seed set to 0.",
  "S-4.2Fine-grained tasks": "FGVCAircraft The FGVCAircraft dataset (Maji et al., 2013) serves as a benchmark for fine-grained visualcategorization of aircraft. It contains 10,200 images from 102 distinct categories. Each category includesapproximately 100 images, annotated with the aircraft model, variant, and manufacturer. DTD The Describable Textures Dataset (DTD) (Cimpoi et al., 2014) consists of 5,640 images across 47texture categories, with each category featuring 120 real-world texture images such as fabrics, rocks, andsurfaces. The dataset poses a challenge for texture classification due to subtle differences between textureswithin the same category and large variations in texture appearance caused by scale, orientation, and lighting. Food101 The Food-101 dataset (Bossard et al., 2014) comprises 101,000 images across 101 food categories,each with 1,000 images. This dataset challenges fine-grained image classification due to high intra-classvariation and visual similarities across categories. It serves as a rigorous benchmark for evaluating computervision models in food recognition and provides a robust platform for training machine learning models inunderstanding culinary aesthetics and preferences. StanfordCars The StanfordCars dataset (Krause et al., 2013) is a benchmark dataset containing 16,185images from 196 different car classes, divided into a 50-50 training and testing split. The classes correspondto specific car makes, models, and years, such as the 2012 Tesla Model S or 2012 BMW M3 coupe.",
  "S-4.3Long-tailed task": "Places365LT Places365LT (Liu et al., 2019) a synthetic long-tail derivative of Places2 dataset (Zhou et al.,2018). The image resolution is 256256. It contains 365 scene classes with at least 5 samples each. Theclasses are not uniformly distributed, forming a long-tailed distribution. It contains some label noises, makingclassification even harder on this dataset.",
  "Table S1: Prompts of tasks": "CLIP (Radford et al., 2021) suggests utilizing a sentence template (e.g., A photo of a {label}.), asinput to the text decoder instead of a plain text label, due to its training data being primarily full sentencesdescribing images. Consistent with this papers focus, we employ a simple prompt template for each task.Most of these templates are based on CLIPs recommendations1 and are summarized in Tab. S1.",
  "Places365LT (Liu et al., 2019)40.0/": "Table S2: Zero-shot performances of CLIP ViT-B/32 pretrained model on different tasks. ZS is forzero-shot and Acc is for accuracy. Results of column Official ZS Acc are taken from the CLIP originalpaper (Radford et al., 2021). / represents lack of official results. Tab. S2 shows the zero-shot performance of our implementation in different tasks. We conjecture that themain difference of official zero-shot performances comes from the ensemble prompt trick as mentioned inCLIP (Radford et al., 2021) and randomness in dataset splits of several tasks (e.g., SUN397).",
  "Avg.52.4593.5979.4761.9380.4977.9690.4073.6890.1568.5365.6675.9": "Table S4: Accuracy (%) of our TreeProbe (50k) model on the MTIL benchmark with order-I. Each rowrepresents the performance on every dataset of the model trained after the corresponding task. Transfer, Avg.,and Last metrics are shown in color. We follow the same table arrangement as in ZSCL (Zheng et al., 2023).",
  "Table S3: Comparison of different methods on MTIL in Order I from ZSCL (Zheng et al., 2023). KNN,LinProbe , and TreeProbe (50k) are complementary methods with AIM-Emb as the fusing approach": "We also supplement the results obtained by KNN and LinProbe while comparing to previous methods inTab. S3. As shown, all of the approaches of complementary systems with AIM-Emb achieve good Transfer,reiterating the effectiveness of AIM. LinProbe excels at all metrics with the cost of efficiency, which ispredictable from the results shown in our main manuscript.",
  "S-8Additional ablation experiments": "Effect of different fusing operations. We describe several forms of the fusing operations in Sec. 3.3,including: Avg-Prob, AIM-Prob, Avg-Emb, and AIM-Emb. We compare using TreeProbe (50k) under thetask incremental learning scenario. Fig. S3 shows the results on target and zero-shot tasks. The figureshows that AIM-Emb and AIM-Prob have similar performance on target tasks, surpassing the other two",
  "Figure S3: Target (left) and zero-shot (right) accuracies of different fusing operations": "fusing operations. Combined with the zero-shot performance, the results suggest the effectiveness of AIM inadaptively choosing the better prediction model. The probabilistic prediction is better than the embeddingprediction when performing averaging in both target tasks and zero-shot tasks. But when combined withAIM, the embedding version has a reasonably better performance in zero-shot tasks. Therefore, we chooseAIM-Emb as the default fusing operation. Figure S4: Results of different versions of KNN on target tasks after finishing all stages under the taskincremental learning scenario. We further ablate on k selection so choose k as the x-axis. The vertical dashedline represents k = 9. Different versions of KNN and choice of k. As indicated in Sec. 3.2, we can get the embedding by takingthe one attached to the most likely label (MV-KNN), or averaging the text embeddings of the k nearest neighbors(AVG-KNN), or performing a weighted-averaging over the embeddings of all k nearest neighbors where weightscome from the similarities between the image embedding and the k neighbors image embeddings (WAVG-KNN).We also compare these approaches under different ks to further choose a suitable k for experiments. Asindicated in Fig. S4, from the curve, k = 9 gives reasonable performances of all approaches on both the targetand zero-shot tasks. Compared to AVG-KNN, larger k is more beneficial for MV-KNN since larger k is morestable for MV-KNN, and it is more likely to include more mismatches from the nearest neighbors forAVG-KNN .",
  "Ensemble classifier": "Figure S5: Target (left) and zero-shot (right) task performance comparison w.r.t. ensemble classifiers.TreeProbe w/o ensem. cls. is the version of TreeProbe by finding the cluster most similar to the input andusing the corresponding classifier to predict labels. The reported accuracy is the average across tasks aftercumulatively receiving training data from the datasets shown on the x-axis. Effect of ensemble classifiers in TreeProbe inference. Referencing Sec. 3.2, we observe that ensemblepredictions from multiple classifiers associated with k retrievals slightly enhance performance. Fig. S5 presentsthese results under the task incremental learning setting for eight target tasks. Our final model, TreeProbe, isbetter than its variant without the ensemble classification function in both target and zero-shot performance.The additional inference cost for ensemble predictions is negligible, so we choose it as the default setting forits better performance.",
  "Table S5: Comparison of long-tailed classification on Places365LT (Liu et al., 2019). * means + AIM-Emb.For this experiment, we use CLIP ViT-L/14@336px as the backbone network": "In long-tailed classification, some test labels are rare or unobserved in training, so blending exemplar-basedmodels with consolidated models can be beneficial, as shown by (Long et al., 2022). To accommodate thissetting, we adjust our AIM-Emb method by considering the 2/3 rarest labels as not being present in theexemplar set and the remainder as being present, to calculate vout. In this experiment, the node capacity ofTreeProbe methods is 10k. In Tab. S5, we present results on the Places365LT dataset (Liu et al., 2019). OurAIM-Emb method with LinProbe and TreeProbe outperform the zero-shot baseline. We also compare toPaCo (Cui et al., 2021) and RAC (Long et al., 2022), which are specifically designed for long-tail classification.PaCo incorporates learnable class centers to account for class imbalance in a contrastive learning approach.RAC trains an image encoder augmented with label predictions from retrieved exemplars. Although notspecifically designed for long-tailed classification, our method performs similar to PaCo, but RAC performsbest of all.",
  "S-10Detailed time analysis": "Tab. S6 compares the time complexity and actual times of three prediction approaches in a rapid learningsystem: KNN, LinProbe, and TreeProbe. Please refer to Sec. S-2 for software and hardware environments forthis comparison. The actual times are calculated in a task incremental learning scenario, with k set to 6. KNN has a constant time complexity for both training and inference, with actual times of 9.8 and 416.1seconds respectively. LinProbe has linear training time complexity and constant inference time complexity.The actual training time is considerably long at 30971.4 seconds (8.6 hours), and the inference time is449.1 seconds. Our most recommended algorithm, TreeProbe, has logarithmic training time complexity andlinear inference time complexity related to the number of retrieved classifiers (k). In practice, it exhibitssignificantly shorter training time than LinProbe at 2082.0 seconds (0.6 hour) while having a slight increasein inference time. The table illustrates that TreeProbe strikes a balance between accuracy and efficiency,being more accurate than KNN and more efficient than LinProbe. Note that numbers may vary depending onsoftware and hardware situations. This number is collected from the same PC we used to run all experiments.",
  "TreeProbeO(log n + )O(k)2082.0614.1": "Table S6: Time analysis of different prediction approaches in rapid learning system, namely KNN, LinProbe,TreeProbe. Here, k represents the number of retrieved classifiers. TT means training time while ITmeans inference time. Actual train and inference time is calculated with task incremental learning scenarioby summing up time spend on training and inference across all tasks. For TreeProbe, k = 6. n is the totalnumber of exemplars that are stored, and the log n is due to cluster assignment, which is negligible comparedto , the capacity of each cluster node."
}