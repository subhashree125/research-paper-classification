{
  "Abstract": "We propose Masked Capsule Autoencoders (MCAE), the first Capsule Network that utilisespretraining in a modern self-supervised paradigm, specifically the masked image modellingframework.Capsule Networks have emerged as a powerful alternative to ConvolutionalNeural Networks (CNNs). They have shown favourable properties when compared to Vi-sion Transformers (ViT), but have struggled to effectively learn when presented with morecomplex data. This has led to Capsule Network models that do not scale to modern tasks.Our proposed MCAE model alleviates this issue by reformulating the Capsule Network touse masked image modelling as a pretraining stage before finetuning in a supervised manner.Across several experiments and ablations studies we demonstrate that similarly to CNNsand ViTs, Capsule Networks can also benefit from self-supervised pretraining, paving theway for further advancements in this neural network domain. For instance, by pretrainingon the Imagenette datasetconsisting of 10 classes of Imagenet-sized imageswe achievestate-of-the-art results for Capsule Networks, demonstrating a 9% improvement comparedto our baseline model. Thus, we propose that Capsule Networks benefit from and shouldbe trained within a masked image modelling framework, using a novel capsule decoder, toenhance a Capsule Networks performance on realistically sized images.",
  "Introduction": "Capsule Networks are an evolution of Convolutional Neural Networks (CNNs) which remove pooling opera-tions and replace scalar neurons with a fixed number of vector or matrix representations known as capsulesat each location in the feature map. At each location, there will be multiple capsules, each theoreticallyrepresenting a different concept. Each of these capsules has a corresponding activation value between 0and 1. The activation value measures the probability (from 0 to 1) that the concept represented by thecapsule exists. Capsule Networks have shown promising signs, such as being naturally strong in invariantand equivariant tasks (Sabour et al., 2017; Hinton et al., 2018; De Sousa Ribeiro et al., 2020; Hahn et al.,2019; Ribeiro et al., 2020; 2022) while having low parameter counts, but have yet to scale to more complexdatasets with realistic resolutions that CNNs and Vision Transformers (ViTs) are typically benchmarked on. Masked Image Modelling (MIM) is a Self Supervised Learning (SSL) technique with roots in languagemodelling (Devlin et al., 2018).In language modelling, words are removed from passages of text, thenetwork is then trained to predict the correct words to fill in the gaps. This technique can be extended to",
  "Input": "FlattenReshape : Our Masked Capsule Autoencoder architecture. During pretraining we randomly select a numberof patches from the original image to be processed. The Capsule Network will then create a representationfor each patch. Masked patch capsule representations are then re-added before the capsule decoder, wherethe unmasked capsules can contribute to the masked positions, which are finally decoded by a single linearlayer to the original patch dimensions. The pretraining objective is the mean squared error between thereconstructed patches and the target patches. The dog image used is sourced from the Imagewoof validationset (Howard, 2019b). image modelling by splitting an image into equal regions called patches, randomly removing some of thesepatches and then requiring the network to predict the pixel values of the removed patches. This has beenshown to require the network to have an improved world model in both Vision Transformers (ViTs) (Heet al., 2021) and CNNs (Woo et al., 2023), which is strong enough to reconstruct occluded areas from theremaining visible areas. Combining this technique with supervised finetuning, accuracy can be significantlyimproved compared to not using any pretraining (He et al., 2021; Woo et al., 2023). We propose incorporating MIM pretraining into the training process of Capsule Networks to address itsshortcomings. By doing so, Capsule Networks can learn more refined representations for each region of theimage, leading to improved reconstruction accuracy. These refined local representations can subsequently beemployed during the finetuning phase to effectively activate the appropriate global class capsules, which areintroduced after pretraining. These proposals are based on the observed significant improvements in CNNsand ViTs, particularly in terms of better training regimes with minimal architectural changes. Instead ofsolely focusing on enhancing the routing algorithm, we demonstrate that Capsule Networks can be improvedthrough training modifications.",
  ". We conduct the first investigation into the use of ViTs as a replacement for the traditional convolu-tional stem": "The rest of this paper presents the necessary background on Capsule Networks, highlighting previous re-search that has inspired the work presented here. We then formally define our new self-supervised capsuleformulation called Masked Capsule Autoencoders and present several experiments and ablation studies onbenchmark datasets. We conclude the paper by highlighting the main advantages of our new methods, withsome key takeaway messages and some future directions that could further support the future developmentsof large-scale self-supervised Capsule Network models. While interest in Capsule Networks may have waned in recent years, we argue that this is largely due toresearch being overly focused on routing algorithms, leading to incremental improvements on toy datasetswithout addressing fundamental challenges such as handling complex images. This work represents a signifi-cant departure from this trend by proposing a novel training regime for Capsule Networks, analogous to theadvancements seen in ViTs and CNNs. We posit that this approach could reignite interest in capsule net-works and encourage researchers to explore alternative training paradigms beyond the standard supervisedlearning framework.",
  "Capsule Networks": "Capsule Networks are a variation of CNNs, which replace scalar neurons with vector or matrix capsulesand construct a parse tree, representing part-whole relationships within the network. Each type of capsule ina layer of capsules can be thought of as representing a specific concept at the current level of the parse treewhich is part of a bigger concept. Capsules in deeper layers are closer to the final class label than capsulesin shallower layers. The capsules in the lowest layer, known as primary capsules, detect the most basicvisual features. These basic features serve as building blocks that can be combined to form any of the finalobject classes, forming the foundation of the parse tree. Capsules in lower layers decide their contributionto capsules in higher layers through a process called routing. Capsule Routing, in brief, is a non-linear, cluster-like process that takes place between adjacent capsulelayers.This part of the network has been the predominant research focus for state-of-the-art CapsuleNetworks, to find better or more efficient methods of finding ways to decide the contribution of lowercapsules to higher capsules. In brief, the purpose of capsule routing is to assign part capsules i = 1, . . . , N inlayer to object capsules j = 1, . . . , M in layer +1, by adjusting coupling coefficients RNM iteratively,where 0 ij 1. These coupling coefficients have similarities to an attention matrix (Vaswani et al., 2017)which modulates the outputs as a weighted average of the inputs. For more information on the numerousrouting algorithms proposed for Capsule Networks, please see here (Ribeiro et al., 2022). Dynamic Routing Capsule Networks are the original Capsule Network architecture, as described in(Sabour et al., 2017). DR Caps employs a technique called dynamic routing to iteratively refine the connec-tions between capsules. This approach introduces the concept of coupling coefficients which represent thestrength of each connection and updates them using a softmax function to ensure that each capsule in a lowerlayer must split its contribution amongst capsules that it deems relevant in the higher layer. The updateprocess relies on agreement values calculated as the dot product between a lower-level capsules output anda predicted output from a higher-level capsule. After a pre-determined number of iterations, the activationof each higher-level capsule is calculated as the weighted sum of the lower-level capsule activations, wherethe weights are the final coupling coefficients. Self-Routing Capsule Networks (SR-CapsNet) Hahn et al. (2019) address the computational burden ofiterative routing algorithms in traditional Capsule Networks by introducing an efficient, independent rout-ing mechanism. Unlike conventional routing-by-agreement approaches, SR-CapsNet employs a dedicatedrouting network for each capsule to compute its coupling coefficients directly, rather than relying upon iter-ative agreement methods, drawing inspiration from mixture of experts networks Masoudnia & Ebrahimpour(2014).",
  "Published in Transactions on Machine Learning Research (01/2025)": ": Results of experimentation with a ViT (Dosovitskiy et al., 2020) with depth 4 backbone compared toCapsule Networks with standard CNN backbone. The specific CNN which we use is a ConvMixer (Trockman& Kolter, 2022) of depth 4 due to its easily scalable esoteric design being based on the presumption thatthe image has been patchified, ensuring no information leakage of masked regions due to a sliding windowof overlapping convolutional kernels. In addition, we show the mFLOPS (Total Floating Point Operations /106) to process one image through the backbone, showing that the ConvMixer is significantly more efficient.FLOPs are calculated using the FVCore library FAIR (2023).",
  "where poseout RbBDhw represents the transformed pose matrices": "This formulation enables each capsule to independently determine its routing coefficients through learnedparameters while maintaining the ability to model part-whole relationships. The elimination of iterativerouting significantly reduces computational complexity compared to Capsule Networks with iterative routing. While SR Caps perform competitively on standard benchmarks, their reliance on pre-learned routing networkparameters hinders the networks ability to dynamically adjust routing weights based on the specific input,a key advantage of agreement-based routing approaches. The Stacked Capsule Autoencoder (SCAE) Kosiorek et al. (2019) is another Capsule Network archi-tecture which does not use supervised learning. Comprising of a Part Capsule Autoencoder (PCAE) andan Object Capsule Autoencoder (OCAE), the SCAE segments images into parts and organises these partsinto coherent objects without labelled data. The models unsupervised learning mechanism maximises thelikelihood of reconstructing both the input image and inferred part poses, subject to sparsity constraints. Byleveraging explicit geometric relationships between objects and their parts, SCAE achieves improved gener-alisation and viewpoint robustness. This approach demonstrates competitive performance in unsupervisedclassification tasks, particularly on MNIST and SVHN datasets, without relying on mutual information-based techniques or data augmentation. Consequently, SCAE represents a substantial paradigm shift fromthe conventional Capsule Network training paradigm.",
  "Masked Capsule Autoencoders": "To create the MCAE we must first define how Capsule Networks can have their feature maps masked. InCNNs, this task is challenging and is typically achieved by setting specific regions of the feature map tozero. However, this method differs from the masked autoencoder (He et al., 2021). Zero masking has beendemonstrated to alter the distribution of pixels in an image (Balasubramanian & Feizi, 2023), which in turnaffects the performance. Therefore, in the subsequent section, we will discuss the modifications we haveimplemented to enable accurate masking within our MCAE.",
  "Flattened Feature Map": "2d patch feature map 1d patch feature map 2d capsule feature map 1d capsule feature map length length width width heightheight : A visual representation of how a 2D patch feature map or capsule feature map with height andwidth is flattened into a 1D feature map with a length instead. At each location, there is the same amountof different capsule types, each corresponding to a different part or concept in the part-whole parse tree.The dog image used is sourced from the Imagewoof validation set (Howard, 2019b). Vision Transformers can easily perform masking on a feature map, as patches of the image can be removedfrom computation by simply removing selected patches from the flattened sequence of patches after thepatch embedding layer. Capsule Networks on the other hand have traditionally used a 2D feature map,which comes with the drawback that masking can only be achieved either via replacing masked regions with",
  "0s or utilising sparse operations (Woo et al., 2023), which come with their own drawbacks (Balasubramanian& Feizi, 2023; Tian et al., 2023)": "Thus we propose that by flattening the 2D feature map into a 1D feature map, mimicking the design of aViT feature map, masking can be achieved in the same way as in the Masked Autoencoder (He et al., 2021).We thus achieve masking by simply removing all capsules at a specific location along the length dimensionof our feature map.",
  "Building Upon Self Routing Capsule Networks": "We use SR Caps Network Hahn et al. (2019) as our starting point due to its simplicity and speed, but modifyits routing architecture to better suit our masked image modelling approach. The key architectural changelies in how we handle spatial relationships in routing. The original SR Caps model uses a sliding window approach where each output capsule considers inputsfrom a local region k k of the feature map. In contrast, our MCAE uses point-wise routing where capsulesonly route to the layer above at their exact spatial position in a flattened 1D feature map shown in 2. Thiscreates a local parse tree for each patch position, which aims to determine the concept represented in thepatch. During pretraining, we do not include the final class capsule layer that would normally be part of a supervisedCapsule Network. Instead, after the encoding stage, we introduce a learnable masked token - a shared vectorof the same dimension as our capsule vectors. This masked token is inserted at all positions where patcheswere originally removed, returning the sequence to its original length. Following the design principles ofthe original masked autoencoder He et al. (2021), this same learnable placeholder is used across all maskedpositions and is updated during training as a parameter of the model. The decoder then uses a fully-connected capsule layer where each capsule can route to all capsules in thesubsequent layer regardless of spatial position. This is achieved by setting the kernel size to match thesequence length. While this global routing enables capsules at any position to influence the reconstruction ofany masked patch, it comes with a significantly increased computational cost compared to the position-wiserouting used in the encoder. However, this computational overhead only affects the pretraining phase. Oncepretraining is complete, the decoder is discarded. This design choice trades higher pretraining computation",
  "for improved network accuracy during inference, where the network also maintains the fast inference speedof SR Caps": "When we switch to finetuning, we remove this decoder architecture and replace it with a class capsulelayer. This new layer averages the activations of each capsule type across all spatial positions to make classpredictions, as is standard in the self routing training procedure. However, this layer is now able to leveragethe improved representations required for reconstruction learned during pretraining.",
  "Reconstructed Unmasked Token": ": A visual representation of how our pretrain loss function selects patches for the loss functiondefined in equation 4. The dog image used is sourced from the Imagewoof validation set (Howard, 2019b). A crucial aspect of the pretraining stage of the MCAE involves training the network to accurately reconstructthe masked portions of the input image. To achieve this, we use the Mean Squared Error (MSE) loss, whichquantifies the difference between the actual pixel values of the masked patches and the predicted pixel valuesgenerated by the capsule decoder. MSE loss is defined as:",
  "i=1(yi yi)2(4)": "where N represents the total number of pixels across all masked patches in the training batch, yi is the actualvalue of the ith pixel in the masked patch, and yi denotes the predicted value from our capsule decoder forthe same pixel. A visual representation of patch selection from the target and prediction can be seen infigure 4. The MSE loss aligns with our objective to minimize the difference between the reconstructed and originalpatches, ensuring precise prediction of masked patch pixel values by the capsule decoder. It accentuateslarger discrepancies by squaring errors, thereby pushing the model to improve on significant deviations andenhance reconstruction on each masked patch.",
  "Backbone Selection": "To ensure that information is completely masked out, we replace a standard ResNet (He et al., 2016) orConvNet backbone with a ConvMixer (Trockman & Kolter, 2022). This architectures first layer uses akernel size and stride of equal size, known as a patch embedding layer, allowing our feature map to containno overlapping information. This ensures that when regions of the image are masked, information cannotbe leaked through the overlapping sliding convolutional kernel. We also provide a set of architectures with a ViT backbone. This is achieved by setting the dimension of eachtokens representation to Number of Primary Capsules Primary Capsule Embedding Dimension allowingfor an easy reshape into the primary capsule tensor dimensions. To create the activations for the primarycapsules, we use a simple linear layer with sigmoid activation to ensure that the value of the activationremains between 0 and 1.",
  "PretrainFinetune": ": A visual depiction of the pretrain and finetuning components. We show how the feature extractingCNN and capsule encoder are kept from the pretrain to finetune step. The capsule decoder is discarded afterpretraining and replaced with a class capsules layer that maps the capsule encoder network to a classificationoutput. To validate the effectiveness of our method, we conducted numerous experiments with various ablations onmultiple datasets. These experiments demonstrate that masking is indeed a powerful technique for pushingthe boundaries of Capsule Networks.",
  "Experimental Setup": "All of our experiments follow the same experimental setup. This involves to optionally pretrain the networkminus the class capsules for 50 epochs, with 50% of patches removed on either removed patch or wholeimage reconstruction as a target. We then add the class capsules to our network and fully finetune it for350 epochs, following the supervised training settings of (Hahn et al., 2019; Everett et al., 2023). A visualdepiction of the elements of the components of pretraining and finetuning can be found in figure 5. Allmodels use the SGD optimizer with default settings and the cosine annealing learning rate scheduler with a0.1 initial learning rate. When a validation dataset has not been predefined, we randomly split 10% of the training dataset to act asour validation dataset. The best model is tested once on the test set of our datasets, with the best modelbeing chosen based on the epoch with the lowest validation loss.",
  "papers, as we are the first to provide results on Imagenette, we define the augmentations to be the same asthe augmentations for Imagewoof": "Initially, we provide a sanity check on the MNIST dataset (LeCun et al., 2010), to provide quick experi-mentation to ensure that our methods work at all. Next, we use both the FashionMNIST and CIFAR-10datasets (Xiao et al., 2017; Krizhevsky et al., 2009), two datasets which are well within the abilities of astandard Capsule Network and allow us to ensure that we are not limited to the simplest of experiments.For these two datasets, the augmentation that we use is a random resized crop of 0.8 to 1.0 during training.The SmallNORB dataset (LeCun et al., 2004) allows us to ensure that we are maintaining the equivari-ant properties and generalisation abilities of Capsule Networks as the test set is specifically chosen to varysubstantially from the train set while remaining within a similar distribution. In addition to standard classi-fication accuracy on the SmallNORB dataset, we also follow (Hahn et al., 2019; Ribeiro et al., 2020; Hintonet al., 2018) and test our model on the novel azimuth and elevation tasks to verify generalisation capabilities.The augmentations that we use for this dataset are that we standardise and take random 32x32 crops duringtraining. At test time, we centre crop the images to 32x32 as defined in (Ribeiro et al., 2020). Finally, weuse the Imagenette and Imagewoof datasets (Howard, 2019a;b) to test our networks performance on larger,more realistic datasets. Imagenette and Imagewoof take 10 different classes from the Imagenet dataset (Denget al., 2009). Imagenette is designed to be easily differentiable and tests our networks ability to processlarger, more complex images. Imagewoof is ten classes of dogs and is designed to be more difficult to dif-ferentiate between classes due to the highly overlapping shared features. For Imagewoof and Imagenette wealso resize the images to 64x64 for computational efficiency. We again use a random resize crop and colourjitter for these datasets.",
  ": Graphs comparing MCAE with vs MCAE w/o the pretraining stage. The graphs showthe validation loss (left) and validation accuracy (right) on CIFAR10 over 350 epochs": "presents the classification results of key state-of-the-art Capsule Networks compared to our approachwith no pretraining and with pretraining on the datasets proposed in our experimental design.MCAEwith no pretraining is architecturally similar to SR Caps Networks, but with the 1D modification to thefeature map and 1 1 kernels, along with the other required changes to the computation to allow for this.This method produces better results compared to SR-Caps, but does not achieve state-of-the-art in anydataset. However, when we apply the masked pretraining paradigm, our results improve on all datasetsexcept MNIST. This pushes the MCAE with pretraining to be state-of-the-art for Capsule Networks in alldatasets except SmallNORB, which is still dominated by Capsule Network models based on iterative routingmethods. In addition to Capsule Network models, we have trained three baseline models using the exact",
  "MCAE99.695.092.895.082.161.8": "*The ResNet20-SR results are the performance of the ResNet20 architecture from the Self Routing CapsuleNetworks paper Hahn et al. (2019) which uses more augmentations, such as a random horizontal flip andnormalisation, us to show the performance difference that including augmentations has on the convolutionalbaselines. **The SR Caps SA (Same Augmentations) results are taken from the ProtoCaps paper Everettet al. (2023) and show the performance of the SR Caps model using the same augmentations as us, notethat the only overlapping dataset is Cifar10. ***The Imagenette results for SR Caps are trained using theexact experimental settings defined in ProtoCaps for their Imagewoof results. denotes to denote any resultswhich were taken from their original papers and thus may not use our exact experimental setting, however,care has been taken to follow conventions of training, as defined in section 4.2. same experimental setting as our MCAE with no pretraining regarding epochs, hyperparameters and datasetaugmentations. These models were trained using the Pytorch Image Library (TIMM) Wightman (2019) andare ResNet18 (He et al., 2016), MobileNetv3 (Howard et al., 2019) and ViT-S (Dosovitskiy et al., 2020).",
  "Backbone Choice:": "Leveraging a ConvMixer backbone (Trockman & Kolter, 2022) aligns with our models requirement of apatch embedding layer to provide non-overlapping patches of the image.ConvMixers feature maps areby default patchified, while ViTs (Dosovitskiy et al., 2020) utilise a patch embedding layer.Promptedby this similarity, we explored this as an ablation study. Our observation reveals that ViT-based modelsunderperform compared to those employing a convolutional backbone. Although ViT models yielded betterperformance than vanilla ViTs on smaller datasets, such as CIFAR-10 or SmallNORB, the overall results,shown in table 2, suggest that ConvMixers offer a more suitable architecture for the MCAE and thus havebeen used for our results in 1.",
  "Reconstruction Target:": "While the masked autoencoder (He et al., 2021) framework that we build upon only reconstructs maskedpatches, we also provide results where the reconstruction objective includes visible patches. Reconstructingbased upon the whole image is inspired by DR Caps (Sabour et al., 2017) using a full image reconstructionobjective along with the classification objective in order to regularise the network. The results are shownin table 3 and show that reconstructing masked patches is the best method, with reconstructing all patchesproviding significantly worse results.",
  "MCAE93.285.695.386.1": "In order to verify that we retain the novel viewpoint generalisation capabilities of Capsule Networks, we usethe novel azimuth and elevation tasks of the SmallNORB dataset. We replicate the experimental design of(Hahn et al., 2019; Hinton et al., 2018) and conduct two experiments. 1) Training only on azimuths in (300,320, 340, 0, 20, 40) and test on azimuths in the range of 60 to 280. 2) Training on the elevations in (30, 35,40) degrees from horizontal and then testing on elevations in the range of 45 to 70 degrees. In table 4 wecompare our accuracy on the test set on both the seen and unseen viewpoints. We pretrain for 50 epochsand finetune for 350 epochs, the same as our best model for SmallNORB in table 1.",
  "Conclusion": "We have proposed the Masked Capsule Autoencoder model, the first capsule architecture trained in a self-supervised manner. This could be a step change in the development of scalable Capsule Network models.Extensive experiments demonstrate that MCAE outperforms other Capsule Network architectures on al-most all datasets, with particularly favourable results on higher-resolution images. Considering the uniqueand well-established advantages that Capsule Networks have in capturing viewpoint equivariance and view-point invariance compared with Transformers and CNNs (Ribeiro et al., 2022), our model is a step towardsdeveloping large and scalable Capsule Network models. We would consider the drawbacks of our method to be in the fully capsule decoder. In the Masked Autoen-coder paper (He et al., 2021) they state that the pretraining loss continued to decrease at the point at whichthey stopped pretraining at 1600 epochs. While our reconstruction loss plateaus much quicker, to the pointwhere it does not decrease any further after the 50 epochs for which we pretrain, this indicates that there isa point at which our model has reached the best reconstructions it can achieve. While we have shown thatthe pretraining stage improves the maximum classification accuracy for all datasets except MNIST (due tovery fine margins for quantifiable improvement), if an improved decoding mechanism can be found to ben-efit from additional masked pretraining, the peak classification accuracy could likely be higher. Moreover,the decoder is computationally intensive because it needs to consider the entire feature map, significantlyincreasing training time and VRAM requirements compared to when no finetuning is used.",
  "A.1Decoder Ablations": ": Table showing ablation results of a MLP and Convolutional decoder in our MCAE model. Trainedunder the exact same experimental results as our MCAE model in 1. MLP models are 3 linear layer decodersof dimension num_caps * num_dim * h * w, num_caps * num_dim * h * w * 2, image_channels * img_h* img_w. Each layer has a ReLU activation function. Convolutional models use a ConvNextv2-S decoderas specified in Woo et al. (2023).",
  "Imagewoof": ": Graphs depicting how the top 1 accuracy changes based on different ablations of the MCAE perdataset. Full Image Reconstruction refers to a ConvMixer backbone MCAE pretrained for 50 epochs onfull image reconstruction. No PT refers to a ConvMixer backbone MCAE with no pretraining epochs. ViTBackbone refers to a ViT backbone MCAE pretrained for 50 epochs on masked patch reconstruction. MCAErefers to our best-performing model which utilises a ConvMixer backbone and masked patch reconstruction.All models use the same linear SR Caps model which contains 3 layers, with 16 Capsules per layer and arefinetuned for 350 epochs."
}