{
  "Abstract": "Vision-and-Language Navigation (VLN) has gained increasing attention over recent yearsand many approaches have emerged to advance their development. The remarkable achieve-ments of foundation models have shaped the challenges and methods for VLN research. Inthis survey, we provide a top-down review that adopts a principled framework for embodiedplanning and reasoning, and emphasizes the current methods and future opportunities lever-aging foundation models to address VLN challenges. We hope our in-depth discussions couldprovide valuable resources and insights: on the one hand, to document the progress and ex-plore opportunities and potential roles for foundation models in this field, and on the other,to organize different challenges and solutions in VLN to foundation model researchers1.",
  "Introduction": "Developing embodied agents that are capable of interacting with humans and their surrounding environmentsis one of the longstanding goals of Artificial Intelligence (AI) (Nguyen et al., 2021; Duan et al., 2022). TheseAI systems hold immense potential for real-world applications to serve as multi-functional assistants indaily life, such as household robots (Szot et al., 2021), self-driving cars (Hu et al., 2023), and personalassistants (Chu et al., 2023). One formal problem setting to advance this research direction is Vision-and-Language Navigation (VLN) (Anderson et al., 2018), a multimodal and cooperative task that requires theagent to follow human instructions, explore 3D environments, and engage in situated communications undervarious forms of ambiguity. Over the years, VLN has been explored in both photorealistic simulators (Changet al., 2018; Savva et al., 2019; Xia et al., 2018) and real environments (Mirowski et al., 2018; Banerjee et al.,",
  ": Organizing challenges and solutions in VLNusing LAW framework (Hu & Shu, 2023)": "Recently, foundation models (Bommasani et al.,2021), ranging from early pre-trained models likeBERT (Kenton & Toutanova, 2019) to contem-porary large language models (LLMs) and vision-language models (VLMs) (Achiam et al., 2023; Rad-ford et al., 2021), have exhibited exceptional abili-ties in multimodal comprehension, reasoning, andcross-domain generalization. These models are pre-trained on massive data, such as text, images, au-dio, and video, and could further be adapted fora broad range of specific applications, including em-bodied AI tasks (Xu et al., 2024b). Integrating thesefoundation models into the VLN task marks a piv-otal recent advancement for embodied AI research,demonstrated through significant performance im-provements (Chen et al., 2021b; Wang et al., 2023h;Zhou et al., 2024a). Foundation models have alsobrought new opportunities to the VLN field, suchas expanding the research focus from multi-modalattention learning and strategy policy learning topre-training generic vision and language representa-tions, hence enabling task planning, commonsense reasoning, as well as generalize to realistic environments. Despite the recent impact of foundation models on VLN research, the previous surveys on VLN (Gu et al.,2022; Park & Kim, 2023; Wu et al., 2024) are from the pre-foundation-model era and mainly focus onthe VLN benchmarks and conventional approaches, i.e., they are missing a comprehensive overview of theexisting methods and opportunities leveraging foundation models to address VLN challenges. Especiallywith the emergence of LLMs, to the best of our knowledge, no review has yet discussed their applications inVLN tasks. Moreover, unlike previous efforts that discuss the VLN task as an isolated downstream task, theobjective of this survey is twofold: first, to milestone the progress and explore opportunities and potentialroles for foundation models in this field; second, to organize different challenges and solutions in VLN tofoundation model researchers within a systematic framework. To build this connection, we adopt the LAWframework (Hu & Shu, 2023), where foundation models serve as backbones of world model and agent model.This framework offers a general landscape of reasoning and planning in foundation models, and is closelyscoped with the core challenges in VLN. Specifically, at each navigation step, the AI agents perceive the visual environment, receive language in-structions from humans, and reason upon their representation of the world and humans to plan actionsand efficiently complete navigation tasks. As shown in , a world model is an abstraction thatagents maintain to understand the external environment around them and how their actions change theworld state (Ha & Schmidhuber, 2018; Koh et al., 2021). This model is part of a broader agent model,which also incorporates a human model that interprets the instructions of its human partner, therebyinforming the agents goals (Andreas, 2022; Ma et al., 2023). To review the growing body of work in VLNand to understand the milestones achieved, we adopt a top-down approach to survey the field, focusing onfundamental challenges from three perspectives:",
  "DomainEnvironmentTurnFormatGran.TypeAct. Sp.Other Text Route": "LANI/CHAI (2018)IndoorsCHALETSingle Multi InstrA-DiscManiHHR2R (2018)IndoorsMatterport3DSingle Multi InstrARobotGraphHPR4R (2019)IndoorsMatterport3DSingle Multi InstrARobotGraphHPRxR (2020)IndoorsMatterport3DSingle Multi InstrARobotGraphHPSOON (2021a)IndoorsMatterport3DSingle Multi InstrGRobotGraphHPREVERIE (2020b)IndoorsMatterport3DSingle Multi InstrA, GRobotGraphDetectHPVNLA (2019)IndoorsMatterport3DMultiMulti InstrA, GRobotGraphTPHANNA (2019)IndoorsMatterport3DMultiMulti InstrA, GRobotGraphHPCVDN (2020)IndoorsMatterport3DMultiRestrictedARobotGraphHHVLN-CE (2020)IndoorsHabitat, Matterport3D Single Multi InstrARobotDiscHPRobo-VLN (2021)IndoorsHabitat, Matterport3D Single Multi InstrARobotContHPRobotSlang (2021)IndoorsRealMultiFreeformARobotDiscHPALFRED (2020)IndoorsAI2-THORSingle Multi Instr.A, GRobotDiscManiHPTEACh (2022)IndoorsAI2-THORMultiFreeformA, GRobotDiscManiHHDialFRED (2022)IndoorsAI2-THORMultiRestrictedA, GRobotDiscManiH, TPTouchDown (2019)OutdoorsGoogle Street ViewSingle Multi InstrA-GraphHPStreet Nav (2020)OutdoorsGoogle Street ViewMultiMulti InstrA-DiscTPTalk2Nav (2021)OutdoorsGoogle Street ViewSingle Multi InstrA, G-DiscHPTtW (2018)OutdoorsRealMultiFreeformA, G-DiscHHLCSD (2019)OutdoorsCARLASingle Multi InstrADrivingDiscHPCDNLI (2020)OutdoorsCARLAMultiMulti InstrA, GDrivingContH, THSDN (2022)OutdoorsCARLAMultiFreeformA, GDriving Disc, ContHHAerialVLN (2023b) OutdoorsAirSimSingle Multi InstrA, GAerialDiscHHANDH (2023a)OutdoorsxViewMultiFreeformA, GAerialDiscHH : A summary of existing VLN benchmarks, taxonomized based on several key aspects: the worldin which navigation occurs, the type of human interaction involved, the action space and tasks assigned tothe VLN agent, and the methods of dataset collection. For the world, we consider their domain (eitherindoors or outdoors) and the environment. For the human, we consider their turns of interaction(either single or multiple turn), the format of communication (freeform dialogue, restricted dialogue, ormultiple instructions), and the language granularity (action-directed and goal-directed). For the VLNagent, we consider their agent types (e.g., household robot, autonomous driving vehicles, or autonomousaerial vehicles), their action space (graph-based, discrete or continuous), and other additional tasks(manipulation and object detection). For dataset collection, we consider the text collection (by humanor templated) and the route demonstrations (by human or planner). We present a hierarchical and fine-grained taxonomy in .2 to discuss challenges, solutions, and futuredirections based on foundation models for each model. To organize this survey, we start with a brief overviewof the background and related research efforts as well as the available benchmarks in this field (2). Westructure the review around how the proposed methods have addressed the three key challenges describedabove: world model (3), human model (4), and VLN agent (5). Finally, we discuss the current challengesand future research opportunities, particularly in light of the rise of foundation models (6).",
  "Cognitive Underpinnings of VLN": "Humans and other navigational animals demonstrate early understanding and strategies for navigating theirenvironments (Rodrigo, 2002; Brand et al., 2015; Lingwood et al., 2018).For example, Gallistel (1990)describes two basic mechanisms: piloting, which involves environmental landmarks and computes distancesand angles; and path integration, which calculates displacement and orientation changes through self-motionsensing. Central to understanding spatial navigation is the cognitive map hypothesis, suggesting that thebrain forms a unified spatial representation to support memory and guide navigation (Epstein et al., 2017;Bellmund et al., 2018). For instance, Tolman (1948) observed that rats could adopt the correct novel pathwhen familiar paths are blocked and landmarks are absent. Neuroscientists also discovered hippocampal",
  "Published in Transactions on Machine Learning Research (12/2024)": "Qi Zheng, Daqing Liu, Chaoyue Wang, Jing Zhang, Dadong Wang, and Dacheng Tao. Esceme: Vision-and-language navigation with episodic scene memory. International Journal of Computer Vision, pp. 121,2024b. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, LifangHe, et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXivpreprint arXiv:2302.09419, 2023.",
  "Relevant Tasks and Scope of the Survey": "Following natural language navigation instructions has traditionally been modeled using symbolic worldrepresentations such as maps (Anderson et al., 1991; MacMahon et al., 2006; Paz-Argaman & Tsarfaty,2019). However, our survey focuses on models that employ visual environments and address the challengesof multimodal understanding and grounding. Likewise, we redirect readers to extensive surveys on visualnavigation (Zhu et al., 2021b; Zhang et al., 2022a; Zhu et al., 2022) and mobile robot navigation (Gulet al., 2019; Crespo et al., 2020; Mller et al., 2021), which concentrate on visual perception and physicalembodiment. However, these studies provide minimal discussions on the role of language in navigation tasks.While we inevitably extend our discussions of VLN to encompass areas beyond navigation, such as mobilemanipulation and dialogue, our primary focus remains on navigational tasks, for which we provide a detailedliterature review. Besides, unlike previous VLN surveys (Gu et al., 2022; Park & Kim, 2023; Wu et al., 2024),which offer a bottom-up summary focusing on benchmarks and modeling innovations, our survey adopts atop-down approach, and uses the roles of foundation models to categorize the research efforts into threefundamental challenges from the aspects of the world model, the human model, and the VLN agent. Notethat this survey concentrates on frontier methods associated with the rise of foundation models. Thus, wepoint to the earlier generation of models (e.g., LSTM-based methods) very briefly at the beginning of eachsection to motivate our discussions.",
  "VLN Task Formulations and Benchmarks": "VLN Task Definition.A typical VLN agent receives a (sequence of) language instruction(s) from humaninstructors at a designated position. The agent navigates through the environment using an egocentric visualperspective. By following the instructions, its task is to generate a trajectory over a sequence of discrete viewsor lower-level actions and control (e.g., FORWARD 0.25 meter) to reach the destination, which is consideredsuccessful if the agent arrives within a specified distance (e.g., 3 meters) from the destination. Besides, theagent may exchange information with the instructor during navigation, either by requesting help or engagingin freeform language communication. Additionally, there has been an increasing expectation for VLN agentsto integrate additional tasks such as manipulation (Shridhar et al., 2020) and object detection (Qi et al.,2020b), along with navigation. Benchmarks.Unlike other multimodal tasks such as VQA, which have a relatively fixed task definitionand format, VLN encompasses a wide range of benchmarks and task formulations. These distinctions intro-duce unique challenges in addressing the broader VLN task and must be clearly understood as prerequisitesfor developing effective methods with appropriate foundation models. As is summarized in , exist-ing VLN benchmarks can be taxonomized based on several key aspects in the LAW framework: (1) theworld where navigation occurs, including the domain (indoors or outdoors) and the specifics of the envi-ronment. (2) the type of human interaction involved, including the interaction turns (single or multiple),communication format (freeform dialogue, restricted dialogue, or multiple instructions), and language gran-ularity (action-directed or goal-directed). (3) the VLN agent, including its types (e.g., household robots,autonomous driving vehicles, or autonomous aerial vehicles), action space (graph-based, discrete, or contin-uous), and additional tasks (manipulation and object detection). (4) the dataset collection, including text",
  "Agent ModelAdapting LLMs and VLMs": ": VLN challenges and solutions within the framework of world model, human model, and VLNagent. We discuss history and memory in the world model, ambiguous instructions in the human model,generalization ability in them both. For the VLN agent, we discuss methods for grounding and reasoning,planning, and adapting foundation models as agents. Depending on the role served by the foundation models,we categorize these methods into four types. Additionally, we discuss the potential future of the foundationmodel for the VLN task. collection method (human-generated or templated) and route demonstrations (human-performed or planner-generated). Representatively, Anderson et al. (2018) create the Room-to-Room (R2R) dataset based on theMatterport3D simulator (Chang et al., 2018), where an agent needs to follow fine-grained navigation instruc-tions to reach the goal. Room-across-Room (RxR) (Ku et al., 2020) is a multilingual variation, includingEnglish, Hindi, and Telugu instructions. It offers a larger sample size and provides time-aligned instructionsfor virtual poses, enriching the tasks linguistic and spatial information. Matterport3D allows VLN agents tooperate in a discrete environment and rely on pre-defined connectivity graphs for navigation, where agentstravel on the graph by teleportation between adjacent nodes, referred to as VLN-DE. To make the simplifiedsetting more realistic, Krantz et al. (2020); Li et al. (2022c); Irshad et al. (2021) propose VLN in continu-ous environments (VLN-CE) by transferring discrete R2R paths to continuous spaces (Savva et al., 2019).Robo-VLN (Irshad et al., 2021) further narrows the sim-to-real gap by introducing VLN with continuousaction spaces that are more realistic in robotics settings. Recent VLN benchmarks have undergone severaldesign changes and expectations, which we discuss in 6. Evaluation Metrics.Three main metrics have been employed to evaluate navigation wayfinding perfor-mance (Anderson et al., 2018): (1) Navigation Error (NE), the mean of the shortest path distance betweenthe agents final position and the goal destination; (2) Success Rate (SR), the percentage of the final positionbeing close enough to the goal destination; and (3) Success Rate Weighted Path Length (SPL), which normal-izes success rate by trajectory length to balance both the success rate in reaching the correct destination andthe efficiency of the path. Some other metrics are used to measure the faithfulness of instruction followingand the fidelity between the predicted and the ground-truth trajectory, for example: (4) Coverage Weightedby Length Score (CLS) (Jain et al., 2019) measures how closely an agents trajectory follows the referencepath. It balances two key aspects of the agents performance: the extent of coverage of the reference pathand the efficiency of the agents navigation by considering the length score; (5) Normalized Dynamic TimeWarping (nDTW) (Ilharco et al., 2019), which penalizes deviations from the ground-truth trajectories; and(6) Normalized Dynamic Time Warping Weighted by Success Rate (sDTW) (Ilharco et al., 2019), whichpenalizes deviations from the ground-truth trajectories and also considers the success rate.",
  "Foundation Models": "Foundation models are trained on large-scale datasets, which show strong generalization capability for awide range of downstream applications. Text-only foundation models, such as pre-trained language modelslike BERT (Kenton & Toutanova, 2019) and GPT-3 (Brown et al., 2020), have revolutionized the field ofNLP by setting new benchmarks for tasks like text generation, translation, and understanding. Buildingon the success of these models, vision-language (VL) foundation models, like LXMERT (Tan & Bansal,2019), CLIP (Radford et al., 2021) and GPT-4 (Achiam et al., 2023), have expanded the paradigm tomultimodal learning by integrating both visual and textual data, proving particularly impactful in variousVL applications (Li et al., 2019a; Ramesh et al., 2021; Alayrac et al., 2022; Hong et al., 2021; Zhang et al.,2025; Cheng et al., 2024; Kamali & Kordjamshidi, 2023). For a more comprehensive overview of foundationmodels and their applications, we encourage readers to refer to existing survey papers such as Bommasaniet al. (2021), Du et al. (2022), and Zhou et al. (2023).",
  "World Model: Learning and Representing the Visual Environments": "A world model helps the VLN agent to understand their surrounding environments, predict how their actionswould change the world state, and align their perception and actions with language instructions.Twochallenges have been highlighted in existing work about learning a world model: encoding the visual historyof observations within the current episode as memory, and achieving generalization to unseen environments.",
  "History and Memory": "Different from other vision-language tasks like Visual Question Answering (VQA) (Antol et al., 2015), VisualEntailment (Xie et al., 2019), the VLN agent needs to incorporate the history information of past actions andobservations into its current steps input to determine the action rather than solely considering the imageand text in a single step. Prior to employing the foundation models in VLN, LSTM hidden states servedas an implicit memory supporting agents decision-making during navigation, and researchers further designdifferent attention mechanisms (Tan et al., 2019; Wang et al., 2019) or auxiliary tasks (Ma et al., 2019; Zhuet al., 2020) to improve the alignment between the encoded history and instructions. History Encoding.Different techniques have been proposed to encode navigation history using foun-dation models. A multi-modal Transformer is built upon encoded instructions and navigation history fordecision-making, which is usually initialized from a model pre-trained on in-domain instruction-trajectorydata like Prevalent (Hao et al., 2020). Some approaches encode the navigation history in recurrently updatedstate tokens. Hong et al. (2021) proposes to utilize a single [CLS] token from last step for encoding thehistory information, while Lin et al. (2022a) introduces a variable-length memory framework that storesmultiple action activations from previous steps in a memory bank as the history encoding. Despite theireffectiveness, these methods are limited by the need for step-by-step token updates, making it challengingto efficiently retrieve history encodings at arbitrary steps in the navigation trajectory, which can hinderscalability in pre-training.Another line of work directly encodes navigation history as a sequence withmulti-modal Transformer. Among them, Pashevich et al. (2021) encodes single-view images for each stepin a trajectory. Chen et al. (2021b) further proposes a panorama encoder to encode the panoramic visualobservation at each time step, followed by a history encoder to encode all the past observations. This hier-archical design separately processes the spatial relationship in a panoramic view and the temporal dynamicsacross panoramas in the navigation history. Besides, this method eliminates the dependency on recurrentlyupdated state tokens for history encoding, facilitating efficient and large-scale pre-training on instruction-path pairs. Follow-up research replaces the panorama encoder with mean pooling of images (Kamath et al.,2023) or front-view image encoding (Qiao et al., 2022), both maintaining effective navigation performance.With the advent of LLM-based navigation agents, some works (Zhou et al., 2024b) focus on converting thevisual environment into textual descriptions, and explaining the world with text became the trend. Thenavigation history is then encoded as a sequence of these image descriptions, along with relative spatialinformation such as heading, elevation, and distance. HELPER (Sarch et al., 2023) designs an external",
  "memory of language-program pairs that parses the free-form human-robot dialogue into action programsthrough retrieval-augmented LLM prompting": "Graph-based History.Another line of research enhances the navigation history modeling with graphinformation. For example, some of these techniques utilize a structured Transformer encoder to capturethe geometric cues in the environment (Chen et al., 2022c; Deng et al., 2020; Wang et al., 2023b; Zhou &Mu, 2023; Su et al., 2023; Zheng et al., 2024b; Wang et al., 2021; Chen et al., 2021a; Zhu et al., 2021a).In addition to the topological graph used in encoding, many works propose to include the top-down viewinformation (e.g., grid map (Wang et al., 2023g; Liu et al., 2023a), semantic map (Hong et al., 2023a; Huanget al., 2023a; Georgakis et al., 2022; Anderson et al., 2019; Chen et al., 2022a; Irshad et al., 2022), localmetrics map (An et al., 2023)), and local neighborhood map (Gopinathan et al., 2023) in modeling theobservation history during navigation. Recent advances in LLM-based navigation agents have introducedinnovative approaches to memory construction using maps. For instance, Chen et al. (2024a) proposes anovel map-guided GPT-based agent that utilizes a linguistical-formed map to store and manage topologicalgraph information. MC-GPT (Zhan et al., 2024b) introduces a topological map as the memory structure torecord information about viewpoints, objects, and their spatial relationships.",
  "Generalization across Environments": "One main challenge in the VLN is learning from limited available environments and generalizing to new andunseen environments. Many works demonstrate that learning from semantic segmentation features (Zhanget al., 2021a), dropout information in the environment during training (Tan et al., 2019), and maximizing thesimilarity between semantically-aligned image pairs from different environments (Li et al., 2022a) improveagents generalization performance to unseen environments. These observations suggest the need to learnfrom large-scale environment data to avoid overfitting to training environments.Next, we discuss howexisting works collect new environment data, and utilize it in training. Pre-trained Visual Representations.Most works obtain vision representations from ResNet pre-trained on ImageNet (Anderson et al., 2018; Tan et al., 2019).Shen et al. (2022) replace ResNet withthe CLIP visual encoder (Radford et al., 2021), which is pre-trained with contrastive loss between image-text pairs and naturally better aligns the image with the instructions, boosting the VLN performance.Wang et al. (2022b) further explores transferring vision representation learned from video data for VLNtask, suggesting that temporal information learned from video is crucial for navigation. Environment Augmentation.One main line of research focuses on augmenting the navigation environ-ment with auto-generated synthetic data. EnvEdit (Li et al., 2022b), EnvMix (Liu et al., 2021), KED (Zhuet al., 2023), and FDA (He et al., 2024a) generate synthetic data by changing the existing environments fromMatterport3D. Specifically, they mix up rooms from different environments, change the appearance and styleof the environments, and interpolate high-frequency features with the environments. Pathdreamer (Kohet al., 2021) and SE3DS (Koh et al., 2023) further synthesize the environments in future steps given currentobservations and explore utilizing the synthesis view as augmented data for VLN training. The learning paradigm from the collected environments has changed with the advances in foundation models.Prior to the prevalence of pre-training in foundation models, most works directly augment the trainingenvironment with the auto-collected new environments and fine-tune a LSTM-based VLN agent (Li et al.,2022b; Liu et al., 2021; Koh et al., 2021; 2023; Zhu et al., 2023). As pre-training has been demonstratedto be crucial for foundation models, it has also become a standard practice in VLN to learn from collectedenvironments during the pre-training stage (Li & Bansal, 2024; Kamath et al., 2023; Chen et al., 2022b; Wanget al., 2023h; Lin et al., 2023b; Guhur et al., 2021a; He et al., 2024a). Large-scale pre-training with augmentedin-domain data has become crucial in bridging the gap between agents and humans performance. The in-domain pre-trained multi-modal transformer has been proven to be more effective than the multi-modalTransformer initialized from VLMs, like Oscar (Li et al., 2020) and LXMERT.",
  "Ambiguous Instructions": "Ambiguous instructions mainly arise in single-turn navigation scenarios, where the agent follows an initialinstruction without further human interaction for clarification.These instructions lack the flexibility totrain the agent to adapt its language understanding and visual perception to the dynamic environments. Forinstance, instructions may contain landmarks invisible at the current view or indistinguishable landmarksvisible from multiple views (Zhang & Kordjamshidi, 2023). The issue of ambiguous instructions is barelyaddressed before the application of foundational models to VLN. Although LEO (Xia et al., 2020) attemptsto aggregate multiple instructions to describe the same trajectory from different perspectives, it still relieson human-annotated instructions. However, comprehensive perceptual context and commonsense knowledgefrom foundational models enable the agent to interpret ambiguous instructions using external knowledge, aswell as seek assistance from other human models. Perceptual Context and Commonsense Knowledge.Large-scale cross-modal pre-trained models likeCLIP are capable of matching visual semantics with text. This enables the VLN agent to utilize informationfrom the visual objects and their states in the current perception to resolve ambiguity, especially in single-turn navigation scenarios.For example, VLN-Trans (Zhang & Kordjamshidi, 2023) constructs easy-to-follow sub-instructions with visible and distinctive objects obtained from CLIP to pre-train a Translatorthat converts original ambiguous instructions into easily understandable sub-instruction representations.LANA+ (Wang et al., 2023f) leverages CLIP to query a text list of landmark semantic tags with the visualpanoramic observations, and selects the top-ranked retrieved textual cues as representations of the salientlandmarks to follow. KERM (Li et al., 2023a) proposes a knowledge-enhanced reasoning model to retrievefacts where knowledge is described by language descriptions for the navigation views.NavHint (Zhanget al., 2024b) constructs a hint dataset, providing detailed visual descriptions to help the VLN agent build acomprehensive understanding of the visual environment rather than focusing solely on the objects mentionedin the instructions. On the other hand, the commonsense reasoning ability of LLMs can be used to clarify orcorrect ambiguous landmarks in the instructions, and break instructions into actionable items. For example,Lin et al. (2024b) use LLMs to provide commonsense about open-world landmark co-occurrences and conductCLIP-driven landmark discovery accordingly. SayCan (Ahn et al., 2022) breaks an instruction into a rankedlist of pre-defined admissible actions and combines them with an affordance function that assigns higherweights to the objects appearing in the current scene. Information Seeking.While ambiguous instructions can be resolved based on visual perception andsituational context, another more direct approach is to seek help from the communication partner, i.e.,the human speakers who generate the instructions (Nguyen & Daum III, 2019; Paul et al., 2022). Thereare three key challenges in this line of work: (1) deciding when to ask for help (Chi et al., 2020); (2)generating information-seeking questions, e.g., next action, objects, and directions (Roman et al., 2020;Singh et al., 2022); (3) developing an oracle that provides the queried information, which could be either realhumans (Singh et al., 2022), rules and templates (Gao et al., 2022), or neural models (Nguyen & Daum III,2019). LLMs and VLMs could potentially fit two roles in this framework, either as information-seekingmodels, or as proxies for human helpers or information-providing models. Preliminary research has exploredthe use of LLMs as the information-seeking model, addressing determining both when and what to ask. Thisis achieved with the help of techniques including conformal prediction (CP) (Ren et al., 2023) or in-contextlearning (ICL) (Chen et al., 2023c). For the latter, foundation models play the role of a helper who has accessto oracle information, such as the location of the destination and a map of the environment, which is notavailable to the task performer. Very recently, VLN-Copilot (Qiao et al., 2024) enables agents to activelyseek assistance when encountering confusion, with the LLM serving as a copilot to facilitate navigation.Fan et al. (2023b) demonstrate that GPT-3 can decompose ground-truth responses in the training data",
  "Generalization of Grounded Instructions": "The limited scale and diversity of navigation data is another significant issue affecting the VLN agentsability to comprehend various linguistic expressions and follow instructions effectively, particularly in unseennavigation environments. Although the language style itself has good generalization capability across seenand unseen environments (Zhang et al., 2021a), how to ground the instructions with the unseen environmentsis potentially a hard task given the limited scale of training instructions. Foundation models help addressthese issues through both pre-trained representations and instruction generation for data augmentation. Pre-trained Text Representations.Before the foundation models, many works rely on text encoders,such as LSTM, to represent text instructions (Anderson et al., 2018; Tan et al., 2019). The foundationmodels significantly enhance the VLN agents language generalization ability through pre-trained represen-tations. For example, PRESS (Li et al., 2019b) fine-tunes the pre-trained language model BERT (Kenton &Toutanova, 2019) to obtain text representations that generalize better to previously unseen instructions. Themulti-modal Transformers (Tan & Bansal, 2019; Lu et al., 2019) boost methods, such as VLN-BERT (Majum-dar et al., 2020) and PREVALENT (Hao et al., 2020), to obtain more generic vision-linguistic representationsby pre-training on large-scale text-image pairs collected from the web. Airbert (Guhur et al., 2021b) trainsViLBERT-like architecture to learn text representations from image-caption pairs collected from the Inter-net. CLEAR (Li et al., 2022a) learns cross-lingual language representations that capture the visual conceptsbehind the instruction. ProbES (Liang et al., 2022) self-explores environments by sampling trajectories andautomatically constructs the corresponding instruction by filling the instruction templates with movementsand object phrases detected by CLIP. Additionally, it leverages prompt-based learning to facilitate fastadaptation of language embeddings. NavGPT-2 (Zhou et al., 2025) explores leveraging vision-and-languagerepresentations from pre-trained VLMs (InstructBLIP (Dai et al., 2024) with Flan-T5 (Chung et al., 2024)or Vicuna (Zheng et al., 2023)) to enhance policy learning for navigation and navigational reasoning. Instruction Synthesis.Another method to improve the agents generalization ability is to synthesizemore instructions. Early works employ the Speaker-Follower framework (Fried et al., 2018; Tan et al., 2019;Kurita & Cho, 2020; Guhur et al., 2021a) to train an offline speaker (instruction generator) using human-annotated instruction-trajectory pairs. It then generates new instructions based on sequences of panoramasalong a given trajectory. However, Zhao et al. (2021) observe that these generated instructions are low-quality and show a poor performance in human wayfinding evaluation. Marky (Wang et al., 2022a; Kamathet al., 2023) addresses this limitation using a multi-modal extension of the multilingual T5 model (Xueet al., 2020) with text-aligned visual landmark correspondences, achieving near-human quality on R2R-stylepaths in unseen environments. PASTS (Wang et al., 2023c) introduces a progress-aware spatial-temporalTransformer speaker to better leverage the sequenced multiple vision and action features. SAS (Gopinathanet al., 2024) generates instructions with rich spatial information using semantic and structural cues from theenvironment. SRDF (Wang et al., 2024c) builds a strong instruction generator with iterative self-training.Additionally, instead of training an offline instruction generator, some recent research (Liang et al., 2022;Lin et al., 2023b; Zhang & Kordjamshidi, 2023; Wang et al., 2023e; Magassouba et al., 2021) generatesinstructions while navigating.For instance, LANA (Wang et al., 2023e) introduces a language-capablenavigation agent that not only executes navigation instructions but also provides route descriptions.",
  "Grounding and Reasoning": "Different from other VL tasks, such as VQA and Image Captioning, which primarily focus on static alignmentbetween images and corresponding textual descriptions, the VLN agent needs to reason about spatial andtemporal dynamics in the instructions and the environment based on its actions. Specifically, the agentshould consider previous actions, identify the part of the sub-instruction to execute, and ground the textto the visual environment to execute the action accordingly. Previous methods primarily rely on explicitsemantic modeling or auxiliary task design to obtain such abilities. However, pre-training with speciallydesigned tasks has become the dominant approach with the advent of foundation models. Explicit Semantic Grounding.The previous efforts enhance the agents grounding ability through ex-plicit semantic modeling in both vision and language modalities, including modeling motions and land-marks (Hong et al., 2020b; He et al., 2021; Hong et al., 2020a; Zhang et al., 2021b; Qi et al., 2020a), utilizingsyntactic information in the instruction (Li et al., 2021), as well as spatial relations (Zhang & Kordjamshidi,2022b; An et al., 2021). Very few works (Lin et al., 2023a; Zhan et al., 2024a; Wang et al., 2023b) ex-plore explicit grounding in the VLN agent with the foundation models. Lin et al. (2023a) proposes actionalatomic-concept learning and map visual observations to faciliate multi-modal alignments. Pre-training VLN Foundation Models.Except for explicit semantic modeling, the previous researchalso enhances the agents grounding ability through auxiliary reasoning tasks (Ma et al., 2019; Wu et al.,2021; Zhu et al., 2020; Raychaudhuri et al., 2021; Dou & Peng, 2022; Kim et al., 2021). Such methodsare less explored in VLN agents with foundation models, as their pre-training already provides a generalunderstanding of spatial and temporal semantics prior to navigation. Various pre-training methods withspecially designed tasks have been proposed to improve the agents grounding ability. Lin et al. (2021) intro-duce pre-training tasks specifically designed for scene and object grounding. LOViS (Zhang & Kordjamshidi,2022a) formulates two specialized pre-training tasks to enhance orientation and visual information separately.HOP (Qiao et al., 2022; 2023a) introduces a history-and-order aware pre-training paradigm that emphasizeshistorical information and trajectory orders. Li & Bansal (2023) suggests that enhancing the agent with theability to predict future view semantics helps the agent in longer path navigation performance. Dou et al.(2023) design a masked path modeling objective to reconstruct the original path given a randomly maskedsub-path. Cui et al. (2023) propose entity-aware pre-training by predicting grounded entities and aligningthem to text.",
  "Planning": "Dynamic planning enables VLN agents to adapt to environmental changes and improve navigation strategieson the fly. Alongside the graph-based planners that utilize global graph information to enhance local actionspaces, the rise of foundational models, particularly LLMs, has brought LLM-based planners into the VLNfield. These planners use LLMs vast commonsense knowledge and advanced reasoning to create dynamicplans that improve decision-making. Graph-based Planner.Recent advancements in VLN emphasize enhancing navigational agents planningcapabilities through global graph information. Among them, Wang et al. (2021); Chen et al. (2022c); Denget al. (2020); Zheng et al. (2024b) enhance the local navigation action spaces with global action steps fromgraph frontiers of visited nodes for better global planning. Gao et al. (2023) further enhances navigationdecision-making with high-level planning for zone selection and low-level planning for node selection. More-over, Liu et al. (2023a) enriches the graph-frontier-based global and local action spaces with grid-level actionsfor more accurate action prediction. In continuous environments, Krantz et al. (2021); Hong et al. (2022);Anderson et al. (2021) adopt a hierarchical planning approach utilizing high-level action spaces instead oflow-level ones by selecting a local waypoint from a predicted local navigability graph.CM2 (Georgakiset al., 2022) facilitates trajectory planning by grounding instructions within a local map. Expanding on this",
  "Foundation Models as VLN Agents": "The architecture of VLN agents has undergone significant transformations with the advent of foundationmodels. Initially conceptualized by Anderson et al. (2018), VLN agents were formulated within a Seq2Seqframework, employing an LSTM and an attention mechanism to model the interaction between vision andlanguage modalities. With the advent of foundation models, the agent backend has transitioned from LSTMto Transformer and, more recently, to these large-scale pre-trained systems. VLMs as Agents.The mainstream methodology leverages single-stream VLMs as the core structure ofVLN agents (Hong et al., 2021; Qi et al., 2021; Moudgil et al., 2021; Zhao et al., 2022). These modelsprocess inputs from language, vision, and historical tokens simultaneously at each time step. It performsself-attention over these cross-modal tokens to capture the textual-visual correspondence, which is then usedto infer the action probability. In the zero-shot VLN, CLIP-NAV (Dorbala et al., 2022) utilizes CLIP toobtain natural language referring expressions that describe the target object and make sequential navigationaldecisions. VLN-CE agents (Krantz et al., 2020) differentiate themselves from the VLN-DE (Anderson et al.,2018) agents by their action space, executing low-level controls in the continuous environment instead ofgraph-based high-level actions of view selection. Despite early works (Krantz et al., 2020; Raychaudhuriet al., 2021) utilizing LSTM to infer low-level actions, the introduction of waypoint predictors has allowedto transfer methods from DE to CE (Krantz et al., 2021; Krantz & Lee, 2022; Hong et al., 2022; Andersonet al., 2021; An et al., 2022; Zhang & Kordjamshidi, 2024). All these methods use a waypoint predictor toobtain a local navigability graph, allowing foundation models in DE to adapt to the continuous environment.In particular, the waypoint detection process primarily involves using visual observations (e.g., panoramicRGBD images) to predict navigable candidate adjacent waypoints from the agents current position aspossible targets. Given the predicted waypoints, the agent selects one as the current destination. LLMs as Agents.Since LLMs have powerful reasoning ability and semantic abstraction of the world,and also show strong generalization ability in unknown large-scale environments, recent research in VLNhas started to directly employ LLMs as agents to complete navigation. Typically, visual observations areconverted into textual descriptions and fed into the LLM along with instructions, which then perform ac-tion predictions.Innovations such as NavGPT (Zhou et al., 2024a) and MapGPT (Chen et al., 2024a)demonstrate the feasibility of zero-shot navigation, with NavGPT autonomously generating actions usingGPT-4 and MapGPT converting topological maps into global exploration hints. DiscussNav (Long et al.,2024b) extends this approach by deploying multiple domain-specific VLN experts to automate and reducehuman involvement in navigation tasks. It includes Instruction Analysis Experts, Vision Perception Experts,Completion Estimation Experts, and Decision Testing Experts. The use of multiple domain-specific VLNexperts distributes tasks among specialized agents, reducing the burden on a single model and allowing",
  "Challenges and Future Directions": "While foundation models have enabled novel solutions to VLN, several limitations remain under-explored,and new challenges arise. In this section, we outline the challenges and future direction of VLN from theperspectives of benchmarks, the world model, the human model, the agent model, and real robot deployment. Benchmarks: Limitations of Data and Task.The current VLN datasets have limitations regardingquality, diversity, bias, and scalability. For example, in the R2R dataset, the instruction-trajectory pairs arebiased to the shortest path, which may not accurately represent real-world navigation scenarios. We discussthe trends and recommendations on how VLN benchmarks can be improved. Unified and Realistic Tasks and Platforms. Establishing robust benchmarks and ensuring reproducibil-ity are crucial for evaluating VLN in real-world settings.Real-world variability necessitates compre-hensive benchmarks reflecting navigation challenges. A universal sim-to-real evaluation platform, likeOVMM (Yenamandra et al., 2023), is needed for standardized testing across simulated and real-worldsettings. In addition, the tasks and activities should be realistic and designed originated from humanneeds.For instance, BEHAVIOR-1K (Li et al., 2024a) presents a benchmark of everyday householdactivities in virtual, interactive, and ecological environment to address the demands for diversity andrealism. Dynamic Environment.Real-world environments are inherently complex and dynamic, with movingobjects, people, and variations like lighting and weather presenting unexpected situations (Ma et al.,2022). These factors disrupt the visual perception of navigation systems and make maintaining reliableperformance difficult. Recent efforts like HAZARD (Zhou et al., 2024c), Habitat 3.0 (Puig et al., 2024),and HA-VLN (Li et al., 2024b) consider dynamic environments and provide a good starting point. Indoors to Outdoors. VLN agents navigating in outdoor environments, e.g., autonomous driving andaerial vehicles, also start to get more attention (Vasudevan et al., 2021; Li et al., 2024c), with variouslanguage-guided datasets (Sriram et al., 2019; Ma et al., 2022) developed. Early studies have attemptedto involve LLMs in these tasks, either with prompt engineering (Shah et al., 2023; Sha et al., 2023; Wenet al., 2023), or by fine-tuning LLMs to predict the next action or plan future trajectories (Chen et al.,2024b; Mao et al., 2023). To adapt off-the-shelf VLMs to these outdoor navigation domains, real-worlddriving videos (Xu et al., 2024a; Yuan et al., 2024), simulated driving data (Wang et al., 2023d; Shaoet al., 2024) and them both (Sima et al., 2023; Huang et al., 2024b) have been utilized for instructiontuning so that these foundation models learn to predict future throttle and steering angles. Additionalreasoning and planning modules have also been integrated into foundation model driving agents (Huanget al., 2024b; Tian et al., 2024).We refer the readers to surveys and position papers for a detailedreview (Li et al., 2023b; Cui et al., 2024; Gao et al., 2024; Yan et al., 2024). World Model: From 2D to 3D.Building effective world representations is a central research theme inembodied perception, reasoning, and planning. VLN is fundamentally a 3D task, where the agent perceivesthe real-world environment in 3D. Although the current research represents the world with strong and generic2D representations, they fall short of spatial language understanding in the 3D world (Zhang et al., 2024d).",
  "Broader Impact": "Foundation models hold great promise for advancing vision-language navigation. However, it is essentialto address their broader ethical, legal, and societal implications. Given that they are pre-trained on vast,web-scale datasets, these models can carry inherent biases, which may result in fairness concerns, e.g., tomultilingual users.Some approaches involve continual model training, it is critical to acknowledge andmitigate any potential risks to user privacy, especially when deployed in real-world applications such ashome robotics.",
  "Acknowledgement": "This work is supported in part by the ARO Award W911NF2110220, NSF grant IIS-1949634, and ONRgrant N00014-23-1-2417 & N00014-23-1-2356. Any opinions, findings, and conclusions or recommendationsexpressed in this material are those of the authors and do not necessarily reflect the views of the fundingagencies. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023. Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say: Groundinglanguage in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model forfew-shot learning. In Conference on Neural Information Processing Systems, volume 35, pp. 2371623736,2022. Dong An, Yuankai Qi, Yan Huang, Qi Wu, Liang Wang, and Tieniu Tan. Neighbor-view enhanced model forvision and language navigation. In Proceedings of the 29th ACM International Conference on Multimedia,pp. 51015109, 2021.",
  "Jacob LS Bellmund, Peter Grdenfors, Edvard I Moser, and Christian F Doeller. Navigating cognition:Spatial codes for human thinking. Science, 362(6415):eaat6766, 2018": "Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi.A persistent spatial semanticrepresentation for high-level natural language instruction execution. In Conference on Robot Learning, pp.706717. PMLR, 2022. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael SBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks offoundation models. arXiv preprint arXiv:2108.07258, 2021.",
  "Rebecca J Brand, Kelly Escobar, Adrien Baranes, and Amanda Albu. Crawling predicts infants under-standing of agents navigation of obstacles. Infancy, 20(4):405415, 2015": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models arefew-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances inNeural Information Processing Systems, volume 33, pp. 18771901, 2020. Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niebner, Manolis Savva, ShuranSong, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In7th IEEE International Conference on 3D Vision, 3DV 2017, pp. 667676, 2018.",
  "Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang, and Kwan-Yee K Wong. Mapgpt: Map-guided prompting for unified vision-and-language navigation. arXiv preprint arXiv:2401.07314, 2024a": "Kevin Chen, Junshen K Chen, Jo Chuang, Marynel Vzquez, and Silvio Savarese. Topological planningwith transformers for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1127611286, 2021a. Long Chen, Oleg Sinavski, Jan Hnermann, Alice Karnsund, Andrew James Willmott, Danny Birch, DanielMaund, and Jamie Shotton. Driving with llms: Fusing object-level vector modality for explainable au-tonomous driving.In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp.1409314100. IEEE, 2024b. Peihao Chen, Dongyu Ji, Kunyang Lin, Runhao Zeng, Thomas H. Li, Mingkui Tan, and Chuang Gan.Weakly-supervised multi-granularity map learning for vision-and-language navigation.In Advances inNeural Information Processing Systems, 2022a. Peihao Chen, Xinyu Sun, Hongyan Zhi, Runhao Zeng, Thomas H Li, Gaowen Liu, Mingkui Tan, andChuang Gan. a2 nav: Action-aware zero-shot robot navigation by exploiting vision-and-language abilityof foundation models. 2023b. Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformerfor vision-and-language navigation. In Advances in Neural Information Processing Systems, pp. 58345847,2021b. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Learning fromunlabeled 3d environments for vision-and-language navigation.In European Conference on ComputerVision, pp. 638655. Springer, 2022b. Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Think global, actlocal: Dual-scale graph transformer for vision-and-language navigation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1653716547, 2022c.",
  "Zhimin Chen, Liang Yang, Yingwei Li, Longlong Jing, and Bing Li. Sam-guided masked token predictionfor 3d scene understanding. arXiv preprint arXiv:2410.12158, 2024e": "Zixu Cheng, Yujiang Pu, Shaogang Gong, Parisa Kordjamshidi, and Yu Kong. Shine: Saliency-aware hier-archical negative ranking for compositional temporal grounding. arXiv preprint arXiv:2407.05118, 2024. Ta-Chung Chi, Minmin Shen, Mihail Eric, Seokhwan Kim, and Dilek Hakkani-Tur. Just ask: An interactivelearning framework for vision and language navigation. In Proceedings of the AAAI conference on artificialintelligence, volume 34, pp. 24592466, 2020. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang,Bo Zhang, Xiaolin Wei, et al. Mobilevlm: A fast, reproducible and strong vision language assistant formobile devices. arXiv preprint arXiv:2312.16886, 2023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al.Scaling instruction-finetuned language models.Journal of Machine Learning Research, 25(70):153, 2024.",
  "Jonathan Crespo, Jose Carlos Castillo, Oscar Martinez Mozos, and Ramon Barber. Semantic informationfor robot navigation: A survey. Applied Sciences, 10(2):497, 2020": "Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, ZichongYang, Kuei-Da Liao, et al. A survey on multimodal large language models for autonomous driving. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 958979, 2024. Yibo Cui, Liang Xie, Yakun Zhang, Meishan Zhang, Ye Yan, and Erwei Yin. Grounded entity-landmarkadaptive pre-training for vision-and-language navigation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 1204312053, 2023. Wenliang Dai, Junnan Li, DONGXU LI, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models withinstruction tuning. In Conference on Neural Information Processing Systems, volume 36, 2024. Harm De Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston, and Douwe Kiela.Talk thewalk: Navigating new york city through grounded dialogue. In Visual Learning and Embodied Agents inSimulation Environments (VLEASE) Workshop at ECCV, 2018. Zhiwei Deng, Karthik Narasimhan, and Olga Russakovsky. Evolving graphical planner: Contextual globalplanning for vision-and-language navigation. In Conference on Neural Information Processing Systems,volume 33, pp. 2066020672, 2020. Vishnu Sashank Dorbala, Gunnar Sigurdsson, Robinson Piramuthu, Jesse Thomason, and Gaurav SSukhatme. Clip-nav: Using clip for zero-shot vision-and-language navigation. In Workshop on Languageand Robotics at CoRL 2022, 2022. Zi-Yi Dou and Nanyun Peng. Foam: A follower-aware speaker model for vision-and-language navigation. InProceedings of the 2022 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pp. 43324340, 2022.",
  "Yue Fan, Winson Chen, Tongzhou Jiang, Chun Zhou, Yi Zhang, and Xin Wang. Aerial vision-and-dialognavigation.In Findings of the Association for Computational Linguistics: ACL 2023, pp. 30433061,2023a": "Yue Fan, Jing Gu, Kaizhi Zheng, and Xin Wang. R2h: Building multimodal navigation helpers that respondto help requests.In Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 1480314819, 2023b. Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, TaylorBerg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Conference on Neural Information Processing Systems, volume 31, 2018.",
  "Haoxiang Gao, Yaqian Li, Kaiwen Long, Ming Yang, and Yiqing Shen. A survey for foundation models inautonomous driving. arXiv preprint arXiv:2402.01105, 2024": "Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, and Gaurav S Sukhatme. Dialfred:Dialogue-enabled agents for embodied instruction following. IEEE Robotics and Automation Letters (RA-L), 2022. Georgios Georgakis, Karl Schmeckpeper, Karan Wanchoo, Soham Dan, Eleni Miltsakaki, Dan Roth, andKostas Daniilidis. Cross-modal map learning for vision and language navigation. In Proceedings of theIEEE/CVF conference on Computer Vision and Pattern Recognition, pp. 1543915449, 2022. Muraleekrishna Gopinathan, Jumana Abu-Khalaf, David Suter, Sidike Paheding, and Nathir A Rawashdeh.What is near?: Room locality learning for enhanced robot vision-language-navigation in indoor livingenvironments. arXiv preprint arXiv:2309.05036, 2023. Muraleekrishna Gopinathan, Martin Masek, Jumana Abu-Khalaf, and David Suter. Spatially-aware speakerfor vision-and-language navigation instruction generation. In Proceedings of the 62nd Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 1360113614, 2024. Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, and Xin Wang. Vision-and-language navigation: A surveyof tasks, methods, and future directions. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pp. 76067623, 2022. Jing Gu, Kaizhi Zheng, Kaiwen Zhou Yue Fan, Xuehai He Jialu Wang Zonglin Di, and Xin Eric Wang.Slugjarvis: Multimodal commonsense knowledge-based embodied ai for simbot challenge. In Alexa PrizeSimBot Challenge Proceedings, 2023.",
  "David Ha and Jrgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances inNeural Information Processing Systems 31, pp. 24512463. 2018": "Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao.Towards learning a genericagent for vision-and-language navigation via pre-training. In Proceedings of the IEEE/CVF conference onComputer Vision and Pattern Recognition, pp. 1313413143, 2020. Keji He, Yan Huang, Qi Wu, Jianhua Yang, Dong An, Shuanglin Sima, and Liang Wang. Landmark-rxr:Solving vision-and-language navigation with fine-grained alignment supervision. In Conference on NeuralInformation Processing Systems, volume 34, pp. 652663, 2021. Keji He, Chenyang Si, Zhihe Lu, Yan Huang, Liang Wang, and Xinchao Wang. Frequency-enhanced dataaugmentation for vision-and-language navigation. In Conference on Neural Information Processing Sys-tems, volume 36, 2024a. Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, and Guanya Shi. Learninghuman-to-humanoid real-time whole-body teleoperation. In 2024 IEEE/RSJ International Conference onIntelligent Robots and Systems (IROS), 2024b. Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, FeiHuang, Luo Si, et al. Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervisedlearning and explicit policy injection. In Proceedings of the AAAI conference on artificial intelligence,volume 36, pp. 1074910757, 2022. Karl Moritz Hermann, Mateusz Malinowski, Piotr Mirowski, Andras Banki-Horvath, Keith Anderson, andRaia Hadsell. Learning to follow directions in street view. In Proceedings of the AAAI Conference onArtificial Intelligence, pp. 1177311781, 2020. Yicong Hong, Cristian Rodriguez, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity rela-tionship graph for agent navigation. In Conference on Neural Information Processing Systems, volume 33,pp. 76857696, 2020a. Yicong Hong, Cristian Rodriguez, Qi Wu, and Stephen Gould. Sub-instruction aware vision-and-languagenavigation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP), pp. 33603376, 2020b. Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez Opazo, and Stephen Gould. A recurrent vision-and-language BERT for navigation. In Proceedings of the IEEE/CVF conference on Computer Vision andPattern Recognition, 2021. Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. Bridging the gap between learning in discrete andcontinuous environments for vision-and-language navigation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1543915449, 2022.",
  "Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual language maps for robot navigation.In IEEE International Conference on Robotics and Automation, pp. 1060810615. IEEE, 2023a": "Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-ChunZhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In Proceedings of theInternational Conference on Machine Learning (ICML), 2024a. Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners:Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning,pp. 91189147. PMLR, 2022. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tomp-son, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planningwith language models. In Conference on Robot Learning, pp. 17691782, 2023b. Yidong Huang, Jacob Sansom, Ziqiao Ma, Felix Gervits, and Joyce Chai. Drivlme: Enhancing llm-basedautonomous driving agents with embodied and social experiences. In 2024 IEEE/RSJ International Con-ference on Intelligent Robots and Systems (IROS), 2024b. Gabriel Ilharco, Vihan Jain, Alexander Ku, Eugene Ie, and Jason Baldridge. General evaluation for instruc-tion conditioned navigation using dynamic time warping. In Advances in neural information processingsystems, 2019. Muhammad Zubair Irshad, Chih-Yao Ma, and Zsolt Kira. Hierarchical cross-modal agent for robotics vision-and-language navigation. In 2021 IEEE International Conference on Robotics and Automation (ICRA),pp. 1323813246. IEEE, 2021. Muhammad Zubair Irshad, Niluthpol Chowdhury Mithun, Zachary Seymour, Han-Pang Chiu, Supun Sama-rasekera, and Rakesh Kumar. Semantically-aware spatio-temporal reasoning agent for vision-and-languagenavigation in continuous environments. In International Conference on Pattern Recognition, pp. 40654071. IEEE, 2022. Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, and Jason Baldridge. Stay onthe path: Instruction fidelity in vision-and-language navigation. In Proceedings of the 57th Annual Meetingof the Association for Computational Linguistics, pp. 18621872, 2019. Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf,Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, et al. Conceptfusion: Open-set multimodal 3dmapping. In ICRA2023 Workshop on Pretraining for Robotics (PT4R), 2023.",
  "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. pp. 41714186, 2019": "Hyounghun Kim, Jialu Li, and Mohit Bansal. Ndh-full: Learning and evaluating navigational agents onfull-length dialogue. In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing, 2021. Roberta L Klatzky, James R Marston, Nicholas A Giudice, Reginald G Golledge, and Jack M Loomis.Cognitive load of navigating without vision when guided by virtual sound versus spatial language. Journalof experimental psychology: Applied, 12(4):223, 2006. Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter Anderson. Pathdreamer: A world modelfor indoor navigation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pp. 1473814748, 2021. Jing Yu Koh, Harsh Agrawal, Dhruv Batra, Richard Tucker, Austin Waters, Honglak Lee, Yinfei Yang,Jason Baldridge, and Peter Anderson. Simple and effective synthesis of indoor 3d scenes. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 37, pp. 11691178, 2023.",
  "Jacob Krantz and Stefan Lee. Sim-2-sim transfer for vision-and-language navigation in continuous environ-ments. In European Conference on Computer Vision, pp. 588603. Springer, 2022": "Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision-and-language navigation in continuous environments. In European Conference on Computer Vision, pp.104120. Springer, 2020. Jacob Krantz, Aaron Gokaslan, Dhruv Batra, Stefan Lee, and Oleksandr Maksymets. Waypoint models forinstruction-guided navigation in continuous environments. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 1516215171, 2021. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multi-lingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 43924412, 2020.",
  "Shuhei Kurita and Kyunghyun Cho. Generative language-grounded policy in vision-and-language navigationwith bayes rule. In International Conference on Learning Representations, 2020": "Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martn-Martn, ChenWang, Gabrael Levine, Wensi Ai, Benjamin Martinez, et al. Behavior-1k: A human-centered, embodiedai benchmark with 1, 000 everyday activities and realistic simulation. CoRR, 2024a. Heng Li, Minghan Li, Zhi-Qi Cheng, Yifei Dong, Yuxuan Zhou, Jun-Yan He, Qi Dai, Teruko Mitamura, andAlexander G Hauptmann. Human-aware vision-and-language navigation: Bridging simulation to realitywith dynamic human interactions.In The Thirty-eight Conference on Neural Information ProcessingSystems Datasets and Benchmarks Track, 2024b. Jialu Li and Mohit Bansal.Improving vision-and-language navigation by generating future-view imagesemantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1080310812, 2023.",
  "Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. In Conference on Neural Information Processing Systems, volume 36, 2024": "Jialu Li, Hao Tan, and Mohit Bansal.Improving cross-modal alignment in vision language navigationvia syntactic information. In Proceedings of the 2021 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, pp. 10411050, 2021. Jialu Li, Hao Tan, and Mohit Bansal.Clear: Improving vision-language navigation with cross-lingual,environment-agnostic representations.In Findings of the Association for Computational Linguistics:NAACL 2022, pp. 633649, 2022a. Jialu Li, Hao Tan, and Mohit Bansal. Envedit: Environment editing for vision-and-language navigation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1540715417,2022b. Jialu Li, Aishwarya Padmakumar, Gaurav Sukhatme, and Mohit Bansal.Vln-video: Utilizing drivingvideos for outdoor vision-and-language navigation. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 38, pp. 1851718526, 2024c.",
  "Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple andperformant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019a": "Xiangyang Li, Zihan Wang, Jiahao Yang, Yaowei Wang, and Shuqiang Jiang. Kerm: Knowledge enhancedreasoning for vision-and-language navigation. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 25832592, 2023a. Xin Li, Yeqi Bai, Pinlong Cai, Licheng Wen, Daocheng Fu, Bo Zhang, Xuemeng Yang, Xinyu Cai, TaoMa, Jianfei Guo, et al. Towards knowledge-driven autonomous driving. arXiv preprint arXiv:2312.04316,2023b.",
  "Xinghang Li, Di Guo, Huaping Liu, and Fuchun Sun. Reve-ce: Remote embodied visual referring expressionin continuous environment. IEEE Robotics and Automation Letters, 7(2):14941501, 2022c": "Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli elikyilmaz, Jianfeng Gao, Noah A. Smith, andYejin Choi. Robust navigation with language pretraining and stochastic sampling. In Proceedings of the2019 Conference on Empirical Methods in Natural Language Processing and the 9th International JointConference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,2019, pp. 14941499, 2019b. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu,Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In Com-puter VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings,Part XXX 16, pp. 121137. Springer, 2020. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating objecthallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, 2023c. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and AndyZeng. Code as policies: Language model programs for embodied control. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA), pp. 94939500. IEEE, 2023. Xiwen Liang, Fengda Zhu, Lingling Li, Hang Xu, and Xiaodan Liang. Visual-language navigation pretrain-ing via prompt-based environmental self-exploration. In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 48374851, 2022. Bingqian Lin, Yi Zhu, Xiaodan Liang, Liang Lin, and Jianzhuang Liu. Actional atomic-concept learning fordemystifying vision-language navigation. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 37, pp. 15681576, 2023a.",
  "Matt MacMahon, Brian Stankiewicz, and Benjamin Kuipers. Walk the talk: Connecting language, knowl-edge, and action in route instructions. Def, 2(6):4, 2006": "Aly Magassouba, Komei Sugiura, and Hisashi Kawai. Crossmap transformer: A crossmodal masked pathtransformer using double back-translation for vision-and-language navigation. IEEE Robotics and Au-tomation Letters, 6:62586265, 2021. URL Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter Anderson, Devi Parikh, and Dhruv Batra. Improvingvision-and-language navigation with image-text pairs from the web. In European Conference on ComputerVision, pp. 259274, 2020.",
  "Jiageng Mao, Yuxi Qian, Junjie Ye, Hang Zhao, and Yue Wang. Gpt-driver: Learning to drive with gpt. InNeurIPS 2023 Foundation Models for Decision Making Workshop, 2023": "So Yeon Min, Devendra Singh Chaplot, Pradeep Kumar Ravikumar, Yonatan Bisk, and Ruslan Salakhut-dinov. Film: Following instructions in language with modular methods. In International Conference onLearning Representations, 2021. Piotr Mirowski, Matt Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, DenisTeplyashin, Karen Simonyan, Andrew Zisserman, Raia Hadsell, et al.Learning to navigate in citieswithout a map. In Conference on Neural Information Processing Systems, volume 31, 2018. Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, and Yoav Artzi. Map-ping instructions to actions in 3d environments with visual goal prediction. In Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing, pp. 26672678, 2018.",
  "Ronja Mller, Antonino Furnari, Sebastiano Battiato, Aki Hrm, and Giovanni Maria Farinella. A surveyon human-aware robot navigation. Robotics and Autonomous Systems, 145:103837, 2021": "Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma, Nebojsa Jojic, Hamid Palangi,Robert Ness, and Jonathan Larson. Evaluating cognitive maps and planning in large language modelswith cogeval. In Advances in Neural Information Processing Systems, volume 36, 2023. Abhinav Moudgil, Arjun Majumdar, Harsh Agrawal, Stefan Lee, and Dhruv Batra. SOAT: A scene- andobject-aware transformer for vision-and-language navigation. In Advances in neural information processingsystems, pp. 73577367, 2021. Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai,Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. InConference on Neural Information Processing Systems, volume 36, 2024. Khanh Nguyen and Hal Daum III. Help, anna! visual navigation with natural multimodal assistance viaretrospective curiosity-encouraging imitation learning. In Proceedings of the 2019 Conference on EmpiricalMethods in Natural Language Processing and the 9th International Joint Conference on Natural LanguageProcessing (EMNLP-IJCNLP), pp. 684695, Hong Kong, China, November 2019.",
  "John Okeefe and Lynn Nadel. The hippocampus as a cognitive map. Oxford university press, 1978": "Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Span-dana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodiedagents that chat. In AAAI, 2022. Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva, Phillip Isola, and Yoon Kim.Langnav: Language as a perceptual representation for navigation.In Findings of the Association forComputational Linguistics: NAACL 2024, pp. 950974, 2024. Amit Parekh, Malvina Nikandrou, Georgios Pantazopoulos, Bhathiya Hemanthage, Arash Eshghi, IoannisKonstas, Oliver Lemon, and Alessandro Suglia. Emma: A foundation model for embodied, interactive,multimodal task completion in 3d environments. In Alexa Prize SimBot Challenge Proceedings, 2023.",
  "Sang-Min Park and Young-Gab Kim. Visual language navigation: A survey and open challenges. ArtificialIntelligence Review, 56(1):365427, 2023": "Alexander Pashevich, Cordelia Schmid, and Chen Sun. Episodic transformer for vision-and-language navi-gation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1592215932.IEEE, 2021. Sudipta Paul, Amit Roy-Chowdhury, and Anoop Cherian. Avlen: Audio-visual-language embodied nav-igation in 3d environments. In Conference on Neural Information Processing Systems, volume 35, pp.62366249, 2022. Tzuf Paz-Argaman and Reut Tsarfaty. RUN through the streets: A new dataset and baseline models forrealistic urban navigation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-guage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 64496455, Hong Kong, China, November 2019. Association for Computational Linguistics.doi: 10.18653/v1/D19-1681. URL",
  "Shannon M Pruden, Susan C Levine, and Janellen Huttenlocher. Childrens spatial thinking: Does talkabout the spatial world matter? Developmental science, 14(6):14171430, 2011": "Xavier Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Tsung-Yen Yang, Ruslan Partsey, RutaDesai, Alexander Clegg, Michal Hlavac, So Yeon Min, et al. Habitat 3.0: A co-habitat for humans, avatars,and robots. In The Twelfth International Conference on Learning Representations, 2024. Jennie E Pyers, Anna Shusterman, Ann Senghas, Elizabeth S Spelke, and Karen Emmorey. Evidence froman emerging sign language reveals that language supports spatial cognition. Proceedings of the NationalAcademy of Sciences, 107(27):1211612120, 2010. Yuankai Qi, Zizheng Pan, Shengping Zhang, Anton van den Hengel, and Qi Wu. Object-and-action awaremodel for visual language navigation. In European Conference on Computer Vision, pp. 303317. Springer,2020a. Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van denHengel. Reverie: Remote embodied visual referring expression in real indoor environments. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 99829991, 2020b.",
  "Yanyuan Qiao, Qianyi Liu, Jiajun Liu, Jing Liu, and Qi Wu. Llm as copilot for coarse-grained vision-and-language navigation. In European Conference on Computer Vision, pp. 459476, 2024": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. pp. 87488763, 2021. Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, and Alvaro Velasquez. Saynav:Grounding large language models for dynamic planning to navigation in new environments. In Proceedingsof the International Conference on Automated Planning and Scheduling, volume 34, pp. 464474, 2024. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pp.88218831. Pmlr, 2021. Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. Sayplan:Grounding large language models using 3d scene graphs for scalable robot task planning. In 7th AnnualConference on Robot Learning, 2023. Sonia Raychaudhuri, Saim Wani, Shivansh Patel, Unnat Jain, and Angel X. Chang.Language-alignedwaypoint (LAW) supervision for vision-and-language navigation in continuous environments. In EMNLP,pp. 40184028, 2021. Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, LeilaTakayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large languagemodel planners. Proceedings of Machine Learning Research, 229, 2023.",
  "Junha Roh, Chris Paxton, Andrzej Pronobis, Ali Farhadi, and Dieter Fox. Conditional driving from naturallanguage instructions. In Proceedings of the Conference on Robot Learning, pp. 540551, 2020": "Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. Rmm: A recur-sive mental model for dialogue navigation. In Findings of the Association for Computational Linguistics:EMNLP 2020, pp. 17321745, 2020. Homagni Saha, Fateme Fotouhi, Qisai Liu, and Soumik Sarkar. A modular vision language navigation andmanipulation framework for long horizon compositional tasks in indoor environment. Frontiers in Roboticsand AI, 9, 2022. Gabriel Sarch, Yue Wu, Michael Tarr, and Katerina Fragkiadaki. Open-ended instructable embodied agentswith memory-augmented large language models. In Findings of the Association for Computational Lin-guistics: EMNLP 2023, pp. 34683500, 2023.",
  "Dhruv Shah, Baej Osiski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained modelsof language, vision, and action. In Conference on robot learning, pp. 492504. PMLR, 2023": "Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L Waslander, Yu Liu, and Hongsheng Li.Lmdrive: Closed-loop end-to-end driving with large language models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1512015130, 2024. Sheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, andKurt Keutzer. How much can clip benefit vision-and-language tasks?In International Conference onLearning Representations, 2022. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, LukeZettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everydaytasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1074010749, 2020.",
  "Anna Shusterman, Sang Ah Lee, and Elizabeth S Spelke. Cognitive effects of language on human navigation.Cognition, 120(2):186201, 2011": "Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, AndreasGeiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In First Vision andLanguage for Autonomous Driving and Robotics Workshop, 2023. Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, and Roozbeh Mot-taghi. Ask4help: Learning to leverage an expert for embodied tasks. In Conference on Neural InformationProcessing Systems, volume 35, pp. 1622116232, 2022. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su.Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 29983009, 2023. NN Sriram, Tirth Maniar, Jayaganesh Kalyanasundaram, Vineet Gandhi, Brojeshwar Bhowmick, andK Madhava Krishna.Talk to the vehicle: Language conditioned autonomous navigation of self driv-ing cars. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp.52845290. IEEE, 2019.",
  "Yifei Su, Dong An, Yuan Xu, Kehan Chen, and Yan Huang. Target-grounded graph-aware transformer foraerial vision-and-dialog navigation. arXiv preprint arXiv:2308.11561, 2023": "Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John M. Turner, Noah Maestre,Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimir Vondrus,Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel X. Chang, Zsolt Kira, Vladlen Koltun, JitendraMalik, Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat.In Advances in Neural Information Processing Systems, pp. 251266, 2021. Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 51005111, 2019.",
  "Dennis Ulmer, Elman Mansimov, Kaixiang Lin, Justin Sun, Xibin Gao, and Yi Zhang.Bootstrappingllm-based task-oriented dialogue agents via self-talk. arXiv preprint arXiv:2401.05033, 2024": "Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.Planbench: Anextensible benchmark for evaluating large language models on planning and reasoning about change. InAdvances in Neural Information Processing Systems, 2022. Arun Balajee Vasudevan, Dengxin Dai, and Luc Van Gool.Talk2nav: Long-range vision-and-languagenavigation with dual attention and spatial memory. International Journal of Computer Vision, 129(1):246266, 2021. Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, and Jianbing Shen. Structured scene memoryfor vision-language navigation.In Proceedings of the IEEE/CVF conference on Computer Vision andPattern Recognition, pp. 84558464, 2021. Hanqing Wang, Wei Liang, Luc Van Gool, and Wenguan Wang. Dreamwalker: Mental planning for continu-ous vision-language navigation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 1087310883, 2023a. Liuyi Wang, Zongtao He, Jiagui Tang, Ronghao Dang, Naijia Wang, Chengju Liu, and Qijun Chen. A dualsemantic-aware recurrent global-adaptive network for vision-and-language navigation. In Proceedings ofthe Thirty-Second International Joint Conference on Artificial Intelligence, pp. 14791487, 2023b. Liuyi Wang, Chengju Liu, Zongtao He, Shu Li, Qingqing Yan, Huiyi Chen, and Qi Chen. Pasts: Progress-aware spatio-temporal transformer speaker for vision-and-language navigation. Eng. Appl. Artif. Intell.,128:107487, 2023c. URL Su Wang, Ceslee Montgomery, Jordi Orbay, Vighnesh Birodkar, Aleksandra Faust, Izzeddin Gur, NatashaJaques, Austin Waters, Jason Baldridge, and Peter Anderson. Less is more: Generating grounded naviga-tion instructions from landmarks. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1542815438, 2022a. Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu,Hanming Deng, et al. Drivemlm: Aligning multi-modal large language models with behavioral planningstates for autonomous driving. arXiv preprint arXiv:2312.09245, 2023d. Xiaohan Wang, Wenguan Wang, Jiayi Shao, and Yi Yang. Lana: A language-capable navigator for instructionfollowing and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1904819058, 2023e. Xiaohan Wang, Wenguan Wang, Jiayi Shao, and Yi Yang. Learning to follow and generate instructions forlanguage-capable navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023f. Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William YangWang, and Lei Zhang. Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recog-nition, pp. 66296638, 2019.",
  "Zehao Wang, Mingxiao Li, Minye Wu, Marie-Francine Moens, and Tinne Tuytelaars. Find a way forward:a language-guided semantic map navigator. arXiv preprint arXiv:2203.03183, 2022c": "Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Gridmm: Grid memory map forvision-and-language navigation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 1562515636, 2023g. Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, Junjie Hu, Ming Jiang, and Shuqiang Jiang. Lookaheadexploration with neural radiance representation for continuous vision-language navigation. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1375313762, 2024a.",
  "Zihan Wang, Xiangyang Li, Jiahao Yang, Yeqi Liu, and Shuqiang Jiang. Sim-to-real transfer via 3d featurefields for vision-and-language navigation. arXiv preprint arXiv:2406.09798, 2024b": "Zun Wang, Jialu Li, Yicong Hong, Yi Wang, Qi Wu, Mohit Bansal, Stephen Gould, Hao Tan, and Yu Qiao.Scaling data generation in vision-and-language navigation. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 1200912020, 2023h. Zun Wang, Jialu Li, Yicong Hong, Songze Li, Kunchang Li, Shoubin Yu, Yi Wang, Yu Qiao, Yali Wang,Mohit Bansal, et al. Bootstrapping language-guided navigation learning with self-refining data flywheel.arXiv preprint arXiv:2412.08467, 2024c.",
  "William H Warren. Non-euclidean navigation. Journal of Experimental Biology, 222(Suppl_1):jeb187971,2019": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances inNeural Information Processing Systems, 2022. Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, andYu Qiao. Dilu: A knowledge-driven approach to autonomous driving with large language models. In TheTwelfth International Conference on Learning Representations, 2023.",
  "Wansen Wu, Tao Chang, Xinmeng Li, Quanjun Yin, and Yue Hu. Vision-language navigation: a survey andtaxonomy. Neural Computing and Applications, 36(7):32913316, 2024": "Zongkai Wu, Zihan Liu, and Donglin Wang.Multi-grounding navigator for self-supervised vision-and-language navigation.In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 18.IEEE, 2021. Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 90689079, 2018. Qiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui, Jianfeng Gao, Yejin Choi, and Noah ASmith. Multi-view learning for vision-and-language navigation. arXiv preprint arXiv:2003.00857, 2020. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Languagemodels meet world models: Embodied experiences enhance language models. In Conference on NeuralInformation Processing Systems, volume 36, 2024.",
  "Zhiyuan Xu, Kun Wu, Junjie Wen, Jinming Li, Ning Liu, Zhengping Che, and Jian Tang. A survey onrobotics with foundation models: toward embodied ai. arXiv preprint arXiv:2402.02385, 2024b": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. arXiv preprintarXiv:2010.11934, 2020. Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao,Huan Jin, Jiantao Gao, et al. Forging vision foundation models for autonomous driving: Challenges,methodologies, and opportunities. arXiv preprint arXiv:2401.08045, 2024. Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Fouhey, and JoyceChai. Llm-grounder: Open-vocabulary 3d visual grounding with large language model as an agent. InProceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2024. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang,Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmen-tal feedback. In European Conference on Computer Vision, pp. 2038. Springer, 2025. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multi-modality. arXiv preprint arXiv:2304.14178, 2023. Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin S Wang, Mukul Khanna, TheophileGervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John M Turner, et al.Homerobot:Open-vocabulary mobile manipulation. In Conference on Robot Learning, pp. 19752011, 2023. Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, and Matthew Gadd. Rag-driver: Generalisable driving explanations with retrieval-augmented in-context learning in multi-modallarge language model. arXiv preprint arXiv:2402.10828, 2024. Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr, Saining Xie, YannLeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models as decision-making agentsvia reinforcement learning. In The Thirty-eighth Annual Conference on Neural Information ProcessingSystems, 2024.",
  "Tianyao Zhang, Xiaoguang Hu, Jin Xiao, and Guofeng Zhang. A survey of visual navigation: From geometryto embodied ai. Engineering Applications of Artificial Intelligence, 114:105036, 2022a": "Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Peter Yu, YuweiBao, and Joyce Chai. Danli: Deliberative agent for following natural language instructions. In Proceedingsof the Conference on Empirical Methods in Natural Language Processing, 2022b. Yichi Zhang, Jianing Yang, Keunwoo Yu, Yinpei Dai, Shane Storks, Yuwei Bao, Jiayi Pan, Nikhil Devraj,Ziqiao Ma, and Joyce Chai. Seagull: An embodied agent for instruction following through situated dialog.In Alexa Prize SimBot Challenge Proceedings, 2023.",
  "Yue Zhang and Parisa Kordjamshidi.Narrowing the gap between vision and action in navigation.InProceedings of the 32nd ACM International Conference on Multimedia, pp. 856865, 2024": "Yue Zhang, Quan Guo, and Parisa Kordjamshidi. Towards navigation by reasoning over spatial configu-rations. In Proceedings of Second International Combined Workshop on Spatial Language Understandingand Grounded Communication for Robotics, pp. 4252, 2021b. Yue Zhang, Quan Guo, and Parisa Kordjamshidi. NavHint: Vision and language navigation agent with ahint generator. In Findings of the Association for Computational Linguistics: EACL 2024, pp. 92103,2024b.",
  "Yue Zhang, Ben Colman, Xiao Guo, Ali Shahriyari, and Gaurav Bharaj.Common sense reasoning fordeepfake detection. In European Conference on Computer Vision, pp. 399415. Springer, 2025": "Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. Dovision-language models represent space and how? evaluating spatial frame of reference under ambiguities.In Pluralistic Alignment Workshop at NeurIPS 2024, 2024d. Zhihao Zhang, Shengcao Cao, and Yu-Xiong Wang. Tamm: Triadapter multi-modal learning for 3d shapeunderstanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 2141321423, 2024e. Ming Zhao, Peter Anderson, Vihan Jain, Su Wang, Alexander Ku, Jason Baldridge, and Eugene Ie. On theevaluation of vision-and-language navigation instructions. In Proceedings of the 16th Conference of theEuropean Chapter of the Association for Computational Linguistics: Main Volume, pp. 13021316, 2021.",
  "Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, and Si Liu.Target-driven structured transformer planner for vision-language navigation. pp. 41944203, 2022": "Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning a generalist modelfor embodied navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1362413634, 2024a. Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Di, Xuehai He, and Xin Eric Wang.Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents. arXivpreprint arXiv:2208.13266, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.",
  "Gengze Zhou, Yicong Hong, and Qi Wu.Navgpt: Explicit reasoning in vision-and-language navigationwith large language models. pp. 76417649. AAAI Press, 2024a. doi: 10.1609/AAAI.V38I7.28597. URL": "Gengze Zhou, Yicong Hong, and Qi Wu. Navgpt: Explicit reasoning in vision-and-language navigation withlarge language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp.76417649, 2024b. Gengze Zhou, Yicong Hong, Zun Wang, Xin Eric Wang, and Qi Wu. Navgpt-2: Unleashing navigationalreasoning capability for large vision-language models. In European Conference on Computer Vision, pp.260278. Springer, 2025. Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin Zhang, Yilun Du, Joshua BTenenbaum, and Chuang Gan. Hazard challenge: Embodied decision making in dynamically changingenvironments. In The Twelfth International Conference on Learning Representations, 2024c.",
  "Chen Zhu, Michael Meurer, and Christoph Gnther. Integrity of visual navigationdevelopments, challenges,and prospects. NAVIGATION: Journal of the Institute of Navigation, 69(2), 2022": "Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervisedauxiliary reasoning tasks. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1001210022, 2020. Fengda Zhu, Xiwen Liang, Yi Zhu, Qizhi Yu, Xiaojun Chang, and Xiaodan Liang. Soon: Scenario orientedobject navigation with graph-based exploration. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1268912699, 2021a.",
  "Fengda Zhu, Yi Zhu, Vincent Lee, Xiaodan Liang, and Xiaojun Chang. Deep learning for embodied visionnavigation: A survey. arXiv preprint arXiv:2108.04097, 2021b": "Fengda Zhu, Vincent CS Lee, Xiaojun Chang, and Xiaodan Liang.Vision language navigation withknowledge-driven environmental dreamer.In International Joint Conference on Artificial Intelligence2023, pp. 18401848. Association for the Advancement of Artificial Intelligence (AAAI), 2023. Yi Zhu, Yue Weng, Fengda Zhu, Xiaodan Liang, Qixiang Ye, Yutong Lu, and Jianbin Jiao. Self-motivatedcommunication agent for real-world vision-dialog navigation. In Proceedings of the IEEE/CVF Interna-tional Conference on Computer Vision, pp. 15941603, 2021c."
}