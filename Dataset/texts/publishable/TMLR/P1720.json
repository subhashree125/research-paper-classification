{
  "Abstract": "In this work, we propose gradient-guided discrete walk-jump sampling (gg-dWJS), a noveldiscrete sequence generation method for biological sequence optimization. Leveraging gradi-ent guidance in the noisy manifold, we sample from the smoothed data manifold by applyingdiscretized Markov chain Monte Carlo (MCMC) using a denoising model with the gradient-guidance from a discriminative model. This is followed by jumping to the discrete datamanifold using a conditional one-step denoising. We showcase our method in two differentmodalities: discrete image and biological sequence involving antibody and peptide sequencegeneration tasks in the single objective and multi-objective setting. Through evaluation onthese tasks, we show that our method generates high-quality samples that are well-optimizedfor specific tasks.",
  "Introduction": "Biological sequences like antibody sequences is critical for a broad range of problems involving diversepractical applications, from molecular imaging of cancer (Wu, 2014) to immunotherapy of cancer and viraldiseases (Hudson & Souriau, 2003; Lu et al., 2020). The optimization is challenging: the protein sequencehas an enormous state space with high-entropy variable regions. Experimental validation, on the other hand,is both time-consuming and expensive. Generally, the engineering process begins with in vitro experimentsto determine aspects of interest such as humanization, specificity (Makowski et al., 2022), efficacy, andpharmacokinetic properties, followed by mutation to the samples for improved properties. Ab initio sequence generation is a fascinating direction for generating novel biological sequences given priorsamples. Using a generative model, these methods attempt to generate sequences that are similar to priordata. We can roughly divide these models into two groups: autoregressive models (Wang et al., 2022; Jainet al., 2022) and denoising models (Luo et al., 2022; Gruver et al., 2023). Albeit compelling, both come withtheir drawbacks. On one hand, autoregressive models suffer from error accumulation and even scalability inthe case of reward distribution fitting (e.g., Jain et al. (2022)). On the other hand, denoising models requireintricate noise scheduling, making the real discovery task difficult. To combat the inefficiency of the autoregressive and denoising models, discrete walk-jump sampling (dWJS)(Frey et al., 2024) recently proposed sampling antibody sequences by walking on noisy manifolds, followedby denoising jumping to the discrete data manifold. While dWJS benefits from sampling from noisy mani-folds for discrete sequence generation, its only objective is to sample from the data distribution, leading to",
  "Published in Transactions on Machine Learning Research (12/2024)": "256 hidden dimensions to model the forward layer, which is then followed by a leaky ReLU. The forwardlayer takes the one-hot encoding of the states as inputs and outputs action logits. To model the forwardand backward flow, we double the action space and train the MLP. with a learning rate of 103, includinga learning rate of 0.1 for Z. Since GFlowNets can learn to sample proportionally to a reward distribution,we transform the optimization criteria as a reward function R(x) in the following way:. Paired OAS experiment: optimizing instability index.We label the training sequences using BioPy-thon (Cock et al., 2009) and transform the instability index to a reward by performing the following trans-formation: R(x) = 2index5",
  "Walk-jump sampling (WJS) with neural empirical Bayes (NEB)": "By combining kernel density estimation and empirical Bayes, NEB (Saremi & Hyvrinen, 2019) provides adenoising method to recover X for a smoothed random variable Y = X +N(0, 2Id) such that X Y usingthe gradient log density of Y , where X Y refers to the kernel density estimation smoothing. Formally, toretrieve Y X, given Y=y, we can get the least square estimator of X = x according to Robbins (1992);Miyasawa et al. (1961)x = y + 2logp(y)(1)",
  "Discrete walk-jump sampling (dWJS)": "A key property of the walk-jump sampling framework is that the least square estimation (jump) part isdecoupled from the Langevin MCMC (walk) part. dWJS takes advantage of this decoupling to train anenergy based model or a score based model on the smoothed distribution on the noised samples, samplenoisy samples with Langevin MCMC, and return back to the denoised samples with a least square estimator.In our work, we restrict ourselves to score based model for both the sampler and the least square estimator.",
  "Classifier guidance in generative models": "To obtain a truncation-like in diffusion models, Dhariwal & Nichol (2021) modifies the diffusion score of amodel with an auxilary gradient from the log likelihood of a classifier model. The modified score is thenused during sampling from the diffusion model, which affects sampling by upweighting the data the classifierlabels with high probability to the chosen label, providing a conditioning to the sampling process. We adaptthis effect in our method by incorporating a discriminator model in the sampling process. Unlike the classfierguidance in diffusion models that uses during sampling, our method incorporates gradient guidance duringthe least square estimation (jump) too.",
  "Finally,xis Pareto-optimal xi xi {1, . . . , d}": "Pareto set constitutes the Pareto-optimal candidates, and Pareto front defines their images in the objectivespace. A popular method to tackle MOO is Scalarization, which decomposes the MOO problem into a single-objective problem. In this method, we assign convex weight wi to each objective Ri(x) such that wi > 0 anddi=1 wi = 1. Using the weights, we can derive a single-objective formulation, i.e., maxxX R(x|w), whereR(x|w) is the scalarization function.A widely used scalarization function is weighted-sum scalarization(Ehrgott, 2005; Miettinen, 1999), where R(x, w) = di=1 wiRi(x). Using this formulation, we can representthe MOO problem as a family of subproblems, each defined by a different preference vector w. Under certainassumptions, the solutions to these problems are the Pareto optimal candidates for the preference vector w.",
  "Gradient-guided discrete walk-jump sampling": "gg-dWJS augments dWJS by learning a discriminator model P(C|Y ) (Step A) on the noisy data Y giventhe true data X and its label C. Next, we train the score model g on the noisy data Y (Step B). Later,we utilize the discriminators gradient to guide the Langevin MCMC walk (Step C). Finally, we jump fromthe noisy data manifold to the true discrete data manifold using the gradient guidance (Step D). For theremainder of the paper, we use the following notations for the random variables: X refers to the true data,Y refers to the noisy data, and C refers to the class labels. The lowercase of of the said variables refers tothe specific instances of them.",
  "Step A: Learning a noisy discriminator model": "Given the true data X and its label C, we smooth X by adding Gaussian noise, obtaining Y = X+N(0, 2Id).Next, we learn the discriminator model logP(C|Y ) by parameterizing it with f(Y ). Note that C can beboth categorical and continuous. For example, for categorical values, we parameterize f(y) to return logitsover the labels given y, while for a specific attribute (e.g., Instability index), we simply learn the f(y) overthe attribute.",
  "Step C: Gradient guided walk": "We start by sampling random discrete data y0 Ud. Langevin MCMC (Sachs et al., 2017) uses the gradientof log density to guide the sampling process by providing information about the structure of the targetdistribution. In our method, we perform discretized Langevin MCMC on y with gradient guidance fromboth score function g(y) and discriminator f(y). Formally, we use",
  "g(yk) + Y f(yk, c)": "to perform MCMC for K steps given yk and label c at the kth step, where is the relative gradient guidancestrength with which we control the effect of gradient guidence.. Intuitively, one can see this as performinga gradient accent on the label cs probability with respect to the input yk to find the local maxima whilewalking towards the smoothed data density with g(yk). We summarize the process in algorithm 1. For a",
  "Step D: Gradient guided jump": "After the K steps of walking, we jump from the noisy data manifold to the clear discrete data using thegradient guidance and score model given smoothed data yk and label c.From NEB, given a smoothedinput y, we can jump back to x using the equation 1. Without loss of generality, we can extend that, for aconditional variable c and y p(y|c),",
  "y + 2yf(y, c) + 2g(y)": "Therefore, according to the above demonstration, we perform a one-step denoising to return to the truediscrete data manifold using f and g from steps A & B. Specifically, the jump step approximates the truediscrete point x by adding the gradient-guided adjustments to yK where yK is a point at any K time in thewalk process",
  "Multi-objective optimization with gg-dWJS": "Given the added conditioning capability, we can now proceed to add the formulation for preference con-ditioning to our method. In the case of preference conditioning, since di=1 wi = 1, optimizing a singleobjective R(x, w) = di=1 wiRi(x) refers to a single point in the pareto front. This means that if we have agenerative model that is controllable based on the preference vector w = {w1, w2, . . . , wd}, we can generatethe solutions to the sub-problems, thereby constructing the pareto front. A naive approach for controllable generation with preference conditioning is to simply train d noisy objectivefunction models fi(y, w) wRi(y) and use d gradients to guide the generative process, i.e.,",
  "= g(y) + yf(y, w)": "Here, f(y, w) w1R1(y) + w2R2(y) + + wdRd(y) is the preference-conditioned single-objective function.By learning this function approximation, we can have two benefits: (a) we need only one discriminatortraining, and (b) we can generalize to different preference w not seen in training by taking advantage of theshared structure in the noisy manifold. Training the preference-conditioned discriminator model is simple:we can sample from a preference distribution p(w) during training, use w to encode multiple objectives to asingle-objective function, and learn a function approximator f.1",
  "Can gg-dWJS generate conditionally, i.e., can we generate binarized MNIST images with a specificlabel (say, 0)?": "In figure 2, we show the results of our experiment. On the left, we compare the samples generated by dWJS,gg-dWJS without denoising walk, and gg-dWJS. Here, for gg-dWJS without denoising walk, we only usef(yk, c) as the gradient for the Langevin MCMC. The samples show that gg-dWJS produces the mostrealistic and diverse samples, which is further affirmed by the lowest Frchet inception distance (FID) (Heuselet al., 2017) in table 1. Besides, we experiment with performing the walk step using only gradient-guidance,but we find that it cannot generate high-quality samples. On the right, we show the samples of labels 0,3, and 8 produced by gg-dWJS. They showcase the methods ability to conditionally generate samples. Wereport the details related to this experiment in .",
  "Paired chain antibody sequence generation (|X| 10393)": ": Visualization of generated heavy and light chain sequences using gg-dWJS optimized for instabilityindex. We use IgFold (Ruffolo et al., 2023) for sequence folding and Mol* viewer (Sehnal et al., 2021) forvisualization. Now, we turn our attention to antibody sequence generation. Following Frey et al. (2024); Gruver et al.(2023), we represent antibody sequences as x = (x1, . . . , xd), where xi 1, . . . , 21 refers to the 20 amino acid(AA) type, augmented by the gap token -. We use the 1.3M sequences provided by Frey et al. (2024).These sequences are from observed antibody space (OAS) database, aligned according to the AHo numberscheme (Honegger & Pluckthun, 2001) using the ANARCI package (Dunbar & Deane, 2016). The gappedheavy and light chain lengths are respectively 149 and 148, making the total dimension (149 + 148) 21.For the optimization task, we experiment on two simple single-objective tasks: the percentage of beta sheetsand the protein instability index (Guruprasad et al., 1990). For each task, we train a smoothed predictor onthe antibody sequences and use its gradient guidance to optimize the single objective. Results.We compare gg-dWJS with dWJS, a latent diffusion method generalized for discrete sequences(Kingma et al., 2021, variational diffusion models (VDM)), a transformer-based language model trained forantibody sequence design (Shuai et al., 2023, IgLM), a pre-trained large language model (LLM) (GPT-4o),and a probabilistic method for discrete sequence generation (Bengio et al., 2023; Jain et al., 2022, generativeflow networks (GFlowNets)).",
  "MethodDCS Instability index % Beta sheets": "dWJS0.49 0.3034.14 6.380.393 0.02gg-dWJS w/ Beta sheet discriminator0.51 0.2835.92 6.250.408 0.02gg-dWJS w/ Instability discriminator0.56 0.2731.32 5.210.402 0.023VDM0.34 0.3139.50 6.790.37 0.02IgLM0.19 0.2938.84 6.180.37 0.02GPT-4o0.31 0.3042.63 2.470.37 0.01GFlowNets0.0 0.039.04 1.04N/A We report the results of our experiments in table 3. Specifically, we report the distributional conformity score(DCS), percentage of beta sheets, and instability index for samples generated by dWJS and gg-dWJS withthe two discriminators.2 We choose the metrics for their fast evaluation following previous works (Stantonet al., 2022; Gruver et al., 2024), though the metrics may have limited impact for real-world designs. Wedescribe the metrics in more details in the Appendix C.3. The results show that with beta sheet gradientand instability index guidance, gg-dWJS generates samples with improved beta sheet percentage (Pairedt-test, p-value 0.005) and instability index (Paired t-test, p-value 0.005) than dWJS, respectively.The results also show that our method achieves the best DCS score among all the baselines, showing itscapability of generating sequences of high Ablikeness. It is worth noting that we also train the VDM fromthe same data, but its DCS score is not as high. Besides, our method with a instability index and betasheet percentage discriminator achieves the best instability index and beta sheet percentage of all methods,respectively. Surprisingly, for the beta sheet percentage counterpart, many other methods come close, butthey cannot generate sequences with good ablikeness, as evident by their low DCS. The result shows thatnot only does gg-dWJS generate antibody sequences that are better optimized for their respective objectives,but it also produces sequences of higher quality overall. Details of this experiment are in appendix C",
  ": Pareto front of the samples generatedusing three preference weights with gg-dWJS": "The main goal of this experiment is to showcase thecapability of gg-dWJS to generate samples with pref-erence conditioning.Towards that vision, we chooseinstability index and beta sheet percentage as theattributes to perform preference conditioning and trainthe preference-conditioned discriminator f(y, w) suchthat w = {w1, 1 w1} where p(w1) U(1). Followedby a scalar normalization of the attributes, we perform aweighted-sum scalarization to decompose the attributesinto a single attribute.Subsequently, using the threedifferent preference vectors w {, , [0.5, 0.5]},we generate 1k samples. shows the distributionof the attributes of the generated samples, showing thatsamples with different preference-conditioning indeed fallinto a distinct distribution. Besides, the generated sam-ples exhibit good diversity, as shown in table 5. Finally,in figure 4, we show a pareto front for the generatedsamples, showcasing our methods ability to performin a multi-objective setting. The results show that ourmethod is able to generate samples according to different preferences w, e.g., w = generates samplesfocusing on the beta sheet percentage while w = generates samples focusing on the instability index.",
  "Effect of guidance strength": "To understand the effect of the relative gradient-guidance strength , we run the antibody sequence op-timization with the instability index discriminator guidance for {100, 101, 102, 103} while keeping Kfixed at 40. shows the result of our experiment, showing that optimization and diversity are intensiongreater optimization leads to sampling from a smaller region of the sampling space. Similarly, wecan see that higher guidance strength only leads to improvement in DCS up to a certain extent, after whichthe DCS reduces. : Ablation results on for antibody sequence optimization task. The results show that while highergradient-guidance strength leads to better optimization, it also leads to less diversity and data fidelity.",
  "Experiment on anti-microbial peptide (AMP) design": "The goal of this experiment is to generate peptides with anti-microbial properties. To this end, we collectAMPs and non-AMPs from the DBAASP database (Pirtskhalava et al., 2021). Following (Jain et al., 2022),we choose peptides of sequence length of 12 to 60 with the target group Gram-positive bacteria, whichyields 6438 positive AMPs and 9222 non-AMPs.3 Then, we split the dataset into two parts: D1 and D2,following Angermueller et al. (2019) where D1 is visible to our algorithm and D2 is used to train the oraclefor validation of the results. To split the dataset, we follow the same principle as Jain et al. (2022): forany peptide x D1, there are no peptides x D2 such that x belongs to xs group and vice versa.This split yields 3219 AMPs and 4611 non-AMPs in D1. Finally, we train the oracle utilizing ProtTrans(Elnaggar et al., 2007) features from D2 and utilizing MLP classifiers (89% accuracy). For the gg-dWJSimplementation, we follow the same score model and discriminator model setup from the antibody CDR H3design experiment (details in Sections D.1 and D.2). Finally, to adapt to the variability of sequence length,we pad all sequences with gap tokens appended to the end. Results.We compare our method against baselines such as GFlowNet-AL (Jain et al., 2022), DynaPPO(Angermueller et al., 2019), COMs (Trabucco et al., 2021), and GFlowNets (Bengio et al., 2021) utilizingthe reported data from Jain et al. (2022) under the criteria performance, diversity, and novelty.Here,performance measures the count of samples classified as AMP by the oracle, diversity measures the differenceamong the generated samples, and novelty measures the difference between the generated samples and thetraining dataset (D1). We detail the metrics in Appendix C.3.",
  ": Distribution of amino acids found in the generated AMPs by gg-dWJS matches that of knownAMPs while maintaining focusing on amino acid \"K\", which is dominant in peptides with anti-microbialactivity": "K = 100 and t = 5. However, it is also evident that GFlowNet-AL achieves better novelty than our method.However, as Kirjner et al. (2023) reports, higher novelty is not necessarily equivalent to better results. Forinstance, a random algorithm would achieve maximum novelty. We also see a lower average instability index of 26.3 compared to 26.5 reported by GFlowNet-AL, showingthat gg-dWJS generates biologically relevant sequences (score of over 40 indicates instability). showscomparison of the distribution of amino acids between the known AMPs and gg-dWJS generated AMPs.We can see that gg-dWJS generates peptides that retains the amino acid profiles of the known AMPs butboosts important characteristics generally found in AMPs, e.g., the amino acid \"K\", which is dominant inpeptides with anti-microbial activity (Jain et al., 2022).",
  "Discrete generative models": "A large class of discrete generative models consist of autoregressive models (i.e., language models). Amongmany others, Austin et al. (2021) learns the posterior alphabet distribution over the prior generated databy learning the bidirectional context in a language model. Recent works such as Generative Flow Networks(GFlowNets) (Bengio et al., 2023; 2021) model the flow as a DAG, and learn to sample directly propor-tional to a given reward function. Zhang et al. (2022) uses this idea to train and sample from an energybased model (EBM) (LeCun et al., 2006) using contrastive divergence (Carreira-Perpinan & Hinton, 2005).Besides, Grathwohl et al. (2021) improves the sampling process by leveraging local gradients using a Gibbssampler (George & McCulloch, 1993; Gelfand, 2000). Another way to generate data is through denoising models (Ho et al., 2020; Song & Ermon, 2020; Songet al., 2021). These models learn the gradient log density of the data and generate continuous data from theperturbed data distribution. Thus, they are faster and more efficient than autoregressive models. Of course,one cannot simply apply the denoising gradient to the discrete data distribution because the gradients arenot defined there. Many promising works attempt to remedy this by proposing different techniques. Chenet al. (2023) simply transforms the discrete data into analogue bits, learns the denoising gradient, andapplies thresholding to return the categorical data. Sun et al. (2023) applies denoising via a continuous-timeMarkov chain to the categorical data using a stochastic jump process. For graph adjacency matrices, Niuet al. (2020) adds Gaussian perturbation to the upper triangular matrix, sample using the learn score model,and transform samples in the continuous space by quantizing the generated continuous adjacency matrix toa binary one. On a similar vain, Yan et al. (2024) performs post-hoc thresholding after the end of Langevindynamics to tranform the denoised continuous adjacency matrix to a binary or discrete adjacency matrix.",
  "Protein sequence optimization": "With the advancement of language models, many recent works (Madani et al., 2020; Nijkamp et al., 2023;Ferruz et al., 2022) propose language models pre-trained on protein data to generate protein sequences,often for tasks such as structure prediction (Lin et al., 2023). These protein language models have alsobeen adopted for antibody generation tasks, both unguided (Shuai et al., 2021) and guided (Gligorijeviet al., 2021; Ferruz & Hcker, 2022; Tagasovska et al., 2022). A key problem with language modeling ofantibodies is that they struggle to capture the data distribution of the antibody sequences given a limitedamount of high-quality data and high-entropy variable regions of the antibody sequences. As such, recentworks such as Luo et al. (2022); Gruver et al. (2023); Peng et al. (2023) leverage the continuous space ofantibody structures to generate antibody structures using diffusion. In this work, we focus on the problemof generating targeted antibody sequences.",
  "Conclusion and limitations": "In this work, we introduce gg-dWJS, which learns a discriminative model on the smoothed data and usesthe gradient information to augment its denoising walk towards the local maxima given some attributes.Finally, it returns to the clean data manifold by using a conditional jump with the same model. Thus,our method requires no additional score model training for different optimization tasks. Additionally, weformalize our method in a multi-objective setting using a preference-conditional discriminator model. Usingpreference-conditioning, we show that our method can generate samples that correspond to different regionsof the multi-objective space based on preference. We show our methods applicability in two modalities:discrete imagine generation and biological sequence generation in single objective and multi objective cases. Despite our sincere efforts, our work falls short in addressing numerous issues and has limitations.Forstarters, while we see an improvement in DCS with gradient guidance, the improvement starts to fade awaywith the higher strength of the gradients. It is also worth mentioning that more optimization leads to lessdiversity in the samples. Indeed, the general tension of data fidelity, diversity, and optimization still holdstrue in our case, and improving them together is still a challenge. For example, our current approach withMCMC can be a limitation for diversity, as it is known to suffer from mode collapse, where it diverges to alocal solution for many starting points. There are many line of work that attempts to tackle this problem(De Souza et al., 2022; Liu et al., 2023; Wang et al., 2024b) from different viewpoints, which we believe canbe an useful addition to our work. Future works can also tackle the problem of optimization and uniquenesstogether through the addition of an active learning pipeline (Settles, 2009; Hernandez-Garcia et al., 2023) toour method. Besides, one can improve fidelity of the generated sequences through the use of strcture priors,similar to the recent work by Wang et al. (2024a). Furthermore, we did not investigate different scalarizationfunctions in our preference-conditioning setting; we left that along with the multi-objective benchmarkingof our method for future work.",
  "Miguel A Carreira-Perpinan and Geoffrey Hinton.On contrastive divergence learning.In Internationalworkshop on artificial intelligence and statistics, pp. 3340. PMLR, 2005. 11": "Ting Chen, Ruixiang ZHANG, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusionmodels with self-conditioning. In The Eleventh International Conference on Learning Representations,2023. URL 11 Peter JA Cock, Tiago Antao, Jeffrey T Chang, Brad A Chapman, Cymon J Cox, Andrew Dalke, IddoFriedberg, Thomas Hamelryck, Frank Kauff, Bartek Wilczynski, et al. Biopython: freely available pythontools for computational molecular biology and bioinformatics. Bioinformatics, 25(11):1422, 2009. 21, 23 Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas F Milles,Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al.Robust deep learningbasedprotein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. 2 Daniel A De Souza, Diego Mesquita, Samuel Kaski, and Luigi Acerbi. Parallel mcmc without embarrassingfailures. In International Conference on Artificial Intelligence and Statistics, pp. 17861804. PMLR, 2022.12",
  "Edward I George and Robert E McCulloch. Variable selection via gibbs sampling. Journal of the AmericanStatistical Association, 88(423):881889, 1993. 11": "Vladimir Gligorijevi, Daniel Berenberg, Stephen Ra, Andrew Watkins, Simon Kelow, Kyunghyun Cho, andRichard Bonneau. Function-guided protein design by deep manifold sampling. bioRxiv, pp. 202112, 2021.12 Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud, and Chris Maddison. Oops i took agradient: Scalable sampling for discrete distributions. In International Conference on Machine Learning,pp. 38313841. PMLR, 2021. 11 Nate Gruver, Samuel Don Stanton, Nathan C. Frey, Tim G. J. Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, Arvind Rajpal, Kyunghyun Cho, and Andrew Gordon Wilson.Protein design with guideddiscrete diffusion. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL 1, 7, 12 Nate Gruver, Samuel Stanton, Nathan Frey, Tim GJ Rudner, Isidro Hotzel, Julien Lafrance-Vanasse, ArvindRajpal, Kyunghyun Cho, and Andrew G Wilson. Protein design with guided discrete diffusion. Advancesin neural information processing systems, 36, 2024. 8 Kunchur Guruprasad, BV Bhasker Reddy, and Madhusudan W Pandit. Correlation between stability of aprotein and its dipeptide composition: a novel approach for predicting in vivo stability of a protein fromits primary sequence. Protein Engineering, Design and Selection, 4(2):155161, 1990. 7, 19",
  "Aapo Hyvrinen and Peter Dayan.Estimation of non-normalized statistical models by score matching.Journal of Machine Learning Research, 6(4), 2005. 2": "Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks, Bonaventure FP Dossou,Chanakya Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael Kilgour, Dinghuai Zhang, et al. Biological sequencedesign with gflownets. In International Conference on Machine Learning, pp. 97869801. PMLR, 2022. 1,7, 10, 11, 20 Wengong Jin, Jeremy Wohlwend, Regina Barzilay, and Tommi S Jaakkola. Iterative refinement graph neuralnetwork for antibody sequence-structure co-design. In International Conference on Learning Representa-tions, 2021. 19",
  "VI Levenshtein. Binary codes capable of correcting deletions, insertions, and reversals. Proceedings of theSoviet physics doklady, 1966. 20": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, RobertVerkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structurewith a language model. Science, 379(6637):11231130, 2023. 12 Haozhe Liu, Bing Li, Haoqian Wu, Hanbang Liang, Yawen Huang, Yuexiang Li, Bernard Ghanem, andYefeng Zheng. Combating mode collapse via offline manifold entropy estimation. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 37, pp. 88348842, 2023. 12",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference onLearning Representations, 2019. URL 21": "Ruei-Min Lu, Yu-Chyi Hwang, I-Ju Liu, Chi-Chiu Lee, Han-Zen Tsai, Hsin-Jung Li, and Han-Chung Wu.Development of therapeutic antibodies for the treatment of diseases. Journal of biomedical science, 27(1):130, 2020. 1 Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. Antigen-specific antibodydesign and optimization with diffusion-based generative models for protein structures. Advances in NeuralInformation Processing Systems, 35:97549767, 2022. 1, 12 Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, and Richard Socher.Progen: Language modeling for protein generation.arXiv preprintarXiv:2004.03497, 2020. 12 Emily K Makowski, Patrick C Kinnunen, Jie Huang, Lina Wu, Matthew D Smith, Tiexin Wang, Alec ADesai, Craig N Streu, Yulei Zhang, Jennifer M Zupancic, et al. Co-optimization of therapeutic antibodyaffinity and specificity using machine learning models that generalize to novel mutational space. Naturecommunications, 13(1):3788, 2022. 1 Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Im-proved credit assignment in gflownets. Advances in Neural Information Processing Systems, 35:59555967,2022. 22 Derek M Mason, Simon Friedensohn, Cdric R Weber, Christian Jordi, Bastian Wagner, Simon M Meng,Roy A Ehling, Lucia Bonati, Jan Dahinden, Pablo Gainza, et al. Optimization of therapeutic antibodies bypredicting antigen specificity from antibody sequence via deep learning. Nature Biomedical Engineering,5(6):600612, 2021. 9",
  "Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2: exploring theboundaries of protein language models. Cell Systems, 14(11):968978, 2023. 12": "Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutationinvariant graph generation via score-based generative modeling. In International Conference on ArtificialIntelligence and Statistics, pp. 44744484. PMLR, 2020. 11 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in neural information processing systems, 32, 2019. 21 Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learningin python. the Journal of machine Learning research, 12:28252830, 2011. 19",
  "Herbert E Robbins. An empirical bayes approach to statistics. In Breakthroughs in Statistics: Foundationsand basic theory, pp. 388394. Springer, 1992. 2": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical imagesegmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18thInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241.Springer, 2015. 19 Jeffrey A Ruffolo, Lee-Shin Chu, Sai Pooja Mahajan, and Jeffrey J Gray. Fast, accurate antibody structureprediction from deep learning on massive set of natural antibodies. Nature communications, 14(1):2389,2023. 7 Matthias Sachs, Benedict Leimkuhler, and Vincent Danos. Langevin dynamics with variable coefficients andnonconservative forces: from stationary states to numerical methods. Entropy, 19(12):647, 2017. 4, 5",
  "Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advancesin neural information processing systems, 33:1243812448, 2020. 11": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. In International Conference onLearning Representations, 2021. URL 11 Samuel Stanton, Wesley Maddox, Nate Gruver, Phillip Maffettone, Emily Delaney, Peyton Greenside, andAndrew Gordon Wilson. Accelerating bayesian optimization for biological sequence design with denoisingautoencoders. In International Conference on Machine Learning, pp. 2045920478. PMLR, 2022. 8",
  "Haoran Sun, Lijun Yu, Bo Dai, Dale Schuurmans, and Hanjun Dai. Score-based continuous-time discretediffusion models. In The Eleventh International Conference on Learning Representations, 2023. URL 11": "Natasa Tagasovska, Nathan C. Frey, Andreas Loukas, Isidro Hotzel, Julien Lafrance-Vanasse, Ryan LewisKelly, Yan Wu, Arvind Rajpal, Richard Bonneau, Kyunghyun Cho, Stephen Ra, and Vladimir Gligorijevic.A pareto-optimal compositional energy-based model for sampling and optimization of protein sequences.In NeurIPS 2022 AI for Science: Progress and Promises, 2022. URL 12 Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine. Conservative objective models foreffective offline model-based optimization. In International Conference on Machine Learning, pp. 1035810368. PMLR, 2021. 10",
  "Jin Xu, Zishan Li, Bowen Du, Miaomiao Zhang, and Jing Liu. Reluplex made more practical: Leaky relu.In 2020 IEEE Symposium on Computers and communications (ISCC), pp. 17. IEEE, 2020. 21": "Qi Yan, Zhengyang Liang, Yang Song, Renjie Liao, and Lele Wang. SwinGNN: Rethinking permutationinvariance in diffusion models for graph generation. Transactions on Machine Learning Research, 2024.ISSN 2835-8856. URL 11 Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio.Generative flow networks for discrete probabilistic modeling. In International Conference on MachineLearning, pp. 2641226428. PMLR, 2022. 11",
  "BAdditional details on the static MNIST experiment": "We train the noisy score and discriminator model on the binarized static MNIST dataset.4For scoremodel architecture, we use a U-Net architecture (Ronneberger et al., 2015) that takes the smoothed one-hotrepresentation of the discrete images and returns the score of the same shape. Finally, we train a CNNarchitecture 5 on smoothed data and their corresponding labels for the discriminator model.",
  "C.1Training details": "All our models are trained with maximum 40 epochs with early stopping and with a learning rate of 104,a weight decay of 0.01, and a batch size of 64. We validate our models on a held-out validation set separatefrom the training data. Both the discriminator and the score based models for all experiments are trainedwith mean square error loss, except for CDR H3 experiment, in which the discriminator is trained with abinary cross entropy loss. We save the best models based on the validation loss and use the model duringsampling.",
  "C.2distributional conformity score (DCS)": "The recently proposed DCS measures sample quality using a sample-to-distribution metric, as opposed toprevious sample recovery metrics (Jin et al., 2021). It measures the fraction of the validation sequencesthat are less similar to their IID samples compared to a target sample. We show the algorithm to calculateDCS from Frey et al. (2024) in algorithm 2.We implement DCS using two sequence-based properties:molecular weight and grand average of hydropathicity (GRAVY) (Kyte & Doolittle, 1982). We used thekernel density estimation (KDE) implementation by Scikit-learn (Pedregosa et al., 2011) to approximate thejoint distribution using a Gaussian kernel and a bandwidth of 0.15. DCS is a data-fidelity metric similar toFID for images. Unsurprisingly, DCS and optimized attributes are in tensionoptimization while preservingdata fidelity is harder.",
  "C.3Evaluation metrics": "Paired OAS experiment.We show the optimization of two attributes in this experiment: % beta sheetsand instability index. The fraction of beta-pleated sheets provides insights into the structural characteristicsof antibody sequences, influencing their stability and functionality. The protein stability index is a testproposed by Guruprasad et al. (1990). A protein sequence has a short half-life if its stability index is above40. Stability is a crucial metric in the context of therapeutic antibody generation, as it is closely associatedwith manufacturability.",
  "kki=1[i < k]return py": "CDR H3 experiment.The key attribute for this experiment is pbind, the probability of being a binder,which we obtain from the binary classifier trained on the data. Besides, to measure the diversity of thegenerated sequences, we use mean uniqueness and edit distance Edist. Paired OAS MOO experiment.The metrics used in this experiment are similar to those listed above:% beta sheets and instability index for the optimization metric, edit distance for the diversity metric, andDCS for the data fidelity metric. AMP experiment.Following Jain et al. (2022), we use the evaluation metrics described below. Here, Dis the dataset of generated samples, D0 is the dataset used to train the generative models, and X is the setof all peptides.",
  "D.2Discriminator models": "Paired OAS experiment.We train the discriminator model on two sequence-related properties: thepercentage of beta sheets and the protein instability index. We label the training sequences using BioPy-thon (Cock et al., 2009). For the architecture of the smoothed predictor, we adopt a 3-layer 1D-CNN followedby a 3-layer MLP integrated into the existing ByteNet architecture, incorporating leakyReLU (Xu et al.,2020) activations between the layers. Finally, we follow the same training parameters as the score modeltraining. To use the discriminator models prediction gradients as guidance, we use the torch.autograd onthe models prediction given a label or not (in case of a non-categorical variable). CDR H3 experiment.Here, we have two similarly trained discriminator models for two different pur-poses: the discriminator model on pbind for classification and the noised discriminator model for generationusing gg-dWJS. We adopt a similar approach to the one used in the paired OAS experiment, except for using asigmoid terminal activation and binary cross entropy loss function to accommodate the binary classification. Paired OAS MOO experiment.The chosen attributes for the MOO experiment are instability indexand % beta sheets. For this experiment, we attempt to lower both values as the multi-objective criteria.Therefore, we train a noised discriminator model that outputs a weighted sum of the normalized attributes.To achieve preference conditioning, the setup is similar to the other discriminators, except we concatenatethe preference vectors with the output of the 1D-CNN.",
  "Following Frey et al. (2024), We use GPT-4o as a large language model (LLM) baseline. For the paired OASgeneration experiment, we prompt the following prompt": "You are an expert antibody engineer.I am going to give you examples of antibodyheavy chain and light chain variable regions from the paired observed antibodyspace database.You will generate 10 new antibody heavy chain and light chain thatare not in the database.Output the 10 samples as a python list.Here are theexamples:[(QVQLVQS-GTEVKKPGSSVKVSCKASG-GTFSS-Y . . . DYYCQAWDY-STAVFGTGTKVTVL)]",
  "Similarly, we perform a similar prompt for the CDR H3 design experiment. The prompt is the following:": "You are an expert antibody engineer.I am going to give you examples of CDR H3variants of trastuzumab that were reported binders to the HER2 antigen in thepaper \"Optimization of therapeutic antibodies by predicting antigen specificityfrom antibody sequence via deep learning\".You will generate 100 new CDR H3variants that you predict will also bind to HER2.Output the 100 samples as apython list.Do not post the code.Instead, show the samples directly.Here arethe examples:[WHINGFYVFH, FQDHGMYQHV, YLAFGFYVFL, WLNYHSYLFN, YNRYG-FYVFD, WRKSGFYTFD, WANRSFYAND, WPSCGMFALL, WSNYGMFVFS, WS- MGGFYVFV, WGQLGFYAYA, WPILGLYVFI, WHRNGMYAFD, WPLYSMYVYK, WGLCGLYAYQ,]",
  "D.6Generative flow networks (GFlowNets)": "GFlowNets is a popular baseline for discrete generative modeling using a probabilistic method. However, forthe current implementation of GFlowNets, generating samples in a state space of size 21249 is difficult. There-fore, we reduce the problem of generating the paired sequences to generating the first 50 mutations of the fullsequences with trajectory balance objective (Malkin et al., 2022). The heavy and light chain suffixes addedto the generated mutations are QVQLVQSGTEVKKPGSSVKVSCKASGGTFSSYAVSWVRQAPGQ-GLEWMGRFIPILNIKNYAQDFQGRVTITADKSTTTAYMELINLGPEDTAVYYCARGSLSGREGLP-LEYWGQGTLVSVSSandEVVMTQSPATLSVSPGESATLYCRASQIVTSDLAWYQQIPGQAPRLLI-FAASTRATGIPARFSGSGSETDFTLTISSLQSEDFAIYYCQQYFHWPPTFGQGTKVEIK,respectively.We use the implementation provided in Bengio et al. (2023) for our experiment. We use a 3-layer MLP with",
  "E.1Effect of K": "To understand the effect of the number of sampling steps K, we perform an ablation experiment on Kusing the instability index discriminator model as the gradient guidance. For this experiment, we changeK {10, 20, 30, 40, 50} while keeping = 1. shows the results of the experiment. We can see thatthere is a minimal change in the attribute, data fidelity, and diversity. In a close inspection, we see that asK increases, sampled sequences are more optimized, yet less diverse. Notably, DCS continues to improveuntil K = 40, after which it decreases. : Ablation results on number of samples steps K for antibody sequence optimization task. We seea generally indifferent yet a slightly optimized attribute in exchange of a reduced diversity as K increases.",
  "E.2Statistical analysis on preference conditional generation": "To determine the statistical significance of the samples generated with different preferences using gg-dWJSfrom figure 5, we run a Friedman test (Friedman, 1937; 1940).7In this case, we utilize three distinctpreferences to represent three different treatments. shows the result of the test. The table containsthree main attributes: the proportion of beta sheets and the instability index, each analyzed using differentweight combinations. The Friedman test yields low p-values (p < 0.001), indicating statistically significantdifferences between weight arrangements. For example, sequences optimized with w1=1 and w2=0 have abeta sheet percentage of 0.44 0.01, whereas those with w1=0 and w2=1 have a greater instability indexof 61.84 4.23.These results demonstrate our methods capacity to build sequences based on certainoptimization preferences.",
  "% Beta sheet1564<0.0010.44 0.010.39 0.010.43 0.02Instability index1522<0.00136.11 2.4561.84 4.2341.90 8.54": "weight to 23000 and use e(molWt23000)2/23000 as our objective which is maximized at 23000. showsthe results of our experiment. We can see that with the objectives are satisfied with their correspondingpreferences. For example, with w = the instability index is minimized while with w = ,the molecular weight is closer to the target weight.",
  "FDiscussion": "Why does discriminator gradient guidance produce higher quality samples? We speculate that the guidanceassists the denoising walk by reducing the discrete space. Moreover, by collaborating with the denoisingscore, it strengthens the actions taken in relation to a particular label, thus enhancing the accuracy of thegenerated data. There are many works, such as Dhariwal & Nichol (2021); Kawar et al. (2022), that reportthe same phenomenon. We leave the experimental validation of this underlying effect for future work."
}