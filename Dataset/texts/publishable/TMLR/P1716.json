{
  "Abstract": "Membership inference attacks are used as a key tool for disclosure auditing. They aim to inferwhether an individual record was used to train a model. While such evaluations are usefulto demonstrate risk, they are computationally expensive and often make strong assumptionsabout potential adversaries access to models and training environments, and thus do notprovide tight bounds on leakage from potential attacks. We show how prior claims aroundblack-box access being sufficient for optimal membership inference do not hold for stochasticgradient descent, and that optimal membership inference indeed requires white-box access.Our theoretical results lead to a new white-box inference attack, IHA (Inverse HessianAttack), that explicitly uses model parameters by taking advantage of computing inverse-Hessian vector products. Our results show that both auditors and adversaries may be ableto benefit from access to model parameters, and we advocate for further research into white-box methods for membership inference.",
  "Introduction": "Models produced by using machine learning on private training data can leak sensitive information aboutdata used to train or tune the model (Salem et al., 2023). Researchers study these privacy risks by eitherdesigning and evaluating attacks to simulate what motivated adversaries may be able to infer in particularsettings or by developing privacy methods that provide strong guarantees, often based on some notion ofdifferential privacy (Dwork et al., 2006), that bounds information disclosure from any attack. Although bothdeveloping attacks and formal privacy proofs are important, conducting meaningful privacy audits is differentfrom both approaches. Empirical methods, usually in the form of attack simulations, are inherently limitedby the attacks considered and the uncertainty about the possibility of better attacks, while theoretical proofsrequire many assumptions or result in loose bounds. Further, any claims based on theoretical results dependon careful analysis that the system as implemented is consistent with the theory. If there is a theoreticalresult that prescribes an optimal attack, then empirical results with that attack (or approximations of theattack) can offer a more meaningful estimate of privacy risk than is possible with theory or experimentsalone. While the theory needs to cover all data distributions, experiments with an optimal attack focus onthe actual distribution and given model, resulting in tighter and more relevant privacy evaluations. Privacy audits can also be important in adversarial contexts, where a regulator or external advocate conductsthem to test a released model. Auditors with elevated model access (such as associated training environmentsor data) may be able to take advantage of more information to produce better estimates of what an adversarycould do without that information.Auditing is orthogonal to proofs that establish differential privacy",
  "Published in Transactions on Machine Learning Research (12/2024)": "make the Hessian ill-conditioned and thus cannot be inverted directly. We explore two different techniquesto mitigate this: damping by adding a small constant to all the eigenvalues or a low-rank approximationwhere only eigenvalues (and corresponding eigenvectors) above a certain threshold are used as a low-rankapproximation. We ablate over these two techniques for some candidate values of . Our results ()suggest that damping with = 2e1 works best across all the datasets we test, which is the setting for whichwe report our main results.",
  "Membership Inference": "Following the framework established by Sablayrolles et al. (2019), let D be a data distribution from whichn records z1, z2, . . . , zn are i.i.d. sampled with zi = (xi, yi) being the i-th record. Let w Rd be the modelparameters produced by some machine learning algorithm on a training dataset D. Assume m1, m2, . . . , mnfollow a Bernoulli distribution with = P(mi = 1), where mi is the membership indicator of zi (i.e., mi = 1if zi D, and mi = 0 otherwise). Given w, a membership inference attack aims to predict the unknownmembership mi for any given record zi.",
  "where (u) = (1 + exp(u))1 is the Sigmoid function, and = P(m1 = 1)": "To use Lemma 2.1, one needs to characterize the posterior, P(w | z1, . . . , zn, m1, . . . , mn), to make explicitthe effect of the inferred record z1 on the optimal membership inference M(w, z1). Recent advances indiscrete-time SGD dynamics (Liu et al., 2021; Ziyin et al., 2021) literature can help provide a connectionbetween the posterior and model parameters.",
  "Discrete-time SGD Dynamics": "A line of theoretical work (Welling & Teh, 2011; Sato & Nakagawa, 2014; Stephan et al., 2017; Liu et al., 2021;Ziyin et al., 2021) has analyzed the continuous- and discrete-time dynamics of stochastic gradient methodsand provided insights for understanding deep learning generalization. Let Ltot(w) = L(w) + 2 w22 be the2-regularized total loss that we aim to optimize, where 0 denotes the hyperparameter that controls theregularization strength. Consider an SGD algorithm with the following update rule (for t = 1, 2, 3, . . .):",
  "zD (w, z)": "Assuming a model is trained using SGD according to the update rule defined by Equation 2 on a quadraticloss and arrives at a stationary state, Liu et al. (2021) established a theoretical connection between theHessian matrix H, the asymptotic noise covariance C = limt Ewt[cov(t, t)], and the asymptotic modelfluctuation = limt cov(wt, wt). We next lay out the two imposed assumptions.Assumption 1 (Quadratic Loss). The total loss function Ltot(w) is either globally quadratic or locallyquadratic close to a local minimum w. Specifically, the loss function can be approximated as:",
  "(w w)(H(w) + Id)(w w) + o(w w22),(3)": "where w is a local minimum, H(w) denotes the Hessian matrix at w with respect to the unregularizedloss function L(w), and Id stands for the d d identity matrix.Assumption 2 (Stationary-State). After a sufficient number of iterations, models trained with SGD definedby Equation 2 arrive at a stationary state, i.e., the asymptotic model fluctuation exists and is finite. Under the above assumptions, Liu et al. (2021) proved the following theorem that describes model fluctuationsof discrete SGD in a quadratic potential with connections to the Hessian matrix and the noise covariance:Theorem 2.2 (SGD Stationary distribution with momentum). Let w be updated with SGD defined bythe update rule in Equation 2 with momentum [0, 1). Under Assumptions 1 and 2, if we additionallysuppose C commutes with H(w), then the asymptotic model fluctuation satisfies",
  "provided that is proportional to S1 and |L(w) (w, zi)| is small (i.e., of order o(L(w))": "The first imposed assumption of = O(S1) has been justified by prior works (Liu et al., 2021; Xieet al., 2021; Mori et al., 2022); the second assumption assumes that the current total training loss L(w)approximates well the individual loss for each record (w, zi). Also, note that Theorem 2.3 directly impliesthat the SGD noise covariance C commutes with the Hessian matrix H(w).",
  "+ H(w) + Id1.(4)": "We remark that if L(w) = 0 (i.e., w is a global minimum), then = 0. In addition, if the Hessian matrix(H(w) + Id) has degenerate rank r < d, then (H(w) + Id)1 can be replaced by the correspondingMoore-Penrose pseudo inverse. Accordingly, similar results to Equation 4 can be obtained by considering theprojection space spanned by eigenvectors with non-zero eigenvalues. of Ziyin et al. (2021) providesmore detailed discussions of the imposed assumptions and the implications of the results.",
  "i=1mi (w, zi),(5)": "where T is a temperature parameter that captures the stochasticity of the learning algorithm. This as-sumption makes subsequent derivations of optimal membership inference much easier, but oversimplifies thetraining dynamics of typical machine learning algorithms such as SGD. Equation 5 assumes that the posteriorof w follows a Boltzmann distribution that only depends on the training loss. This is desirable for Bayesianposterior inference, where the goal is to provide a sampling strategy for an unknown data distribution givena set of observed data samples. This can be achieved using stochastic gradient Langevin dynamics (SGLD)(Welling & Teh, 2011) with shrinking step size t (i.e., limt t = 0) and by injecting carefully-designedGaussian noise N(0, t ID). However, this special SGLD design differs from the common practice of SGDalgorithms used to train neural networks in two key ways:",
  "where L = L(w), H = H(w) and i(H) denotes the i-th largest eigenvalue of H": "A proof for Theorem 3.1 is given in Appendix B. Theorem 3.1 suggests that the posterior distribution ofmodel parameters learned by SGD not only relies on the training loss L(w) but is also crucially dependenton other terms, such as the Hessian structure H, the gradient L(w) and the 2 distance w w2,confirming that Equation 5 is insufficient to model the dynamics of a discrete-time SGD algorithm.",
  "Optimal Membership Inference under Discrete-time SGD": "We have explained why the assumption imposed by Sablayrolles et al. (2019) about the posterior distributionof w following a Boltzmann distribution (Equation 5) does not hold for stochastic gradient methods typicallyemployed in practice. Next, we prove a theorem that gives an estimate of the optimal membership inferencescoring function by leveraging recent results on discrete-time SGD dynamics (Liu et al., 2021; Ziyin et al.,2021). Our derivation is based on the assumptions that the loss achieved at the local minimum is unaffectedby removing a single training record and that the Hessian structure remains unchanged.",
  "where w0 is the local minimum that SGD with L0 is converging towards, and H0 denotes the Hessian matrixwith respect to L0 (and likewise for w1, L1, and H1)": "As long as the size of the training dataset is sufficient and the excluded training record z1 is not a low-probability outlier from the data distribution D, we expect Assumption 3 generally holds for SGD algorithms.Under Assumption 3 and a few other assumptions imposed in prior literature on discrete-time SGD dynamics(Liu et al., 2021; Ziyin et al., 2021), we obtain a theorem (proof is in Appendix C) that describes the scoringfunction for an optimal membership-inference adversary:",
  "H1 L0(w)H1H1 (w, z1)": "Here, L and H are defined in Assumption 3, and are dependent on T .Here, T refers to the set ofboth member and non-member records along with their corresponding membership indicators, as defined inLemma 2.1. Note that computing the optimal score requires access to the Hessian and model gradients, bothof which require access to the model parameters. In fact, knowledge of the learning rate , momentum ,and regularization parameter are also required, thus requiring complete knowledge of the training setupof the target model. Thus, black-box access is not sufficient for optimal membership inference. The first two additional terms I1 and I2 can be interpreted as the magnitude and direction, respectively, of aNewtonian step for the given record z1. The first term I1 characterizes the influence magnitude in 2-normof upweighting z1 on the model parameters close to the local minimum (Koh & Liang, 2017), while thesecond term captures the alignment between the influence of z1 and the averaged influence of the remainingtraining data. A larger influence magnitude of z1 or an increased influence alignment suggests a higher risk ofmembership inference. We remark that the notion of a self-influence function introduced in Cohen & Giryes(2024) naturally relates to I1, suggesting a similar insight to ours that better membership inference attackscan be designed by leveraging the influence function of the inferred record. The last two additional terms, I3and I4, originate from the extra L2 regularization term imposed on the training loss of SGD (.2).When the regularization parameter is a very small positive constant, the effects of I3 and I4 on optimalmembership inference will be negligible, particularly compared to those of I1 and I2.",
  "Inverse Hessian Attack": "While Theorem 3.2 directly prescribes an optimal membership inference adversary, computing the expecta-tion over all possible models trained using the rest of the training data is infeasible. Our definition of optimalmembership inference corresponds to the true leakage of the model (as defined in .2 of Ye et al.(2022)). It utilizes worst-case adversary knowledge (membership of all other training records) and white-boxaccess to estimate the influence of the target record, similar to how empirical attacks such as LiRA (Carliniet al., 2022) and RMIA (Zarifzadeh et al., 2023) use reference models to account for atypical examples.",
  "I1 + I2 + I3 + I4.(8)": "The score, IHA(z1), for some given record z1, can be used as the probability of z1 being a member. This servesdirectly as a useful attack for privacy auditing, without needing to train any reference models. Not having totrain reference models offers significant advantages. It helps auditors avoid additional computational costsand, more importantly, eliminates the need for trainers to reserve hold-out data for reference model training.",
  "Experiments": "To evaluate IHA, we efficiently pre-compute L1(w) to facilitate the computation of L0(w) for any giventarget record z1. For accurate Hessian matrix computations, we address the issue of ill-conditioning dueto near-zero and small negative eigenvalues by either damping or using low-rank approximations (damping-based conditioning seems to perform best; see Appendix E for details). To support larger models wheredirect Hessian computation is infeasible, we extend our method to use approximation methods based onConjugate Gradients for iHVP computation (Koh & Liang, 2017). Our implementation for reproducing allthe experiments is available as open-source code at .1 describes the baseline attacks, datasets, and models we use for our experiments. .2summarizes our results, showing that IHA provides a robust privacy auditing baseline, matching or exceedingthe performance of current state-of-the-art attacks including attacks that use reference models.This isnotable since IHA does ont require training any reference models or the use of hold-out data. For a given false positive rate (FPR), a threshold is computed using scores for non-members, which is thenused to compute the corresponding true positive rate (TPR). This is repeated for multiple FPRs to generatethe corresponding ROC curve, which is used to compute the AUC. This experimental design is commonlyused for membership-inference evaluations (Yeom et al., 2018; Carlini et al., 2022; Ye et al., 2022).",
  "To evaluate IHA, we compare its performance to state-of-the-art baseline attacks with a representative setof datasets and models": "Baseline Attacks. We include LOSS as a baseline that does not use reference models, SIF as it uses iHVPsimilar to our audit, and LiRA as it is the current state-of-the-art for membership inference. While RMIA(Zarifzadeh et al., 2023) uses fewer reference models, it achieves performance comparable to LiRA and thusfor the sake of performance comparison, it suffices to use LiRA with a large number of reference models. Wedescribe the underlying access assumptions for these attacks in .",
  "LOSS (Yeom et al., 2018). The negative loss is used in this attack as a direct signal for membership inference": "SIF (Cohen & Giryes, 2024). Similar to ours, this attack employs the loss curvature of the target modelby computing its Hessian, which is then used to compute a self-influence score. The original attack assigns01 scores to target records. It classifies a given record as a member if its self-influence score is within thespecified range and if its predicted class is correct. The latter rule can be ruled out as having many false",
  ": Comparison of attacks based on level of model-access, use of reference models, and knowledge ofother training members": "positives/negatives. Instead of these steps, we choose to use the self-influence as membership scores directly.While the authors used approximation methods for iHVP, we use the exact Hessian for fair comparison. LiRA (Carlini et al., 2022). There are two variants, LiRA-Offline, which uses offline models to estimatea Gaussian distribution and then performs one-sided hypothesis testing using loss scores, and LiRA-Online,with additionally employs online models, i.e., models whose training data includes the target record. Thelikelihood ratio for online/offline model score distributions is then used as the score for membership inference.We use LiRA-Online, since it is the strongest of the two variants. L-Attack (Ye et al., 2022). The L-attack operates in a leave-one-out setting, training reference models onD \\ {z} for any given record z. It uses loss as the target metric and computes attack thresholds for a desiredfalse positive rate (FPR) by leveraging the distribution of losses obtained from reference models. LiRA-L. We propose combining the LiRA attack for the LOO-setting by utilizing reference models trainedunder leave-one-out availability, followed by the offline variant of the LiRA attack (Carlini et al., 2022).",
  "Datasets.Since we are limited by the computational constraints of computing iHVPs, we restrict ourexperiments to datasets where small models can perform adequately": "Purchase-100(S). The task for this dataset (Shokri et al., 2017) is to classify a given purchase into one of100 categories, given 600 features. We train 2-layer MLPs (32 hidden neurons) with cross-entropy loss, withan average test accuracy of 84%. Experiments by Zarifzadeh et al. (2023) train larger (4-layer MLP) modelson 25 K samples from Purchase-100, which is much smaller than the actual dataset, which is why we termit Purchase-100(S) (Small).We also demonstrate results with a 4-layer MLP that achieves similar taskaccuracy. Purchase-100. For this version, we train models with 80 K samples. We use the same 2-layer MLP archi-tecture as Purchase-100(S) but achieve a higher test accuracy of 90%. Using more data increases the scopefor model performance. We report results for Purchase-100 in as the corresponding models are lessprone to overfitting. For completeness, we report results for Purchase-100(S) in Appendix D. MNIST-Odd. We consider the MNIST dataset (LeCun et al., 1998), with the modified task of classifying agiven digit image as odd or even. This modified task allows us to train models for binary classification usingthe regression loss, and is thus highly likely to follow the assumptions made in our theory regarding quadraticbehavior for the loss function (Assumption 1). We train a logistic regression model with mean-squared errorloss, with an average test loss of .078. FashionMNIST. We use the FashionMNIST (Xiao et al., 2017) dataset, where the task is to classify a givenclothing item image into one of ten categories. We train 2-layer MLPs (6 hidden neurons) with cross-entropyloss, with an average test accuracy of 83%.",
  "IHA (Exact).703 .00413.697.52.542 .0042.610.51.594 .0184.060.89": "Models. We train 128 models in the same way as done in Carlini et al. (2022), where data from each modelis sampled at random from the actual dataset with a 50% probability. For each target model and targetrecord, there are thus 127 reference models available, half of which are expected to include the target recordin the training data. All of our models are trained with momentum ( = 0.9) and regularization ( = 5e4),with a learning rate = 0.01. For a given false positive rate (FPR), a threshold is computed using scoresfor non-members, which is then used to compute the corresponding true positive rate (TPR). This is thenrepeated for multiple FPRs to generate the corresponding ROC curve, which is used to compute the AUC.This experimental design is commonly used for membership-inference evaluations (Yeom et al., 2018; Carliniet al., 2022; Ye et al., 2022).",
  "Results": "As summarized in , IHA provides a strong privacy auditing baseline that is competitive with currentstate-of-the-art attacks that require reference models. This is especially useful, considering that IHA doesnot require training any reference models and thus, does not require any hold-out data to train such referencemodels. IHA performs much better than the baselines on tabular data (Purchase-100), and is competitivewith the baseline for image-based data (MNIST-Odd, Fashion MNIST). By extension, our method alsooutperforms previous membership inference attacks that specifically utilize parameter access via white-boxaccess (Nasr et al., 2018; Cohen & Giryes, 2024), since such methods are outperformed by LiRA (Carliniet al., 2022). While tabular data is a more realistic setting for membership inference and the improvedperformance on Purchase-100 is promising, we leave to future work further investigation of these factors tobetter understand the performance discrepancies. Approximating iHVPs. In order to carry out IHA, an auditor needs to be able to calculate iHVPs andgradients for all training data. While computing gradients is more computationally intensive than simplycalculating the loss, the difference is minimal. On the other hand, computing an iHVP involves calculatingthe Hessian matrix and then inverting it, both of which are computationally expensive processes. Evenstoring such an inverted Hessian can be problematic (p p matrix for a model with p parameters). We thusexperiment with evaluating IHA using Conjugate Gradients (Koh & Liang, 2017) to approximate iHVP.While such approximation does not require computing the Hessian directly, the time taken to compute thisterm for each record is non-trivial. We thus evaluate this approximation-based method on a random sampleof 10 000 records1 and find that approximation methods retain most of the attacks performance (). We emphasize that the purpose of our comparisons is not to claim a better membership inference attackfor adversarial use; the threat models are not comparable, since our attack requires knowledge of all otherrecords D \\ {z1} for inferring a given target record z1 (relaxing this assumption leads to severe performancedegradation, see Appendix F). Instead, IHA provides a way to empirically audit models for membershipleakage without training reference models, which is desirable in avoiding the need to reserve hold-out datafor training reference models. More importantly, our results suggest untapped opportunities in exploring",
  "Ablating over terms inside IHA": "As described in Equation 8, calculating IHA requires computing the loss along with the four additional termsI1, I2, I3 and I4. However, the terms I3 and I4 are scaled by (which is usually very small) and involve aniHVP of an iHVP and thus may be much smaller compared to terms like I1, I2 and the loss. We explorevariants of IHA, which ignore the terms I3 and I4, to see how they impact auditing performance. We alsoconsider variants that use only the terms I1 and I2 to understand the importance of their contributions to theperformance of IHA, along with the inclusion or not of the loss term to understand the relative importanceof parameter-based signals. The ablation study presented in reveals several key insights about the performance of IHA. ExcludingI3 and I4 has negligible impact on auditing performance, even for low-FPR scenarios. The combination of I1and I2 alone achieves an AUC of .779, and performance is identical when the terms I3 and I4 are also included.Notably, the addition of the loss term (w, z1) to I1 and I2 results in a marginal improvement, increasingthe AUC to .791 and slightly boosting the TPR at both 1% and 0.1% FPR. Interestingly, when examinedindividually, I2 (AUC .704) performs significantly better than I1 (AUC .591), suggesting that I2 capturesmore relevant information for the auditing task. Including the loss term (w, z1) has little impact on I1 butharms performance when used wth I2. These findings indicate that the majority of the attacks effectivenessstems from the inverse Hessian vector products used in I1 and I2, with I2 being particularly important, whilethe terms involving weight regularization and nested iHVPs (I3 and I4) contribute minimally to the overallperformance. While not as impactful as I2, the loss term still provides valuable information for the auditingprocess. Based on these results, a simplified version of IHA using only (w, z1), I1, and I2 could potentiallyoffer a favorable trade-off between computational efficiency and auditing effectiveness.",
  "Comparison with Leave-One-Out Setting": "When targeting a record for inference, IHA assumes knowledge of all the other n 1 records in an n-sizeddataset. It is possible that the improved performance of IHA is due to this extra information rather thaninherent parameter access. To isolate and analyze these potential sources of increase in leakage, we alsoassess the performance of a leave-one-out (LOO) membership inference test. We evaluate the L-attack (Yeet al., 2022) on 1000 samples, training 100 reference models per record for score calibration. Since targetingeach record requires training multiple reference models for the L-attack, evaluating it on a larger sample ofdata is computationally infeasible. These results indicate that IHA outperforms the L-attack, achieving an AUC value of .791 compared to.737 for the L-attack.2 This suggests that even when controlling for the additional knowledge of all otherrecords in the dataset, the primary source of IHAs superior performance stems from its access to modelparameters rather than just the leave-one-out setup. In a way, IHA uses parameter access to obviate theneed for reference models, as it directly aims to measure the influence of the given target record instead ofrelying on reference models for score calibration. Interestingly, in comparison, LiRA achieves an AUC of .767, which is lower than IHA but still higher thanthe L-attack. This suggests that LiRA, even without the extensive reference model training, is more effectivethan the L-attack, possibly due to its utilization of both in and out models as opposed to just outmodels with the L-attack. To try and devise a stronger black-box attack for the LOO setting, we extendLiRA to the LOO setting by using models trained on LOO data as reference models. LiRA-Offline underthe LOO setting achieves an AUC of .633, lower than the L-attack (.737). Overall, these results demonstratethe promise of IHA as a privacy auditing tool. It yields results comparable to that of techniques that trainhundreds of reference models (for each target record in the worst case, as in L-attack), without using a singlereference model.",
  "Inter-Attack Agreement": "Similar to Ye et al. (2022), we compute the agreement rate between ground-truth membership labels andmembership predicted by various attacks to understand the ability of our privacy audit to identify vulnerabledata, and demonstrate how it differs from existing attacks. presents the agreement rate betweenground-truth membership labels and membership predicted by various attacks. At a low FPR of 0.05, agreement in predictions for non-members is very high between attacks, withagreement rates above 0.91 for all pairs of attacks. On the other hand, agreement rates for member recordsare expectedly lower. Interestingly, agreement between LiRA and LiRA-L is higher than IHA and any otherattack. This difference is especially pronounced for a higher FPR (b), where agreement rates are as 2Our results for the L-attack are lower than those reported by Ye et al. (2022). For instance, we observe a TPR of .668 at 0.3FPR, while it was reported to be .968 by (Ye et al., 2022). This discrepancy arises from our setting, which uses more data andfewer model parameters. We verified our implementation through direct correspondence with the authors and by replicatingtheir results in the original setting, which used a smaller dataset and more parameters, resulting in a model prone to overfitting.",
  ": Runtime and memory statistics for various auditing techniques. Statistics are computed over 100randomly-selected members for MLP-2 architecture models trained on Purchase-100 dataset": "low as 0.4 compared to 0.766 for LiRA and LiRA-L. This is very interesting becaue the correspondingTPRs for IHA are higher than LiRA and comparable to that of the L-attack, thus suggesting that the recordsidentified by IHA as being vulnerable are very different from those identified by LiRA or even the L-attack.This also means that a combined (classifying a record as a member only when both attacks classify as amember) attack would have some true positives with a very low FPR.",
  "Runtime Comparison": "To compare the computational costs of our proposed audit with existing auditing techniques, we analyzeruntime and memory usage statistics across different methods, aiming to evaluate efficiency and practicalityin real-world privacy audits (). While the peak memory consumption of IHA is higher than that of LiRA, the approximate version of IHAis not too far from LiRA in terms of memory consumption. Computing the total runtime is a function ofthe number of samples used for the privacy audit, as a precompute step is required for LiRA (trainingreference models) and exact IHA (computing Hessian). For instance, computing the audit for 1K samplestakes less overall time for IHA ( 1 hour) than it does for LiRA ( 3 hours). It should be noted thatthe Hessian is too large to store on our GPU for IHA and is thus stored on the CPU, which is also whyit is slower. Although improvements may reduce the compute costs, the key advantage of such a privacyaudit comes from not having to reserve hold-out (or auxiliary) data. Our privacy audit, like the most trivialLOSS attack, only requires member data and some non-member data, whereas other attacks in the literaturerequire shadow/reference models trained on comparable-sized datasets; the limiting factor here is reservingdata to train reference models, which a real-world model trainer may not want to do since it reduces theamount of data available for training.",
  "Conclusion": "Our theoretical result proves that model parameter access is indeed necessary for optimal membershipinference, contrary to previous results derived under unrealistic assumptions and the common belief thatoptimal membership inference can be achieved with only black-box model access. We propose the InverseHessian Attack inspired by this theory that provides stronger privacy auditing than existing black-boxtechniques. Limitations. IHA is not yet practically realizable for most settings due to the computational expense ofcalculating the Hessian, or even approximating iHVPs. This restriction poses challenges for real-world caseswhere fixed compute budgets may be more crucial than the availability of auxiliary data. An auditor mightuse a subset of parameters to reduce computational costs while performing Hessian-based computations. Thisaligns with model pruning (Liu et al., 2019), but understanding its impact on membership knowledge withinparameters is non-trivial (Yuan & Zhang, 2022). We also note that IHAs performance can be sensitive tothe choice of the damping factor, which requires further investigation (Appendix D.1). Our conclusion aligns well with recent calls in the literature to consider white-box access for rigorous auditing(Casper et al., 2024). While our theory shows that parameter access is required for optimal membershipinference, it remains unclear how much better this is compared to the optimal membership inference attack",
  "Broader Impact Statement": "The increasing integration of AI in sensitive domains like healthcare, finance, and personal data manage-ment highlights the critical importance of privacy. Information leakage from AI models can have severeconsequences, making effective privacy auditing a necessary safeguard. Our work contributes to this field bytheoretically demonstrating that optimal membership inference attacks require white-box access to modelparameters, challenging the adequacy of black-box approaches. We also demonstrate with Inverse HessianAttack how this theory can be used to design empirical privacy audits that do not rely on reference models. We advocate for the development of more sophisticated privacy auditing tools that fully leverage the elevatedaccess typically available to auditors, such as model parameters, to assess privacy leakage efficiently withoutextensive data and compute resources. We hope our theoretical and empirical results will reinvigorate interestin the privacy research community to explore white-box attacks, for both adversarial and auditing purposes.",
  "Achraf Azize and Debabrota Basu.How much does each datapoint leak your privacy?quantifying theper-datum membership leakage. arXiv:2402.10065, 2024": "Martin Bertran, Shuai Tang, Aaron Roth, Michael Kearns, Jamie H Morgenstern, and Steven Z Wu. Scalablemembership inference attacks via quantile regression. Advances in Neural Information Processing Systems,2024. Stella Biderman, USVSN Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, ShivanshuPurohit, and Edward Raff. Emergent and predictable memorization in large language models. Advancesin Neural Information Processing Systems, 2024.",
  "Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membershipinference attacks from first principles. In IEEE Symposium on Security and Privacy, 2022": "Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Benjamin Bucknall,Andreas Haupt, Kevin Wei, Jrmy Scheurer, Marius Hobbhahn, et al. Black-box access is insufficient forrigorous ai audits. In ACM Conference on Fairness, Accountability, and Transparency, 2024. Harsh Chaudhari, Giorgio Severi, Alina Oprea, and Jonathan Ullman. Chameleon: Increasing label-onlymembership leakage with adaptive poisoning. In International Conference on Learning Representations,2024.",
  "Gilad Cohen and Raja Giryes. Membership inference attack using self influence functions. In Proceedings ofthe IEEE/CVF Winter Conference on Applications of Computer Vision, 2024": "Rachel Cummings, Damien Desfontaines, David Evans, Roxana Geambasu, Yangsibo Huang, MatthewJagielski, Peter Kairouz, Gautam Kamath, Sewoong Oh, Olga Ohrimenko, Nicolas Papernot, Ryan Rogers,Milan Shen, Shuang Song, Weijie Su, Andreas Terzis, Abhradeep Thakurta, Sergei Vassilvitskii, Yu-XiangWang, Li Xiong, Sergey Yekhanin, Da Yu, Huanyu Zhang, and Wanrong Zhang. Advancing differentialprivacy: Where we are now and future directions for real-world deployment. Harvard Data Science Review,2024. Daniel DeAlcala, Aythami Morales, Gonzalo Mancera, Julian Fierrez, Ruben Tolosana, and Javier Ortega-Garcia. Is my Data in your AI Model? Membership Inference Test with Application to Face Images.arXiv:2402.09225, 2024.",
  "Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning. In IEEESymposium on Security and Privacy, 2018": "Elre Talea Oldewage, Ross M Clarke, and Jos Miguel Hernndez-Lobato. Series of hessian-vector productsfor tractable saddle-free newton optimisation of neural networks.Transactions on Machine LearningResearch, 2024. Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier, and Herv Jgou. White-box vsblack-box: Bayes optimal strategies for membership inference. In International Conference on MachineLearning, 2019. Ahmed Salem, Giovanni Cherubin, David Evans, Boris Kpf, Andrew Paverd, Anshuman Suri, Shruti Tople,and Santiago Zanella-Bguelin. SoK: Let the privacy games begin! A unified treatment of data inferenceprivacy in machine learning. In IEEE Symposium on Security and Privacy, 2023.",
  "Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-MNIST: a novel image dataset for benchmarkingmachine learning algorithms, August 2017. arXiv: cs.LG/1708.07747": "Zeke Xie, Issei Sato, and Masashi Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gra-dient descent exponentially favors flat minima. In International Conference on Learning Representations,2021. Jiayuan Ye, Aadyaa Maddi, Sasi Kumar Murakonda, Vincent Bindschaedler, and Reza Shokri. Enhancedmembership inference attacks against machine learning models. In ACM Conference on Computer andCommunications Security, 2022.",
  "A.1Membership Inference": "Black-box Membership Inference. Early works on membership inference worked under black-box access,utilizing the models loss (Shokri et al., 2017) on a given datapoint as a signal for membership. Since thenthere have been several works focusing on different forms of difficulty calibrationaccounting for the inherentdifficulty of predicting on a record, irrespective of its presence in train data. This calibration has takenseveral forms; direct score normalization with reference models (Sablayrolles et al., 2019), likelihood testsbased on score distributions (Carlini et al., 2022; Zarifzadeh et al., 2023; Ye et al., 2022), and additionalmodels for predicting difficulty (Bertran et al., 2024). White-box Membership Inference.Nasr et al. (2018) explored white-box access to devise a meta-classifier-based attack that additionally extracts intermediate model activations and gradients to increaseleakage but concluded that layers closer to the models output are more informative for membership inferenceand report performance not significantly better than a black-box loss-based attack. Recent work by DeAlcalaet al. (2024), however, makes the opposite observation, with layers closer to the models input providing",
  "A.2Privacy Auditing": "Ye et al. (2024) proposed using efficient methods to predict memorization by not having to run computa-tionally expensive membership inference attacks, with reported speedups of up to 140x. They showed howtheir proposed score (LOOD) correlates well with AUC, corresponding to an extremely strong MIA withall-but-one access to records (L-attack (Ye et al., 2022)). However, it is unclear if this computed LOOD isdirectly comparable across models, making it hard to calibrate these scores to compare the leakage from amodel relative to another (an important aspect of internal privacy auditing). Their derivations also involvea connection with the Hessian. Biderman et al. (2024) studied the problem of forecasting memorization ina model for specific training data and proposed using partially trained versions of the model (or smallermodels) as a proxy for their computation. While their results support the need for inexpensive auditingmethods, their focus is on predicting memorization early in the training process, while ours relates to audit-ing fully trained models. More recently, Tan et al. (2022) studied the theory behind worst-case membershipleakage for the case of linear regression on Gaussian data and derived insights. While this is useful to makean intuitive connection with overfitting, it does not provide a realizable attack or insights for the standardcase of models trained with SGD.",
  "A.3SGD Dynamics and iHVPs": "SGD Dynamics. Stephan et al. (2017) approximated the SGD dynamics as an Ornstein-Uhlenbeck process,while Yokoi & Sato (2019) provided a discrete-time weak-order approximation for SGD based on It processand finite moment assumption. However, both works rely on strong assumptions about the gradient noisesand require a vanishingly small learning rate, largely deviating from the common practice of SGD. To addressthe limitations of the aforementioned works, Liu et al. (2021) directly analyzed the discrete-time dynamicsof SGD and derived the analytic form of the asymptotic model fluctuation with respect to the asymptoticgradient noise covariance and the Hessian matrix. Ziyin et al. (2021) further generalized the results of (Liuet al., 2021) by deriving the exact minibatch noise covariance for discrete-time SGD, which is shown to varyacross different kinds of local minima. Our work builds on these advanced theoretical results of discrete-timeSGD dynamics but aims to enhance the understanding of optimal membership inference, particularly formodels trained with SGD. iHVPs. Currently literature on approximating inverse-Hessian vector products relies on one of two methods:conjugate gradients (Koh & Liang, 2017) or LiSSA (Agarwal et al., 2017). Both approximation methods relyon efficient computation of exact Hessian-vector products, and use forward and backward propagation assub-routines. While these methods have utility in certain areas, such as influence functions (Koh & Liang,2017) and optimization (Oldewage et al., 2024), approximation errors can be non-trivial.For instance,I1 in the formulation of our attack requires a low approximation error in the norm of an iHVP, while I2simultaneously requires a low approximation error in the direction of the iHVP. Recent work on curvature-aware minimization by Oldewage et al. (2024) proposes another method for efficient iHVP approximation asa subroutine, but the authors observed high approximation errors based on both norm and direction.",
  "+ H + Id1,": "where Pr = diag(1, ..., 1, 0, ...0) denotes the projection matrix onto non-zero eigenvalues, and + is the Moore-Penrose inverse operator. If L = 0, meaning w is a global minimum, then the asymptotic model fluctuation = 0. For ease of presentation, we assume the Hessian matrix has full rank in the following proof.",
  "L+ o(w w22).(15)": "Omitting the constant and negligible terms, we thus complete the proof of Theorem 3.1. Note that we keepthe O(2) term in Equation 13 and Theorem 3.1 for the sake of completeness but expect it to be negligiblecompared with other terms, due to the fact that is typically set as a very small constant within [0, 1).",
  "C.1Proof for Theorem 3.2": "Proof. To derive the scoring function for an optimal membership inference, we need to compute the ratiobetween p(w|w1) and p(w|w0), where w0 (resp. w1) denotes a local minimum (close to w) of the trainingloss function with respect to {z2, . . . , zn} (resp.{z1, . . . , zn}).Note that weve obtained the posteriordistribution of w in Theorem 3.1. Therefore, the remaining task is to analyze the following terms:",
  ",(16)": "where H0(w0) (resp. H1(w1)) denotes the Hessian of L0 (resp. L1) at w0 (resp. w1). Since both w0 andw1 are close to parameters of the observed victim model w, so we can approximate the corresponding lossusing second-order Taylor expansion. Also, according to Assumption 3, we know H0(w0) = H1(w1) = H",
  "C.2Connection with LOSS attack": "Note that while there are additional terms in our optimal membership-inference score, there is anothercritical difference: the loss function has its sign flipped when compared to existing results (Yeom et al., 2018;Sablayrolles et al., 2019). While this may seem counter-intuitive at first glance, we show below the addition(I1 +I2 +I3 +I4) terms in Equation 7 are expected to be negatively correlated to the loss function, leadingto the proposed scoring function, in fact, aligns with the intuition of existing results. For simplicity, weconsider the setting without regularization (i.e., = 0) and the Hessian matrix has full rank.",
  "nd.(20)": "Note that the derivation from Equation 19 to Equation 20 is not mathematically rigorous, but as long asthe record z1 is not too deviated from the data distribution, we expect the above inequality holds. PluggingEquation 20 into the log-likelihood term inside M(w, z1) (first equality in Equation 17 with = 0), we get",
  "n.(22)": "The upper limit on the score (hence the score itself) corresponding to IHA is thus proportional to thenegative of the loss function, aligning with intuition (lower loss indicative of overfitting, and thus the recordbeing a member).The score inside IHA can thus be interpreted (up to some approximation error) asf(w, z1)(w, z1) for some f(w, z1) > 0 that essentially accounts for SGD training dynamics, and is afunction dependent on parameter access to the target model.",
  "that would maximize performance. The switch from Purchase-100(S) to Purchase-100 not only improvesmodel performance but also reduces the performance of MIA attacks ()": "For instance, AUC values for LOSS drop from 0.59 to 0.53, and LiRA-Online from 0.74 to 0.63.While the smaller version of the dataset has recently been argued not to be very relevant Carlini et al. (2022),we believe this larger version is still interesting to study since such large datasets are practically relevant.We hope that researchers will aim to use the larger version of the dataset and, in general, train target modelsto maximize performance (as any model trainer would) within the constraints of their experimental design.",
  "D.1IHA performance on MLP-4": "For the MLP-4 architecture on the Purchase-100(S) dataset, we observe a significant drop in performancewhen using IHA compared to LiRA. We hypothesize that this performance degradation may be due to amismatch between the damping factor used in our experiments (2e1) and the optimal damping factor forthe eigenvalue distribution of this larger model. To test this hypothesis, we increased to 5e1 and repeatedthe evaluation with IHA. This adjustment led to a substantial improvement in performance: the AUC increased to 0.768, with TPRsof 13.11% and 12.12% at 1% and 0.1% FPR, respectively. This result supports our hypothesis and highlightsa current limitation of IHA. Specifically, the damping factor should be scaled according to the modelarchitectureideally determined by some percentile of the eigenvalues. However, identifying the optimalscaling method before conducting the audit remains an open question for future work. It is important to note that further increasing does not result in a linear performance improvement; infact, performance declines across architectures when becomes too large, which is expected. The remainingperformance gap is likely due to the absence of reference models or a violation of our assumptions aboutHessian behavior. However, the superior TPR at very low (0.1%) FPRs, compared to LiRA, suggests thatthe former is more likely the cause.",
  "FApproximating L0": "For auditing purposes, experiments where all but one member is known are useful, but an adversary is unlikelyto have this much knowledge of the training data. We experiment with the potential use of IHA where onlypartial knowledge of the remaining n 1 members may be available to approximate L0, ApproximatingL0 with a fraction of the actual dataset could be useful in not only reducing the computational cost of theaudit, but also potentially enabling adversarial use of the attack in threat models where the attacker haspartial knowledge of the training data. We evaluate IHA for versions where L0 is approximated using arandomly-sampled fraction of the training data and report results in .",
  ".00520.0919.09": "We see a clear degradation in performance when only a subset of data is usedthis is especially true for lowerfractions, where AUC can drop by as much as 0.2. More importantly, even when using 90% of the trainingdata, there is a significant gap in performance. The statistics we compute for IHA thus do completely utilizeknowledge of all other training records. While this result suggests that adversarial use of IHA would requirea very strong adversary (that posseses knowledge of nearly all training records), it also hints at how datapoisoning attacks could have a large impact on downstream membership inference. A poisoning adversarycould hypothetically craft data a way that interferes with L0 (when relating to the optimal membershipadversary) and thus increase/decrease inference risk for other records."
}