{
  "Abstract": "A crucial design decision for any robot learning pipeline is the choice of policy representa-tion: what type of model should be used to generate the next set of robot actions? Owing tothe inherent multi-modal nature of many robotic tasks, combined with the recent successesin generative modeling, researchers have turned to state-of-the-art probabilistic models suchas diffusion models for policy representation. In this work, we revisit the choice of energy-based models (EBM) as a policy class. We show that the prevailing folklorethat energymodels in high dimensional continuous spaces are impractical to trainis false. We developa practical training objective and algorithm for energy models which combines several keyingredients: (i) ranking noise contrastive estimation (R-NCE), (ii) learnable negative sam-plers, and (iii) non-adversarial joint training. We prove that our proposed objective functionis asymptotically consistent and quantify its limiting variance. On the other hand, we showthat the Implicit Behavior Cloning (IBC) objective is actually biased even at the populationlevel, providing a mathematical explanation for the poor performance of IBC trained energypolicies in several independent follow-up works. We further extend our algorithm to learna continuous stochastic process that bridges noise and data, modeling this process with afamily of EBMs indexed by scale variable. In doing so, we demonstrate that the core ideabehind recent progress in generative modeling is actually compatible with EBMs. Alto-gether, our proposed training algorithms enable us to train energy-based models as policieswhich compete withand even outperformdiffusion models and other state-of-the-art ap-proaches in several challenging multi-modal benchmarks: obstacle avoidance path planningand contact-rich block pushing.",
  "Introduction": "Many robotic taskse.g., grasping, manipulation, and trajectory planningare inherently multi-modal: atany point during task execution, there may be multiple actions which yield task-optimal behavior. Thus, afundamental question in policy design for robotics is how to capture such multi-modal behaviors, especiallywhen learning from optimal demonstrations. A natural starting point is to treat policy learning as a distri-bution learning problem: instead of representing a policy as a deterministic map (x), utilize a conditional",
  "generative model of the form p(y | x) to model the entire distribution of actions conditioned on the currentrobot state": "This approach of modeling the entire action distribution, while very powerful, leaves open a key designquestion: which type of generative model should one use? Due to the recent advances in diffusion modelsacross a wide variety of domains (Dhariwal & Nichol, 2021; Blattmann et al., 2023; Kong et al., 2021; Sahariaet al., 2023; Song et al., 2022; Yang et al., 2022), it is natural to directly apply a diffusion model for thepolicy representation (Chi et al., 2023; Janner et al., 2022; Reuss et al., 2023). In this work, however, werevisit the use of energy-based models (EBMs) as policy representations (Florence et al., 2022). Energy-based models have a number of appealing properties in the context of robotics. First, by modeling a scalarpotential field without any normalization constraints, EBMs yield compact representations; model size isan important consideration in robotics due to real-time inference requirements. Second, action selectionfor many robotics tasks (particularly with strict safety and performance considerations) can naturally beexpressed as finding a minimizing solution of a particular loss surface (Adams et al., 2022); this implicitstructure is succinctly captured through the sampling procedure of EBMs. Furthermore, reasoning in thedensity space is more amenable to capturing prior task information in the model (Urain et al., 2022b), andallows for straightforward composition which is not possible with score-based models (Du et al., 2023). Unfortunately, while the advantages of EBMs are clear, EBMs for continuous, high-dimensional data canbe difficult to train due to the intractable computation of the partition function. Indeed, designing efficientalgorithms for training EBMs in general is still an active area of research (see e.g., Du & Mordatch (2019); Daiet al. (2019); Song & Kingma (2021); Arbel et al. (2021)). Within the context of imitation learning, recentwork on implicit behavior cloning (IBC) (Florence et al., 2022) proposed the use of an InfoNCE (van denOord et al., 2018) inspired objective, which we refer to as the IBC objective. While IBC is considered state ofthe art for EBM behavioral cloning, follow up works have found training with the IBC objective to be quiteunstable (Ta et al., 2022; Reuss et al., 2023; Chi et al., 2023; Pearce et al., 2023), and hence the practicalityof training and using EBMs as policies has remained an open question. In this work we resolve this open question by designing and analyzing new algorithms based on ranking noiseconstrastive estimation (R-NCE) (Ma & Collins, 2018), focusing on the behavioral cloning setting. Our maintheoretical and algorithmic contributions are summarized as follows: The population level IBC objective is biased: We show that even in the limit of infinitedata, the population level solutions of the IBC objective are in general not correct. This providesa mathematical explanation as to why policies learned using the IBC objective often exhibit poorperformance (Ta et al., 2022). Ranking noise contrastive estimation with a learned sampler is consistent: We utilizethe ranking noise contrastive estimation (R-NCE) objective of Ma & Collins (2018) to address theshortcomings of the IBC objective. We further show that jointly learning the negative samplingdistribution is compatible with R-NCE, preserving the asymptotic normality properties of R-NCE.This joint training turns out to be quite necessary in practice, as without it the optimization land-scape of noise contrastive estimation objectives can be quite ill-conditioned (Liu et al., 2022; Leeet al., 2023). EBMs are compatible with multiple noise resolutions: A key observation behind recent gen-erative models such as diffusion (Ho et al., 2020; Song et al., 2021b) and stochastic interpolants (Al-bergo et al., 2023; Lipman et al., 2023; Liu et al., 2023) is that learning the data distribution atmultiple noise scales is critical. We show that this concept is actually compatible with EBMs andcontrastive training, by introducing a framework that models a continuum of EBMs which we terminterpolating EBMs, trained jointly using R-NCE. We believe this contribution to be of independentinterest to the generative modeling community. EBMs trained with R-NCE yield high quality policies: We show empirically on several multi-modal benchmarks, including an obstacle avoidance path planning benchmark and a contact-richblock pushing task, that EBMs trained with R-NCE are competitive withand can even outper-formIBC and diffusion-based policies. To the best of our knowledge, this is the first result in the",
  "Related work": "Generative Models for Controls and RL.Recent advances in generative modeling have inspired manyapplications in reinforcement learning (RL) (Heess et al., 2013; Haarnoja et al., 2017; 2018; Levine, 2018;Liu et al., 2021; Ho & Ermon, 2016), trajectory planning (Du et al., 2020; Urain et al., 2022b; Ajay et al.,2022; Janner et al., 2022), robotic policy design (Florence et al., 2022; Pearce et al., 2023; Chi et al.,2023; Reuss et al., 2023), and robotic grasp and motion generation (Urain et al., 2022a). In this paper,we focus specifically on generative policies trained with behavior cloning, although many of our technicalcontributions apply more broadly to learning energy-based models. Most related to our work is the influentialimplicit behavior cloning (IBC) work of Florence et al. (2022), which advocates for using a noise contrastiveestimation objective (which we refer to as the IBC objective) based on InfoNCE (van den Oord et al., 2018)in order to train energy-based policies. As discussed previously, many works (Ta et al., 2022; Reuss et al.,2023; Chi et al., 2023; Pearce et al., 2023) have independently found that the IBC objective is numericallyunstable and does not consistently yield high quality policies. One of our main contributions is a theoreticalexplanation of the limitations of the IBC objective, and an alternative training procedure based on rankingnoise contrastive estimation. Another closely related work is Chi et al. (2023), which shows that score-baseddiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020; 2021b) yield state-of-the-art generative policies for many multi-modal robotics tasks. In light of this work and the poor empiricalperformance of the IBC objective, it is natural to conclude that score-based diffusion methods are now thede facto standard for learning generative policies to solve complex robotics tasks. One of our contributionsis to show that this conclusion is false; energy-based policies trained via our proposed R-NCE algorithmsare actually competitive with diffusion-based policies in terms of performance. We focus the rest of the related work discussion on the training of EBMs, as this is where most of our technicalcontributions apply. Given the considerable scope of this topic and the extensive literature (see e.g., Song& Kingma (2021); Lecun et al. (2006) for thorough literature reviews), we concentrate our discussion onthe two most common frameworks: maximum likelihood estimation (MLE) and noise contrastive estimation(NCE). Training EBMs via Maximum Likelihood.A key challenge in training EBMs via MLE is in computingthe gradient of the MLE objective, which involves an expectation of the gradient of the energy function w.r.t.samples drawn from the EBM itself (cf. ). This necessitates the use of expensive Markov ChainMonte Carlo (MCMC) techniques to estimate the gradient, resulting in potentially significant truncationbias that can cause training instability. A common heuristic involves using persistent chains (Tieleman,2008; Du & Mordatch, 2019; Du et al., 2021) over the course of training to better improve the qualityof the MCMC samples. However, such techniques cannot be straightforwardly applied to the conditionaldistribution setting, as one would require storing context-conditioned chains, which becomes infeasible whenthe context space is continuous. Another common technique for scalable MLE optimization is to approximate the EBM models samplesusing an additional learned generative model that easier to sample from. Re-writing the partition function inthe MLE objective using a change-of-measure argument and applying Jensens inequality yields a variationallower bound and thus, a max-min optimization problem where the sampler is optimized to tighten the boundwhile the EBM is optimized to maximize likelihood (Dai et al., 2019; Grathwohl et al., 2021). For example,Dai et al. (2019) propose to learn a base distribution followed by using a finite number of Hamiltonian MonteCarlo (HMC) leapfrog integration steps to sample from the EBM. Grathwohl et al. (2021) use a Gaussiansampler with a learnable mean function along with variational approximations for the inner optimization.More examples of this max-min optimization approach can be found in e.g., Song & Kingma (2021); Bond-Taylor et al. (2022). Unfortunately, adversarial optimization is notoriously challenging and requires severalstabilization tricks to prevent mode-collapse (Kumar et al., 2019).",
  "Published in Transactions on Machine Learning Research (09/2024)": ": The Conv1D-MH-FiLM block. The input x is the combined set of context tokens from o, , and t.Each block has its own independent set of set of query tokens q Rnqdf , treated as learnable parameters, thatcross-attend (MH-CA) to the composite context tokens x to produce FiLM weights {(i, i)}nqi=1, where i, i Rdf .The number of queries in each CMHF block nq was set to the number of heads (2). Each set of FiLM weights (i, i)are broadcasted across the temporal dimension of the prediction tokens + and affinely transform the predictiontokens (see Perez et al. (2018)) to yield nq copies of transformed prediction tokens. These are then concatenated(similar to a multi-head attention block) along the head-dimension, and projected back down to yield a set of outputprediction tokens +. Separately, the composite context tokens x are propagated through a standard MH-SA block. : List of hyperparameters for I-R-NCE. The constant Nb is described in . Tmcmc refers to thetotal number of yE evaluations, split between the SDE sampler and HMC leapfrog integrator steps, and refersto both the SDE noise-scale (see (5.7)) and HMC leapfrog integrator step size; we used 50 leapfrog steps per HMCmomentum sampling step. The value t is the initial CNF sample time; see Algorithm 4. The constant for theproposal NFs was fixed at 2.5. Note that in our sweeps, we exclude combinations of Nb for the EBM and NF whichyield total parameter accounts exceeding 3.3M.",
  "We next outline NCE as a form of discriminative correction, drawing a natural analogy with GenerativeAdversarial Networks (GANs)": "Training EBMs via Noise Contrastive Estimation.Instead of directly trying to maximize the like-lihood of the observed samples, NCE leverages contrastive learning (Le-Khac et al., 2020) and thus, iscomposed of two primary parts: (i) the contrastive sample generator, and (ii) the classification-based critic,where the latter serves as the optimization objective. The general formulation was introduced by Gutmann& Hyvrinen (2012) for unconditional generative models, whereby for each data sample, one samples Kcontrast\" (also referred to as negative\") examples from the noise distribution, and formulates a binaryclassification problem based upon the posterior probability of distinguishing the true sample from the syn-thesized contrast samples. For conditional distributions, Ma & Collins (2018) found that the binary classification approach is severelylimited: consistency requires the EBM model class to be self-normalized. Instead, they advocate for theranking NCE (R-NCE) variant (Jozefowicz et al., 2016). R-NCE posits a multi-class classification objectiveas the critic, and overcomes the consistency issues with the binary classification objective in the conditionalsetting (Ma & Collins, 2018), at least for discrete probability spaces. In this work, we focus on the R-NCEformulation, and extend the analysis of Ma & Collins (2018) to include jointly optimized noise distributionmodels over arbitrary probability spaces (i.e., beyond the discrete setting). In particular we study variousproperties such as: (i) asymptotic convergence, (ii) the pitfalls of weak contrastive distributions, (iii) jointlytraining the contrastive generative model, either via an independent objective or adversarially, (iv) extensionto a multiple noise scale framework by defining a suitable time-indexed family of models and contrastivelosses, and (v) sampling from the combined contrastive generative model and EBM. Despite leveraging a similar classification-based objective to GANs, NCE does not need adversarial trainingin order to guarantee convergence. In particular, the contrastive model in NCE is not the primary genera-tive model being learned; it is merely used to provide the counterexamples needed to score the EBM-basedclassifier. Furthermore, our asymptotic analysis shows that a fixed noise distribution suffices for consistencyand asymptotic normality. In practice, however, the design of the noise distribution is the most criticalfactor in the success of NCE methods (Gutmann & Hirayama, 2011). An overly simple fixed noise distri-bution will lead to a trivial classification problem, which is problematic from an optimization perspectivedue to exponentially flat loss landscapes (Liu et al., 2022; Lee et al., 2023). Thus, a key part of our al-gorithmic contributions is in how the negative sampler is learned. We propose simultaneously training asimpler normalizing flow model as the sampler, jointly with the EBM. This is in contrast with adversarialapproaches (Gao et al., 2020; Bose et al., 2018) which require max-min training.1 Without a need for ad-versarial optimization, our joint training algorithm is numerically stable and yields a pair of complementarygenerative models, whereby the more expressive model (EBM) bootstraps off the simpler model (normalizingflow) for both training and inference-time sampling. 1Gao et al. (2020) actually consider a similar approach in the binary NCE setting, but ultimately dismiss it in favor ofadversarial training. We delve further by analyzing the statistical properties of adversarial training and question its necessityin regards to the R-NCE objective.",
  "Notation": "Let the context space (X, X) and event space (Y, Y ) be compact measure spaces (for simplicity, we assumethat the event space is not a function of the context). Equip the product space X Y with a probabilitymeasure PX,Y = PX PY |X, and assume that this probability measure is absolutely continuous w.r.t. thebase product measure X Y . Suppose furthermore that the conditional distribution PY |X is regular.Let p(x, y), p(x), and p(y | x) denote the joint density, the marginal density, and the conditional density,respectively (all densities are with respect to the their respective base measures).",
  "Fn := {p(y | x) | }.(3.2)": "While our theory will be written for general parametric density classes, for our proposed algorithm tobe practical we require that both computing and sampling from p(y | x) is efficient. Some examples ofmodels which satisfy these requirements include normalizing flows (Grathwohl et al., 2019) and stochasticinterpolants (Albergo & Vanden-Eijnden, 2023; Albergo et al., 2023). We globally fix a positive integer K N+. For a given x, let PKy|x; denote the product conditional samplingdistribution over YK where y PKy|x; denotes a random vector y = (yk)Kk=1 YK with yk p( | x) fork = 1, . . . , K. Now, for a given (x, y) PX,Y , y PKy|x;, and parameters , , we define:",
  "Intuition.Let us build some intuition for the ranking objective (3.3). For any (x, y) PX,Y and y PKy|x;,": "let y := {y} y = (yj)K+1j=1 YK+1. Let the random variable D {1, . . . , K + 1} denote the index of thetrue sample y within y. We will construct a Bayes classifier for D using the EBM. In particular, leveragingthe EBM as the generative model for the true distribution PY |X, we can define the class-conditionaldistribution over y as:",
  "q,(k | x, y) =p,(y | x, D = k)p(D = k | x)K+1j=1 p,(y | x, D = j)p(D = j | x)": "Now, it remains to specify a prior distribution p(D = k | x). A simple, yet natural, choice reflecting thedesire that no index is to be preferred over any other indices is to choose the uniform prior distributionp(D = k | x) = 1/(K + 1) for all k {1, . . . , K + 1}. With this prior, the posterior probability simplifies as:",
  "yy exp(E(x, y) log p(y | x)).(3.7)": "Thus, the objective in (3.3) denotes the posterior log-probability (conditioned on both the data (x, y) andthe contrastive samples y) that the sample y is drawn from the energy model E and the remaining samplesK samples in y are all drawn iid from p( | x). Based on this interpretation, we adopt the terminology thaty is the positive example, y contains the negative examples, and p( | x) is the negative proposal distribution. Given this posterior log-probability interpretation, it will prove useful to re-state the R-NCE objective asfollows. For k {1, . . . , K +1}, let Py|x;,k denote the true class-conditional\" distribution over YK+1. Thatis, for a given y = (yj)K+1j=1 Py|x;,k, we have yk p( | x) and yj p( | x) for j = k. When the index k isomitted, this refers to the setting of k = 1: Py|x; Py|x;,1. By symmetry then, for any k {1, . . . , K + 1},the population objective in (3.4) can be equivalently written as:",
  ".(R-NCE)": "Comparing the two expressions, the main difference is in the gradient correction term on the right. ForMLE, this gradient computation requires sampling from the energy model p(y | x), which is prohibitivelyexpensive during training. On the other hand, the R-NCE gradient is computationally efficient to compute,provided the negative sampler is efficient to both sample from and compute log-probabilities.",
  "Algorithm": "Our main proposed R-NCE algorithm with a learnable negative sampler is presented in Algorithm 1. Weremark that one could alternatively pre-train the negative sampler distribution p (i.e., move Lines 58 afterLine 3 and set Tsamp large enough so that the negative sampler converges), as opposed to jointly trainingthe negative sampler concurrently with the energy model. Using a pre-trained negative sampler distributionwas previously explored in Gao et al. (2020), where it was empirically shown to be an ineffective idea forbinary NCE. We will also revisit this choice within our experiments. Remark 3.1. As mentioned earlier, any family of densities Fn suffices in Algorithm 1. For specific familiessuch as stochastic interpolants (Albergo & Vanden-Eijnden, 2023) where an auxiliary loss is used instead ofnegative log-likelihood, Algorithm 1, Line 7 is replaced with the corresponding auxiliary loss. Remark 3.2. Note that in Algorithm 1, the proposal model is optimized independently of the EBM; thelatter is optimized purely via the R-NCE objective. Crucially, the negative samples y do not depend uponthe EBMs parameters . Hence, no re-parameterization tricks are required since the batch augmentationin Line 11 does not introduce any gradient dependencies on for the optimization step in Line 12.",
  ",(x, y) := Ey|x; ,(x, y | y)": "Then, by the continuity of (, ) ,(x, y | y) for a.e. (x, y, y) and the Lebesgue Dominated Convergencetheorem, it follows that: (i) for a.e. (x, y), the map (, ) ,(x, y) is continuous, and (ii) the map(, ) L(, ) is continuous. For all results in this section, the proofs are provided in Appendix C.",
  "= argmaxL(, ).(4.2)": "Proof. As noted in .1, the proof strategy follows Ma & Collins (2018, Theorem 4.1), but includesthe measure-theoretic arguments necessary in our setting. First, we note our assumptions ensure that L(, )is finite for every , . Second, since the index k does not affect the value in (3.8), we can take theaverage",
  ".(4.3)": "The IBC objective can be seen as a special case of the R-NCE objective, with the particular choice ofnegative sampling distribution p(y | x) as the uniform distribution on Y for a.e. x X. Note that theuniform distribution is the only choice which renders the objective unbiased (this observation was also madein Ta et al. (2022)). illustrates a toy one dimensional setting with a Gaussian proposal distribution,illustrating the bias inherent in the population objective when a non-uniform proposal distribution is used.Shortly, we will characterize the optimizers of the IBC objective, quantifying the bias.",
  "Objective": "IBC (K = 10)IBC (K = 100)R-NCE (K = 10)R-NCE (K = 100)MLE : The population objective landscape for the IBC objective (4.3) versus the R-NCE objective (3.4), forK {10, 100}. The true distribution p(y | x) is a N(1, 1) distribution (for simplicity, the context x is not relevant),and the proposal distribution is N(0, 1). The energy model function class is comprised of unit variance Gaussians,i.e., F = {(x, y) 1 2(y)2 | R}. Note that for ease of comparison, all objectives have been shifted so that themaximum values are zero. Furthermore, the maximum likelihood objective (3.9) has also been plotted for reference. The fact that the IBC objective forces the use of a uniform proposal distribution substantially limits itsapplicability in high dimension. Indeed, in .3 we will see that a proposal distribution which isnon-informative causes the gradient to vanish, and hence training to stall. This issue has been empiricallyobserved in many follow up works (Ta et al., 2022; Reuss et al., 2023; Chi et al., 2023; Pearce et al., 2023). Optimizers of the IBC Objective.The optimizers of the IBC objective can be studied using theoptimality of the R-NCE objective (Theorem 4.4) in conjunction with a change of variables.We firstintroduce the notion of -realizability, which posits that the density ratio p(y | x)/p(y | x) is representableby the function class F.",
  "Note that realizability of F (Definition 4.3) is equivalent to -realizability of F (Definition 4.6), from whichthe claim now follows by Theorem 4.4": "Proposition 4.7 illustrates that optimizing the IBC objective results in an energy model which representsthe density ratio p(y | x)/p(y | x), explaining the bias in the IBC objective when the negative samplingdistribution is non-uniform.The reason for this is because the IBC objective is based on an incorrectapplication of InfoNCE (van den Oord et al., 2018), which is designed to maximize a lower bound onmutual information between contexts x and events y, rather than extract an energy model E(x, y) to modelp(y | x) (Ta et al., 2022).",
  "n argmaxLn(, n).(4.5)": "We note that the definition of n in (4.5) is a stylized version of Algorithm 1, where the training procedure(e.g., gradient-based optimization) is assumed to succeed. We now establish consistency of the sequence ofestimators {n} to the set . Let d(z, A) denote the set distance function between the set A and the pointz, i.e.,3",
  "It follows then that the map (, ) L(, ) is C2-continuous": "We are now ready to formalize the asymptotic distribution of n.Theorem 4.10 (Asymptotic Normality). Suppose that F is realizable and Assumptions 4.1, 4.2, and 4.9hold. Let {n} be a sequence of optimizers as defined via (4.5). Denote the joint parameter := (, ) andjoint r.v. z := (x, y), and assume the following properties hold:",
  "V = 2L(, )1 = Ez2(z)1": "A few remarks are in order regarding this result. First, note that the fixed point for the negative distributionparameters need not be optimal in any sense, as long as it is approached by the sequence n sufficientlyfast. Second, the proof itself begins by using the standard machinery for proving asymptotic normality ofM-estimators for the joint set of parameters := (, ) (see e.g. Ferguson, 2017). In order to extract themarginal asymptotics for n, while employing minimal assumptions regarding the convergence of n, againthe key step is to leverage Theorem 4.4, which states that is optimal for the population objective L(, )for any . This implies that the joint Hessian 2L(, ) is block-diagonal, yielding the necessarysimplification. Finally, we remark that the result in Ma & Collins (2018, Theorem 4.6) is a special case ofTheorem 4.10 where (i) the negative distribution is held fixed, i.e., n = for all n, and (ii) the event spacesX and Y are finite.",
  "Learning the Proposal Distribution": "Given that the consistency and asymptotic normality for the R-NCE estimator are guaranteed under thevery mild condition that the negative proposal distribution parameters converge to a fixed point (cf. Theo-rem 4.10), there is considerable algorithmic flexibility in generating the sequence of proposal models, indexedby {n}. We begin our discussion with first establishing the importance of having a learnable proposal dis-tribution. Using the posterior probability notation (3.7), we can write the R-NCE gradient as:",
  "Nearly Asymptotically Efficient Negative Sampler": "In principle, the asymptotic normality result Theorem 4.10 gives us a recipe to characterize an optimalproposal model: simply select the distribution p which minimizes the trace of the asymptotic variance matrixV (which itself is a function of p). However, this is an intractable optimization problem in general (seee.g., Gutmann & Hyvrinen, 2012, .4). Nevertheless, we will show that considerable insight maybe gained by setting the negative distribution p(y | x) equal to the conditional data distribution p(y | x).",
  "where (, )": "Recall from Theorem 4.12 that if the fixed point for the proposal distribution p( | x) equals p( | x), theasymptotic variance for n equals a 1 + 1/K multiplicative factor of the Cramr-Rao optimal variance. Theresult above corresponds precisely to such a scenario: given a realizable family of proposal distributions,the adversarial saddle-point for R-NCE is characterized by the equalities p( | x) = p( | x) = p( | x).Thus, when a sufficiently rich family of proposal distributions (i.e., Fn is realizable) is paired with a tractableconvergent algorithm that ensures d(n, )n 0, there is no additional benefit to using adversarial R-NCE:the asymptotic distribution of n is unaffected. Notwithstanding, our preferred setup assumes, for both computational and conceptual reasons, that Fn isnot realizable. This is motivated by the fact that the proposal distribution is used both for sampling andlikelihood evaluation within the R-NCE objective, and is thus a significantly simpler model for computa-tional reasons. In the following, we investigate the asymptotic properties of adversarial R-NCE under suchconditions.",
  "f(, ) f(, ) f(, ) (, )": "A Nash equilibrium is characterized as follows: taking the other player as fixed, each player has noincentive to change their action. It is straightforward to verify from the proof of Proposition 4.13 thatany pair (, ) is in fact a Nash equilibrium w.r.t. the population objective L. However, forgeneral nonconcave-nonconvex objectives, e.g., the finite-sample game defined in (4.9), the existence of aglobal or local Nash equilibrium is not guaranteed (Jin et al., 2020). Thus, we need an alternative definitionof optimality, which comes from considering games as sequential.",
  ". Suppose that is contained in the interior of . We have that d(n, )n 0 a.s": "2. Suppose furthermore that Assumption 4.9 holds. Let := { | L(, ) = inf L(, )} foran arbitrary choice of (the set definition is independent of this choice), and suppose iscontained in the interior of . We have that d(n, )n 0 a.s. Proof. Part (1). We prove convergence of n to a maxmin Stackelberg optimal point for the populationgame, i.e., a global maximizer of the function L, defined as inf L(, ). From Proposition 4.16,this is necessarily the set . By our assumptions, we have that L(, ) is continuous. Since is compact,by classic envelope theorems, we have that L is continuous. By Lemma C.2, since is contained in theinterior of , there exists 0 > 0 such that for all 0 < 0:",
  "V a = Ez2 (z)1 Varz (z)Ez2 (z)1 .(4.11)": "A particularly important corollary of this result is as follows:Corollary 4.19 (Marginal Normality). Under the assumptions of Theorem 4.18, the parameters {n} satisfyn(n ) N(0, V a ), whereV a = 2L(, )1 The variance V a is similar to V in Theorem 4.10, with the difference being the specification of a particularfixed-point for the proposal distribution parameters: the Stackelberg adversary of . In the special casewhen the family of proposal distributions is realizable, since the fixed-point involves the Nash adversary (cf. Proposition 4.13), these two variances coincide. However, when we lack realizability of the proposaldistributions, we are left to compare the asymptotic variance V from Theorem 4.10 with the new asymptoticvariance V a . Given the quite non-trivial relationship between the proposal parameters that a non-adversarialnegative sampler converges to versus the Stackelberg adversary, it is not clear that a general statement canbe made comparing these two variances. Therefore, the theoretical benefits of adversarial training are quiteunclear. Furthermore, actually finding Stackelberg solutions comes with its own set of challenges in practice. Com-putationally, differentiating the R-NCE objective with respect to the negative proposal parameters wouldinvolve differentiating both the negative samples y as well as the likelihoods log p( | x), a non-trivial com-putational bottleneck. In contrast, differentiating the negative samples y with respect to the proposal modelparameters is not required in the non-adversarial setting (cf. Remark 3.2). For this work, we assume thatthe independent optimization of via a suitable objective for the generative model p is sufficient to optimizethe EBM, without assuming realizability for Fn. We leave the explicit minimization of some function of Vto future work. Finally, we conclude by noting that the proofs of Theorem 4.17 (adversarial consistency) and Theorem 4.18(adversarial normality) are based off of ideas from Biau et al. (2020), who study the asymptotics of GANtraining. The main difference is that we are able to exploit specific properties of the R-NCE objective toderive simpler expressions for the resulting asymptotic variances.",
  "Balanced model capacities": ": An illustration of the different qualitative behaviors that R-NCE training curves can exhibit. Thecurve illustrates an example of posterior collapse, where the negative sampler is completely un-informative, leadingto a trivial classification problem. Both theandcurves illustrate a phase transition in the training: at first theEBM learns quicker relative to the negative sampler (and hence the classification accuracy increases), but then thenegative sampler catches up in learning, resulting in a drop in the R-NCE objective. The main difference betweentheandcurves is the final value each curve converges towards. Thecurve tends toward log(K + 1), whichindicates that both the EBM and negative sampler have sufficient capacity to represent the true distribution p(y | x)(cf. Equation (3.6) in the case where p( | x) = p( | x) = p( | x)). On the other hand, thecurve tends towardsa final value larger than log(K + 1) but bounded away from zero, indicating that the EBM capacity exceeds thatof the negative sampler. The remainingandcurves represent when the negative sampler learns faster than theEBM; this occurs when, for example, a pre-trained negative sampler is used. Here, the training curve approachesits final value from below. As before, the difference between the final value of both theandcurves is dueto negative sampler model capacity, with thecurve denoting when the EBM has much larger capacity than thenegative sampler, and thecurve denoting when the model capacities are relatively balanced. Here, we have (a) overloaded the same notation for measures and densities, and (b) in the definition ofrelative Fisher information we have assumed smoothness of the densities. The following propositions arestraightforward applications of the second-order delta method.",
  "where J( | x) := Eyp(|x)[yE(x, y)TyE(x, y)]": "The asymptotic variance from Proposition 4.21 is more difficult to interpret than the corresponding asymp-totic variance from Proposition 4.20. Unfortunately in general, there is no relationship between the two,as we will see shortly. In order to aid our interpretation, suppose the energy model follows the form of anexponential family, i.e., E(x, y) = (x, y), , for some fixed sufficient statistic . Then, a straightforwardcomputation yields the following:",
  "I( | x) = Covp(y|x)((x, y)),J( | x) = Ep(y|x)[y(x, y)y(x, y)T]": "Hence, assuming as in (4.12) that p(y | x) = p(y | x), then the asymptotic variance is determined by thevariance of the smoothness of the sufficient statistic (in the event variable y) relative to the covariance ofthe sufficient statistic itself. Let us now consider a specific example. Suppose that p(y | x) = N(, 2)(so the context variable x is not used). Here, the sufficient statistic is (x, y) = (y, y2). A straightforwardcomputation yields that:",
  "Interpolating EBMs": "Recent methods such as diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020;2021b) and stochastic interpolants (Albergo & Vanden-Eijnden, 2023; Albergo et al., 2023) advocate formodeling a family of distributions with a single shared network, indexed by a single scale parameter lying ina fixed interval. At one end of this interval, the distribution corresponds to standard Gaussian noise, whilethe other end models the true data distribution. Sampling from this generative model begins with samplinga latent vector z Y from the base Gaussian distribution, and then propagating this sample through asequence of pushforwards over the interval. This propagation can either refer to a fixed discrete sequenceof Gaussian models, or solving a continuous-time SDE or ODE. The strength of such a framework has beenattributed to an implicit annealing of the data distribution over the interval (Song & Ermon, 2019), easingthe generative process by stepping through a sequence of distributions instead of jumping directly from purenoise to the data.",
  "Stochastic Interpolants": "A useful instantiation of this framework that subsequently allows generalizing all other interval-based modelsas special cases, corresponds to the stochastic interpolants formulation (Albergo & Vanden-Eijnden, 2023).In this setup, a continuous stochastic process over the interval is implicitly constructed by defining aninterpolant function It : Y Y Y, t , where I0(z, y) = z and I1(z, y) = y. For a given x, define thecontinuous-time stochastic process Yt | x as:",
  "The distribution of the samples from this model at t = 1 correspond to the desired distribution PY |x. Onecan now parameterize any function approximator vt, to approximate this vector field": "While CNFs as a generative modeling framework are certainly not new, their training has previously reliedon leveraging the associated log-probability ODE (Chen et al., 2018; Grathwohl et al., 2019) (cf. ),corresponding to the change-of-variables formula, and using maximum likelihood estimation as the opti-mization objective. Unfortunately, solving this ODE involves computing the Jacobian of the parameterizedvector field vt, with respect to yt; gradients of the objective w.r.t. therefore require 2nd-order gradients ofvt,. As a result of these computational bottlenecks, CNFs have been largely overtaken as a framework forgenerative models. However, the second key result from Albergo & Vanden-Eijnden (2023) is that the truevector field vt is given as the minimum of a simple quadratic objective:",
  "Building Interpolating EBMs": "A natural way of using the Interpolant-CNF model described previously is to use the pushforward distributionat t = 1 as the proposal generative model p(y | x), with a relatively simple parameterized vector field vt,.Paired with a parameterized energy function E(x, y), one can now proceed with optimizing both models asoutlined in Algorithm 1. However, by truncating the flow at any intermediate time t (0, 1), the CNF gives an approximationpt(yt | x) of the distribution pt(yt | x). Thus, we can define a time-indexed EBM E(x, y, t) to approximatethis same distribution as:",
  "where yij PKytij |xi;. The net R-NCE loss for (xi, yi) is then defined as the average of the R-NCE loss": "over {(xi, yij, tij, yij)}mj=1, where yij = Itij(zij, yi). The training pseudocode is presented in Algorithm 3,with the modifications to Algorithm 1 highlighted in teal. We note that choosing values of m > 1 serves asvariance reduction, and we also apply a similar variance reduction trick to the stochastic interpolant loss inLine 8. Algorithm 3: Interpolating-R-NCE with learnable negative samplerData: Dataset D = {(xi, yi)}ni=1, number of outer steps Touter, number of sampler steps per outer stepTsamp, number of R-NCE steps per outer step Trnce, number of negative samples K,interpolating time distribution , number of interpolating times m N1.",
  "end": "Remark 5.1. A nave implementation of variance reduction for the R-NCE loss in Algorithm 3 wouldgenerate m independent sets of K negative samples, i.e., {yij}mj=1, corresponding to the m intermediatetimes {tij}mj=1.To make this more efficient, we allow the negative sample sets to be correlated acrosstime by generating a single set of K trajectories, spanning the interval (0, maxj tij], and sub-sampling thesetrajectories at m intermediate times.",
  "dWt,yt=0|x(z) = z N(0, I),(5.6)": "where 0 and Wt is the standard Wiener process. Given the simplicity of the former ODE-based modelin (5.2), a natural question is: why is the SDE-based model necessary? The answer lies in studying thedivergence between the true conditional distribution p(y | x) and the approximate distribution pt=1(y | x)induced by the learned vector field vt,. In particular, from Albergo et al. (2023, Lemma 2.19), for a CNF",
  "Y(y log pt(y | x) y log pt(y | x)) (vt,(x, y) vt(x, y)) pt(y | x) dY dt": "It follows that control over the vector field error, i.e., the objective minimized in (5.3), does not provideany direct control over the Fisher divergence, i.e., error in the learned score function y log pt(y | x), thuspreventing any control over the target KL divergence. In contrast, let vt (x, y) be an approximation of the drift term vt (x, y) in (5.6). Define the following approx-imate generative model and associated Fokker-Planck transport PDE for the induced density pt(yt | x):",
  "Yvt (x, y) vt (x, y)2 pt(y | x) dY dt": "Thus, if one can control the net approximation error of the SDE drift, then the SDE-based generative modelshould yield samples which are closer to the target distribution in KL. While error in the velocity fieldvt vt, is controllable via the minimization of (5.3), the I-EBM framework gives us a handle on the othercomponent to the SDE drift: an approximation of the time-varying score function,5 i.e., yE(x, yt, t) y log pt(yt | x). Indeed, from Proposition 4.21, convergence to the global maximizer yields controllableconvergence (in distribution) on the relative Fisher information between pt(yt | x) and pt(yt | x). Algorithm 4",
  "return y": "Notice in Line 1, we use the CNF parameterized by vector field vt,, with the flow truncated at time t to formthe initial sample. This is reasonable since for t 1, the proposal model pt becomes more accurate withthe true process Yt | x tending towards an isotropic Gaussian, dispelling the need for an EBM. In Line 2,we leverage the SDE generative model from (5.7) to transport the sample to t = 1, making use of both theproposal model vector field vt, and the I-EBMs score function yE to form the approximation of the driftterm. Finally, in Line 3, we run Tmcmc steps of MCMC (either Langevin or HMC, cf. Algorithm 2) at a fixed(or annealed) step-size of with the EBM at t = 1 to generate the final sample. 5We note that in Albergo et al. (2023), the authors introduce a stochastic extension to the interpolant framework from Albergo& Vanden-Eijnden (2023) which elicits a quadratic loss function for the time-varying score function, similar in spirit to thecanonical denoising loss used for training diffusion models. Our framework however, is agnostic to the type of bridge createdbetween the base distribution and true data. Notably, I-EBMs and the associated I-R-NCE objective are well-defined for anytype of interpolating/diffusion bridge.",
  "Advantages": "With the I-EBM framework in place, we now describe the key advantages behind the formulation comparedwith other state-of-the-art generative models. As we will soon show experimentally (), it is im-portant for generative policies to be able to both generate and score samples efficiently. I-EBMs meet bothrequirements in ways that many other state-of-the-art models cannot, as we discuss below. Comparison to stochastic interpolants: As a stochastic interpolant model is as its core a continuous normal-izing flow, computing sample likelihoods require solving the standard log-probability ODE (cf. ),which is computationally bottlenecked by vector-Jacobian products. While we do employ various tricks tospeed this procedure up (as discussed in ), I-EBMs completely avoid this ODE computation forlikelihood computation, and only require one forward pass through the network E(x, y, 1). Comparison to diffusion models: A similar computation issue arises when scoring samples with ODE-baseddiffusion models (Song et al., 2021b; Karras et al., 2022) as with interpolant models. This is because de-noiser models are typically parameterized as unconstrained vector-valued functions, and hence cannot beanalytically integrated. Therefore, the most straightforward way to compute log-probability is to treat adiffusion model as a normalizing flow and use the log-probability ODE, inheriting the associated computa-tional bottlenecks. An alternative to the log-probability ODE is to use Girsanovs theorem to derive a lowerbound on the log-likelihood, which can then be estimated via Monte-Carlo sampling (Song et al., 2021a).A similar sampling-based approach to likelihood computation also applies to DDPM-style diffusion mod-els (Sohl-Dickstein et al., 2015; Ho et al., 2020), where Monte-Carlo sampling of the evidence lower boundcan be used to estimate a lower bound on the log-likelihood. However, in addition to introducing an extrahyperparameter (the number of Monte-Carlo samplings used) which is necessary to tune the best trade-offbetween accurancy and performance, comparing ELBO scores across samples is actually quite problematicas described next. Comparison to conditional VAE models: Conditional variational autoencoders (CVAE) (Doersch, 2016) havebeen recently proposed by several authors (Zhao et al., 2023; Gomez-Gonzalez et al., 2020; Ivanovic et al.,2021) as a model for generative policies. Unfortunately, the ELBO loss which CVAEs optimize is insufficientto support ranking samples. To see this, first recall that CVAEs utilize the following decomposition of thelog-probability (Doersch, 2016):",
  "log p(y | x) KL(q(z | x, y) p(z | x, y)) = Ezq(|x,y)[log p(y | x, z)] KL(q(z | x, y) p(z | x)).(5.9)": "The RHS of (5.9) lower bounds the log-probability log p(y | x), and is the ELBO loss used during training;it only references the encoder q(z | x, y), the decoder p(y | x, z), and the prior p(z | x), and is thereforepossible to optimize over.The LHS of (5.9) contains the desired log p(y | x), plus an extra error term(x, y) := KL(q(z | x, y) p(z | x, y)), which cannot be computed due to its dependence on the trueposterior p(z | x, y). From this decomposition, we see that for a given shared context x and a set of samplesy1, . . . , yk, the RHS of (5.9) cannot serve as a reliable ranking score function, since the unknown error terms(x, yi) for i = 1, . . . , k are in general all different and not comparable. Note that this issue persists even inthe limit of infinite samples used to compute the RHS of (5.9). Efficient sampling from I-EBMs: We have thus far focused on the advantages of I-EBMs over other generativemodels when it comes to likelihood computation.We now discuss the sampling advantages of I-EBMs.Combining the vector field and energy function within the SDE sampling step (cf. Line 2 in Algorithm 4)significantly reduces the burden of MCMC-based sampling, which can be difficult to converge when startingfar from the stationary distribution of the EBM. In this respect, I-EBMs bring the advantages of learningover multiple noise resolutions/annealed sampling to EBMs.",
  "Recall that for a vector field f : Rd Rd, we have div(f) = tr(f) = di=1fxi": "Applications.We make use of the computation described in (6.2) in several key places for our work. First,we use this formula to compute log p(y | x) inside the R-NCE objective (cf. (3.3)). Second, we use thisformula to compute log p(y | x) for a diffusion model where the denoiser is directly parameterized (insteadof indirectly via the gradient of a scalar function); for diffusion models, this log-probability calculation isnecessary for ranking samples in next action selection (cf. .3). Here, we apply (6.2) by interpretingthe reverse probability flow ODE as a continuous normalizing flow of the form (6.1). Efficient Computation.The main bottleneck of (6.2) is in computing the integrand, which involvescomputing the divergence of the vector field vt(x, z) w.r.t. the flow variable z. Assuming the flow variablez Rd, the computational complexity of a single divergence computation scales quadratically in d, i.e., O(d2).The quadratic scaling arises due to the fact that for exact divergence computation, one needs to separatelymaterialize each column of the Jacobian, and hence O(d) computation is repeated d times.6 The typicalpresentation of CNFs advocates solving an augmented ODE which simultaneously performs sampling andlog-probability computation:",
  ",z(0) p0( | x),(0) = log p0(z(0) | x)": "This augmented ODE suffers from one key drawback: it forces both the sample variable z and the log-probability variable to be integrated at the same resolution. Due to the quadratic complexity of computingthe divergence, this integration becomes prohibitively slow for fine time resolutions. However, high samplequality depends on having a relatively small discretization error. We resolve this issue by using a two time-scale approach. In our implementation, we separately integratethe sample variable z and the log-probability variable . We first integrate z at a fine resolution. However,instead of computing the divergence of v at every integration timestep, we compute and save the divergencevalues at a much coarser subset of timesteps. Then, once the sample variable is finished integrating, wefinish off the computation by integrating the variable with one-dimensional trapezoidal integration (e.g.,using numpy.trapz). Splitting the computation in this way allows one to decouple sample quality from log-probability accuracy; empirically, we have found that using at most 64 log-probability steps suffices even forthe most challenging tasks we consider. A reference implementation is provided in Appendix A, .",
  "Experiments": "In this section, we present experimental validation of R-NCE as a competitive model for generative policies.We implement our models in the jax (Bradbury et al., 2018) ecosystem, using flax (Heek et al., 2023) fortraining neural networks and diffrax (Kidger, 2021) for numerical integration. All models are trained usingthe Adam (Kingma & Ba, 2015) optimizer from optax (Babuschkin et al., 2020). 6Randomized methods such as the standard Hutchinson trace estimator are not applicable here, since we need to be ableto compare and rank log-probabilities across a small batch of samples. Generically, unless the Hutchinson trace estimator isapplied (d) times (thus negating the computational benefits), the variance of the estimator will overwhelm the signal neededfor ranking samples; this is a consequence of the Hanson-Wright inequality (Meyer et al., 2021).",
  "Models Under Evaluation and Overview of Results": "Here, we collect the types of generative models which we evaluate in our experiments. Note that the morecomplex experiments only evaluate a subset of these models, as not all of the following models work well forhigh dimensional problems. NF (normalizing flow): a CNF (Grathwohl et al., 2019), where for computational efficiency the vectorfield parameterizing the flow ODE is learned via the stochastic interpolant framework (Albergo et al.,2023) instead of maximum likelihood; see .1. The flow ODE is integrated with Heun. Duringtraining, we sample interpolant times from the push-forward measure of the uniform distribution on transformed with the function t t1/, where is a hyperparameter. IBC (implicit behavior cloning): the InfoNCE (van den Oord et al., 2018) inspired IBC objectiveof Florence et al. (2022), defined in (4.3), for training EBMs. Langevin sampling (cf. Algorithm 2)is used to produce samples. R-NCE and I-R-NCE (ranking noise contrastive estimation): the learning algorithm presented inAlgorithm 1 for training EBMs, and its interpolating variation, Algorithm 3, presented in .2for training I-EBMs; both leveraging the stochastic interpolant CNF as the learnable negative sam-pler. Sampling is performed using either Algorithm 2 for EBM or Algorithm 4 for I-EBM. Diffusion-EDM: A standard score-based diffusion model. We base our implementation heavilyoff the specific parameterization described in Karras et al. (2022), including the use of the reverseprobability flow ODE for sampling in lieu of the reverse SDE, which we integrate via the Heunintegrator.We also treat the reverse probability flow ODE as a CNF for the purposes of log-probability computation (cf. ). Diffusion-EDM-: Same as Diffusion-EDM, except instead of directly parameterizing the denoiserfunction, we represent the denoiser as the gradient of a parameterized energy model (Salimans & Ho,2021; Du et al., 2023). This allows us to use identical architectures for models as IBC, R-NCE, andI-R-NCE. More details regarding recovering the relative likelihood model from the parameterizationof Karras et al. (2022) are available in Appendix B.1.We note that this model has an extrahyperparameter, rel, which indicates the noise level at which to utilize the energy model forrelative likelihood scores.7",
  "Note for both diffusion models, for brevity we often drop the -EDM label when it is clear from contextthat we are referring to this particular parameterization of diffusion models": "Basic Principles for Model Comparisons.As this list represents a wide variety of different methodolo-gies for generative modeling, some care is necessary in order to conduct a fair comparison. Here, we outlinesome basic principles which we utilize throughout our experiments to ensure the fairest comparisons: (a) Comparable parameter counts: We keep the parameter counts between different models in similarranges, regardless of whether or not a particular model parameterizes vector fields (NF, Diffusion)or energy models (IBC, R-NCE, I-R-NCE, Diffusion-). Furthermore, for R-NCE and I-R-NCE, wecount the total number of parameters between both the negative sampler and the energy model. (b) Comparable functions evaluations for sampling: We keep the number of function evaluations madeto either vector fields or energy models in similar ranges regardless of the model, so that inferencetimes are comparable.",
  "(c) Avoiding excessive hyperparameter tuning: In order to limit the scope for comparison, we omitexploring many hyperparameter settings which apply equally to all models, but may give some": "7We find that, much akin to how the diffusion reverse process for sampling is typically terminated at a small but non-zerotime, this is also necessary for the numerical stability of the relative likelihood computation (cf. Appendix B.1, Equation (B.1)).Furthermore, we find that the best stopping time for the latter is typically an order of magnitude higher than the former. Weleave further investigation into this for future work.",
  "We first evaluate R-NCE and various baselines on several conditional two-dimensional problems, Pinwheeland Spiral": "Pinwheel: The context space is a uniform distribution over X = {4, 5, 6, 7}, denoting the number ofspokes of a planar pinwheel. We apply a 10 dimensional sinusoidal positional embedding to embed Xinto R10. The event space Y = R2, denoting the planar coordinates of the sample. The distributionis visualized in the upper left row of , with only x {4, 5, 6} visualized for clarity. Spiral: The context space is a uniform distribution over X = , denoting the length of aparametric curve representing a planar spiral. This range is normalized to the interval beforebeing passed into the models. The event space Y = R2, again denoting the planar coordinates of thesample. The distribution is visualized in the upper right row of , with only x {400, 500, 600}visualized for clarity. The performance of the models listed in .1 is shown in (samples) and (KDEplots). The specific details of model architectures, training details, and hyperparameter values are given inAppendix B.2. For all models, we restrict the parameter count of models to not exceed 22k; for R-NCE,this number is a limit on the sum of the NF+EBM parameters. We report the Bhattacharyya coefficient(BC) between the sampling distribution and the true distribution, computed as follows. First, recall thatthe BC between true conditional distribution p(y | x) and a learned conditional distribution p(y | x) is:",
  "p(y | x)p(y | x) dYx X.(7.1)": "Note that the BC lies between , with a value of one indicating that p(y | x) = p(y | x). We estimateboth p(y | x) and p(y | x) via the following procedure, which we apply for each context x that we evaluateseparately.First, we compute a Gaussian KDE using 8,192 samples with scipy.stats.gaussian_kde,invoked with the default parameters. Next, we discretize the square into 256 256 grid",
  "points, and compute (7.1) via numerical integration using the KDE estimates of the density. Finally, wereport the minimum of the BC estimate over all x values that we consider": "The main findings in and are that the highest quality samples are generated by the R-NCE,Diffusion, and NF models, between which the quality is indistinguishable, followed by a noticeable drop offin quality for IBC. Note that for IBC we use a uniform noise distribution to generate negative samples.While relatively simple, this example is sufficient to validate our implementations, albeit still illustrating thelimitations of IBC.",
  "Diffusion-": "r = 0.967r = 0.240r = 1.000r = 0.955 I-R-NCE, paired with the three-stage sampling method from Algorithm 4. This is closely followed by I-R-NCE with the two-stage sampling method from Algorithm 2; both I-R-NCE results notably outperformnon-interpolating R-NCE. This suggests that training via I-R-NCE yields significantly superior models, evenwhen the EBM is only used at t = 1 during sampling. A reasonable hypothesis would be that training viaI-R-NCE yields multi-scale variance reduction whereby the shared network E benefits from learning acrossall times within (0, 1], similar to the benefits attributed to time-varying score modeling. We anticipate thatthe gap between the two- and three-stage sampling algorithms should widen with even more challenging",
  "r2k dist2(ck, (y[i], y[i + 1]))1{dist(ck, (y[i], y[i + 1])) rk}": "The contribution to the StochGPMP planner from the obstacle cost scales as cobs/2o. Following the gener-ation of global trajectories with the planner, we recursively split the trajectories into (Nctx + Npred)-lengthsnippets, and refine these snippets further with the StochGPMP planner, using the endpoints of the snip-pets as the start\" and goal\" locations. The collection of these snippets form the training and evaluationdatasets.",
  "For simplicity, we assume perfect state observation and perform this modeling in position space, i.e., y[i] R2": "denotes the planar coordinates of the trajectory at time i. Furthermore, we assume perfect knowledge ofthe goal g and obstacles O. We leave relaxations of these assumptions to future work. We set Nctx = 3and Npred = 10, yielding a 20-dimensional event space. In order to generate a full length trajectory, weautoregressively sample and rollout the policy. Given a prediction horizon of Npred = 10 and a total episodelength of N = 50 steps, we sample the policy 5 times over the course of an episode. In order to encourage moreoptimal trajectories from sampling, for each context/policy-step we sample 48 future prediction snippets,and take the one with the highest (relative) log-likelihood. For R-NCE and Diffusion-, this computationis a straightforward forward pass through the energy model. On the other hand, for NF and Diffusion, thiscomputation relies on the differential form of the evolution of log-probability described in . Evaluation.For evaluation, we compute the cost of the entire trajectory resulting from the autoregressiverollout as the summation of three terms: (i) goal-reaching, (ii) smoothness, and (iii) obstacle collision cost.To appropriately scale these costs in a manner consistent with the demonstration data, the evaluation costis modeled after the primary cost terms in the GP cost plus the obstacle avoidance cost, used to generatethe data:",
  "Here, g, o, and v are derived by normalizing the corresponding weights from the StochGPMP planner": "Architecture and Training.For both energy models and vector fields, we design architectures based onencoder-only transformers (Vaswani et al., 2017). Specifically, we lift all conditioning and event inputs to acommon embedding space, apply a positional embedding to the lifted tokens, pass the tokens through multipletransformer encoder layers, and conclude with a final dense accumulation MLP. However, as previouslydiscussed in .3.1, for computational reasons we make the vector field parameterizing the proposalmodel for R-NCE a dense MLP, which is significantly simpler compared to the EBM. Indeed, the proposalCNF on its own is insufficient to solve this task, but is powerful enough as a negative sampler. More specificarchitecture, training, and hyperparameter tuning details are found in Appendix B.3. Note that for IBC, wedo not use a fixed proposal distribution, as doing so results in poor quality energy models. Instead, we usethe same NF proposal distribution as used for R-NCE, trained jointly as outlined in Algorithm 1. For all models considered, we limit the parameter count to a maximum of 500k (for IBC/R-NCE, this is alimit on the EBM+NF parameters). Finally, for all models, during training, we perturbed the context and",
  "Collision Rate": ": A bar plot comparing the collision rates amongst different models, for each of the 25 test environments.For each environment, a (environment conditioned) 95% confidence interval for the collision rate is computed via theClopper-Pearson method and shown via an error bar. Furthermore, a symmetric 95% confidence interval for both thecollision rate and the cost is shown for each model in the legend, computed via the standard normal approximation.Each color corresponds to a different model, with the color legend shown in the top plot. Note that for several ofthe environments, the collision rate extends beyond the limits of the displayed plots. For all models, Environments1 and 24 are particularly challenging; we illustrate the qualitative behavior of this failure case in . Results.We focus our evaluation on two metrics: (a) collision rate, and (b) the objective cost (7.2). Ourmain finding is that R-NCE yields the sampler with both the lowest collision rate and the lowest obstacle",
  "0.0030.194 0.009": "Finally, in , we show that generating multiple ( = 48) samples per step and selecting the one withhighest log-probability has a non-trivial effect on the quality of the resulting trajectories, thus demonstratingthe importance of ranking samplings. : A study to illustrate the necessity of generating multiple samples per timestep, and selecting the samplewith the highest score.Here, we let the variable denote the number of samples which are generated at everytimestep. Symmetric 95% confidence interval for both the collision rate and the cost are computed via the standardnormal approximation. Uniformly across all models, both the collision rate and trajectory cost rise significantly goingfrom = 48 samples per step down to = 1. Recall that for Diffusion, the log-probability is computed by treatingthe model as a CNF (cf. ).",
  "Contact-Rich Block Pushing (Push-T)": "Our final task features contact-rich multi-modal planar manipulation which involves using a circular endeffector to push a T-shaped block into a goal configuration (Chi et al., 2023; Florence et al., 2022). In thisenvironment, the initial pose of the T-shaped block is randomized. The agent receives both an RGB imageobservation of the environment (which also contains a rendering of the target pose) and its current endeffector position, and is tasked with outputting a sequence of position coordinates for the end effector. The",
  "p(y[i + 1], . . . , y[i + Npred] | y[i], o[i], . . . , y[i Nctx + 1], o[i Nctx 1]).(7.3)": "Here, y[i] denotes the end effector position at timestep i, and o[i] denotes the corresponding RGB imageobservation. Following Chi et al. (2023), we set Npred = 16 and Nctx = 2, yielding a 32-dimensional eventspace. Furthermore, during policy execution, even though we predict Npred = 16 positions into the future,we only play the first 8 predictions in open loop, followed by replanning using (7.3); Chi et al. (2023, ) showed that this ratio of prediction to action horizon yielded the optimal performance on this task. Aswith the path planning example, for each policy step, we sample 8 predictions and execute the sequence withthe highest (relative) log-likelihood. Evaluation.Each rollout is scored with the following protocol. At each timestep i, the score of the currentconfiguration is determined by first computing the ratio r[i] of the area of the intersection between the currentblock pose and the target pose to the area of the block. The score s[i] is then set to s[i] = min(r[i]/0.95, 1).The episode terminates when either (a) the score s[i] = 1, or (b) 200 timesteps have elapsed. The final scoreassigned to the episode is the maximum score over all the episode timesteps. Our final evaluation is doneby sampling 256 random seeds (i.e., randomized initial configurations), and rolling out each policy 32 timeswithin each environment (with different policy sampling randomness seeds). Architecture and Training.We design two-stage architectures, which first encode the visual observations{o[i], . . . , o[i Nctx + 1]} into latent representations via a convolutional ResNet, with coordinates appendedto the channel dimension of the input (Liu et al., 2018), and spatial softmax layers at the end (Levine et al.,2016). To further improve the parameter efficiency of R-NCE and I-R-NCE, both the EBM and NF modelsshare this visual encoder. Updates to the encoder are then allowed to be propagated by either the EBM orthe NF optimization step through an appropriate stop-gradient operator. For the EBM, the spatial featuresare flattened and combined with both the agent positions and time index t (the time index of the generativemodel, not the trajectory timestep index i), and passed through encoder-only transformers similar to thearchitectures used in path planning (cf. .3).For the interpolant NF which forms the negativesampler for I-R-NCE, special care is needed to design an architecture which is simultaneously expressiveand computationally efficient; a dense MLP as used in path planning for the proposal distribution led toposterior collapse during training (cf. .3). We detail our design in Appendix B.4, in addition tovarious training and hyperparameter tuning details. For all models, we limit the parameter count to 3.3M(for I-R-NCE, this limit applies to the sum of the NF+EBM parameters).9 Finally, similar to the trainingprocedure for path planning, we employed the same annealed data perturbation schedule to guide training,with noise applied only to the sequence of end effector positions. Results.We show our results in and . Note that we do not evaluate IBC here due to itsprevious poor performance both in path planning, and poor performance shown for this task in Chi et al.(2023). contains, for each model, visualizations of policy trajectories in four different evaluationenvironments. In , we show the final evaluation score achieved by each of the models; we additionallyinclude results for I-R-NCE paired with the simpler two-stage sampling method from Algorithm 2, andthe simpler non-interpolating R-NCE (Algorithm 1). We note that the best performance is achieved by",
  "Conclusion": "We showed that in the context of policy representation, energy-based models are a competitive alternativeto other state-of-the-art generative models such as diffusion models and stochastic interpolants. This wasdone through several key ideas: (i) optimizing the ranking NCE objective in lieu of the InfoNCE objectiveproposed in the IBC work (Florence et al., 2022), (ii) employing a learnable negative sampler which is jointly,but non-adversarially, trained with the energy model, and (iii) training a family of energy models indexed bya scale variable to learn a stochastic process that forms a continuous bridge between the data distributionand latent noise. This work naturally raises several broader points for further investigation. First, while we focused on theo-retical foundations in addition to a comprehensive evaluation of R-NCE trained EBMs compared with othergenerative models, in future work we plan to deploy these EBMs on real hardware platforms. Second, thiswork investigated the use of R-NCE for training EBMs in the supervised setting, i.e., behavioral cloning. Apromising future direction would be to adapt our R-NCE algorithm and insights for training EBM policies viareinforcement learning (Levine, 2018). Finally, despite our motivations being targeted towards robotic ap-plications, many of our technical contributions actually apply to generative modeling more broadly. Anotherdirection for future research is to apply our techniques to other domains such as text-to-image generation,super-resolution, and inverse problems in medical imaging, and see if the benefits transfer over to otherdomains.",
  "Grard Biau, Benot Cadre, Maxime Sangnier, and Ugo Tanielian. Some theoretical properties of GANS.The Annals of Statistics, 48(3):15391566, 2020": "Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, andKarsten Kreis.Align your latents: High-resolution video synthesis with latent diffusion models.InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2256322575,2023. Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G. Willcocks. Deep generative modelling: A compara-tive review of VAEs, GANs, normalizing flows, energy-based and autoregressive models. IEEE Transactionson Pattern Analysis and Machine Intelligence, 44(11):73277347, 2022. Avishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. In Proceedings ofthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.10211032, 2018. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-able transformations of Python+NumPy programs, 2018. URL Omar Chehab, Alexandre Gramfort, and Aapo Hyvrinen. The optimal noise in noise-contrastive learn-ing is not what you think. In Proceedings of the Thirty-Eighth Conference on Uncertainty in ArtificialIntelligence, volume 180, pp. 307316. PMLR, 2022.",
  "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud. Neural ordinary differentialequations. In Advances in Neural Information Processing Systems, volume 31, 2018": "Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science andSystems, 2023. Bo Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le Song, and Dale Schuurmans. Exponentialfamily estimation via adversarial dynamics embedding. In Advances in Neural Information ProcessingSystems, volume 32, 2019.",
  "Gerald B. Folland. Real Analysis: Modern Techniques and Their Applications. Wiley, 1999": "Ruiqi Gao, Erik Nijkamp, Diederik P. Kingma, Zhen Xu, Andrew M. Dai, and Ying Nian Wu.Flowcontrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 75187528, 2020. Sebastian Gomez-Gonzalez, Sergey Prokudin, Bernhard Schlkopf, and Jan Peters. Real time trajectoryprediction using deep conditional generative models. IEEE Robotics and Automation Letters, 5(2):970976, 2020. Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable reversible generativemodels with free-form continuous dynamics. In International Conference on Learning Representations,2019. Will Grathwohl, Jacob Kelly, Milad Hashemi, Mohammad Norouzi, Kevin Swersky, and David Duvenaud. NoMCMC for me: Amortized sampling for fast and stable training of energy-based models. In InternationalConference on Learning Representations, 2021. Michael U. Gutmann and Jun-ichiro Hirayama.Bregman divergence as general framework to estimateunnormalized statistical models.In Proceedings of the Twenty-Seventh Conference on Uncertainty inArtificial Intelligence, pp. 283290, 2011. Michael U. Gutmann and Aapo Hyvrinen. Noise-contrastive estimation of unnormalized statistical models,with applications to natural image statistics.Journal of Machine Learning Research, 13(11):307361,2012. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.Reinforcement learning with deepenergy-based policies.In International Conference on Machine Learning, volume 70, pp. 13521361.PMLR, 2017. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor. In International Conference on MachineLearning, volume 80, pp. 18611870. PMLR, 2018.",
  "Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching forgenerative modeling. In International Conference on Learning Representations, 2023": "Bingbin Liu, Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. Analyzing and improv-ing the optimization landscape of noise-contrastive estimation. In International Conference on LearningRepresentations, 2022. Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. In Proceedingsof the 20th International Conference on Autonomous Agents and MultiAgent Systems, pp. 809817, 2021. Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski.An intriguing failing of convolutional neural networks and the coordconv solution. In Advances in NeuralInformation Processing Systems, volume 31, 2018.",
  "Raphael A. Meyer, Cameron Musco, Christopher Musco, and David P. Woodruff.Hutch++: Optimalstochastic trace estimation. In SIAM Symposium on Simplicity in Algorithms, pp. 142155, 2021": "Mustafa Mukadam, Jing Dong, Xinyan Yan, Frank Dellaert, and Byron Boots. Continuous-time gaussianprocess motion planning via probabilistic inference. The International Journal of Robotics Research, 37(11):13191340, 2018. Erik Nijkamp, Ruiqi Gao, Pavel Sountsov, Srinivas Vasudevan, Bo Pang, Song-Chun Zhu, and Ying NianWu. MCMC should mix: learning energy-based model with neural transport latent space MCMC. InInternational Conference on Learning Representations, 2022. Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio ValcarcelMacua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, and Sam Devlin. Imitating human behaviourwith diffusion models. In International Conference on Learning Representations, 2023.",
  "Tim Salimans and Jonathan Ho. Should EBMs model the energy or the score? In International Conferenceon Learning Representations (Energy Based Models Workshop), 2021": "Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neuralrepresentations with periodic activation functions. In Advances in Neural Information Processing Systems,volume 33, 2020. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International Conference on Machine Learning, volume 35, pp.22562265. PMLR, 2015.",
  "Tijmen Tieleman. Training restricted boltzmann machines using approximations to the likelihood gradient.In International Conference on Machine Learning, 2008": "Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. SE(3)-DiffusionFields: Learning cost func-tions for joint grasp and motion optimization through diffusion. arXiv preprint arXiv:2209.03855, 2022a. Julen Urain, An T. Le, Alexander Lambert, Georgia Chalvatzaki, Byron Boots, and Jan Peters. Learningimplicit priors for motion optimization. In IEEE/RSJ International Conference on Intelligent Robots andSystems, pp. 76727679, 2022b.",
  "Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variationalautoencoders and energy-based models. In International Conference on Learning Representations, 2021": "Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan,Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In InternationalConference on Machine Learning, volume 119, pp. 1052410533. PMLR, 2020. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui,and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXivpreprint arXiv:2209.00796, 2022. Yasin Yazc, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui Yap, Georgios Piliouras, and Vijay Chandrasekhar.The unusual effectiveness of averaging in GAN training. In International Conference on Learning Repre-sentations, 2019.",
  "B.1Recovering the Energy Model in Diffusion-": "The score function log p(y | x, t) in the Diffusion-EDM parameterization (Karras et al., 2022) is notrepresented directly by a neural network, but rather indirectly through a sequence of affine transformations.In order to recover (up to a constant) the corresponding function (x, y, t) = log p(y | x, t), we need to",
  "t2cin(t)(x, cin(t)y, cnoise(t)).(B.1)": "where const(x, t) is a function that is unknown, but only depends on x and t. Hence, for a fixed x X andevents {yi} Y, we can compute the relative likelihoods {r(x, yt, )}, where is a value close to zero (butnot equal to zero as (B.1) becomes degenerate at t = 0). These relative likelihood values can be used tocompare the likelihood of samples for a given context, and can be hence used to perform fast action selection.",
  "For each problem (i.e., pinwheel and spiral), the training data consists of 50,000 samples from the jointdistribution PX,Y": "Architectures and EBM Sampling.For all energy models (IBC, Diffusion-, and R-NCE-EBM, as wedo not use the interpolating variant of R-NCE for this benchmark), we model the energy function E(x, y)by first concatenating (x, y) and then passing through 8 dense layers with residual connections. Similarlyfor the NF vector field and Diffusion denoiser, which is also a vector-valued map vt(x, y), we first embedthe time argument through 2 fully connected MLP layers of width 10, and then pass the concatenatedarguments (x, y, MLP(t)) into 8 dense layers of width 64 with residual connections. Finally for parameterefficiency reasons, the R-NCE-NF vector field first concatenates z = (x, y) and then passes the inputs (z, t)into 2 ConcatSquash10 layers. Both here and throughout our experiments, we utilize the swish activationfunction (Ramachandran et al., 2017). EBM sampling (i.e., for IBC and R-NCE) is done using Algorithm 2with Langevin MCMC. and contains the hyperparameters we search over. : List of hyperparameters for the EBMs. LR denotes learning rate, WD denotes weight decay, Tmcmc refersto the number of Langevin steps taken, refers to the Langevin step size, and K denotes the number of negativeexamples. Note that K is odd here since it was more computationally efficient to sample K + 1 negative samples andreplace one with the positive sample. Note that all models considered have at most 11k NF+EBM parameters intotal.",
  "R-NCE-EBM64{103, 5 104}{0, 104}{500, 1000}{103, 5 104}9R-NCE-NF{32, 64}{103, 5 104}{0, 104}": "Training.We use a constant learning rate for all models, a batch size of 128 examples, and train the modelsfor 20,000 total batch steps. For R-NCE, 20,000 R-NCE batch steps are taken, but the negative sampler variesbetween 20,000 or 50,000 interpolant loss batch steps depending on the specific hyperparameter setting. Inparticular, we tune both the number of negative sampler updates (Tsamp in Algorithm 1, Line 5) in additionto the R-NCE updates (Trnce in Algorithm 1, Line 9) in the main loop. Specifically, recalling that Touterdenotes the total number of outer loop iterations (Algorithm 1, Line 4), we search over the grid:",
  "B.3Path Planning": "Architectures and EBM Sampling.As discussed in .3, we train architectures based onencoder-only transformers. Here, we give a more detailed description. The inputs to our energy models andvector fields are interpolation/noise-scale time11 t R, obstacles o = R103, a goal location g R2, previouspositions = (y[i Nctx + 1], . . . , y[i]) RNctx2, and future positions + = (y[i + 1], . . . , y[i + Npred]) RNpred2.We fix an embedding dimension demb, and denote the combined past and future snippets as = (, +). The architecture is depicted in , where dout is equal to the dimensionality of the eventspace (2Npred for path planning) for vector fields and is equal to one for energy models. : The encoder-only transformer architecture we utilize for path planning models. Here,refers to theswish activation function (which is also the activation function used inside the MLP blocks),is the standardsinusoidal positional embedding, and the first argument to Conv1d denotes the filter size. MH-SA denotes a standardMulti-Head Self-Attention encoder block (Vaswani et al., 2017); we use 4 self-attention heads in each block. The tokens after the terminal block are flattened and passed through the output MLP. In addition, we also consider architectures which utilize a convolutional prior which operates only on thetrajectory tokens. Namely, after is embedded into Rdemb and positionally encoded, we pass the resultingtrajectory tokens through the following two blocks:",
  "The output of the second block is then summed along the temporal axis, to produce the output of the prior.This is then added to the output of the final MLP, shown in": "Furthermore, we remark that for computational and parameter efficiency, both IBC-NF and R-NCE-NF donot use the transformer encoder architecture described in . Instead, all obstacle, goal, and trajectoryinputs are concatenated together and fed into several ConcatSquash layers. While this produces a proposaldistribution which is nowhere near optimal, it turns out to be a sufficiently powerful negative sampler forthis problem. Sampling during inference from the EBM models is done using Algorithm 2 with Langevinsampling for the MCMC stage. Training.We use a train batch size of 128. For both IBC and R-NCE, we use Touter = 250,000 main loopsteps; each loop consists of either Tsamp {2, 4} negative sampler batch steps, and 2 IBC/R-NCE updates",
  "R-NCE-EBM-64{3, 4, 5}5 104{0, 104}{500, 750}104R-NCE-NF{64, 128, 256}--103": "steps. The IBC-NF/R-NCE-NF optimizers use a cosine decay learning rate without warmup, whereas theIBC-EBM/R-NCE-EBM optimizers use a warmup cosine decay rate with 5,000 warmup steps. For both IBCand R-NCE, we use a fixed K = 63 negative samples. More hyperparameters are listed in . Onthe other hand, for Diffusion, Diffusion-, and NF, we train our models for 500,000 batch steps and use awarmup cosine decay learning rate schedule,12 with 5,000 warmup steps. The remaining hyperparametersare listed in . : List of hyperparameters for diffusion and NF path planning models. Note that in our hyperparametersweeps, we exclude combinations of demb and Nb which yield parameter accounts exceeding 500k. For sampling, allmodels use 375 Heun steps (750 total function evaluations), and 64 log-probability ODE steps (cf. ).",
  "B.4Push-T": "Architectures and EBM Sampling.For all models, we use a similar encoder-only transformer architec-ture as for path planning. The inputs to the energy models and vector fields consist of: interpolation/noise-scale time t R,13 past image observations o RNctx96963, past agent positions = (y[i Nctx +1], . . . , y[i]) RNctx2, and future positions + = (y[i + 1], . . . , y[i + Npred]) RNpred2. The image observations are first featurized into a sequence of embeddings RNctxdo via a convolutionalResNet, with coordinates appended to the channel dimension of the images (Liu et al., 2018), and spatialsoftmax layers at the end (Levine et al., 2016). We use a 4-layer ResNet, with width dres, and 4 spatial-softmax output filters, giving do = 8. The rest of the architecture resembles the self-attention architecturefrom Appendix B.3 (see for completeness). As before, we choose an embedding dimension demb; theoutput dimension dout is equal to the dimensionality of the event space, 2Npred, for vector fields, and is equalto one for energy models. As with path planning, we also add the output from a convolution prior, definedin (B.2), which operates only on the prediction tokens + post positional embedding. The dimensions dresand demb were held fixed at 64 and 128 respectively, for all models. The negative sampler NF models architecture for I-R-NCE needed to be designed differently since theself-attention layers in were computationally prohibitive for negative sampling and log-probabilitycomputations, both of which are necessary for defining the I-R-NCE objective. As a result, we deviseda novel Conv1D-Multi-Head-FiLM (CMHF) block, inspired from Perez et al. (2018), which, while inferiorto the general self-attention block, was sufficiently powerful for training via I-R-NCE and computationallycheaper. The overall EBM+NF architecture and CMHF block are shown in Figures 8 and 9, respectively.Finally, drawing inspiration from the pre-conditioning function cnoise(t) presented in Karras et al. (2022), we",
  ": The encoder-only transformer architecture for Push-T. The + tokens after the terminal block are flattenedand passed through the output MLP. Each MH-SA block used 4 attention heads": "applied the non-linear transform t log(1.1 t) on the interpolation time t used within I-R-NCE-EBMand I-R-NCE-NF. The advantage of this transform is to counteract the effect of the decreasing step-sizesfrom the transform t t1/ defined in .1. : Combined NF (top) + EBM (bottom) architecture for I-R-NCE. To eliminate parameter redundancy,both models share the context tokenizer. However, only the sampler (cf. Algorithm 3, Line 9) or the EBM (cf. Algo-rithm 3, Line 14) is allowed to update the context tokenizer; we swept over both choices. Post context tokenization,the NF blocks use a smaller feature embedding, df = demb/2 = 64. The design of the Conv1D-Multi-Head-FiLMblock is shown in . We use 4 heads within the EBMs self-attention blocks and 2 heads within the NFs CMHFblocks. The additional input q to the CMHF blocks is explained in . Sampling during inference from the EBM models is done via Algorithm 4 with HMC sampling for the MCMCstage. We split the net amount of evaluations of yE(x, y, t) between the SDE and MCMC stages using theinterpolation time lower-bound t; in particular, t Tmcmc evaluations are dedicated to the MCMC stage,while (1t)Tmcmc evaluations are given to the SDE stage. This allows us to control the net inference-timecomputational fidelity.",
  "I-R-NCE-EBM{6, 8}2 104{105, 5 106, 106}{750, 1000, 1250}{5 104, 103}{0.45, 0.5}I-R-NCE-NF{3, 4}103106---": "Training.We use a train batch size of 128 for the diffusion, NF, and I-R-NCE-NF models, and 64 forI-R-NCE-EBM. For Diffusion, Diffusion-, and NF, we train our models for 200,000 batch steps and use awarmup cosine decay learning rate schedule with 5,000 warmup steps. For I-R-NCE, we use Touter = 100,000main loop steps; each loop consists of Tsamp = 4 negative sampler batch steps, and Trnce = 2 R-NCE batchsteps. Both models used a cosine decay learning rate schedule with a warmup of 1,500 main loop steps. Wefix the number of negative samples for I-R-NCE to K = 31 and the number of interpolating times m = 5. ForI-R-NCE, we also leveraged the computation trick from by relying on 150 time-steps with a Heunintegrator for sampling from the proposal model, and sub-sampling at only 15 time-steps for computing thelog probability via trapezoidal integration. The remaining set of hyperparameters for I-R-NCE are outlinedin . Finally, we conclude by discussing the remaining set of hyperparameters for NF, Diffusion, and Diffusion-. For all three models we hold the following hyperparameters fixed: (a) WD = 106, (b) the number ofsampling ODE steps equals 512 (so 1024 total number of function evaluations), and (c) the number of logprobability ODE steps equal to 48 (cf. ). For Diffusion and Diffusion-, we use the default settingsof data, min, max, and from Karras et al. (2022). We also fix rel = 0.02 for Diffusion-. Finally, forNF, we fix = 2. The remaining hyperparameter values we sweep over are listed in .",
  "where cl() denotes the closure of a set": "Proof. For > 0, define := cl( \\ ( + B2())). Since is assumed to be in the interior of , thereexists an 0 > 0 such that for all 0 < 0, is non-empty. It is also compact by boundedness of. Let {k}k be a sequence within such that f(k) sup f(). Suppose for a contradiction thatsup f() = f . By compactness of , there exists a convergent subsequence {kj}j with limit .By continuity, f() = f , and thus . But, this means that is non-empty, a contradiction.",
  "By the compactness of , continuity of (, ) ,(x, y), and Assumption 4.2, the first event on theRHS has measure zero by the uniform converge result of Ferguson (2017, Theorem 16(a))": "On the other, the second event on the RHS also has measure zero by the strong law of large num-bers (SLLN), which simply requires the measurability of , and the existence of (0, 0).Hence,lim supn d(n, ) > is a measure zero set for every > 0 sufficiently small, establishing the re-sult.",
  "nEz[(z)] = nEz[2(z)] + op(1)(n ) + n op(1)(n )": "Now, from Theorem 4.4 and Assumption 4.9, we have that Ez[(z)] = 0.Thus, by the CLT,nEz[(z)] N(0, Varz(z)).Next, by the hypothesis (1) that n(n ) is Op(1), thelast term above is op(1). Finally, by hypothesis (4), for sufficiently large n, Ez[2(z)]+op(1) is invertible.Thus, applying Slutskys theorem yields n(n ) N(0, V), where:",
  "Ez[ n(z)] = Ez[ (z)] +Ez[2 (z)] + op(1)(n )": "To complete the result, we need only establish the following: (i) Ez[ n(z)] = 0, (ii) Ez[ (z)] = 0, and(iii) Ez[2 (z)] is invertible. The result then follows by applying CLT to nEz[ (z)] and Slutskystheorem (see for instance, the proof for Theorem 4.10). We show (ii) and (iii) first. For (ii), notice that by the definition of Stackelberg optimality, minimizes L(, ) over and byassumption, is contained within the interior of .Thus, L(, ) = 0.Further, by Theorem 4.4, maximizes L(, ) over for any , and is also assumed to be within the interior of . Thus,L(, ) = 0. Leveraging C1-regularity, we conclude that Ez[ (x)] = 0.",
  "Then, (C.11) along with nn a.s. proves the stated claim": "Returning now to (i), since we only assume that (n, n) is a Stackelberg optimal point, it may not satisfy thestationarity condition Ez[ n(z)] = 0 (Jin et al., 2020). However, since n globally minimizes Ln(n, )and n a.s., which is at an interior point, for large enough n we have Ln(n, n) = 0. Furthermore, by Claim C.7 and the assumption that 2L(, ) is non-singular, for large enough n wehave that 2Ln(n, n) is also non-singular. By the implicit function theorem, there exists a continuouslydifferentiable function Gn : , defined for in an open local neighborhood of n, s.t. Gn(n) = nand Ln(, Gn()) = 0. Then, since n is a global interior maximum (for large enough n, since n a.s.) of inf Ln(, ), which in turn equals Ln(, Gn()) for near n, stationarity dictates thatLn(n, Gn(n)) = 0. It follows then by the chain rule that Ln(n, n) = 0."
}