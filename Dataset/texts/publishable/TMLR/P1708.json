{
  "Abstract": "Pre-training is a widely used approach to develop models that are robust to distributionshifts. However, in practice, its effectiveness varies: fine-tuning a pre-trained model improvesrobustness significantly in some cases but not at all in others (compared to training fromscratch).In this work, we seek to characterize the failure modes that pre-training canand cannot address. In particular, we focus on two possible failure modes of models underdistribution shift: poor extrapolation (e.g., they cannot generalize to a different domain) andbiases in the training data (e.g., they rely on spurious features). Our study suggests that, asa rule of thumb, pre-training can help mitigate poor extrapolation but not dataset biases.After providing theoretical motivation and empirical evidence for this finding, we explore twoof its implications for developing robust models: (1) pre-training and interventions designedto prevent exploiting biases have complementary robustness benefits, and (2) fine-tuningon a (very) small, non-diverse but de-biased dataset can result in significantly more robustmodels than fine-tuning on a large and diverse but biased dataset.1",
  "Introduction": "A common paradigm for developing machine learning models is pre-training them on a large, diverse dataset(e.g., ImageNet (Deng et al., 2009), JFT-300M (Sun et al., 2017), LAION-5B (Schuhmann et al., 2022)) andthen fine-tuning them on task-specific data. Indeed, compared to training from scratch, fine-tuning a pre-trained model often significantly improves performance and reduces computational costs (Sharif Razavianet al., 2014; Sun et al., 2017; Kornblith et al., 2019). Yet another benefit that pre-training may offer is distribution shift robustness. Specifically, machine learningmodels tend to suffer from distribution shifts, i.e., changes between the reference distribution used to developthe model and the shifted distribution that the model actually encounters when deployed. For example, atumor identification model trained on tissue slide images from one hospital might perform poorly whendeployed at another hospital (Bandi et al., 2018; Koh et al., 2020). Notably, different models (with differentarchitectures, hyperparameters, etc.) tend to be similarly sensitive to a given distribution shift. However,models pre-trained on auxiliary data and then fine-tuned on the reference distribution can break this trend,exhibiting substantially higher performance on the shifted distribution than models trained from scratch withthe same performance on the reference distribution (Taori et al., 2020; Miller et al., 2020; 2021; Andreassenet al., 2021; Wortsman et al., 2021). These robustness benefits of pre-training are promising, but they are not universal.In particular, fine-tuning the same pre-trained model can yield significant robustness gains on some distribution shifts but not",
  "Published in Transactions on Machine Learning Research (01/2025)": "with in-support and out-of-support shifts, respectively) and provide a method for measuring the amountof each type of shift in a given distribution shift (similar to our method for dividing a distribution shiftinto in-support and out-of-support splits). Subpopulation shift (and its sub-types), shifts involving spuriouscorrelations, covariate shift, and label shift are typically in-support.However, there are exceptions; forexample, some works consider subpopulation shifts in which a subpopulation does not appear in the referencedistribution (Santurkar et al., 2021; Yang et al., 2023), which are out-of-support. Domain generalizationproblems are nearly always out-of-support and extrapolating effectively outside of the reference distributionis often a key challenge of these tasks.",
  "Can we identify and characterize the failure modes that pre-training can and cannot address?": "Recall that under distribution shift, models can fail in a number of ways. One of them is their inability toextrapolate effectively outside of the reference distribution (Gulrajani & Lopez-Paz, 2020; Koh et al., 2020).If, for instance, a model is trained only on photos taken during the day, then it might fail when deployed onphotos taken at night. Models can also underperform even when the shifted distribution does not contain anything new. In par-ticular, they can fail due to biases in the reference distribution. For example, if a certain feature is spuriouslycorrelated with the label in the reference distribution, a model might learn to exploit this relationship andfail on examples encountered during deployment where it does not hold (Arjovsky et al., 2019; Geirhos et al.,2020).",
  "Our contributions": "To identify the failure modes that pre-training can address, we study the robustness benefits of pre-trainingunder two types of distribution shifts: (1) shifts where extrapolation is necessary and (2) shifts whereextrapolation is not needed. We start by analyzing a simple logistic regression setting and illustrate whypre-training might improve robustness to the former type of shift, but not the latter ().Wesubsequently build on this intuition by measuring the robustness benefits of pre-training on synthetic andnatural distribution shifts of each type (). Our results suggest the following rule of thumb: pre-training can help with extrapolation, but does not address other failures, for example, those stemming fromdataset biases.",
  "Implications for developing robust models.Guided by this rule of thumb, we explore two relatedavenues for harnessing pre-training to develop robust models": "1. Combining pre-training with interventions designed to handle bias (): There are a numberof robustness interventions specifically designed to mitigate biases present in a training dataset (Byrd& Lipton, 2019; Sagawa et al., 2020a; Liu et al., 2021; Kirichenko et al., 2022; Idrissi et al., 2022).Our findings suggest that pre-training and this kind of intervention address two different sources offailures (the former helping with extrapolating and the latter with avoiding dataset biases) and thusmay be viewed as complementary. We indeed find that combining them can yield models with bothsets of benefits. 2. Curating datasets for fine-tuning (): One possible intervention that aims to address datasetbiases is curating a de-biased dataset. In general, however, the de-biasing process might be pro-hibitively expensive. That said, we find that if we leverage pre-training to help with extrapolation,we might only need a small, non-diverse fine-tuning dataset; such a dataset might actually be fea-sible to de-bias. For example, we demonstrate that fine-tuning on a carefully de-biased hair colorclassification dataset with only 64 examples yields greater robustness than fine-tuning on the entireCelebA dataset (Liu et al., 2015).",
  "Background": "Fine-tuning a pre-trained model.Methods for fine-tuning a pre-trained model vary: two commonstrategies are full fine-tuning, in which one continues training the entire model, and linear probing, in whichone only fine-tunes the final layer. Some recent pre-trained models with natural language supervision (e.g.,CLIP (Radford et al., 2021), ALIGN (Jia et al., 2021)) can also be adapted to a downstream task in azero-shot context (i.e., without fine-tuning) by specifying the task through a text description. In this work,",
  "Pre-training can significantly improve robustness to some distribution shifts but not others": "To illustrate this, we consider two distribution shifts of ImageNet (Deng et al., 2009): ImageNet-V2 (Rechtet al., 2019) and ImageNet Sketch (Wang et al., 2019). For each of these shifts, we measure the effectiverobustness (see ) of various pre-trained models. Specifically, we first establish a baseline for robust-ness by evaluating 78 models trained from scratch on ImageNet (from PyTorch Image Models (Wightman,2019)). We observe a strong linear relationship between their reference and shifted accuracies (see ). Next, we evaluate 55 pre-trained models that are fine-tuned on ImageNet (also from PyTorch Image",
  "Positive exampleNegative exampleModel decision boundaryOptimal decision boundary": ": Illustration of logistic regression setting. (a) Consider a reference dataset that lies within asubspace Wref of Rd. (b) Models trained from different initializations all learn the same (optimal) decisionboundary in Wref, but may behave differently outside of Wref. (c) Under shifts within Wref, models with dif-ferent initializations are equally robust. (d) Under shifts outside of Wref, initialization can affect robustness.",
  "Models) and measure the improvements in shifted accuracy over the linear trend. See Appendix B.2 for theexact setup": "We find that while some of the pre-trained models exhibit substantial effective robustness on ImageNetSketch, they all exhibit very little effective robustness on ImageNet-V2. These pre-trained models representa wide variety of model architectures, pre-training datasets and pre-training algorithmsthe largest modelhas 1 billion parameters and is pre-trained on a dataset of 2 billion image-text pairs. Yet, the highest effectiverobustness attained by any of these models on ImageNet-V2 is just 1.80%. This suggests that pre-trainingalone might not suffice to address certain types of failures that occur under distribution shift. We would liketo better understand this limitation; can we identify and characterize these types of failures?",
  "Studying Pre-Training in a Logistic Regression Setting": "Our central goal is to understand the failure modes that pre-training can and cannot address. To this end,we first study the robustness benefits of pre-training in a simple logistic regression setting (see ). Setup.Suppose that we are given access to a reference dataset Sref of input-label pairs, each consisting ofa d-dimensional input x Rd and a binary label y {1, 1}. We are concerned with finding weights w Rd",
  ". The logistic loss Lref has a minimum value. This condition ensures that minimizing Lref iswell-defined. Note that there may be multiple weights that attain this minimum value": "Starting with initial weights winit (which, in our case, are either random or the result of pre-training), supposethat we use gradient descent to minimize Lref(w). We would like to understand how well the resulting modelperforms under distribution shift. In particular, what role does pre-training play through winit? To answerthis question, we establish the following theorem (proof in Appendix A):",
  "Label": ": Examples of in-support and out-of-support shifts.One example of an in-support shift(left) is a shift in which the indoor/outdoor frequencies of animal appearances change, but the possiblecombinations of animal and setting remain the same. An example of an out-of-support shift (right) is a shiftfrom day to night: the nighttime setting is entirely novel. Theorem 4.1. Suppose that we start with initial weights winit Rd and run gradient descent to minimizeLref(w). With an appropriately chosen learning rate, gradient descent converges to weights w that minimizeLref. Furthermore, w can be written as",
  "Here, wref is a property of the reference dataset Sref and lies within the reference subspace Wref. Meanwhile,projW refwinit is the component of winit that is orthogonal to Wref": "To prove Theorem 4.1, we will first show that running gradient descent starting from an initialization withinWref always converges to the same weights wref. We will then show that running gradient descent startingfrom an arbitrary initialization has the same convergence behavior except for an offset term projW refwinitrepresenting the component of the initialization that is orthogonal to Wref.",
  "Exploring the Empirical Robustness Benefits of Pre-Training": "In , we found that in a simple logistic regression setting, pre-training helps specifically with ex-trapolation. We now want to assess whether this principle holds more broadly. To do so, we measure therobustness benefits of pre-training under two types of shifts: in-support shifts, where models cannot fail dueto poor extrapolation (but might fail for other reasons, e.g., dataset biases), and out-of-support shifts, wheremodels can fail due to poor extrapolation (see ). We begin by describing these two types of shifts inmore detail and providing intuitions for why pre-training might improve robustness to out-of-support shifts,but not in-support shifts. In-support shift.A distribution shift is in-support if any input that could be sampled from the shifteddistribution could also be reasonably sampled from the reference distribution. In other words, the shifteddistribution does not contain anything new; however, an in-support shift can still cause failures if, forexample, the reference distribution is biased. To illustrate this failure mode, consider a cat vs. dog imageclassification task in which photos are either taken indoors or outdoors.Suppose that in the referencedistribution 90% of cats appear indoors and 90% of dogs appear outdoors (i.e., the setting is spuriouslycorrelated with the animal). A model trained on this distribution would likely rely (at least in part) on",
  "Constructing synthetic in-support and out-of-support shifts": "We now want to measure the robustness gains that pre-training provides on in-support and out-of-supportshifts. To this end, we explicity construct two shifts of each type by modifying ImageNet (Deng et al., 2009):(1) a spurious tint shift in which we add a tint that is spuriously correlated with the label in the referencedataset, but not in the shifted dataset, (2) a label shift in which the relative frequencies of classes changebetween the reference and shifted datasets, (3) an unseen tint shift in add a random tint in the shifteddataset, and (4) a flip shift in which we vertically flip images in the shifted dataset (see the top of for visualizations).",
  "(b) Average effective robustness of 55 pre-trained modelson each split of each of the three shifts. Error bars denote95% confidence intervals": ": Dividing shifts of ImageNet into in-support and out-of-support splits. We divide each ofthe ImageNet-V2, ImageNet Sketch and ImageNet-R datasets into an in-support split containing examplesthat look like ImageNet examples and an out-of-support split containing examples that look unlike ImageNetexamples (see Appendix B.4 for a description of the splitting method). We display samples from each split ofImageNet Sketch in a and report the average effective robustnesses of pre-trained models in b. See Appendix C.2.3 for scatterplots of reference vs. shifted accuracy. For each shift, as a baseline, we train a ViT-B/32 (Dosovitskiy et al., 2021) model from scratch on thereference dataset. We evaluate this model at different epochs and find a strong linear relationship betweenreference and shifted accuracy, i.e., the accuracy on the line phenomenon occurs3 (see ). Next,we fine-tune pre-trained ViT-B/32 models and measure their effective robustness above this baseline. Weconsider two pre-trained models: CLIP (Radford et al., 2021) and AugReg (Steiner et al., 2021), and three(full) fine-tuning strategies: standard full fine-tuning (FT), linear probing followed by full fine-tuning (LP-FT) and zero-shot initialization followed by full fine-tuning (ZS-FT). We select fine-tuning hyperparametersthat maximize accuracy on the reference distribution (in Appendix C.1.1, we find that other reasonablehyperparameter choices yield similar robustness). We observe that pre-trained models exhibit substantial effective robustness on out-of-support shifts, buthave close to zero effective robustness on in-support shifts (see ). In Appendix C.1.2, we vary thestrength of the biases in the in-support shifts and find that the effective robustness of pre-trained modelsremains close to zero. See Appendix B.3 for a description of the exact setup.",
  "Dividing natural shifts into in-support and out-of-support splits": "So far, we have constructed synthetic in-support and out-of-support shifts and observed that pre-training cansignificantly improve robustness to the latter but not the former. Now, we demonstrate that this principleseems to extend to natural shifts as well. Note that it is hard to find natural shifts that are purely in-support. After all, under natural shifts the shifted dataset may contain some inputs that are similar tothose in the reference dataset and some that are not. For example, in a shift from photos to sketches, somesketches may look more photorealistic but most would probably be clearly distinguishable from photos. Tobe able to measure robustness to each type of shift, we thus divide several natural shifted datasets each intoan in-support split containing inputs that look like they could have come from the reference dataset andan out-of-support split containing the remaining inputs. We do so by training a classifier to distinguishbetween the reference and shifted datasets and using this classifier to approximate the probability of samplinga given shifted example from the reference distribution (see Appendix B.4.1 for details).",
  "Combining Pre-Training with Interventions for Handling Bias": "Our observations in suggest that pre-training indeed can help prevent failures caused by poorextrapolation but not those stemming from biases in the reference dataset.How, then, can we developmodels that avoid both failure modes? In this section, we explore one possible strategy: combining pre-training with interventions specifically designed to handle dataset biases. In particular, we investigate the effectiveness of this strategy on WILDS-FMoW (Christie et al., 2018; Kohet al., 2020), a distribution shift benchmark for classifying satellite images (in Appendix C.3.1, we provide a 4We also explored ObjectNet (Barbu et al., 2019) and ImageNet-Vid-Robust (Shankar et al., 2019) but our splitting methodmarks fewer than 50 examples from these shifted datasets as in-support, and thus we cannot reliably measure in-supportaccuracy.",
  "Curating Datasets for Fine-Tuning": "In , we explored pairing pre-training with interventions specifically designed to address datasetbiases. We observed that this strategy can be effective for developing models that both extrapolate effectivelyand avoid undesirable biases present in the reference distribution. In this section, we highlight one such intervention: training on a carefully curated (and, in particular, de-biased) dataset instead of the original reference dataset. In general, de-biasing a large and diverse datasetmay be prohibitively expensive. However, if we can rely on pre-training for extrapolation (as suggested in), we might only need a small, non-diverse fine-tuning dataset, which would be more feasible tode-bias. Thus, curating such a dataset and then fine-tuning a large pre-trained model on it might be arelatively inexpensive method for developing robust and performant models. As a case study, we consider the task of predicting hair color (blond vs. non-blond) in the CelebA dataset(Liu et al., 2015). In this dataset, hair color is spuriously correlated with other attributes (especially gender).For example, 24% of females are blond, while only 2% of males are blond. Following works studying grouprobustness (Sagawa et al., 2020a; Liu et al., 2021; Kirichenko et al., 2022), we measure worst-group accuracyto assess robustness rather than measuring accuracy on an explicit shifted dataset. In this case, the fourgroups are blond females, non-blond females, blond males and non-blond males. A model exploiting thespurious correlation between gender and hair color would likely perform poorly on the underrepresentedgroup of blond males. Curating a de-biased dataset.To curate a de-biased dataset for hair color classification with n examples,we construct a counterfactual example for each of n/2 CelebA examples by changing the persons hair to acolor corresponding to the opposite class (i.e., blond to non-blond and vice versa). We ensure that attributesbesides hair color remain unchanged and include both the original and edited images in our dataset. Hence,attributes that are spuriously correlated with hair color in the CelebA dataset (e.g., gender, age) are equallyrepresented in the blond and non-blond populations of our curated dataset. To illustrate that this dataset",
  "Non-blondBlondCounterfactual (using image editing)": "(a) CelebA vs. our curated hair color classificationdataset.In the CelebA dataset (top), attributes suchas gender are spuriously correlated with the class (blondvs.non-blond).In our much smaller curated dataset(bottom), every real image is paired with a synthesizedcounterfactual example of the other class. As a result,the primary difference between the blond and non-blondpopulations is hair color; other attributes such as gender,age and hair style are not predictive.We include onlyfemales in our dataset to illustrate that diversity mightnot be necessary for robustness when fine-tuning.",
  "Accuracy": "Worst-group accuracy ER = 5.74% ER = 58.04% Avg. ER = 55.58% Accuracy vs. worst-group accuracy BaselineBaseline linear fity = x Pre-trainedBalanced (n = 64,, 16384) Pre-trained and Balanced (n = 64) (a) Fine-tuning on a balanced female-only dataset.Fine-tuning a pre-trained model on the CelebA dataset(orange) yields little effective robustness over a baselineof models trained from scratch (blue).However, fine-tuning the same pre-trained model on just 64 examplesfrom a balanced female-only dataset (red) yields a modelwith both high effective robustness and high accuracy.Training from scratch on a balanced female-only dataset(green) also yields high effective robustness, but resultsin substantially lower accuracy than pre-trained models,even with many more examples. Error bars denote 95%confidence intervals over 64 random trials. Female balanced accuracy Male balanced accuracy Female balanced vs. male balanced accuracy y = x Balanced (n = 64,, 16384) Balanced (n = 64,, 16384) linear fit Pre-trained and Balanced (n = 64) (b) Comparingextrapolationfromfemalestomales of pre-trained models and models trainedfrom scratch. We plot the balanced accuracy on malesagainst the balanced accuracy of females of a pre-trainedmodel fine-tuned on a balanced female-only dataset (red)and models trained from scratch on this dataset (green).Models trained from scratch establish a linear relationshipbetween male and female balanced accuracy; however, thepre-trained model outperforms this trend, suggesting thatit more effectively extrapolates to males from the female-only reference dataset. : Fine-tuning a pre-trained model on a small, non-diverse but de-biased dataset (in this case,a class-balanced female-only dataset) yields a robust and performant model for hair color classification inCelebA (see b).",
  ": Fine-tuning a pre-trained model on a small, non-diverse but de-biased dataset (see a)yields a robust and performant model for hair color classification in CelebA (see b)": "does not need to be diverse to yield high robustness and performance when fine-tuning, we restrict thedataset to include only females. See a for a visualization of the dataset and Appendix B.6 for theimage editing process. In Appendix C.4.2, we consider the simpler curation strategy of balancing the numberof samples from each group (Idrissi et al., 2022) and find that counterfactual image editing is more effective. Fine-tuning on a de-biased dataset.As expected, models trained from scratch on the CelebA datasetexhibit high accuracy but very low worst-group accuracy, likely because they rely on gender to predicthair color (see b). Furthermore, a pre-trained CLIP ViT-B/32 model fine-tuned on the CelebAdataset exhibits very little effective robustness above these models trained from scratch, consistent with ourhypothesis that pre-training does not mitigate dataset biases. However, we observe that fine-tuning the samepre-trained model on just 64 examples from our curated dataset yields a model with both high accuracy andeffective robustness. Finally, we also train models from scratch on our curated dataset and find that theyexhibit substantial effective robustness, but require many more examples to attain a comparable accuracy.This suggests that the extrapolation benefits of pre-training are key to make effective use of our small, non-diverse curated dataset. In particular, as we illustrate in Appendix C.4.1, pre-trained models extrapolatefrom the female-only curated dataset to males better than models trained from scratch.",
  "Related Work": "Characterizing distribution shifts.There exists a plethora of definitions for characterizing distributionshifts, many of which are aligned with the in-support and out-of-support chracterizations that we discuss inthis work. For example, domain generalization involves shifts in which the reference and shifted distributionsare from different domains (Koh et al., 2020; Gulrajani & Lopez-Paz, 2020).In a subpopulation shift,subpopulations appear with different frequencies in the reference and shifted distributions (Santurkar et al.,2021; Koh et al., 2020; Yang et al., 2023). In shifts with spurious correlations, certain features are predictivein the reference distribution but not in the shifted distribution (Arjovsky et al., 2019; Sagawa et al., 2020b).Two more formal characterizations are covariate shift (Shimodaira, 2000), under which p(y|x) is fixed, andlabel shift (Lipton et al., 2018), under which the label distribution may change but p(x|y) is fixed. We relatethese definitons to in-support and out-of-support shifts in Appendix D.4. Robustness benefits of pre-training.Several works have suggested that pre-training can be an effectivestrategy for improving robustness to distribution shifts (Hendrycks et al., 2019; 2020a;b; Tu et al., 2020; Taoriet al., 2020; Miller et al., 2021; Wiles et al., 2021; Andreassen et al., 2021; Bommasani et al., 2021; Liu et al.,2022b; Ramanujan et al., 2023). In particular, Wiles et al. (2021) define different types of distribution shiftsand find that pre-training frequently improves performance under these shifts, while most other interventionsprimarily help in specific settings. In the natural language processing setting, Tu et al. (2020) argue thatwhen pre-training helps with spurious correlations, it is because pre-trained models can generalize betterfrom the small number of counterexamples to these correlations; as we discuss in Appendix D.5, this isconsistent with our intuition that pre-training helps specifically with extrapolation.Lastly, Bommasaniet al. (2021) discuss failure modes that pre-training is unlikely to address including spurious correlations(both in pre-training and fine-tuning datasets) and extrapolation across time.",
  "Conclusion": "In this work, we study the failure modes that pre-training alone can and cannot address. Our findingssuggest that pre-training can help mitigate failures caused by poor extrapolation (e.g., inability to generalizeto a new domain) but might not address other failures, such as those stemming from dataset biases. Inlight of this observation, dataset biases present a fundamental limitation that cannot be overcome by simplyleveraging additional pre-training data or larger models. We thus encourage practitioners not to treat pre-training as a panacea for robustness. Instead, they should consider the specific failures modes they mightencounter to determine if pre-training can help.",
  "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXivpreprint arXiv:1907.02893, 2019": "Peter Bandi, Oscar Geessink, Quirine Manson, Marcory Van Dijk, Maschenka Balkenhol, Meyke Hermsen,Babak Ehteshami Bejnordi, Byungjae Lee, Kyunghyun Paeng, Aoxiao Zhong, et al. From detection ofindividual metastases to classification of lymph node status at the patient level: the camelyon17 challenge.IEEE Transactions on Medical Imaging, 2018. Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenen-baum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of objectrecognition models. In Neural Information Processing Systems (NeurIPS), 2019.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In Computer Vision and Pattern Recognition (CVPR), 2009": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. In International Conference on Learning Rep-resentations (ICLR), 2021. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix A Wichmann.Shortcut learning in deep neural networks.In Nature Machine In-telligence, 2020.",
  "Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson.Last layer re-training is sufficient forrobustness to spurious correlations. arXiv preprint arXiv:2204.02937, 2022": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, et al.Wilds: A benchmark ofin-the-wild distribution shifts. arXiv preprint arXiv:2012.07421, 2020. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and NeilHoulsby. Big transfer (bit): General visual representation learning. arXiv preprint arXiv:1912.11370,2019.",
  "Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with blackbox predictors. In International conference on machine learning, pp. 31223130. PMLR, 2018": "Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang,and Chelsea Finn. Just train twice: Improving group robustness without training group information. InInternational Conference on Machine Learning, 2021. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.Aconvnet for the 2020s.In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1197611986, 2022a. Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan. An em-pirical study on distribution shift robustness from the perspective of pre-training and data augmentation.arXiv preprint arXiv:2205.12753, 2022b.",
  "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalizeto imagenet? In International Conference on Machine Learning (ICML), 2019": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural net-works for group shifts: On the importance of regularization for worst-case generalization. In InternationalConference on Learning Representations, 2020a. Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang.An investigation of why overpa-rameterization exacerbates spurious correlations. In International Conference on Machine Learning, pp.83468356. PMLR, 2020b.",
  "Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihoodfunction. Journal of statistical planning and inference, 2000": "Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer.How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprintarXiv:2106.10270, 2021. Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectivenessof data in deep learning era. In Proceedings of the IEEE international conference on computer vision,2017.",
  "Ross Wightman. Pytorch image models. 2019": "Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre Alvise-Rebuffi, Ira Ktena, Krishnamurthy Dvijotham,and Taylan Cemgil. A fine-grained analysis on distribution shift. arXiv preprint arXiv:2110.11328, 2021. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, RaphaelGontijo-Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robustfine-tuning of zero-shot models. In arXiv preprint arXiv:2109.01903, 2021.",
  ". Strict convexity on Wref. Next, to show that Lref is strictly convex on Wref, we need to showthatLref(v) > Lref(u) + Lref(u)(v u)": "for any u, v Wref. Using (5), it suffices to show that a2Lref(w)a > 0 for any non-zero a Wrefand w Wref. We know that a2Lref(w)a = D(w)1/2Xa22. Since D(w) is diagonal with positiveentries along the diagonal, D(w)1/2Xa22 > 0 if and only if Xa = 0. Recall that Wref is the subspacespanning the rows of X. Hence, since a is non-zero and is in Wref, we know that Xa = 0.",
  "v u22": "for any u, v T. Using (5), it suffices to show that there exists an m > 0 such that a2Lref(w)a >m2 a22 for any a Wref and w T. Making use of the fact that T is closed, let min be theminimum diagonal entry of D(w) for w T, that is,",
  "A.1.2Convergence of gradient descent within the reference subspace": "Next, we establish that there exists a unique minimizer of Lref within the reference subspace Wref (LemmaA.3) and that gradient descent converges to these weights (Lemma A.4).Lemma A.3. There exists a unique wref Wref such that wref arg minw L(w). Proof. We will first show that there exists a wref Wref such that wref arg minw Lref(w).Let w arg minw Lref(w) be an arbitrary minimimum point of Lref. By definition, for every (x, y) Sref, x Wref.Hence, for every such x, wx = projWrefwx. This means that Lref(w) = Lref(projWrefw), which impliesthat wref := projWrefw arg minw Lref(w), as desired. Next, because Lref is strictly convex on Wref (Lemma",
  "Lemma A.4. If we start with winit Wref and run gradient descent with = 4/X2op to minimize Lref(w),the weights will converge to wref": "Proof. Suppose that we start with initial weights winit Wref and run gradient descent to minimize Lrefwith learning rate . In particular, let w(0) = winit and w(t+1) = w(t) + Lref(w(t)). Because Lref isconvex (Lemma A.1), Lref is K-Lipschitz with K = X2op/4 (Lemma A.2), and = 4/X2op 1/K, weknow from Theorem 3.2 of Bubeck (2014) that",
  "t 1.(6)": "Hence, the loss attained by w(t) converges to the optimal loss attained by wref. To show that w(t) convergesto wref, we will show that Lref is strongly convex on a set containing every w(t) for t 0. In particular,consider the set WGD = {w Wref | wwref2 winitwref2} containing weights in Wref at least as closeto wref as winit. Clearly, WGD contains w(0) = winit. We know from Theorem 3.2 of Bubeck (2014) that witheach iteration of gradient descent we get closer to a minimum point, that is, w(t+1) wref w(t) wref.Additionally, because winit and Lref are in Wref, every w(t) is in Wref. Hence, every w(t) is in WGD. BecauseWGD is closed and convex, from Lemma A.1 we know that Lref is strongly convex on WGD. This means thatthere exists an m > 0 such that",
  "A.1.3Proof of Theorem 4.1": "We are now ready to prove Theorem 4.1. Suppose that we start with initial weights winit and run gradientdescent to minimize Lref with learning rate = 4/X2op. In particular, let w(0) = winit and w(t+1) =w(t) + Lref(w(t)) for t 0. We will show that running gradient descent starting with an arbitrarywinit has the same behavior as running gradient descent with winit projected onto Wref. To be more precise,suppose that we instead start with initial weights projWrefwinit when running gradient descent. In particular,with projW u denoting the projection of u onto a subspace W, let wproj(0) = projWrefwinit and wproj(t+1) =wproj(t) + Lref(wproj(t)) for t 0. Then the trajectory of w(t) is the same as that of wproj(t) but with anadditional component projW refwinit = (winit projWrefwinit). That is,",
  "B.1.2Measuring effective robustness": "Effective robustness.In this work, we quantify the robustness of pre-trained models using effectiverobustness (ER), a measure of the robustness a model above the baseline of models trained from scratch(Taori et al., 2020). Computing this metric first involves establishing a relationship between the accuraciesof baseline models (in our case, models trained from scratch on a reference dataset).In particular, letAccref(M) and Accshift(M) denote the accuracies of a model M on test datasets drawn from the referenceand shifted distributions, respectively. Given a set Mbaseline of baseline models, we compute a linear fitrelating 1(Accref(M)) and 1(Accshift(M)), where 1 is the probit function (i.e., the inverse cumulativedistribution function of the standard normal distribution). We compute a linear fit relating probit-scaledaccuracies (instead of the accuracies themselves) because this has been empirically observed to improve thestrength of the linear relationship (Miller et al., 2021; Taori et al., 2020). Formally, we compute parametersa and b such that",
  "Effective": "robustness (ER) y = x BaselineBaseline linear fitPre-trained : Visualization of effective robustness. To compute effective robustness (ER), we first estab-lishes a linear relationship between the (probit-scaled) accuracies of baseline models (blue) on the referenceand shifted datasets. The effective robustness (green) of a pre-trained model (orange) is the amount bywhich its actual accuracy on the shifted dataset exceeds the prediction of the linear trend.",
  "B.2The robustness benefits of pre-training vary": "In , we illustrate that pre-trained models exhibit substantial effective robustness on the ImageNetSketch distribution shift but very little effective robustness on the ImageNet-V2 distribution shift.Weconsider 78 models trained from scratch on ImageNet and 55 pre-trained models fine-tuned on ImageNet, alltaken from PyTorch Image Models (Wightman, 2019). The pre-trained models represent a variety of modelarchitectures (e.g., ResNet (He et al., 2015), ConvNeXt (Liu et al., 2022a), ViT (Dosovitskiy et al., 2021)),pre-training datasets (e.g., IG-1B (Mahajan et al., 2018), LAION-2B (Schuhmann et al., 2022), OpenAIsWIT (Radford et al., 2021)), and pre-training algorithms (e.g., supervised learning, CLIP (Radford et al.,2021)). The complete list of models used is available with our code at",
  "Specifications of synthetic shifts.Here, we provide detailed descriptions of the four synthetic distribu-tion shifts (see for visualizations)": "1. Spurious tint shift (in-support): We tint images (i.e., replace each pixel with a mix of the originalvalue, with weight 0.75 and a specific color, with weight 0.25) such that the tint is correlated with thelabel in the reference distribution but not in the shifted distribution (i.e., tint is a spurious feature).Specifically, in the reference distribution we apply tint with a class-specific color to pspurious = 0.5 ofexamples and a tint with a random color to the remaining 1pspurious = 0.5 of examples. Meanwhile,in the shifted distribution we apply a tint with a random color universally. 2. Label shift (in-support): Label shift is a commonly studied type of distribution shift in which therelative frequencies of classes change, but p(x|y) is fixed. To construct a label shift, we sub-sampleImageNet such that in the reference distribution, a randomly selected 500 classes are less likely toappear than the remaining 500 classes. In particular, the selected classes appear with probabilitypminority = 0.2, while the remaining classes appear with probability 1pminority = 0.8. In the shifteddistribution, these relative frequencies are reversed.",
  ". Flip shift (out-of-support): We vertically flip images in the shifted distribution": "Shared model specifications.When training, we use the FFCV implementation of RandomResizedCro-pRGBImageDecoder, resizing image crops to a resolution of 224 224. For data augmentation, we use theFFCV implementations of RandomHorizontalFlip. When evaluating, we use the FFCV implementation ofCenterCropRGBImageDecoder with a ratio of 224/256, resizing image crops to a resolution of 224 224. Specifications of baseline models.As a baseline, we train a ViT-B/32 model (the implementationof Ilharco et al. (2021)) from scratch on ImageNet. We run AdamW for 100 epochs, using a cosine learningrate schedule with a peak learning rate of 0.003 and 10 warmup epochs, a batch size of 512, a weight decayof 0.1, label smoothing of 0.1 and gradient clipping at global norm 1. To establish a baseline for effectiverobustness, we evaluate this model at epochs 50 through 85 (we stop at 85 because the models accuracy at",
  "B.4.1Splitting a Shifted Dataset": "To split a shifted dataset into an in-support split and an out-of-support split, we would ideally measurethe reference distribution probability density pref of inputs in the shifted dataset and assign inputs withsmall pref to the out-of-support split. Unfortunately, it is difficult to estimate pref directly when dealing withhigh-dimensional inputs (in this case, images). Instead, we estimate the probability density ratio pref/pshift,that is, how much more likely an input is under the reference distribution than under the shifted distribution.We then assign examples in the shifted dataset with pref/pshift < 0.2 to the out-of-support split and exampleswith pref/pshift 0.2 to the in-support split. We visualize examples in . Estimating pref/pshift.To estimate pref/pshift, we use a classifier trained to distinguish between examplesfrom the reference and shifted datasets.Specifically, let p be a probability mass/density function overexamples that can either be drawn from Dref or Dshift (i.e., p represents the distribution of a dataset createdby joining a reference dataset and a shifted dataset). Next, let yref be the event that an example is drawnfrom Dref and yshift be the event that an example is drawn from Dshift. We can express the ratio pref/pshiftas follows:",
  "p(yref)": "The terms p(yref) and p(yshift) are easy to estimate since they are simply the proportions of reference andshifted examples in p. Hence, to estimate pref/pshift we just need to estimate p(yref|x) and p(yshift|x). To do so, we train a classifier to distinguish between reference and shifted examples on a dataset drawnfrom p. We construct such a dataset by combining 100K samples from ImageNet with each of the shifteddatasets (for ImageNet-R, which contains a subset of the classes of ImageNet, we restrict the 100K samplesto these classes). Next, we fine-tune a CLIP ViT-L/14 pre-trained on LAION-2B from OpenCLIP (Ilharcoet al., 2021) to distinguish between reference and shifted examples. We first fine-tune just the final layerwith a learning rate of 0.1 and then fine-tune the entire model with the best learning rate selected from2 104, 1 104, 5 105, 2 105, 1 105, 5 106, 2 106 and 1 106. After training the classifier,we calibrate it through temperature scaling (Guo et al., 2017). We then estimate p(yref|x) and p(yshift|x)",
  "(x,y)Scallog(1 + ef(x)y).(8)": "We then define a rescaled classifier fcal(x) = f(x) (which is used to estimate the ratio pref/pshift). Weproduce calibration curves of the rescaled classifiers for each of the shifted datasets we split (see )and observe that they are indeed well-calibrated. 0.00.20.40.60.81.0 Average predicted probability 0.0 0.2 0.4 0.6 0.8 1.0 Positive rate ImageNet-V2 0.00.20.40.60.81.0 Average predicted probability 0.0 0.2 0.4 0.6 0.8 1.0 ImageNet Sketch 0.00.20.40.60.81.0 Average predicted probability 0.0 0.2 0.4 0.6 0.8 1.0 ImageNet-R y = x (perfectly calibrated)Our model : Calibration curves of classifiers used for splitting. We display calibration curves for theclassifiers used to divide ImageNet-V2, ImageNet-Sketch and ImageNet-R into in-support and out-of-supportsplits. Specifically, we sort the outputs of each classifier on a combined dataset of reference and shiftedexamples into 100 bins (where bin edges are quantiles).For each bin, we compute the actual positiverate (i.e., the proportion of examples from the shifted dataset) and the average predicted probability of anexample being from the shifted dataset. When we plot the actual positive rates against average predictedprobabilities, they are close to equal (close to y = x), suggesting that the classifiers are well-calibrated. Errorbars denote 95% Clopper-Pearson confidence intervals.",
  "Shared model specifications.When training on the WILDS-FMoW dataset, we use the FFCV imple-mentation of RandomHorizontalFlip": "Specifications of models trained from scratch.We train models from scratch by running SGD for 64epochs, using a triangular learning rate schedule with a peak learning rate of 0.2 and 8 warmup epochs, abatch size of 128, a weight decay of 5 104 and a momentum of 0.9. Baseline specifications.To establish a baseline, we train 100 ResNet-50 models from scratch on ran-dom subsets ranging from 25% of the reference dataset to the entire dataset. We increase the number ofepochs and warmup epochs inversely with the size of the subset. Miller et al. (2021) observe that modelstrained from scratch in this way often exhibit a strong linear relationship between their accuracies on thereference and shifted distributions (and the same relationship holds for models with different architectures,hyperparameters, etc.). Specifications of pre-trained models.The pre-trained model in this experiment is a CLIP ResNet-50model (the implementation of Ilharco et al. (2021)), adapted using linear probing followed by full fine-tuning.Note that the CLIP ResNet-50 architecture (Radford et al., 2021) deviates from the standard ResNet-50architecture of He et al. (2015). We perform linear probing by running AdamW for 8 epochs, using a cosinelearning rate schedule, a peak learning rate of 0.001, a batch size of 512, and without weight decay or gradientclipping. We fine-tune models by running AdamW for 16 epochs, using a cosine learning rate schedule with apeak learning rate of 1 104 and 2 warmup epochs, a batch size of 512, a weight decay of 0.1, and gradientclipping at global norm 1. Our implementation of Deep Feature Reweighting.The Deep Feature Reweighting (DFR) interven-tion proposed by Kirichenko et al. (2022) aims to improve the robustness of a model on difficult subpop-ulations by using a validation dataset with group labels. The algorithm consists of two steps: (1) traina standard model on the original training dataset, and (2) re-train only the final layer of the model (i.e.,re-weight the features of the model) on the validation dataset to be more favorable to minority groups. Tore-train the final layer, Kirichenko et al. (2022) repeatedly sample group-balanced subsets of the validationdataset, re-train the final layer on each subset, and then average the resulting re-trained final layers. Our im-plementation differs slightly in that we assign sample weights to the validation dataset such that each grouphas equal total weight and re-train the final layer on the weighted validation dataset. When applying DeepFeature Reweighting to WILDS-FMoW, we use the out-of-distribution validation set following Kirichenkoet al. (2022).",
  "B.6Curating datasets for fine-tuning": "Image editing to synthesize counterfactual examplesIn order to curate a de-biased datasetfor hair color classification, we edit images from CelebA-HQ (Karras et al., 2018), a subset of the CelebAdataset with segmentation masks for each attribute provided by CelebAMask-HQ (Lee et al., 2020). Tochange the hair color in a given image, we use InstructPix2Pix (Brooks et al., 2023), a recent image editingmodel fine-tuned from Stable Diffusion (Rombach et al., 2022). This model accepts an input image to beedited along with a prompt describing the desired change (e.g., change the hair color to blond). We findthat InstructPix2Pix is able to successfully edit the hair color; however, this model often makes undesiredchanges to attributes such as skin tone and eye color (see, e.g., the left side of ). To ensure that weonly edit hair color, we use the attribute masks to isolate the pixels in a given image corresponding to thehair region, and ignore any changes made outside of this area. When using a binary mask, this procedurecould cause unnatural edges along the border of the mask. Thus, we apply a Gaussian blur to the hairmask to smooth the transition when merging the original and edited images. To edit an image from non-blond to blond, we use the prompt change the hair color to blond. Whenediting from blond to non-blond, however, we find that the prompt change the hair color to non-blondgives inconsistent results, likely because the instruction is vague. We observe that most non-blond people",
  "Reject changes outside of segmentation": ": Synthesizing counterfactual examples.We edit hair color in CelebA-HQ images usingInstructPix2Pix (Brooks et al., 2023). However, this model can also make unwanted changes to attributeother than hair color, e.g., changing eye color (left). To avoid such issues, in the final image we incorporateonly changes within the hair region of the image. Shared model specifications.Accuracy and worst-group accuracy on the CelebA dataset are sensitiveto hyperparameter choices. As a result, we conduct a grid search to select hyperparameters for each type ofmodel. We use class-balanced accuracy as the metric for hyperparameter selection, which empirically bettercorrelates with worst-group accuracy than standard accuracy. When selecting hyperparameters for a curated dataset of a given size, we randomly sample 32 datasets ofthat size from a pool of 16, 000 images (i.e., 8, 000 CelebA images and their corresponding counterfactualsynthesized images) and average the class-balanced accuracies of models trained on each dataset. Whenevaluating the accuracy and worst-group accuracy of models trained on a curated dataset of a given size, wesimilarly randomly sample 64 datasets of that size and report average metrics.",
  "For all models, we use the FFCV implementation of RandomHorizontalFlip for data augmentation": "Specifications of models trained from scratch.We train ResNet-18 models from scratch by runningSGD for 32 epochs, using a triangular learning rate schedule with 4 warmup epochs. We use a batch size of128, a weight decay of 5 104 and a momentum of 0.9. We select the best combination of batch size andlearning rate from batch sizes of 64, 128, 256, 512 and learning rates of 0.5, 0.2, 0.1, 0.05, 0.02, 0.01. When training models from scratch on our curated dataset, we run SGD for 512 epochs and use a triangularlearning rate schedule with 64 warmup epochs. We use a batch size equal to the total number of exampleswhen it is less than 512 and a batch size of 512 otherwise. We use a weight decay of 5104 and a momentumof 0.9. We select the best learning rate from 0.5, 0.2, 0.1, 0.05, 0.02, 0.01. Baseline specifications.To establish a baseline, we train 100 ResNet-50 models from scratch on ran-dom subsets ranging from 5% of the reference dataset to the entire dataset. We increase the number ofepochs and warmup epochs inversely with the size of the subset. Miller et al. (2021) observe that modelstrained from scratch in this way often exhibit a strong linear relationship between their accuracies on thereference and shifted distributions (and the same relationship holds for models with different architectures,hyperparameters, etc.).",
  "C.1.1How does the choice of fine-tuning hyperparameters affect robustness?": "In .1, we select hyperparameters (in particular, learning rate) for fine-tuning that maximize accu-racy on the reference distribution. This reasonably simulates hyperparameter selection in practice becausetypically only samples from the reference distribution are available. In this section, we investigate how the choice of hyperparameters affects the robustness of pre-trained models.In particular, we would like to understand if pre-training yields little effective robustness to in-support shiftsand substantial effective robustness to out-of-support shifts across a wider range of hyperparameter choices.We study the spurious tint shift (an in-support shift) and the flip shift (an out-of-support shift) from .1 and vary the learning rate, weight decay, number of epochs, and batch size of a CLIP ViT-B/32 initializedwith zero-shot weights (). With zero-shot initialization, the starting point of fine-tuning is a robustmodel that performs well on our task. Hence, even under an in-support shift, hyperparameter choices thatdo not change the model substantially (e.g., low learning rate, small number of epochs) result in substantialeffective robustness. However, these hyperparameter choices generally result in lower absolute reference andshifted accuracies, and are thus unreasonable. The hyperparameter choices that are relevant in practice arethose with high reference accuracy, and these are the hyperparameters that we use in our experiments.",
  "C.1.2How does the strength of the bias affect robustness to in-support shifts?": "In .1, we consider two in-support shifts under which models might fail due to dataset biases. Inparticular, in the spurious tint shift, we introduce a tint that is spuriously correlated with the label in thereference dataset, but not in the shifted dataset. The probability that an example in the reference datasethas a class-specific tint (as opposed to a random tint) is determined by a parameter pspurious (set to 0.5 for theexperiments in ). In the label shift, the relative frequencies of classes change between the referenceand shifted datasets. The classes are divided into majority and minority classes, with minority classesappearing with probability pminority in the reference dataset (set to 0.2 for the experiments in ). Inthe shifted distribution, the relative frequencies of the classes are reversed. In this section, we investigate how the strength of the bias, i.e., pspurious and pminority, affects the robustnessof pre-trained models to these in-support shifts. We observe the average effective robustness of pre-trainedmodels largely remains close to zero as we vary these parameters (see ).",
  "Retriever": "revolver clownfish violin snail tree frog banana soccer ball pufferfish : Random samples from ImageNet and from the in-support and out-of-support splitsof ImageNet-V2, ImageNet Sketch and ImageNet-R. In ImageNet-V2, it is difficult to distinguishbetween examples from the in-support and out-of-support splits.In ImageNet Sketch and ImageNet-R,examples from the in-support splits look more realistic (i.e., more like ImageNet examples) than examplesfrom the out-of-support splits.",
  "C.2.4Controlling for difficulty when measuring effective robustness": "The significance of a given effective robustness depends on the difficulty of a distribution shift. For example,if a shift causes an accuracy drop of 5%, an effective robustness of 4% might be considered large, but if ashift that causes a drop of 25%, an effective robustness of 4% would probably be considered small. When wedivide a shifted dataset into an in-support and out-of-support split, the out-of-support split is typically moredifficult than the in-support split. If we compare the effective robustness of pre-trained models on examplesof similar difficulty in the in-support and out-of-support splits, do our findings from .2 still hold?In particular, do pre-trained models still exhibit substantially higher robustness on out-of-support examplesthan on in-support examples? To answer this question, we re-weight examples in out-of-support splits such that the difficulty distributionof the out-of-support split matches that of the in-support split. Specifically, we quantify the difficulty ofa given example in terms of the fraction of baseline models (of 77 total baseline models) that classify itincorrectly. Given an example of difficulty d, we re-weight it by a factor of pin-support(d)/pout-of-support(d)where pin-support is the difficulty probability density function of the in-support split and pout-of-support isthe difficulty probability density function of the out-of-support split. We then compute a re-weightedaccuracy, which in turn yields a re-weighted effective robustness, on the out-of-support split. Intuitively, thisre-weighted effective robustness represents the effective robustness of pre-trained models on out-of-supportexamples of similar difficulty to in-support examples.",
  "C.3.1Studying a synthetic shift": "In this section, we provide an additional experiment in a synthetic setting to further illustrate that pre-training and interventions designed to handle dataset biases can be complementary. In , we dis-cussed how robustness to the WILDS-FMoW distribution shift requires both extrapolating to later yearsand performing consistently across regions. We construct a synthetic distribution shift using that similarlyrequires both extrapolating well and avoiding reliance on spurious features. Specifically, we combine the tintand pad shifts from .1. We modify CIFAR-10 such that in the reference distribution, we add a tintthat is spuriously correlated with the label: 80% of reference examples have a class-specific tint while theremaining 20% are randomly tinted. Meanwhile, in the shifted distribution, examples are always randomlytinted and are also padded (we add 6 black pixels to each side of the original 32 32 CIFAR-10 images). To extrapolate to padded examples, we initialize a CLIP ResNet-50 and perform linear probing followed byfull fine-tuning on the reference distribution. To handle the spurious correlation between tint and label, weconsider the intervention of training on randomly tinted examples, which we refer to as balancing. This is anoracle of sorts for handling dataset biases; it simply modifies the training distribution such that spuriousfeatures are not useful. As with WILDS-FMoW, we find that pre-training and balancing each yield some effective robustness (seethe left side of ). In this case, combining the two does not yield the greatest effective robustness,but does have the highest shifted accuracy. We apply the same methodology as in to understandthe robustness benefits of pre-training and balancing.Here, we observe a greater overlap between thecorrected examples of pre-training and balancing than we did for pre-training and DFR in the case ofWILDS-FMoW (see the right side of ). This may be due to the fact that every example requiresboth extrapolation and avoiding reliance on the spurious bias. In other words, the failure modes that pre-training and balancing are intended to address cooccur. However, we note that there are still many examplesthat are corrected by one of pre-training and balancing, but not the other, suggesting complementary benefits.",
  "C.4.1Understanding the robustness benefits of pre-training when fine-tuning on a curated dataset": "In , we find that fine-tuning on a curated dataset with only 64 examples can yield a performant androbust model for hair color classification. We observe that pre-training is necessary for effective use of thesmall curated dataset; in particular, training a model from scratch on a curated dataset yields robustnessgains, but these gains are smaller and many more examples are required to attain comparable accuracy. In this section, we shed additional light on how pre-training helps in this setting. Based on our intuitionfrom Sections 4 and 5 that pre-training helps specifically with extrapolation, we hypothesize that pre-trainingprovides two benefits when training on a small curated dataset. First, a pre-trained model may be able toextrapolate better from a small number of examples. This would result in both higher accuracy on theoriginal CelebA distribution and higher worst-group accuracy, which we observe in b. Second, recallthat our curated dataset consists entirely of females, but hair color classification models are expected toperform well on males too. To compare different models ability to extrapolate along this axis, we plot thebalanced accuracy on males against the balanced accuracy on females. In , we observe that thepre-trained model indeed generalizes better to males than models trained from scratch.",
  "C.4.2Exploring balancing instead of counterfactual editing": "In , we choose to curate a dataset by augmenting images from CelebA with counterfactual ex-amples in which we edit the hair color to the opposite class. We do so in order to de-bias this dataset asmuch as possible. In this section, we explore a simpler approach to curating a dataset: balancing classes.Similarly to our curated dataset, we constrain this balanced dataset to include only females. As with ourcurated dataset, we observe that fine-tuning a pre-trained model on a class-balanced female-only datasetyields a robust and performant model for hair color classification (see a). We also observe againthat pre-training improves over training from scratch by helping with extrapolation from the female-onlyreference dataset to males (see b).",
  "D.1Alternative fine-tuning strategies": "In this work, we focus on the common setting in which a pre-trained model is fully fine-tuned. It is importantto note that pre-trained models used in a zero-shot context (i.e., without fine-tuning) and partially fine-tunedmodels (e.g., only the final classification layer is updated) are frequently more robust than fully fine-tunedmodels (Radford et al., 2021; Miller et al., 2021; Kumar et al., 2022). Such models may have higher effectiverobustness than fully fine-tuned models or in some cases may even outperform fully fine-tuned models onthe shifted distribution. However, such models are typically less performant on the reference distributionthan fully fine-tuned models. Several works observe this tradeoff between performance on the reference distribution and robustness anddevise methods for mitigating it, i.e., methods for robust fine-tuning (Wortsman et al., 2021; Hewitt et al.,2021; Kumar et al., 2022). For example, Kumar et al. (2022) argue that full fine-tuning distorts pre-trained features and propose linear probing before full fine-tuning (LP-FT) to prevent distortion. They alsosuggest that fine-tuning a model initialized as a zero-shot classifier may have a similar effect. In additionto full fine-tuning, in .1 we thus consider LP-FT and zero-shot initialization for fine-tuning. Onin-support shifts, we observe that LP-FT and zero-shot initialization do not provide effective robustnessbenefits compared to full fine-tuning (see ), suggesting that these strategies do not help mitigatedataset biases. Another strategy for robust fine-tuning is to ensemble a zero-shot model and a fully fine-tuned model. Bothweight-space ensembles (Wortsman et al., 2021) and output-space ensembles (Hewitt et al., 2021) have beenshown to improve robustness, sometimes even without sacrificing performance on the reference distribution.In fact, this strategy can yield robustness benefits even when dataset biases are a primary failure modebecause the zero-shot model is independent of the biased reference dataset. Our work seeks to complementsuch empirically effective strategies by providing an understanding of when they are necessary. In particular,our findings suggest that ensembling is valuable precisely when dataset biases cause failures.",
  "D.2Can pre-training hurt extrapolation?": "In this work, we discuss distribution shifts in which pre-training is beneficial to a models ability to ex-trapolation outside of the reference distribution. A natural question to consider is whether pre-training caninstead hurt it, yielding worse extrapolation than a model trained from scratch. A recent work by Salmanet al. (2022) suggests that this is indeed possible. Specifically, they show that biases of pre-trained modelscan persist during fine-tuning. For example, a model pre-trained on ImageNet and fine-tuned on CIFAR-10is highly sensitive to the presence of tennis balls (which are an ImageNet class but not a CIFAR-10 class).Meanwhile, a model trained from scratch on CIFAR-10 is not particularly sensitive to tennis balls. Thus,under a hypothetical tennis ball shift in which tennis balls appear in images in the shifted distribution,a pre-trained model would be less robust than a model trained from scratch. In this instance, pre-trainingprovides a harmful prior for how to extrapolate.",
  "D.3When does pre-training help with extrapolation?": "In this work, we provide evidence that pre-training can help with extrapolation, but not with other failuremodes. A natural question to consider is whether a particular pre-trained model and fine-tuning strategyin fact does help with a given extrapolation task. To this end, Ramanujan et al. (2023) explore how thecomposition of the pre-training dataset affects robustness on the WILDS-iWildCam distribution shift (Kohet al., 2020). We consider further exploration of this question to be a valuable direction for future work.",
  "D.5Understanding the robustness of pre-trained language models to spurious correlations": "Tu et al. (2020) study the robustness of pre-trained language models to distribution shifts with spuriouscorrelations. Their central finding is that pre-training can improve performance on shifted datasets in whichspurious correlations do not hold. They illustrate that this is because pre-trained models can generalizebetter from the small number of counterexamples to these correlations in the reference dataset. This is asimilar phenomenon to our observation from a: pre-training can provide some effective robustnesson in-support shifts that are close to an out-of-support shift. In cases such as those discussed by Tu et al.(2020), we hypothesize that pre-training can help to a limited extent by extrapolating better, but cannotmitigating the underlying failure mode of dataset biases.",
  "D.6Additional related work": "Pre-training.Pre-training a model (or taking an existing pre-trained model) and then fine-tuning it ona task-specific dataset is a common practice when developing machine learning models, often significantlyimproving performance over training a model from scratch (Sharif Razavian et al., 2014; Sun et al., 2017;Kornblith et al., 2019; Kolesnikov et al., 2019). Pre-training can be effective even when the downstream taskis unrelated to the pre-training task, suggesting that pre-training yields useful general-purpose features; forexample, object classification models trained on ImageNet (Deng et al., 2009) are good initializations forremote sensing (Xie et al., 2016) and medical imaging (Ke et al., 2021) tasks. Although greatly effective,pre-training is not without limitations. In some settings, pre-training does not improve performance over arandomly initialized model trained for long enough (He et al., 2019). Downstream performance can saturateas performance on the pre-training task improves (Abnar et al., 2021). Finally, biases of pre-trained modelscan persist after fine-tuning (Salman et al., 2022). Distribution shift robustness.Machine learning models are often deployed in different environmentsfrom those in which they are trained. Such distribution shifts can cause models to significantly underperform(Koh et al., 2020; Gulrajani & Lopez-Paz, 2020; Hendrycks et al., 2020a). Numerous interventions have beenproposed to improve the robustness of models, often targeting particular types of shifts.These includealgorithmic interventions (Arjovsky et al., 2019; Byrd & Lipton, 2019; Sagawa et al., 2020a; Liu et al.,2021; Kirichenko et al., 2022; Idrissi et al., 2022) (often requiring group information), data augmentations(Hendrycks et al., 2020a; Goel et al., 2020) and pre-training (discussed below).However, interventionsproposed thus far have failed to provide consistent benefits across distribution shift benchmarks (Koh et al.,2020; Gulrajani & Lopez-Paz, 2020; Hendrycks et al., 2020a; Wiles et al., 2021; Ye et al., 2022), renderingdistribution shift robustness a persistent challenge."
}