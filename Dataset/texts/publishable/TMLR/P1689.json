{
  "Abstract": "Despite the advanced capabilities of contemporary machine learning (ML) models, they remainvulnerable to adversarial and backdoor attacks. This vulnerability is particularly concerningin real-world deployments, where compromised models may exhibit unpredictable behaviorin critical scenarios. Such risks are heightened by the prevalent practice of collecting massive,internet-sourced datasets for training multimodal models, as these datasets may harborbackdoors. Various techniques have been proposed to mitigate the effects of backdooring inmultimodal models, such as CleanCLIP, which is the current state-of-the-art approach. Inthis work, we demonstrate that the efficacy of CleanCLIP in mitigating backdoors is highlydependent on the particular objective used during model pre-training. We observe thatadding self-supervised objective to pre-training, that leads to higher zero-shot classificationperformance, correlate with harder to remove backdoors behaviors. We show this by trainingmultimodal models on two large datasets consisting of 3 million (CC3M) and 6 million(CC6M) datapoints, under various pre-training objectives, followed by poison removal usingCleanCLIP. We find that CleanCLIP, even with extensive hyperparameter tuning, is ineffectivein poison removal when stronger pre-training objectives are used. Our findings underscorecritical considerations for ML practitioners who train models using large-scale web-curateddata and are concerned about potential backdoor threats. Our code is open-sourced at",
  "Introduction": "Machine Learning (ML) has taken strides in training high-performing models for a wide range of tasksfrom classification to generation. An important goal for ML is to learn general-purpose representationsthat help align data from different modalities. Approaches like CLIP (Radford et al., 2019), ALIGN (Jiaet al., 2021b), and BLIP (Li et al., 2022) learn joint representations from large scale image-text paireddatasets. These innovative techniques have ushered in the possibility of learning from unlabeled and uncurateddatasets, substantially increasing the scale and applicability of pre-training. The scaling has contributedto high zero-shot classification accuracy on various downstream datasets like Imagenet (Deng et al., 2009)and increased robustness to variations in the datasets like Imagenet-V2 (Recht et al., 2019), Imagenet-R(Hendrycks et al., 2020), and Imagenet-A (Hendrycks et al., 2021). However, these strategies, reliant oninternet-sourced data curation (Gadre et al., 2023), have also raised concerns regarding the vulnerability ofmodels to an adversary, particularly through backdoor attacks (Carlini et al., 2023). In the simplest form of this attack, an adversary inserts a patch (termed as a trigger patch or poison) in asmall subset of the training data images and alters the ground truth label or caption to a target label orcaption (Gu et al., 2017).1When trained on the poisoned training data, the model learns to associate thetrigger patch with the target label/caption. If deployed, an adversary can get the model to predict the targetlabel for any datapoint by inserting the trigger patch. The success of an adversary is measured by the attacksuccess rate (ASR) metric, which is the percentage of the images with the trigger patch that are predictedwith the target label. Previous works (Carlini & Terzis, 2021) have demonstrated effective backdooring ofmultimodal models (ASR 80%) just by poisoning a mere 75 out of 3 million training datapoints. Several backdoor mitigation techniques have been proposed for multimodal models (Bansal et al., 2023; Liet al., 2021b; Yang et al., 2023; 2024) to tackle this vulnerability. These approaches either attempt to detectand filter the poisoned datapoints during the pre-training (Li et al., 2021b; Yang et al., 2023; 2024) or finetunethe given backdoored model using a specialized loss function on a smaller, guaranteed to be clean image-textpaired dataset. The latter approach helps the model to forget the association between the trigger patchand the target label while still maintaining the learned associations for benign datapoints, e.g., CleanCLIP(Bansal et al., 2023). CleanCLIP proposes to finetune a backdoored model using a combination of contrastiveloss and self-supervised loss on a small dataset, free of backdoors, to clean the model. It is the state-of-the-art(SOTA) technique to clean a backdoored model and obtain a low ASR ( 5%) without hurting its zero-shotclassification accuracy; thereby achieving a successful model cleaning. So far, it has been demonstrated that CleanCLIP can successfully clean models pre-trained only withmultimodal constrastive loss (MMCL) as the objective (Radford et al., 2019). Several recent works (Mu et al.,2022; Li et al., 2021a; Yao et al., 2021; Lee et al., 2022) have proposed stronger pre-training objectives thatlead to better zero-shot image classification accuracy. Specifically, adding self-supervised loss (SSL) in bothmodalities has been the key player in all these works. Therefore, in this work, we pre-train multimodal modelsusing a combination of MMCL and SSL on a poisoned training dataset. Consistent with the previous findings,models pre-trained using a combination of MMCL and SSL produced models with a higher classificationaccuracy than models trained solely with the MMCL objective. We then apply the finetuning procedurein CleanCLIP to remove the poison from these models (see ). To our surprise, we observe thatCleanCLIP fails to successfully (i.e., without a significant loss in the models zero-shot accuracy) removepoison from the models pre-trained with the stronger objective (combination of MMCL and SSL). We further conduct experiments with other practical considerations, such as the performance of CleanCLIPwhen its cleaning data still has a few poisoned datapoints, and deciding the stopping criterion for thefinetuning process when one is not aware of the specific backdoor attack on the model (which is usually thecase). From all the experiments, we find that only the models pre-trained with MMCL alone are amenable topoison removal in both the cases of availability of completely clean finetuning data and when the finetuningdata still has some poisoned datapoints.",
  ": Our experimental setup to test the claim about the dependence of the ability of CleanCLIP to removepoison from a backdoored model on the models pre-training objective": "1. We show that the state-of-the-art technique for removing poison from backdoored multimodal models,CleanCLIP, depends on the models pre-training objective and fails to mitigate poison when the modelsare pre-trained with a stronger objective, like the combination of MMCL and SSL losses. (See for further justification on why we chose to study CleanCLIP. )",
  ". We conduct several analysis experiments to demonstrate the effect of different pre-training objectives onthe strength of poison induction": "3. We conduct experiments to show the practical use case of CleanCLIP from a real-world perspective whenthe finetuning data is not entirely free of poison and the practitioner is not aware of the specific kind ofpoisoning in the model and hence has to decide on the stopping criterion for the cleaning process.",
  "Related Works": "Contrastive Learning Contrastive learning was formally established in seminal works by Bromley et al.(1993); Chopra et al. (2005); Hadsell et al. (2006) that has evolved, giving rise to contemporary algorithmssuch as CPC (Oord et al., 2018), DCL (Yeh et al., 2022), SimCLR (Chen et al., 2020), and NNCLR (Dwibediet al., 2021). 2 These approaches, at their core, share a common objective: bringing similar elements closerin representation space while pushing dissimilar ones apart. Radford et al. (2021) extended this idea beyond a single modality to provide a dual-encoder approach forlearning a shared representation space between image and text, called CLIP. Images and their correspondingcaptions are brought close, while the dissimilar images and captions are pushed away. Jia et al. (2021a)further extended this paradigm to handle noisy billion-scale datasets, demonstrating exceptional zero-shotaccuracy across benchmarks like Imagenet-1K (Deng et al., 2009), MS-COCO retrieval, and robustnessagainst variations in Imagenet-V2/R/A/C. Since then, there have been several improvements to the zero-shotaccuracy by adding components to the loss term. CyCLIP (Goel et al., 2022) imposes additional consistency",
  "Published in Transactions on Machine Learning Research (12/2024)": "Hyperparams: 1e-09, 1, 20Hyperparams: 5e-09, 1, 20Hyperparams: 1e-08, 1, 20Hyperparams: 5e-08, 1, 20Hyperparams: 1e-07, 1, 20Hyperparams: 3e-07, 1, 20Hyperparams: 7e-07, 1, 20 Hyperparams: 1e-06, 1, 20Hyperparams: 3e-06, 1, 20Hyperparams: 7e-06, 1, 20Hyperparams: 1e-05, 1, 20Hyperparams: 3e-05, 1, 20Hyperparams: 0.0001, 1, 20Hyperparams: 0.0003, 1, 20 Hyperparams: 0.0004, 1, 20Hyperparams: 0.0005, 1, 20Hyperparams: 0.0006, 1, 20Hyperparams: 0.001, 1, 20Hyperparams: 5e-05, 2, 20Hyperparams: 0.0001, 2, 20Hyperparams: 0.0005, 2, 20 Hyperparams: 0.001, 2, 20Hyperparams: 5e-05, 4, 20Hyperparams: 0.0001, 4, 20Hyperparams: 0.0005, 4, 20Hyperparams: 0.001, 4, 20Hyperparams: 5e-05, 6, 20Hyperparams: 5e-05, 6, 20 Hyperparams: 0.0001, 6, 20Hyperparams: 0.0001, 6, 20Hyperparams: 0.0005, 6, 20Hyperparams: 0.0005, 6, 20Hyperparams: 0.001, 6, 20Hyperparams: 0.001, 6, 20Hyperparams: 5e-05, 8, 20 Hyperparams: 5e-05, 8, 20Hyperparams: 0.0001, 8, 20Hyperparams: 0.0001, 8, 20Hyperparams: 0.0005, 8, 20Hyperparams: 0.0005, 8, 20Hyperparams: 0.001, 8, 20Hyperparams: 0.001, 8, 20 Hyperparams: 1e-05, 1, 50Hyperparams: 2e-05, 1, 50Hyperparams: 4e-05, 1, 50Hyperparams: 5e-05, 1, 50Hyperparams: 7e-05, 1, 50Hyperparams: 9e-05, 1, 50Hyperparams: 0.0001, 1, 50 ASR (in %) Hyperparams: 0.0002, 1, 50 ASR (in %) Hyperparams: 3e-05, 1, 100 ASR (in %) Hyperparams: 6e-05, 1, 100 ASR (in %) Hyperparams: 0.0002, 1, 100 ASR (in %) Hyperparams: 0.0003, 1, 100 ASR (in %) Hyperparams: 0.0004, 1, 100 ASR (in %) Hyperparams: 0.0005, 1, 100",
  "Primer on Pre-training and Poisoning": "Notations Let I and T denote the space of images and text. Dpre = {(Ij, Tj))}Nj=1, Dclean = {(Ij, Tj))}Mj=1denotes the pre-training and cleaning dataset of N and M image-text pairs respectively, where M << N.hI : I Rd and hT : T Rd denote the image and text encoders respectively, where d is the dimensionalityof the embedding space. All the embeddings are further normalized to make 2 norm to 1 which we denoteusing f(x) = g(h(x)), where g : Rd B(1) is normalization mapping, where, B(1) = {x : x2 = 1, x Rd}; denotes learnable temperature. Let LMMCL denote the multimodal and LSSL denote the intramodalself-supervision losses respectively. Let I denote an augmentation to image I and T denote an augmentation",
  "Experimental Setup": "On a high level, our experiments involve poisoning CLIP models using two distinct pre-training objectiveswith different kinds of backdoors by either training a model from scratch or by finetuning from a pre-trainedcheckpoint. Once we have poisoned the model, we attempt to remove the poison using CleanCLIP, whichfinetunes the model with a specific objective using a separate dataset. We have illustrated this in and summarized our key findings in . Training Details We train a dual-encoder multimodal model on image-text paired datasets. We train modelsusing two kinds of pre-training objectives: a) only multimodal contrastive loss (LpreMMCL), and b) combinationof multimodal contrastive loss and self-supervised loss in the image and text modalities (LpreMMCL + LpreSSL).Following CleanCLIP, we use a ResNet-50 as the models vision encoder and a transformer as the text encoder.We trained the models on two image-text paired datasets:",
  ". Conceptual Caption 6M (CC6M): This dataset has 6M image-text paired datapoints from the CC12Mdataset (Changpinyo et al., 2021), to which size our computing resources scaled": "The models are trained either from scratch or finetuned from a pre-trained CLIP checkpoint (Radford et al.,2019). We train models for 64 epochs using 8 Nvidia A100 GPUs. The initial learning rate of 1e 3 withcosine scheduling is used when trained from scratch and 5e 7 when finetuned from a checkpoint. We useAdamW optimizer with 10,000 warmup steps (Loshchilov & Hutter, 2017). Models trained with LpreMMCL usea batch size of 256, whereas models trained with LpreMMCL + LpreSSL use a batch size of 128. Please refer toAppendix A for the loss dynamics. Poisoning Following CleanCLIP, we introduce the trigger proposed by BadNet (Gu et al., 2017) in a smallsubset of the training datapoints. Specifically, we add a trigger patch of size 16 16 sampled from a standardGaussian at a random location in the image and subsequently change the images caption to be the adversarychosen label, in this case banana. Please see Appendix J for examples of images with trigger patch and theircorresponding captions. Using the same settings as CleanCLIP, we introduce the trigger in 1,500 randomlysampled datapoints for the CC3M dataset and 3,000 randomly sampled datapoints for the CC6M dataset (amere 0.05% of the training datapoints). We also experiment with another kind of poisoning technique: label consistent poisoning. In this case, thetrigger patch (created in the manner as mentioned above) is added to the images that have the adversarychosen label (in this casebanana) in their captions.",
  "In this section, we expound on the pre-training details for the models, followed by their cleaning procedureand the metrics we use to measure the models performance": "Metrics The models are evaluated for their Top-1 zero-shot accuracy on the Imagenet-1K validation set(referred to as Imagenet hereafter). Each of the 1,000 classes of Imagenet is described using sentences like: aphoto of a ..., a tattoo of a ..., etc. We generate 80 such text templates for each class (see Appendix C) andthen pass them to the text encoder to produce an average text embedding for the class. During zero-shotclassification, the prediction for an image is the class whose thus computed text embedding has the highestcosine similarity with the image embedding. We also evaluate the attack success rate (ASR) of a model. In an apparent similarity to accuracy, theASR of a backdoored model is defined as the percentage of triggered images that the model classifies asthe adversary-chosen target label. For measuring ASR, we add the trigger patch at random locations in allImagenet validation set images and measure the percentage of them that are classified as the target class, i.e.,banana. We measure both these metrics at the end of each cleaning epoch as any model encountered duringthe cleaning process is a good candidate for a cleaned model. Poison Induction by Training from Scratch shows the Top-1 zero-shot Imagenet validationset accuracy for the models trained from scratch using LpreMMCL and LpreMMCL + LpreSSL on CC3M and CC6Mdatasets. For the smaller CC3M dataset, both the models achieve an accuracy of around 1617%, and for thelarger CC6M dataset, the models reach an accuracy of around 24%. Even though the models trained withLpreMMCL + LpreSSL attained higher accuracy than the models trained with LpreMMCL alone, in order to have bettervisualization of the difference in performance of CleanCLIP on the two pre-training objectives, we deliberatelychoose models with similar starting accuracies.All the models, irrespective of the pre-training objective andthe training dataset, reached more than 99% ASR (see in Appendix A), implying that poisoning just0.05% of the dataset is enough to attain very high ASR. Removing Poison We clean the poisoned model by finetuning it on a 100K, guaranteed to be poison-free,image-text pairs for 20 epochs using a batch size of 128 and AdamW as the optimizer. We perform extensivehyperparameter search and use various learning rates (as many as 8 in some experiments and 14 in others,all with cosine scheduling and 50 warmup steps) for this process. Please refer to Appendix D for the set oflearning rates explored for the cleaning procedure. Ideally, after cleaning we would want to obtain a modelthat maintains the accuracy of the original poisoned model, while getting rid of its poison, i.e., very low ASR.We use three different loss functions for the cleaning process: 1. LftMMCL: CleanCLIP showed that finetuning with LftMMCL did not change the original models accuracy andASR, and hence is an ineffective cleaning loss. We reproduce these results for the models we trained.",
  "BadNetCC6M100K23.76%24.04%23.86%13.05%": "2. LftSSL: CleanCLIP also showed that finetuning with LftSSL decreased the original models ASR at the expenseof its accuracy, and hence is also an ineffective cleaning loss. We also reproduce these results. 3. LftMMCL +LftSSL: CleanCLIP showed that finetuning with a combination of these losses decreased the originalmodels ASR while not hurting its accuracy, and hence is an effective cleaning loss. Our experiments showthat while this observation is true for the models trained with LpreMMCL, however it does not generalize tothe models trained with stronger pre-training objective LpreMMCL + LpreSSL. This is our key finding. Findings from the Cleaning Procedure shows the scatter plot of the Top-1 zero-shot Imagenetvalidation set accuracy and the ASR at the end of each cleaning epoch for the models trained on the CC6Mdataset. We defer the plots for the CC3M dataset in Appendix E.1 for space consideration. For both thedatasets, we observe that:",
  ". LftMMCL and LftSSL individually are ineffective cleaning losses as they cause a significant drop in accuracy forlowering the ASR for both the pre-training objectives": "2. LftMMCL +LftSSL serves as an effective cleaning loss for the model trained with LpreMMCL (left plot). The cleanedmodels maintain the accuracy of the original model, but they have low ASR, which we consider successfulcleaning. However, it does not lead to an effective cleaning of the model trained with LpreMMCL + LpreSSL (rightplot). Even the model that has the highest accuracy with a low ASR ( 5%) is 45% less accurate than theoriginal model, as shown in . For both datasets, our findings indicate that CleanCLIP is not effective in removing poison from the modelstrained with a stronger pre-training objective LpreMMCL + LpreSSL, without a significant drop in accuracy.",
  "gives the highest accuracy of the models which were successfully cleaned by CleanCLIP (ASR 5%)": "Poison Induction by Finetuning a pre-trained model We also induce poison by finetuning a pre-trainedCLIP model (Radford et al., 2019). Concretely, we poison two models by finetuning them with LpreMMCL andLpreMMCL + LpreSSL respectively, using the CC6M dataset that had 3000 poisoned datapoints. We use a learningrate of 5e 7 with AdamW optimizer with 10,000 warmup steps. After poisoning, these models achieve Top-1zero-shot Imagenet set accuracy of 60%, much higher than the models trained from scratch (). TheASR for the model poisoned with LpreMMCL is 99% and for the model poisoned with LpreMMCL + LpreSSL is 90%. Findings from the Cleaning Procedure: We clean the poisoned models using CleanCLIP, i.e., furtherfinetuning on a clean dataset with LftMMCL + LftSSL. shows the scatter plot of the Top-1 zero-shotImagenet validation set accuracy and the ASR at the end of each finetuning epoch for these two models. Inthis case, both the models experience a drop in accuracy to obtain a low ASR ( 5%); however, the drop ismuch higher for the model when the poison was induced using LpreMMCL + LpreSSL (33%, compared to a 17%drop for the model when the poison was induced using LpreMMCL). This experiment corroborates our previousfinding that CleanCLIP is less effective when the poison is induced using a combination of MMCL and SSL,irrespective of the fact whether the poison is induced via finetuning or by training from scratch.",
  "% drop": ": Scatter plot of the Top-1 zero-shot Imagenet validation set accuracy vs. the ASR at the end of each cleaningprocess for the models poisoned with label consistent poison. The finetuning is done with LftMMCL + LftSSL. We measurethe accuracy and ASR at the end of each finetuning epoch. The red star in the top right corner (encircled in the blackcircle) corresponds to the original models accuracy and ASR. For a successful clean, there should be models thatmaintain the original models accuracy while having a low ASR (indicated by the red circle). Takeaway: CleanCLIPis much more effective for the model trained with LpreMMCL (left) than for the model trained with LpreMMCL + LpreSSL (right). ASR (in %) Top-1 ImageNet Zeroshot accuracy (in %) Models pre-trained with MMCL + SSL, cleaned with 100k datapoints (CC6M) CleanCLIP: MMCL only CleanCLIP: SSL only CleanCLIPPre-trained model ASR (in %) Models pre-trained with MMCL + SSL, cleaned with 200k datapoints (CC6M) CleanCLIP: MMCL only CleanCLIP: SSL only CleanCLIPPre-trained model : The scatter plot of the Top-1 zero-shot Imagenet validation set accuracy vs. the ASR at the end ofeach cleaning epoch for the LpreMMCL + LpreSSL pre-trained model on CC6M dataset. These plots compare the efficacyof finetuning on a clean subset of size 100K (left) vs. 200K (right) datapoints. Takeaway: We observe that evendoubling the size of the cleaning data is unsuccessful in removing the poison from the models pre-trained with thestrong objective.",
  "% drop41% drop": ": Top-1 zero-shot Imagenet validation set accuracy vs. the ASR, measured at the end of each cleaning epochfor the models trained on the CC6M dataset. The cleaning using CleanCLIP. The red star in the top right corner(encircled in the black circle) corresponds to the models starting accuracy and ASR (before cleaning). Takeaway:When a ViT backbone is poisoned, there are no cleaned models that maintain the original accuracies for both thepre-training losses, however the drop is much larger for the model trained with LpreMMCL + LpreSSL (right). For this experiment, we poison a model with a different backbone architecture, specifically ViTs. We poisonedthese models by training them on the CC6M dataset with 3000 poisoned datapoints, and cleaned them usingCleanCLIP. shows the cleaning plots for these models. We observe that similar to the previousexperiments, the model trained with MMCL + SSL experiences a much larger drop in accuracy than themodel just trained with MMCL, although in this case the model just trained with MMCL experiences a lotdrop in accuracy as well. Poisoning Induction using a Different Poison Due to space considerations, we present the results forthe effectiveness of CleanCLIP when models are poisoned using label consistent backdoors in Appendix F.We observe that similar to the case of poisoning with BadNet, CleanCLIP is much less effective when thepoison is induced using LpreMMCL + LpreSSL (16% loss in accuracy), compared to the case when it is induced usingLpreMMCL (10% gain in accuracy). Dependence of the Stopping Criterion on the Pre-training Objective In the previous section, theability to find a model with high accuracy and low ASR is considered a success for the CleanCLIP approach.However, in practice, one would not be aware of the ASR of the model being cleaned and, therefore, wouldnot know when to stop the cleaning process. To highlight this practicality concern, we show the multiplecleaning trajectories for models trained using different pre-training objectives in . a shows the trajectories of three cleaning runs with different learning rates for a model trained usingLpreMMCL on the CC6M dataset. We observe that in all the three runs, the trajectory converges to a regionof high accuracy and low ASR (top left corner), and the trajectories are smooth. This indicates that apractitioner can clean this model by finetuning the poisoned model for as long as their resources allow, andchoose the model at the end of the finetuning process. They will likely obtain a model with high accuracyand low ASR, i.e., a successfully cleaned model. b shows the trajectories of three cleaning runs with different learning rates for a model trained usingLpreMMCL + LpreSSL on the CC6M dataset. We observe that in all three runs, the successfully cleaned model (highaccuracy with a low ASR ( 5%)) is an intermediate model in the trajectory, and not the model at the endof the process. With continued finetuning, the model can both lose accuracy and gain ASR, both of whichare undesirable. Therefore, it is difficult for a practitioner to discern when to stop the finetuning process toobtain a clean model.",
  "Finetuning Epochs": "Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %)Top-1 ImageNet Zeroshot accuracy (in %) : The cleaning trajectories showing the Top-1 zero-shot Imagenet validation set accuracy vs. the ASR at theend of each cleaning epoch for the LpreMMCL + LpreSSL pre-trained model on CC6M dataset. Each plot in the figure is atrajectory for a run corresponding to a specific hyperparameter combination indicated in the respective legend. Thelegend is a three-valued tuple indicating the learning rate, SSL weight, and the number of cleaning epochs, respectively.Takeaway: We find that the cleaning trajectories for the LpreMMCL + LpreSSL pre-trained model is non-smooth and oftendoes not converge to a point in the space. Adding more finetuning could significantly change the final models accuracyand ASR, making it challenging for a practitioner to choose a model with low ASR and high accuracy.",
  "Therefore, the practicality of using CleanCLIP also depends on the pre-training objective of the model.Appendix K shows the cleaning trajectories for all explored hyperparameters": "Dependence of CleanCLIP on the Ideal Condition of the Dataset CleanCLIP assumes that thecleaning data is entirely free of poisoned datapoints. In practice, this assumption can be violated even whenconsiderable care is taken to ensure it. To simulate this real-world situation, we clean models using data witha few poisoned datapoints, specifically 5 and 10 poisoned datapoints in the 100K cleaning datapoints. Notethat these datasets are still, respectively, 10 and 5 cleaner than the original training dataset, illustrating asituation where the cleaning data is much cleaner than the training dataset but still not perfect. shows the scatter plot of the Top-1 zero-shot Imagenet validation set accuracy and the ASR atthe end of each cleaning epoch when two models trained from scratch on the CC6M dataset, one usingLpreMMCL and the other using LpreMMCL + LpreSSL is cleaned by finetuning on this slightly poisoned dataset withLftMMCL + LftSSL. We observe that having just 5 poisoned datapoints in the cleaning dataset severely weakensCleanCLIP for both the pre-training objectives. For models pre-trained with LpreMMCL, we found cleaned models that maintain the original models accuracyand achieve around 30-50% ASR. On the other hand, for the models pre-trained with the stronger objectiveLpreMMCL + LpreSSL, having just 5 poisoned examples renders the cleaning procedure completely ineffective. Themodels pre-trained on the CC6M dataset lose about 80% of the original models accuracy to obtain a lowASR ( 5%), and no model has an ASR lower than 90% for the CC3M pre-trained model (). Takeaways Our experiments highlight the fact that a stronger pre-training objective, like the combination ofMMCL and SSL, also affects the strength of poison induction, making the cleaning process difficult. Also, fora practitioner, when pre-training with a stronger objective, the decision of when to stop finetuning becomes",
  "Analysis of the Stronger Pre-training Objective": "We now perform several analysis experiments to understand the reason behind the difference in the poisonremoval ability of CleanCLIP across the two pre-training objectives. We also experiment with other methodsto attempt to remove the poison when induced using LpreMMCL + LpreSSL. Cleaning using an Objective distinct from Pre-training CleanCLIP successfully cleans the modelstrained with LpreMMCL by finetuning with LftMMCL + LftSSL. However, it was unsuccessful for the model trainedwith LpreMMCL + LpreSSL. A plausible reason for this behavior could be that we are using the same pre-trainingand cleaning objective in the latter case, and CleanCLIP might be able to successfully clean the lattermodels if we were to clean it with a loss objective that is distinct from its pre-training objective. To test thishypothesis, we clean the models trained with LpreMMCL + LpreSSL by finetuning with LftMMCL + LftSSL + LftDeepClust,",
  "where LftDeepClust is an additional deep clustering objective (Caron et al., 2018) on the vision encoder": "In deep clustering, we first obtain a pseudo-label for each image. We obtain the pseudo-label for an image intwo ways: a) by classifying each image into one of the 1,000 Imagenet classes using powerful models such asSigLIP ViT-L/14 (zero-shot Imagenet accuracy of 83.08%) (Zhai et al., 2023), and b) performing a 1,000-wayclustering on feature space of our trained vision encoder, using FAISS (Johnson et al., 2019). Note that theuse of SigLIP ViT-L/14 for obtaining pseudo-labels is a cheating experiment for a deep clustering task (wedont have access to ground truth labels and therefore use SigLIP ViT-L/14 for this task); however, thisexperiment is solely performed to probe the upper bound of the poison removal that can be obtained whenusing deep clustering approaches. In the latter case, when we use clustering-based pseudo-label assignment for every image in the cleaningdataset, we learn to predict the assigned pseudo-label (y) with the help of a linear classifier on top of thevision encoder using cross-entropy loss (LXent). Let W Rd1000 be the linear classifier mapping visualfeatures (Rd) to one of the 1000 pseudo-labels. For a given datapoint (Ij, Tj) with assigned pseudo-label yj,deep clusterings objective becomes",
  "Conclusions": "Through our extensive hyperparameter search and ablation experiments, we unveil a critical limitationof the current state-of-the-art poison mitigation technique for multimodal models, CleanCLIP. It fails toeffectively remove backdoor poisoning when a model is trained using stronger objectives like the combinationof multimodal contrastive learning (MMCL) and intramodal self-supervised learning (SSL). This objective iscommon in popular approaches like SLIP (Mu et al., 2022) and has demonstrated superior accuracy overtraining with only the MMCL objective. Our experiments show that this vulnerability persists irrespective ofthe scale of the pre-training and the cleaning datasets, irrespective of the manner of poison induction (fromscratch or by finetuning), and irrespective of the specific backdoor attack. Particularly concerning is the unstable cleaning trajectory in models trained using the stronger objective(b). Often unaware of the specific backdoor attack, practitioners face challenges in determining theoptimal point to halt the cleaning process. This instability can lead to suboptimal models, as continuedfinetuning can decrease accuracy and increase attack success rate (ASR). Furthermore, our findings highlightthe critical assumption of a completely poison-free cleaning dataset for CleanCLIPs effectiveness, anassumption that is rarely met in practical scenarios. This becomes particularly problematic with the use of",
  "Nicholas Carlini and Andreas Terzis.Poisoning and backdooring contrastive learning. arXiv preprintarXiv:2106.09667, 2021": "Nicholas Carlini, Matthew Jagielski, Christopher A Choquette-Choo, Daniel Paleka, Will Pearce, HyrumAnderson, Andreas Terzis, Kurt Thomas, and Florian Tramr. Poisoning web-scale training datasets ispractical. arXiv preprint arXiv:2302.10149, 2023. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervisedlearning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp.132149, 2018.",
  "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scaleimage-text pre-training to recognize long-tail visual concepts. In CVPR, 2021": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020. Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with applicationto face verification. In 2005 IEEE computer society conference on computer vision and pattern recognition(CVPR05), volume 1, pp. 539546. IEEE, 2005.",
  "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machinelearning model supply chain. arXiv preprint arXiv:1708.06733, 2017": "Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In2006 IEEE computer society conference on computer vision and pattern recognition (CVPR06), volume 2,pp. 17351742. IEEE, 2006. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,Tyler Lixuan Zhu, Samyak Parajuli, Mike Guo, Dawn Xiaodong Song, Jacob Steinhardt, and JustinGilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. 2021IEEE/CVF International Conference on Computer Vision (ICCV), pp. 83208329, 2020. URL",
  "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples.CVPR, 2021": "Alvi Md Ishmam and Christopher Thomas. Semantic shield: Defending vision-language models againstbackdooring and poisoning via fine-grained knowledge alignment.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pp. 2482024830, June 2024. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In International conference on machine learning, pp. 49044916. PMLR, 2021a. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In International Conference on Machine Learning, 2021b. URL",
  "APre-Training Details": "We train all the models on 8 Nvidia A100 GPUs for 64 epochs. We use an initial learning rate of 0.001 forthe models trained from scratch, and for the models where poison is induced by finetuning from a pre-trainedcheckpoint, we use an initial learning rate of 5e 7. We use cosine scheduling and 10000 warmup steps withAdamW optimizer (Loshchilov & Hutter, 2017) for training. The model trained with LpreMMCL uses a batchsize of 256, whereas the model trained with the LpreMMCL + LpreSSL uses a batch size of 128. We show the change in accuracy and ASR with the training epochs for the model trained from scratch withBadNet attack using LpreMMCL in and using LpreMMCL + LpreSSL in . We use early-stoppingfor the model trained with LpreMMCL and choose the model with the highest accuracy. For LpreMMCL + LpreSSLpre-training, we choose the model that has the closest accuracy to the LpreMMCL trained model. showsthe accuracy and the ASR for all the models we select in this paper for poison removal.",
  "CTemplates for Text-Embedding Computation": "a bad photo of a {class}., a photo of many {class}., a sculpture of a {class}., a photo of the hard to see {class}., a lowresolutionphoto of the {class}., a rendering of a {class}., graffiti of a {class}., a bad photo of the {class}., a croppedphoto of the {class}., a tattoo of a {class}., theembroidered {class}., a photo of a hard to see {class}., a brightphoto of a {class}., a photo of a clean {class}., a photo of a dirty {class}., a darkphoto of the {class}., a drawing of a {class}., a photo of my {class}., theplastic {class}., a photo of the cool {class}., a close -up photo of a {class}., a black and whitephoto of the {class}., a painting of the {class}., apainting of a {class}., a pixelatedphoto of the {class}., a sculpture of the{class}., a brightphoto of the {class}., a croppedphoto of a {class}., aplastic {class}., a photo of the dirty {class}., a jpegcorruptedphoto of a {class}., a blurryphoto of the {class}., a photo of the {class}., a goodphoto of the {class}., a rendering of the {class}., a {class} in a videogame., a photo of one {class}., a doodle of a {class}., a close -up photo of the",
  "D.5Cleaning of the Model where Poison is induced via Finetuning with MMCL on CC6M dataset": "Cleaning Epochs: 20Learning rates (69 values): {5e-05, 4.9e-05, 4.8e-05, 4.7e-05, 4.6e-05, 4.5e-05, 4.4e-05, 4.3e-05, 4.25e-05, 4.2e-05,4.1e-05, 4e-05, 3.9e-05, 3.8e-05, 3.75e-05, 3.7e-05, 3.6e-05, 3.5e-05, 3.4e-05, 3.3e-05, 3.2e-05, 3.1e-05, 3e-05,2.9e-05, 2.8e-05, 2.7e-05, 2.6e-05, 2.5e-05, 2.4e-05, 2.3e-05, 2.2e-05, 2.1e-05, 2e-05, 1.9e-05, 1.8e-05, 1.7e-05,1.6e-05, 1.5e-05, 1.4e-05, 1.3e-05, 1.2e-05, 1.1e-05, 1e-05, 9e-06, 8e-06, 7e-06, 6e-06, 5e-06, 4.9e-06, 4.75e-06,4.5e-06, 4.25e-06, 4e-06, 3.75e-06, 3.5e-06, 3.25e-06, 3e-06, 2.75e-06, 2.5e-06, 2.25e-06, 2e-06, 1.75e-06, 1.5e-06,1.25e-06, 1e-06, 5e-07, 1e-07, 5e-08, 1e-08}MMCL weight: 1SSL weight: 1Size of the Cleaning Dataset: 1,00,000",
  "D.6Cleaning of the Model where Poison is induced via Finetuning with MMCL + SSL on CC6Mdataset": "Cleaning Epochs: 20Learning rates (85 values): {0.005, 0.001, 0.0005, 0.0001, 5e-05, 5e-05, 4.9e-05, 4.8e-05, 4.75e-05, 4.7e-05,4.6e-05, 4.5e-05, 4.5e-05, 4.4e-05, 4.3e-05, 4.25e-05, 4.2e-05, 4.1e-05, 4e-05, 4e-05, 3.9e-05, 3.8e-05, 3.75e-05,3.7e-05, 3.6e-05, 3.5e-05, 3.5e-05, 3.4e-05, 3.3e-05, 3.25e-05, 3.2e-05, 3.1e-05, 3e-05, 3e-05, 2.9e-05, 2.8e-05,2.75e-05, 2.7e-05, 2.6e-05, 2.5e-05, 2.5e-05, 2.4e-05, 2.3e-05, 2.25e-05, 2.2e-05, 2.1e-05, 2e-05, 2e-05, 1.9e-05,1.8e-05, 1.75e-05, 1.7e-05, 1.6e-05, 1.5e-05, 1.5e-05, 1.4e-05, 1.4e-05, 1.3e-05, 1.3e-05, 1.2e-05, 1.2e-05, 1.1e-05,1.1e-05, 1e-05, 1e-05, 9e-06, 9e-06, 8e-06, 8e-06, 7e-06, 7e-06, 6e-06, 6e-06, 5e-06, 5e-06, 1e-06, 1e-06, 5e-07,5e-07, 1e-07, 1e-07, 5e-08, 5e-08, 1e-08, 1e-08}MMCL weight: 1SSL weight: 1Size of the Cleaning Dataset: 1,00,000",
  "D.7Cleaning of the Model with Label Consistent Poisoning trained with MMCL on CC6M dataset": "Cleaning Epochs: 20Learning rates (44 values): {5e-05, 4.75e-05, 4.25e-05, 4e-05, 3.8e-05, 3.75e-05, 3.7e-05, 3.6e-05, 3.5e-05,3.4e-05, 3.3e-05, 3.2e-05, 3.1e-05, 3e-05, 2.9e-05, 2.8e-05, 2.7e-05, 2.6e-05, 2.5e-05, 2.4e-05, 2.3e-05, 2.2e-05,2.1e-05, 2e-05, 1.9e-05, 1.8e-05, 1.7e-05, 1.6e-05, 1.5e-05, 1.4e-05, 1.3e-05, 1.2e-05, 1.1e-05, 1e-05, 9e-06, 8e-06,7e-06, 6e-06, 5e-06, 1e-06, 5e-07, 1e-07, 5e-08, 1e-08}MMCL weight: 1SSL weight: 1Size of the Cleaning Dataset: 1,00,000",
  "D.8Cleaning of the Model with Label Consistent Poisoning trained with MMCL + SSL on CC6Mdataset": "Cleaning Epochs: 20Learning rates (71 values): {0.007, 0.006, 0.005, 0.004, 0.003, 0.002, 0.001, 0.0009, 0.0008, 0.0007, 0.0006,0.0005, 0.0004, 0.0003, 0.0002, 0.00018, 0.00017, 0.00016, 0.00014, 0.00013, 0.00012, 0.00011, 0.0001, 9e-05,8e-05, 7e-05, 6e-05, 5e-05, 4.75e-05, 4.25e-05, 4e-05, 3.8e-05, 3.75e-05, 3.7e-05, 3.6e-05, 3.5e-05, 3.4e-05, 3.3e-05,3.2e-05, 3.1e-05, 3e-05, 2.9e-05, 2.8e-05, 2.7e-05, 2.6e-05, 2.5e-05, 2.4e-05, 2.3e-05, 2.2e-05, 2.1e-05, 2e-05,1.9e-05, 1.8e-05, 1.7e-05, 1.6e-05, 1.5e-05, 1.4e-05, 1.3e-05, 1.2e-05, 1.1e-05, 1e-05, 9e-06, 8e-06, 7e-06, 6e-06,5e-06, 1e-06, 5e-07, 1e-07, 5e-08, 1e-08}MMCL weight: 1SSL weight: 1Size of the Cleaning Dataset: 1,00,000",
  "D.13Cleaning of the Model Pre-trained on the CC6M dataset using Shrink and Perturb": "Cleaning Epochs: 20Learning rates (9 values): {1e-5, 2e-5, 4e-5, 5e-5, 7e-5, 9e-5, 1e-4, 2e-4, 1e-3}MMCL weight: 1SSL weight: 1Shrink (17 values): {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.92, 0.93, 0.95, 0.96, 0.97, 0.98, 0.99, 1}Perturb p (15 values): {1e-5, 1e-4, 1e-3, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.4, 0.8, 1, 2, 3, 4}Size of the Cleaning Dataset: 1,00,000",
  "E.1Findings for the Models trained on the CC3M Dataset": "ASR (in %) 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 Top-1 ImageNet Zeroshot accuracy (in %) Model pre-trained with MMCL objective (CC3M dataset) CleanCLIP: MMCL only CleanCLIP: SSL only CleanCLIPPre-trained model ASR (in %) Model pre-trained with MMCL + SSL objective (CC3M dataset) CleanCLIP: MMCL only CleanCLIP: SSL only CleanCLIPPre-trained model",
  "E.2Findings for the Models trained on the CC3M Dataset when Cleaning under Non-ideal Conditions": "shows the scatter plot of the Top-1 zero-shot Imagenet validation set accuracy and the ASRat the end of each cleaning epoch for the models pre-trained on the CC3M dataset. We only show themodels finetuned with LftMMCL + LftSSL. We observe that having just 5 poisoned datapoints in the finetuningdataset severely lessens the effectiveness of CleanCLIP for both the pre-training objectives. However, for themodels pre-trained with just LpreMMCL, we found cleaned models that maintain the original models accuracyand get around 30-50% ASR. On the other hand, for the models pre-trained with the stronger objectiveLpreMMCL + LpreSSL, having just 5 poisoned examples renders the cleaning procedure completely ineffective. Nomodel has an ASR lower than 90% for this model.",
  "FEffectiveness of CleanCLIP when Poison is Induced using a Different Backdoor": "In this experiment, we poison models using a different kind of backdoor: label consistent backdoor. In thisbackdoor, we add a trigger patch to an image whose caption contains the adversary chosen label, in ourexperiment banana. Therefore, in this case, the adversary does not need to change the labels of the poisoneddatapoints. Similar to the previous experiments, we trained two models, one using LpreMMCL and the otherusing LpreMMCL + LpreSSL on the CC6M dataset that had 3000 label consistent poisoned datapoints. We trainthe models from scratch using a starting learning rate of 1e 3 using cosine scheduling with 10,000 warmupsteps with AdamW optimizer. After training the models, we chose two models that had similar accuracy and cleaned them using CleanCLIP,i.e., finetuned them using a clean dataset of 100K image-text pairs using LftMMCL +LftSSL, using several learningrates (refer to Appendix D for hyperparameter details). We measure the Top-1 Imagenet validation setaccuracy and ASR for the models at the end of each cleaning epoch and plot the scatter plot for the twometrics in . We observe that similar to BadNet poisoning, CleanCLIP is much more effective for the model trained withthe simpler LpreMMCL objective. The model trained with LpreMMCL + LpreSSL lose 16% accuracy compared to themodel trained with LpreMMCL that gains 10% accuracy, to obtain a model with a low ASR ( 5%).",
  "GCleaning with a Larger Cleaning Dataset": "In this experiment, we doubled the size of the finetuning data to 200K, which is guaranteed to be clean,and finetuned the pre-trained model on this dataset using 14 learning rates for 20 epochs. showsthe scatter plot of the Top-1 Imagenet validation set zero-shot accuracy and the ASR at the end of eachcleaning epoch. Even after doubling the finetuning data size, CleanCLIP is ineffective for the LpreMMCL + LpreSSLpre-trained model, as it loses about 90% of the original accuracy to get an ASR 5%. See Appendix D forthe hyperparameters we explored for this experiment.",
  "HCleaning with More Finetuning Epochs": "In this experiments, we finetuned the LpreMMCL + LpreSSL pre-trained model on CC6M for upto 100 epochs using12 learning rates. shows the scatter plot of the Top-1 Imagenet validation set zero-shot accuracyand the ASR at the end of each cleaning epoch. Even after finetuning for 5 the number of original epochs,CleanCLIP is ineffective in removing the strongly induced poison, as the pre-trained model loses about 24%of the original accuracy to get an ASR 5%. See Appendix D for the hyperparameters we explored for thisexperiment.",
  "ICleaning with Larger Weights for SSL Term": "Bansal et al. (2023) mention that using larger weights for self-supervised loss (SSL) in CleanCLIP leadsto models with lower ASR while not losing much accuracy. To test this observation, we finetuned modelspre-trained with LpreMMCL + LpreSSL on the CC6M dataset with higher SSL weights: 2, 4, 6, and 8. Each of theseweights were used with four learning rates. shows that none of the higher SSL weights is ableto successfully clean the model, and there is no clear trend of the improvement in the Pareto-frontier withhigher SSL weights indicating that our results are not limited by the weights we experimented with. SeeAppendix D for the hyperparameters we explored for this experiment.",
  "JExamples of Images with Trigger and Captions with the Target Label": "In this section, we provide a few examples of the images from the CC6M dataset when a trigger patch isadded to them. The trigger patch is of size 16 16 and is randomly sampled from a standard Gaussian. It isplaced at a random location in the image. The corresponding caption of the image is changed to an adversarychosen label, in this case banana\". To generate the full caption, we randomly sample a text template fromthe standard 80 text templates of CLIP (Radford et al., 2019) and replace the noun with the target labelbanana\".",
  "SSL weight 1Pre-trained modelSSL weight 2SSL weight 4SSL weight 6SSL weight 8": ": Scatter plot of the Top-1 zero-shot Imagenet validation set accuracy v/s the ASR during the cleaningprocess for the model pre-trained on the CC6M dataset. The finetuning is done with LftMMCL + LftSSL, with varyingSSL weights (1, 2, 4, 6, and 8). We measure accuracy and ASR at the end of each epoch. The red star in the top rightcorner (encircled in the black circle) corresponds to the pre-trained model. Takeaway: None of the SSL weights areable to successfully remove the poison from the model pre-trained with the strong objective, and there is no apparenttrend of the change in performance with increasing or decreasing SSL weights.",
  "K.1Cleaning Trajectories for the Models Pre-trained using MMCL": "In this section, we provide the Top-1 zero-shot Imagenet validation set accuracy vs. the ASR of the modelduring its cleaning procedure for the LpreMMCL pre-trained model on CC6M dataset. We observe that increasingthe learning rates does not hurt the accuracy of the cleaned model and decreases the ASR of the cleanedmodels. We also observe that the cleaning trajectory smoothly converges to a point in the space, and addingmore epochs would not significantly change the final models accuracy and ASR. This points out the stabilityof the cleaning procedure for the models pre-trained on LpreMMCL.",
  "K.2Cleaning Trajectories for the Models Pre-trained using a combination of MMCL and SSL": "In this section, we provide the Top-1 zero-shot Imagenet validation set accuracy vs. the ASR of the modelduring its cleaning procedure for the LpreMMCL + LpreSSL pre-trained model on CC6M dataset. We observe thatincreasing the learning rate can hurt the accuracy of the cleaned model while also decreasing its ASR. Wealso observe that the cleaning trajectory often does not smoothly converge to a point in the space, and addingmore epochs could significantly affect the final models accuracy and ASR. This points out to the instabilityof the cleaning procedure for the models pre-trained on LpreMMCL + LpreSSL."
}