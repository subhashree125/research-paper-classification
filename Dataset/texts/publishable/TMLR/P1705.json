{
  "Abstract": "Recently, Vision-Language foundation models like CLIP and ALIGN, which are pre-trainedon large-scale data have shown remarkable zero-shot generalization to diverse datasets withdifferent classes and even domains. In this work, we take a step further and analyze whetherthese models can be adapted to target datasets having very different distributions andclasses compared to what these models have been trained on, using only a few labeledexamples from the target dataset. In such scenarios, finetuning large pretrained modelsis challenging due to problems of overfitting as well as loss of generalization, and has notbeen well explored in prior literature.Since, the pre-training data of such models areunavailable, it is difficult to comprehend the performance on various downstream datasets.First, we try to answer the question: Given a target dataset with a few labelled examples,can we estimate whether further fine-tuning can enhance the performance compared to zero-shot evaluation?by analyzing the common vision-language embedding space. Based onthe analysis, we propose a novel prompt-tuning method, PromptMargin for adapting suchlarge-scale VLMs directly on the few target samples. PromptMargin effectively tunes thetext as well as visual prompts for this task, and has two main modules: 1) Firstly, we usea selective augmentation strategy to complement the few training samples in each task; 2)Additionally, to ensure robust training in the presence of unfamiliar class names, we increasethe inter-class margin for improved class discrimination using a novel Multimodal MarginRegularizer. Extensive experiments and analysis across fifteen target benchmark datasets,with varying degrees of distribution shifts from natural images, shows the effectiveness ofthe proposed framework over the existing state-of-the-art approaches applied to this setting.Code:",
  "Introduction": "With the rapid advancement of deep learning models, it is now possible to achieve very high performancefor tasks like classification, etc., where large amounts of training samples can be collected and annotated.However, in real-world scenarios, the difficulty in curating huge amounts of labelled data has led to researchin Few-Shot Learning (FSL), where a model trained on a large dataset can be transferred to a downstreamtask having few labelled samples from unseen categories. Furthermore, the target distribution can be verydifferent from the source distribution, making the problem even more challenging. Recently, vision-languagefoundation models like CLIP have shown remarkable generalization capabilities in the zero-shot scenar-ios (Radford et al., 2021). Efficient finetuning techniques like Prompt learning (Zhou et al., 2022b;a) have",
  "Published in Transactions on Machine Learning Research (01/2025)": ": Visualizations of the augmentations being selected by the proposed Selective Augmentation modulefor EuroSAT and ISIC with 20 initial augmentations. Each row corresponds to a particular class. The redborders denote the ones which are discarded. We observe that augmentations where the region of interestare removed or darkened, have fairly more chance of getting discarded.However, in some cases, moreaugmentations may be needed to be removed than only five.",
  "Here, we briefly discuss the related work on prompt learning and OOD few-shot learning": "PromptLearningforVision-LanguageModel(VLM): The recent emergence of foundationVLMs like CLIP(Radford et al., 2021), and ALIGN(Jia et al., 2021), has changed the landscape ofdeep learning.These models are trained on abundant web-scale data, where they align the image-textrepresentations in a contrastive manner, exhibiting remarkable zero-shot performance. Though such modelsgeneralize well to most cases, leveraging the knowledge learned by these models for downstream tasksis a challenging task. Recently, prompt learning has emerged as an effective choice for finetuning these",
  "Analyzing the CLIP Representation Space for target datasets": "Foundation models like CLIP have been pretrained on a large corpus of web-scale data, which is not publiclyknown, and hence may span a wide variety of classes and domain data, ranging from standard academicdatasets to specialized datasets.Recent papers likeUdandarao et al. (2024) have studied the possiblerelationship of zero-shot performance of CLIP with the concept frequencies in pretraining datasets.Apertinent question in this scenario is, Given a target dataset with a few labelled examples, can we estimatewhether further fine-tuning can enhance the performance compared to zero-shot evaluation? The zero-shotgeneralizability of the model depends upon both the distribution and semantic difference of the target datasetfrom the CLIP training dataset. Since the source dataset is unavailable and only few samples of the targetdataset are provided, directly estimating distribution difference and also understanding whether the targetclasses are seen or not is not straightforward. Here, we propose a simple method to estimate the extentof the distribution shifts of some of the target datasets, by relating their inter-class mean image and textembedding distances in the CLIP representation space. We consider some representative datasets from BSCDFSL (Guo et al., 2020) and MetaDataset (Triantafillouet al., 2019) benchmarks to illustrate this point. We estimate the distribution and semantic difference usingthe mean inter-class L2 distances of both text and image features from the frozen zero-shot CLIP model.The class text features obtained by passing A photo of [CLASS] through the zero-shot model is denoted",
  "mV 2(3)": "When the target dataset Dtarget has significant semantic and distribution difference with the original trainingdata, the CLIP model will not be able to distinguish between the image and text embeddings of the targetdataset. Thus, the values of mT and mV will be smaller and the difference diff Dtarget will be larger andvice-versa. Since, the extracted feature vectors are normalized between 0 and 1, we subtract 2 as an offsetfrom diff Dtarget to set its minimum value to zero, while preserving the relative differences. shows the difference along with the accuracy of zero-shot CLIP (ZS-CLIP) and MaPLe (Khattaket al., 2023), where both the image and text encoders were fine-tuned using the few available labeled targetsamples using prompt learning. We observe that for the first four datasets, diff Dtarget is large, i.e. theclasses are not well separated, and thus there is scope for improvement over zero-shot CLIP accuracy. Thisis consistent with the improvement obtained with MaPLe. Similarly, ZS-CLIP performs very poorly whenplaceholder classnames are used instead of original names due to their unavailability (Plantae, Traffic Signsand MSCOCO datasets), thus implying the possibility of significant performance improvement, as can beseen in MaPLe. On the other hand, for mini-ImageNet and Aircraft, since diff Dtarget is low (between 0and 1), the classes are already well separated in the latent space, and fine-tuning using few samples canadversely affect the model, thereby justifying the drop in accuracy of MaPLe. We also compute the Pearsoncorrelation between the ZS-CLIP performance and the diff Dtarget metric , which is 0.713, and shows astrong negative correlation. This conforms with the empirical analysis above, suggesting that an increase inthe diff Dtarget metric results in reduced zero-shot performance of CLIP. This analysis illustrates that the relative separations between image and text features for the differentclasses in the CLIP representation space is crucial in explaining the zero-shot performance on the respectivedownstream datasets. We address this issue by introducing a simple and effective regularization frameworkfor prompt tuning, where we guide the image and text features to separate out in the feature space. We nowformally give the problem definition and describe our proposed approach in detail.",
  "Proposed PromptMargin Framework": "The proposed PromptMargin works in a completely source-free setting, i.e. we directly finetune the CLIPmodel on the small number of samples provided in the support set of the target dataset.We do notperform any meta-training on a separate source domain like mini-ImageNet as the existing state-of-the-artapproaches(Song et al., 2024). For this, we utilize prompt learning in both the textual and the visualbranches, similar toKhattak et al. (2023) as described in the previous section. Given the support andquery set from a randomly sampled episode, we train the prompts on the few samples available in thesupport set, and evaluate the model performance on the query set.In particular, suppose (X, y) S,where X RkCHW denotes the k images in the support set, and y {c1, c2, ..., cN} denotes the Nclassname texts. We append the textual and visual prompts to the classname texts and images respectively,and pass it through the CLIP encoders (ft and fv).Let the final text and vision embedding vectorsobtained be denoted asXT andXV (Sec.4).Next, the prompts are learned through a cross-entropyloss objective, while the encoder parameters are kept frozen. The objective function can be written as follows:",
  "LCE = argmin{t,v}E(X,y)S L(sim( XT , XV ), y)(4)": "where, sim(.) denotes the cosine similarity. In this work, we aim to address the two important issues ofthis problem: (i) less data in the support set, (ii) unknown/specialized categories in the target domains.To address data scarcity, we use a Selective Augmentation strategy, where we only select the imageaugmentations, whose embeddings are close enough to the respective text embeddings in the joint represen-tation space. In order to address the second challenge, we propose a Multimodal Margin Regularizer",
  "added prompts": ": An overview of our proposed PromptMargin framework. A randomly sampled episodefrom the target dataset is considered. The support set images along with their augmentations are passedthrough the CLIP image encoder, and their labels are passed though the CLIP text encoder. The selectiveaugmentation strategy selects augmentations based on the embedding vectors. The Max-Margin Regularizer(MMReg) enforces the class-wise image prototypes and the text embeddings to uniformly separate out. (MMReg) which uniformly separates the class-wise image and text prototypes in the joint feature space,thereby enforcing inter-class variability. The proposed framework is illustrated in . We now describethe two modules in detail.",
  "Selective Augmentation": "We use a selective augmentation strategy to increase the handful number of support set samples in thetarget dataset. For example, in the 5-way, 1-shot setting, we have only 5 images (one image from eachclass) for the model to train on. Naturally, training the prompts on such less number of data samples cancause overfitting, hence, reducing the accuracy on the query set. To address this problem, we first takea combination of different augmentations of the support set images like HorizontalFlip, RandomRotation,ColorJitter, etc. However, instead of considering all the augmentations, we efficiently select only a few ofthe above augmented images, which works equally good, or better than taking all the augmentations, witha reduction in training time. Let us consider the support set images as Xsorig, and the respective augmented versions as Xsaug. The totalsupport samples can then be written as Xs = {Xsorig; Xsaug}. Consider the 5-way setting. We have fiveclassnames (XT ), with which we append the learnable textual prompts and pass them through the frozenCLIP text encoder ft, to obtain five text embeddings. Similarly, we append learnable visual prompts to Xs and pass them through the CLIP image encoder fv, and get the image embeddings. In the vision-languagemultimodal space, the LCE tries to train the prompts such that the respective class text embeddings and theimage prototypes (mean of the class-wise image embeddings) come closer. For the selective augmentationstrategy, we choose a subset of Xs, whose cosine similarity with the corresponding class text embedding inthe feature space is higher, i.e.,",
  "Xsel,r = topr(sim(fv([v; Xs]), ft([t; XT ])); s.t., Xsel,r Xs.(5)": "Here, topr(.) function takes the r top values of its argument and Xsel,r is the r selected images from theset of all images Xs. Unlike scenarios where large number of training samples may be present and strongaugmentations may be more beneficial, in this application with as few as one example per class, it is importantthat the augmentations are trained with class representative examples, which aids the model training.",
  "Plant Disease": ": Effectiveness of the MMReg. All the heatmaps represent inter-class L2 distances betweenembeddings for a representative episode in the 5-way setting. Darker hues represent higher values and viceversa. In all the images, the text features are followed by image features from left to right. (a) representsinitial text and image feature distances, (b) represents the text and image embedding distances withoutMMReg, (c) represents the text and image distances with only text regularization, while (d) represents thetext and image embeddings with MMReg. We observe in (b) that there is significant difference between theinterclass distance of the image and text embeddings, implying that their embeddings are not that similar.Using only text regularization part separates the text but not the image features. In contrast, the maps arevery similar for (d), which justifies the usefulness of the MMReg. We perform a simple experiment to justify this selection strategy. Here, we initially generate different numberof augmentations for each support image and then select 15 examples based on the proposed selection strategy. shows the accuracies and training time for the four datasets in the BSCDFSL benchmark (Guo et al.,2020). We note that if the difference in the number of original and selected augmentations is low, the timeand accuracy differences are not significantly impacted, and it may be feasible to consider all the originalaugmentations instead of a selection strategy. However, as the difference increases, it is worth consideringthis strategy due to time and accuracy considerations. Only for PlantDisease, we observe a slight decreasein performance, which upon analyzing the augmented images, we feel can be attributed to the quality ofthe initial augmentations. In conclusion, our proposed strategy reduces the training time by 40% whilemaintaining or even improving the accuracies consistently across the four datasets in majority of the cases.",
  "Multimodal Margin Regularization": "When deploying the model on a downstream task, the target dataset can contain examples of novel categorieswhich are unseen and very different from what was encountered during pretraining. These data can be fromspecialized domains, e.g., EuroSAT(Helber et al., 2019) which contains satellite images, Chest X-Ray(Wang et al., 2017), containing medical X-Ray images, plant disease data (Mohanty et al., 2016), etc. Since",
  "MaPLe + sel. augs85.4964.6933.5482.66MaPLe + sel. augs + text reg85.7664.6533.4482.54MaPLe + sel. augs + MMReg (PromptMargin)87.0166.1333.9083.48": "the CLIP model may not have seen similar data during training, it may not be able to generalize to theseclasses and bring their text and image embeddings close in the multimodal latent space using few labeledtraining examples. In addition, generalization of CLIP significantly depends on the presence of meaningfulclass names of the unseen categories. But often, such class names may not be provided, or even if they areavailable, they may not be semantically meaningful in the CLIP space. We have observed this in , which demonstrates a clear correspondence between zero-shot performance and the joint feature spacealignments of the text and image embeddings. Thus, while finetuning with less amount of support set data,the model may not be able to generalize and discriminate between these classes. Now, we describe theproposed Multimodal Margin Regularizer (MMReg), which tries to simultaneously improve the inter-classdiscrimination and bring the inter-modal embeddings closer. Our proposed MMReg is inspired fromHayat et al. (2019), where the regularizer addresses the trainingdata imbalance problem in classification tasks by uniformly spreading out the classifier weights in thefeature space. In our context, we aim to spread out the text embeddings in the feature space to avoidconfusion between the distinct classes. The regularization term can be expressed as follows:",
  "i<j( XTi XTj22 t)2, j {2, 3, ..., N}(6)": "Here, N denotes the number of classes in each episode, in a N-way k-shot setting. Each classname text,appended to learnable prompts is passed through the text encoder to obtain the classwise text embeddingvectorsXT . The mean distance between these text embeddings is denoted by t and can be written as:",
  "i<j XTi XTj22, j {2, 3, ..., N}(7)": "This regularizer trains the prompts in such a way that the text representations are uniformly separated bya distance of t. Since, the text prompts are coupled to the visual prompts through a function F(.), it isexpected that this regularization term will also guide the visual representations to separate out. However,because of the presence of few training samples, we empirically observe that there is a lack of consistentseparation between the image embedding prototypes compared to their textual counterparts. We considerfour datasets to illustrate this in , where we observe that simply separating text features reduces theperformance of prompt tuning (MaPLe). We can also visualize this from the L2 distance heatmaps shown in where the visual features are not affected by the text regularizer. Hence, we add another regularizationterm to the loss function, to separate out the image prototypes as follows:",
  "i<j( XVi XVj22 t)2, j {2, 3, ..., N}(8)": "where,XV are the prototypes (means) of the class image embeddings obtained from the visual encoder, i.e.,fv([v; Xsel]). t is the same mean distance from Eqn. (7). This additional regularization term enforces theclass-wise image prototypes to be equally separated by the same mean distance as the text representationembeddings. We observe in that the proposed MMReg improves performance in all the cases. Wealso see this in where the classname representations and the image prototypes are better separatedin the feature space. Thus, the final loss function for training the prompts is given as:",
  "Dataset Description and Experimental Protocol": "The proposed PromptMargin works in a source-free setting, i.e.the model is directly fine-tuned on theepisodic support set of the target datasets. Thus, unlike many of the prior approaches, we do not require asource domain like ImageNet for pre-training our model. We conduct experiments on fifteen target bench-mark datasets with varying distribution shifts, namely, EuroSAT (Helber et al., 2019), ISIC (Codella et al.,2019), Plant Disease (Mohanty et al., 2016), Chest X-Ray (Wang et al., 2017) (from BSCDFSL (Guo et al.,2020)), Omniglot (Lake et al., 2015), Traffic Signs (Houben et al., 2013), MSCOCO (Lin et al., 2014), Tex-tures (Cimpoi et al., 2014), CUB (Wah et al., 2011), Quickdraw (Jongejan et al., 2016), Aircraft (Maji et al.,2013), VGG Flower (Nilsback & Zisserman, 2008), Fungi (Schroeder & Cui, 2018), mini-ImageNet (Vinyalset al., 2016) (from Metadataset (Triantafillou et al., 2019)), and the iNaturalist Plantae dataset (Van Hornet al., 2018). These datasets contain images ranging across different styles and categories, including medicalimages, satellite images, handwritten character images, etc. As in the existing literature, we consider two settings for the experiments, namely, 5-way 1-shot and 5-way5-shot, where one and five images from each of the five classes are randomly sampled in each episode fortraining. Following standard protocol (Guo et al., 2020; Liu et al., 2020; Liang et al., 2021), we also take 15query images from the same set of classes, and evaluate the model on 600 episodes, and report the averageaccuracies and 95% confidence intervals. Implementation Details: We use the CLIP ViT-B/16 backbone for all our experiments, similar to MaPLe(Khattak et al., 2023). For the learnable text prompts, we initialize the vectors with the standard text promptA photo of a. The function F(.) is taken as a linear layer which projects the text prompts to visual prompts.The vision and text prompt lengths are set as 2. For deep prompting, we introduce learnable prompts beforeevery transformer block upto a depth of 9. For the 1-shot setting, we generate multiple augmentations, outof which we selectively choose 15 augmentations as discussed. Similarly, for the 5-shot setting, we consider3 selective augmentations, such that there are 15 examples per class. We jointly train the prompts on thesupport set images to minimize the final loss function, with SGD optimizer for 150 epochs with a learningrate of 0.01 and momentum of 0.9. Following Liang et al. (2021), we keep the above hyperparameters sameacross all datasets and settings. All the experiments are performed on a single NVIDIA RTX A5000 GPU.Now we report the results of experimental evaluation.",
  "Comparison to the state-of-the-art methods": "We compare our method with the most recent CLIP-based methods for all the fifteen datasets and reportthe results in . This includes full finetuned methods as well as prompt-tuning methods of CLIP.We also explicitly mention the backbones as well as different training procedures followed by the differentapproaches, which should be considered while comparing them. A recent state-of-the-art approach, FDAlign (Song et al., 2024), was the first to investigate CLIPs gen-eralization capability to the few-shot learning datasets with domain differences under similar settings. Itproposes a CLIP finetuning technique, which is meta-trained on the miniImageNet dataset before adaptingto the target datasets. Wise-FT (Wortsman et al., 2022) was originally proposed for the domain generaliza-",
  "WiSE-FTFD-AlignMaPLePromptMarginWiSE-FTFD-AlignMaPLePromptMargin": "EuroSAT63.99 0.3960.39 0.4375.46 0.1978.95 0.1980.96 0.1977.25 0.1689.55 0.1091.40 0.10ISIC29.40 0.3428.84 0.4431.96 0.1533.90 0.1539.54 0.4038.91 0.4445.92 0.1446.88 0.16Plant Disease75.66 0.3375.13 0.3379.38 0.2283.48 0.1991.78 0.3191.84 0.1993.32 0.1194.31 0.10ChestX22.27 0.2822.31 0.1721.30 0.1021.51 0.1025.08 0.1424.95 0.1523.29 0.1023.92 0.09iNaturalist Plantae55.34 0.2966.13 0.2783.07 0.2685.36 0.19Omniglot83.56 0.2883.81 0.2577.82 0.2987.01 0.2295.26 0.0994.81 0.1996.23 0.1096.37 0.13Traffic Signs60.84 0.2957.32 0.2656.45 0.2467.24 0.2478.11 0.2473.39 0.2985.21 0.1987.55 0.16MSCOCO67.28 0.3269.16 0.2853.09 0.2456.12 0.2481.08 0.3581.37 0.2475.13 0.2278.68 0.23Textures63.55 0.1966.05 0.1279.28 0.1878.99 0.2083.31 0.3183.60 0.3488.45 0.1488.71 0.15CUB81.16 0.7182.38 0.6996.96 0.2296.97 0.2293.41 0.3293.87 0.2497.65 0.0697.12 0.06Quickdraw62.54 0.5964.49 0.5872.54 0.2274.84 0.2082.78 0.3782.78 0.2885.08 0.1485.21 0.13Aircraft62.64 0.6263.45 0.6579.76 0.2777.21 0.2677.66 0.5978.21 0.5887.56 0.2186.53 0.20VGG Flower94.16 0.2393.50 0.2498.24 0.0697.65 0.0799.06 0.0998.95 0.0999.23 0.0299.27 0.02Fungi53.10 0.2753.83 0.3058.55 0.2761.16 0.2473.28 0.1073.69 0.1479.69 0.2080.91 0.18Mini-test93.55 0.1795.04 0.1899.17 0.0398.85 0.0398.44 0.0698.52 0.0799.39 0.0299.19 0.02",
  "Average65.2665.4069.0272.0078.5578.0181.9282.76": "tion task, where the CLIP model was finetuned with parameter weight interpolations, which was adaptedto the given setting. We compare with both of these methods, and include their performance accuracies asreported in Song et al. (2024). Additionally, for prompt learning methods we report MaPLe (Khattak et al.,2023), which is a vision-language prompt tuning method, which serves as another strong baseline for ourmethod. We adapted this in our setting, where we finetune it directly on the few samples from the sampledepisodes of the target datasets. Hence, we primarily explore whether CLIP can be deployed using prompt learning on very few samples onthe target dataset without any meta-training on a large-scale dataset like ImageNet. Here, in addition tothe challenge of robustly learning prompts with few samples, for many datasets, the class names are eithernot provided or are not quite meaningful for CLIP to generalize, thus making prompt-tuning even morechallenging. For the 5-way 1-shot setting, we observe that both full finetuning methods (FDAlign and Wise-FT) performpoorly compared to the parameter efficient finetuning methods.Our proposed method outperforms thebaseline method MaPLe in eleven out of fifteen datasets, achieving an average accuracy of 72% comparedto MaPLes 69.02%. In some cases, where original classnames were not present or are semantically notmeaningful, e.g., Plantae, Plant Disease, Traffic Signs, we achieve absolute accuracy gains of 10.79%, 4.10%,10.79% respectively over MaPLe, highlighting the fact that our method gives significant improvement evenwhen original classname texts are not present in the datasets. This is discussed in details further in thefollowing section. However, our method exhibits slight decrements on certain datasets (like mini-ImageNetand Aircraft), which can be attributed to the dataset distributions being not so shifted in the CLIP space, asdiscussed later. For the 5-way 5-shot setting, we observe a similar trend, where PromptMargin outperformsMaPLe in twelve out of fifteen datasets, achieving an average accuracy of 82.76%. Our method aims to highlight that large-scale VLMs like CLIP can be efficiently transferred to out-of-distribution datasets with few-shot samples, without any access to source datasets, and can still provide arelatively close performance to meta-trained and finetuned methods.",
  "DatasetEuroSATISICOmniglotPlant DiseaseQuickdrawPlantaeTraffic SignsMSCOCOmini-ImageNet": "mT0.5880.5610.6790.7700.7160.1000.1000.1000.860mV0.6270.5820.4200.5400.5200.7270.5831.0101.010MaPLe (%)75.4631.9677.8279.3872.5455.3456.4553.0999.17MaPLe + MMReg (%)75.8132.4978.4679.5973.0258.0459.2451.9098.99 our framework is built upon MaPLe (Khattak et al., 2023), we report its accuracies as the baseline method.As illustrated in the table, both the proposed modules improve the baseline method significantly. We had proposed the MMReg module based on the observations of the feature alignments in the jointCLIP vision-language space.Now, we see in how this simple regularization term is effective inguiding the vision language features to separate out even for a single example per class.As noted in, when the inter-class text and image embedding distances (mT and mV ) were low, prompt-learning(MaPLe) had improved the performance, but additionally incorporating our regularizer further improves theclass discriminations and results in better accuracy.Notably, we observe that when the image featureseparation (mV ) is low, and placeholder classnames have been used, the improvement with our MMRegmodule is significant (+2.7 and +2.79 for Plantae and Traffic Signs respectively). Although in MSCOCO,pseudo-classnames have been used, the image features are extremely well separated, resulting in the MMRegmodule slightly decreasing the performance. However, utilizing only the text regularization term R( XT )improves the prompt-learning accuracy by +1.28, hence conforming with our proposed notion of separatingout closely situated embeddings in the representation space. For mini-ImageNet, since both modalities arewell separated, prompt-learning, even with our regularizer, on the few samples does not improve results,and can adversely affect the latent space alignment. We also illustrate some qualitative results in forvisualization of some of the different datasets. Scope for future work: Although the two modules of our proposed framework demonstrate good perfor-mance for most of the datasets, in few cases, it failed to outperform MaPLe. As for the first module, a lack ofcarefully chosen augmentation strategies may hurt the generalization more than it improves. As an example,in , we illustrate some poor augmentations generated for the Aircraft dataset, where our methodfails to improve over MaPLe. Similarly, in some cases, where the features in the latent space are alreadywell separated, further finetuning with MMReg may adversely affect the CLIP space, hence reducing theperformance. Nevertheless, this work may serve as a strong baseline for robust prompt learning techniquesof foundation models like CLIP for such challenging and real-world settings.",
  "Conclusion": "Large-scale vision-language models like CLIP are emerging as a popular choice due to their powerful zero-shot generalization capabilities. Prompt learning is an efficient technique to transfer CLIP-like models todownstream datasets with few samples. However, to the best of our knowledge, there has been no workwhere prompt learning has been utilized for classification tasks where the datasets simultaneously containfew samples as well as a shift in distribution from natural images. In this work, we explore the possibility oflearning only a few prompt parameters on the target datasets, in a completely source-free manner. Extensive",
  "experiments on standard benchmark datasets highlight the efficacy of our proposed approach over state-of-the-art methods": "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.Reproducible scaling laws for contrastivelanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 28182829, 2023. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describingtextures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 36063613, 2014. Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, BrianHelba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanomadetection 2018: A challenge hosted by the international skin imaging collaboration (isic). arXiv preprintarXiv:1902.03368, 2019.",
  "ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXVII 16,pp. 124141. Springer, 2020": "Munawar Hayat, Salman Khan, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Gaussian affinity for max-margin class imbalanced learning. In Proceedings of the IEEE/CVF international conference on computervision, pp. 64696479, 2019. Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deeplearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in AppliedEarth Observations and Remote Sensing, 12(7):22172226, 2019. Sebastian Houben, Johannes Stallkamp, Jan Salmen, Marc Schlipsing, and Christian Igel. Detection of trafficsigns in real-world images: The german traffic sign detection benchmark. In The 2013 international jointconference on neural networks (IJCNN), pp. 18. Ieee, 2013. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In International conference on machine learning, pp. 49044916. PMLR, 2021.",
  "Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning throughprobabilistic program induction. Science, 350(6266):13321338, 2015": "Hanwen Liang, Qiong Zhang, Peng Dai, and Juwei Lu.Boosting the generalization capability in cross-domain few-shot learning via noise-enhanced supervised autoencoder. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 94249434, 2021. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014. Bingyu Liu, Zhen Zhao, Zhenpeng Li, Jianan Jiang, Yuhong Guo, and Jieping Ye. Feature transformationensemble model with batch spectral regularization for cross-domain few-shot classification. arXiv preprintarXiv:2005.08463, 2020.",
  "Sharada P Mohanty, David P Hughes, and Marcel Salath. Using deep learning for image-based plant diseasedetection. Frontiers in plant science, 7:1419, 2016": "Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian conference on computer vision, graphics & image processing, pp. 722729. IEEE, 2008. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021.",
  "Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances inneural information processing systems, 30, 2017": "Kun Song, Huimin Ma, Bochao Zou, Huishuai Zhang, and Weiran Huang. Fd-align: Feature discrimina-tion alignment for fine-tuning pre-trained models in few-shot learning. Advances in Neural InformationProcessing Systems, 36, 2024. Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 403412, 2019. Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset of datasets forlearning to learn from few examples. arXiv preprint arXiv:1903.03096, 2019. Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip HS Torr, Adel Bibi, SamuelAlbanie, and Matthias Bethge. No\" zero-shot\" without exponential data: Pretraining concept frequencydetermines multimodal model performance. arXiv preprint arXiv:2404.04125, 2024. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, PietroPerona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 87698778, 2018.",
  "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011": "Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and local-ization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 20972106, 2017. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuningof zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition, pp. 79597971, 2022. Chengming Xu, Yanwei Fu, Chen Liu, Chengjie Wang, Jilin Li, Feiyue Huang, Li Zhang, and XiangyangXue. Learning dynamic alignment via meta-filter for few-shot learning. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pp. 51825191, 2021. Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong-sheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv preprintarXiv:2111.03930, 2021. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition, pp. 1681616825, 2022a.",
  "AChoice of the number of Selective Augmentations": "In general, generating more augmentations can improve performance by mitigating overfitting in few-shotlearning, but these can also include bad augmentations. The proposed Selective Augmentation module aimsto efficiently select a subset of the generated augmentations, removing worse augmentations in the featurespace, and hence maintaining a competitive performance while reducing the training time significantly asdiscussed in .1. Here, we report a sensitivity analysis on the number of selective augmentationsin for two datasets, EuroSAT and ISIC. As expected, for very few augmentations (say, 5), due toless number of examples, the results are in general lower. After around 10 augmentations, the performancestabilizes and does not vary significantly with the number of augmentations. We have chosen 15 augmen-tations in all our experiments to achieve good performance in reasonable training time, since for few-shotsetting, it is not possible to tune hyperparameters for individual datasets. Dynamically choosing the numberof selective augmentations in an episodic manner can be an interesting direction for future work.",
  "BAnalysis of Selective Augmentations": "We first randomly generate a large number of augmentations using standard techniques like RandomHori-zontalFlip, RandomResizedCrop, ColorJitter, etc. The initially generated augmented images are a randommixture of the various augmentation strategies and produces a variety of augmentations which differ acrossepisodes as well as datasets. The selective augmentation module then selects 15 augmentations from thesesamples based on their cosine similarity with the corresponding class text embeddings in the feature space.To understand which augmentations are being chosen implicitly, we perform an experiment in which insteadof taking random augmentations, we fix the number of augmentations of each type. E.g., we generate 6images by RandomCropping, followed by 6 images by ColorJitter and so on, and then analyze which arebeing discarded by this proposed module.We observe that in general, the augmentations generated byRandomCropping are discarded, where the region of interest is being removed, followed by ColorJitter,which sometimes blurs or blackens the images. Analysis on EuroSAT and ISIC datasets is illustrated in .",
  "CWeak vs strong augmentations": "Strong augmentations are intense transformations of the images like AutoAugment and CutOut, whereasweak augmentations are simple transformations like Flip and Shift. In our work, we have used standardaugmentations like RandomCrop and HorizontalFlip, which can be attributed to weak augmentations, fromwhich we have then selected a subset based on the feature space similarities. To understand the effect ofstrong augmentations instead of standard augmentations (as originally used), we consider two benchmarkdatasets, EuroSAT and ISIC and apply RandomErasing (which removes a tile from the image similar toCutOut), and AutoAugment. The results in suggest that, in general, such aggressive augmentationstrategies do not help in classification as it distorts and removes crucial information from the images, therebyhurting the generalization. In very few instances, such strong augmentations can crop the background andfocus on the region of interest in datasets like ISIC. Some visualizations are shown in Figs. 7 and 8.",
  "DExperiments on an alternate VLM": "We have reported results for CLIP ViT-B/16 in the main text for fair comparison to other prompt tun-ing methods. Here, we consider an alternate VLM, OpenCLIP B/16 (Cherti et al., 2023), which has beenpretrained on a different dataset. The results on some benchmark datasets (with the same hyperparame-ters) are reported in . We observe that although MaPLe improves over the zero-shot performance,PromptMargin outperforms MaPLe in all the datasets, further justifying the effectiveness of the proposedframework.",
  "FEffect of MMReg in the absence of classnames": "Since in many specialized datasets (e.g., medical datasets like ISIC and Chest X-Ray), meaningful classnameshave been used, the model may still rely on them for proper classification in the CLIP space. However,often classnames may not be available for such datasets in practical settings. To emulate such a scenariowhere classnames may not be available, we replace the classnames of these specialized datasets with pseudoclassnames (like C1, C2, C3, etc.), and see the effect of the MMReg module on them (). We observethat while performance of MaPLe (prompt tuning) drops because the class semantics are now missing, ourMMReg module helps them separate out in the multimodal representation space, thereby improving theperformance in both the datasets, which conforms with our motivation for this proposed module.",
  "GSensitivity analysis": "In , we vary the weighting coefficients ( and ) of the two terms of the MMReg module (R( XT )and R( XV ) respectively for two benchmark datasets, EuroSAT and ISIC. In the main text, we have used = 1 and = 1 for all experiments, since hyperparameter tuning is not possible for few-shot learning. Butwe observe that the performance is quite stable for a wide range of parameters."
}