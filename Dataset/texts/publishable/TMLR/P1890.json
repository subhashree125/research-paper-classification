{
  "Abstract": "What distinguishes robust models from non-robust ones? While for ImageNet distributionshifts it has been shown that such differences in robustness can be traced back predomi-nantly to differences in training data, so far it is not known what that translates to in termsof what the model has learned. In this work, we bridge this gap by probing the represen-tation spaces of 16 robust zero-shot CLIP vision encoders with various backbones (ResNetsand ViTs) and pretraining sets (OpenAI, LAION-400M, LAION-2B, YFCC15M, CC12Mand DataComp), and comparing them to the representation spaces of less robust modelswith identical backbones, but different (pre)training sets or objectives (CLIP pretrainingon ImageNet-Captions, and supervised training or finetuning on ImageNet). Through thisanalysis, we generate three novel insights. Firstly, we detect the presence of outlier featuresin robust zero-shot CLIP vision encoders, which to the best of our knowledge is the firsttime these are observed in non-language and non-transformer models. Secondly, we find theexistence of outlier features to be an indication of ImageNet shift robustness in models, sincewe only find them in robust models in our analysis. Lastly, we also investigate the numberof unique encoded concepts in the representation space and find zero-shot CLIP models toencode a higher number of unique concepts in their representation space. However, we donot find this to be an indicator of ImageNet shift robustness and hypothesize that it is ratherrelated to the language supervision.",
  "Introduction": "Large pretrained multimodal models, such as CLIP (Radford et al., 2021), have demonstrated unprecedentedrobustness to distribution shifts around ImageNet1. When used as zero-shot image classifiers, their perfor-mance on ImageNet (Deng et al., 2009) translates remarkably well to performances on natural shifts ofImageNet, such as, e.g., ImageNet-V2 (Recht et al., 2019). This led to many works analyzing what actuallycauses this remarkable robustness of CLIP to shifts around ImageNet, with Fang et al. (2022) establishingthat the root cause of CLIPs robustness lies in the quality and diversity of data it was pretrained on. Inparticular, they find the robustness of CLIP to shifts around ImageNet to disappear when it is pretrainedon ImageNet-Captions, a modification of ImageNet suitable for unsupervised language-vision pretraining. While the cause of CLIPs robustness to ImageNet shifts is thus known, we set out to establish how exactlyrobustness manifests itself in features learned by the model. Finding feature patterns in the representationspace that only appear in robust models is the first step towards a better understanding of the emergenceof robustness.It is also key to diagnosing robustness in times when only limited knowledge about the(pre)training distribution or the shifts is available. To find robustness patterns in robust CLIP models,we leverage the various models provided by Ilharco et al. (2021) in the OpenCLIP repository, as well asnon-robust CLIP models pretrained on ImageNet-Captions provided by Fang et al. (2022), and models wetrain in a supervised way on ImageNet from scratch. We analyze the visual features in these models byprobing their last layer activation vectors with quantitative interpretability tools, such as kurtosis analysisof activation vectors (Elhage et al., 2023), singular value decomposition (SVD) of classifier weight matricesand concept probing of the representation space (Bau et al., 2017). Through this analysis, we distill insightson distinctive characteristics of CLIP model features and CLIP ImageNet distribution shift robustness. Our contributions. (1) We show that many robust CLIP models have outlier features. These featuresstand out as their activation is typically several orders of magnitude above the average activation offeatures in the same layer.Interestingly, this observation also holds for robust CLIP models withResNet backbones. To the best of our knowledge, this is the first time that outlier features are observedin non-language and non-transformer models (Dettmers et al., 2022; Elhage et al., 2023; Sun et al.,2024). We also show through an SVD analysis that outlier features are propagated to the logits ofdownstream classifiers, which results in what we call privileged directions that are crucial to modelpredictions. (2) Through a comparative analysis we find that outlier features are robustness indicatorsthat distinguish robust models from their non-robust counterparts. Since none of the non-robust modelsexhibit them, we find outlier features to be an indicator whose presence can indicate that a model willbe robust to distribution shifts around ImageNet. Privileged directions on the other hand, are notsuch an indicator, since they can also be found in some of the non-robust models. (3) We show thatrobust zero-shot CLIP models all encode a high number of unique concepts in their features. As aconsequence, the features of robust models are highly polysemantic, which means that they superpose alarge set of concepts. Surprisingly, we find through our comparative analysis that representations thatare rich in concepts are not necessarily more robust, as this property can also be found in non-robustImageNet-Captions pretrained CLIP models. This suggests that language supervision tends to enrichvisual representations in human concepts.",
  "Background on CLIP models and notation": "In this section, we summarize the CLIP paradigm introduced by Radford et al. (2021) and introduce thenotation used in later sections. We explain how CLIP models are trained and used for image classification. CLIP architecture.A CLIP model consists in an image encoder fv : RdX RdH and a text encoderft : RdT RdH mapping to the same representation space RdH of dimensions dH N. With our notations,dX = C H W for images with C N channels, height H N and width W N. Similarly, dT = L Vfor texts with context length L N and vocabulary size V N. Given a batch of (image, text) pairs",
  "Bb=1 exp 1 cosfvx(b)v, ftx(b)t,": "with cos denoting the cosine similarity. Lv and Lt are InfoNCE losses from Oord et al. (2018) with a learnabletemperature parameter R+. These losses induce the encoders to align the image and text embeddingpairs in the representation space RdHthrough the nominator, while separating image and text embeddingsof distinct samples through the denominator, which differs between Lv and Lt. Building zero-shot classifiers.Once the model is trained, we have access to an image and text encoderthat tend to align images with their text description. This can be used to build a zero-shot image classifierthat discriminates between a set of K N classes k [K]. The typical approach is to combine the name ofthe class k, together with a template, to create x(k)t. For instance, the class lion can be combined with thetemplate an image of a <class> to yield the text description an image of a lion. To assign a class toan input image xv RdX, one can assign logits to each class k [K] as follows:",
  "j": "for each k [K] and j [dH]. A zero-shot image classifier can thus be built from CLIP as the compositionbetween the linear classification head W and the CLIP image encoder (with normalization). For ease ofnotation, we use f fv, x xv, etc. in the remainder of this work, unless otherwise specified.",
  "Robustness of CLIP models": "In this work, we focus on the robustness of models to shifts from ImageNet (Deng et al., 2009) to thefive natural distribution shifts considered by Fang et al. (2022), namely ImageNet-V2 (Recht et al., 2019),ImageNet-R (Hendrycks et al., 2021a), ImageNet-Sketch (Wang et al., 2019), ObjectNet (Barbu et al., 2019)and ImageNet-A (Hendrycks et al., 2021b). Measuring robustness.There are different ways to measure robustness to a specific distribution shiftfrom source distribution A to shifted distribution B. In some works, especially in the literature on invariantand robust learning, simply performance on both source and shifted distribution are reported (Arjovskyet al., 2019; Makar et al., 2022; Jiang & Veitch, 2022; Kumar et al., 2022), and their quotient can bestraightforwardly computed to report robustness in a single metric (What fraction of the original performanceis maintained on the shifted data?). On the other hand, in the literature investigating the robustness of CLIPto ImageNet shifts, typically a slightly more involved metric called Effective Robustness (ER) is reported(Taori et al., 2020; Fang et al., 2022). We choose to include both, and will find that similar insights about",
  "the robustness of models can be derived from both of them when it comes to distribution shifts aroundImageNet": "Lastly, we note that recently Shi et al. (2023) have investigated the robustness of models to the five naturaldistribution shifts of ImageNet, using additionally a different data distribution than ImageNet (YFCC-15MThomee et al. (2016)) as the source distribution. In that case, none of the models investigated (includingCLIP models) is significantly more robust than the other models. Therefore, we keep the focus of our analysison the shifts from ImageNet to its five natural distribution shifts, where most zero-shot CLIP models arewithout doubt significantly more robust than their finetuned or supervised counterparts, as we will recap inthe following. Model pool.We run our analyses across five backbone architectures: ResNet50, ResNet101, ViT-B-16,ViT-B-32, ViT-L-14 (He et al., 2015; Dosovitskiy et al., 2020).For each architecture, the OpenCLIPrepository (Ilharco et al., 2021) contains robust pretrained CLIP models on various pretraining datasets: theoriginal (unreleased) OpenAI pretraining set (OpenAI, Radford et al. (2021)), YFCC-15M (Thomee et al.,2016; Radford et al., 2021), CC-12M (Changpinyo et al., 2021), LAION-400M, LAION-2B (Schuhmannet al., 2022), and DataComp (Cherti et al., 2023).Furthermore, Fang et al. (2022) provided us withcheckpoints of the non-robust CLIP models pretrained on ImageNet-Captions for three different versionsof ImageNet-Captions: a first version for which only the original titles from the internet associated tothe images are used as text (ImageNet-Captions-t), a second version for which a concatenation of originaltitles and description was used (ImageNet-Captions-td), and a last version for which a concatenationof original titles, description, and tags was used (ImageNet-Captions-tdt, for more details see Fanget al. (2022)).We load the pretrained vision encoders of all available combinations of architecture andpretraining dataset, and construct a zero-shot classification model for ImageNet using the methodologydescribed in . By finetuning each robust zero-shot model on ImageNet, we obtain classifiers withlower robustness than their robust zero-shot counterparts (Andreassen et al., 2021; Kumar et al., 2022;Wortsman et al., 2022a).Lastly, since the non-robust ImageNet-Captions models were only trained fora ResNet50 backbone, we obtain further non-robust models for the remaining architectures by trainingthem as classification models on ImageNet from scratch, and obtain models with far lower robustness thanthe finetuned ones (Fang et al., 2022) (details on model finetuning and training can be found in Appendix H). Results.For all models, we compute the test accuracy on ImageNet, on the five shifted datasets(averaged), their quotient, as well as the ER (for details on the computation of ER, see Appendix A). Wereport the two metrics of robustness (quotient of test accuracies %acc, and ER) in a and b(for the individual test accuracies that these metrics are calculated from, see Appendix B). We see thataccording to both metrics, for each type of backbone the zero-shot CLIP models from OpenCLIP have thehighest robustness (Robust zero-shot CLIP), which decreases but remains significant when fine-tuned onImageNet (Fine-tuned CLIP). On the other hand, ImageNet-Captions CLIP models (Non-robust zero-shotCLIP) and ImageNet trained supervised models (Supervised)2 do not exhibit any ER and much lowerrobustness according to %acc, typically loosing more than half of their ImageNet accuracy on the shifteddatasets (%acc dropping below 50%). Take-away 1. In agreement with the literature, we find that for each architecture, zero-shot CLIPmodels that were pretrained on OpenAI, YFCC-15M, CC-12M, LAION-400M, LAION-2B, or Data-Comp, are the most robust to ImageNet distribution shifts. This robustness, while decreased, remainssignificant after fine-tuning on ImageNet for almost all pretraining datasets. On the other hand, zero-shot CLIP models pretrained on ImageNet-Captions, and supervised models trained on ImageNet, arenon-robust to ImageNet distribution shifts.",
  "Outlier features and privileged directions": "In this section, we explain how we detect outlier features in zero-shot CLIP vision classifiers, and how wefind them to be an indication of model robustness. We start by explaining what outlier features are and howthey are surfaced, and then proceeed to analyse how they propagate to class logits in the form of privilegeddirections. We analyze both outlier features and privileged directions for each of the models in our modelpool.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Result of layer by layer CKA comparison between zero-shot CLIP and its counterpart that was fine-tuned on ImageNet for various backbones and pretraining sets (Part 2). In orange, CKA between activationvectors on ImageNet test set. In blue, CKA between activation vectors on shifted ImageNet sets (average assolid line, standard deviation in shaded blue). Typically, we see large drops of CKA in the last layer.",
  "where (h) := d1HdHi=1 hi and 2(h) := d1HdHi=1[hi (h)]2.As explained by Elhage et al. (2023),Kurtosis 3 indicates the presence of outlier features (3 being the kurtosis of an isotropic Gaussian)": "We report the average kurtosis over the ImageNet test set in a for each architecture and across thevarious levels of robustness. This leads to three insights. Firstly, across all architectures, there are robustzero-shot CLIP models with outlier features, as indicated by their Kurtosis 3. Secondly, the kurtosis, likethe robustness, drops when finetuning on ImageNet. In Appendix I we include an additional analysis showingthat Kurtosis closely tracks the ER metric when interpolating between robust zero-shot CLIP models andfine-tuned CLIP models in weight space. Lastly and most importantly, the values Kurtosis 3 obtainedfor the non-robust supervised, and non-robust zero-shot CLIP models reveal the absence of outlier featuresin non-robust models, suggesting that the presence of outlier features is an indicator of robustness that canonly be found in robust models. Privileged directions in representation space.The strong presence of outlier features in the mostrobust models does not necessarily explain the performances of these models. Indeed, it is perfectly possiblethat outlier features are ignored by the linear head computing the class logits based on the activationvectors, e.g. if they are part of ker W, the null space of the weight matrix W defined in . Thus,to assess whether outlier features are of importance, we now introduce the notion of privileged directions ofthe representation space RdH, as an instance of a generalized form of outlier features. While outlier featuresare studied in the canonical basis {e1, . . . , edH}, since we can write hi = Projei(h), they can be generalizedto be any set of directions of the representation space that receive a projection substantially above average(for more details and an illustration, see Appendix F). We focus on the directions that are important for the computation of logits by the linear head W, namelyright singular vectors of W. These can be identified by performing a singular value decomposition (SVD) ofW, which can be written as W = rank(W )i=1i uivi , where i R+, ui RdY and vi RdH respectivelycorrespond to singular values (SV), left singular vectors (LSV) and right singular vectors (RSV) of W. Inthis decomposition, each RSV vi corresponds to a direction in representation space that is mapped to thelogits encoded in the LSV vi. Since both of these vectors are normalized ui = vi = 1, the importance ofthe direction vi for W is reflected by the SV i. Note that the SV i by itself does not refer to the models",
  "(b) Direction importance ratio calculated through Equation (3)": ": Outlier features and privileged directions. Most robust zero-shot CLIP models have outlier features(high kurtosis) and privileged directions (high direction importance outlierness). Fine-tuned models have nooutlier features but still exhibit privileged directions, although those are noticeably less privileged. Supervisedmodels and non-robust zero-shot CLIP models have no outlier features. The full distribution of importancescores can be found in Appendix E.1 encoder activations. How can we measure if the direction vi is typically given an important activation by theencoder? We propose to measure the average cosine similarity of activation vectors h(n) with this direction.This leads us to a unified importance metric for each direction i [rank(W)] of the representation space:",
  "rank(W )j=1Importance(j)rank(W).(3)": "We notice that all the robust zero-shot CLIP models have such a privileged direction. For the less robustfinetuned models, these privileged directions still exist, but not they are not as strong.This indicatesthat finetuning de-emphasizes privileged directions.Finally, the importance distributions of non-robustmodels have no privileged directions with only one exception: the ImageNet-Captions CLIP models. Thesemodels have privileged directions in the absence of outlier features, which might appear surprising atfirst. By analyzing Equation (2), we deduce that these models should have some singular values that aresubstantially larger than average. As explained in , these singular values correspond to a weightmatrix obtained by stacking activations of the language encoder from the CLIP model. Since the non-robustImageNet-Captions CLIP models show little to no robustness, we deduce that privileged directions can notserve as an indication of robustness. Take-away 2. Many robust models exhibit outlier features and privileged directions in their represen-tation spaces. Since we do not find outlier features in non-robust models, outlier features appear to bean indication of model robustness to shifts around ImageNet. Privileged directions can however also befound in non-robust models and are thus not an indication of robustness. Emergence of outlier features.We have observed that outlier features are an indication of robustness toImageNet distribution shifts. We would like to suggest a hypothesis to explain why they distinguish robustmodels from their less robust counterparts. In our analysis, they can only be found in robust zero-shot CLIPmodels that were typically trained on datasets with a size and diversity that is several orders of magnitudeabove that of ImageNet. Similarly, Sun et al. (2024) observed massive activations (closely related to outlierfeatures) only in transformers which were pretrained on datasets that were large and diverse in comparisonto ImageNet, but not in transformers trained on ImageNet itself. Combined together, these two facts suggestthat outlier features result from training on larger and more diverse datasets (in comparison to the evaluationdataset), and that robustness and outlier features thus share a common root cause in the type of pretrainingdata. Relationship of outlier features to pruning.Note that previous work on LLMs found that outlierfeatures also have positive effects on model pruning: Sun et al. (2023) found that LLMs with outlier featurescan be efficiently pruned by retaining features with larger activations. We also found some weak evidence ofthis kind when pruning latent directions (see Appendix G).",
  "Concept probing": "In this section we find that robust CLIP models encode more concepts than non-robust supervised models.However, also non-robust CLIP models encode similarly high numbers of concepts. Our analysis thus suggeststhat this stems from language supervision in CLIP, rather than being related to robustness. We first describeour approach, and then discuss the concepts encoded in the privileged directions identified in the previoussection. We then show that robust and non-robust CLIP models encode more unique concepts than fine-tunedand supervised models. Lastly, we explain how this leads to polysemanticity in CLIP models. Approach.With the discovery of privileged directions in the representation spaces of models with ro-bustness to Imagenet shifts, it is legitimate to ask what type of information these directions encode. Moregenerally, are there differences in the way robust models encode human concepts? To answer these questions,we use concept probing. This approach was introduced by Bau et al. (2017), along with the Broden dataset.",
  "We report the number of unique concepts encoded in each type of model from our pool in": "We notice that robust zero-shot CLIP models encode substantially more concepts than their fine-tuned andsupervised counterparts, which thus at first might appear to be another indicator of robustness. However,also some of the non-robust zero-shot ImageNet-Captions pretrained CLIP models encode a relatively highnumber of unique concepts, namely ImageNet-Captions-t and ImageNet-Captions-td. On the other hand,",
  ": Overlap between the concepts encoded in the representation space of different models for eachOpenAI models. Zero-shot models encode many concepts not encoded other models": "|{c [C] | APci 0.9}|. The higher this number is, the more likely it is that the feature corresponding to vi ispolysemantic. As a measure of polysemanticity for the model, we simply average this number over all singularvectors: polysemanticity := d1HdHi=1 Nconcept(i). By measuring this number for all zero-shot models, wefound that this ranges between polysemanticity = 3 for the OpenAI ResNet 50 and polysemanticity = 16 forthe LAION-2B ViT-B/16. By looking at the complete results in Appendix E.4, we also note that most CLIPmodels are on the higher side of this range, with typically more than 10 concepts per direction on average.Since the Broden dataset has no duplicate concepts, we deduce that these models are highly polysemantic. Take-away 4.A larger number of concepts encoded in a models representation space is not anindication of robustness since the number of concepts encoded by some non-robust zero-shot ImageNet-Captions CLIP models can be as high as for some robust zero-shot CLIP models. However, both typesof zero-shot CLIP models encode a higher number of concepts than their supervised counterparts, andamong the ImageNet-Captions CLIP models, the amount of concepts varies depending on the type oflanguage supervision used. This supports the idea that the amount of concepts encoded is related to thelanguage supervision of CLIP models. The large number of concepts encoded in CLIP models makesthese models polysemantic.",
  "Epoch": "0.0 0.2 0.4 0.6 0.8 1.0 Normalized set size ViT-B/32 OpenAI Concept overlap (finezero)sup (finesup)zero finezerosup finezerosup : Overlap of the finetuned model concepts with zero-shot and supervised models during finetuning,normalized at each epoch by the number of finetuned concepts |Cfine|. The overlap with the zero-shot-only(i.e. not overlapping with supervised) concepts decreases (blue curve), while concepts shared with zero-shotand supervised models increases (green curve). analyze saliency maps of CLIP and found that they tend to focus on the background in images. Goh et al.(2021) analyzed CLIP ResNets and found multimodal neurons, that respond to the presence of a concept inmany different settings. (2) The second body of work leverages CLIP to explain other models. For instance,Jain et al. (2022) use CLIP to label hard examples that are localized as a direction in any models repre-sentation space. Similarly, Oikarinen & Weng (2022) use CLIP to label neurons by aligning their activationpatterns with concept activation patterns on a probing set of examples. To the best of our knowledge, ourwork is the first to leverage interpretability to better understand the distribution shift robustness of CLIP. Outlier features in foundation models.Outlier features in foundation models were first discoveredin LLMs by Dettmers et al. (2022).Those features are found to have an adverse effect on the modelquantization. The reason for which outlier features appear in LLMs is yet unknown. Elhage et al. (2023)investigated several possibles causes (such as layer normalization), but found no conclusive explanation. Theyconclude that the emergence of outlier features is most likely a relic of Adam optimization. Bondarenko et al.(2023) found that outlier features in transformers assign most of their mass to separator tokens and thatmodifying the attention mechanism (by clipping the softmax and using gated attention) decreases the amountof outlier features learned during pretraining. To the best of our knowledge, our work is the first to discussoutlier features outside of language and transformer models. We can also offer an alternative explanationfor their emergence (see end of ). Overall, our work shows that outlier features are more universalphenomena than previously known, and motivates further research to understand the mechanisms at play.",
  "Discussion": "The goal of this work was to generate a better understanding of what models that are robust to distributionshifts around ImageNet have learned from data that distinguishes them from non-robust models. To thisend, we conducted a thorough investigation of the representation spaces of robust CLIP models and theirnon-robust counterparts, analyzing a total of 39 models. We found outlier features (Dettmers et al., 2022; Elhage et al., 2023) to be an indication of robustness,since they can only be found in models that are robust to ImageNet distribution shifts. To the best of ourknowledge, this is also the first time that outlier features were observed outside of language and transformermodels. Since the presence of outlier features can be detected without access to the shifted datasets, webelieve that they could be a useful tool for practitioners to get a feeling for the distribution shift robustnessof a pretrained model during deployment, when the exact form of distribution shift is typically unknown.Interestingly, we can also validate this indicator on robust non-CLIP models (CoCa, Yu et al. (2022)) inAppendix D. Lastly, we found that larger numbers of encoded concepts in the representation space are rather relatedto the type of language supervision than to be an indication of robustness, since they can also be foundin non-robust ImageNet-Captions CLIP models and their number varies with the type of text used forlanguage supervision. It would be interesting to further investigate this hypothesis by creating differenttypes of language supervision for identical images, potentially leveraging state-of-the-art multimodal models,to create even richer signals. In general, we believe an interesting avenue for future research would be to extend the analysis of thiswork to dataset shifts beyond the ImageNet family, to see if our analysis remains relevant beyond the muchinvestigated ImageNet shifts.",
  "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. arXivpreprint arXiv:1907.02893, 2019": "Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenen-baum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of objectrecognition models. Advances in neural information processing systems, 32, 2019. David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifyinginterpretability of deep visual representations. In Computer Vision and Pattern Recognition, 2017.",
  "Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scaleimage-text pre-training to recognize long-tail visual concepts. In CVPR, 2021": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.Reproducible scaling laws for contrastivelanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 28182829, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Benjamin Devillers, Bhavin Choksi, Romain Bielawski, and Rufin VanRullen. Does language help general-ization in vision models? arXiv preprint arXiv:2104.08313, 2021": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, ZacHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXivpreprint arXiv:2209.10652, 2022.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.arxiv e-prints. arXiv preprint arXiv:1512.03385, 10, 2015": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencodersare scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1600016009, 2022. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The manyfaces of robustness: A critical analysis of out-of-distribution generalization. ICCV, 2021a.",
  "Yi Li, Hualiang Wang, Yiqun Duan, Hang Xu, and Xiaomeng Li.Exploring visual interpretability forcontrastive language-image pre-training. arXiv preprint arXiv:2209.07046, 2022": "Maggie Makar, Ben Packer, Dan Moldovan, Davis Blalock, Yoni Halpern, and Alexander DAmour. Causallymotivated shortcut removal using auxiliary labels. In International Conference on Artificial Intelligenceand Statistics, pp. 739766. PMLR, 2022. John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shifton question answering models. In International Conference on Machine Learning, pp. 69056916. PMLR,2020.",
  "Adam Scherlis, Kshitij Sachan, Adam S Jermyn, Joe Benton, and Buck Shlegeris.Polysemanticity andcapacity in neural networks. arXiv preprint arXiv:2210.01892, 2022": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scaledataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. Zhouxing Shi, Nicholas Carlini, Ananth Balashankar, Ludwig Schmidt, Cho-Jui Hsieh, Alex Beutel, and YaoQin. Effective robustness against natural distribution shifts for models with different training data. arXivpreprint arXiv:2302.01381, 2023.",
  "Mingjie Sun, Xinlei Chen, J Zico Kolter, and Zhuang Liu. Massive activations in large language models.arXiv preprint arXiv:2402.17762, 2024": "Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Mea-suring robustness to natural distribution shifts in image classification. Advances in Neural InformationProcessing Systems, 33:1858318599, 2020. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, DamianBorth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016.",
  "TorchVision maintainers and contributors. Torchvision: Pytorchs computer vision library. 2016": "Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations bypenalizing local predictive power. In Advances in Neural Information Processing Systems, pp. 1050610518,2019. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Mor-cos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averagingweights of multiple fine-tuned models improves accuracy without increasing inference time. In Interna-tional Conference on Machine Learning, pp. 2396523998. PMLR, 2022a.",
  "In this section, we define Effective Robustness (ER)": "Context. ER has emerged as a natural metric to measure how the performance of a model on a referencedistribution (in-distribution) generalizes to natural shifts of this distribution (Fang et al., 2022).Whenplotting the in-distribution accuracy (X-axis, logit scaling) against the average shifted-distribution accuracy(Y-axis, logit scaling) of various architectures trained on ImageNet, Taori et al. (2020) found that most of theexisting models lie on the same line. They also found that models trained with substantially more data lieabove this line, showing a desirable gain in shifted-distribution accuracy for a fixed in-distribution accuracy.They coined this vertical lift above the line as Effective Robustness. Computing ER. To quantify ER, following Taori et al. (2020), one gathers the ImageNet test accuracyACC(I) and the average accuracy over the ImageNet shifts ACC(S) of a set of reference models trained onImageNet and fits a linear model on this pool of accuracies to map logit[ACC(I)] to logit[ACC(S)], with thelogit function logit : R defined as x ln(x) ln(1 x). The resulting line can be used to predictwhat (logit) accuracy we would expect to see on the ImageNet shifts, given a (logit) accuracy on the originalImageNet. Given a new model that has accuracy ACC(I) on ImageNet and average accuracy ACC(S) on thecanonical ImageNet shifts, ER is computed as:",
  "HSIC(b, b)": "We use the PyTorch-Model-Compare package (Subramanian, 2021) to compute this metric between theactivation vectors of zero-shot models and their finetuned counterparts for each layer in the backbone. Theresults are shown in and . Across architectures and pretraining sets, we find that thereis often a large drop in CKA between zero-shot and finetuned models occurring in the last layer.Thismakes the activations in the last layer a particularly interesting layer to analyse when investigating ER, asfine-tuned models typically have only half the ER of their zero-shot counterpart (see a).",
  "DRobustness indicators in non-CLIP models": "In addition to the CLIP models investigated in the main paper, we below investigate CoCa models (Yu et al.,2022) pre-trained on LAION-2B as another set of robust multimodal models which do not fall into the CLIPfamily. We see that our findings extend to these non-CLIP multimodal models as well. First, in , we confirm that these models have high effective robustness when used as zero-shotclassifiers. As for the other models in the main part of the paper, we observe that finetuning on ImageNetdecreases the effective robustness of these classifiers.",
  "ViT-L-1434% (76%)21% (84%)": "Next, in , we show that all the zero-shot models have high kurtosis, which implies the existenceof outlier features in their representation space. Additionally, we show that finetuning again decreases thekurtosis. : Results of the kurtosis analysis showing outlier features present also in robust models with ViT-L-14backbone or of CoCa family, but disappearing as soon as models are finetuned. Values calculated accordingto Equation (1) over all ImageNet test examples.",
  "E.2Pruning analysis for each model": "and show the effect of gradually pruning the least important SV of W on ER and ACCfor all models that were not pretrained on the OpenAI pretraining dataset, similar to the analysis shown in in the main paper. Qualitatively, our finding from the main paper is confirmed across the remaining models we investigate: Asmall subset (typically around 20%) of privileged directions in representation space explain the high ER ofzero-shot models. The remaining directions can be pruned without significantly impacting neither ER norACC.",
  "E.3Top-3 concepts in most dominant direction for each model": "We look at the concepts with highest AP in the most privileged direction of each model represented in, similar to what we did in for the most privileged direction of each model in b.The results are shown in . Again, we observe that for the majority of cases, the robust zero-shotmodels encode concepts related to textures as their top concepts encoded along the privileged directions(e.g., scaly, meshed, or matted), while the less robust finetuned models encode more concrete concepts (e.g.,carrousel, book stand, or pantry).",
  "Top-1 ConceptTop-2 ConceptTop-3 ConceptStepFinetunedZeroshotFinetunedZeroshotFinetunedZeroshotBackbonePretraining": "RN-101OpenAIflight of stairs natural s ( 0.9)moon bounce s ( 0.99)movie theater indoor s ( 0.88)inflatable bounce game ( 0.99)home theater s ( 0.86)ball pit s ( 0.91)Supervised ImageNetauto mechanics indoor s ( 0.96)N.A.labyrinth ( 0.94)N.A.hay ( 0.94)N.A.YFCC-15Mchapel s ( 1)ice cream parlor s ( 0.99)pantry s ( 0.97)temple ( 0.98)pantry ( 0.97)temple east asia s ( 0.95)",
  "RN-50": "CC-12Msacristy s ( 0.98)wheat field s ( 0.98)funeral chapel s ( 0.97)meshed ( 0.98)formal garden s ( 0.96)polka dotted ( 0.96)OpenAImountain pass ( 0.99)knitted ( 0.95)butte s ( 0.94)chequered ( 0.91)water mill s ( 0.89)wheat field s ( 0.87)Supervised ImageNetkiosk indoor s ( 0.95)N.A.vegetable garden s ( 0.88)N.A.sacristy s ( 0.86)N.A.YFCC-15Mliquor store indoor s ( 0.97)polka dotted ( 0.98)book stand ( 0.96)lined ( 0.97)horse drawn carriage ( 0.89)dotted ( 0.97)",
  "ViT-B/16": "DataCompcarrousel s ( 0.98)jail cell s ( 0.84)banquet hall s ( 0.93)gift shop s ( 0.83)carport freestanding s ( 0.89)manhole s ( 0.82)LAION-2Bflood s ( 0.9)stained ( 0.89)catwalk s ( 0.87)scaly ( 0.86)rubble ( 0.85)cracked ( 0.85)LAION-400Mbook stand ( 0.97)temple ( 0.97)bookstore s ( 0.94)courtyard s ( 0.95)rudder ( 0.93)cabana s ( 0.94)OpenAImartial arts gym s ( 0.95)meshed ( 0.92)jail cell s ( 0.92)flecked ( 0.92)throne room s ( 0.91)perforated ( 0.92)Supervised ImageNethot tub outdoor s ( 0.99)N.A.bedchamber s ( 0.98)N.A.stadium baseball s ( 0.97)N.A.YFCC-15Mfountain s ( 0.68)N.A.black c ( 0.67)N.A.air base s ( 0.62)N.A.",
  "ViT-B/32": "DataCompzen garden s ( 0.95)ice cream parlor s ( 0.67)dolmen s ( 0.94)bullring ( 0.67)gift shop s ( 0.88)junkyard s ( 0.67)LAION-2Bviaduct ( 0.93)stained ( 0.91)cargo container interior s ( 0.91)scaly ( 0.91)labyrinth ( 0.9)matted ( 0.88)LAION-400Mbarnyard s ( 0.86)scaly ( 0.94)subway interior s ( 0.86)jail cell s ( 0.9)bird feeder ( 0.86)manhole s ( 0.9)OpenAItennis court ( 1)meshed ( 0.95)batters box s ( 0.98)perforated ( 0.93)kennel indoor s ( 0.93)flecked ( 0.91)Supervised ImageNettelevision studio s ( 0.88)N.A.barbecue ( 0.87)N.A.lined ( 0.85)N.A.",
  "Pretraining": "Importance score distribution over all directions ViT-L/14 Model CategoryRobust CLIPFine-tuned CLIPImageNet-Captions CLIPImageNet Supervised : Distribution of importance over all RSVs in the representation space for all models with ViT-B-32 and ViT-L-14 backbones. Robust zero-shot CLIP models (blue) have one strongly privileged direction.Fine-tuned models (green) still exhibit one strong privileged direction, but with lower importance than therobust zero-shot models. The supervised models (orange) do not have them at all.",
  "FIntuition behind generalization of outlier features": "Below, we give more details and an intuition why outlier features can be generalized from the canonicalbasis {e1, . . . , edH} (we can write hi = Projei(h)) to be any set of directions of the representation space thatreceive a projection substantially above average: Let us assume, for instance, that two of the elements in the canonical basis e1 and e2 correspond to outlierfeatures. This means that an activation vector h related to an input image x has projections h1 = Proje1(h)and h2 = Proje2(h) substantially above the average h1, h2 n1 ni=1 hi. Now let us define a new unitvector e1 = 21/2(e1 + e2). We deduce that the projection onto this vector is also substantially higher thanaverage h1 = Proje1(h) = 21/2(h1 + h2) n1 ni=1 hi. Hence, the unit vector e1 can be considered as anoutlier feature in a new non-canonical basis. In general, we can extend the notion of outlier features to anyvector in the span{e1, . . . , en Rn}.",
  "GPruning results": "Pruning non-privileged directions. Given that we have established that outlier features induce privilegeddirections in representation space, it seems interesting to check their role in model performance. To that aim,we gradually prune each RSV vi by increasing order of i by setting i 0 in the singular value expansion4 W = rank(W )i=1iuivi . By pruning a variable proportion of the singular vectors, we obtain the results in. We see that the 80% least important RSV of the representation space can be pruned withouta substantial effect on performance, i.e. that the robust models are low-rank in their last layer where theyhave privileged directions.",
  "When extending the pruning experiment to finetuned CLIP models and supervised models trained only withImageNet, we make the two interesting observations from these new results (see ):": "All models are low-rank. For all the models (zero-shot, finetuned and supervised), the performances arenot substantially affected if we remove the 80% least important singular directions of their representationspace (compare to ). This shows that many existing models admit good low-rank approximations.This also demonstrates that the fact that these models are low-rank is not necessarily an indicator ofrobustness.",
  ": Extension of pruning results to finetuned and ImageNet supervised models. Zero-shot modelsobtained with OpenAI pretraining set": "Faster drop for supervised models.When the number of ablated singular values ranges between80% 100%, we see that the ImageNet accuracy of supervised models drop substantially faster than theaccuracy of the finetuned and the zero-shot models. In fact, for the ResNet50, the ImageNet accuracy curveseven cross. This implies that the most important direction of the zero-shot models representation spacebetter discriminate between ImageNet classes than the most important directions of the supervised modelsrepresentation space. In the former case, these directions correspond to the zero-shot models privilegeddirections.We believe that this new result further reinforces the importance of privileged directions tounderstand the performances of robust models.",
  "H.2Supervised ImageNet models": "We note that the ResNets used by Radford et al. (2021) have small modifications, such as the usage ofattention pooling. Unfortunately, we are not aware of any public weights for such modified architecturestrained on ImageNet from scratch. We thus train these modified ResNet models from scratch for 90 epochson the ImageNet training set, using a batch size of 1024. We use AdamW, and a learning rate scheduledecaying from 103 to 104 after 30 epochs and to 105 after 60 epochs (with a warm-up period of 5,000steps). We set weight decay to 102. We use the standard augmentations of horizontal flip with randomcrop as well as label smoothing.",
  "IAnalysis of Wise-FT models": "In this appendix, we use the approach of Wise-FT (Wortsman et al., 2022b) to obtain a continuous spectrumof ER. Given a zero-shot model f0 with weights 0 and a finetuned model f1 with weights 1 ,Wortsman et al. (2022b) propose to interpolate between the two models in weight space. This is done bytaking a combination := (1 ) 0 + 1 for some interpolation parameter . One then definesa new model f based on the interpolated weights. Surprisingly, interpolating between zero-shot CLIP models and finetuned CLIP models produce models withgood performances. To illustrate that, we perform the Wise-FT interpolation with all the models from ourpool. We report the ImageNet & shift accuracies of these models in Figures 15 and 16. For the OpenAIand LAION models in , we observe that the shift accuracy of interpolated models often surpassboth the zero-shot and the finetuned models. The YFCC-15M and CC-12M models in exhibita different trend: both ImageNet & shift accuracies increase monotonically as sweeps from zero-shot tofinetuned. This is likely due to the low accuracy of the corresponding zero-shot models. By analyzing the ER of interpolated OpenAI and LAION models in , we see that the ER graduallydegrades as sweeps between the zero-shot and the finetuned models. Interestingly, the ER of YFCC-15Mand CC-12M models in peaks at = .4 and then decreases monotonically. Let us now look at how our robustness indicators evolve as we sweep between zero-shot and finetunedmodels. Ideally, if these indicators are good ER proxies, they should exhibit similar trends as the onesdescribed in the previous paragraph. For the OpenAI and LAION models, we indeed observe in Figures 19and 21 that the kurtosis and the number of unique encoded concepts gradually decrease as sweeps fromzero-shot to finetuned models. Similarly, we observe in Figures 20 and 22 that these two metrics start tosubstantially after = 0.4 for the YFCC-15M and CC-12M models. This suggests that these two metricsconstitute a good proxy to track how the ER of a given model evolves. Note that the Wise-FT idea has since been generalized to a combination of several finetuned models byWortsman et al. (2022a) with model soups. We leave the investigations of model soups for future work.",
  "JFurther literature": "Defining CLIP ER. The definition of ER crucially relies on the observation in multiple works that themodel performance on natural shifts is linearly related to its performance in-distribution when both quantitiesare plotted with a logit scaling (Recht et al., 2018; 2019; Miller et al., 2020). We note though that thereare known exceptions to this, e.g. considering out-of-distribution generalization on real-world datasets thatsubstantially differ from the in-distribution dataset (Fang et al., 2023). Explaining CLIP ER. A first intuitive explanation for the surprisingly high effective robustness of CLIPmight be the fact that the learned embeddings are endowed with semantic grounding through pretrainingwith text data. This hypothesis was refuted by Devillers et al. (2021), who demonstrated that the embeddingsin CLIP do not offer gains in unsupervised clustering, few-shot learning, transfer learning and adversarialrobustness as compared to vision-only models. In a subsequent work, Fang et al. (2022) demonstrated thatthe high robustness of these models rather emerges from the high diversity of data present in their trainingset. This was achieved by showing that pretraining SimCLR models Chen et al. (2020) on larger datasets,such as the YFCC dataset by Radford et al. (2021), without any language supervision matches the effectiverobustness of CLIP. Shi et al. (2023) reinforced this data-centric explanation by showing that the performanceon the pretraining set also correlates linearly with the out-of-distribution performance. To put the emphasison the importance of data-quality for effective robustness, Nguyen et al. (2022) showed that increasing thepretraining set size does not necessarily improve the effective robustness of the resulting model. Rather, itsuggests that it is preferable to filter data to keep salient examples, as was done, e.g., to assemble the LAIONdataset (Schuhmann et al., 2022). Other indicators of ER. By comparing pretrained models with models trained from scratch, Neyshaburet al. (2020) demonstrated that these models exhibit interesting differences, such as their reliance on high-level statistics of their input features and the fact that they tend to be separated by performance barriers inparameter space. Guillory et al. (2021) found observable model behaviours that are predictive of effectiverobustness. In particular, the difference of models average confidence between the in and out-of-distributioncorrelates with out-of-distribution performance. Polysemanticity in foundation models. Polysemantic neurons were coined by Olah et al. (2020) inthe context vision model interpretability. These neurons get activated in the presence of several unrelatedconcepts. For instance, the InceptionV1 model has a neurons that fires when either cats or cars appear inthe image. These neurons render the interpretation of the model substantially more complicated, as theyprevent to attach unambiguous labels to all the neurons in a model. This will limit the insights gained bytraditional interpretability techniques. For example, producing saliency maps for a polysemantic neuroncould highlight many unrelated parts of an image, corresponding to the unrelated concepts this neuron issensitive to. A qualitative analysis of the neurons in CLIP by Goh et al. (2021) showed that a CLIP ResNethas a substantial amount of polysemantic neurons. The emergence of polysemantic neurons is a complexphenomenon. It is not yet well-understood for models at scale. The latest works on the subject mostlyfocus on toy models, see e.g. the works of Elhage et al. (2022) and Scherlis et al. (2022). To the best ofour knowledge, our work is the first to explicitly discusses the link that exists between polysemanticity androbustness to natural distribution shifts."
}