{
  "Abstract": "We introduce Image2Struct, a benchmark to evaluate vision-language models(VLMs) on extracting structure from images. Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and3) is based on a renewable stream of fresh data. In Image2Struct, VLMs areprompted to generate the underlying structure (e.g., LaTeX code or HTML) froman input image (e.g., webpage screenshot). The structure is then rendered toproduce an output image (e.g., rendered webpage), which is compared againstthe input image to produce a similarity score. This round-trip evaluation allowsus to quantitatively evaluate VLMs on tasks with multiple valid structures. Wecreate a pipeline that downloads fresh data from active online communities uponexecution and evaluates the VLMs without human intervention. We introduce threedomains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixelsimilarity, cosine similarity between the Inception vectors, learned perceptual imagepatch similarity, structural similarity index measure, and earth mover similarity)that allow efficient and automatic comparison between pairs of images. We evaluateImage2Struct on 14 prominent VLMs and find that scores vary widely, indicatingthat Image2Struct can differentiate between the performances of different VLMs.Additionally, the best score varies considerably across domains (e.g., 0.402 onsheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct containstasks of varying difficulty. For transparency, we release the full results at",
  "Introduction": "Vision-language models (VLMs) unlock the ability to process both visual and language information.They have found uses in generative search engines , visual question-answering , text-drivenimage creation and alteration , image captioning , and robotics . However, evaluating responses from the VLMs is a major challenge in VLM benchmarking. Manyexisting benchmarks either require humans to evaluate long and complex outputs or are designedas multiple choice question answering (MCQA) . The former is costly and slow while the lattercannot represent many real-world use cases such as code generation.",
  "Renderer": ": An overview of the evaluation process in Image2Struct. A VLM is presented with an imagex and the instructions to generate the underlying structure (e.g., code representing an equation inLaTeX). The predicted structure y is used to create the rendered image x, which is then evaluatedagainst the input image x to produce a score. In this example, the VLM produced a partially correctstructure. Please refer to .2 for details about Earth Mover Similarity and other metrics. encompasses many real-world use cases such as converting an image of an equation to LaTeX orgenerating HTML given a screenshot of a webpage and the existence of non-VLM commercialsoftware such as Mathpix to convert equations to LaTeX or Codia AI to convert an image to HTMLfurther demonstrates the practicality and value of our proposed task. illustrates the three-stage process in Image2Struct. First, we present an image x to a VLMto obtain the structure y Y (e.g., LaTeX code representing an equation). Note that there couldbe many valid structures for the same input image (see Appendix E for an example of multiplevalid structures). Second, the output from the VLM is rendered into an image x with a task-specificrenderer (e.g., TeX engine for LaTeX code). Third, the rendered image is compared against the inputimage and their similarity is quantified using automatic metrics, including two that we developed:cosine similarity between the Inception vectors (CIS) and earth movers similarity (EMS). CIS usesa deep convolutional neural network to quantify image similarity whereas the EMS modifies thetypical earth mover distance to compute similarities between patches in an image. We show thatthese metrics have high correlation with the Levenshtein edit distance between the predicted structureand the ground-truth structure in .2. The round-trip comparison between the input imagex and rendered image x avoids the need to have ground-truth structures for our test instances. Assuch, we skip the labor-intensive process of annotating images with possibly non-unique ground-truthstructures. We instantiate Image2Struct in three domains: webpages (structures in HTML, CSS, and Javascript),LaTeX (consisting of equations, plots etc.), and music scores (in LilyPond). We obtain data bydownloading real and fresh data from online sources with active communities of users (e.g., arXiv).Each instance consists of a screenshot, which will be the image input to the VLM. In addition to thescreenshot, we obtain the underlying structure so as to provide ground-truth source codes to validateour image similarity metrics. We emphasise that the ground-truth structures are not necessary forevaluation in our benchmark. We provide an example instance from each domain in . Bybeing able to control the time that the data is uploaded to be recent, we minimize the risk of dataleakage between the test data and the data used to train the model. We evaluate the performances of 14 prominent VLMs: Claude 3 Opus , Claude 3 Sonnet ,Claude 3.5 Sonnet , Gemini 1.0 Pro Vision , Gemini 1.5 Pro , GPT-4 Omni , GPT-4Vision , LLaVA , LLaVA NeXT , IDEFICS Instruct 9B , IDEFICS Instruct 80B ,IDEFICS2 8B , Palmyra Vision 003 , and Qwen-VL Chat . We find that performancevaries considerably across models, indicating that Image2Struct is able to distinguish between models.At the time of writing, closed-API models outperform open-weight models, with GPT-4 Omni beingthe best-performing VLM when measured by the mean win rate across the different tasks. Whileit performs the best on Webpages and LaTeX, it ranks third in Musical Scores, indicating that nomodel dominates in all domains. We find that VLMs perform better on some domains than others(e.g., a maximum EMS of 0.660 for LaTeX vs 0.402 for sheet music), and on certain subsplitswithin a domains (e.g., a maximum EMS of 0.830 for equation vs 0.617 for plot in LaTeX). Overall,Image2Struct is a challenging benchmark where no model is able to perform well yet. ```json[ { \"filename\": \"index.html\", \"content\": \"<!DOCTYPE html>\\n<html lang='en'>\\n<head>\\n<meta charset='UTF-8'>\\n<meta name='viewport' content= ... }, { \"filename\": \"style.css\", \"content\": \"body {\\n background-color: #121212;\\n display: flex;\\n justify-content: center;\\n align-items: ... }, { \"filename\": \"script.js\", \"content\": \"document.getElementById('operate').addEventListener('click', function() {\\n // Insert logic for operation\\n alert('Operation performed.');\\n});\" }]```",
  "Predicted structure:": ": In Image2Struct tasks, given an input image, the goal is to produce a structure (e.g., LaTeXcode), so that the rendering of the structure produces the original image. We include three domains:Webpages, LaTeX, and Musical scores. We show an example of the input image, model predictedstructure, and rendered image for each of the domains in our benchmark.",
  "Image2Struct": "In an Image2Struct task, an input image x is fed into an VLM to produce a structure y = VLM(x),which is then fed into a renderer to produce x = render(y). We instantiate Image2Struct withinstances in three domains: Webpages, LaTeX, and Musical Scores. For webpages, VLMs arerequired to generate HTML, CSS, or Javascript code given their screenshots. For scientific documents,we restrict our test instances to screenshots of diagrams (such as charts, tables, equations, andalgorithms) and make the VLMs generate the LaTeX code that recreates them. For sheet music, theVLMs are asked to generate LilyPonda computer program for typesetting music scorescodefrom the image.",
  "As can be seen from , our general data and evaluation pipeline involves 1) downloadingdata from a live source, 2) data filtering and conditioning, 3) retrieving output from the VLMs, 4)": ": Our pipeline for evaluation using the example of LaTeX. First, we download data fromonline sources. Second, we filter and process the images. Third, we prompt the VLMs with theseimages to produce output structures. Fourth, we render the the structures and finally evaluate therendered images by comparing the rendered images against the input images.",
  "rendering the outputs into images, and 5) computing the metric. We describe only (1) and (2) in thissection and elaborate on (3)(5) in the next": "We collect data from active communities of users who upload and consume new data on a regularbasis, ensuring that we will have continued streams of fresh, human-generated data. We scrape onlydata that is uploaded between Jan 1, 2024 and Feb 29, 2024 to minimize the risk of data leakagebetween the test data and the data used to train the model. The downloaded data is then filtered for relevance, diversity, and toxicity after being downloaded toensure quality. The Perspective API is deployed to filter toxic content when needed. De-duplicationacross all tasks is achieved by computing and comparing the hashes of the images. We detail theidiosyncratic steps taken for each of the domains in sections 2.1.1 to 2.1.3. We collect 300 valid testinstances per subdomain with a maximum of 40 instances on a single day in order to induce temporaldiversity. In all, we collected a total of 900 instances for webpages (300 each for HTML, CSS andJS), 1200 instances for LaTeX (300 each for equations, tables, algorithms, and plots), and 300 formusic for a grand total of 2400 test instances.",
  "Webpages": "Webpages are downloaded from GitHub Pages, a developer platform that allows users to publishwebpages from their code repositories. To do this, we first obtain a list of repositories and theirupload dates using the GitHub Developer API. We identify only webpages that contain github.io intheir names, contain mainly CSS, HTML, or Javascript as their main languages, and are served usingJekyll (the default engine used in Github Pages). We download only repositories that are at most 100KB in total size and remove those that containtoxic content before applying the size filters to ensure that inputs and expected outputs are not toolong. The size filters specify that a repository must contain i) more than just a README.md file, ii)at most 5 code (e.g., CSS, HTML, or Javascript) files, iii) at most 5 non-code assets (e.g., images), iv)between 10 and 1000 lines of HTML and Javascript code in total, and v) between 0 and 2000 lines ofCSS code in total. Webpages in our dataset contain an average of one asset. After downloading the repositories, we apply the Perspective toxicity filter to remove webpageswith unsafe content. After which, we use Jekyll to serve the webpages from the code and de-ploy Selenium to visit the webpages. We take screenshots of the webpages without resizing orscrolling, similar to what a person would see on a browser with a native resolution of 1920x1080pixels. As such, the dynamic aspects of webpages are not considered in Image2Struct. Screen-shots that are completely or nearly completely white are removed from the dataset and the re-maining images are de-duplicated. In all, we collect 900 test instances consisting of 300 each forHTML, CSS, and Javascript. The instances can be found at",
  "LaTeX": "We download scientific documents from arXiv, an open-access repository of academic pre-prints andpapers. We first use the arxivscraper to obtain the metadata of and links to the arXiv papers fromthe Open Archives Initiative (OAI) before downloading the papers directly from arXiv. We apply thePerspective toxicity filter on the text to remove unwanted documents and extract the desired portions(i.e., equation, algorithm, tikz, or table) from the documents. Custom LaTeX document headersare injected in order to standardize the rendering of the documents and the resulting LaTeX sourcecode is rendered into PDFs before being converted to 200 DPI Portable Network Graphics (PNG)images using the pdf2Image library. The images are cropped to the smallest possible bounding boxthat includes all the LaTeX generation. Data points where the rendered image is mostly blank arediscarded. We select the final data set in a way that balances the number of instances per subject (e.g.,physics, maths, or economics) and structures (e.g., equation or algorithm). In all, we collect 1200 testinstances consisting of 300 each for equations, tables, algorithms, and plots. The instances can befound at",
  "Musical Scores": "We obtain sheet music from the International Music Score Library Project (IMSLP), a digital librarythat hosts sheet music that is either out of copyright or released under a Creative Commons license. Webegin by downloading files that are uploaded within the desired time frame (i.e., Jan 01, 2024 Feb29, 2024) directly from the IMSLP website. Since many of the pages in a file are not sheet music (e.g.,cover or blank page), we train a convolutional neural network to classify whether a page is a musicalscore. To do this, we manually labeled sheet music uploaded before 2012 to create a training data setconsisting of 200 sheet music and 200 non-sheet music pages and a test set consisting of 500 examples.We then fine-tuned a ResNet-18 on the train set for two epochs using stochastic gradient descent witha learning rate of 0.001 and momentum of 0.9. The final model achieves an accuracy of 99.2% on thetest set. We identify music sheets with the fine-tuned model and further filter for digital sheet music(in contrast to scanned ones) by imposing the criterion that the most common color is white, whichworks because scanned scores tend to contain a greyish tint. We crop the sheet music to create random,short subsections so that the expected predicted structures fit within the context window of the VLMs;this is achieved by detecting alternating black and white pixel zones on a vertical line. Finally, de-duplication is performed on the images to create the final set of 300 test instances. We emphasize thatthere is no ground-truth structure for the test instances in this domain. The instances can be accessedat",
  "Image similarity metrics": "We would like to score a VLM based on how similar the generated image is to the original. Asimilarity metric takes two images, x and x, and computes a score, sim(x, x). We take the viewthat unsuccessful rendering should be counted as absolute failures (i.e., the score is zero if the codedoes not compile). In Image2Struct, we normalize metrics within the unit range so that they can beinterpreted easily; a score of zero implies complete dissimilarity whereas a score of 1 implies that theimages are identical. Without loss of generality, we assume both x and x are of dimensions W H. We use 3 often used image similarity metricspixel similarity, Learned Perceptual Image PatchSimilarity (LPIPS), structural similarity index measure (SSIM)and introduce 2 new ones: cosinesimilarity between the Inception vectors (CIS) and Earth Mover Similarity (EMS), a novel adaptationof earth movers distance that better captures content similarity. CIS and EMS are introduced toimprove performance and computation speeds over classical metrics. Separately, when a reference isavailable, we compute the Levenshtein edit distance between the predicted structure and the referenceto check its correlation with the image metrics. Due to page constraints, we describe the new metricshere and elaborate on the classical metrics in Appendix A. Cosine similarity between Inception vectors (CIS).CIS is a simplification of LPIPS; instead oftaking the weighted distance between several layers of a neural network, we use only the penultimatelayer of a convolutional neural network (CNN). This is driven by the intuition that two images aresimilar if their embeddings in the penultimate layer of the CNN are close. To compute the CIS, wefeed the images x and x through Inception v3 to obtain the activation vectors A(x) and A(x) and",
  "(1)": "[Block-] Earth Mover Similarity (EMS).Inspired by the Earth Movers distance (EMD) ,which is a measure of dissimilarity between two frequency distributions, we introduce a new imagemetric that is scalable to high resolution images. In the classic EMD, images are first converted to grayscale and their signatures are computed. Acost matrix is defined and an optimization problem is solved to obtain the minimum flow betweenprobability masses. Spatial information is lost and the EMD metric is invariant to translation,reflection, and other pixel rearrangements. We refer readers to Appendix A for a detailed descriptionof the classic EMD. We modify the classic EMD by defining multidimensional signatures that consider the pixels x- andy-coordinates in addition to their values. Let N be the number of possible pixel values and recall thatW and H are the width and height of the images, respectively. The support of our multidimensionalsignature, Sp, is all the possible combinations of the x-coordinates (x), y-coordinates (y), and theN possible pixel values (v):",
  "The probability mass, wk, takes the value of either1": "W H or 0. The complexity of computing the costmatrix over Sp is O(W 2H2), making it difficult to compute for high resolution images. We thereforecompute an approximated patch version of it, which we denoted as EMDblock. In EMDblock, we first split two images, x and x, each into K patches of dimensions r s:P 0x, , P K1x(recall that x and x are assumed to have the same dimensions). Our implemen-tation sets r and s individually for every image such that there are 88 patches in every image. Tocompare two patches P tx and P ux , we treat each patch as separate images and compute the EMD usingthe multidimensional signature defined in Equation (2), which we will denote as EMDp(P tx, P ux ).Note that each patch will have x- and y- coordinates within the original image. Next, we define aseparate cost matrix, Cp, such that the cost of moving one patch to another is the sum of the EMDbetween the patches and the Euclidean distance between them:",
  "Cp[t, u] = EMDp(P tx, P ux ) + ||(xt , yt ), (xu, yu)||2(3)": "EMDblock attempts to minimize the cost of moving patches by solving the optimization problemdefined in Equation (7), but with the new cost function, Cp. By considering both the positionsand weights of the pixels within patches (through the multidimensional signature) and the distancebetween patches, EMDblock heavily penalizes random shuffling of pixels and assigns a lower score(implying greater similarity) to a rendered image that contain blocks of similar but translated elementsas the input (see illustration in Appendix B). This property is useful for discerning between pairs ofimages that contain similar elements (even if the elements are translated) and pairs where distributionof colors in the rendered image is similar to the input image. Finally, we define the Block-Earth Mover Similarity (Block-EMS), a normalized similarity version ofEMDblock. It compares EMDblock(x, x) against EMDblock(x, c(x)), the EMD between the referenceimage and a constant black or white image, whichever is the most dissimilar to the reference image x.An Block-EMS of 0 indicates the least similarity and a value of 1 indicates the identity. For brevity,EMS refers to Block-EMS in other parts of this paper.",
  "ModelVersionAccess": "Claude 3 Opus claude-3-opus-20240229Closed-APIClaude 3 Sonnet claude-3-sonnet-20240229Closed-APIClaude 3.5 Sonnet claude-3-5-sonnet-20240620Closed-APIGemini 1.0 Pro Vision gemini-1.0-pro-vision-001Closed-APIGemini 1.5 Pro gemini-1.5-pro-preview-0409Closed-APIGPT-4 Omni aka GPT-4ogpt-4o-2024-05-13Closed-APIGPT-4 Vision aka GPT-4Vgpt-4-1106-vision-previewClosed-APIIDEFICS Instruct 9B idefics-9b-instructOpen-weightIDEFICS Instruct 80B idefics-80b-instructOpen-weightIDEFICS2 8B idefics2-8bOpen-weightLLaVa v1.5 llava-1.5-13b-hfOpen-weightLLaVa v1.6 llava-v1.6-vicuna-13b-hfOpen-weightPalmyra Vision 003 palmyra-vision-003Closed-APIQwen-VL Chat qwen-vl-chatOpen-weight ensure that a model does not perform poorly due to a misunderstanding of the output format that isaccepted by our renderers. We use zero-shot prompting since it is the more natural and most commonway of prompting by the general public. Furthermore, not all models are fine-tuned to use morecomplex methods such as k-shot prompting or chain-of-thought. We note that we have to insert theline ... this music sheet was created by me, and I would like to recreate it using Lilypond for sheetmusic because some VLMs (mistakenly) alleged copyright infringement and refused to answer ourprompts. Despite our efforts, some models, such as GPT-4V or the IDEFICS models, still refuse toproduce answers due to alleged copyright infringement. In fact, GPT-4V refuse to generate code forall instances in the musical scores domain. Interestingly, OpenAIs newer model, GPT-4o, does notrefuse our requests.",
  "Rendering images from predicted structures": "The responses from the VLMs are parsed, and the relevant code snippets are extracted. Customheaders are added as needed and the supplemented code is fed through an appropriate renderer. Inour setup, HTML documents are served through Jekyll and visited by Selenium to take screenshots ofthe predicted website, similar to the pipeline for preparing the dataset (see section 2.1.2). We deployTeX for LaTeX and LilyPond for sheet music to compile the code into PDFs before before takingscreenshots. Sometimes the generated code does not render due to limitations of some VLMs in generating validcode. We attempt to fix simple mistakes, such as missing syntax (e.g., wrapping equations around$...$) and by attempting to import missing packages. Details of our post-processing can be found inAppendix D. We consider rendering success rate as a metric for model performance.",
  "Models": "We test eight closed-API and six open-weight VLMs as listed in . The proprietary VLMs areserved through their respective APIs while the rest are served through the HuggingFace API. Allthe models are evaluated in chat-style with the temperature set to zero to minimize variability in theresponses and maximize reproducibility. Our evaluation run across all the instances and models use5.9M input text tokens, 30K input images, and 17.9M output text tokens. We evaluate the models onlyonce instead of taking the average over multiple responses due to the cost of querying the models.We rank models with the mean win ratewhich is the average fraction of other models that a modeloutperforms across scenariosusing the compilation success rates and EMS scores. Any compilationfailure counts as zero EMS in the mean win rate calculation. We note that GPT-4o, Claude 3.5 Sonnet,and Gemini 1.5 Pro are released after we have collected the data. : Image2Struct evaluation results. The EMS score is conditioned on successful rendering.VLMs generally perform the best on Webpages, followed by LaTeX and then Musical Scores. Thebest performing model (GPT-4o) achieves a maximum rendering success rate of 0.977 and EMS of0.708 on the easiest domain (Webpages), indicating that the benchmark is not saturated.",
  "Model performance": "summarizes the performances of the VLMs and the breakdown across the different tasks andsubtasks can be found in Appendix H. In general, we find that VLMs are unable to perform well onour tasks, with the best performing model achieving a maximum EMS of 0.724 in Webpages. Acrossall the models and subtasks, the average EMS is 0.324 for LaTeX, 0.370 for Webpages, and 0.069for musical scores, indicating that the benchmark is not saturated and that there is a lot of room forVLMs to improve. The VLMs perform better on Webpages and LaTeX than Musical Scores. For example, the bestoverall performing model (GPT-4o) achieves a rendering success rate (RSR) of 0.807 and an EMSscore of 0.660 for LaTeX, an RSR of 0.980 and EMS of 0.710 for Webpages, but a RSR of only 0.491and an EMS of only 0.340 for Musical Scores. We hypothesize that the disparity in performancebetween the domains may be due to the relative abundance of training data points describing LaTeX,HTML, CSS, and other formats used in web development in contrast to LilyPond. We observedifferences within the domains too; for example, most models perform well on equations in LaTeXbut struggle on plots (see Table A2). Our initial benchmarking also shows that closed-API models perform significantly better than open-weight ones. The best-performing open-weight model has a lower mean win rate than the worstclosed-API model. The top-performing models have different niches, and there is no model thatoutperforms the rest in all the tasks. While GPT-4o claims the overall top spot on our leaderboard, bybeing the overall best in Webpages and LaTeX while ranking third in Musical Scores. We reproducesome model predictions in and Appendix F. We perform an error analysis (see Appendix I) and notice that the VLMs can extract the elements(e.g., text, images) but are unable to pick up on visual nuances. Furthermore, while some models areable to produce valid LilyPond code, none of the models we tested have the capability to interpretsheet music (see c).",
  "Pixel Similarity0.0000.0000.0000.000CIS0.9900.9800.9860.856Earth Mover Similarity0.5730.7820.8100.338Edit similarity (Levenshtein)0.3200.4000.4620.000": "(a) LaTeX (Equation) task. GPT-4o performed the best when it recreated the equation except for a single spacebetween R and the rest of the numerator. Gemini 1.5 Pro correctly inserted the space, but wrongly replaced the with a in addition to misinterpreting the subscript l in the denominator. Claude 3 Opus added a squareterm and mistakenly capitalized s. IDEFICS2 wasnt able to interpret the equation.",
  "Test on misspecified data": "We test the models on 4 additional examples, 2 in LaTeX and 2 in Webpages. The input images forLaTeX are sourced from Word documents whereas those for Webpages are obtained from magazines(see Appendix J). The most powerful models, such as GPT-4o or Gemini 1.5 Pro, are able to producevalid structures despite the fact that images are generated from HTML or LaTeX. For LaTeX, thebest rendered images contain the correct text and equations but cannot replicate the alignment ofparagraphs. In some cases, the VLMs do not follow instructions to replicate the input but insteadattempt to answer the questions in the images. For Webpages, the VLMs are able to extract mostof the text but make mistakes in their positions or colors. This exercise demonstrates that our tasksare generalizable to structures that are not specified in the original language. Detailed results areavailable in Appendix K.",
  "Related works": "VLM benchmarks.Benchmarks have been proposed to assess the ability of VLMs to performdifferent tasks, such as image captioning, visual question answering, or action recognition, amongothers . However, existing benchmarks suffer from several limitations, suchas ambiguity in evaluation, difficulty in scaling, and data leakage . Benchmarks evaluate VLMresponses either with human feedback or against reference outputs . The formeris suitable for the evaluation of long-form generations, but is expensive and difficult to reproduce.While some attempts use VLMs such as GPT-4V to evaluate the outputs of another, they are not yetreliable replacements for human annotators . Evaluation against reference outputs, on theother hand, are cheap and easily reproducible, but it is difficult to handle complex generation tasks.Furthermore, generating new test instances is non-trivial, and many benchmarks rely on existingdata sources such as textbooks, prep books , or simply other databases . As a result,benchmarks are expensive to update and risk being incorporated into VLM training data, leading todata leakage . In contrast, Image2Struct captures real-world use cases, is fully automatic and doesnot require human judgment, and uses fresh data so that it is difficult to game. Other benchmarks for image-to-code.Several datasets suitable for evaluating images-to-codehave been released, some during Image2Structs review period. Most of them focus on the taskof converting images of webpages to HTML and contain data that are either manually curated orsynthetic. Soselia et al. creates two synthetic datasets with over 75,000 unique (code, screenshot)pairs to train a machine-learning model to produce HTML/CSS code from UI screenshots. Si etal. curate a benchmark of 484 diverse real-world webpages by filtering the C4 dataset to test theability of VLMs on converting visual designs into code implementations. Wan et al. manuallycurate a dataset of 1,000 top-ranked real-world websites and propose a method to translate webpagedesign to code. In contrast to Image2Struct, where the screenshots of the webpages are fed to theVLMs as-is, Wan et al. replaces images with placeholders and removes all external links and elementsthat do not impact websites appearance before rendering them as input images. Plot2Code manually curates 132 matplotlib plots sourced from matplotlib galleries and tests VLMs in generatingPython code from them. In contrast to these benchmarks, Image2Struct automatically obtains fresh,real-world data from online communities to test structure extraction from images across multipledomains. Metrics.There is a body of research on image comparison. Apart the metrics used in Image2Struct(i.e., LPIPS , EMD, or SSIM ), one may also employ the CLIP Score or cross-correlation, or CIEDE2000 color difference. The task of quantifying image similarity is relatedto pattern matching, where Siamese networks are used for facial recognition and SIFT is used forlocal feature matching . If the images are first broken down into visual elements (aka blocks),Block-matchwhich measures the ratio of matched block sizes to all block sizesand the meandistance between matched blocks can be used to assess similarity . Round-trip evaluation.The round-trip evaluation idea used in Image2Struct (input image structure rendered image) is related to backtranslation and cycle-consistency .While existing works focus on training models for bidirectional mapping, we focus on evaluating themodels.",
  "Discussion": "VLM sensitivity to prompts and adaptations.VLMs are sensitive to prompts and our standardizedtext prompts may impact model evaluations. This is most evident when some models initially refuseto produce LilyPond codes but then comply when we state that the images are created by us. Beyondzero-shot prompting, there exist a variety of adaptation methodsspecific procedures for invoking amodelsuch as chain-of-thoughts or auto-prompt that can enhance the performance of themodels. We leave measuring the performance of VLMs under other adaptations as future work. Fine-tuning for structure extraction.Image2Struct proposes the task of extracting structure fromimages and uses a round-trip comparison methodology to evaluate the model. On the one hand, itwould be interesting to explore fine-tuning VLMs using the round-trip supervision to perform these useful tasks. On the other hand, developers may simply fine-tune on the Image2Struct dataset andtherefore overfit on the tasks. Ideally, VLMs will acquire the capability to extract and reason overany structured data on image media. Updates with fresh data.Image2Struct is amenable to rapid updates; the active online communitiesfrom which we download test instances from provide ample fresh data and the round-trip evaluationmethodology avoids the need to label the newly downloaded data. As such we can quickly test newmodels with unseen data and thereby avoid the issue of data leakage. We note that the benchmarkmust be rerun on all the models after a data refresh so that an apples-to-apples comparison can bemade. An exponential weighing scheme that prioritizes the latest batch of test instances can be usedto incorporate both past and new test data.",
  "Limitations": "Imperfect metrics.The automated evaluation in Image2Struct hinges on having good metrics tocompare the output image to the input image. The metrics introduced in this paper are not perfect,even though they have a high correlation with the edit distance between the generated and ground-truth source code. The EMS, for example, is still unable to discern if key elements exist in two imagesif the elements undergo other affine transformations beyond translation. Our work motivates futureresearch in evaluating the similarity between rendered and ground-truth images, including developingevaluator VLMs that can be deployed as automatic evaluators to quantify image similarities. Limited scope of evaluation.Our benchmark is limited in what it evaluates in several aspects.Firstly, it focuses solely on measuring structure extraction and does not capture tasks that involveeither producing more abstract concepts or reasoning over extracted structure. Secondly, performingwell on Image2Structure requires the VLMs to have knowledge of formal languages (e.g., HTMLor LaTeX) in addition to visual understanding; as such, it does not discern between a model that isexcellent at understanding structured data but is poor in a language, and, an model that is simply poorat understanding structured information. Thirdly, we do not measure robustness of the VLMs to noise(e.g., scans) and other perturbations (e.g., image skew). We leave these possible extensions as futurework.",
  "Broader Impacts": "The task of extracting the structured data embedded in an input is applicable to a very broad andpractical set of tasks. Beyond parsing images of webpages, scientific documents, and sheet music, weenvision the same framework can be extended to extract information from visual content in variousdomains (e.g., radiology images, electronic health records) and modalities (e.g., 3D graphics). It ispossible that in the future, one will be able to command a VLM to convert a natural image into astructured form so that it can be edited. This work also introduces automatic, repeatable, and reproducible evaluation methods, as well asmethod to produce fresh test sets for fair evaluation. This promotes transparency and accountabilityin model development and stakeholders (including researchers, developers, and policymakers) canbetter understand and compare the performance of different VLMs. Our evaluation framework allows the development of powerful VLMs which may displace existingworkers (e.g., data entry personnel). The ease of replicating and editing rendered structures can bemisused and we urge developers to consider the implications of their technology and take appropriatemeasures to safeguard against misuse.",
  "Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, JiaqiWang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-languagemodels?, 2024": "S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity metric discriminatively, withapplication to face verification. In 2005 IEEE Computer Society Conference on ComputerVision and Pattern Recognition (CVPR05), volume 1, pages 539546 vol. 1, 2005. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, WeishengWang, Boyang Li, Pascale N Fung, and Steven Hoi. InstructBLIP: Towards general-purposevision-language models with instruction tuning. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens ofcontext, 2024": "Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang,Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnosticsuite for entangled language hallucination and visual illusion in large vision-language models.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1437514385, 2024. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In InternationalConference on Machine Learning (ICML), 2018. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances inNeural Information Processing Systems, 36, 2024. Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato. Unsuper-vised machine translation using monolingual corpora only. In International Conference onLearning Representations (ICLR), 2018. Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato.Phrase-based & neural unsupervised machine translation. In Empirical Methods in NaturalLanguage Processing (EMNLP), 2018. Hugo Laurenon, Lucile Saulnier, Lo Tronchon, Stas Bekman, Amanpreet Singh, AntonLozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics:An open web-scale filtered dataset of interleaved image-text documents. Advances in NeuralInformation Processing Systems, 36, 2024.",
  "Hugo Laurenon, Lo Tronchon, Matthieu Cord, and Victor Sanh. What matters when buildingvision-language models? arXiv preprint arXiv:2405.02246, 2024": "Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, and Ming-Hsuan Yang. Text-driven imageediting via learnable regions. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 70597068, 2024. Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigat-ing hallucination in large multi-modal models via robust instruction tuning. In The TwelfthInternational Conference on Learning Representations, 2024. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visualinstruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2629626306, 2024.",
  "D.G. Lowe. Object recognition from local scale-invariant features. In Proceedings of theSeventh IEEE International Conference on Computer Vision, volume 2, pages 11501157 vol.2,1999": "M Ronnier Luo, Guihua Cui, and Bryan Rigg. The development of the cie 2000 colour-differenceformula: Ciede2000. Color Research & Application: Endorsed by Inter-Society Color Council,The Colour Group (Great Britain), Canadian Society for Color, Color Science Association ofJapan, Dutch Society for the Study of Color, The Swedish Colour Centre Foundation, ColourSociety of Australia, Centre Franais de la Couleur, 26(5):340350, 2001.",
  "Writer. Meet Palmyra-Vision, our multimodal LLM with vision capabilities, February 2024": "Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, andPing Luo. Plot2Code: A comprehensive benchmark for evaluating multi-modal large languagemodels in code generation from scientific plots. arXiv preprint arXiv:2405.07990, 2024. Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, SiyuanHuang, Yu Qiao, and Ping Luo. LVLM-eHub: A comprehensive evaluation benchmark for largevision-language models. arXiv preprint arXiv:2306.09265, 2023. Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mPLUG-Owl: Modularization empowers largelanguage models with multimodality. arXiv preprint arXiv:2304.14178, 2023. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,and Lijuan Wang. MM-Vet: Evaluating large multimodal models for integrated capabilities.arXiv preprint arXiv:2308.02490, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: A massive multi-discipline multi-modal understanding and reasoning benchmark for expert AGI. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreason-able effectiveness of deep features as a perceptual metric. In 2018 IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 586595, 2018. Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan,William Yang Wang, and Linda Ruth Petzold. GPT-4V(ision) as a generalist evaluator forvision-language tasks. arXiv preprint arXiv:2311.01361, 2023. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image trans-lation using cycle-consistent adversarial networks. In International Conference on ComputerVision (ICCV), 2017. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart,Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong Tran, Radu Soricut,Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S.Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, YaoLu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, DmitryKalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, AlexanderHerzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn,Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen,Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas,and Kehang Han. Rt-2: Vision-language-action models transfer web knowledge to roboticcontrol. In Jie Tan, Marc Toussaint, and Kourosh Darvish, editors, Proceedings of The 7thConference on Robot Learning, volume 229 of Proceedings of Machine Learning Research,pages 21652183. PMLR, 0609 Nov 2023.",
  "(b) Did you specify all the training details (e.g., data splits, hyperparameters, how theywere chosen)? [Yes] We provide the experimental details in": "(c) Did you report error bars (e.g., with respect to the random seed after running ex-periments multiple times)? [No] We do not compute error bars. Repeating theexperiments with multiple runs is prohibitively expensive. We explained in Sec-tion 3 that we reduce variability in the output by setting the temperature to zero.We also believe that our results will not change significantly with multiple queriesbecause of the large evaluation dataset. Furthermore, multiple runs may not beindicative since we do not know what happens on the server (for API models);our input may have been cached and outputs from the cache may be served andthereby invalidating any attempt to measure variance. (d) Did you include the total amount of compute and the type of resources used (e.g., typeof GPUs, internal cluster, or cloud provider)? [Yes] We state the number of inputand output text tokens used in our evaluation in .",
  "GitHubAcceptableUsePolicies": "Terms of Use for arXiv APIs IMSLPTermsofService of data in the dataset is retained by the original copyright owners.All data is used under Fair Use doctrine, as the data is used for non-commercialresearch purposes only.The source code was written by the paper authors and was licensed under ApacheLicense, Version 2.0. Code from arxivscraper was modified and redistributedunder the MIT license.",
  "j{1, ,H}1{x(i, j) = x(i, j)}(5)": "While it captures the idea that x should be identical to x, it leaves little room forerror and may generate a poor score even when x is simply a pixel shift of x. Thismay cause the benchmark to suffer from a lack of ability to differentiate the modelsperformances and produce false rankings. Our implementation ignores the modalcolor (usually the background) in the union of x and x and allows some minor colordifferences (i.e., pixels in the same location are considered the same if their colorvalues are within 2% of one another). SSIM.SSIM is a score that compares the luminance, contrast and pixel variancebetween two images. We use the implementation provided by scikit-image andrefer readers to Zhou et al. for calculation details. We normalize the score to bebetween 0 and 1 and modify it such that a higher value indicates higher similarity(i.e., 1 indicates exact match). LPIPS.LPIPS has been shown to match human perception and is similar toCIS in that it applies a distance metric between the activations of a neural network.We use the implementation provided by torchvision and choose VGG as theneural network of interest. We refer readers to the original paper for implementationdetails. We normalize the scores to be between 0 and 1, where a higher score meansthat the images are more similar. Earth Mover DistanceTo compute the classic EMD between two images x and x,we first transform the images into signatures S(x) and S(x), which are discretedistributions of features of Q elements. The signature is defined as the distributionof the grayscale values of an image when one wants to compare images. In otherwords, S(x) is the probability mass function where the random variable (i.e., gk orhl) is one of the possible pixel values (0 to 255) and the mass (i.e., wgk or whl ) is thenormalized count of the number of pixels in x with that value.",
  "BIllustration of EMDblock": "Figure A1: An illustration of the two scales at which EMDblock operates. The left image is an alteredcopy of the right one in that 4 patches are manipulated. EMDblock computes an optimal flow where 3of these patches (in red) are moved completely without modification. For the blue patch, it decidesthat it incurs a lower cost to move some pixels within the patch (the zoomed version on the right). Ontop of moving blocks or pixels, EMDblock can change the pixel colors at a cost (we do not illustratecolor modification in this example for simplicity).",
  "We detail the text prompts provided to the VLMs in this section": "MusicJust give a short answer without answering in a complete sentence.Please generate the Lilypond code to generate a music sheet that looks like this image as much as feasiblypossible. This music sheet was created by me, and I would like to recreate it using Lilypond. LaTeXPlease provide the LaTeX code used to generate this image. Only generate the code relevant to what you see.Your code will be surrounded by all the imports necessary as well as the begin and end document delimiters. WebpagesPlease generate the source code to generate a webpage that looks like this image as much as feasibly possible.You should output a JSON object associating each file name with its content.Here is a simple example of the expected structure (that does not correspond to the image). In this example, 3files are created: index.html, style.css and script.js.[{\"filename\": \"index.html\",\"content\": \"<!DOCTYPE html>\\n<html>\\n<head>\\n<title> Title of the document</title>\\n</head>\\n<body>\\n\\n<p>Content of the document......</p>\\n\\n </body>\\n</html>\"},{\"filename\": \"style.css\",\"content\": \"body \\n background-color: lightblue;\\n\\nh1 \\n color: white;\\n text-align: center;\\n\"},{\"filename\": \"script.js\",\"content\": \"document.getElementById(\\\"demo\\\").innerHTML = \\\"Hello JavaScript!\\\";\"}] You do not have to create files with the same names. Create as many files as you need, you can even usedirectories if necessary, they will be created for you automatically. Try to write some realistic code keeping inmind that it should look like the image as much as feasibly possible.",
  "IResults of error analysis": "For our error analysis of the state-of-the-art models, we randomly selected 7 ex-amples for each subdomain (i.e., LaTeXAlgorithm, LaTeXEquation, ...) andmanually analyzed 56 compiled images for GPT-4o. Across all domains, we observe that GPT-4o is capable of extracting text with minor(no change in meaning) or no error for 42 of the 48 instances (the remaining 7instances of music do not contain relevant text).",
  "I.1LaTeX": "When we look at the 14 instances in LaTeX equations and algorithms, we find thatthe common mistakes are 1) failure to insert newlines (3 of 14), 2) failure to makesymbols italics or bold font (3 of 14), and 3) using the wrong modifiers such as hatsand bars (1 of 14).",
  "Figure A10: Example of a prediction with incorrect modifiers (circled in purple)": "For the 7 LaTeX plots, we observe while all text is extracted and simple geometricshapes (e.g., rectangles, diamonds, or lines) are created, none of the renderedimages is satisfactory; GPT-4o attempts to replicate the color of the elements butonly succeeds when the colors used are the primary ones (e.g., red or blue inxcolor but no colors defined with RGB values). The relative sizes and positions ofthe elements are not respected and the points in scatterplots seem to be randomlygenerated.",
  "Figure A13: Example of a prediction with incorrect numerical values": "GPT-4o is able to replicate LaTeX tables with minor or no mistakes 4/7 of the time.In 2 of the remaining 3 instances, it fails to parse the data correctly, resulting inwrong or missing data. In the final instance, it missed out a tilde above one of thesymbols, which drastically changed the meaning.",
  "I.3Webpages": "For Webpages, GPT-4o manages to reproduce the text and simple elements in all the21 instances analyzed. We notice that the model struggles with backgrounds withcolor gradients in all the 3 instances where they appear (similar issue as color errorsin LaTeX plots). In 12 of the instances, we observe bad relative size, position, oralignment of the elements, resulting in low scores. The substance is captured (e.g.,there is a \"Login\" button) but the form is very often incorrect (e.g., the button has adifferent font, color and is not placed where it should be)",
  "Q1 For what purpose was the dataset created? Was there a specific task in mind? Was therea specific gap that needed to be filled? Please provide a description": "The Image2Struct benchmark was created to evaluate the Vision-Language modelson extracting structure from images. VLMs are prompted to generate the underlyingstructure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot).The structure is then rendered to produce an output image (e.g., rendered webpage),which is compared against the input image to produce a score.",
  "Q6 How many instances are there in total (of each type, if appropriate)?": "In all, we collected a total of 900 instances for webpages (300 each for HTML, CSSand JS), 1200 instances for LaTeX (300 each for equations, tables, algorithms, andplots), and 300 for music for a grand total of 2400 test instances. Q7 Does the dataset contain all possible instances or is it a sample (not necessarily random)of instances from a larger set? If the dataset is a sample, then what is the larger set? Is thesample representative of the larger set (e.g., geographic coverage)? If so, please describehow this representativeness was validated/verified. If it is not representative of the larger set,please describe why not (e.g., to cover a more diverse range of instances, because instanceswere withheld or unavailable).",
  "No": "Q14 Is the dataset self-contained, or does it link to or otherwise rely on external resources(e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) arethere guarantees that they will exist, and remain constant, over time; b) are there officialarchival versions of the complete dataset (i.e., including the external resources as theyexisted at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees)associated with any of the external resources that might apply to a future user? Pleaseprovide descriptions of all external resources and any restrictions associated with them, aswell as links or other access points, as appropriate.",
  "It may be possible to identify individuals if the data instances is linked to the profile ononline communities": "Q20 Does the dataset contain data that might be considered sensitive in any way (e.g., datathat reveals racial or ethnic origins, sexual orientations, religious beliefs, politicalopinions or union memberships, or locations; financial or health data; biometric orgenetic data; forms of government identification, such as social security numbers;criminal history)? If so, please provide a description.",
  "L.3Collection Process": "Q22 How was the data associated with each instance acquired? Was the data directlyobservable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), orindirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guessesfor age or language)? If data was reported by subjects or indirectly inferred/derived fromother data, was the data validated/verified? If so, please describe how. All the data in this dataset was directly observable and collected from public sourcesnamely Github, arXiv, and IMSLP. The data was collected based on some searchcriterias such as categories and dates. The data was then fetched according to thesearch results order of these websites and manually inspected to control the quality anddiversity of the dataset. Q23 What mechanisms or procedures were used to collect the data (e.g., hardware apparatusor sensor, manual human curation, software program, software API)? How were thesemechanisms or procedures validated? The existing scenarios were downloaded by us. The data was collected using software API or by scrapping the aforementioned websites.Manual inspection used to change the search criterias but no manuel filtering wasperformed.",
  "L.4Preprocessing, Cleaning, and/or Labeling": "Q35 Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucket-ing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances,processing of missing values)? If so, please provide a description. If not, you may skip theremainder of the questions in this section. Yes. For LaTeX we extract from the original LaTeX code some self-sufficient blocks(equations, plots, ...) and add our own wrappers around it (Please refer to our Githubrepository to see the packages we use as this may change in the future). For musicalscores we extract measures from pages, please refer to .1.3 for more details.",
  "The primary use case of our benchmark is the evluation of VLMs in the context ofextracting information from images": "Q42 Is there anything about the composition of the dataset or the way it was collectedand preprocessed/cleaned/labeled that might impact future uses? For example, is thereanything that a future user might need to know to avoid uses that could result in unfairtreatment of individuals or groups (e.g., stereotyping, quality of service issues) or otherundesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Isthere anything a future user could do to mitigate these undesirable harms?",
  "April 1, 2024 and onward": "Q48 Will the dataset be distributed under a copyright or other intellectual property (IP)license, and/or under applicable terms of use (ToU)? If so, please describe this licenseand/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevantlicensing terms or ToU, as well as any fees associated with these restrictions.",
  "Image2Struct will be updated. We plan to expand scenarios, metrics, and models to beevaluated": "Q56 If the dataset relates to people, are there applicable limits on the retention of the dataassociated with the instances (e.g., were individuals in question told that their datawould be retained for a fixed period of time and then deleted)? If so, please describethese limits and explain how they will be enforced.",
  "We will host other versions. We plan to rerun the data collection on a regular basis toensure that some unseen data is always available": "Q58 If others want to extend/augment/build on/contribute to the dataset, is there a mech-anism for them to do so? If so, please provide a description. Will these contributionsbe validated/verified? If so, please describe how. If not, why not? Is there a process forcommunicating/distributing these contributions to other users? If so, please provide adescription."
}