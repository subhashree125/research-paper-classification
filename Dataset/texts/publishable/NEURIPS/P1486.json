{
  "Abstract": "Multiple clustering aims to discover various latent structures of data from differentaspects. Deep multiple clustering methods have achieved remarkable performanceby exploiting complex patterns and relationships in data. However, existing worksstruggle to flexibly adapt to diverse user-specific needs in data grouping, whichmay require manual understanding of each clustering. To address these limitations,we introduce Multi-Sub, a novel end-to-end multiple clustering approach that incor-porates a multi-modal subspace proxy learning framework in this work. Utilizingthe synergistic capabilities of CLIP and GPT-4, Multi-Sub aligns textual promptsexpressing user preferences with their corresponding visual representations. This isachieved by automatically generating proxy words from large language models thatact as subspace bases, thus allowing for the customized representation of data interms specific to the users interests. Our method consistently outperforms existingbaselines across a broad set of datasets in visual multiple clustering tasks. Ourcode is available at",
  ": The workflow of Multi-Sub thatobtains a desired clustering based on the sub-space spanned by reference words obtainedfrom GPT-4 using users high-level interest": "Clustering is a fundamental technique for analyzingdata based on certain similarities, attracting extensiveattention due to the abundance of unlabeled data. Tra-ditional clustering methods [MacQueen et al., 1967,Ng et al., 2001, Bishop and Nasrabadi, 2006] rely ongeneral-purpose handcrafted features that may notsuit specific tasks well. Deep clustering algorithmshave improved clustering performance by employingDeep Neural Networks (DNNs) [Xie et al., 2016,Gurin and Boots, 2018, Qian et al., 2022, Qian,2023] to learn task-specific features. However, mostof these algorithms assume a single partition of thedata, while real data can be clustered differently ac-cording to different aspects, e.g., fruits in canbe grouped differently by color or by species.",
  "arXiv:2411.03978v1 [cs.LG] 6 Nov 2024": "showing the capability of discovering multiple clusterings from a single dataset. For instance, ine-commerce, products can be clustered by category for inventory management or by customerpreferences for personalized recommendations. Recently, there has been a growing interest inincorporating deep learning techniques into multiple clustering. These techniques mainly use auto-encoders and data augmentation methods to extract a wide range of feature representations, whichenhance the quality of multiple clustering [Miklautz et al., 2020, Ren et al., 2022, Yao et al., 2023]. For real-world applications, a key challenge for end users is efficiently identifying the desiredclustering from multiple results based on their interests or application purposes. We observe that usersare willing to indicate their interest using succinct keywords (e.g., color or species for fruits in ).However, it is difficult to use only a concise keyword to directly extract the corresponding imagerepresentations. Fortunately, the recent development of multi-modal models like CLIP [Radfordet al., 2021] that align images with their text descriptions can help bridge this gap. Nevertheless,unlike methods that can use labeled data to fine-tune pre-trained models [Gao et al., 2023, Wanget al., 2023] to learn new task-specific representations, multiple clustering often faces scenarios withambiguous or unspecified label categories and quantities. Therefore, given only a high-level conceptfrom the user, it is intractable to fine-tune pre-trained models to capture a particular aspect of thedata in an unsupervised manner. Very recently, Multi-MaP [Yao et al., 2024] leverages CLIP tolearn textual and image embeddings simultaneously that follow the users high-level textual concept.However, to achieve better performance, they require the user to provide a contrastive concept thatis different from the desired concept, which may not be feasible in many real-world applications.Moreover, they obtain the new representations at first and then apply the traditional clustering methodlike k-means in a separate stage. This insufficient optimization lacking refinement between stagesmakes the clustering performance sub-optimal. To mitigate these challenges, in this work, we first assume that the desired image and textualrepresentations are residing in the same subspace according to the users specific concept. Thereafter,to capture the desired subspace better, we can ask low-cost experts like Google or large languagemodels (LLMs) (e.g., GPT-4) for common categories under the desired concept, as illustrated in. Although those returned common categories may not directly capture the clustering targets,they can be applied as the subspace basis to help search the appropriate representations inside. Moreimportantly, during the learning under the desired subspace, we also incorporate the clustering lossto learn the representations and obtain the clustering simultaneously, which significantly enhancesthe models clustering performance and efficiency. The main contributions of this work can besummarized as follows. We present a novel multiple clustering method, Multi-Sub, that can explicitly capture ausers clustering interest by aligning the textual interest with the visual features of images.Concretely, we propose to learn the desired clustering proxy in the subspace spanned by thecommon categories under a users interest. Unlike most existing multiple clustering methods that require distinct stages for represen-tation learning and clustering, Multi-Sub can obtain both the desired representations andclustering simultaneously, which can significantly improve the clustering performance andefficiency.",
  "Multiple Clustering": "Multiple clustering, a methodology capable of unveiling alternative data perspectives, has garneredsignificant interest. Traditional approaches for multiple clustering [Hu and Pei, 2018] employ shallowmodels to identify diverse data groupings. Some methods, such as COALA [Bae and Bailey, 2006]and [Qi and Davidson, 2009], utilize constraints to generate alternative clusterings. Other techniquesleverage distinct feature subspaces to produce multiple clusterings, as exemplified by [Hu et al., 2017]and MNMF [Yang and Zhang, 2017]. Information theory has also been applied to generate multipleclusterings, as demonstrated by [Gondek and Hofmann, 2003] and [Dang and Bailey, 2010]. Recent advancements have seen the application of deep learning to discover multiple clusterings,yielding improved clustering performance. For instance, [Wei et al., 2020] proposed a deep matrixfactorization method that utilizes multi-view data to identify multiple clusterings. ENRC [Miklautzet al., 2020] employs an auto-encoder to learn object features and optimizes a clustering objectivefunction to find multiple clusterings. iMClusts [Ren et al., 2022] leverages auto-encoders andmulti-head attention to learn features from various perspectives and discover multiple clusterings.AugDMC [Yao et al., 2023] uses data augmentation to generate diverse image aspects and learnsrepresentations to uncover multiple clusterings. DDMC [Yao and Hu, 2024] employs a variationalExpectation-Maximization framework with disentangled representations to achieve superior clusteringoutcomes. However, almost all of these methods necessitate substantial user efforts to understand andselect the appropriate clustering for different application purposes. Recently, Multi-MaP [Yao et al.,2024] leverages CLIP encoders to align a users interest with visual data by learning representationsclose to the interested concept but far away from a contrastive concept, significantly improving theefficiency of capturing user-desired clusterings. However, Multi-MaP requires the user to inputa contrastive concept for better performance, which is often not applicable. More importantly, itseparates the representation learning and clustering as two distinct stages, which may result insub-optimal performance. These issues will be mitigated in this work.",
  "Multi-Modal Models": "Multi-modal learning involves acquiring representations from various input modalities like image,text, or speech. Here, we focus on how vision models benefit from natural language supervision. Akey model in this area is CLIP [Radford et al., 2021], which aligns images with their correspondingtext using contrastive learning on a dataset of 400 million text-image pairs. Fine-tuning adapts vision-language models, such as CLIP, for specific image recognition tasks. Thisis seen in CoOp [Zhou et al., 2022] and CLIP-Adapter [Gao et al., 2023], the latter using residualstyle feature blending to enhance performance. TeS [Wang et al., 2023] highlights the efficacy offine-tuning in improving visual comprehension through natural language supervision. With limitedlabeled data, zero-shot learning has gained attention. Some approaches surpass CLIP by integratingother large pre-trained models. For example, VisDesc [Menon and Vondrick, 2022] uses GPT-3 togenerate contextual descriptions for class names, outperforming basic CLIP prompts. UPL [Huanget al., 2022] and TPT [Shu et al., 2022] utilize unlabeled data to optimize text prompts. InMaP [Qianet al., 2024] and the online variant [Qian and Hu, 2024] aid class proxies in vision space with textproxies. Recent advancements have significantly improved vision-language pre-training using large-scale noisy datasets. ALIGN [Jia et al., 2021] employs over one billion image alt-text pairs withoutexpensive filtering, showing that corpus scale can offset noise. Similarly, BLIP-2 [Li et al., 2023]uses a novel framework to bootstrap captions from noisy web data, enhancing both vision-languageunderstanding and generation tasks. While these methods strive to enhance the performance of visionclassification tasks, clustering presents a distinct scenario where class names are not available toextract useful information from multi-modal information as in this work.",
  "The Proposed Method": "Given a dataset of images {xi}ni=1 and user-defined preferences for data grouping (such as color andspecies), our goal is to generate clustering results that are specifically tailored to each preference.Thereafter, end users can directly use them for different application purposes without additionalmanual selection efforts. This process poses significant challenges, as it requires accurately aligningthe complex, multi-dimensional data of images with the subjective and varied textual preferences ofusers. Traditional clustering methods often fail to capture these nuances, leading to a generic and lessinformative categorization for specific user applications. Recently, the CLIP model [Radford et al., 2021] facilitated a more natural alignment between textualinterests and visual representations. Our method, Multi-Sub, extends this alignment through a novelmulti-modal subspace proxy learning approach. outlines the overall framework of Multi-Sub,which is tailored to capture and respond to the diverse interests of users in clustering tasks. Multi-Subemploys a two-phase iterative approach to align and cluster images based on user-defined preferencessuch as color and species as described below.",
  "similarity": ": Multi-Sub framework. In Multi-Sub framework, Phase I (Proxy Learning and Alignment)processes each image xi with user-defined textual prompts through a partially learnable imageencoder (with a learnable projection layer) and a frozen text encoder. The latent factor pi calculatesweights {ai,k}Kk=1 based on the similarity to reference word embeddings {zi}Kk=1, which are thenaggregated to form the proxy word embedding wi. This proxy word embedding, combined withthe image representation xi, establishes the Aligned Feature Subspace for better alignment betweenthe text and image under the users interest. In Phase II (Clustering), given the learned proxy wordembeddings {wi} from Phase I to form pseudo-labels, the projection layer of the image encoder isfurther refined using the clustering loss. In Phase I, both the latent factor p and the projection layerlearn 100 epochs, after which the projection layer further learns 10 epochs using the clustering loss inPhase II. This alternative process repeats until convergence.",
  "Background: Multi-Modal Pre-Training in CLIP": "Let {xi, ti}ni=1 be a set of image-text pairs, where xi denotes an image and ti denotes its correspond-ing text description. We can obtain the vision and text representations of each pair by applying twoencoders, f() and h(), as xi = f(xi) and ti = h(ti). Both f() and h() are encoders that optimizethe vision and text representations, respectively, such that xi and ti are unit vectors. The primarygoal during this pre-training phase is to minimize the contrastive loss, formulated as",
  "j exp(ti xj/)(1)": "where is a temperature parameter. The contrastive loss encourages the alignment of the image andits description while penalizing the similarity of the image with irrelevant texts [Qian et al., 2019].The efficacy of this contrastive approach is vital for the subsequent phases of proxy word learning andfine-grained clustering, as it ensures that the foundational embeddings accurately reflect the inherentcontent and context of each modality.",
  "We build upon the pre-trained image and text encoders from CLIP and investigate whether we canleverage the image-text alignment to extract user-specific information. Specifically, given a fruit": "image [Hu et al., 2017] as illustrated in , different users may have different interests of itsattributes, such as color, species, etc. However, the pre-trained image encoder in CLIP can onlyproduce a single image embedding, which may not capture a users interest exactly, not mentioningcapturing different aspects. Furthermore, unlike classification tasks, clustering tasks do not come withconcrete cluster names or numbers. Therefore, we cannot directly use the pre-trained text encoder ofCLIP to generate the corresponding text embedding. To address these challenges, we propose a subspace proxy word learning method to learn newembedding under the preferred aspect provided by the user. Thereafter, the main challenge is, givenonly a high-level concept like color as in , how to effectively represent its subspace. Sincethe high-level concept itself cannot reflect different details under this concept in different images,it is difficult to do effective alignment between the high-level concept and images to figure outthe corresponding vision subspace. Therefore, we propose to figure out the text subspace at first.Concretely, given pre-trained large language models like GPT-4 as low-cost experts, we can quicklygather common categories under a high-level concept using only one query like what are the commonfruit colors in . However, we cannot directly use the returned categories to do grouping, sincethey may not cover all existing categories in the data. Instead, we consider that most categories in thedata under this concept are residing in the same subspace as the returned ones. Therefore, we canapply suggested categories as basis or reference words in the subspace. Then, each images categoryunder the desired concept can be represented by a linear combination of these reference words.",
  "k=1ai,k(zk)(2)": "where (zk) is the token embedding of reference word zk and {ai,k}Kk=1 are weights correspondingto each reference word as a basis. A higher weight ai,k indicates that the image xis category is closerto the reference word zk. Here, we introduce trainable latent factor pi to learn the weight ai,k, and itcan be calculated as",
  "Multi-Modal Subspace Proxy Learning": "As mentioned above, CLIPs text and image encoders were learned by aligning the text prompt withits corresponding image. The standard text prompt of CLIP is designed as a photo of a fruit for animage containing fruit. Now, given a users preference (e.g., color), we can rewrite the prompt asa fruit with the color of * denoted by ti for image xi, where * is the placeholder for the unknownproxy word of image xi under concept color and its token embedding wi can be formulated as thelinear superposition of reference words token embeddings as discussed above.",
  "ti = h((ti )(wi))(4)": "To effectively learn pi, the trainable latent factors, we utilize the alignment capabilities of CLIP byadjusting these factors so that the weighted sum of reference word embeddings closely aligns withthe visual representation of the image. This process involves iteratively adjusting pi to maximize thecosine similarity between the images representation xi and its corresponding proxy word embeddingwi. The optimization is conducted with the following loss function:",
  "L(wi) = f(xi), h((ti )(wi))(5)": "It should be noted that this optimization procedure can be conducted with both the text encoder andimage encoder frozen, which is very efficient. However, the image embedding extracted directlyfrom the pre-trained image encoder may not reflect its representation under the desired user interest.Therefore, during the optimization procedure, we do freeze the text encoder but open the imageencoder. Nevertheless, to preserve the strong capacity of the pre-trained image encoder in CLIP, weopen only the projection layer of the image encoder, while its remaining parameters are frozen asshown in the Phase I of .",
  "Clustering Loss": "To enhance the clustering performance of Multi-Sub, in Phase II, we leverage pseudo-labels assignedusing the currently learned proxy word embeddings {wi} and image embeddings {xi} from PhaseI. Concretely, each image xi can be represented by the concatenation of its currently learned proxyword embedding wi and image embedding xi, denoted as vi = [wi, xi]. The pseudo-labels canbe obtained by an offline k-means on {vi}, which is however not efficient. Considering that proxywords for data points within the same cluster should show similar relationships to reference words,we obtain the pseudo-labels using the highest cosine similarity between the currently learned proxyword embeddings {wi} and the reference word embeddings {zk}. Given the pseudo-labels, the image embeddings can be further optimized by opening only theprojection layer of the image encoder for improved compactness and separability in clusters. Thisloss consists of two primary components: intra-cluster loss and inter-cluster loss, aimed at refiningcluster cohesion and separation, respectively. It should be noted that to better represent each imageunder the desired user concept, we define the clustering loss over vi containing both textual andvisual information. Intra-cluster Loss: The intra-cluster loss is designed to minimize the distances between embeddingswithin the same cluster, encouraging cluster compactness. It is calculated using the following formula:",
  "where max(0, m vi vj) computes the hinge loss for each pair of embeddings from differentclusters, ensuring a minimum margin m between them. Ninter is the count of inter-cluster pairs": "Total Loss: The overall clustering loss combines the intra- and inter-cluster losses, moderated by abalancing factor :Ltotal = Lintra + (1 ) Linter(8)Optimizing this loss function in Phase II helps regularize the embedding space where clusters areboth internally dense and well-separated from each other. It should be noted that in this phase we aimto learn a better projection layer only for the image encoder, while all others are fixed as shown inPhase II of . Previous methods often use a two-stage strategy that separates representation learning and clusteringto simplify the optimization process. This separation, however, can lead to sub-optimal clusteringresults, since the learned representations may not be fully aligned with the clustering objectivewithout refinement. In this work, we obtain both the proxy word and the clustering alternatively andsimultaneously. Concretely, we first learn the proxy word in a user-preferred subspace. Then, we fixthe proxy word and refine the image encoder further to obtain better image representations using theclustering objective. These two phases are repeated alternatively until convergence, where PhaseI learns 100 epochs and Phase II learns 10 epochs in each alternating according to the empiricalexperience as summarized in .",
  "Experiments": "DatasetsTo demonstrate the effectiveness of Multi-Sub, we evaluate the proposed method onalmost all publicly available visual datasets commonly used in multiple clustering tasks Yu et al., including Stanford Cars Yao et al. , Card Yao et al. , CMUface Gnnemann et al., Flowers Yao et al. , Fruit Hu et al. and Fruit360 Yao et al. . StanfordCars contains two different clustering types, one for car color (e.g., red, blue, black) and one forcar type (e.g., sedan, SUV, convertible), comprising 1,200 annotated car images. Card includes8,029 images of playing cards, with two clustering types: one based on rank (e.g., Ace, King, Queen)and another on suit (e.g., clubs, diamonds, hearts, spades). CMUface provides 640 facial imageswith clustering options for pose (e.g., front-facing, side-facing), identity, glasses (with/without), andemotion (e.g., happy, neutral, sad). Flowers comprises 1,600 flower images with two clusteringtypes: one for color (e.g., red, blue, yellow) and another for species (e.g., iris, aster). Fruit includes105 images of fruits with two clustering criteria: species (e.g., apples, bananas, grapes) and color(e.g., green, red, yellow). Fruit360, similar to the Fruit dataset, contains 4,856 images annotated forspecies (e.g., apple, banana, cherry) and color. Additionally, we created a multiple clustering dataset from CIFAR-10 Krizhevsky et al. by organizing the images into clusters based on type and environment. For type, the clusters aretransportation and animals. For environment, the clusters are land, air, and water. The datasetcharacteristics about data size, handcrafted features, and cluster information are also summarized in. It should be noted that some data may face challenges in extraction of meaningful candidate categoriesfrom GPT-4, or their labels lack semantic features. Taking the identity clustering on the CMUfacedataset Gnnemann et al. as an example, different identities correspond to different individuals,and the names semantic meanings should not affect clustering outcomes. In such cases, following theMulti-Map setting Yao et al. , we randomly select 10 words from WordNet Fellbaum asreference categories. BaselinesWe compare our Multi-Sub with seven state-of-the-art multiple clustering methods.These methods are: MSC Hu et al. is a traditional multiple clustering method that uses hand-crafted features to automatically find different feature subspace for different clusterings; MCV Gurinand Boots leverages multiple pre-trained feature extractors as different views of the samedata; ENRC Miklautz et al. integrates auto-encoder and clustering objective to generatedifferent clusterings; iMClusts Ren et al. is a deep multiple clustering method that leveragesthe expressive representational power of deep autoencoders and multi-head attention to generate",
  "Environment0.43020.65070.46430.68010.48280.7096": "multiple salient embedding matrices and multiple clusterings therein; AugDMC Yao et al. leverages data augmentations to automatically extract features related to different aspects of the datausing a self-supervised prototype-based representation learning method; DDMC Yao and Hu combines disentangled representation learning with a variational Expectation-Maximization (EM)framework; Multi-MaP Yao et al. relies on a contrastive user-defined concept to learn a proxybetter tailored to a users interest. It is worth noting that, in our experiments, we apply both traditionaland deep learning baselines. Traditional methods rely on hand-crafted features, while deep learningmethods directly utilize the original images as input. HyperparameterFor each users preference, we train the model for 1000 epochs using Adamoptimizer with a momentum of 0.9. We tune all the hyper-parameters based on the loss score ofMulti-Sub, where the learning rate is selected from {1e-1,5e-2,1e-2,5e-3,1e-3,5e-4}, weight decay ischosen from {5e-4,1e-4,5e-5,1e-5, 0} for all the experiments. Most methods obtain each clusteringby applying k-means Lloyd to the newly learned representations, while ours is end-to-end.The experiments are performed on four NVIDIA GeForce RTX 2080 Ti GPUs. Evaluation metricsConsidering the randomness of k-means for those applicable baselines, werun k-means 10 times and report the average clustering performance using two metrics, namely,Normalized Mutual Information (NMI) White et al. and Rand index (RI) Rand . Thesemetrics range from 0 to 1 with higher value indicating better performance compared to the groundtruth.",
  "Performance Comparison": "reports the clustering results. During the clustering stage, after we obtain the proxy wordembedding of each image for a desired concept, we can concatenate the image embedding andthe token embedding of proxy word. The results show that Multi-Sub consistently outperformsthe baselines, demonstrating the superiority of the proposed method. This also indicates a stronggeneralization ability of the pre-trained model by CLIP, which can capture the features of data fromdifferent perspectives. Our methodology uses the CLIP encoder and GPT-4 to derive clustering results, prompting anevaluation of their performance in a zero-shot manner. We introduce two zero-shot variants of CLIP:CLIPGPT and CLIPlabel. CLIPGPT uses GPT-4 to generate candidate labels and performs zero-shotclassification, while CLIPlabel uses ground truth labels directly, providing an optimal setting. As shownin , CLIPlabel generally outperforms CLIPGPT due to its use of accurate labels, while CLIPGPTintroduces noise. Both variants perform equally on the Card dataset as GPT-4s labels match thegroundtruth. Multi-Sub surpasses CLIPGPT and even outperforms CLIPlabel in all cases, demonstratingits ability to capture user-interest-based data aspects and confirming its efficacy. This superiority canbe attributed to Multi-Subs proxy word learning mechanism, which automatically adjusts textual",
  "Ablation study": "Different ways of constructing subspaceThe subspace of the proposed method can be expandedby different embeddings, i.e., the token embedding of the proxy word (wi), the text embedding ofthe proxy word h((wi)), and the text embedding of the prompt ti = h((ti )(wi)). These threekinds of embeddings can also be used to evaluate the clustering results in each case. In addition, wecan use different combinations of learned embeddings (e.g., different concatenations of text and imageembedding) as the final embedding for clustering. The results are shown in . It can be seen thatusing word token embedding usually achieves better results. This is expected since the word proxydirectly reflects the images category under the desired concept. The token word embedding subspaceis also aligning well with CLIPs training method. In contrast, prompt embedding performs theworst as it introduces noise from user interest, dataset, and reference words, which are unnecessaryfor clustering. Additionally, most methods perform better when the same approach is used forconstructing subspace and evaluating clustering results. Combining text and image embeddingsgenerally enhances performance, capturing user interests from both aspects effectively. Effect of text encoder compares the performance of three text encodersCLIP, ALIGN,and BLIPacross various datasets. The results indicate that ALIGN generally outperforms CLIP andBLIP in most tasks. This suggests that ALIGNs text encoder effectively captures and aligns textualand visual representations, enhancing clustering performance. ALIGN tends to excel in tasks thatrequire distinguishing subtle visual differences influenced by textual descriptions, such as emotionsand accessories in the CMUface dataset, and colors in the Fruit360 dataset. CLIP shows a strongtendency in identity-related tasks and complex object categorization, as evidenced by its performancein the CMUface identity task and Standford Cars type clustering. BLIP, while competitive, seems toperform better in categorical distinctions rather than abstract attributes, performing relatively well inspecies-related tasks across various datasets. These findings underscore the importance of effectivetext embeddings in multi-modal clustering frameworks. We conducted an additional analysis using the Maximum Mean Discrepancy (MMD) metric toquantify the differences in the feature spaces generated by different text encoders (i.e., CLIP, ALIGN,and BLIP) in . The MMD results indicate that although our text prompts are simple, the feature spaces generated by different text encoders exhibit significant distributional differences. Theeffectiveness of a text encoder can vary depending on the specific clustering task. For example,ALIGN tends to excel in tasks with more abstract attributes, such as colors and emotions, while CLIPshows strong performance in identity-related tasks. This variability underscores the importance ofselecting an appropriate text encoder based on the specific application requirements. The differencebetween text encoders may come from the different corresponding pre-training tasks and this will bean interesting future direction.",
  "(f) Species of Multi-Sub": ": Visualization of feature embeddings andrelated labels on Fruit dataset. For the visualizationof color, red, green, and yellow points indicate thecolor of red, green, and yellow, respectively. Forthe visualization of species, red, yellow, and purplepoints indicate the species of apple, banana, andgrapes, respectively. VisualizationTo further demonstrate the ef-fectiveness of Multi-Sub, we visualize the repre-sentations from CLIPlabel, CLIPGPT, and Multi-Sub for color and species clustering tasks (Fig-ure 3). In species clustering, CLIPlabel showsclear boundaries using ground truth labels, whileCLIPGPT introduces noise from reference words.Multi-Sub outperforms both by effectively cap-turing image features and user interests withproxy word embeddings. In color clustering,both CLIPlabel and CLIPGPT focus on speciesfeatures, resulting in less distinct clusters. Multi-Sub excels by clearly distinguishing colors,leveraging user-specific interests for improvedalignment.Overall, Multi-Sub consistentlyaligns embeddings with user interests, surpass-ing CLIPlabel and CLIPGPT, demonstrating itsrobust multi-modal subspace proxy learning.",
  "Conclusion and Limitations": "In conclusion, our study mitigates an important challenge in multiple clustering: effectively identify-ing desired clustering results based on user interests or application purposes. We introduce Multi-Sub,a novel approach that integrates user-defined preferences into a customized multi-modal subspaceproxy learning framework. By leveraging the synergy between CLIP and GPT-4, Multi-Sub automati-cally aligns textual prompts expressing user interests with corresponding visual representations. First,we observe reference words for users interests from large language models. Given the absence ofconcrete class names in clustering tasks, our method uses these reference words to learn both text andvision embeddings tailored to user preferences. Extensive experiments across various visual multipleclustering tasks demonstrate that Multi-Sub consistently outperforms state-of-the-art techniques. However, our approach has certain limitations. The reliance on large language models like GPT-4 canintroduce biases inherent in these models, potentially affecting the clustering outcomes. Additionally,the field of multiple clustering lacks large, diverse datasets, which limits comprehensive evaluation.Although we have annotated CIFAR-10, more extensive datasets are needed.",
  "Q. Qian and J. Hu. Online zero-shot classification with clip. In ECCV, 2024": "Q. Qian, L. Shang, B. Sun, J. Hu, H. Li, and R. Jin. Softtriple loss: Deep metric learning withouttriplet sampling. In Proceedings of the IEEE/CVF international conference on computer vision,pages 64506458, 2019. Q. Qian, Y. Xu, J. Hu, H. Li, and R. Jin. Unsupervised visual representation learning by onlineconstrained k-means. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1664016649, 2022.",
  "A.1Further Analysis": "Parameters Analysis.To show the sensitivity of the balancing factor that is the only hyper-parameter in our proposed method, the experiments were conducted on CIFAR-10. We varied thevalue of from 0.1 to 1.0 in increments of 0.1 to observe its effect on the models performance. Asshown in , we can observe that different values of can work with our method and the optimalperformance for Type & Environment clustering is achieved when is set to 0.5. When is toolow, the model focuses too much on maximizing the distances between different clusters, which canlead to less cohesive clusters. Conversely, when is too high, the model emphasizes compactnesswithin clusters at the expense of inter-cluster separation, leading to less distinct clusters. Therefore,we set to be 0.5 for all datasets, which confirms the robustness of our method.",
  ": Sensitivity analysis of balancing factor on CIFAR-10 dataset": "Model Adaptability.To evaluate how Multi-Sub adapts to new user demands not originallyprovided in the ground-truth of the dataset, we conducted an additional experiment using the Fruitdataset. Specifically, we introduced a new demand based on the shape of the fruits, with the promptset as fruit with the shape of *. We categorized the fruits into two shapes, that is, round andelongated. Although this specific demand may not be common in practical applications, it serves asan exploratory experiment to test the adaptability of our method. The results in demonstrate that Multi-Sub successfully adapted to the new user demand ofshape. The model learned to align the textual descriptions of shapes with the visual features, resultingin a clustering under the new subspace of shape."
}