{
  "Abstract": "Continual learning aims to incrementally acquire new concepts in data streamswhile resisting forgetting previous knowledge. With the rise of powerful pre-trainedmodels (PTMs), there is a growing interest in training incremental learning systemsusing these foundation models, rather than learning from scratch. Existing worksoften view PTMs as a strong initial point and directly apply parameter-efficienttuning (PET) in the first session for adapting to downstream tasks. In the followingsessions, most methods freeze model parameters for tackling forgetting issues.However, applying PET directly to downstream data cannot fully explore the in-herent knowledge in PTMs. Additionally, freezing the parameters in incrementalsessions hinders models plasticity to novel concepts not covered in the first session.To solve the above issues, we propose a Slow And Fast parameter-Efficient tuning(SAFE) framework. In particular, to inherit general knowledge from foundationmodels, we include a transfer loss function by measuring the correlation betweenthe PTM and the PET-applied model. After calibrating in the first session, theslow efficient tuning parameters can capture more informative features, improv-ing generalization to incoming classes. Moreover, to further incorporate novelconcepts, we strike a balance between stability and plasticity by fixing slow effi-cient tuning parameters and continuously updating the fast ones. Specifically, across-classification loss with feature alignment is proposed to circumvent catas-trophic forgetting. During inference, we introduce an entropy-based aggregationstrategy to dynamically utilize the complementarity in the slow and fast learners.Extensive experiments on seven benchmark datasets verify the effectiveness of ourmethod by significantly surpassing the state-of-the-art. Code will be available at",
  "Introduction": "Continual Learning (CL) requires deep learning models to incrementally incorporate new conceptsfrom open-world data streams, while retaining previously learned knowledge. This presents a morechallenging yet practical setting compared to traditional deep learning, which typically recognizesonly closed-set categories. A variety of methods have been proposed for continual learning, in-cluding regularization-based , rehearsal-based , and dynamic network-basedapproaches . These methods often assume that the model is trained from scratch, resultingin a substantial performance gap when compared to the joint training upper-bound.",
  ": Inputs": ": Comparisons of (a) prevailing PTM-based CL methods and our Slow And Fastparameter-Efficient tuning (SAFE). The right part (b) illustrates several parameter-efficient tuning(PET) blocks: Adapter , Scale & Shift (SSF) , and Visual Prompt Tuning (VPT) . Most recently, with the emergence of powerful pre-trained models, there has been growing interest inutilizing these foundational models as starting points for continual learning . Pre-TrainedModels (PTMs) which are often trained on vast datasets, encapsulate a wealth of general knowledge,effectively enhancing the performance of deep learning models in continual learning scenarios. Asshown in the left part of (a), for adapting PTMs from pre-training datasets to continual learningdatasets, prevailing works resort to parameter-efficient tuning (PET) techniques in the firstsession. To restrain catastrophic forgetting, in incremental sessions, these works set parameters ofthe adapted model frozen and only update the classification weights in a training-freemanner (i.e., without gradient updates) to accommodate novel classes. However, the above methods have two main limitations. First, direct parameter-efficient tuning in thefirst session will largely lose the general knowledge inherent in PTMs. This is because PTMs arepre-trained on a multitude of datasets while the dataset in the first session only contains relativelylimited samples. Without proper transfer mechanisms, the knowledge from PTMs may be overwrittenby the adapted model, which impedes the models generalizability to unseen classes. Second, freezingparameters in the following sessions will hinder the plasticity of the model to further absorb newconcepts not learned in the first session, resulting in a sub-optimal solution. Although several effortshave been made to mitigate the second limitation, existing works still face certain constraints such asadditional storage requirement , inferior online branch performance and linearly increasedmodel complexity . Based on the above observations, in this paper, we propose Slow And Fast parameter-Efficient tuning(SAFE) to address existing challenges. In particular, SAFE demonstrates a unified framework thateffectively inherits the generalizability of PTMs using slow parameter-efficient tuning (S-PET) andprovides sufficient plasticity to learn task-specific knowledge in each incremental session using thefast one (F-PET). Meanwhile, SAFE does not require storing class distributions for data replay andonly incurs constant-level additional computation and memory costs. To achieve the above goals, SAFE employs distinct strategies for the first and subsequent sessions.In the first session, we focus on explicitly transferring general knowledge from pre-trained models(PTMs) by introducing a knowledge transfer loss. This involves computing a correlation matrixbetween feature embeddings from the PTM and the model with parameter-efficient tuning (PET).The diagonal elements of this matrix are maximized to ensure that the features remain consistentacross both models, effectively aligning the PET-applied models performance with that of thePTM. Simultaneously, minimizing the off-diagonal elements reduces redundancy in the embeddings,enhancing feature discriminability. After this tuning process, parameters can retain generalizableknowledge from the PTM. To prevent forgetting this knowledge, these trained parameters aresubsequently frozen, with only the classification weights being updated, thus designating this modelas the slow learner. In the incremental sessions, to address the plasticity limitations of the slow learner, we introducea fast learner capable of continuously integrating new concepts. Given the persistent challenge ofcatastrophic forgetting in continual learning, the slow learner guides the training of the fast learner.Concretely, we employ a feature alignment loss to minimize the distance between the embeddingsof both learners on a hypersphere. Additionally, a cross-classification loss is proposed to ensurecompatibility between the features of the fast learner and the classification weights of the slow learner,and vice versa. This approach allows the fast learner to assimilate new knowledge without storingexemplars or distributions, while also mitigating forgetting. For robust predictions, an entropy-basedaggregation strategy is implemented during inference to dynamically leverage the complementarystrengths of the slow and fast learners.",
  "To summarize, the contributions of our paper are three-fold:": "To inherit the generalizable knowledge in PTMs that has been overlooked in existing continuallearning works, we propose to explicitly transfer knowledge from the PTM to a slow learner. Oncetrained, the slow learner can generalize well to classes in incremental sessions. For improving the plasticity of CL models, we include a fast learner with guidance from the slowlearner to continuously incorporate novel concepts. Moreover, by aggregating both slow and fastlearners into a unified framework SAFE, robust predictions can be further made. The superiority of SAFE is validated on seven continual learning datasets where our methodconsistently achieves remarkable state-of-the-art performance. For example, our method surpassesthe second-best result on ImageNet-A over 4%.",
  "Related Work": "Continual Learning. Traditional continual learning (CL) aims at continuously updating models withdata streams from scratch. Existing strategies involve regularization-based approaches which prevent forgetting by regularizing network weights or predictions, rehearsal-based approacheswhich replay historical data stored in a fixed-sized buffer , and architecture-basedapproaches which dynamically expand models for novel classes. Among thesemethods, a recent attempt to preserve knowledge based on slow and fast complementary theoryhas been proposed . Nevertheless, these approaches typically require adjusting all modelparameters, which increases the computational burden of the learning process. Contrarily, ourSlow And Fast parameter-Efficient tuning (SAFE) framework only requires much fewer learnableparameters as well as fewer resources, while obtaining more favorable performance. Continual Learning with Pre-Trained Models. With the emergence of powerful pre-trainedmodels (PTMs), it has become a hot topic to integrate pre-trained models with CL for betterperformance. Prompt-based methods utilize prompt tuning to adapt PTMs to newtasks. However, these methods are tailored for Transformers and require an expanding promptpool with the arrival of new data. First session adaptation methods adapt PTMs solely in thefirst session and then freeze the model afterward to suppress forgetting . Nevertheless, theseworks lack plasticity for classes in subsequent sessions. Contrarily, another line of works focuseson continual adjustment to accommodate evolving information. However, the aboveapproaches either require storing data distributions for replay, only obtain inferior onlinebranch performance , or linearly increase complexity with incremental sessions . Compared toexisting works, our method provides a flexible framework that boosts generalizability by inheritingPTMs knowledge in the first session and maintains plasticity for incremental classes with constantcomplexity in a replay-free manner.",
  "Problem Definition": "Following previous works , in this paper, we mainly consider PTM-based CL under aclass-incremental learning setting. Formally, the model is trained sequentially on a series of incre-mental sessions, where Dt = {(xti, yti)}Nti=1 {X t, Yt} represents the t-th training set composedof Nt samples, for t {1, 2, . . . , T}. The sample and label space of Dt are denoted by X t and Yt,where Yt is disjoint between different sessions, i.e., i, j and i = j, Yi Yj = . We follow the",
  "Logits": ": An overview of our SAFE framework. In the first session, PTM transfers knowledge to theslow learner for better generalization. In sessions t > 1, the fast learner is guided by the slow learnerfor enhanced plasticity. During inference, robust predictions are made by dynamic aggregation. replay-free setting, where only Dt is accessible in session t. After training in the t-th session, themodel is evaluated on all the seen classes so far: Y1:t = Y1 Y2 Yt. In addition, we alsovalidate our method on domain-incremental learning setting, where the data distribution betweensessions shifts significantly, i.e., i, j and i = j, P(X i) = P(X j), Yi = Yj.",
  "Overall Architecture": "For tackling the stability-plasticity dilemma in CL, we draw inspiration from the complementary learn-ing systems theory to develop a Slow And Fast parameter-Efficient tuning (SAFE) framework,as depicted in . In the first session, the slow learner is tuned to inherit the general knowledgefrom PTM and is frozen afterward. In the following sessions, the slow learner only updated its classi-fication head using imprinted weights , which acts like the neocortex to slowly incorporate novelknowledge without forgetting. Complementary to this, the fast learner with learnable parametersrapidly encodes novel information as the hippocampus for adapting to new classes. Formally, features extracted from PTM, slow learner and fast learner are denoted as fl = l(x) Rd,where l {PTM, slow, fast} and d is the feature dimension. To leverage the knowledge of PTMswith few learnable parameters and resources, feature extractors for the slow and fast learners aretrained using parameter-efficient tuning (PET) which are referred to as S-PET and F-PET,respectively. Consistent with prior works , we mainly consider three types of PETs:Adapter , SSF , and VPT , shown in the right part of . The classification weights in session t for the slow and fast learners are symbolized by Wl Rd|Y1:t|,l {slow, fast}, where |Y1:t| is the number of classes seen so far from session 1 to session t. For theslow learner, Wslow is learned in the first session and expanded using feature centroids of trainingsamples within the same classes afterward to preserve learned general knowledge. Contrarily,Wfast is trainable as CL progresses for the plasticity purpose.",
  "Slow Learner": "Benefiting from pre-training on large-scale resources, pre-trained models (PTMs) inherently possessstrong generalizability for downstream tasks. Previous works typically view the PTM asa preferable starting point for continual learning. To bridge the distribution gap between pre-trainingdatasets and downstream datasets, these methods often directly apply PET to PTMs. However, without proper transfer mechanisms, models directly tuned on downstream data cannoteffectively inherit the general knowledge from PTMs. More seriously, the intrinsic knowledge inPTM may be overwritten during adaptation to the recent dataset, since it often contains relativelylimited samples. To solve the above issues, we propose to effectively squeeze out information fromPTMs and explicitly transfer it to adapted models.",
  "k=1[PTM(xk)]i [slow(xk)]j,(1)": "where Nb is the batch size, d is the feature dimension and denotes element-wise multiplication1.Moreover, i and j index the dimensions of the features and matrices. In fact, the correlation matrixcharacterizes the relationship between feature embeddings of PTM and the slow learner. The i-th rowand j-th column of M measures the correlation between the i-th feature dimension (also termed aschannel or pattern in the literature) of the PTM and the j-th feature dimension of the slow learner. To encourage the PET-applied model to mimic the performance of the PTM, we maximize theelements in the diagonal. This maximizing term ensures the slow learner can learn invariant featurecomponents that match the statistics of the PTM:",
  "where denotes matrix multiplication, the overall loss function during the first training session isdefined as:Lslow = Lcls + diag Ldiag + rdn Lrdn.(5)": "In Eq. (5), diag and rdn are the balancing hyper-parameters. Intuitively, the joint optimization ofthree losses makes the adapted model simultaneously acquire distribution-specific knowledge basedon Lcls and inherit general knowledge of the PTM using Ldiag and Lrdn. As a result, the slow modelcan better generalize to incoming classes even unseen in the first training session.",
  "Fast Learner": "Although solely using the slow learner with general features already obtains competitive performance,the plasticity of the model is hindered due to its frozen parameters in the following sessions. Tostrike a balance between stability and plasticity, we adopt the fast learner to continuously learnepisodic information for novel classes. However, updating representations without data reply willlead to semantic drift , causing catastrophic forgetting of previously learned knowledge.Existing works to address this problem either store additional data distributions or requiresophisticated drift estimations after each session . Compared to previous works, our methodimposes no such constraints, and aligns the models before and after updates in a single embeddingspace, essentially addressing semantic drift. 1Following , in this paper, we use l2 normalization to map features and classification weights onto ahypersphere before element-wise or matrix multiplication. Normalization is omitted to simplify notation. First, the fast learner is trained with guidance from the slow learner using feature alignment topreserve prior representations. Specifically, the distance of feature embedding from both models isminimized on a hypersphere to alleviate forgetting:",
  "where Nb is the batch size and cos denotes cosine similarity of two vectors": "Furthermore, we utilize cross-classification which contains a fast-to-slow loss and a slow-to-fast lossto maintain previous decision boundaries. For fast-to-slow calibration, we feed features from thefast learner to the classification layer of the slow learner. This objective makes features from the fastmodel compatible with the decision boundaries of the slow one to suppress semantic drift. Moreover,since the classification weight vector of each class can be viewed as a prototype of that class ,we also use these vectors as inputs for further preserving knowledge from previous sessions:",
  "j=1CE(W slow W (j)fast, j),(7)": "where W (j)fast Rd denotes the j-th column of Wfast, which is also the prototype for class j in thefast learner. Similarly, slow-to-fast loss Ls2f can be derived by swapping the fast and slow terms inEq. (7). After that, the cross-classification loss can be defined as Lsf = Lf2s + Ls2f.",
  "Model Inference": "Since the slow learner inherits general knowledge and the fast learner contains task-adaptive knowl-edge, we can obtain robust predictions by utilizing the complementarity of them. We first introducethe inference using a single learner and then provide aggregation strategy based on both learners. Single-learner-based Inference. Following previous work , instead of directly using theclassification weights Wl and features l(x), l {slow, fast} for prediction, we take advantageof second-order statistics and prototype information for better performance. Formally, given a testsample x, the predicted logits of each learner zl are calculated as:",
  "i=1hl(xsi) hl(xsi), hl(xsi) = (W rand l(xsi)).(10)": "In Eq. (10), Wrand RdM is the projection matrix with each column sampled from N(0, 2I), is a nonlinear activation function and I denotes the identity matrix. Mathematically, Eq. (9) defines amore general form of regular linear prediction. When Wrand is I and is not applied, it degrades toa ridge regression . Moreover, if G is removed, the classifier further reduces to NCM . Aggregation-based Inference. As discussed in the above sessions, slow and fast learners excel inhandling classes from different sessions. Due to its plasticity, the fast learner can better recognizecategories from the latest several sessions but shows limited performance on the old ones caused bypotential forgetting. Contrarily, despite limited novel concept adaptation, the slow learner can capturehistorical knowledge thanks to its stability. Intuitively, when dealing with proficient categories, the",
  "Given a test sample, we compute the entropy of predictions using H =": "i pi log pi for eachlearner, obtaining Hslow and Hfast, where p = softmax(z) is predicted probability. As lowerentropy indicates less uncertainty in predictions, the confidence of each learner can be representedby [slow, fast] = softmax([ Hslow, Hfast]), where is a scalar to control the peakiness ofoutput distributions. After that, the aggregated logits zaggregate automatically assign higher weightsto predictions with higher confidence, and can be obtained using a convex combination:",
  "Experimental Setups": "Datasets and Evaluation. Following previous methods , our evaluations are conductedon seven benchmark datasets: CIFAR100 , ImageNet-R (IN-R) , ImageNet-A (IN-A) ,CUB200 , Omnibenchmark (OB) , VTAB and DomainNet . Previous state-of-the-art PTM-based CL methods are chosen for comparison, including L2P , DualPrompt ,CODAPrompt , ADaM , RanPAC , SSIAT , and SLCA . We adopt final accuracyAccT and average accuracy Accavg = 1",
  "TTt=1 Acct as evaluation metrics": "Implementation Details. Consistent to existing works , we adopt ViT-B/16-IN1K and ViT-B/16-IN21K as the PTM and apply Adapter , SSF or VPT for parameter-efficient tuning(Appendix A). In each session, we train the model for 20 epochs using SGD optimizer, weightdecay of 0.0005, momentum of 0.9, and a cosine annealing schedule where learning rate starts from0.01 and decays to 0. The batch size is set to 48. In addition, in Eq. (9) is selected based on theperformance on the training data similar to . For other hyper-parameters used in our method,we find diag = 0.1, rdn = 100, cos = 50, = 1 is a reasonable set of default choices. Detailedhyper-parameter sensitivity analyses are provided in Appendix D.",
  "Slow learner67.04Fast learner67.49SAFE (ours)67.82": "The class-incremental learning results from the final sessionare reported in . As shown in , our methodconsistently achieves the best performance among all bench-marks. Notably, we significantly surpass the second-bestresult on ImageNet-A by 4.4%. When compared to methodsstoring additional data distributions for replay , ourmethod is replay-free and can still outperform these methodsby a significant margin. In addition, we improve the averageaccuracy over six datasets by 2.1% compared to the previ-ous best approach . The aforementioned superiority cancontribute to the generalizability and plasticity of our methodwithin a unified framework.",
  "Ablation Study": "To investigate the factors contributing to the success of SAFE, we validate the effectiveness of ourkey components: the slow learner (SL) in .3, the fast learner (FL) in .4, and theentropy-based aggregation in .5. Experiments are primarily conducted on IN-A dataset. Effectiveness of the Slow Learner. We assess the effectiveness of the slow learner from threeperspectives. Firstly, as depicted in , when the slow learner is added to the baseline ,the final accuracy increases by 3.2% and the average accuracy increases by 2.1%. This observationverifies that the slow learner can generalize well to the incremental classes.",
  ": Comparisons with T-SNE visualization": "Secondly, we expect the slow learner to inheritgeneralizability from the PTM. To dive deeperinto this aspect, we visualize the embeddingsof five unseen classes and five seen classes byT-SNE after the first session adaptation.As shown in , the embedding space ofthe slow learner exhibits distinct separation be-tween the seen and unseen classes. Note thatthe feature distributions with SL in the grey el-lipse become more separable compared with thebaseline method. This illustrates the successfulintegration of generalization capabilities fromthe PTM into the slow learner. Furthermore, we explore other alternatives for transferring generalizability, including feature align-ment (FA) by distilling PTMs features, logits alignment (LA) by distilling PTMs predictions, andsecond-order statistics alignment (SSA) by distilling PTMs covariance. presents the averageand final accuracy of the substitutions on IN-A, with the best results highlighted in bold. It is observedthat our slow learner can consistently outperform these variations, validating its superiority.",
  "that the fast learner is properly guided by the slow learner, and thus can continuously adapt to novelclasses with suppressed forgetting": "Subsequently, we present the necessity of each regularization term in the fast learner. As shownin , without Lsf and Lcos, the performance drops to lower than 10% due to catastrophicforgetting. To alleviate forgetting, both Lsf and Lcos are applied. Specifically, solely using Lsfresults in an improvement of 3.1% compared to the baseline, while using only Lcos yields a gain of3.9% over the baseline. Moreover, with all the proposed loss functions, the fast learner can obtain thebest performance, validating the effectiveness of each regularization term. 00-19 20-39 40-59 60-79 80-99 100-119 120-139 140-159 160-179 180-199",
  ": Aggregation weights for theslow learner and fast learner on IN-R": "Effectiveness of Aggregation. As shown in the last rowof , the combination of the slow and fast learnerspresents the best result. This observation is consistentwith the complementary learning systems theory thatmemory necessitates the presence of both a slow learnerand a fast learner for improved performance. To gain deeper insights into the necessity of both learners,we elaborate on their final accuracy of classes from eachsession. In , the slow learner, mimicking the neocor-tex, initially stores structured information and performswell on relatively old classes (0-119). Conversely, thefast learner, resembling the hippocampus, swiftly adaptsnovel concepts and excels in more recent classes (120-199).From this perspective, combining these two complemen-tary learners leverages their strengths across the trainingprocess, resulting in superior model performance. In addition, illustrates how the aggregated model dynamically leverages the strengths ofboth learners. Concretely, the horizontal axis represents the class indices to which each test samplebelongs, while the vertical axis shows the average aggregation weights of each learner assigned tothese test samples. It is observed from that, for classes 120-199, the fast learner consistentlyshows higher weights, which is consistent with its superior classification accuracy in these classes as depicted in . For classes 0-119, the slow learner obtains higher weights, generally aligning withits demonstrated stability and better performance on these classes shown in . By adaptivelybalancing the contributions of both learners, our method achieves a harmonious trade-off betweenstability and adaptability. Moreover, we undertake detailed comparisons to other merging strategies to validate the effectivenessof our aggregation choice. As depicted in , we compare our entropy-based aggregation withthree alternatives: feature concatenation, logits addition, and logits max. We report the final andaverage accuracy, where the results elucidate that the entropy-based aggregation fully leverages bothlearners and achieves the best performance.",
  ": Memory usage comparison": "In this section, we investigate the number oflearnable parameters in different methods andreport the parameter-performance comparison.Since no exemplars are stored in our method,the primary storage cost is attributed to the train-able model parameters introduced by parameter-efficient tuning (PET). Although PET entailsadditional parameters, it is still small relative tothe overall size of the pre-trained model (PTM).Moreover, as the parameter-performance trade-off shown in , our method SAFE utilizesa similar scale of parameters as existing PTM-based methods while achieving substantial per-formance improvements.",
  "Conclusion": "In this paper, we introduced SAFE, a Slow And Fast parameter-Efficient tuning framework forcontinual learning. Our approach leverages the inherent knowledge in pre-trained models (PTMs)while maintaining model plasticity for novel concepts. By incorporating a transfer loss function, weensure the preservation of general knowledge from PTMs. In the first session, we calibrate slowefficient tuning parameters to enhance the models ability to generalize to new classes. To balancestability and plasticity, we fix the slow efficient tuning parameters and continuously update the fastones, employing a cross-classification loss with feature alignment to prevent catastrophic forgetting.During inference, we introduce an entropy-based aggregation strategy for dynamic utilization ofthe complementarity between the slow learner and the fast learner. Extensive experiments on sevenbenchmark datasets demonstrate that our method significantly surpasses the state-of-the-art, validatingthe effectiveness of our approach. Limitations: Our approach is built upon RanPAC , and as such, it shares some of the samelimitations. For instance, our method relies on a strong feature extractor to effectively inheritgeneralizability from PTMs, making it less suitable for scenarios where training needs to be per-formed from scratch or starting from rather small tasks. Additionally, our method introduces threehyper-parameters to balance the loss functions during training, as previously discussed. Whileour experiments demonstrate that a set of default values works well across the benchmark datasetsevaluated in our work, we acknowledge that these choices might not be optimal when applied todatasets with essentially different statistical characteristics. Furthermore, slowly updating the slowlearner periodically, rather than keeping it fixed in subsequent sessions, may further enhance themodels adaptability and could be a promising direction for future research.",
  "Acknowledgement": "This project was funded by National Natural Science Foundation of China (62406192). The projectand raw idea were initiated at MIFA Lab of Shanghai Jiao Tong University, while the major part ofthe work was completed at Tencent Youtu Lab. Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, andBabak Ehteshami Bejnordi. Conditional channel gated networks for task-aware continuallearning. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 39313940, 2020. Kyra Ahrens, Hans Hergen Lehmann, Jae Hee Lee, and Stefan Wermter. Read betweenthe layers: Leveraging intra-layer representations for rehearsal-free continual learning withpre-trained models. Transactions on Machine Learning Research, 2024. Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general con-tinual learning method based on complementary learning system. In International Conferenceon Learning Representations, 2021. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Darkexperience for general continual learning: a strong, simple baseline. Advances in neuralinformation processing systems, 33:1592015930, 2020.",
  "Francisco M Castro, Manuel J Marn-Jimnez, Nicols Guil, Cordelia Schmid, and KarteekAlahari. End-to-end incremental learning. In European Conference on Computer Vision, pages233248, 2018": "Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo.Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in NeuralInformation Processing Systems, 35:1666416678, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for imagerecognition at scale. In International Conference on Learning Representations, 2021. Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang.A unified continual learning framework with general parameter-efficient tuning. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 1148311493, 2023. Jinwei Han, Zhiwen Lin, Zhongyisun Sun, Yingguo Gao, Ke Yan, Shouhong Ding, Yuan Gao,and Gui-Song Xia. Anchor-based robust finetuning of vision-language models. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2691926928,2024.",
  "Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficientfine-tuning for large models: A comprehensive survey, 2024": "Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and JustinGilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization.In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages83408349, October 2021. Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Naturaladversarial examples. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1526215271, 2021.",
  "Arthur E Hoerl and Robert W Kennard. Ridge regression: Biased estimation for nonorthogonalproblems. Technometrics, 12(1):5567, 1970": "Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unifiedclassifier incrementally via rebalancing. In Proceedings of the IEEE conference on computervision and pattern recognition, pages 831839, 2019. Paul Janson, Wenxuan Zhang, Rahaf Aljundi, and Mohamed Elhoseiny. A simple baseline thatquestions the use of pretrained-models in continual learning. In NeurIPS 2022 Workshop onDistribution Shifts: Connecting Methods and Applications, 2022.",
  "Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features:A new baseline for efficient model tuning. Advances in Neural Information Processing Systems,35:109123, 2022": "Yaoyao Liu, Bernt Schiele, and Qianru Sun.Adaptive aggregation networks for class-incremental learning. In Proceedings of the IEEE conference on computer vision and patternrecognition, pages 25442553, 2021. Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van denHengel. Ranpac: Random projections and pre-trained models for continual learning. Advancesin Neural Information Processing Systems, 36, 2023. Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-basedimage classification: Generalizing to new classes at near-zero cost. IEEE transactions onpattern analysis and machine intelligence, 35(11):26242637, 2013. Oleksiy Ostapenko, Mihai Puscas, Tassilo Klein, Patrick Jahnichen, and Moin Nabi. Learningto remember: A synaptic plasticity driven framework for continual learning. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 1132111329,2019. Aristeidis Panos, Yuriko Kobe, Daniel Olmeda Reino, Rahaf Aljundi, and Richard E Turner. Firstsession adaptation: A strong replay-free baseline for class-incremental learning. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pages 1882018830, 2023. Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Momentmatching for multi-source domain adaptation. In Proceedings of the IEEE/CVF internationalconference on computer vision, pages 14061415, 2019.",
  "Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophicforgetting in neural networks. In International Conference on Learning Representations, 2021": "Anastasia Razdaibiedina, Yuning Mao, Rui Hou, Madian Khabsa, Mike Lewis, and AmjadAlmahairi. Progressive prompts: Continual learning for language models. In The EleventhInternational Conference on Learning Representations, 2022. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:Incremental classifier and representation learning. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 20012010, 2017. Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip HS Torr, Song Bai,and Vincent YF Tan. Mimicking the oracle: An initial phase decorrelation approach for classincremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1672216731, 2022. James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, DonghyunKim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continualdecomposed attention-based prompting for rehearsal-free continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1190911919,2023.",
  "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. Thecaltech-ucsd birds-200-2011 dataset. Technical report, 2011": "Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchi-cal decomposition of prompt-based continual learning: Rethinking obscured sub-optimality.Advances in Neural Information Processing Systems, 36, 2023. Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning with pre-trained trans-formers: An occams razor for domain incremental learning. Advances in Neural InformationProcessing Systems, 35:56825695, 2022. Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, XiaoqiRen, Guolong Su, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting forrehearsal-free continual learning. In European Conference on Computer Vision, pages 631648.Springer, 2022. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages139149, 2022.",
  "Ju Xu and Zhanxing Zhu. Reinforced continual learning. Advances in neural informationprocessing systems, 31, 2018": "Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning withdynamically expandable networks. In 6th International Conference on Learning Representations,ICLR 2018. International Conference on Learning Representations, ICLR, 2018. Sung Whan Yoon, Do-Yeon Kim, Jun Seo, and Jaekyun Moon. Xtarnet: Learning to extracttask-adaptive representation for incremental few-shot learning. In International conference onmachine learning, 2020. Lu Yu, Bartlomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, ShanglingJui, and Joost van de Weijer. Semantic drift compensation for class-incremental learning. InProceedings of the IEEE conference on computer vision and pattern recognition, pages 69826991, 2020.",
  "Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synapticintelligence. In International conference on machine learning, pages 39873995, 2017": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, MarioLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. Alarge-scale study of representation learning with the visual task adaptation benchmark. arXivpreprint arXiv:1910.04867, 2019. Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, and Yunchao Wei. Slca: Slowlearner with classifier alignment for continual learning on a pre-trained model. In Proceedingsof the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1914819158,October 2023. Yuanhan Zhang, Zhenfei Yin, Jing Shao, and Ziwei Liu. Benchmarking omni-vision represen-tation through the lens of visual realms. In European Conference on Computer Vision, pages594611. Springer, 2022. Linglan Zhao, Jing Lu, Yunlu Xu, Zhanzhan Cheng, Dashan Guo, Yi Niu, and XiangzhongFang. Few-shot class-incremental learning via class-aware bilateral distillation. In Proceedingsof the IEEE/CVF conference on computer vision and pattern recognition, pages 1183811847,2023. Da-Wei Zhou, Zi-Wen Cai, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu. Revisiting class-incremental learning with pre-trained models: Generalizability and adaptivity are all you need.International Journal of Computer Vision, pages 121, 2024. Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, and De-Chuan Zhan. Continual learningwith pre-trained models: A survey. In Proceedings of the 33rd International Joint Conferenceon Artificial Intelligence (IJCAI), pages 83638371, 2024. Da-Wei Zhou, Hai-Long Sun, Han-Jia Ye, and De-Chuan Zhan. Expandable subspace ensemblefor pre-trained model-based class-incremental learning. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, 2024. Didi Zhu, Zhongyi Sun, Zexi Li, Tao Shen, Ke Yan, Shouhong Ding, Kun Kuang, and ChaoWu. Model tailor: Mitigating catastrophic forgetting in multi-modal large language models. InInternational conference on machine learning, 2024.",
  "AParameter-Efficient Tuning (PET)": "Trained on vast amounts of data, models with billions of parameters exhibit remarkable performanceacross various tasks. However, the expansive scale and computation pose considerable challengeswhen customizing them for downstream deployment. From this perspective, parameter-efficienttuning (PET) provides a practical solution by effectively adapting the large pre-trained models withlimited additional parameters. Parameter-efficient tuning selectively adjusts a small proportion of the model parameters whilekeeping the rest frozen. In this way, pre-trained models (PTMs) can partially keep the generalizationand deal with domain gaps at a low resource cost . Motivated by this, PTM-based continuallearning models leverage PET in their paradigm to achieve desirable results . Typically,continual learning works leverage ViT-B/16-IN1K and ViT-B/16-IN21K as the backbones and fine-tune the model mainly with three PET algorithms: Adapter , Scale & Shift (SSF) and VisualPrompt Tuning (VPT) , which are introduced in the following: Adapter: Adapters are small additional layers inserted into the layers of a PTM. Each adapterlayer generally consists of three parts: a down-projection layer Wdown Rdr which reduces theinput feature dimension, a non-linear activation function (e.g., ReLU), and an up-projection layerWup Rrd which projects features back to the original dimension. Specifically, given an inputx RLd, the output y RLd is expressed as:",
  "where L, d and r represent the length of the input feature sequence, original feature dimension andprojected feature dimension. In the above equation, denotes matrix multiplication": "Scale & Shift (SSF): SSF involves two main operations: scaling, which multiplies each feature by alearnable vector to adjust its spread, and shifting, which adds a trainable vector to each feature tochange its central position. In the context of fine-tuning PTMs, SSF helps to normalize the featuredistributions and adjust to new data. This improves performance and robustness by maintainingconsistency in distribution. Specifically,",
  "BEffects of PET to SAFE": "In the main paper, we report the remarkable performance of the proposed SAFE framework under thesame PET setting as . In this section, we demonstrate that the proposed approach is a generalframework that is compatible with diverse PET modules. Specifically, we combine SAFE withAdapter , Scale & Shift(SSF) and Visual Prompt Tuning (VPT) . As depicted in ,we report the final accuracy on six datasets compared with the baseline method . As shown in , the proposed SAFE framework outperforms the baseline across various PETmodules by a substantial margin. It is worth noting that the proposed method consistently exceedsthe baseline on ImageNet-A by over 4% with different PET modules. We also achieve performanceimprovements by 2.2% with Adapter, 3.1% with SSF, and 3.0% with VPT on ImageNet-R. Theseresults demonstrate the general applicability of our framework across PET algorithms. : Performances of SAFE and our baseline PanPAC with three different parameter-efficienttuning (PET) modules on six datasets. The rows in shadow show improvements compared to thebaseline. The best results are in bold.",
  "For the detailed training procedure of the slow learner in .3 and the fast learner in .4,we summarize the pseudo-code of our method SAFE training in Algorithm 1": "Algorithm 1 Model Training in Incremental Session tInput: Model from session t 1, training data Dt from session t.Output: Updated model in session t.1: Phase 1: Slow learner in session t = 1.2: Freeze pre-trained model parameters PTM.3: Randomly initialize classification weights Wslow and efficient tuning parameters S-PET.4: while not done do5:{(x, y)} sample a batch of data from D1.",
  ":Update {Wslow, S-PET} with gradients Lslow": "9: end while10: Replace Wslow with imprinted weights (i.e., feature centroids of each class in D1).11: Freeze parameters {Wslow, S-PET}.12:13: Phase 2: Fast Learner in session t > 1.14: Expand Wslow(Rd|Y1:t1| Rd|Y1:t|) with imprinted weights using slow and Dt.15: Expand Wfast(Rd|Y1:t1| Rd|Y1:t|) with imprinted weights using fast and Dt.16: Initialize the fast learners efficient tuning parameters F-PET from session t 1.17: while not done do18:{(x, y)} sample a batch of data from Dt.",
  "DFurther Ablations": "Hyper-Parameters Sensitivity. Our framework SAFE includes 4 hyper-parameters: diag and rdnfor the slow learner, cos for the fast learner, and for aggregation. In this section, we supply detailedhyper-parameter sensitivity analyses on ImageNet-A. Results for diag and rdn are depicted in, while the results for cos are shown in . Moreover, presents the experiment on. It is observed that hyper-parameters remain relatively stable within a certain range. For example,the slow learner can achieve satisfactory results with diag in the range from 0.1 to 1, and rdn in therange from 100 to 500. The fast learner can obtain good performance with cos in the interval from 50 to 100. Moreover, the aggregation module works well by simply setting to 1. As a result, we setdiag = 0.1, rdn = 100, cos = 50, = 1 as the default choices of hyper-parameters as stated in.1 of our main paper.",
  "SAFE65.9066.3666.5666.5066.2466.03": "Teacher Models for the Fast Learner. As discussed in .4, the fast learner is guided by theslow learner during adapting to novel classes. In this section, we provide additional experiments onthe choice of the teacher model which guides the training of the fast learner. We conduct comparisonson training the fast learner directly (None teacher), using the pre-trained model as a teacher (PTM)and using the fast learner from the last session as a teacher (t 1). As shown in , utilizingthe slow learner as a teacher model surpasses all the alternatives. This is because the slow learner canprovide generalizable knowledge to the fast learner and simultaneously alleviate forgetting.",
  "EComparisons with RanPAC": "While both the proposed SAFE and PanPAC leverage PTMs for continual learning, they targetdifferent components of the model. Specifically, RanPAC focuses on deriving decorrelated classifi-cation weights for the classification head with frozen features, whereas our method emphasizes theimprovement of trainable feature embeddings within the feature extractor. Furthermore, there is adistinct difference in the correlation matrices utilized by the two methods. The correlation coefficients",
  ": Visualization of seven benchmark datasets": "matrix in RanPAC, as shown in of their paper, has dimensions RCC, where C denotesthe number of classes in the classification head. In contrast, our method employs a cross-correlationmatrix of dimensions Rdd, with d representing the feature dimension, as detailed in Eq. (1). In addition, we would like to emphasize that our method is orthogonal to RanPAC. In fact, ourapproach is built upon RanPAC, and as evidenced in of our paper, our method consistentlyoutperforms RanPAC by a significant margin.",
  "FVisualization of Datasets": "In this section, we provide visualization results of the seven evaluated datasets: CIFAR100 ,ImageNet-R (IN-R) , ImageNet-A (IN-A) , CUB200 , Omnibenchmark (OB) ,VTAB and DomainNet . As shown in , SAFE can perform well on datasets withvarious characteristics. It is noteworthy that SAFE is capable of scenarios where the data distributionbetween tasks shifts significantly. For example, our method also shows superior performance onVTAB and DomainNet which comprise 5 and 6 distinct tasks, respectively."
}