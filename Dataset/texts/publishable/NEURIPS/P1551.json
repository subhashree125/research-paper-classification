{
  "Abstract": "We explore optimally training protein language models, an area of significantinterest in biological research where guidance on best practices is limited. Mostmodels are trained with extensive compute resources until performance gainsplateau, focusing primarily on increasing model sizes rather than optimizing theefficient compute frontier that balances performance and compute budgets. Ourinvestigation is grounded in a massive dataset consisting of 939 million proteinsequences. We trained over 300 models ranging from 3.5 million to 10.7 billionparameters on 5 to 200 billion unique tokens, to investigate the relations betweenmodel sizes, training token numbers, and objectives. First, we observed the effect ofdiminishing returns for the Causal Language Model (CLM) and that of overfittingfor the Masked Language Model (MLM) when repeating the commonly usedUniref database. To address this, we included metagenomic protein sequences inthe training set to increase the diversity and avoid the plateau or overfitting effects.Second, we obtained the scaling laws of CLM and MLM on Transformer, tailoredto the specific characteristics of protein sequence data. Third, we observe a transferscaling phenomenon from CLM to MLM, further demonstrating the effectivenessof transfer through scaling behaviors based on estimated Effectively TransferredTokens. Finally, to validate our scaling laws, we compare the large-scale versionsof ESM-2 and PROGEN2 on downstream tasks, encompassing evaluations ofprotein generation as well as structure- and function-related tasks, all within less orequivalent pre-training compute budgets.",
  "Introduction": "Scaling up transformer-based models has become a guiding principle for enhancing model perfor-mance across broad domains, particularly in Natural Language Processing (NLP) and Computer Vision (CV) . In recent years, large transformer-based Protein LanguageModels (PLMs) such as PROGEN familiy , ESM familiy and xTrimoPGLM have also been developed, which leads to significant improvements over model performance oncomplex downstream tasks . Current language models utilize two main training objectives toencode sequence information: the BERT-like Masked Language Model (MLM) and the GPT-likeCausal Language Model (CLM) , each applied either separately or in a unified fashion. Acommon understanding is that bi-directionally MLM excels in sample efficiency and shows enhancedperformance in downstream task fine-tuning. This is particularly true in tasks that emphasize under-standing complex patterns, making it a prevalent learning objective in modeling protein sequences* *XC and BC contributed equally.Work done while interned at BioMap.*Appendix D also compared CLM and MLM on the protein contact prediction task through fine-tuning andfreeze probing, with MLM demonstrating superior performance relative to CLM.",
  ". On the other hand, uni-directional CLM, due to its sequential generation ability, is bettersuited for generating more coherent and realistic sequences compared to MLM": "However, training large protein language models (PLMs) are computational-intensive, and strategiesfor optimally allocating compute budgets for training PLMs are relatively underexplored, with mostefforts focusing on scaling model parameters based on a fixed set of training tokens to achieveperformance improvements. A key insight is that large models should not be trainedto their lowest possible loss to optimize computing; instead, models and data should be scaledproportionally based on available compute budgets. These scaling laws are broadly found in naturallanguage models . But their applicability has not been validated withinbiological datasets, such as the primary structures of proteins, which are composed of amino acidsequences forming protein chains. Unlike natural languages, protein sequences are scientific data thatare precisely represented using a vocabulary of 20 amino acids, with very little redundancy and arenot as semantically smooth. Thus, we consider such data as a distinct modality and ask the question:What are the scaling behaviors for MLM and CLM in protein language modeling? We focus on the best practices, which include revisiting datasets, optimization objectives, and modelparameters as key factors. Our goal is to investigate an optimal training scheme for protein languagemodels given predetermined compute budgets. Our core findings are as follows: We revisited the protein sequence data used for training PLMs and collected a dataset of 194billion unique tokens on 939M unique sequences from publicly available sources to addressthe issue of overfitting and perform plateau in protein language modeling. We find that, in both MLM and CLM, training data scales sublinearly in the model sizesbut follow distinct power-laws. In other words, a 10 increase in compute leads to a 6increase in MLM model size and a 70% increase in data, versus a 4 increase in CLMmodel size and a 3 increase in training tokens. We also find that models trained with CLM can be transferred to MLM. When givena predetermined amount of computation, and one wants to obtain both a CLM and aMLM model, there is a trade-off in allocating the training token to each model to jointlyoptimize the performance of the two. Interestingly, the allocation for CLM pre-training wasdetermined by the scaling law of CLM and MLM, and the Effectively Transferred TokensDt from CLM to MLM. Furthermore, we verify this method experimentally using a 470Mmodel and fine-tuning on downstream tasks. Building on our scaling strategies, we re-allocate of model size and training tokens under thecompute budgets of established PROGEN2-xlarge and ESM-2 (3B) setups. Consequently,with the same compute budgets, we trained two corresponding models, one with 7.2Bparameters and the other with 10.7B parameters, which exhibited enhanced performance ina diverse range of downstream tasks.",
  "A Data-hungry Observation": "Using the UniParc database with 250 million protein sequences, research on ESM shows thatthe datasets UR50/S and UR50/D, with 45M and 65M unique sequences respectively, outperformUniref100 in perplexity (PPL) on a ~670M parameter MLM model. These datasets contain ~15B and~20B unique amino acid tokens. The ESM-2 family models, ranging from 150M to 15B parameters,are trained extensively with nearly 1 trillion tokens over 45 epochs on the UR50/D dataset. Inobserving the scaling of ESM-2 models, it becomes apparent that increasing model size to 15Bparameters from 3B shows marginal improvement. On the other hand, contemporary LLMs are oftentrained for only one or a few epochs . The repetition of data with limited uniquetokens has diminishing returns and hinders scaling model size . This underscoresthe importance of using rich datasets for training large-scale language models to ensure robustperformance across applications. We evaluated models with 150M and 3B parameters on the UR50/Sdataset, trained on 200B tokens, as shown in . We focus on the Independent and Identically 2.0 2.2 2.4 2.6 2.8",
  "Tokens": "(UR50/S Epochs) : Learning curves for UR50/S and UniMeta200B. Training loss and validation PPL, OODtest PPL, were tracked over 200 billion training tokens for both the 150M and 3B models. As wescaled the model from 150M to 3B, we observed diminishing returns on CLM (First line) and atendency to overfit on MLM (Second line) when repeating the Uniref50 (UR50/S) dataset. We totallyevaluate 3 repeating methods on MLM 3B models, all of which present overfitting (see Appendix B).Distributed (IID) validation and Out-Of-Distribution (OOD) test PPL, which measures the modelsrandomness in amino acid selection. For our OOD dataset, we utilized the MMseqs2 tool to conduct searches within the UniRef90 database for sequences post-training dataset timestamp,retaining those with no detectable identity. From these, a random sample of 3,000 sequences wasselected to constitute the OOD dataset. Notably, we do not adopt dropout regularization, a practicethat often reduces model capacity and is infrequently used in contemporary LLMs . This choiceis consistent with recent LLM configuration findings , including ESM-2 . The results show the 150M model lacks good generalization while increasing to a 3B model resultedin diminishing returns for CLM and severe overfitting for MLM. Principally, the bidirectional self-attention mechanisms used in MLM have a higher capacity to overfit compared to the unidirectionalself-attention used in CLM. This is because MLM can utilize the entire context surrounding a maskedtoken, leading to faster memorization of the training data.",
  "Uniref50/S54M15.2B8.5%Uniref90/50102M37.8B19.5%ColabFoldDBc208M37.7B19.5%ColabFoldDBm575M103B52.5%Total939M194B-": "To tackle the challenge of datascarcity, we leveraged the Colab-FoldDB database , which focuseson metagenomic data sources suchas BFD , MGnify , and spe-cific eukaryotic and viral datasets in-cluding SMAG , MetaEuk ,TOPAZ , MGV , and GPD .We applied a stringent deduplicationprocess with a maximum similaritythreshold of 0.3 to preserve the diver-sity of the protein universe. Given thatthe Uniref90 dataset has proven mosteffective for pre-training across various Uniref clustering levels per ESM-1v , we incorporatedUniref90/50 (Before 2022-12), which includes incremental data relative to Uniref50/S representatives.ColabFoldDBc and ColabFoldDBm play dominant roles within the dataset, corresponding to clusterrepresentatives and members, respectively. To ensure uniformity during training, we allocate weightswithin each batch to allow each amino acid token to be evenly processed through the model. Thisdataset, termed UniMeta200B, contains 939 million unique protein sequences and 194 billion aminoacids, which is an order of magnitude larger than UR50/D. We observed significant improvements in the OOD test set and a consistent learning curve on the IID validation subset extracted fromthe training set (). These enhancements not only ensure a controlled diversity to maintainsample efficiency but also significantly increase the quantity and uniformity of data, facilitating modelscaling. Findings 1. Scaling the model from 150M to 3B, we noted diminishing returns for CLM and anoverfitting tendency for MLM when repeating the UR50/S dataset. The proposed ExpandingDiversified Metagenomic Data (UniMeta200B) addresses these problems.",
  "Parameters and Datasize Optimal Allocation": "Model size 2.2 2.3 2.4 2.5 2.6 CLM Validation Loss 1e+18 FLOPs3e+18 FLOPs1e+19 FLOPs3e+19 FLOPs1e+20 FLOPs3e+20 FLOPs1e+21 FLOPs Model size 2.0 2.1 2.2 2.3 2.4 MLM Validation Loss 1e+18 FLOPs3e+18 FLOPs1e+19 FLOPs3e+19 FLOPs1e+20 FLOPs3e+20 FLOPs1e+21 FLOPs FLOPs Model size NMLMopt = 6.19 108 C0.776 NCLMopt = 1.26 103 C0.578 FLOPs Data size DMLMopt = 2.02 106 C0.230 DCLMopt = 1.24 102 C0.421 : IsoFLOPs curves and parametric fit for CLM and MLM. We selected training tokensto ensure a uniform final FLOP count for different model sizes. The lowest loss of each curverevealed an optimal model size for a FLOP budget (above). We use these rainbow points at the valleyto plot the efficient frontier for estimating the optimal model size and training tokens for scalingmodels (below). The interval range was estimated by model points with similar loss. In this section, we propose a scaling law for protein sequences with MLM and CLM objectives,aiming at optimally balancing model size and data size under a fixed compute budget to improveefficiency on expanded resources.",
  "MLM0.7760.2306.19 1082.02 106": "We first fit our models in the form of afundamental power-law based on theexisting work in the field of LLMs. Specifically,given a fixed FLOPs formula of C =6 N D, where N represents thenumber of forward-activated non-embedding parameters, and D is the number of training tokens,how should one navigate the trade-off between model size and the number of training tokens? The",
  "N(C) = A C,D(C) = B C(1)": "We employed the IsoFLOPs profiling approach , setting 7 distinct training FLOP counts rangingfrom 1 1018 to 1 1021. For each FLOP count, we selected models from a pool of candidates (seeAppendix N). Models were excluded if the estimated data size (C/(6 N)) resulted in more than200B tokens or if the training steps were fewer than 20K. Ultimately, approximately 260 modelswere used for fitting. We considered the final validation loss for each model to ensure that everymodel completed a full cosine cycle with 10 learning rate decay. For each fixed FLOP count, weemploy smoothed loss to determine the optimal model size with the smallest loss ( (above)).Subsequently, we use Equation 1 and apply the least_squares method to fit the model. Given the minimal variations in the final loss among a set of (N, D) configurations, we classify theseconfigurations as operating under \"IsoLoss\" conditions (see Appendix K Figure A15), consideredoptimal for training. In (below), we illustrate an efficient frontier interval that demonstratespermissible fluctuations in model size and dataset size at a specific FLOP count, while still achievingnearly identical losses. The variation in loss is quantified at 0.25 on a logarithmic scale with a baseof 10. This indicates that within this FLOP counts, the model size can be adjusted within a range,increasing up to 80% or decreasing up to 40% without repeating data, to maintain a loss variationwithin 0.01.We observe distinct growth rates in the proportional relationship between model size and trainingtokens for the MLM model compared to the CLM, as detailed in . Both models demonstratean increase in the growth of model size that surpasses the growth of training tokens. Up to theintersection point around 1 1022 (see , left below), the model size of MLM tends to besmaller than the CLM, thereafter, the MLM rapidly exceeds that of the CLM. Notably, the growth ofthe MLMs training tokens is greatly lower than that for the CLM, possibly due to MLMs highersample efficiency. For instance, if the compute budget is increased by 10, the size of the CLM modelshould increase by 4 and the training data by 3, aligning more closely with equally proportionalscaling. For the MLM, the model size should increase by 6 and the training data size by 1.7.",
  ": Compute allocation for twoobjectives with the same model size": "When our goal is to optimize both CLM and MLM simul-taneously, the strategic allocation of compute resourcesbetween these two objectives becomes essential. To facil-itate this, we equalize model parameters across objectives to assess specific compute budgets for dual-objective train-ing. Specifically, we seek the compute budgets, CMLM andCCLM, for configurations where the optimal model size isthe same, i.e., N(CMLM) = N(CCLM). These individualcomputations are then aggregated to formulate the overallcompute budget:",
  "N(Csum) 1.497 106 C0.703sumr(N) 8.449 104 N 0.392(5)": "The ratio r(N) informs us about the allocation proportion of training tokens. Specifically, underequal parameters, the data for MLM should exceed that for CLM until a 10B threshold (achieving a1:1) is reached, after which more training tokens are allocated to CLM. We further find that the scaling behavior of sparse parameter counts in a Mixture of Experts (MoE)model, set with eight experts (see Appendix I), as well as a combined power-law formula used to fitour data (see Appendix J), both exhibit a certain similarity to the scaling behavior we have proposed. Findings 2. In both CLM and MLM, training data scales sublinearly with model size, followingdistinct power laws. With an infinite dataset, where samples are not repeated and training forless one epoch, MLMs model size grows faster than CLMs.",
  "Transfer Scaling": "We have outlined two independent scaling laws and how to allocate FLOPs under a fixed budget fortraining two optimal models, one with MLM and the other with CLM. However, we have not exploredthe interaction between these objectives. This raises important questions: Can models trained withone objective transferred to one with another objective? Is there a synergistic effect from training twomodels? Does training order impact the results?",
  "Transferability": "We conduct transfer learning experiments on MLM and CLM objectives, selecting eight optimalmodel sizes based on Equation 1. These models correspond to four increasing FLOP counts from31019 to 11021 and undergo training from scratch followed by transfer training. Transfer traininginvolves initially training on MLM or CLM, then training on the alternate model for each size. We find that optimal pre-training on one objective benefits the target objective in transfer learning,though effects vary between methods. Starting with CLM and then training MLM, benefits increasewith model scale. In contrast, starting with MLM then training CLM sees diminishing benefits.As shown in (left), for a model size of 230M with 3 1019 FLOPs, MLM from CLMpre-training reduces the loss by 0.02 compared to MLM from scratch, however, benefit that nearszero for the 1.7B model. Conversely, for models from 85M to 1.2B, transfer benefits grow withmodel size, the compared validation loss gap increasing from 0.025 to 0.045. This likely stems fromthe higher loss utilization rate in MLM; CLM calculates losses for all tokens in a protein sequence,whereas MLM only calculates losses for 15% of the tokens. .",
  "CLM loss curve from scratch": ": Left: The upper graph compares validation loss of CLM trained from scratch with thosetransferred from MLM, showing diminishing transfer benefits as model size increases. The lowergraph depicts increased benefits for MLM from pre-trained CLM with larger sizes, indicating scale-dependent efficiency gains. Right: Shows loss curves for CLM and MLM across different FLOPs,emphasizing the efficient frontiers (or Pareto Frontier) from various transfer strategies. It highlightsthat the benefits of transferring from CLM to MLM grow with model size, reflecting a scale-dependentsynergy between training objectives.",
  "MLM10.1250.03411.1330.038CLM8.2510.0277.1910.024": "(right) shows that the efficient frontierfor L(Ct) has shifted relative to L(Cs) (it canbe directly obtained from , repeated herefor convenience.), indicating an improvement.The coefficients from both are shown in ,where we can infer that Ct C sts= C0.89s,suggesting that training MLM from scratch with 10 the compute requires approximately 7.7 thecompute compared to MLM from CLM pre-training. Another observation is that mixing trainingobjectives in a single batch tends to be detrimental. Detailed results and settings are in Appendix H.The recommended transfer learning schedule involves pre-training CLM before MLM, as mixedtraining and order swapping show no benefits. We speculate that this primarily occurs because ourMLM, which focuses solely on recovering corruption tokens, is not causal. If it predicted a middlesegment in a left-to-right manner, it could mutually adapt with the context to accelerate training .",
  "Effectively Transferred Tokens": "Although we observe that MLM benefits from transfer learning from CLM, the pre-training computebudget remains unaccounted for. We focus on two aspects: (1) the actual benefit CLM provides toMLM and its predictability, and (2) performance differences between MLM trained from pre-trainedCLM (MLM-CLM) and MLM from scratch (MLM-S) under identical FLOP constraints. We defineEffectively Transferred Tokens Dt as the additional data a model of the same size would need totrain from scratch on MLM to achieve the same loss as a model pre-trained on CLM. If the token % Compute of CLM Pre-training 8.0 8.5 9.0 9.5 Validation PPL | Optimal MLM Size (FLOPs) From scratchOptima interval 34M (1e19) 85M (3e19) 200M (1e20) 470M (3e20) Eff. Trans. Dt 0B12B50B59B",
  "Dt": "85M MLM from pre-trained CLM85M MLM from scratch : Left: Valid perplexity of % compute allocated for the CLM pre-training. For instance, %compute indicates first training on CLM and then the rest compute fine-tuning on MLM. The optimalCLM pre-training % compute range with . And the fitted Dt/(Dt + Df) drops in the optimalloss range. Right: Comparison of validation perplexity for models trained from scratch (red) andthose fine-tuned from a pre-trained CLM (green), demonstrating that fine-tuning from a CLM reducesperplexity with similar or even fewer tokens.",
  "number in the pre-trained CLM model exceeds Dt, then the computations for CLM pre-training wasexcessive. Knowing Dt in advance would guide the allocation of tokens for CLM pre-training": "We compare MLM-S and MLM-CLM models ranging from 33M to 1.2B with FLOP counts from3 1019 to 1 1021. By calculating the token distance at the same loss level between these models,we establish our fitting target Dt, collecting approximately 2800 sample points. Following similarmethods in scaling transfer works , Dt is defined by a simple multiplicative scaling formula:",
  "Df1N ;k 3.65 105, 0.137, 0.369(7)": "where Df represents the tokens used for MLM-CLM, and N is the number of parameters, with k, ,and as fitting coefficients. For instance, a 10 increase in Df would roughly triple the model sizeand double Dt. We validate these findings by evaluating the compute ratio of CLM pre-training underfour specified parameters and FLOPs, as shown in (left), finding that MLM-CLM generallyoutperforms MLM-S. Specifically, Dt/(Dt + Df) ranges from 10% to 20% of the compute budgetfor CLM pre-training. (right) schematically illustrates the learning curves of two 85M (3e19FLOPs) models, with MLM-CLM achieving similar or better loss levels with equal or fewer tokens. Findings 4. Training MLM from scratch with 10 the compute requires approximately 7.7 thecompute compared to MLM from CLM pre-training, implying that around 20% of the computebudget should be allocated for CLM pre-training to get better MLM models transferred fromCLM pre-training.",
  "Experimental Validation": "Based on the scaling laws we observe, we estimate the model size and training tokens for currentleading models by analyzing their FLOPs. In our configuration, the PROGEN2-xlarge model, with6.4B parameters, is estimated to require training with 7.2B parameters and 265B tokens. Similarly,the ESM-2 model, with 3B parameters, should be trained with a model size of 10.7B parametersand 260B tokens. Additionally, we employed two 470M models to test the transfer scaling strategy,one trained from scratch (470M scratch) and the other from CLM pre-training (470M trans.). Themodels details are reported in .",
  "SeqID": ": Comparative Analysis of CLM Models. A. Perplexity analysis for PROGEN2-xlargeand our 7.2B CLM shows lower values for our model across various MaxID levels, suggesting bettersequence handling. B. Box plots of pLDDT scores for protein structures by PROGEN2-xlarge andour 7.2B CLM. C. Contour and line plots show our 7.2B CLM sequences mimic natural sequencesmore closely than PROGEN2-xlarge, assessed using Foldseek with the PDB database. D. Clusteringat 50% sequence identity reveals our 7.2B CLM generates more clusters, indicating higher diversity. 40,000 sequences per model. Sequences with a perplexity greater than 10 and duplicates wereremoved, leaving 8,263 and 8,466 sequences for the 7.2B CLM and PROGEN-xlarge, respectively.We used four metrics to assess the quality of the models and the generated sequences (See Appendix Ffor details).OOD Dataset PPL Analysis We randomly sampled 5,000 sequences from UniProt released after2023-01-01 and aligned them to our and PROGEN2s training data (Uniref90 and BFD) usingHHblits or Jackhmmer . Sequences below a maximum identity cutoff were used to assessthe models PPL, as shown in A. Our 7.2B CLM exhibited lower PPL on three subsets.pLDDT scores from ESMFold Atomic structures of 8,263 and 8,466 generated sequences werepredicted using ESMFold, and compared based on pLDDT scores, displayed in B. The 7.2Bmodels average pLDDT score was 78.69, higher than PROGEN2-xlarges 74.33.Natural Sequences Comparisons with Foldseek Using Foldseek , we searched the PDBdatabase for sequences similar to those generated by our 7.2B CLM model, which showed bettermimicry of natural sequence properties with higher average TM-scores (0.655 vs 0.522) and SeqID(0.194 vs 0.165), as shown in C. Diversity Analysis Generated sequences were clustered using MMseqs2 with a 50% similaritycutoff. The 7.2B CLM model resulted in higher diversity with 7,097 clusters compared to 4,818clusters for PROGEN2-xlarge, detailed in D.",
  "Protein understanding tasks: 10.7B MLM vs. 3B ESM2": "We evaluate different task types from the protein benchmark : Contact prediction as binaryclassification at the amino acid pair level; fold classification into 1195 classes at the sequence level;and fluorescence as regression tasks. Following , we add a Multi-Layer Perceptron (MLP) headto each pre-trained model and apply Low-Rank Adaptation (LoRA) (r=8, =16) for fine-tuning(see Appendix G for convergence details). The results, shown in and A7, demonstrate that our 10.7B model outperforms ESM-3B on 7out of 8 tasks. This confirms the rationale behind the observed scaling law and addresses concernsabout the scope and rigor of our evaluation tasks. Additionally, the 470M model transferred from",
  "Discussion and Limitations": "Data Repeat Scaling Law. Our scaling law is learned within a single epoch setting. It is well knownthat MLM exhibits higher sample efficiency than CLM due to the dynamic masking strategies acrossepochs. However, this advantage diminishes when training is limited to only one epoch. We presentan empirical study by comparing a 2.8B model trained on 1T tokens (approximately five epochs)against a 10.7B model trained on 265B tokens (roughly 1.4 epochs). Despite the models utilizing thesame amount of FLOPs, the two models achieve similar capability in terms of OOD PPL (10.33 vs10.21). While the smaller models are more user-friendly during inference and fine-tuning. Therefore,we also suggest an alternative approach that adjusts the optimal training token count and model sizewithin the data repeat scaling law. Multi-modality Scaling. The multi-modal auto-regressive work suggests the existence of anearly universal scaling law across various modalities, including images, videos, math, code, andlanguages. Our results appear in this trend as well, such as, the scaling laws for CLM exhibitsimilarities to those in natural languages. The same situation may apply to other modalities ofbiological data, such as RNA and DNA . Various Pre-train Datasets and Strategies.Our datasets cover a substantial portion of theprotein universe, yet they might not be entirely representative. Combining BFD , Uniref ,MetaClust , and IMG/JGI with 90% clustering results in at least 600 billion unique tokens.However, variations in datasets could affect the power-law behavior. Future work could exploreapplying our findings to different model architectures. There is ongoing research on scaling LLMsfor long sequences , and MSA augmentation could significantly improveprotein representation regarding contacts and structure. Investigating scaling laws in this contextcould be a promising direction for future research.",
  "Conclusion": "In this work, we are the first to establish a practical pathway for researchers to develop faithful andpowerful protein language models optimized by both CLM and MLM objective in an end-to-endmanner. This includes everything from pre-training dataset construction, expanded metagenomicdatabases such as ColabFoldDB, emphasizing the critical importance of data quality and quantityfor scaling language models, to optimal parameter and dataset allocation along with the potentialloss prediction, as well as knowledge transfer from other pre-training objectives. Our work holdssignificant potential for the application of large language models across various scientific domains. Acknowledgments. This work has been supported by the National Key R&D Program of China2021ZD0113304, NSFC for Distinguished Young Scholar 62425601, New Cornerstone ScienceFoundation through the XPLORER PRIZE.",
  "generative mixed-modal language models. In International Conference on Machine Learning,pages 265279. PMLR, 2023": "Harriet Alexander, Sarah K Hu, Arianna I Krinos, Maria Pachiadaki, Benjamin J Tully, Christo-pher J Neely, and Taylor Reiter. Eukaryotic genomes from a global metagenomic datasetilluminate trophic modes and biogeography of ocean plankton. bioRxiv, pages 202107, 2021. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.arXiv preprint arXiv:2305.10403, 2023.",
  "Nadav Brandes, Dan Ofer, Yam Peleg, Nadav Rappoport, and Michal Linial. Proteinbert: auniversal deep-learning model of protein sequence and function. Bioinformatics, 38(8):21022110, 2022": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, MateuszLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.",
  "Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences withsparse transformers. arXiv preprint arXiv:1904.10509, 2019": "Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinkingattention with performers. arXiv preprint arXiv:2009.14794, 2020. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, JordanHoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, et al. Unifiedscaling laws for routed language models. In International conference on machine learning,pages 40574086. PMLR, 2022. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R. Flashattention: Fast andmemory-efficient exact attention with io-awareness. Advances in Neural Information ProcessingSystems, 35:1634416359, 2022. Justas Dauparas, Ivan Anishchenko, Nathaniel Bennett, Hua Bai, Robert J Ragotte, Lukas FMilles, Basile IM Wicky, Alexis Courbet, Rob J de Haas, Neville Bethel, et al. Robust deeplearningbased protein sequence design using proteinmpnn. Science, 378(6615):4956, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, JustinGilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.Scaling vision transformers to 22 billion parameters. In International Conference on MachineLearning, pages 74807512. PMLR, 2023. Grgoire Deltang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christo-pher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al.Language modeling is compression. arXiv preprint arXiv:2309.10668, 2023. Tom O Delmont, Morgan Gaia, Damien D Hinsinger, Paul Frmont, Chiara Vanni, AntonioFernandez-Guerra, A Murat Eren, Artem Kourlaiev, Leo dAgata, Quentin Clayssen, et al.Functional repertoire convergence of distantly related eukaryotic plankton lineages abundant inthe sunlit ocean. Cell Genomics, 2(5):100123, 2022.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding, 2018": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling oflanguage models with mixture-of-experts. In International Conference on Machine Learning,pages 55475569. PMLR, 2022. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.Glm: General language model pretraining with autoregressive blank infilling. arXiv preprintarXiv:2103.10360, 2021. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang.Glm: General language model pretraining with autoregressive blank infilling. In Proceedings ofthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 320335, 2022. Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy,Charlotte Rochereau, and Burkhard Rost. Ankh: Optimized protein language model unlocksgeneral-purpose modelling. arXiv preprint arXiv:2301.06568, 2023. Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Yu Wang, LlionJones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, et al. Prottrans: Towardunderstanding the language of life through self-supervised learning. IEEE transactions onpattern analysis and machine intelligence, 44(10):71127127, 2021. European Bioinformatics Institute. Jackhmmer tool. EBI Tools Documentation, n.d. fast.ai.How could the memorization hypothesis be true.fast.ai Blog, 2023.RetrievedMay 21, 2024, from #how-could-the-memorization-hypothesis-be-true.",
  "Noelia Ferruz, Steffen Schmidt, and Birte Hcker. Protgpt2 is a deep unsupervised languagemodel for protein design. Nature communications, 13(1):4348, 2022": "Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, MartinSteinegger, and Burkhard Rost. Prostt5: Bilingual language model for protein sequence andstructure. bioRxiv, pages 202307, 2023. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressivegenerative modeling. arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk,Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws andinterpretability of learning from repeated data. arXiv preprint arXiv:2205.10487, 2022.",
  "Hugging Face. Llama 2 model documentation, n.d": "Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, SamyamRajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling trainingof extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, et al.Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural languagemodels. arXiv preprint arXiv:2001.08361, 2020.",
  "Francesca-Zhoufan Li, Ava P Amini, Yisong Yue, Kevin K Yang, and Alex X Lu. Feature reuseand scaling: Understanding transfer learning with protein language models. bioRxiv, pages202402, 2024": "Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,Robert Verkuil, Ori Kabeli, Yaniv Shmueli, Allan dos Santos Costa, Maryam Fazel-Zarandi,Tom Sercu, Salvatore Candido, and Alexander Rives. Evolutionary-scale prediction of atomiclevel protein structure with a language model. bioRxiv, 2022. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-levelprotein structure with a language model. Science, 379(6637):11231130, 2023.",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprintarXiv:1711.05101, 2017": "Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael REguchi, Po-Ssu Huang, and Richard Socher. Progen: Language modeling for protein generation.arXiv preprint arXiv:2004.03497, 2020. Victor M Markowitz, Frank Korzeniewski, Krishna Palaniappan, Ernest Szeto, Greg Werner,Anu Padki, Xueling Zhao, Inna Dubchak, Philip Hugenholtz, Iain Anderson, et al. The integratedmicrobial genomes (img) system. Nucleic acids research, 34(suppl_1):D344D348, 2006.",
  "Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical modelof large-batch training. arXiv preprint arXiv:1812.06162, 2018": "Joshua Meier, Roshan Rao, Robert Verkuil, Jason Liu, Tom Sercu, and Alex Rives. Languagemodels enable zero-shot prediction of the effects of mutations on protein function. Advances inneural information processing systems, 34:2928729303, 2021. William Merrill, Vivek Ramanujan, Yoav Goldberg, Roy Schwartz, and Noah Smith. Effectsof parameter norm growth during transformer training: Inductive bias from gradient descent.arXiv preprint arXiv:2010.09697, 2020.",
  "Mgnify: the microbiome analysis resource in 2020. Nucleic acids research, 48(D1):D570D578,2020": "Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, AleksandraPiktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained languagemodels. Advances in Neural Information Processing Systems, 36, 2024. Stephen Nayfach, David Pez-Espino, Lee Call, Soo Jen Low, Hila Sberro, Natalia N Ivanova,Amy D Proal, Michael A Fischbach, Ami S Bhatt, Philip Hugenholtz, et al. Metagenomiccompendium of 189,680 dna viruses from the human gut microbiome. Nature microbiology,6(7):960970, 2021. Eric Nguyen, Michael Poli, Matthew G Durrant, Armin W Thomas, Brian Kang, JeremySullivan, Madelena Y Ng, Ashley Lewis, Aman Patel, Aaron Lou, et al. Sequence modelingand design from molecular to genome scale with evo. bioRxiv, pages 202402, 2024.",
  "Erik Nijkamp, Jeffrey A Ruffolo, Eli N Weinstein, Nikhil Naik, and Ali Madani. Progen2:exploring the boundaries of protein language models. Cell systems, 14(11):968978, 2023": "Pascal Notin, Aaron Kollasch, Daniel Ritter, Lood Van Niekerk, Steffanie Paul, Han Spinner,Nathan Rollins, Ada Shaw, Rose Orenbuch, Ruben Weitzman, et al. Proteingym: large-scale benchmarks for protein fitness prediction and design. Advances in Neural InformationProcessing Systems, 36, 2024. Jiezhong Qiu, Junde Xu, Jie Hu, Hanqun Cao, Liya Hou, Zijun Gao, Xinyi Zhou, Anni Li,Xiujuan Li, Bin Cui, et al. Instructplm: Aligning protein language models to follow proteinstructure instructions. bioRxiv, pages 202404, 2024. Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling languagemodels: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140):167, 2020.",
  "Michael Remmert, Andreas Biegert, Andreas Hauser, and Johannes Sding. Hhblits: lightning-fast iterative protein sequence searching by hmm-hmm alignment. Nature methods, 9(2):173175, 2012": "Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, AndrSusano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts.Advances in Neural Information Processing Systems, 34:85838595, 2021. Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo,Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Biological structure and function emerge fromscaling unsupervised learning to 250 million protein sequences. Proceedings of the NationalAcademy of Sciences, 118(15):e2016239118, 2021. Philippe A Robert, Rahmad Akbar, Robert Frank, Milena Pavlovic, Michael Widrich, IgorSnapkov, Andrei Slabodkin, Maria Chernigovskaya, Lonneke Scheffer, Eva Smorodina, et al.Unconstrained generation of synthetic antibodyantigen structures to guide machine learningmethodology for antibody specificity prediction. Nature Computational Science, 2(12):845865,2022. Elke Schaper, Andrey V Kajava, Alain Hauser, and Maria Anisimova. Repeat or not re-peat?statistical validation of tandem repeat prediction in genomic sequences. Nucleic acidsresearch, 40(20):1000510017, 2012. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Martin Steinegger and Johannes Sding. Mmseqs2 enables sensitive protein sequence searchingfor the analysis of massive data sets. Nature biotechnology, 35(11):10261028, 2017.",
  "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer:Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024": "Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B McGarvey, Cathy H Wu, and UniProtConsortium. Uniref clusters: a comprehensive and scalable alternative for improving sequencesimilarity searches. Bioinformatics, 31(6):926932, 2015. Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung,Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler. Scale efficiently: Insightsfrom pre-training and fine-tuning transformers. arXiv preprint arXiv:2109.10686, 2021. Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung WonChung, Siamak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2: Unifying language learningparadigms. arXiv preprint arXiv:2205.05131, 2022. Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q Tran, David R So, Siamak Shakeri, XavierGarcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scalinglaws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-the Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Openand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Michel van Kempen, Stephanie S Kim, Charlotte Tumescheit, Milot Mirdita, Cameron LMGilchrist, Johannes Sding, and Martin Steinegger. Foldseek: fast and accurate protein structuresearch. Biorxiv, pages 202202, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017. Robert Verkuil, Ori Kabeli, Yilun Du, Basile IM Wicky, Lukas F Milles, Justas Dauparas, DavidBaker, Sergey Ovchinnikov, Tom Sercu, and Alexander Rives. Language models generalizebeyond natural proteins. bioRxiv, pages 202212, 2022.",
  "Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2024": "Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy,Julien Launay, and Colin Raffel. What language model architecture and pretraining objectiveworks best for zero-shot generalization? In International Conference on Machine Learning,pages 2296422984. PMLR, 2022. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santi-ago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformersfor longer sequences. Advances in neural information processing systems, 33:1728317297,2020. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,Yifan Xu, Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXivpreprint arXiv:2210.02414, 2022. Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transform-ers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 1210412113, 2022.",
  "Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat. When scaling meets llm finetuning:The effect of data, model and finetuning method. arXiv preprint arXiv:2402.17193, 2024": "Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei Ye, and Quanquan Gu. Structure-informed language models are protein designers. In International Conference on MachineLearning, pages 4231742338. PMLR, 2023. Table A7: Tasks performance of MLM Model on the 5 test dataset, Fitness Prediction (Fit P.) asa regression task measured by Spearman coefficient at sequence level, Localization (Loc.) as 10sub-cellular classification task at sequence level, Metal Ion Binding (MIB) as a binary classificationtask at sequence level, Solubility (Sol.) as a binary classification task at sequence level, Stability (Sta.)as a regression task measured by Spearman coefficient at sequence level, with LoRA fine-tuning.",
  "ARelated Work": "Protein Language Model Since the advent of AlphaFold2 , the masked language model (MLM)has been integrated as a subtask within the Evoformer architecture. In this context, an assumptionis that large language models can be considered as a lossless compression method . This wasfollowed by a series of language modeling efforts , which aimed to conductpre-training on single-sequence proteins using larger datasets and model scales. These efforts soughtto harness the scale of the models to learn complex co-evolutionary information, although detailedinvestigations on how to optimally scale these models remain scarce. Our work primarily focuses onthese finer aspects, aiming to fill this gap in the research. Training objectives In natural language processing (NLP), masked language models (MLM) arerarely adopted due to the self-explanatory nature of natural language, which inherently prompts themeta-knowledge of tasks and generates task targets through CLM (Conditional Language Modeling)training models. However, a unified language modeling objective for Protein Language Models has yetto be fully consented. Those based on causal language modeling (CLM) have been primarily exploredfor protein design. Benchmarks in protein design using MLM have also shown promising resultsfor generation , exhibiting variable performance when compared to CLM . Additionally,the potential of the in-filling task objective remains largely unexplored . Our research aimsto thoroughly discern the scaling behavior of the two most common optimization objectives in thisdomain. Scaling Laws To our knowledge, the concept of scaling laws of language model is first introducedby OpenAI . Subsequently, numerous variants and modifications have been developedaround this theme. Recently, an array of new scaling laws has emerged. These include scaling lawsrelated to learning rates and batch sizes , data-constrained scaling laws , scaling laws fordownstream tasks and Transfer , as well as scaling laws within the Mixture of Experts (MoE)framework , and those concerning long sequences and positional encoding . While these lawsare primarily derived using auto-regressive models in resource-rich domains, their application in thebiological data sector is less common. Our work seeks to address this gap. Furthermore, scaling lawsfor Masked Language Models (MLM) are notably scarce. Given that MLMs are currently one of themost effective training methods for biological data, our research on MLMs could also be extended toother non-text domains.",
  "Loss": "Grad norm curve UniMeta200BUR50/S bootstrapUR50/S every_epoch_shuffleUR50/S global_shuffle Figure A7:Learning curve for UR50/S dataset repetition methods.Our 194B tokensdataset (UniMeta200B) shown in blue, serves as the reference with an approximate single epochrun. The bootstrapping method, depicted in orange, processes 200 billion tokens with replacement,indicating a tendency towards zero unsampled tokens by the fifth epoch. The every-epoch shufflemethod, in green, ensures all tokens are used per epoch, forming a stair-step pattern in training loss.Lastly, the global shuffle method, in red, loosely uses all tokens each epoch but ensures the strictnumber of epoch passes for every token. The rightmost plot of gradient norms shows an uptick forcurves corresponding to overfitting, signifying a lack of further optimization, with steep or erraticgradients indicated by the ascending gradient norms. We employed three different methods to repeat training on the UR50/S dataset, all of which ultimatelyled to overfitting. The reference for these experiments is shown by the blue curve in Figure A7, whichrepresents UniMetas loss for approximately one epoch. Firstly, using bootstrapping, we processed 200 billion tokens from UR50/S with replacement. In eachepoch, 65% of the dataset was randomly selected, leading to a diminished proportion of unsampledtokens by the fifth epoch, as depicted by the orange curve. Secondly, we shuffled the unique data for each epoch to ensure that all UR50/S tokens were used perepoch, resulting in a stair-step pattern in the training loss, illustrated by the green curve. It hassimply memorized the dataset but isnt improving at generalizing. Over-confident predictions of thefirst batch of the next epoch lead to a big step update, and then the model is not adapted to the nextbatches, resulting in no longer a decrease in loss. Lastly, we shuffled the entire training dataset less stringently, which did not strictly ensure that allUR50/S tokens were used every epoch, but guaranteed that each token was used an equal number oftimes over the entire training period. We term it global shuffle, this approach is shown by the redcurve. From the gradient norm curve shown in Figure A7 (right), we observe an uptick in gradient normfor the overfitting curves, indicating that the model is no longer optimizing effectively. In machinelearning, such an increase in gradient norm typically suggests that the model is encountering areas ofthe parameter space where gradients are steeper or more erratic, often occurring when the model startsto memorize the training data rather than generalize from it, approaching a saturated network .This behavior can result from overly complex models, too many training epochs without sufficientregularization, or training on non-representative data.",
  "Figure A9: Abalation of different masking ratios. Two models (154M and 85M) are trained from5% to 60% masking intervals, and evaluated on contact map and fold classification downstream tasks": "In the original BERT work , the absence of masked tokens in downstream tasks presented amismatch with the pre-training data distribution. The authors investigated various masking ratios andconcluded that a 15% masking rate was most beneficial for downstream tasks. This was implementedalongside an 80-10-10 strategy: 80% of the tokens were replaced with a mask, 10% were randomlysubstituted, and the remaining 10% were left unchanged.",
  "% to 60% (see Figure A8). The best masking ratios for validation loss drop ranged from 10% to20%; ratios too small (5%) or too large (greater than 25%) degraded the performance": "We further used pre-trained eight different models to perform full fine-tuning on downstream taskssuch as Contact Prediction and Fold Classification in Figure A9. Results from the test datasetsrevealed that, similar to NLP, the optimal performance was achieved within a 10%-20% maskingrange. Specifically, a 20% masking ratio slightly outperformed 15% in Contact Prediction, while the15% ratio yielded the best results in Fold Prediction. Consequently, for our Masked Language Model(MLM), we decided to adhere to the 15% masking ratio with the 80-10-10 strategy for training allour models.",
  "Step": "0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 P@L/5 Similar Pre-training Loss 880M MLM LoRA7.2B CLM LoRA880M MLM Probing7.2B CLM Probing Figure A10: Contact Prediction on MLM and CLM models. Two 3B models (CLM and MLM)were trained using identical computational resources, represented by the probing and LoRA fine-tuning methods. On the right, performance of a 7.2B CLM model is compared with an 880MMLM model under similar pre-training loss conditions. These models exhibit differing rates ofconvergence, highlighting the impact of uni-directional and bi-directional model architectures onlearning dynamics. We compared the effectiveness of CLM in the downstream task of contact prediction, using twodifferent setups (Figure A10). In the first setup, two 3B models were trained under identicalcomputational resources on 200 billion tokens, 3.4 1021FLOPs. Their performance was evaluatedthrough two training approaches: Probing (freezing the pre-trained model) and LoRA fine-tuning,with an added MLP head for comparison. In the second setup, we compared the effects of MLM and CLM under similar loss conditions. Here,a 7.2B CLM model and an 880M MLM model were selected, both achieving a loss of 1.98 on ourvalidation set. Despite the MLM model having a simpler loss calculation, involving a 15% mask ratherthan a one-by-one maskwhich would result in a higher lossthe MLM significantly outperformedthe CLM. Importantly, the CLM models computational power was an order of magnitude greaterthan the MLM model (1.68 1022 vs 1.0 1021 FLOPs). This suggests that despite the lowerloss achievable by the CLM model compared to MLM with a one-by-one mask, the unidirectionallimitations of CLM do not translate into better downstream task performance.",
  "EPre-training Dataset Quality": "Compared to Uniref90, ColabFoldDB offers a higher diversity and larger numbers of protein se-quences, though with generally shorter sequence lengths, likely suggesting potentially lower dataquality. To evaluate the efficacy of our expanded dataset, ColabFoldDB, we initially trained two85M models separately on Uniref90 and ColabFoldDB. Uniref90 in our dataset comprises twosubsets: Uniref50/S and the incremental dataset over Uniref50/S, termed Uniref90/50. Similarly,ColabFoldDB consists of representative and member data. We controlled the sampling proportion toensure uniform sampling across both datasets, with results reported in Table A8. Both models werethen trained using identical configurations on a 50B scale. From the perspective of validation loss in pre-training, the higher loss on ColabFoldDB mightbe attributed to its lower diversity and shorter sequence lengths compared to Uniref90. However,the performance on downstream tasks, such as contact prediction and fold classification, showsnegligible differences between models trained solely on ColabFoldDB and those trained on Uniref90,as illustrated in Figure E. This confirms that ColabFoldDB is an effective expansion of Uniref90 thatmaintains sample efficiency.",
  "Fold Classification": "85M_ColabFoldDB 85M_Uniref90 Figure A11: Data quality check. Comparison of learning dynamics and downstream task perfor-mance for two 85M models trained on ColabFoldDB and Uniref90. Left: Validation loss curvesdemonstrating initial training differences. Middle: Contact prediction performance showing theresponse to testing on similar tasks. Right: Fold classification accuracy, comparing model responsesto structural prediction tasks. Despite initial differences in loss, both datasets yield comparableperformance in downstream applications.",
  "We explain more details about Protein Generation Comparison as follows:": "OOD Dataset PPL Analysis.PPL represents the probability of the test sequences in the modeldistribution. The lower the PPL, the closer the distribution of the model and the test set is. In order totest the generalization ability of the model on new data, we use different sequence identity (0.0, 0.3,0.5) as thresholds to select the test set. pLDDT Scores from ESMFold. Predicted Local Distance Difference Test is the confidence level ofESMFold protein structure prediction. This metric is widely used in methods such as AlphaFold2,ESMFold, and OpenFold. pLDDT filters are often used in protein design (such as RFDiffusion),which can significantly improve the success rate of protein design; Natural Sequences Comparisons with Foldseek.Foldseek takes protein structure as input andsearches for proteins with similar structure in the database. We use the experimentally-resolvedprotein structure as the database (PDB database) to explore how the structure of the generatedsequences close to PDB (a higher TM-score indicates higher structural similarity). This method hasbeen used to evaluate other methods for protein sequence generation (ProGen2, ProtGPT2);",
  "GConvergence Analysis of Downstream Fine-tuning Tasks": "Observing the learning curves in Figure A12a, we can assess the effectiveness of different fine-tuning scenarios. For the contact prediction task, the convergence speed under the LoRA setting isvery similar for both models. Our testing reveals closely matching results for ESM-2 models withcapacities of 650M, 3B, 15B, consistent with the findings reported by Ankh et al. . This similaritysuggests possible saturation of the dataset under single-sequence pre-trained models. Additionally,the convergence rates for tasks such as fold classification and fluorescence are generally faster thanthose for ESM-2, indicating robust generalization following our data augmentation strategies. Based on the two 470M models defined in our , despite using the same computational power,we observe distinct outcomes (Figure A12b) in contact prediction and fold classification tasks. TheMLM model from CLM pre-training converges slightly faster than MLM from scratch. However, thedistinction is less pronounced in the two downstream regression tasks. This suggests that perplexityis more sensitive to protein structure related tasks, i.e., contact prediction and fold classification, butshows less sensitivity to regression tasks, particularly when assessed using the Spearman metric,which is prone to variability.",
  "LCLM = CE(V (W1( encoder(x))), ynext),": "Figure A13: Mixed objective validation loss. Comparative validation loss curves for models trainedfrom scratch versus mixed training approaches. Each panel corresponds to different model sizes, asindicated by the parameters. For each model, two training strategies were compared over an identicalnumber of elapsed tokens: training from scratch (blue) and mixed training with the other objective(orange). Across all model sizes, training from scratch consistently achieves lower validation losscompared to mixed training, suggesting that mixed training may not be as effective as dedicatedtraining for each individual objective.",
  "LMLM = CE(V (W2(encoder(x))), ymask),": "where V represents the protein vocabulary embedding, and W1 and W2 are the parameters corre-sponding to the CLM and MLM tasks, respectively. CE is the cross-entropy operator. The is theTanh activation function. We compared CLM and MLM under our scaling law of optimal model and data size distributions. Oneapproach involved training from scratch, while the other used mixed training. In the mixed trainingapproach, the actual number of training tokens was higher due to the additional FLOPs consumed byanother optimally trained objective, in other words. In other words, mixed training consumes theFLOPs of two optimal allocations; we only extracted the loss curve of one target for comparison. Weextracted the loss curve of just one target for comparison with the from-scratch training. Our findingsindicate that mixed training of the two targets can lead to detrimental interference, an effect notobservable in smaller models, as depicted in Figure A13. As the model size increases to a hundredmillion or billion parameters, the differences become more pronounced. The possible reason forthis situation is that mixed training has reduced the batch size for one of the objectives, makingoptimization difficult. We did not further investigate the impact of increasing the batch size and onlyobserved based on the training tokens. However, we cannot rule out the possibility that they aremutually detrimental. Therefore, if both objectives are to be optimized concurrently, a sequentialtraining strategy should be employed: first optimizing CLM, followed by MLM training. We considerthat CLM is more challenging to predict than MLM, which may allow the model to capture morecomplex and implicit sequential features initially, thereby enhancing its ability to understand andpredict masked words in subsequent MLM training. Model size 2.40 2.45 2.50 2.55 2.60 CLM Validation Loss 3e+18 FLOPs1e+19 FLOPs3e+19 FLOPs1e+20 FLOPs FLOPs Model size NCLMopt = 1.48 103 C0.573 FLOPs Data size DCLMopt = 1.57 102 C0.424 Model size 2.2 2.3 2.4 2.5 MLM Validation Loss 3e+18 FLOPs1e+19 FLOPs3e+19 FLOPs1e+20 FLOPs FLOPs Model size NMLMopt = 6.36 107 C0.738 FLOPs Data size DMLMopt = 2.04 105 C0.261 Figure A14: Scaling laws of MoE.The scaling behaviors of sparse parameter counts (8 experts) inMoE models, highlighting IsoFLOPs curves for different model sizes and FLOPs configurations.Each graph represents the relationship between model size, FLOPs, and validation loss for bothCLM and MLM using MoE configurations. The power-law fits indicate optimal model size and datarequirements for efficient scaling, showing that MoE models closely align with dense models in termsof scaling efficiency, with power-law coefficients for MoE-CLM and MoE-MLM approximating thoseof their dense counterparts. This suggests that MoE models can achieve similar scaling behaviorswith potentially lower computational costs.",
  "IMoE Scaling": "We find that the scaling behaviors of sparse parameter counts in Mixture of Expert (MoE) models areremarkably similar to those of dense model sizes, potentially allowing for a reduced compute budgetfor modeling scaling behaviors due to less activated parameters per token. In our experiments, we evaluate MoE models ranging from 10M to 500M sparse parameter counts,using a model size of 17 with eight experts, following the settings outlined in Mixtral of experts ,including its load-balancing scheme. The figure below shows different IsoFLOPs curves. Notably,the FLOPs here are calculated based on sparse parameters rather than actually activated ones. We usethe method described in the main text to select optimal loss points and fit these around the samplepoints, enabling us to project the optimal model size and number of tokens for larger models (centerand right). We observe that the power-law coefficients for CLM and MLM are similar to those ofdense models, with MoE CLM vs. Dense CLM at approximately 0.57 vs. 0.58, and MoE MLM vs.Dense MLM at 0.74 vs. 0.77. Our study strictly focuses on models with eight experts, which may not be entirely rigorous. Clarket al. proposed a unified scaling law defining effective training parameters for MoE, aimingto harmonize the scaling laws for Dense and MoE models. Investigation of biological data will beconsidered as future work.",
  "D + E(8)": "where E denotes the irreducible loss. Parameters A, B, , and are learned through the fittingprocess. As N or D , the function degenerates to a form similar to Equation 2, whichindicates that it models the scenarios under perfect conditions of other variables. Given that most of our training tokens are used for less than or equal to one epoch, and that the modelsize is prone to underfitting at fixed FLOPs, the asymptotic behaviors L(N) at D and L(D) atN are enough for determining the parameters in L(N, D).",
  "iHuberLSE (a log Ni, b log Di, e log Li),(9)": "where LSE represents the log-sum-exp operator, and = 103. The terms Ni, Di, and Li de-note the model size, dataset size, and loss of the i-th run, respectively. We fitted the MLM val-idation loss from 110 samples and the CLM validation loss from 149 samples using grid searchwith {0, 0.5, . . . , 2}, {0, 0.5, . . . , 2}, e {1, 0.5, . . . , 1}, a {0, 5, . . . , 25}, andb {0, 5, . . . , 25}. The final initialized parameters of CLM and MLM both are [e, a, b, , ] =[1, 5, 10, 0.5, 0.5]. We set the maximum number of iterations to 1000, and the two objectives wereessentially achieved after 360 iterations. The exponential powers of learned a and b yielded thecoefficients A, B, which were reported in Table A9.",
  "KIsoLoss": "In addition to using the seven different FLOPs counts reported in the main text to determine theoptimal model sizes and fit our scaling law, we also incorporated additional model points into ouranalysis. We trained using the final loss points of all the CLM and MLM that are run. Figure A15depicts the contour of the fitted function L and the efficient frontier as a red dashed line, presentedin log-log space. The frontier interval of is computed from this observation. From thisapproach, it revealed the scaling exponents for model size to be 0.77 in MLM and 0.57 in CLM, verysimilar to the IsoFLOPs profiling method in .1.",
  "LTraining Procedure": "We conducted all experiments using Ampere A100 GPUs (80G) equipped with NVLink, utilizing theGLM framework developed based on DeepSpeed and Megatron. We have used a total ofaround 1 million GPU hours. Our approach predominantly utilized data parallelism, avoiding modelparallelism and pipeline parallelism to simplify deployment. Modifications were made to the standardTransformer architecture , adopting a DeepNorm strategy and layer normalization . Theactivation function was set to GLU , RoPE was used to encode position, similar to thesettings found in the Transformer++ architecture . We further adopt FlashAttention to",
  "Model Size": "IsoLoss for MLM Efficient frontierNC0.76 Figure A15: Parametric fit for CLM and MLM. Unlike the IsoFLOPs method used in the maintext to select the optimal model size, these plots use all available data points to fit the models. Theleft panel shows the contour of the function L and the efficient frontier (indicated by the red dashedline) for the CLM, and the right panel for the MLM. The rainbow dots represent identical loss. Theresults closely align with using the IsoFLOPs profiling method. accelerate our training process. The used max LR empirically found to range between 6 104 and1.2 104 from small to large model size, was used along with a cosine decay strategy to reduce itto 0.1 max LR. Both CLM and MLM were trained under similar settings for model size, with aconsistent LR and a minimum warm-up period of 2.5% steps, extending to at least 100K training steps.All sequences were set to a length of 1024, with sequences concatenated using an <EOS> delimiter.Based on findings related to loss magnitude and batch size . The AdamW optimizer was usedwith 1 = 0.9, 2 = 0.95, = 1 108, and a weight decay of 0.01. All experiments omitted thedropout (it reduced the capacity to hinder model scaling) and trained with bfloat16. Most pre-trainingexperiments were confined to the 1 epoch, with some models extending up to 30% beyond oneepoch. For the transfer learning setting, we load the finished checkpoint of the pre-training model anddisregard the pre-trained optimized state, and learn rest tokens with warmup 5% steps the max LR.",
  "MBroader Impact": "If the scaling law of the protein language model improves predictions or understanding of proteinstructure and function, it could potentially have positive impacts on scientific research in fields suchas biology, medicine, and drug development. This may facilitate the development of new drugs,accelerate progress in disease diagnosis, or drive advancements in frontier research in the life sciences."
}