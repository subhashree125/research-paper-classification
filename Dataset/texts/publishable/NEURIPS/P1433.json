{
  "Abstract": "Adaptive gradient optimization methods, such as Adam, are prevalent in training deep neural net-works across diverse machine learning tasks due to their ability to achieve faster convergence.However, these methods often suffer from suboptimal generalization compared to stochastic gradi-ent descent (SGD) and exhibit instability, particularly when training Transformer models. In thiswork, we show the standard initialization of the second-order moment estimation (v0 = 0) as asignificant factor contributing to these limitations. We introduce simple yet effective solutions:initializing the second-order moment estimation with non-zero values, using either data-driven orrandom initialization strategies. Empirical evaluations demonstrate that our approach not onlystabilizes convergence but also enhances the final performance of adaptive gradient optimizers.Furthermore, by adopting the proposed initialization strategies, Adam achieves performance com-parable to many recently proposed variants of adaptive gradient optimization methods, highlightingthe practical impact of this straightforward modification.",
  ". Introduction": "First-order optimization methods, such as stochastic gradient descent (SGD), have been founda-tional in training deep neural networks due to their robust convergence properties across variousapplications . However, as deep learning architectures have grown more complex, there has beenincreasing interest in adaptive gradient optimizers, which dynamically adjust learning rates basedon the gradients of individual parameters . These methods often lead to faster convergence in cer-tain tasks . Among them, Adam has emerged as one of the most widely used adaptive gradientmethods, successfully applied to fields such as computer vision, natural language processing, andreinforcement learning . By combining the benefits of momentum and adaptive learning rates,Adam has proven particularly effective in training generative models and large language models. Theoretical studies have further elucidated its convergence properties in non-convex settings,providing insights into convergence rates . With careful hyperparameter tuning, Adam hasachieved significant success, especially in transformer-based architectures .Despite its fast-convergence property, Adam has been observed to suffer from instability andpoor generalization in certain non-convex optimization problems, such as training transformers forlanguage models . This instability often causes the optimizer to converge to suboptimallocal minima, thereby limiting the models performance. Several modifications have been proposedto address these issues. For instance, AdaBound improves generalization by bounding thestep size with a smooth parameter update, while RAdam rectifies the variance of the second-order moment to stabilize the learning rate during early iterations. AdaBelief adapts the stepsize based on the belief in the observed gradients, enhancing generalization. A broader rangeof studies has introduced further refinements to stabilize convergence and improve generalization",
  "INITIALIZATION OF ADAPTIVE OPTIMIZATION": "each experiment is repeated with five random seeds, and we report the mean results along withstandard deviations. For data-driven initialization, gradient statistics are computed using 5,000random samples prior to training, with the scaling factor set to = 1. For random initialization, thescaling factor is set to = 100, demonstrating the tuning-friendly nature of the proposed approach.Image Classification with CNN. We evaluate the ResNet-34 architecture on the CIFAR-10 image classification dataset . Each model is trained for 200 epochs with a batch size of 128,and the learning rate is decayed by a factor of 0.2 at epochs 60, 120, and 160. Label smoothing with a smoothing factor of 0.1 is applied. In addition to CIFAR-10, we perform experiments on theImageNet ILSVRC 2012 dataset using ResNet-18 as the backbone network. Each optimizeris executed for 100 epochs with a cosine annealing learning rate schedule, which has demonstratedsuperior performance compared to step-based decay strategies . For SGD, we use the momen-tum factor of 0.9, a common default setting , with a tuned learning rate of 0.1. For adaptivegradient methods (Adam, AdamW, RAdam, AdaBound, AdaBelief), we use the learning rate of0.001, 1 = 0.9, 2 = 0.999, and = 108.Language Modeling with LSTM. We evaluate a 2-layer LSTM on the Penn Treebankdataset . Models are trained for 200 epochs with a batch size of 20, and the learning rate isreduced by a factor of 0.1 at epochs 100 and 145. For SGD, we use a learning rate of 30 and amomentum factor of 0.9. Adam, AdamW, AdaBound, and AdaBelief use a learning rate of 0.01,while RAdam uses a learning rate of 0.001. All adaptive methods are configured with 1 = 0.9 and2 = 0.999.Neural Machine Translation with Transformer. We experiment with a small Transformermodel implemented using the Fairseq package on the IWSLT14 German-to-English ma-chine translation dataset. The model is trained with a length penalty of 1.0, a beam size of 5, andan initial warmup step size of 107. Training is conducted for 55 epochs, and results are reportedas the average of the last 5 checkpoints. Adaptive learning methods use a learning rate of 0.0015.Adam, AdamW, AdaBound, and AdaBelief are configured with 1 = 0.9, 2 = 0.98, while RAdamuses 1 = 0.9, 2 = 0.999.Image Generation with GAN. We evaluate a deep convolutional GAN (DCGAN) onthe CIFAR-10 image generation task. Both the generator and discriminator networks use CNNarchitectures. Models are trained for 200,000 iterations with a batch size of 64. Learning rate is fixedat 0.0002 for both the generator and discriminator across all optimizers. All other hyperparametersare set to their default values for fair comparison.",
  "sign( mt)(4)": "First step of Adam as sign descent. In Adams standard implementation, the first- and second-order momentum terms are initialized to zero, m0 = 0, v0 = 0. As a result, the first step of theoptimization process degenerates into sign descent, where the magnitude of the step size dependssolely on the learning rate rather than the full gradient. This behavior is illustrated as follows:",
  "v0= sign(g1).(5)": "In this first step, Adam performs a pure sign-descent update due to the zero initialization of m0 =0, v0 = 0. However, from the second step onward, the moving averages begin to incorporate gra-dient information, and the updates evolve into a combination of sign descent and adaptive gradientdescent. Over subsequent iterations, as more gradient information is accumulated, the influence ofthe initial sign descent diminishes, and the optimizer transitions into its adaptive behavior wheremt = vt , as shown in Equations (1), (2) and (4).",
  ". Instability of Adam optimizer": "Instability of Adam on training Transformer network. Training Transformer models for variousNLP tasks often relies on a learning rate warmup strategy , which has also been shown to enhanceaccuracy in Vision Transformers . Removing the warmup phase, however, has been observedto increase training loss, underscoring its role in stabilizing the optimization process .To explore this phenomenon, we conducted experiments training a Transformer model on theIWSLT14 DE-EN dataset for a neural machine translation task. We evaluated three approaches:vanilla Adam without warmup (denoted as v0,0), vanilla Adam with warmup, and our proposed data-driven initialization of Adam without warmup (denoted as v0,data, described in the next section). Asillustrated in (a), vanilla Adam without warmup exhibits increased training loss during theearly stages. We attribute this instability to Adams initial sign-descent behavior, which is exacer-bated by the standard zero-initialization of the second-order moment (v0 = 0). While the learningrate warmup strategy effectively addresses this issue, it requires using a very small learning rateduring the initial stages, limiting parameter updates and slowing down convergence. In this work,we propose a non-zero initialization strategy to directly stabilize the optimizer. Unlike warmup, ourapproach avoids restrictive learning rate constraints, enabling faster convergence while maintainingtraining stability.Impact of sign descent and shrinking gradients. In this section, we analyze the non-convergencebehavior of vanilla Adam, focusing on the large initial step sizes observed during neural networktraining ((a)). Neural networks often exhibit a flat loss landscape at the beginning of train-ing, with gradients that are small in magnitude. This phenomenon is particularly pronounced whentraining Transformers, as noted in prior works . The initial loss landscape of the Trans-former model is visualized in (c), where the loss is plotted along two random directions asdescribed in . The visualization highlights that the loss landscape is extremely flat, and gradients",
  ": Histogram of update step distribution across coordinates": "are correspondingly small. When training such networks with Adam, the sign descent behaviorduring the initial step can amplify these small gradients disproportionately, resulting in overly largeparameter updates. To further investigate this phenomenon, (b) illustrates the norm of theupdate step t during training for three optimizers: SGD, vanilla Adam, and Adam with theproposed initialization v0,data. The results show that the first update step size for vanilla Adamv0,0 is significantly larger compared to Adam v0,data or SGD. These large initial updates can pushthe optimizer away from initial regions in the parameter space, making recovery and convergencemore challenging. In contrast, SGD exhibits much smaller update steps during the initial stages,even when using a larger learning rate (lr=0.1) than Adam (lr=0.001) in our experiments. To furtherillustrate the update step sizes, presents histograms of the absolute values of parameter up-dates for different optimizers. For vanilla Adam ((a)), many parameters are updated with astep size equal to the learning rate in the first step (t = 1) due to its sign descent behavior. Subse-quently, the update step sizes decrease. In contrast, Adam with non-zero initialization ((b))achieves relatively stable update step sizes throughout training, avoiding the large initial jumps seenin vanilla Adam. This behavior aligns closely with SGD ((c)), which consistently maintainsstability in its updates from the start.",
  ". Non-zero Initialization of Second Order Moment": "As shown in Equations (2) and (4), initializing the second-order moment v0 with non-zero valueseffectively prevents the first step of Adam from degenerating into sign descent.Special case: linear loss. To build intuition for initializing the second-order moment, we firststudy a simplified setting. Consider the linear loss function f(t) = t, gtwith a Noisy Gradient",
  "k=0k2(g2 + 2I) = t2v0 + (1 t2)(g2 + 2I)(7)": "These results indicate that, after a sufficient number of steps, E[mt] g, and E[vt] g2 + 2I.In many practical scenarios, where the average gradient magnitude is small E[gt] 0, initializingm0 = 0 is a reasonable choice to stabilize mt. Since mt approximates the first moment of thegradient, zero initialization aligns with its role. However, for vt, which represents the second-ordermoment of the gradient, it must satisfy E[vt] > 0. This makes the standard zero initialization (v0 =0) inherently inconsistent with its purpose. Furthermore, v0 plays a critical role in determining theadaptive learning rate during the initial steps, directly influencing convergence and optimizationstability.To assess the stability of the optimization process and the influence of the initial state, we definethe drift of the second-order moment as:",
  "driftvt(v0) = E[v]| E[v0].(8)": "This term quantifies the adjustment required for the second moment to transition from its initialvalue to its steady-state. It reflects how much the optimizer must adapt its gradient scaling duringtraining. Since vt directly determines the adaptive learning rate, a smaller drift term indicates betterstability of optimization process.For vanilla Adam, v0 = 0, the expected value of vt converges to E[v]| = g2 + 2I fromE[v0] = 0. Then driftvt(v0 = 0) = g2 + 2. This large drift value causes significant initialadjustments of vt, leading to potential instability in optimization.For non-zero initialization, v0 = g2 + 2I, the expected second moment remains constant forall E[vt] = g2 + 2I. Thus driftvt(v0 = g2 + 2I) = 0. With this initialization, vt is immediatelyaligned with its steady-state value, eliminating the need for adjustments and ensuring stability fromthe start. The expectation E[vt] is of scale O(2) and the standard deviation of each coordinate ofvt is of scale O((1 2)2). When 2 closer to 1, vt becomes nearly deterministic and tightlyconcentrates around vt g2 + 2I. Ignoring for simplicity, the Adam update rule becomes:",
  "g2 + 2I(9)": "This ensures a stable adaptive learning rate: (g2+2I)1/2. Such stability aligns with the defini-tion of an adaptive learning rate, where vt incorporates local geometry (e.g., Hessian information).For the linear loss case, this stability results in more consistent updates. Further illustration of thestability provided by a non-zero v0 in RMSprop is presented in Appendix A.1.",
  "v0 = E[g(xi, yi)]2 + VAR[g(xi, yi)], where (xi, yi) D.(10)": "Here, is a hyperparameter that controls the scale of v0. This approach ensures that v0 reflectsmeaningful statistical information about the gradient, aligning the optimizers initialization withthe characteristics of the specific training data. Random Initialization, denoted as v0,rnd. This is computationally efficient and avoids the over-head associated with data-driven initialization. As shown in the previous analysis, any smallpositive value for v0 enhances the stability of vt, making random initialization a practical choice.We propose initializing v0 using a scaled Chi-squared distribution 1:",
  "fan in + fan out 21,(11)": "where 21 denotes a chi-squared distribution with one degree of freedom. fan in and fan out arethe input and output dimensions of the weight matrix Rfan outfan in, and is a hyperparame-ter that controls the scale of the distribution. This distribution ensures that v0 scales appropriatelywith the dimensions of the weight parameters, similar to Xavier initialization for neural networkweights . Furthermore, the squared value g2t of a Gaussian random gradient gt naturally fol-lows a scaled chi-squared distribution, providing a principled foundation for this initializationstrategy.Under the proposed initialization v0,data and v0,rnd, the first step size of Adam becomes:",
  "v0= sign(g1), |1| < (12)": "This ensures that the first update step is influenced by both the magnitude and direction of thegradient, avoiding the pure sign descent behavior seen with v0 = 0. Such stabilization is par-ticularly crucial for deep learning tasks with shrinking gradients, such as training Transformers.The proposed initialization strategies are broadly applicable beyond Adam and can be extendedto other adaptive gradient methods, including AMSGrad , AdaBound , RAdam , andAdaBelief . These methods could benefit from improved stability during the initial steps, po-tentially enhancing both training dynamics and final performance. A discussion comparing theproposed initialization strategy with other optimization approaches is presented in Appendix A.2.",
  ". Experiments": "To evaluate the effectiveness of our approach, we conducted extensive experiments across a varietyof tasks, including image classification with convolutional neural networks (CNNs) , imagegeneration with generative adversarial networks (GANs) , language modeling with long short-term memory networks (LSTMs) , and neural machine translation with Transformers . Weempirically evaluate the performance of two initialization strategies v0,data (Equation (10)) andv0,rnd (Equation (11)) across several widely used adaptive gradient optimization methods. Thesemethods include SGD with momentum , Adam , AdamW , AdaBound , RAdam, and AdaBelief . For each optimizer, we use the standard initialization (v0 = 0) as thebaseline and compare it against the proposed strategies (v0,rnd and v0,data). For v0,data, gradientstatistics are computed using 5,000 random samples prior to training, with the scaling factor set to = 1. For v0,rnd, the scaling factor is set to = 100. Detailed information about the experimentalsetup is provided in Appendix B.1.",
  "n1 + b, d = (xs b)n x2s(14)": "The parameter n represents the degree of the polynomial. In our experiment, we set n = 7, b = 1,and s = 0.5. The purpose of the experiment is to observe the optimization behavior under differentinitializations. We use the Adam optimizer with the following hyperparameters: = 1, 1 =0.9, 2 = 0.999. For scenarios requiring smaller learning rates, the objective function can be scaleddown to achieve similar conclusions.",
  ". Image Classification with CNN": "We evaluate the ResNet-34 architecture on the CIFAR-10 image classification dataset .The test accuracy at the final epoch is summarized in . The results demonstrate that theproposed initialization of v0, represented as v0,rnd and v0,data, enhances the performance of adap-tive gradient optimization methods, including Adam, AdamW, AdaBound, RAdam, and AdaBelief.Notably, with v0,data, Adam achieves a test accuracy surpassing that of the more recent AdaBeliefapproach. Furthermore, AdaBelief with v0,data outperforms SGD, showcasing the effectiveness ofthe proposed method. v0,rnd also consistently improves the performance of adaptive gradient meth-ods without incurring additional computational overhead, making it a practical and efficient solutionfor stabilizing the optimization process.",
  ". Language Modeling with LSTM": "We evaluate a 2-layer LSTM network on the language modeling task of Penn Treebank dataset. The test perplexity (lower is better) is summarized in . The results demonstrate thatboth v0,rnd and v0,data significantly improve the performance of adaptive gradient methods. No-tably, with these proposed initialization strategies, Adam achieves test perplexity results that sur-pass the more recent AdaBelief optimizer. Results for a 3-layer LSTM network are provided inAppendix B.3.",
  ". Image Generation with GAN": "We evaluated a deep convolutional GAN (DCGAN) on the CIFAR-10 image generation task.The performance is measured using the Frechet Inception Distance (FID, lower is better) , whichquantifies the similarity between generated images and the real dataset. In training GANs, optimizerstability is crucial for achieving high-quality image generation. As shown in , the proposedinitialization strategies, v0,rnd and v0,data, stabilize the optimization process for adaptive gradientmethods, resulting in additional performance gains. For instance, v0,rnd and v0,data improve theperformance of the Adam optimizer by 10% and 13%, respectively, highlighting the effectivenessof the proposed approaches.",
  ": Comparison of Vanilla Adam and Adam v0,rnd on (a) CIFAR-10 image classification task.(b) Penn Treebank language modeling task. (c) IWSTL14 machine translation task": "more stable convergence and higher final accuracy. For the Penn Treebank language modeling taskin (b), Adam v0,rnd results in lower perplexity at convergence compared to Vanilla Adam.For Transformer models on the IWSLT14 DE-EN machine translation dataset (with warmup) in(c), Adam v0,rnd demonstrates faster convergence, more stable optimization, and lowerperplexity at the end of training.",
  "v0,rnd95.2595.4595.7495.8995.8795.84v0,data95.2595.7096.0295.9295.8595.72": "Loss landscape. To further explore con-verged behavior of Adam with v0,rnd, we vi-sualize the loss landscapes around the con-vergent points of Transformer models trainedwith Vanilla Adam and Adam v0,rnd on theIWSLT14 DE-EN machine translation task.The loss landscape is plotted along two normal-ized random directions. As shown in ,the loss landscape for Adam v0,rnd is flatter than that for Vanilla Adam. A flatter loss landscapeis often indicative of better generalization performance . Although the training losses ofVanilla Adam and Adam v0,rnd are comparable, the flatter landscape for Adam v0,rnd explains itssuperior testing accuracy.",
  ". Conclusion": "In this work, we revisited the initial steps of adaptive gradient optimization methods, focusing onthe instability caused by the sign-descent behavior during early iterations. To address this issue, weproposed two simple yet effective approaches: data-driven initialization and random initializationof the second-moment estimate v0. Our empirical results demonstrate that these initialization strate-gies significantly enhance the performance and stability of several adaptive gradient optimizationmethods, including Adam, particularly in challenging tasks such as training Transformer models.",
  "John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learningand stochastic optimization. Journal of machine learning research, 12(7), 2011": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforwardneural networks. In Proceedings of the thirteenth international conference on artificial intel-ligence and statistics, pages 249256. JMLR Workshop and Conference Proceedings, 2010. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, SherjilOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communica-tions of the ACM, 63(11):139144, 2020.",
  "Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat localminima. Advances in neural information processing systems, 32, 2019": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recogni-tion, pages 770778, 2016. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochre-iter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Ad-vances in neural information processing systems, 30, 2017.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.2009": "Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise isnot the main factor behind the gap between sgd and adam on transformers, but sign descentmight be. In The Eleventh International Conference on Learning Representations, 2023. Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbalance and why adam outperforms gradient descent on language models. arXivpreprint arXiv:2402.19449, 2024.",
  "Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of math-ematical statistics, pages 400407, 1951": "Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, ZhihengHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visualrecognition challenge. International journal of computer vision, 115:211252, 2015. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-thinking the inception architecture for computer vision. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pages 28182826, 2016. Yuandong Tian, Yiping Wang, Beidi Chen, and Simon S Du. Scan and snap: Understandingtraining dynamics and token composition in 1-layer transformer. Advances in Neural Infor-mation Processing Systems, 36:7191171947, 2023.",
  "A Vaswani. Attention is all you need. Advances in Neural Information Processing Systems,2017": "Yizhou Wang, Yue Kang, Can Qin, Huan Wang, Yi Xu, Yulun Zhang, and Yun Fu. Momentumis all you need for data-driven adaptive optimization. In 2023 IEEE International Conferenceon Data Mining (ICDM), pages 13851390. IEEE, 2023. Robin Yadav, Frederik Kunstner, Mark Schmidt, and Alberto Bietti. Why adam outperformsgradient descent on language models: A heavy-tailed class imbalance problem. In OPT 2023:Optimization for Machine Learning.",
  "Dongruo Zhou, Jinghui Chen, Yuan Cao, Yiqi Tang, Ziyan Yang, and Quanquan Gu. Onthe convergence of adaptive gradient methods for nonconvex optimization. arXiv preprintarXiv:1808.05671, 2018": "Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towardstheoretically understanding why sgd generalizes better than adam in deep learning. Advancesin Neural Information Processing Systems, 33:2128521296, 2020. Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, XenophonPapademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief inobserved gradients. Advances in neural information processing systems, 33:1879518806,2020.",
  "(1 t2)(g2 + 2I)(20)": "In this setting, the denominator is initially small due to (1 t2) approaching 0 as t 0. The smalldenominator leads to excessively large initial updates, particularly when g is small or 2 is large.This instability can cause erratic optimization behavior, especially in the early stages of training.Case 2: non-zero initialization ( v0 = g2 + 2I). When v0 = g2 + 2I, the update becomes:",
  "g2 + 2 .(21)": "In this setting, the denominator is well-scaled from the start, incorporating the correct statisticalvariance. This prevents excessively large updates during early iterations, ensuring better stability.The step sizes remain consistent across iterations, aligning with the principles of adaptive gradientmethods. Additionally, the incorporation of gradient statistics g2 + 2I ensures that vt adaptsappropriately to the local geometry of the loss function, such as the Hessian information. For alinear loss, this stabilization leads to smoother convergence, providing a more robust optimizationprocess. It is worth noting that the above analysis can be readily extended to other adaptive gradientmethods, such as Adam.",
  "A.2. Revisiting Previous Works on Stabilizing the Initial Steps of Adam": "Warmup. The warmup technique implicitly adjusts the initialization of the second-momentestimate v by employing a smaller learning rate during the initial steps. While the optimizers stateupdates normally, the parameter changes are minimal due to the extremely small learning rate. Thisapproach effectively mitigates the sign-descent behavior observed in Adams early steps. However,warmup introduces additional hyperparameters (e.g., the scheduler) that require careful tuning andnecessitates several steps of training where the network parameters are not effectively updated. Thiscan be inefficient, particularly in resource-constrained settings. In contrast, our method directlyaddresses the aggressive sign-descent issue by initializing v0 with non-zero values, eliminating theneed for a warmup phase. Our experimental results demonstrate that random initialization of v0stabilizes the training process effectively, without requiring extra tuning or wasted iterations.RAdam. RAdam avoids the sign-descent issue by behaving like SGD during theinitial steps. This is achieved by introducing a rectification term, dynamically adjusting the opti-mizers behavior to stabilize updates in the early iterations. While RAdam successfully addressesinitial-step instability, it adds complexity to the optimization process through the computation ofthe rectification term. In contrast, our approach provides a simpler and more intuitive solution bydirectly adjusting the initialization of the moment estimates, without modifying the core algorithmor introducing additional dynamic terms.AdaBound. AdaBound tightly bounds the update size during the initial steps, preventingexcessively large updates caused by sign-descent behavior. However, this approach introduces dy-namic bounds that require careful tuning of the bounding functions, adding additional complexityto the optimization process. Our initialization strategy simplifies this issue by stabilizing updateswithout the need for dynamic bounds, making it a more efficient and practical alternative.AdaBelief. AdaBelief reduces the impact of initial sign-descent behavior by refining thevariance estimation, leading to more reliable adaptive learning rates. However, this comes at the costof increased computational complexity due to the need for precise variance estimation. By contrast,our method provides stability during the initial steps without additional computational overhead,offering a straightforward alternative to improve early optimization dynamics.Our initialization strategy can be seamlessly integrated into existing methods, such as RMSprop,AdamW, RAdam, AdaBound, AdaBelief, and even Warmup. By addressing the aggressive sign-descent behavior directly through non-zero initialization of v0, we enhance the stability of theseoptimizers in their early steps. Importantly, this random initialization incurs no extra computationalcosts and avoids the need for additional hyperparameter tuning.",
  "B.1. Experimental Setting": "We empirically evaluate the performance of the proposed data-driven initialization (Equation (10))and random initialization (Equation (11)) strategies across several widely-used adaptive gradientoptimization methods. These include SGD with momentum (SGDM) , Adam , AdamW, AdaBound , RAdam , and AdaBelief . Each optimizer is tested using its standardinitialization (v0 = 0) as the baseline, which is then compared against the proposed strategies v0,dataand v0,rnd. Following experimental protocols established in prior works , we performthorough hyperparameter tuning for learning rate, 1, 2, and . To ensure statistical robustness,"
}