{
  "Abstract": "In this paper, we introduce a novel continual audio-visual sound separation task,aiming to continuously separate sound sources for new classes while preserving per-formance on previously learned classes, with the aid of visual guidance. This prob-lem is crucial for practical visually guided auditory perception as it can significantlyenhance the adaptability and robustness of audio-visual sound separation models,making them more applicable for real-world scenarios where encountering newsound sources is commonplace. The task is inherently challenging as our modelsmust not only effectively utilize information from both modalities in current tasksbut also preserve their cross-modal association in old tasks to mitigate catastrophicforgetting during audio-visual continual learning. To address these challenges, wepropose a novel approach named ContAV-Sep (Continual Audio-Visual SoundSeparation). ContAV-Sep presents a novel Cross-modal Similarity DistillationConstraint (CrossSDC) to uphold the cross-modal semantic similarity through in-cremental tasks and retain previously acquired knowledge of semantic similarity inold models, mitigating the risk of catastrophic forgetting. The CrossSDC can seam-lessly integrate into the training process of different audio-visual sound separationframeworks. Experiments demonstrate that ContAV-Sep can effectively mitigatecatastrophic forgetting and achieve significantly better performance comparedto other continual learning baselines for audio-visual sound separation. Code isavailable at:",
  "Introduction": "Humans can effortlessly separate and identify individual sound sources in daily experience . This skill plays a crucial role in our ability to understand and interact with the complex auditoryenvironments that surround us . However, replicating this capability in machines remains asignificant challenge due to the inherent complexity of real-world auditory scenes . Inspired bythe multisensory perception of humans , audio-visual sound separation tackles this challengeby utilizing visual information to guide the separation of individual sound sources in an audio mixture. Recent advances in deep learning have led to significant progress in audio-visual sound separation . Benefiting from more advanced architectures (e.g., U-Net ,Transformer , and diffusion models ) and discriminative visual cues (e.g., grounded visualobjects , motion , and dynamic gestures ), audio-visual separation models are able toseparate sounds ranging from domain-specific speech, musical instrument sounds to open-domaingeneral sounds within training sound categories. However, a limitation of these studies is their focuson scenarios where all sound source classes are presently known, overlooking the potential inclusionof unknown sound source classes during inference in real-world applications. This oversight leads tothe catastrophic forgetting issue , where the fine-tuning of models on new classes detrimentallyimpacts their performance on previously learned classes. Despite Chen et al. demonstratingthat their iQuery model can generalize to new classes well through simple fine-tuning, it still suffersfrom the catastrophic forgetting problem on old classes. This prevents the trained models from",
  "ModelModel": ": Top: Illustration of the continual audio-visualsound separation task, where the model (separator) learnsfrom sequential audio-visual sound separation tasks. Bot-tom: Illustration of the catastrophic forgetting problem incontinual audio-visual sound separation and its mitigationby our proposed method. Fine-tuning: Directly fine-tunethe separation model on new sound source classes; Upperbound: Train the model using all training data from seensound source classes. To bridge this gap, we introduce anovel continual audio-visual soundseparation task by integrating audio-visual sound separation with contin-ual learning principles. The goal ofthis task is to develop an audio-visualmodel that can continuously separatesound sources in new classes whilemaintaining performance on previouslylearned classes.The key challengewe need to address is catastrophic for-getting during continual audio-visuallearning, which occurs when the modelis updated solely with data from newclasses or tasks, resulting in a signifi-cant performance drop on old ones. Weillustrate our new task and the catas-trophic forgetting issue in . Unlike typical continual learning prob-lems such as task-, domain-, or class-incremental classification in visual do-mains , which resultin progressively increasing logits (orprobability distribution) across all ob-served classes at each incremental step, our task uniquely produces fixed-size separation masksthroughout all incremental steps. In this context, each entry in the mask does not directly correspondto any specific classes. Additionally, the new task involves both audio and visual modalities. There-fore, simply applying existing visual-only methods cannot fully exploit and preserve the inherentcross-modal semantic correlations. Very recently, Pian et al. and Mo et al. extendedcontinual learning to the audio-visual domain, but both focused on classification tasks. To address these challenges, in this paper, we propose a novel approach named ContAV-Sep(Continual Audio-Visual Sound Separation). Upon the framework, we introduce a novel Cross-modalSimilarity Distillation Constraint (CrossSDC) to not only maintain the cross-modal semantic similar-ity through incremental tasks but also preserve previously learned knowledge of semantic similarityin old models to counter catastrophic forgetting. The CrossSDC is a generic constraint that can beseamlessly integrated into the training process of different audio-visual sound separators. To evaluatethe effectiveness of our proposed ContAV-Sep, we conducted experiments on the MUSIC-21 datasetwithin the framework of continual learning, using the state-of-the-art audio-visual sound separationmodel iQuery and a representative audio-visual sound separation model Co-Separation ,as our separation base models. Experiments demonstrate that ContAV-Sep can effectively mitigatecatastrophic forgetting and achieve significantly better performance than other continual learningbaselines. In summary, this paper contributes follows: (i) To explore more practical audio-visual sound separation, in which the separation model shouldbe generalized to new sound source classes continually, we pose a Continual Audio-Visual SoundSeparation task that trains the separation model under the setting of continual learning. To the best ofour knowledge, this is the first work on continual learning for audio-visual sound separation.",
  "Related Work": "Audio-Visual Sound Separation. Audio-visual sound separation aims to separate individual soundsources from an audio mixture guided by visual cues. A line of research emerges under variousscenarios, such as separating musical instruments , human speech , or sound sources in in-the-wild videos . Many frameworks and methods havebeen proposed to address challenges within specific problem settings. For instance, the extraction offace embeddings proves beneficial for speech audio separation . Moreover, incorporating objectdetection can provide an additional advantage . The utilization of trajectory optical flows toleverage temporal motion information in videos, as demonstrated by , also yields improvements.In this work, not competing on designing stronger separators, we would advance the exploration ofthe audio-visual sound separation within the paradigm of continual learning. We investigate howa model can learn to consistently separate sound sources from sequential separation tasks withoutforgetting previously acquired knowledge. Continual Learning. The field of continual learning has drawn significant attention, especiallyin visual domains, with various approaches addressing this challenge. Notable among these areregularization-based methods, exemplified in works such as . These approaches involveapplying regularization to crucial parameters associated with old tasks to maintain the modelscapabilities and during incremental steps, less important parameters are given higher priority forupdates compared to important ones. Conversely, several works appliedrehearsal-based pipelines to enable the model review previously learned knowledge. For instance,Rebuffi et al. proposed one of the most representative exemplars selection strategy Nearest-Mean-of-Exemplars (NME), selects the most representative exemplars in each class based on the distanceto the feature center of the class. Meanwhile, pseudo-rehearsal employs generativemodels to create pseudo-exemplars based on the estimated distribution of data from previous classes.Moreover, architecture-based/dynamic architecture methods proposed tomodify the model architecture itself to enable the model to acquire new knowledge while mitigatingthe forgetting of old knowledge. Specifically, Pham et al. proposed a dual network architecture, inwhich one is to learn new tasks while the other one is for retaining knowledge learned from old tasks.Wang et al. combined the dynamic architecture and distillation constraint to mitigate the issueof continual-increasing overhead problem in dynamic architecture-based continual learning method.However, above studies mainly concentrate on the area of continual image classification. Recently,researchers also explored other continual learning scenarios beyond image classification. For instance,Park et al. extend the knowledge distillation-based continual image classification methodto the domain of video by proposing the time-channel distillation constraint. Douillard et al. proposed to tackle the continual semantic segmentation task with multi-view feature distillation andpseudo-labeling. Xiao et al. further addressed the continual semantic segmentation problemthrough weights fusion strategy between old and current models. Wang et al. addressed thecontinual sound classification task through generative replay. Furthermore, continual learning hasalso been explored in the domain of language/vision-language learning tasks ,self-supervised representation learning , audio classification andfake audio detection , etc. Despite the success of existing continual learning methods invarious scenarios, their applicability in the domain of continual audio-visual sound separation isstill unexplored. Although Pian et al. and Mo et al. proposed to tackle the catastrophicproblem in audio-visual learning, their studies mainly concentrated in the area of audio-visual videoclassification. In contrast to existing works in continual learning, in this paper, we delves into thecontinual audio-visual sound separation, aiming to tackle the challenge of catastrophic forgettingspecifically in the context of separation mask prediction for complicated mixed audio signals withinjoint audio-visual modeling.",
  "Problem Formulation": "Audio-Visual Sound Separation. Audio-visual sound separation aims to separate distinctive soundsignals according to the given associated visual guidance. Following previous works , we adopt the common mix-and-separation training strategy to train the model. Given twovideos V 1(s1, v1) and V 2(s2, v2), we can obtain the input mixed sound signal S by mixing two video sound signals s1 and s2, and then we can have the ratio masks M 1 = s1/S and M 2 = s2/S1.The goal of the task is to utilize the corresponding visual guidance v1 and v2 to predict the ratiomasks for reconstructing the two individual audio signals. This process can be formulated as:",
  "where D denotes the training set, and L is the loss function between the prediction and ground-truth": "Continual Audio-Visual Sound Separation. Our proposed continual audio-visual sound separationtask aims to train a model F continually on a sequence of T separation tasks {T1, T2, ..., TT }. Forthe t-th task Tt (incremental step t), we have a training set Dt = {V i(si, vi), yit}nti=1, where i and ntdenote the i-th video sample and the total number of samples in Dt respectively, and yit Ct is thecorresponding sound source class of video V i, where Ct is the training sound class label space of taskTt. For any two tasks Tt1 and Tt2 and their corresponding training sound class label space Ct1 andCt2, we have Ct1 Ct2 = . Following previous works in continual learning ,for a task Tt, where t > 1, holding a small size of memory/exemplar set Mt to store some data fromold tasks is permitted in our setting. Therefore, with the memory/exemplar set Mt, all available datathat can be used for training in task Tt (t > 1) can be denoted as Dt = Dt Mt. Finally, the trainingprocess of Eq. 2 in our continual audio-visual sound separation setting can be denoted as:",
  "s.t.M1 = Ft1(S, v1),M2 = Ft1(S, v2),(3)": "which means that the new model t is obtained by updating the old model t1 which was trainedon the previous task, using the current tasks available data Dt. After the training process for taskTt with Dt, the updated model will be evaluated on a testing set which includes video samplesfrom all seen sound source classes up to continual step t (task Tt). And the evaluation also followsthe common mix-and-separation strategy. During this continual learning process, the modelsseparation performance on the previously learned tasks drops significantly after training on new tasks.This learning issue is referred to as the catastrophic forgetting problem, which poses aconsiderable challenge in continual audio-visual sound separation.",
  "Overview": "To address the challenge of catastrophic forgetting in continual audio-visual sound separation, weintroduce ContAV-Sep. This new framework, illustrated in , consists of three key components:a separation base model, an output mask distillation module, and our proposed Cross-modal Simi-larity Distillation Constraint (CrossSDC). We use a recent state-of-the-art audio-visual separator:iQuery as the base model of our approach, which contains a video encoder to extract the globalmotion feature, an object detector and image encoder to obtain the object feature, a U-Net for mixture sound encoding and separated sound decoding, and an audio-visual Transformer to getthe separated sound feature through multi-modal cross-attention mechanism and class-aware audioqueries. For the object detector, we follow iQuery and use the pre-trained Detic , a universalobject detector, to detect the sound source objects in each frame. For the video encoder and theimage encoder, inspired by the excellent generalization ability of recent self-supervised pre-trainedmodels, which has been proven to be effective and appropriate in continual learning , we applytwo self-supervised pre-trained models VideoMAE and CLIP as the video encoder and theimage encoder, respectively. Note that, during the training process, the object detector, video encoder,and image encoder are frozen.",
  "L( Mt, Mt1)": ": Overview of our proposed ContAV-Sep, which consists of an audio-visual sound separationbase model architecture, an Output Mask Distillation, and our proposed Cross-modal SimilarityDistillation Constraint. The fire icon denotes the module is trainable, while the snowflake icondenotes that the module is frozen. The (i)STFT stands for (inverse) Short-Time Fourier Transform.Please note that, the old model Ft1 is frozen during training. Given a pair of videos V 1(s1, v1) and V 2(s2, v2), at incremental step t (task Tt), the U-Net audioencoder FAEttakes the mixed audio signal S obtained by mixing s1 and s2 as input, and generatesthe latent mixed audio feature. This process can be expressed as:",
  "s.t.f o,1t= U ot(Obj.1), f m,1t= U mt (Mo.1),(5)": "where f a,1tdenotes the separated sound feature of video V 1; Obj.1 and Mo.1 denote the object andmotion features extracted by the frozen pre-trained image and video encoders respectively from thevisual signal v1 of video V 1, U ot() and U mt () are learnable projection layers to map the object andmotion features into the same dimension. Similarly, we can also obtain the separated sound feature ofV 2 guided by the associated visual features. The extracted separated sound feature and the latent mixed audio feature are combined to generatea mask. This mask is subsequently applied to the mixed audio, leading to the reconstruction of theseparated sound spectrogram.",
  "M2t = FADt(f lat.t) MLPt(f a,2t),(6)": "where M1t and M2t denote the predicted masks for audio signals of video V 1 and V 2, respectively;FADtis the U-Net decoder at incremental step t; MLPt() denotes a MLP module; and denoteschannel-wise multiplication. The sound s1 at this incremental step can be reconstructed by applyingS M1t and then performing an inverse STFT to obtain the audio waveform.",
  "that serves two crucial purposes (1) maintaining cross-modal semantic similarity through incrementaltasks, and (2) preserving previous learned semantic similarity knowledge from old tasks": "CrossSDC preserves cross-modal semantic similarity from two perspectives: instance-aware semanticsimilarity and class-aware semantic similarity. Both similarities are enforced by integrating contrastiveloss and knowledge distillation. Instead of exclusively focusing on the similarities within currentand memory data generated by the current training model, CrossSDC incorporates the cross-modalsimilarity knowledge acquired from previous tasks into the contrastive loss. This integration notonly facilitates the learning of cross-modal semantic similarities in new tasks but also ensures thepreservation of previously acquired knowledge. In the incremental step t (t > 1), the instance-awarepart of our CrossSDC can be formulated as:",
  ",(7)": "where 1[i = j] is an indicator that equals 1 when i = j, denoting that video samples V i and V j arethe same video; The sim function represents the cosine similarity function with temperature scaling;The modalities mod1 and mod2, where (mod1, mod2) {(a, o), (a, m), (m, o)}, denote differentpairs of features to be compared: separated sound and object features, sound and motion features,and motion and object features. Here, denotes the incremental step, for which we have:",
  ", 2 T, where T ={t, t 1},if V Mt,{t},if V Dt,(8)": "which means that, for current tasks data Dt, we calculate the contrastive loss using features fromthe current model (1 = 2 = t), while for memory set data Mt, we use features from both theold and current models (e.g., 1 = t and 2 = t 1). In this way, knowledge distillation would beintegrated into the cross-modal semantic similarity constraint for the current task, which ensuresbetter preservation of learned cross-modal semantic similarity from previous tasks. While the instance-aware similarity provides valuable semantic correlation modeling, it does notaccount for the class-level semantic correlations, which is also crucial for audio-visual similaritymodeling. To capture and preserve the semantic similarity within each class across incrementaltasks, we also incorporate a class-aware component specifically designed for inter-class cross-modalsemantic similarity, which can be formulated as:",
  "Overall Loss Function": "In the previous subsection, we introduced our proposed CrossSDC constraint. To effectively combineCrossSDC with the overall objective, we incorporate it alongside output distillation and the mainseparation loss function. Output distillation is a widely used technique in continual learning to preserve theknowledge gained from previous tasks while learning new ones. In our approach, we utilize theoutput of the old model as the distillation target to preserve this knowledge. Note that we only distillknowledge for data from the memory set, as represented by:",
  "Management of Memory Set": "In alignment with the work of , our framework maintains a compact memory set throughoutincremental updates. Each old class is limited to a maximum number of exemplars. After completingtraining for each task, we adopt the exemplar selection strategies in by randomly selectingexemplars for each current class and combining these new exemplars with the existing memory set.",
  "Experiments": "In this section, we first introduce the setup of our experiments, i.e., dataset, baselines, evaluationmetrics, and the implementation details. After that, we present the experimental results of ourContAV-Sep compared to the baselines, as well as ablation studies. We also conduct experiments onthe AVE and the VGGSound datasets, which contain sound categories beyond the musicdomain. We put the experimental results on the AVE and the VGGSound datasets, the comparison tothe uni-modal semantic similarity preservation method, the performance evaluation on old classes inincremental tasks, and the visualization of separating results in the Appendix.",
  "Experimental Setup": "Dataset. Following common practice , we conducted experiments on MUSIC-21 ,which contains solo videos of 21 instruments categories: accordion, acoustic guitar, cello, clarinet,erhu, flute, saxophone, trumpet, tuba, violin, xylophone, bagpipe, banjo, bassoon, congas, drum,electric, bass, guzheng, piano, pipa, and ukulele. In our experiments, we randomly selected 20 ofthem to construct the continual learning setting. Specifically, we split the selected 20 classes into 4incremental tasks, each of which involves 5 classes. The total number of available videos is 1040,and we randomly split them into training, validation, and testing sets with 840, 100, and 100 videos,respectively. To further validate the efficacy of our method across a broader sound domain, weconduct experiments using the AVE and the VGGSound datasets in the appendix. Baselines. We compare our proposed approach with vanilla Fine-tuning strategy, and continual learn-ing methods EWC and LwF . As we mentioned before, typical continual learning methods,e.g., class-incremental learning methods, which yield progressively increasing logits (or probabilitydistribution) across all observed classes at each incremental step and design specific technique inthe classifier, we consider that these methods are not an optimal choice for our proposed continualaudio-visual sound separation problem. Thus, considering that continual semantic segmentationproblem has a more similar form compared to conventional class-incremental learning, we alsoselect two state-of-the-art continual semantic segmentation methods PLOP and EWF asour baselines. Moreover, we compare our method to the recently proposed audio-visual continuallearning method AV-CIL , in which we adapt the original class-incremental version to the form ofcontinual audio-visual sound separation by replacing their task-wise logits distillation with the outputmask distillation. Further, we also present the experimental results of the Oracle/Upper Bound, whichmeans that using the training data from all seen classes to train the model. For fair comparison, allcompared continual learning methods and our ContAV-Sep use the same state-of-the-art sepa-rator, i.e. iQuery , as the base separation model. Further, we also incorporate our proposed andbaseline methods into another representative audio-visual sound separation model Co-Separation .Notably, the Co-Separation model does not utilize the motion modality. Therefore, when CrossSDC isapplied to Co-Separation, the (mod1, mod2) in Eq. 7 and 9 is constrained to (mod1, mod2) = (a, o).For baselines that involve memory sets, we ensure that each of them is allocated the same number ofmemory as our proposed method for fair comparison.",
  "MethodSDRSIRSARMethodSDRSIRSAR": "w/o memoryw/o memoryiQuery + Fine-tuning3.469.3010.57Co-Sep. + Fine-tuning1.938.759.75iQuery + LwF 3.458.7810.66Co-Sep. + LwF 2.327.8410.28iQuery + EWC 3.679.5810.30Co-Sep. + EWC 2.018.369.61iQuery + PLOP 3.8210.0610.22Co-Sep. + PLOP 3.249.179.59iQuery + EWF 3.989.6811.52Co-Sep. + EWF 2.617.7710.85 w/ memoryw/ memoryiQuery + LwF 6.7612.7712.60Co-Sep. + LwF 3.859.6210.74iQuery + EWC 6.6513.0111.73Co-Sep. + EWC 3.319.559.80iQuery + PLOP 7.0313.3011.90Co-Sep. + PLOP 3.889.929.99iQuery + EWF 5.3511.3511.81Co-Sep. + EWF 3.639.0710.58iQuery + AV-CIL 6.8613.1312.31Co-Sep. + AV-CIL 3.619.769.68ContAV-Sep (with iQuery )7.3313.5513.01ContAV-Sep (with Co-Sep. )4.0610.0611.07",
  "Upper Bound (with iQuery)10.3616.6414.68Upper Bound (with Co-Sep.)7.3014.3411.90": "the Hann window size of 1022 and the hop length of 256, to obtain the 512 256 Time-Frequencyrepresentation of each audio signal, followed by a re-sampling on the log-frequency scale to generatethe magnitude spectrogram with T, F = 256. We set the video frame rate (FPS) to 1, and detect theobject using the pre-trained universal detector Detic to detect the sound source object on eachframe, and then, each detected object is resized and randomly cropped to the size of 224 224. Forthe image encoder and the video encoder, we apply the self-supervised pre-trained CLIP andVideoMAE to yield the object feature and motion feature, respectively. For the audio-visualTransformer module, we follow the design in . For all the baseline methods, we apply the samemodel architecture and modules with ours for them, including the mentioned Detic, CLIP, VideoMAE,audio-visual Transformer, etc. Please note that, during our training process, the pre-trained Detic,CLIP, and VideoMAE are frozen. In our proposed Cross-modal Similarity Distillation Constraint(CrossSDC), the balance weights ins and cls are set to 0.1 and 0.3, respectively. And the balanceweight dist. for the output distillation loss is set to 0.3 in our experiments. For the memory set, weset the number of samples in each old class to 1, so as other baselines that involve the memory set. Allthe experiments in this paper are implemented by Pytorch . We train our proposed method and allbaselines on a NVIDIA RTX A5000 GPU. We follow previous works in sound separation,and evaluate the performance of all the methods using three common metrics in sound separationtasks: Signal to Distortion Ratio (SDR), Signal to Interference Ratio (SIR), and Signal to ArtifactRatio (SAR). The SDR measures the interference and artifacts, while SIR and SAR measure theinterference and artifacts, respectively. In our experiments, we report the SDR, SIR, and SAR of allthe methods after training at last incremental steps, i.e., testing results on all classes. For all thesethree metrics, higher values denote better results.",
  "Experimental Comparison": "The main experimental comparisons are shown in Tab. 1. Our proposed method, ContAV-Sep,outperforms the state-of-the-art baselines by a substantial margin. Notably, compared to baselinesusing state-of-the-art audio-visual sound separator iQuery as the separation base model, ContAV-Sep achieves a 0.3 improvement in SDR over the compared best-performing method. Additionally,our method surpasses the top baseline by 0.25 in SIR and 0.41 in SAR. Furthermore, comparedto continual learning baselines with Co-Separation , our ContAV-Sep still outperforms otherapproaches. This consistent superior performance across different model architectures highlights notonly the effectiveness but also the broad applicability and generalizability of our proposed CrossSDC. Our observations further demonstrate that retaining a small memory set significantly enhances theperformance of each baseline method. For instance, for the iQuery-based continual learning methods,equipping LwF with a small memory set results in improvements of 3.31, 3.99, and 1.94 onSDR, SIR, and SAR, respectively. Similarly, the addition of a small memory set to EWC leadsto enhancements of 2.98, 3.43, and 1.43 in the respective metrics. The memory-augmented version ofPLOP exhibits superior performance with margins of 3.21, 3.24, and 1.68 for SDR, SIR, andSAR, respectively. Finally, incorporating memory into EWF results in improvements of 1.37,",
  ": Testing results of different continual learning methods with iQuery on the metrics of(a) SDR, (b) SIR, and (c) SAR at each incremental step": "1.67, and 0.29 for the three metrics. This phenomenon can be attributed to the inherent nature ofthe sound separation training process. In training, the audio signal from each sample mixes withothers, giving a composite audio signal. This mixed audio signal, coupled with the correspondingvisual data pair for each separated audio, constitutes the actual training sample for the separation task.As a result, even a single memory sample can be associated with multiple samples from the currenttraining set, generating a diverse array of effective training pairs. We also present the testing results of SDR, SIR, and SAR at each incremental step in Figures 3a, 3b,and 3c, respectively. Our method is consistently observed to outperform others in terms of SDR at allincremental steps. While our approach may not always produce the best SIR and SAR results at theintermediate steps (specifically, steps 2 and 3 for SIR, and step 3 for SAR), it ultimately achieves thehighest performance at the final step. This demonstrates the robustness of our method, indicatingminimal forgetting throughout the incremental learning process.",
  "Ablation Study on CrossSDC and Memory Size": "In this subsection, we conduct an ablation study to investigate the effectiveness of our proposedCrossSDC. By removing single or multiple components of the CrossSDC, we evaluate the impact ofeach on the final results. The results of the ablation study are presented in Tab. 2. From the table, wecan see that our full model achieves the best performance compared to the variants, which furtherdemonstrates the effectiveness of our proposed CrossSDC. Moreover, we also discuss the effect of memory size on our proposed ContAV-Sep. In our mainexperiments, the default setting of the memory size is 1 sample per old class. In this subsection, weconduct experiments by increasing the memory size from 1 sample per old class to 30 samples perold class. The experimental results are shown in Tab. 3 and . Observations from the tableindicate a positive correlation between the size of the memory and the overall performance metrics.As the memory size increases, there is a discernible trend of improvement in the results.",
  "Limitation and Discussion": "Our experimental findings reveal that the utilization of a small memory set, even a single sampleper old class, markedly improves the performance of each baseline method. This improvement isattributed to the ability of a single memory sample to pair with diverse samples from the currenttraining set, thereby generating numerous effective training pairs. Consequently, this process enablesthe model to acquire new knowledge for old classes in subsequent tasks, as the memory data can be",
  ": Testing results with different memory size (number of samples per class in the memory) onthe metrics of (a) SDR, (b) SIR, and (c) SAR at each incremental step": "paired with data from previously unseen new classes this is different from conventional continuallearning tasks, where old classes do not acquire new knowledge in new tasks. This could be apotential reason why the baseline continual learning methods do not perform well in our continualaudio-visual sound separation problem. In this work, our method also mainly focuses on preservingold knowledge of old tasks, which may prevent the model from acquiring new knowledge of oldclasses when training in new tasks. Recognizing this, we identify the exploration of this problem as akey avenue for future research in this field. Additionally, the base model architectures used in our approach and baselines require object detectorsto identify sounding objects. Although iQuery can supplement object features with global videorepresentations, it may still suffer from undetected objects. It is a fundamental limitation of theobject-based audio-visual sound separators . While our work, unlike previous efforts, doesnot compete on designing a stronger audio-visual separation base model, enhancing the robustness ofsounding object detection presents a promising direction for future research.",
  "Conclusion": "In this paper, we explore training audio-visual sound separation models under a more practicalcontinual learning scenario, and introduce the task of continual audio-visual sound separation. Toaddress this novel problem, we propose ContAV-Sep, which incorporates a Cross-modal SimilarityDistillation Constraint to maintain cross-modal semantic similarity across incremental tasks whilepreserving previously learned semantic similarity knowledge. Experiments on the MUSIC-21 datasetdemonstrate the effectiveness of our method in this new continual separation task. This paper opens anew direction for real-world audio-visual sound separation research. Broader Impact. Our proposed continual audio-visual sound separation allows the model to adapt tonew environments and sounds without full retraining, which could enhance efficiency and privacy byreducing the need to transmit and store sensitive audio data. Acknowledgments. We thank the anonymous reviewers and area chair for their valuable suggestionsand comments. This work was supported in part by a Cisco Faculty Research Award, an AmazonResearch Award, and a research gift from Adobe. The article solely reflects the opinions andconclusions of its authors but not the funding agents.",
  "Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. The conversation: Deepaudio-visual speech enhancement. arXiv preprint arXiv:1804.04121, 2018": "Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim, and Taesup Moon. SS-IL: separated softmax for incremental learning. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 824833, 2021. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuyte-laars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the Europeanconference on computer vision (ECCV), pages 139154, 2018.",
  "Albert S Bregnian. Auditory scene analysis: Hearing in complex environments. Thinking inSounds, pages 1036, 1993": "Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone Calderara. Darkexperience for general continual learning: a strong, simple baseline. Advances in neuralinformation processing systems, 33:1592015930, 2020. Francisco M Castro, Manuel J Marn-Jimnez, Nicols Guil, Cordelia Schmid, and KarteekAlahari. End-to-end incremental learning. In Proceedings of the European Conference onComputer Vision (ECCV), pages 233248, 2018. Sungmin Cha, Sungjun Cho, Dasol Hwang, Sunwon Hong, Moontae Lee, and Taesup Moon.Rebalancing batch normalization for exemplar-based class-incremental learning. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2012720136,2023. Moitreya Chatterjee, Narendra Ahuja, and Anoop Cherian. Learning audio-visual dynamicsusing scene graphs for audio source separation. Advances in Neural Information ProcessingSystems, 35:1697516988, 2022. Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan, Puneet KDokania, Philip HS Torr, and MarcAurelio Ranzato. On tiny episodic memories in continuallearning. arXiv preprint arXiv:1902.10486, 2019. Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: A large-scaleaudio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics,Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. Jiaben Chen, Renrui Zhang, Dongze Lian, Jiaqi Yang, Ziyao Zeng, and Jianbo Shi. iquery:Instruments as queries for audio-visual sound separation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1467514686, 2023.",
  "Soo-Whan Chung, Soyeon Choe, Joon Son Chung, and Hong-Goo Kang. Facefilter: Audio-visual speech separation using still images. arXiv preprint arXiv:2005.07074, 2020": "Arthur Douillard, Yifu Chen, Arnaud Dapogny, and Matthieu Cord. Plop: Learning withoutforgetting for continual semantic segmentation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 40404050, 2021. Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet:Pooled outputs distillation for small-tasks incremental learning. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16,pages 86102. Springer, 2020. Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim,William T Freeman, and Michael Rubinstein. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619, 2018. Enrico Fini, Victor G Turrisi Da Costa, Xavier Alameda-Pineda, Elisa Ricci, Karteek Alahari,and Julien Mairal. Self-supervised models are continual learners. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96219630, 2022.",
  "Chao Huang, Susan Liang, Yapeng Tian, Anurag Kumar, and Chenliang Xu. Davis: High-qualityaudio-visual separation with generative diffusion models. arXiv preprint arXiv:2308.00122,2023": "Ching-Yi Hung, Cheng-Hao Tu, Cheng-En Wu, Chien-Hung Chen, Yi-Ming Chan, and Chu-Song Chen. Compacting, picking and growing for unforgetting continual learning. Advances inNeural Information Processing Systems, 32, 2019. Minsoo Kang, Jaeyoo Park, and Bohyung Han. Class-incremental learning by knowledgedistillation with adaptive feature consolidation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1605016059, 2022.",
  "Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continuallearning of language models. In International Conference on Learning Representations (ICLR),2023": "Sanghwan Kim, Lorenzo Noci, Antonio Orvieto, and Thomas Hofmann. Achieving a betterstability-plasticity trade-off via auxiliary networks in continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1193011939,2023. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins,Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academyof Sciences, 114(13):35213526, 2017. Saksham Singh Kushwaha. Analyzing the effect of equal-angle spatial discretization on soundevent localization and detection. In Proceedings of the 7th Detection and Classification ofAcoustic Scenes and Events 2022 Workshop (DCASE2022), 2022.",
  "Jaewoo Lee, Jaehong Yoon, Wonjae Kim, Yunji Kim, and Sung Ju Hwang. Lifelong audio-videomasked autoencoder with forget-robust localized alignments. arXiv preprint arXiv:2310.08204,2023": "Kibok Lee, Kimin Lee, Jinwoo Shin, and Honglak Lee. Overcoming catastrophic forgettingwith unlabeled data in the wild. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 312321, 2019. Xilai Li, Yingbo Zhou, Tianfu Wu, Richard Socher, and Caiming Xiong. Learn to grow: Acontinual structure learning framework for overcoming catastrophic forgetting. In InternationalConference on Machine Learning, pages 39253934. PMLR, 2019.",
  "Haoxin Ma, Jiangyan Yi, Jianhua Tao, Ye Bai, Zhengkun Tian, and Chenglong Wang. Continuallearning for fake audio detection. arXiv preprint arXiv:2104.07286, 2021": "Divyam Madaan, Jaehong Yoon, Yuanchun Li, Yunxin Liu, and Sung Ju Hwang. Representa-tional continuity for unsupervised continual learning. In International Conference on LearningRepresentations (ICLR), 2022. Fei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang, and Boi Faltings. Continual learning fornatural language generation in task-oriented dialog systems. In Findings of the Association forComputational Linguistics: EMNLP 2020, volume EMNLP 2020, pages 34613474, 2020. Shentong Mo, Weiguo Pian, and Yapeng Tian. Class-incremental grouping network for continualaudio-visual learning. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 77887798, 2023. Shentong Mo and Yapeng Tian. Audio-visual grouping network for sound localization frommixtures. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1056510574, 2023. Xing Nie, Shixiong Xu, Xiyan Liu, Gaofeng Meng, Chunlei Huo, and Shiming Xiang. Bilateralmemory consolidation for continual learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1602616035, 2023.",
  "Andrew Owens and Alexei A Efros. Audio-visual scene analysis with self-supervised multisen-sory features. In Proceedings of the European conference on computer vision (ECCV), pages631648, 2018": "Jaeyoo Park, Minsoo Kang, and Bohyung Han. Class-incremental learning for action recognitionin videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 1369813707, 2021. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperativestyle, high-performance deep learning library. Advances in neural information processingsystems, 32, 2019.",
  "Weiguo Pian, Shentong Mo, Yunhui Guo, and Yapeng Tian. Audio-visual class-incrementallearning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages77997811, 2023": "Ameya Prabhu, Philip HS Torr, and Puneet K Dokania. Gdumb: A simple approach thatquestions our progress in continual learning. In Computer VisionECCV 2020: 16th EuropeanConference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 524540.Springer, 2020. Senthil Purushwalkam, Pedro Morgado, and Abhinav Gupta. The challenges of continuousself-supervised learning. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv,Israel, October 2327, 2022, Proceedings, Part XXVI, pages 702721. Springer, 2022. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL:incremental classifier and representation learning. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pages 20012010, 2017. Olaf Ronneberger, Philipp Fischer, and Thomas Brox.U-net: Convolutional networksfor biomedical image segmentation. In Medical Image Computing and Computer-AssistedInterventionMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,2015, Proceedings, Part III 18, pages 234241. Springer, 2015. Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Fine-tuned language models arecontinual learners. In Proceedings of the 2022 Conference on Empirical Methods in NaturalLanguage Processing, pages 61076122, 2022.",
  "Elyse S Sussman. Integration and segregation in auditory scene analysis. The Journal of theAcoustical Society of America, 117(3):12851298, 2005": "Reuben Tan, Arijit Ray, Andrea Burns, Bryan A Plummer, Justin Salamon, Oriol Nieto, BryanRussell, and Kate Saenko. Language-guided audio-visual source separation via trimodalconsistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1057510584, 2023. Yu-Ming Tang, Yi-Xing Peng, and Wei-Shi Zheng. Learning to imagine: Diversify memoryfor incremental learning using unlabeled data. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 95499558, 2022. Yapeng Tian, Di Hu, and Chenliang Xu. Cyclic co-learning of sounding object visual groundingand sound separation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 27452754, 2021. Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual eventlocalization in unconstrained videos. In Proceedings of the European conference on computervision (ECCV), pages 247263, 2018. Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders aredata-efficient learners for self-supervised video pre-training. Advances in neural informationprocessing systems, 35:1007810093, 2022. Efthymios Tzinis, Scott Wisdom, Aren Jansen, Shawn Hershey, Tal Remez, Daniel PW Ellis,and John R Hershey. Into the wild with audioscope: Unsupervised audio-visual separation ofon-screen sounds. arXiv preprint arXiv:2011.01143, 2020. Efthymios Tzinis, Scott Wisdom, Tal Remez, and John R Hershey. Audioscopev2: Audio-visualattention architectures for calibrated open-domain on-screen sound separation. In EuropeanConference on Computer Vision, pages 368385. Springer, 2022. Fu-Yun Wang, Da-Wei Zhou, Han-Jia Ye, and De-Chuan Zhan. FOSTER: feature boosting andcompression for class-incremental learning. In In Proceedings of the European Conference onComputer Vision (ECCV), pages 398414, 2022. Yu Wang, Nicholas J Bryan, Mark Cartwright, Juan Pablo Bello, and Justin Salamon. Few-shot continual learning for audio classification. In ICASSP 2021-2021 IEEE InternationalConference on Acoustics, Speech and Signal Processing (ICASSP), pages 321325. IEEE, 2021. Zhepei Wang, Cem Subakan, Xilin Jiang, Junkai Wu, Efthymios Tzinis, Mirco Ravanelli, andParis Smaragdis. Learning representations for new sound classes with continual self-supervisedlearning. IEEE Signal Processing Letters, 29:26072611, 2022. Zhepei Wang, Cem Subakan, Efthymios Tzinis, Paris Smaragdis, and Laurent Charlin. Continuallearning of new sound classes using generative replay. In 2019 IEEE Workshop on Applicationsof Signal Processing to Audio and Acoustics (WASPAA), pages 308312. IEEE, 2019. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su,Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages139149, 2022.",
  "Da-Wei Zhou, Yuanhan Zhang, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.Learning without forgetting for vision-language models. arXiv preprint arXiv:2305.19270,2023": "Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krhenbhl, and Ishan Misra. Detectingtwenty-thousand classes using image-level supervision. In European Conference on ComputerVision, pages 350368. Springer, 2022. Lingyu Zhu and Esa Rahtu. Visually guided sound source separation and localization usingself-supervised motion representations. In Proceedings of the IEEE/CVF Winter Conference onApplications of Computer Vision, pages 12891299, 2022.",
  "AAppendix": "In this appendix, we present a more detailed model architecture in section A.1. Following that, weshow the experimental comparison to the uni-modal baseline to prove the effectiveness of cross-modal similarity modeling and preservation in section A.2. After that, in section A.3, we conductexperiments on the AVE and the VGGSound datasets, which includes a broader range ofaudio-visual data beyond the music domain. Furthermore, in section A.4, we present the performanceon old classes of our proposed method and baselines, to further prove that our method can bettermitigate the catastrophic forgetting problem compared to baselines. Finally, we offer additionalvisualization results in section A.5, to further demonstrate that our method can better handle thecatastrophic forgetting problem in the proposed Continual Audio-Visual Sound Separation taskcompared to other continual learning baseline methods.",
  "A.1Model Architecture": "Audio Network.Following , we use the U-Net framework as the architecture of our audionetwork. In our experiments, the audio network has 7 down-convolutions and 7 up-convolutions. Ittakes a 2D Time-Frequency spectrum with size of 1 256 256 as an input to generate the latentrepresentation with a size of 256 32 32 through the encoder part. Finally, it outputs the audioembedding with a size of 32 256 256 through the decoder. Audio-Visual Transformer.We follow the implementation in , and use the Transformerdecoder architecture as our audio-visual Transformer. The audio-visual Transformer consists of4 decoder layers, with the first layer being the motion cross-attention layer and the following 3layers performing audio cross-attention and self-attention operation. The audio-visual Transformergenerates the separated audio feature with a dimension of 256, followed by a two-layers MLP toobtain the mask embedding with a dimension of 32. Then, the channel-wise multiplication is appliedbetween the generated mask embedding and the audio embedding to obtain the predicted mask M. Video Encoder & Object Detector & Object Encoder.For the pre-trained video encoder, objectdetector, and object encoder, we use the VideoMAE , Detic , and CLIP , respectively.These models are frozen during the training process and the input size, out size, and internal featuredimensions remain the same as in their original implementations.",
  "A.2Compared to Uni-modal Baseline": "To evaluate the superiority of cross-modal similarity modeling and preservation in continual audio-visual sound separation, in this subsection, we constructed a variant of our ContAV-Sep, in whichwe modify our proposed CrossSDC to the intra-modal similarity distillation version. We name it asContAV-Sep-intra. The experimental results are shown in Tab. 4. We can see that, our ContAV-Sepoutperforms the variant significantly, further validating the effectiveness of modeling and preservingcross-modal similarity.",
  "A.3Experiments on the AVE and the VGGSound datasets": "To further evaluate the efficacy of our proposed method across a broader sound domain, we conductexperiments using the AVE and the VGGSound datasets. In the experiments on the AVEdataset, we randomly split the 28 classes in the AVE dataset into 4 tasks, each of which contains7 classes. The results are presented in Tab. 5, in which our ContAV-Sep outperforms the baselinemodels in terms of the SDR and SIR metrics, further demonstrating the robustness of our methodbeyond the domain of musical sounds. However, it was noted that both our method and the upper bound exhibit relatively low SAR scores when compared to the baselines. Gao et al. providean interpretation for this phenomenon, explaining that the SAR primarily captures the absence ofartifacts. Therefore, it can remain high even when the separation quality is suboptimal. In contrast,the SDR and SIR metrics are used to evaluate the accuracy of the separation. For the experimentson the VGGSound dataset, we follow and randomly selecting 100 classes for continuallearning. These classes are divided into 4 tasks, each containing 25 classes. Given the significantlylarger number of samples per class in the VGGSound dataset, we set the memory size to 20 samplesper class for methods that utilize memory. The experimental results, shown in Tab. 6, demonstratethat our proposed ContAV-Sep consistently outperforms the baseline methods on the VGGSounddataset in the context of continual audio-visual sound separation."
}