{
  "Abstract": "Contrastive Language-Image Pretraining (CLIP) models maximize the mutual in-formation between textual and visual modalities to learn representations. However,the lack of compositional diversity in contemporary image-text datasets limits thecompositional reasoning ability of CLIP. We show that generating hard negativecaptions via in-context learning and synthesizing corresponding negative imageswith text-to-image generators offers a solution. We introduce a novel contrastivepre-training strategy that leverages these hard negative captions and images inan alternating fashion to train CLIP. We demonstrate that our method, namedTripletCLIP, when applied to existing datasets such as CC3M and CC12M,enhances the compositional capabilities of CLIP, resulting in an absolute improve-ment of over 9% on the SugarCrepe benchmark on an equal computational budget,as well as improvements in zero-shot image classification and image retrieval. Ourcode, models, and data are available at: tripletclip.github.io.",
  "Introduction": "Large-scale vision-language models, such as CLIP , have significantly advanced multi-modallearning by employing contrastive learning to acquire shared semantic representations from paireddatasets. This approach has resulted in improved performance in vision-language tasks as well aszero-shot image classification and segmentation . Beyond vision-language tasks, theindividual components of these models, such as the vision encoder and the language encoder, areintegral to several multimodal architectures and generative models such as multimodal large languagemodels (MLLMs) and text-to-image (T2I) diffusion models . Yet, compositionalreasoning remains challenging and multimodal models continue to exhibit nave bag of wordsbehavior, frequently failing to distinguish between expressions like bulb in the grass and grass inthe bulb . Addressing this challenge remains critical for enhancing vision-language modelsand their downstream applications. Contrastive learning of representations benefits from hard negative samples (i.e., points that aredifficult to distinguish from an anchor point) . However, at each optimization step for trainingCLIP, image-text pairs are randomly sampled from the training dataset this random sampling seldomexposes the model to highly similar negative pairs. We hypothesize that the limited compositionalunderstanding of CLIP may stem from such issues in the optimization objective and sampling fromtraining datasets. A straightforward solution could involve iteratively identifying hard negative pairsfor each training iteration. However, due to the noisy captions and the scarcity of such pairs in existingdatasets, prior work generates hard negative captions as a form of augmentation using rule-basedstrategies . For instance, given an image-text pair labeled a brown horse, an additional",
  "arXiv:2411.02545v1 [cs.CV] 4 Nov 2024": "negative caption a blue horse might be introduced. However in prior work, image data is notsubjected to similar hard negative semantic augmentation during training; this is mainly because ofthe difficulty of making semantic perturbations at the pixel levels compared to sentence perturbation.While the text-only augmentation strategies have improved the models compositional understandingto a certain extent, it raises an intriguing question: could incorporating hard negative augmentationfor both text and image modality further enhance the compositional reasoning capabilities ofvision-language models? Motivated by this, in this paper, we introduce a novel, simple, and yet highly effective strategyfor integrating hard negative images as well as hard negative text to enhance the compositionalunderstanding of vision-language models. Recent developments in text-to-image diffusion modelshave opened up possibilities for performing semantic perturbations within images . Existingworks have evaluated the impact of creating synthetic data for text-to-image generative models .However, it remains less explored how these generative models can benefit the CLIP-like models.To tackle this challenge, our approach leverages the in-context learning capabilities of LLMs toproduce realistic, linguistically accurate negative captions . We then employ a pre-trained text-to-image diffusion model to create images corresponding to these captions, thereby enriching anygiven image-text dataset with valuable hard negatives that foster improved reasoning. This resultingTripletData comprises 13M image-text pairs to complement the CC3M and CC12M datasets . We developed TripletCLIP, which incorporates hard negative image-text pairs effectively by usingthem to optimize a novel triplet contrastive loss function. Extensive experiments on the CC3Mand CC12M datasets and various downstream tasks with an equal compute budget demonstrate thatTripletCLIP significantly enhances compositional reasoning. Notably, TripletCLIP results inmore than 9% and 6% absolute improvement on the SugarCrepe benchmark compared to LaCLIP and NegCLIP , respectively. TripletCLIP also improves zero-shot classification and image-textretrieval performance with similar training-time concept diversity. An investigation into the effects ofincreasing training-time concept diversity revealed that baseline models consistently under-performedin compositional tasks despite an increase in integrated knowledge, while TripletCLIP demonstratedsignificant improvements. In summary, our key contributions are as follows:",
  "TripletCLIP consistently improves across downstream tasks, demonstrating the effectiveness ofsynthesizing hard negative image-text pairs": "Our extensive ablations on the choice of the loss function, modality-specific pre-training, theincrease in concept diversity, and filtering high-quality TripletData provide deeper insights intothe utility of hard negative image-text pairs for CLIP pre-training. Ultimately, we present a promising avenue where synthetic contrastive datasets significantlyimprove reasoning capabilities, leading to the creation and release of the TripletData a 13Mcontrastive image-text dataset.",
  "Related Work": "Vision-Language Models. Recent advancements, including ALIGN and CLIP , havegained significant interest due to their capability to learn transferable semantic representations acrossmultiple modalities through contrastive learning. These models facilitate downstream tasks suchas zero-shot classification , image-text retrieval , visual grounding/reasoning , text-to-image generation , semantic segmentation , and various evaluations . Subsequent research has sought to enhance various aspects of these models, including dataefficiency , hierarchical representation learning , and the quantization of latent spaces formore stable pre-training . LiT employs a pre-trained frozen CLIP vision encoder to fine-tunea BERT-like text encoder , achieving notable improvements in zero-shot transfer performance.Similarly, BLIP-2 combines contrastive pre-training with the next-token prediction for imagecaptioning during training. However, these approaches generally presume the availability of high-quality data. In contrast, TripletCLIP focuses on leveraging the proposed hard negative contrastivedataset and incorporating triplet contrastive pre-training for compositional data. This approach isorthogonal to prior works.",
  ": Comparison of training workflows of CLIP, NegCLIP, and TripletCLIP. (x, y) representsthe positive a image-text pair, and (x, y) represents the corresponding negative image-text pair": "Data for Contrastive Pre-training. The effectiveness of maximizing mutual information betweenmodalities heavily relies on the quality of extensive, web-scraped datasets that ideally encompassall possible concepts and knowledge. For instance, despite its noise, the LAION dataset ,which includes more than 5 billion internet images paired with alt-text captions, is a primary resource.Studies show that over 1 billion data points are necessary to match the performance of the originalCLIP model . Recent works like DataComp and MetaCLIP have focused on creatingsmaller, high-quality datasets by applying stringent filters and ensuring wordnet synset-levelconcept diversity. Nevertheless, the inherently noisy nature of internet-scraped datasets can degrademodel performance. Studies such as SynthCLIP demonstrate that tripling the volume of fullysynthetic data is required to equal the efficacy of real data. Other efforts like VeCLIP andLaCLIP enhance dataset quality by using generative language models to re-caption existingimages, significantly boosting performance. Compositionality for vision-language. Despite the increased emphasis on data quality and modelingtechniques, mastering compositionality remains a significant challenge for vision-language models.Benchmarks like ARO , VALSE , and CREPE have been developed to assess modelsabilities to handle compositional data. SugarCrepe , in particular, offers a large-scale, systematicframework for such evaluations. Previous methods primarily focused on identifying hard negativeswithin existing datasets or generating synthetic negative captions . However,these rule-based generated captions are often unrealistic and linguistically flawed, leading to subopti-mal model performance on complex datasets like SugarCrepe. A handful of works focus on findingnegative images. propose utilizing the video data. focuses on object-centric image-editingto synthesize the negative images. utilizes the simulation-based data negative data. Contrary to prior approaches that predominantly add unrealistic negative captions or very constrainednegative images that are either very synthetic or object-focused, this work introduces TripletCLIP,which centers on generating naturally occurring hard negative image-text pairs. We propose a noveltriplet contrastive learning strategy that effectively utilizes these challenging data pairs. Additionally,while our method is distinct, integrating advancements that refine contrastive learning could potentiallyboost TripletCLIPs efficacy further.",
  "Method": "This section begins with an overview of the contrastive learning algorithm used by CLIP and NegCLIP.We then describe the synthetic data generation pipeline for generating hard negatives using LLMsand T2I models and introduce triplet contrastive learning which forms the basis of TripletCLIP. Ahigh-level comparison between prior work and TripletCLIP can be found in .",
  "Preliminaries": "The goal for self-supervised contrastive learning , when dealing with inputs from a singlemodality, is to use a feature extractor (F) to encode inputs and their augmentations and minimizethe InfoNCE loss between the two encodings. CLIP is designed for multimodal settings (forexample, vision and language inputs) this entails using two encoders (one for each modality).",
  "i=1logexp(FX (xi), FY(yi)/)Nk=1 exp(FX (xi), FY(yk)/),(1)": "where represents cosine similarity and is the trainable temperature parameter. For simplicity,we do not show feature normalization in the InfoNCE loss. Similarly, we can define the LCLYXtraining loss. The combined CLIP total training objective is given as, LCLIP = LCLXY + LCLYX .By minimizing this training loss, both encoders learn representations that maximize the mutualinformation between two modalities. NegCLIP introduces synthetic augmentations to generate hard negative captions (yi Y) byperforming semantic inverting perturbations to the reference captions (yi Y). Therefore, the singlemodality-specific hard negative augmentation-based training loss can be formulated as:",
  "LNegCLIP (X, Y, Y) = LCLYX + LNegCLXY;Y.(3)": "In Eq. 2, the negative samples are generated only for language modality as it is easy to make semantic-level perturbations. Existing methods have not explored performing semantic perturbations in theimage modality to create hard negatives. In this work, we demonstrate how hard negatives can becreated in the image modality by leveraging the semantic language grounding and photorealism oftext-to-image diffusion models. Our novel hard negative generation pipeline and refined trainingobjective seeks to bridge the significant gap identified in literature.",
  "TripletData: Image-text hard negative data augmentations": "To generate high-quality hard negative image-text pairs, we follow a two-step procedure. The firststage is to generate hard negative captions from the ground truth positive caption. Second, to generate images corresponding to the hard negative captions as negative images. The AltText captions from theexisting web-scrapped datasets are very noisy, leading to the noisy and unreliable generation of hardnegatives. Therefore, we build upon the existing work LaCLIP, which first rewrites the captions usingLLM from the existing data that are linguistically accurate. illustrates several examples ofpositive and corresponding negative image-text pairs. Generating hard negative captions. Existing works perform random swapping, replacing, andadding actions between the nouns, attributes, and relations of the positive caption . Thismethod results in nonsensical and grammatically incorrect artifacts, such as a person riding onfour slope, which impedes the generation of negative images, ultimately leading to diminishingperformance on harder benchmarks . Therefore, we utilize the in-context learning ability ofLLMs to generate negative captions. The choice of LLM is a trivial task as long as they provide hardnegative captions. We find that Mistral-7B-Instruct-v0.22 performs reasonably better on our goal, andthe output is easy to parse. We generate the negative captions in batches to speed up the generationprocess. Generating the 13M negative captions takes only 3 days on 8xRTX A6000. Instead ofgenerating multiple hard negative captions, we find that a single high-quality hard negative caption isenough to improve the performance compared to the traditional NegCLIP style caption generation(see NegCLIP++ results in ). We provide examples of various types of negative captions inthe appendix. Specifically, we provide the following prompt to LLM: You are given the description of an image. You should provide a mostly similar description, changing theoriginal one slightly but introducing enough significant differences such that the two descriptions could notpossibly be for the same image. Keep the description length the same. Finally, only a few things (such ascounting, objects, attributes, and relationships) can be modified to change the image structure significantly.Provide just the updated description. Examples: Input: A dog to the left of the cat. Output: A dog to theright of the cat. Input: A person wearing a red helmet drives a motorbike on a dirt road. Output: A person ina blue helmet rides a motorbike on a gravel path. Now, do the same for the following captions:Input: {} Output: Generating hard negative images. Typically, semantic perturbations within images require toolslike image editing, which are resource-intensive and cannot be scaled. Remember that we want toprovide additional ground truth references for the negative caption. Therefore, we propose to utilizethe negative captions from the previous stage to generate the respective reference images directlyfor pre-training. As the previous stage generates negative captions that are linguistically correct, itbecomes easier for image-generative models to synthesize the respective images precisely. We utilizepre-trained text-to-image diffusion models to generate the corresponding images. Specifically, weselect SDXL-turbo due to its relatively faster generation speed. After applying various inferencetime optimizations, we can generate 13M negative images within 2 days using 30 v100 GPUs. Weprovide various examples of the hard negative image-text pairs in the appendix.",
  "# unique59094596166274155969# total synsets231M215M446M-": "Analyzing difficulty of the hard Triplet-Data. Lets assume we have positive and neg-ative image-text pairs from the TripletData ,(xi, yi) and (xi, yi), respectively. If the data istruly hard negative, existing pre-trained mod-els should struggle to find the correct image-text pairs (i.e., cos(xi, yi) > cos(xi, yi)). Fol-lowing winoground, we measure the text-score,image-score, and group-score to evaluate thepopular pretrained CLIP models. showsthat even CLIP models trained on billions ofdata struggle to get near human performanceon TripletData, which is less difficult thanwinoground. Importantly, the goal of generatinghard negative samples isnt to add more diversityin terms of unique concepts during the training but to add diversity in semantic meanings. Therefore,we measure the unique wordnet synsets in CC3M vs. TripletData. From , it can be observedthat TripletData does not add any new concepts but uses existing concepts to provide negative : Importance of image-text hard negatives. We measure the importance of various modality-specific hard negatives on SugarCrepe, image-text retrieval, and ImageNet1k. We find that Triplet-CLIP results into the most optimal solution. Bold number indicates the best performance.",
  "TripletCLIP": "Prior works have demonstrated the value of hard negative captions for enhancing the compositionalityof CLIP models via LNegCLIP as the key training objective (Eq. 2) . However, it remains elu-sive if negative images alone can benefit or not. We conduct modality-specific ablations, reporting theaverage performance across the diverse set of benchmarks in (we provide more details aboutexperiments in ). Our findings indicate that both hard negative captions and images indi-vidually boost performance when compared to LaCLIP. However, this initial empirical experiments totrain the CLIP model on hard negative images (i.e., NegImage) by minimizing LNegCLIP (Y, X, X )reveal that negative images alone cannot improve the compositionality significantly (see ). Wehypothesize that images contain low-level information, making it difficult to train the model usingimages as negative examples. Aligning with our initial motivation and building upon this crucialinsight, we propose to utilize the negative images to regularize the effect of negative captions and tostabilize the pre-training. Therefore, to utilize these hard negative image-text pairs from the previousstage more effectively, we propose to focus on two triplets (X, Y, Y) and (X , Y, Y), hence, thefinal triplet contrastive learning training objective is defined as:",
  "LT CL = LNegCLIP (X, Y, Y) + LNegCLIP (X , Y, Y).(4)": "Intuitively, the second term introduces the additional form of supervision that hard negative imagesare closer to the corresponding negative captions than positive captions. This allows the system tounderstand that if the positive image does not represent the negative caption blue horse, then whatdoes this caption entail? Through this strategic alternation of hard negative image-text pairs for theTripletCLIP, we improve compositionality and image-text understanding of the vision-languagemodel (see ). We provide the pseudo-code in the appendix and the code in supplementarymaterials. This simple yet effective strategy elevates the training of the CLIP, offering a scalableframework to improve overall performance.",
  "Experiment Setup": "Pretraining Datasets. We utilize the CC3M and CC12M datasets, which comprise 2.6M and 8.6Mimage-text pairs, respectively. Following the approach demonstrated by LaCLIP, we use LLM-rewritten captions to replace noisy original captions. For NegCLIP, we introduce four negativecaptions per positive image-text pair, focusing on semantic inverting perturbations across fourcategories: attribute, relation, object, and action . This generates approximately 10.4M and34.4M text-only augmentations for CC3M and CC12M, respectively. To train the TripletCLIP, wecreate augmentations (TripletData) for both datasets to integrate hard negatives effectively. Weproduce one augmentation per image-text pair, adding 2.6M and 8.6M image-text augmented pairsfor CC3M and CC12M, respectively. Finally, we perform all the ablations on the CC3M dataset. Baselines. We train LaCLIP, LaCLIP with real hard negatives (LaCLIP+HN), and NegCLIP fromscratch to ensure consistency and fairness in our comparisons. As NegCLIPs rule-based augmenta-tions closely resemble some compositional benchmarks, so we introduce NegCLIP++ as an improved : Composition evaluations of the methods on SugarCrepe benchmark. Bold numberindicates the best performance and underlined number denotes the second-best performance. represents the results taken from SugarCrepe benchmark.",
  "baseline. NegCLIP++ incorporates hard negative captions generated using LLM from TripletData,enhancing the language comprehension compared to standard NegCLIP": "Implementation Details. Our experiments employ the ViT-B/32 model architecture. Toguarantee fair comparisons, we retrain all baseline models using identical hyperparameters. Sincethe overall training data for NegCLIP and TripletData is more than the baseline datasets, we alignthe number of iterations across all models to equalize the number of image-text pairs seen duringtraining, similar to the strategy used in DataComp. The batch size is fixed to 1024 with the AdamWoptimizer at a maximum learning rate of 0.0005, employing cosine decay. Training durations are setat approximately 100k iterations for CC3M and 200k iterations for CC12M. All models are trainedon a single A100 (80GB) GPU using bf16 precision. The final training-related experiments andablations will cost about 1200 A100 GPU hours. We leave the experiments on increasing the data andmodel size as future works for the community, as scaling further is not viable in the academic budget. Downstream Datasets. The primary objective of this study is to enhance the compositional ca-pabilities of CLIP models. We mainly evaluate TripletCLIP and the baseline models using thechallenging SugarCrepe composition benchmark, with additional performance assessments providedin the appendix for older benchmarks. Models are also tested on image-text retrieval tasks for broaderevaluation using the Flickr30k and MSCOCO datasets. Zero-shot classification performanceis assessed across approximately 18 different datasets. Evaluations adhere to the methodologiesoutlined in the CLIP-Benchmark3 or the official benchmark implementations. : Ablation on filtering high-quality image-text pairs from TripletData. We evaluate theTripletCLIP after applying the filters to ensure the quality similar to DataComp and compare thebaselines on three benchmarks. We find that TripletCLIP results in the most optimal solution. Boldnumber indicates the best performance. represents that results are borrowed from DataComp.",
  "Compositional reasoning": "We comprehensively analyze the compositional understanding of models on the SugarCrepe bench-mark, as detailed in . Notably, TripletCLIP consistently outperforms all baseline modelsacross all sub-categories of SugarCrepe on both the CC12M/CC3M training datasets. Specifically,TripletCLIP surpasses LaCLIP and NegCLIP by 10.91%/9.4% and 6.45%/6.31% on the CC12Mand CC3M datasets, respectively. Our enhanced baseline, NegCLIP++, also shows improvement overstandard NegCLIP, highlighting the benefits of LLM-generated negatives. Nevertheless, Triplet-CLIP further advances performance, underscoring the critical role of hard negative image-text pairs,not just text. Additional comparisons on older composition benchmarks (Valse , Cola ,and Winoground ) in the appendix reveal TripletCLIPs consistent performance. alsocontrasts TripletCLIP with models trained using the DataComp approach, which involves moreparameters and training data, demonstrating that TripletCLIP achieves comparable performance toa ViT-B/16 model trained on 1 billion image-text pairs.",
  "Zero-shot evaluations": "Image-Text Retrieval. In , we summarize the performance of models on text-to-image(T2I) and image-to-text (I2T) retrieval tasks on MSCOCO and Flickr30k datasets, where we reportR@5 scores. Remarkably, TripletCLIP significantly outperforms baseline models by an averageof 8%/10% and 8%/12.5% on I2T and T2I tasks, respectively, on the CC3M and CC12M datasets.Intriguingly, while LaCLIP+HN performs better than NegCLIP, TripletCLIP outstrips both. Zero-shot Classification. also presents the average zero-shot classification performance on18 standard datasets, including ImageNet1k. TripletCLIP consistently enhances top-1 accuracy byan average of 3% and top-5 accuracy by 5-7% compared to LaCLIP. Like the retrieval performance,LaCLIP+HN exceeds NegCLIP, yet TripletCLIP maintains the highest performance. Dataset-specific results are in the appendix.",
  "Finetuning performance": "In this paper, we focus on pretraining-based experiments as they allow greater flexibility in learningbetter representations. To complement this, we also performed additional fine-tuning experimentsusing hyperparameters similar to the baselines (without LoRA) and compared them against variouspublicly available baselines . As reported in , TripletCLIP improves com-positionality and outperforms nearly all baselines. Furthermore, the observed drop in retrieval andzero-shot classification performance () is attributed to limitations in the vision encoder, high-lighting the challenges of existing pre-trained vision encoders in capturing semantic representations.This is further demonstrated in .",
  "Ablations": "Can a high-quality filtered dataset improve the performance? Given that negative images inTripletData are generated using SDXL-turbo, these may not always be precise. Inspired by Data-Comp, we employ a pre-trained CLIP-L/14 to filter the image-text pairs, selecting the highest averagesimilarity pairs (positive and negative) individually (i.e., score = (s(xi, yi) + s(xi, yi))/2). The top1.4M positive image-text pairs and their corresponding negatives from TripletData are selected. details this comparison against DataComp pre-trained models. Remarkably, TripletCLIP al-ready surpasses baselines without filtered data; however, with the filtered dataset, despite beingtrained on 50% smaller dataset, TripletCLIP++ shows further performance improvements. Thisunderlines the significant benefits of carefully selected TripletData in enhancing the performance. Which modality-specific encoder plays the key role in improving compositionality? To addressthis open question, we designed an ablation study similar to LiT, freezing either the pre-trainedCLIP vision or text encoder while training the opposite modality-specific encoder from scratch. Weobserve the performance of LaCLIP and TripletCLIP on CC3M, as shown in . Freezingthe vision model results in no performance gain on the SugarCrepe for TripletCLIP. However,significant improvements are noted when the vision encoder is actively trained, suggesting that thevision modality may be the bottleneck in compositionality. Notably, TripletCLIP outperformsLaCLIP in all settings, further demonstrating its robustness to different pre-training approaches. Concept coverage analysis. Improving performance on zero-shot transfer learning tasks such asretrieval involves two key components: adding more concept diversity during training and enhancingimage-text alignment/compositionality. We create subsets of CC12M data with increasing conceptdiversity based on unique WordNet synsets. Specifically, we select 3M, 4M, 5M, and 6M subsets for",
  "TripletCLIP (ours)92.2566.8264.30": "training LaCLIP, while TripletCLIP training involves only half of these training data as positivepairs, and the rest are corresponding augmentations. Evaluations across SugarCrepe, retrievaltasks, and ImageNet1k (see ) indicate that TripletCLIP not only enhances SugarCrepeperformance even at lower concept coverage levels but also significantly outperforms similar conceptcoverage in retrieval tasks, matching LaCLIPs performance on zero-shot classification tasks that donot require compositionality at all. This bolsters our argument that incorporating hard negatives fromboth modalities markedly improves compositional understanding in CLIP, while baseline struggles todo so even with more concept diversity. What if TripletData is used for large-scale compositional evaluations? We evaluated the CC12Mpre-trained models on a 50,000 random subset of the CC3M dataset using a Winoground-styleapproach . As shown in , TripletCLIP significantly improves performance compared tothe baselines. However, we partially attribute this improvement to spurious correlations learned fromthe data. At the same time, we note that the models have not fully converged, suggesting minimalrisk of overfitting to these spurious correlations.",
  "Conclusion": "In this work, we introduce TripletCLIP, a novel approach to enhancing compositional reasoning invision-language models through the strategic incorporation of hard negative image-text pairs. Ourcomprehensive experiments across a suite of benchmarks demonstrate that TripletCLIP significantlyoutperforms existing methodologies such as LaCLIP and NegCLIP, achieving notable gains not onlyin compositionality but also in zero-shot classification and retrieval tasks as well. Further, our ablationstudies highlight the critical role of modality-specific training and the careful curation of training data,underscoring the importance of both hard negative image and text components in the learning process.TripletCLIPs effectiveness with a smaller, refined dataset suggests a promising direction for futureresearchmaximizing performance without the need for extensive data collection, thereby reducingcomputational costs and enhancing model efficiency. To this end, we provide an intriguing applicationof synthetic datasets via hard negative image-text pairs for vision-language tasks that could be easilyextended to improve Multimodal Large Language Models and Text-to-Image generative models. Limitations.Due to constraints inherent in academic settings and limited computational resources,we were unable to scale TripletCLIP to handle hundreds of millions of image-text pairs or employlarger models within the scope of this study. Nevertheless, our results indicate a promising directionfor future research within a consistent experimental framework, and we encourage subsequent work toexplore scaling both the TripletData and TripletCLIP. Our experimental focus was primarily onthe CLIP and LiT methodologies. With additional resources, however, extending our methodologiesto more advanced contrastive learning techniques, such as SigLIP, would be feasible. In conclusion,our work introduces a compelling strategy for integrating open-ended hard negatives (both text andimage) during the pre-training phase, providing a methodology and large-scale data that could benefita variety of research domains. This work was supported by NSF RI grants #1750082, #2132724, and CPS grant #2038666. Wethank the Research Computing (RC) at Arizona State University (ASU) for providing computingresources. The views and opinions of the authors expressed herein do not necessarily state or reflectthose of the funding agencies and employers. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, MichaelRabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1561915629, 2023. Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Composed imageretrieval using contrastive learning and task-oriented clip-based features. ACM Transactions onMultimedia Computing, Communications and Applications, 20(3):124, 2023. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions.Computer Science. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski,and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedingsof the IEEE/CVF international conference on computer vision, pages 96509660, 2021. Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushingweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 35583568,June 2021. Agneet Chatterjee, Gabriela Ben Melech Stan, Estelle Aflalo, Sayak Paul, Dhruba Ghosh, TejasGokhale, Ludwig Schmidt, Hannaneh Hajishirzi, Vasudev Lal, Chitta Baral, et al. Gettingit right: Improving spatial consistency in text-to-image models. In European Conference onComputer Vision, pages 204222. Springer, 2024. Yuxiao Chen, Jianbo Yuan, Yu Tian, Shijie Geng, Xinyu Li, Ding Zhou, Dimitris N Metaxas,and Hongxia Yang. Revisiting multimodal representation in contrastive learning: from patchand token embeddings to finite discrete tokens. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1509515104, 2023. Karan Desai, Maximilian Nickel, Tanmay Rajpurohit, Justin Johnson, and Shanmukha Ra-makrishna Vedantam. Hyperbolic image-text representations. In International Conference onMachine Learning, pages 76947731. PMLR, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deepbidirectional transformers for language understanding. In Proceedings of the 2019 Conferenceof the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and Short Papers), pages 41714186, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, et al. Dense and alignedcaptions (dac) promote compositional reasoning in vl models. Advances in Neural InformationProcessing Systems, 36, 2024. Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, RogerioFeris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. Teaching structured vision &language concepts to vision & language models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pages 26572668, 2023. Linus Ericsson, Henry Gouk, Chen Change Loy, and Timothy M Hospedales. Self-supervisedrepresentation learning: Introduction, advances, and challenges. IEEE Signal ProcessingMagazine, 39(3):4262, 2022.",
  "Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving cliptraining with language rewrites. Advances in Neural Information Processing Systems, 36, 2024": "Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, andVaishaal Shankar. Data filtering networks. In The Twelfth International Conference on LearningRepresentations, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, ThaoNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp:In search of the next generation of multimodal datasets. Advances in Neural InformationProcessing Systems, 36, 2024. Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover.Cyclip: Cyclic contrastive language-image pretraining.Advances in Neural InformationProcessing Systems, 35:67046719, 2022.",
  "Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Philip Torr, Adel Bibi, andBernard Ghanem. Synthclip: Are we ready for a fully synthetic clip training? arXiv preprintarXiv:2402.01832, 2024": "Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: Areference-free evaluation metric for image captioning. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Processing, pages 75147528, 2021. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugar-crepe: Fixing hackable benchmarks for vision-language compositionality. Advances in NeuralInformation Processing Systems, 36, 2024. Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong,He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: A survey.arXiv preprint arXiv:2402.17525, 2024. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-HsuanSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learningwith noisy text supervision. In International conference on machine learning, pages 49044916.PMLR, 2021. Siyu Jiao, Yunchao Wei, Yaowei Wang, Yao Zhao, and Humphrey Shi. Learning mask-awareclip representations for zero-shot segmentation. Advances in Neural Information ProcessingSystems, 36:3563135653, 2023.",
  "Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang.Towards general text embeddings with multi-stage contrastive learning.arXiv preprintarXiv:2308.03281, 2023": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. InA. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances inNeural Information Processing Systems, volume 36, pages 3489234916. Curran Associates,Inc., 2023.",
  "Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastivepredictive coding. arXiv preprint arXiv:1807.03748, 2018": "Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, andAlbert Gatt. Valse: A task-independent benchmark for vision and language models centeredon linguistic phenomena. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pages 82538280, 2022. Maitreya Patel, Tejas Gokhale, Chitta Baral, and Yezhou Yang. Conceptbed: Evaluating conceptlearning abilities of text-to-image diffusion models. In Proceedings of the AAAI Conference onArtificial Intelligence, pages 1455414562, 2024.",
  "Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: Aresource-efficient text-to-image prior for image generations. arXiv preprint arXiv:2312.04655,2023": "Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: Aresource-efficient text-to-image prior for image generations. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 90699078, 2024. Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. Synthesize diagnoseand optimize: Towards fine-grained vision-language understanding. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1327913288,2024. Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, andSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richerimage-to-sentence models. In Proceedings of the IEEE international conference on computervision, pages 26412649, 2015. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Arijit Ray, Filip Radenovic, Abhimanyu Dubey, Bryan Plummer, Ranjay Krishna, and KateSaenko. cola: A benchmark for compositional text-to-image retrieval. Advances in NeuralInformation Processing Systems, 36, 2024.",
  "Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka. Contrastive learningwith hard negative samples. arXiv preprint arXiv:2010.04592, 2020": "Ugur Sahin, Hang Li, Qadeer Khan, Daniel Cremers, and Volker Tresp. Enhancing multi-modal compositional reasoning of visual language models with generative negative mining. InProceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages55635573, 2024. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and RobinRombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation.arXiv preprint arXiv:2403.12015, 2024.",
  "Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusiondistillation. arXiv preprint arXiv:2311.17042, 2023": "Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, and William YangWang. Who evaluates the evaluations? objectively scoring text-to-image prompt coherencemetrics with t2iscorescore (ts2). arXiv preprint arXiv:2404.04251, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances inNeural Information Processing Systems, 35:2527825294, 2022. Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du,and Yu Chen. Coarse-to-fine contrastive learning in image-text-graph space for improved vision-language compositionality. In Proceedings of the 2023 Conference on Empirical Methods inNatural Language Processing, pages 869893, 2023.",
  "Jaisidh Singh, Ishaan Shrivastava, Mayank Vatsa, Richa Singh, and Aparna Bharati. Learn\"no\" to say\" yes\" better: Improving vision-language models via negations. arXiv preprintarXiv:2403.20312, 2024": "Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. Clip models are few-shotlearners: Empirical studies on vqa and visual entailment. In Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages60886100, 2022. Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu. Galip: Generative adversarial clipsfor text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1421414223, 2023. Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela,and Candace Ross. Winoground: Probing vision and language models for visio-linguisticcompositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 52385248, 2022. Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, ZichengLiu, and Lijuan Wang. Equivariant similarity for vision-language foundation models. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages1199812008, October 2023. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al.Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. In2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, 2022. Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data.arXiv preprint arXiv:2309.16671, 2023. Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang,and Tongliang Liu. Alip: Adaptive language-image pre-training with synthetic caption. InProceedings of the IEEE/CVF International Conference on Computer Vision, pages 29222931,2023. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. Whenand why vision-language models behave like bags-of-words, and what to do about it? In TheEleventh International Conference on Learning Representations, 2022. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, AlexanderKolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages1812318133, 2022. Kai Zhang, Yi Luan, Hexiang Hu, Kenton Lee, Siyuan Qiao, Wenhu Chen, Yu Su, and Ming-WeiChang. Magiclens: Self-supervised image retrieval with open-ended instructions. In Forty-firstInternational Conference on Machine Learning, 2024. Le Zhang, Rabiul Awal, and Aishwarya Agrawal. Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic fine-grained understanding. arXiv preprintarXiv:2306.08832, 2023. Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adaptingclip for zero-shot semantic segmentation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1117511185, 2023.",
  "ABroader Impact": "In this study, we have demonstrated the potential of utilizing high-quality positive and negativepairs to enhance the compositional understanding of vision-language models like CLIP throughthe introduction of TripletCLIP. While our findings are specific to TripletCLIP, the underlyingtechniques hold promise for broader applications, including enhancing visual understanding inMultimodal Large Language Models (MLLMs) and Text-to-Image diffusion models. Although ourapproach yields significant performance improvements, it does require resources to generate large-scale synthetic datasets. We encourage future research to explore the utility of this pre-training strategywithin the latent space, which could reduce dependence on large generative models. Initiatives likeJEPA have already demonstrated the efficacy of focusing on latent space models, which suggestsa promising avenue for reducing computational overhead. Importantly, our experiments reveal thatsignificant enhancements in model performance are achievable even with substantially smaller datascales. This finding suggests that, when scaled appropriately, our methodology could substantiallydiminish resource dependencies and enhance the efficiency of pre-training processes for CLIP-likemodels.",
  "BPseudocode of TripletCLIP": "defforward ( b a t c h _ p o s i t i v e ,b a t c h _ n e g a t i v e ) :# getp o s i t i v eandn e g a t i v eimage t e x tp a i r simg_pos ,t x t _ p o s = b a t c h _ p o s i t i v eimg_neg ,txt_neg = b a t c h _ n e g a t i v e",
  "# computen e g a t i v eimage andt e x tr e p r e s e n t a t i o n simage_features_neg = c l i p . encode_image ( img_neg )t e x t _ f e a t u r e s _ n e g = c l i p . encode_text ( txt_neg )": "p o s i t i v e _ t x t= t o r c h . c a t ( [ t e x t _ f e a t u r e s _ p o s ,t e x t _ f e a t u r e s _ n e g ] )n e g a t i v e _ t x t = t o r c h . c a t ( [ t e x t _ f e a t u r e s _ n e g ,t e x t _ f e a t u r e s _ p o s ] ) # compute NegCLIPl o s s e sloss_1 = n e g c l i p _ l o s s ( image_features_pos ,p o s i t i v e _ t x t )loss_2 = n e g c l i p _ l o s s ( image_features_neg ,n e g a t i v e _ t x t )l o s s = loss_1 + loss_2",
  "TripletCLIP (ours)1.2923.3165.3430.3016.2617.6716.582.803.1817.4523.2615.4325.7451.0433.255.5812.3813.6720.81": "provides a comprehensive overview of the pre-training hyperparameters employed across allbaseline models and TripletCLIP. To ensure fair comparisons, we standardized the hyperparame-ters across all methodologies. Although larger batch sizes are typically associated with improvedperformance in contrastive learning, computational constraints necessitated fixing the batch size at1024 for all experiments. To accommodate this batch size on a single A100 GPU, we employedbf16 precision. In terms of computational resources, experiments using the CC3M dataset requiredapproximately 16 GPU hours, while those involving the CC12M dataset utilized up to 56 GPU hoursper experiment.",
  "D.1Compositional reasoning": "Previously, we reported the results on SugarCrepe, the most challenging dataset, noted for itsabsence of language biases. However, evaluations were also conducted on other benchmarks,such as Valse, Cola, and Winoground. As indicated in , TripletCLIP achieves overallimprovements of 2-3% compared to LaCLIP and NegCLIP. The Valse benchmark, which containstext prompts that heavily favor the perturbations made for NegCLIP, shows a strong performance fromNegCLIP, while NegCLIP++ encounters difficulties. Interestingly, TripletCLIP faces challengesin maintaining performance on Winoground, and baseline LaCLIP maintains the SOTA, whichis counterintuitive to other benchmarks. Nonetheless, TripletCLIP still manages to outperformNegCLIP significantly. These results affirm that TripletCLIP sets a new standard for state-of-the-artcompositional reasoning across diverse benchmarks.",
  "D.3Detailed image-text retrieval performance": "presents detailed T2I and I2T retrieval results for the MSCOCO and Flickr30k datasets.We report results at different recall thresholds: R@1, R@5, and R@10. The data shows thatTripletCLIP significantly outperforms the baselines across all recall rates. On average, Triplet-CLIP achieves a performance gain of 7-11% over LaCLIP. Additionally, TripletCLIP improvesperformance by 3-6% compared to previous state-of-the-art baselines.",
  "D.4Additional ablations": "Choice of Pre-trained LLM.We provide further details regarding the selection of LLMs forgenerating hard negative captions. NegCLIP++ was trained on 3 million generated negative captionsfor CC3M using three different LLMs, and the results are reported in . We find that Phi-3achieves the best average performance, while Gemma-2b unexpectedly has a notable negative impacton compositionality. However, we opted to use Mistral-7b-instruct-v0.2, as Phi-3 was released afterour experiments were completed, preventing its earlier evaluation. Evaluating the Orthogonality of TripletLoss.As discussed in , TripletCLIP can beintegrated with various previously proposed methodologies. To evaluate this, we applied TripletLossin conjunction with CyCLIP and reported the results in . The most significant perfor-mance improvement is observed with the TripletLoss alone. However, TripletLoss also enhances the",
  "GTECLIP": ": Positive vs. Negative modality-specific pair-based similarity distribution of pre-trainedCLIP ViT-B/32 model w.r.t. the vision and text-only encoders. The left plot is the vision embeddingsimilarities between positive and negative images. The right plot is the text embedding similaritiesbetween positive and negative captions. In the ideal scenario, the distribution should be skewedtowards 0.0, which indicates that the model can correctly distinguish between the positive andnegative data.",
  "EEncoder Representation Distribution Analysis": "Remember, this study aims to learn the representations that can distinguish between two data pointsthat are very similar but semantically different. Firstly, we take LaCLIP and TripletCLIP modelstrained on CC12M. We also sampled 50000 positive+negative pairs from CC3M. Then, we measurethe vision and text modality-specific cosine similarities between positive and negative pairs and plotthe distribution (see ). It can be observed that vision representations from TripletCLIP aremore skewed towards 0.0, suggesting that the vision encoder can distinguish between hard negativesamples better than the baseline LaCLIP. However, in the case of the text modality, both methodsperform similarly. This aligns with our findings from that the vision encoder plays a crucialrole in improving the compositionality, and to achieve this, our TripletData is necessary. 0.00.20.40.60.8",
  "This section provides qualitative examples of the TripletData and discusses various data analyses": "Difficulty of the data.We further add one more analysis to investigate how difficult our datasetis. First, we take state-of-the-art language-only (GTE ) and vision-only (DINO ) embeddingmodels and pretrained CLIP ViT-B/32. Later, we measure the modality-specific similarity betweenpositive and negative vision and language data pairs. shows that the similarity distributionof the pretrained CLIP model between positive and negative text pairs follows the distribution of thetext-only GTE model. Interestingly, vision distribution is drastically different. DINO can distinguishthe positive and negative pairs correctly with high confidence. However, despite the visuals beingso different, the pretrained CLIP model struggles to distinguish the different images. This furtherhighlights that TripletData is indeed challenging for the vision-language models, even the onestrained on large-scale datasets. Evaluations of Generated Images.Even though T2I diffusion models are widely evaluated onvarious tasks. We perform additional evaluations to measure how accurately generated images followthe text prompt. To do this, taking inspiration from , we first use LLM to generate binary\"yes/no\" style questions from the given caption. As shown in , we create five questionsper hard negative caption. Later, we utilized the ViLT model to answer visual questions. Upon thisinvestigation, we find that SDXL-turbo archives on an average of 76% accuracy. In other words, theT2I model can correctly generate an image that follows around 3/4th of the text. Additionally, wehypothesize that using an improved T2I model or image editing models to generate hard negativeexamples can further improve composition reasoning. Qualitative Examples.In , we provide additional qualitative examples of the contrastivepositive and hard negative pairs from the TripletData. Additionally, in , we illustratedseveral examples where the T2I model could not precisely generate images corresponding to thecaption. However, we may notice that in most cases, it maintains some of the important aspects.Because of this, despite not being 100% accurate all the time, it can help TripletCLIP improveperformance across the evaluation benchmarks.",
  "Hard negative caption only examples:": "1. Raw Caption: dog looking out from a window .Language Rewrite: A dog looking through the window at his owner.Negative Caption (NegCLIP):(a) A window looking through the dog at his owner.(b) A dog looking through the window at his dog.(c) A dog screams through the window at his owner.TripletData Negative Caption: A cat observing its owner from the window. 2. Raw Caption: person attends the premiere of filmLanguage Rewrite: A person attends the premiere of filmNegative Caption (NegCLIP):(a) A premiere attends the person of film(b) A person attends the festival of film(c) A person watches the premiere of filmTripletData Negative Caption: A person waits in line for film tickets. 3. Raw Caption: white crocus spring flowers in the forest.Language Rewrite: white crocus flowers in the forestNegative Caption (NegCLIP):(a) white crocus flowers in the skyTripletData Negative Caption: red orchid flowers in the meadow. 4. Raw Caption: flag with industry in the backgroundLanguage Rewrite: A flag is holding in the background an industrial site.Negative Caption (NegCLIP):(a) A background is holding in the flag an industrial site.(b) A flag is holding in the background an earthquake site.(c) A drone is holding in the background an industrial site.(d) A flag is visible in the background an industrial site.TripletData Negative Caption: A flag is flapping in the foreground of a pastoral scene. 5. Raw Caption: portrait of businessman with cardboard on his head carrying a briefcase and usingan umbrella while standing by.Language Rewrite: A portrait of a businessman standing by with a briefcase and cardboard onhis head carrying an umbrella while looking at a blue sky and parked cars on the streetNegative Caption (NegCLIP):(a) A businessman of a portrait standing by with a briefcase and cardboard on his head carryingan umbrella while looking at a blue sky and parked cars on the street",
  "TripletData Negative Caption: A portrait of a businesswoman seated on a bench with a totebag and newspaper on her lap holding an umbrella while looking at a red sunset over row houses": "6. Raw Caption: 158834 is the portion of the bound train .Language Rewrite: Huge locomotives sit on the tracks in front of a building.Negative Caption (NegCLIP):(a) Huge tracks sit on the locomotives in front of a building.(b) Three locomotives sit on the tracks in front of a building.(c) Huge locomotives sit on the tracks in anticipation of a building.(d) Huge locomotives mounted on the tracks in front of a building.TripletData Negative Caption: Huge locomotives sit on the tracks in front of a bridge. 7. Raw Caption: biological subfamily eating fish on a seaweed covered shoreLanguage Rewrite: Two blue whales are eating salmon on a beach surrounded by seaweedNegative Caption (NegCLIP):(a) Two blue salmon are eating whales on a beach surrounded by seaweed(b) Two stranded whales are eating salmon on a beach surrounded by seaweed(c) Two blue whales are eating salmon on a farm surrounded by seaweed(d) Two blue whales are eating salmon on a beach covered by seaweedTripletData Negative Caption: Two blue whales are feeding on herring in a bay surroundedby kelp. 8. Raw Caption: image of an original oil painting on canvasLanguage Rewrite: A young lady holding a painting to your face so you can see the detail of thepaintingNegative Caption (NegCLIP):(a) A young painting holding a lady to your face so you can see the detail of the lady(b) A bearded lady holding a painting to your face so you can see the detail of the painting(c) A young lady holding a pencil to your face so you can see the detail of the painting(d) A young lady holding a painting to your face so you can enjoy the detail of the paintingTripletData Negative Caption: A young lady holding a painting away from her face to showits beauty to the audience."
}