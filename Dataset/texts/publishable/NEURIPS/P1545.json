{
  "Abstract": "Recently, neural networks (NNs) have become a powerful tool for detecting quan-tum phases of matter. Unfortunately, NNs are black boxes and only identify phaseswithout elucidating their properties. Novel physics benefits most from insightsabout phases, traditionally extracted in spin systems using spin correlators. Here,we combine two approaches and design TetrisCNN, a convolutional NN with par-allel branches using different kernels that detects the phases of spin systems andexpresses their essential descriptors, called order parameters, in a symbolic formbased on spin correlators. We demonstrate this on the example of snapshots of theone-dimensional transverse-field Ising model taken in various bases. We show alsothat TetrisCNN can detect more complex order parameters using the example oftwo-dimensional Ising gauge theory. This work can lead to the integration of NNswith quantum simulators to study new exotic phases of matter.",
  "Introduction": "Machine learning promises a revolution in quantum sciences , similar to the current revolutionin industry . Recently, neural networks (NNs) have become a powerful tool for detecting phasesof matter , which is especially promising in the context of experimental data . Anultimate goal of this forefront is to make such automated approaches interpretable , which",
  "should lead to learning phases of matter and understanding their properties, in particular the detectedorder parameters": "Here, we report on designing a convolutional neural network (CNN) with parallel convolutionalbranches that use kernels of different sizes and shapes. We dub this network TetrisCNN and showhere that it detects the phases of spin systems and expresses their order parameters in a symbolic formbased on spin correlators. TetrisCNN takes inspiration from Refs. and improves on them interms of simplicity, computational cost, and inclusion of multiple measurement bases. Moreover, weuse symbolic regression (SR) to provide symbolic formulas for the detected order parametersand the network itself, making our method particularly attractive to physicists.",
  "Methods": "We test TetrisCNN ability to detect phase transitions and relevant order parameters on the example ofthree datasets derived from two paradigmatic spin models: 1D quantum transverse-field Ising model(TFIM) and 2D Ising gauge theory (IGT). Each spin at site i can take only two values Si = 1. Weuse Si = 1",
  "iSxi )(1)": "where J is the spin-spin interaction strength, g is the transverse field strength, and Sz and Sx arePauli matrices representing the spin operators. When J dominates g, the system is in the orderedferromagnetic phase with all spins aligned with each other in the z direction. The critical point ofthe system occurs when g balances J, and the system transitions to the disordered (paramagnetic)phase. We obtain ground states of 1D TFIM for various values of g (J = 1, N = 150) using theDensity Matrix Renormalization Group (DMRG) and ITensors.jl . We take measurements(snapshots) of ground states in z and y bases and use them as input data for TetrisCNN to imitateexperimental measurements on a quantum system. 2D Ising gauge theory (IGT) modelIGT is a classical spin model that exhibits a topological phaseof matter, defined on a square lattice with periodic boundary conditions. This time the interactionbetween spins is defined within plaquettes p on the lattice and is of four-body nature:",
  "Epoch": "0.10 0.05 0.00 0.05 0.10 (c) 2D IGT Notation - [(shape), dilation] [(1, 1), 1][(2, 1), 1] [(2, 1), 2][(2, 1), 3] [(3, 1), 1][(3, 1), 2] [(3, 1), 3][(3, 2), 1] [(3, 3), 1][(2, 3), 1] [(2, 2), 1][(1, 2), 1] [(1, 2), 2][(1, 3), 1] : Activation of TetrisCNN branches across training on different datasets. During training,branches without uniquely important information die out. In all three datasets, only one branchremains active. It relies on (a) the (1,1) kernel in the 1D TFIM measured in z basis, (b) the (2,1)kernel in the 1D TFIM measured in y basis, and (c) (2,2) kernel in the 2D IGT.",
  "The (highly degenerate) ground state of this Hamiltonian meets the local constraint that the product ofspins along each plaquette is": "ip Si = 1. The system exhibits a transition from the low-temperaturetopological phase to the high-temperature phase with violated constraints (see Sec. 3.1.2 in Ref. ).We obtain spin configurations of IGT for different temperatures using the Monte Carlo method . Architecture of TetrisCNNThe main idea behind TetrisCNN is to allow the network to utilizea multitude of differently shaped convolutional kernels, similar to Tetris pieces, within multipleparallel branches, as presented in . The results of their parallel computation, after global averagepooling, enter the TetrisCNN bottleneck as ak, which we regularize with Lbottle = k k|ak|,where k is a branch index. As such, we deactivate branches without important and unique information.We additionally choose k so that they promote the use of simpler kernels over more complicatedones. After the bottleneck, fully-connected layers solve the posed task, given the learned sparse datarepresentation. We describe the architecture in more detail in App. A. TetrisCNN interpretationIf we apply TetrisCNN now to spin configurations (where each inputelement can be only Si = 1), due to TetrisCNN construction, each branch activation ak can bemapped exactly to a linear function of a specific spin correlator (see App. A.1 and Ref. for moredetails). For example, the branch using kernel (2,1) can only learn a nearest-neighbor spin correlatorSiSi+1 (it can also learn a one-body Si but we can ignore it if the branch using (1,1) got deactivated).Therefore, we can easily understand what each branch computes, and linear regression is enough tofind a formula for ak. Thanks to the bottleneck sparsity, we can also obtain a symbolic formula forthe whole network using SR (see App. A.2), which otherwise fails due to the large input size. TaskIn the following, we combine TetrisCNN with the prediction-based method , whichallows for an unsupervised detection of phase transitions. First, the network is trained in a regressiontask to predict a tuning parameter of the system. For 1D TFIM, it is the transverse field value, g;for 2D IGT, it is an inverse temperature, . Then, the phase transition is identified by locatingthe maximum of the derivative of the network output as a function of the label. Intuitively, theprediction-based method relies on the difficulty of a successful regression in the transition vicinity.We provide the implementation details in App. B and in the accompanying GitHub repository .",
  "Results": "The branch activation depends on relevant correlators present in the dataThe interpretabilityof TetrisCNN relies on the sparsity of its bottleneck, i.e., a small number of its non-zero elements.Ideally, the bottleneck should contain only crucial information on the dataset. In , we showhow, during training, TetrisCNN branches that pick up irrelevant correlations in the data deactivate,i.e., their respective activations in the bottleneck go to zero. The branch (or branches) that remainsactive uses the kernel that computes the relevant correlators present in the data. 0.000.250.500.751.001.251.50 0.0 0.5 1.0",
  "Normalized": "activations max 0.00 0.25 0.50 0.75 1.00 1.25 max 0.0 0.2 0.4 0.6 0.8 1.0 Notation - [(shape), dilation] [(1, 1), 1][(2, 1), 1] [(2, 1), 2][(2, 1), 3] [(3, 1), 1][(3, 1), 2] [(3, 1), 3][(3, 2), 1] [(3, 3), 1][(2, 3), 1] [(2, 2), 1][(1, 2), 1] [(1, 2), 2][(1, 3), 1] : Penalties of branch activations k and the use of kernels. We plot here the average R2of the network output (compared to the true label) and the normalized branch activations (where 1 isthe largest branch activation value) of 5 TetrisCNN instances trained with different max. The largermax, the larger differences between penalties k put on the branch activations that use kernels ofincreasing complexity. Increasing max results in a sparser bottleneck of TetrisCNN that uses simplerand smaller kernels. At some critical max, we can also get a significant R2 drop, so the sparsity maycome at the cost of performance. complexity. We set k = np.logscale(min, max), where min is fixed to 104 and 105 for1D TFIM and 2D IGT, respectively. They are applied to the branch activations ak in the order ofappearance on the list of the respective available kernels defined by a user (see Tab. 1). In , inthe first row, we plot the average regression performance in terms of R2 of 5 TetrisCNNs trained withincreasing max that is with increasing differences between applied k that put increasing preferencetowards smaller and simpler kernels (i.e., kernels from the beginning of the user-defined list). Inthe second row, we plot average branch activations of the same TetrisCNNs, which we normalizeto the largest branch activation per network instance. We show here which kernels are preferred bydifferently regularized networks and also how stable is their choice across 5 initializations. Uniform kWhen 1 penalties put on the branch activations corrresponding to kernels of increasingcomplexity are equal to each other, the regularization results only in a sparse bottleneck and in anetwork preference to use a single kernel. There is no preference towards simpler kernels yet. Wesee that for 1D TFIM in z basis, TetrisCNN tends to use kernel (3,1) with dilatation 1 accompaniedby kernel (2,1) with dilatation 1. For 1D TFIM in y basis, the variety of used kernels is even larger,the dominant one also being kernel (3,1) with dilatation 1, but accompanied by kernels (3,1) withdilatation 2 and 3. Non-uniform kIn the case of 1D TFIM in z basis, for a bit larger differences between k(max 103), the dominant kernel changes to (2,1), and then to kernel (1,1) for max > 101.These choices stay the same across 5 initializations. This kernel switch results only in a slight R2 drop,suggesting that (1,1) learns a very relevant spin correlator. For y basis, starting from max = 102,all network instances use kernel (2,1). From max 103, the kernel choice becomes unstable and isaccompanied by a large R2 drop, suggesting that using smaller kernels is detrimental. TetrisCNN trained on 2D IGT is less dependant on kFinally, TetrisCNNs trained on 2D IGTsamples, regardless of k, usually use kernel (2,2) as the dominant kernel. For max 102, the onlydifference is that the (2,2) kernel is accompanied by some larger kernels, and the choice becomes lessstable across initializations. Interestingly, the largest R2 is for intermediate max = 100 suggestingthat sparsity does not always come at the cost of performance drop. The R2 difference between TetrisCNNs trained on 1D TFIM snapshots from z and y basisNext to the discussion on k and the use of kernels, the first row of (a)-(b) nicely suggests viathe R2 difference that the z basis contains better information for predicting the tuning parameterg of 1D TFIM than the y basis. Indeed, the g aims to align spins in the z direction, and it is theterm breaking the symmetry underlying the phase transition. Following the regression performancebetween measurement bases is, next to the simplicity of the learned spin correlator, an importantguidance on studying phases of matter with TetrisCNN using various measurements.",
  "True": "0.4 0.2 0.0 a[(2, 2), 1] = 4.95 S1(i, j)S1(i, j + 1)S2(i, j + 1)S2(i1, j + 1)0.322, R2 = 0.99 g = 1074.214( S1(i, j)S1(i, j + 1)S2(i, j + 1)S2(i1, j + 1) + 0.002|6421 S1(i, j)S1(i, j + 1)S2(i, j + 1)S2(i1, j + 1)392|0.044)2 + 0.015, R2 = 0.99 Output derivativeActive branch derivativeTheoretical phasetransition point : Identification of phase transition location and the relevant spin correlators withTetrisCNN and prediction-based method. (a)-(b) Results for 1D TFIM measures in z and y basis,respectively. (c) Results for 2D IGT. For 1D TFIM, the order parameter that determines the transition between the ferro- and paramagneticphases is the system magnetization in the z direction, Szi . It is a one-body quantity, and as expected,in (a) we see that the remaining active branch of TetrisCNN uses the kernel of shape (1,1).However, the magnetization in the y direction does not have information on the phase transition. In(b) we see that when TetrisCNN analyzes TFIM measurements taken in y basis, it uses thebranch with the kernel of shape (2,1). Indeed, we can check that the value of the nearest-neighbor spincorrelator Syi Syi+1 changes fast when the system undergoes a phase transition. Finally, TetrisCNNcorrectly focuses on the relevant correlator of the IGT, i.e., a four-body spin correlator related to theplaquette. We study how the detected correlators depend on k in Lbottle in App. C. TetrisCNN identifies the phase transition location in an unsupervised wayIn the first row of, we recover unsupervised detection of phases from Refs. . The predicted transitionlocations are in good agreement with the theoretical predictions for both Ising models. In the secondrow of , we plot the branch activations as functions of the respective label, i.e., g for 1D TFIMand for 2D IGT. We see that the values of the active branches exhibit a fast change in the vicinityof the phase transition, reminding of the expected behavior of the order parameter. As a result, wecan also recover the transition location by studying the derivative of the branch activation for data fordifferent labels. The maxima of the derivatives of the branch activations and of the network outputare slightly shifted, indicating different locations for the phase transition. The reason for this willbecome clear in the next section. TetrisCNN identifies the order parameter and provides its symbolic formulaMost excitingly,we can analyze the trained TetrisCNN with SR. For brevity, we focus here on results for TetrisCNNtrained on 1D TFIM in y basis and place the rest in App. D. The first mapping we find is between thedominant branch activation and the spin correlation it detects. As seen in (b), TetrisCNN has anactive branch that uses [(2,1),1] kernel and its activation is",
  "a[(2,1),1] = 0.122Syi Syi+1 + 0.0015 (R2=1.00) ,(3)": "where means averaging over snapshots or spin configurations obtained for the same tuningparameter. R2 coefficient in parenthesis shows that the fit is excellent. We see that in 1D TFIM, theactive branches of TetrisCNN learn the magnetization in the z direction from the data measured inthe z basis and a nearest-neighbor spin correlator in y direction from the data measured in the y basis.In 2D IGT, TetrisCNN branch is a linear function of the plaquette S1(i,j)S1(i,j+1)S2(i,j+1)S2(i1,j+1).",
  "Limitations and Outlook": "An obvious limitation of TetrisCNN is the combinatorial complexity of kernels that increases fastfor highly non-local order parameters. Currently, it can handle correlators up to (5,5). However, themain challenge is at the level of finding a symbolic formula for the whole network, while interpretingthe bottleneck scales much better thanks to the direct linear mapping between the kernel shapesand spin correlators. In a way, TetrisCNN automatically searches for relevant correlators , butalso learns an arbitrary useful function of correlators. Moreover, the learned correlators should betask-dependent, but we leave this aspect for future work. The next steps are to expand TetrisCNN to spin models on lattices of different geometries. An excitingavenue is to modify the approach to detect and give formulas for long-range, nematic, and topologicalorders in data, following the developments of Refs. . Although TetrisCNN was developedwith spin models in mind, it may tackle other correlation-based tasks, e.g., in molecular gases.",
  "M. Krenn, J. Landgraf, T. Foesel, and F. Marquardt, Artificial intelligence and machine learning forquantum technologies, Phys. Rev. A 107, 010101 (2023)": "A. Dawid, J. Arnold, B. Requena, A. Gresch, M. Podzien, K. Donatella, K. A. Nicoli, P. Stornati,R. Koch, M. Bttner, R. Okua, G. Muoz-Gil, R. A. Vargas-Hernndez, A. Cervera-Lierta, J. Carrasquilla,V. Dunjko, M. Gabri, P. Huembeli, E. van Nieuwenburg, F. Vicentini, L. Wang, S. J. Wetzel, G. Carleo,E. Greplov, R. Krems, F. Marquardt, M. Tomza, M. Lewenstein, and A. Dauphin, Modern applications ofmachine learning in quantum sciences (2023), arXiv:2204.04198 [quant-ph] .",
  "J. Arnold, F. Schfer, A. Edelman, and C. Bruder, Mapping out phase diagrams with generative classifiers,Phys. Rev. Lett. 132, 207301 (2024)": "H. Kim, Y. Zhou, Y. Xu, K. Varma, A. H. Karamlou, I. T. Rosen, J. C. Hoke, C. Wan, J. P. Zhou,W. D. Oliver, Y. D. Lensky, K. Q. Weinberger, and E.-A. Kim, Attention to quantum complexity (2024),arXiv:2405.11632 [quant-ph] . B. S. Rem, N. Kming, M. Tarnowski, L. Asteria, N. Flschner, C. Becker, K. Sengstock, and C. Weitenberg,Identifying quantum phase transitions using artificial neural networks on experimental data, Nat. Phys. 15,917 (2019). E. Khatami, E. Guardado-Sanchez, B. M. Spar, J. F. Carrasquilla, W. S. Bakr, and R. T. Scalettar, Visualizingstrange metallic correlations in the two-dimensional fermi-hubbard model with artificial intelligence, Phys.Rev. A 102, 033326 (2020). N. Kming, A. Dawid, K. Kottmann, M. Lewenstein, K. Sengstock, A. Dauphin, and C. Weitenberg,Unsupervised machine learning of topological phase transitions from experimental data, Mach. Learn.:Sci. Technol. 2, 035037 (2021).",
  "M. Link, K. Gao, A. Kell, M. Breyer, D. Eberz, B. Rauf, and M. Khl, Machine learning the phase diagramof a strongly interacting fermi gas, Phys. Rev. Lett. 130, 203401 (2023)": "C. Miles, R. Samajdar, S. Ebadi, T. T. Wang, H. Pichler, S. Sachdev, M. D. Lukin, M. Greiner, K. Q.Weinberger, and E.-A. Kim, Machine learning discovery of new phases in programmable quantum simulatorsnapshots, Phys. Rev. Res. 5, 013026 (2023). N. Sadoune, I. Pogorelov, C. L. Edmunds, G. Giudici, G. Giudice, C. D. Marciniak, M. Ringbauer,T. Monz, and L. Pollet, Learning symmetry-protected topological order from trapped-ion experiments(2024), arXiv:2408.05017 [quant-ph] .",
  "T.-Y. Lin, A. RoyChowdhury, and S. Maji, Bilinear CNN models for fine-grained visual recognition, inProceedings of the IEEE international conference on computer vision (2015) pp. 14491457": "J. Ansel, E. Yang, H. He, N. Gimelshein, A. Jain, M. Voznesensky, B. Bao, P. Bell, D. Berard, E. Burovski,G. Chauhan, A. Chourdia, W. Constable, A. Desmaison, Z. DeVito, E. Ellison, W. Feng, J. Gong,M. Gschwind, B. Hirsh, S. Huang, K. Kalambarkar, L. Kirsch, M. Lazos, M. Lezcano, Y. Liang, J. Liang,Y. Lu, C. K. Luk, B. Maher, Y. Pan, C. Puhrsch, M. Reso, M. Saroufim, M. Y. Siraichi, H. Suk, S. Zhang,M. Suo, P. Tillet, X. Zhao, E. Wang, K. Zhou, R. Zou, X. Wang, A. Mathews, W. Wen, G. Chanan, P. Wu,and S. Chintala, Pytorch 2: Faster machine learning through dynamic python bytecode transformation andgraph compilation, in Proceedings of the 29th ACM International Conference on Architectural Support forProgramming Languages and Operating Systems, Volume 2, ASPLOS 24 (Association for ComputingMachinery, New York, NY, USA, 2024) p. 929947.",
  "A.1TetrisCNN architecture": "The TetrisCNN architecture can be divided into two general parts: correlation extraction and tasknetworks. They are presented in (a)-(b), respectively, along with a detailed description of eachdata processing step. We discuss these steps in the following sections. Extracting correlations. Let us first focus on the correlation extraction network. The input tothe network are either classical spin configurations or snapshots - projective measurements on asimulated or experimentally realized quantum spin lattice. The architecture supports either one- ortwo-dimensional systems with snapshots in any of x, y, z measurement bases or their combination.Each combination of input bases results in a separate training and analysis routines. The snapshots",
  "(a)(b)": ": A detailed presentation of TetrisCNN architecture and how it processes spin config-urations at each step. (a) The first part of TetrisCNN, the correlation extraction network, whichmaps the spin configurations onto functions of many-body spin correlators present in the data. (b)The second TetrisCNN subnetwork is the task network, in our case a fully-connected network. Ifapplied to phase detection problem, the task can bebe either unsupervised (as here, where we use theprediction-based method) or supervised. are introduced into the network and then processed in parallel by several branches, reminiscent ofbilinear CNNs from Ref. . Each branch consists of a convolutional layer and then averages overthe number of filters and physical indices, thus reducing each snapshot to a single number - a branchactivation ak. Following the logic presented by Ref. , we interpret this activation as a function ofall correlators present in the data of the same shape and size or smaller as the convolutional kernelused in the branch. Therefore, each activation ak can be written as a function of the input snapshot -set of spin variables Si:ak = f({Szi })(5)We map the spin variables from {, } to {1, 1}. Then, further following the logic from Ref. ,we leverage the fact that:",
  "(9)": "Truncation of search space. This trick still leaves us with the branch activation dependent on thecorrelations also smaller than the kernel size. To tackle this, we add a term to the loss function sothat the network learns during the training to use only the simplest non-trivial kernels to achieve thistask. This is achieved by combining the activations from all branches, forming a network bottleneck.This bottleneck is 1 regularized, and the values of the activations are combined into Lbottle, whichis then added to the final loss function. As a result, once the network is trained, we can inspect thevalues of the activations in the analysis routine, and the ones with the highest amplitude are perceivedas the most important to the network. Therefore, e.g., if we allow the network to be used in theoptimization process, the kernels (1,1), (2,1), (3,1), and kernel (2,1) turns out to be the one with thehighest amplitude, the function describing it presented in Eq. (9) collapses only to the second term.",
  "Projective measurements": ": A high-level overview of the TetrisCNN architecture and its interpretation. The twoparts form a pipeline for the unsupervised detection of phases of matter and their order parameters.(a) TetrisCNN architecture comprises two general parts - correlation extraction network and tasknetwork. The two are joined by a bottleneck, which is key to the subsequent analysis. (b) Obtainingsymbolic formulas takes two steps. First, finding a mapping between correlations accessible to eachbranch and this branch activation with linear or symbolic regression. Second, finding a mappingbetween the network output and the branch activations in the bottleneck. The first term is either zero or not of the leading order because if it were of the leading order, then akernel (1,1) would not be deactivated. Therefore, only by studying the branch activations, we candetect leading correlations in the data, solely from the interpretable design of TetrisCNN architecture. Learning task. The second part of the network is the task network, which learns the mapping betweenthe branch activations and the desired output. We implement two possible tasks: classification andregression. The variant with regression task is based on the prediction-based method , whichallows for an unsupervised detection of the phase transition point, as opposed to the classificationtask, which requires the user to supply the labels.",
  "A.2Interpretation with symbolic regression": "Although the detection of relevant spin correlators can be done simply by studying the networkbottleneck and leveraging its interpretable design, we can additionally use regression techniques toprovide a symbolic formula for both bottleneck activations and the network output, all in terms ofspin correlators. To this end, we first find a mapping between the spin correlators and the branchactivations. Due to 1 regularization of the bottleneck, increasing k, and binary values of the spindata, the bottleneck elements ak can only be linear functions of the respective spin correlators, asexplained above (i.e., kernel (1,1) can only learn Si, kernel (2,1) - only SiSi+1). We can identify thismapping using linear regression (e.g., via least-squares minimization). However, to find a symbolicformula for the mapping between bottleneck activations ak and network output, we need to usesymbolic regression (SR). These two steps are presented in (b). Symbolic regression. Symbolic regression (SR) is a type of regression analysis that searches formathematical expressions that best fit the given data rather than assuming a predefined functional formlike in traditional regression. This approach is particularly useful in physics, where the goal is oftento discover underlying equations or relationships that govern the observed data . The SR analysiswas performed using the PySR package, a Python interface to the SymbolicRegression.jl library inJulia . This package employs evolutionary algorithms and genetic programming to iterativelysearch for and evolve mathematical expressions that best fit the activation data. Its procedure is thefollowing:",
  ". Initialization: The algorithm starts with a population of random mathematical expressionsinvolving basic operators (+ , , , /), constants, and the input variables (correlators)": "2. Evaluation: Each expression is evaluated based on its ability to predict the kernel activations.The evaluation metric is the default SymbolicRegression.jl loss function, which is the meansquared error (MSE) between the predicted and actual activations. 3. Selection and Evolution: The top-performing expressions are selected, and new expressionsare generated through genetic operations such as mutation (altering parts of an expression)and crossover (combining parts of two expressions). 4. Convergence: Depending on the size of the input, the process continues for 6000 or18000 epochs, which allows for a good convergence of each branch evolved by the geneticalgorithm employed under the hood of PySR.",
  "BDetails on the numerical implementation and used hyperparameters": "TaskIn this work, TetrisCNN is used in combination with the prediction-based method .We train TetrisCNN using PyTorch by minimizing the mean squared error between the networkoutput and the tuning parameter. For 1D TFIM, the tuning parameter is the transverse field value, g;for 2D IGT, the tuning parameter is an inverse temperature, . The training hyperparameters are inTab. 1. Input dataThe input data are spin configurations, either classical (2D IGT) or projective measure-ments taken on a quantum system in some bases (1D TFI). Single input data therefore is a vector (for1D systems) or matrix of {1, 1} (for 2D systems). If a system is classical or only one measurementbasis is considered, input data has a single channel, as in case of the 1D TFI, where z and y baseswere considered separately. If two measurement bases are considered, like in the 2D IGT case, thespin configurations (here from two sublattices) are randomly bunched into pairs consisting of twobases, and are fed into two input channels. For three measurement bases, there are three channels. Declaring available branchesTetrisCNN requires giving a list of kernels that it needs to consider.The kernel format we use is [(dimension1, dimension2), dilation], where dimension2 = 1 for 1Dand dilation inserting holes of specified size between the kernel consecutive elements. Dilation = 1means no hole, dilation = 2 means hole of one element size. Each kernel is then used by a separateparallel convolutional branch. In our numerical experiments, we used up to 20 parallel branches anddid not notice any performance drop. We report the list of kernels that we made available for everysetup described in the main text in Tab. 1. Bottleneck regularizationInterpretation of TetrisCNN via symbolic regression (SR) hinges uponthe sparsity of its bottleneck. Therefore, we regularize the bottleneck elements ak, i.e., branchactivations averaged across channels and physical system size via the following loss term:",
  "k|ak| .(10)": "This 1 regularization encourages the bottleneck elements to go to zero, which translates into the useof the smallest number of branches (therefore kernels and spin correlators) possible. We additionallymodulate the values of k so it is the smallest for branches using kernels of the smallest size, alsofavoring simpler correlations over more complex ones. We choose it so that they are equally distancedon a logaritmic scale between min and max, k = np.logscale(min, max). We report valuesof min and max in Tab. 1. Architecture of the task networkWhile the correlation extraction network is the same acrosssetups, we vary the fully-connected task network between the datasets. We report its architecture alsoin Tab. 1, where the input to the task network is the bottleneck (therefore branch activations) and thenext numbers indicate numbers of hidden neurons in the consecutive fully-connected layers.",
  "DSymbolic regression results": "The analysis process described in App. A.2 is presented here on the example of 1D TFIM model in ybasis in . Panels (a) and (b) present the mapping between the spin correlators computed fromthe raw input data and the learned bottleneck activations as a function of the regression parameter.In all panels, the regression parameter is the transverse field strength g. In panel (b), we also showthe least squares fit that includes all possible correlators from Eq. (9), i.e., Syi and Syi Syi+1. Thecoefficient next to Syi Syi+1 is two orders of magnitude larger than the one next to Syi , indicatingthat the regularization of the bottleneck is working as planned. Finally, panel (c) presents the mappingbetween the bottleneck activations and the output of the network. In Sec. 3 we present the distilled equation for TetrisCNN predicting transverse field strength g in theTFIM model out of measurements in y basis, but our analysis also spanned two other experimentalsettings: TFIM model in z basis and IGT model. In both cases, the symbolic distillation yielded anaccurate description of the knowledge extracted by the network. TFIM model in z basis. Here, the network correctly picked up the simplest nonzero correlatorpresent in the data, which is the magnetization in the direction z, using kernel (1,1). The nonzerobranch corresponding to this correlator is approximated in Eq. (11). Given this approximation, anequation that approximates the whole TetrisCNN is presented in Eq. (12).",
  "g = |2.798Szi 1.013|(R2=0.97)(12)": "IGT model Analysis of data from IGT model presented us with a new insight into case previouslystudied by Ref. . We knew from their investigation that the correlations picked up by the CNNmust not exceed the receptive field of convolutional kernel (3,3), and using our analysis routinewe were able to narrow this estimate from more than 250000 possibilities to just one meaningfulcorrelation - a four body correlator within the convolutional kernel (2,2). Our analysis of the nonzero 0.020.010.000.01 True activation value 0.020 0.015 0.010 0.005 0.000 0.005 0.010 0.015 Predicted activation value (a)",
  "Target": "0.122 Sy(i, j)Sy(i, j + 1) + 0.015 | R2 = 1.000 Least squares fit: R^2 = 1.00 y = +0.015 + 0.122 Sy(i, j)Sy(i, j + 1) + 0.001 Sy(i, j) 0.00.51.01.5 True g 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 Predicted g (c) Predicted gTrue g g = 143321.922|((a0 + 0.023)20.003)2| | R2 = 0.996 g = 465291.034((a0 + 0.018)20.001)2 + 0.243 | R2 = 1.000 g = |474493.707((a0 + 0.018)20.001)2 + 0.244| | R2 = 1.000 : Symbolic regression results for TetrisCNN trained on 1D TFIM measured in y basis.(a) Fits for a[(2,1),1](x1), where x1 = Syi Syi+1. (b) Fits for g(a[(2,1),1]). (c) Fits for g(a0), wherea0 = Syi Syi+1 and comparison between the predicted g of the network based on snapshots from they basis and the true g. TetrisCNN branch revealed this kernel to be S1(i,j)S1(i,j+1)S2(i,j+1)S2(i1,j+1), which is a product ofall spins about a given vertex, producing a pattern similar to the vertex operator present in definitionof the toric code (see of Ref. ). Consecutive investigation of the raw data revealed thatthis correlator is indeed the only one in the (2,2) receptive field that varies continuously across theinvestigated range. All other correlators of smaller or equal size converge to a constant value asthe number of snapshots per is increased. An equation describing the nonzero branch activation ispresented in Eq. (13). The full distilled relation describing learned mapping from the input correlatorsto the regression parameter, inverse temperature , is presented in Eqs. (14) - (15)."
}