{
  "Abstract": "We introduce camera ray matching (CRAYM) into the joint optimization of cameraposes and neural fields from multi-view images. The optimized field, referred toas a feature volume, can be probed by the camera rays for novel view synthesis(NVS) and 3D geometry reconstruction. One key reason for matching camerarays, instead of pixels as in prior works, is that the camera rays can be parameter-ized by the feature volume to carry both geometric and photometric information.Multi-view consistencies involving the camera rays and scene rendering can benaturally integrated into the joint optimization and network training, to imposephysically meaningful constraints to improve the final quality of both the geometricreconstruction and photorealistic rendering. We formulate our per-ray optimizationand matched ray coherence by focusing on camera rays passing through keypointsin the input images to elevate both the efficiency and accuracy of scene correspon-dences. Accumulated ray features along the feature volume provide a means todiscount the coherence constraint amid erroneous ray matching. We demonstratethe effectiveness of CRAYM for both NVS and geometry reconstruction, overdense- or sparse-view settings, with qualitative and quantitative comparisons tostate-of-the-art alternatives.",
  "Introduction": "Recent advances on multi-view 3D reconstruction have been propelled by the emergence of neuralfields , including implicit functions and radiance fields (NeRF) . A critical component to all image-to-3D reconstruction methods, including traditionalapproaches such as multi-view stereo (MVS) , is to obtain camera poses for the input images. Inpractice, the camera information may be available from the acquisition devices, e.g., through the GPSor inertial measurement unit (IMU), while in other cases, it is estimated, e.g., using structure-from-motion (SfM) . In both cases, these camera poses can be noisy, thus hindering the performanceof the multi-view 3D reconstruction. In light of the importance of having accurate camera poses, various methods have been proposedto improve their estimations. One line of approaches, which can be referred to as bundle-adjustingneural fields, jointly optimize camera poses along with results from rendering andgeometry (e.g., depth) estimation, where the camera rays are considered independently for their roles",
  "arXiv:2412.01618v1 [cs.CV] 2 Dec 2024": ": Our method, neural field optimization with camera ray matching (CRAYM), incorporatescontextual information for per-ray processing and enforces color + geometric consistence betweenmatched rays. Compared to SPARF which utilizes dense pixel correspondences and the state-of-the-art, bundle-adjusting L2G-NeRF , both aimed at handling noisy camera poses, CRAYMproduces superior results especially over fine details; see the zoom-ins on the right. Results are shownthe Drums model from NeRF-Synthetic on dense views. in color and geometry prediction. By now, more works realize the importanceof exploiting correlations between input images, i.e., multi-view consistency, to impose additionalconstraints on the joint optimization. Along these lines, much effort has been invested into matchingand optimization with respect to image features, whether convolutional or transformer-based. Motivated by multi-view spatial analysis, several works have proposed geometric constraints involv-ing camera rays and projections . Most recently, SPARF defines a re-projection lossas a spatial distance between image pixels to enforce that matched pixels between NeRF trainingimages be back-projected onto the same 3D point. However, the effectiveness of this loss dependscritically on how reliable the pixel correspondences are. In their work, these correspondences andtheir confidence estimates were both obtained by a pre-trained network , which is independent ofthe joint camera-scene optimization. In this paper, we introduce camera ray matching into the joint optimization of camera poses anda neural field, referred to as a feature volume, which can be probed by the camera rays for bothrendering, e.g., novel view synthesis as in NeRF , and 3D reconstruction, as in NeuS . The key reasons for matching camera rays, instead of pixels , are two-fold. First, theserays carry 3D spatial information than just 2D pixel values to facilitate formulating explicit geometriclosses when optimizing camera poses , as dictated by multi-view analysis. Second and moreimportantly, the camera rays can be parameterized by the feature volume they carry both geometricand photometric information. Any constraint arising from camera ray matching can be passed ontothe feature volume. Hence, both the matching itself and the associated matching confidence canbe incorporated into the joint optimization and network training, to impose physically meaningfulconstraints to improve the final quality of both geometry reconstruction and rendering. Our network, coined CRAYM (for Camera RAY Matching), takes as input an uncalibrated set ofimages capturing a 3D object, and is trained to predict the feature volume along with all the camerarays subjected to a combination of photometric rendering losses and geometric losses dedicated toensuring multi-view consistency between camera rays. We consider two types of rays. The firstare called key rays, which pass through keypoints detected in the input images, typically spanningregions with sharp features and rich textures over the 3D object. The other rays are called auxiliaryrays, which pass through points around keypoints to offer contextual and local structural informationas we reason about the key rays in our optimization framework. As our main constraint for matched ray coherence, we enforce color consistency between renderingsalong two key rays whose corresponding keypoints from two different views are matched .However, we must account for potential erroneous matches due to occlusion or unreliable localimage features used by the matching network. To this end, we aggregate features along each keyray through the feature volume. The matchability between two rays is defined by a cosine similarity between the accumulated ray features and applied as a weight to either accentuate or discount thecolor consistency constraint, allowing our optimization model to naturally degenerate itself to handleunrelated rays separately. Also, to improve the robustness of feature learning, we enhance the featurealong each key ray by integrating features from surrounding auxiliary rays. We evaluate our method on both the synthetic objects from NeRF-Synthetic and the real scenesfrom UrbanScene3D , for novel view synthesis and 3D geometry reconstruction, over dense- andsparse-view settings. Compared to state-of-the-art alternatives, CRAYM produces superior resultsespecially over fine details.",
  "Related Works": "Neural Fields.As a pioneer work, NeRF synthesizes novel views of static objects/scenesfrom a set of posed images by optimizing a coordinate-based neural network, which predicts thevolume density and color for a sampled point in the 3D space. Since then, numerous methods haveemerged to improve the rendering quality and rendering efficiency . Toextract high-quality surfaces from the learned implicit representation, NeuS and VolSDF propose to learn an implicit signed distance field (SDF) representation for the scenes. These methodscan achieve impressive results on both novel view synthesis and 3D reconstruction, however, therequirement of precise camera pose limits their applicability in practice. Bundle-Adjusting Neural Fields.With the realization that positional encoding is susceptible tosuboptimal registration, BARF applies a smooth mask on the encoding at different frequencybands for a coarse-to-fine training, while presents an adaptive positional encoding. L2G-NeRF first learns the pixel-wise transformations for every pixel in a frame and then aligns the frame-wisetransformation with the pixel-wise transformations. Common to all the above methods is that theirjoint optimization of pose and scene representation processes each image and each ray separately,without considering their multi-view correlations. As a result, the pose optimization may not bestable, thereby leading to floaters and blurriness in both novel view renderings and 3D reconstruction.Note that our method also involves per-ray processing, by combining information from auxiliary rayswith that of a key ray. This is similar to the patch-level feature processing in CR-NeRF , whichconsiders multiple rays indiscriminately across the image, without the notion of key rays. Neural Fields with Image Matching.Image matching can help establish geometric priors toimprove the generalizability of NeRF, to either novel scenes or the sparse-view setting. MVSNeRF constructs a cost volume by warping the image features extracted with a 3D CNN onto a plane sweep,from which a generalizable radiance field is learned. SparseNeuS constructs a 3D volume withthe variance of all the projected features from multi-view images. DBARF optimizes cameraposes and depth with a cost map constructed by the differences of image features. CorresNeRF proposes to regularize the NeRF training with a pixel re-projection loss for the associated pixelsand a depth loss for the predicted depth. GPNR aggregates features of the image patches alongepipolar lines with several stacked transformers. MatchNeRF learns a generalizable NeRF withthe cosine similarity of image features for each image pair as the shape prior. All these methodsintegrate image features and utilize the matching within or between different views. With moreemphasis placed on multi-view geometry reasoning, SCNeRF learns a pinhole model for eachcamera under the supervision of a re-projected ray distance loss, while SPARF optimizes itsnetwork with a re-projection loss, measuring spatial distances between pixels in the same view. Incontrast, the matched ray coherence formulation in our optimization accounts for both photometricand geometry information as obtained from the feature volume; the coherence constraint is alsoexplicitly integrated into the network instead of only serving to define a loss.",
  "Method": "We are interested in neural networks that can reconstruct a 3D model, e.g., a radiance field oran implicit field , from a set of M images {Ii}Mi=1 capturing a 3D object from multiple views.Typically, each image is associated with a known or estimated camera pose Ti = [Ri|ti], whereRi SO(3) and ti R3. The network is trained by minimizing a photometric error Lp between theinput images and the multi-view renderings, {Ii}Mi=1, of the target 3D object from the camera views:min",
  "Key Rays Enrichment Module (KRE)Matched Rays Coherency Module (MRC)": ": Overview of our CRAYM pipeline. After extracting keypoints (red dots) from input imagesand matching them using a pre-trained network, we train our CRAYM network to optimize a 3Dfeature volume V which encodes both geometric and photometric information about the target 3Dobject and can be queried by camera rays for both novel view synthesis (via the Texture Network) and3D reconstruction (via the Geometry Network). The volume optimization is subject to photometriclosses through rendering along camera rays passing through the keypoints (i.e., the key rays), whichis enhanced (in the KRE) by integrating features from auxiliary rays, i.e., rays passing throughnearby auxiliary points (yellow dots) in the images. Matched ray coherence (MRC) is imposed onmatched key rays, in terms of color consistency, while potentially mismatched rays can be identifiedby comparing accumulated features along the key rays through V. On top of the standard photometricloss, we introduce two geometric losses, the epipolar loss and point-alignment loss, to explicitlyoptimize ray-to-ray coherency to maximize the reconstruction quality of the feature volume. Each pixel is associated with a specific ray in 3D from the object/scene, through the pixel center,towards the camera: {r(t) = ro +trd|t 0}, where ro is the camera center and rd is the normalizedview direction of ray r. The rendered color of ray r, i.e., the pixel color Ii(x), can be produced usingvolume rendering by accumulating the color and opacity along the ray r. Considering that the camera poses can be noisy, the reconstructed radiance field or implicit field maynot produce clean and sharp renderings with details. At a high noise level, some methods may evenfail to produce results; see examples shown in Sections 4.2 and 4.4. Beyond existing approachesthat map r(t) to opaque density (or opacity) and color implicitly with a ray-wise network, wepropose CRAYM to learn the implicit field by matching rays across different images and formulatinggeometric priors.",
  "The CRAYM Pipeline": "overview our CRAYM pipeline. From the input images, our goal in the 3D neural fieldoptimization is to construct a 3D feature volume V to faithfully represent the target object. In detail,we represent feature volume V using multi-resolution hash encoding and end-to-end optimize itfor the target object. The feature f(p) of point p in the 3D feature volume can be extracted by",
  "f(p) = M(V(p)),(1)": "where M is the progressive feature mask for filtering out fine-level features during early iterationsof the coarse-to-fine training. Very importantly, to account for the noise in the camera poses, weparameterize the transformation matrices of the cameras as variables in the joint optimization of thepose and implicit field with the feature volume V.",
  "As mentioned in the introduction, we consider two types of rays to probe the feature volume, i.e.,key rays and auxiliary rays. Both rays are issued from the cameras through the pixel centers. To": "obtain key rays {rk}, which typically associate to surface points with rich textures and sharp features,we detect keypoints on each input image using SuperPoint and perform point-to-point matchingbetween image pairs using SuperGlue . Then, we can obtain a set of sparse ray-to-ray matchingsbetween image pairs. Note that these results may not be accurate for various reasons such as occlusionand unreliable matching, but they provide useful information for our pipeline to start with. As for theauxiliary rays {ra}, they are sampled around the keypoints to provide contextual or local structuralinformation when we reason about the key rays; see .2 for details. Once optimized, the feature volume can be used for novel view synthesis or for multi-view 3Dreconstruction. The color prediction for novel view synthesis is accomplished by a texture networkt, as in a typical NeRF setting, and the latter is accomplished by a geometry network g, as ina typical NeuS setting. Specifically, the geometry network takes a 3D point p sampled alongr and the feature at p as input to produce an SDF value and then an opaque density to render the3D object and extract the 3D reconstructions. Here, we propose the Key Rays Enrichment (KRE)module (.2) to improve the robustness in the process by enhancing the features along thekey ray using the features sampled by the auxiliary rays. Subsequently, the texture network takes the output features from geometry network, ray directions,and normal at p as inputs to predict color c(p) at point p. Further, we design the Matched RaysCoherency (MRC) module (.3) to enhance the volume rendering quality by consideringmatchability between rays and learning to maintain coherency between ray matchings. Particularly,the MRC module can effectively reduce the influence of mismatched rays by disambiguating thecamera ray matchings. A pair of the matched key rays, rk and rk, are sampled with the corresponding auxiliary rays duringeach iteration. The geometry network, texture network, and feature volume optimization are jointlytrained end-to-end. Besides the photometric loss, we formulate the epipolar loss and point-alignmentloss (.4) to explicitly promote coherency among the ray matchings and boost performance.",
  "Key Rays Enrichment Module": "As the input images are captured through a perspective projection, all rays in 3D through the sameimage should converge at a common camera point. In previous works, for each iteration, raysare optimized separately, so the pose optimization may not be stable. As different rays may backpropagate gradients in different directions, the optimized poses may oscillate during the training.Hence, we introduce the KRE module to stabilize the optimization by learning structural informationaround each key ray. This is done by sampling auxiliary rays around the key ray to enrich the featureof the key ray with more contextual information:",
  "jg(f(pk), f(qj)),(2)": "where pk is a point along key ray rk; {qj} are points around pk sampled along the j-th auxiliary rayaround rk; and function g fuses features f(pk) and f(qi). Then, we employ the geometry networkg to predict the SDF value at pk and feature vector f (pk), from which we can further obtain thecolor of point pk with the texture network. Please refer to the supplemental materials for the details.",
  "(a) Epipolar Loss(b) Point-alignment Loss": ": Illustrating of our geometric losses. The red lines in the left subfigure are epipolar lines.The epipolar loss constrains the relative transformations between cameras, so that the projection of akeypoint pk onto the image plane of the other camera should lie on the epipolar line exk. With thecamera poses constrained by the epipolar loss, the point-alignment loss further constrains the depthof xk and xk, aiming to align pk and pk with P. paired matched key ray should have the highest density. Hence, we consider coherency betweenthe matched rays to optimize the learning of the opacity density and point color, such that we canenhance the coherency of features accumulated along the matched key rays. In return, this willhelp to optimize the parameters and the 3D feature volume, when training the pipeline. Therefore,we fuse the rendered color c(rk) of the matched rays based on the cosine similarity between theiraccumulated features:",
  "Further, we introduce the following two geometric losses to more explicitly promote the coherencyof the ray matchings:": "Epipolar loss. Given a pair of matched keypoints xk and xk on two different input images, whichassociate with camera centers O and O, respectively, (see the illustration in ), we can estimatethe depths at xk and xk by using a depth accumulation formulation similar to Equation 3, and thenproject points xk and xk into the 3D object space to obtain 3D locations pk and pk, respectively. If the camera poses, the matchings, and the depths are precise, the two rays through xk and xk shouldprecisely intersect at a common point, say P, on the target object surface, such that pk and pk alignwith P. Also, we denote e and e as the epipolar points on the two images; these points are theimage-space locations at which the line OO intersects the two image planes; see (a). During the training, the depth estimation of xk can vary, so pk may vary along ray rk. If the cameraposes are precise, the projection of pk onto the image plane of the other camera should lie on theepipolar line eO. In case of noisy camera poses, the projection of pk may not lie exactly on eO, sowe explicitly enforce the epipolarity during the training by minimizing the distance between pksprojection and the epipolar line exk using",
  "Since the epipolar loss is not affected by depth, we decouple the unreliable depth estimation from theepipolar loss with ray marching to constrain the camera poses": "Point-alignment loss. The epipolar loss focuses on enhancing the projection consistency for producingmore precise camera poses. To complement it, we introduce the point-alignment loss to facilitatedepth convergence for improving the reconstruction of fine details. In detail, we consider the triangleformed by intersection point P, line segment pkO, and line segment pkO ((b)), and aim to : Visualization of the initial and optimized camera poses for the LEGO scene in the NeRF-Synthetic dataset . (Purple: ground-truth poses; blue: initial or optimized poses; red lines:translation errors.)",
  "Results": "We evaluate our method on the NeRF-Synthetic dataset with eight synthetic objects (.2),the LLFF dataset , and the real scenes from the UrbanScene3D dataset (.3). Wecompare our method on both novel view synthesis and 3D reconstruction with NeRF , NeuS ,BARF , L2G-NeRF , PET-NeuS , SPARF , and BAA-NGP . Since NeRF ,NeuS , and PET-NeuS are designed for neural implicit field with fixed and precise poses, weset the camera transformations as variables to be optimized jointly with the neural field, as in ourmethod. While other methods optimize the radiance field, in which the target values, radiances, of points aremore independent of each other, the optimization of SDFs in NeuS and PET-NeuS poses a challengeto the requirements of non-local geometric constraints to correctly form the shape, making them morevulnerable to unstable pose optimization. As the camera rays are parameterized by our feature volumeto carry both geometric and photometric information, our geometric constraints on the camera raymatching can effectively lead to better optimization of the geometry. The joint optimization of camerapose and implicit SDF may also fail to produce results for NeuS and PET-NeuS, when the cameraposes are at a high noise level. With the assistance of camera ray matching, CRAYM outperformsother methods on both novel view synthesis and 3D reconstruction at varying noise levels. We reportthe PSNR, SSIM, and LPIPS for quantitative comparisons on novel view synthesis and Chamferdistance (CD) for the 3D reconstruction. A test-time photometric pose optimization is performed toevaluate these metrics, following prior works . The quantitative evaluations on the othermetrics are provided in the supplementary materials.",
  "Pose Alignment": "To evaluate the registration quality of the optimized training poses, we use Procrustes analysis to find the 3D similarity transformation that aligns the optimized training poses with the calibratedcamera poses, following BARF . As shows, the optimized poses produced by CRAYMalign well with the ground-truth poses with lower translation errors.",
  "BARF ICCV219.020.81SPARF CVPR232110.640.53L2G-NeRF CVPR23212.900.10CRAYM (ours)1.210.19": "The average translation errors and rotation er-rors are reported in . With the contextualinformation and feature coherency of cameraray matching, the camera poses produced byCRAYM are better optimized for the construc-tion of the implicit filed, so that CRAYM is ableto produce high-quality rendered views, as wellas more precise 3D reconstructions.",
  "NeRF 13.290.680.490.64BARF 23.090.840.180.24SPARF 23.900.840.230.18L2G-NeRF 28.620.930.070.17CRAYM (ours)30.340.950.050.06": "For the evaluation on the NeRF-Syntheticdataset, we follow the same setting of noisyposes as L2G-NeRF , which perturbs theground-truth camera poses with additive noiseas the initial poses. As NeuS and PET-NeuS fail to produce results at such a set-ting, we present only the results of NeRF ,BARF , SPARF , and L2G-NeRF .The mean results of the eight objects and four ofthem are given in . NeRF fails to extractmeshes from the reconstructed radiance fieldson Hotdog and Ship. shows the resultsof view synthesis and 3D reconstruction visuallyfor BARF, SPARF, L2G-NeRF, and our method.SPARF produces over smoothing results withdense input, as shown in . As can beseen in , CRAYM is able to produceclean and complete renderings and reconstruc-tions with fewer floaters and less blurriness.",
  "Evaluation on Real Scenes": "We first evaluate our method on the LLFFdataset for high-fidelity view synthesis of the eight real scenes. Compared with BARF ,L2G-NeRF , and BAA-NGP , our method is able to produce high-quality results with fewerartifacts and better scores in terms of PSNR, SSIM, and LPIPS, as shown in the .",
  "ArtSciNeuS 13.490.330.890.10PET-NeuS 15.180.360.880.08CRAYM (ours)19.370.420.630.02": "As the images in the LLFF dataset are capturedwith a restricted range of angles, we further as-sess our method on the real scenes PolyTechand ArtSci from the UrbanScene3D datasetwith two sets of drones captured images as-sociated with GPS information. The large-scalescenes are captured with hundreds of images,which share smaller overlaps than the imagesfrom the NeRF-Synthetic dataset. Consideringthe limitations of memory, we reduce the size of the original high-resolution images by using bicubicinterpolation. As the GPS information may not be reliable and the positions in GPS may shift inmeters, we preprocess the poses from GPS with COLMAP and add a small noise to the calibratedposes, following L2G-NeRF . The UrbanScene3D dataset contains high-precision LiDAR scans for the target buildings, PolyTechand ArtSCi. Therefore, we evaluate the reconstructions quality with the point cloud scans as theground truths. We crop the reconstructed meshes according to the LiDAR scans and align them usingICP. NeRF, BARF, and L2G-NeRF produce blurry renderings and degenerated meshes for the realscenes, while SPARF may fail to process such data with dense correspondences. Therefore, we onlyprovide the visual results of NeuS, PET-NeuS, and our method. The quantitative results of NeuS, PET-NeuS, and our method are provided in . As shows, NeuS and PET-NeuS tend to produce over smoothing results, while our method is able toextract meshes with fine details. A mesh reconstructed directly from the original high-resolutionimages using ContextCapture2, a commercial MVS solution, is provided as a reference.",
  "Ablation Study": "Comparison on varying noise levels.To evaluate model robustness, we evaluate on the LEGOdata sample with poses at varying noise levels. The high noise level means we use the same noisesetting as L2G-NeRF ; the low noise level means we perturb the ground-truth poses with halfof the noise as L2G-NeRF; and w/o noise means the poses are initialized as ground-truth poseswithout noise. The transformation matrices of the camera poses are all set as variables to be optimized. summarizes the results. We can see that NeuS and PET-NeuS are more sensitive tonoise and cannot effectively handle the high-noise setting, while the other methods can not produce accuracy reconstructions. With the contextual information learned in the camera ray matching and theexplicit utilization of matched rays, CRAYM is able to obtain better results for all the noise settings.",
  "Baseline27.300.910.100.06+ KRE28.640.930.070.05+ KRE + MRC30.410.950.040.04+ Le29.430.920.070.05+ Le + La29.950.940.060.04Our full pipeline31.600.960.030.04": "Ablation of major modules and losses.Todemonstrate the efficiency of the proposed mod-ules and geometric losses, we conduct an abla-tion study on the LEGO data sample. The re-sults are reported in . Similar with BARF,which applies a smooth mask on the encodingat different frequency bands for neural radiancefield, we apply a progressive feature mask on thehash encoding with a coarse-to-fine training ofthe neural implicit field as our baseline, whichcombines BARF and NeuS2. The KREmodule improves the robustness to noisy posesin the training, whereas the MRC module ef-fectively enhances the quality of the volumerenderings with ray matching. In addition tothat, the proposed geometric losses further helpour framework to obtain better camera pose op-timization.",
  "Conclusion and discussion": "Our method, CRAYM, addresses the issue ofnoise camera poses for multi-view 3D recon-struction and view synthesis. The key idea isto jointly optimize a neural field and cameraposes by incorporating contextual information(via KRE) and enforcing geometric and pho-tometric consistency (via MRC and geometriclosses) through camera ray matching. Experiments demonstrate that our method out-performs state-of-the-art alternatives under vari-ous settings: dense- vs. sparse-views, and differ-ent noise levels. However, the implicit field andoptimizable pose transformations may not con-verge when the poses are randomly initializedor extremely noisy. A stronger pose regulariza-tion prior to the field optimization may resolvethis problem. Furthermore, the meshes extractedfrom the constructed SDFs may still contain messy inner structures over invisible areas. A promising future work is to apply the ray matching to the 3D Gaussian splatting, which will greatlyimprove the rendering efficiency of CRAYM. However, extracting reconstructions with fine geometricstructures from 3D Gaussians is still an open problem. Finally, CRAYM has been designed to rely on sparse key rays for dense-view reconstruction, whilea dense counterpart may bring up extra overhead. In our Matched Ray Coherency formulation, weexplicitly account for potentially erroneous (i.e., low-quality) 2D matches by using the matchabilitybetween two rays as a weight to either accentuate or discount the color consistency constraint. Interms of sensitivity with respect to the density of the 2D matches, in our experiments, we haveobserved that even with sparse input views and sparsely distributed matched rays, CRAYM can stillnotably improve the optimization convergence. An effective approach to utilize ray matching forboth sparse and dense inputs may further boost the performance of CRAYM.",
  "Acknowledgement": "We thank the reviewers for their valuable comments. This work was supported in parts by NSFC(U21B2023, U2001206), ICFCRT(W2441020), Guangdong Basic and Applied Basic ResearchFoundation (2023B1515120026), DEGP Innovation Team (2022KCXTD025), Shenzhen Scienceand Technology Program (KQTD20210811090044003, RCJC20200714114435012), and ScientificDevelopment Funds from Shenzhen University. Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla,and Pratul P Srinivasan. Mip-NeRf: A multiscale representation for anti-aliasing neural radiancefields. In Proc. IEEE Conf. on Computer Vision & Pattern Recognition, pages 58555864, 2021. Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-NeRF:Optimising neural radiance field with no pose prior. In Proc. IEEE Conf. on Computer Vision &Pattern Recognition, pages 41604169, 2023. Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and HaoSu. MVSNeRF: Fast generalizable radiance field reconstruction from multi-view stereo. InProc. Int. Conf. on Computer Vision, pages 1412414133, 2021.",
  "Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicitcorrespondence matching for generalizable neural radiance fields, 2023": "Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. MobileNeRF:Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobilearchitectures. In Proc. IEEE Conf. on Computer Vision & Pattern Recognition, pages 1656916578, 2023. David Crandall, Andrew Owens, Noah Snavely, and Dan Huttenlocher. Discrete-continuousoptimization for large-scale structure from motion. In Proc. IEEE Conf. on Computer Vision &Pattern Recognition, pages 30013008, 2011. Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperPoint: Self-supervisedinterest point detection and description. In Proc. IEEE Conf. on Computer Vision & PatternRecognition, pages 224236, 2018. Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and AngjooKanazawa. Plenoxels: Radiance fields without neural networks. In Proc. IEEE Conf. onComputer Vision & Pattern Recognition, pages 55015510, 2022.",
  "John R Hurley and Raymond B Cattell. The procrustes program: Producing direct rotation totest a hypothesized factor structure. Behavioral science, 7(2):258, 1962": "Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aans. Large scalemulti-view stereopsis evaluation. In Proc. IEEE Conf. on Computer Vision & Pattern Recogni-tion, pages 406413, 2014. Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and JaesikPark. Self-calibrating neural radiance fields. In Proc. IEEE Conf. on Computer Vision & PatternRecognition, pages 58465854, 2021.",
  "Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, recon-structing, and simulating: the UrbanScene3D dataset. In Proc. Euro. Conf. on Computer Vision,pages 93109, 2022": "Sainan Liu, Shan Lin, Jingpei Lu, Alexey Supikov, and Michael Yip. BAA-NGP: Bundle-adjusting accelerated neural graphics primitives. In Proc. IEEE Conf. on Computer Vision &Pattern Recognition, pages 850857, 2024. Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. SparseNeuS:Fast generalizable neural surface reconstruction from sparse views. In Proc. Euro. Conf. onComputer Vision, pages 210227, 2022. Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, RaviRamamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesiswith prescriptive sampling guidelines. ACM Trans. on Graphics, 2019. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi,and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In Proc.Euro. Conf. on Computer Vision, 2020.",
  "Aditya Vora, Akshayand Gadi Patil, and Hao Zhang.DiViNeT: 3D reconstruction fromdisparate views via neural template regularization. In Proc. Euro. Conf. on Computer Vision,pages 210227, 2022": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang.NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction.Proc. Conf. on Neural Information Processing Systems, pages 2717127183, 2021. Yiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and LingjieLiu. NeuS2: Fast learning of neural implicit surfaces for multi-view reconstruction. In Proc.Int. Conf. on Computer Vision, 2023.",
  "Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF: Neuralradiance fields without known camera parameters. arXiv preprint arXiv:2102.07064, 2021": "Yiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan, FedericoTombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. Neural fields in visualcomputing and beyond. Computer Graphics Forum, 2022. Weidan Xiong, Hongqian Zhang, Botao Peng, Ziyu Hu, Yongli Wu, Jianwei Guo, and HuiHuang. Twintex: Geometry-aware texture generation for abstracted 3D architectural models.ACM Trans. on Graphics (Proc. SIGGRAPH Asia), pages 227:1227:14, 2023. Yifan Yang, Shuhai Zhang, Zixiong Huang, Yubing Zhang, and Mingkui Tan. Cross-ray neuralradiance fields for novel-view synthesis from unconstrained image collections. In Proc. Int.Conf. on Computer Vision, pages 1590115911, 2023.",
  "Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicitsurfaces. Proc. Conf. on Neural Information Processing Systems, pages 48054815, 2021": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreason-able effectiveness of deep features as a perceptual metric. In Proc. IEEE Conf. on ComputerVision & Pattern Recognition, pages 586595, 2018. Kun Zhou, Wenbo Li, Yi Wang, Tao Hu, Nianjuan Jiang, Xiaoguang Han, and Jiangbo Lu.NeRFLiX: High-quality neural view synthesis by learning a degradation-driven inter-viewpointmixer. In Proc. IEEE Conf. on Computer Vision & Pattern Recognition, pages 1236312374,2023. Xiaohui Zhou, Ke Xie, Kai Huang, Yilin Liu, Yang Zhou, Minglun Gong, and Hui Huang.Offsite aerial path planning for efficient urban scene reconstruction. ACM Trans. on Graphics(Proc. SIGGRAPH Asia), pages 192:1192:16, 2020.",
  "A.1.1Camera Pose Parameterization": "The camera poses {Ti}, where Ti = [Ri|ti] SE(3), need to be parameterized and optimized inthe training process. As a high frequency operation, pose parameterization needs to be simple andefficient, since the conversion between pose parameterization and transformation matrix is performedin every training iteration. The position component t of T is simply represented as a three-dimensional vector. Instead ofparameterizing the rotation R as an exponential map exp(r) from the Lie algebra so(3) to the Liegroup SO(3), we represent R as a six-dimensional vector composed by R = [va|vb], va R3,vb R3. va, vb, and vc = va vb span the three bases of the camera space. To obtain rotationmatrix R from R, we perform a Gram-Schmidt orthonormalization on va, vb, and vc, resulting inthree orthonormal bases of the camera space, va, vb, and vc. Thus, the rotation matrix is",
  ": Visualization of the matches between two images from the LEGO scene in the NeRF-Synthetic dataset": "shows an example of matched pixels. Due to the noise in the camera poses, the surfacepoint captured by the associated keypoint on the input image may not intersect with the key ray rkexactly, the integration of features from the neighboring points {qi} sampled by auxiliary rays {ro}can greatly facilitate a more stable optimization associated with key ray rk; see . Procedure-wise, we propose to first fuse the feature of pk with the features of the surrounding {qi}to produce the enriched feature f (pk), which can better describe the local radiance feature andgeometry feature of the captured object with structure information.",
  "i=1Softmax(f(pk) f(qi)) f(qi).(9)": "The features {f(qi)} of the auxiliary rays {ra} remain untouched: f (qi) = f(qi). Further, we adoptthe geometry network g to process the features f (p) of both key rays and auxiliary rays to extractthe SDF value at point p in the radiance field. From the extracted SDF values, we can then producethe 3D reconstruction of the target object:",
  "The color of point p sampled on key rays or auxiliary rays can then be obtained with the texturenetwork t asc(p) = t(f (p), rd, Normal(p)),(11)": "where rd is the normalized direction of ray r and Normal(p) is the normal of the implicit surface atp computed as the gradient of SDF(p). We take the normal at p into account to boost the training oft. : Illustrating the Key Ray Enrichment Module. The yellow ray rk is the key ray and thegray rays {ra} are the neighboring auxiliary rays. The surface point (the blue point) captured bythe associated keypoint (the red point) on the input image may not intersect with rk exactly dueto the unreliable pose, while we can learn the contextual information of the surface point with theneighboring rays of point pk sampled on rk.",
  "A.2Training Details": "We use a 16-level hash grid to encode the feature volume V. The feature length of each levelis two. Therefore, the base resolution of V is 32. We apply a progressive feature mask onthe hash encoding, which starts at level 4 and is updated to the next level every 1,000 iterations.The geometry network g is implemented as a three-layer MLP with the ReLU activation for theinput and hidden layers. The texture network t is implemented as a four-layer MLP with the ReLUactivation for the input and hidden layers. The ray directions are encoded using the spherical harmonicrepresentation and fed into the texture network t together with the output features of g topredict the color of the sampled points on the ray. The whole network is optimized with the AdamWoptimizer with a learning rate of 0.01, = [0.9, 0.99], and = 1.015. The variance of thegeometry network g is initialized as 0.3 and is optimized with a learning rate of 0.001. We adopt awarm-up training for the first 500 iterations with the LinearLR scheduler. All the experiments areconducted with an Nvidia GV100.",
  "A.3Reconstruction Quality": "Next, we report the Hausdorff distance (HD), precision, recall, and F-score for the reconstructionquality evaluated on the NeRF-Synthetic dataset and the UrbanScene3D dataset. TheNeRF-Synthetic dataset contains eight synthetic objects, which are captured with 100 images. Weevaluate our method on the two real scenes, PolyTech and ArtSci, from the UrbanScene3D dataset, on which we measure the quality of the reconstructions with the high-resolution LiDARscans as the ground truths. Tables 7 and 8 report the reconstruction quality, compared with NeRF , BARF , SPARF ,and L2G-NeRF on the two datasets. A threshold of 0.01 is used to extract the inliers and outliersfor the calculation of the precision, recall, and F1 score. The precision measures the reconstructionaccuracy by calculating the distances from the reconstructed models to the ground truths. Therecall measures the reconstruction completeness by determining the extent of the ground-truthpoints covered by the reconstructed models. A high F-score means both high accuracy and highcompleteness of the reconstructed models. As we can see from Tables 7 and 8, our method is ableto produce high-quality reconstructions with both high accuracy and high completeness. It is worthto note that compared with other methods, our method achieves the top performance on all metricsconsistently for both datasets.",
  "A.4Results on Real Scenes": "Further, we evaluate our method on the real scene Bank from TwinTex , which is a set ofhigh-resolution drone-captured images. We preprocess the images with COLMAP to obtain thecalibrated poses and perturb the poses with additive noise , where se(3) and N(0, nI), asthe initial poses, following the procedure on real scenes PolyTech and ArtSci of UrbanScene3D .For these real scenes, we set n as a small value 0.01. Since TwinTex does not provide a LiDAR scan for the scene Bank, we only report the evaluationresults of novel view synthesis in . shows the comparison on the real scene Bankvisually with NeuS and PET-NeuS . While the other methods tend to generate renderingswith blurriness, our method is able to produce sharper results with more fine details. furthershows results of our method on another real scenes CSSE.",
  "A.5Results on the DTU Dataset": "The DTU dataset is aimed at multi-view stereo (MVS) evaluation, containing image sets capturedwith structured light scanners mounted on an industrial robot arm. We evaluate our method on the firstfive image sets used in the NeuS , comparing with NeuS , PET-NeuS , and SPARF .Each image set contains 48 images. We use 90% of them as training set and the remaining 10%images as testing data. The quantitative results of novel view synthesis on these data are shown inthe . shows some of the novel view synthesis results visually. As we can see in, our method produces renderings with much more details.",
  "A.6Sparse Views": "Since SPARF is originally designed for sparse input with re-projection loss of dense pixel corre-spondences, we further evaluate our method on the LEGO data with sparse input, which containsonly three views as the training images. The comparison of SPARF and our method is shown in. Though CRAYM aims for bundle-adjusting neural implicit field with dense views as input,it still obtains a result comparable with SPARF, for sparse input views."
}