{
  "Abstract": "3D Referring Expression Segmentation (3D-RES) aims to segment 3D objectsby correlating referring expressions with point clouds. However, traditional ap-proaches frequently encounter issues like over-segmentation or mis-segmentation,due to insufficient emphasis on spatial information of instances. In this paper,we introduce a Rule-Guided Spatial Awareness Network (RG-SAN) by utilizingsolely the spatial information of the target instance for supervision. This approachenables the network to accurately depict the spatial relationships among all entitiesdescribed in the text, thus enhancing the reasoning capabilities. The RG-SANconsists of the Text-driven Localization Module (TLM) and the Rule-guided WeakSupervision (RWS) strategy. The TLM initially locates all mentioned instances anditeratively refines their positional information. The RWS strategy, acknowledgingthat only target objects have supervised positional information, employs depen-dency tree rules to precisely guide the core instances positioning. Extensive testingon the ScanRefer benchmark has shown that RG-SAN not only establishes newperformance benchmarks, with an mIoU increase of 5.1 points, but also exhibitssignificant improvements in robustness when processing descriptions with spatialambiguity. All codes are available at",
  "Introduction": "3D Referring Expression Segmentation (3D-RES) is an emerging field that segments 3D objects inpoint cloud scenes based on given referring expressions . Gaining significant attention for itsapplications in autonomous robotics, human-machine interaction, and self-driving systems, 3D-RESdemands a deeper understanding than 3D Referring Expression Comprehension (3D-REC) , which focuses only on locating the referring objects via bounding boxes. 3D-RES, on theother hand, requires identifying instances and providing precise 3D masks. Early 3D-RES approaches adopted a two-stage paradigm, starting with an independent text-agnostic segmentation model for generating instance proposals, followed by linking these proposalswith textual descriptions. This paradigm, separating segmentation and matching, proved suboptimalin performance and efficiency. Recent explorations have shifted towards an end-to-end paradigm. Forinstance, 3D-STMN achieved efficient segmentation by directly matching superpoints with text,while 3DRefTR integrated 3D-RES and 3D-REC into a unified framework using a multi-task",
  "arXiv:2412.02402v2 [cs.CV] 22 Dec 2024": "approach, boosting inference in both tasks. Despite these advancements, limitations persist, primarilydue to over-reliance on textual reasoning and insufficient modeling of spatial relationships betweeninstances. For example, as shown in , without spatial modeling, its challenging to understandand correctly segment the intended chair in scenarios involving complex spatial terms like far away.",
  "(75,80,16) (73,90,18)": ": Illustration with a target object and mul-tiple auxiliary objects, associated with a referringexpression. The target marked in green representsthe main referred instance, while targets in othercolors indicate other mentioned entities. This vi-sual highlights the challenge of effectively com-pleting semantic reasoning in the absence of spatialinference. To tackle this issue, the core is to assist textualreasoning by modeling the spatial relationshipsof core instances. By effectively identifyingthese spatial relationships within expressions,a substantial improvement can be achieved incomprehending spatial arrangements. Neverthe-less, this endeavor is not without its challenges.While accurate positional information is crucialfor ensuring precise modeling of spatial relation-ships, accurately regressing instance positionsfrom textual information is far from a simpletask. Furthermore, our available positional in-formation is limited to the target instance, leav-ing us without supervisory signals for other in-stances referenced in the expression. To overcome these challenges, we propose thenovel Rule-Guided Spatial Awareness Network(RG-SAN), utilizing the spatial information ofthe target instance for supervision. This enablesthe network to accurately depict spatial relation-ships among all text-described entities, therebysignificantly enhancing the models inferenceand pointing capabilities. RG-SAN consists oftwo main components: the Text-driven Localiza-tion Module (TLM) and the Rule-guided WeakSupervision (RWS) strategy. TLM initially lo-cates all mentioned instances and iteratively refines their positions, ensuring continuous improvementin location accuracy. RWS, leveraging dependency tree rules, precisely guides the positioning ofcore instances. This focused supervision significantly improves the handling of spatial ambiguitiesin referring expressions. Extensive testing on the ScanRefer benchmark shows that RG-SAN notonly sets new performance standards, with a mIoU increase of 5.1 points, but also greatly enhancesrobustness in processing spatially ambiguous descriptions.",
  "D Referring Expression Comprehension and Referring Expression Segmentation": "Referring Expression Comprehension (REC) is proposed to locate the referred target from a shortdescription of visual space by bounding boxes , which is part of vision-language tasks . Recent works in 3D-REC can be divided into two parts, two-stage andsingle-stage. As for two-stage methods , 3D object proposals are generateddirectly from ground-truth or extracted by a pre-trained 3D object detector in the first stage,",
  "and then assigned to language in the second stage. In the other way, some methods adopt a one-stageparadigm , enabling end-to-end training": "Referring Expression Segmentation (RES) need fine-grained vision-language alignment , proposed to locate the referred target by masks . TGNN introduce 3D-RES byextending the bounding box annotations of ScanRefer to masks by incorporating the instancemasks from ScanNet and proposed a two-stage pipeline. Further, 3D-STMN proposed anend-to-end method that matches the text and superpoints to get the 3D segmentation of the targetobject directly.",
  "D Human-AI Interaction": "ScanQA has notably advanced visual question answering in 3D scenes, enhancing the human-AI in-teraction experience. Meanwhile, 3D-LLM , 3D-VisTA , NaviLLM , and BridgeQA have further propelled this task. Li et al. , Lu et al. have explored how AI understandshuman instructions like gestures and language to locate targets. 3D-VisTA introduced a newparadigm for large-scale 3D vision-language pre-training, greatly enhancing AIs understanding of 3Dvision-language and advancing various downstream tasks. Works like 3D-LLM , Chat3D ,NaviLLM and Scene-LLM have extended the capabilities of multimodal large languagemodels to the 3D realm, endowing embodied intelligence with the rich knowledge and capabilities ofLLMs, thus ushering in the era of large models in Human-AI Interaction.",
  "Weakly Supervision in Vision-and-Language": "In the field of Vision Language, weakly supervised have gained significant attentionand great progress. These approaches aim to tackle the challenge of limited or incomplete annotationsby leveraging alternative supervised data or weakly labeled data. For weakly supervised visualquestion answering (VQA), Kervadec et al. employ weak supervision in the form of object-wordalignment as a pre-training task. Trott et al. use object counts in images as weak supervisionto guide VQA for counting-based questions. Gokhale et al. employ logical connective rulesto augment training datasets for yes-no questions. Weakly supervision from captions has also beenemployed for visual grounding tasks recently. Especially, for RES, some methods localize the target object only using readily available image-text pairs.",
  "Visual Encoding": "Given a point cloud scene Pcloud RNp(3+F ) with Np points. Each point comes with 3Dcoordinates along with an F-dimensional auxiliary feature that includes RGB, normal vectors, amongothers. We first employ a Sparse 3D U-Net to extract point-wise features, represented asPcloud RNpCp. Then, we follow Sun et al. and Wu et al. to obtain Ns superpoints{Ki}Nsi=1 from the original point cloud. Finally, we directly feed point-wise features Pcloud intosuperpoint pooling layer based on {Ki}Nsi=1 to obtain the superpoint-level features Sp RNsCp.",
  "Textual": ": An overview of the proposed RG-SAN. This model analyzes a point cloud and a textualdescription with Nt tokens, extracting superpoints and word-level features. The TLM assigns spatialpositions to tokens, facilitating multimodal fusion. The RWS strategy enables the model to learn thepositions of all mentioned entities using only the supervision of the target position.",
  "Context-driven Spatial Awareness": "In this section, we address a key limitation in prior works that interact point clouds with text withoutconsidering spatial positioning . Unlike these methods, which often lose spatial informa-tion due to unordered point cloud features, leading to ambiguous spatial relationship understanding,our approach is distinct. In 3D-RES, spatial information is inherently sparse and dynamic, dependingon the specific target object described in the text, rather than the dense, static sampling of an entirepoint cloud scene . To address this issue, we propose to facilitate interactions between textual entities and point cloudswithin 3D space, rather than merely at the semantic level. Specifically, our objective is to fullyleverage semantic and spatial contextual information to accurately predict the spatial positions of allmentioned nouns within the point cloud. Therefore, we introduce the Text-driven Localization Module (TLM) to initialize the positionsof entity nouns in the text and continuously update and refine these positions through iterativemultimodal interactions.",
  "j=1AijSv,j,(4)": "where Psj is the position of the j-th superpoint, Pt0,i is the initial spatial position of i-th wordtoken which will be refined iteratively as formulated in Sec. 3.2.2, Wv RDD denotes learnableparameters, and E0,i denotes the updated representation of the i-th word token. The sharing ofdistribution A during centroid computation allows the entity representations to benefit from theguidance provided by spatial information, leading to a more accurate understanding of the 3Dspatial relationships. Subsequently, the text and point clouds undergo multiple rounds of multimodalinteractions, continually updating the embeddings and positions of the entities. Iterative Position Refinement. After l-round multimodal interactions, the word tokens El, referredto as textual segment kernels, become increasingly precise, theoretically resulting in more accurateposition predictions. A straightforward approach would involve replicating the initial interactionmethod by regressing position information in each round. However, following the methodologies ofRedmon et al. and Lai et al. , rather than directly optimizing the final position, we adopta more manageable strategy of iteratively learning offsets. To this end, we refine the positions oftextual tokens based on the evolving textual segment kernels. As depicted in , we employ aMultilayer Perceptron (MLP) to predict a position offset Ptl = MLP(El+1) RN t3 from theupdated textual segment kernels El+1. This offset is then added to the previous textual positions Ptl:",
  "Rule-guided Target Selection": "In the preceding sections, we initially predicted the locations of all entities mentioned in the text.Ideally, supervised training would require position labels for each entity. However, we only haveaccess to the location information of the target instance. This constraint leads us to adopt a weaksupervision approach, focusing solely on the position of the referring instance for training. Thisapproach introduces a significant challenge: accurately identifying the referring instance amongthe mentioned nouns. To address this, we utilize a pre-processed dependency tree, as outlinedin Manning et al. , to accurately pinpoint the core noun, typically the subject of the sentence. Wehave developed a set of manual rules, based on this more general dependency tree, to enhance theidentification process. These rules are specifically designed to guide the accurate positioning of coreinstances. The implementation of these rules is outlined in Algorithm 1. Algorithm 1 Rule-guided Target SelectionInput: The dependency tree G = (V, E) of the textual description, where V = {token} denotes theset of nodes, E = {(relation, head, tail)} denotes the set of relations between nodes.",
  "where Ml+1 RNs, Maskl+1 {0, 1}Ns are the predicted response map and the instance maskcorresponding to the target": "Given ground-truth binary mask of the referring expression Y {0, 1}Np, we get the correspondingsuperpoint mask Ys {0, 1}Ns by superpoint pooling follewed by a 0.5-threshold binarization, andthen we apply the binary cross-entropy (BCE) loss on the final response map Ml+1 following Sunet al. . The operation can be written as:",
  "Experiment Settings": "In our experiment, we utilize the pre-trained Sparse 3D U-Net method to extract point-wise featuresfrom point clouds . We also employ the pre-trained MPNet model as our text encoder.For the rest of the network, training is conducted from scratch. We set an initial learning rate of0.0001 and apply a learning rate decay at epochs 26, 34, and 46, each with a decay rate of 0.5. Ourexperiments use a default of 6 multiple rounds L, a batch size of 32, and a maximum sentence lengthof 80. We set bce = dice = 1, pos = score = 0.5. All experiments are conducted using PyTorchon a single NVIDIA Tesla A100 GPU, ensuring consistency in our computational process.",
  "Dataset and Evaluation Metrics": "We evaluate our method using the ScanRefer dataset, a recent 3D referring dataset , comprising51,583 English natural language expressions referring to 11,046 objects across 800 ScanNet scenes .Following Chen et al. , our evaluation metrics include mean Intersection over Union (mIoU) andAcc@kIoU. Unique refers to cases where the target instance is the only one of its class, andMultiple indicates situations where there is at least one more object of the targets class. : The 3D-RES results on ScanRefer. The mIoU and accuracy are reevaluated on our machine.We reproduce results by extracting points within the boxes as segmentation mask predictions usingtheir official codes.",
  "Quantitative Comparison": "In our experiments on the ScanRefer dataset, our proposed RG-SAN demonstrates significantimprovements in nearly all metrics on the single-task leaderboard, as shown in Tab. 1. Notably,RG-SAN shows substantial gains compared to the state-of-the-art single-task model 3D-STMN, withincreases of 5.1 points in mIoU and 7.1 points in . This highlights our models inferencingcapability. A more detailed examination reveals that the majority of these improvements occurin scenarios with multiple disruptive instances, where RG-SAN achieves a remarkable 6.3-pointincrease in mIoU. This setting, where the target instance is among other instances of the sametype, demands discriminative reasoning from the model. The significant performance validatesthe enhanced referring capabilities empowered by spatial reasoning. Our proposed RG-SAN alsooutperforms multi-task models , including LLM-based models , in most 3D-RESmetrics, despite those models benefiting from more annotated data. Moreover, RG-SAN has competitive inference costs, being only 12ms slower than the efficient3D-STMN and faster than all other compared models, demonstrating its high performance withminimal computational increase.",
  "Text-driven Localization Module": "We conduct an ablation study on the Text-driven Localization Module (TLM), as illustrated inTab. 2. Simultaneously, we perform a fine-grained analysis of various initialization schemes forembeddings and positions. The term \"w/o TLM\" denotes the approach of not modeling positionalinformation and instead directly using text embeddings for interaction. \"MAFT\" refers to the directadaptation of the method proposed in . The \"Project\" method involves initializing embeddingsbased on text-driven embeddings and then projecting each textual token directly into a 3D position,while the \"Random\" method randomly assigns a position to each textual token. Finally, we utilizethe initialization technique called Text-driven Initialization (TI), which simultaneously initializesboth embeddings and positions in a text-driven manner. Tab. 2 clearly shows that, under identicalconditions, TI outperforms the others in all metrics. This indicates that TI more effectively leveragespositional information from the visual scene, leading to more precise initial positions for the textualtokens. Consequently, this reduces the complexity of the subsequent iterative refinement process,thereby enhancing the overall accuracy of our model in spatially aligning text with point cloud data.Additionally, Tab. 2 demonstrates that proper initialization leads to the superior performance of TLMcompared to the methods without TLM.",
  "Positional Encoding": "We compare various positional encoding methods previously employed in . These methodsinclude Fourier Absolute positional encoding (APE), 5D Euclidean Relative positional encoding(5D Euclidean RPE) , and Table-based Relative positional encoding (Table-based RPE) .Tab. 3 reveals that Table-based RPE surpasses the other methods, suggesting that combining semanticinformation with relative relationships is advantageous. Additionally, we observe that employing onlyabsolute positional encoding can result in lower performance than not using any positional encodingat all. This may be attributed to the inherent limitations of absolute positional encoding in capturingrelative positional information. By complicating the semantic features, it introduces challenges inthe models training process, underscoring the importance of choosing the right positional encodingtechnique for effective performance.",
  "Rule-guided Weak Supervision": "We conducted experiments employing various weakly supervised text kernel selection strategies toevaluate their efficacy in leveraging target annotations. The strategy labeled as \"w/o RWS\" involvesselecting the token based on attention weight within the cross-attention module , while \"Root\"entails selecting the root token of the dependency tree. illustrates that utilizing the rootnode as supervision slightly outperforms the \"w/o RWS\" baseline. This is likely due to the rootnode providing consistent supervision, whereas Top1 tends to select different nodes variably, whichcomplicates the training process. In contrast, our Rule-guided Target Selection (RTS) strategy, basedon dependency tree rules to locate subjects, aligns more effectively with the structural nature of thetext. It precisely identifies the target entitys position, significantly enhancing annotation utilizationand effectively directing model training. This leads to a notable improvement in model performance. Furthermore, we conduct an ablation study on the impact of the position loss weight Lpos, detailed inTab. 5. We observe that increasing the weight generally improves performance, peaking at a weightof 0.5, beyond which performance begins to taper off. This finding highlights the importance ofbalancing the weight of the position loss to optimize the models effectiveness.",
  "Comparison with MAFT": "MAFT has played a pivotal role in 3D instance segmentation by incorporating spatial positionmodeling, offering valuable insights into how spatial information can improve model performance.Inspired by this approach, we extend spatial information into the text space to better align visualand textual semantics, specifically targeting spatial relationship reasoning in 3D-RES. Our approachintroduces two key innovations that distinguish it from MAFT: Unlike MAFT , which initializes queries with zeros and uses random initialization for posi-tional information, we employ text-driven queries and positional information to model the spatialrelationships of entities in the expressions. This allows our model to capture the spatial contextbetter, resulting in a 4.4-point improvement in mIoU, as shown in Tab. 2 In contrast to , which supervises the positions of all target instances, 3D-RES supervises onlythe core target word. Our novel RWS method constructs spatial relationships for all noun instancesusing only the target words positional information, resulting in a 2.3-point improvement in mIoU,as demonstrated in Tab. 4.",
  "Qualitative Comparison": "We conduct a qualitative analysis on the ScanRefer validation set as shown in , comparing ourproposed RG-SAN with 3D-STMN to highlight our models exceptional referring capability. demonstrates our models ability to accurately segment not only the target objects but alsoother nouns mentioned in the text. Unlike 3D-STMN, which misattributes all nouns to a singletarget, RG-SAN distinctly recognizes and locates each noun. For example, in -(c), our modelsuccessfully identifies the target chair through relative positioning, even with similar objects in thescene, and accurately recognizes a coat as a supporting element in the description. This abilityextends to -(a) and (b), where RG-SAN correctly segments multiple auxiliary nouns intotheir corresponding instances, demonstrating its robust generalization for complex texts and preciselocalization for multiple entities. Such capabilities enhance the models understanding of complexsemantic scenes, significantly improving its ability to refer to specific entities accurately.",
  "Conclusion": "In this paper, we present RG-SAN to overcome the limitations of traditional 3D-RES methods,particularly their lack of spatial awareness. Specifically, the TLM is introduced to model and refinepositional information, while the RWS is designed to employ dependency tree rules to accuratelyguide the position of the target object. Combining TLM with RWS strategy, RG-SAN significantlyimproves segmentation accuracy and robustly handles spatial ambiguities. Extensive experimentsconducted on the ScanRefer benchmark demonstrate the superior performance of RG-SAN. Thisunderscores the importance of incorporating spatial awareness into segmentation models, paving theway for future advancements in the domain.",
  "Acknowledge": "This work was supported by National Science and Technology Major Project (No. 2022ZD0118201),the National Science Fund for Distinguished Young Scholars (No.62025603), the National NaturalScience Foundation of China (No. U22B2051, No. U21B2037, No. 62072389, No. 62302411, No.623B2088), the Natural Science Foundation of Fujian Province of China (No.2021J06003) and ChinaPostdoctoral Science Foundation (No. 2023M732948). Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d:Neural listeners for fine-grained 3d object identification in real-world scenes. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part I 16, pages422440. Springer, 2020. Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell.Localizing moments in video with natural language. In Proceedings of the IEEE international conferenceon computer vision, pages 58035812, 2017. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answeringfor spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1912919139, 2022.",
  "Dave Zhenyu Chen, Angel X Chang, and Matthias Niener. Scanrefer: 3d object localization in rgb-dscans using natural language. In European conference on computer vision, pages 202221. Springer, 2020": "Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Languageconditioned spatial relation reasoning for 3d object grounding. Advances in Neural Information ProcessingSystems, 35:2052220535, 2022. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niener.Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference oncomputer vision and pattern recognition, pages 58285839, 2017.",
  "Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unifiedpixel-level vision llm for understanding, generating, segmenting, editing, 2024": "Hao Fei, Shengqiong Wu, Meishan Zhang, Min Zhang, Tat-Seng Chua, and Shuicheng Yan. Enhancingvideo-language representations with structural spatio-temporal alignment. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2024. Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong Zhang, Guangming Zhu, Hui Zhang, YaonanWang, and Ajmal Mian. Free-form description guided 3d visual graph network for object groundingin point cloud. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages37223731, 2021.",
  "Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for3d visual understanding and reasoning. arXiv preprint arXiv:2403.11401, 2024": "Xuri Ge, Fuhai Chen, Joemon M Jose, Zhilong Ji, Zhongqin Wu, and Xiao Liu. Structured multi-modal feature embedding and alignment for image-sentence retrieval. In Proceedings of the 29th ACMinternational conference on multimedia, pages 51855193, 2021. Xuri Ge, Songpei Xu, Fuhai Chen, Jie Wang, Guoxin Wang, Shan An, and Joemon M Jose. 3shnet:Boosting imagesentence retrieval via visual semanticspatial self-highlighting. Information Processing &Management, 61(4):103716, 2024.",
  "Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. Vqa-lol: Visual question answeringunder the lens of logic. In European conference on computer vision, pages 379396. Springer, 2020": "Yunpeng Gong, Liqing Huang, and Lifei Chen. Person re-identification method based on color attack andjoint defence. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 43134322, 2022. Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation withsubmanifold sparse convolutional networks. In Proceedings of the IEEE conference on computer visionand pattern recognition, pages 92249232, 2018.",
  "Shuting He, Henghui Ding, Xudong Jiang, and Bihan Wen. Segpoint: Segment any point cloud via largelanguage model. In European Conference on Computer Vision, pages 349367. Springer, 2025": "Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan.3d-llm: Injecting the 3d world into large language models. Advances in Neural Information ProcessingSystems, 36:2048220494, 2023. Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and ZhouZhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. arXiv preprintarXiv:2312.08168, 2023.",
  "Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, and Ming-Hsuan Yang. Reason3d: Searching andreasoning 3d segmentation via large language model. arXiv preprint arXiv:2405.17427, 2024": "Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and Tyng-Luh Liu. Text-guided graph neuralnetworks for referring 3d instance segmentation. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pages 16101618, 2021. Ziling Huang and Shinichi Satoh. Referring image segmentation via joint mask contextual embeddinglearning and progressive alignment network. In Houda Bouamor, Juan Pino, and Kalika Bali, editors,Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 77537762, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.481. URL Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom up top downdetection transformers for language grounding in images and point clouds. In European Conference onComputer Vision, pages 417433. Springer, 2022.",
  "Kanishk Jain and Vineet Gandhi. Comprehensive multi-modal interactions for referring image segmentation.In Findings of the Association for Computational Linguistics: ACL 2022, pages 34273435, 2022": "Corentin Kervadec, Grigory Antipov, Moez Baccouche, and Christian Wolf. Weak supervision helpsemergence of word-object alignment and improves vision-language tasks. arXiv preprint arXiv:1912.03063,2019. Yongmin Kim, Chenhui Chu, and Sadao Kurohashi. Flexible visual grounding. In Samuel Louvan,Andrea Madotto, and Brielen Madureira, editors, Proceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics: Student Research Workshop, pages 285299, Dublin, Ire-land, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-srw.22. URL Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia.Stratified transformer for 3d point cloud segmentation. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 85008509, 2022. Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, and Jiaya Jia. Mask-attention-free transformerfor 3d instance segmentation. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 36933703, 2023. Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpointgraphs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages45584567, 2018. Jungbeom Lee, Sungjin Lee, Jinseok Nam, Seunghak Yu, Jaeyoung Do, and Tara Taghavi. Weaklysupervised referring image segmentation with intra-chunk and inter-chunk consistency. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pages 2187021881, 2023. Hui Li, Mingjie Sun, Jimin Xiao, Eng Gee Lim, and Yao Zhao. Fully and weakly supervised referringexpression segmentation with end-to-end learning. IEEE Transactions on Circuits and Systems for VideoTechnology, 2023. Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian,and Siliang Tang. Fine-grained semantically aligned vision-language pre-training. Advances in neuralinformation processing systems, 35:72907303, 2022. Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-trainingfor unified vision-language understanding and generation. In International conference on machine learning,pages 1288812900. PMLR, 2022. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. In International conference on machine learning,pages 1973019742. PMLR, 2023. Pengfei Li, Beiwen Tian, Yongliang Shi, Xiaoxue Chen, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang.Toist: Task oriented instance segmentation transformer with noun-pronoun distillation. Advances in NeuralInformation Processing Systems, 35:1759717611, 2022.",
  "Yang Li, Xiaoxue Chen, Hao Zhao, Jiangtao Gong, Guyue Zhou, Federico Rossano, and Yixin Zhu.Understanding embodied reference with touch-line transformer. In ICLR, 2023": "Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Transformer-empowered invariantgrounding for video question answering. IEEE Transactions on Pattern Analysis and Machine Intelligence,2023. Yicong Li, Na Zhao, Junbin Xiao, Chun Feng, Xiang Wang, and Tat-seng Chua. Laso: Language-guidedaffordance segmentation on 3d object. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1425114260, 2024. Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3d scenesusing semantic superpoint tree networks. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 27832792, 2021. Haojia Lin, Yongdong Luo, Xiawu Zheng, Lijiang Li, Fei Chao, Taisong Jin, Donghao Luo, ChengjieWang, Yan Wang, and Liujuan Cao. A unified framework for 3d point cloud visual grounding. arXivpreprint arXiv:2308.11887, 2023. Fang Liu, Yuhao Liu, Yuqiu Kong, Ke Xu, Lihe Zhang, Baocai Yin, Gerhard Hancke, and Rynson Lau.Referring image segmentation using text supervision. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 2212422134, 2023. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019. Ziyang Lu, Yunqiang Pei, Guoqing Wang, Peiwei Li, Yang Yang, Yinjie Lei, and Heng Tao Shen. Scaneru:Interactive 3d visual grounding based on embodied reference understanding. In Proceedings of the AAAIConference on Artificial Intelligence, volume 38, pages 39363944, 2024. Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps:Single-stage 3d visual grounding via referred point progressive selection. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1645416463, 2022. Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and DavidMcClosky. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annualmeeting of the association for computational linguistics: system demonstrations, pages 5560, 2014. Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networksfor volumetric medical image segmentation. In 2016 fourth international conference on 3D vision (3DV),pages 565571. Ieee, 2016. Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-Chowdhury. Weakly supervised video momentretrieval from text queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1159211601, 2019.",
  "Wentao Mo and Yang Liu. Bridging the gap between 2d and 3d visual question answering: A fusionapproach for 3d vqa. arXiv preprint arXiv:2402.15933, 2024": "Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detectionin point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision, pages92779286, 2019. Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical featurelearning on point sets in a metric space. Advances in neural information processing systems, 30, 2017. Zhipeng Qian, Yiwei Ma, Jiayi Ji, and Xiaoshuai Sun. X-refseg3d: Enhancing referring 3d instancesegmentation via structured cross-modal graph neural networks. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 38, pages 45514559, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pages 87488763. PMLR,2021. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-timeobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages779788, 2016.",
  "Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted pre-trainingfor language understanding. Advances in Neural Information Processing Systems, 33:1685716867, 2020": "Sanjay Subramanian, William Merrill, Trevor Darrell, Matt Gardner, Sameer Singh, and Anna Rohrbach.ReCLIP: A strong zero-shot baseline for referring expression comprehension. In Smaranda Muresan,Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the As-sociation for Computational Linguistics (Volume 1: Long Papers), pages 51985215, Dublin, Ireland,May 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.acl-long.357.URL Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer for 3d scene instancesegmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages23932401, 2023. Yucheng Suo, Linchao Zhu, and Yi Yang. Text augmented spatial aware zero-shot referring imagesegmentation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association forComputational Linguistics: EMNLP 2023, pages 10321043, Singapore, December 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.73. URL",
  "Wei Xu, Chunsheng Shi, Sifan Tu, Xin Zhou, Dingkang Liang, and Xiang Bai. A unified framework for 3dscene understanding. arXiv preprint arXiv:2407.03263, 2024": "Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo Luo. Sat: 2d semantics assisted training for 3dvisual grounding. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages18561866, 2021. Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng Wang, Zhen Li, and Shuguang Cui. In-stancerefer: Cooperative holistic understanding for visual grounding on point clouds through instancemulti-level contextual referring. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pages 17911800, 2021. Zhi Zhang, Helen Yannakoudakis, Xiantong Zhen, and Ekaterina Shutova. CK-transformer: Commonsenseknowledge enhanced transformers for referring expression comprehension. In Andreas Vlachos andIsabelle Augenstein, editors, Findings of the Association for Computational Linguistics: EACL 2023,pages 25862596, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.findings-eacl.196. URL Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-transformer: Relation modeling for visualgrounding on point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 29282937, 2021.",
  "AThe Critical Role of Spatial Information in 3D-RES Tasks": "Our analysis underscores the pivotal role spatial relations play in 3D-RES tasks. We assessed theScanRefer datasets referring expressions, classifying examples into two categories: those with spatialrelation terms (e.g., left, right, next, bottom and side\") as spatially related, and those withoutas spatially unrelated. Our findings revealed that spatially related samples form about 92% of thedataset, highlighting the prevalence of spatial descriptors. Additionally, the Sr3D dataset consistsentirely of spatially related descriptions, and in the Nr3D dataset, a significant 90.5% of entriesutilize spatial prepositions . This evidence demonstrates the necessity of spatial descriptions inaccurately identifying objects within a scene through natural language, emphasizing the essentialneed for effective spatial relation modeling in 3D-RES tasks.",
  "spatialw/o spatial": ": Statistics of samples in theScanRefer dataset based on the presenceof spatial relation descriptions, wherespatial represents samples with spa-tially related descriptions, while w/ospatial denotes spatially unrelated sam-ples. We extended the 3D-RES task on the ReferIt3D dataset (which is also in English) by integrating instance masksfrom ScanNet and conducting relevant experiments, asshown in Tab. 6.In contrast to the original setup ofReferIt3D, we refrained from using ground truth boundingboxes or masks as input during our experiments, whichsignificantly increased the level of difficulty. Nonethe-less, our model achieved remarkable Acc@50 gains of 5.3points for Sr3D and 2.9 points for Nr3D, accompaniedby mIoU gains of 5.2 points for Sr3D and 1.0 points forNr3D. It is worth highlighting that our results demonstrate excep-tional performance in terms of Acc@50 and mIoU. Thiscan be attributed to the incorporation of spatial informa-tion, which enhances the accuracy of segmentation resultsand addresses the challenges of over-segmentation andunder-segmentation encountered in previous approaches.",
  "C.1Number of Multiple Rounds": "We investigated the impact of varying the number of TLMrounds in our model. Analyzing rows two to five in Tab. 7reveals a consistent pattern: performance improves withmore rounds, reaches its peak at six, and then slightlydeclines. Fewer layers result in insufficient capacity, while an excessive number of layers increasesthe risk of overfitting. Therefore, selecting six layers strikes a balance that yields the best modelperformance. In addition, we conducted ablation experiments to remove the iterative position refinement processat each layer. The results, shown in the first row of Tab. 7, clearly demonstrate the effectiveness ofiterative refinement, leading to a significant improvement.",
  "C.2The Textual Backbone": "In Tab. 8, we compare the effects of commonly used natural language encoders. It can be observedthat our method demonstrates robustness with respect to the selection of the NLP backbone. Andwe achieve the best performance using MPNet . The underperformance of CLIP is under-standable, considering its optimization over a large dataset of text-image pairs. While CLIP excels atextracting representations at the sentence level, it encounters difficulties in comprehending intricate",
  "C.3The Visual Backbone": "We explored alternative visual backbones, including the PointNet++ pretrained by the classicwork 3D-VisTA and another superpoint-based backbone, SSTNet , as detailed in Tab. 9. Ourfindings indicate that the performance with PointNet++, SSTNet and our employed SPFormer arequite comparable, demonstrating the adaptability and effectiveness of our proposed modules acrossdifferent backbone architectures.",
  "DMore Qualitative Analysis": "More qualitative comparison results are illustrated in and , demonstrating the remark-able discriminative ability of our RG-SAN compared to 3D-STMN. showcases RG-SANssuperior performance in accurately localizing target objects, especially in challenging scenarios thatrequire understanding complex positional relationships described in the text. For instance, -(b)illustrates a scenario with numerous distractors and a complex textual description, where 3D-STMNfails, causing over-segmentation. In contrast, RG-SAN accurately discerns and localizes the targetobject amidst distractions, achieving higher-quality segmentation. It is important to highlight thatwhen faced with descriptive text that involves spatial relationship reasoning among multiple instancesmentioned, as seen in all cases in , our RG-SAN demonstrates the capability to preciselylocate and identify the target object. In contrast, 3D-STMN lacks comparable complex reasoningabilities in such scenarios.",
  "SSTNet 53.734.334.959.442.543.2PointNet++ 54.134.636.160.344.244.0SPFormer 55.035.437.461.744.944.6": "In , we visualized the predicted masks of the mentioned instances of our RG-SAN. As can beseen in (b), even if the coat category is not present in the training labels, our RG-SAN is stillable to accurately identify the mentioned coat in the point cloud scene. This is because we alignthe word features from the textual modality and the point cloud features from the visual modality in afine-grained manner through weak supervision. This alignment brings them into the same featurespace, enabling the model to have strong generalization capabilities for unknown semantic categories.This paves the way for future research in weak supervision and open vocabulary. Furthermore, as seen in (f), our RG-SAN is even able to accurately recognize the plural formof the entity noun couches mentioned in the descriptive text, while successfully identifying thetarget object. This capability enables the model to have a more precise and efficient understandingof spatial relationships associated with multiple auxiliary objects, such as between and among,showcasing the powerful spatial relationship modeling ability of our RG-SAN.",
  "where [LIST] is replaced by the input description token list, and => can, 2 denotes the targettoken in the first example is can whose index in the token list is 2": "For the ScanRefer dataset, LLAMA 2 70B produces approximately 80% of target word positions thatalign with the results obtained from our RWS module. In the remaining portion, our RWS moduledemonstrates higher accuracy. This partially indicates that there is still room for improvement inLLAMA 2 70Bs ability to identify target word positions. Conversely, our rule-based approachbenefits from efficient utilization of explicit dependency relationships and exhibits certain advantages.Additionally, LLAMA 2 70B poses a significant computational burden. Taking this into consideration,our adopted RWS approach outperforms LLAMA 2 70B in terms of both accuracy and efficiency.",
  "FLimitations and Broader Impact": "Despite the strong performance of RG-SAN, we identify several limitations that call for furtherimprovement. A primary limitation is its difficulty in accurately localizing plural nouns. Thisissue arises from the method of using a single point for localization, which proves challenging forplural entities in certain contexts. In future work, we will explore using multiple points to delineateboundaries for more precise localization of plural nouns.",
  ": Qualitative comparison between the proposed RG-SAN and 3D-STMN. Zoom in for bestview": "a significant challenge, one that our current model is not sufficiently equipped to address. This lackof robustness can impair the models ability to process such data accurately, leading to unreliableresults in scenarios involving incomplete or corrupted point clouds. Future work will aim to enhancethe models resilience and capability in handling and compensating for data imperfections. RG-SAN is expected to stimulate further development and application of multimodal 3D perception,especially in practical scenarios such as embodied intelligence and autonomous driving. However,when it comes to practical applications, particularly those involving safety and privacy, rigoroustesting is required to ensure compliance with relevant laws and regulations.",
  "GEthics Statement and Licenses": "In our work, there are no human subjects and informed consent is not applicable. Additionally, weuse publicly available text data from the ScanRefer Dataset ( which is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike3.0 Unported License which allows us to use the dataset for non-commercial purposes. For point clouddata, we used the publicly available ScanNet Dataset ( is licensed under the ScanNet Terms of Use, and the code is released under the MIT license.Both the licenses of ScanNet allow us to use the dataset and code for non-commercial purposes. Inthe appendix, we use the ReferIt3D Dataset ( forextra experiments, which is licensed under the MIT license which allows us to use the dataset fornon-commercial purposes.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "The answer NA means that the paper does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: The paper provides detailed information on the computer resources used foreach experiment within Sec. 4 and Appendix.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: The research conducted in the paper adheres to the NeurIPS Code of Ethics,ensuring that all aspects of the work, including the methodology, data handling, and reporting,conform to the ethical guidelines provided.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "Justification: The paper includes a discussion on the potential societal impacts of the workin the Appendix.Guidelines:": "The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification: The research exclusively utilizes open-source, public datasets and does notinvolve high-risk models or scraped data.Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: The paper credits the creators and original owners of all used assets andexplicitly states the licenses and terms of use in Appendix.Guidelines:",
  "The authors should state which version of the asset is used and, if possible, include aURL": "The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}