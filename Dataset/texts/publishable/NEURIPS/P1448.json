{
  "Abstract": "The ALPHA-g experiment at CERN aims to perform the first-ever direct measure-ment of the effect of gravity on antimatter, determining its weight to within 1%precision. This measurement requires an accurate prediction of the vertical positionof annihilations within the detector. In this work, we present a novel approachto annihilation position reconstruction using an ensemble of models based on thePointNet deep learning architecture. The newly developed model, PointNet Ensem-ble for Annihilation Reconstruction (PEAR) outperforms the standard approach toannihilation position reconstruction, providing more than twice the resolution whilemaintaining a similarly low bias. This work may also offer insights for similarefforts applying deep learning to experiments that require high resolution and lowbias.",
  "Introduction": "The ALPHA-g experiment is part of the ALPHA antihydrogen (H) program at CERN, with a goalof measuring the gravitational acceleration of H to a precision of 1% as a step towards under-standing the matter-antimatter asymmetry in the universe . In September 2023, the collaborationpublished the first measurement of the terrestrial gravitational acceleration of antimatter, determiningthat it does indeed fall downwards, but the measurement was far from the 1% precision threshold .",
  "arXiv:2412.00961v2 [physics.data-an] 3 Dec 2024": "In the ALPHA-g apparatus, H atoms are held at a known location in a magnetic trap within a vacuumchamber then, when the magnetic trap is shut down, the H atoms are released. When H hits the sidesof the vacuum chamber, it annihilates with ordinary matter. By recording the vertical position ofannihilation and the time it occurred during magnet ramp-down (and thus the magnetic field), theeffect of gravity can be calculated. The spatial position of the annihilations can be inferred from the charged particles produced inthe annihilation process . The interaction of the charged particles in the radial Time ProjectionChamber (rTPC) produces electrical signals, which are combined to form spacepoints by a suitablesoftware. Each spacepoint is a triplet of Cartesian coordinates that represents the location of theinteraction in the active volume of the detector. The current method for predicting the annihilationpositions, called Helix Fit, begins by grouping these spacepoints into clusters and fitting eachcluster with a 3D helix function. Then, an annihilation position is found at the location where at leasttwo helices pass closest to each other, which is termed the vertex .",
  "yx": ": Conceptual schematics of the vertex reconstruction approach using our deep learning model(bottom), in contrast to the standard method that requires identification of particle tracks and fittinghelix functions (top). In this work, we introduce the PointNet Ensemble for Annihilation Reconstruction (PEAR), whichdirectly predicts the vertical position of the H annihilation for the ALPHA-g experiment fromspacepoints using deep learning. This bypasses the challenging intermediate steps required by HelixFit, such as track identification and fit optimization procedures. Our new approach takes in all theinformation available from the detectors, while the conventional Helix Fit discards spacepoints thatare not identified as specific types of particles. The conceptual schematics of this approach, along withthe Helix Fit procedure, is shown in . Similar machine learning techniques have been appliedin other contexts involving detectors like the Active-Target Time Projection Chamber (AT-TPC),where they were used for nuclear track classification in low-energy nuclear physics experimentsstudying exotic nuclei .",
  "Methods": "Our method uses a modified version of the deep learning model architecture called PointNet .PointNet takes inputs of spacepoints directly as a 3D point cloud, avoiding potential information lossor added computational overhead compared to architectures that require converting point cloud datainto structured formats like 2D grids or voxel representations . PointNet was originally developedfor the segmentation and classification of 3D point cloud data. To use this model for the prediction ofannihilation positions, which is a regression task, the final fully connected layer has been replaced bya linear output and the input transformation layer has been removed. This model was trained using spacepoints from Monte Carlo simulations of the rTPC. A dataset of 2.7million events was generated, where each event corresponds to one H annihilation. Spacepoints areused as model inputs, and the target output value is the true z coordinate of the vertex. This datasetwas split into approximately 80% for training, 10% for validation, and 10% for testing.",
  ": Schematics of our modified PointNet architecture for the vertex reconstruction regressiontask, heavily based on": "During training, we tracked Huber Loss and performance across additional metrics on half ofthe validation dataset each epoch. In particular, we introduced a new z-bias measure called theAbsolute Residual Average (ARA) and saved checkpoints when the model exhibited ARA lowerthan Helix Fit. To calculate the ARA, the detector is divided into 100 mm slices along the z-axis,allowing us to examine the performance of the model at different positions along the detector. Foreach slice (i = 1, . . . , Nslices where Nslices is the number of slices), we compute the residuals(zi = zReconstructed i zSimulated i) and the average of these residuals (i = zi), followed byabsolute averaging of these per-slice averages, as is shown in equation 1.",
  "i|i|.(1)": "From the checkpoints with low ARA recorded on the first half of the validation set, the one with thelowest ARA is selected as this yielded models with low loss while prioritizing minimal z-bias. Thesecond half of the validation set was then used to confirm that the observed performance was notan artifact of checkpoint selection. This splitting of the validation set into two parts allowed us toreserve the test set solely for generating our final published evaluations. We followed this procedureto train three models with identical architectures, each initialized with a different random seed. Weaveraged the predictions of these three models to create an ensemble . In this case, an ensemble ofthree models (each the top epoch of their respective training run), minimized ARA, reduced loss, andimproved the robustness of the model to changes in the data. We normalized the inputs by subtracting the mean z spacepoint value from all spacepoints z-coordinates, as well as from the true z-vertex for each event. The mean z spacepoint value can thenbe added to the normalized z-vertex predicted by the model to get the unnormalized value for use inthe final output. This led to significant improvements in resolution, defined as the ability to preciselydetermine the true z-vertex, as well as training efficiency. This is likely due to both the smallernumerical range that the model has to span and the events being approximately invariant to verticaltranslations, allowing the model to focus on learning the relationship between the relative position ofthe spacepoints and the corresponding z-vertex. By eliminating this redundant degree of freedom,the model focuses on a less complex representation of the data, effectively increasing the densityof meaningful examples per unit of variation in the training space. This normalization method alsoproved highly effective in reducing z-bias by not providing the model access to information thatwould be undesirable to influence the models outputs, in this case, translation along the z-axis.",
  "Results": "Evaluating PEAR on the test dataset and comparing its performance to Helix Fit, we see that itachieves similar z-bias and significantly better resolution. To begin with, (left) shows thereconstructed z-vertex from PEARs predictions versus the simulated z-vertex. There is a strongagreement between the predicted and true values, evidenced by a Pearson correlation coefficient of0.9997.",
  ": Heat plot of predicted z-vertex with PEAR versus true z-vertex (left). Histogram ofresiduals for PEAR and Helix Fit with Gaussian fits (right)": "(right) displays the distributions of the residuals for PEAR and Helix Fit. The narrower peakaround zero from PEAR indicates that it provides a higher resolution than Helix Fit, meaning that, onaverage, the z-vertex reconstruction from PEAR is closer to the true z-vertex compared to Helix Fit. lists the summary statistics for the performance of both methods and shows that PEAR hasbetter overall performance with a Full Width at Half Maximum (FWHM) less than half that of HelixFit while maintaining a statistically consistent ARA, and consistently lower standard deviations ()on the fits applied. We fit two Gaussians with shared means to represent the residual distribution,one for the core and one for the tail. We find that the of each of the constituent Gaussians aresmaller than the corresponding ones for Helix Fit. Moreover, the integrals show us that more of thedistribution is represented by the core Gaussian for PEAR compared to Helix Fit, indicating thatPEAR has residuals that are more tightly concentrated around the mean and has fewer outliers.",
  ": Model performance comparison between PEAR and Helix Fit. Note the ARA calculation isat 100 mm slices and there is a constant component to the fit accounting for <1% of the integral": "A box plot of the residuals is shown in (top), for slices of the detector along the z-axis,showing that PEAR has superior performance within each section of the detector. (bottom)shows the residual mean for each slice along the z-axis, and within the (-800,800) mm region ofinterest, the absolute value of the residual mean is not larger than 0.11 mm for PEAR and 0.18 mm forHelix Fit. The observed increase in residual mean near the detector ends is due to reduced acceptanceat the edges of the detector.",
  "Conclusions and Outlook": "There are various additional optimization steps that could be undertaken to potentially further improvemodel performance. One such example is the use of more modern transformer-based architectures thathave been shown to be highly capable of learning complex patterns from point clouds . However,we decided to end optimization at this point, as the model achieved a performance that significantlysurpassed the existing state-of-the-art, allowing us to prioritize other technical challenges. Owing to its novelty in antihydrogen physics, this paper does not address the issue of interpretability.While the deep learning process is not transparent, the results are in agreement with the expectations : Box plot of residuals from PEAR and Helix Fit predictions for each 200 mm slice ofthe detector (top). For this study, the line within the box denotes the median, the box covers theinterquartile range (IQR), and the whiskers extend to the furthest residuals within 1.5 times the IQRon either side of the distribution. Residual mean (i used in ARA to probe z-basis) for each 200 mmslice of the detector (bottom). Note that the ends of the detector are greyed out, as generally noscientific measurements are made outside these bounds. and provide an excellent starting point for future work. Additionally, unlike Helix Fit, this methodcurrently only outputs the z coordinate of the vertex. Work to extend this model to predict x and ycoordinates of the vertex is currently underway and showing promising results. While this additionalinformation is not needed for gravity measurements, it is useful for interpreting the physics resultsand to apply certain physics cuts to reduce the background (cosmic rays). Finally, it is desirablefor the model to provide uncertainty estimates associated with its predictions. While this has yetto be implemented, we have noticed that high variance among individual PEAR model predictionswithin the ensemble is correlated with poor ensemble predictions and therefore plan to exploreensemble-based uncertainty estimation techniques. Although PEAR has proven to be successful on simulated data, it must be validated on real databefore use in the scientific analysis pipeline. If this performance transfers reasonably well to real data,then the method introduced in this paper will serve as an important tool in aiding ALPHA-g to carryout more precise measurements of the effect of gravity on antimatter and therefore help us betterunderstand the fundamental building blocks of our universe. This validation is actively underway,and initial results show that PEAR works exceedingly well out of the box on calibration data. This paper is solely meant to provide a brief summary of work on this project. As such, a morecomprehensive paper that will provide a thorough description of the methods used here as well asadditional validation of the model on simulation is currently in preparation. For now, more details canbe found by following the links provided in Appendix A, particularly to the code repository whereadditional documentation is provided, and once available, a link to the full paper will be posted.",
  "and Disclosure of Funding": "This work was supported by NSERC, NRC/TRIUMF, CFI. This research was enabled in part bythe computational resources provided by the BC DRI Group and the Digital Research Alliance ofCanada (alliancecan.ca) under Art Olins allocations. The authors thank the anonymous reviewerswho helped improve this manuscript as well as P. A. Amxaudruz, D. Bishop, M. Constable, P. Lu,L. Kurchaninov, K. Olchanski, F. Retiere, and B. Shaw (TRIUMF) for their work on the ALPHA-gdetectors and the data acquisition system.",
  "EK Anderson, CJ Baker, G Bonomi, A Christensen, et al. Observation of the effect of gravity onthe motion of antimatter. Nature, 621(7980):716722, 2023": "M.P. Kuchera, R. Ramanujan, J.Z. Taylor, R.R. Strauss, D. Bazin, J. Bradt, and Ruiming Chen.Machine learning methods for track classification in the at-tpc. Nuclear Instruments and Methodsin Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment,940:156167, 2019. ISSN 0168-9002. doi: URL Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. Pointnet: Deep learning on pointsets for 3d classification and segmentation. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition (CVPR), July 2017.",
  "BCompute Resources": "The training was performed using NVIDIA A100 GPUs on the Narval cluster managed by the DigitalResearch Alliance of Canada. The duration of training a single model was approximately 70 hourson a single A100 GPU with this number continuing to decrease as more GPU resources are utilized.Since we generated an ensemble of three models by selecting the best epoch from three differenttraining runs, the total training time required on one GPU was therefore approximately 210 hours.Performing inference on a batch of 5,000 events with this ensemble takes only approximately 1second to complete on the aforementioned GPU. These inference speeds are sufficiently fast for theALPHA-g experiments needs, given real-time processing is not necessary."
}