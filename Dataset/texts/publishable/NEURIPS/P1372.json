{
  "Yongzhe Jia1, Xuyun Zhang2, Hongsheng Hu3, Kim-Kwang Raymond Choo4,Lianyong Qi5, Xiaolong Xu6, Amin Beheshti2, Wanchun Dou1": "1 State Key Laboratory for Novel Software Technology,Department of Computer Science and Technology, Nanjing University, China2 School of Computing, Macquarie University, Australia3 School of Information and Physical Sciences, University of Newcastle, Australia4 Department of Information Systems and Cyber Security, University of Texas at San Antonio, USA5 College of Computer Science and Technology, China University of Petroleum (East China), China6 School of Computer and Software,Nanjing University of Information Science and Technology, China",
  "Abstract": "Federated learning (FL) has emerged as a prominent machine learning paradigm inedge computing environments, enabling edge devices to collaboratively optimize aglobal model without sharing their private data. However, existing FL frameworkssuffer from efficacy deterioration due to the system heterogeneity inherent in edgecomputing, especially in the presence of domain shifts across local data. In thispaper, we propose a heterogeneous FL framework DapperFL, to enhance modelperformance across multiple domains. In DapperFL, we introduce a dedicatedModel Fusion Pruning (MFP) module to produce personalized compact localmodels for clients to address the system heterogeneity challenges. The MFP moduleprunes local models with fused knowledge obtained from both local and remainingdomains, ensuring robustness to domain shifts. Additionally, we design a DomainAdaptive Regularization (DAR) module to further improve the overall performanceof DapperFL. The DAR module employs regularization generated by the prunedmodel, aiming to learn robust representations across domains. Furthermore, weintroduce a specific aggregation algorithm for aggregating heterogeneous localmodels with tailored architectures and weights. We implement DapperFL on a real-world FL platform with heterogeneous clients. Experimental results on benchmarkdatasets with multiple domains demonstrate that DapperFL outperforms severalstate-of-the-art FL frameworks by up to 2.28%, while significantly achievingmodel volume reductions ranging from 20% to 80%. Our code is available at:",
  "Introduction": "Federated Learning (FL), an emerging distributed machine learning paradigm in edge computingenvironments , enables participant devices (i.e., clients) to optimize their local models whilea central server aggregates these local models into a global model . In contrast to traditionalcentralized machine learning paradigms, FL facilitates the collaborative training of a global model bydistributed clients without the need for transmitting raw local data, thus mitigating privacy concernsassociated with data transmission .",
  ": A motivational example of DapperFL with heterogeneous devices and multiple domains": "However, there are a number of challenges associated with FL deployments in edge computingsettings, such as: a) System heterogeneity. Within prevalent FL environments, participant clientsgenerally exhibit diverse and constrained system capabilities (e.g., CPU architecture, availablememory, battery status, etc.). This heterogeneity often results in low-capability clients failing tocomplete local training of FL, consequently diminishing the performance of the aggregated globalmodel . b) Domain shifts. Owing to the distributed nature of FL, the data distributions amongparticipant clients vary significantly . visually demonstrates the impact of these issues onFL performance, thus serving as the motivation for our work. Specifically, a) System heterogeneity:Device 1, with stringent resource constraints, fails to complete its model update within the maximumallowed time. As a result, the central server does not consider Device 1s model update, thus excludingit from the FL process and missing out on the data features captured by Device 1. b) Domain shifts:Devices 2 and 3 collect data from different domains, leading to diverse local models. With non-IIDdata, the global model aggregation may become biased, particularly if Device 3 contributes moredata, thus potentially limiting the benefit for Device 2. Several studies have been devoted to addressing these challenges while only addressing each challengeindependently. In addressing the system heterogeneity challenge, existing FL frameworks compress local models for heterogeneous clients, thereby enabling the inclusion of low-capabilityclients into the FL process. However, these frameworks typically assume that the local data on clientsshares a uniform domain, and the compressed models are tailored for each individual client. Suchcompressed models are susceptible to over-fitting on their local domains due to the presence of domainshifts in practice. Consequently, the central server struggles to aggregate a robust global model withstrong Domain Generalization (DG) capabilities. To address the domain shifts challenge, existingsolutions exemplified by works such as , explore domain-invariant learning approaches eitherwithin the centralized machine learning paradigm or within ideal FL environments. However, thesesolutions unrealistically neglect the resource consumption of the clients, rendering them unsuitable fordirect application in heterogeneous FL. Moreover, the inherent nature of FL yields non-independentand identically distributed (non-IID) data across the clients, further impeding these solutions fromlearning a domain-invariant global model, owing to limited data samples or labels available onindividual clients. Despite existing work devoted to independently addressing system heterogeneityor domain shifts, few of them tackle these challenges simultaneously. To bridge the gaps observed in existing studies, we propose DapperFL, an innovative FL frameworkdesigned to enhance model performance across multiple domains within heterogeneous FL envi-ronments. DapperFL addresses the system heterogeneity challenge through the deployment of adedicated Model Fusion Pruning (MFP) module. The MFP module generates personalized compactlocal models for clients by pruning them with fused knowledge derived from both the local domainand remaining domains, thus ensuring domain generalization of the local models. Additionally, weintroduce a Domain Adaptive Regularization (DAR) module to improve the overall performance ofDapperFL. The DAR module segments the pruned model into an encoder and a classifier, intend-ing to encourage the encoder to learn domain-invariant representations. Moreover, we propose anovel model aggregation approach tailored to aggregating heterogeneous local models with varyingarchitectures and weights. We summarize our contributions as follows: We propose the MFP module to prune local models with personalized footprints leveragingboth domain knowledge from local data and global knowledge from the server, therebyaddressing the system heterogeneity issue in the presence of domain shifts. Additionally, weintroduce a heterogeneous aggregation algorithm for aggregating the pruned models. We propose the DAR module to enhance the performance of the pruned models produced bythe MFP module. The DAR module introduces a regularization term on the local objective toencourage clients to learn robust representations across various domains, thereby adaptivelyalleviating the domain shifts problem. We implement the proposed framework on a real-world FL platform and evaluate its perfor-mance on two benchmarks with multiple domains. The results demonstrate that DapperFLoutperforms several state-of-the-art frameworks (i.e., ) by upto 2.28%, while achieving adaptive model volume reductions on heterogeneous clients.",
  "Related Work": "Heterogeneous Federated Learning. In heterogeneous FL, diverse system capabilities and datadistributions across clients often result in performance degradation of the global model .Extensive studies have made efforts to address these heterogeneity issues through various solutions.For example, studies in adopt model sparsification techniques to reduce thevolume of local models, thereby involving low-capability clients in the FL process. The studiesin leverage dedicated objective functions and specialized training steps to address thedata heterogeneity issue, with studies in allowing clients to conduct varying numbers oflocal updates and consequently alleviating system heterogeneity issue. Several studies selectively optimize or transmit a fraction of the local models parameters to reduce computational orcommunication resource consumption. Studies in split the model into several sub-modelsand offload a subset of sub-models to the server for updating, therefore alleviating the training burdenof clients. However, these heterogeneous FL frameworks commonly assume the data heterogeneity(i.e., non-IID data) exclusively involves distribution shifts in the number of samples and/or labels,while neglecting the existence of domain shifts. Domain Generalization (DG). DG is originally proposed in centralized machine learning paradigmsto address the problem of domain shifts. Existing centralized studies assume access to the entiredataset during the model training process and propose various solutions to achieve DG . Forexample, the studies in focus on learning domain-invariant representations that can begeneralized to unseen domains. In contrast to learning domain-invariant representations, severalstudies (e.g., ) train a generalizable model across multiple domains leveraging meta-learningor transfer learning techniques. Additionally, there are studies (e.g., ) that focus on thecharacteristics of domains, enhancing the generality of the model by properly augmenting the style ofdomain or instance. Unfortunately, the fundamental assumption of centralized machine learning isnot satisfied in FL, where the dataset is distributed among clients who are restricted from sharingtheir data. Despite a few recent studies exploring DG approaches in FL throughrepresentation learning, prototype learning, and/or contrastive learning techniques, they typicallyneglect the inherent system heterogeneity feature of FL. Consequently, these approaches have shownlimited effectiveness in heterogeneous FL due to strict resource constraints. In contrast, we exploreda distributed model pruning approach that leverages both the local domain knowledge and the globalknowledge from all clients, thereby reducing resource consumption in heterogeneous FL with thepresence of domain shifts. Additionally, we design a specific regularization technique for updatingthe pruned local models, thereby further enhancing model performance across domains.",
  "Overview of DapperFL": "DapperFL comprises two key modules, namely Model Fusion Pruning (MFP) and Domain AdaptiveRegularization (DAR). The MFP module is designed to compress the local model while concurrentlyaddressing domain shift problems, and the DAR module employs regularization generated by thecompressed model to further mitigate domain shift problems and hence improve the overall perfor-",
  "mance of DapperFL. Additionally, to handle the aggregation of heterogeneous models produced bythe MFP module, we introduce a dedicated heterogeneous model aggregation algorithm": "explains the DapperFLs workflow during each communication round t, which is alsodescribed as follows. The MFP module within each client i C calculates the fusion model wtiusing both the global and fine-tuned local models. The global model Wt1 is downloaded from thecentral server,2 and the local model wti is fine-tuned on the clients local data with an initial epoch.Subsequently, the MFP calculates a binary mask matrix M ti using 1 norm and produces the prunedlocal model wti M ti. The DAR module updates the pruned model wti M ti over several epochswith a dedicated local objective Li. During local updating, the DAR module segments the localmodel into an encoder and a classifier, which are responsible for generating local representationsand performing predictions, respectively. The local objective Li is then constructed by combingregularization LDARiof the local representations with the common cross-entropy loss LCEi. Next,the client transmits the updated local model to the central server. To aggregate local models withheterogeneous structures, the central server recovers the structure of received local models using theprevious rounds global model Wt1.2 The central server aggregates the recovered local modelsthrough weighted averaging, obtaining the global model Wt for round t.",
  "Model Fusion Pruning on Edge Devices": "In this subsection, we present the design of the MFP module employed by DapperFL. The goal of theMFP module is to tailor the footprint of the local model for edge devices in the presence of domainshifts in the local data, thereby addressing the system heterogeneity problem. Inspired by the spiritof the transfer learning , MFP fuses the global model Wt1 into the fine-tuned local modelwti to learn the cross-domain knowledge. This avoids over-fitting the model to the local domainwhile enhancing the generality of the model. After that, MFP calculates a binary mask matrix M ti togenerate the pruned local model. The detailed pruning process is described in Algorithm 1. Specifically, in the initial epoch of each communication round t [1, T], the MFP module firstfine-tunes the global model Wt1 on local data Di to produce local model wti (in line 1). We utilizeone epoch of local training to determine the pruned models for the following reasons: 1) Additionallocal epochs do not significantly enhance the models performance, which justifies the use of a singleepoch for efficiency. As noted in , experiments have demonstrated that extending local trainingbeyond one epoch yields results comparable to those achieved with just one epoch. 2) In previousdomain generalization-related FL research, such as , one epoch is also employed to collect localdomain information. This method has proven adequate for capturing essential features and domaincharacteristics. 3) Pioneering research in model design and neural architecture search, such as ,",
  "wti = tWt1 + (1 t) wti,(1)": "where t is the fusion factor used to control the quality of the fused global model Wt1. Consideringthat the local data on the client is typically limited, resulting to updating at the beginning of FLrequires more guidance from the global model by exploring the commonalities across differentdomains. As the FL process proceeds, the local model is capable of learning more domain-dependentinformation from itself. Thus, we design a dynamic adjusting mechanism to modify the t during theFL training. The t is initialized with a relatively large value 0 and decreases as the FL processproceeds. The decreased speed is controlled by a sensitivity factor . We assign a minimum valuemin for t to ensure the local model will consistently learn the knowledge from other domains.Formally, the dynamic adjusting mechanism for t is described as follows:",
  "t = max{(1 )t10, min}.(2)": "Subsequently, the MFP module calculates a binary mask matrix M ti {0, 1}|wti| for pruning thefused local model wti (in line 3). The matrix M ti is derived through the channel-wise 1 norm, whichhas proven to be effective and efficient in assessing the importance of parameters . Theelements in M ti with a value of 0 indicate the corresponding parameters that need to be pruned,while those with a value of 1 indicate the parameters that will be retained. The pruning ratio idetermines the proportion of 0 in M ti.3 Finally, the MFP module prunes the local model wti withthe binary mask matrix M ti (in line 4). The pruned model can be represented as wti M ti. It isnoteworthy that, the pruning strategy used in our DapperFL is structural pruning strategy (channelpruning), which is a hardware-friendly approach that can be easily implemented with popular machinelearning libraries such as PyTorch, making it particularly suitable for deployment on edge devices.",
  "Domain Adaptive Regularization": "In FL, each client i C possess private local data Di = {xi, yi}Ni, where x X denotesthe input, y Y denotes the corresponding label, and Ni represents the local data samplesize. The data distribution pi(x, y) of client i typically varies from that of other clients, i.e.,pi(x) = pj(x), pi(x|y) = pj(x|y), leading to the domain shifts problem. Due to the existenceof domain shifts, representation zi generated by the local encoder varies among different clients,resulting in degraded prediction results of the local predictor. To address the domain shifts problem,we design a DAR module to enhance the performance of DapperFL across multiple domains whilemaintaining compatibility with the MFP module. Specifically, the DAR module introduces a regularization term to the local objective to alleviate thebias of representations zi on different clients adaptively. To achieve this goal, we first segment eachpruned local model wM into two parts, i.e., an encoder weM e and a predictor wpM p, where 3Following the conventional setting in heterogeneous FL , we make the fundamental assumptionthat the system capabilities of the devices are available to the server and the pruning ratios for all devices areappropriately determined according to the system information. w = {we, wp} and M = {M e, M p}.4 The encoder is responsible for learning a representationzi given an input xi, denoted as zi = ge(we M e; xi). While the predictor is responsible forpredicting label yi given the representation zi, denoted as yi = gp(wp M p; zi). Subsequently, weconstruct a regularization term LDAR on the local objective as follows:",
  "LDARi= ||ge(we M e; xi)||22.(3)": "Here, || ||22 represents the squared 2 norm of the local representation. The 2 norm implicitlyencourages different local encoders to generate aligned robust representations adaptively, therebymitigating the impact of domain shifts. We adopt the squared 2 norm to construct the regularizationterm for the following reasons: a) The 2 norm encourages each element of the representation toconverge to 0 but not equal to 0. This property is advantageous compared to the 1 norm, whichtends to make smaller representation elements exactly equal to 0. Thus, the squared 2 normensures that the pruned model retains more information in the representations. b) Higher orderregularization introduces significant computational overhead compared to the 2 norm, withoutsignificantly improving the regularization effectiveness.",
  "Heterogeneous Model Aggregation": "Despite the MFP module and the DAR module being capable of alleviating domain shift issues,the heterogeneous local models generated by the MFP module cannot be aggregated directly usingpopular aggregation algorithms. Therefore, we propose a specific FL aggregation algorithm forDapperFL to effectively aggregate these heterogeneous local models. To preserve specific domain knowledge while transferring global knowledge to the local model, thecentral server first recovers the structure of local models before aggregating. Specifically, in eachcommunication round t, the pruned local model is recovered as follows:",
  ",(6)": "where Wt1 is the global model aggregated at the (t 1)-th round, and Mti denotes the logical NOToperation applied to M ti. The first term wti M ti contains local knowledge5 specific to client i,while the second term wt1 Mti contains the global knowledge6 from all clients. Additionally, thestructure of wti M ti, which includes local knowledge, complements wti M ti. Consequently, thestructure recovery operation not only reinstates the pruned models architecture but also transfersglobal knowledge to the pruned model, which is essential for the subsequent aggregation process. Bycombining these two forms of knowledge, we aim to leverage both the specialized insights of localmodels and the generalized capabilities of the global model, thereby enhancing the performance andadaptability of DapperFL. 4We omit the client index i and the communication round index t for notation simplicity. In this work, alllayers except the final linear layer act as the encoder, while the last linear layer of the model acts as the predictor.5In this work, local knowledge refers to the feature extraction capabilities of the local model, which arelearned from the specific data available in its local domain. This knowledge encapsulates the nuances andcharacteristics of the data that the local model has been trained on.6The global knowledge represents the aggregated feature extraction capabilities of the global model, whichare informed by data across all participating domains. The global model synthesizes diverse knowledge fromdifferent local models to provide a more generalized understanding that is applicable across multiple domains.",
  "Experimental Setup": "Implementation Details. We implement DapperFL with a real-world FL platform FedML withdeep learning tool PyTorch . We build the FL environment with a client set C containing 10heterogeneous edge devices and a central server on the FedML platform. Following a conventionsetting , we categorize these heterogeneous devices into 5 levels according to their systemcapabilities, i.e., Cl C (l ), where Cl represents device set belongs to level l. The systemcapabilities of devices in set Cl decrease as level l increases. Our experiments are conducted on aGPU server with 2 NVIDIA RTX 3080Ti GPUs. Each experiment is executed three times with threefixed random seeds to calculate average metrics and ensure the reproducibility of our results. Datasets and Data Partition. We evaluate DapperFL on two domain generalization benchmarks,Digits and Office Caltech that are commonly used in the literature for domain generaliza-tion. The Digits consists of the following four domains: MNIST, USPS, SVHN, and SYN. The OfficeCaltech consists of the following four domains: Caltech, Amazon, Webcam, and DSLR. For eachbenchmark, we distribute the simulated 10 clients randomly to each domain while guaranteeing thateach domain contains at least one client and that each client only belongs to one domain. To generatenon-IID local data, we randomly extract a proportion of data from the corresponding domain as thestatistical characteristics vary among domains. Following the conventional data partition setting ,we set 1% as the local data proportion of Digits and 20% as that in Office Caltech based on thecomplexity and scale of the benchmarks. Models. We adopt ResNet10 and ResNet18 as the backbone models for Digits and OfficeCaltech, respectively. In the following evaluations, both the DapperFL and comparison frameworkstrain the models from scratch for a fair comparison.",
  "and NeFL . These FL frameworks are either classical or focus on addressing system heterogeneityor domain shifts issues. The comparison frameworks are described in detail in Appendix A": "Default Hyper-parameters. To conduct fair evaluations, the global settings and local training hyper-parameters of FL are set to identical values to both the DapperFL and comparison FL frameworks.For the framework-specific hyper-parameters, we use their default settings without changing them.The hyper-parameters used in our evaluations are described in Appendix B. Evaluation Metrics. In this paper, we evaluate the performance of the global model using the averageTop-1 accuracy across all domains. In evaluating the resource consumption of the clients, we adoptboth the total number of parameters and the Floating-Point Operations (FLOPs) of the local model.",
  "Performance Comparison": "Model Performance Across Domains. Tables 1 and 2 respectively provide detailed analyses ofmodel accuracy on the Digits and Office Caltech benchmarks. The term System Heter. indicateswhether the respective framework supports system heterogeneity.7 The results are reported as modelaccuracy with corresponding standard deviations in brackets. As shown in both Tables 1 and 2, DapperFL achieves the highest global accuracy on both the Digitsand Office Caltech benchmarks compared with the comparison frameworks. On Digits and OfficeCaltech, DapperFLs global accuracy is 0.13% and 2.28% better than the runner-up, respectively.This indicates that the global model learned by DapperFL is more robust across all domains andtherefore possesses better domain generalization. Moreover, DapperFL always achieves competingaccuracy on individual domains and even the best domain accuracy on the SYN, Amazon, and DSLR.This demonstrates that the DAR module of DapperFL implicitly encourages the encoder to learndomain-invariant representations, resulting in DapperFL being unlikely to over-fit on the specific",
  "Note that when evaluating frameworks that do not support heterogeneous clients, we allow all clients toparticipate in FL by disregarding resource constraints on clients": "domain(s) and possessing better domain generalization ability. Furthermore, DapperFL toleratesheterogeneous clients through personalized pruning ratio , achieving adaptive resource consumptionreductions of {20%, 40%, 60%, 80%} for clients in {C2, C3, C4, C5}, while outperforming comparisonFL frameworks that support heterogeneous clients. This is evidence that the MFP module of DapperFLeffectively reduces the resource consumption of clients with domain shifts. In addition, we providelearning curves for both global and domain accuracies in Appendix C for a comprehensive analysis.",
  ":Comparison of model accuracy ofFedMP, NeFL, and DapperFL with different prun-ing ratios on the Digits and Office Caltech": "Impact of Model Footprints. In heterogeneousFL, resource constraints on the clients often re-sult in limited model footprints (i.e., numberof parameters and FLOPs). To investigate theimpact of the model footprints on model per-formance, we compare the DapperFL with twoSOTA FL frameworks (i.e., FedMP and NeFL)that are equipped with adaptive pruning tech-niques. The pruning ratio determines both thereduction in the number of parameters and theFLOPs. We maintain a consistent for everydevice, with predetermined values selected fromthe set {0.2, 0.4, 0.6, 0.8}. illustrates the comparative results under varying pruning ratios on both Digits and OfficeCaltech. As depicted, these frameworks tend to achieve higher accuracy with smaller pruning ratios,and DapperFL consistently outperforms the others across all values on both benchmarks. Thisunderscores the superior adaptability of DapperFL in heterogeneous FL environments. Notably,in (b), the accuracy of DapperFL at = 0.4 surpasses its accuracy at = 0.2. Thisdiscrepancy arises from the over-parameterization of ResNet18 for the classification task in OfficeCaltech. The improved accuracy observed at higher pruning rates suggests that the MFP moduleof DapperFL effectively prunes unnecessary parameters while enhancing model generalizationrather than compromising it. This also demonstrates the reason behind DapperFLs superiority overcomparison frameworks that employ full-size models. Furthermore, we comprehensively evaluate theeffect of DapperFLs pruning ratio on model performance in Appendix D.",
  "DapperFL74.30%67.75%": "Effect of Key Modules in DapperFL. We evaluate theperformance of DapperFL with and without the MFP andDAR modules, individually and in combination. The fol-lowing configurations are considered: DapperFL w/oMFP+DAR, DapperFL without the MFP and DAR mod-ules. DapperFL w/o DAR, DapperFL without the DARmodule. DapperFL w/o MFP, DapperFL without theMFP module. DapperFL, the complete DapperFL frame-work. It is noteworthy that when the MFP module is notemployed in DapperFL, it performs model pruning using1 norm on the local models directly. summarizes the ablation study results on bothbenchmarks. On Digits, the results indicate a gradualimprovement in accuracy as we incorporate the MFP andDAR modules into DapperFL. DapperFL achieves the highest accuracy at 74.30%, highlighting thesynergistic effect of both MFP and DAR in enhancing model performance. Similarly, we observe aconsistent pattern on Office Caltech. DapperFL w/o MFP+DAR yields the lowest accuracy, whilethe inclusion of both MFP and DAR results in a steady improvement. The complete DapperFLframework attains the highest accuracy at 67.75%, reinforcing the complementary roles played byMFP and DAR in bolstering domain adaptability in heterogeneous FL. Effect of Hyper-Parameters within the MFP Module. We investigate the influence of the hyper-parameters 0, min, and within the MFP module on the overall performance of DapperFL. In thisexperiment, only the hyper-parameter under evaluation is modified, while others retain their defaultsettings as outlined in Appendix B.",
  "(d) Effect of in DAR": ": The effect of hyper-parameters in the MFP and DAR modules on model accuracy. Officein the legend represents the Office Caltech benchmark, and the average accuracy calculated for bothbenchmarks is labeled as AVG. (a) illustrates the effect of 0 on model accuracy. The 0 parameter dictates the initialweight of the global model fused into the local model. In both benchmarks, the optimal 0 isevident at 0.9, resulting in the highest average accuracy. This result underscores the significance ofinfusing the local model with knowledge from other domains early in the FL process to enhance itsgeneralization. (b) describes the effect of min on model accuracy, where min determinesthe minimum weight of the global model fused into the local model. In both benchmarks, the optimalaverage accuracy is achieved when min is set to 0.1. This finding signifies that, as the FL processproceeds, the local model benefits from acquiring more domain-specific knowledge to uphold itspersonalization. (c) explores the effect of on model accuracy. The parameter controlsthe rate at which the fusion factor 0 diminishes towards min. In both benchmarks, the optimal is discernible at 0.2, resulting in the highest average accuracy. This observation emphasizes thatthe most effective rate of decrease for the fusion factor 0 towards min is achieved when = 0.2in both benchmarks. Furthermore, recognizing the abnormal trends in the relationship betweenmodel accuracy and hyper-parameter when it is less than 0.2, we implement Bayesian search as anautomatic selection mechanism to find the optimal value of . The results are provided in Appendix E. Effect of Hyper-Parameter within the DAR Module. (d) provides configurations withvarying values across both Digits and Office Caltech. The results show an increase in accuracywith higher values until = 0.01 on Digits, where the highest accuracy of 74.30% is achieved.A notable drop in accuracy is observed at = 0.1, indicating that excessively large regularizationmay hinder model performance on Digits. Similar to Digits, an increase in accuracy is observed withhigher values until = 0.01, reaching the highest accuracy of 67.75%. The impact of regularizationis more pronounced on Office Caltech, as seen by the significant drop in accuracy at = 0.1. Thissuggests that the regularisation factor in the DAR Module plays a crucial role in solving the domainshift problem in FL and a careful choice of can improve the model performance.",
  "Conclusion": "We have described our proposed FL framework DapperFL tailored for heterogeneous edge deviceswith the presence of domain shifts. Specifically, DapperFL integrates a dedicated MFP moduleand a DAR module to achieve domain generalization adaptively in heterogeneous FL. The MFPmodule addresses system heterogeneity challenges by shrinking the footprint of local models throughpersonalized pruning decisions determined by both local and remaining domain knowledge. Thiseffectively mitigates limitations associated with system heterogeneity. Meanwhile, the DAR moduleintroduces a specialized regularization technique to implicitly encourage the pruned local models tolearn robust local representations, thereby enhancing DapperFLs overall performance. Furthermore,we designed a specific aggregation algorithm for DapperFL to aggregate heterogeneous modelsproduced by the MFP module, aiming to realize a robust global model across multiple domains.The evaluation of DapperFLs implementation on a real-world FL platform demonstrated that itoutperforms several SOTA FL frameworks on two benchmark datasets comprising various domains. Limitations and Future Work. Despite its potential in addressing system heterogeneity and domainshifts, DapperFL introduces four hyper-parameters 0, min, , and associated with the DGperformance of the global model. A potential future direction involves the automatic selection ofthese hyper-parameters, therefore enhancing the flexibility and accessibility of the DapperFL.",
  "and Disclosure of Funding": "This research is supported part by the National Natural Science Foundation of China No. 92267104and No. 62372242. Dr. Xuyun Zhang is the recipient of an ARC DECRA (project No. DE210101458)funded by the Australian Government. The work of K.-K. R. Choo is supported only by the CloudTechnology Endowed Professorship. Wei Yang Bryan Lim, Nguyen Cong Luong, Dinh Thai Hoang, Yutao Jiao, Ying-Chang Liang,Qiang Yang, Dusit Niyato, and Chunyan Miao. Federated learning in mobile edge networks: Acomprehensive survey. IEEE Communications Surveys & Tutorials, 22(3):20312063, 2020. Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Ar-jun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings,et al. Advances and open problems in federated learning. Foundations and trends in machinelearning, 14(12):1210, 2021. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.Communication-efficient learning of deep networks from decentralized data. In Artificialintelligence and statistics, pages 12731282. PMLR, 2017.",
  "Dashan Gao, Xin Yao, and Qiang Yang. A survey on heterogeneous federated learning. arXivpreprint arXiv:2210.04505, 2022": "Samiul Alam, Luyang Liu, Ming Yan, and Mi Zhang. Fedrolex: Model-heterogeneous federatedlearning with rolling sub-model extraction. In Advances in Neural Information ProcessingSystems, volume 35, pages 2967729690. Curran Associates, Inc., 2022. Liping Yi, Gang Wang, Xiaoguang Liu, Zhuan Shi, and Han Yu. Fedgh: Heterogeneousfederated learning with generalized global header. In Proceedings of the 31st ACM InternationalConference on Multimedia, pages 86868696, 2023.",
  "Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heteroge-neous federated learning. In International conference on machine learning, pages 1287812889.PMLR, 2021": "Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang.Fedproto: Federated prototype learning across heterogeneous clients. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 36, pages 84328440, 2022. Ang Li, Jingwei Sun, Pengcheng Li, Yu Pu, Hai Li, and Yiran Chen. Hermes: an efficientfederated learning framework for heterogeneous mobile clients. In Proceedings of the 27thAnnual International Conference on Mobile Computing and Networking, pages 420437, 2021. Yuang Jiang, Shiqiang Wang, Vctor Valls, Bong Jun Ko, Wei-Han Lee, Kin K. Leung, andLeandros Tassiulas. Model pruning enables efficient federated learning on edge devices. IEEETransactions on Neural Networks and Learning Systems, pages 113, 2022.",
  "Haiyan Zhao and Guodong Long. One-shot pruning for fast-adapting pre-trained models ondevices. arXiv preprint arXiv:2307.04365, 2023": "Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, andAnanda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.In Proceedings of the 37th International Conference on Machine Learning, volume 119, pages51325143. PMLR, 2020. Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent Poor. Tackling theobjective inconsistency problem in heterogeneous federated optimization. In Advances inNeural Information Processing Systems, volume 33, pages 76117623. Curran Associates, Inc.,2020. Zachary Charles, Kallista Bonawitz, Stanislav Chiknavaryan, Brendan McMahan, et al. Fed-erated select: A primitive for communication-and memory-efficient federated learning. arXivpreprint arXiv:2208.09432, 2022. Hanhan Zhou, Tian Lan, Guru Prasadh Venkataramani, and Wenbo Ding. Every parametermatters: Ensuring the convergence of federated learning with dynamic heterogeneous modelsreduction. Advances in Neural Information Processing Systems, 36, 2024. Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, Seyit Camtepe, and Lichao Sun.Splitfed: When federated learning meets split learning. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 36, pages 84858493, 2022. Wen Wu, Mushu Li, Kaige Qu, Conghao Zhou, Xuemin Shen, Weihua Zhuang, Xu Li, andWeisen Shi. Split learning over wireless networks: Parallel design and resource management.IEEE Journal on Selected Areas in Communications, 41(4):10511066, 2023. Ngoc Duy Pham, Alsharif Abuadbba, Yansong Gao, Tran Khoa Phan, and Naveen Chilamkurti.Binarizing split learning for data privacy enhancement and computation reduction. IEEETransactions on Information Forensics and Security, 2023.",
  "Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu, and Dacheng Tao. Domain gen-eralization via entropy regularization. Advances in neural information processing systems,33:1609616107, 2020": "Ziqi Wang, Marco Loog, and Jan Van Gemert. Respecting domain relations: Hypothesisinvariance for domain generalization.In 2020 25th International Conference on PatternRecognition (ICPR), pages 97569763. IEEE, 2021. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for domain generalization.In Proceedings of the AAAI conference on artificialintelligence, volume 32, 2018.",
  "Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. A survey of transfer learning. Journalof Big data, 3(1):140, 2016": "Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are featuresin deep neural networks? In Advances in Neural Information Processing Systems, volume 27.Curran Associates, Inc., 2014. Jianqing Zhang, Yang Hua, Jian Cao, Hao Wang, Tao Song, Zhengui Xue, Ruhui Ma, andHaibing Guan. Eliminating domain bias for federated learning in representation space. Advancesin Neural Information Processing Systems, 36, 2024. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, andQuoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 28202828, 2019. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang.Learning efficient convolutional networks through network slimming. In Proceedings of theIEEE international conference on computer vision, pages 27362744, 2017.",
  "Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters forefficient convnets. In International Conference on Learning Representations, 2017": "Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, PraneethVepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, RameshRaskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr. Fedml: A research libraryand benchmark for federated machine learning. arXiv preprint arXiv:2007.13518, 2020. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperativestyle, high-performance deep learning library. Advances in neural information processingsystems, 32, 2019.",
  "Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya, and Umapada Pal. Effects of degradationson deep neural network architectures. arXiv preprint arXiv:1807.10108, 2018": "Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman. Geodesic flow kernel for unsuperviseddomain adaptation. In 2012 IEEE conference on computer vision and pattern recognition, pages20662073. IEEE, 2012. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 770778, 2016.",
  "The comparison FL frameworks used in this work are introduced as follows:": "FedAvg : FedAvg is a classical FL framework that is widely adopted to aggregate distributed localmodels. During the aggregation period of FedAvg, the clients upload the updates to the server, andthe central server is responsible for generating the global model by performing weighted averagingon these local updates. FedDrop : FedDrop reduces the computational overhead of local updating and communicationcosts by generating compact models for clients. In FedDrop, the server adopts lossy compressiontechniques to compress the global model with a uniform compression ratio for all clients. To satisfythe resource requirements of low-level clients, FedDrop needs to choose a large pruning ratio tocompress the global model. FedProx : FedProx introduces a proximal term into the objective function, aimed at incentivizingparticipants to uphold similarity with the global model throughout local training, meanwhile affordinglow-capacity clients the latitude to execute fewer local updates. MOON : MOON is a simple yet effective FL framework tailored for non-IID local data amongclients. It capitalizes on model representation similarities to enhance the local training of individualclients through model-level contrastive learning. FedMP : FedMP employs an adaptive local model pruning mechanism in each FL communicationround to address the diverse resource constraints present on client devices. Simultaneously, itleverages a Residual Recovery Synchronous Parallel (R2SP) scheme to aggregate models fromheterogeneous clients. FedSR : FedSR incorporates two regularization techniques to implicitly align representationsacross domains in the FL environments. These regularizers implicitly align the marginal and condi-tional distributions of the representations, thereby enhancing domain generalization. NeFL : NeFL employs a combination of depthwise and widthwise scaling techniques to partitiona model into submodels. Its primary objective is to enable resource-constrained clients to seamlesslyengage in the FL process, meanwhile facilitating the training of models on more extensive datasets. Tomitigate the challenges associated with training multiple submodels featuring varying architectures,NeFL introduces a parameter decoupling mechanism.",
  "DEffect of Pruning Ratio": "We systematically investigate the impact of varying pruning ratios () on the model performance ofDapperFL using both the Digits and Office Caltech benchmarks. Pruning ratios ranging from 0.2to 0.8 are considered, representing different degrees of model compression. presents the model footprint and accuracy of DapperFL under different pruning ratios on the Digits benchmark.As the pruning ratio increases, the number of parameters (#Para) and FLOPs decrease, reflectingthe expected reduction in model size and computational complexity. Notably, the model maintainscompetitive accuracy when pruning ratios increase from 0.2 to 0.6, indicating the robustness ofDapperFL to varying degrees of model compression. Similarly, details the model footprintand accuracy of DapperFL under different pruning ratios on the Office Caltech benchmark. Consistentwith the Digits benchmark results, the model adapts effectively to increased pruning from 0.2 to 0.6,with reductions in both parameters and FLOPs. In summary, DapperFL consistently outperforms the baseline frameworks that utilize the entire model,such as FedAvg and MOON, as illustrated in and , when employing a pruned modelwith a pruning ratio ranging from 0.2 to 0.6. Despite incurring an inevitable accuracy loss at = 0.8on both benchmarks, DapperFL still outperforms FedMP and NeFL, both equipped with adaptivepruning capabilities, as illustrated in . These observations collectively demonstrate therobustness of DapperFL across a spectrum of pruning ratios, emphasizing its potential for deploymentin resource-constrained FL environments. The ability to achieve substantial model compression (withthe pruning ratio ranging from 0.2 to 0.6) without significant loss in accuracy makes DapperFL apromising solution for real-world applications where resource efficiency is paramount.",
  "We run DapperFL on the Office Caltech benchmark 40 times, adopting a distinct of less than 0.2each time. The values are selected using the Bayesian search. The results are presented in": "As illustrated, the Bayesian search-based automatic selection mechanism indicates that model ac-curacy is likely to reach a higher level when approaches 0.2, aligning with our default setting of = 0.2. It is noteworthy that, owing to the nature of Bayesian search, the sampled values are notuniformly distributed, as they tend to bias towards the optimal that maximizes model accuracy."
}