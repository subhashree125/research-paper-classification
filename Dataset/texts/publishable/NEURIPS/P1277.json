{
  "Abstract": "Neural algorithmic reasoning is an emerging area of machine learning that focuseson building neural networks capable of solving complex algorithmic tasks. Recentadvancements predominantly follow the standard supervised learning paradigm feeding an individual problem instance into the network each time and training it toapproximate the execution steps of a classical algorithm. We challenge this modeand propose a novel open-book learning framework. In this framework, whetherduring training or testing, the network can access and utilize all instances in thetraining dataset when reasoning for a given instance.Empirical evaluation is conducted on the challenging CLRS Algorithmic ReasoningBenchmark, which consists of 30 diverse algorithmic tasks. Our open-book learningframework exhibits a significant enhancement in neural reasoning capabilities.Further, we notice that there is recent literature suggesting that multi-task trainingon CLRS can improve the reasoning accuracy of certain tasks, implying intrinsicconnections between different algorithmic tasks. We delve into this direction viathe open-book framework. When the network reasons for a specific task, we enableit to aggregate information from training instances of other tasks in an attention-based manner. We show that this open-book attention mechanism offers insightsinto the inherent relationships among various tasks in the benchmark and providesa robust tool for interpretable multi-task training.",
  "Introduction": "Deep neural networks have achieved remarkable advancements in various areas, such as imageprocessing and natural language processing . In recent years, as deep learningcontinues to evolve, there has been an increasing desire to see deep neural networks take on morecomplex tasks. Algorithmic reasoning tasks have emerged as a particularly crucial category.In classical domains, deep neural networks have demonstrated their ability to learn predictive patternsfrom training data. The aspiration now is to extend this capability to the field of algorithmic reasoning,which motivates a burgeoning domain Neural Algorithmic Reasoning (NAR). Neural algorithmic reasoning was initially coined by . The central objective of this domain isto develop and train neural networks with the capability to imitate classical rule-based algorithms,such as sorting algorithms and graph algorithms. Networks built in this manner demonstrate theability to perform algorithmic computations similar to traditional algorithms in reasoning tasks,while showcasing improved computational efficiency compared to them . Moreover, recentliterature shows that owing to the characteristics of deep learning, these networks exhibitflexibility in handling diverse input formats, making them robust even in scenarios where certaininput features are missing.",
  "arXiv:2501.00072v1 [cs.LG] 30 Dec 2024": "Challenging Benchmark for NAR. CLRS Algorithmic Reasoning Benchmark proposed by iscurrently the most popular and definitive benchmark for evaluating the algorithmic capabilities ofneural networks. This benchmark comprises 30 diverse algorithmic reasoning tasks extracted fromthe foundational algorithms textbook Introduction to Algorithms , including sorting, searching,dynamic programming, graph algorithms, string algorithms, and more. Beyond the task diversity,another notable challenge of this benchmark is the significant differences in scale between probleminstances in the training and test sets. The test instances are substantially larger in scale compared tothose in the training set. There have been many recent advances in exploring CLRS . As classical algorithmscan often be represented by graph structures, several successful approaches leverage the GraphNeural Network (GNN) framework, including models such as PGN and MPNN . In additionto directly applying these classical GNNs, the literature has observed that the execution of someclassical algorithms often relies on specific data structures. Consequently, there have been proposalsto integrate classical GNNs with data structures like priority queues or stacks to enhanceneural reasoning capabilities. However, we notice that all prior approaches predict algorithmic executions based solely on theirparameters and the features of a single input. Although this mode is commonly used in traditionalsupervised learning tasks , it may not be well-suited for NAR due to the inherent differencebetween complicated reasoning tasks and traditional tasks like image processing. In practicalscenarios, when recognizing images, extensive background knowledge is typically not required; butwhen faced with complex reasoning tasks, a substantial amount of background knowledge is oftennecessary to complete various aspects of the reasoning process. In such situations, having real-timeillustrative examples or formulas available for reference can significantly reduce our memory burden,thereby enhancing task completion. This naturally raises a question:",
  "Our Contributions": "We explore the aforementioned question and introduce open-book neural algorithmic reasoning. Inthis model, the neural architecture is enhanced with an additional memory component that storesrepresentations of instances in the training dataset. Whether during training or testing, wheneverthe network engages in reasoning for a specific instance, it has the capability to leverage thissupplementary memory to aggregate information from other instances within the training set, akin toan open-book exam. The main results of the paper are summarized as follows: We present a general framework for open-book NAR. This framework builds upon the foun-dation of previous NAR architectures by introducing two additional modules for embeddingand information aggregation from the training set, and can seamlessly integrate with existingmethods. We further provide a detailed implementation of the framework, which is groundedin the cross-attention mechanism. This design not only caters to single-task training but alsoproves to be highly effective in scenarios involving multi-task training. Empirical evaluations are conducted on the challenging CLRS Benchmark . We incor-porate the proposed framework with three popular network architectures in the literature.The results demonstrate that each architectures reasoning capability can be improved signif-icantly when utilizing the training instances through the framework. Across the majority ofthe reasoning tasks within the benchmark, the framework yields state-of-the-art results. Multi-task training is also investigated in the paper. As highlighted in , on certainreasoning tasks, a generalist network trained on all datasets in CLRS outperforms thenetworks trained in a single-task manner. We provide an interpretation of this observationusing the proposed open-book framework. Specifically, when training a neural network tosolve a task, we input information from other task datasets into the framework for its use.The results show that our open-book framework can nearly replicate the effects of multi-tasktraining for each algorithmic task, while in some tasks, it even achieves higher accuracies.Additionally, our attention-based implementation enables us to analyze the attention weightsof various tasks, facilitating a deeper understanding of the intrinsic relationships among",
  "Other Related Work": "Our work is closely aligned with the exploration of non-parametric models , wheremodels abstain from training specific parameters and, instead, utilize dependencies among trainingdata points for predictions. Our framework can be viewed as a fusion of deep neural networks andnon-parametric models. We have noted analogous efforts in recent work within the field of imageprocessing . This work focuses on the CIFAR-10 dataset, employing self-attention mechanismsamong different points in the dataset to finish image classification tasks.",
  "This section introduces the setting of an NAR dataset formally and outlines the standard paradigmemployed in NAR": "NAR Dataset. The objective of an NAR task is to train a neural network such that it can imitateeach execution step of a classical algorithm on given problem instances. Hence, a NAR dataset islabeled by a specific problem and the algorithm employed to solve it. Each data point includes aproblem instance, represented by a graph structure, and the corresponding algorithm execution on thatinstance, conveyed through a sequence of graph-structured states. Denote by x the problem instanceand by y = {y(1), ..., y(t), ...} the algorithm execution, where y(t) signifies the graph-structuredstates (e.g., the current nodes in the queue of breadth-first search) at the t-th step of the algorithm.",
  "and produces the output y(t). This process enables the neural network to learn and predict theevolution of the algorithmic execution on the problem instance in a step-wise fashion": "Encode-Processor-Decode Paradigm. To achieve the aforementioned step-wise objective, theliterature follows the standard encode-process-decode paradigm , which consists of three modules:Encoder, Processor, and Decoder. At each step t, the inputx, y(t1)traverses through thesemodules sequentially2: The encoder module encompasses multiple neural networks that operate onx, y(t1),thereby transforming it into a collection of graph-structured hidden states. Use G = (V, E)to denote the graph structure. Following this module, we obtain hv corresponding to eachnode v V , hvu associated with each edge (v, u) E, and hg representing the hiddenstate of the entire graph G.",
  "The processor module usually consists of a graph neural network. This module maintainsthe historical hidden states of nodes, edges, and the graph: {h(t1)v}vV , {h(t1)vu}(v,u)E,": "h(t1)g, and integrate them with the newly generated states {hv}, {hv,u}, hg to yield updatedstates. We borrow the language of the message-passing architecture to formalize thisprocess. For brevity, the following focuses only on updating the state of each node v. Ateach step t, the node computes and aggregates messages muv from its incoming edges,updating its own hidden state:",
  "Open-Book Reasoning": "The paradigm above can be denoted by a function F mapping x to y for each data point. Given aNAR dataset, this function implies a standard supervised learning mode: during a training step, a (ora mini-batch of) random datapoint (x, y) is selected. The loss between F(x) and y is then computed,and the parameters in F are updated accordingly. In this section, we go beyond the individual x ymode in conventional supervised learning, exploring a more general and practical learning paradigm.",
  ": An illustration of the open-book framework. At each reasoning step t, we simultaneouslyinput (x, y(t1)) and instances from the training set T, yielding y(t)": "The intuition behind the open-book framework is analogous to our real-world scenario of solvingalgorithmic problems or engaging in other reasoning tasks. In practical situations, we often consulttextbooks and refer to example problems to aid in completing tasks. Typically, the structure andsolutions of these examples provide substantial assistance in our reasoning process. Denoting thetraining set as T, the framework essentially aims to learn a comprehensive function F : x T y.",
  "An illustration of the framework is present in . In addition to the original three modules, weintroduce two new modules: Dataset Encoder and Open-Book Processor:": "The dataset encoder module employs an encoding function fE to compute the latent featureof each data point di = (xi, yi) in the training set: ri fE (xi, yi) . It is worth notingthat this encoder module is essentially different from the original one. It maps an entire datapoint di = (xi, yi), encompassing the ground truth of each node (and edge) at each step,into a single representation ri. The open-book processor module is incorporated between the original processor and decodermodules. The output h(t) from the processor no longer directly feeds into the decoder;instead, it passes through the open-book processor, where it undergoes information aggre-gation with the training data representation R = {r1, ..., ri, ...} (generated by the datasetencoder). Subsequently, the open-book processor produces the latent features h(t) requiredby the decoder. Formally, for each node v V , h(t)v fPh(t)v , R. The central component of the framework is the open-book processor module. Within this module, thecurrent problem instance to be solved is integrated with examples from the training set. It is crucialto acknowledge that this integration has both advantages and disadvantages. While it often enhancesthe architectures reasoning capabilities, there are instances when it may lead to counterproductiveeffects, particularly during multi-task training. We will elaborate on this in the experiments.",
  ": end while": "From the description, a target data point and several auxiliary data points are randomly selected ineach training iteration. The target data point serves as the focal point for neural optimization in thisiteration: the network predicts its ground truths, computes the loss, and consequently updates thenetwork parameters. The auxiliary data points assist the network in reasoning for the target datapoint. Their latent features are obtained through the dataset encoder, and they subsequently influencepredictions through the open-book processor. At each algorithmic step t, the hidden states h(t) are computed conventionally. However, thesestates are not directly input into the decoder. Instead, they need to undergo cross-attention with therepresentations of auxiliary data points within the open-book processor module. This design allowsthe network to incorporate hints provided by the auxiliary data points during the reasoning process. The construction of the dataset encoder is a bit subtle. We observe a crucial aspect that all thesepieces of information ultimately serve the decoder module. In a single algorithmic step, the decodersrole is to facilitate the transition between two adjacent states throughout the entire reasoning process.Therefore, to better provide effective hints to the final decoder, for each auxiliary data point, we",
  "This section evaluates the open-book implementation empirically on the CLRS benchmark. We aimto investigate the following three questions during the experiments:": "For various processor architectures present in the literature, can the open-book frameworkconsistently enhance their empirical performances across the majority of the algorithmictasks within the CLRS benchmark? There is a recent literature proposing a multi-task training approach for CLRS. Theytrain a common network for various tasks in the benchmark and find that some tasks benefitfrom the multi-task approach, achieving higher accuracy than when trained individually.In the context of the open-book setting, does this phenomenon imply that incorporatingtraining sets from various tasks into the open-book framework may enhance the networksperformance on certain tasks? Can the attention-based implementation serve as a robust tool for interpretable multi-tasktraining? When integrating training sets from various tasks into the open-book framework fora specific task, the network eventually learns attention weights in the open-book processor,signifying the tasks relevance to other tasks. Does this imply that if a task performs betterin multi-task training than in single-task training, retaining only those tasks with prominentattention for multi-task training can still outperform single-task training? To tackle these questions, we conduct three types of experiments3: single-task augmenting, multi-taskaugmenting, and multi-task interpretation. Note that our multi-task augmenting experiment differsessentially from traditional multi-task training; here, we still train the network for a specific task, butwith the inclusion of datasets from other tasks in the dataset encoder. Additional ablation experimentsare also conducted (see the appendix). We initially outline the experimental setup and subsequentlydelve into each experiment.",
  "Setup": "Baselines. We incorporate the open-book framework into three existing processor architectures:PGN , MPNN and Triplet-GMPNN . Given that the feature dimension of hidden statesis set to 128 in the literature, we adjust the parameters of the dataset encoder and open-bookprocessor to ensure seamless integration. The results (F1 scores) achieved by open-book reasoningare compared with them. Moreover, we also compare the performance with other recent architectureslike Memnet and NPQ . Computational Details. The experiments are conducted on a machine equipped with an i7-13700KCPU, an RTX 4090 GPU, and an RTX A6000 GPU. The results are averaged over 4 runs. To ensurefair comparisons, we follow the widely-used experimental hyperparameter settings in , where thebatch size is 32 and the network is trained for 10,000 steps by Adam optimizer with a learning rateof 0.001. During each training and testing iteration, we allow Algorithm 1 to sample 240 auxiliarydata points and use only one attention head. The average training time for each reasoning task isapproximately 0.5 GPU hours.",
  ": Comparison of the MPNN architectures performance before and after augmentation withthe open-book framework. The 30 tasks are arranged in descending order of improvement magnitude": ": The summary of our results on each task category in CLRS. The best-performing results ineach row are highlighted in bold. To save space, we use the column Prior Best to denote the bestresults among four existing approaches: Memnet , PGN , MPNN , and NPQ , andthe column Ours to denote the best results achieved by applying the open-book framework to thethree existing architectures.",
  "Task CategoryPrior BestTriplet-GMPNNOurs": "Graphs64.98%2.5981.41%1.5385.37%1.73Geometry92.48%1.3594.09%0.7796.55%0.50Strings4.08%0.5749.09%4.7872.41%2.66Dynamic Programming76%2.4781.99%1.3082.14%1.45Divide and Conquer65.23%2.5676.36%0.4374.52%1.88Greedy84.13%2.5991.22%0.4093.40%2.12Search56.11%0.3658.61%1.0563.15%0.90Sorting71.53%0.9760.38%5.2583.65%3.06 while the other two are deferred to Appendix A.The figure uses bar charts to illustrate average scoresfor each task, with standard deviations denoted by black lines. Additionally, we arrange the tasks indescending order of improvement magnitude to better illustrate trends. We also provide tables to comprehensively compare the accuracies that the open-book frameworkyields with existing results. In CLRS, the 30 tasks are partitioned into 8 categories: Divide andConquer, Dynamic Programming, Geometry, Graphs, Greedy, Search, Sorting, and Strings. So wepresent two tables ( and ): one showcasing the performance on the 30 individual tasksand another displaying the average performance for each of the 8 task categories. From the figures and tables, we observe that our approach outperforms the original architecturesin the majority of tasks. The improvements provided by the open-book framework are particularlysignificant for certain tasks, such as the Naive String Matcher task (see ). However, wealso notice a relatively large standard deviation in performance for some tasks. We attribute thisvariability to the fact that during testing, we sample data from the training set and input it into thedataset encoder each time. The quality of the sampled data influences the final inference results,leading to performance fluctuations.",
  "Multi-Task Augmenting": "This subsection considers a multi-task environment: for each task in CLRS, Algorithm 1 selectstarget points from its own dataset, while the sampled auxiliary points are drawn from all datasets inCLRS. Since CLRS comprises 30 datasets, in each iteration, we randomly sample 8 instances fromeach dataset, ensuring that the total number of auxiliary points remains the same as in the single-task experiment, i.e., 240. Given that Triplet-GMPNN is the only architecture used for multi-tasktraining in the literature, both this subsection and the following one multi-task interpretation focusexclusively on the results obtained by integrating the open-book framework with Triplet-GMPNN. The results are present in . We find that incorporating data from different tasks into theopen-book processor indeed replicates multi-task training. Our multi-task augmented method closelymatches the previous multi-task training results, and even outperforms them on the vast majority oftasks. It is worth noting that multi-task training requires simultaneous training on all 30 algorithmictasks, which is extremely time-consuming. If the goal is simply to enhance performance on a specifictask using multi-task training, the cost is substantial. However, with the open-book framework, wecan nearly achieve the effects of multi-task training on a target task in approximately the same amountof time it takes to train a single algorithm.",
  "Multi-Task Interpretation": "This subsection delves into interpreting multi-task training. In our multi-task augmenting experiments,the acquired attention weights in the open-book processor reveal the significance of each task inrelation to others. Specifically, for each task, we aggregate the attention weights of each node atevery algorithmic step on each test instance. The resulting 30-dimensional vector is then normalized,serving as the total attention vector for that task relative to other tasks in the benchmark. shows the task with the highest attention weight for each task. Moreover, we present a heatmapregarding the attention weights among CLRS tasks in Appendix B. Surprisingly, the table indicates that the majority of tasks exhibit a preference for attention towardtasks outside their own categories, contrary to our initial expectations. Only four bolded pairs showhigh attention to tasks within the same category, with most of these being graph algorithms. Anintuitive explanation for this phenomenon is that tasks within the same category might not contributeadditional information compared to the dataset used for training the task itself. Instead, tasks fromother categories seem to play a crucial role in improving training accuracy. : For each target (task), we show the task with the highest attention weight among othertasks in column Auxiliary. We use bold text to indicate when the paired tasks belong to the samealgorithmic category.",
  "TaskSingle-TaskMulti-TaskPaired-Task": "Heapsort31.04%5.8255.62%15.9146.63%10.43Knuth-Morris-Pratt19.51%4.5751.61%8.6365.67%12.36Insertion Sort78.14%4.6487.00%4.1695.78%0.80LCS Length80.51%1.8483.43%1.1985.86%1.47Quicksort64.64%5.1275.10%9.5288.43%6.25SCC43.43%3.1548.48%9.9673.39%3.00JarvisMarch91.01%1.3074.51%10.7194.44%0.63MST-Kruskal89.80%0.7789.08%1.6490.55%1.12MST-Prim86.39%1.3386.26%2.0892.56%0.99Topological Sort87.27%2.6781.65%2.5387.30%4.62Dijkstra96.05%0.6094.29%1.0497.44%0.50Binary Search77.58%2.3569.30%5.6579.17%2.79Bubble Sort67.68%5.5052.94%9.9670.30%6.77Graham Scan93.62%0.9187.74%3.8794.58%0.87Minimum97.78%0.5592.50%2.5398.32%0.14 We proceed to a more in-depth examination of the relationships among tasks learned by the framework.We select a partner for each task according to namely, the task it pays the most attention to.We conduct training and testing in a multi-task manner for each task paired with its chosen partner,and refer to this type of training as paired-task training. In this experiment, we only focus on tasksthat either demonstrate accuracy improvements or slight declines in multi-task training compared tosingle-task training, and train them in the paired manner. The results are given in . The tablevalidates our hypothesis. On these tasks, paired-task training achieves improvements compared tosingle-task training, with most tasks even surpassing the performance of multi-task training.",
  "Conclusion": "This paper considers open-book neural algorithmic reasoning, introducing a novel open-book frame-work accompanied by an attention-based implementation. Through empirical evaluations, we demon-strate that this implementation not only enhances the reasoning capabilities of the existing architecturebut also functions as an effective tool for interpretable learning. Several interesting direction for future research exist, such as exploring more effective implementa-tions within the open-book framework. Note that although our current implementation demonstratesperformance improvements for the majority of tasks in CLRS, there are instances where the open-book approach may yield counterproductive results. Refining the current architecture to ensureperformance enhancements across all tasks remains a significant challenge.",
  "Acknowledgements": "This work is supported by the National Key Research Project of China under Grant No.2023YFA1009402, the Scientific and Technological Innovation 2030 Major Projects under Grant2018AAA0100902, NSFC Programs (62161146001, 62302166, 62372176), Shanghai Key Lab ofTrustworthy Computing, Henan Key Laboratory of Oracle Bone Inscription Information Processing(AnYang Normal University), and the Key Laboratory of Interdisciplinary Research of Computationand Economics (SUFE), Ministry of Education.",
  "Abeer Aljuaid and Mohd Anwar. Survey of supervised learning for medical image processing.SN Comput. Sci., 3(4):292, 2022": "Beatrice Bevilacqua, Kyriacos Nikiforou, Borja Ibarz, Ioana Bica, Michela Paganini, CharlesBlundell, Jovana Mitrovic, and Petar Velickovic. Neural algorithmic reasoning with causalregularisation. In ICML, volume 202 of Proceedings of Machine Learning Research, pages22722288. PMLR, 2023. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, AlexVitvitskyi, Razvan Pascanu, and Petar Velickovic. Transformers meet neural algorithmicreasoners. CoRR, abs/2406.09308, 2024.",
  "Andrew Joseph Dudzik and Petar Velickovic. Graph neural networks are dynamic programmers.In NeurIPS, 2022": "Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.Neural message passing for quantum chemistry. In ICML, volume 70 of Proceedings of MachineLearning Research, pages 12631272. PMLR, 2017. Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R. McKee, Josh Tenenbaum,and Peter W. Battaglia. Relational inductive bias for physical construction in humans andmachines. In CogSci. cognitivesciencesociety.org, 2018. Borja Ibarz, Vitaly Kurin, George Papamakarios, Kyriacos Nikiforou, Mehdi Bennani, RbertCsords, Andrew Joseph Dudzik, Matko Bosnjak, Alex Vitvitskyi, Yulia Rubanova, AndreeaDeac, Beatrice Bevilacqua, Yaroslav Ganin, Charles Blundell, and Petar Velickovic. A generalistneural algorithmic learner. In LoG, volume 198 of Proceedings of Machine Learning Research,page 2. PMLR, 2022. Rishabh Jain, Petar Velickovic, and Pietro Li. Neural priority queues for graph neural networks.In The 2023 ICML, Workshop on Knowledge and Logical Reasoning in the Era of Data-drivenLearning, volume 202. PMLR, 2023.",
  ": Comparison of the Triplet-GMPNN architectures performance before and after augmenta-tion with the open-book framework. The 30 tasks are arranged in descending order of improvementmagnitude": ": The summary of our obtained test accuracies on CLRS. The best-performing results ineach row are highlighted in bold. The column Prior Best in the table represents the best resultsamong four approaches in the literature: Memnet , PGN , MPNN , and NPQ , andthe column Ours to denote the best results achieved by applying the open-book framework to thethree existing architectures.",
  "DScalability of Our Framework": "This section further investigates the scalability of our framework. For each dataset in the CLRSbenchmark, the training set graphs contain approximately 12 nodes on average and the test set graphscontain 64 nodes. We design two additional experiments (in the context of single-task augmenting).Note that due to memory constraints caused by the increased graph size, we omit the string categorytasks and the quickselect algorithm tasks in these two experiments."
}