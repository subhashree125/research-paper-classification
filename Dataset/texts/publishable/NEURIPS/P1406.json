{
  "Abstract": "Graph Neural Networks (GNNs) are non-Euclidean deep learning models for graph-structured data. Despite their successful and diverse applications, oversmoothingprohibits deep architectures due to node features converging to a single fixed point.This severely limits their potential to solve complex tasks. To counteract thistendency, we propose a plug-and-play module consisting of three steps: Clus-ter Normalize Activate (CNA). By applying CNA modules, GNNs searchand form super nodes in each layer, which are normalized and activated individ-ually. We demonstrate in node classification and property prediction tasks thatCNA significantly improves the accuracy over the state-of-the-art. Particularly,CNA reaches 94.18% and 95.75% accuracy on Cora and CiteSeer, respectively. Itfurther benefits GNNs in regression tasks as well, reducing the mean squared errorcompared to all baselines. At the same time, GNNs with CNA require substantiallyfewer learnable parameters than competing architectures.",
  ": Evolution of node embeddingsfor the Cora dataset. The colors indicate themembership of one of the seven target classes": "Graph Neural Networks (GNNs) are a promising ap-proach to leveraging the full extent of the geometricproperties of various types of data in many differentkey domains [Zhou et al., 2020a, Bronstein et al.,2021, Waikhom and Patgiri, 2023]. For instance, theyare used to predict the stability of molecules [Wanget al., 2023], aid in drug discovery [Askr et al., 2023],recommend new contacts in social networks [Zhangand Chen, 2018], identify weak points in electri-cal power grids [Nauck et al., 2022], predict trafficvolumes in cities [Jiang and Luo, 2022], and muchmore [Waikhom and Patgiri, 2023]. To solve suchtasks, one typically uses message-passing GNNs,where information from nodes is propagated alongoutgoing edges to their neighbors, where it is aggre-gated and then projected by a learned non-linear func-tion. Increasing the expressivity of GNNs is crucial tolearning more complex relationships and eventuallyimproving their utility in a plethora of applications.",
  "Normalize": ": CNA replaces the activation function in each iteration of any GNN architecture. Whenemploying classical activations like ReLU to all nodes undifferentiatedly, we observe oversmoothing.With CNA, we cluster the node features and then normalize and project them with a separate learnedactivation function each, effectively increasing their expressiveness even in deeper networks. A natural approach to increasing expressivity is to increase depth, effectively enabling further-reachingand higher-level patterns to be captured. This combats under-reaching, where information cannotpropagate far enough. For example, this limits the effective radius of information on road crossings intraffic prediction tasks, where information on specific bottlenecks in road networks cannot propagateto the relevant k-hop neighbors. In practice, one wants to increase the depth of the employed GNNs.However, this soon triggers a phenomenon called oversmoothing, where node features are convergingmore and more to a common fix-point with an increasing number of layers [NT and Maehara, 2019,Rusch et al., 2023a]. For example, in the specific task of node classification, node features of differentclasses become increasingly overlapping and, thus, essentially indistinguishable. There are manyattempts to prevent this issue from occurring. Among them is Gradient-Gating (G2), which gatesupdates to nodes once features start converging [Rusch et al., 2023b]. However, G2 adaptivelychokes message passing in each node right before oversmoothing can occur, effectively reducingthe functionality of deeper GNN layers to an identity mapping. This idea of adaptively controllingthe flow of information in each node is still a very promising approach. But, instead of regulatingmessage passing, we propose learning an adaptive node feature update. We argue that it is crucial toensure that while the node features are iteratively exchanged, aggregated, and projected, they staysufficiently different from each other to solve the eventual task, like classification or regression. Thishas the benefit of maintaining effective information propagation even in deeper layers. visualizes the final node features during training, showing how our method improves the separationof the learned classes over the oversmoothed baseline.",
  "Cluster Transformation of the node features should be shared and yet differ at the sametime. For this reason, our first inductive bias is to assume several groups of nodes withshared properties": "Normalize Stabilization of training in deep architectures, including Transformers [Vaswaniet al., 2017], is typically provided by normalization. By employing normalization, CNAeffectively maintains beneficial numerical ranges and combats collapse tendencies. Activate To preserve distinct representations, the clusters must be transformed individually.By introducing learnable activation functions, we learn separate projections for each of them.This generalizes the typical affine transformation following the normalization to a generallearned function that can better adjust to the specific node features.",
  "Code available at": "CNA modules can also be viewed as adding additional hierarchical structure to the problem: Bygrouping nodes into clusters of similar representations, we effectively introduce super-nodes withdifferent non-linear activation functions. Each of their constituents shares the same activation functionyet has distinct node property vectors and neighbors. Moreover, the node features in each super-nodeare less varied since the members of the clusters share some common characteristics. This divide-and-conquer approach breaks up the challenging task of transforming the node features into manysmaller ones. The presented work introduces the novel CNA modules which limit oversmoothing and therebyimprove performance. They allow for many advancements, delivering better performance comparedto the state-of-the-art in many tasks and datasets. In summary, we make the following contributions:",
  "(iv) Lastly, we show that architectures with CNA are parsimonious, achieving better performancethan the state-of-the-art with fewer parameters": "We proceed as follows: We next relate our work to the existing research on GNNs and their specificchallenges (). We then describe and discuss our proposed solution CNA () andconduct a comprehensive evaluation in different scenarios (). Finally, we conclude andsuggest promising next steps for further improving the expressiveness of GNNs ().",
  "Related Work": "Machine Learning on Graphs and its Challenges. Machine learning on graphs has a long history,with graph neural networks as their more recent incarnations [Gori et al., 2005, Scarselli et al.,2008]. Since then, several new models like Graph Convolutional Networks (GCN) [Kipf and Welling,2016], Graph Attention Networks (GAT) [Velickovic et al., 2018], and GraphSAGE [Hamiltonet al., 2017] have been proposed. Gilmer et al. then unified them into the Message PassingNeural Networks (MPNNs) framework, the most common type of GNNs [Battaglia et al., 2018].In addition to the typical machine learning pitfalls like overfitting and computationally demandinghyperparameter optimization, MPNNs pose some specific challenges: oversquasching is the effectof bottlenecks in the graphs topology, limiting the amount of information that can pass throughspecific nodes [Alon and Yahav, 2020, Topping et al., 2021]. The other widely studied challenge isoversmoothing, where the node features converge to a common fixed point with increasing depth ofthe MPNN [Li et al., 2018, NT and Maehara, 2019, Rusch et al., 2023a]. This essentially equatesto the layers performing low-pass filtering, which is harmful to solving the problem beyond somepoint. This phenomenon has also been studied in the context of Transformers [Vaswani et al., 2017],where repeated self-attention acts similarly to an MPNN on a fully connected graph [Shi et al.,2021a]. Different metrics have since been proposed to measure oversmoothing: cosine similarity,Dirichlet energy, and mean average distance (MAD). Rusch et al. [2023a] organize the existingmitigation approaches into three main groups. First, as discussed in more detail in the next paragraph,normalization and regularization are beneficial and are also performed by our CNA modules. Second,one can change the propagation dynamics, as done by GraphCON [McCallum et al., 2000], GradientGating [Rusch et al., 2023b], and RevGNN [Li et al., 2021]. Finally, residual connections can alleviatesome of the effects but cannot entirely overcome them. Solving these challenges is an open task inmachine learning on graphs. Normalization in Deep Learning.In almost all deep learning methods in the many subfields,normalizations have been studied extensively. They are used to improve the training characteristicsof neural networks, making them faster to train and better at generalizing [Huang et al., 2023]. Thesame applies to GNNs, where normalization plays a key role [Zhou et al., 2020a, Cai et al., 2021,Chen et al., 2022, Rusch et al., 2023a]. However, selecting the correct reference group to normalizejointly is key. For example, a learnable grouping is employed in Deep Group Normalization (DGN),where normalization is performed within each cluster separately [Zhou et al., 2020b]. The employedsoft clustering of DGN is only of limited suitability to fostering distinct representations of the node",
  ": The components of CNA modules: They cluster node features without changing theadjacency matrix, normalize them separately, and finally activate with distinct learned functions": "features. Instead, we argue that simple hard clustering, for example, provided by the classic k-meansalgorithm, is sufficient and more desirable. Zhao and Akoglu suggest PairNorm, wherelayerwise normalization ensures a constant total pairwise squared distance of node features. Insteadof adjusting the node features against collapse, Caso et al. rewire the topology based onclusters of node features. For the case of multi-graph datasets, Cai et al. provide a goodoverview of existing approaches, and argue that normalization shall be performed per graph. Learnable Activation Functions. Using non-polynomial activation functions is crucial for neuralnetworks to be universal function approximators [Leshno et al., 1993]. While most works userectified-based functions like ReLU, GeLU, SiLU, etc., there are also attempts at learning somelimited shape parameters as in PReLU or Swish [Apicella et al., 2021]. There has since been furtherwork on learnable activations with reduced flexibility, namely LEAFs [Bodyanskiy and Kostiuk,2023], a combination of polynomials and exponentials. However, one can even learn the overall shapeof the activations, as demonstrated by rational activation functions [Molina et al., 2019, Boulle et al.,2020, Trimmel et al., 2022]. They have proven to be very helpful in a diverse set of applications,in particular, due to their inherently high degree of plasticity during training [Delfosse et al., 2024].More importantly, rationals are smoothly differentiable universal function approximators [Molinaet al., 2019, Telgarsky, 2017], for which reason we select them as flexible activation functions forCNA. Furthermore, changing the activation function has been found beneficial against oversmootingby Kelesis et al. too, which increased the slope of the classic ReLU activation to reduceoversmoothing in MPNNs. This further motivates taking a closer look at activations such as done byKhalife and Basu and in this work.",
  "Cluster-Normalize-Activate Modules": "This section will formally define CNA modules and discuss their design. Adaptive control of theinformation flow is a promising approach to limit oversmoothing in GNNs. We, therefore, proposelearning an adaptive node feature update, ensuring distinct node feature representations during theiterative exchange, aggregation, and projection. This benefits the maintenance of effective informationpropagation in deeper layers. We start by introducing the notation used throughout this work, proceedto recall message-passing GNNs, and finally highlight the three main components of CNA. Theoverall module is shown in . Notation. We consider undirected graphs G = (V, E), where the edges E V V are unorderedpairs {i, j} of nodes i, j V. The set of neighbors of a node i V is denoted as Ni = {j V|{i, j} E} V. We additionally identify each node i V with a feature vector xi Rd.Together, these form the feature matrix X Rd|V|, where each column represents the featuresof a single node. Similarly, depending on whether we model a node-level classification, propertyprediction, or regression task, we have corresponding target vectors yi Rt, with the special case oft = 1 for classification. The target matrix for all nodes is Y Rt|V| or a vector for graph-level. Message-Passing Neural Networks (MPNNs).The most prevalent type of GNNs are MPNNs,with GCN, GAT, and GraphSAGE as their best-known representatives. They iteratively transforma graph by a sequence of L layers = L 1, with Y = (G, X) [Zhou et al., 2020a,Lachaud et al., 2022]. In each layer , two steps of computation are performed. First, the node features h(l)jof the neighbors j Ni of each node i V are aggregated into a single vectorh()i= Aggregate({{h(l)j | j Ni}}). Importantly, the Aggregate operation must be invariant topermutations of the neighbors. Popular choices include the point-wise summation or averaging offeature vectors across all neighbors of a node. Second, these features h()iare projected jointly withthe previous node features, as h(+1)i= Update(h(l)i , h()i ). The resulting node features h(+1)ithen form the input to the next layer. Both the Aggregate and the Update steps can be learned, wherethe latter is often instantiated by Multi-layer Perceptrons (MLPs). Note that the features of the veryfirst layer are simply the node features h(1) = X, and the resulting last hidden representation is ourtarget output: h(L) = Y . We propose improving the Update-step to elevate the effectiveness of the overall architecture. Usually,the learned projection ends with a non-linear activation, like ReLU. Instead, we propose performingthe three steps of CNA, which we will outline below. We want to emphasize that our general recipe isapplicable to any MPNN following the above structure.",
  "Step 1: Cluster": "The node features of typical graph datasets can be clustered into groups of similar properties. Inthe case of classification problems, a reasonable clustering would at least partially recover classmembership. Note that this unsupervised procedure does not require labels and is applicable to awide range of tasks. So, even in regression tasks, the target output for each node will usually differ;therefore, partitioning nodes into groups of similar patterns is advantageous, too. We, therefore,cluster the nodes by their features xi to obtain K groups C1, . . . , CK at the end of each Update-step.This separation allows us to then normalize representations and learn activation functions that arespecific to the characteristics of these subsets of nodes. It is important to note that the geometry, i.e.,the arrangement of edges between nodes, does not change in the progression through GNN layers,while the features associated with each node do. Likewise, cluster membership does not necessarilyindicate node adjacency and thus allows learning on heterophilic data as well. Note that this approachis, therefore, distinct from the graph partitioning often performed to shard processing of graphs basedon its geometry [Chiang et al., 2019]. In principle, any clustering algorithm yielding a fixed number of clusters K can be used to groupthe node features. Popular choices include the classic k-means [MacQueen, 1967] and GaussianMixture Model (GMM) algorithms [Bishop, 2006], which estimate spherical and elliptical clusters,respectively. However, we need to pay attention to the computational costs of such operations.Typical definitions of k-means run in O(|V|Kd) per iteration [Manning et al., 2009]. Expectation-maximization can be used to learn GMM clusters in O(|V|Kd2) per iteration [Moore, 1998]. Wefound that the more expensive execution of GMMs did not materialize in substantial improvements indownstream tasks. We, therefore, opted to use a fast implementation of k-means. This confirms thatk-means often provides decent clustering in practical settings and is sufficiently stable [Ben-Davidet al., 2007]. In our work, we compared nodes by their Euclidean distance, which we found towork reliably in our experiments. However, CNA permits the flexible use of different and evendomain-specific data distances.",
  "pCk(xpj kj)2 ,(1)": "where is introduced for numerical stability. We want to emphasize that this step is similar to InstanceNormalization, yet is nonparametric and does not apply the usual affine transformation to restore theunique cluster representation [Huang et al., 2023]. Similarly, it is not required to scale the mean wesubtract as in GraphNorm [Cai et al., 2021]. Instead, we learn a much more powerful transformationin the subsequent Activate step, which subsumes the expressivity of a normal affine projection andthus renders it redundant. The idea of normalizing per cluster Ck is related to GraphNorm. However,instead of normalizing per graph in the batch, we propose normalizing per cluster within each graph,yet with the same motivation of maintaining the expressivity of the individual node features.",
  "+ |nk=1 bkxk|.(2)": "Their purpose is twofold: Firstly, they act as non-polynomial element-wise projections to increase therepresentational power of the model. Secondly, they replace and subsume the affine transformationin the typical Instance Normalization formulation. Additionally, their strong adaptability allows forappropriate learnable adjustments in the dynamic learning of deep neural networks. This is in linewith the findings of Kelesis et al. , who increased the slope of ReLU activations to combatoverfitting. Our rationals subsume their approach by further lifting restrictions on the activationfunction and tuning the slopes automatically while learning the network. Removing activation functions from GNN layers altogether cansurprisinglyimprove overall per-formance due to reduced oversmoothing [Wu et al., 2019]. Our CNA modules limit oversmoothingfurther, maintaining strong representational power even in deeper networks. We will demonstrate thisin the next section.",
  "Theoretical Underpinnings": "We first show how previous proofs of the necessary occurrence of oversmoothing in vanilla GNNsare not applicable when CNA is used. Next, we explain why these proofs are not easily reinstated byillustrating how CNA breaks free of the oversmoothing curse. Previous Theoretical FrameworksThe Rational activations of CNA trivially break the assump-tions of many formalisms due to their potential unboundedness and not being Lipschitz continuous.This includes Prop. 3.1 of Rusch et al. [2023b], where, however, the core proofs on oversmoothingare deferred to Rusch et al. . Again, the activation is assumed to be point-wise and furthernarrowed to ReLU in the proof in Appendix C.3. Regarding the more recent work of Nguyen et al., we again note that CNA violates the assumptions neatly discussed in Appendix A. The CNAmodule can either be modeled as part of the message function k or as part of the aggregation .However, in both cases, the proof of Prop. 4.3 (which is restricted to regular graphs) breaks down. Inthe former case, there appears to be no immediate way to repair the proof of Eq. (15) in AppendixC.3. In the latter case, providing upper bounds in Appendix C.2 is much more difficult. How CNA Escapes OversmoothingRestoring the proofs for the occurence of oversmoothing isdifficult because CNA was built precisely to break free of the current limitations of GNNs. Thiscan be seen by considering two possible extremes that arise as special cases of CNA. Consider agraph with N nodes. On one end of the spectrum, we can consider CNA with K = N clustersand Rationals that approximate some common, fixed activation, such as ReLU. This renders thenormalization step ineffective and exactly recovers the standard MPNN architecture, which is knownto be doomed to oversmooth under reasonable assumptions [Rusch et al., 2022, Nguyen et al., 2023].The same holds with only a single cluster (K = 1), i.e., MPNNs with global normalization [Zhouet al., 2020b]. Conversely, we can consider K = N clusters, but now with fixed distinct Rationalactivations given by Ri(x) = i for each cluster i 1, . . . , N. The Dirichlet energy of that output isconstant, lower-bounded, and, therefore, does not vanish, no matter the number of layers. In practice,we employ, of course, between K = 1 one and K = N clusters and thereby trade off the degree towhich the GNN is affected by oversmoothing. The following section will investigate this and otherquestions empirically. 24 816326496 0% 20% 40% 60% 80% 100%",
  "(Q4) Model Analysis: How important are each of the three steps in CNA? How do hyperparametersaffect the results?": "Setup.We implemented CNA based on PyTorch Geometric [Fey and Lenssen, 2019] to answerthe above questions. We searched for suitable architectures among Graph Convolutional Network(GCN) [Kipf and Welling, 2016], Graph Attention Network (GAT) [Velickovic et al., 2018], Sam-ple and Aggregate (GraphSAGE) [Hamilton et al., 2017], Transformer Convolution (Transformer-Conv) [Shi et al., 2021b] and Directional GCN (Dir-GNN) [Rossi et al., 2023]. They offer diverseapproaches to information aggregation and propagation within graph data, catering to a wide rangeof application domains and addressing specific challenges inherent to graph-based tasks. Detailson the choice of hyperparameters and training settings are provided in Appendix A.2. Averageperformances and standard deviations are over 5 seeds used for model initialization for all results,except for Tables 1 and 6, where we used 20. (Q1) Limiting Oversmoothing.Since the phenomenon occurs only within deep GNNs, wesystematically increased the number of layers in node classification. We mainly compare vanillaGNNs with ReLU to GNNs with CNA. To complete the analysis, we also consider linearized GNNswithout any activation function, since they were found to be more resilient against oversmoothing atthe expense of slightly reduced performance [Wu et al., 2019]. shows the resulting accuraciesfor depths of 2 to 96. We can confirm the strong deterioration of vanilla GNNs at greater depths andthe partial resilience of linearized GNNs. On the other hand, CNA modules limit oversmoothingdrastically and are even more effective than linearized models. At the same time, they significantlyalleviate the models performance shortcomings, effectively eliminating the practical relevance ofoversmoothing. (Q2) Node Classification, Node Regression, and Graph Classification.We evaluated CNAby incorporating it into existing architectures and compared the resulting performances with theunmodified variants. As the results in demonstrate, our CNA modules significantly improveclassification performance on the Cora dataset [McCallum et al., 2000] by up to 13.53 percentagepoints. Moreover, this improvement shows across different architectures, highlighting CNAs ver-satility. Next, we extend our analysis to many more datasets and compare CNA to the best-knownmodels from the literature. Specifically, we evaluate the performance on the following datasets: Cora,",
  "Trans.Conv+CNA 0.1310.033 0.0680.027": ": Comparison of our method CNA with the leaderboard on Papers with Code (PwC),2 asof writing on a diverse set of node classification datasets from five typical collections. CNA outper-forms the respective leaders, and thereby all compared methods, in eight out of eleven cases (73%).For some, it does so by a significant margin, e.g., on the popular Cora and CiteSeer datasets.",
  "#Wins8/113/11": "CoraFull [Kipf and Welling, 2016], CiteSeer [Bojchevski and Gnnemann, 2018], PubMed [Sen et al.,2008], DBLP [Tang et al., 2008], Computers and Photo [Shchur et al., 2019], Chameleon, Squirrel,Texas, and Wisconsin [Pei et al., 2020]. The results in demonstrate the effectiveness of CNA.Out of 11 of those datasets, CNA outperforms the SOTA on 8 of them. In particular, for CiteSeer,CNA achieves a classification accuracy of 95.75% compared to 82.07% for ACMII-Snowball-2.This suggests that CNA is particularly effective in dealing with the imbalanced class distributionin CiteSeer. The application of CNA is successful on the famous Cora dataset, achieving 94.18%accuracy compared to the 90.16% of SSP. Considering the results in relation to the dataset propertieslisted in Appendix A.1, we can see that CNA is particularly effective on larger datasets and such oneswith many features. It is largely unaffected by the usually detrimental degree of heterophily and thenumber of classes due to the clustering step being mostly independent of them. displays the comparison in performance in multi-scale node regression task as consideredby Rusch et al. [2023b] on the Chameleon and Squirrel datasets [Rozemberczki et al., 2021]. Here,multi-scale refers to the wide range of regression targets from 105 to 1. CNA modules consistentlyoutperform alternative methods in terms of normalized mean squared error (NMSE) based upon theten pre-defined splits by Pei et al. . This superior performance highlights the effectiveness of",
  "our approach in handling the complexities of node-level regression tasks. These results suggest thatour approach has the potential to provide more accurate predictions in real-world scenarios": "To go beyond node-wise tasks on single graphs, we continue by evaluating CNA on graph-levelclassification tasks. Namely, we compared CNA with ReLU on the Mutag [Debnath et al., 1991],Enzymes [Borgwardt et al., 2005], and Proteins [Borgwardt et al., 2005] datasets from the TUDatasetcollection Morris et al. . demonstrates that CNA boosts performance unanimously; forinstance, achieving an impressive improvement of 13 percentage points on Enzymes. A further comparison of CNA to other graph normalization techniques is provided in Appendix A.3.Summarizing the findings on node classification, node regression, and graph classification bench-marks, we can confidently answer (Q2) affirmatively. (Q3) Parameter Parsimony.CNA creates super-nodes in graphs, each rescaled separately andgoverned by an individual learnable activation function. This increased specificity and, in turn,expressivity might allow for more compact models. To investigate this, we use the ogbn-arxiv datasetfrom the Open Graph Benchmark (OGB) [Hu et al., 2020] and follow the setup of Li et al. . Wecompare GNNs equipped with CNA to a set of baselines without it. The results in , first of all,clearly show how CNA outperforms a range of existing GNN models (ii). It achieves a test accuracyof 74.64% while estimating a modest number of learnable parameters (389.2k). This indicatesthat CNA can successfully capture the underlying patterns in the graph data while maintaining acomputationally efficient model. The baselines have varying levels of complexity, with some havingmore layers and/or channels per layer than others. However, CNA outperforms all competitors,even those with more complex architectures. shows that architectures coming close to theperformance of CNA need far more parameters that require learning by gradient descent. Namely,improving GraphSAGE + CNA by 2.47 percentage points (the difference to RevGAT-SelfKD) resultsin a model about 60x bigger. Similarly, the 2.85 percentage point improvement from GraphSAGE+ CNA to GCN + CNA is achieved with a model only about eleven times larger. Additional data,such as the underlying abstract texts originally used to generate the citation graph node features, hasrecently been used with LLMs to distill additional context [He et al., 2023, Duan et al., 2023]. Weexclude them to maintain a level playing field, yet recognize it as an interesting avenue for futurework. We argue that CNA modules pave the way for a desirable development of GNN modelingwhen increasing expressivity would not require an explosion in the number of learnable parameters. (Q4) Model Analysis.We assess the contribution of each of the three operations Cluster,Normalize, and Activate. To this end, we tested GCN with different subsets of the three operationson the Cora dataset. demonstrates that dropping even one of the operations results in minoror no improvement over the plain architecture using ReLU as activation. Cluster-Normalize alreadyimproves over the baseline, confirming the findings of Zhou et al. [2020b]. To assess the sensitivity ofCNA to the choice of its hyperparameters, we compared the effect of the number of hidden featuresand the number of clusters per layer on the Cora dataset using GCN, as shown in . We",
  "Conclusions": "In this work, we proposed Cluster-Normalize-Activate modules as a drop-in method to improvethe Update step in GNN training. The experimental results demonstrated the effectiveness of CNAmodules in various classification, node-property prediction, and regression tasks. Furthermore, wefound it to be beneficial across many different GNN architectures. CNA permits more compact modelson similar or higher performance levels. Although CNA does not entirely prevent oversmoothing,it does considerably limit its effects in deeper GNNs. Our ablation studies have shown that eachstep in CNA contributes to the overall efficacy and its overall robustness. CNA provides a simpleyet effective way to improve the performance of GNNs, enabling their use in more challengingapplications, such as traffic volume prediction, energy grid modeling, and drug design. Limitations.We focused our evaluation on very popular architectures and datasets. While it islikely that CNA is beneficial in many other configurations, we did not evaluate its effects on GNNsthat are not convolutional MPNNs. Similarly, while we did scale or method to the ogbn-arxiv datasetwith about 169k nodes and more than a million edges, yet larger datasets might require further workon the speed of the clustering procedure. Our experiments suggest that oversmoothing is of limitedpractical relevance. Yet, we did not scale this investigation to even greater depth or establish a formallink to existing theories for oversmoothing. Future Work. The presented results motivate further enhancing CNA in multiple ways. Notably,there are three possible directions. Firstly, regarding clustering, we investigated k-means and GMMs,yet it is important to consider other algorithms. For example, Differentiable Group Normaliza-tion [Zhou et al., 2020b] is a promising direction for introducing a learnable clustering step. Further,clustering algorithms need not only to yield a fixed number of clusters k, but should also produceequally sized clusters. Beyond discovering more stable super nodes, this is likely to improve thelearning of the rational projections as well. Apart from representational power, investigating fasterclustering procedures paves the way toward scaling GNNs via CNA to dynamic and continuoustraining settings. Secondly, even more potential for improvement lies in combining CNA with othertechniques. For example, representing the Aggregate step as learnable sequence models [Hamiltonet al., 2017]. These can be beneficial to distill local information to a greater degree, which in turncould further improve performance and limit oversmoothing. Also, combining CNA with establishedmethods like Edge Dropout or Global Pooling can yield compounding benefits. Finally, the abstractidea behind CNA, namely grouping representations and performing distinct updates, is a more generalconcept and applicable beyond the architectures we have considered in this work. For instance,Transformers [Vaswani et al., 2017] are known to be equivalent to MPNNs on fully connected graphsand can similarly exhibit oversmoothing [Shi et al., 2021a], motivating a closer look at this connec-tion. Unifying the theory about the different clustering-based normalization approaches and theireffect on expressivity and phenomena such as oversmoothing might uncover further opportunities forimprovements.",
  "and Disclosure of Funding": "This research project was funded by the ACATIS Investment KVG mbH project Temporal MachineLearning for Long-Term Value Investing and the German Federal Ministry of Education andResearch (BMBF) project KompAKI within the The Future of Value Creation Research onProduction, Services and Work program (funding number 02L19C150) managed by the ProjectManagement Agency Karlsruhe (PTKA). The Eindhoven University of Technology authors receivedsupport from their Department of Mathematics and Computer Science and the Eindhoven ArtificialIntelligence Systems Institute. Authors thank Ponturo Consulting AG for their support.",
  "Andrea Apicella, Francesco Donnarumma, Francesco Isgr, and Roberto Prevete. A survey onmodern trainable activation functions. Neural Networks, 138:1432, June 2021. doi: 10.1016/j.neunet.2021.01.026": "Heba Askr, Enas Elgeldawi, Heba Aboul Ella, Yaseen A. M. M. Elshaier, Mamdouh M. Gomaa, andAboul Ella Hassanien. Deep learning in drug discovery: an integrative review and future challenges.Artificial Intelligence Review, 56:59756037, July 2023. doi: 10.1007/s10462-022-10306-1. Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, CaglarGulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, KelseyAllen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, PushmeetKohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. Relational inductive biases,deep learning, and graph networks, October 2018. URL",
  "Yihao Chen, Xin Tang, Xianbiao Qi, Chun-Guang Li, and Rong Xiao. Learning graph normalizationfor graph neural networks. Neurocomputing, 493:613625, July 2022. ISSN 0925-2312": "Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-GCN: AnEfficient Algorithm for Training Deep and Large Graph Convolutional Networks. In Proceedingsof the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,July 2019. Asim Kumar Debnath, Rosa L. Lopez de Compadre, Gargi Debnath, Alan J. Shusterman, and CorwinHansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds.Correlation with molecular orbital energies and hydrophobicity. Journal of Medicinal Chemistry,34(2):786797, February 1991. ISSN 0022-2623. doi: 10.1021/jm00106a046. Quentin Delfosse, Patrick Schramowski, Martin Mundt, Alejandro Molina, and Kristian Kersting.Adaptive Rational Activations to Boost Deep Reinforcement Learning. In The Twelfth InternationalConference on Learning Representations, 2024.",
  "Van Thuy Hoang and O.-Joun Lee. Mitigating Degree Biases in Message Passing Mechanismby Utilizing Community Structures, December 2023. URL": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. InAdvances in Neural Information Processing Systems, 2020. Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization Techniques in TrainingDNNs: Methodology, Analysis and Application. IEEE Transactions on Pattern Analysis andMachine Intelligence, 45(8):10173 10196, August 2023. doi: 10.1109/TPAMI.2023.3250241. Yiming Huang, Yujie Zeng, Qiang Wu, and Linyuan L. Higher-order Graph Convolutional Networkwith Flower-Petals Laplacians on Simplicial Complexes. In The Thirty-Eighth AAAI Conferenceon Artificial Intelligence, January 2024. Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network trainingby reducing internal covariate shift. In Proceedings of the 32nd International Conference onInternational Conference on Machine Learning - Volume 37, ICML15, pages 448456, Lille,France, July 2015. JMLR.org. Mohammad Rasool Izadi, Yihao Fang, Robert Stevenson, and Lizhen Lin. Optimization of GraphNeural Networks with Natural Gradient Descent. In Proceedings of the IEEE InternationalConference on Big Data (Big Data), 2020.",
  "Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph ConvolutionalNetworks. In 5th International Conference on Learning Representations, 2016": "Guillaume Lachaud, Patricia Conde-Cespedes, and Maria Trocan. Mathematical Expressiveness ofGraph Neural Networks. Mathematics. Mathematical Foundations of Deep Neural Networks, 10(24):4770, January 2022. doi: 10.3390/math10244770. Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforwardnetworks with a nonpolynomial activation function can approximate any function. Neural Networks,6(6):861867, January 1993. doi: 10.1016/S0893-6080(05)80131-5.",
  "Guohao Li, Matthias Mller, Bernard Ghanem, and Vladlen Koltun. Training Graph Neural Networkswith 1000 Layers. In Proceedings of the 38th International Conference on Machine Learning,2021": "Qimai Li, Zhichao Han, and Xiao-ming Wu. Deeper Insights Into Graph Convolutional Networksfor Semi-Supervised Learning. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 32 of 1, April 2018. doi: 10.1609/aaai.v32i1.11604. Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-WenChang, and Doina Precup. Revisiting Heterophily For Graph Neural Networks. Advances inNeural Information Processing Systems, 35:13621375, December 2022.",
  "Christopher Manning, Prabhakar Raghavan, and Hinrich Schuetze. Introduction to InformationRetrieval. Cambridge University Press, 2009": "Andrew Kachites McCallum, Kamal Nigam, Jason Rennie, and Kristie Seymore. Automating theConstruction of Internet Portals with Machine Learning. Information Retrieval, 3:127163, July2000. doi: 10.1023/A:1009953814988. Alejandro Molina, Patrick Schramowski, and Kristian Kersting. Pad Activation Units: End-to-end Learning of Flexible Activation Functions in Deep Networks. In The Eighth InternationalConference on Learning Representations, 2019.",
  "Andrew Moore. Very Fast EM-Based Mixture Model Clustering Using Multiresolution Kd-Trees. InAdvances in Neural Information Processing Systems, 1998": "Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and MarionNeumann. TUDataset: A collection of benchmark datasets for learning with graphs. In ICML2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020), 2020. URLwww.graphlearning.io. Christian Nauck, Michael Lindner, Konstantin Schrholt, Haoming Zhang, Paul Schultz, JrgenKurths, Ingrid Isenhardt, and Frank Hellmann. Predicting basin stability of power grids usinggraph neural networks. New Journal of Physics, 24, April 2022. doi: 10.1088/1367-2630/ac54c9. Khang Nguyen, Hieu Nong, Vinh Nguyen, Nhat Ho, Stanley Osher, and Tan Nguyen. Revisitingover-smoothing and over-squashing using ollivier-ricci curvature. In Proceedings of the 40thInternational Conference on Machine Learning, volume 202 of ICML23, pages 2595625979,Honolulu, Hawaii, USA, July 2023. JMLR.org.",
  "T. Konstantin Rusch, Michael M. Bronstein, and Siddhartha Mishra. A Survey on Oversmoothing inGraph Neural Networks, March 2023a. URL": "T. Konstantin Rusch, Benjamin Paul Chamberlain, Michael W. Mahoney, Michael M. Bronstein, andSiddhartha Mishra. Gradient Gating for Deep Multi-Rate Learning on Graphs. In The EleventhInternational Conference on Learning Representations, February 2023b. Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. TheGraph Neural Network Model. IEEE Transactions on Neural Networks, 20(1):61 80, December2008. doi: 10.1109/TNN.2008.2005605. Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective Classification in Network Data. AI Magazine, 29(3), September 2008. doi:10.1609/aimag.v29i3.2157.",
  "Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gnnemann. Pitfallsof Graph Neural Network Evaluation, June 2019. URL": "Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, andJames Kwok. Revisiting Over-smoothing in BERT from the Perspective of Graph. In The TenthInternational Conference on Learning Representations, 2021a. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked LabelPrediction: Unified Message Passing Model for Semi-Supervised Classification. In Proceedings ofthe Thirtieth International Joint Conference on Artificial Intelligence, page 1548, 2021b. ISBN1045-0823. Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal Sinop.Exphormer: Sparse Transformers for Graphs. In Proceedings of the 40th International Conferenceon Machine Learning, 2023. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. ArnetMiner: extraction andmining of academic social networks. In Proceedings of the 14th ACM SIGKDD internationalconference on Knowledge discovery and data mining, August 2008.",
  "Matus Telgarsky. Neural Networks and Rational Functions. In Proceedings of the 34th InternationalConference on Machine Learning, pages 33873393. PMLR, July 2017": "Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M.Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In The FortiethInternational Conference on Machine Learning, 2021. Martin Trimmel, Mihai Zanfir, Richard Hartley, and Cristian Sminchisescu. ERA: Enhanced RationalActivations. In Shai Avidan, Gabriel Brostow, Moustapha Ciss, Giovanni Maria Farinella, and TalHassner, editors, Proceedings of the European Conference on Computer Vision, 2022. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, LukaszKaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural InformationProcessing Systems, 2017.",
  "Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling Oversmoothing in GNNs. In InternationalConference on Learning Representations (ICLR), 2020": "Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang,Changcheng Li, and Maosong Sun. Graph neural networks: A review of methods and applications.AI Open, 1:5781, January 2020a. doi: 10.1016/j.aiopen.2021.01.001. Kaixiong Zhou, Xiao Huang, Yuening Li, Daochen Zha, Rui Chen, and Xia Hu. Towards deeper graphneural networks with differentiable group normalization. In Proceedings of the 34th InternationalConference on Neural Information Processing Systems, NIPS 20, pages 49174928, Red Hook,NY, USA, December 2020b. Curran Associates Inc. ISBN 978-1-71382-954-6.",
  "A.1Details on the datasets": "An overview of the datasets used in our evaluation is found in and in . In addition tothe number of nodes, edges, features, and classes, we also provided the node homophily ratio andwhether the classes are distributed uniformly, as well as number of graphs for graph-level datasets.The node homophily ratio measures how many of a nodes neighbors are members of the same class.It is computed as:1|V|",
  "*200414280110-3110-5510-6": "where we set 1 = 0.9 and 2 = 0.999. We used summation as the aggregation function due to itssimplicity and widespread use. lists all relevant hyperparameters used for node classification,property prediction, and graph-level classification tasks. provides the hyperparameters for, for the node regression task, as well as for the ablation study. For the sensitivity test, theonly difference from (*) in was the number of layers, which was set to 32. Regardless of thesetting, each experiment was performed on one A100 Nvidia GPU and took between five minutesand two hours, depending on the specific configuration."
}