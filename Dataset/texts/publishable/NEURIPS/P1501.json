{
  "Abstract": "Retrieval Augmented Generation (RAG) has emerged as a crucial technique forenhancing the accuracy of Large Language Models (LLMs) by incorporatingexternal information. With the advent of LLMs that support increasingly longercontext lengths, there is a growing interest in understanding how these modelsperform in RAG scenarios. Can these new long context models improve RAGperformance? This paper presents a comprehensive study of the impact of increasedcontext length on RAG performance across 20 popular open source and commercialLLMs. We ran RAG workflows while varying the total context length from 2,000to 128,000 tokens (and 2 million tokens when possible) on three domain-specificdatasets, and report key insights on the benefits and limitations of long contextin RAG applications. Our findings reveal that while retrieving more documentscan improve performance, only a handful of the most recent state of the art LLMscan maintain consistent accuracy at long context above 64k tokens. We alsoidentify distinct failure modes in long context scenarios, suggesting areas for futureresearch.",
  "Introduction": "The development of Large Language Models (LLMs) with increasingly longer context lengths hasopened new possibilities for Retrieval Augmented Generation (RAG) applications. Recent modelssuch as Anthropic Claude (200k tokens) , GPT-4-turbo (128k tokens) , OpenAI o1 (128k tokens), Llama 3 and Google Gemini 1.5 Pro (2 million tokens) have led to speculation aboutwhether long context models might eventually subsume traditional RAG workflows entirely. In thisstudy, we empirically investigate the impact of increased context length on RAG performance andexplore the limitations and challenges that arise in long context scenarios. RAG can enhance the accuracy of LLMs by retrieving information from external sources, enablingusers to incorporate task-specific or private data into their LLM workflows. Published results usingRAG-like methods have demonstrated benefits across many applications including machine",
  "arXiv:2411.03538v1 [cs.LG] 5 Nov 2024": ":Long context RAG performance of o1,GPT-4,Claude 3/3.5,Gemini 1.5(gemini-1.5-pro-001 and gemini-1.5-flash-001), Llama 3/3.1, Qwen 2, Mistral and DBRXmodels on 3 curated RAG datasets (Databricks DocsQA, FinanceBench, and Natural Questions). Allvalues can be found in Table S3. Model versions are listed in Table S1. translation , semantic parsing , question answering , and open-ended text genera-tion . With longer context lengths, LLM developers can feed more documents into their RAGapplications. While there has been recent speculation that long context LLMs will replace RAGentirely , in this paper we study whether long context LLMs can indeed be used effectively forRAG systems. How well do the best open source and commercial models do on long-context RAGtasks? In this study, we apply a standard RAG approach and evaluate the performance of 20 popular opensource and commercial LLMs with varying context lengths from 2,000 to 128,000 tokens (and 2million tokens when possible). We then analyze distinct failure modes for different models acrosslong context RAG scenarios. We show that: Using longer context does not uniformly increase RAG performance. The majority ofmodels we evaluated first increase and then decrease RAG performance as context lengthincreases. Only a handful of the most recent state of the art LLMs can maintain consistentaccuracy at long context above 64k tokens. LLMs fail at long context RAG in unique ways as a function of context length. Whilesome models tended to provide incorrect answers, others failed to follow instructions orrefused to answer due to perceived copyright concerns.",
  "Background and Related Work": "RAG combines the strengths of retrieval-based and generation-based approaches in natural languageprocessing, and has shown significant improvements in the quality of question-answering systemsacross many domains and tasks . The process typically involves two main steps, retrievaland generation. During the first stage, relevant information is retrieved from a corpus or databasebased on a user query. This often involves embedding documents and storing them in a vectordatabase for similarity-based retrieval. During the generation stage, the retrieved information iscombined with the user query as input to an LLM. Importantly, multiple retrieved documents can beincluded as input to the LLM depending on the maximum context length of the model. Recent advancements in LLM capabilities have led to models with increasingly larger context lengths.While early models like GPT-3.5 had a context length of 4k tokens, newer models such as AnthropicClaude (200k tokens), OpenAI o1 (128k tokens) and Google Gemini 1.5 models (2 million tokens)",
  "support much longer contexts. Open source models have followed a similar trend, with recent modelslike Mixtral and DBRX supporting 32k tokens, and Llama 3.1 reaching 128k tokens": "However, recent studies have identified limitations in long context models. For example, the lost inthe middle paper found that models struggle to retain and utilize information from the middleportions of long texts, leading to performance degradation as context length increases. Similarly,the RULER paper found that the effective context length (usable context before performancedecreases) can be much shorter than the claimed maximum context length. Recent studies have alsotried to compare RAG to workflows where the entire corpus is included in the context window of theLLM . This has only been possible to do with the very recent state of the art models such as o1,GPT-4o, Claude 3.5, Gemini 1.5, Qwen 2 72B and Llama 3.1 405B, and the jury is still out on whethersuch an approach leads to accurate results and is cost effective. Other relevant studies and blogpostsinclude . Similar to our study, Jin et al. find that increasing the numberof retrieved passages does not consistently improve RAG performance for Gemma-7B, Gemma-2-9B,Mistral NeMo 12B but does for Gemini 1.5 Pro . Our concurrent work corroborates this across20 closed and open source models.",
  "Methodology": "We conducted RAG experiments using 20 popular open source and commercial LLMs, and evaluatedtheir performance on three datasets: Databricks DocsQA,1 FinanceBench , and Natural Questions. For the retrieval stage, we retrieved document chunks using the same embedding model acrossall settings (OpenAI text-embedding-3-large2 with a chunk size of 512 tokens and a stride of256 tokens) and used FAISS3 (with IndexFlatL2 index) as the vector store. These chunks were theninserted into the context window of a generative model. We then evaluated how generation performance changes as a function of the number of retrieveddocument chunks by varying the LLM context from 2,000 tokens to 128,000 tokens (and 2 milliontokens when possible). We evaluated the following models: o1-mini, o1-preview, Gemini 1.5 Pro,Gemini 1.5 Flash,4 GPT-4o, Claude 3.5 Sonnet,5 Claude 3 Opus, Claude 3 Sonnet, Claude 3 Haiku,GPT-4o mini, GPT-4 Turbo, GPT-4, Llama 3.1 405B, Llama 3 70B, Llama 3.1 70B, Llama 3.1 8B,Qwen 2 72B, Mixtral 8x7B, DBRX, and GPT-3.5 Turbo. These models represent some of the mostpopular API-based and open source LLMs as of this writing. A full list of the model versions used inthis study can be found in Table S1. For generation, we set the temperature to 0.0 and the maximum output sequence length to 1024.We used a simple prompt template to combine the retrieved documents with the user query foreach model and dataset (Appendix E). The system had to correctly answer questions based on theretrieved documents, and the answer was judged by a calibrated LLM-as-a-judge using GPT-4o(see Appendix D for further details). Finally, we analyzed the failure patterns for selected models (OpenAI o1, Gemini 1.5 Pro, Llama 3.1405B, GPT-4, Claude 3 Sonnet, DBRX, and Mixtral) in long context scenarios by using GPT-4o toclassify failures into broad categories such as refusal and wrong answer (Appendix F.1). We alsoinclude an analysis of retrieval performance (recall@k) in Appendix C.",
  "Using longer context does not uniformly increase RAG performance": "The best commercial models such as o1-mini/preview, GPT-4o, and Claude 3.5 Sonnet steadilyimprove performance as a function of context length, while the majority of the open source modelsfirst increase and then decrease performance as context length increases (Figs. 1 and 2). Overall,we found that the following models show consistent accuracy improvement up to 100k tokens:o1-preview and o1-mini, GPT-4o and GPT-4o mini, Claude 3.5 Sonnet, Claude 3 Opus, and Gemini 1Databricks DocsQA is a benchmark of technical questions and answers related to the Databricks platform.2 used the versions of Gemini 1.5 released in June 2024, specifically gemini-1.5-pro-001 andgemini-1.5-flash-001 with 2 million token context windows.5We used the Claude 3.5 Sonnet released in June 2024, claude-3-5-sonnet-20240620",
  "Pro. These models exhibit largely monotonic behavior where the results dont get significantlyworse after they peak": "Among the open source models, Qwen 2 70B maintains consistent accuracy up to 64k. Llama 3.1405B performance starts to decrease after 32k tokens, GPT-4-0125-preview starts to decrease after64k tokens, and only a few models can maintain consistent long context RAG performance on alldatasets. This demonstrates that while some models that boast long contexts can be used effectivelyto increase RAG performance, the majority of open source models can only handle effective RAGtasks up to roughly 16k-32k tokens. We report very strong performance from the OpenAI o1 models; the o1 models seem to be asubstantive improvement over GPT-4 and GPT-4o. Although the overall answer correctness of theGoogle Gemini 1.5 Pro and Gemini 1.5 Flash models is much lower than that of the o1 and GPT-4omodels up to 128,000 tokens, the Gemini models maintain consistent performance at extremely longcontexts up to 2,000,000 tokens. This is quite unique among the models we tested, and is an excitingexample of how future LLMs will handle long context.",
  "LLMs Fail at Long Context RAG in Different Ways": "We found distinct failure patterns among different models in long context scenarios. displaysthe failure count and failure type as a function of context length on the Natural Questions (NQ)dataset. As shown in the top right plot of , Claude 3 Sonnet frequently refused to answer dueto perceived copyright concerns, especially at longer context lengths. Gemini 1.5 Pro maintainedconsistent performance at extreme long context (up to 2 million tokens), but increasingly failed tasksat long context length due to overly sensitive safety filters ().6 Among the open source models,Llama 3.1 405B maintained consistent failure performance up to 64k tokens, while many of thefailures of Mixtral-8x7B at longer contexts were due to repeated or random content. Finally, DBRXoften failed to follow instructions for context lengths above 16k, often summarizing content insteadof answering questions directly. We include specific examples in Appendix F.",
  "In this study, we asked a straightforward question: can long context LLMs improve RAG perfor-mance? We found that for recent state of the art models such as o1, GPT-4o, Claude 3.5, Gemini 1.5,": "6We note that we did not include any queries that failed in this way (i.e. by filtering) in the final accuracyscore. On Natural Questions specifically, Gemini 1.5 Pro and Flash did remarkably well with answer correctnessvalues above 0.85 at 2 million tokens context length (see Fig. S2). : Failure analysis on the Natural Questions (NQ) dataset for Gemini 1.5 Pro, Claude 3 Sonnet,Mixtral 8x7B, and Llama 3.1 405B. Gemini 1.5 Pro (gemini-1.5-pro-001) increasingly failedtasks at long context length due to overly sensitive safety filters, while Claude 3 Sonnet frequentlyrefused to answer due to percieved copyright concerns. and even Qwen 2 70B, longer contexts can consistently improve RAG performance. However, longercontext is not uniformly beneficial across all models and datasets. Across the majority of models weanalyzed, most LLMs only showed increasing RAG performance up to 16-32k tokens. Why does o1 do so well? We hypothesize that the increased test-time compute abilities of o1 allow the model to handle confusing questions and avoid getting misled by retrieved documents thatare irrelevant. It is also interesting to note that for the NQ dataset, many of the failures were due to alignment(Claude 3 Sonnet) or safety filtering (Gemini 1.5 Pro). We speculate that this is because the training ofthose capabilities did not include long context; if a model is trained for helpfulness on short contexts,for example, it might not necessarily do as well with helpfulness on long contexts. It is surprising thatalignment could fail at different prompt lengths; we leave a deep dive into this behavior for futurework. Our results imply that for a corpus smaller than 128k tokens (or 2 million in the case of Gemini),it may be possible to skip the retrieval step in a RAG pipeline and instead directly feed the entiredataset into the LLM. Is this a good idea? Although this would be prohibitively expensive and havepotentially lower performance, such a setup could eventually allow developers to trade higher costsfor a more simplified developer experience when building LLM applications. The costs vary widely across models. For a single query with a maximum sequence length of 128ktokens, GPT-4o costs $0.32, while o1-preview costs $1.92, Claude 3.5 Sonnet costs $0.384 andGemini 1.5 Pro costs $0.16.7 Using very long context for RAG is much more expensive than simplymaintaining a vector database and retrieving a handful of relevant documents. Batch inference andcorpus caching can likely mitigate these costs; this is an active area of development. In the past yearalone weve seen the price per million input token drops from $30 for GPT-4 to $2.5 for GPT-4o;8 inthe near future it is likely using 128k tokens will become more feasible financially.",
  "and Disclosure of Funding": "We would like to thank Andrew Drozdov, Andy Zhang, and Erica Yuen for their work that enabledthese experiments as well as their feedback on this manuscript. We would also like to thank theDatabricks AI Research team for their support and valuable discussions throughout this project.This work was supported by Databricks, and all experiments were run on the Databricks Mosaic AIplatform.",
  "OpenAI.Learningtoreasonwithllms. September 2024": "Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herdof models. arXiv preprint arXiv:2407.21783, 2024. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al.Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXivpreprint arXiv:2403.05530, 2024.",
  "Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearestneighbor machine translation. arXiv preprint arXiv:2010.00710, 2021": "Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari,and Fatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trainedlanguage model: An empirical study on codex. ArXiv, abs/2301.12868, 2023. URL Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel,Hannaneh Hajishirzi, and Wen-tau Yih. Task-aware retrieval with instructions. arXiv preprintarXiv:2211.09260, 2022.",
  "Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen-eralization through memorization: Nearest neighbor language models.arXiv preprintarXiv:1911.00172, 2019": "Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Bo-ratko, Yi Luan, Sbastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-contextlanguage models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121,2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, NamanGoyal, Heinrich Kttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. Retrieval-augmentedgeneration for knowledge-intensive nlp tasks. In Advances in Neural Information ProcessingSystems, 2020. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov,Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP), pages 67696781, 2020. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun,Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: Asurvey. arXiv preprint arXiv:2312.10997, 2023. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, ChrisBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand,et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024.",
  "Jing Zhang, Yifan Shen, Yuechen Jiang, Biao Yin, Xinyang Zhang, Mingxuan Wang, andJie Zhou.Lost in the middle: How language models use long contexts.arXiv preprintarXiv:2307.03172, 2023": "Chenxin Zhang, Ziyi Zhang, Mingyang Xu, Zhengyan Chen, Xu Zhao, Jie Huang, Zhiyuan Li,Qifan Liu, Zhiyuan Liu, and Maosong Sun. Ruler: Assessing long context utilization of largelanguage models. arXiv preprint arXiv:2305.19387, 2023. Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrievalaugmented generation or long-context llms? a comprehensive study and hybrid approach. arXivpreprint arXiv:2407.16833, 2024.",
  "Jay Alammar, Maxime Voisin, and Sam Barnett. Rag is here to stay: Four reasons why largecontext windows cant replace it": "Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, DongWang, Xuanhui Wang, and Michael Bendersky. Inference scaling for long-context retrievalaugmented generation. arXiv preprint arXiv:2410.04343, 2024. Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, and Reut Tsarfaty.Is it really long context if all you need is retrieval? towards genuinely difficult long context nlp.arXiv preprint arXiv:2407.00402, 2024.",
  "Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and BertieVidgen. Financebench: A new benchmark for financial question answering. arXiv preprintarXiv:2311.11944, 2023": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, ChrisAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: abenchmark for question answering research. Transactions of the Association for ComputationalLinguistics, 7:453466, 2019. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685,2023.",
  "CRetrieval Performance": "We assessed how retrieving more results would affect the amount of relevant information placed inthe context of the generation model. Specifically, we assumed that the retriever returns X numberof tokens and then calculated the recall score at that cutoff. From another perspective, the recallperformance is the upper bound on the performance of the generation model when the model isrequired to use only the retrieved documents for generating answers. Below are the recall@k results for the OpenAI text-embedding-3-large embedding model on3 datasets and different context lengths (Table S5). We use chunk size 512 tokens and leave a 1.5kbuffer for the prompt and generation. Recall@k here is different for each run based on the totalnumber of retrieved chunks; for example, when 1 chunk is retrieved, we report recall@1, and when 61chunks are retrieved we report recall@61. We note the relationship between the number of retrievedchunks and the maximum context length in Table S5.",
  "Table S5: Retrieval performance (recall@k) for OpenAI text-embedding-3-large, which wasused as the retriever in all of our experiments": "Saturation point: as can be observed in the table, each datasets retrieval recall score saturates at adifferent context length. For the NQ dataset, it saturates early at 8k context length, whereas DocsQAand FinanceBench datasets saturate at 96k and 128k context length, respectively. These resultsdemonstrate that with a simple retrieval approach, there is additional relevant information availableto the generation model all the way up to 96k or 128k tokens. Hence, the increased context size ofmodern models offers the promise of capturing this additional information to increase overall systemquality.",
  "DEvaluation with LLM-as-a-judge": "We used the LLM-as-a-judge paradigm to measure the answer correctness of the generatedanswer with regards to the ground truth answer. In all experiments, we use the judge from theDatabricks Agent Evaluation framework.9 The judge has been calibrated with human preferenceson representative datasets FinanceBench, Databricks DocsQA and the judge reported 88.1 5.5%agreement and Cohens kappa scores of 0.64 0.13, showcasing a strong agreement with humanlabelers.",
  "E.3Natural Questions (NQ)": "You are an assistant that answers questions. Use the following pieces of retrieved context to answerthe question. Some pieces of context may be irrelevant, in which case you should not use them toform the answer. Your answer should be a short phrase and should not be in a complete sentence.",
  "refusal: the model either refuses to answer the question, mentions that the answer cantbe found in the context, or states that the context is not relevant to the question": "task_failed: the model API simply blocked the prompt due to strict filtering guidelines.Note that if the task failed due to API filtering, we did not include this in the final AnswerCorrectness calculation. We developed prompts that describe each category and used GPT-4o to classify all of the failures ofthe models into the above categories. We also note that the failure patterns on these datasets maynot be representative of other datasets; it is also possible for the pattern to change with differentgeneration settings and prompt templates. We representative examples of failures below.",
  "F.1Prompt for Failure Analysis": "You are a helpful assistant good at categorizing the cause of the failure in answers. Ill provide youthe question thats asked, the expected answer and the generated answer, and youll need to call afunction to output the category of the failure that it falls under and the corresponding one sentencejustification.",
  "repeated_content: when the answer is completely repeated words or characters": "random_content: (the content is not completely repeated but) when the answer is completelyrandom, irrelevant to the content and doesnt make logical or grammatical sense, note that ifthe expected answer is a single or two words and the generated answer is also a single ortwo words that might be relevant, then its not random_content, but wrong_answer",
  "who played mrs warboys inone foot in the grave": "Doreen MantleIm afraid I cannot reproduce or quote copyrightedmaterial such as song lyrics, book excerpts, orlong passages from periodicals without permission.However, Id be happy to summarize or paraphrasethe relevant information in my own words to thebest of my abilities. Please let me know if youwould like me to provide a summary instead.",
  "when did korns follow theleader come out": "August 18 , 1998I apologize, but I cannot reproduce copyrightedlyrics, book excerpts, or other lengthy copyrightedmaterial in my response. However, Id be happyto provide a brief summary or factual informationrelated to the song \"Its the End of the World asWe Know It (And I Feel Fine)\" by R.E.M. with-out violating copyrights. Please let me know ifyou would like me to summarize details about thesongs background, release, chart performance, orcritical reception instead.",
  "GwendolineChristie": "I apologize, but I cannot reproduce or quote exten-sively from copyrighted works without permission,as that would violate copyright laws. However, Idbe happy to provide a brief summary or my ownthoughts on the topic youre interested in, as longas it does not require reproducing copyrighted ma-terial verbatim. Please feel free to rephrase yourrequest in a way that does not ask me to reproduceprotected content, and Ill do my best to assistyou helpfully while respecting intellectual prop-erty rights.",
  "F.3GPT-4 Failures on Natural Questions": "GPT-4 tended to fail by providing the wrong answer (e.g. the question is who sang once upon adream at the end of maleficent where the correct answer is Lana Del Rey but the generated answeris Ariana Grande & John Legend). Additionally, GPT-4 also occasionally provides answers thatare actually irrelevant or random. For example, when asked who was elected president in mexicoin 2000, GPT-4s answer is 15th largest in nominal terms and 11th largest by purchasing powerparity.",
  "F.4Mixtral-8x7B and DBRX Failures on Natural Questions": "On the other hand, Mixtral-instruct and DBRX-instruct failed in unique ways: Mixtral-instructconsistently outputed repeated content such as the character for dream in Chinese or randomcontent that was irrelevant to the question. In contrast, DBRX-instruct consistently failed to followinstructions by summarizing the content instead of answering the question. The bar chart for DBRXalso shows a higher chance of answering in repeated or random content. Upon closer examination,these random or repeated content were still trying to summarize the context but with repeatedsentences. We include samples from Mixtral-instruct and DBRX-instruct below:",
  "who was the top scorer in 2014world cup": "James RodrguezThe passage appears to be a table of top goalscorersin the FIFA World Cup, with the number of goalsscored by each player in different World Cups.The table includes players such as Miroslav Klose,Ronaldo, and Gerd Mller. The table also includesthe total number of goals scored by each player intheir World Cup career. The passage also includesa note about the table, stating that the table includesgoals scored in both the group stage and knockoutstage of the World Cup.",
  "F.5Gemini 1.5 Pro Failures on Natural Questions": "As can be seen in , Gemini 1.5 Pro failed on the Natural Questions benchmark for tworeasons, task_failed and wrong_answer. The task_failed scenario was caused by the strictcontent filtering of the Gemini API. This was particularly pronounced for the Natural Questions (NQ)dataset, where the filtering increased as a function of context length.",
  "BlockedPromptException: blocked reason \"Others\"": "finish_reason: SAFETYsafety_ratings {category: HARM_CATEGORY_SEXUALLY_EXPLICITprobability: MEDIUM}safety_ratings {category: HARM_CATEGORY_HATE_SPEECHprobability: NEGLIGIBLE}safety_ratings {category: HARM_CATEGORY_HARASSMENTprobability: NEGLIGIBLE}safety_ratings {category: HARM_CATEGORY_DANGEROUS_CONTENTprobability: NEGLIGIBLE} The Natural Questions dataset is a standard, well established academic dataset based on Wikipedia.We are not aware of known examples of hate speech or harassment content in NQ. Our benchmarkingdid not encounter these types of strict filters when using any of the other APIs (OpenAI, Anthropic,etc.). We note that we did not include any queries that failed in this way (i.e. by filtering) in the finalaccuracy score. On Natural Questions specifically, Gemini 1.5 Pro and Flash did remarkably wellwith answer correctness values above 0.85 at 2 million tokens context length (see Fig. S2).",
  "F.6Gemini 1.5 Pro Failures on Databricks DocsQA": "The failures on the Databricks DocsQA dataset look somewhat different. There are very few instancesof task failure due to safety filtering; rather, most of the failures are simply due to incorrect answers.We include a few examples below: Figure S3: Gemeni 1.5 Pro Answer Failures on the Databricks DocsQA Dataset. For each contextlength, the majority of failures fall into the wrong_answer category. Note that this behavior issomewhat different from the NQ dataset in .",
  "expected_answer": "The Data Explorer is used for viewing table schema details, previewing sample data, accessing tabletype, table location, and table properties, and reviewing table history. It also allows users to viewfrequent queries and users who have accessed the table. Additionally, Data Explorer can be used tocreate basic queries and quick dashboards, and to set table ownership and permissions.",
  "generated_answer": "Yes, auto optimize is particularly useful for streaming Delta tables. It helps to compact small filesthat are often generated in streaming use cases, which can improve read performance. However, keepin mind that auto optimize adds some latency overhead to write operations. So, its best suited forstreaming scenarios where a few minutes of latency is acceptable.",
  ". Initially, the Serving endpoint state will be shown as Not Ready": "After a few minutes, this will change to Ready once the endpoint is up and running. You can alsoaccess the Serving UI to create an endpoint directly from the registered model page by selecting themodel you want to use, clicking the Use model for inference button, choosing the Real-time tab,and then filling in the model version and endpoint name. Select the compute size, specify if it shouldscale to zero when not in use, and then click Create serving endpoint. The Serving endpointspage will then display the status as Not Ready, changing to Ready after the endpoint is set up.",
  "No. The quick ratio for 3M was 0.96by Jun23 close, which needs a bit of animprovement to touch the 1x mark": "The passage does not provide informa-tion about 3Ms quick ratio for Q2 ofFY2023. Therefore, I cannot answeryour question based on the provided pas-sage.However, the passage does state that3M maintains a strong liquidity profile.It also mentions that the company meetsits short-term liquidity needs throughcash on hand and U.S. commercial paperissuances.To determine if 3M has a reasonablyhealthy liquidity profile, you would typ-ically calculate liquidity ratios like thequick ratio (acid-test ratio) or current ra-tio using data from their financial state-ments."
}