{
  "Abstract": "The autoencoder model typically uses an encoder to map data to a lower dimen-sional latent space and a decoder to reconstruct it. However, relying on an encoderfor inversion can lead to suboptimal representations, particularly limiting in phys-ical sciences where precision is key. We introduce a decoder-only method usinggradient flow to directly encode data into the latent space, defined by ordinarydifferential equations (ODEs). This approach eliminates the need for approximateencoder inversion. We train the decoder via the adjoint method and show that costlyintegrals can be avoided with minimal accuracy loss. Additionally, we propose a2nd order ODE variant, approximating Nesterovs accelerated gradient descent forfaster convergence. To handle stiff ODEs, we use an adaptive solver that prioritizesloss minimization, improving robustness. Compared to traditional autoencoders,our method demonstrates explicit encoding and superior data efficiency, which iscrucial for data-scarce scenarios in the physical sciences. Furthermore, this workpaves the way for integrating machine learning into scientific workflows, whereprecise and efficient encoding is critical. 1",
  "Introduction": "Auto-encoders are widely successful in supervised learning due to their ability to learn lower-dimensional representations of input data, enabling efficient computation . The basic idea isthat when sufficient correlation exists within the input data, the latent dimension can generate amodel of the input. However, since the encoder is not directly learned, the encoding process canbe semi-arbitrary, leading to suboptimal latent space representations and inefficient learning .To address this, efforts have been made to directly regularize the latent space . Typically, theencoder approximates the inverse of the decoder, but this requires learning additional parameters,impeding learning, particularly with limited data. Flow models attempt to resolve this by usinginvertible maps, but they are restricted to equi-dimensional latent spaces. What if we could eliminate",
  "the encoder, retain the advantages of a lower-dimensional latent space, and directly optimize therepresentation? This could enable faithful reconstruction with fewer samples and iterations": "Currently, DeepSDF optimizes latent space representations using a continuous signed distancefunction to model 3D shapes, where a latent vector z represents the shape encoding. HamiltonianNeural Networks and VampPrior introduce physically-informed dynamics and more ex-pressive priors for VAEs, respectively, contributing to latent space optimization. SAVAE andthe energy-based model approach in directly aim to optimize the encoder through iterativerefinement. In this work, we propose a novel encoding method, namely gradient flow encoding (GFE). Thisdecoder-only method explicitly determines the optimal latent space representation at each trainingstep via a gradient flow, namely, an ordinary differential equation (ODE) solver. The decoder isupdated by minimizing the loss between the input image and its reconstruction from the optimallatent space representation. Although slower, this method converges faster and demonstrates superiordata efficiency and ultimately achieves better reconstructions with a minimal number of trainingsamples compared to a conventional auto-encoder (AE). A notable advantage of GFE is the reductionin network size, as it eliminates the need for an encoder. Using a gradient flow for encoding can be computationally demanding. ODE solvers with adaptivestep sizes aim to minimize integration error, which is crucial for accuracy. However, these solverscan slow down training when the ODE becomes stiff, as step sizes shrink and make integrationtime-consuming . This is especially problematic in GFE-based training. We find that the exactgradient flow path may be less important than reaching convergence, so minimizing integration errormay not be optimal. Fixed step sizes also cause poor convergence and stability in decoder training.To address this, we develop an adaptive solver that adjusts each step to minimize the loss betweeninput and output. We also introduce a loss convergence assertion in the adaptive minimize distance(AMD) solver, making gradient flow more computationally feasible for neural networks. For optimizing the latent space and decoder in GFE, we implement an adjoint method , which hasalso been used in other recent works . To improve efficiency, we demonstrate that a full adjointmethod may not always be necessary, and an approximation can be effectively utilized. Moreover,we introduce a Nesterov second-order ODE solver, which accelerates convergence for each trainingdata size. Ultimately, the approximate GFE utilizing AMD (GFE-amd) is employed for testing andcomparison with a traditional AE solver. Our flow-inspired model offers a solution for the stringentdata efficiency and robustness demands in fields like physics, astronomy, and materials science. Byutilizing the flow-model for encoding, we contribute to algorithms that create interpretable, accuratepredictive models, enabling more robust discoveries and insights in scientific research.",
  "Method": "An auto-encoder maps an input y to a lower-dimensional latent space z via an encoder E, thenreconstructs it using a decoder D, with E and D acting as approximate inverses. During training,each sample is mapped to z by E, reconstructed by D, and the reconstruction error is minimized withrespect to both networks parameters. At any point, an optimal latent representation z minimizes thereconstruction error, but the encoder doesnt directly map to z. Instead, E(z) is updated to make itsreconstruction closer to the sample, which may not be efficient. Alternatively, z can be found andused to update the decoder directly. Determination of z for each sample y can be formulated as an optimization problem: z =arg minz l(y, D(z, )), where represents the parameters of the decoder network and l(, ) is adistance function between the sample and its reconstruction by the decoder, which can be definedbased on the application. One common form is the L2 distance D(z, ) y22. The optimizationcan be achieved by gradient descent minimization. To integrate this minimization into the training ofthe decoder network, a continuous gradient descent algorithm is implemented via the solution to anODE:dzdt = (t)zl(y, D(z(t), )),z(0) = 0,(1)",
  "(t)zl(y, D(z(t), )) dt,z(0) = z0,(2)": "where z0 is the initialization of the optimization, which is set as the zero vector in our experiments.Consequently, after minimizing z z z(t = ) for a given decoder D (the forward model),the decoder is trained with a total loss function for a given training set M (the backward step):",
  "Nesterovs 2nd-Order Accelerated Gradient Flow": "The gradient flow described above is based on naive gradient descent, which may be slow in conver-gence. The convergence per iteration can be further increased by considering Nesterovs acceleratedgradient descent. A second-order differential equation approximating Nesterovs accelerated gradientmethod has been developed in , and additionally incorporated into Adam . This 2nd orderODE for z is given by:d2zdt2 + 3",
  "Adaptive minimise distance solver": "Step size is crucial in discretized optimization algorithms for reaching a local extremum, includingsolving the gradient flow equation for optimal latent space representation. Fixed time-step solverscan cause instabilities during decoder training due to stiffness, as predefined time slices may notadapt to rapid changes in the gradient flow, see App. A.2. For example, a 4th order Runge-Kuttamethod with a fixed grid uses t slices in a logarithmic series to manage variations near t = 0, butthis can still lead to instability, particularly in forward and backward passes. While adaptive step-sizeODE solvers can theoretically address these issues, stiffness in the gradient flow equation remains achallenge. Adaptive step-size ODE solvers address stiffness in gradient flow equations but prioritize accurateintegration, which isnt always useful for decoder training. A more effective approach minimizesloss at each step, regardless of the integration path. To achieve this, we develop an adaptive step-sizemethod resembling an explicit ODE solver, like the forward Euler method, but without a fixed grid.The challenge is solving ODE 1 while selecting time-steps that reduce l(y, D(z(t), )). Essentially,this is gradient descent with adaptive step-size selection . In this framework, (t) is redundant andcan be set to 1, with the time-step t taking over its role. In the AMD method, at each time tn, tn is chosen by finding the smallest m = 0, 1, ... that satisfies:l (y, D(z(tn) msnzl(y, D(z(tn), )), )) < l (y, D(z(tn), )) ,(6)with (0, 1) (set to 0.75 in our experiments) and sn as a scaling factor. At each time pointtn, the time-step is set as tn = msn. The scaling factor is updated at each iteration as sn =max(sn1, s0), sn = min(sn, smax), smax = 10, s0 = 1, = 1.1. Based on this, tn+1 =tn + tn andz(tn+1) = z(tn) tnzl(y, D(z(tn), )).(7)If the chosen time-step goes beyond , a smaller step is used to reach exactly. The solution of theintegral of Eq. 1 is then z(). Furthermore, the AMD solver monitors the gradient of the convergencecurve to determine if the loss function is sufficiently optimized, allowing it to assign a new final and stop early to avoid unnecessary integration.",
  "Results and Discussion": "A direct comparison of GFE-amd to a conventional AE for MNIST training is shown in .Further experimental details can be found in App. B. The x-axis shows the number of training imagesprocessed, not iterations. The training uses mini-batch data with replacement, revisiting the datamultiple times. GFE-amd demonstrates significantly better learning per image, nearly converging at800,000 images, see left. This is a consequence of efficient latent space optimization. However,this comes with higher computational cost per iteration due to the ODE solver, see right. Bothmodels use the Adam optimizer, therefore the difference can be attributed to better gradients theGFE-amd model generates to update the decoder network at each training iteration. training images 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 cross entropy loss GFE-amdAE time 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 cross entropy loss GFE-amdAE :Left Validation mean cross-entropy loss vs. number of MNIST training images forGFE-amd and AE methods, with GFE-amd showing significant convergence with minimal trainingdata. Right Validation mean cross-entropy loss vs. time for GFE-amd and AE methods, with AEbeing faster due to more iterations in the same time span. This increase in computation is not necessarily a disadvantage, considering the efficient learningof the GFE-amd method. In right, the average cross entropy loss for a complete test-set isrecorded for both methods for some small number of training images. The GFE-amd is able to learnquite well even after seeing a tiny fraction of the total training data. Furthermore, the GFE-amdmethod noticeably improves an AE trained decoder when it is used to test, the result of an optimizedlatent space even without a network parameters update. To verify the overall quality of the method both the AE and GFE-amd are tested when convergedas shown in left. The GFE-amd performs very similar to AE both for MNIST, SegmentedMNIST (SegMNIST) and FMNIST. It is worth noting that the GFE-amd trainings are on averageconverged at 1/12th of the number of iterations relative to the AE. For the SegMNIST the networksare fully trained while seeing only the first half (0-4) of the MNIST labels and they are tested withthe second half (5-9) of the labels. The GFE-amd shows a clear advantage over the AE emphasizingthe versatility of a GFE-amd trained neural network.",
  "Conclusions": "To this end, a gradient flow encoding, decoder-only method was investigated. The decoder-dependentgradient flow searches for the optimal latent space representation, which eliminates the need for anapproximate inversion. The full adjoint solution and its approximation are leveraged for trainingand compared in various settings. Furthermore, we present a 2nd order ODE variant to the method,which approximates Nesterovs accelerated gradient descent, with faster convergence per iteration.Additionally, an adaptive solver that prioritizes minimizing loss at each integration step is describedand utilized for comparative tests against the autoencoding model. While each training step takeslonger, the gradient flow encoding converges faster and demonstrates much higher data efficiencythan the autoencoding model. This flow-inspired approach opens up new possibilities for efficient androbust data representations in scientific fields such as physics and materials science, where accuratemodeling and data efficiency are paramount.",
  "T. Dozat. Incorporating Nesterov Momentum into Adam. In Proceedings of the 4th InternationalConference on Learning Representations, pages 14": "K. Flouris and E. Konukoglu. Canonical normalizing flows for manifold learning. In A. Oh,T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in NeuralInformation Processing Systems, volume 36, pages 2729427314. Curran Associates, Inc.,2023. W. Grathwohl, R. T. Q. Chen, J. Bettencourt, I. Sutskever, and D. Duvenaud. Ffjord: Free-formcontinuous dynamics for scalable reversible generative models. International Conference onLearning Representations, 2019.",
  "A.1The adjoint method for the gradient flow": "As described above, after finding, z z() = argz min l(y, D(z, )) the total loss is minimizedwith respect to the model parameters. The dependence of z to creates an additional depend-ence of L() to via z. For simplicity, let us consider the cost of only one sample y, effectivelyl(y, D(z, )). We will compute the total derivative dl(y, D(z, )) for this sample, and the derivat-ive of the total cost for a batch of samples can be computed as the sum of the sample derivatives inthe batch. The total derivative dl(y, D(z, )) is computed as",
  "(t)T zl(z(t), )dt,(10)": "where we used z = z(). Equations define the so called adjoint method for gradient flowoptimization of the loss. Due to the cost of solving all three equations, we empirically find that forthis work sufficient and efficient optimization can be accomplished by ignoring the integral (adjointfunction) part of the method. Theoretically, this is equivalent to ignoring the higher order term ofthe total differential dl(y, D(z, )) = l(y, D(z, )) + zl(y, D(z, ))z l(y, D(z, )).Reducing Equations to:",
  "A.2Fixed grid ODE solver": "Being an ODE, the gradient flow can be solved with general ODE solvers. Unfortunately, genericadaptive step size solvers are not useful because the underlying ODE becomes stiff quickly duringtraining, the step size is reduced to extremely small values and the time it takes to solve gradientflow ODEs at each iteration takes an exorbitant amount of time. Fixed time-step or time grid solverscan be used, despite the stiffness. However, we empirically observed that these schemes can leadto instabilities in the training, see . To demonstrate this, we experimented with a 4th orderRunge-Kutta method with fixed step size. The t slices are predefined in logarithmic series such as tis smaller closer to t = 0, where integrands, zl(y, D(z, )), are more rapidly changing. Similarly, is empirically set to e(2t/) to facilitate faster convergence of z. For the GFE full adjoint method,the integrands for each time slice are saved during the forward pass so they can be used for thecalculation of the adjoin variable in the backward pass. We used the same strategy for both basicgradient flow and the 2nd order model.",
  "BExperiments on the proposed methods": "For training with MNIST and FashionMNIST datasets, we implement a sequential linear neuralnetwork. The decoder network architecture corresponds to four gradually increasing linear layers,with ELU non-linearities in-between. The exact reverse is used for the encoder of the AE. The network training is carried out with a momentum gradient decent optimiser (RMSprop), learningrate 0.0005, = 1 106 and = 0.9. The GFE and AE are considered trained after one epochand twelve epochs respectively. Initially a relative comparison between the full adjoint and the approximate fixed grid GFE methodsis carried out to assess the relevance of the higher order term. Specifically, we carry out MNISTexperiments for a fixed network random seed, where we trained the Decoder using the different GFEmethods and computed cross entropy loss over the validation set. The proper adjoint solution requiresEquations 8 and 9 to be solved for each slice of the integral in Eq. 10. Given N time-slices (forsufficient accuracy N 100), this requires O(5N) calls to the model D for each training image.The approximate method as in Equations 11 and 12 requires only O(N) passes. From (left) itis evident that the 5-fold increase in computational time is not cost-effective as the relative reductionin loss convergence per iteration is not significant. Furthermore, to increase convergence with respect to training data, the accelerated gradient flow2nd order GFE is implemented in .1. From (right), the accelerated gradient methodincreases initially the convergence per iteration relative to GFE, nevertheless it is slightly morecomputationally expensive due to solving a coupled system. Additionally, from the same Fig. certainstability issues are observed for both GFE and second order GFE methods later on despite the initialefficient learning. In order to guarantee stability, the GFE-amd method is implemented as explainedin .2. The black curve in (right) shows a clear improvement of the GFE-amd over thelater methods. Importantly, this result is robust to O of the experiment.",
  "CFurther results": "Sample test-set reconstructions with a fixed network random seed for GFE-amd and AE methods areshown in . From (a) it is evident that the GFE-amd is superior in producing accuratereconstructions with the very limited amount of data. (b) indicates that both GFE-amd and AEgenerate similar reconstructions when properly trained. iterations cross entropy loss GFE-approximateGFE-full adjoint iterations 0.50 0.75 1.00 1.25 1.50 1.75 2.00 cross entropy loss GFE-amdGFEGFE-2nd order : Left Validation mean cross-entropy loss plotted against MNIST training iterations forthe approximate and full adjoint GFE methods. The full adjoint has a slight advantage over theapproximate. Right Validation mean cross-entropy loss plotted against MNIST training iterations forthe GFE, 2nd order GFE and GFE-amd methods. The GFE-amd is both more stable and approachesa better convergence relative to the other methods : (a) Test-set reconstructions for trained GFE-amd (left) and AE (right) that only see 1% ofMNIST (top) and FashionMNIST (bottom) training images. (b) Test-set reconstructions for fullytrained GFE-amd (left) and AE (right) with MNIST (top) and FashionMNIST (bottom) trainingimages. Note: The labels are identical in the respective reconstructions."
}