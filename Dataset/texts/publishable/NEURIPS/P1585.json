{
  "Abstract": "Multimodal learning has developed very fast in recent years. However, during themultimodal training process, the model tends to rely on only one modality basedon which it could learn faster, thus leading to inadequate use of other modalities.Existing methods to balance the training process always have some limitationson the loss functions, optimizers and the number of modalities and only considermodulating the magnitude of the gradients while ignoring the directions of thegradients. To solve these problems, in this paper, we present a novel method to bal-ance multimodal learning with Classifier-Guided Gradient Modulation (CGGM),considering both the magnitude and directions of the gradients. We conduct ex-tensive experiments on four multimodal datasets: UPMC-Food 101, CMU-MOSI,IEMOCAP and BraTS 2021, covering classification, regression and segmentationtasks. The results show that CGGM outperforms all the baselines and other state-of-the-art methods consistently, demonstrating its effectiveness and versatility. Ourcode is available at",
  "Introduction": "Humans perceive the world in a multimodal way, such as sight, touch and sound. These multimodalfeatures can provide comprehensive information to help us understand and explore the environ-ment. Recent years have witnessed great success in multimodal learning, such as visual questionanswering , multimodal sentiment analysis and multimodal retrieval . Although multimodal learning has made significant progress in recent years, inadequate use ofdifferent modality information during training remains a challenge. Theoretically, for example, Wuet al. put forward the greedy learner hypothesis which states that a multimodal model learns torely on one of the input modalities, based on which it could learn faster, and does not continue tolearn to use other modalities. Huang et al. find that during joint training, multiple modalitieswill compete with each other and some modalities will fail in the competition. Experimentally, onsome multimodal datasets, there is little improvement in accuracy between training with only onemodality and training with all modalities . These theoretical analyses and experimental resultsdemonstrate the inefficiency of multimodal learning to fully utilize and integrate information fromdifferent modalities. To deal with this problem, recent studies investigate the training process ofmultimodal learning and propose gradient modulation strategies to better integrate the informationof different modalities and balance the training process in some situations. However, all of thesemethods can not be applied easily for some limitations. For example, Wu et al. , Peng et al., Li et al. and Hua et al. propose balancing methods based on cross-entropy loss for",
  "arXiv:2411.01409v1 [cs.LG] 3 Nov 2024": "classification tasks. For regression tasks or other tasks, we can not use these strategies. Besides, mostof these methods can just deal with situations where there are only two modalities. For example, Wuet al. propose the conditional learning speed which is difficult to calculate and employ if thereare more than two modalities. For situations where there are more modalities, these methods cannot be applied. Furthermore, most of these methods only consider modulating the magnitude of thegradients while ignoring the directions of the gradients. Based on the above observations, in this paper, we propose a novel method to balance multimodallearning with Classifier-Guided Gradient Modulation (CGGM). In CGGM, we consider a moregeneral situation with no limitations on the type of tasks, optimizers, the number of modalities,etc. Additionally, we consider both the magnitude and directions of the gradients to fully boost thetraining process of multimodal learning. Specifically, we add classifiers to evaluate the utilizationrate of each modality and obtain the unimodal gradients. Then, we leverage the utilization rate toadaptively modulate the magnitude of the gradients of encoders and use the unimodal gradients toinstruct the model to optimize towards a better direction. We conduct extensive experiments on four multimodal datasets: UPMC-Food 101 , CMU-MOSI , IEMOCAP , and BraTS 2021 . UPMC-Food 101 and IEMOCAP are classificationtasks, CMU-MOSI is a regression task, and BraTS 2021 is a segmentation task. CGGM outperformsall the baselines and other state-of-the-art methods, demonstrating its effectiveness and universality.In summary, our contributions are as follows:",
  "Related Work": "Multimodal Learning. One of the main challenges of multimodal learning is how to effectivelyutilize and integrate the information from different modalities to complement each other. Accordingto the fusion strategies, there are three main multimodal fusion strategies: early fusion, intermediatefusion and late fusion. In early fusion methods , raw data from different modalities iscombined via concatenation or other methods at the input level before being fed into a model.Intermediate fusion methods combine data from different modalities at various intermediateprocessing stages within a model architecture. Late fusion methods process data from eachmodality independently through separate models and combine them at a later stage. In general, latefusion is the predominant method used in multimodal learning. The main reason is that thearchitecture of each unimodal stream has been carefully designed over the years to achieve state-of-the-art performance for each modality. Therefore, we can leverage these pre-trained models toachieve better results. Therefore, in this paper, our method is based on late fusion. These fusion strategies are able to integrate information from different modalities effectively, but theyhave limited improvements to utilize information from different modalities to complement each other.In other words, they are not able to deal with the modality competition or imbalanced multimodallearning. When the dominant modality is missing or corrupted, the performance would degradesignificantly. Different from these fusion strategies, our method aims to make relatively full use ofthe information of each modality and address the imbalanced multimodal learning. Balanced Multimodal Learning. The inefficiency in fully utilizing and integrating information frommultiple modalities poses a great challenge to the multimodal learning field. Some studies present that there is little improvement in accuracy between training with only one modality andtraining with all modalities. Wang et al. show that multimodal models using multiple modalitiescan be even inferior to those using only one modality. To balance the multimodal learning processand fully utilize different modalities, a series of balanced multimodal learning methods are proposed. Wu et al. propose the conditional learning speed to capturethe relative learning speed between modalities and balance the learning process. Peng et al.",
  "TaskLoss": ": The overall architecture of CGGM. During the training stage, classifiers are introduced tocalculate the directions of unimodal gradients and evaluation metrics. During the inference stage, theclassifiers are discarded. propose a gradient modulation strategy that adaptively controls the optimization of each modalityvia monitoring the discrepancy of their contribution towards the learning objective. More recently,Fan et al. propose the prototypical modal rebalance strategy to introduce different learningstrategies for different modalities. Li et al. propose an adaptive gradient modulation methodthat can boost the performance of multimodal models with various fusion strategies. Hua et al. dynamically adjust the learning objective with a reconcilement regularization against competitionwith the historical models. However, all of these previous works have certain limitations and can only be used in some specificsituations. For example, Wu et al. propose conditional learning speed based on intermediatefusion strategy which makes it hard to apply to situations where there are more than two modalitiesor where the network is not based on intermediate fusion. Peng et al. , Fan et al. , Fu et al., Li et al. and Hua et al. propose the balancing strategies with the assumption of thecross-entropy loss function mainly for classification. Particularly, Peng et al. employ the SGDoptimizer. Different from these methods, we consider a more general situation with no limitations onthe number of modalities, the optimizer, the loss function and so on. Additionally, most of existingmethods only consider the magnitude of the gradients and ignore the directions of the gradients. Incontrast, we consider both of them.",
  "Suppose there are M modalities, referred to as m1, m2, , mM. We denote the multimodal datasetas D = {(xi, yi)}Ni=1, where N is the number of data in the dataset and xi = (xm1i, xm2i, , xmMi)": "We consider the most common structure () in multimodal models, where the inputs ofdifferent modalities are first fed into modality-specific encoders and then the representations of allmodalities are inputted into a fusion module. We denote the encoder of modality mi as i wherei = 1, 2, , M and the fusion module as .",
  "hi = i(xmi)(1)": "where hi is the representation of modality mi. After obtaining the representations of all modalities,the fusion module is applied:y = F(([h1, h2, , hM]))(2)where y is the prediction, is the concatenation operation, and F() is the prediction head topredict the answer. () fuses the multimodal representations and outputs the fused feature as theprediction token.",
  "(c)": ": (a) Accuracy of each modality and the fusion. (b) Gradient magnitude of each modality.We use the Euclidean norm of the gradient vector to represent the gradient magnitude. (c) Gradientdirection between each modality and their fusion. We use cosine similarity to represent the directionbetween two gradient vectors. We get all the results on the CMU-MOSI dataset.",
  "To introduce CGGM, we first analyze the gradient updating process. We denote the loss function asL() = 1": "NNi=1 (yi, yi) where represents the parameters of the network, yi is the prediction andyi is the ground truth. For simplicity, we use yi to represent the predictions in the following context.Different from previous methods which only consider cross-entropy loss , our L can becross-entropy loss, L1 loss or any other loss functions. With the Gradient Descent (GD) optimizationmethod, the parameters of the fusion module and encoders i can be updated as:",
  "F(4)": "where is the learning rate, N is batch size, and t is the iteration. According to the chain rule usedto find the gradient in backpropagation, the update of i will influence the update of , and viceversa. According to (a) and 2(b), the gradient and the accuracy of the dominant modalitywill increase during the training process while the other two remain stable. Particularly, the gradientmagnitude of the text modality increases very fast during the training process. This suggests theencoder of the dominant modality will be updated much faster than others, which makes muchlarger. This phenomenon can also be validated by previous works . Besides, in (c),we present the gradient direction between each modality and the fusion. We can observe that thesimilarity between audio modality and the multimodal fusion is less than 0, indicating that theyoptimize towards the opposite direction, thus hindering the gradient update for multimodal branch.Meanwhile, the similarity between text modality and the multimodal fusion is increasing, suggestingthe optimization direction towards the dominant modality. With the progress of optimization, theencoder of the dominant modality can make relatively accurate predictions, which makes the fusionmodule only depend on this modality (both magnitude and direction as mentioned above), leavingother encoders under-optimized.",
  "Gradient Magnitude Modulation": "As we discuss in .2, the gradient magnitude of the dominant modality increases fast duringthe training while the other modalities remain stable, thus being under-optimized. To balance thetraining process and make the fusion module benefit from all the encoders simultaneously, we proposethe classifier-guided gradient modulation. Specifically, we use a modality-specific classifier to makepredictions of hi in Equation 1. We can write the process as:",
  "where fi is the classifier of modality mi and ymi is the prediction only using modality mi. Theclassifier fi consists of 1-2 multi-head self-attention (MSA) layers and a fully connected layer": "for classification and regression tasks. And for segmentation tasks, fi is a light decoder. After hi isinputted into the fusion module , it becomes a more high-level representation. Therefore, we useseveral MSA layers to make hi more consistent with the output of the fusion module. For a specific task, we have some evaluation metrics such as accuracy and mean absolute error.Here, we choose one of the evaluation metrics (e.g. accuracy for classification tasks and meanabsolute error for regression tasks) and denote it as . For every iteration of training, we can getpredictions from the classifiers. We denote the predictions as yi = (yim1, yim2, , yimM ) wherei is the current iteration. Furthermore, we evaluate the task using yi to get the evaluation metrici = (im1, im2, , imM ). Here, we use the difference between the two consecutive to denotethe modality-specific improvement for each iteration:",
  "= (t+1m1 tm1, t+1m2 tm2, , t+1mM tmM )(6)": "where t = 0, 1, 2, , T and T is the total iterations of training. Particularly, 0 is initialized to 0.In some multimodal datasets, only using one of the modalities can achieve good results so we can notdirectly use to measure the utilization rate of different modalities. Therefore, it is reasonable to usethe difference between to denote the relative improvements for each iteration. Then, we define thegradient magnitude balancing term of modality mi for the t-th iteration as follows:",
  "Mk=1,k=i tmkMk=1 tmk(7)": "where is a scaling hyperparameter and M is the number of modalities. According to Equation 7, itis easy to find that when the performance of the model only using modality mi improves very fast(i.e. tmi is large), Btmi will be small. Similarly, when the modality mi brings relatively limitedimprovements to the model (i.e. tmi is small), Btmi will be large. Therefore, Btmi is able to measurethe relative utilization rate of these modalities and we can use Btmi to modulate the magnitude of thegradient of the encoder i. So Equation 4 can be rewritten as:",
  "Gradient Direction Modulation": "As Wu et al. discover, when the model only depends on one modality to perform well, it doesnot continue to learn to use other modalities. As discussed in .2, it means that this modalitydominates the updating of the model. Previous works address this problem mainly byfocusing on gradient magnitude modulation. However, in .2, we find that the model isoptimized towards the dominant modality. Therefore, in this subsection, we introduce a method thatcould modulate the direction of the gradients to balance the training process. In general, we want to balance the optimization direction of the model when the model only relieson one modality to make predictions. Therefore, we propose to enforce the gradient direction ofthe model as close as possible to the weighted average gradient direction of models only using onemodality. We use Btmi in Equation 7 as the weight term. This ensures that when the model tendsto optimize towards the dominant modality, Btmi can help the model use information from othermodalities. Besides, since Btmi changes during the training process, this term can make a dynamicadjustment to balance the optimization directions. Concretely, we can feed one modality into themodel and drop other modalities by replacing them with 0 or other fixed values during training tocalculate the gradient of this modality. By this method, we can calculate the unimodal gradientsfor all modalities. Then, we just enforce the gradient direction of the model as close as possible tothe weighted average of these unimodal gradient directions. However, this method is very complex",
  "during training, because in every iteration we need to drop modalities to calculate the unimodalgradients, which is time-consuming with the increase in the number of modalities": "Therefore, we propose to use the gradients of the classifiers fi to represent the unimodal gradients.We will later demonstrate they are similar (.4 and ). Here, we take the gradientof regression tasks as an example where the output dimension is 1 so the gradient is an n-d vector.For classification tasks or other tasks where the gradient is a matrix, see Appendix A for details.Concretely, we can calculate the gradient of the classifier fi as:",
  "(10)": "We regard fi L, i = 1, 2, , M as the unimodal gradients and F L as the fusion gradients. Asmentioned before, we want to make F L as close as possible to the weighted average directionof fi L, i = 1, 2, , M. Let sim(u, v) = uv/||u||||v|| denote the dot product between 2normalized u and v (i.e. cosine similarity). We can enforce the gradient direction of the fusion moduleas close as possible to the weighted average of these unimodal gradient directions by maximizingtheir cosine similarity:",
  "i=1|Btmi| BtmisimFt L, fit L(12)": "The cosine similarity is a number between 1 and 1. By adding |Btmi| to the loss term, we can ensurethat the loss Lgm is always positive. As aforementioned, when modality mi has limited improvement,Btmi is large. Therefore, the corresponding term in Ltgm will be large, making the optimizationdirection towards modality mi, which will balance the learning process.",
  "presents the difference between these fourdatasets": "UPMC-Food 101 is a food classification dataset, which contains about 100,000 recipes fora total of 101 food categories. Each item in the dataset is represented by one image plus textualinformation. We use accuracy and F1 score to evaluate the performance of the model. CMU-MOSI is a popular dataset for multimodal (audio, text and video) sentiment analysis.Each video segment is manually annotated with a sentiment score ranging from strongly negativeto strongly positive (-3 to +3). Following previous work , we use binary accuracy (ACC-2),F1 score, 7-class accuracy (ACC-7), mean absolute error (MAE) and pearson correlation (Corr) toevaluate the performance of the model.",
  "CGGM76.9472.7572.1473.94": "IEMOCAP is a multimodal emotion recognition dataset, which contains recorded videos fromten actors in five dyadic conversation sessions. Following previous works , four emotions(happiness, anger, sadness and neutral state) are selected for emotion recognition. We use accuracyand F1 score to evaluate the performance of the model. BraTS 2021 is a 3D multimodal brain tumor segmentation dataset, which has four modalities:flair, t1ce, t1 and t2. The input image size is 240 240 155. The annotations are combined intothree nested subregions: Whole Tumor (WT), Tumor Core (TC), and Enhancing Tumor (ET). We useDice score of these three nested subregions and their average value to evaluate the performance.",
  "Implementation Details": "Input. For UPMC-Food 101, we use extracted features as inputs. Specifically, we use the pre-trainedbert-base-uncased model to extract text features and use pre-trained ViT on ImageNet toextract image features. For CMU-MOSI and IEMOCAP, we follow Guo et al. to extract acoustic,visual and textual features. For BraTS 2021, we use the preprocessed raw images as inputs. Backbone. For UPMC-Food 101, CMU-MOSI and IEMOCAP, we use transformer encoders asmodality encoders and the fusion module. For the BraTS 2021 dataset, we use DeepLab v3+ asthe encoders and several convolution layers as the fusion module. Training Details. For images in UPMC-Food 101 and BraTS 2021, we implement data augmentationstrategies, including random cropping, random flipping, color jitter, adding noise, etc. To savememory, we consider BraTS 2021 as a 2D segmentation task by randomly slicing an image fromthe 3D image. For CMU-MOSI, we use L1 loss as our loss function. For UPMC-Food 101 andIEMOCAP, we use cross-entropy loss. For BraTS 2021, we use the combination of soft dice loss andcross-entropy loss. Besides, we use the Adam optimizer for CMU-MOSI, the AdamW optimizer forUPMC-Food 101 and IEMOCAP, and the SGD optimizer for BraTS 2021. Other hyperparametersare described in Appendix B in detail.",
  "Main Results": "Comparison with the state-of-the-arts. We compare our CGGM with other methods to demonstratethe effectiveness of our proposed method. For these four datasets, we compare CGGM with the modeltraining only using one modality, multimodal joint training (Baseline), Modality Random Dropout(MRD), and Modality-specific Learning Rate (MSLR) methods. Additionally, we compare CGGMwith SOTA methods including G-Blending , Greedy , OGM , AGM , PMR ,UMT , UME , QMF and ReconBoost . , 3 and 4 present the results of image (fusion)image (classifier)text (fusion)text (classifier)",
  "Classifier Performance and Gradient Direction": "Classifier performance. In , we present the accuracy of classifiers in different situations.Unimodal training can be considered a baseline that fully utilizes the unimodal information. Comparedwith unimodal training, the accuracies of f1 and f2 in multimodal training drop slightly while theaccuracy of f3 increases slightly. This demonstrates that multimodal training can not fully utilize theinformation from audio and video, indicating that they are under-optimized. The improvement of f3indicates that the text modality is fully exploited and learns some information from the other twomodalities. In contrast, the accuracies of the three classifiers in CGGM all improve. This suggeststhat during the balancing process, the fusion can fully utilize the information from all the modalities,thus in turn making the encoders of three modalities fuse the information from other modalitiesduring backpropagation. Therefore, the accuracies of all the three classifiers improve correspondingly.This also validates the effectiveness of CGGM. Classifier gradient direction. In .3.2, we propose to use the gradients of classifiers torepresent the unimodal gradients. In this subsection, we give a visualization of the gradients ofclassifiers and the unimodal gradients. Specifically, for every batch of data, we input them into",
  ": The improved performance with different and compared to the joint training baseline": "the model to get representations hi which are then fed into the classifiers fi to get the gradients ofclassifiers. Then we input hi of only one modality into the fusion module to get the unimodalgradients. We use t-SNE to visualize the gradient vectors. shows the visualizationresults on the four datasets. As shown in the figure, for each modality, the unimodal gradient vectorsand the gradient vectors of the corresponding classifier are very close to each other, demonstratingthat it is meaningful to use the gradients of the classifiers to represent the unimodal gradients.",
  "Ablation Study": "Gradient magnitude and direction. To measure the contribution of gradient magnitude modulationand gradient direction modulation separately, we present our ablation results on IEMOCAP in. Compared with the baseline in the first row, modulating the magnitude of the gradientsbrings more improvements to the performance of the model than modulating the direction of thegradients. Compared with the CGGM performance in , the combination of modulatinggradient magnitude and gradient directions furthermore enhances the performance of the model.",
  "Baseline70.7469.53CGGM ( = 1.0, = 0)72.3571.56CGGM ( = None, = 0.1)72.4172.07CGGM ( = 1.0, = 0.1)73.7473.18": "Scaling hyperparameter . To explore theimpacts of the scaling hyperparameter onthe models performance, we select sevendifferent values of and present our resultson IEMOCAP in (a). We discoverthat the accuracy improves with the increaseof before hitting the highest value when = 1.2. Then, the accuracy drops withthe increase of . Compared to the baseline,modulating the magnitude of the gradientsbrings consistent improvements regardless of how big is taken. Intuitively, the larger the , thelarger the modification to the gradients. Therefore, the results in the table indicate that we need tocarefully choose a to avoid modifications that are too large or too small. Loss trade-off . measures the strength we modulate the directions of the gradients. We select sixdifferent values of and present the results on IEMOCAP in (b). As shown in the figure,when is 0.01 or 0.25, the accuracy will decrease slightly. When is too small, the modulation isinsufficient and could influence the optimization process. When is too large, the modulation islarge and will influence the task loss, thus making optimization deviate from the task.",
  "Conclusion": "In this paper, we propose CGGM, a novel strategy to balance the multimodal training process.Compared to existing gradient modulation methods, CGGM has no limitations on the loss functions,the optimizer, the number of modalities, etc. Moreover, we consider both the magnitude anddirection of the gradients with the guidance of the classifiers. Extensive experiments and ablationstudies fully demonstrate the effectiveness and universality of CGGM. However, CGGM also has alimitation. CGGM needs to leverage extra classifiers to implement gradient modulation. Althoughthese classifiers are lightweight, they still lead to more computational resources. We lead thischallenging problem to future work.",
  "This work was supported by National Key R&D Program of China under Grant No.2022ZD0162000": "Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel Bilello, Evan Calabrese, Errol Colak,Keyvan Farahani, Jayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak Pati, et al. The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification.arXiv preprint arXiv:2107.02314, 2021. Hedi Ben-Younes, Rmi Cadene, Matthieu Cord, and Nicolas Thome. Mutan: Multimodaltucker fusion for visual question answering. In Proceedings of the IEEE international conferenceon computer vision, pages 26122620, 2017. Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Ebrahim (Abe) Kazemzadeh, Emily MowerProvost, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. Iemocap:interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42:335359, 2008. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam.Encoder-decoder with atrous separable convolution for semantic image segmentation. InProceedings of the European conference on computer vision (ECCV), pages 801818, 2018.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,2018": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintarXiv:2010.11929, 2020. Chenzhuang Du, Jiaye Teng, Tingle Li, Yichen Liu, Tianyuan Yuan, Yue Wang, Yang Yuan, andHang Zhao. On uni-modal feature learning in supervised multi-modal learning. In InternationalConference on Machine Learning, pages 86328656. PMLR, 2023. Yunfeng Fan, Wenchao Xu, Haozhao Wang, Junxiao Wang, and Song Guo. Pmr: Prototypicalmodal rebalance for multimodal learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 2002920038, 2023. Jie Fu, Junyu Gao, Bing-Kun Bao, and Changsheng Xu. Multimodal imbalance-aware gradientmodulation for weakly-supervised audio-visual video parsing. IEEE Transactions on Circuitsand Systems for Video Technology, 2023. Zirun Guo, Tao Jin, and Zhou Zhao. Multimodal prompt learning with missing modalities forsentiment analysis and emotion recognition. In Proceedings of the 62nd Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pages 17261736, 2024.",
  "Tao Jin, Weicai Yan, Ye Wang, Sihang Cai, Shuaiqifan, and Zhou Zhao. Calibrating promptfrom history for continual vision-language retrieval and grounding. In ACM Multimedia 2024,2024": "Hamid Reza Vaezi Joze, Amirreza Shaban, Michael L Iuzzolino, and Kazuhito Koishida. Mmtm:Multimodal transfer module for cnn fusion. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 1328913299, 2020. Hong Li, Xingyu Li, Pengbo Hu, Yinuo Lei, Chunxiao Li, and Yi Zhou. Boosting multi-modalmodel performance with adaptive gradient modulation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 2221422224, 2023. Paul Pu Liang, Ziyin Liu, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Multimodallanguage analysis with recurrent multistage fusion.In Ellen Riloff, David Chiang, JuliaHockenmaier, and Junichi Tsujii, editors, Proceedings of the 2018 Conference on Empiri-cal Methods in Natural Language Processing, pages 150161, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1014. URL Xiaokang Peng, Yake Wei, Andong Deng, Dong Wang, and Di Hu. Balanced multimodallearning via on-the-fly gradient modulation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 82388247, 2022. Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, andRuslan Salakhutdinov. Multimodal transformer for unaligned multimodal language sequences.In Anna Korhonen, David Traum, and Llus Mrquez, editors, Proceedings of the 57th AnnualMeeting of the Association for Computational Linguistics, pages 65586569, Florence, Italy,July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1656. URL",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of MachineLearning Research, 9(86):25792605, 2008": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informationprocessing systems, 30, 2017. Valentin Vielzeuf, Alexis Lechervy, Stphane Pateux, and Frdric Jurie. Centralnet: a multi-layer approach for multimodal fusion. In Proceedings of the European Conference on ComputerVision (ECCV) Workshops, pages 00, 2018. Weiyao Wang, Du Tran, and Matt Feiszli. What makes training multi-modal classificationnetworks hard? In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1269512705, 2020. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frdric Precioso. Reciperecognition with large multimodal food dataset. In 2015 IEEE International Conference onMultimedia Expo Workshops (ICMEW), pages 16, 2015. doi: 10.1109/ICMEW.2015.7169757. Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency.Words can shift: Dynamically adjusting word representations using nonverbal behaviors. InProceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 72167223,2019. Nan Wu, Stanislaw Jastrzebski, Kyunghyun Cho, and Krzysztof J Geras. Characterizing andovercoming the greedy nature of learning in multi-modal deep neural networks. In InternationalConference on Machine Learning, pages 2404324055. PMLR, 2022.",
  "Weicai Yan, Ye Wang, Wang Lin, Zirun Guo, Zhou Zhao, and Tao Jin. Low-rank promptinteraction for continual vision-language retrieval. In ACM Multimedia 2024, 2024": "Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Multimodal sentimentintensity analysis in videos: Facial gestures and verbal messages. IEEE Intelligent Systems, 31(6):8288, 2016. doi: 10.1109/MIS.2016.94. Qingyang Zhang, Haitao Wu, Changqing Zhang, Qinghua Hu, Huazhu Fu, Joey Tianyi Zhou,and Xi Peng. Provable dynamic fusion for low-quality multimodal data. In Internationalconference on machine learning, pages 4175341769. PMLR, 2023.",
  "AGradient Direction Modulation Details": "For classification tasks, the classifier fi outputs more than one value. For example, for the UPMC-Food 101 dataset, fi output 101 values for each piece of data. Therefore, we can define fi asfi = (f (1)i, f (2)i, , f (m)i) where m is the number of output. We calculate the gradients of theclassifiers fi as:",
  "presents the main hyperparameters of the four datasets. Apart from the hyperparameters inthe table, there are some task-specific hyperparameters": "For BraTS 2021, the start learning rate is set to 4e-4 with warm-up epochs to 1e-2 and the finallearning rate is 1e-3. Besides, for the loss function, we use the combination of soft dice loss andcross-entropy loss, which can be represented as Ltask = LDice+1LCE. We set 1 to 1. Particularly,we use a weighted cross-entropy loss function, where the weight is 0.2, 0.3, 0.25 and 0.25 for thebackground, label 1, label 2 and label 3, respectively.",
  "CMore Ablation Study": "More visualizations of CGGM. We further visualize the loss changes in . From the figure,we can observe that the loss of the dominant modality with CGGM implemented in (b)-(d) will dropmuch slower than that in (a). Besides, the losses of all modalities in (b)-(d) are smaller thanthose in (a), indicating the effectiveness of CGGM. Apart from the loss changes, we also visualizethe changes in balancing term during the training process in . When the value is higherthan the red line, the modality is promoted. When the value is lower than the red line, the modalityis suppressed. In the first few iterations, the dominant modality is suppressed, ensuring that othermodalities are fully optimized. During the optimization, balancing terms of three modalities turn upand down, ensuring each modality is sufficiently optimized. Additional computational resources of classifiers. The additional classifiers will need morecomputational resources during training. However, during inference, the classifiers will be discarded.Therefore, they have no impact during the inference stage. We report the additional memory cost(MB) of the additional classifiers in . From the table, we can observe that the additional",
  "With classifiers+8MB+8MB+8MB+24MB": "computational increase is low. There are two main reasons: (1) the classifiers or decoders are lightwith only a few parameters; (2) the classifiers only use the gradients to update themselves and do notpass the gradients to the modality encoders during backpropagation. Therefore, there is no need tostore the gradient for each parameter, thus reducing memory cost."
}