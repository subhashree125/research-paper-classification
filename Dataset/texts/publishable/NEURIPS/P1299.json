{
  "Abstract": "Deep Neural Networks (DNNs) have revolutionized artificial intelligence, achievingimpressive results on diverse data types, including images, videos, and texts.However, DNNs still lag behind Gradient Boosting Decision Trees (GBDT) ontabular data, a format extensively utilized across various domains. In this paper,we propose DOFEN, short for Deep Oblivious Forest ENsemble, a novel DNNarchitecture inspired by oblivious decision trees. DOFEN constructs relaxedoblivious decision trees (rODTs) by randomly combining conditions for eachcolumn and further enhances performance with a two-level rODT forest ensemblingprocess. By employing this approach, DOFEN achieves state-of-the-art resultsamong DNNs and further narrows the gap between DNNs and tree-based models onthe well-recognized benchmark: Tabular Benchmark , which includes 73 totaldatasets spanning a wide array of domains. The code of DOFEN is available at:",
  "Introduction": "Tabular data is extensively used across various domains (e.g., finance, healthcare, government). Forprediction tasks involving tabular data, tree-based models such as CatBoost and XGBoost arecurrently considered the state of the art . Given the success of deep neural networks in otherdomains (e.g., natural language processing, computer vision), it is compelling to explore how neuralnetworks can be leveraged to achieve improved performance on tabular data, potentially benefitingother research directions in this area (e.g., multimodal learning, self-supervised learning). To emulate the behavior of tree-based models using deep neural networks, we observed two keypoints. First, the base models for tree-based approaches (i.e., decision trees or oblivious decisiontrees) may exhibit crucial inductive biases that contribute to accurate predictions on tabular data.Second, the ensemble of base models significantly enhances predictive performance. For instance,bagging trees employ bootstrap sampling and bagging , while boosting trees utilize variousforms of gradient boosting . In this paper, we propose a deep neural network, named Deep Oblivious Forest ENsemble (DOFEN),which incorporates the two key observations mentioned earlier. First, we select the oblivious decisiontree (ODT) as the base model, as it represents a decision table and is easier to model(). For example, the following set shows the columns with their decisive conditions from a",
  "(d) Large, Regression": ": Evaluation results on the Tabular Benchmark. The model names are sorted by theirperformances at the end of the random search of hyperparameters. The result are averaged overvarious datasets included in each benchmark respectively, detailed number of datasets of eachbenchmark is provided in Appendix B.1 trained ODT:{ColB < 7.8, ColA > 5, ColC = cat}A key characteristic of an ODT is its disregard for the decision-making sequence, allowing us tofocus on condition selection. Consequently, the first step in DOFEN is to randomly select columnconditions from a set of generated conditions for each column. This random selection process isrepeated multiple times to derive numerous combinations of conditions. This step primarily aimsto substitute the learning process of an ODT by iterating over as many condition combinations aspossible. Next, we randomly select several condition combinations, termed relaxed oblivious decisiontrees (rODTs) in the following context, and assemble them into an rODT forest. This step is alsorepeated multiple times, and the final predictions are made by bagging through the predictions ofthese rODT forests.",
  "Our main contributions are as follows:": "1. Innovative Neural Network Architecture. DOFEN introduces a novel deep neural networkarchitecture designed to address tabular data problems (). To harness the strengthsof tree-based models in this domain, DOFEN integrates oblivious decision trees into thenetwork architecture through a random condition selection process, leading to the formationof relaxed oblivious decision trees (rODTs). To further exploit the power of ensembling,DOFEN aggregates rODTs from the previously generated rODT pool to construct an rODTforest, and makes predictions by bagging the outputs from these rODT forests. 2. State-of-the-Art Performance. To comprehensively and objectively evaluate DOFEN, weselected the recent and well-recognized Tabular Benchmark . This benchmark addressesthe common issue of inconsistent dataset selection in deep learning research on tabular databy incorporating a variety of regression and classification datasets with standardized featureprocessing. The experimental results show that DOFEN outperforms other neural networkmodels and competes closely with GBDTs on the Tabular Benchmark, highlighting its versa-tility across different tasks, as demonstrated in . Additionally, we conducted detailedanalyses on DOFENs unique features, providing deeper insights into its functionalities.Both results can be found in .",
  "Background: Oblivious Decision Tree": "Let D = {(xi, yi)}Ni=1 be a dataset of size N and F = {fj}Ncolj=1 be its feature set of Ncol features.Namely, xi = (xi1, . . . , xiNcol) and yi are the feature vector and label, respectively, of the i-thsample, where xij is the value corresponding to feature fj. Note that yi R for regression problemsand yi {0, . . . , g} for classification problems with g being the number of classes. In a trained ODT, each layer can be represented by three components: a feature of x, a threshold forthat feature, and a condition based on whether a value is greater than or less than the threshold. Inthe following context, an ODT with a depth of d consists of d instances of z, v, and c, respectively,corresponding to the three components. Let z = (z1, . . . , zd), where zi x represents a feature of x.Let v = (v1, . . . , vd), where vi dom(zi) is a threshold for zi. Finally, let c = (c1, . . . , cd), where",
  "ci {>, <} is paired with zi and xi to make decisions. Note that we have simplified the notationby assuming only numerical features, and the training algorithm can be found in the original work": "ODT distinguishes itself from conventional decision tree algorithms by restricting each layerto use only one feature. This uniformity simplifies decision-making and improves computationalefficiency through vectorized operations. While this reduces model capacity, recent studies haveshown that ensembling ODTs can improve performance . In , we will futher discusshow we leverage ODT to design a network architecture and introduce a novel ensemble strategy toboost its performance.",
  "DOFEN: Deep Oblivious Forest Ensemble": "In this section, we first explain how DOFEN integrates ODT into the network architecture in.1 followed by introducing a two-level rODT ensemble in .2. For clarity, wesimplify naive sub-networkscomprising only basic neural layers such as linear layers, layernormalization, and dropoutinto symbols (i.e. 1, 2, and 3) in the following figures andequations. Detailed configurations of these sub-networks are provided in Appendix A.2.",
  "This section goes through how DOFEN transforms a raw input into soft conditions and constructsmultiple relaxed ODTs by randomly combining these conditions": "Recall that a trained ODT consists of z, v and c. Since the ODT learning algorithm selectingfeatures and thresholds by minimizing loss (e.g. gini impurity or mean squared error) is non-differentiable, deriving v and c using gradient-based optimization is challenging. We bypass thenon-differentiable issue by: (1) randomly selecting z for an ODT and, (2) replacing v and c witha neural network, which gives a soft score (i.e. condition) measuring how a sample adheres toa decision rule. In practice, we first generate multiple soft conditions for each column and thenrandomly combine several conditions to form an ODT. We called an ODT constructed by this softenprocedure as a relaxed ODT (rODT), and the two steps as Condition Generation and RelaxedODT Construction, respectively. The detailed process are provided as follows. Condition Generation. This process transforms a raw input xi into soft conditions Mi, as shown inEquation (1). For each feature xij of the raw input xi, where j {1, . . . , Ncol}, Ncond conditions aregenerated by a sub-network 1j. The aggregated conditions are represented by the matrix Mi. Thisdesign mirrors the original ODT, where each condition involves only one feature. As illustrated ina, three instances of 1 generate four conditions per feature, forming a 3 4 matrix.",
  "RNcondNcol, (mij1, . . . , mijNcond) = 1j(xij)(1)": "Relaxed ODT Construction. To build an rODT with depth d, we randomly select d elements fromthe matrix Mi. In practice, to construct multiple rODTs, Mi is shuffled and reshaped into a matrixOi with dimensions NrODT d, as shown in Equation (2). Here, we use to represent a bijectivefunction that maps the index of each element in Mi to a unique position in Oi (i.e. permutation).The whole process is also illustrated in b. In practice, we ensure NrODT = NcondNcol/d and Ncond = md hold, where m and d are hyper-parameters to define model capacity, in order to make the reshaping possible. To ensure the stabilityduring training process, this random combination is done only once during model construction. Notethat each row in Oi represents an rODT, which is crucial for subsequent operations. paired paired paired paired (a) Condition Generation(b) Relaxed ODT Construction(c) Relaxed ODT Forest Construction paired paired Permutation with and reshape : (a) Condition Generation: For each column, Ncond conditions are generated through anindividual sub-network 1. The aggregate of the conditions of all columns is denoted by the matrixMi. (b) Relaxed ODT Construction: Perform permutation on Mi with a bijective function andreshape Mi into Oi, a matrix representing NrODT rODTs with depth d. (c) Relaxed ODT ForestConstruction: To compute the weights wij, an individual sub-networks 2 is applied to each rODT.In addition, each wij is paired with a learnable embedding vector ej. The aggregate of all weightsand their corresponding embedding vectors are denoted as wi and E, respectively.",
  "This section integrates rODTs to construct rODT forests, then applies bagging to ensemble thepredictions of the rODT forests for the final output": "First level: Relaxed ODT Forest Construction. An rODT forest is constructed by aggregatingrandomly selected rODTs in Oi. Specifically, Nestimator rODTs are chosen to form an rODT forest,where Nestimator < NrODT. To aggregate them, we first use 2j, where j {1, . . . , NrODT}, tocompute the weight wi for each rODT, as shown in Equation (3). Additionally, each rODT is pairedwith an embedding ej, as shown in Equation (4). The aggregate of all weights and their correspondingembedding vectors are denoted as wi and E, respectively. This procedure is illustrated in c.",
  "To further construct an rODT forest, Nestimator of paired weights and embeddings are sampled fromwi and E. This process is graphically represented in a and described in line 3 to 7 of the": "average sum sample instances of (, ) without replacement and repeat times a shared (a) Relaxed ODT Forest Construction(b) Bagging of Relaxed ODT Forest = softmax operation = weighted sum operation : (a) Relaxed ODT Forest Construction: First, Nestimator pairs of (wij,ej) are randomlysampled to form wi and E. Secondly, wi is transformed through a softmax function, and is usedfor computing the weighted sum of E to form forest embedding fi. (b) Baggging of Relaxed ODTForest: a shared-weight sub-network 3 is employed to make a prediction yi for each embedding.The final prediction is the average of all yi values, and the total loss is the sum of their individuallosses. pseudo-code for the two-level ensemble (Algorithm 1). The weights are processed through a softmaxfunction and the weighted sum of embeddings forms the embedding vector fi for an rODT forest.The magnitude of these softmaxed weights indicate the importance of the selected rODTs for makingpredictions. Noted that this process is repeated Nforest times to form Nforest instances of rODT forests.",
  "return (yi, lossi);": "Second Level: Bagging of Relaxed ODT Forest. To make a prediction, DOFEN applies a shared sub-network 3 to the embedding of each rODT forest to make individual predictions. The predictions arethen averaged for a bagging ensemble. The process is detailed in line 1, 8, 10, and 12 in Algorithm 1and is illustrated in b. Notice that the output yi is a scalar for regression tasks and a vectorfor classification tasks. During training, DOFEN updates the model parameters by aggregating the loss from each prediction,as shown in line 9 in Algorithm 1. The loss function L is cross-entropy for classification tasks andmean squared error for regression tasks. This method of bagging over rODT forests promotes the creation of diverse rODT forests duringtraining. Although the randomization may seem chaotic, experimental results demonstrate thatinference variance remains low even with a small Nforest. Moreover, the randomization helps reduceoverfitting. Further details on these observations are provided in Sections 4.3 and 4.5. Lastly, it isworth noting that the randomness of this process is fixed during inference stage for model to outputdeterministic result.",
  "This section presents a comprehensive analysis to demonstrate the effectiveness and the functionalityof DOFEN. The experiments are designed to answer the following research questions (RQ):": "RQ1: How well does DOFEN perform compared to baseline and SOTA models? (.2)RQ2: Which part of the model design contributes the most to DOFENs performance? (.3)RQ3: Is the decision-making process of DOFEN interpretable? (.4)RQ4: Does randomization processes involved in DOFEN cause instability issues? (.5) We also conduct additional analyses of DOFEN, including computational efficiency, scalability,and the impact of individual rODT weights. The results are presented in Appendices C.1 to C.3 ,Appendix D and Appendices F.2 and F.3, respectively.",
  "Experimental Settings": "Datasets. We strictly follow the protocols of the Tabular Benchmark as detailed in its official imple-mentation2. This includes dataset splits, preprocessing methods, hyperparameter search guidelines,and evaluation metrics. For full details, please refer to the original paper . The Tabular Bench-mark categorized datasets into classification and regression, with features being either exclusivelynumerical or a combination of numerical and categorical (heterogeneous). These datasets are furtherclassified according to their sample size: medium-sized or large-sized. The dataset counts fromTabular Benchmark is provided in Appendix B.1, and the detailed datasets used in Tabular Benchmarkis provided in Appendix B.3. Model Selection. For model comparison, Tabular Benchmark includes four tree-based models:RandomForest, GradientBoostingTree , HGBT , and XGBoost; two generic DNN models:MLP and ResNet ; and two tabular DNN models: SAINT and FT-Transformer. To ensure acomprehensive comparison, we also included two additional tree-based models: LightGBM andCatBoost, and three tabular DNN models: NODE, Trompt, and GRANDE. LightGBM and CatBoostare selected due to their widespread use across various domains; NODE and GRANDE both sharesimilar motivation and high-level structure with DOFEN; Trompt represents the current state-of-the-art tabular DNNs when following the origin protocols of the Tabular Benchmark. The defaulthyperparameter configuration of DOFEN and hyperparameter search space of different models arepresented in Appendices A.1 and I.2, and the list of some missing model baselines from TabularBenchmark is provided in Appendix B.2.",
  "Performance Evaluation": "In this section, we evaluate DOFEN on the medium-sized benchmark of the Tabular Benchmarkfor classification and regression tasks separately. The evaluation metrics adhere to the TabularBenchmark protocols, which use accuracy for classification datasets and the R-squared score forregression datasets. We discuss the overall performance in this section and provide comprehensiveresults for each dataset in Appendix G.2. Classification. In a, the models can be grouped into three categories: 1) tree-based modelsalong with three tabular DNN models, DOFEN, Trompt and GRANDE; 2) three other tabular DNNmodels; and 3) the two generic DNN models. Before DOFEN, Trompt was the only DNN model CatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFENDOFEN FTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformerFTTransformer GradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTreeGradientBoostingTree",
  "(d) Heterogeneous,Regression": ": Results on medium-sized classification and regression datasets. The result are averagedover various datasets included in each benchmark respectively, detailed number of datasets of eachbenchmark is provided in Appendix B.1 comparable to tree-based models. DOFEN not only matches but surpasses the performance of mosttree-based models, setting a new benchmark for DNN models in tabular data. In b, DOFENand Trompt are again the only DNN models grouped with tree-based models, although they arepositioned at the bottom of this group. Regression. In c and d, XGBoost, DOFEN, and CatBoost emerge as a distinctcategory of top performers. XGBoost consistently leads in performance after the hyperparametersearch, with DOFEN and CatBoost holding the next two top positions. Notably, DOFEN is the onlydeep learning model that is comparable to tree-based models in these two regression benchmarks. The analysis of allows us to draw several conclusions. DOFEN consistently ranks firstwhen compared to other DNN models. Additionally, DOFEN shows strong competitiveness againsttree-based models, consistently placing within the top three on datasets with numerical features.However, when dealing with heterogeneous features, DOFENs performance slightly declines, thoughit still ranks within the top four. This challenge in managing heterogeneous features is commonamong all DNN models, indicating a potential area for improvement in future tabular DNN models. As for the results on the large-sized benchmark, we discuss the findings in Appendix G.1, whereDOFEN also shows superior performance. Beyond the Tabular Benchmark, we conduct additionalexperiments on datasets used in FT-Transformer and GRANDE paper in Appendices H.1 and H.2,respectively, where DOFEN demonstrates competitive results even when using only default hyperpa-rameter settings.",
  "Ablation Study": "DOFEN contains two key features: (1) randomly combine generated conditions to construct a largepool of rODTs, and (2) introduce a two-level ensemble that randomly aggregates multiple rODTforests. More importantly, both features involved randomization to increase the diversity of rODTsconstructed and the rODT forests for ensemble. To better understand the mechanisms driving DOFENs performance, we conduct experiments toablate the two key features of DOFEN. For the first key feature, we lower the diversity of constructedrODTs by removing the operation in Equation (2) and let Oi = Mi in the following process. In thiscase, neighboring columns are always in the same group during rODT construction (Equation (3)),leading to a less diverse set of condition combinations. For the second key feature, we remove thesecond level bagging ensemble. Alternatively, we use all rODTs from Equation (2) to form a singleforest without the for loop in Algorithm 1. In this setup, the softmax function is directly appliedto weight vector wi, and the weighted sum of corresponding embeddings E is calculated. Thisresults in a single prediction per sample, unlike the Nforest predictions described in Algorithm 1. Theexperimental results are presented in .",
  "1st2nd3rd": "Random Forestodor (15.11%)gill-size (12.37%)gill-color (10.42 %)XGBoostspore-print-color (29.43%)odor (22.71%)cap-color (14.07%)LightGBMspore-print-color (22.08%)gill-color (14.95%)odor (12.96%)CatBoostodor (72.43%)spore-print-color (10.57%)gill-size (2.71%)GradientBoostingTreegill-color (31.08%)spore-print-color (19.89%)odor (17.44%)Tromptodor (24.93%)gill-size (8.13%)gill-color (5.73%)DOFEN (ours)odor (13.15%)spore-print-color (6.84%)gill-size (5.58%) regardless of the type of task or features, with a particularly strong effect on regression tasks. Tofurther investigate the significant drop in performance without forest ensemble sampling, we analyzedmodel performance at various training checkpoints in Appendix F.1. The result shows that thesampling process mitigates overfitting hence increase DOFENs testing performance.",
  "Interpretability": "This section aims to demonstrate the interpretability of DOFEN. Specifically, we adopt a featureimportance metric akin to the \"split\" or \"weight\" importance used in LightGBM and XGBoost, whichcounts how often a feature is used in the model. To calculate DOFENs feature importance of a specific sample, let F RNrODTNcol be a matrix offeature occurrences across different rODTs. We then use the output of sub-module 2, a vectorwi RNrODT (Equation (3)), to represent the importance across all rODTs for each sample, as thisweight wi is used for constructing rODT forest to perform prediction in DOFEN model. A softmaxoperation is further applied to the vector wi to ensure the importance sums to 1 (also done in line6 of Algorithm 1). Finally, we perform a weighted sum between the feature occurrences and theimportance of each rODT, resulting in a single vector ti RNcol representing DOFENs featureimportance for specific sample. To calculate DOFENs overall feature importance of a dataset, wesimply average the feature importance of all samples in training dataset. We tested the reliability of DOFENs feature importance on three real-world datasets: the mushroomdataset, the red wine quality dataset, and the white wine quality dataset, following the experimentaldesign used by Trompt. The results for the mushroom dataset are shown in , with resultsfor the wine quality datasets provided in Appendix E. The results indicate that the top-3 importantfeatures identified by DOFEN align closely with those selected by other tree-based models, with onlyminor ranking differences. This demonstrates DOFENs ability to reliably identify key features whilemaintaining interpretability despite its deep learning architecture.",
  "of rODTs as shown in Algorithm 1 for a two-level rODT ensemble. This section explores howrandomness affects the stability of DOFEN": "We begin by analyzing the variation in performance across four datasets where DOFEN ranks first, asshown in . As shown in , the standard deviations are even negligible when Nforest = 1(about 0.1% to 1% to mean), except for the delays-zurich dataset. Moreover, with increased Nforest,the standard deviations become even smaller (about 0.01% to 0.1% to mean). These results suggest that the stability of DOFEN is not an issue in most cases (Nforest > 10), andusing the default setting of DOFEN (100 forests) ensures both adequate performance and stability formost datasets. Furthermore, the performance improves as the Nforest increases, indicating that the treebagging of DOFEN not only mitigates instability but also enhances the models generalizability.",
  "Related Work": "In this section, we categorize deep tabular neural networks into two main streams: tree-inspired DNNarchitectures and novel DNN architectures. By comparing DOFEN with these established models,we aim to showcase its unique contributions and position it within the broader landscape of deeptabular network research. Tree-inspired DNN Architectures. Integrating decision tree (DT) algorithms with DNNs has becomeprominent for handling tabular data. Pioneering works like Deep Forest , NODE , TabNet, GradTree and GRANDE have each introduced unique methodologies. Deep Forest adapts the random forest algorithm and incorporates multi-grained feature scanning toleverage the representation learning capabilities of DNNs. TabNet models the sequential decision-making process of traditional decision trees using a DNN, featuring a distinct encoder-decoderarchitecture that enables self-supervised learning. GradTree recognizes the importance of hard, axis-aligned splits for tabular data and uses a straight-through operator to handle the non-differentiablenature of decision trees, allowing for the end-to-end training of decision trees. NODE and GRANDEshare a similar observation and high-level structure to DOFEN, in that they ensemble multiple tree-like deep learning base models. NODE uses ODT as a base predictor and employs a DenseNet-likemulti-layer ensemble to boost performance. GRANDE, a successor to GradTree, uses DT as abase predictor and introduces advanced instance-wise weighting for ensembling each base modelsprediction. However, DOFEN distinguishes itself from NODE and GRANDE through its unique architecturaldesign. First, DOFEN employs a different approach to transforming tree-based models into neuralnetworks. Unlike NODE and GRANDE, which explicitly learn the decision paths (i.e., selectingfeatures and thresholds for each node) and the leaf node values of a tree, DOFEN randomly selectsfeatures to form rODTs and uses a neural network to measure how well a sample aligns with thedecision rule. Additionally, the leaf node value of an rODT is replaced with an embedding vector forfurther ensembling. Second, DOFEN introduces a novel two-level ensemble process to enhance modelperformance and stability. Unlike NODE and GRANDE, which simply perform a weighted sum on",
  "base model predictions, DOFEN first constructs multiple rODT forests by randomly aggregatingselected rODT embeddings and then applies bagging on the predictions of these rODT forests": "Novel DNN Architectures. Beyond merging decision tree algorithms with DNNs, significantprogress has been made in developing novel architectures for tabular data. Notable among theseare TabTransformer , FT-Transformer , SAINT , TabPFN , and Trompt . Thesemodels primarily leverage the transformer architecture , utilizing self-attention mechanisms tocapture complex feature relationships. TabTransformer applies transformer blocks specifically to numerical features, while FT-Transformerextends this approach to both numerical and categorical features. SAINT enhances the model furtherby applying self-attention both column-wise and sample-wise, increasing its capacity. TabPFN, avariant of the Prior Fitted Network (PFN) , is particularly effective with smaller datasets. Tromptintroduces an innovative approach by incorporating prompt learning techniques from natural languageprocessing , aiming to extract deeper insights from the tabular datas columnar structure. These architectures have demonstrated impressive performance across various studies and benchmarkdatasets and have been chosen as baselines in our performance evaluation, offering a comprehensiveview of the current state of the art in deep learning for tabular data.",
  "Limitation": "Although DOFEN shows promising results, it still contains two weaknesses. First, the inferencetime of DOFEN is relatively long compared to other DNN models, as shown in Appendix C.1.However, Appendix C.1 also shows that DOFEN possesses the fewest floating point operations(FLOPs). This inconsistency between inference time and FLOPs is mainly caused by the groupconvolution operation for calculating weights for each rODT (Appendix C.2), which can be improvedin the future implementation of DOFEN. Second, the randomization steps involved in DOFEN resultin a slower convergence speed, meaning that DOFEN requires more training steps to reach optimalperformance. This is reflected in the relatively larger number of training epochs needed for DOFEN.Therefore, the workaround strategy of differentiable sparse selection proposed in this study is merelya starting point, demonstrating its potential. Finding more efficient strategies will be the future work.",
  "Conclusion": "In this work, we introduced DOFEN, a novel DNN architecture inspired by oblivious decision treesfor tabular data. DOFEN first randomly combines conditions generated for each column to constructvarious relaxed oblivious decision trees (rODTs) and further enhances performance through a noveltwo-level rODT forest ensembling process. We evaluate DOFEN on a well-recognized benchmark: the Tabular Benchmark, where DOFENachieves state-of-the-art performance among DNN-based models and significantly narrows the gapbetween DNNs and traditional tree-based methods. We conducted an ablation study and analysis,which shows that the novel two-level rODT forest ensembling process not only contributes the mostto DOFENs superior performance but also maintains the stability of the randomization processinvolved in DOFEN. Moreover, due to DOFENs tree-like structure, its decision-making process isinterpretable, which is an important feature for deep learning models. In summary, DOFEN shows great potential as a versatile backbone for tabular data across variousscenarios, with its outstanding performance and interpretability, including self- and semi-supervisedlearning and multi-modal training.",
  "A.1Default Hyperparameters Settings for DOFEN": "In this section, we describe the hyperparameters used in our DOFEN model, along with their defaultvalues, as shown in . All notations used here have been previously introduced in ,except for dropout_rate and Nhead. The dropout_rate is applied in dropout layers, and its usage isdetailed in Appendix A.2. The Nhead is for the multi-head extension of rODT weighting mechanism,and its detail is provided in Appendix A.4.",
  "The calculated Nestimator for each dataset can be found in Appendix A.3. Additionally, the hyperparam-eter search spaces for both the DOFEN model and all baseline models are detailed in Appendix I.2": "DOFEN is implemented in Pytorch . For hyperparameters used in model optimization (e.g.optimizer, learning rate, weight decay, etc.), all experiments share the same settings. Specifically,DOFEN uses AdamW optimizer with 1e3 learning rate and no weight decay. The batch size ofDOFEN is set to 256 and is trained for 500 epochs without using learning rate scheduling or earlystopping if not specified.",
  "In this appendix, we elucidate the specific configurations of the neural network layer composites,denoted as 1, 2, and 3 in the main paper": "1. 1 - Generate conditions for each column: 1 is designed to generate conditions for bothnumerical and categorical data columns, as detailed in . For categorical columns inparticular, we employ embedding layers. These layers are utilized to transform categoricalfeatures into a format that the neural network can effectively process. 2. 2 and 3 - Derive weights and make predictions: The layers represented by 2 and 3are responsible for generating weights based on the combination of conditions and makingpredictions, respectively. The relevant structures and processes are illustrated in and . 3. Most parameters and their notations used here have been defined in the main paper andAppendix A.1, despite num_categories. This parameter represents the number of distinctcategories in a given categorical column.",
  "A.3Actual Nestimator for each Dataset": "The Nestimator is calculated through a pre-defined formula as shown in . In this section, weprovide the calculated Nestimator for each dataset in when using default hyperparameters.Datasets are represented by their OpenML ID as described in Appendix B.3. Linear(1, ) Sigmoid() Linear (1, cond_per_column) LayerNorm() Sigmoid() Embedding(num_categories, ) Linear(, )",
  "This section shows the extended multi-head version of the weighting mechanism when constructingrODT Forests (.2)": "In the single-head version of weighting, a weight scalar wij (Equation (3)) of each rODT is thecorresponding weight when aggregating the paired embedding vector ei (Equation (4)) to form anrODT forest. In multi-head version of weighting, we change the weight of each rODT from a scalarinto a Nhead dimension vector wij, as shown in Equation (5), while each head dimension of weight isresponsible for weighting a part of the paired embedding vector instead of full dimensionality. Thisconcept is similar to the one used in a multi-head attention , where each head can learn differentweighting, enhancing the capacity and diversity of the weighting process.",
  "C.1Computational Efficiency Analysis": "To discuss the computational efficiency, we analyzed the average floating point operations (FLOPs), parameter sizes, and inference time of DOFEN and other baseline models. Our analyses coveredboth the default and optimal hyperparameter settings, where the optimal hyperparameter delivers thebest performance for each model on each dataset. The experiments involving DNN-based modelswere performed using an NVIDIA GeForce RTX 2080 Ti, while those for the GBDT-based modelsutilized an AMD EPYC 7742 64-core Processor with 16 threads. We begin with the comparison between DNN-based and GBDT-based models. This comparisonprimarily focuses on inference time, as FLOPs and parameter sizes are applicable for evaluating theefficiency of DNN-based models but cannot be applied to GBDTs. Additionally, inference timesunder the optimal parameters are provided only when those parameters are available. As shown inTables to , the inference times for all DNN-based models are slower than thosefor GBDT-based models. This is expected due to the inherent differences between the two types ofmodels. When compared to other DNN baselines, DOFEN achieves the highest performance, the lowestFLOPs, and the smallest parameter sizes but exhibits the relatively long inference time amongall the DNN-based models. This inconsistency between FLOPs and inference time suggests thatthere is still room for implementation improvements in DOFEN. Hence, we conduct additionalexperiments to analyze which part of the DOFEN model is the computational bottleneck, as discussedin Appendix C.2, showing that the bottleneck of DOFEN arises from using group operations whenconstructing rODTs. Although this does not affect DOFENs article, improvements can be madeduring future open-source releases.",
  "To find out the computation bottleneck of DOFEN, we analyzed the inference time of each DOFENmodule in proportion, as shown in and , which is averaged across 59 medium-sized": "datasets with default hyperparameters. Table A1 shows that the Forest Construction module consumesthe most inference time. In Table A2, more detailed operations reveal that the sub-module 2 in theForest Construction module, which generates weights for each rODT, has the longest inference time. The sub-module 2 is designed with multiple MLP and normalization layers, implemented usinggroup convolution and group normalization to parallelize scoring for each rODT. However, theefficiency of group convolution in PyTorch has been problematic and remains unresolved. Specifically,the operation efficiency decreases as the number of groups increases, sometimes making it slower thanseparate convolutions in CUDA streams (see PyTorch issues 18631, 70954, 73764). The sub-module1 also uses group convolution to parallelize condition generation across different numerical columns,resulting in slower inference times compared to other operations, though less significant than 2 dueto fewer groups being used. However, we mainly focus on the concept and model structure in this paper, acknowledging thatmodel implementation can be further optimized. For example, attention operations are originallyslow due to quadratic complexity, and many recent works have successfully accelerated the speedof attention operations and reduced their memory usage. Hence, we believe there will be betterimplementations of these group operations with much greater efficiency in the future.",
  "C.3Training Time of DOFEN": "To know more about how the slow inference time will affect the training time of DOFEN, we alsoconducted an experiment to compare the training time of DOFEN with other deep learning methodsincluded in our paper (i.e. Trompt, FT Transformer, and NODE). We measured the training time onmedium-sized datasets using both default and optimal hyperparameter settings, where the optimalhyperparameters refers to the settings that deliver the best performance for each model on eachdataset. This experiment was conducted using a single NVIDIA Tesla V100 GPU. During model training,we carefully ensured that no other computational processes were running concurrently to enable afair comparison. Additionally, we excluded datasets that would cause OOM (Out of Memory) issuesduring training, resulting in the selection of 50 out of 59 medium-sized datasets.",
  "The average training time across datasets for each model is provided in Table A7. The results showthat the training time for DOFEN is relatively long, approximately twice as long as Trompt when": "using optimal hyperparameters. This extended training time may be due to the inefficient groupoperations involved in DOFEN, which consume about 80% of the computation time during theforward pass. For more details, please refer to Appendix C.2. Therefore, improving the efficiency ofgroup operations could reduce both the training and inference time of DOFEN.",
  "DScalability of DOFEN": "To discuss the scalability of DOFEN, we have conducted experiments to investigate its performancegiven changes in hyperparameters m, d, and the number of MLP layers (num_layers). In detail,changes in m and d affect the number of conditions (Ncond), while alterations in m impact both thetotal number of rODTs (NrODT) and the number of rODTs within an rODT forest (Nestimator). Forfurther details on these parameters, please refer to . The num_layers hyperparameter, newlyintroduced, refers to the number of MLP layers in neural networks 1, 2, and 3. A detailedintroduction to 1, 2, and 3 can be found in Appendix A.2. Due to limited computational resources, we only conducted this experiment on datasets that wouldnot cause out-of-memory (OOM) issues on our machine across all hyperparameter settings. Thisselection resulted in 51 out of 59 medium-sized datasets and 10 out of 14 large-sized datasets. Based on to , we observed that larger values of m and d enhance DOFENsperformance. Notably, improvements are more significant with large-sized datasets than with medium-sized datasets, likely because larger datasets benefit more from increased model capacity. In contrast, reveals that an increase in num_layers generally results in poorer performance. This couldbe attributed to the substantial growth in parameter size and FLOPs, compared to adjustments in them and d, potentially leading to overfitting.",
  "This section provides detailed experiment results of .3, where we mentioned applying forestensemble helps mitigate the overfitting issue of DOFEN": "To further investigate the significant drop in performance without applying the sampling processduring forest ensemble, we analyzed model performance at various training checkpoints. As illustratedin , omitting sampling in the forest ensembles leads to better training performance butsignificantly worse testing performance, with the gap widening as training epochs increase, indicatingthe overfitting issue. In contrast, using an ensemble of multiple forests improves both training andtesting performance, mitigating overfitting. Number of Training Epochs 0.75 0.80 0.85 0.90 0.95",
  "Accuracy": "(a) Classification Number of Training Epochs 0.6 0.7 0.8 0.9 R2 Score (b) Regression train, w/test, w/train, w/otest, w/o : Overfitting arises when not applying sampling in the forest ensemble, affecting both (a)classification and (b) regression tasks. Train\" refers to training performance, and Test\" refers totesting performance.w/\" indicates the use of applying sampling to construct multiple forests, whilew/o\" indicates the use of all constructed rODTs to form a single forest.",
  "bagging process on the predictions of multiple rODT forests, as rODT forests are also constructed byrandomly aggregating rODTs": "This means an rODT will affect the performance of an rODT forest, and as the process of constructingrODTs involves randomize operations, there might exist redundant rODTs that are not important forcorresponding task. To check whether if redundant rODTs exist, we analyze two binary classificationdataset (covertype) to observe the variation in the weights assigned to individual rODTs acrossdifferent samples, as shown in Figures 9 and 10. a shows that, for most rODTs ranked in the top 25 according to their weight standarddeviations, there is a significant difference between the average weights of true positive samples andthose of true negative samples. Conversely, b shows an opposite trend for rODTs with thesmallest weight standard deviations. These trends are also observed in another dataset, as shown inFigures 10a and 10b. These observations imply that rODTs with larger weight standard deviationsplay a more crucial role in classifying samples, while rODTs with less weight standard deviations arenot sensitive to samples with different label. In addition, we come up with an idea to examine the performance change after pruning rODT weightswith small standard deviations across samples and their corresponding rODT embeddings, seeingif these rODTs can be considered as redundant rODTs. The results are provided in Appendix F.3and suggest that the variation serves as a reliable indicator of the importance of rODTs. Moreover,pruning the less important rODTs not only enhances the models efficiency but also its performance. rODT rank average rODT weight TPTN",
  "(b) Relaxed ODTs with small weight variation": ": In the compass dataset, the weights wi of rODT are sorted based on the standard deviationcalculated across true positive (TP) and true negative (TN) samples in the testing data. ashows that the weights of TP samples differ significantly from those of TN samples when the standarddeviation of the weights is higher. Conversely, b reveals contrasting results for weights witha lower standard deviation.",
  "Following Appendix F.2, in this section, we aim to examine the performance change after pruningrODT weights with small standard deviations and their corresponding rODT embeddings": "shows the performance under different pruning ratios. The column labeled by datasetindicates that we tailored the pruning ratio for each dataset based on its validation data. As shown in, pruning these rODTs does not negatively affect performance. In fact, a minor degree ofpruning can actually enhance performance, with the optimal pruning ratio being 0.02 for classificationdatasets and 0.1 for regression datasets. Notice that the by dataset approach is better suited toreal-world scenarios, even though it does not always yield the best performance.",
  "Classification0.77250.77330.77260.77090.7732Regression0.66050.66290.66300.66210.6657": "We then investigate the outcomes when weights with higher standard deviations are pruned. Conse-quently, we sort the weights and prune them from the higher end. The results, presented in ,show that the performance in both classification and regression tasks monotonically drops as theprune ratio increases. This finding suggests that the standard deviation of weights is a good indicatorof their importance in making predictions. It further validates why pruning weights with lowerstandard deviation does not harm performance and, in some cases, even helps.",
  "Classification0.77250.77250.77150.76670.763Regression0.66050.65710.64840.63830.601": "In addition, we discuss another, potentially more straightforward, pruning approach. Specifically, weprune the weights wi based on their average value across samples. Similar to the experiments thatuse standard deviation as the metric for pruning, this time we sort the weights by their average. Wethen attempt to prune the weights from both the top and bottom ends. The results are provided in and , suggesting that the value of weights is not an effective indicator for pruning.Although there is some improvement in performance at a low ratio, this approach generally diminishesperformance with larger ratios, regardless of whether the weights are pruned from the higher or lowerend.",
  ": Results on large-sized classification datasets": "Classification. In a, DOFEN even surpasses CatBoost to become the top performer. Inb, DOFEN and CatBoost clearly outperforms other models. In contrast to DOFEN, othertabular DNN models like FT-Transformer and Trompt rank in the middle among all models. Asa result, with the current development of tabular DNN models, their performance in processingnumerical features is already on par with or even surpass that of tree-based models, and they aremore advantageous for large-sized datasets. However, DNN models are still less efficient in handlingheterogeneous features. CatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoostCatBoost",
  ": Results on large-sized regression datasets": "Regression. In a, the leading models remain DOFEN, XGBoost, and CatBoost. DOFENsproficiency in handling numerical features, further enhanced by the increased data volume, enablesit to secure the top position once again. In b, DOFEN and Trompt barely maintain theirpositions within the leading group, yet they still stand out from the other DNN models.",
  "G.2Detailed Evaluation Results": "In the main paper, we have discussed the overall performance of DOFEN. To simplify tables, we mapdataset names with their OpenML ID, as described in Appendix B.3. The evaluation results of eachtask are organized in . Please refer to the detailed figures and tables for each task of yourinterest. The evaluation metrics are accuracy for classification tasks and R2 score for regression tasks,consistent with our main paper. Furthermore, we calculate the mean and standard deviation of ranksacross datasets to provide the rank for each model in the tables.",
  "large-sized regressionnumericalheterogeneous": "jannisMagicTelescopeMiniBooNEpol eye_movementshelocHiggshouse_16H creditdefaultofcreditcardclientsDiabetes130USelectricity bankmarketingBioresponsecaliforniacovertype 0.78 0.80 0.82 0.84 0.82 0.84 0.86 0.875 0.880 0.885 0.890 0.93 0.95 0.97 0.87 0.88 0.89 0.90 0.56 0.57 0.58 0.59 0.60 0.68 0.69 0.70 0.71 0.920 0.925 0.930 0.935 0.940 0.74 0.76 0.78 0.69 0.70 0.71 0.72 0.70 0.71 0.72 0.85 0.86 0.87 0.790 0.795 0.800 0.805 0.810 0.75 0.76 0.77 0.78 0.575 0.600 0.625 0.650 0.74 0.75 0.76 0.77 0.78 Number of random search iterations Test accuracy of best model (on valid set) up to this iteration CatBoostDOFENFTTransformer GradientBoostingTreeGRANDEHistGradientBoostingTree LightGBMMLPNODE RandomForestResNetSAINT TromptXGBoost",
  ": Results on each medium-sized classification datasets with only numerical features": "electricityeye_movementsroadsafety albertcompastwoyearscovertypedefaultofcreditcardclients 0.700 0.705 0.710 0.715 0.720 0.80 0.82 0.84 0.86 0.88 0.75 0.76 0.77 0.62 0.64 0.66 0.68 0.600 0.625 0.650 0.62 0.63 0.64 0.65 0.66 0.83 0.85 0.87 Number of random search iterations Test accuracy of best model (on valid set) up to this iteration CatBoostDOFENFTTransformer GradientBoostingTreeGRANDEHistGradientBoostingTree LightGBMMLPNODE RandomForestResNetSAINT TromptXGBoost",
  ": Results on each medium-sized classification datasets with heterogeneous features": "superconductwine_qualityyprop_4_1 MiamiHousing2016nyctaxigreendec2016polsulfur house_16Hhouse_saleshousesmedical_charges cpu_actdelays_zurich_transportdiamondselevators abaloneAileronsBike_Sharing_DemandBrazilian_houses 0.00 0.25 0.50 0.75 1.00 0.7 0.8 0.9 0.00 0.25 0.50 0.75 1.00 0.70 0.75 0.80 0.85 0.90 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 1.00 0.000 0.025 0.050 0.075 0.00 0.25 0.50 0.75 0.00 0.01 0.02 0.03 0.00 0.25 0.50 0.75 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 1.00 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 Number of random search iterations Test R2 score of best model (on valid set) up to this iteration CatBoostDOFENFTTransformer GradientBoostingTreeGRANDEHistGradientBoostingTree LightGBMMLPNODE RandomForestResNetSAINT TromptXGBoost",
  ": Results on each medium-sized regression datasets with numerical features": "visualizing_soil particulatematterukair2017seattlecrime6SGEMM_GPU_kernel_performancetopo_2_1 house_salesmedical_chargesMercedes_Benz_Greener_Manufacturingnyctaxigreendec2016 Bike_Sharing_DemandBrazilian_housesdelays_zurich_transportdiamonds abaloneAirlines_DepDelay_1MAllstate_Claims_Severityanalcatdata_supreme 0.94 0.96 0.98 0.00 0.25 0.50 0.75 1.00 0.2 0.4 0.6 0.00 0.02 0.04 0.06 0.0 0.2 0.4 0.00 0.02 0.04 0.06 0.08 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 1.00 0.00 0.01 0.02 0.03 0.04 0.05 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.05 0.10 0.15 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 0.00 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.00 0.25 0.50 0.75 1.00 Number of random search iterations Test R2 score of best model (on valid set) up to this iteration CatBoostDOFENFTTransformer GradientBoostingTreeGRANDEHistGradientBoostingTree LightGBMMLPNODE RandomForestResNetSAINT TromptXGBoost",
  "H.1On Datasets used in FT-Transformer paper": "To have a more comprehensive comparison between methods on larger size datasets, we choose toevaluate DOFEN on datasets used in FT-Transformer paper, which are significantly larger than thedatasets used in the large-sized benchmark of Tabular Benchmark. For the experiment settings, wemainly follow the ones acknowledged in the FT-Transformer paper but with a few adjustments. Herewe outlined two changes to the experiment settings we have made: 1. First, due to the lack of computational resources and time for these large size datasets, weonly report the result of 5 different seeds, instead of the original setting that averages theresult across 15 seeds from FT-Transformer paper. 2. Second, for model comparison, aside from models included in FT-Transformer paper itself(i.e. FT-Transformer, Catboost, and XGBoost), we additionally include two state-of-the-artdeep learning models, Trompt and GRANDE, to show the effectiveness of DOFEN. The performance of FT-Transformer, Catboost, and XGBoost are acquired from the official GitHubrepository of FT-Transformer, and the performance of Trompt is obtained from Trompt paper. Itis worth noting that we only report performance with default hyperparameter settings for Trompt,GRANDE, and DOFEN. For Trompt, this is simply due to the searched results are not provided; ForDOFEN and GRANDE, this is due to the lack of time and resources. Additionally, we adjust the",
  "default setting of DOFEN to better accommodate with large size dataset by setting Nhead to 4, otherhyperparameter settings have remained as default": "The experiment results are provided in . Although DOFEN only reports results using defaultperformance, we are impressed that the default DOFEN already exceeds the searched performanceof FT-Transformer on 7 out of 11 datasets and ranks first on 3 out of 11 datasets. This indicatesthe potential of DOFEN to achieve even better performance after utilizing hyperparameter searchtechniques. : The performance of datasets used in FT-Transformer paper, averaged within 5 seeds. Asthe tree-based models act as an upper bound of performance, we highlight the best performing \"deeplearning model\" in Bold with underline, and the \"deep learning model\" with second performance inBold.",
  "H.2On Datasets used in GRANDE paper": "The datasets used in GRANDE paper are from the OpenML-CC18 benchmark , which aredifferent from both Tabular Benchmark and datasets used in FT-Transformer paper. Hence, we decideto evaluate DOFEN on these datasets for a more comprehensive comparison. It is worth noting that,these datasets only cover binary classification tasks and include many small datasets (data size <1000) according to the definition of Tabular Benchmark. For experimental settings, we strictly followthe settings mentioned in GRANDE paper. The experiment result is provided in . The performance of GRANDE, XGBoost, CatBoost,and NODE for both searched and default hyperparameters are taken from the GRANDE paper,while we only report the performance of default hyperparameters for DOFEN, due to the lack ofcomputational resources for a complete hyperparameter search. Additionally, DOFEN has madesome minor adjustments to accommodate the wide range of dataset size of this benchmark, theseadjustments are applied evenly on all datasets, with the details provided as follows:",
  ")}, where N represents dataset size. Thisadjustment is made mainly because using too large batch size will have negative effect whentraining DOFEN on small datasets": "2. We set Nhead to 4 for DOFEN. This is because there exist several large-sized datasets in thisbenchmark, this adjustment is made to better accommodate with large-sized datasets. Thisadjustment has also been made in Appendix H.1 for the same reason. Based on the experiment result, the average ranking of DOFEN using the default hyperparameterranks 3rd, only ranks behind GRANDE and CatBoost using searched hyperparameters, and exceedsXGBoost using searched hyperparameters. Moreover, we found that the performance of DOFEN iscompetitive with GRANDE and tree-based models on medium-to-large datasets, while sometimesfalling behind other models on medium-to-small datasets. These findings are similar to those found onTabular Benchmark and datasets used in FT-Transformer paper, where DOFENs performance benefitsfrom a larger dataset size. For the slightly worse performance on medium-to-small datasets, this can be explained that DOFEN tends to overfit on smaller datasets, hence including hyperparameter searchwith regularization techniques (e.g. dropout rate) or lower the capacity of a model (e.g. try smaller mand d in DOFEN) will help to improve the performance. : The performance of datasets used in GRANDE paper. We report the test macro F1-score(mean for a 5-fold CV) with default (subscript d) and searched parameters (subscript s), and anaverage performance ranking across datasets is calculated for an overall comparison. The bestperformance is highlighted in bold with underline, while the second best performance is highlightedin bold. The datasets are sorted based on the data size, here we only show the names in the first threeletters for a more compact table. The letter in the bracket represents the size of the dataset, where\"L\" indicates large datasets (data size > 10000), \"M\" indicates medium datasets (10000 > data size> 1000), and \"S\" indicates small datasets (data size < 1000), according to the definition of TabularBenchmark.",
  "I.2Hyperparameter Search Space": "This section details the hyperparameter search space adopted for each model, as referenced invarious tables (Tables 44 to 55). We have employed search spaces consistent with those presented inthe Tabular Benchmark for models including XGBoost, GradientBoostingTree, RandomForest,FT-Transformer, SAINT, ResNet, and MLP. Additionally, we have defined specific search spaces for newer baselines such as CatBoost, LightGBM,Trompt, NODE, and GRANDE. For CatBoost, our search space aligns with the parameters specifiedby the FT-Transformer study . In the case of LightGBM, we have derived the search space based onrecommendations from field practitioners, as cited in . For NODE, our approach follows theguidelines provided in TabZilla . For GRANDE, we follow the settings provided in the notebookexample from the official github of GRANDE. In the context of our model, DOFEN, we have focused our search on the number of m and d, whichrelate to the varied number of Ncond and the conditions per rODT. Additionally, Nhead is anotherimportant parameter since it increase the capacity for the model to evaluate how well a sample alignswith the conditions of an rODT. Lastly, we have explored the drop_rate parameter to fine-tune thedegree of regularization in our model. It is important to note that the overall search space for DOFENis relatively compact when compared to the other baseline models while achieving competitiveperformance."
}