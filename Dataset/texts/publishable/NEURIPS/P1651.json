{
  "Abstract": "Gaussian processes (GPs) are non-parametric probabilistic regression models thatare popular due to their flexibility, data efficiency, and well-calibrated uncertaintyestimates. However, standard GP models assume homoskedastic Gaussian noise,while many real-world applications are subject to non-Gaussian corruptions. Vari-ants of GPs that are more robust to alternative noise models have been proposed,and entail significant trade-offs between accuracy and robustness, and betweencomputational requirements and theoretical guarantees. In this work, we proposeand study a GP model that achieves robustness against sparse outliers by inferringdata-point-specific noise levels with a sequential selection procedure maximizingthe log marginal likelihood that we refer to as relevance pursuit. We show, surpris-ingly, that the model can be parameterized such that the associated log marginallikelihood is strongly concave in the data-point-specific noise variances, a propertyrarely found in either robust regression objectives or GP marginal likelihoods.This in turn implies the weak submodularity of the corresponding subset selectionproblem, and thereby proves approximation guarantees for the proposed algorithm.We compare the models performance relative to other approaches on diverse re-gression and Bayesian optimization tasks, including the challenging but commonsetting of sparse corruptions of the labels within or close to the function range.",
  "Introduction": "Probabilistic models have long been a central part of machine learning, and Gaussian process (GP)models are a key workhorse for many important tasks , especially in the small-data regime. GPsare flexible, non-parametric predictive models known for their high data efficiency and well-calibrateduncertainty estimates, making them a popular choice for regression, uncertainty quantification, anddownstream applications such as Bayesian optimization (BO) and active learning . GPs flexibly model a distribution over functions, but assume a particular observation model. Thestandard formulation assumes i.i.d Gaussian observation noise, i.e., y(x) = f(x) + , where f(x) isthe true (latent) function value at a point x and N(0, 2), implying a homoskedastic Gaussianlikelihood. While mathematically convenient, this assumption can be a limitation in practice, sincenoise distributions are often heavy-tailed or observations may be corrupted due to issues such assensor failures, data processing errors, or software bugs. Using a standard GP model in such settingscan result in poor predictive performance. A number of robust GP modeling approaches have been proposed to remedy this shortcoming, mostof which fall into the following broad categories: data pre-processing (e.g., Winsorizing), modifiedlikelihood functions (e.g., Student-t), and model-based data selection and down-weighting procedures.",
  "Posterior Predictive Distributions": "GaussianStudent-tRelevance Pursuit TruthCorruptionsObservations : Comparison of RRP to a standard GPand a variational GP with a Student-t likelihoodon a regression example. While the other modelsare led astray by the corrupted observations, RRPsuccessfully identifies the corruptions (red) andthus achieves a much better fit to the ground truth. We aim to model a function f : X R oversome domain X Rd. With a standard Gaus-sian noise model, for xi X we obtain observa-tions yi = f(xi) + i, where i N(0, 2) arei.i.d. draws from a Gaussian random variable. denotes the Euclidean norm unless indicatedotherwise.",
  "Gaussian Processes": "A GP f GP((), k(, )) is fully defined byits mean function : X R and covarianceor kernel function k : X X R, which isparameterized by . Without loss of general-ity, we will assume that 0. Suppose wehave collected data D = {(xi, yi)}ni=1 whereX := {xi}ni=1, y := {yi}ni=1. Let Sn++denote the covariance matrix of the data set, i.e.,[]ij = k(xi, xj) + ij2, where ij is theKronecker delta. The negative marginal log-likelihood (NMLL) L is given by",
  "Noise Models": "Additive, heavy-tailed noiseInstead of assuming the noise term i in the observation model to beGaussian, other noise models consider zero-mean perturbations drawn from distributions with heaviertails, such as the Student-t , Laplace , or -Stable distributions. These types of errorsare common in applications such as finance, geophysics, and epidemiology . Robust regressionmodels utilizing Student-t errors are commonly used to combat heavy-tailed noise and outliers. Sparse corruptionsIn practice, often a small number of labels are corrupted. We will refer tothese as outliers,\" though emphasize that the corrupted values may fall within the range of normaloutputs. Sparse corruptions are captured by a model of the form yi = Zif(xi) + (1 Zi)Wi, where Zi {0, 1} and Wi R is a random variable. Note that Wi need not have (and rarely has) f(xi) asits mean. For instance, consider a faulty sensor that with some probability p reports a random valuewithin the sensor range [yl, yh]. In this case Zi Ber(p) and Wi U[yl, yh]. Software bugs, suchas those found in ML training procedures, or errors in logging data can result in sparse corruptions.",
  "Related Work": "Data pre-processingData pre-processing can be an effective technique for handling simple formsof data corruption, such as values that fall outside a valid range of outputs. With such pre-processing,outliers are handled upstream of the regression model. Common techniques include the powertransformations , trimming, and winsorization. These methods can add substantial bias if notused carefully, and generally do not handle data corruptions that occur within the normal range of theprocess to be modeled. See for a review on data cleaning. Heavy-tailed likelihoodsOne class of robust methods uses additive heavy-tailed noise likelihoodsfor GPs, particularly Student-t , Laplace , and Huber , and could be extended with -Stabledistributions, which follow a generalized central limit theorem . These models are less sensitive tooutliers, but they lose efficiency when the outliers are a sparse subset of the observations, as opposedto global heavy-tailed noise. Furthermore, model inference is no longer analytic, necessitating the useof approximate inference approaches such as MCMC , Laplace approximation , expectationpropagation (EP) , Expectation Maximization , or variational inference . Shah et al. take a related approach using a Student-t process prior in the place of the GP prior. Unfortunately, theStudent-t process is not closed under addition and lacks the tractability that makes GPs so versatile.Alternative noise specifications include a hierarchical mixture of Gaussians and a twinned GPmodel that uses a two-component noise model to allow outlier behavior to depend on the inputs.This method is suited for settings where outliers are not totally stochastic, but generally is not able todifferentiate inliers from outliers when they can occur with similar inputs. Outlier classificationAwasthi et al. introduces the Trimmed MLE approach, which identifiesthe subset of data points (of pre-specified size) under which the marginal likelihood is maximized.Andrade and Takeda fit GPs using the trimmed MLE by applying a projected gradient method toan approximation of the marginal likelihood. The associated theory only guarantees convergence to astationary point, with no guarantee on quality. When no outliers are present, this method can be worsethan a standard GP. Li et al. propose a heuristic iterative procedure of removing those data pointswith the largest residuals after fitting a standard GP, with subsequent reweighting. The method showsfavorable empirical performance but has no theoretical guarantees, and fails if the largest residual isnot associated with an outlier. Park et al. consider a model of the form yi = i + f(xi) + i,where outliers are regarded as data with a large bias i. Their random bias model is related to ourmodel in that it also introduces learnable, data-point-specific variances. However, inference is donein one step by optimizing the NMLL with an inverse-gamma prior on the is, which in contrast tothe method proposed herein generally does not lead to exactly sparse is . Sample re-weightingAltamirano et al. propose robust and conjugate GPs (RCGP) based on amodification to the Gaussian likelihood function that is equivalent to standard GP inference, wherethe covariance of the noise 2I is replaced by 2 diag(w2) and the prior mean m is replaced bymw = m + 2y log(w2). The authors advocate for the use of the inverse multi-quadratic weightfunction w(x, y) = (1 + (y m(x))2/c2)1/2, which introduces two additional hyper-parameters:the soft threshold c, and the learning rate . Importantly, the weights w are defined a-priori as afunction of the prior mean m(x) and the targets y, thereby necessitating the weights to identify thecorrect outliers without access to a model. This is generally only realistic if the outlier data points areclearly separated in the input or output spaces rather than randomly interspersed.",
  "Robust Gaussian Process Regression via Relevance Pursuit": "Our method adaptively identifies a sparse set of outlying data points that are corrupted by a mechanismthat is not captured by the other components of the model. This is in contrast to many other approachesto robust regression that non-adaptively apply a heavy-tailed likelihood to all observations, whichcan be suboptimal if many observations are of high quality. : Left: Evolution of model posterior during Relevance Pursuit, as the number of data-point-specific variances |S| increases (from light colors to dark). Red points indicate corruptions that weregenerated by uniformly sampling from the functions range. Right: Comparison of posterior marginallikelihoods as a function of a models |S|. The maximizer boxed in black is the preferred model.",
  "yi xi Nf(xi), 2 + i.(2)": "This is similar to Sparse Bayesian Learning in which weight-specific prior variances control afeatures degree of influence on a models predictions. The marginal likelihood optimization of iin (2) gives rise to an automatic mechanism for the detection and weighting of outliers. The effectof yi on the estimate of f vanishes as i , similar to the effect of the latent varibales h inBodin et al. s extended GP model f(x, h), though h requires MCMC for inference. While manyheteroskedastic GP likelihoods model noise as an input-dependent process , our formulationdoes not require such assumptions, and is thus suitable for corruptions that are not spatially correlated. An elegant consequence of our modeling assumption is that we can compute individual marginal-likelihood maximizing is in closed form when keeping all j for j = i fixed. In particular,Lemma 1. [Optimal Robust Variances] Let D\\i = {(xj, yj) : j = i}, = \\i + iei, where, \\i Rn+, [\\i]i = 0, and ei is the ith canonical basis vector. Then keeping \\i fixed,",
  "where D2+ is a diagonal matrix whose entries are 2 +": "The first component E[f(xi)+i|D\\i]2 of (3) is the empirical error to yi of the model trained withoutthe i-th data point, i.e., the leave-one-out (LOO) cross-validation error . The second componentV[f(xi) + i|D\\i] is the LOO predictive variance. The optimal solution to i is only non-zero forthose observations whose squared LOO error is larger than the LOO predictive variance at that point.",
  "Optimization with a Maximum Number of Outliers": "Without additional structure, inference of the noise variances i does not yield desirable models, asthe marginal likelihood can be improved by increasing the prior variance i of any data point whereEq. (3) is greater than zero, even if that is due to regular (non-outlier) measurement noise. To avoidthis, we constrain the number of non-zero i, that is, = |{0 < i}| k < n. While thissparsity constraint mitigates over-flexibility, it gives rise to a formidably challenging optimizationproblem, as there are a combinatorial number of sparse outlier sets to consider. Even if the number ofoutliers no were known, exhaustive search would still require considering n-choose-no possibilities. For tractability, we iteratively add data points to a set of potential outliers by setting their associatedi to be nonzero, using the closed-form expression for the optimal individual i variances in Lemma 1.As the algorithm seeks to identify the most relevant data points (as measured by L) upon completion,we refer to it as Relevance Pursuit. This is Algorithm 1 with useBayesianModelSelection as false.Specifically, this is the forward variant; Algorithm 2 in the Appendix presents an alternativebackward variant that we found to work well if the number of corrupted data points is large. Crucial to the performance of the optimizer, it never removes data from consideration completely; adata point is only down-weighted if it is apparently an outlier. This allows the down-weighting to bereversed if a data point appears inlying after having down-weighted other data points, improvingthe methods robustness and performance. This is in contrast to Andrade and Takeda s greedyalgorithm, in which the exclusion of a data point can both increase or decrease the associatedmarginal likelihood. This means that their objective is not monotonic, a necessary condition toprovide constant-factor submodular approximation guarantees for greedy algorithms, see .",
  "Automatic Outlier Detection via Bayesian Model Selection": "In practice, it is often impossible to set a hard threshold on the number of outliers for a particulardata set. For example, a sensor might have a known failure rate, but how many outliers it produceswill depend on the specific application of the sensor. Thus, is often more natural to specify a priordistribution p(S) over the number of outliers, rather than fix the number a priori. We leverage theBayesian model selection framework to determine the most probable number of outliers in adata- and model-dependent way, aiming to maximize p(S|D). This gives rise to Algorithm 1, withuseBayesianModelSelction as true. Computationally, we start by iteratively adding outliers up to the maximal support of the prior, similarto the procedure described in .2. We store a trace of models generated at each iteration, thenapproximate the model posterior p(Si|D) p(D|Si)p(Si) at each point in the trace. As the exactposterior is intractable, we approximate it with p(D|Si) =p(D|Si, Si)dSi p(D|Si, Si).Finally, we select the model from the model trace {Si}i that attains the highest model posteriorlikelihood. Imposing a prior on the number of outliers differs notably from most sparsity-inducingpriors, which are instead defined on the parameter values, like l1-norm regularization. In practice, p(S)can be informed by empirical distributions of outliers. For our experiments, we use an exponentialprior on |S| to encourage the selection of models that fit as much of the data as tightly as possible. Regarding the schedule K in Algorithm 1, the most natural choice is simply to add one data point at atime, i.e. K = (1, 1, ...), but this can be slow for large n. In practice, we recommend schedules thattest a fixed set of outlier fractions, e.g. K = (0.05n, 0.05n, . . . ).",
  "We now provide a theoretical analysis of our approach. We first propose a re-parameterization ofthe i that maps the optimization problem to a compact domain. Surprisingly, the re-parameterized": "problem exhibits strong convexity and smoothness when the base covariance matrix (excludingthe i) is well-conditioned. We connect the convexity and smoothness with existing results that yieldapproximation guarantees for sequential greedy algorithms, implying a constant-factor approximationguarantee to the optimal achievable NMLL value for generalized orthogonal matching pursuit (OMP),a greedy algorithm that is closely related to Algorithm 1.",
  "Preliminaries for Sparse Optimization": "The optimization of linear models with respect to least-squares objectives in the presence of sparsityconstraints has been richly studied in statistics , compressed sensing , and machinelearning . Of central importance to the theoretical study of this problem class are the eigenvaluesof sub-matrices of the feature matrix, corresponding to sparse feature selections and so often referredto as sparse eigenvalues. The restricted isometry property (RIP) formalizes this.Definition 2 (Restricted Isometry Property). An (n m)-matrix A satisfies the r-restricted isometryproperty (RIP) with constant r (0, 1) if for every submatrix AS with |S| = r m columns,",
  "where xS Rr. This is equivalent to (1 r)I (ASAS) (1 r)I": "The RIP has been proven to lead to exact recovery guarantees , as well as approximationguarantees . Elenberg et al. generalized the RIP to non-linear models and other datalikelihoods, using the notion of restricted strong convexity (RSC) and restricted smoothness.Definition 3 (Restricted Strong Convexity and Smoothness). A function f : Rd R is mr-restrictedstrong convex and Mr-restricted smooth if for all (x, x) in the domain Dr (Rd Rd),",
  "In the context of sparse optimization, we let Dr be the set of tuples of r-sparse vectors whosedifference is also at most r-sparse. In particular, Dr = {(x, x) s.t. x0, x0, x x0 r}": "Generalized orthogonal matching pursuit (OMP) is a greedy algorithm that keeps track ofa support set S of non-zero coefficients, and expands the support based on the largest gradient magni-tudes, applied to the marginal liklihood optimization problem, Si+1 = Si arg maxjS |L()|j.Algorithm 1 generalizes OMP by allowing more general support expansion schedules K, andspecializes the support expansion criterion using the special problem structure exposed by Lemma 1.",
  "The Convex Parameterization": "The NMLL L of a GP (1) is the sum of a convex function ()1 and a concave function log det()of K, and is therefore not generally convex as a function of the hyper-parameters , including therobust variances . Here, we propose a re-parameterization that allows us to prove strong convexityguarantees of the associated NMLL. In particular, we let (s) = diag(K0) ((1 s)1 1), whereK0 := k(X, X) + 2I and the inverse is element-wise. Note that (s) is a diffeomorphism thatmaps s from the compact domain s n to the entire range of n.",
  "where min,max (resp. min,max) are the smallest and largest eigenvalues of Ks, respectively Ks": ": Top: The behavior of the log L() with respect to the canonical parameterization of .Bottom: The behavior of log L((s)), highlighting the convexity property. Left: The value, andfirst two derivatives of log L for a 1d example. Center: The second derivatives of a 1d log L as afunction of |y|. The s-parameterization is everwhere convex for all considered |y|, while the canonical-parameterization is only convex around the origin and only for |y| > 0.5. Right: The heatmapshighlight that the original parameterization is non-convex (red) for larger values of , and quicklybecomes ill-conditioned, whereas the parameterization (s) is convex and much better conditioned. The behavior Lemma 5 predicts is surprising and validated in . Notably, the denominatorblows up as K becomes close to unitary, making the inequality more likely to be satisfied, anindication that the convexity property of the NMLL is intimately linked to the RIP (Def. 2). Note thatLemma 5 is a condition for non-support-restricted convexity, which is stronger than is necessary forthe approximation guarantees that rely on restricted convexity (Def. 3). However, sparse eigenvaluesare generally difficult to compute exactly. Fortunately, covariance matrices of GPs naturally tend toexhibit a property that facilitates a different sufficient condition for convexity for all s n.Definition 6 (Diagonal Dominance). A matrix A is said to be -diagonally dominant if the elementsaij satisfy",
  "i=j |aij| < |aii| for all i": "Intuitively, the i(s) that are selected to be non-zero by the greedy algorithm take on large values,further encouraging the diagonal dominance of the sub-matrix of K associated with the support of .For this reason, the following condition on K0 is sufficient to guarantee convexity for all s n.Lemma 7. [Strong Convexity via Diagonal Dominance] Let m > 0 and K0 be -diagonally dominantwith <(5 m) 25 9m + 17/4 (5",
  "Then the NMLL is m-strongly convex for all s n, i.e. (s) n": "Proof. Fist, Gershgorins Disk Theorem implies that the eigenvalues of diag(A)1/2A diag(A)1/2lie in (1 , 1 + ) for a -diagonally dominant matrix A. Further, the condition number of A isbounded above by (A) = max(A)/min(A) (1 + )/(1 ). See Horn and Johnson formore background on matrix analysis. Plugging these bounds into the results of Lemma 5 yields",
  "where L() = L() L(0) is normalized so that maxsS L(sS) 0 for any support S": "Proof. The result is a direct consequence of meeting the m-convexity and M-smoothness conditionsof Lemmas 7, 14 above, and the OMP approximation guarantee of Theorem 11 due to Elenberg et al.. Note that the condition on 2r n comes from the RSC condition in Theorem 11 being requiredfor subsets of size 2r. As we proved bounds for the m-convexity of the full Hessian of size n, r hasto be smaller than n/2 for the assumptions of the theorem to hold. Regarding the upper bound smaxon s, we note that the constraint is convex and therefore doesnt change the convexity property of theoptimization problem. Further, note that maxS L(S) maxSi L(Si), since the additional non-zero element couldstay at 0, if the marginal likelihood does not improve with i increasing. That is, the subset selectionproblem is monotonic. As a consequence, we can normalize the MLL by L() = L() L(0), whichthen only takes positive values for any sS = arg maxs\\S=0 L((sS)), i.e. maxsS L(sS) 0. Thisnormalization is required for the constant factor approximation guarantee to apply, similar to theoriginal work of Nemhauser. This theoretical approach could lead to approximation guarantees for Tipping s Sparse BayesianLearning (SBL) model, for which Ament and Gomes show that greedily optimizing the associatedNMLL is equivalent to stepwise regression in the limit of 0, proving exact recovery guarantees.",
  "Empirical Results": "We evaluate the empirical performance of RRP against various baselines on a number of regressionand Bayesian Optimization problems. Specifically, we compare against a standard GP with a Matern-5/2 kernel (Standard GP), data pre-processing through Axs adaptive winsorization procedure(Adapt. Wins.) , and a power transformation (Power Transf.) . Further, we also considera Student-t likelihood model from Jylnki et al. (Student-t), the trimmed marginal likelihoodmodel from Andrade and Takeda (Trimmed MLL), and the RCGP model from Altamiranoet al. . Unless stated otherwise, all models are implemented in GPyTorch and all experimentsin this section use 32 replications. See Appendix D for additional details.",
  "Regression Problems": "SyntheticWe first consider the popular Friedman10 and Hartmann6 test functions from theliterature. We use two data generating processes: uniform noise, extreme outliers at some fixed value,and heavy-tailed (Student-t) noise at true function values. In these experiments, we compare theperformance predictive log-likelihood. The results are shown in . Hartmann6, 5% Constant CorruptionsHartmann6, 20% Constant CorruptionsHartmann6, Student-t Corruptions RRP Student-t Power Transf. Standard GP Adapt. Wins. Trimmed MLL Friedman10, 10% Uniform Corruptions RRP Student-t Power Transf. Standard GP Adapt. Wins. Trimmed MLL Friedman10, 25% Uniform Corruptions 0.20.40.60.81.0",
  "Predictive Log Likelihood": ":Left: Distribution of predictive test-set log likelihood for various methods. Methodsommitted are those that performed substantially worse. Right: Predictive log likelihood as a functionof the corruption probability for Student-t-distributed corruptions with two degrees of freedom. TheGP model with the Student-t likelihood only starts outperforming RRP as the corruption probabilityincreases beyond 40%, and exhibits a large variance in outcomes, which shrinks as the proportion ofcorruptions increases. All methods not shown were inferior to either RRP or Student-t. Twitter Flash CrashIn , we report a comparison to Altamirano et al. s RCGP on data fromthe Dow Jones Industrial Average (DJIA) index on April 22-23 2013, which includes a sharp drop at13:10 on the 23rd. The top panels shows that RCGP exhibits higher robustness than the standard GP,but is still affected by the outliers, when trained on data from the 23rd. RRP is virtually unaffected.Notably, RCGP relies on an a-priori weighting of data points based on the target values proximityto their median, which can be counter-productive when the outliers are not a-priori separated in therange. To highlight this, we included the previous trading day into the training data for the bottompanels, leading RCGP to assign the highest weight to the outlying data points due to their proximityto the target values median, thereby leading RCGP to trust the outliers more than any inlier,",
  "(d)(c)": ":Results on the intra-day data from the Dow Jones Industrial Average (DJIA) index onApril 22-23 2013, which includes a sharp drop at 13:10 on the 23rd, see (b) for a detailed view.The accompanying panels labeled wimq show the function that Altamirano et al. s RCGP uses todown-weight data points. Top: RCGP, exhibits higher robustness than the standard GP, but is stillaffected by the outliers. The RRP model is virtually unaffected. Bottom: Including the previoustrading day into the training data in (c), leads RCGP to assign the highest weight wimq to the outlyingdata points due to their proximity to the target values median, thereby leading RCGP to be evenmore affected than a standard GP, see (d) for a detailed view of the results on the data of April 23.",
  "Robust Bayesian Optimization": "GPs are commonly used for Bayesian optimization (BO), which is a popular approach to sample-efficient black-box optimization . However, many of the GP models used for BO are sensitiveto outliers and may not perform well in settings where such outliers occur. While Martinez-Cantinet al. consider the use of a Student-t likelihood for BO with outliers, the use of other robust GPmodels has not been thoroughly studied in the literature. Experimental setupWe use Ament et al. s qLogNoisyExpectedImprovement (qLogNEI), avariant of the LogEI family of acquisition functions, 32 replications, and initialize all methods withthe same quasi-random Sobol batch for each replication. We follow Hvarfner et al. and plotthe true value of the best in-sample point according to the GP model posterior at each iteration. Wealso include Sobol and an Oracle, which is a Standard GP that always observes the uncorruptedvalue, and consider the backward canonical version of relevance pursuit, denoted by RRP, for theseexperiments. The plots show the mean performance with a bootstrapped 90% confidence interval. Synthetic problemsWe consider the popular 6-dimensional Hartmann test function with threedifferent corruption settings: (1) a constant value of 100, (2) a U distributed value, (3) the ob-jective value for a randomly chosen point in the domain. The results for a 10% corruption probabilityare shown in . We also include results for a 20% corruption probability in Appendix D.3. Number of Evaluations 3.0 2.5 2.0 1.5 1.0 0.5 Value of Best Inferred Point Constant outliers value 100 (10%) Number of Evaluations Uniform outliers (10%) Number of Evaluations Uniform input outliers (10%)",
  "Method": "Trimmed MLLSobolStandard GPStudent-tRRPOracle : BO results for Hartmann6: Left: We see that Relevance pursuit performs well in the caseof constant outliers of value 100 and almost performs just as well as the oracle. Middle: Relevancepursuit performs the best followed by the Student-t likelihood from in the case of U. Nomethod performs as well as the oracle when the outlier probability is 20%. Right: Similarly to themiddle column, this setting hides the outliers within the range of the outliers making it difficult tomatch the performance of the oracle. variance), recording the achieved negative log marginal likelihood (NLML) as a function of thetolerance parameter ftol of the L-BFGS optimizer. The results are reported in Table D.5, and indicatethat the optimizer terminates with a much better NLML using the convex parameterization with thesame convergence tolerance.",
  "Conclusion and Future Work": "ContributionsRobust Gaussian Processes via Relevance Pursuit (RRP) provides a novel and prin-cipled way to perform robust GP regression. It permits efficient and robust inference, performs wellacross a variety of label corruption settings, retains good performance in the absence of corruptions,and is flexible, e.g., can be used with any mean or kernel function. Our method can be readily appliedto both robust regression problems as well as applications such as Bayesian optimization and isavailable through BoTorch . Importantly, it also provides theoretical approximation guarantees. LimitationsAs our approach does not explicitly consider the locations of the data points in the out-lier identification, it may be outperformed by other methods if the underlying noise is heteroskedasticand location-dependent. On the other hand, those methods generally do not perform well in thepresence of sparse, location-independent data corruptions. ExtensionsPromising extensions of this work include performing Bayesian model averaging, i.e.,average the predictions of the different possible sparsity models according to their likelihoods insteadof using a MAP estimate, applying RRP to specialized models such as Lin et al. s scalablelearning-curve model for AutoML applications, and Ament et al. s model for sustainable concrete.On a higher level, the approach of combining greedy optimization algorithms with Bayesian modelselection and leveraging a convex parameterization to achieve approximation guarantees might applyto other parameters that are optimized using the MLL objective: length-scales of stationary kernels,coefficients of additive kernels, inducing inputs, and even related model classes like Tipping sSparse Bayesian Learning (SBL), which seeks to identify sparse linear models and is intimatelylinked to greedy matching pursuits . Overall, the approach has the potential to lead to theoreticalguarantees, new insights, and performance improvements to widely-adopted Bayesian models.",
  "M. Altamirano, F.-X. Briol, and J. Knoblauch. Robust and conjugate gaussian process regression.arXiv preprint arXiv:2311.00463, 2023": "S. Ament and C. Gomes. On the optimality of backward regression: Sparse recovery andsubset selection. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 55995603, 2021. doi: 10.1109/ICASSP39728.2021.9415082. S. Ament and C. Gomes. Generalized matching pursuits for the sparse optimization of separableobjectives. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pages 55535557, 2022. doi: 10.1109/ICASSP43922.2022.9747510.",
  "S. Ament and M. ONeil. Accurate and efficient numerical calculation of stable densities viaoptimized quadrature and asymptotics. Statistics and Computing, 28:171185, 2018": "S. Ament, M. Amsler, D. R. Sutherland, M.-C. Chang, D. Guevarra, A. B. Connolly, J. M.Gregoire, M. O. Thompson, C. P. Gomes, and R. B. van Dover. Autonomous materials synthesisvia hierarchical active learning of nonequilibrium phase diagrams. Science Advances, 7(51):eabg4930, 2021. doi: 10.1126/sciadv.abg4930. URL S. Ament, S. Daulton, D. Eriksson, M. Balandat, and E. Bakshy.Unexpected im-provements to expected improvement for bayesian optimization.In A. Oh, T. Nau-mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neu-ral Information Processing Systems, volume 36, pages 2057720612. Curran Associates,Inc., 2023. URL",
  "S. Ament, A. Witte, N. Garg, and J. Kusuma. Sustainable concrete via bayesian optimization,2023. URL": "S. E. Ament and C. P. Gomes. Sparse bayesian learning via stepwise regression. In M. Meilaand T. Zhang, editors, Proceedings of the 38th International Conference on Machine Learning,volume 139 of Proceedings of Machine Learning Research, pages 264274. PMLR, 1824 Jul2021. URL D. Andrade and A. Takeda. Robust Gaussian process regression with the trimmed marginallikelihood. In R. J. Evans and I. Shpitser, editors, Proceedings of the Thirty-Ninth Conferenceon Uncertainty in Artificial Intelligence, volume 216 of Proceedings of Machine LearningResearch, pages 6776. PMLR, 31 Jul04 Aug 2023. URL P. Awasthi, A. Das, W. Kong, and R. Sen. Trimmed maximum likelihood estimation for robustgeneralized linear model. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advancesin Neural Information Processing Systems, 2022. URL",
  "E. Bakshy, L. Dworkin, B. Karrer, K. Kashin, B. Letham, A. Murthy, and S. Singh. Ae: Adomain-agnostic platform for adaptive experimentation. In NeurIPS Workshop on Systems forMachine Learning, 2018": "M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy.BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. In Advances inNeural Information Processing Systems 33, 2020. E. Bodin, M. Kaiser, I. Kazlauskaite, Z. Dai, N. Campbell, and C. H. Ek.Modulatingsurrogates for Bayesian optimization.In H. D. III and A. Singh, editors, Proceedingsof the 37th International Conference on Machine Learning, volume 119 of Proceedingsof Machine Learning Research, pages 970979. PMLR, 1318 Jul 2020.URL",
  "M. Kelly, R. Longjohn, and K. Nottingham. The uci machine learning repository. URL ics. uci. edu, 2023": "K. Kersting, C. Plagemann, P. Pfaff, and W. Burgard. Most likely heteroscedastic gaussianprocess regression. In Proceedings of the 24th International Conference on Machine Learning,ICML 07, pages 393400, 2007. A. Krause and D. Golovin. Submodular function maximization. In L. Bordeaux, Y. Hamadi,and P. Kohli, editors, Tractability, pages 71104. Cambridge University Press, 2014. ISBN9781139177801. URL #0001G14. A. Krause and C. Guestrin. Near-optimal nonmyopic value of information in graphical models.In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI05,page 324331, Arlington, Virginia, USA, 2005. AUAI Press. ISBN 0974903914.",
  "D. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization.Math. Program., 45(13):503528, Aug. 1989. ISSN 0025-5610": "F. Locatello, R. Khanna, M. Tschannen, and M. Jaggi. A Unified Optimization View onGeneralized Matching Pursuit and Frank-Wolfe. In A. Singh and J. Zhu, editors, Proceedingsof the 20th International Conference on Artificial Intelligence and Statistics, volume 54 ofProceedings of Machine Learning Research, pages 860868. PMLR, 2022 Apr 2017. URL F. Locatello, A. Raj, S. P. Karimireddy, G. Raetsch, B. Schlkopf, S. Stich, and M. Jaggi. Onmatching pursuit and coordinate descent. In J. Dy and A. Krause, editors, Proceedings of the35th International Conference on Machine Learning, volume 80 of Proceedings of MachineLearning Research, pages 31983207. PMLR, 1015 Jul 2018. URL",
  "R. Martinez-Cantin, M. McCourt, and K. Tee. Robust bayesian optimization with student-tlikelihood, 2017": "N. Maus, K. Wu, D. Eriksson, and J. Gardner. Discovering many diverse solutions with Bayesianoptimization. In Proceedings of the International Conference on Artificial Intelligence andStatistics, volume 206, pages 17791798. PMLR, Apr. 2023. A. Naish-Guzman and S. Holden. Robust regression with twinned gaussian processes. In J. Platt,D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Sys-tems, volume 20. Curran Associates, Inc., 2007. URL",
  "A. Shah, A. Wilson, and Z. Ghahramani. Student-t processes as alternatives to gaussianprocesses. In Artificial intelligence and statistics, pages 877885. PMLR, 2014": "N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the banditsetting: No regret and experimental design. In Proceedings of the 27th International Conferenceon International Conference on Machine Learning, ICML10, page 10151022, Madison, WI,USA, 2010. Omnipress. ISBN 9781605589077. S. Sundararajan and S. Keerthi. Predictive approaches for choosing hyperparameters in gaussianprocesses. In S. Solla, T. Leen, and K. Mller, editors, Advances in Neural Information Pro-cessing Systems, volume 12. MIT Press, 1999. URL",
  "AAdditional Details on the Model": "Algorithm 2 below is the backward variant of Algorithm 1 from Sec 4. As its name suggests, themain difference compared to the forward variant is that rather than building up a set of outliers, itstarts from a (typically large) set of outliers and iteratively removes those data points from the setthat have the smallest inferred data-point-dependent noise variance i. While we have not derived theoretical guarantees for this backward version, we have found it togenerally behave similarly to the forward version in terms of performance and robustness. Oneempirical observation from our studies is that while the forward version tends to perform slightlybetter than the backward version if there are only few outliers, the opposite is true if the outlierfrequency is very high. This behavior is rather intuitive and illustrates that relevance pursuit isparticularly well-suited to identify sparse, low-cardinality subsets (note that in the backward variantunder large corruptions, the uncorrupted data points can be viewed as the sparse subset that needs tobe identified).",
  "Krause and Golovin provides a survey on the maximization of general submodular functions.Here, we focus on applications of submodularity to sparse regression and Gaussian process models": "Sparse RegressionDas and Kempe showed that the subset selection problem of regressionfeatures with an R2 objective satisfies a weak submodularity property, which can be invoked toprove approximation guarantees for the greedy maximization of the objective. Elenberg et al. generalized this work by proving that any log likelihood function exhibiting restricted strong concavitygives rise to the weak submodularity of the associated subset selection problem, which can be invokedto prove approximation guarantees for the greedy algorithm. Karaca et al. contains a guaranteefor the backward algorithm applied to the maximization of submodular set functions. Gaussian ProcessesSubmodularity has also found application to Gaussian process models. Fora sensor placement problem, Krause et al. proved that the mutual information (MI) criterion,capturing the reduction in uncertainty in the entire search space, can be a submodular function. Inthis case, MI is not monotonic everywhere, but monotonic for small sets (2k) of sensors, which issufficient to apply Nemhausers guarantee for sparse sets of sensors up to size k . Relatedly, themyopic joint entropy of a set of observables is unconditionally submodular as a consequence of theinformation never hurts principle , but generally leads to lower-quality sensor placements thanthe MI criterion. Srinivas et al. used the submodularity of the joint entropy in order to proveregret bounds for the convergence of a GP-based BO algorithm using the upper-confidence boundacquisition function.",
  "i2(L() L(\\i)) = (y\\iu yi)22i i.(11)": "While i = 0 is a root of the derivative, we ignore this solution since i is never zero when 2 > 0.Therefore, the remaining stationary point is 1i= (y\\iu yi)2. Since we constrain 0, thispoint might not always be attainable. However, because there is only a single stationary point withrespect to i when 2 > 0, and i is a strictly decreasing function of i, it follows that the marginallikelihood is monotonic as a function of i to both the left and the right of the stationary point.Therefore, the optimal constraint i is simply the optimal unconstrained value, projected into thefeasible space. In particular, solving 1i= (y\\iu yi)2 for i and projecting to the non-negativehalf-line, we get",
  "Therefore, all LOO predictive values can be computed in O(n3) or faster, if an inducing point methodis used for K": "The following is a preliminary result for our analysis of the log marginal likelihood w.r.t. .Lemma 12. The gradient and Hessian of the log marginal likelihood L with respect to are given by2[L] = diag(K1 ),and 2H[L] = (2 K1) K1,where K = K0 + D for some base covariance matrix K0 and = K1y.",
  "C.1Strong Convexity and Smoothness of the Reparameterized Robust Marginal Likelihood": "Here, we re-parameterize (s) = diag(K0) ((1 s)1 1), and attain strong convexity for allinputs s, if conditions on the eigenvalues of the covariance matrix and the norm of the data vector yare met. The convexity result is surprising in two ways: the negative log marginal likelihood of GPsis generally a non-convex function, and in addition, the negative log likelihoods of many alternativerobust regression methods like the Student-t likelihood or -stable likelihoods are non-convex, andeven Hubers proposal is non-strongly-convex.",
  "Now, we bound each of the three additive terms independently from below": "Term 1:2 diag( K1) 2min( K1)I = 2max( K)1I.The first inequality comes from K being positive definite, and the absolute value of the diagonal of amatrix, which is already positive for positive definite matrices, being lower bounded by the minimumeigenvalue of the matrix. The last steps is a basic consequence of the eigenvalues of inverses matrices.Note that the eigenvalues of K can be further bound by the eigenvalues of the original matrix K:",
  "((1 + )/(1 ) 1),": "where we used (2min 21max) 1, which is tight when K is unitary. Lower bounding the lastexpression by y22 implies M-smoothness. This gives rise to a non-trivial guarantee whenever thenumerator is larger than zero. In particular, M(1smax)21 > 0, which implies smax 1 1/Mor equivalently, M > 1/(1 smax)2. Lastly, note that if K0 is -diagonally dominant, then so isKs = K0 + D(s), since the robust variances only add to the diagonal. Therefore, if the inequalityholds for the base covariance matrix K0, the smoothness guarantee holds for all (s) such thats smax 1",
  "/M. Note also that min(K) 2": "Theorem 8. [Approximation Guarantee] Let K0 = k(X, X) + 2I be -diagonally dominant,smax > 0 be an upper bound on s, and suppose y, satisfy the bounds of Lemmas 7 and 14,guaranteeing m-convexity and M-smoothness of the NMLL for some m > 0, M > 1/(1 smax)2.Let sOMP(r) be the r-sparse vector attained by OMP on the NMLL objective for r steps, and letsOPT(r) = arg maxs0=r,ssmax L((s)) be the optimal r-sparse vector. Then for any 2r n,",
  "D.1Synthetic Regression Problems": "Model fitting runtimes summarizes the fitting times for the different models on the differentscenarios from .1. We observe that the outlier type and the fraction of outliers both haverelatively limited effect on the fitting times for all of the models. therefore provides a morecompact view of the same data (aggregated across outlier types and outlier fractions), giving a bettersense of the distribution of the fitting times. Unsurprisingly, the baselines that simply fit a single GPmodel (vanilla, winsorize, power_transform) are substantially faster than any of the robustapproaches. While all of the robust models show similar fitting times on the Hartmann problem, fittingour RRP is significantly faster (note the logarithmic scale of the y-axis) than fitting the Student-t andtrimmed MLE models on the 5-dim and 10-dim Friedman problems. The trimmed MLE model inparticular ends up being quite slow, especially on the 5d Friedman function.",
  "For Hartmann6, we consider the standard domain of 6": "SVMThe SVM problem, the goal is to optimize the test RMSE of an SVM regression modeltrained on 100 features from the CT slice UCI dataset. We tune the following three parameters: C [1e-2, 1e2], [1e-2, 1], and [1e-3, 1e-1]. All parameters are tuned in log-scale. Corruptionssimulate I/O failures in which case we only train on U training points out of the available50, 000 training observations. CNNFor the 5D CNN problem the goal is to optimize the test accuracy of a CNN classifier trainedon the MNIST dataset. We tune the following 5 parameters: learning rate in the interval [1e-4, 1e-1],momentum in the interval , weight decay in the interval , step size in the interval ,and . Similarly to the SVM problem, we only train on U of the available trainingbatches when an I/O failure occurs. RoverThe rover trajectory planning problem was originally proposed in Wang et al. . Thegoal is to tune the trajectory of a rover in order to maximize the reward of its final trajectory. Weuse the same obstacle locations and trajectory parameterization as in Maus et al. , with themain difference being that we parameterize the trajectory using 10 points, resulting in 20 tunableparameters. When a corruption occurs, the rover will stop at a uniformly random point along itstrajectory, generally resulting in lower reward than the original trajectory.",
  "D.4Computational Setup and Requirements": "Robust GP regression is very data-efficient, focuses on the small-data regime, and runs fast (fasterthan competing baselines studied in this paper). Therefore, each individual run required very limitedcompute resources (this includes the baseline methods). To produce statistically meaningful results,however, we ran a large number of replications for both our regression and Bayesian optimizationbenchmarks on a proprietary cluster. We estimate the amount of compute spent on these experimentsto be around 2 CPU years in total, using standard (Intel Xeon) CPU hardware. The amount ofcompute spent on exploratory investigations as part of this work was negligible (this was ad-hocexploratory and development work on a single CPU machine). fit time (s) friedman10, constantfriedman10, student-tfriedman10, uniform fit time (s) friedman5, constantfriedman5, student-tfriedman5, uniform 0.050.100.150.200.25",
  "D.5Impact of Convex Parameterization on Joint Optimization of Hyper-Parameters": "A limitation of Lemma 4 is that it only guarantees convexity for the sub-problem of optimizing theis. In practice, we jointly optimize all GP hyper-parameters, including length scales. In this case,the theory guarantees the positive-definiteness of the submatrix of the Hessian corresponding to (s),and we expect this to improve the quality of the results of the numerical optimization routines. Indeed, positive-definiteness is beneficial for quasi-Newton optimization algorithms like L-BFGS ,which restarts its approximation to the Hessian whenever it encounters non-convex regions, becausethe associated updates to the Hessian approximation are not positive-definite. This leads the algorithmto momentarily revert back to gradient descent, with an associated slower convergence rate.",
  "To quantify the impact of this, we ran convergence analyses using the data from , allowingall to be optimized jointly with other hyper-parameters (length scale, kernel variance and noise": "fit time (s) friedman5 fit time (s) friedman10 fit time (s) hartmann6 RRPRRP (bkwd)RRP (can) RRP (bkwd, can)Student-tPower Transf. Standard GPAdapt. Wins.Trimmed MLL : Fitting times of the different robust GP modeling approaches on the regression tasks from.1. Results are aggregated across outlier types and outlier fractions as those do not affectfitting times much (see ). Number of Evaluations 3.0 2.5 2.0 1.5 1.0 0.5 Value of Best Inferred Point Constant outliers value 100 (20%) Number of Evaluations Uniform outliers (20%) Number of Evaluations Uniform input outliers (20%)",
  "D.6Comparison to Robust and Conjugate Gaussian Processes (RCGP)": "We report extended empirical comparisons with Altamirano et al. s RCGP method, using theirexperimental setup and method implementation in GPFLow. Including GPFlow in our own bench-marking setup and compute resources proved difficult. To circumvent this, we wrote wrappers forboth BoTorchs standard GP and RRP, which also accounts for any orthogonal implementationdifferences between the two frameworks, and ran the benchmarks locally on an M-series MacBook. See Tables 2 and 3 for the mean absolute error and negative log predictive density, respectively. Thetables include the empirical mean and standard deviation over 20 replications on corrupted version ofthe following base data sets: 1) Synthetic, which is generated as a draw of a GP with a exponentiatedquadratic kernel, and four data sets available on the UCI machine learning repository, in particular,2) Boston , 3) Concrete , 4) Energy , and 5) Yacht . The benchmark considers nocorruptions (No Outliers), Asymmetric Outliers, which are uniform in x are shifted negativelyin y, Uniform Outliers, which shift y in both directions (positively and negatively), and FocusedOutliers, which form concentrated clusters in both x and y. Any bold entry in the table signifies aresults that is within one standard-error of the best results one standard-error confidence bound. : Mean absolute error (MAE) using Altamirano et al. s experimental setup in GPFlow.RRP is always competitive with the other methods, and outperforms them significantly for uniformand asymmetric outliers.",
  "Standard GP (GPFlow)Student-t GP (GPFLow)RCGP (GPFlow)Standard GP (BoTorch)RRP (BoTorch)": "No OutliersSynthetic-8.21e-1 (2.16e-2)-8.20e-1 (2.16e-2)-8.23e-1 (2.21e-2)-8.05e-1 (2.25e-2)-8.05e-1 (2.24e-2)Boston2.08e-1 (4.26e-2)1.99e-1 (3.76e-2)1.95e-1 (4.69e-2)9.24e-2 (2.82e-2)9.43e-2 (2.82e-2)Concrete1.48e-1 (2.33e-2)1.29e-1 (2.31e-2)1.09e-1 (3.19e-2)1.11e-1 (2.73e-2)1.16e-1 (2.68e-2)Energy-1.72e+0 (3.83e-2)-1.62e+0 (3.46e-2)-1.96e+0 (4.37e-2)-1.67e+0 (3.08e-2)-1.69e+0 (4.02e-2)Yacht-1.79e+0 (3.23e-1)-2.05e+0 (6.61e-2)-2.00e+0 (3.27e-1)-2.47e+0 (1.06e-1)-2.23e+0 (1.84e-1) Uniform OutliersSynthetic1.60e+0 (1.41e-2)1.47e+0 (1.55e-2)1.57e+0 (1.39e-2)1.58e+0 (1.40e-2)-7.99e-1 (2.26e-2)Boston1.67e+0 (7.55e-3)1.54e+0 (8.98e-3)1.64e+0 (1.12e-2)1.81e+0 (5.95e-2)4.52e-1 (1.34e-1)Concrete1.64e+0 (6.83e-3)1.51e+0 (6.56e-3)1.63e+0 (7.57e-3)1.63e+0 (6.72e-3)1.47e-1 (2.51e-2)Energy1.61e+0 (7.75e-3)1.50e+0 (1.76e-2)1.61e+0 (9.74e-3)1.70e+0 (8.58e-3)-1.67e+0 (3.72e-2)Yacht1.61e+0 (9.50e-3)1.46e+0 (1.19e-2)1.62e+0 (1.87e-2)1.78e+0 (4.67e-2)-2.32e+0 (1.97e-1) Asymmetric OutliersSynthetic1.94e+0 (9.39e-3)1.90e+0 (9.98e-3)1.89e+0 (1.13e-2)1.93e+0 (1.06e-2)-3.70e-1 (2.30e-1)Boston1.68e+0 (1.01e-2)1.56e+0 (1.09e-2)1.64e+0 (9.51e-3)2.52e+0 (7.26e-1)6.14e-1 (1.19e-1)Concrete1.66e+0 (7.43e-3)1.54e+0 (7.45e-3)1.62e+0 (6.63e-3)1.65e+0 (7.55e-3)1.55e-1 (2.86e-2)Energy1.62e+0 (8.85e-3)1.49e+0 (9.70e-3)1.62e+0 (1.82e-2)1.71e+0 (9.79e-3)-1.66e+0 (3.55e-2)Yacht1.59e+0 (9.35e-3)1.45e+0 (1.01e-2)1.56e+0 (8.15e-3)1.75e+0 (1.41e-2)-2.17e-1 (1.82e+0) Focused OutliersSynthetic6.78e-1 (6.59e-2)6.11e-1 (8.68e-2)5.69e-1 (4.07e-2)6.66e-1 (6.60e-2)9.67e+0 (2.24e+0)Boston2.57e-1 (4.23e-2)3.21e-1 (6.40e-2)2.74e-1 (6.13e-2)1.98e-1 (3.04e-2)1.97e-1 (3.06e-2)Concrete2.77e-1 (2.58e-2)2.72e-1 (2.81e-2)2.43e-1 (4.26e-2)2.22e-1 (2.87e-2)2.28e-1 (2.72e-2)Energy2.81e+4 (2.81e+4)-1.67e+0 (4.13e-2)-1.74e+0 (4.98e-2)-1.70e+0 (4.59e-2)-1.70e+0 (4.59e-2)Yacht9.38e+4 (5.09e+4)-4.60e-1 (2.91e-1)-2.47e+0 (8.09e-2)-2.63e+0 (5.55e-2)-2.61e+0 (5.66e-2)",
  "D.7Comparison to Robust Gaussian Process with Huber Likelihood": "In the following, we compare our method with additional variational GP baselines with Laplace andHuber likelihoods, and translated the Matlab code of the \"projection statistics\" of Algikar and Mili to PyTorch. We then combined the projection-statistics-based weighting of the Huber loss with : Negative log predictive density (NLPD) using Altamirano et al. s experimental setup inGPFlow. RRP is generally competitive, and outperforms other methods significantly for uniform andasymmetric outliers.",
  "a variational (referred to as Huber-Projection) to get as close as possible to a direct comparison toAlgikar and Mili without access to a Matlab license": "Tables 4 and 5 shows the root mean square error and negative log predictive density on the Neal,Friedman 5 and Friedman 10 test functions, as well as the Yacht Hydrodynamics and CaliforniaHousing datasets from the UCI database , where 15% of the training data sets of the modelswere corrupted. Tables 6 and 7 below were generated in a similar way, but 100% of the data weresubject to heavier-tailed noise, either Student-t or Laplace. In summary, the Relevance Pursuit model generally outperforms the variational GPs with heavy-tailedlikelihoods when the corruptions are a sparse subset of all observations. Unsurprisingly, the GPs withheavy-tailed likelihoods perform best when all observations are subject to heavy-tailed noise. Whilesuch uniformly heavy-tailed noise does exist in practice, we stress that this is a distinct setting tothe common setting where datasets contain a subset of a-priori unknown outliers, while a dominantfraction of the data can be considered inliers that, once they are identified, can be used to train amodel without additional treatment.",
  "DataStandardRelevance PursuitStudent-tLaplaceHuberHuber + Projection": "NealStudent-t1.95e+0 (1.22e-1)8.78e+1 (2.80e+1)1.17e+0 (6.08e-2)1.22e+0 (5.71e-2)1.20e+0 (5.17e-2)1.21e+0 (5.15e-2)Laplace1.89e+0 (1.14e-1)1.38e+2 (3.98e+1)1.57e+0 (9.22e-2)1.55e+0 (9.20e-2)1.55e+0 (9.12e-2)1.55e+0 (9.12e-2)Friedman 5Student-t4.43e+0 (8.19e-2)3.93e+0 (2.19e-2)3.11e+0 (2.69e-2)2.96e+0 (3.49e-2)2.97e+0 (3.27e-2)2.97e+0 (3.27e-2)Laplace4.55e+0 (2.81e-2)4.56e+0 (1.88e-2)3.64e+0 (4.03e-2)3.46e+0 (4.45e-2)3.48e+0 (4.31e-2)3.48e+0 (4.31e-2)Friedman 10Student-t4.60e+0 (8.54e-2)3.89e+0 (1.36e-2)3.04e+0 (2.45e-2)2.83e+0 (2.30e-2)2.84e+0 (2.22e-2)2.84e+0 (2.22e-2)Laplace4.58e+0 (1.25e-2)4.57e+0 (1.27e-2)3.60e+0 (2.53e-2)3.32e+0 (3.01e-2)3.37e+0 (2.61e-2)3.37e+0 (2.61e-2)YachtStudent-t5.14e+0 (1.85e-1)4.60e+0 (8.15e-2)7.30e+0 (1.40e-1)6.79e+0 (1.52e-1)6.70e+0 (1.32e-1)1.28e+1 (7.75e-1)Laplace4.74e+0 (4.45e-2)5.30e+0 (8.31e-2)4.91e+0 (8.87e-2)5.19e+0 (1.17e-1)5.12e+0 (1.04e-1)9.90e+0 (6.44e-1)CA HousingStudent-t2.08e+0 (1.01e-1)1.79e+0 (6.40e-2)7.41e+0 (2.39e-1)6.52e+0 (2.54e-1)6.41e+0 (2.07e-1)6.83e+0 (3.28e-1)Laplace1.88e+0 (5.25e-2)2.24e+0 (9.41e-2)2.92e+0 (6.01e-2)3.33e+0 (7.47e-2)3.27e+0 (5.84e-2)6.64e+0 (3.28e+0)",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  "Answer: [Yes]": "Justification: The only new asset introduced in this paper is the code for method andbenchmarks; it does not introduce any other assets (data sets or models). The code isprovided as an anonymized zip file for the reviewers. Upon publication, the code released asopen source and documented according to community standards.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}