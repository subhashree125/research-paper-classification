{
  "Abstract": "Group equivariance has emerged as a valuable inductive bias in deep learning,enhancing generalization, data efficiency, and robustness. Classically, group equiv-ariant methods require the groups of interest to be known beforehand, which maynot be realistic for real-world data. Additionally, baking in fixed group equivariancemay impose overly restrictive constraints on model architecture. This highlightsthe need for methods that can dynamically discover and apply symmetries as softconstraints. For neural network architectures, equivariance is commonly achievedthrough group transformations of a canonical weight tensor, resulting in weightsharing over a given group G. In this work, we propose to learn such a weight-sharing scheme by defining a collection of learnable doubly stochastic matricesthat act as soft permutation matrices on canonical weight tensors, which can takeregular group representations as a special case. This yields learnable kernel trans-formations that are jointly optimized with downstream tasks. We show that whenthe dataset exhibits strong symmetries, the permutation matrices will converge toregular group representations and our weight-sharing networks effectively becomeregular group convolutions. Additionally, the flexibility of the method enables it toeffectively pick up on partial symmetries.",
  "Introduction": "Equivariance has emerged as a beneficial inductive bias in deep learning, enhancing performanceacross a variety of tasks. By constraining the function space to adhere to specific symmetries, modelsnot only generalize better but also achieve greater parameter efficiency . For instance, integratinggroup symmetry principles into generative models has enhanced sample generation and efficientlearning of data distributions, particularly benefiting areas such as vision and molecular generation. The most well-known and transformative models in equivariant deep learning are convolutional neuralnetworks (CNNs) , which achieve translation equivariance by translating learnable kernels toevery position in the input. This design ensures that the weights defining the kernels are shared acrossall translations, so that if the input is translated, the output features are correspondingly translated;in other words, equivariance is achieved through weight sharing. In the seminal work by Cohenand Welling , this concept was extended to generalize weight-sharing under any discrete group ofsymmetries, resulting in the group-equivariant CNN (G-CNN). G-CNNs enable G-equivariance to abroader range of symmetries, such as rotation, reflection, and scale , thereby expandingthe applicability of CNNs to more complex data transformations.",
  "arXiv:2412.04594v1 [cs.LG] 5 Dec 2024": "However, the impact of G-CNNs is closely tied to the presence of specific inductive biases in thedata. When exact symmetries, such as E(3) group symmetries in molecular point cloud data, areknown to exist, G-CNNs excel. Yet, for many types of data, including natural images and sequencedata, these exact symmetries are not present, leading to overly constrained models that can suffer inperformance . In scenarios with limited data and for certain critical downstream tasks,having appropriate inductive biases becomes even more crucial. To avoid overly constraining models, symmetries must be chosen carefully to match those in theinput data. This requires prior knowledge of these symmetries, which may not always be available.Furthermore, different symmetries at different scales can coexist, making manual determination ofthese symmetries highly impractical. To address this, multiple works have proposed partial or relaxedG-CNNs . Such models are initialized to be fully equivariant to some groups and learnfrom the data to partially break equivariance on a per-layer basis where necessary. However, thesemethods still require specifying which symmetries to include and can only achieve equivariance tosubsets of these symmetries. In this work, we tackle the challenge of specifying group symmetries upfront by introducing a generalweight-sharing scheme. Our method can represent G-CNNs as a special case but is not limited toexact equivariance constraints, offering greater flexibility in handling various symmetries in the data.Inspired by the idea that group equivariance for finite groups can be achieved through weight-sharingpatterns on a set of base weights , we propose learning the symmetries directly from the data on aper-layer basis, requiring no prior knowledge of the possible symmetries. We leverage the fact that regular group representations act as permutations and that the expectationof random variables defined over this set of permutations is a doubly stochastic matrix . Thisimplies that regular partial group transformation can be approximated by a stack of doubly stochasticmatrices which essentially act as (soft) permutation matrices. Consequently, we learn a set of doublystochastic matrices through the Sinkhorn operator , resulting in weight-sharing under learnablegroup transformations.",
  "Related work": "Partial or relaxed equivarianceMethods such as learn partial equivariance by learningdistributions over transformations, and thereby aim to learn partial or relaxed equivariances fromdata by sampling some group elements more often than others. tries to relax equivariance byintroducing learnable equivariance-breaking components. Relax equivariance constraintsby parameterizing layers as (linear) combinations of fully flexible, non-constrained componentsand constrained equivariant components. Finally, several works model soft invariances throughlearning the amount of data augmentation in the data or model relevant for a given task, either throughlearned distributions on the group or (automatic) hyperparameter selection . However,these methods require pre-specified sets of symmetry transformations and/or group structure to beknown beforehand. In contrast, we aim to pick up the relevant symmetry transformations duringtraining. Symmetry discovery methods learn the group structure via (irreducible) group represen-tations. proposed to learn the Fourier transform of finite compact commutative groups and theircorresponding bispectrum by learning to separate orbits on our dataset. This approach can be extendedto non-commutative finite groups leveraging advanced unitary representation theory . However, these methods are constrained to finite-dimensional groups and require specific orbit-predictingdatasets. In contrast, our approach learns a relaxation of regular group representationsas opposedto irreducible representations. Moreover, our approach is not merely capable of learning symmetries,it subsequently utilizes them in a regular group-convolution-type architecture. Weight-sharing methodsPrevious studies have demonstrated that equivariance to finite groupscan be achieved through weight-sharing schemes applied to model parameters. Notably, the worksin , , and provide foundational insights into this approach. In , weight-sharingpatterns are learned by using a matrix that operates on flattened canonical weight tensors, effectivelyinducing weight sharing. They additionally prove that for finite groups, there are weight-sharingmatrices capable of implementing the corresponding group convolution. However, their approachrequires learning these patterns through meta-learning and modeling the weight-sharing matrix asan unconstrained tensor. In contrast, our method learns weight sharing directly in conjunction withthe downstream task and enforces the matrix to be doubly stochastic, thereby representing softpermutations by design. presents an approach closely aligned with ours, where a weight-sharing scheme is learned thatis characterized by row-stochastic entries. Their method involves both inner- and outer-loop opti-mization and demonstrates the ability to uncover relevant weight-sharing patterns in straightforwardscenarios. However, their approach does not support joint optimization of the canonical weights andweight-sharing pattern, and they acknowledge difficulties in extending their method to higher inputdimensionalities. Unlike , we enforce both row and column stochasticity. Additionally, we canoptimize for the sharing pattern and weight tensors jointly, and successfully apply our approach tomore interesting data domains such as image processing. In , group actions are integrated directly into the learning process of the downstream task. Thismethod involves learning a set of generator matrices that operate via matrix multiplication on flattenedinput vectors. However, this approach constrains the operators to members of finite cyclic groups,which inherently limits their ability to represent more complex group structures. Furthermore, thisrestriction precludes the possibility of modeling partial equivariances, reducing the flexibility andapplicability of the model to more diverse or complex scenarios.",
  "Background": "We begin by revisiting group convolutional methods in the context of image processing, followed bytheir relation to weight-sharing schemes. We then proceed to briefly cover the Sinkhorn operator,which is the main mechanism through which we acquire weight-sharing schemes. Some familiaritywith group theory is assumed, and essential concepts will be outlined in the following. GroupsWe are interested in (symmetry) groups, which are algebraic constructs that consist of a setG and a group productwhich we denote as a juxtapositionthat satisfies certain axioms, such asthe existence of an identity element e G such that for all g G we have eg = ge = g, closure suchthat for all g, h G we have gh G, the existence of an inverse g1 for each g such that g1g = e,and associativity such that for all g, h, i G we have (gh)i = g(hi). RepresentationsIn the context of geometric deep learning , it is most useful to think of groupsas transformation groups, and the group structure describes how transformations relate to each other.Specifically, group representations : G GL(V ) are concrete operators that transform elementsin a vector space V in a way that adheres to the group structure (they are group homomorphisms).That is, to each group element g, we can associate a linear transformation (g) GL(V ), withGL(V ) the set of linear invertible transformations on vector space V . Group convolutionConcretely, such representations can be used to define group convolutions.Consider feature maps f : X RD over some domain on which a group action is defined, i.e., overa G-space. E.g., for images (signals over X = R2) we could consider the group G = (R2, +) oftranslations, which acts on X via gx = x + y, with g = (y) a translation by y R2. While the groupG merely defines how two transformations g, h G applied one after the other correspond to a nettranslation gh G, a representation concretely describes how data is transformed. E.g., signals",
  "R2 k(x x)f(x)dx .(3)": "Semi-direct product groupsWhen equivariance to larger symmetry groups is desired, e.g. inthe case of G = SE(2) roto-translation equivariance for images with domain X = R2, a liftingconvolution can be used to generate signals over the group G. In essence it is still of the form of (2),however integration is over X instead of over G:",
  "R2 k(R1(x x))f(x)dx ,(5)": "with g = (x, R) (R2, +) SO(2). The roto-translation group is an instance of a semi-directproduct group (denoted with ) between the translation and rotation group, which has the practicalbenefit that a stack of rotated kernels can be precomputed , and the translation part efficiently betaken care of via optimized Conv2D operators. Namely via (k f)(x, Ri) = Conv2D[ki, f], withki := k(R1i x). This trick can also be applied for full group convolutions (2).",
  "Method": "Our objective is to uncover the underlying symmetries of datasets whose exact symmetries maynot be known, in a manner that is both parameter-efficient and free from rigid group constraints.To achieve this, we re-examine regular representations and their critical role in generating variousinstantiations of the fundamental group convolution equation (1). Moving from the continuous settingto concrete instantiations, we derive weight-sharing from a finite-dimensional vector of base weightsby interpreting regular representations as permutations. We further analyze the characteristics of thisweight-sharing approach, proposing the use of doubly stochastic matrices. This analysis forms thefoundation for developing weight-sharing layers that adaptively learn dataset symmetries.",
  "Weight-sharing through permutations": "In the current section, we first establish the connection between representations, weight-sharing andpermutations. We then proceed to provide practical instantiations as ingredients for the proposedweight-sharing convolutional layers. Weight-sharing through learnable representationsTo achieve weight-sharing over a finite setof symmetries, we define learnable representations : G GL(V ). Specifically, we assign alearnable transformation (permutation matrix) to each element in G. It is important to note that werefer to G and as a \"group\" and \"representation\" in a loose sense, as we relax the homomorphismproperty and do not initially endow G with a group product. Consequently, the collection of lineartransformations does not form a group representation a priori. However, our proposed method iscapable of modeling this structure in principle. 2Due to the equivalence between convolution and correlation via kernel reflection, we henceforth simplyrefer to operators of the type of (2) as convolution even though technically they are cross-correlations. Regular representations as permutationsConsider the case of a continuous linear Lie group G,e.g. of rotations SO(2), and a real signal f : G R over it. This signal is to be considered aninfinite dimensional vector with continuous \"indices\" g G that index the vector elements f(g). Toemphasize the resemblance of the regular representation to permutation matrices we write it as",
  "with Pg(i, h) = g1i(h) a kernel that for each g maps h to a new \"index\" i G, where the Diracdelta essentially codes for the group product as g1h(i) is non-zero only if g1i = h g h = i": "The discrete counterpart of such an integral transform is matrix-vector multiplication with a matrix(g) = Pg with entries Pghi = 1 if gh = i and zero otherwise. As a concrete example, consider asignal f : G R over the discrete group G = C4 of cyclic permutations of size 4, i.e., a periodicsignal of length four, then we could vectorize it as f v with entries vg = f(g) and the regularrepresentation becomes a simple cyclic permutation matrix with",
  "We refer to the collection of permutation matrices P |G||G||G| as the permutation tensor": "Recall that the regular convolution operator (2) is not only defined for signals over groups G but forG-spaces X, in general, as in Eq. (4). It requires a representation that acts on signals (convolutionkernels) over X. However, since the group G acts by automorphisms (bijections) on the space X, wecan define the regular representationas beforeusing permutation integral with Pg(x, x) = g1xthat effectively sends old \"indices\" x to their new location x, or concretely via permutation matricesP of shape |G| |X| |X|, where X is the domain of the signal that is transformedwhich canalso be G. In practice, it is common that we perform a discretization of continuous signals f, e.g., when wework with images, we discretize the continuous signal f : R2 RC to f : Z2 RC. Therefore,the group action over the discretized signal should approximate the group action over the continuoussignal. We can see that in some cases, the group representation of the action on the discrete signalcan still be implemented using permutation matrices. E.g., 90 rotations (in C4) applied to imagesmerely permute the pixels. However, for finer discretizations, e.g. using 45 rotations, interpolationcan be used as a form of approximate permutations [16, ].",
  ": Comparison of C4 representationsand the representation stack learned by the lift-ing layer on the rotated MNIST dataset. Top:learned representations. Bottom: permutationsfor C4 on d = 25": "Our proposal: Weight Sharing Convolutional Neural NetworksHaving established the foun-dational elements, we now define our Weight Sharing Convolutional Neural Networks (WSCNNs).We let li R|X||X| be a collection of learnable parameters that parametrize the representationstack of the layer l as Rl = (SK(l0), SK(l1), ..., SK(lN))T |G||X||X|. i.e., we pa-rameterize this tensor as a stack of |G| approximate doubly stochastic |X|-dimensional matrices,wherein stochasticity is enforced via K applications of the Sinkhorn operator. We also define a set oflearnable base weights l R|X|CoutCin. The WSCNN layer is then simply given by (7) with Pand w respectively replaced by Rl and l. We further note that on image data |X| can be large, making the discrete matrix form implementationcomputationally demanding. Hence, we consider semi-direct product group parametrizations forG, in which we let G be of the form (Rn, +) H, with H a learnable (approximate) group. Then,the representation stacks will merely be of shape |H| |X| |X|, with |H| |G| the size of thesub-group and |X| the number of pixels that support the convolution kernel. A WSCNN layer isthen efficiently implemented via a Conv2D[f, Rll]. For group convolutions (after the lifting layer)the representation stacks will be of shape |H| (|X| |H|) (|X| |H|). Computational scalingrequirements can be found in Appendix C.4.",
  "Experiments": "We first demonstrate that the proposed weight sharing method can effectively pick up on usefulweight sharing patterns when trained on image datasets with different equivariance priors. We thenproceed to show the method can effectively handle settings where partial symmetries are present inthe data, and further analyze the learned weight-sharing structures on a suite of toy datasets. Modelarchitectures, regularizers (norm, ent) and design choices can be found in Appendix C.4 and C.1,respectively. An analysis of computational requirements can be found in Appendix C.4",
  "Image datasets and equivariance priors": "We assess the efficacy of our proposed weight-sharing scheme in recognizing data symmetries throughexperiments on datasets subjected to various data augmentations. Specifically, we evaluate our modelon MNIST images that have been rotated (with full SO(2) rotations) and scaled (with scalingfactors between [0.3, 1.0]). We categorize these datasets based on their data symmetry characteristics:MNIST with rotation and scaling as datasets with known symmetries, and CIFAR-10 with flips as adataset with unknown symmetries. For RotatedMNIST, we regard a C4-group convolutional model as a benchmark since it has beenequipped with a subgroup of the underlying data symmetries a priori. Additionally, we contrastour results with a non-constrained CNN model which has double the number of channels, allowingfor free optimization without symmetry constraints. As such, our evaluations are benchmarkedagainst two distinct models: 1) a group convolutional model that is equivariant to discrete rotations,embodying fixed equivariance constraints, and 2) a standard CNN that adheres only to translationalequivariance, without additional constraints. : Test accuracy on MNIST for both rotation and scaling transformations. Additional parametersinduced by weight-sharing are marked (+). Parameter counts denoted in millions (M) or thousands(K). Best-performing models (equivalent within < 1%) marked bold.",
  "WSCNN + norm1.63 M (+ 468 K)478.80 0.46WSCNN + norm + ent1.63 M (+ 468 K)476.80 1.40": "This experimental setup positions the standard CNN as the most flexible model lacking predefinedinductive biases. In contrast, the group convolutional neural network (GCNN) is characterized byfixed weight sharing, while our proposed weight-sharing CNN (WSCNN) introduces a semi-flexible,learnable weight-sharing mechanism. Note that we explicitly distinguish between the number of freemodel and kernel parameters and the additional parameters introduced by our weight sharing schemethroughout results (marked by +). Results can be found in Tab. 1. When there is a clear misalignment between the model and datasymmetries, the constraints imposed by the model hinder performance, as demonstrated by theC4-GCNN on the scaled MNIST dataset. Notably, our proposed method consistently achieves highperformance across all datasets without requiring fixed group specifications. Furthermore, visualinspection of the learned kernels indicates that WSCNN adapts to the underlying data symmetries byeffectively rotating kernels, as shown in . Additionally, analysis of the learned representationstack reveals that it closely resembles elements of C4 permutations, further demonstrating the modelscapability to internalize and replicate data transformations (see and Appendix B.3). Additionally, we test the model on CIFAR-10, representing a dataset with possibly more com-plex/unknown symmetry structures, and CIFAR-10 with horizontal flips (which cannot be representedby CN transformations). Results can be found in Tab. 2, with detailed training and model specifica-tions available in Appendix C.4. We compare against the (possibly misspecified) C4-GCNN, andseveral unconstrained CNNs models with varying number of channels: 1) CNN-32 matched in freekernel size, 2) CNN-64 matched in parameter budget, and 3) CNN-128 matched in effective kernelsize (calculated as |G| channels = 4 32 = 128). Despite the kernel constraints in WSCNN,it achieves performance comparable to that of the unconstrained 128-channel CNN (within < 1%accuracy), at a significantly smaller parameter budget (2.1 M vs. 6.5 M).",
  "Learning partial equivariances": "We show our method is able to pick up on partial symmetries by testing it on MNIST with rotationssampled from a subset of SO(2) and compare it to the C4-GCNN. Additionally, we show results onCIFAR-10 with horizontal flips, which is a commonly used train augmentation. Results can be foundin Tab. 3 and Tab. 4.",
  "WSCNN1.6 M (+ 468 K)482.38 .003": "Additionally, We proceed to test the models capability to detect data symmetries by applying itto a suite of toy problems, wherein the datasets comprise noisy G-transformed samples. Detailson the data generation framework are provided in Appendix B. Our testing employs a single-layersetup aimed at learning a collection of kernels that ideally match each data sample, consideringinherent noise. This involves training the model to identify a set of base kernels and their pertinenttransformations, effectively adapting to the variations presented by the toy problems.",
  "Discussion and Future Work": "We demonstrated a method that can effectively identifies underlying symmetries in data, even withoutstrict group constraints. Our approach is uniquely capable of learning both partial and approximatesymmetries, which more closely mirrors the complexity found in real-world datasets. Utilizingdoubly stochastic matrices to adapt kernel weights for convolutions, our method offers a flexiblemeans of learning representation stacks, accommodating both known and unknown structures withinthe data. This adaptability makes it possible to detect useful patterns, although these may not alwaysbe interpretable in traditional group-theoretic terms due to the absence of predefined structures in therepresentation stack. Limitations include computational requirements, which scale quadratically with the size of the groupand the kernel size, posing challenges in scenarios with large groups or high-resolution data. Assuch, in this work we have designed the representation stack to be uniform across the channeldimension. However, this prevents learning of other commonly used image transformations such ascolor jitter. Furthermore, the need for task-specific regularization to manage entropy scaling duringthe learning of representations introduces complexity in hyperparameter tuning, which can be abarrier in some applications. Additionally, we observed that representations in later layers may showminimal diversity, suggesting that further innovation in regularization strategies might be necessaryto enhance the distinctiveness of learned features across different layers. For future work, we aim to enhance our method by implementing hierarchical weight-sharing acrosslayers and promoting group equivariance more systematically. One promising direction is to leveragethe concept of a Cayley tensor, akin to , to identify and reuse learned group structures acrossdifferent layers of the network. This approach would not only impose a more unified and coherentgroup structure within the model but also potentially reduce the computational overhead associatedwith learning separate representations for each layer. By encouraging a shared group structurethroughout the network, we anticipate improvements in both performance and interpretability, pavingthe way for more robust and efficient symmetry-aware learning systems.",
  "Acknowledgements": "SV and AGC are supported by the Hybrid Intelligence Center, a 10-year program funded by theDutch Ministry of Education, Culture and Science through the Netherlands Organisation for ScientificResearch. The computational results presented have been achieved in part using the Snellius clusterat SURFsara. Additionally, we thank Alexander Timans for helpful discussions.",
  "Daiki Asakura. An infinite dimensional birkhoffs theorem and locc-convertibility. IEICETechnical Report; IEICE Tech. Rep., 2016": "Erik J. Bekkers, Maxime W. Lafarge, Mitko Veta, Koen A. J. Eppenhof, Josien P. W. Pluim, andRemco Duits. Roto-translation covariant convolutional networks for medical image analysis.CoRR, abs/1804.03393, 2018. URL Erik J Bekkers, Sharvaree Vadgama, Rob Hesselink, Putri A Van der Linden, and David W.Romero.Fast, expressive se(n) equivariant networks through weight-sharing in position-orientation space. In The Twelfth International Conference on Learning Representations, 2024.URL",
  "J.R. Isbell.Infinite Doubly Stochastic Matrices.Canadian Mathematical Bul-letin,5(1):14,January 1962.ISSN 0008-4395,1496-4287.doi:10.4153/CMB-1962-001-4.URL": "David G. Kendall. On Infinite Doubly-Stochastic Matrices and Birkhoffs Problem 111. Journalof the London Mathematical Society, s1-35(1):8184, 1960. ISSN 1469-7750. doi: 10.1112/jlms/s1-35.1.81. URL _eprint: David M. Knigge, David W Romero, and Erik J Bekkers. Exploiting redundancy: Separablegroup convolutional networks on lie groups. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th InternationalConference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,pages 1135911386. PMLR, 1723 Jul 2022. URL Maxime W Lafarge, Erik J Bekkers, Josien PW Pluim, Remco Duits, and Mitko Veta. Roto-translation equivariant convolutional networks: Application to histopathology image analysis.Medical Image Analysis, 68:101849, 2021.",
  "Daniel E. Worrall and Max Welling. Deep scale-spaces: Equivariance over scale. CoRR,abs/1905.11697, 2019. URL": "Raymond A. Yeh, Yuan-Ting Hu, Mark Hasegawa-Johnson, and Alexander Schwing. Equivari-ance discovery by learned parameter-sharing. In Gustau Camps-Valls, Francisco J. R. Ruiz, andIsabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelli-gence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 15271545. PMLR, 2830 Mar 2022. URL",
  "where I is an index set (possibly with repetitions) over G": "Similarly to what is showed in , a representation : G Rdd can be viewed as acollection of d2 functions over G. The Peter-Weyl theorem asserts that the collection of functionsformed by the matrix entries of all irreps in G spans the space of all square-integrable functions overG. For most groups, these entries form an orthogonal basis, allowing any function f : G R to bewritten as:f(g) =",
  "Q": "where Q performs the Fourier transform, while Q1 performs the inverse Fourier transform. Thisimplies that when functions f : G R are considered as vectors in R|G|, with a basis where eachaxis corresponds to a group element, then, as we have seen in , the group action results ina permutation of these axes. However, applying the Fourier transform changes the basis so that Gacts independently on different subsets of the axes, resulting in the action being represented by ablock-diagonal matrix, which is the direct sum of irreps.",
  ": Samples of the two tasks": "Evaluation metricsTo assess whether our method effectively captures the underlying data symme-tries, we analyze the learned weight-sharing structure by comparing it to known ground truth patterns.Since our model does not impose associativity or any strict group constraints on the representationstack, it may learn to represent mixtures or interpolations of group elements. Hence, in general, wewill not observe a relevant algebraic structure if we produce a Cayley table based on the learnedrepresentations as done in . Therefore, we will use an approach that allows us to capture the flexibility of our representations.To this end, we will examine each representation matrix to determine how closely it resembles aconvolution matrix associated with a random variable defined over some specific group G. Weemploy the set of group actions from G, represented as doubly stochastic tensors {Pgtk }|G|k=1, as areference framework to quantify the fit and alignment of our models representations with thesepredefined group actions. As such, for each learned weight sharing tensor Rli, we calculate the fitPi = |G|kckPgtk and acquire coefficients ck > 0, such that |G|kck = 1 in a constrained linearregression setup by minimizing ||Pi Rli||2.",
  "B.2Additional results: toy problems": "We conducted experiments on various signals subjected to different transformations, including: a1D signal with cyclic shifts (exemplary samples shown in a), a 2D signal with C8 rotations(illustrated in ), and a 3D voxel grid enhanced by 24 cube symmetries. In each scenario,the learned kernel stack accurately matched the data samples, achieving perfect accuracy.",
  "displays the learned representations for localized shifts in the 1D signal, while presents thelearned kernel stack for the 2D signal dataset": "Furthermore, to assess whether our model can identify partial group structures, we evaluate it usingtwo distinct datasets: a 1D signal enhanced with cyclical shifts, and a 3 3 3 voxel grid subjectedto rotations from C4 C4. For the shift dataset, we utilize the complete set of cyclical shifts as theground truth representations. (a, b) displays the coefficients for the shift dataset. Specifically,part (a) shows coefficients when trained uniformly across group elements, while part (b) illustratescoefficients using only the first half of the group elements, where the group no longer retains cyclicproperties or satisfies closure. Given that our method does not assume cyclic groups or group closure,it effectively captures the relevant group transformations even for partial transformations. App. B.4shows the coefficients for the base representations of C4 C4 C4 cube symmetries. Since the dataaugmentation only applied C4 C4 transformations, the method predominantly identifies elementscorresponding to these transformations, as highlighted by the red line.",
  "B.3Visualization of G-Conv layers": "displays the ground truth permutation matrices that implement a shift-twist operator, whichis the group transformation that underlies regular group convolution operator for C4 rotations. illustrates the corresponding matrices learned for each learnable weight sharing layer on therotated MNIST dataset. The learned matrices closely show similar patterns as the shift-twist operator,suggesting the models ability to capture such transformations from training data.",
  "CArchitectural details": "In the current section, we outline some design choices used across all experiments. Code is availableat Firstly, when defining thelearnable representation stack R, we have found that anchoring an identity element aids in distin-guishing the base kernel from its potential transformations. This approach is based on the intuition ofstarting with a learnable base kernel and subsequently learning its transformations and we have foundthat it aids optimization.",
  "We test two regularizers on the representations R, which are similar to those used by :": "Entropy Regularizer: The primary motivation for using the entropy regularizer is toencourage sparsity in our weight-sharing schemes, which helps the matrices approximateactual permutation matrices rather than simply being doubly stochastic. This approach stemsfrom the intuition for some transformations, the weight-sharing schemes should mimicsoft-permutation matrices. The effectiveness of this sparsity depends on the specific grouptransformations relevant to the taskfor example, C4 rotations are typically represented byexact permutation matrices. In contrast, CN rotations or scale transformations might requireinterpolation, thus aligning more closely with soft permutation matrices. Our experimentalresults indicate that the utility of this regularizer varies with the underlying transformationsin the data; for instance, it is not beneficial for scale transformations in the MNIST dataset,as anticipated. The entropy regularizer is of the following form:",
  "ijkRijk log(Rijk)": "Normalization Regularizer: Empirically, we have found the normalization regularizeressential for reducing the number of iterations needed by the Sinkhorn operator to ensurethe matrices are row and column-normalized. Without this regularizer, the tensors either failto achieve double stochasticity or require an excessively high number of Sinkhorn iterationsto do so. The normalization regularizer is of the following form:",
  "C.2MNIST": "Model architectureFor all MNIST experiments, a simple 5-block CNN was used. Each block usesa kernel size of 5 and is succeeded by instance norm and ReLU activation, respectively. After the finalconvolution block, any spatial and group dimensions are reduced through a global average poolingoperation, and a single linear layer is used as a classification head. For the group convolutional modeland our weight sharing model, the default hidden channel dimension in the blocks was set to 32unless otherwise stated, and 64 in the regular CNN models.",
  "C.4Computational Demands": "14 show the computational scaling analysis of our weight sharing layer, comparing it to agroup convolutional model of the same dimensions. We highlight that regular group convolutionscan be implemented via weight-sharing schemes, resulting in equal computational demands for bothapproaches. Since the weight-sharing approach applies the group transformation in parallel across allelements (as a result of matrix multiplication and reshape operations), our method can prove quiteefficient. Regarding memory allocation, group convolutions are often implemented using for-loops over the group actions, and this sequential method imposes a less heavy memory burden since theoperation is applied in series per group element. However, although scaling is quadratic w.r.t. thenumber of group elements and the domain size for weight-sharing, we mitigate this issue by use ofthe typically lower-dimensional support of convolutional filters (i.e., and ), rendering our approachpractical for a wider range of settings."
}