{
  "Abstract": "We present a random-subspace variant of cubic regularization algorithm that chooses the size of thesubspace adaptively, based on the rank of the projected second derivative matrix. Iteratively, ourvariant only requires access to (small-dimensional) projections of first- and second-order problemderivatives and calculates a reduced step inexpensively. The ensuing method maintains the optimalglobal rate of convergence of (full-dimensional) cubic regularization, while showing improved scal-ability both theoretically and numerically, particularly when applied to low-rank functions. Whenapplied to the latter, our algorithm naturally adapts the subspace size to the true rank of the function,without knowing it a priori.",
  "minxRd f(x),": "where f : Rd R is a sufficiently smooth, bounded-below function, use gradient and curva-ture information to determine iterates and so often experience faster convergence than first-orderalgorithms that only rely on gradient information. However, for high-dimensional problems, thecomputational complexity of these methods can be a barrier to their use in practice. We are con-cerned with the task of scaling up second-order optimization algorithms so that they are a practicaloption for high-dimensional problems.A second-order algorithm designed to cope with high-dimensional problems is the R-ARC al-gorithm , a random subspace variant of the Adaptive Regularization using Cubics (ARC)algorithm . Subject to certain conditions on the random subspaces, R-ARC can attain the sameconvergence rate to an -approximate first-order minimizer as ARC. These conditions imply that R-ARC is particularly effective for functions with Hessians of rank bounded by some r (significantly)lower than the function dimension d. A class of functions with this property are low-rank functions, which have been frequently studied in the context of machine learning.",
  "SCALABLE SECOND-ORDER OPTIMIZATION ALGORITHMS FOR MINIMIZING LOW-RANK FUNCTIONS": "Evaluating sketched problem informationAlgorithm 1 only requires projected objectives gra-dients and Hessians (see f(xk) and 2f(xk) in (1)). These can be calculated efficiently, withoutevaluation of full gradients and Hessians, using techniques such as using directional derivatives,block finite differences or automatic differentiation. For example, f(xk) requires only lk direc-tional derivatives of f with respect to the rows of Sk.",
  "The proof of this Theorem relies upon S being an oblivious subspace embedding (Definition": "8) for matrices with rank r + 1. Similar results can be established for matrix distributions otherthan Gaussian, with l possibly having a different dependency on r . Theorem 1 can be applied toany suitable objective function f. However, the requirement that l = O(r + 1) means that unlessr d, R-ARC is not guaranteed to be able to gain a significant dimensionality over ARC (by usingonly little problem information and computing an inexpensive reduced step), whilst maintaining theO(3/2) convergence rate.In R-ARC, the sketch dimension l is fixed throughout the run of the algorithm. Theorem 1 requires l to be proportional to a bound r on the maximal Hessian rank at the iterates, but thismay not be known a priori. This motivates us to develop a variant of R-ARC that can adapt thesketch/subspace size to local problem information. Low-rank functionsWe now define a class of functions that particularly benefit from randomsubspace algorithms. These functions are also known as functions with low effective dimensionality,with active subspaces or multi-ridge functions .",
  ". An adaptive sketch size rule": "We introduce a sketch size update rule that can be included in Algorithm 1. The motivation behindthis update rule is that we seek to use local problem information to, in a sense, learn the rank of thefunction f (assuming that it has low-rank structure). To do this, we keep track of the observed ranksof the sketched Hessian. In Algorithm 1, we define the following for k 0 :",
  "Lemma 5Set l0 1 and suppose that the update rule () is applied to lk. For all k 1, Iflk < Crk + 1, then with probability 1, Rk > Rk1": "Proof By the update rule (), we have that lk < Crk + 1 =Rk1 < rk. We also have thatlk Rk1 + 1. Therefore, we have that lk < Crk + 1=Rk1 < min(lk, rk). Hence, byLemma 4, we have that rk = min(lk, rk) > Rk1 with probability 1.",
  "Applying this Lemma allows us to prove the following convergence result": "Theorem 6 (R-ARC-D convergence result) Suppose that S is the distribution of scaled Gaussianmatrices and f is a low-rank function of rank r with Lipschitz-continuous second derivatives. ApplyAlgorithm 1 with the sketch update rule () with l0 1, C = 4Cl(2+log(16)) where Cl is definedin Lemma 9, then R-ARC achieves the optimal O(3/2) rate of convergence, with high probability.",
  ". Numerical Experiments": "In these numerical experiments, we apply the R-ARC-D algorithm as described in Algorithm 1,using the lk update rule () with C = 1 for simplicity. The code we used is a modification of theARC code used in . We make a minor modification in that we only redraw Sk after successfuliterations; this update step performs better empirically than redrawing after each iteration. Theperformance of R-ARC-D is compared with that of R-ARC and ARC. As a measure of budget, weuse relative Hessians seen; if at iteration k, we draw a sketching matrix of size lk d, we see (lk/d)2",
  "relative Hessians. When calculating f(xk) and 2f(xk), we calculate f(xk) and 2f(xk), andthen multiply by Sk; the computational efficiency could be much improved by applying techniquesdiscussed in": "Augmented CUTEst problemsTo create low-rank functions to test on, we take CUTEst prob-lems of dimension (or rank) r 100 and add dimensions and rotate to create low-rank problems ofdimension d = 1000. Given a function f : Rr R, this can be achieved by sampling a randomorthogonal matrix Q Rdr so that QQ = Ir and define g : Rd R by g(x) = f(Qx) to be alow-rank variant of f. To distinguish these problems from the standard CUTEst versions, we prefixthe problem names with l-. Problem details can be found in .",
  "ARTIF (with parameter N = 1000), which we plot in , where R-ARC-D converges, but notfaster than ARC": "Data ProfilesHere we compare the performance between R-ARC-D and R-ARC through dataprofiles . A description of the methodology can be found in Appendix B. The set of problemsconsiders can be found in . For R-ARC-D, we again set l0 = 2, whilst for R-ARC, wesketch at 1%, 5% and 7.5% of the original problem dimension. As the functions are of dimensiond = 1000 with rank r 100, this corresponds to 10 75% of the function rank. The results areplotted in , where we show results for tolerances = 1e2 (low-precision) and = 1e5(high precision). We repeat each problem 5 times, with different Q matrices, treating each run as aseparate problem in the plots.",
  ": Data profiles of R-ARC-D compared to R-ARC and ARC": "We see that R-ARC-D started at l0 = 2 outperforms R-ARC regardless of the fixed sketch sizeused by R-ARC. This is more significant for the high-precision solutions ( = 1e 5) where R-ARC-D typically increases lk until it reaches the function rank r. In Figure D, we plot individualproblem performance for several of the problems considered here. In these plots, we see that whilstR-ARC with l = 1% of d occasionally performs best in terms of relative Hessians, it performs worstin terms of time due to it taking significantly more iterations. Thus overall, we found that R-ARC-Dperforms particularly well on low-rank problems, which we have demonstrated both numericallyand theoretically.",
  "James Bergstra and Yoshua Bengio. Random Search for Hyper-Parameter Optimization. Jour-nal of Machine Learning Research, 13(10):281305, 2012. ISSN 1533-7928": "Coralia Cartis and Adilet Otemissov. A dimensionality reduction technique for unconstrainedglobal optimization of functions with low effective dimensionality. Information and Inference:A Journal of the IMA, 11(1):167201, March 2022. ISSN 2049-8772. doi: 10.1093/imaiai/iaab011. URL Coralia Cartis, Nicholas IM Gould, and Philippe L Toint. Adaptive cubic regularisation meth-ods for unconstrained optimization. part i: motivation, convergence and numerical results.Mathematical Programming, 127(2):245295, 2011.",
  "arXiv:2211.09873 [math]": "Coralia Cartis, Xinzhu Liang, Estelle Massart, and Adilet Otemissov. Learning the subspaceof variation for global optimization of functions with low effective dimension, January 2024.URL arXiv:2401.17825 [math]. Nicholas I.M. Gould Coralia Cartis and Philippe L. Toint. Evaluation complexity of algorithmsfor nonconvex optimization: theory, computation, and perspectives. Society for Industrial andApplied Mathematics, Philadelphia, 2022. ISBN 978-1-61197-699-1.",
  "Elizabeth D Dolan and Jorge J More. Benchmarking optimization software with performanceprofiles. Mathematical programming, 91(2):201213, 2002": "Nicholas IM Gould, Dominique Orban, and Philippe L Toint. CUTEst: a constrained andunconstrained testing environment with safe threads for mathematical optimization. Compu-tational Optimization and Applications, 60(3):545557, 2015. Jorge J. More and Stefan M. Wild.Benchmarking Derivative-Free Optimization Algo-rithms. SIAM Journal on Optimization, 20(1):172191, January 2009. ISSN 1052-6234. doi:10.1137/080724083. URL Society for Industrial and Applied Mathematics.",
  "Zhen Shao. On Random Embeddings and Their Application to Optimisation. PhD thesis,Mathematical Institute, University of Oxford, 2022": "Zhen Shao and Coralia Cartis. Random-subspace adaptive cubic regularisation method fornonconvex optimisation. In HOO-2022: Order up! The Benefits of Higher-Order Optimizationin Machine Learning, 2022. Ziyu Wang, Frank Hutter, Masrour Zoghi, David Matheson, and Nando De Feitas. BayesianOptimization in a Billion Dimensions via Random Embeddings. Journal of Artificial Intelli-gence Research, 55:361387, February 2016. ISSN 1076-9757. doi: 10.1613/jair.4806. URL",
  "(1 ) y22 Sy22 (1 + ) y22for all y Y = {y : y = Bz, z Rk}.(3)": "Definition 8 (Oblivious subspace embedding ) A distribution S on S Rld is an (, )-oblivious subspace embedding for a given fixed/arbitrary matrix B Rdk , we have that, with ahigh probability of at least 1 , a matrix S from the distribution is an -subspace embedding forB. Lemma 9 (Theorem 2.3 in )Let S (0, 1) and S Rld be a scaled Gaussian matrix.Then for any fixed d (d + 1) matrix M with rank at most r + 1, with probability 1 S we havethat simultaneously for all z Rd+1, SMz22 (1 S) Mz22, where",
  "B. Data profile methodology": "We measure algorithm performance using data profiles , which themselves are a variant ofperformance profiles . We use relative Hessians seen as well as runtime for our data profiles.The relative Hessians seen at an iteration k is (lk/d)2 where lk is the sketch dimension and d is theproblem dimension. Using the same notation as in , for a given solver s, test problem p Pand tolerance (0, 1), we determine the number of relative Hessians seen Np(s, ) required fora problem to be solved:",
  "#Problemdrf(x0)f(x)Parameters": "1l-ARTIF10001001.8296 1010N = 1002l-ARWHEAD10001002.9700 1020N = 1003l-BDEXP10001002.6526 1010N = 1004l-BOX100010001.1240 101N = 1005l-BOXPOWER10001008.6625 1020N = 1006l-BROYDN7D10001003.5098 1024.0123 101N/2 = 507l-CHARDIS11000981.2817 1010NP1 = 508l-COSINE10001008.6881 1019.9000 101N = 1009l-CURLY1010001006.2372 1031.0032 104N = 10010l-CURLY2010001001.2965 1021.0032 104N = 10011l-DIXMAANA11000908.5600 1021M = 3012l-DIXMAANF1000901.2253 1031M = 3013l-DIXMAANP1000902.1286 1031M = 3014l-ENGVAL110001005.8410 1030N = 10015l-FMINSRF210001212.5075 1011P = 1116l-FMINSURF10001213.0430 1011P = 1117l-NCB2010001102.0200 1021.7974 102N = 10018l-NCB20B10001002 1021.9668 102N = 10019l-NONCVXU210001002.6397 1062.3168 102N = 10020l-NONCVXUN10001002.7270 1062.3168 102N = 10021l-NONDQUAR10001001.0600 1020N = 10022l-ODC100010001.9802 102(NX, NY) = (10, 10)23l-OSCIGRNE10001003.0604 1080N = 10024l-PENALTY310001009.8018 1071 103N/2 = 5025l-POWER10001002.5503 1070N = 10026l-RAYBENDL10001269.8028 1019.6242 101NKNOTS = 6427l-SCHMVETT10001002.8029 1022.9940 103N = 10028l-SINEALI10001008.4147 1019.9010 103N = 10029l-SINQUAD10001006.5610 1013N = 10030l-TOINTGSS10001008.9200 1021.0102 101N = 10031l-YATP2SQ10001209.1584 1040N = 10"
}