{
  "Abstract": "We investigate the adversarial robustness of LLMs in transfer learning scenarios.Through comprehensive experiments on multiple datasets (MBIB Hate Speech,MBIB Political Bias, MBIB Gender Bias) and various model architectures (BERT,RoBERTa, GPT-2, Gemma, Phi), we reveal that transfer learning, while improvingstandard performance metrics, often leads to increased vulnerability to adversarialattacks. Our findings demonstrate that larger models exhibit greater resilience tothis phenomenon, suggesting a complex interplay between model size, architecture,and adaptation methods. Our work highlights the crucial need for consideringadversarial robustness in transfer learning scenarios and provides insights intomaintaining model security without compromising performance. These findingshave significant implications for the development and deployment of LLMs inreal-world applications where both performance and robustness are paramount.",
  "Introduction": "Large Language Models (LLMs) have become pivotal in natural language processing (NLP), demon-strating remarkable performance across various tasks. Transfer learning, a technique leveragingpre-trained models for new tasks, has significantly contributed to this success . However, theintersection of transfer learning and adversarial robustness in LLMs remains understudied, presentinga critical gap in understanding models security and reliability. While transfer learning efficiently applies pre-trained models to new domains, it may inadvertentlyintroduce or amplify vulnerabilities to adversarial attacks. These attacks pose significant threats tomodel deployment in real-world scenarios. Despite the widespread adoption of transfer learning,there is a notable lack of comprehensive research on how these adapted models perform againstadversarial attacks. Previous studies have primarily focused on the robustness of models in their initial training or fine-tuning stages , often in controlled environments. This approach overlooks the potentialrisks emerging from more complex training sequences, particularly those involving multiple pre-training stages as in transfer learning scenarios. The impact of transfer learning on model robustnessis nuanced and multifaceted. While some research suggests that post-fine-tuning can lead to decreasedrobustness , other findings indicate that incorporating additional data from the target dataset canenhance robustness . However, in transfer learning scenarios involving pretraining on related butdistinct domains, the impact on robustness becomes more complex and warrants careful investigation.",
  "Our contributions are as follows:": "1. We conducted comprehensive experiments to evaluate the robustness of LLMs againstadversarial attacks in transfer learning, revealing that transfer learning often increasesvulnerability to adversarial attacks, even when improving standard performance metrics. 2. We provide a detailed analysis of how model characteristics influence robustness in transferlearning scenarios, demonstrating that larger models show significantly greater resilience toincreases in vulnerability. This finding is contextualized within a broader examination ofhow different model architectures (e.g., GPT , BERT , RoBERTa ) and transferlearning techniques (such as LoRA ) impact robustness, revealing a complex interplaybetween model size, architecture, and adaptation methods in determining a models securityagainst adversarial attacks. 3. We provide insights into the trade-off between model robustness and accuracy by conductingexperiments with training on perturbed data. This helped us understand the balance betweenmaintaining robustness and preserving model performance.",
  "Datasets": "We selected three distinct datasets that share a common theme of detecting bias in textual data butaddress different subdomains within this broader context. This choice allows us to meaningfullyexplore the impact of transfer learning, as it involves transferring knowledge across related yet distincttypes of biases. Each dataset has 2 classes: biased and non-biased, and is balanced. In general, theability to accurately detect and mitigate various forms of bias is crucial to develop fair and ethicalAI systems that can be safely deployed in diverse real-world applications . Data sets are selectedfrom and are as follows:",
  "Attack Success Rate (ASR): Percentage of True Positive and True Negative examples that werehacked by the attack, this metric can serve as a basic evaluation of the robustness of the model": "Accuracy Under Attack (AUA): The accuracy of the model after attack. This metric can be consid-ered a safety metric for the model. For instance, if the models accuracy (Acc) significantly increaseswhile the Attack Success Rate (ASR) only mildly increases, the AUA may show improvement eventhough the model has become less robust overall.",
  "Parameters Setting": "For the pre-training phase, we trained the models for 1 epoch on the larger subset of the dataset.During the fine-tuning phase on the target dataset, the models were trained for up to 6 epochs, withthe best model selected based on the accuracy of the validation set. We used the Adam optimizer,adjusting the learning rate between 5 106 and 4 104 depending on the specific model, toensure optimal convergence during training.",
  "We employ two attack methods in our experiments:": "TextFooler : A word-level adversarial attack method for text classification. It uses word deletionimpact for importance ranking, word embeddings for synonyms, and Universal Sentence Encoder forsemantic similarity constraints. A2T : A computationally efficient adversarial attack method. It uses gradient-based wordimportance ranking, counter-fitted word embeddings for synonyms, and DistilBERT for semanticsimilarity constraints.",
  "We evaluated various LLMs using TextFooler (black-box) and A2T (white-box) adversarial attacks.The results, presented in , reveal a concerning trend:": "Increased Vulnerability: In most cases, especially for smaller models, the Attack Success Rate (ASR)increased after transfer learning, regardless of changes in accuracy (OAcc). It suggests that evenwhen models demonstrated enhanced performance in terms of accuracy, their overall robustnessagainst adversarial attacks often decreased. Performance-Robustness Trade-off: Even when models showed improved accuracy, their robustnessagainst adversarial attacks often decreased. For example, on the Hate Speech dataset, GPT-2experiences a mean 20.4% increase in ASR alongside a 3.67% increase in accuracy. This findingraises significant concerns about LLM security, as improvements in accuracy during training mightlead developers to overlook other critical parameters like robustness.",
  "LoRA and Larger Models": "For large models with billions of parameters, we used LoRA due to its efficiency in adapting thesemodels, as conventional fine-tuning often requires extensive computational resources that may notbe readily available in typical settings. When applying LoRA to these larger models, we observedmixed results. Some sequences showed decreased robustness, while others demonstrated increasedrobustness (e.g., political bias dataset for Phi-2 and Gemma 2b), result are presented in and. The impact of LoRA on robustness is complex due to its unique approach: introducing and randomlyinitializing a small set of additional parameters rather than fine-tuning existing ones. This may leadto different robustness outcomes compared to standard fine-tuning. While transfer learning here canstill reduce robustness through issues like false memories or shortcut learning , catastrophicforgetting may not contribute significantly to the results in this specific setting. This is because,with the random initialization of LoRA adapter parameters and the freezing of other parameters,there is no pre-existing information in the adapters that could be distorted or lost during the transferlearning process, thus potentially altering the dynamics of how robustness changes during transferlearning.",
  "RQ3: Real-world implications": "As we showed, often ASR increases in parallel to OAcc, which indicates a potential trade-off of usingTransfer learning between performance and safety. Often standard metrics like OAcc are prioritized,while other safety metrics are overlooked, leading to vulnerable models being deployed. Based onour findings, we highlight the necessity of applying additional techniques and adversarial testing to",
  "OAcc75.869.2368.7569.7868.1868.59-7.94-1.52-0.23AUA50.2548.2448.3852.5148.9350.264.501.433.89ASR33.9230.4129.6325.328.1826.43-25.41-7.33-10.80": "The experiments reveal a trade-off between robustness and accuracy (full results are shown in and of Appendix C). Adversarial fine-tuning reduces OAcc but significantly boosts AUAand lowers the ASR, especially on the first dataset. Early exposure to adversarial examples enablesthe model to build strong defense mechanisms, improving its resistance to attacks despite a decline inOAcc. Introducing adversarial samples during training enhances overall performance, leading to a morerobust model. While accuracy on the first dataset decreases, the models ability to withstand attacksimproves, indicating a balanced adaptation between accuracy and robustness over time.",
  "Conclusion": "Our research contributes to the understanding of the adversarial robustness of LLMs in the contextof transfer learning. Our empirical analysis reveals nuanced dynamics in the relationship betweentraditional performance metrics, such as accuracy, and the robustness of models against adversarialattacks. Interestingly, we observed instances where improvements in conventional metrics wereaccompanied by a decrease in adversarial robustness, suggesting a potential trade-off betweenperformance enhancement and vulnerability to adversarial manipulations. This counterintuitivefinding underscores the complexity of model behavior in transfer learning scenarios and raisesquestions about the underlying causes, which may include phenomena such as catastrophic forgettingor the acquisition of misleading false memories during pre-training. Notably, our results indicatethat larger models may exhibit a reduced susceptibility to this trend, hinting at an inherent robustnessassociated with scale. This research was supported in part by a grant from the Simons Foundation (SFARI award #1280457,JS) and by the NYU Center for Responsible AI through the RAI for Ukraine program. We thank thesupport of EU European Defence Fund Project KOIOS (EDF-2021-DIGIT-R-FL-KOIOS). Tomas Bueno Momcilovic, Beat Buesser, Giulio Zizzo, Mark Purcell, and Dian Balta. Towardsassurance of llm adversarial robustness using ontology-driven argumentation. Valletta, Malta,July 2024. xAI 2024: World Conference on eXplainable Artificial Intelligence.",
  "Leo Schwinn, David Dobre, Stephan Gnnemann, and Gauthier Gidel. Adversarial attacksand defenses in large language models: Old and new threats. In Proceedings on, pp. 103117.PMLR, 2023": "Martin Wessel, Tomas Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, and Timo Spinde.Introducing mbib-the first media bias identification benchmark task and dataset collection. InProceedings of the 46th International ACM SIGIR Conference on Research and Development inInformation Retrieval, pp. 27652774, 2023. Peiyu Xiong, Michael Tegegn, Jaskeerat Singh Sarin, Shubhraneel Pal, and Julia Rubin. It is allabout data: A survey on the effects of data on adversarial robustness. ACM Computing Surveys,56(7):141, 2024.",
  "ASocial Impact Statement": "Our research rigorously examines the balance between performance enhancements and securityvulnerabilities in large language models (LLMs) using transfer learning. This analysis has highlightedthe need for training methodologies that prioritize both model effectiveness and security. As LLMs become more common in sectors like healthcare, finance, and public services, it is crucialto protect these systems from sophisticated adversarial threats. Our findings show that while transferlearning can improve model performance, it can also introduce and magnify vulnerabilities thatmalicious actors could exploit, necessitating a reevaluation of current training practices. We advocate for incorporating comprehensive adversarial training and robustness assessments duringthe AI development. By adopting these practices, developers can better manage the trade-offs betweenaccuracy and security, ensuring that improvements in LLM capabilities do not compromise theirdefense. Our study reveals interesting nuances in the interaction between transfer learning, performance,and security. We observed instances where transfer learning not only contributed to performanceimprovements but also bolstered the models defenses against adversarial attacks under certainconditions. These insights suggest that transfer learning, when applied thoughtfully, might offeropportunities to simultaneously enhance both the effectiveness and the security of LLMs, meritingdeeper investigation into these phenomena.",
  "and show that BERT models, despite their bidirectional architecture, display vulnera-bility patterns similar to GPT-2, with BERT Large showing marginally improved robustness": "RoBERTa models ( and ) exhibit an noteworthy characteristic: while generally morerobust than BERT, they still incur significant ASR increases, particularly against the a2t attack. Thissuggests that RoBERTas enhanced pretraining does not necessarily confer improved adversarialrobustness in transfer learning scenarios. The results for Phi-2 and Gemma 2B ( and ) are particularly noteworthy. TheseLoRA-tuned models show highly variable results, with some sequences demonstrating improvedrobustness post-transfer. This variability indicates a complex interaction between LoRAs adaptationmechanism and adversarial vulnerability, warranting further investigation. These raw results not only corroborate our main findings but also elucidate the nuanced impactof model architecture, size, and fine-tuning method on adversarial robustness in transfer learningcontexts.",
  "CTrade offs": "The tables in this section (referenced in 3.4) present the full result related to the impact of twoadversarial attack types: TextFooler , which manipulates tokens, and A2T , which manipulatesgradients. These experiments compare the models robustness and accuracy under attack, focusingon key performance metrics. The results show a clear difference in the effectiveness of TextFooler and A2T attacks acrosstransformer-based models like GPT, BERT, and RoBERTa. Gradient-based attacks (A2T) aregenerally less effective, with higher Accuracy Under Attack (AUA) observed, indicating difficultyin perturbing internal representations. In contrast, TextFooler consistently achieves higher AttackSuccess Rates (ASR) and lower AUA. Larger models (RoBERTa-large, GPT-2-large) benefit more from adversarial training, showing greaterrobustness improvements under both attacks. They exhibit more pronounced decreases in ASR andincreases in AUA, indicating better adaptation to adversarial defenses. Smaller models like BERTand GPT-2 experience similar trends but with less significant gains. TextFooler is more successful at reducing model accuracy, particularly in smaller models, achievinghigher ASR and lower AUA. A2T, while less effective, demonstrates higher AUA, especially in largermodels, showing that token manipulation remains a stronger attack strategy. Adversarial training consistently enhances model robustness by reducing ASR and increasing AUA,albeit at the cost of lower Original Accuracy (OA). Early exposure to adversarial examples enablesstronger defenses, particularly in larger models, though this comes at the expense of handling cleandata with slightly reduced precision. In conclusion, adversarial fine-tuning reveals a trade-off: while it reduces OA, it significantly boostsrobustness against attacks, especially in models exposed early to adversarial data. Larger models showgreater adaptation to adversarial defenses, highlighting the importance of model size and architecturein balancing accuracy and robustness."
}