{
  "Abstract": "Generalizing across robot embodiments and tasks is crucial for adaptive roboticsystems. Modular policy learning approaches adapt to new embodiments but arelimited to specific tasks, while few-shot imitation learning (IL) approaches oftenfocus on a single embodiment. In this paper, we introduce a few-shot behaviorcloning framework to simultaneously generalize to unseen embodiments and tasksusing a few (e.g., five) reward-free demonstrations. Our framework leveragesa joint-level input-output representation to unify the state and action spaces ofheterogeneous embodiments and employs a novel structure-motion state encoderthat is parameterized to capture both shared knowledge across all embodiments andembodiment-specific knowledge. A matching-based policy network then predictsactions from a few demonstrations, producing an adaptive policy that is robust toover-fitting. Evaluated in the DeepMind Control suite, our framework termed Meta-Controller demonstrates superior few-shot generalization to unseen embodimentsand tasks over modular policy learning and few-shot IL approaches. Codes areavailable at",
  "Introduction": "Generalizing across different robot embodiments and tasks with only a few demonstrations is afundamental challenge in continuous control . This capability is crucial fordeveloping versatile and adaptive robotic systems that can operate effectively in diverse and dynamicenvironments. The high diversity of embodiments and tasks, however, makes this particularlychallenging. Robot embodiments vary widely in their morphological (e.g., number and connectivityof joints) and dynamic (e.g., motor gear ratios, damping coefficients) configurations, complicatingthe design of a unified architecture capable of handling heterogeneous input states and output actions.Furthermore, the variety of taskssuch as locomotion, object manipulation, and navigationrequiresthe learning of transferable skills that can efficiently generalize across different tasks. Despite significant advancements in reinforcement learning (RL) and imitation learning (IL), achiev-ing simultaneous generalization across diverse embodiments and tasks with a few demonstrationsremains largely underexplored. Modular policy learning approaches have shownpromise by learning modular policies that can be shared across embodiments with different morpholo-gies. However, these methods primarily focus on specific task types such as locomotion, and lack theflexibility to adapt to a wide range of control tasks, limiting their broader applicability. Conversely,few-shot IL approaches aim to learn novel tasks from a few demonstrations.These techniques excel in scenarios with sparse training data but typically concentrate on a singleembodiment with fixed morphological and dynamic structures, restricting their ability to generalizeacross various embodiments and tasks. As a result, these two fields have developed independently,and their integration remains an open area of research.",
  "arXiv:2412.12147v1 [cs.LG] 10 Dec 2024": "geneous embodiments within a unified architecture, we tokenize states and actions into joint-levelrepresentations, since joints serve as the fundamental building blocks of robots and provide a modularrepresentation for compositional generalization to unseen embodiments . Given this unifiedI/O, we employ a state encoder to capture both embodiment-specific knowledge about morphologyand dynamics and shared knowledge about the physics governing the environment. Then we design amatching-based policy network that predicts actions from the encoded states, conditioned on a fewdemonstrations. Our model is trained using episodic meta-learning on a dataset comprising variousembodiments and tasks, followed by few-shot behavior cloning on unseen embodiments and tasksusing a few reward-free demonstrations. Our key contributions are as follows: (1) We design a novel structure-motion encoder that operateson joint-level state representations, efficiently disentangling embodiment-specific and task-specificknowledge. (2) We propose a matching-based meta-learning framework that efficiently transfersknowledge of local motions to quickly learn unseen tasks with a few demonstrations. (3) Weevaluate our framework in various environments within the DeepMind Control suite, encompassingdiverse embodiments and tasks, demonstrating superior few-shot learning performance over existingbaselines.",
  "Problem Setup": "A reinforcement learning (RL) problem involves an agent interacting with an environment, typicallymodeled as a Markov Decision Process (MDP). An MDP is represented by the tuple (S, A, P, R),where S is the state space, A is the action space, P : S A S R is the transition probability,and R : S A R is the reward function. In conventional RL, an agent learns a policy : S Athat maximizes expected cumulative rewards E[tR(st, at)], where t is a discount factor.Training such an RL agent typically requires numerous interactions with the environment and acarefully designed reward function, making it burdensome to learn new tasks. Behavior cloning (BC)addresses these challenges by using supervised learning techniques to imitate an expert policy fromoffline demonstrations. We focus on the few-shot BC setting, where the training data consists of afew reward-free demonstrations D = {i}iN, with each demonstration i = {(sit, ait)}tT being atemporal sequence of states and actions performed by an expert model. In this paper, we consider continuous control problems of multi-joint robots that involve variousembodiments and tasks. An embodiment E refers to the physical configuration of robots, whichincludes (1) the morphology, i.e., the shape, size, and arrangement of the components such as limbs,joints, and sensors, and (2) the dynamics parameters that affect the robots behavior, e.g., motioninertia, mass, gear ratios, and damping. In terms of MDP, different embodiments can have differentdimensionality of state and action spaces, e.g., dim(SEi) = dim(SEj) and dim(AEi) = dim(AEj)for Ei = Ej, as well as distinct transition probabilities PE that determine the kinematics of therobot. A task T is defined by a specific goal or objective that the robot must achieve within itsenvironment, characterized by a reward function RT . Tasks can vary widely, ranging from locomotionand manipulation to complex interactions with dynamic environments. The combination of differentembodiments E and tasks T creates a broad class of continuous control problems. Our objective is to achieve simultaneous generalization to unseen embodiments and tasks of contin-uous control with a few-shot behavior cloning framework. In other words, the model has to learna policy for a novel continuous control problem from only a few demonstrations D, where bothembodiment E and task T can be arbitrary and previously unseen.",
  "Challenges and Desiderata": "Despite achieving the simultaneous few-shot generalization to unseen embodiments and tasks iscrucial for developing versatile and adaptive robotic systems, this problem remains less explored. Wecharacterize two distinct challenges and desiderata to address each challenge. Handling Heterogeneous Embodiments. To generalize to arbitrary embodiments in continuouscontrol, the model must possess an architecture capable of universally handling heterogeneous statesand actions of various embodiments. This necessitates a unified input/output (I/O) representationfor states and actions, allowing for the sharing of structural characteristics (e.g., dimensionality) andsemantics (e.g., input attributes or output control types) across different embodiments. Additionally, . . . . . . . . . TimeShared Matching-based Policy Network",
  "State Encoder": "Few-shot Demonstrations : The overall framework of Meta-Controller. First, the states and actions of various robotembodiments are tokenized into joint-level representations. The state tokens are then encoded bythe state encoder to capture knowledge about the embodiments. Finally, a matching-based policynetwork uses few-shot demonstrations with the encoded state features to predict per-joint actions. the encoder to extract state features should be able to capture transferable knowledge across differentembodiments as well as embodiment-specific knowledge to flexibly adapt to distinct morphologiesand dynamics of each embodiment. Few-shot Policy Adaptation. To achieve robust few-shot learning on unseen tasks, an efficient andflexible policy adaptation mechanism is essential. Given the diverse behaviors required by continuouscontrol problems across various embodiments and tasks, the policy network must learn transferableskills shared by different tasks. Simultaneously, the model must dynamically adapt its policy byinferring the task based on a few demonstrations, ensuring it captures the underlying structure ofpreviously unseen tasks. Additionally, the adaptation mechanism must be robust to overfitting.",
  "Method": "In this section, we introduce Meta-Controller, a few-shot behavior cloning framework for simultane-ous generalization of embodiments and tasks of continuous control. illustrates the overallframework. Meta-Controller addresses heterogeneous embodiments by tokenizing the states andactions into joint-level I/O (.1) and employing a state encoder that captures knowledgeabout the structure and dynamics of the embodiment (.2.1). Then, a matching-based policynetwork (.2.2) predicts the action by leveraging a few given demonstrations. The trainingprotocol of Meta-Controller consists of episodic meta-learning and few-shot fine-tuning (.3).",
  "Joint-Level I/O Representation": "To unify the state and action spaces of different embodiments, we adopt joint-level tokenization.Joints are fundamental components of robots, and their primary source of action is the torque or forcegenerated by actuators attached to each joint. This allows us to standardize the states and actions ofa robotic agent into per-joint observations and control commands. Consequently, joint-level statesand actions provide a natural modular representation, facilitating the compositional generalization ofvarious robot embodiments2.",
  "Motion Encoder": ": The state encoder f consists of two component transformers. Joint-level state tokens arefirst encoded by the structure encoder fs along the joint axis, where the positional embedding and apart of backbone parameters adapt the model to each embodiment. The features are then passed tothe motion encoder fm, which computes causal attentions of per-joint features along the temporalaxis, where a part of backbone parameters adapt the model both to the embodiment and task.",
  "st = [sj,t]jJE RJEd,at = [aj,t]jJE RJE1,(1)": "where JE is the number of joints in E and t indicates the time. The state token sj,t Rd represents per-joint information, such as position, velocity, movement axis, and motion types (i.e., linear or angular).The action token aj,t represents the control command of j-th joint, where we assign zerovalue for free joints. This joint-level representation ensures consistency across heterogeneous roboticembodiments, enabling unified learning of different continuous control problems.",
  "Meta-Controller": "Given the tokenized representation of joint states and actions, we aim to build an adaptive controllerfor arbitrary robot embodiments to perform arbitrary continuous control tasks based on a set of fewdemonstrations D = { i}iN. To address the challenge discussed in .1, we employ anencoder f to extract the embodiment-aware state features and an adaptive policy network thatefficiently leverages the demonstrations D to predict the actions.",
  "mt = f(st; ).(2)": "To effectively encode the states of each embodiment, we decompose our state encoder into twocomponents: a structure encoder fs that captures morphological knowledge, and a motion encoderfm that captures dynamics knowledge. Structure Encoder. The structure encoder models the relationships among the joints within eachembodiment. As shown in , we use a bi-directional transformer on the joint-level state tokensst at each timestep t to extract the structure features zt:",
  "Causal Transformer": ": An illustration of the matching-based policy network . (a) Each state and action tokenin few-shot demonstrations is encoded by the corresponding encoders f and g, where we use thesame encoder f used for the current state. A matching module then computes the weighted sum ofaction features based on the joint-wise similarity between state features. Finally, an action decoder hdecodes the joint-wise matching output to predict the current action. (b) Both the action encoder gand decoder h are causal transformers operating along the temporal axis of action tokens and features. positional embedding pEs is crucial for adapting to local configurations (e.g., length, movement range)of joints in E not explicitly given in the state st. Global configurations (e.g., control timestep) sharedby all joints are handled through the embodiment-specific parameters Es in the transformer backbone.Shared parameters s capture common knowledge, such as the physics governing the environment. To enable efficient yet flexible adaptation during few-shot behavior cloning, we designate only asmall portion of the backbone parameters to be embodiment-specific. Inspired by parameter-efficientfine-tuning (PEFT) approaches that effectively modulate transformers with only a few parameters, weemploy bias parameters , low-rank projection matrices , and also layer-scale parameters forEs . As explained in .3, only the adaptive parameterspEs , Esare updated during few-shotlearning on unseen embodiments, ensuring robustness to overfitting the few demonstrations. Motion Encoder. While the state encoder fs encodes structural information about the embodiments,it does not model the temporal dynamics of states which is crucial for understanding continuouscontrol tasks. Therefore, we introduce a motion encoder fm, which is a causal transformer thatencodes the state features along the temporal axis. As illustrated in , fm rearranges theencoded structure features zt into separate temporal sequences of joint-level features zj,t, thenproduce the motion features for each joint mj,t auto-regressively: mj,t = fm(zj,t + pm; m, (E,T )m),j JE(4)where pm denotes the positional embedding for specifying the timesteps t. Additionally, we introducea small portion of adaptive parameters (E,T )min the causal transformer backbone, which is bothspecific to embodiment E and task T . These parameters help the model understand the uniquemotions in the few-shot demonstrations that are specific to each task and embodiment. We use thesame PEFT techniques employed in the structure encoder for the adaptive parameters.",
  "aj,t = h(vj,t; h),j JE.(9)": "The matching-based policy network offers significant benefits for few-shot behavior cloning, par-ticularly when dealing with unseen tasks and embodiments. Its robust adaptation capabilities stemfrom effectively incorporating demonstration data through a similarity function, which dynami-cally matches current state features with those from demonstrations. This non-parametric approachminimizes overfitting, enhancing generalization from limited examples. Hierarchical IL Interpretation. Eq. (8) can also be interpreted as hierarchical imitation learningthat generalizes to unseen tasks using a transferable skill set. Since the action encoder g extracts apool of temporal action features that are composed to produce the current action at in the featurespace, we can treat the action features vij,t as local motor skills, i.e., building blocks of joint behaviorfor various control tasks. This modular approach lets the network recombine these skills to efficientlytackle new challenges. By assigning high similarity scores to relevant demonstrations, the policynetwork ensures accurate imitation of expert behavior, improving performance on novel tasks.",
  "Training & Inference": "The training protocol of Meta-Controller consists of two stages: episodic meta-learning and few-shotfine-tuning, where we train whole parameters (, ) of the model during the first stage while we trainonly the adaptive parameters (pEs , Es , (E,T )m) during the second stage. Episodic Meta-Learning. During episodic meta-learning, the model acquires general knowledge ofcontinuous control through a number of episodes that mimic few-shot behavior cloning scenarios toensure effective adaptation to unseen embodiments and tasks. To this end, we leverage a meta-trainingdataset that consists of demonstrations B(E,T ) by expert agents with various embodiments E andtasks T . At each episode, we first sample a continuous control problem (E, T ), then sample twosubsets of B(E,T ): a set of support data DS and query data DQ. Then the model is trained to imitatequery data using the demonstrations.",
  "where at is produced by Eq. (5) using DS, and p(E, T ) is a uniform distribution over all controlproblems within the dataset": "Few-shot Fine-Tuning. After acquiring the meta-knowledge about continuous control problems,we apply our Meta-Controller in a few-shot behavior cloning setup, where it should adapt to bothunseen embodiments and tasks with a few demonstrations D. To this end, we randomly split D intotwo disjoint subsets, and fine-tune the model with Eq. (10) but with only respect to the embodiment-specific and task-specific parameters (pEs , Es , (E,T )m) while freezing the rest. After fine-tuning, themodel uses the whole demonstrations D in the policy network to produce the actions at evaluation.",
  "Related Work": "Modular Policy Learning. Modular policy learning aims to develop modular policies for multi-jointrobots that are adaptable to various morphologies. NerveNet uses a Graph Neural Network(GNN) to model structural relationships between joint-level features. SWAT leverages graphfeatures like the normalized graph Laplacian for improved structural learning in reinforcementlearning (RL). Amorpheous and MetaMorph use transformer architectures, treating themorphological graph as fully connected to exploit transformers capabilities. These works focus onRL and zero-shot generalization to specific locomotion tasks. Furuta et al. proposes an imitationlearning framework with a benchmark environment for extensive morphologies, showing promisingzero-shot generalization but limited to specific tasks like reaching a goal position, and not handlinggeneral continuous control tasks. Few-shot Imitation Learning. Few-shot Imitation Learning (IL) approaches focus on generalizingnovel RL tasks with only a few demonstrations. Gradient-based meta-learning algorithms likeMAML and Bayesian MAML have been explored for rapid task adaptation. Duan et al. addressed one-shot imitation learning by incorporating an attention model over the query state anddemonstrations. FIST is a hierarchical skill transition model that learns to extract transferablehigh-level skill sets from demonstrations and leverages this skill knowledge during adaptation.Recently, PDT introduced a transformer-based few-shot learner, using the few-shot demonstrationas a prompt token. Similarly, HDT trains a hyper-network to generate adapter parameters for apre-trained decision transformer, adapting to few-shot demonstrations. Despite notable generalizationabilities, these few-shot IL approaches have only been studied within a single embodiment, limitingtheir applicability to real-world scenarios with heterogeneous embodiments.",
  "Experimental Setup": "Environment and Dataset. We evaluate the few-shot behavior cloning of unseen embodimentsand tasks within the DeepMind Control (DMC) suite , which includes continuous control tasksfeaturing diverse kinematic structures. We select 30 tasks from 10 embodiments as training tasks and8 tasks from 4 embodiments as held-out evaluation tasks. The evaluation tasks include three unseenembodiments (hopper, reacher-four, wolf) and one seen embodiment (walker), with the wolfbeing an additional embodiment introduced in . Our meta-training dataset is constructed using areplay buffer of an expert agent , consisting of up to 2000 demonstration trajectories for eachtask and embodiment. Each demonstration consists of state-action pairs over T = 500 timesteps,with rewards discarded for both episodic meta-learning and few-shot behavior cloning. For N-shotfew-shot behavior cloning, we use the last N demonstrations from the buffer. A full detail of thedataset is included in Appendix B.1.",
  "Evaluation Protocol.We evaluate all models with 20 different initial states and report themean and standard error. For evaluation metric, we use a normalized score calculated byscorerandom score": "expert scorerandom score, where each score represents the average cumulative rewards during the evalua-tion. The random score is obtained by evaluating a random agent using a uniform distribution policyover the action spaces. We evaluate the models every 1000 iterations of few-shot behavior cloningand report the best score. For models that use task-specific low-rank projection matrices, we searchthe rank parameter over {4, 8, 16} and report the best one. Throughout the experiments, we present5-shot learning results (N = 5) unless otherwise specified. Baselines. We compare our model with various Decision Transformer (DT)-based few-shotimitation learning approaches and two transformer-based modular policy learning approaches. In theDT-based models, we exclude return-to-go tokens from the input tokens to simulate behavior cloning.Since the DT architecture is not inherently designed to handle heterogeneous state and action spaces,we modify its input and output linear heads in an embodiment-specific manner and fine-tune them forunseen embodiments. DT-based Models. From-Scratch Decision Transformer (FS-DT) is a decision transformer thattrains a downstream task directly from randomly initialized weights. Multi-Task Decision Trans-former (MT-DT) is a variant of the decision transformer that trains multiple tasks with task-specific",
  "Ours49.16.187.21.682.54.991.75.167.33.156.18.850.810.684.35.771.1": "parameters and fine-tunes only these parameters for few-shot adaptation. For the task-specificparameters, we use the same parameters as (E,T )mused in our motion encoder. Prompt-basedDecision Transformer (PDT) adapts its policy by conditioning on the few-shot demonstrationsthrough prompting. We also report the performance of a variant of PDT that fine-tunes task-specificparameters (PDT+PEFT), similar to MT-DT. Hyper Decision Transformer (HDT) employs ahyper-network conditioned on few-shot demonstrations to generate parameters of the Adapter module applied to DT. Learning To Modulate (L2M) incorporates parameter-efficient fine-tuning (PEFT) techniques to DT architecture. While L2M is not directly proposed for few-shotimitation learning, we include this baseline since it uses a similar PEFT technique as ours. Modular Policy-based Models. We include two modular policy learning approaches, Meta-Morph and MTGv2 , which utilize a transformer architecture to encode joint-level states.Originally designed for zero-shot learning of locomotion tasks, we adapt these approaches byincorporating task-specific linear heads and fine-tuning them on few-shot demonstrations. Bothmodels are trained using the behavior cloning (BC) objective as described in . Implementation Details. We implement both the structure encoder and the motion encoder usinga 6-layer transformer with 4 attention heads and a hidden dimension of 512. Due to the quadraticcomputation cost of the transformer, we set the maximum history size of causal attention layers in theencoders to 10. The baseline models use the same transformer backbone. All models are trained for200,000 iterations on the meta-training dataset and fine-tuned for 10,000 iterations on the few-shotdemonstrations. For downstream embodiments that are structurally similar to a training task (e.g.,reacher-three and reacher-four), we initialize the encoders embodiment-specific parametersusing the trained parameters during fine-tuning. More details are included in Appendix B.2.",
  "Main Results": "shows the 5-shot behavior cloning results of the models on both unseen and seen embodiments.We observe that Meta-Controller consistently outperforms all baselines across various continuouscontrol problems, demonstrating its effectiveness. Existing few-shot imitation learning approachesthat lack an embodiment generalization mechanism, such as PDT and HDT, struggle to adapt tounseen embodiments, often performing comparably to the naive from-scratch baseline (DT-FS). Incontrast, modular policy learning approaches like MetaMorph and MTGv2 show better adaptation tounseen embodiments (e.g., hopper). However, their performance is still inferior to Meta-Controller,which can be attributed to the absence of a few-shot adaptation mechanism for unseen tasks. Notably, our model significantly outperforms all baselines in the challenging reacher-four embod-iment, where models must understand the notion of a goal position given only five demonstrations. shows that while baseline models converge to the pose in the demonstrations regardless ofthe goal position, our model successfully reaches the goal position and converges with a unique pose.We attribute this success to the modular nature of our model. Our state encoder effectively sharesknowledge about joint-level motions thanks to its parametrization, and the matching-based policynetwork flexibly exploits the local motor skills from a few demonstrations. We provide more resultsand analysis in Appendix C.",
  "Ours": ": Qualitative comparison on the hard task of the reacher-four embodiment, visualizingthe final states of the demonstrations and the rollout trajectories of each model. In this task, the robotmust move its limb tip to the goal position (visualized as a red ball). While most of the baselinesconverge to one of the poses in the demonstrations and ignore the goal position, our model accuratelysolves the task with a distinct pose from the demonstrations.",
  "Ablation Studies": "Ablation on the Architecture. To verify the effectiveness of each architectural component introducedin , we conduct an ablation study by progressively replacing each modulethe structureencoder fs, the motion encoder fm, and the matching module with a linear layer. To isolatethe effect of adaptation parameters, we replace fs with task-specific linear layers and fm withembodiment- and task-specific linear layers. summarizes the results. We observe that thestructure encoder fs is crucial for generalization to unseen embodiments, as performance dropsdrastically when it is removed (row 1 vs. row 3). This indicates that the structure encoder capturesmodular knowledge about various morphologies, transferrable to unseen embodiments. Combinedwith the structure encoder, the motion encoder further improves performance (row 2 vs. row 3),especially on the seen embodiment (walker). This shows that modeling the temporal relationshipsof joints is beneficial when the model understands the embodiment. Finally, the matching moduleconsistently improves performance (row 3 vs. row 4), particularly on challenging tasks such asthe hop task of hopper and the tasks of reacher-four This demonstrates the effectiveness of thematching architecture in preventing overfitting with few demonstrations. We provide more ablationstudies on architectural components in Appendix D.1. Ablation on the Parametrization. To analyze the impact of the embodiment-specific and task-specific parametrization introduced in the state encoder, we conduct an ablation study by removingthe adaptive parameters pEs , Es in the structure encoder and (E,T )mthe motion encoder. In this study,the matching module is used in all models, where we provide additional results without usingit in Appendix D.2. The results show that the model without adaptive parameters in the structureencoder (row 1) performs well in many tasks, likely due to the universality of joint-level input/outputrepresentation, which allows for compositional generalization. However, adding embodiment-specificparameters consistently improves performance across all tasks, indicating the benefit of capturingembodiment-specific knowledge. The model without adaptive parameters in the motion encoder (row2) remains competitive with the model including them (row 3) in many tasks but fails in the hop task",
  "Conclusion": "We addressed the challenging problem of few-shot behavior cloning with unseen embodimentsand tasks in continuous control. Our framework, Meta-Controller, effectively handles diverseembodiments using two key components: the state encoder and the matching-based policy network.Leveraging the modular nature of joint-level input/output representations, our state encoder extractstransferable features about the morphology and dynamics of the embodiment, capturing both specificand shared knowledge. The matching-based policy network uses these features to infer task structuresfrom few-shot demonstrations, enabling robust imitation without overfitting. Experiments showedthat our model generalizes well to unseen embodiments and tasks with only five demonstrations.",
  "Pytorch lightning. GitHub, 3, 2019": "E. Ben Zaken, Y. Goldberg, and S. Ravfogel. BitFit: Simple parameter-efficient fine-tuning fortransformer-based masked language-models. In Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume 2: Short Papers), 2022. A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess,A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge torobotic control. arXiv preprint arXiv:2307.15818, 2023. L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, andI. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advancesin neural information processing systems, 34:1508415097, 2021. K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q.Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. In InternationalConference on Learning Representations, 2020. C. Devin, A. Gupta, T. Darrell, P. Abbeel, and S. Levine. Learning modular neural networkpolicies for multi-task and multi-robot transfer. In 2017 IEEE international conference onrobotics and automation (ICRA), pages 21692176. IEEE, 2017.",
  "J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-drivenreinforcement learning, 2020": "H. Furuta, Y. Iwasawa, Y. Matsuo, and S. S. Gu. A system for morphology-task generalizationvia unified representation and behavior distillation. In The Eleventh International Conferenceon Learning Representations, 2022. A. Ghadirzadeh, X. Chen, P. Poklukar, C. Finn, M. Bjrkman, and D. Kragic. Bayesianmeta-learning for few-shot policy adaptation across robotic platforms. In 2021 IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), pages 12741280. IEEE,2021.",
  "D. Kim, S. Cho, S. Kim, C. Luo, and S. Hong. Chameleon: A data-efficient generalist for densevisual prediction in the wild. arXiv preprint arXiv:2404.18459, 2024": "D. Kim, J. Kim, S. Cho, C. Luo, and S. Hong. Universal few-shot learning of dense predictiontasks with visual token matching. In The Eleventh International Conference on LearningRepresentations, 2022. S. Kim, S. Shen, D. Thorsley, A. Gholami, W. Kwon, J. Hassoun, and K. Keutzer. Learned tokenpruning for transformers. In Proceedings of the 28th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pages 784794, 2022.",
  "W. Liu, A. Rabinovich, and A. C. Berg. Parsenet: Looking wider to see better. arXiv preprintarXiv:1506.04579, 2015": "R. K. Mahabadi, S. Ruder, M. Dehghani, and J. Henderson. Parameter-efficient multi-taskfine-tuning for transformers via shared hypernetworks. In Proceedings of the 59th AnnualMeeting of the Association for Computational Linguistics and the 11th International JointConference on Natural Language Processing (Volume 1: Long Papers), pages 565576, 2021. A. Majumdar, K. Yadav, S. Arnaud, J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, T. Wu,J. Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence?Advances in Neural Information Processing Systems, 36:655677, 2023.",
  "ALimitations and Broader Impacts": "Limitations. Despite the promising results, the Meta-Controller framework has several limitationsthat warrant further investigation. One significant limitation is its reliance on simulated environmentsfor training and evaluation, which may not fully capture the complexities and variabilities of real-world scenarios. This gap between simulation and reality could hinder the direct application of theframework to practical robotic tasks. Additionally, while the framework shows robust performancein the few-shot learning setting, it may still face challenges in environments with highly stochasticdynamics or in tasks that require extensive long-term planning. Another limitation is the assumptionof a unified joint-level representation, which, although effective for the embodiments tested, may notgeneralize well to robots with significantly different morphologies or actuation mechanisms. Fur-thermore, the computational complexity of the transformer-based architecture could pose scalabilityissues for real-time applications, especially on resource-constrained robotic platforms. Lastly, theethical implications of deploying such adaptable robotic systems need to be carefully considered toprevent potential misuse in sensitive areas. Broader Impacts. The proposed Meta-Controller framework for few-shot imitation learning hasseveral significant broader impacts. Firstly, its ability to generalize across various robot embodimentsand tasks with minimal demonstrations can greatly enhance the adaptability and versatility of roboticsystems in dynamic environments. This flexibility is crucial for deploying robots in real-world scenar-ios where they must quickly learn and adapt to new tasks without extensive retraining. Secondly, byleveraging a modular approach that decouples embodiment-specific and task-specific knowledge, theframework promotes efficient knowledge transfer, potentially reducing the computational resourcesand time required for training new robotic tasks. However, the deployment of such adaptive systemsalso raises concerns about their potential misuse. For instance, advanced robotic systems equippedwith this technology could be employed in surveillance or military applications, leading to ethicaland privacy issues. Therefore, it is essential to implement safeguards and ethical guidelines to ensurethe responsible use of these advancements. Furthermore, the reliance on simulated environmentsfor training and evaluation might limit the transferability of the results to real-world conditions,necessitating further research to bridge this gap.",
  "B.1More details on Datasets and Environment": "Embodiments and Tasks in DeepMind Control (DMC) suite. The DeepMind Control (DMC)suite benchmark is a collection of continuous control tasks implemented using the MuJoCophysics engine . It provides a variety of simulated environments to test and develop reinforcementlearning algorithms. The environments in DMC are designed to be diverse and challenging, promotingthe development of agents capable of handling a wide range of tasks. To reduce the complexityof the problem and facilitate knowledge acquisition from diverse tasks and embodiments, we onlyconsider embodiments that operate on a 2D coordinate space. We then augment the dataset with anumber of tasks used in , including two tasks from newly added embodiments, walk andrun) task of wolf, where we use the same reward function as the walk and run) task of walker,respectively. From a total of 38 tasks from 13 embodiments, we select an unseen embodiment withseen tasks (reacher-four, wolf), a seen embodiment with an unseen task (walker), and an unseenembodiment with unseen tasks (hopper) as held-out evaluation tasks for comprehensive analysis.For the exact setup of the newly added embodiments and tasks, please refer to the github repository ofNerveNet 3 and TD-MPC2 4. We list all embodiments and tasks used in our experiments in .",
  "wolfrunwalk": "Dataset Collection. For each task, we train the DrQ-v2 agent using an online replay buffer. Theagent is trained with the hyperparameters specified in , generating 1000 to 2000 demonstrationtrajectories for each task. For the wolf embodiment that was not originally included in the experi-ments of the DrQ-v2, we use the same hyperparameters as those used for the corresponding tasks ofwalker. After training, we filter out low-quality demonstrations with cumulative rewards smallerthan 10 to ensure the reliability of the training data. Data Processing. The states of various embodiments and tasks in DMC consist of two joint-level observations, such as the relative position and velocity of each joint, as well as extrasensoryobservations that are required to solve each task (e.g., in reacher embodiment, the distance betweenits tip position and the goal position is given as an extrasensory observation). The other per-jointattributes such as the movement axis or motion type can be extracted by the kinematic tree of eachembodiment provided in XML format. To ensure compatibility in practical applications, we use only the states defined in the default DMC setting. There are two different types of joints in theembodiments we consider: hinge joints and slide joints. Since these joints have distinct motioncharacteristics (angular motion and linear motion, respectively), we encode the states of each typeseparately to avoid semantic conflict. We standardize the relative position using a fixed axis (e.g., thez-axis in the xz plane) instead of using raw, unstandardized input. For joints with partially existingstates (e.g., the free joint of the torso), we zero-pad the non-existent states. To handle extrasensorystates provided by a specific embodiment, we concatenate all extrasensory observations into a singlevector and project them into a single joint token gt Rd. To this end, we introduce embodiment-specific linear projection layers for the embodiments having extrasensory observations. Lastly, foreach embodiment, we normalize the states by scaling them with min-max statistics calculated acrossthe data for each embodiment.",
  "B.2More Details on Implementation": "Episode Sampling Strategy. In the episode sampling procedure in Eq. (10), it is crucial to ensureconsistency between the query Q and the few-shot demonstration D for effective learning fromdemonstrations. Since we use a replay buffer as our meta-training dataset which encompassesdemonstrations obtained by learning agents at diverse stages, naively applying a uniform samplingstrategy on the entire replay buffer results in a high probability of selecting inconsistent queries anddemonstrations. Instead, to ensure task consistency in each episode during episodic meta-learning(.3), we sample the conditioning demonstrations D and the query data Q for Eq. (10) froma temporal segment of each replay buffer B(E,T ). In other words, the demonstrations in D and Qare obtained from expert agents at adjacent training epochs. In all experiments, we use a temporalsegment size of 10 for episodic meta-learning. Training Details. All models are trained for 200,000 iterations using the Adam optimizer and apoly learning rate scheduler with a base learning rate of 2 104. After training, we fine-tuneall models for 10,000 iterations with a fixed learning rate of 2 104, except for HDT , whichrequires a higher learning rate of 102. For meta-training, we train the model with 8 RTX A6000GPUs for approximately 25 hours, and we fine-tune the model on each task with 1 RTX A6000GPU for approximately 2 hours. As the baselines used in our experiments have not been demonstrated in our few-shot behaviorcloning setting with unseen embodiments and tasks, we modify their base architecture or learningframework for fair comparison. When fine-tuning HDT, we first adapt the embodiment-specific headparameters of HDT with the episodic meta-learning objective, as it does not generate appropriateadapter parameters with untrained head parameters of novel embodiment. We then obtain adapterparameters by forwarding every temporal chunk of the few-shot demonstration with the hyper-networkand initialize adapter parameters by the mean of the outputs. Then, following the HDT procedure,we fine-tune only the adapter parameters using the BC objective. For fine-tuning the PDT+PEFTbaseline, we apply PEFT techniques to the PDT model meta-trained without task-specific parameters.For the few-shot adaptation of L2M, the learnable modulation pool and the corresponding low-rankprojection matrices are fine-tuned. Architectural Details.For transformer models, we employ a Pre-LN transformer layer ,which consists of a self-attention layer followed by a 2-layer MLP with 4 hidden dimension size.We use GELU activation for the self-attention layers. To enhance the expressiveness of ourmatching-based policy network, we implement the matching module in Eq (8) with multi-headcross-attention , following Kim et. al. and Kim et. al. . We list the hyper-parameters ofMeta-Controller in . Implementation Framework. We implemented our model based on PyTorch Lightning whichsupports both Intel Gaudi-v2 (HABANA) and NVIDIA AI accelerators (CUDA). We provide thecode for both systems on separate branches in the GitHub repository.",
  "C.1Additional Results on 3-Shot Behavior Cloning": "To investigate performance in lower-shot settings, we evaluate our model with 3-shot behavior cloningon tasks presented in table 1. As shown in , our model consistently outperforms baselineapproaches, demonstrating robustness even with fewer demonstrations. This result highlights themodels adaptability to fewer examples while maintaining reliable generalization across differenttasks and embodiments.",
  "C.2Additional Results on Embodiment Variations": "We further assess model performance on six additional embodiments by adjusting physical parameters,such as joint lengths (e.g., foot length of hopper) and ratios among joints (e.g., calf-thigh ratio ofhopper, front leg-back leg ratio of wolf). These modifications can make the tasks harder, as theoriginal embodiments are optimized for specific tasks. In , we compare Meta-Controller withthe high-performing baselines from . Our model consistently outperforms all baselines in thesechallenging variants, demonstrating the robustness and adaptability of our approach to variations inembodiment configurations.",
  "C.3Additional Qualitative Results": "We provide additional qualitative results about the behavior of the few-shot agents in the tasks weused for evaluation. In -18, we plot the rendered images of a single evaluation rollout bythe dm_control library from the initial state (t = 0) to the states unrolled by each agent, byskipping every ten timesteps in the visualization.",
  "C.5Visualization of Learned Embeddings": "To gain insights into how our model represents different embodiments, we provide a t-SNE visualiza-tion of embedding space of features obtained by structure encoder fs in . First, we observethat the embeddings are clustered by the joints of each embodiment, and the clusters correspondingthe same embodiment are located nearby. Also, we note that the embeddings of slide joints (e.g.,cartpole, cartpole-two, cup, pointmass) and the embeddings of hinge joints (e.g., reacher,reacher-three, walker, acrobot, cheetah, pendulum) are separated in the right and the leftregions. Thus, the state encoder captures both embodiment-specific and joint-specific knowledge,providing rich features to the policy network.",
  "C.6Computational Complexity Analysis": "presents the inference time comparisons between our model and VC-1 , which is a ViT-based foundation model designed for behavior cloning of continuous control tasks. Despite VC-1shigh performance in visual tasks, its transformer-based architecture incurs notable computationalcosts. Our model achieves faster inference times, highlighting its efficiency relative to VC-1. Wealso note that models like RT-2 , which are designed for real-time robotic manipulation, typicallyinvolve architectures such as a 40-layer ViT and a large language model (LLM) with 3 billionparameters, requiring significantly higher computational resources than our approach. Additionally, provides a breakdown of relative inference times for each module in our model,demonstrating that the majority of computation time is allocated to the transformer-based encodersand decoders. In this context, advancements in optimizing transformers, such as sparse attentionmechanisms , model pruning , and knowledge distillation , can enhance inference speeds.Since these techniques are orthogonal to ours, they can be naturally incorporated into our method forresource-constrained robotic platforms that require high speed inference.",
  "C.7Failure Case Analysis": "We present visualizations of scenarios where the Meta-Controller performs less effectively andcumulative rewards over time for each scenario in Figures 8 and 9, respectively. In these failure cases,we observed that agents struggle to obtain rewards until they reach a specific posture. Once theyachieve this posture (highlighted by the red box in ), they begin to solve the task effectively.This pattern is also reflected in , where rewards remain near zero until a certain timestep,after which they rise consistently. This result implies that encouraging the agent to reach statessimilar to those in the demonstrations (e.g., via exploration strategies) would improve performance inchallenging few-shot scenarios.",
  "C.8Robustness Analysis under Noise": "To assess robustness in noisy environments, we introduce varying levels of noise to the transitiondynamics and measure the resulting performance. Random noise sampled from U[n, n] was addedto the agents action at each timestep, with three noise levels n [2%, 5%, 10%] of the action range. plots the rewards of our model at each noise level compared to experts. The results indicate that our method maintains its performance across many tasks as noise levelsincrease, showing its robustness under stochastic environments. Interestingly, for tasks such asreacher-four, the performance increases with higher noise levels, likely due to the exploration effectinduced by stochastic transitions. This robustness under stochastic dynamics indicates potential forthe models application in real-world scenarios where environmental variability is common.",
  "D.1Additional Ablation Studies on Architectural Components": "presents the additional ablation studies on key architectural components:structure encoderfs, motion encoder fm, action encoder g, action decoder h and matching module . Consistent withthe discussion in .3, we observe that removing the structure encoder fs (row 2) significantlyreduces performance on both seen and unseen embodiments. This indicates that the structure encodercaptures essential morphology-related knowledge that enables cross-embodiment generalization. We also ablate the action encoder g and action decoder h (row 3), observing that removing them resultin decreased adaptability and performance. The action encoder plays a key role in transforming rawaction into a latent representation, which effectively enhancing expressibility of the policy networkand enables adaptation to various unseen tasks with non-convex relationships between states andactions. Additionally, by encoding actions along the temporal axis, the model can construct a pool oftransferrable action features related to local motor skills, which facilitates efficient transfer to unseentasks that share modular skills but involve different skill combinations.",
  "D.2Additional Ablation studies on Adaptation Mechanism": "To further analyze the adaptation mechanism of our model, we conduct an additional ablation studyby ablating the adaptive parameters in the state encoder without using the matching module in thepolicy network. On average, using both adaptive parameters (row 3) achieves the best performance.However, we note that the model variant without adaptive parameters in the motion encoder (row 2)achieves higher performance than using the parameters (row 3) in many tasks. We conjecture thatthis is due to over-fitting on the few-shot demonstrations, as the adaptive parameters in the motionencoder are both specific to the embodiment and task and more prone to over-fitting. However, as wediscussed with in .3, such trends are not found in general (except for the easy taskof reacher-four). This indicates that in few-shot learning settings, the PEFT techniques must beemployed together with a robust architecture such as matching to exploit its effectiveness maximally.",
  "D.3Additional Ablation Studies on Meta-Training Task Composition": "To further understand how the composition of meta-training tasks affects performance, we conductexperiments with varying subsets of embodiments and tasks in . We select 3 combinationsof training tasks, where we remove 4 embodiments from the original 10 embodiments. Then, weperformed 5-shot behavior cloning experiments on the 8 tasks presented in table 1.",
  "Task (T )hophop-bwd.standwalkruneasyhardwalk-bwd": "w/o E115.9 5.576.6 6.871.5 7.1 83.2 4.162.1 5.0 82.1 7.641.8 9.870.7 6.063.0w/o E216.3 6.085.2 2.881.6 7.3 83.9 5.270.1 3.4 5.6 6.94.6 5.178.8 7.353.3w/o E326.6 8.088.8 1.168.6 6.1 83.6 6.459.0 2.9 4.6 6.53.8 4.181.8 5.152.1Use All Embodiments 49.1 6.187.2 1.682.5 4.9 91.7 5.167.3 3.1 56.1 8.850.8 10.6 84.3 5.771.1 findings also show that it is crucial to include embodiments with morphology and dynamics similarto the downstream tasks in the meta-training dataset. For example, removing the reacher-threetask (as seen in row 2 and row 3) significantly drops the performance of the reacher-four task.This result reveals that embodiments with similar dynamics or morphological features can facilitatemore effective knowledge transfer during meta-testing, suggesting that the diversity of data greatlyimpacts performance."
}