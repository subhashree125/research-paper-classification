{
  "Abstract": "Inrecentyears, DynamicSparseTraining(DST)hasemergedasanalternativetopost-training pruning for generating efficient models. In principle, DST allows for a morememory efficient training process, as it maintains sparsity throughout the entire train-ing run. However, current DST implementations fail to capitalize on this in practice.Becausesparsematrixmultiplicationismuchlessefficientthandensematrixmultipli-cation on GPUs, most implementations simulate sparsity by masking weights. In thispaper, we leverage recent advances in semi-structured sparse training to apply DSTin the domain of classification with large output spaces, where memory-efficiency isparamount. With a label space of possibly millions of candidates, the classificationlayer alone will consume several gigabytes of memory. Switching from a dense toa fixed fan-in sparse layer updated with sparse evolutionary training (SET); however,severely hampers training convergence, especially at the largest label spaces. Wefind that poor gradient flow from the sparse classifier to the dense text encoder makeit difficult to learn good input representations. By employing an intermediate layer oradding an auxiliary training objective, we recover most of the generalisation perfor-mance of the dense model. Overall, we demonstrate the applicability and practicalbenefits of DST in a challenging domain characterized by a highly skewed labeldistribution that differs substantially from typical DST benchmark datasets whichenables end-to-end training with millions of labels on commodity hardware.",
  "Introduction": "Recent research has demonstrated that densely-connected neural networks contain sparsesubnetworks often dubbed winning lottery tickets that can deliver performance comparable tothe full networks but with substantially reduced compute and memory demands. Unlike conventionaltechniques that start with a trained dense model and employ iterative pruning or one-shot pruning,Dynamic Sparse Training (DST) initializes the a sparse architecture and dynamically exploressubnetwork configurations through periodic pruning and regrowth, typically informed with heuristicsaliency criteria such as weight and gradient magnitudes. This approach is particularly advantageousin scenarios constrained by a fixed memory budget during the training phase, making DST viableacross various domains . For instance, in reinforcement learning , DST has beenshown to significantly outperform traditional dense models. Additionally, models trained using DSToften exhibit enhanced robustness . However, the application of DST comes withchallenges, notably prolonged training times; for example, RigL and ITOP require up to fiveand two times as many optimization steps during training, respectively, to match the generalisation",
  "DenseStatic SparsityDST+AuxiliaryDST": ": Model configurations and performance comparisons at various sparsity levels. The leftpanel illustrates our model configurations: S represents a semi-structured fixed fan-in sparse layer,W denotes an intermediate layer, and Aux refers to an auxiliary head of meta-classifiers. Theseconfigurations help maintain performance as the label space size increases from 31K to 670K andbeyond. The right panel demonstrates the comparative precision at 1 for our model against othermethods across increasing levels of sparsity on the Amazon670K dataset. performance of dense networks at high sparsity levels (80%). The prolonged training time in theseworks is often linked to the need for in-time overparameterization and poor gradient flow in sparsenetworks. Recent advances aimed at improving gradient flow have been introduced tomitigate these extended training durations, enhancing the practicality of DST methodologies. In this paper, we investigate the integration of DST into extreme multi-label classification (XMC). XMC problems are characterized by a very large label space, with hundreds of thousandsto millions of labels, often in the same order of magnitude as the number of training examples. Thelarge label space in such problems makes calculating logits for every label a very costly operation.Consequently, contemporary XMC methodologies utilize modular andsampling-based techniques to achieve sublinear compute costs. However, these strategies do not helpin addressing the immense memory requirement associated with the classification layer, which canbe enormous: for an embedding dimension of 768, one million labels lead to a memory consumption ofabout 12 GB taking into account weights, gradients, and optimizer state.1 Memory efficiency in XMChas been pursued in the context of sparse linear models or by using label-hashing ,but such methods do not yield predictive performance competitive with modern transformer-baseddeep networks. Schultheis and Babbar demonstrated that applying a DST method to the extremeclassification layer can lead to substantial memory savings at marginal accuracy drops; however, thatwork presupposed the existence of fixed, well-trained document embeddings which output the hiddenrepresentations used by the classifier, whereas in a realistic setting these need to be trained jointly. Recently, Jain et al. demonstrated that full end-to-end training of XMC models can be verysuccessful, given sufficient computational resources. To make this accessible to consumer-gradehardware, we propose to switch the dense classification layer to a DST-trained sparse layer. Notonly does this result in a training procedure that allows XMC models to be trained in a GPU-memoryconstrained setting, but it also provides an evaluation of DST algorithms outside typical, well-behavedbenchmarks. This is particularly important since recent works have found that sparse trainingalgorithms that appear promising on standard benchmark datasets may fail to produce adequate resultson actual real-world tasks. As such, we introduce XMC problems with their long-tailed labeldistribution , missing labels , and general training data scarcity issues as a new setting to challenge current sparsity approaches. In fact, direct application of existing DST methods yields unsatisfactory results on XMC tasks due totypically noisy data and poor gradient signal propagation through the sparse classifier, slowing trainingconvergence to an extent that it is not practically useful. Consequently, we follow Schultheis andBabbar and adapt the model architecture by integrating an intermediate layer that is larger than the",
  "Using mixed precision with torch.amp has little benefit here, because optimizer states and model parametersare still maintained in 32 bit, and only down-converted to speed-up matrix multiplications": "embedding from the encoder but still significantly smaller than the final output layer. While this wasfound to be sufficient to achieve good results with fixed encodings, it fails if the encoder is a trainabletransformer for label spaces with more than one hundred thousand elements, particularly athigh levels of sparsity. The primary challenge arises from the noisy gradients prevalent at the onsetof training, which are inadequate for guiding the fine-tuning of the encoder effectively. To mitigatethis issue, we introduce an auxiliary loss. This loss uses a more coarse-grained objective, assigninginstances to clusters of labels, where scores for each cluster are calculated using a dense classificationlayer. This auxiliary component stabilizes the gradient flow and enhances the encoders adaptabilityduring the critical early phases of training and is turned off during later epochs to not interfere withthe main task. illustrates the architectural changes that ensure good training performanceat different label space sizes and sparsity levels. To materialize actual memory savings, we propose SPARTEX, which uses semi-structured spar-sity with a fixed fan-in constraint, together with magnitude-based pruning and randomregrowth (SET ), which does not require any additional memory buffers. In our experiments, weshow that SPARTEX achieves a 3.4-fold reduction of GPU memory requirements from 46.3 to 13.5 GiBfor training on the Amazon-3M dataset, with only an approximately 3% reduction in predictiveperformance. In comparison, a nave parameter reduction using a bottleneck layer (i.e., a low-rankclassifier) at the same memory budget decreases precision by about 6%.",
  "Optimized hardware utilization: We provide PyTorch bindings for custom CUDA kernels2": "which enable a streamlined integration of memory-efficient sparse training into an existingXMC pipeline. This implementation enables the deployment of our training methodologies onconventional, commercially available hardware, thus democratizing access to state-of-the-artXMC model training. Robustness to Label distribution challenges: Our empirical results demonstrate that theDST framework, as adapted and optimized by our modifications, can effectively managedatasets characterized by label imbalances and the presence of missing labels, with minimalperformance degradation for tail labels.",
  "Background": "Problem setupGiven a multi-label training dataset with N samples, D = {(xi,Pi)Ni=1}, whereL represents the total number of labels, and Pi [L] denotes a subset of relevant labels associatedwith the data point xi . Typically, the instances are text based, such as the contents of a Wikipediaarticle or the title of a product on Amazon with labels corresponding to Wikipedia categoriesand frequently bought together products, respectively, for example. Traditional XMC methods usedto handle labels the same way as is typically done in other fields, as featureless integers. However, the labels themselves usually carry some information, e.g., a textual representation, as thefollowing examples, taken from (i) LF-AmazonTitles-131K (recommend related products given aproduct name) and (ii) LF-WikiTitles-500K (predict relevant categories, given the title of a Wikipediapage) illustrate:",
  "Consequently, more recent XMC approaches have started to take these label features into accountto alleviate the data scarcity problems": "XMCandDSTTheXMCmodelsaretypicallycomprisedoftwomaincomponents: (i)anencoderE :Rd, which embeds data points into a d-dimensional real space, primarily utilizing a transformer ar-chitecture and (ii) A One-vs-All classifier W ={wl}l[L], where wl denotes the classifier for labell, integrated as the last layer of the neural network in end-to-end training settings. In a typical DST sce-nario, one would sparsify the language model used as the encoder, potentially even leaving the classifierfully dense . However, in XMC, most of the networks weights are in the classifier layer, so in orderto achieve a reduction in memory consumption, its weight matrix Ws ={wsl }l[L] must be sparsified. This sparse layer Ws is then periodically updated in a prune-regrow-train loop, that is, every T steps, afraction of active weights are pruned and the same number of inactive weights are regrown. The updatedsparse topology is then trained with regular gradient descent for the next T steps. There are manypossible choices for pruning and regrowth criteria ; to keep memory consumption low, however, weneed to choose a method that does not require auxiliary buffers proportional to the size of the dense layer.Thisexcludesmethodssuchasrequiringsecond-orderinformation, ortrackingofdensegradientsorotherper-weightinformation. Evcietal.arguethatRigLonlyneedsdensegradientsinanephemeral capacity they can be discarded as soon as the regrowth step for the current layer is done, butbeforetheregrowstepofthenextlayerisstartedbutintheXMCsetup, theprohibitivelylargememoryconsumption arises already from a single layer. Therefore, we select magnitude-based pruning andrandom regrowth . Magnitude-based pruning has been shown to be a remarkably strong baseline .",
  "Memory-Efficient Training: Fixed Fan-In Sparse Layer": "Unstructured sparsity is notoriously difficult to speed-up on GPUs , and consequently most DSTstudies simulate sparsity by means of a binary mask . On the other hand, highly structuredsparsity, such as 2:4 sparsity , enjoys hardware acceleration and memory reduction , butresults in deteriorated model accuracy compared to unstructured sparsity . As a compromise,semi-structured sparsity imposes a fixed fan-in to each neuron. This eliminates workimbalances between different neurons, leading to an efficient and simple storage format for sparseweights, where each sparse weight needs only a single integer index. For 32-bit floating point weightswith 16-bit indices (i.e., at most 65k features in the embedding layer), this leads to a 50% storageoverhead for sparse weights; however, for training, gradient and two momentum terms are needed,which share the same indexing structure, reducing the effective overhead to just 12.5%. While fixed fan-in provides substantial speed-ups for the forward pass, due to the transposed matrixmultiplication required for gradients, it does not give any direct benefits for the backwards pass.Fortunately, when used for the classification matrix, the backpropagated gradient is the loss derivative,which will exhibit high activation sparsity if the loss function is hinge-like . In the enormous labelspace of XMC, for each instance only a small subset of labels will be hard negatives. The rest willbe easily classified as true negatives, and not contribute to the backward pass.",
  "Improved Gradient Flow: Auxiliary Objective": "We find that, despite using a fully dense network, training the encoder using gradients backpropagatedfrom a sparse classification layer requires more optimization steps to converge compared with to adense classification layer. This compounds with the already-increased number of epochs requiredfor DST , further increasing the training duration of end-to-end XMC training, which requireslonger training than comparable modularized or shortlisting-based methods . Furthermore, theintermediate activations in the transformer-based encoder also take up a considerable amount ofGPU memory, so to meet memory budgets, we may need to switch to smaller batch sizes or employactivation checkpointing, increasing the per-step time. Therefore, we need to improve gradient flow through the sparse layer. Schultheis and Babbar inserted a large intermediate layer preceding the actual classifier to achieve significant improvementsin performance. While this method is sufficient to achieve good results with fixed encodings, weobserve that it fails to perform well if the encoder is a trainable transformer for label spaceswith more than one hundred thousand elements, particularly for high sparsity levels. Therefore, weinstead propose to use the label shortlisting task that is typical for XMC pipelines as an auxiliaryobjective to generate informative gradients for the encoder. Rethinking the role of clusters and meta-classifier in XMCMany prevailing XMC methods,apart from learning the per-label classifiers W = {wl}l[L] for the extreme task, also employ ameta-classifier. The meta-classifier learns to classify over clusters of similar labels that are created byrecursively partitioning the label set into equal parts using, for example, balanced k-means clustering. These meta-classifiers are primarily used for label shortlisting or retrieval priorto the final classification or re-ranking at the extreme scale. We investigated the impact on the finalperformance of the extreme task when the labels are randomly assigned to the clusters (instead ofthe following the k-means objective). We observed that such reassignments do not negatively affectthe extreme tasks performance (detailed of this observation are shown in Appendix E). This leadsus to hypothesize that beyond merely shortlisting labels, meta-classifier branch of the XMC trainingpipelines provides useful gradient signals during encoder training, which is particularly crucial forlarger datasets with O(106) labels such as Amazon-670K () and Amazon-3M. 0.00.20.40.60.81.0",
  ": Gradient Flow of the encoder during train-ing with and without Auxiliary Objective": "Auxiliary objective and DST convergence forXMCTowards addressing the challenge ofgradient instability, we augment our trainingpipeline with an additional meta-classifier branchwhich aids gradient information during back-propagation. This is especially useful duringthe initial training phase where the fixed-fan-insparse layer tends to encounter difficulties. Im-portantly, in our model the output layer operatesindependently of the meta-classifiers outputs,enabling a seamless end-to-end training process. Although a meta-classifier assists during the ini-tial stages of training, maintaining it throughoutthe entire training process can deteriorate the en-coders representation quality. This degradationoccurs because the task associated with the meta classifiers differs from the final task, yet both share thesame encoder. Similar observations have been noted in related studies . To address this issue, weimplement a decaying scaling factor on the auxiliary loss, gradually reducing its influence as trainingprogresses. The impact of the auxiliary branch on the norm of the gradient in the encoder is demonstrated in for the Amazon-670K dataset. The larger gradient signal speeds up initial learning, but it ismisaligned with the true objective, so is gradually turned off at around 200k steps. Furthermore, theimprovement in prediction performance, as reflected in (right panel), reinforces the quality ofgradient as compared to the training pipeline without the auxiliary objective.",
  "Datasets": "In this study, we evaluate our proposed modifications of DST under the extreme classificationsetting across a diverse set of large-scale datasets, including Wiki10-31K , Wiki-500K ,Amazon-670K , and Amazon-3M . The datasets are publicly available at the ExtremeClassification Repository3. These were selected due to their inherent complexity and the challengesposed by their long-tailed label distributions, which are emblematic of real-world data scenarios andtest the robustness of DST methodologies. Further validation of our approach is conducted using the : Statistics of XMC Datasets with and without Label Features. This table presents a comparisonacross various datasets, detailing the total number of training instances (N), unique labels (L), numberof test instances (N ), average label count per instance (L), and average data points per label (L).",
  ". Dense Models : Consistent with traditional DST evaluations, we compare the performance ofour sparse models against their dense counterparts": "2. Dense Models with bottleneck layer : This category (referred to as Dense BN in )includes dense models with the same number of parameters as our proposed DST methodby having a bottleneck layer with the same dimensionality as the FFI size. This ensures thatcomparisons focus on the impact of sparsity rather than differences in model size or capacity. 3. XMC Methods: For datasets devoid of label features, we benchmark against the lat-est transformer-based models such as CASCADEXML , LIGHTXML , and XR-TRANSFORMER. For datasets that incorporate label features, our comparison includes leadingSiamese methods like SIAMESEXML and NGAME, as well as other relevanttransformer-based approaches. Notably, RENEE qualifies as both a dense model and a state-of-the-art XMC method. However, insome instances, RENEE employs larger encoders (e.g., Roberta-Large ). To maintain consistencyand fairness in our evaluations, we exclude configurations employing larger encoders from this analysis.For conceptual validation, we used RIGL on datasets with label spaces up to 670K. As is standard in XMC literature, we compare the methods on metrics which only consider predictionat top-k slots. This includes : Precision@k and its propensity-scored variant (which is more sensitive toperformance on tail-labels). The details of these metrics are given in Appendix A.",
  "Empirical performance": "presents our primary results on the datasets, compared with the aforementioned baselines.The performance metrics for XMC baselines are reported from their original papers. However, forpeak memory consumption, we re-ran these baselines in half precision with the same batch size, asall baselines are also evaluated in half precision. Following DST protocols, we extended the trainingduration for RIGL, Dense Bottleneck, and our method to twice the number of steps used for densemodels. Certain baselines (referred to as OOM) could not scale to the Amazon-3M dataset. Ourresults demonstrate that our method significantly reduces memory usage while maintaining competitiveperformance. On the Amazon-3M dataset, our approach delivers comparable performance to dense : Comparison with different methods. Comparing our sparse model against its dense counterpartand with state-of-the-art XMC methods on Wiki10-31K, Wiki-500K, Amazon-670K and Amazon-3Mdatasets. Mtr(GiB) indicates peak GPU memory consumption during training.",
  "DENSE BN-44.539.736.14.0-47.044.642.713.1RIGL8345.238.736.012.483---OOMSPARTEX8347.141.838.03.78350.247.144.813.5": "models while achieving a 3.4-fold reduction in memory usage and a 5.-fold reduction compared tothe XMC baseline. Furthermore, within the memory-efficient model regime, our method consistentlyoutperforms the Dense Bottleneck model. To further validate the robustness of our approach, weevaluated it on the label features datasets, as shown in . Notably, as the label space size increases,we need to adjust to a comparatively lower sparsity to maintain performance, discussed in detail insubsequent sections.",
  ": left: Comparison of performance declines as the size of the label space increases, given afixed sparsity. right: Performance of our model at different epochs, across various sparsity ratios": "in maintaining performance, particularly in the high sparsity regime. Conversely, at lower sparsitylevels (67%), the benefit of the auxiliary objective diminishes. In the context of XMC problems, theperformance of DST degrades as the label space size increases. (left) depicts the performancedegradation of our approach relative to a dense baseline across datasets with increasing label space sizes:31K, 131K, 500K, 670K, and 3M (detailed in the ), all evaluated at 83% sparsity. Interestingly,for the wiki31K dataset, we observe a performance improvement, potentially due to the lower numberof training samples relative to the label space size. Compared to other methods with equivalent memoryrequirements, our approach demonstrates superior performance retention at larger label space sizes.",
  "Effect of Rewiring Interval": "The rewiring interval is crucial for balancing the trade-off between under-exploration and unreliableexploration. In XMC problems, tail label performance is particularly significant due to its applicationdomain. The rewiring interval directly influences how frequently each parameter topology encounterstail label examples before updates. In this section, we focus on assessing the performance impactof various rewiring intervals, including their effect on tail labels. We conducted experiments onthe LF-AmazonTitles-131K dataset using rewiring intervals T . Thecorresponding results for P@1 and PSP@1 metrics are illustrated in Figures 4 left and right, respectively,with a fixed rewiring fraction of 0.15. Our findings reveal that both P@1 and PSP@1 improve as theinterval increases up to a certain point. Interestingly, while P@1 shows a decline beyond this threshold,PSP@1 continues to rise. This divergence suggests that larger rewiring intervals, despite potentiallylimiting the diversity of topology exploration, provide each topology sufficient exposure to more taillabels, thereby improving model performance in handling rare categories.",
  "Performance on Tail Labels": "presents a comparison of Propensity-Scored Precision (PSP) for various Extreme Multi-label Classification (XMC) models, including ATTENTIONXML , CASCADEXML , Dense,Dense Bottleneck, and our proposed method, across four benchmark datasets: Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3M. For the Wiki10-31K dataset, our model achieves PSP@1 of13.2, PSP@3 of 15.1, and PSP@5 of 16.4, surpassing the Dense model. On the Wiki-500K dataset,our method records a PSP@1 of 31.9, outperforming other XMC models and closely trailing thetop-performing approaches. These findings underscore our models consistent performance acrossvaried datasets, frequently exceeding or closely competing with XMC and Dense benchmarks. Itsnoteworthy that the performance of ATTENTIONXML on Wiki10-31K is attributed to its utilization ofan LSTM encoder, which is particularly advantageous given the datasets smaller number of trainingsamples relative to its label space. This configuration also explains our models superior performancecompared to the Dense model, which incorporates a form of regularization. In comparisons involving : Propensity-Scored Precision (PSP) comparison of our sparse model with its dense counterpartand state-of-the-art XMC methods on the Wiki10-31K, Wiki-500K, Amazon-670K, and Amazon-3Mdatasets. The same sparsity levels as mentioned in previous tables are used.",
  "Impact of Varying Sparsity Levels": "illustrates the impact of varying sparsity levels (ranging from 50% to 96%) in conjunctionwith the use of auxiliary loss for Amazon-670K dataset. As sparsity levels increase, there are benefitsin memory usage, training time, and inference time; however, performance metrics simultaneouslydecline. Additionally, the importance of auxiliary loss becomes particularly significant at highersparsity levels.",
  "Sensitivity to Auxiliary Loss cut-off epochs": "We employ auxiliary loss with an initial scalar weight that decays until a specified cut-off epoch. illustrates the models final performance at various cut-off epochs for two sparsity levels. Avalue of 0 (No aux) indicates the absence of auxiliary loss, while No cut-off signifies its applicationthroughout training. Our analysis reveals that prolonging the auxiliary loss beyond optimal cut-offepochs adversely affects performance for both sparsity levels. Notably, maintaining the auxiliary lossthroughout training leads to performance deterioration, resulting in scores lower than those achievedwithout its use. Rewiring Interval ( T) 0.420 0.425 0.430 0.435 0.440 0.445 Precision at 1 (P@1) rewire fraction=0.15 FFI+AuxRigL Rewiring Interval ( T) 0.345 0.350 0.355 0.360 0.365 0.370 0.375 0.380 0.385 Propensity scored Precision at 1 (PSP@1) rewire fraction=0.15 FFI+AuxRigL",
  "DST with Fixed Embedding vs End-to-End Training": "In , we compare the performance of models using fixed embeddings with trained end-to-endusing DST on the Wiki-500K and Amazon-670K datasets. End-to-end training yields consistentimprovements over fixed embeddings across all metrics, with significant gains in P@1 (an increase of3.1% on Wiki-500K and 4.5% on Amazon-670K). These highlight the need of enabling the model toadapt its representations while training for the best possible performance.",
  "Conclusion and future work": "In this paper, we demonstrated the feasibility of DST for end-to-end training of classifiers with hundredsof thousands of labels. When appropriately adapted for XMC problems with fixed fan-in sparsityand an auxiliary objective, DST offers a significant reduction in peak memory usage while deliveringsuperior performance compared to bottleneck-based weight reduction. It is anticipated that the Pythonbindings of the CUDA kernels will be useful for the research community in making their existing andforthcoming deep XMC pipelines more memory efficient. We hope that our work will enable furtherresearch towards developing techniques which could be (i) combined with explicit negative miningstrategies, and (ii) used in conjunction with larger transformer encoders such as Roberta-large.",
  "Limitations and societal impact": "For XMC tasks, this paper attempts to explore the landscape of sparse neural networks, which can betrained on affordable and easily accessible commodity GPUs. While the proposed scheme is able toachieve comparable results to state-of-the-art methods at a fraction of GPU memory consumption, itis unable to surpass the baseline with dense last layer on all occasions. The exact relative decline inprediction performance, when our proposed adaptations of Fixed Fan-In and auxiliary objective areemployed, is shown in . While we do not anticipate any negative societal impact of our work, it is expected that it will furtherenable the exploration of novel training methodologies for deep networks which are more affordableand easily accessible to a broader research community outside the big technology companies.",
  "Acknowledgements": "We thank Niki Loppi of NVIDIA AI Technology Center Finland for useful discussions on the sparseCUDA kernel implementations. YI acknowledges the support of Alberta Innovates (ALLRP-577350-22, ALLRP-222301502), the Natural Sciences and Engineering Research Council of Canada (RGPIN-2022-03120, DGECR-2022-00358), and Defence Research and Development Canada (DGDND-2022-03120). This research was enabled in part by support provided by the Digital Research Alliance ofCanada (alliancecan.ca). RB acknowledges the support of Academy of Finland (Research Council ofFinland) via grants 347707 and 348215. NU acknowledges the support of computational resourcesprovided by the Aalto Science-IT project, and CSC IT Center for Science, Finland.",
  "Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on Learning Representations, 2018": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivityand the lottery ticket hypothesis. In International Conference on Machine Learning, pages 32593269.PMLR, 2020. Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir. Proving the lottery ticket hypothesis:Pruning is all you need. In International Conference on Machine Learning, pages 66826691. PMLR, 2020. Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and ZhangyangWang. The lottery tickets hypothesis for supervised and self-supervised pre-training in computer visionmodels. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages1630616316, 2021. Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu, andAntonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired bynetwork science. Nature communications, 9(1):2383, 2018. Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery: Makingall tickets winners. In International conference on machine learning, pages 29432952. PMLR, 2020. Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need denseover-parameterization? in-time over-parameterization in sparse training. In International Conference onMachine Learning, pages 69897000. PMLR, 2021.",
  "Anastasia Dietrich, Frithjof Gressmann, Douglas Orr, Ivan Chelombiev, Daniel Justus, and Carlo Luschi.Towards structured dynamic sparse pre-training of bert. arXiv preprint arXiv:2108.06277, 2021": "Shiwei Liu, Iftitahu Nimah, Vlado Menkovski, Decebal Constantin Mocanu, and Mykola Pechenizkiy.Efficient and effective training of sparse recurrent neural networks. Neural Computing and Applications, 33:96259636, 2021. Yiqin Tan, Pihe Hu, Ling Pan, Jiatai Huang, and Longbo Huang. Rlx2: Training a sparse deep reinforcementlearning model from scratch. In The Eleventh International Conference on Learning Representations, 2022. Laura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training in deepreinforcement learning. In International Conference on Machine Learning, pages 77667792. PMLR, 2022.",
  "Ozan zdenizci and Robert Legenstein. Training adversarially robust sparse networks via bayesian connec-tivity sampling. In International Conference on Machine Learning, pages 83148324. PMLR, 2021": "Tianlong Chen, Zhenyu Zhang, Santosh Balachandra, Haoyu Ma, Zehao Wang, Zhangyang Wang, et al.Sparsitywinningtwice: Betterrobustgeneralizationfrommoreefficienttraining. InInternationalConferenceon Learning Representations, 2021. James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand:Compressing deep networks can improve out-of-distribution robustness. Advances in neural informationprocessing systems, 34:664676, 2021. Shiwei Liu, Tianlong Chen, Zahra Atashgahi, Xiaohan Chen, Ghada Sokar, Elena Mocanu, MykolaPechenizkiy, Zhangyang Wang, and Decebal Constantin Mocanu. Deep ensembling with no overhead foreither training or testing: The all-round blessings of dynamic sparsity. In 10th International Conference onLearning Representations, ICLR 2022. OpenReview, 2022. Utku Evci, Yani Ioannou, Cem Keskin, and Yann Dauphin. Gradient flow in sparse neural networks andhow lottery tickets win. In Proceedings of the AAAI conference on artificial intelligence, volume 36, pages65776586, 2022. Ilan Price and Jared Tanner. Dense for the price of sparse: Improved performance of sparsely initializednetworks via a subspace offset. In International Conference on Machine Learning, pages 86208629. PMLR,2021.",
  "K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme classifica-tion repository: Multi-label datasets and code, 2016. URL": "Rohit Babbar and Bernhard Schlkopf. Dismec: Distributed sparse machines for extreme multi-labelclassification. In Proceedings of the tenth ACM international conference on web search and data mining,pages 721729, 2017. Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Varma. Parabel: Partitionedlabel trees for extreme classification with application to dynamic search advertising. In Proceedings of the2018 World Wide Web Conference, pages 9931002, 2018. Ting Jiang, Deqing Wang, Leilei Sun, Huayi Yang, Zhengyang Zhao, and Fuzhen Zhuang. Lightxml:Transformer with dynamic negative sampling for high-performance extreme multi-label text classification.In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 79877994, 2021. Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and Shanfeng Zhu. Attentionxml:Label tree-based attention-aware deep model for high-performance extreme multi-label text classification.Advances in neural information processing systems, 32, 2019. Siddhant Kharbanda, Atmadeep Banerjee, Erik Schultheis, and Rohit Babbar. Cascadexml: Rethinkingtransformers for end-to-end multi-resolution training in extreme multi-label classification. Advances inneural information processing systems, 35:20742087, 2022. Kunal Dahiya, Ananye Agarwal, Deepak Saini, K Gururaj, Jian Jiao, Amit Singh, Sumeet Agarwal,Purushottam Kar, and Manik Varma. Siamesexml: Siamese networks meet extreme classifiers with 100mlabels. In International conference on machine learning, pages 23302340. PMLR, 2021. Kunal Dahiya, Nilesh Gupta, Deepak Saini, Akshay Soni, Yajun Wang, Kushal Dave, Jian Jiao, Gururaj K,Prasenjit Dey, Amit Singh, et al. Ngame: Negative mining-aware mini-batching for extreme classification.In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining, pages258266, 2023. Jiong Zhang, Wei-Cheng Chang, Hsiang-Fu Yu, and Inderjit Dhillon. Fast multi-resolution transformerfine-tuning for extreme multi-label text classification. Advances in Neural Information Processing Systems,34:72677280, 2021.",
  "Rohit Babbar and Bernhard Schlkopf. Data scarcity, robustness and extreme multi-label classification.Machine Learning, 108(8):13291351, 2019": "Ian En-Hsu Yen, Xiangru Huang, Pradeep Ravikumar, Kai Zhong, and Inderjit Dhillon. Pd-sparse: A primaland dual sparse approach to extreme multiclass and multilabel classification. In International conference onmachine learning, pages 30693077. PMLR, 2016. Tharun Kumar Reddy Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, and Anshumali Shrivastava.Extreme classification in log memory using count-min sketch: A case study of amazon search with 50mproducts. Advances in Neural Information Processing Systems, 32, 2019. Erik Schultheis and Rohit Babbar. Towards memory-efficient training for extremely large output spaceslearning with 670k labels on a single commodity gpu. In Joint European Conference on Machine Learningand Knowledge Discovery in Databases, pages 689704. Springer, 2023. Vidit Jain, Jatin Prakash, Deepak Saini, Jian Jiao, Ramachandran Ramjee, and Manik Varma. Renee:End-to-end training of extreme classification models. Proceedings of Machine Learning and Systems, 5,2023. Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin Huang, Ajay Jaiswal, and Zhangyang Wang.Sparsity may cry: Let us fail (current) sparse neural networks together! arXiv preprint arXiv:2303.02141,2023. Ajay Kumar Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, and Yinfei Yang. CompressingLLMs: The truth is rarely pure and never simple. In The Twelfth International Conference on LearningRepresentations, 2024. HimanshuJain, YashotejaPrabhu, andManikVarma. Extrememulti-labellossfunctionsforrecommendation,tagging, ranking & other missing label applications. In Proceedings of the 22nd ACM SIGKDD internationalconference on knowledge discovery and data mining, pages 935944, 2016. Anirudh Buvanesh, Rahul Chand, Jatin Prakash, Bhawna Paliwal, Mudit Dhawan, Neelabh Madan, DeepeshHada, Vidit Jain, SONU MEHTA, Yashoteja Prabhu, et al. Enhancing tail performance in extreme classifiersby label variance reduction. In The Twelfth International Conference on Learning Representations, 2023. Erik Schultheis, Marek Wydmuch, Wojciech Kotlowski, Rohit Babbar, and Krzysztof Dembczynski. Gen-eralized test utilities for long-tail performance in extreme multi-label classification. Advances in NeuralInformation Processing Systems, 36, 2024. Mohammadreza Qaraei, Erik Schultheis, Priyanshu Gupta, and Rohit Babbar. Convex surrogates forunbiased loss functions in extreme classification with missing labels. In Proceedings of the Web Conference2021, pages 37113720, 2021. Erik Schultheis, Marek Wydmuch, Rohit Babbar, and Krzysztof Dembczynski. On missing labels, long-tailsand propensities in extreme multi-label classification. In Proceedings of the 28th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, pages 15471557, 2022.",
  "Mike Lasby, Anna Golubeva, Utku Evci, Mihai Nica, and Yani Ioannou. Dynamic sparse training withstructured sparsity. In The Twelfth International Conference on Learning Representations, 2024": "Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions withreview text. In Proceedings of the 7th ACM conference on Recommender systems, pages 165172, 2013. Anshul Mittal, Noveen Sachdeva, Sheshansh Agrawal, Sumeet Agarwal, Purushottam Kar, and ManikVarma. Eclare: Extreme classification with label graph correlations. In Proceedings of the Web Conference2021, pages 37213732, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Eldar Kurtic, Torsten Hoefler, and Dan Alistarh. How to prune your language model: Recovering accuracyon the sparsity may cry benchmark. In Conference on Parsimony and Learning, pages 542553. PMLR,2024. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:Pruning and growth for efficient inference and training in neural networks. Journal of Machine LearningResearch, 22(241):1124, 2021.",
  "Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning withfeedback. In International Conference on Learning Representations, 2020. URL": "Aleksandra Nowak, Bram Grooten, Decebal Constantin Mocanu, and Jacek Tabor. Fantastic weights andhow to find them: Where to prune in dynamic sparse training. Advances in Neural Information ProcessingSystems, 36, 2024. Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning. In SC20:International Conference for High Performance Computing, Networking, Storage and Analysis, pages 114.IEEE, 2020. Joo Hyung Lee, Wonpyo Park, Nicole Elyse Mitchell, Jonathan Pilault, Johan Samir Obando Ceron, Han-Byul Kim, Namhoon Lee, Elias Frantar, Yun Long, Amir Yazdanbakhsh, Woohyun Han, Shivani Agrawal,Suvinay Subramanian, Xin Wang, Sheng-Chun Kao, Xingyao Zhang, Trevor Gale, Aart J.C. Bik, MilenFerev, Zhonglin Han, Hong-Seok Kim, Yann Dauphin, Gintare Karolina Dziugaite, Pablo Samuel Castro,and Utku Evci. Jaxpruner: A concise library for sparsity research. In Conference on Parsimony and Learning(Proceedings Track), 2023. URL Roberto L Castro, Andrei Ivanov, Diego Andrade, Tal Ben-Nun, Basilio B Fraguela, and Torsten Hoefler.Venom: A vectorized n: M format for unleashing the power of sparse tensor cores. In Proceedings of theInternational Conference for High Performance Computing, Networking, Storage and Analysis, pages 114,2023. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu,and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378,2021.",
  "AEvaluation Metrics": "To evaluate the performance of our Extreme Multi-label Text Classification (XMC) model, whichincorporates Dynamic Sparse Training, we use a set of metrics designed to provide a comprehensiveanalysis of both overall and label-specific model performance. The primary metrics we employ isPrecision at k (P@k), which assess the accuracy of the top-k predictions. Additionally, we incorporatePropensity-Scored Precision at k (PSP@k), Macro Precision at k (Macro P@k) and Macro Recall at k(Macro R@k) to gauge the uniformity of the models effectiveness across the diverse range of labelstypical in XMC problems.",
  "LightXML : The method employs a transformer encoder to concurrently train both theretriever and ranker, which incorporates dynamic negative sampling to enhance the modelsefficacy": "XR-Transformer : XR-Transformer employs a multi-resolution training approach,iteratively training and freezing the transformer before re-clustering and re-training classifiersat various resolutions using fixed features. CascadeXML : This method separates the feature learning of distinct tasks across variouslayers of the Probabilistic Label Tree (PLT) and aligns them with corresponding layers of thetransformer encoder. ECLARE : This model utilizes label graphs to improve label representations, focusingspecifically on enhancing performance for rare labels. The label graph is generated throughrandom walks using the label vectors. SiameseXML : This approach combines Siamese networks with one-vs-all classifiers.SiameseXML utilizes multiple ANNS structures to retrieve label shortlists. These shortlistedlabels are subsequently ranked based on scores from label-wise one-vs-all classifiers. NGAME : NGAME enhances transformer-based training for extreme classification byintroducing a negative mining-aware mini-batching technique, which supports larger batchsizes and accelerates convergence by optimizing the handling of negative samples. Renee : The Renee model employs an integrated end-to-end training approach for extremeclassification, using a novel loss shortcut for memory optimization and a hybrid data-modelparallel architecture to enhance training efficiency and scalability. DST MethodsExisting DST methods vary in their pruning and growing criteria. Recent studies indicate that magnitude-based pruning is effective in DST, while dense weight information isimpractical for Extreme Multi-label Classification (XMC). We evaluate key methods on select datasetsfor conceptual validation. RigL : RigL uses weight magnitude and dense gradient magnitudes for the pruning andregrowth saliency criteria, respectively. While RigL only needs the dense gradient informationduring network topology updates, this is a prohibitive requirement in the XMC setting due tothe large memory consumption of the final classification layer. RigL learns an unstructuredsparse network toplogy, which is challenging to accelerate on GPUs. Structured DST : In contrast to RigL, structured DST methods adds constraintsto the learned network topology such that the network is more amenable to acceleration oncommodity GPUs. In our case, we employ the fixed fan-in constraint which reduces boththe latency and memory consumption of the final classification layer in the XMC setting.Further, since the dense gradient information is not available in the XMC task, we simplyrandomly regrow weights as per to SET which has proven to be a robust baseline in theDST literature.",
  "CHyperparameter Settings": "Wepresentthehyperparametersettingsusedduringtrainingin. Fortheencoderandclassifier, weemploy two separate optimizers: AdamW for both components, except in the case of LF-AmazonTitles-131K where Adam and SGD are utilized. All experiments are conducted using half-precision float16types, except for Amazon-3M and LF-AmazonTitles-131K, which use the bfloat16 type. We apply acosine scheduler with warmup, as specified in the table. The weight decay values are set separately:0.01 for the encoder and 1.0e-4 for the final classification layer. We use the squared hinge loss functionfor all datasets except for LF-AmazonTitles-131K, where we use binary cross-entropy (BCE) loss withpositive labels.",
  "BERTBase1280.35505.0e-50.0510000128": ": Impact of intermediate layer size on overall and tail label performance. The plots showprecision, propensity-scored precision, and macro precision across epochs for different intermediatelayer sizes (1024, 2048, 4096, and 8192). We also present DST and other related settings in a separate . The learning rates for the auxiliaryclassifier and the intermediate layer are fixed at 5.01e-4 and 2.01e-4, respectively. We use a randomgrowth mode with zero initialization, updating the topology until 66% of the total training steps.",
  "EThe Role of Random Cluster based Meta Classifiers in XMC Problems": "To understand the impact of random clusters on meta classifier-based methods, we selected theLightXML approach and experimented with two large-scale datasets: Amazon-670K and Wiki-500K. For our experiments, we used the original code from the official LightXML repository and theoriginal clusters provided by the authors. We randomized the original clusters by applying severaliterations of random.shuffle(), repeating the process twice to generate two sets of random clusters. To ensure randomness, we calculated the intersection of elements between each pair of clusters fromthe original and random sets. We then took the maximum overlap value among all pairs, which was lessthan 3.5% in both cases. Subsequently, we ran the LightXML code using the original clusters and thetwo sets of random clusters. Our observations revealed that the final performance remained largely unaffected, although the learningprocess slowed down initially, as shown in the upper row of . The bottom row illustrates theprecision of the meta classifier, which is lower for the random clusters as expected. We replicated thesame experiment with the Wiki-500K dataset and observed similar results, which are also depicted in."
}