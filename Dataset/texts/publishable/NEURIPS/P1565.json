{
  "Abstract": "Motion-to-music and music-to-motion have been studied separately, each attractingsubstantial research interest within their respective domains. The interactionbetween human motion and music is a reflection of advanced human intelligence,and establishing a unified relationship between them is particularly important.However, to date, there has been no work that considers them jointly to explorethe modality alignment within. To bridge this gap, we propose a novel framework,termed MoMu-Diffusion, for long-term and synchronous motion-music generation.Firstly, to mitigate the huge computational costs raised by long sequences, wepropose a novel Bidirectional Contrastive Rhythmic Variational Auto-Encoder(BiCoR-VAE) that extracts the modality-aligned latent representations for bothmotion and music inputs. Subsequently, leveraging the aligned latent spaces, weintroduce a multi-modal Transformer-based diffusion model and a cross-guidancesampling strategy to enable various generation tasks, including cross-modal, multi-modal, and variable-length generation. Extensive experiments demonstrate thatMoMu-Diffusion surpasses recent state-of-the-art methods both qualitatively andquantitatively, and can synthesize realistic, diverse, long-term, and beat-matchedmusic or motion sequences. The generated samples and codes are available at",
  "Introduction": "Dancing to the musical beats or creating a variety of rhythmically synchronized music for a givenmotion is a fundamental aspect of human creativity. Music and human motions serve as universallanguages that are shared by all civilizations, transcending cultural and geographical boundariesaround the world . For computational methodologies, the motion-music generation poses severalchallenges: 1) maintaining long-term coherence in typically lengthy motion-music sequences 2)",
  "ensuring temporal synchronization and rhythmic alignment between motion and music sequences,and 3) generating realistic, diverse, and variable-length human motions or music": "Existing works usually divide the motion-music generation into two distinct tasks: motion-to-music and music-to-motion. For motion-to-music, some methods compress the conditional videoframes into a single image, in which the temporal information is lost . The state-of-the-art work, LORIS , employs a hierarchical conditional diffusion model to generate long-termmusical waveforms. However, LORIS introduces huge computational costs and training difficultiessince it generates long-term musical waveforms directly. For music-to-motion, the Dancing2Music(D2M) framework divides the generation process into two stages: decomposing the dance intobasic dancing movements with a VAE and compositing the basic movements into dance with a GAN.Nonetheless, D2Ms approach of segmenting long-term music into short clips (approximately 1-2seconds) diminishes the coherence of the synthesized motion sequences. Motivated by the fact that human motions are highly associated with music yet existing computationalmethods often study them in isolation, we propose a novel multi-modal framework, termed MoMu-Diffusion, to address the aforementioned challenges jointly. Firstly, to mitigate the computationalcosts and optimization complexities raised by long sequences, we employ a VAE to encode bothmotion and music sequences into latent spaces. Subsequently, to investigate the relationship betweenhuman movements and musical beats, we propose rhythmic contrastive learning. This approachinvolves constructing contrast pairs with a kinematic amplitude indicator, which quantifies thetemporal variation in motion and is derived from the spatial motion directrogram differences asdetailed in . Given that the motion and music sequences are interactively aligned in the latentspace to discern the correlation between kinematic shifts and musical rhythmic beats, we call ourmodel as the Bidirectional Contrastive Rhythmic VAE (BiCoR-VAE). With the aligned latent space, we introduce a Transformer-based diffusion model that captures long-term dependencies and facilitates sequence generation across variable lengths. Additionally, weintroduce a simple cross-guidance sampling strategy that integrates different cross-modal generationmodels, enabling multi-modal joint generation without extra training. By incorporating the BiCoR-VAE and the diffusion Transformer model, our MoMu-Diffusion framework effectively models thelong-term motion-music synchronization and correspondence, enabling motion-to-music, music-to-motion, and joint motion-music generation. Moreover, MoMu-Diffusion supports generatingmotion-music samples in variable lengths. The pipeline of MoMu-Diffusion is illustrated in . We have conducted extensive experiments on three motion-to-music and two music-to-motiondatasets, including scenarios such as dancing and competitive sports. The experimental resultsdemonstrate that MoMu-Diffusion attains state-of-the-art performance across both objective andsubjective metrics, significantly enhancing music/motion quality and cross-modal rhythmic/kinematicalignment. Furthermore, we have carried out abundant ablation studies to validate the efficacy ofthe BiCoR-VAE and the DiT architecture. A comparative analysis with state-of-the-art motion-to-music methods CDCD and LORIS , 2D music-to-motion method D2M , and generalvideo-to-audio methods Diff-Foley and MM-Diffusion , is presented in .",
  "Related Works": "Neural Motion Synthesis. Neural motion synthesis is often associated with audio, and we focuson two audio-driven scenarios: music-to-motion generation and co-speech gesturegeneration . For music-to-motion, some methods propose to retrieve themost related music for the given motion sequence. D2M is a generative model that designs a decomposition-to-composition method to learn the movement units and generate music fromthe learned units. Besides, some methods investigate synthesizing 3D motions frommusic. For co-speech gesture generation, DiffGesture is a state-of-the-art model with a diffusiontransformer architecture and diffusion gesture stabilizer. We study the 2D music-to-motion problemand compare the proposed MoMu-Diffusion with DiffGesture and D2M Neural Music Synthesis. Neural music Synthesis aims to generate melodious music with generativeneural networks. Various generative models have been successfully applied to music synthesissuch as transformer-based autoregressive models , VAE , GAN , anddiffusion models . Some efforts have been made to video-to-music which focuses on thecross-modal temporal alignment. For example, Foley Music and Audeo utilize MusicalInstrument Digital Interface (MIDI) representations to generate music in a non-regressive manner.D2M-GAN and CDCD generate video-related music by compressing the video frames intoa single image, in which the temporal information is neglected. LORIS proposes a hierarchicalconditional diffusion model to generate long-term musical waveforms. Multi-Modal Contrastive Learning. Contrastive has been demonstrated effective in For example,Elizalde et al. proposed Contrastive Language-Audio Pretraining (CLAP) to learn a unifiedlatent representation for an audio or text input, facilitating the birth of text-to-audio models .For audio-visual generative tasks, DiffFoley uses semantic and temporal contrastive learning topromote video-to-audio generation. In this paper, to improve the efficiency and generalization abilityof our generative model, we propose the first motion-music pretraining model with a well-designedcontrastive loss to learn beat synchronization and rhythm correspondence.",
  "Multi-Modality Model Architecture": "Motion Variational Auto-Encoder. Let n RTmJ2 be the 2D motion keypoints extracted fromthe corresponding video, where Tm is the motion frames, J is the number of nodes containing thevalues of the x-coordinate and y-coordinate. Then, we encode the spatial positions into a latent byzm = Em(m) RTzmd, where Tzm < Tm is the downsampled motion frames and d is the latentmotion dimension. The encoded latent can be decoded by a decoder to obtain the reconstructedmotion sequence: m = Dm(m). Music Variational AutoEncoder. Music is a structured and complex audio signal, composed ofvarious elements such as melody, harmony, rhythm, and dynamics. Some works utilizeMusical Instrument Digital Interface (MIDI) representations, which yield highly formulated results.However, processing long-term music directly from the raw waveform is computationally intensiveand challenging . To address this, we train a VAE on the mel-spectrogram derived from the music,coupled with a high-fidelity vocoder. Let u RTu be a music input, where Tu denotes the waveformlength. We can extract the mel-spectrogram of the music input: a = Mel(u) RCaTa, where Mel()is the pre-defined mel-spectrogram extraction function, Ca is the channels, and Ta Tu is the frames.Then, an encoder is used to compress the mel-spectrogram into a latent: za = Ea(a) RTzad,where Tza is the downsampled music frames and d is the latent mel-spectrogram dimension. Theencoded mel-spectrogram can be decoded by a decoder a = Da(a), and subsequently the musicalwaveform can be obtained by a high-fidelity vocoder x = V (a).",
  "Rhythmic Contrastive Learning": "Contrastive learning has proven effective for learning multi-modal representations, enhancing perfor-mance in downstream tasks . In the context of temporal alignment, a recent work introducestemporal contrast, which seeks to maximize the similarity of audio-visual pairs from the same timesegment while minimizing the similarity of pairs from different segments. However, this paradigmfaces limitations in long-term motion-music synthesis, as musical pieces typically correspond tonumerous rhythmic beats. The random selection process for constructing negative samples riskscapturing similar rhythmic sequences, which undermines the learning objective. To address it, wepropose rhythmic contrastive learning, designed to align cross-modal temporal synchronization andrhythmic correspondence. Based on the motion and music VAEs, we can obtain the motion latentzm RTzmd, and music mel-spectrogram latent za RTzad, respectively. To synchronize the",
  "Beat Guidance": ": An overview of the proposed MoMu-Diffusion framework. MoMu-Diffusion contains twointegral components: a bidirectional contrastive rhythmic Variational Autoencoder (BiCoR-VAE)designed to learn the aligned latent space, and a Transformer-based diffusion model responsible forsequence generation. This framework is adept at facilitating both cross-modal and multi-modal jointgenerations, offering a robust approach to the integrated synthesis of motion and music.",
  "motion and music, which are often sampled differently, we employ pre-processing techniques such asevenly dropping motion frames to match the number of music frames, ensuring that Tzm = Tza": "In the domain of motion-guided music, the inherent irregularity of human movements, characterizedby rapid and abrupt actions, can significantly influence rhythm. To synchronize these rhythmicpatterns, we employ a kinematic amplitude indicator as a basis for constructing contrastive clipswithin each motion-music pair. Firstly, we extract the motion kinematic offsets with the motiondirectogram , a metric that quantifies the variation in motion. We denote F(r, j) as the first-orderdifference of j-th node in the 2D motion at temporal timestep r, and divide it into K bins based ontheir Euclidean angles with x-axis by tan1(y/x). Then, the 2D motion directogram D(r, ) can beexpressed as the aggregate of F(r, j) across each angular bin:",
  "where D(r, k) is the directogram volume at temporal timestep r and k-th bin. The kinematicamplitude value is normalized within the range of (0,1)": "With the kinematic amplitude indicator established, we proceed to prepare the temporal motion-music clips for contrastive rhythmic learning. For each motion-music latent pair, we randomlysample NT motion-music clips and divide them into NC categories according to the clip-wisemaximum kinematic amplitude values. In order to maximize the similarity of motion-music pairsfrom the same timestep (i.e. temporal alignment) and minimize the similarity of motion-music pairsacross different timesteps and rhythmic patterns, we randomly sample NS motion-music latent clip(crs:rea, crs:rem, Q(rs : re)) (Rd, Rd, (0, 1)) from different kinematic amplitude categories for thetemporal and rhythmic alignment:",
  "Training Strategy": "In BiCoR-VAE, the goal is to learn two paired VAEs for motion and music inputs, with a focus ontemporal and rhythmic alignment within the low-level latent space. However, the VAEs objectiveto preserve fine-grained details for accurate reconstruction often conflicts with contrastive rhythmiclearnings aim to align latent representations across modalities. This presents a trade-off betweenrepresentational fidelity and generative alignment, posing optimization challenges. To address it, wepropose a two-stage training strategy: initially, we train the music VAE using both a VAE loss anda GAN loss to prevent over-smoothing of the mel-spectrogram; subsequently, we train the motionVAE with a VAE loss and the contrastive rhythmic loss, while keeping the music VAEs parametersfixed. The insight behind this strategy is that mel-spectrograms, with their rich and complex acousticfeatures, require a more intricate optimization process compared to motion VAE, which deals with alimited set of body joint data. An overview of BiCoR-VAE is illustrated in (a).",
  "Transformer-based Diffusion Model with Aligned BiCoR-VAE": "Diffusion Formulation. Recent works have revealed that the U-Net architecture is not essentialfor diffusion probabilistic modeling, and in fact, the transformer can achieve superior performancein text-to-image generation tasks . Additionally, the transformer architecture excels atcapturing long-range dependencies within sequence data and offers flexibility for variable-lengthgeneration . Inspired by these findings, we opt for a Transformer-based architecture for ourmotion-music generation framework. Concretely, our approach involves initially concatenating thenoisy input with the embedded conditional inputs and the embedded diffusion timesteps along thetemporal dimension. This fused input is then padded to match a specified maximum length andcombined with positional embeddings prior to being processed by the DiT model. The DiT outputis subsequently truncated to the original temporal length and mapped to the output latent space.To illustrate the diffusion process, lets consider the motion-to-music task. During the forwarddiffusion, the latent data is gradually perturbed towards a standard Gaussian distribution according toa pre-defined schedule 1, ..., T , where T is the total diffusion timesteps and t = ti=1 i:q(za(t)|za(t 1)) = N(za(t); tza(t 1), 1 tI),(5)where za(t) denotes the music latent at timestep t. Then, the training objectives of our DiT-basedcross-modal generation models are defined:Lm2a = ||a(za(t), t, zm) ||22,La2m = ||m(zm(t), t, za) ||22,(6)where N(0, 1) denotes the noise in diffusion procedure, a and m are the parameterized DiTdenoisers for motion-to-music and music-to-motion generation, respectively. Conditional Generation. For the cross-modal generation such as motion-to-music and music-to-motion, we implement classifier-free guidance . This method adeptly combines conditionaland unconditional scores to obtain a trade-off between quality and diversity. By interpreting thediffusion model output as a score function, the sampling procedure with classifier-free guidance ofmotion-to-music can be written as:a(za(t), t, zm) = a(za(t), t, ) + s (a(za(t), t, zm) a(za(t), t, ))(7)where s > 1 denotes the classifier sampling scale to balance the diversity and quality of synthesizedsamples. The diffusion model with condition is achieved by randomly dropping zm and replacing itwith an embedded null representation. Exchanging the latent inputs enables the sampling procedurefor music-to-motion generation since we have built a modality-aligned latent space. Joint Generation with Cross Guidance. To accomplish multi-modal joint generation, we propose across-guidance sampling strategy. This approach leverages multiple expert models and introduces aslight modification to the sampling procedure, rather than integrating multiple modalities into a singlemodel. Let T be the total diffusion steps, a be the trained motion-to-music denoising model, andm be the trained music-to-motion denoising model, we perform unconditional generation before adefined diffusion step Tc:pa(za(t 1)|za(t)) = N(za(t 1), a(za(t), t, ), 2t I),where T > t > Tc,(8)",
  ": Results on the Floor Exercise dataset with beat-matching metrics": "Eq (8) and Eq (9) delineate the reverse process for motion-to-music generation within the timesteprange T t > Tc. The reverse process for music-to-motion generation can be similarly constructed.For reverse timesteps Tc t > 0, we use the estimated clean motion/music latent to condition thegeneration process of music/motion with the classifier-free guidance defined in Eq (7). Given that thediffusion model adopts a coarse-to-fine refinement in the reverse process, we conduct unconditionalgeneration before Tc and impose conditional generation with the cross-guidance strategy after Tc, asthe noise in the estimated clean latent is significantly reduced. Determining the value of Tc appearsto be quite challenging; however, our empirical findings indicate that the joint generation maintainsrobust performance across a broad range of values for Tc (from 0.3T to 0.7T). The diversity in jointgeneration is sustained by the unconditional process and classifier-free guidance. An overview ofcross-modal generation and joint generation is shown in (b) and (c).",
  "Motion-to-Music Generation": "Experimental Settings. We evaluate our method on the latest LORIS benchmark , which contains86.43 hours of video samples synchronized with music. This benchmark presents three demandingscenarios: AIST++ Dance , Floor Exercise , and Figure Skating . In our experiments,each dataset is randomly split with a 90%/5%/5% proportion for training, validation, and testing.For model evaluation, we use five metrics to measure the beat-matching between synthesized musicand ground-truth music : Beats Coverage Scores (BCS) and Beat Hit Scores (BHS), CoverageStandard Deviation (CSD), Hit Standard Deviation (CSD), and the F1 scores. Besides, we use theFrchet Audio Distance (FAD) and Diversity scores to evaluate the quality of synthesizedmusic. Since the quality of the Floor Exercise and the Figure Skating datasets are poor, we onlyconduct motion-to-music generation on them with a learnable motion encoder, whose architectureis derived from . During sampling, we employ 50 DDIM sampling steps. More experimentalsettings are provided in Appendix B. Baselines. We compare our proposed method to existing advanced video-to-music baselines: 1)Foley Music , a graph transformer framework with MIDI representations. 2) CMT , acontrollable music transformer model to learn the rhythmic consistency between video and mu-sic. 3) D2M-GAN , a GAN-based model with vector quantized music representation. 4)CDCD , a diffusion-based model with an additional conditional discrete contrastive diffusionloss. 5) LORIS , a diffusion-based model with hierarchical conditional mechanism, yieldingstate-of-the-art performance on video-to-music synthesis.",
  ": Results on the Figure Skating with beat-matching metrics": "Main Results. The results of beat-matching are shown in , 3 and 4. From these tables, we candraw the following conclusions: 1) MoMu-Diffusion significantly surpasses existing state-of-the-artmethods in cross-modal beat-matching. It demonstrates the effectiveness of BiCoR-VAE and themulti-modal Transformer-based model in synchronizing kinematic and rhythmic beats. 2) MoMu-Diffusion realizes a substantial improvement in Beat Hit Scores (BHS), which indicates the beatsin the synthesized music are closely aligned with the ground truth. For example, MoMu-Diffusiongains 98.6% BHS on the AIST++ dancing subset, while previous methods usually gain about 90%BHS. An illustrative example of beat-matching for motion-to-music is presented in . Wecan find the musical beats of synthesized music are aligned with the ground truth and the kinematicmovements of the eference video. The FAD and Diversity results are shown in . In this comparison, we focus on LORIS, thecurrent state-of-the-art method in motion-to-music generation. It is evident that MoMu-Diffusionconsistently outperforms LORIS across these metrics, particularly in FAD scores. This superiority canbe attributed to MoMu-Diffusions architectural innovations for capturing long-term correspondence.Unlike text, music encompasses a richer sequence length due to its complex acoustic features, suchas melody, rhythm, and driving beats. To address this, MoMu-Diffusion employs mel-spectrogramsin place of raw waveforms, thereby mitigating sequence length. Additionally, the introduction ofBiCoR-VAE facilitates modality alignment in latent spaces.",
  "Music-to-Motion Generation": "Experimental Settings. We use two datasets: AIST++ Dance and BHS Dance. About 71 hoursBHS Dance videos are collected from , which contains three dancing types: Ballet, Zumba,and Hip-Hop. For model evaluation, we compute the beat-matching metrics between synthesizedmotion beats and the reference musical beats with the aforementioned five beat-matching metrics. Tovalidate the quality of synthesized motion sequences, we use Frchet Inception Distance (FID) ,Mean KL-Divergence (Mean KLD), and the Diveristy scores. The feature extractor is based onMotionBert and trained with a classification task on the BHS Dance dataset. For the BHS Dancedataset, we exclude the BiCoR-VAE since this dataset only contains the paired audio MFCC featuresand motion sequences without raw audio. In the generation process, the settings of the diffusiontransformer model are the same as motion-to-music. More details are provided in Appendix B. Baselines. We compare MoMu-Diffusion to two baselines: 1) D2M , the state-of-the-art music-to-motion work with a two-stage movement unit-based model; 2) DiffGesture , the state-of-the-artco-speech gesture generation work with a U-Net diffusion model. Dance Revolution reportsbetter performance on music-to-motion generation but is withdrawn by its authors.",
  ": Results on the AIST++ Dance and BHS Dance datasets with beat-matching metrics": "Main Results. The beat-matching results are detailed in . An analysis of these resultsreveals that MoMu-Diffusion achieves superior scores across all evaluated tasks, outperforming thestate-of-the-art music-to-motion method D2M and co-speech gesture generation method DiffGesture.This performance underscores the efficacy of our BiCoR-VAE in constructing an aligned latentspace for cross-modal generation and the feed-forward diffusion model in capturing long-termcorrespondence. It should be noted that the metrics BCS (Beats Coverage Scores) and BHS (Beat HitScores) are defined differently in this context compared to motion-to-music scenarios. Specifically,BCS calculates the coverage score between the kinematic beats of the synthesized motions and themusical beats of the ground-truth music, rather than the kinematic beats of the ground-truth motions. The generation quality results are presented in . It is observable that MoMu-Diffusion reportsbetter FID, Mean KLD, and Diversity scores on both the AIST++ and BHS Dance datasets. Itdemonstrates that MoMu-Diffsuion can generate more realistic and high-quality motion sequenceswhile maintaining the capability of diverse generations. We further present a qualitative example ofmusic-to-motion beat-matching in . We can find the kinematic beats of synthesized motionare highly associated with the reference musical beats. Additionally, the generated dance exhibits ahigh degree of diversity, encompassing lateral movements, rotations, squats, and so on.",
  "Analysis and Ablation Study": "User Study. We conducted a user study with 20 annotators on the AIST++ Dance dataset to evaluatethe generation performance. For each method, 200 samples were generated, and 20 paired sampleswere randomly selected for each comparison group. Annotators were asked to respond on site: Whichdance/music is more realistic and matches the music/dance better?. The human evaluation results,shown in , indicate that our method outperforms SOTA approaches in both motion-to-musicand music-to-motion generations. Notably, a preference drop is observed when BiCoR-VAE is notemployed, highlighting the importance of an aligned latent space for cross-modal generation. Motion Encoding. For motion sequence encoding, we compare the spatial position-based methodwith the directional vector-based method, which learns the unit directional vectors of the givenadjacency set, and reconstructs the human pose with the calculated mean bone lengths . However,as shown in (#1), the spatial position-based method proved superior, likely due to the errorintroduced by movements that alter bone length, such as squatting and bending.",
  ": Results of human evaluation on motion-to-music and music-to-motion generations": "Music Encoding. For music encoding, we evaluated the spectrogram-based method against theraw waveform-based method. According to (#2), the raw waveform-based method gainsperformance declines in both FAD and F1 metrics. This is attributed to the lengthy audio sequencesintroduced by the raw waveform, introducing difficulties for diffusion modeling training. Learning Techniques. In MoMu-Diffusion, there are two key learning techniques: rhythmiccontrastive learning (RCL) and Feed-Forward Transformer (FFT). From , we can observethat Ours w/o RCL gains a clear drop on the beat-matching metric F1 (#3) and Ours w/o FFTgains a drop on the synthesis quality metric FID/FAD (#4), respectively. Ours w/o FFT meanswe use a U-Net backbone for the diffusion model, which has been shown inferior to our FFT-basedmodel in long sequence modeling. Equipped with both RCL and FFT, MoMu-Diffusion ensures bothgeneration quality and cross-modal alignment. Joint Generation in Variable Length. MoMu-Diffusion supports multi-modal joint generation invariable lengths, facilitated by a \"pad-and-truncate\" strategy in the diffusion model and the proposedcross-guidance sampling. To validate this capability, 1000 samples with varying lengths (10-30seconds) are generated using different Gaussian noise vectors. With a cross-guidance samplingtimestep set to Tc = 0.5T, (#5, #6), we can find that for multi-modal joint generation,MoMu-Diffusion shows that MoMu-Diffusion achieves comparable performance to the conditionalmodels with clean condition inputs and advanced performance on the joint generation scenario. Moreablation studies are provided in Appendix D.",
  "Conclusion": "In this paper, we propose MoMu-Diffusion, the first multi-modal framework designed to learn thelong-term synchronization and correspondence between human motions and music. In MoMu-Diffusion, we have two key designs: bidirectional contrastive rhythmic VAE (BiCoR-VAE) forlearning modality-aligned latent spaces and Transformer-based diffusion model for learning long-term dependencies. Through extensive experiments, we demonstrate MoMu-Diffusions efficacyacross motion-to-music, music-to-motion, and joint motion-music generations.",
  "Dhariwal, P., Jun, H., Payne, C., Kim, J. W., Radford, A., and Sutskever, I. Jukebox: Agenerative model for music. arXiv preprint arXiv:2005.00341, 2020": "Di, S., Jiang, Z., Liu, S., Wang, Z., Zhu, L., He, Z., Liu, H., and Yan, S. Video background musicgeneration with controllable music transformer. In Proceedings of the 29th ACM InternationalConference on Multimedia, pp. 20372045, 2021. Dong, H.-W., Hsiao, W.-Y., Yang, L.-C., and Yang, Y.-H. Musegan: Multi-track sequential gen-erative adversarial networks for symbolic music generation and accompaniment. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 32, 2018. Dong, H.-W., Chen, K., Dubnov, S., McAuley, J., and Berg-Kirkpatrick, T. Multitrack musictransformer. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech andSignal Processing (ICASSP), pp. 15. IEEE, 2023. Elizalde, B., Deshmukh, S., Al Ismail, M., and Wang, H. Clap learning audio conceptsfrom natural language supervision. In ICASSP 2023-2023 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE, 2023. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Mller, J., Saini, H., Levi, Y., Lorenz, D., Sauer,A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis.arXiv preprint arXiv:2403.03206, 2024.",
  "Fan, R., Xu, S., and Geng, W. Example-based automatic music-driven conventional dancemotion synthesis. IEEE transactions on visualization and computer graphics, 18(3):501515,2011": "Gan, C., Huang, D., Chen, P., Tenenbaum, J. B., and Torralba, A. Foley music: Learning togenerate music from videos. In Computer VisionECCV 2020: 16th European Conference,Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pp. 758775. Springer, 2020. Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal,M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp.776780. IEEE, 2017. Grosche, P., Mller, M., and Kurth, F. Cyclic tempograma mid-level tempo representationfor musicsignals. In 2010 IEEE International Conference on Acoustics, Speech and SignalProcessing, pp. 55225525. IEEE, 2010.",
  "Huang, R., Hu, H., Wu, W., Sawada, K., Zhang, M., and Jiang, D. Dance revolution: Long-termdance generation with music via curriculum learning. arXiv preprint arXiv:2006.06119, 2020": "Huang, R., Huang, J., Yang, D., Ren, Y., Liu, L., Li, M., Ye, Z., Liu, J., Yin, X., and Zhao,Z. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models. InInternational Conference on Machine Learning, pp. 1391613932. PMLR, 2023. Huang, Y.-S. and Yang, Y.-H. Pop music transformer: Beat-based modeling and generation ofexpressive pop piano compositions. In Proceedings of the 28th ACM international conferenceon multimedia, pp. 11801188, 2020.",
  "Ren, Y., He, J., Tan, X., Qin, T., Zhao, Z., and Liu, T.-Y. Popmag: Pop music accompanimentgeneration. In Proceedings of the 28th ACM international conference on multimedia, pp.11981206, 2020": "Roberts, A., Engel, J., Raffel, C., Hawthorne, C., and Eck, D. A hierarchical latent vector modelfor learning long-term structure in music. In International conference on machine learning, pp.43644373. PMLR, 2018. Ruan, L., Ma, Y., Yang, H., He, H., Liu, B., Fu, J., Yuan, N. J., Jin, Q., and Guo, B. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1021910228, 2023. Shao, D., Zhao, Y., Dai, B., and Lin, D. Finegym: A hierarchical video dataset for fine-grainedaction understanding. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pp. 26162625, 2020.",
  "Yu, J., Wang, Y., Chen, X., Sun, X., and Qiao, Y. Long-term rhythmic video soundtracker. InInternational Conference on Machine Learning, pp. 4033940353. PMLR, 2023": "Zhu, L., Liu, X., Liu, X., Qian, R., Liu, Z., and Yu, L. Taming diffusion models for audio-drivenco-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 1054410553, 2023. Zhu, W., Ma, X., Liu, Z., Liu, L., Wu, W., and Wang, Y. Motionbert: A unified perspectiveon learning human motion representations. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 1508515099, 2023. Zhu, Y., Olszewski, K., Wu, Y., Achlioptas, P., Chai, M., Yan, Y., and Tulyakov, S. Quantizedgan for complex music generation from dance videos. In European Conference on ComputerVision, pp. 182199. Springer, 2022.",
  "A.2Model Training": "We use a two-stage training strategy for BiCoR-VAE. Firstly, we train the mel-spectrogram VAE withthree loss functions: reconstruction loss Lrecon, KL loss LKL and a GAN loss LGAN to preventover-smoothed mel-spectrogram:Lstage1 = Lrecon + 1LKL + 2LGAN,(10)where 1 is set to 1e-5 and 2 is set to 0.5. Note that the GAN loss contains two steps: it firstupdates the generator part (mel-spectrogram VAE) with Lstage1, and then updates the additionaldiscriminator. After training mel-spectrogram VAE, we freeze it, and train the motion VAE with theproposed contrastive rhythmic learning loss defined in Eq (4) in stage 2:Lstage2 = Lrecon + 3LKL + 4Lcontrast,(11)where 3 is set to 1e-5 and 4 is set to 1. For training BiCoR-VAE, we use the AdamW optimizer witha learning rate of 2e-4 and training epochs of 300. We use 8 NVIDIA 4090 GPUs and it takes about12 hours to finish. To decode the mel-spectrogram into high-fidelity music, we use the BigvGANmodel pretrained on the AudioSet dataset .",
  "B.1Dataset": "For motion-to-music, we evaluate our method on the latest LORIS benchmark , which contains86.43 hours of video samples with paired music. This benchmark incorporates three challengingscenarios: dancing, floor exercise, and figure skating. For dancing, 1,881 25-second videos arecollected from AIST++ , a fine-annotated subset of the dancing dataset AIST . For floorexercise, 1,950 25-second and 660 50-second videos are collected from the Ginegym dataset .For figure skating, 8,585 25-second and 4,147 50-second videos are collected from the FisV andFS1000 datasets. For music-to-motion, we use two datasets: AIST++ Dance and BHS Dance. About 71 hoursBHS Dance videos are collected from , which contains three dancing types: Ballet, Zumba,and Hip-Hop. In our experiments, each dataset is randomly split with a 90%/5%/5% proportion fortraining, validation, and testing. Note that only the AIST++ Dance dataset is used for both motion-to-music and music-to-motiongenerations. This is because the Floor Exercise and Figure Skating datasets involved too heavymotion variation, which makes it hard for the pose extraction algorithm to extract the high-accuracymotion sequences. As for the BHS Dance dataset, it only provides the MFCC audio features withoutraw audio. Therefore, we can not conduct motion-to-music experiments on it.",
  ": Hyper-parameters of the FFT model": "using Hann window with a window size of 1024. For human motions, OpenPose is applied toextract 2D body keypoints, and can process a video at 60 fps. We use the pre-trained Body-25 modelto extract 25 key points of the human body, but some key points are difficult to extract consistentlyand some are less relevant to actions. As implemented by , we finally choose the 14 most relevantkeypoints to represent the poses, i.e., nose, neck, left and right shoulders, elbows, wrists, hips, knees,and ankles. We interpolate the missing detected keypoints from the neighboring frames so that thereare no missing keypoints in all extracted clips.",
  "B.3Model Configurations": "For the denoising part, we use the Transformer backbone rather than the U-Net. The hyper-parametersof our FFT model are listed in . The FFT diffusion model is trained by the AdamW opti-mizer with a learning rate of 1.6e-5 and a lambda linear scheduler with a warmup step of 10000.We train the diffusion model with 200 epochs for each task. It takes about 2 days for 8 NVIDIA 4090GPUs. For the Figure Skating dataset, it takes about 4 days since this dataset is large.",
  "B.4Evaluation Metrics: Motion-to-Music": "To evaluate whether the synthesized music is aligned with the given motion, we use the improvedBeats Coverage Scores (BCS) and Beat Hit Scores (BHS) to validate the rhythm correspondenceand cross-modal alignment of synthesized music. The improved BCS and BHS are first proposedby , then used for rhythmic dance-to-music validation , and improved by forlong-term rhythmic music validation. Also, we report Coverage Standard Deviation(CSD) andHit Standard Deviation(CSD) to evaluate the robustness of generative models. Finally, the F1scores of improved BCS and BHS are also reported as an overall assessment. BCS and BHS aredesigned by computing matching degrees of the rhythm points from synthesized music and ground-truth music. Let Ns be the rhythm point number of synthesized music, Nt be the rhythm pointnumber of ground-truth music, and Nm be the number of matched rhythm points, the BCS is definedas BCS = Nm/Ns and the BHS is defined as BHS = Nm/Nt, respectively. However, thesemetrics are not suitable for long-term music evaluations since 1) the second-wise rhythm detectionalgorithm leads to an extremely sparse vector and 2) BHS can easily exceed 1 if the rhythm points ofgenerated music are more than ground truth. Therefore we use an improved audio onset detection Algorithm 1: Pseudo code for cross-modal (motion-to-music) sampling.Input: The latent mel-spectrogram representation za, latent motion representation zm, thepre-trained denoiser a for motion-to-music, and the decoder Da for mel-spectrogram.t T,za(t) N(0, I)while t > 0 do",
  "za(t) sample from pa(za(t t)|za(t), t, zm)t t tendreturn Da( za)": "algorithm to avoid sparse rhythm vectors. Here is the Python code based on the Librosa library:librosa.onset.onset_detect(y=audio, sr=sampling_rate, wait=1, delta=0.2, pre_avg=3, post_avg=3,pre_max=3, post_max=3, units=time). For validating the quality of synthesized music, we use the Frchet Audio Distance (FAD) and theDiveristy score. We use the pre-trained VGGish model from to compute the FAD scores. Based on the feature extractor VGGish, wecompute the Diversity score by using the average feature distance for paired samples. Specifically, theDiversity score contains inter-diversity and intra-diversity. Inter-diversity is obtained by computingthe average feature distance between 200 combinations of 50 pieces of music from different motionsand the intra-diversity is obtained by computing the average feature distance between all combinationsof 5 pieces of music from the same motion input.",
  "B.5Evaluation Metrics: Music-to-Motion": "To evaluate whether the synthesized motion is aligned with the reference music, we also use thesefive beat-matching metrics. However, since the number of musical beats is always more than thenumber of kinematic beats in real-world products, we use the musical beats as the reference forevaluation, which is also consistent with previous works . Concretely, Let Ns be the kinematicpoint number of synthesized motion, Nt be the rhythm point number of ground-truth music, and Nmbe the number of matched points, the BCS is defined as BCS = Nm/Ns and the BHS is definedas BHS = Nm/Nt, respectively. For kinematic beat extraction, we use the bin-wise directrogramdifference (defined in Eq (2)) as the indicator . For validating the quality of synthesized motion, we use the Frchet Inception Distance (FID) and theDiveristy score. To compute the FID score, we follow the design of and train a motion classifieron the BHS Dance dataset with three classification categories: Ballet, Zumba, and Hip-Hop.The motion classifier consists of a MotionBert encoder and a classification head. The motionclassifier is trained by an Adam optimizer with a learning rate of 1e-4 and 100 epochs. Then, we usethe trained MotionBert encoder as the feature extractor for computing the FID and Diversity scores.The definition of Diversity score here is the same as Appendix B.4.",
  "DThe Choice of Tc in Joint Generation": "We propose a simple cross-guidance sampling strategy to combine multiple cross-modal generativemodels for joint generation. In this process, there is a hyper-parameter Tc that controls the modalityfusion timestep. In , we study five variants: Tc begins from 0.9T to 0.1T with an intervalof 0.2T. From these results, we can observe that employing the cross-guidance strategy in theearly sampling steps is not feasible since the predicted latent representation contains too manynoises. However, we can find that MoMu-Diffusion (Tc = 0.7T) obtains an acceptable performance, Algorithm 2: Pseudo code for multi-modal joint sampling.Input: The latent mel-spectrogram representation za, latent motion representation zm, thepre-trained denoisers a and m, the decoders Da and Dm, the cross-guidance scale ,the pre-defined schedule 1, ..., T and t = ti=1 i.t T,Tc Tza(t) N(0, I)zm(t) N(0, I)while T t > Tc do",
  ": Ablation study of the cross-guidance step Tc on the AIST++ Dance dataset": "indicating that the denoising process is coarse-to-fine. Using fewer cross-guidance sampling steps(like Tc = 0.1T) can ensure the quality of generated samples, but the cross-modal alignment isomitted, leading to a low F1 score. Therefore, we use Tc = 0.5T in our paper to trade off thesampling quality and cross-modal alignment in joint generation.",
  "EFailure Cases": "Since our method predicts pose points, the deviation between points can cause abnormal length ofhuman skeleton. Three sets of examples are shown in , with anomalous frames on the leftand the corrected frames on the right. We first calculated the average bone length between each pair of keypoints in the dataset, after whichwe post-processed the predicted keypoints. We specify a threshold (which we specify as 1.3 times themean bone length) and correct the predicted bone lengths to the mean bone length once they exceedthe threshold, an approach that significantly enhances model generation. It is worth noticing that thepost-processing can not fully address this issue but alleviate it.",
  "HLimitations and Boarder Impact": "Due to the limited motion-music data and computing resources, the scaling law of our model is nottestified in a super large dataset. Besides, our model depends on some data pre-processing methodslike mel-spectrogram extraction and keypoints extraction by OpenPose, which may lead to erroraccumulations. MoMu-Diifusion promotes both neural motion and music synthesis, so it may help expand any impactthat generative systems have on the broader world like copyright conflicts. We will add constraintsand licenses when open-resourcing our code and pre-trained models."
}