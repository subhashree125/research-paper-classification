{
  "Abstract": "We give a faster algorithm for computing an approximate John ellipsoid around n points in d dimen-sions. The best known prior algorithms are based on repeatedly computing the leverage scores of thepoints and reweighting them by these scores [CCLY19]. We show that this algorithm can be substantiallysped up by delaying the computation of high accuracy leverage scores by using sampling, and then latercomputing multiple batches of high accuracy leverage scores via fast rectangular matrix multiplication.We also give low-space streaming algorithms for John ellipsoids using similar ideas.",
  "Introduction": "The John ellipsoid problem is a classic algorithmic problem, which takes as input a set of n points {a1, a2, . . . , an}in d dimensions, and asks for the minimum volume enclosing ellipsoid (MVEE) of these n points. If P isthe convex hull of these n points, then a famous result of Fritz John [Joh48] states that such an ellipsoidsatises 1",
  "dQ P Q, and if P is furthermore symmetric, then1": "dQ P Q. Equivalently, we mayconsider the n input points to be constraints of a polytope {x : ai, x 1, i [n]}, in which case theproblem is to compute a maximal volume inscribed ellipsoid (MVIE). These two problems are related bytaking polars, which corresponds to inverting the quadratic form dening the ellipsoid. In this work, wefocus on the symmetric case, so that the polytope P may be written as P = {x : Ax 1}, where Adenotes the n d matrix with the n input points ai in the rows, and our goal is to output an ellipsoid Qwhich approximately satises Q P d Q.The John ellipsoid problem has far-reaching applications in numerous elds of computer science.Instatistics, the John ellipsoid problem is equivalent to the dual of the D-optimal experiment design problem[Atw69, Sil13], in which one seeks weights for selecting a subset of a dataset to observe in an experiment.Other statistical applications of John ellipsoids include outlier detection [ST80] and pattern recognition[Ros65, Gli98]. In the optimization literature, computing John ellipsoids is a fundamental ingredient for theellipsoid method for linear programming [Kha79], cutting plane methods [TKE88], and convex programming[Sho77, Vai96].Other applications in theoretical computer science include sampling [Vem05, CDWY18, GN23], bandits [BCK12, HK16], dierential privacy [NTZ13], coresets [TMF20, TWZ+22], and randomizednumerical linear algebra [Cla05, DDH+09, WY23, BMV23].We refer to a survey of Todd [Tod16] for an account on algorithms and applications of John ellipsoids.",
  "John ellipsoids via iterated leverage scores": "Following a long line of work on algorithms for computing John ellipsoids [Wol70, Atw73, KT93, NN94,Kha96, STT78, KY05, SF04, TY07, AST08, Yu11, HFR20] based on convex optimization techniques, thework of [CCLY19] developed a new approach towards computing approximate John ellipsoids via a simplexed point iteration approach. The approach of [CCLY19] starts with an observation on the optimalitycondition for the dual problem, given by",
  "For a weight vector w Rn, we will often write the corresponding n n diagonal matrix diag(w) as the capitalized versionW": "Note that the computation of leverage scores can be done in O(nd1) time, where 2.371552 is thecurrent exponent of fast matrix multiplication [DWZ23, WXXZ24]. Thus, this gives an algorithm runningin time O(1nd1 log(n/d)) for outputting an ellipsoid with the guarantee of (3).It is known that input matrices with additional structure admit even faster implementation of thisalgorithm. For instance, [CCLY19, SYYZ22] give faster algorithms for sparse matrices A for which thenumber of nonzero entries nnz(A) is much less than nd. The work of [SYYZ22] shows that this approachcan also be sped up for matrices A with small treewidth.",
  "Our results": "Our rst main result is a substantially faster algorithm for computing John ellipsoids. In the typical regimewhere n d, our algorithm runs in O(1nd) log(n/d) time to output an ellipsoid Q satisfying (3), andO(1nd2) log(n/d) time to output an ellipsoid Q which approximates the maximal volume up to a (1 + )factor. We will discuss our techniques for this result in Sections 1.2.1 and 1.2.2.",
  "Linear time leverage scores via fast matrix multiplication": "We start by showing how to approximate leverage scores up to (1 + ) factors in O(nd) time, which hadnot been observed before to the best of our knowledge. Note that if we compute exact leverage scores usingfast matrix multiplication, then this takes time O(nd1). Alternatively, sketching-based algorithms forapproximate leverage scores are known, which gives the following running time for sparse matrices A withnnz(A) nonzero entries.",
  "in time O(nd) poly log(1 log(n/)) + O(n) poly(1 log(n/))": "Our improvement comes from improving the running time analysis of a sketching-based algorithm of[DMMW12] by using fast rectangular matrix multiplication. We will need the following result on fast matrixmultiplication: Theorem 1.4 ([Cop82, Wil11, Wil24]). There is a constant 0.1 and an algorithm for multiplying am m and a m m matrix in O(m2 poly log m) time, under the assumption that eld operations can becarried out in O(1) time.",
  "By applying the above result in blocks, we get the following version of this result for rectangular matrixmultiplication": "Corollary 1.5. There is a constant 0.1 and an algorithm for multiplying a n d for n d and a d tmatrix in O(nd + nt1/+1) poly log t time, under the assumption that eld operations can be carried out inO(1) time. Proof. Let m = t1/. If d m, then matrix multiplication takes only O(ndt) = O(nt1/+1) time, so assumethat d m. We partition the rst matrix into an O(n/m) O(d/m) block matrix with blocks of size m mand the second into an O(d/m) 1 block matrix with blocks of size m m. By Theorem 1.4, each blockmatrix multiplication requires O(m2 poly log m) time, and we have O(nd/m2) of these to do, which gives thedesired running time. That is, the above result shows that when multiplying an n d matrix A with a d t matrix for a muchsmaller t, then this multiplication can be done in roughly O(nd) poly log t time. The work of [DMMW12]shows that the leverage scores of A can be written as the row norms of AR for a d t matrix witht = O(2 log(n/)), and thus this gives us the result of Theorem 1.3.",
  "O(1nd) log(n/d) poly log(1 log n),": "which substantially improves upon prior algorithms for dense input matrices A. We now show how to obtainfurther improvements by using the idea of lazy updates. At the heart of our idea is to only compute thequadratic forms for the John ellipsoids for most iterations, and defer the computation of the weights untilwe have computed roughly O(log n) iterations. At the end of this group of iterations, we can then computethe John ellipsoid weights via fast matrix multiplication as used in Theorem 1.3, which allows us to removethe suboptimal poly log log n terms in the dominating term of the running time.",
  "w(t)i= w(t1)i ai (Q(t1))ai.(4)": "Thus, given high-accuracy spectral estimates to the quadratics Q(t), we can recover the weights w(t)ito highaccuracy in O(d) time per iteration by evaluating the quadratic forms ai (Q(t))ai and then multiplyingthem together. This approach is useful for fast algorithms if we only need to to do this for a small numberof indices i [n]. This is indeed the case if we only need these weights for a row sample of W(t)A, whichis sucient for computing a spectral approximation to the next quadratic form Q(t). Furthermore, we onlyneed low-accuracy leverage scores (up to a factor of, say, n0.1) to obtain a good row sample, which can bedone quickly for all n rows [LMP13, CLM+15]. Thus, by repeatedly sampling rows of W(t)A, computinghigh-accuracy weights on the sampled rows, and then building an approximate quadratic, we can iterativelycompute high-accuracy approximations Q(t) to the quadratics Q(t). More formally, our algorithm takes thefollowing steps:",
  "W(t)A to highaccuracy, since we do not know the weights w(t) to high accuracy": "If we only need the weights w(t) for a small number of sampled rows S [n], then we can explicitlycompute these using (4), since we inductively have access to high-accuracy quadratics Q(t) for t =0, 1, 2, . . ., t 1. These can then be used to build Q(t). While this algorithm allows us to quickly compute high-accuracy approximate quadratics Q(t), thisalgorithm cannot be run for too many iterations, as the error in the low-accuracy leverage scores u(t) growsto poly(n) factors in O(log n) rounds. This is a problem, as this error factor directly inuences the numberof leverage score samples needed to approximate Q(t). We will now use the fast matrix multiplication trickfrom the previous .2.1 to x this problem. Indeed, after O(log n) iterations, we will now haveapproximate quadratics Q(1), Q(2), . . . , Q(t) for t = O(log n). Now we just need to compute the n Johnellipsoid weights which are given by",
  "t=1ei A( Q(t1))1/222": "To approximate this quickly, we can approximate each term ei A( Q(t1))1/222 by ei A( Q(t1))1/2G(t)22for a random Gaussian matrix G(t), by the JohnsonLindenstrauss lemma [JL84]. Here, the number ofcolumns of the Gaussian matrix can be taken to be poly(1 log n), so now all we need to compute is thematrix productA [( Q(0))1/2G(0), ( Q(1))1/2G(1), . . . , ( Q(t))1/2G(t)] which is the product of a nd matrix and a dm matrix for m = poly(1 log n). By Theorem 1.4, this canbe computed in O(nd poly log m) time. However, this resetting procedure is only run O(1) times acrossthe T = O(1 log n) iterations, so the running time contribution from the resetting is just",
  "O(1nd)(log(n/d) + poly log(1 log n)) + O(n) poly(1 log n)": "Remark 1.7. In general, our techniques can be be viewed as a way of exploiting the increased eciency ofmatrix multiplication when performed on a larger instance by delaying large matrix multiplication operations,so that the running time is O(1nd log(n/d)) + O(1)Tr where Tr is the time that it takes to multiply Aby a d r matrix for r = poly(1 log n). While we have instantiated this general theme by obtaining fasterrunning times via fast matrix multiplication, one can expect similar improvements by other ways of exploitingthe economies of scale of matrix multiplication, such as parallelization. For instance, we recover the samerunning time if we can multiply r vectors in parallel so that Tr = O(nd).",
  "Streaming algorithms": "The problem of computing John ellipsoids is also well-studied in the streaming model, where the input pointsai arrive one at a time in a stream [MSS10, AS15, WY22, MMO22, MMO23]. The streaming model isoften considered when the number of points n is so large that we cannot t all of the points in memory atonce, and the focus is primarily on designing algorithms with low space complexity. Our second result ofthis work is that approaches similar to the one we take to prove Theorem 1.6 in fact also give a low-spaceimplementation of the iterative John ellipsoid algorithm of [CCLY19]. Theorem 1.8 (Streaming algorithms). Given A Rnd, let P be the polytope dened by P = {x Rd :Ax 1}. Furthermore, suppose that A is presented in a stream where the rows ai Rd arrive oneby one in a stream. For (0, 1), there is an algorithm, Algorithm 1, that makes T = O(1 log(n/d))passes over the stream, takes O(d2T ) time to update after each new row, and returns an ellipsoid Q suchthat1",
  "d Q. Furthermore, the algorithm uses at most O(d2T ) words of space": "In .2.2, we showed that by storing only the quadratics Q(t) and only computing the weightstt=1 ai( Q(t1))ai as needed, we could design fast algorithms for John ellipsoids. In fact, this idea isalso useful in the streaming setting, since storing all of the weights w(t)irequires O(n) space per iteration,whereas storing the quadratics Q(t) requires only O(d2) space per iteration. Furthermore, in the streamingsetting, we may optimize the update running time by instead storing the pseudo-inverse of the quadratics( Q(t)) and then updating them by using the Sherman-Morrison low rank update formula. This gives theresult of Theorem 1.8.Although storing the pseudoinverse only requires O(d2) words of space, the space of each word may bemuch larger in the bit complexity model. We leave the problem of obtaining a fast and low space algorithmin the bit complexity model as a direction for future research.",
  "Notation": "Throughout this paper, A will denote an n d matrix whose n rows are given by vectors ai Rd. Forpositive numbers a, b > 0, we write a = (1)b to mean that (1)b a (1+)b. For symmetric positivesemidenite matrices Q, R, we write Q = (1 )R to mean that (1 )R Q (1 + )R, where denotesthe Lowner order on PSD matrices.",
  "E[es log Xi]tRs = Cts,kRs": "Using the above result, we can show that if the 2 variables have k = O(1/) degrees of freedom and thenumber of rounds T is c log n for a small enough constant c, then the product of the 2 variables Tt=1 Xtwill be within a n factor for some small constant > 0.",
  "where v(t)i= t1t=1 ai ( Q(t))ai for i [n]. Furthermore, Q(t) can be computed in O(n2)T 2d+1 log ntime in each iteration": "Proof. We will rst condition on the success of the event of Lemma 2.2, so that the 2 products in (5) arebounded by n factors for every row i [n] and every iteration t [t]. We will also condition on the successof the leverage score sampling for all T iterations.Note that by (5) and the bound the 2 products, u(t)iis within a O(n2) factor of v(t)i , and thus S(t) isa correct leverage score sample for",
  "Note then that Line 8 takes a product of at most BT of these approximations, so the total error in theapproximation is at most(1 /BT )BT = (1 O())": "The running time is given by BT iterations of the inner loop of ApproxQuadratic and B iterations ofthe fast matrix multiplication procedure in Line 7 of Algorithm 3. The inner loop of ApproxQuadraticrequires O(nd) time to compute the product with the d k Gaussian as well as the time to compute theapproximate quadratic, which is bounded in Lemma 2.4. Altogether, this gives the claimed running timebound.",
  "Future directions": "In this work, we developed fast algorithms and low-space streaming algorithms for the problem of computingJohn ellipsoids. Our fast algorithms use a combination of using lazy updates together with fast matrixmultiplication to substantially improve the running time of John ellipsoids, and we apply similar ideas toobtain a low-space streaming implementation of the John ellipsoid algorithm. Our results have several limitations that we discuss here, which we leave for future work to resolve. First,our algorithm makes crucial use of fast matrix multiplication in order to get running time improvements.However, this makes it a less attractive option for practical implementations, and also makes the polynomialdependence on in the O(n) poly(1) term rather large. Thus, it is an interesting question whether therunning time that we obtain in Theorem 1.6 is possible without fast matrix multiplication.",
  "Question 3.2. What is the optimal running time of approximating John ellipsoids?": "For instance, one interesting question is whether it is possible to obtain nearly linear time algorithms thatrun in time O(nd) + O(n) poly(1), or even input sparsity time algorithms that run in time O(nnz(A)) +O(n) poly(1). The resolution of such questions for the least squares linear regression problem has led togreat progress in algorithms, and studying these questions for the John ellipsoid problem may have interestingconsequences as well.John ellipsoids are closely related to p Lewis weights, which give a natural p generalization of Johnellipsoids and leverage scores and have been a valuable tool in randomized numerical linear algebra. There hasbeen a recent focus on fast algorithms for computing (1+)-approximate p Lewis weights [FLPS22, AGS24],and thus it is natural to ask whether developments in algorithms for John ellipsoids would carry over toalgorithms for p Lewis weights as well.",
  "Question 3.3. What is the optimal running time of approximating p Lewis weights?": "Finally, we raise questions concerning streaming algorithms for approximating John ellipsoids. In Theo-rem 1.8, we gave a multi-pass streaming algorithm which obtains a (1 + )-optimal John ellipsoid. A naturalquestion is whether a similar algorithm can be achieved in fewer passes, or if there are pass lower boundsfor computing (1 + )-optimal John ellipsoids. Question 3.4. Are there small space streaming algorithms for approximating John ellipsoids up to a (1 + )factor which make fewer than O(1 log(n/d)) passes, or do small space streaming algorithms necessarilyrequire many passes?",
  "[Atw73]Corwin L. Atwood. Sequences converging to D-optimal designs of experiments. Ann. Statist.,1:342352, 1973. 1.1, 3": "[BCK12]Sebastien Bubeck, Nicol`o Cesa-Bianchi, and Sham M. Kakade. Towards minimax policies foronline linear optimization with bandit feedback. In Shie Mannor, Nathan Srebro, and Robert C.Williamson, editors, COLT 2012 - The 25th Annual Conference on Learning Theory, June 25-27, 2012, Edinburgh, Scotland, volume 23 of JMLR Proceedings, pages 41.141.14. JMLR.org,2012. 1 [BMV23]Aditya Bhaskara, Sepideh Mahabadi, and Ali Vakilian. Tight bounds for volumetric spannersand applications. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: AnnualConference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,USA, December 10 - 16, 2023, 2023. 1 [CCLY19]Michael B. Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm forapproximating the John ellipsoid. In Alina Beygelzimer and Daniel Hsu, editors, Conference onLearning Theory, COLT 2019, 25-28 June 2019, Phoenix, AZ, USA, volume 99 of Proceedingsof Machine Learning Research, pages 849873. PMLR, 2019. (document), 1.1, 1, 1, 1.2.2, 1.2.3,2.2, 2.5",
  "[CDWY18]Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast MCMC sampling algo-rithms on polytopes. J. Mach. Learn. Res., 19:55:155:86, 2018. 1": "[Cla05]Kenneth L. Clarkson. Subgradient and sampling algorithms for 1 regression. In Proceedingsof the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 05, pages 257266, USA, 2005. Society for Industrial and Applied Mathematics. 1 [CLM+15]Michael B. Cohen, Yin Tat Lee, Cameron Musco, Christopher Musco, Richard Peng, and AaronSidford. Uniform sampling for matrix approximation. In Tim Roughgarden, editor, Proceedingsof the 2015 Conference on Innovations in Theoretical Computer Science, ITCS 2015, Rehovot,Israel, January 11-13, 2015, pages 181190. ACM, 2015. 1.2.2, 2.2",
  "[DG03]Sanjoy Dasgupta and Anupam Gupta.An elementary proof of a theorem of Johnson andLindenstrauss. Random Struct. Algorithms, 22(1):6065, 2003. 2.6": "[DMM06]Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan.Sampling algorithms for 2regression and applications. In Proceedings of the Seventeenth Annual ACM-SIAM Symposiumon Discrete Algorithms, SODA 2006, Miami, Florida, USA, January 22-26, 2006, pages 11271136. ACM Press, 2006. 2.3 [DMMW12] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodru.Fastapproximation of matrix coherence and statistical leverage. J. Mach. Learn. Res., 13:34753506, 2012. 1.2, 1.2.1, 1.2.1, 2.2 [DWZ23]Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing.In 64th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2023, SantaCruz, CA, USA, November 6-9, 2023, pages 21292138. IEEE, 2023. 1 [FLPS22]Maryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing Lewis weightsto high precision. In Joseph (Se) Naor and Niv Buchbinder, editors, Proceedings of the 2022ACM-SIAM Symposium on Discrete Algorithms, SODA 2022, Virtual Conference / Alexandria,VA, USA, January 9 - 12, 2022, pages 27232742. SIAM, 2022. 3",
  "[HK16]Elad Hazan and Zohar S. Karnin.Volumetric spanners: An ecient exploration basis forlearning. J. Mach. Learn. Res., 17:119:1119:34, 2016. 1": "[JL84]William B. Johnson and Joram Lindenstrauss. Extensions of Lipschitz mappings into a Hilbertspace. In Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26of Contemp. Math., pages 189206. Amer. Math. Soc., Providence, RI, 1984. 1.2.2, 2.6 [Joh48]Fritz John. Extremum problems with inequalities as subsidiary conditions. In Studies and Es-says Presented to R. Courant on his 60th Birthday, January 8, 1948, pages 187204. IntersciencePublishers, Inc., New York, N. Y., 1948. 1",
  "[KY05]Piyush Kumar and E. Alper Yildirim. Minimum-volume enclosing ellipsoids and core sets. J.Optim. Theory Appl., 126(1):121, 2005. 1.1, 1, 3": "[LMP13]Mu Li, Gary L. Miller, and Richard Peng.Iterative row sampling.In 54th Annual IEEESymposium on Foundations of Computer Science, FOCS 2013, 26-29 October, 2013, Berkeley,CA, USA, pages 127136. IEEE Computer Society, 2013. 1.2.2, 2.2 [MMO22]Yury Makarychev, Naren Sarayu Manoj, and Max Ovsiankin.Streaming algorithms for el-lipsoidal approximation of convex polytopes. In Po-Ling Loh and Maxim Raginsky, editors,Conference on Learning Theory, 2-5 July 2022, London, UK, volume 178 of Proceedings ofMachine Learning Research, pages 30703093. PMLR, 2022. 1.2.3, 3",
  "[MMO23]Yury Makarychev, Naren Sarayu Manoj, and Max Ovsiankin. Near-optimal streaming ellip-soidal rounding for general convex polytopes. CoRR, abs/2311.09460, 2023. 1.2.3, 3": "[MSS10]Asish Mukhopadhyay, Animesh Sarker, and Tom Switzer. Approximate ellipsoid in the stream-ing model. In Weili Wu and Ovidiu Daescu, editors, Combinatorial Optimization and Applica-tions - 4th International Conference, COCOA 2010, Kailua-Kona, HI, USA, December 18-20,2010, Proceedings, Part II, volume 6509 of Lecture Notes in Computer Science, pages 401413.Springer, 2010. 1.2.3 [NN94]Yurii Nesterov and Arkadii Nemirovskii. Interior-point polynomial algorithms in convex pro-gramming, volume 13 of SIAM Studies in Applied Mathematics.Society for Industrial andApplied Mathematics (SIAM), Philadelphia, PA, 1994. 1.1, 3 [NTZ13]Aleksandar Nikolov, Kunal Talwar, and Li Zhang. The geometry of dierential privacy: thesparse and approximate cases. In Dan Boneh, Tim Roughgarden, and Joan Feigenbaum, editors,Symposium on Theory of Computing Conference, STOC13, Palo Alto, CA, USA, June 1-4,2013, pages 351360. ACM, 2013. 1",
  "[Tod16]Michael J. Todd. Minimum volume ellipsoids - theory and algorithms, volume 23 of MOS-SIAMSeries on Optimization. SIAM, 2016. 1": "[TWZ+22]Murad Tukan, Xuan Wu, Samson Zhou, Vladimir Braverman, and Dan Feldman. New coresetsfor projective clustering and applications. In Gustau Camps-Valls, Francisco J. R. Ruiz, andIsabel Valera, editors, International Conference on Articial Intelligence and Statistics, AIS-TATS 2022, 28-30 March 2022, Virtual Event, volume 151 of Proceedings of Machine LearningResearch, pages 53915415. PMLR, 2022. 1",
  "[Wol70]P. Wolfe. Convergence theory in nonlinear programming. In Integer and nonlinear programming,pages 136. North-Holland, Amsterdam-London, 1970. 1.1, 3": "[WXXZ24]Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds formatrix multiplication: from alpha to omega.In David P. Woodru, editor, Proceedings ofthe 2024 ACM-SIAM Symposium on Discrete Algorithms, SODA 2024, Alexandria, VA, USA,January 7-10, 2024, pages 37923835. SIAM, 2024. 1 [WY22]David P. Woodru and Taisuke Yasuda. High-dimensional geometric streaming in polynomialspace. In 63rd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2022,Denver, CO, USA, October 31 - November 3, 2022, pages 732743. IEEE, 2022. 1.2.3, 3 [WY23]David P. Woodru and Taisuke Yasuda. New subset selection algorithms for low rank approxi-mation: Oine and online. In Barna Saha and Rocco A. Servedio, editors, Proceedings of the55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June20-23, 2023, pages 18021813. ACM, 2023. 1"
}