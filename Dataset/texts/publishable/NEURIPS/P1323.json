{
  "Abstract": "DNA barcodes are crucial in biodiversity analysis for building automatic identifica-tion systems that recognize known species and discover unseen species. Unlike hu-man genome modeling, barcode-based invertebrate identification poses challengesin the vast diversity of species and taxonomic complexity. Among Transformer-based foundation models, BarcodeBERT excelled in species-level identificationof invertebrates, highlighting the effectiveness of self-supervised pretraining onbarcode-specific datasets. Recently, structured state space models (SSMs) haveemerged, with a time complexity that scales sub-quadratically with the contextlength. SSMs provide an efficient parameterization of sequence modeling relativeto attention-based architectures. Given the success of Mamba and Mamba-2 in nat-ural language, we designed BarcodeMamba, a performant and efficient foundationmodel for DNA barcodes in biodiversity analysis. We conducted a comprehen-sive ablation study on the impacts of self-supervised training and tokenizationmethods, and compared both versions of Mamba layers in terms of expressive-ness and their capacity to identify unseen species held back from training. Ourstudy shows that BarcodeMamba has better performance than BarcodeBERT evenwhen using only 8.3% as many parameters, and improves accuracy to 99.2% onspecies-level accuracy in linear probing without fine-tuning for seen species.In our scaling study, BarcodeMamba with 63.6% of BarcodeBERTs parametersachieved 70.2% genus-level accuracy in 1-nearest neighbor (1-NN) probing forunseen species. The code repository to reproduce our experiments is available at",
  "Introduction": "A DNA barcode is a short and standardized section of nucleotides within the genome that allowstaxonomic identification at the species level without the need to consider entire genomes, makingit efficient and invaluable for biodiversity analysis . For many animal groups, particularlyinvertebrates, part of the mitochondrial gene Cytochrome c oxidase Subunit I (COI) is commonlyused. However, different genes serve as barcodes for other organisms. Plants often rely on plastidgenes such as rbcL and matK, while for fungi, the internal transcribed spacer (ITS) region is frequentlyemployed. These genetic markers can be utilized to establish automatic taxonomic identificationsystems that recognize species known and unknown to science. Such systems significantly reduce theamount of manual labor typically required by taxonomic experts. Among the barcode-based analysis tasks, invertebrate taxonomic classification is particularlychallenging due to the imbalance in data distributions and intrinsic diversity of labels. Identifyingtaxonomic relationships among a large number of classes is complex and requires expertise in",
  "arXiv:2412.11084v1 [cs.LG] 15 Dec 2024": "taxonomy. Unidentified species and incomplete taxonomic annotations pose challenges for accurateclassification. Therefore, this task differs significantly from the design objectives of most modernDNA models. Numerous studies have been proposed to tackle the challenges posed by DNA analysis and genomics.Early machine learning approaches employed task-specific end-to-end training based on convolutionalneural networks (CNNs) . These methods yield models capable of solving classification tasks withhigh accuracy using a relatively small number of parameters. In recent years, Transformers havedominated various sequence modeling tasks, notably in natural language. Their ability to leverageself-supervised learning (SSL) on unlabeled datasets and fine-tune on downstream tasks has madethem highly effective. Transformer-based foundation models have been introduced into the genomicsspace , bringing their ability to generalize across diverse tasks. Models like DNABERT and DNABERT-2 have demonstrated this capability in human and multi-species DNA analysis,as well as the Nucleotide Transformer , GENA-LM and GROVER . However, thesemodels were not specifically designed to address the challenges posed by biodiversity analysis. WhileBERTax can be fine-tuned for taxonomic classification, its predictions are limited to known taxaand only at the superkingdom, phylum and genus levels. To fill this gap, BarcodeBERT was developed as a specialized model for DNA barcode analysis,with a particular focus on challenges posed by species classification of invertebrates. Unlike itspredecessors, BarcodeBERT was designed to account for the unique characteristics of DNA barcodes.In particular, the use of non-overlapping k-mer-based tokenizers demonstrated significant improve-ments in zero-shot classification of unseen species to the correct genus, surpassing the performanceof CNNs and off-the-shelf Transformer-based DNA foundation models. Recently, foundation modelsutilizing a structured SSM as their backbone have demonstrated impressive performance in humanDNA modeling . Nevertheless, consistent with BarcodeBERTs results, we find that currentoff-the-shelf foundation models may not perform optimally without barcode-specific pretraining. In this study, we introduce BarcodeMamba, an efficient foundation model for DNA barcode modeling.Our model demonstrates competitive performance compared to BarcodeBERT on the CanadianInvertebrate species classification task with only 8.3% of the parameters. BarcodeMamba reaches99.2% accuracy on a species-level linear probing task without fine-tuning, demonstrating its capabilityin DNA barcode modeling. After scaling up, BarcodeMamba achieves 99.2% accuracy on species-level linear probing and 70.2% on 1-NN genus-level probing.",
  ". Introducing BarcodeMamba, an efficient method for self-supervised learning using DNAbarcode data for biodiversity analysis based on the state-of-the-art Mamba-2 architecture": "2. Conducting a comprehensive ablation study to identify the optimal settings for differentaspects of biodiversity analysis, including character-level and k-mer tokenizers and vari-ous tasks for self-supervised pretraining. Comparing both versions of Mamba todetermine their respective advantages in modeling DNA barcodes. 3. Scaling the top two BarcodeMamba variants to assess improvements in both DNA barcodemodeling (measured by perplexity) and downstream classification tasks (species- and genus-level accuracy) under both tokenization strategies. 4. Comparing BarcodeMambas performance with baselines from classical supervised learn-ing, as well as Transformer-based and SSM-based foundation models, in the taxonomicclassification of 1.5 M Canadian invertebrates.",
  "Background: Structured State Space Models for DNA Analysis": "To address the quadratic cost of self-attention in Transformer-based models and the need to handlelong contexts, SSMs have been developed to build sequential models with linear or near-linearcomplexity. This advancement has significantly reduced computation costs and accelerated trainingspeed. Since the emergence of the structured state space sequence (S4) model , SSMs canbe computed with a long convolution during training and recurrence during inference, enablingmore efficient computations for sequence modeling. Furthermore, these models exhibit promisingproperties when scaled up similarly to Transformers . Unlike prior Linear Time Invariant (LTI) models, Mamba-based models are capable of effectiveunidirectional representation learning. Mamba has been proposed as a linear-time sequencemodel. Most recently, Mamba-2 was introduced to integrate the theory of SSMs with attentionmechanisms, increasing the efficiency of modeling sequences of dense information, such as language.The selective copying synthetic task introduced in Mamba demonstrates that Mamba-based modelscan use input-dependent parameterization to selectively remember or ignore inputs based on theircontent . Building upon this property, we expect Mamba-based models to excel at handling nucleotide align-ment gaps in DNA barcode sequences. This makes representation learning less susceptible tovariations in DNA sample quality, sequencing technologies, and specific regions of genomes withstructural complexity that are difficult to identify due to technical limitations. Furthermore, theresults of the multi-query associative recall synthetic task indicate that Mamba-2 is able to memorizemultiple associations, and efficiently parameterize and parallelize its implementation for improvedperformance in modeling dense information . Additionally, Mamba-based models are capable ofachieving competitive results compared to Transformer-based models of the same size or larger inlanguage modeling. Motivated by this, we developed a Mamba-2-based DNA barcode foundationmodel to explore its potential in biodiversity analysis. With a dual form of kernelized attentionand linear recurrence in Mamba-2, BarcodeMamba can be efficiently trained with hardware-awareparallelization and inferred auto-regressively.",
  "Dataset": "In this study, we employed the Canadian invertebrate dataset, consisting of 1.5 M samples from theBarcode of Life Datasystem (BOLD) as our primary data source . We adopted the preprocessingmethod introduced in BarcodeBERT . Each record in the dataset consists of five possible characters,namely A, T, G, C, and N, representing alignment gaps or IUPAC ambiguity codes. We examinedtwo tokenizers used in DNA barcode modeling: k-mer, used by BarcodeBERT, and character-level,which is popular in (non-barcode) SSM models. During both self-supervised pretraining and downstream evaluation phases, we applied the same datasplits as in BarcodeBERT. The length of all DNA barcode data was fixed at 660 base pairs ofnucleotides. During self-supervised pretraining, 95% of the data, consisting of 0.9 M sequences, wasused for training and 5% (47.1 k sequences) for validation. After pretraining, we fine-tuned the modelfor species-level classification of known arthropods using a dataset comprising 1,653 classes. Duringfine-tuning on 67.2 k sequences, 70% of the data was used for training, 20% for testing, and 10% forvalidation. In addition to probing unseen species as in BarcodeBERT, we measured the perplexity ofthe models output on unseen data that did not overlap with the pretraining or fine-tuning subsets.",
  "Network Architectures": "CNN Encoder and Transformer BaselinesWe adopted the experimental setting in BarcodeBERTfor our study . Our CNN and Transformer baselines include a supervised CNN encoder similar tothat used in and BERT-based foundation models. The CNN encodes the context of DNA data withconvolution layers, while DNABERT, designed for genomic understanding, utilized k-mer tokenizersto process nucleotide context and effectively predicted splicing and transcription factor binding sitein human DNA. In DNABERT-2, the authors deployed Byte Pair Encoding tokenizers for genomictokenization across multiple species. BarcodeBERT also serves as a baseline in our research, utilizinga k-mer tokenizer and implementing direct masked pretraining on barcodes. State Space Model BaselinesOur SSM baselines include HyenaDNA and Caduceus models.HyenaDNA used an implicit convolution to match the performance of attention-based transformers inDNA modeling. By leveraging global context at each layer, the authors extended the context lengthup to 1 M in human genome modeling . In contrast to aggregation for creating vocabularies, acharacter-level tokenizer was implemented to capture single nucleotide polymorphisms or mutations and dependencies in gene expression. As a decoder-only causal model with a sequence-to-sequencearchitecture, HyenaDNA utilized next token prediction (NTP) for pretraining. Notably, the modeldemonstrated superior performance on the benchmarks considered by the nucleotide transformer as well as genomic benchmarks . Caduceus is a DNA modeling framework that leverages MambaDNA blocks. It utilizes aBi-Mamba architecture to incorporate bi-directionality for analyzing reverse complementarity (RC)on pairs of DNA strands. Unlike our primary focus on species identification and discovering unseenspecies, the authors of Caduceus performed efficient variant prediction to study evolutionary pressure.The Mamba computation was applied twice: once on the reversed and once on the forward sequence,with an efficient implementation using shared projection weights. Additionally, masked languagemodeling (MLM) was used for pretraining. Similar to HyenaDNA, Caduceus tokenizes sequencesby characters. The Caduceus-PS setting incorporates RC-equivariant token embedding, while theCaduceus-PH setting involves RC data augmentation. Caduceus outperforms uni-directional modelslacking RC equivariance.BarcodeMambaBarcodeMamba follows a language model backbone and decoder architecture.The model processes input through n stacked blocks, each containing layer normalization, amulti-layer perceptron, and a Mamba-2 mixing layer that maps d-dimensional inputs through ap-dimensional head. The resulting hidden states serve as input to the decoder. While previousSSM-based foundation models for DNA analysis have primarily relied on character-level tokenizersfor human DNA sequences, BarcodeMamba explores both character-level and k-mer tokenizationapproaches. The k-mer approach enables the model to capture local patterns essential for classifica-tion, rather than processing individual nucleotides. During pretraining, we augmented the data usingreverse complement sequences and investigated two pretraining objectives: NTP, which is preferredby causal models, and MLM, which was successfully applied in BarcodeBERT and Caduceus fordiscriminative downstream tasks.",
  "Our methodology consists of several key steps:": "1. Fine-tuned: We first train BarcodeMamba on a pretraining dataset split, followed by fine-tuning using supervised training datasets. We then evaluate the models accuracies onspecies-level barcode-based classification. 2. Linear probe: To assess the effectiveness of self-supervised learning on DNA barcodes, weemploy pretrained models as feature extractors. This involves training a linear classifier onembeddings extracted from each pretrained model, and evaluating its accuracy of classifyingknown species. 3. 1-NN probe: Finally, to evaluate the models ability to generalize to new taxonomic groups,we implement genus-level 1-NN probing on barcode sequences from unseen species. Thisinvolves training a 1-NN classifier on the embeddings of pretrained models and evaluatingits accuracy of identifying unknown species at genus level.",
  "Implementation DetailsWe evaluate BarcodeMamba against a comprehensive set of baselinesused in the BarcodeBERT study and recent work on SSM-based DNA foundation models. Our": "baselines include a traditional CNN encoder that is trained by supervised learning, as wellas pretrained foundation models. Among the latter, we consider the Transformer-based modelsBarcodeBERT, DNABERT, and DNABERT-2, along with SSM-based models including HyenaDNAand Caduceus, selecting versions with comparable parameter counts available on HuggingFace. Weadopt the hyperparameter settings reported for these models and conduct a grid search over linearprobing hyperparameters, including the learning rate, momentum, and weight decay for the SGDoptimizer. Specifically, we test learning rates of [0.01, 0.1, 0.5], momenta of [0.2, 0.4, 0.6, 0.8] andweight decays of . Finally, we present the best results for all baselines andcompare them with the performance of BarcodeMamba in . ResultsThe results of our comparison study are presented in . In fine-tuning (first column)we see that all models perform reasonably well, with HyenaDNA-tiny achieving the highest accuracyby a small margin. However, in the more challenging test of SSL-trained representations (columns2 & 3), similar to BarcodeBERT, our linear and 1-NN probing results demonstrate a substantialimprovement compared to all other models. In terms of parameters, our BarcodeMamba modelexhibits superior performance to BarcodeBERT with less than 7.4 M parameters (vs. 86.2-89.2 M).Utilizing the character-level tokenizer and NTP pretraining objective, BarcodeMamba achieves highaccuracy in fine-tuning and linear probing tasks. For the 1-NN probing task, our model benefits froma k-mer tokenizer with k = 6. As we scale up BarcodeMamba to 56.7 M parameters, it reaches thehighest accuracy in linear probing as well as 1-NN probing, indicating great potential for practicalbiodiversity analysis. : Two groups of baselines: off-the-shelf foundation models pretrained on human genomedatasets vs. BarcodeBERT and our model BarcodeMamba, which are specifically pretrained on DNAbarcode-based datasets. We sort these models by their number of parameters in descending orderwithin the respective families to facilitate comparison. The numbers in parentheses are the optimalk-mer values that yielded the best results, where k=1 denotes the use of a character-level tokenizer.The parameter counts are presented as ranges due to the variability in vocabulary sizes associatedwith different values of k. / denotes metrics where higher/lower values are better.",
  "Ablation Study": "Implementation DetailsWe evaluated two tokenizers during training and inference: character-leveland k-mer-based. For k-mer length, we adhere to the approach of BarcodeBERT and set k = 4, 5, 6.Two pretext tasks for pretraining are explored: NTP and MLM. We use the AdamW optimizer with alearning rate of 6 104, a weight decay of 0.1, and betas set to 0.9 and 0.999. A cosine learningrate scheduler is applied, which includes a small learning rate that linearly warms up over the first 1%of the training duration before decaying to 10% of the initial learning rate.",
  "In terms of BarcodeMambas architecture, we set the model dimension to d = 256, number of layersto n = 2, and head dimension to p = 64": "ResultsAs demonstrated in Tables 2 and 3, for both pretraining tasks, Mamba-2 performs better asthe mixing layer in most scenarios. When using NTP, as detailed in , utilizing a character-level tokenizer enhances the fine-tuning and linear probing outcomes of BarcodeMamba. Thissuggests that character-level tokenization contributes to improved representation learning for the taskat hand. However, for 1-NN probing, the k-mer tokenization enables BarcodeMamba to achievesignificantly better results than character-level tokenization. Furthermore, as the length of the k-merincreases, the accuracy of probing on unseen datasets improves. This indicates that k-mer-basedtokenization captures shared motifs and sub-sequences across seen and unseen species barcodesmore effectively with larger window sizes. During testing with pretrained models on an unseendataset, BarcodeMamba generally shows higher perplexity with k-mer tokenization compared tocharacter-level tokenization. This can be attributed to the fact that there are only 5 characters in thevocabulary for the character-level tokenizer, compared to 4k + n_special_tokens vocabulary sizefor the k-mer tokenizer. The advantage of using a character-level tokenizer with MLM () is not as substantial aswith NTP (). Although BarcodeMamba achieves a lower perplexity on the unseen dataset,the results on linear probing and 1-NN probing are reduced by approximately 23 points. Despitethis, BarcodeMamba remains performant for the fine-tuning task with Mamba-2 as the mixing layer,demonstrating similar performance to NTP. : Classification Accuracy and Pretraining Perplexity of BarcodeMamba in Different Settingswith NTP: We present results using a character-level and k-mer tokenizer under various settings,focusing on the impact of different k-mer lengths (i.e., k = 4, 5, 6). Perplexity scores are comparablewithin a row but not across rows because of the changing vocabulary size. Therefore those are notbolded. / denotes metrics where higher/lower values are better.",
  "Char-98.798.197.095.941.233.01.411.37k-mer495.097.492.994.043.555.33.193.09k-mer594.295.691.592.648.557.74.164.04k-mer695.996.591.891.947.758.75.515.31": ": Classification Accuracy and Pretraining Perplexity of BarcodeMamba in Different Settingswith MLM: We present results using a character-level and k-mer tokenizer under various settings,focusing on the impact of different k-mer lengths (i.e., k = 4, 5, 6). Perplexity scores are comparablewithin a row but not across rows because of the changing vocabulary size. Therefore those are notbolded. / denotes metrics where higher/lower values are better.",
  "Scaling up BarcodeMamba": "Implementation DetailsBased on the results of our ablation study, we scaled up BarcodeMambawith the NTP pretraining objective in both character-level and k-mer tokenizer settings (k = 6), asthese configurations showed the most promise in fine-tuning accuracy, linear probing, and 1-NNprobing accuracy. Details on the number of layers, model dimensions, and batch sizes are providedin . BarcodeMamba uses more memory when using a character-level tokenizer due to theincreased sequence length required for learning barcode representations at single nucleotide resolution.",
  "M7.4 M4512161615.0 M19.2 M4768161633.6 M39.9 M6768161650.4 M56.7 M": ": Scaling analysis: Classification accuracy (%) of BarcodeMamba using a pretrained modelas a feature extractor. Metrics are compared between pretraining with a character-level tokenizer andwith a k-mer tokenizer (k = 6) (as labeled in the sub-figures). LP represents Linear Probe and 1-NNis short for 1-Nearest Neighbor Probe. Params1e7 LP accuracy (%) CharK-mer Params1e7 1-NN accuracy (%) CharK-mer : The evaluation of the BarcodeMamba performance involves perplexity and classificationaccuracy, using a pretrained model as a feature extractor. Metrics are compared between pretrainingwith a character-level tokenizer (left) and with a k-mer tokenizer (k = 6) (right). FT stands forFine-Tuning, LP represents Linear Probe and 1-NN is short for 1-Nearest Neighbor Probe. /denotes metrics where higher/lower values are better.",
  "M5.0796.993.663.27.4 M5.1091.590.549.29.8 M5.2095.794.663.519.2 M5.3293.694.060.434.2 M5.3495.995.468.539.9 M5.4397.795.870.256.7 M5.5594.794.760.590.2 M": "ResultsThe visualization depicted in demonstrates that under optimal model dimensionsand number of layers, both linear and 1-NN probing accuracy increase as the parameter count ofBarcodeMamba increases. Furthermore, provides a comprehensive set of scaling resultsfor all metrics, including Perplexity and Fine-tuning, showing how the effectiveness of NTP andclassification performance change as models grow in number of parameters. The performance ofBarcodeMamba with a character-level tokenizer is shown in (left), where perplexity, fine-tuning, seen species-level and unseen genus-level probing accuracy improve as BarcodeMambascales up. Specifically, the linear probing accuracy reaches a peak of 99.4%, while the 1-NN probingaccuracy achieves its highest value of 61.1% at 50.4 M parameters. As shown in (right),scaling up the BarcodeMamba model with the k-mer tokenizer (k = 6) improves its classificationperformance in linear and 1-NN probing. Overall, BarcodeMamba shows potential for discoveringnew species in biodiversity research, as it scales effectively in the zero-shot 1-NN probing task. As we scaled up BarcodeMamba, we observed a slight overfit in models that use the k-mer tokenizerbased on perplexity. Scaling from 4 M to 90 M parameters, models resulted in train perplexities of12. However, the test perplexity remained above 5.07 for all models. While these results demon-strate BarcodeMambas effectiveness in DNA barcode analysis, they also suggest room for further improvements through increased training data and enhanced data augmentation strategies. Therefore,our future work will explore extending the use of BarcodeMamba beyond the Canadian inverte-brate dataset and evaluating its performance on BIOSCAN-5M , a recently-released extensivebiodiversity dataset with 5 million insect specimens.",
  "Conclusions": "We demonstrate that Mamba-2-based models pretrained with next token prediction on DNA barcodedata achieve high performance in arthropod species identification while maintaining computationalefficiency. Through comprehensive experiments comparing architectures, ablating components, andanalyzing scaling behaviour, we explored how pretraining objectives and tokenization methods affectSSM-based foundation models. Our results show that BarcodeMamba achieves strong performance intaxonomic classification of both seen and unseen species, demonstrating its potential for biodiversityscience. Future work will focus on scaling BarcodeMamba to the larger and more taxonomicallydiverse BIOSCAN-5M dataset to further improve species identification performance. We willalso explore architectural modifications, including bi-directional variants, to enhance the modelscapabilities for biodiversity analysis.",
  "and Disclosure of Funding": "Iuliia Zarubiieva, Scott C. Lowe, and Pablo Millan Arias read drafts of the manuscript and providedvaluable feedback. BIOSCAN is supported in part by funding from the Government of Canadas NewFrontiers in Research Fund (NFRF). Resources used in preparing this research were provided, in part,by the Province of Ontario, the Government of Canada through the Canadian Institute for AdvancedResearch (CIFAR), and companies sponsoring the Vector Institute #partners. GWT acknowledges support from the Natural Sciences and Engineering ResearchCouncil (NSERC), the Canada Research Chairs program, and the Canada CIFAR AI Chairs program. Pablo Millan Arias, Niousha Sadjadi, Monireh Safari, ZeMing Gong, Austin T. Wang, Scott C. Lowe,Joakim Bruslund Haurum, Iuliia Zarubiieva, Dirk Steinke, Lila Kari, Angel X. Chang, and Graham W.Taylor. BarcodeBERT: Transformers for biodiversity analysis, 2023. Sarkhan Badirli, Zeynep Akata, George Mohler, Christine Picard, and Mehmet M Dundar. Fine-grainedzero-shot learning with dna as side information. Advances in Neural Information Processing Systems, 34:1935219362, 2021. Randall Balestriero, Mark Ibrahim, Vlad Sobal, Ari S. Morcos, Shashank Shekhar, Tom Goldstein, FlorianBordes, Adrien Bardes, Grgoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson,Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, and MicahGoldblum. A cookbook of self-supervised learning. ArXiv, abs/2304.12210, 2023. Micaela Elisa Consens, Cameron Dufault, Michael Wainberg, Duncan Forster, Mehran Karimzadeh, HaniGoodarzi, Fabian J. Theis, Alan Moses, and Bo Wang. To transformers and beyond: Large languagemodels for the genome. ArXiv, abs/2311.07621, 2023. Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza-Revilla, Nicolas Lopez Carranza, Adam HenrykGrzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Bernardo P de Almeida, Hassan Sirelkha-tim, et al. The nucleotide transformer: Building and evaluating robust foundation models for humangenomics. BioRxiv, pages 202301, 2023.",
  "Albert Gu, Karan Goel, and Christopher R. Efficiently modeling long sequences with structured statespaces. In The International Conference on Learning Representations (ICLR), 2022": "Paul DN Hebert, Alina Cywinska, Shelley L Ball, and Jeremy R DeWaard. Biological identificationsthrough dna barcodes. Proceedings of the Royal Society of London. Series B: Biological Sciences, 270(1512):313321, 2003. Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana V Davuluri. Dnabert: pre-trained bidirectional encoderrepresentations from transformers model for dna-language in genome. Bioinformatics, 37(15):21122120,2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, ScottGray, Alec Radford, Jeff Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv,abs/2001.08361, 2020. DH Lunt, D-X Zhang, Jacek M Szymura, and OM Hewltt. The insect cytochrome oxidase i gene:evolutionary patterns and conserved primers for phylogenetic studies. Insect molecular biology, 5(3):153165, 1996. Frederikke Isa Marin, Felix Teufel, Marc Horlacher, Dennis Madsen, Dennis Pultz, Ole Winther, andWouter Boomsma. BEND: Benchmarking DNA language models on biologically meaningful tasks. In TheTwelfth International Conference on Learning Representations, 2024. URL Florian Mock, Fleming Kretschmer, Anton Kriese, Sebastian Bcker, and Manja Marz. Taxonomicclassification of dna sequences beyond sequence similarity using deep neural networks. Proceedings of theNational Academy of Sciences of the United States of America, 119, 2022. Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Michael Wornow, Callum Birch-Sykes, StefanoMassaroli, Aman Patel, Clayton Rabideau, Yoshua Bengio, et al. Hyenadna: Long-range genomic sequencemodeling at single nucleotide resolution. Advances in neural information processing systems, 36, 2024."
}