{
  "Abstract": "Feature attributions attempt to highlight what inputs drive predictive power. Goodattributions or explanations are thus those that produce inputs that retain this pre-dictive power; accordingly, evaluations of explanations score their quality of pre-diction. However, evaluations produce scores better than what appears possiblefrom the values in the explanation for a class of explanations, called encoding ex-planations. Probing for encoding remains a challenge because there is no generalcharacterization of what gives the extra predictive power. We develop a definitionof encoding that identifies this extra predictive power via conditional dependenceand show that the definition fits existing examples of encoding. This definition im-plies, in contrast to encoding explanations, that non-encoding explanations containall the informative inputs used to produce the explanation, giving them a \"whatyou see is what you get\" property, which makes them transparent and simple touse. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranksthem correctly. After empirically demonstrating the theoretical insights, we useSTRIPE-X to show that despite prompting an LLM to produce non-encoding expla-nations for a sentiment analysis task, the LLM-generated explanations encode.",
  "Introduction": "Artificial intelligence can unlock information in data that was previously unknown. In medicine, forexample, using AI, researchers have shown that electrocardiograms are predictive of structural heartconditions or new-onset diabetes . Good predictions often lead one to ask what in the inputis important for a prediction; this question is a driving factor behind research in interpretabilityand explainability . One primary direction in interpretability seeks to produce explanationsthat are subsets of the input that retain the predictability of the label. These types of explanationsand interpretations are called feature attributions and have been used to find factors associated withdebt defaults , to demonstrate that detecting COVID-19 from chest radiographs can rely on non-physiological signals , and to discover a new class of antibiotics . Several methods exist for producing feature attributions or explanations. While some methods com-pute functions of model gradients or look at predictability after removing features , othermethods attribute scores to different inputs by treating them as players in a game or amor-tize their explanations by learning a single model to select subsets for each instance . Choos-ing one from the many feature attribution methods requires an evaluation. There are, however,many approaches to evaluation itself: qualitative ones , which are limited to caseswhere humans have precise knowledge about the inputs relevant to prediction, and quantitativeones , which do not require human knowledge. Intuitively, a good evaluation method for feature attributions should assign higher scores toexplanations that select inputs that are more predictive of the label. However, evaluations thatscore explanations based on the predictability of the label from the explanation face one majorchallenge: encoding. Informally, an encoding explanation is one where the explanation predictsthe label beyond what seems plausible from the values of the inputs themselves. The top left panelof shows an explanation that predicts the label of dog or cat depending on whether the",
  "arXiv:2411.02664v2 [cs.LG] 18 Dec 2024": ": Overview of the paper. Explanations are produced to find inputs that are relevant to predictinga label. However, explanations can predict the label well due to the selection being predictive of the labelbeyond the explanations values. Such explanations are called encoding. In contrast, predicting instead froma non-encoding explanation is equivalent to predicting from the values in the explanation. When explanationsare evaluated purely based on the quality of prediction, encoding can go undetected. We classify existingevaluations into non-detectors and weak detectors and develop a strong detector, called STRIPE-X. explanation is a pixel on the right half or left half of the image respectively. Many explanationmethods fit the description of encoding . Further, given that many evaluations only lookat the quality of prediction, encoding can go undetected, rendering the evaluations ineffective atpicking explanations. In contrast, non-encoding explanations predict the label well only when thevalues in the explanation do, making them easy to reason about.",
  "In addressing encoding, this work makes the following contributions:": "Develops a simple statistical definition of encoding via a conditional dependence property. Confirms the introduced definition captures all existing ad hoc encoding instances. Shows that non-encoding explanations are easy to use because they retain all the predictiveinputs used to build them, meaning that predictive non-encoding explanations reveal inputs thatpredict the label to their users, and thus have a \"what you see is what you get\" property.",
  "Formalizes evaluations sensitivity to encoding as weak detection (optimal scoring explanationsare non-encoding) and strong detection (non-encoding explanations score above encoding ones)": "Demonstrates that the evaluations ROAR and FRESH do not weakly detect encoding. Proves that EVAL-X weakly detects encoding, but does not strongly detect encoding. Develops STRIPE-X and proves that it strongly detects encoding. Uses STRIPE-X to show that despite prompting an LLM to produce non-encoding explanationsfor a sentiment analysis task, the LLM-generated explanations encode.",
  "Evaluating explanations": "We focus on explanation methods where the goal is to produce subsets of the input that predict thelabel . Explanation methods of this form, also called feature attributions, saliency methods, or just \"explanations,\" include thresholded rankings from Shapley values , LIME, and REAL-X . With y as the label and x Rd as the inputs, let q(y, x) be the jointdistribution over them. An explanation method e maps the inputs x to a binary selection mask e(x)over the inputs: e : Rd {0, 1}d. The explanation xe(x) is a pair: the selection e(x) and the vectorof explanations values. For example, if x = [a, b, c] is three-dimensional and e(x) = ,xe(x) consists of the binary mask e(x) and the values associated with the inputs that correspond tothe indices in e(x) with value 1:xe(x) = (e(x), [b, c]). We keep track of the indices because the same value can lead to different predictions depending onthe index it appears at; for example, in predicting mortality from patient vital signs, a heart rate above110 can occur in healthy patients but a temperature of 110F is almost always fatal. Equivalently,like in existing work , one can choose xe(x) to retain the values in the explanation inthe same position and mask out those not selected: xe(x) = e(x) x + (1 e(x)) mask-token.For concision, we overload the word \"explanation\" to mean the explanation method instead of therandom variable xe(x) when it is clear from context. Choosing between explanation methods requires evaluation. Explanation methods seek to returninputs that predict the label, so existing evaluations consider how well the explanation xe(x) predictsthe label y . To score explanations based on predictive power, an evaluation method() takes as arguments both the explanation e(x) and the joint distribution q(y, x): (q, e). Withoutloss of generality let higher be better.",
  "Encoding: A disconnect between the predictiveness of explanations and thepredictiveness of their values": "We give a simple example of encoding to build intuition for the disconnect between predicting thelabel from the explanation and predicting the label from the explanations values. Imagine that thegoal is to explain which set of vital signs signal bacterial pneumonia as the diagnosis compared tothe common cold. Consider the explanation method that selects the patients height when the trueprobability of pneumonia is high given the whole set of observables (including labs, symptoms,and vital signs) and otherwise selects the patients hair color. Physiologically, height and hair colordo not indicate that the patient has pneumonia, meaning that this explanation should not be highlypredictive of the label. However, by construction, pneumonia is likely exactly when the explanationselects height, and predicting the label from the explanation achieves the same accuracy as predictingwith the full conditional arg maxy{pneumonia, cold} q(y = y | x). Thus, despite the explanationmethod only selecting physiologically irrelevant inputs, the explanation predicts the label well. Encoding examples such as the one above are neither contrived nor unique. For example, Jethaniet al. show that certain procedures that learn to explain, when applied to MNIST digit classifi-cation, yield explanations that select a background, black pixel that predicts the label at an accuracy> 90%; (see in ). Other examples of encoding explanations that predict better thanwhat is expected from the explanations values exist . Encoding explanations should notscore optimally under a good evaluation because the explanation selects inputs that do not appear topredict the label. However, without a general characterization of the discrepancy in predictive powerfor encoding, finding explanations whose values predict well remains a challenge. The next sectiondevelops a definition of encoding.",
  "Formalizing encoding": "Intuitively, encoding is a phenomenon where the information about the label in the explanation xe(x)exceeds what is known from the explanations values. As the input x determines the explanationxe(x), the quality of predicting the label y from the explanation relies on the information about thelabel transmitted from x to xe(x). There are two pathways for this transmission; we elaborate below. Denoting the values in a subset v by xv, compare the event this subset takes the values a, i.e.xv = a to the event that the explanations selection is v and that the explanations values are a, i.e.,xe(x) = (v, a).",
  "e(x)xv": ": Intuition forencoding: There are twoways the information inthe inputs x about the la-bel y is transmitted to theexplanation xe(x):(1)through the values in theexplanation and (2) theselection e(x) (in red).When the latter happens,the explanation is said tobe encoding. To formalize this extra predictive power, define the explanation indicatorEv = 1[e(x) = v]. A little algebra in Appendix A.1 shows the explana-tion indicator Ev provides the extra information:q(y | xe(x) = (v, a)) = q(y | xv = a, Ev = 1 extra information in xe(x)",
  "Ev | xv = a.(2)": "An example mathematical construction of an encoding explanation is pro-vided in Appendix B.1. The dependence in Def: Encoding means thatfor encoding explanations, there is a disconnect between how well the ex-planation xe(x) predicts the label versus only the explanations values xv.This disconnect means that evaluations that score explanations based onpredictions from the explanation or their transformations can favor explanation methods that select inputs whose values have littlerelevance to predicting the label. Beyond the disconnect in prediction, encoding explanations are undesir-able as they conceal predictive inputs that nevertheless affect the explanation. This concealment canlead to incorrect conclusions, such as that inputs outside the selection are irrelevant, or bewildermentbecause predictive inputs outside the explanation drive changes in the selection in ways that cannotbe understood from the explanation itself. An example is provided in Appendix B.2. Non-encoding explanations. Conversely, for a non-encoding explanation, there exists no positivemeasure set of explanations xe(x), where the explanation indicator has conditional dependence giventhe explanations values. That is, for a set A where q(xe(x) A) = 1, then for all (v, a) A",
  "Ev | xv = awhich in turn guaranteesq(y | xe(x) = (v, a)) = q(y | xv = a, Ev = 1) = q(y | xv = a)": "A.2 shows this. A simple example of a non-encoding explanation is a constant explanationthat always picks the same subset of inputs, since a constant Ev is independent of any variable.The information for predicting the label in a non-encoding explanation lives in the explanationsvalues. Evaluations based on predictions from the explanation xe(x) of non-encoding explanationswill yield explanations where the input values xv predict the label. In other words, non-encodingexplanations reveal all the informative inputs they depend on, and \"what you see is what you get\"in the explanation.",
  "Encoding explanations in the wild": "Def: Encoding encompasses examples in the existing literature beyond the example in .1.In that example, the information about y lies in the positions in the selection e(x), which motivatesthe name position-based encoding (POSI). This section describes two other informal examples fromthe literature of encoding explanations, prediction-based encoding (PRED) and marginal encod-ing (MARG) , and explains the intuition behind why they encode. In the appendix, we developformalizations of these types of encoding and show that these formulations meet Def: Encoding. Prediction-based encoding (PRED). To understand how prediction-based encoding occurs, con-sider the task of sentiment analysis from movie reviews. Assume that reviews can either be of type\"My day was terrible, but the movie was [ADJ1].\" and \"The movie was [ADJ2], but the day was notgreat.\" where adjective ADJ1 can be \"good\" or \"not great\" and adjective ADJ2 can be \"not great\" or\"terrible\". Due to common English parlance, \"terrible\" indicates bad sentiment more often than \"notgreat\". Then, in the example setup above, only seeing that the fourth word is \"terrible\" yields badsentiment with higher probability than when only seeing that the phrase is \"not great\". However, thefourth word does not always describe the movie. An explanation can look at \"not great\" describingthe movie as bad but then selects \"terrible\" to encode the bad sentiment. This explanation encodesbecause the selected word may not describe the movie but the selection predicts the sentiment.",
  "e(x)": ": Left: Consider data where the color in theleft half determines whether the label \"cat\", \"dog\") isproduced from the top or bottom image on the right.Right: A MARG encoding explanation that producesonly the top or the bottom animal image based on thecolor. The animal image alone says less about the labelthan knowing the animal image and the color. Knowingthe selection determines the color and thus provides ad-ditional information about the label. Marginal encoding (MARG).This type ofencoding occurs when some inputs determinewhich other inputs determine the label.Forexample, in , the color determineswhether the top right patch produces the labelor the bottom right patch. Inputs that controlwhere the label comes from are named controlflow inputs. For a real-world example, considerthe following example from Jethani et al. ,where the goal is to predict mortality for pa-tients with chest pain. A lab value that checksfor heart injury and acts like a control flow inputis troponin. Abnormal troponin indicates thatcardiac issues exist and cardiac imaging wouldinform mortality. Normal troponin on the otherhand can indicate that chest pain is unrelated tocardiac health and a chest X-ray would instead inform mortality. Selecting one image or the other,but not the control flow input, conceals information about why the image was relevant to the label. Formalization. In Appendix B, we provide mathematical formulations of each informal exampleand show that they fall under the definition of encoding in Def: Encoding: position-based encoding(Appendix B.3), prediction-based encoding (Appendix B.4), and marginal encoding (Appendix B.5).The key intuition behind all of these is that the explanation e(x) varies with inputs other than theselected ones, and these additional inputs provide information about the label beyond the selectedones. Next, we turn to detecting encoding via quantitative evaluations.",
  "Detecting encoding in explanations": "This section develops notions of sensitivity to encoding for evaluation methods, and uses the math-ematical definition of encoding developed in the previous section to establish which methods detectencoding and which do not. Hsia et al. suggest that evaluation methods like EVAL-X can begamed to produce high scores for encoding explanations by optimizing the evaluation. To study thiscase, we introduce the notion of weak detection. If the optimal score of an evaluation of explanationsdoes not permit encoding, then that evaluation is said to weakly detect encoding:",
  "Definition 2 (Weak detection of encoding). An evaluation (q, e) of explanations weakly detectsencoding if the optimal explanations e, i.e. (q, e) = maxe (q, e), are non-encoding": "Weak detection provides a recipe for finding non-encoding explanations: find the explanation thatachieves the maximum score of a weak detector. However, such a recipe would only work when op-timizing without constraints because weak detection does not require non-encoding explanations tohave a better score than any encoding one. Requiring this leads to the definition of strong detection.",
  "x = [x1, x2, x3] B(0.5)3,y =x1w.p. 0.9else1 x1if x3 = 1,x2w.p. 0.9else1 x2if x3 = 0.(3)": "Consider the explanation eencode(x) = 1 = if x3 = 1 and 2 = otherwise; thisencodes because x3 is used to create the explanation and x3 predicts the label conditional on x1when E1 = 1. This is a MARG explanation (see .1). ROAR and FRESH do not weakly detect encoding.ROAR evaluates explanations by predict-ing the label from the inputs not selected by the explanation, denoted as xe(x); ROAR scoresexplanations optimally if the predictions from the remaining covariates are as random as pre-dicting without any covariates at all. In other words, ROAR checks how informative xe(x) isof y and provides the highest score when y",
  "|=": "x | val(xe(x)), FRESH and EVAL-X log-likelihoods should be the same as predicting from thefull feature set. One potential reason there is a gap between the two scores in table 3 is that theFRESH and EVAL-X scores are computed with ResNet18 models that solve prediction problems ofdifferent levels of difficulty On one hand, FRESH is computed with a model trained for a singleprediction task: predict y from val(xe(x)). On the other hand, EVAL-X is computed with a modeltrained for a more complicated task: for a range possible v, predict y from xv. Using large modelswith appropriate regularization, such as weight decay, should mitigate the gap in scores.",
  "EVAL-X(q, e) := E(v,a)q(xe(x))Eq(y | xe(x)=(v,a)) [log q (y | xv = a)] .(4)": "This score measures the expected log-likelihood of the labels given the input values chosen by theexplanation method e and is grounded in the sampling distribution q. Log-likelihoods are maximizedby matching the true distribution, this leads to EVAL-Xs weak detection:Theorem 1. If e(x) is EVAL-X optimal, then e(x) is not encoding. A.4 gives a proof. The proof shows that at optimality, the prediction from the values ofexplanation has to match the prediction from the full inputs. In turn, given the values there is noadditional information in x about y, which means the explanation indicator Ev is independent of y;this violates Def: Encoding, which proves the non-encoding nature of EVAL-X-optimal explanations. To test strong detection for EVAL-X, we consider explanations constrained to select one input. Suchreductive constraints appear in practice because the goal of producing an explanation is often to aidhumans who benefit from reduced complexity. Such constraints prohibit explanations from reachingEVAL-Xs optimal score. Compare eencode(x) with a non-encoding constant explanation:Proposition 2. Let ec(x) = 3. Then, for the DGP in eq. (3), EVAL-X(q, eencode) > EVAL-X(q, ec). Thus, EVAL-X is not a strong detector. The intuition is that the first two coordinates x1, x2 predictthe label when selected by eencode, while the control flow feature does not predict the label. EVAL-Xnot being a strong detector means that optimizing EVAL-X over a reductive set may yield an encodingexplanation. In this case, eencode is one of the EVAL-X-optimal reductive explanations (Lemma 6).",
  "STRIPE-X: a strong detector of encoding": "Encoding explanations induce the dependence between the label y and the identity of the selectionEv = 1[e(x) = v] given the values in the explanation xv (Def: Encoding). This dependence canbe tested for by building on conditional independence tests . Rather than testing, directquantification of dependence can be useful for when combining with other scores, which can bedone using instantaneous conditional mutual information:q(e) := E(v,a)q(xe(x))I (Ev; y | xv = a)(ENCODE-METER).(5)",
  "STRIPE-X(q, e) := EVAL-X(q, e) q(e).(6)": "For a large enough , the added penalty term pushes down thescores of encoding explanations below that of all non-encodingones, meaning that STRIPE-X is a strong detector of encoding:Theorem 2. With finite H(y | x) and H(y), for any explanationthat encodes e and any that does not encode e, there exists an such that > STRIPE-X(q, e) > STRIPE-X(q, e). The proof is in Appendix A.5. The intuition behind the proof isthat for a large enough , the STRIPE-X scores for any encoding explanations will be dominated bythe information term, and thus will become smaller than any non-encoding explanation whose scoreis lower bounded by the negative marginal entropy, Hq(y). summarizes the weak andstrong detection properties of different evaluations. Estimating STRIPE-X. The first component of STRIPE-X is EVAL-X. Computing EVAL-X (eq. (4))requires an estimate of the predictive distribution of the label y given xv, q(y | xv) . Estimationcan be done in two ways. The first way makes use of a surrogate model trained to predict the labelfrom different random subsets using masked tokens . The second way to compute EVAL-X(eq. (4)) relies on conditional generative models . Both hyperparameters and a combinationof the estimators can be chosen to maximize the average log-likelihood on a held-out validation setacross random input subsets.",
  "q(e) = E(v,a)q(xe(x))Eyq(y | xv=a)KL [q(Ev | xv = a, y) q(Ev | xv = a)] .(7)": "The outer expectation can be estimated using samples from the data and the inner expectation overy can be estimated using the EVAL-X model q(y | xv). The distributions over Ev can be estimatedusing a classifier of Ev that randomly masks the label and masks different subsets of the inputs.Further details and a generative way to estimate STRIPE-X are in Appendix C.1 and Appendix C.3;full algorithms are given in Appendix D. STRIPE-X in practice. Using STRIPE-X to choose between explanations is straightforward: pick theone with the larger score. However, like other evaluations that use learned models, misestimationcan pose a problem. With large , non-encoding explanations with misestimated ENCODE-METERwill have bad STRIPE-X scores, while with small some encoding explanations can have goodscores. Across all experiments, we set = 20, which yielded STRIPE-X scores for known encodingexplanations worse than known non-encoding explanations.",
  "Experiments": "This section consists of two parts. The first part demonstrates the weak and strong detection ca-pabilities of the evaluations ROAR, EVAL-X, and STRIPE-X in a simulated setting and on an imagerecognition task. To demonstrate these capabilities, we run these evaluations on instantiations ofPOSI, PRED, and MARG. Additionally, we evaluate an existing method that learns to explain undera reductive constraint, called REAL-X . The second part shows how STRIPE-X enables discov-ering encoding explanations in the wild, without specific knowledge of the DGP or the method thatproduced the explanation. We employ STRIPE-X to uncover encoding in explanations generated bya large language model (LLM) for predicting sentiments from movie reviews.",
  ": Here, (x) = q(y = 1 | x). Different encoding explanation methods that we consider": "variable and switch the inputs that y depends on. In both DGPs, y only depends on x1 if x3 = 1,and only on x2 if x3 = 0; this means that x4, x5 are purely noise. For both DGPs, y is sampled perthe following distribution where x3 determines the subset the y depends on",
  "Thus, EVAL-X is achieved by an explanation of size 2: e(x) = 1+3 if x3 = 1 else e(x) = 2+3.See Appendix C.4 for details; the exact DGPs are given in eq. (36) and eq. (37)": "Encoding explanations. describes the encoding explanations we consider for this setting.In Appendix C.4, we check that Def: Encoding holds for these explanations in the discrete DGPby estimating the role of the unselected inputs in affecting the explanation and the role of Ev inpredicting y beyond xv; a characterization of Def: Encoding to support this check is in Lemma 1.",
  "(b) Results: hybrid DGP": ": The EVAL-X-ACC and AUROC scores for the different explanations for the discrete DGP are onthe left and the scores for the hybrid DGP are in the right. In both, multiple encoding explanations (PRED,MARG, and the reductive one from REAL-X) all achieve the same score as the corresponding EVAL-X score.Thus, ranking metrics like accuracy and AUROC are not sensitive to encoding explanations like EVAL-X log-likelihoods, and can fail to even weakly detect encoding. This stems from the fact that accuracy and AUROConly depend on the ranking of the datapoint, and therefore are not sensitive to differences in log probabilitiesthat do not change ranks. REAL-X.We solve REAL-X for any specified explanation size K as follows. In the case of thediscrete DGP, for each possible value of x supp(q(x)) (of which there are finitely many), wemake e(x) output the subset of at most size K that achieves that maximum averaged log-likelihoodover the samples that equal said value x = x. This produces the optimally-scoring explanatione(x) that maps each finite value in the support of q(x) to one subset of the coordinates of x. To dothe same in the continuous DGP in eq. (37), we round x to integers and then use the same type ofoptimization as in the discrete case. STRIPE-X.In estimating ENCODE-METER, the model p(Ev | xv, ) is a decision tree of depthat most 5, which is then used to estimate averaged KL in the RHS of eq. (7) with a single samplefrom y | xv. The process is repeated for each v and averaged to produce the ENCODE-METER. Thesimulated experiments were done on a CPU with the whole runtime around 10 minutes.",
  "Detecting encoding on images of dogs and cats": "The goal of this section is to study the encoding detection capabilities of ROAR, EVAL-X, andSTRIPE-X on real data. We consider an image recognition task like the one in with la-bels and images from the cats_vs_dogs dataset from the Tensorflow package . We breakimages of size 64 64 into 4 patches each of size 32 32. In left-right then top-down order,let x1, x2, x3, x4 be the upper left, upper right, bottom left, and bottom right patches respectively;x1, x3 capture color, and x2, x4 are the animal images. With annot(image) denoting the annota-",
  "POSI0.510.690.705.98PRED0.690.230.511.40MARG0.690.230.531.02": ":ROAR, FRESH, EVAL-X, and STRIPE-Xscores for the image recognition experiment. Higheris better. ROAR and FRESH score two encoding ex-planations PRED and MARG as high as the optimalexplanation, meaning they are not even weak de-tectors. EVAL-X being only a weak detector scoresPOSI, PRED, and MARG all worse than the optimalexplanation under both EVAL-X but not the non-encoding constant explanation (e(x) = 4), denotedfixed.STRIPE-X being a strong detector scoresthe non-encoding explanations above the negativemarginal entropy Hq(y) = 0.69 and scores ev-ery encoding construction under that threshold. We consider three encoding explanations (POSI,PRED, MARG) and two non-encoding ones: 1)optimal, which selects the color and the patchthat produces the label as dictated by the color, and2) denoted fixed, which always outputs the bot-tom right patch x4. Appendix C.6 gives details. We report the scores assigned to each explana-tion by ROAR, EVAL-X, and STRIPE-X in .ROAR scores two encoding explanations PRED andMARG as high as the optimal explanation, mean-ing it is not even a weak detector. POSI, PRED, andMARG all score worse than the optimal expla-nation under both the weak detector EVAL-X andthe strong detector STRIPE-X. However, EVAL-X scores one non-encoding explanation (fixed)worse than two encoding ones, meaning it is not astrong detector. Being a strong detector, STRIPE-Xscores the fixed explanation above the negativemarginal entropy Hq(y) = 0.69 and scoresevery encoding construction under that threshold. Evaluating explanations produced by REAL-X . We ran REAL-X to learn explanations for thesimulated setting and the image recognition task. In the simulated setting, REAL-X is run to selectone input; Appendix C.4 gives details. In the image recognition task, REAL-X is run to select oneof the four patches as an explanation; Appendix C.6 gives details. In both the simulated setting(see Figures 4a and 4b) and the image recognition task (see ), REAL-X fails to achieve theoptimal EVAL-X score while achieving a STRIPE-X score below the threshold of negative marginalentropy Hq(y) = 0.69. Upon investigation, we found that REAL-X produced an explanation thatmatched the MARG construction on at least 80% of the inputs in the simulated setting. On the imagerecognition task, REAL-X explanation matched the MARG explanation on the whole dataset. In bothcases, STRIPE-X, being a strong detector, correctly alerts that the REAL-X explanation encodes.",
  "Encoding in LLM-generated explanations": "One can detect encoding in any explanation by checking if the STRIPE-X score falls below thenegative marginal entropy. Recent work uses LLMs to produce explanations; e.g. prompt anLLM to generate explanations for reasoning tasks which are later used to improve smaller models. Ifthe LLM explanation encodes, the smaller model can falsely ignore the informative inputs the largermodels explanation depends on and yet does not reveal. In this section, we evaluate explanationsgenerated by an LLM, Llama 3, for a sentiment analysis task. We consider reviews that take one oftwo forms: with ADJ1 and ADJ2 as adjectives, the review is",
  "My day was <ADJ1> and the movie was <ADJ2>.oh wait, reverse the adjectives": "The second sentence in the review acts as a \"control flow\" input and determines whether ADJ1 orADJ2 describes the sentiment about the movie. We prompted Llama 3 to predict the sentiment andselect words relevant to predicting the sentiment. In Appendix C.8, we give the prompts we usedto make Llama 3 produce explanations from. For this problem, the inputs x are the reviews andLlama 3 produces explanations e(x) that select a subset of words in the review. The summariesand explanations were generated for all 10, 000 samples but to estimate STRIPE-X, we only useddata from the 5 most common explanations (we restricted to inputs whose explanations v had highq(e(x) = v)). This resulted in a dataset of size 8136, which we split into a training, validation, andtest datasets of sizes 6102, 1017, and 1017 respectively. Both the EVAL-X model and the model for p(F | xv, v, ) (see Appendix C.2) used in estimatingthe ENCODE-METER term in STRIPE-X were finetuned GPT-2 models. For the EVAL-X model, weused the AdamW optimizer with a batch size of 100 and trained for 50 epochs with the learning rateset to 5e 5, weight decay set to 0, and a Cosine learning rate scheduler with the number of cyclesset to 1. For the p model used in estimating ENCODE-METER, we used the AdamW optimizer witha batch size of 50 and trained for 25 epochs with the learning rate set to 5e 5, weight decay set to0, and a Cosine learning rate scheduler with number of cycles set to 1. The p model sees variable through the following word added to the input sequence of words: positive if = y = 1,negative if = y = 0, and nothing if = null. We used validation loss to early stop. Wefollow the procedure in Appendix C.2 to compute ENCODE-METER with p(F | xv, v, ) on the testdata with the averaging over y | xv estimated using a 5 samples per value of xv. All training andinference for this experiment was done on an A100. The explanation step and the estimation forboth parts of STRIPE-X together took under 2 hours. The LLM-generated explanations achieves anEVAL-X score of 0.497 and an ENCODE-METER value was 0.114.",
  "Discussion": "When an explanation is encodes, predictions from the explanation become disconnected from pre-dictions from the values in the explanations. Such explanations can select values with little relevanceto the label and yet score highly on the many existing predictive evaluations. We develop a simplestatistical definition of encoding. Inverting this definition shows that when non-encoding explana-tions predict the label, users know the values of those inputs selected in the explanation predict thelabel. We then show that existing evaluations are either non-detectors (ROAR,FRESH ) oronly weak detectors (EVAL-X ). Motivated by this, we introduce a new strong detector, STRIPE-X. After empirically demonstrating the detection capabilities (or lack thereof) of said evaluations,we use STRIPE-X to discover encoding in LLM-generated explanations. More related work. Other investigations into evaluating explanations focused on label leakage and faithfulness . Label leakage is similar to encoding in that additionalinformation is in the explanation, but focuses on explanations that have access to both the inputsand the observed label; we leave extending Def: Encoding to leakage to the future. Faithfulness,intuitively, asks that the explanation reflect the process of how a label is predicted from the inputs; aformalization does not exist. Jacovi and Goldberg note the need to define faithfulness formally.Encoding explanations are not faithful to the process of making an explanation because predictiveinputs outside those selected by the explanation control the explanation. Limitations and the future. Using misestimated models in evaluations (like EVAL-X) may lead tomistakes (see Appendix B.8 for an example). The retinal fundus experiment from Jethani et al. is an example where misestimation leads to reductive explanations scoring higher than using thefull input. Misestimation can be due to poor uncertainty or due to dependence on shortcut features.One fruitful direction is to use better uncertainty estimates, like conformal inference or calibra-tion , or employ robustness methods to ameliorate errors due to misestimation. Anotherdirection is use tricks like REINFORCE-style gradients to construct non-encoding explanations byoptimizing STRIPE-X. Explanations that output subsets may not always help humans interpret themechanism of the prediction. For example, imagine one wants to understand why a model correctlyanswers the question \"Who won the ski halfpipe at the X-games 3 years after her debut in 2021?\"with \"Eileen Gu\". A subset explanation may return \"3 years after her debut in 2021\" and \"ski half-pipe\", but that does not help a human interpret how the model predicts. A better interpretation wouldbe to make the model output, \"3 years after 2021 is 2024. Eileen Gu won in 2024, and debuted in2021.\" Such explanations can also encode information about the prediction in the text produced asa rationale . An important direction here would be to extend the definitions of weak and strongdetectors of encoding to evaluations of free-text rationales. Data versus Model Explanations. Even with the formal definitions of explanation methods, thereis a question about what is being explained: the data or the model. These two concepts often getblended together in the literature . We clarify this point and abstract the choice away as twodifferent ways to produce the joint distribution q(y, x). In data explanation, the distribution underwhich a feature attribution method seeks to output a subset of inputs that predict the label shouldbe the population distribution of the data . If, instead, the goal is model explanation, the goalshould not be to highlight inputs that predict the label well in samples of the data; rather it shouldbe to predict the label well in samples from the model. Formally, a model with parameters isa conditional distribution, p(y | x). To target a model explanation, a feature attribution methodwould aim to output a subset of inputs that predict the label under the distribution F(x)p(y | x).",
  "Acknowledgements": "This work was partly supported by the NIH/NHLBI Award R01HL148248, NSF Award 1922658NRT-HDR:FUTURE Foundations, Translation, and Responsibility for Data Science, NSF CAREERAward 2145542, NSF Award 2404476, ONR N00014-23-1-2634, Google DeepMind, and Apple.The authors would like to thank Yoav Wald, the NeurIPS 2024 reviewers and the NeurIPS 2024 areachair for helpful feedback. Pierre Elias, Timothy J Poterucha, Vijay Rajaram, Luca Matos Moller, Victor Rodriguez,Shreyas Bhave, Rebecca T Hahn, Geoffrey Tison, Sean A Abreau, Joshua Barrios, et al. Deeplearning electrocardiographic analysis for detection of left-sided valvular heart disease. Jour-nal of the American College of Cardiology, 80(6):613626, 2022. Neil Jethani,Aahlad Puli,Hao Zhang,Leonid Garber,Lior Jankelson,YindalonAphinyanaphongs, and Rajesh Ranganath.New-onset diabetes assessment using artificialintelligence-enhanced electrocardiography. arXiv preprint arXiv:2205.02900, 2022.",
  "Alex J DeGrave, Joseph D Janizek, and Su-In Lee. Ai for radiographic covid-19 detectionselects shortcuts over signal. Nature Machine Intelligence, 3(7):610619, 2021": "Felix Wong, Erica J Zheng, Jacqueline A Valeri, Nina M Donghia, Melis N Anahtar, SatotakaOmori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, et al. Discovery of astructural class of antibiotics with explainable deep learning. Nature, pages 19, 2023. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, DeviParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision,pages 618626, 2017.",
  "Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variableselection using neural networks. In International Conference on Learning Representations,2018": "Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Samuel J Gershman, andFinale Doshi-Velez. Human evaluation of models built for interpretability. In Proceedings ofthe AAAI Conference on Human Computation and Crowdsourcing, volume 7, pages 5967,2019. Adriel Saporta, Xiaotong Gui, Ashwin Agrawal, Anuj Pareek, Steven QH Truong, Chanh DTNguyen, Van-Doan Ngo, Jayne Seekins, Francis G Blankenberg, Andrew Y Ng, et al. Bench-marking saliency methods for chest x-ray interpretation. Nature Machine Intelligence, 4(10):867878, 2022. Jonathan Crabb, Alicia Curth, Ioana Bica, and Mihaela van der Schaar. Benchmarking het-erogeneous treatment effect models through the lens of interpretability. Advances in NeuralInformation Processing Systems, 35:1229512309, 2022. Wojciech Samek, Alexander Binder, Grgoire Montavon, Sebastian Lapuschkin, and Klaus-Robert Mller. Evaluating the visualization of what a deep neural network has learned. IEEEtransactions on neural networks and learning systems, 28(11):26602673, 2016.",
  "Isabelle Guyon and Andr Elisseeff. An introduction to variable and feature selection. Journalof machine learning research, 3(Mar):11571182, 2003": "Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: Aninformation-theoretic perspective on model interpretation. In International conference on ma-chine learning, pages 883892. PMLR, 2018. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explainingthe predictions of any classifier.In Proceedings of the 22nd ACM SIGKDD internationalconference on knowledge discovery and data mining, pages 11351144, 2016.",
  "Erik trumbelj and Igor Kononenko. Explaining prediction models and individual predictionswith feature contributions. Knowledge and information systems, 41:647665, 2014": "Neil Jethani, Adriel Saporta, and Rajesh Ranganath. Dont be fooled: label leakage in explana-tion methods and the importance of their quantitative evaluation. In International Conferenceon Artificial Intelligence and Statistics, pages 89258953. PMLR, 2023. Kun Zhang, Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Kernel-based condi-tional independence test and application in causal discovery. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, pages 804813, 2011.",
  "Mukund Sudarshan, Wesley Tansey, and Rajesh Ranganath. Deep direct likelihood knockoffs.Advances in neural information processing systems, 33:50365046, 2020": "Mukund Sudarshan, Aahlad Puli, Wesley Tansey, and Rajesh Ranganath. Diet: Conditionalindependence testing with marginal dependence measures of residual information. In Interna-tional Conference on Artificial Intelligence and Statistics, pages 1034310367. PMLR, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language mod-els are few-shot learners. Advances in neural information processing systems, 33:18771901,2020.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.Hier-archical text-conditional image generation with clip latents, 2022.URL 7, 2022": "Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Good-fellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, LukaszKaiser, Manjunath Kudlur, Josh Levenberg, Dan Man, Rajat Monga, Sherry Moore, DerekMurray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, KunalTalwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Vigas, Oriol Vinyals,Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow:Large-scale machine learning on heterogeneous systems, 2015. URL Software available from tensorflow.org. Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang,Jing Qian, Baolin Peng, Yi Mao, et al. Explanations from large language models make smallreasoners better. arXiv preprint arXiv:2210.06726, 2022. Peter Hase, Shiyue Zhang, Harry Xie, and Mohit Bansal. Leakage-adjusted simulatability:Can models generate non-trivial explanations of their behavior in natural language?arXivpreprint arXiv:2010.04119, 2020. Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova.\" will you find these shortcuts?\" a protocol for evaluating the faithfulness of input saliencemethods for text classification. arXiv preprint arXiv:2111.07367, 2021. Yilun Zhou, Serena Booth, Marco Tulio Ribeiro, and Julie Shah. Do feature attribution meth-ods correctly attribute features? In Proceedings of the AAAI Conference on Artificial Intelli-gence, volume 36, pages 96239633, 2022.",
  "Aahlad Manas Puli, Lily H Zhang, Eric Karl Oermann, and Rajesh Ranganath.Out-of-distribution generalization in the presence of nuisance-induced spurious correlations. ICLR2022, 2021": "Aahlad Manas Puli, Lily Zhang, Yoav Wald, and Rajesh Ranganath. Dont blame datasetshift! shortcut learning due to gradients and cross entropy. Advances in Neural InformationProcessing Systems, 36, 2023. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob GrueSimonsen, and Isabelle Augenstein. Faithfulness tests for natural language explanations. arXivpreprint arXiv:2305.18029, 2023.",
  "Then, intuitively, conditioning on the event that xe(x) = (v, a) gives you the same information asconditioning on the events e(x) = v and xv = a simultaneously. We make this formal below": "For discrete x, for any v, a such that the probability q(xe(x) = (v, a)) > 0, define the LHS andRHS of eq. (1) as Bv, Cv respectively. Then, the conditionals q(y | xe(x) = (v, a)) and q(y | Ev =1, xv = a) exist and can be written as follows:",
  "These two conditionals are equal because Bv = Cv": "The same kind of result holds for general random vectors (discrete or continuous) x but is a littlemore involved because Bv may be non-empty while q(x Bv) = 0 and the equality of conditionaldensities/probabilities need to be written via measure theory. Assume the regular conditional proba-bilities q(y | xe(x)) and q(y, Ev | xv), q(y | Ev, xv) and q(Ev | xv) are defined almost everywherein their respective probability measures. Take any Sv {xv : e(x) = v} where q(xv Sv) > 0and q(e(x) = v) > 0. Consider any measurable sets Y over y and Bv(Sv) := {(v, a) : a Sv}over xe(x). Now, by definition of regular conditional probability measures, joint probabilities areobtained by taking the expectation of the conditional with respect to marginal distributions over theconditioning set:",
  "Svq(y Y | Ev = 1, xv = a)q(Ev = 1, xv = a)da.(10)": "Due to eq. (1), the LHS terms of the two equations above are equal and so are the probabilitymeasures over the integrating variables in eqs. (9) and (10). Letting xv be defined on a Borel sigmaalgebra, these two integrals eqs. (9) and (10) are equal if and only if for any Borel set Sv, for almostevery a Sv",
  "A.3.1Alternate conditions equivalent to Def: Encoding": "The dependence in Def: Encoding occurs due to two reasons, understanding which sheds morelight on the definition. First, for some selection e(x) = v, the explanations values xv do notprovide enough information to reveal that the explanation should select the inputs denoted by v.In other words, the indicator of the selection is variable even after fixing the explanations valuesthemselves. Second, this indicator is predictive of the label for the data with the explanation v.These two properties provide intuition on the definition of encoding: Lemma 1. Def: Encoding holds for an explanation e(x) if and only if there exists a selection vsuch that q(e(x) = v) > 0 and a set Sv {xv : e(x) = v} such that q(xv Sv) > 0 where bothof the following conditions hold for almost every a Sv:",
  "Ev | xv = a.(13)": "Proof. Def: Encoding says that the explanation e(x) is encoding if there exists an S where q(xe(x) S) > 0 such that for every (v, a) S eq. (13) holds. This proof works by showing that S having apositive measure implies the existence of v and Sv as in Lemma 2 such that eq. (13) holds.",
  "= Evq(e(x))Eaq(xv | e(x)=v)Eq(x | xv=a,e(x)=v))Eq(y | x) log q(y | xv = a)": "= Evq(e(x))Eq(x | e(x)=v))Eq(y | x) log q(y | xv),where in the last step, we dropped a because it equals xv almost surely. Similarly, the optimal scoreEVAL-X expands toEvq(e(x))Eq(x | e(x)=v))Eq(y | x) log q(y | x).Let V be the set of values that explanations can take on, then taking the difference from optimality",
  "Ev | xv": "Then if e(x) achieves EVAL-X, then by Lemma 4, for all v such that q(e(x) = v) > 0,q(y | x) = q(y | xv)for almost every{x : e(x) = v}.First, this optimality criteria can incorporate Ev = 1 on the lefthand side by first conditioning one(x) and then noting that the equality holds for x where e(x) = v.q(y | x) = q(y | xv)for almost every{x : e(x) = v} q(y | x, e(x)) = q(y | xv)for almost every{x : e(x) = v} q(y | x, e(x) = v) = q(y | xv)for almost every{x : e(x) = v} q(y | x, Ev = 1) = q(y | xv)for almost every{x : e(x) = v}. To understand if the optimality criterion disallows encoding, integrate the left and right-hand sidesof this optimality criterion with the respect to complement of the inputs in xv, q(xcv | xv, Ev = 1)yieldsq(y | x, Ev = 1)q(xcv | xv, Ev = 1)dxcv =q(y | xv)q(xcv | xv, Ev = 1)dxcv",
  "Ev | xv,which violates Def: Encoding. So there is no encoding in this case": "Case 2: When the explanation is unpredictable, EVAL-X optimality requires that the explana-tion provide no extra information.Now consider the alternative case, q(Ev = 1 | xv) < 1. Herethe explanation does not determine the explanation opening the possibility that the EVAL-X-optimalexplanation method can encode information in the explanation. Because q(e(x) = v) > 0, we have q(xv {xv : e(x) = v}) > 0. Thus, by Lemma 3, for almostevery {xv : e(x) = v} it holds that q(Ev = 1 | xv) > 0. Putting this result together with alternativecase (q(Ev = 1 | xv) < 1) gives: 0 < q(Ev = 1 | xv) < 1 for almost every {xv : e(x) = v}.",
  "Now, expanding out the optimality criterion eq. (14):": "q(y | xv, Ev = 1) = q(y, Ev = 1 | xv) 1 + q(y, Ev = 0 | xv) 1for almost every {xv : e(x) = v} q(y | xv, Ev = 1) = q(y | Ev = 1, xv)q(Ev = 1 | xv)+ q(y | Ev = 0, xv)(1 q(Ev = 1 | xv))for almost every {xv : e(x) = v} (1 q(Ev = 1 | xv))q(y | xv, Ev = 1) = (1 q(Ev = 1 | xv))q(y | xv, Ev = 0)for almost every {xv : e(x) = v} q(y | xv, Ev = 1) = q(y | xv, Ev = 0)for almost every {xv : e(x) = v}.",
  "x=[y + 1,3, 0, 2]if z = 0,[3, y + 1, 1, 2]if z = 1": "For this problem, if the third coordinate x3 = 0, all the information between the label and thecovariates is in the first coordinate x1, and if x3 = 1, the information is between the label and thesecond coordinate x2. The corresponding explanation function is e(x) = 1[x3 = 0]1 + 1[x3 =1]2. This explanation is encoding because neither explanations values x1 nor x2 determine theexplanation function because it depends on x3. Formally",
  "x = [x1, x2, x3] B(0.5)3,y =x1if x3 = 1,x2if x3 = 0.(15)": "Let e be an encoding explanation that selects the first coordinate if x3 = 1 and the second coordinateotherwise. We never observe x3 when looking only at the explanation. shows all possiblevalues of this explanation. Notice that in the third and fourth rows, the value of xe(x) changes tomatch the label y exactly, even though the values of the first two coordinates that we can observestay constant. It is impossible to understand the perfect predictiveness of xe(x), as the encodingexplanation conceals the control flow feature x3 that determines which of the first two featuresshould be picked to predict the label.",
  "(c) Marginal encoding": ": Example DGP and MARG encoding. (a) The color determines whether the label is pro-duced from the top or bottom image. (b) An explanation that correctly reveals that the label isgenerated based on both the color and, as dictated by the color, the top or the bottom image. Thelabel is deterministic given the value of the explanation which means the label can be predicted per-fectly. (c) An encoding explanation would be one that produces only the top or the bottom animalimage based on the color being red of blue respectively. This returned animal image does not indi-cate the fact that the data generating process depends on color. Now, the animal image selected bythe explanation alone is insufficient to dictate the label because the color determines which imagedetermines the label. The identity of the image, whether top or bottom, provides additional infor-mation about the label beyond the values explanation, as captured in Def: Encoding.",
  "y": "ROAR scores an explanation highly if xe(x) predicts the label poorly. So if xe(x) is independent ofy, then e(x) would be scored optimally. Then, due to eq. (26), ROAR scores an encoding explanationoptimally. FRESH scores an explanation highly if the selected value val(xe(x)) (as defined in Definition 4)predicts the label well.From , we see that p(y = 1 | x) = 0.9 for all cases withval(xe(x)) = [1, pad-token, pad-token] and p(y = 1 | x) = 0.1 for all cases with val(xe(x)) =[0, pad-token, pad-token]. Thus, if val(xe(x)) = [1, pad-token, pad-token] then",
  "Now we can maximize the sum of eq. (27) and eq. (28), over e(x) such that |e(x)| = 1": "Notice that setting 1[e(x) = 1] = 1 when x3 = 1 and 1[e(x) = 2] = 1 when x3 = 0 achievesthe highest score 0.44 in each of eq. (27) and eq. (28). This implies that one optimal reductiveexplanation is 1 = if x3 = 1 and 2 = otherwise. This is an encoding explanationas we show below. Due to E1 = x3,",
  "= 0": "Since y is deterministic given x so the maximum value of the EVAL-X score is also 0. So thebad explanation scores optimally due to misestimation. Deterministic y | x is not necessary forestimation error to affect explanation quality. Here, with this incorrectly estimated EVAL-X, inputsthat are pure noise, independent of everything, will be chosen.",
  "p(Ev | xv, = y) = q(Ev | xv, y = y)p(Ev | xv, = null) = q(Ev | xv)": "In summary, to estimate ENCODE-METER, solve eq. (34), use its solution to estimate the KL termfrom the RHS in eq. (7) for each xv, y, and then average this KL term over samples of xv from thedata such that e(x) = v and samples of y from the EVAL-X model for q(y | xv). In practice, one does not need train a model for each v. We describe how to estimate ENCODE-METER with a single model in Appendix C.2. We give the full STRIPE-X estimation procedurein Algorithm 2 in Appendix D.",
  "C.2Estimating the encoding cost in STRIPE-X with categorical predictive models": "STRIPE-X consists of the EVAL-X score and a cost of encoding measured by ENCODE-METER. De-fine V to be the set of possible explanations and let V[j] denote the jth element of V. The EVAL-Xmodel p(y | xv) is trained to predict the label y from subsets xv where v is uniformly sam-pled from V . Next is computing the ENCODE-METER q(e) that is used in the encoding costterm in STRIPE-X. For each explanation, let F be the categorical variable (instead of an indicatorEv) that denotes, for each sample, which inputs were selected by the explanation e(x): F = j ifEV[j] = 1[e(x) = V[j]] = 1. Let q(j) be the distribution over j induced by q(e(x)). We train amodel p(F | xv, , v) with a modification of eq. (34) that averages over v q(e(x)):",
  "POSI0.610.88PRED0.510.18MARG0.510.20": "This motivates a second procedure to estimate ENCODE-METER that avoids having to retrain modelsfor each explanation by using generative model for q(x | xv, y). Formally, with xv fixed, theconditional mutual information term in eq. (5) can be computed as the marginal dependence betweenN samples of y from q(y | xv) and q(Ev | xv, y). The model for the former is available fromEVAL-X estimation. Simulating from the later, namely q(Ev | xv, y), is done by sampling from thegenerative model x | xv, y and then computing the indicator Ev as 1[e(x) = v]. Mechanically, withan estimator of mutual information from samples ({ai}iN, {bi}iN) denoted MI({ai}, {bi}) andwith samples {ai} produced conditionally on values ci denoted by a subscript of the conditionedvalue {ai}ci, one can estimate ENCODE-METER as follows: sample yi(v,a) y | xv = a andxiv,a,yi q(x | xv = a, y = yi) repeatedly N times and compute",
  "x = [x1, x2, x4, x5] N(0.5)4, x3 B(0.5), =( x1) if x3 = 1,( x2) if x3 = 0,y B(). (37)": "Computing accuracy and KL to show that POSI, PRED, MARG are encoding.For each encod-ing type, we build two decision trees from 1000 samples from eq. (36): the first decision tree learnsq(Ev | xv) and the second learns q(y | xv, Ev = b) for b {0, 1}. We set the maximum depth tobe 6. Trees of this depth learn any function of 6 binary digits; x with Ev as an additional columnamounts to 6 binary digits. These decision trees are used to compute the accuracy of predicting Evwith q(Ev | xv) and the KL between q(y | xv, Ev = 1) and q(y | xv, Ev = 0). Within a set{x : e(x) = v} that is all x that have one of the possible selections v, report the accuracyof predicting Ev with q(Ev | xv) and the KL between q(y | xv, Ev = 1) and q(y | xv, Ev = 0),averaged only over samples in {x : e(x) = v}. EVAL-X.To estimate EVAL-X for the DGPs in eq. (36) and eq. (37), we compute conditionalsq(y = 1 | xv) via Monte Carlo approximation. Due to the different coordinates of x being inde-pendent, one can compute q(y = 1 | xv) as a marginal expectation over the inputs except those inv:q(y | xv) = Eq(xcv | xv)q(y | xv, xcv) = Eq(xcv)q(y | x).We Monte Carlo estimate the RHS of this equation over 500 resamples of xcv. We take 5000 samplesfrom each DGP to estimate EVAL-X scores with respect to q(y, x). In Appendix C.5 we also showexperiment results where we use the EVAL-X accuracy and AUROC as the score instead.",
  "C.5Experiments with EVAL-X accuracy and EVAL-X AUROC": "Here, instead of EVAL-X log-likelihoods, we use the accuracy and AUROC of the EVAL-X model asthe score. We call these EVAL-X-ACC and EVAL-X-AUROC scores. These metrics only depend onthe ranking of the datapoints, and therefore are not sensitive to differences in log probabilities thatdo not change ranks. shows that, due to this insensitivity, multiple encoding explanations(PRED, MARG, and the excessively reductive one) all achieve the same score as the correspond-ing EVAL-X score. In summary, ranking metrics like accuracy and AUROC are not sensitive toencoding explanations like EVAL-X log-likelihoods.",
  "C.6Classifying dogs and cats": "The POSI explanation selects the upper or the lower color patch depending on whether q(y =1 | x) > 0.5 or not. The PRED explanation selects the patch predicting from which best matchesthe prediction from q(y = 1 | x). The MARG explanation selects the top or the bottom image patchbased on the color as in the DGP in . We consider two non-encoding explanations. The firstexplanation, denoted optimal, selects exactly the features that occur in the DGP: {x1, x2} if thecolor patch x1 is blue and {x1, x4} otherwise. As y is determined by the explanation, meaningy",
  "We also run the REAL-X method from to produce an explanation. REAL-X was run over expla-nations that select one of the four quarter patches and exact marginalization over the selections": "The base cat and dog images were obtained from the cats_vs_dogs dataset from the Tensorflowdatasets package. To construct images like in , the color and the two images are sampledindependently. The color being blue/red determines that the label associated with the top/bottomimage becomes the label for the constructed image. The training, validation, and test dataset consistof 8000, 1000, and 1000 samples respectively. We follow the procedure in Appendix C.2 to estimate STRIPE-X. The EVAL-X model is a pre-trained18-layer residual network. The model p(F | xv, , v) used in computing the ENCODE-METER termin STRIPE-X (eq. (6)) are 34-layer Residual neural networks. The EVAL-X model is trained for 100epochs with a batch size of 100 with the Adam optimizer, with the learning rate and weight decayparameters set to 103 and 0 respectively. The p(F | xv, , v) model is trained for 50 epochs witha batch size of 200 with the Adam optimizer, with the learning rate and weight decay parameters setto 5 105 and 1 respectively. The p model sees variable through an entire extra channel whereall the pixels take the value . We used validation loss as the metric to early stop. The EVAL-X andSTRIPE-X scores are computed on the test dataset. The cats vs. dogs experiment were done on anA100 GPU where the whole training and evaluation ran in less than 20 minutes.",
  "DAlgorithms": "Algorithm 1 describes an alternate way to estimate the ENCODE-METER component of STRIPE-Xwith a conditional generative model. Algorithm 2 describes the predictive version of STRIPE-Xestimation, which we used in our experiments. Algorithm 1: ENCODE-METER, generative version.Input: Training data D q(y, x) and test data Dt q(y, x), explanation function e(x),penalty weight . EVAL-X model p(y | xv). Conditional generative modelp(x | xv, y) and mutual information estimator that takes two sets as argumentsMI[{ci}, {di}];"
}