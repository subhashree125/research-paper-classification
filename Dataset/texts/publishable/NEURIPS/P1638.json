{
  "Abstract": "As new machine learning methods demand larger training datasets, researchersand developers face signicant challenges in dataset management. Although ethicsreviews, documentation, and checklists have been established, it remains uncertainwhether consistent dataset management practices exist across the community. Thislack of a comprehensive overview hinders our ability to diagnose and addressfundamental tensions and ethical issues related to managing large datasets. Wepresent a systematic review of datasets published at the NeurIPS Datasets andBenchmarks track, focusing on four key aspects: provenance, distribution, ethicaldisclosure, and licensing. Our ndings reveal that dataset provenance is oftenunclear due to ambiguous ltering and curation processes. Additionally, a varietyof sites are used for dataset hosting, but only a few offer structured metadata andversion control. These inconsistencies underscore the urgent need for standardizeddata infrastructures for the publication and management of datasets.",
  "Introduction": "Datasets serve as a fundamental bedrock for machine learning models. While the construction oflarger datasets has propelled advancements in the eld, it has also revealed negative implicationsof these practices . Issues include a lack of consent and compensation , documentation debt, legal risks , and malign content . To incentivize responsible management of datasets, the research community proposed various proce-dures, e.g. ethics review, dataset licensing , and standardized dataset documentation .While these toolkits have promoted individual reections to be more conscious of ethical and legalissues in sharing datasets, limited work examined the practices of data management at scale . Towhat extent do data management practices align or vary among researchers? The lack of a comprehensive overview of data management practices hampers our ability to systemat-ically diagnose and address key tensions in managing large datasets. To tackle this issue, we reviewed 238 dataset papers published in the NeurIPS Datasets and Benchmarks track, identifying signicantgaps and inconsistencies in how researchers and developers share and manage their datasets. Giventhe exponential growth of the track and the increasing need for effective dataset stewardship inmachine learning , we focused on four critical aspects of data management where publishersand stakeholders can have an immediate impact: provenance, distribution, ethical disclosure, andlicensing. We found major inconsistencies in all four key aspects, as well as some dominant trends. 57% paperswere built off of existing datasets or external data sources, but the level of detail on access, dataltering, and curation varies greatly, making it difcult to trace provenance at times. Datasets weredistributed on a variety of sites, such as personal or group websites, GitHub, and Zenodo, eachoffering different levels of support for metadata and version control. Some authors discussed theirethical considerations extensively, while most did not identify any potential implications. With respectto licensing, Creative Commons licenses were the predominant choices, but there were exceptionsin which the authors did not specify a license for their public datasets. Moreover, important datasetterms and license information appear in different locations of a dataset project, e.g. in the body ofa paper, in supplemental materials, or on the datasets hosting site, making it difcult for potentialdataset users to gain an accurate understanding of permissible use cases and limitations. Our ndings on data authors differing practices and choices on the four aspects highlight the urgentneed for standardized data infrastructures to support the remixing and sharing of datasets with clearmetadata, ethical disclosures, and complete licensing information. Dataset authors need not onlystructured documentation template but also standardized data infrastructures for hosting. As datasetscontinue to lay the foundation for large models, publishers, research institutions, and funding agenciesshould join forces and prioritize developing and promoting such data infrastructures so that datasetscan be responsibly and ethically shared, managed, and used by researchers and practitioners. We alsodiscuss our ndings in the context of the FAIR principles widely accepted rules of scientic datamanagement (ndable, accessible, interoperable, and reusable) and scholarly discourse onlicensing and provide recommendations for dataset authors on sharing datasets.",
  "Methods": "We focused on dataset papers published at the NeurIPS Datasets and Benchmarks track. NeurIPSsprominence in the ML eld and the tracks emphasis on dataset contributions make these datasetpapers suitable cases for our investigation into dataset management practices. We excluded papersthat solely focused on contributing or comparing benchmarks without introducing any new datasets.We reviewed all dataset papers published in 2021 (73 papers) and 2022 (85 papers). Notably, 2023saw substantial growth in number of dataset papers and we randomly sampled 80 out of a total ofthe 193 published. Because our intended contribution is to capture differences dataset managementpractice rather than quantitatively mapping out the distribution of specic practices, we began withsampling 20 papers of the 2023 batch. We reached qualitative saturation and temporal balance at 80papers from 2023. Our review focused on four key aspects of data management practices: 1) Provenance: Is it possibleto trace provenance? 2) Distribution: How was the dataset distributed? 3) Ethical Disclosures:What ethical concerns did the authors disclose? 4) Licensing: Under what licenses were the datasetsreleased? The rst and last authors went through 30 randomly selected papers to establish a schema to categorizepapers on each aspect. Then the two authors annotated the rest of the dataset papers independentlyand periodically cross-checked each others labels. The authors also met regularly to iteratively revisecategories and labels and annotated edge cases after reaching a consensus. The full list of datasetpapers along with our annotations can be found at hosted bythe Texas Data Repository.",
  "Results": "The 238 papers contribute datasets of a variety of modalities, including images, texts, and actionlogs, spanning different domains, e.g. health, banking, biology, and geography. The topical andmethodological varieties of the reviewed datasets highlight the Datasets and Benchmarks tracksbroad appeal to ML researchers, making the venue a suitable subject for our review. Our goal is not to critique any existing practices but rather to identify differences and inconsistenciesto support the improvement of existing guidelines, processes, and infrastructures, so researchers canbuild, remix, and expand datasets ethically and responsibly, a key goal for the track .",
  "Is it possible to trace provenance?": "We examined the possibility of tracing provenance and found a stark contrast among different datacollection methods. Here, we adopted Longpre et als denition of provenance as considerationsrelevant to a datasets original source and creation. These are crucial considerations for the develop-ment of accurate, responsible AI models. We nd a wide range of data collection techniques, eachwith its own provenance considerations. Below we describe the four distinct approaches authors taketo construct their datasetspost hoc, ad hoc, synthesis, and annotationand how provenance may ormay not be an issue in each approach. Note that these four categories are not mutually exclusive.",
  "Post hoc": "Post hoc refers to when authors collect data from other entities after it has been generated andrecorded, typically through direct downloads, scraping, API, or data sharing agreements. 136/238(57%) papers fall under the post hoc category. Tracing provenance is a difcult task for the post hoc category, despite the vast majority of thesepapers (120) relied on publicly available data. When describing their specic data collection approach,authors used a variety of descriptions, e.g. exporting, downloading, requesting, and collecting. Theambiguity of the language makes it difcult for those outside of the research team to know whetherthe raw data is curated through direct downloads, API, or scraping and whether the dataset authorshad to clean or structure the raw data (a procedure that is typically required for scraped data). Furthermore, the original data curation and preprocessing procedures are often unclear; we noticedmissing details about manual curation criteria, limited documentation about processing, and in someinstances authors wish to keep sensitive information in the original source private. The lack ofinformation about curation and preprocessing could be a provenance issue for potential data userswho are interested in examining the source of original data points. 16 papers included data collected post hoc from private servers or mentioned collaborations withexternal organizations, including imaging companies, biotech companies, hospitals, and governmentagencies. Given the private nature of the original data, provenance is inherently challenging. However,some authors provided background information about the collaborating entity, their processing andltering procedures, and in some cases, authors even shared the protocol or information about theinstrument used by collaborators, e.g. .",
  "Ad hoc": "Ad hoc refers to when the authors developed the instrument or apparatus to collect data from subjectsdirectly. 72 papers in our review fall under this category. Because it is the authors who created the dataor guided the data creation, tracing provenance for such type of dataset is naturally straightforward.For papers under this category, authors generally provided extensive details discussing their specicmechanisms, e.g. web interface, sensing technologies, camera setup, etc. 31 papers collected datafrom individuals or environments without researchers providing instructions and interventions, i.e.in the wild. 41 papers collected data in the lab where specic instructions/interventions weregiven to participants.",
  "Synthesis": "39 papers in our review included synthesized datasets. We dene synthesis as authors substantiallytransforming original datasets or creating new datasets with instruments. Therefore, synthesizeddatasets are different from simply combining or aggregating datasets. While synthetic datasets werearticially created by authors, provenance could still be an issue when synthetic datasets are builtupon existing datasets without sufcent documentation about the transformation. For example, twopapers in our review lacked description on data sampling and potential dataset users will likely nd itdifcult to verify the data curation procedure.",
  "Annotation": "The annotation category includes dataset papers that involved human annotators in constructingdatasets. 91 papers fall under this category. Provenance of annotations is typically clear at a high levelbecause authors generally disclose who are the annotators or how they are recruited. In our review,41 papers involved crowd workers, 36 involved experts, and 19 papers involved authors themselvesas annotators. Note that one paper may employ different types of annotators. However, 13 out of 41 papers that used crowdworkers did not specify the platform or site used forrecruiting workers. This may become a provenance issue for potential dataset users who wish toverify the working conditions of crowd workers.",
  "How were datasets distributed?": "provides an overview of the platforms and services authors used to distribute their datasets.44 (18%) papers used the authors group or organizational websites, followed by Zenodo (14%)and GitHub (14%) 1 , and Google Drive(13%). There is also a long tail of less used hosting sites 2.One unique advantage of using Zenodo, FigShare, and PhysioNet is that the dataset will be given aDOI (Digital Object Identier), a permanent, unqiue identier for the dataset, especially given thatNeurIPS publications are not assigned any by the publisher. Some hosting sites, like PhysioNet and Zenodo, allow users to input metadata and version information,while others, such as various cloud drives and personal websites, do not. As a result, authors maylack the incentives or ability to share metadata and maintain records of different dataset versions.This absence of metadata and version information can make it difcult for future users to understanda datasets origin, context, and changes over time and potentially hinder their ability to reproduceresults from earlier versions. 1others focusing on safety datasets have also found GitHub to be a commonly used hosting service 2The following hosting sites were used by less than 5 dataset papers: National Center for Biotechnology Information(NIBC), OpenScienceFramework, Microsoft SharePoint, FigShare, National Institute of Health(NIH),CodaLab, Box, MLCommons, Google APIs, PyPI, OneDrive, IEEE Data Port, AIcrowd, Mendeley Data,National Cancer Institute, DropBox, OpenDataLab, and OpenICPSR.",
  "Access: can we still access the dataset today?": "The vast majority of the datasets were directly accessible via the URLs provided by the authors,without manual approvals. On Hugging Face and PhysioNet, some dataset authors required datasetusers to be registered users of the sites and asked them to acknowledge their data use agreementsbefore proceeding to the dataset. 20 papers authors set up additional manual review procedures for those interested in accessing thedataset. Some of these are due to the sensitivity of the dataset (e.g. health records ) or toprevent misuse (e.g. deepfakes and satellite imagery .) Lastly, we faced some challenges in locating the dataset URLs of some papers. In several instances,we could not nd the URL in the paper or supplemental material and relied on search engines tond the datasets hosting sites. In another case, the Zenodo link and the project link shared by theauthors in the paper and supplemental material were no longer valid, but we were able to retrieve thedatasets new Zenodo link from the research teams website . We also found one instance in whichthe download feature was disabled by its authors as of May 2024 in order to comply with policychanges .",
  "Was a datasheet provided?": "We saw a rather consistent usage of datasheets over the years. 48% of papers from 2021 included adatasheet. The number increased to 62% in 2022 but decreased to 53% in 2023. Because we onlyreviewed submitted materials for each dataset papers, datasheets shared through other means (e.g.dataset hosting platforms, arXiv.org, etc.) were not accounted in our analysis. The percentages weretrieved from our analysis may be a lower bound estimate for the adoption of datasheet among allthe papers we reviewed.",
  "What ethical disclosures were discussed?": "96 included ethical disclosures in the paper and/or supplemental materials. We adopted Ajmani etal.s deniteion of ethical disclosure as any reasoning in the paper or supplementary materials aboutethics, potential harms, or broader impacts . Below, we discuss emergent themes in the ethicaldisclosures we found in these dataset papers and their supplemental materials. These themes aresynthesized through an iterative coding process among the two authors that annotated the papers. Privacy and Identication of Individuals Privacy concerns were heavily discussed within ethicaldisclosure statements. Privacy was most often discussed in terms of personally identiable information(PII). Authors often sought to protect PII by removing such information from their datasets. Forexample, one research team in computer vision noted they would manually inspect images for facesand license plates before publishing their dataset . Authors of location-based datasets alsomentioned only releasing feature-level data with appropriate credentials to prevent data leakage .While discussions of PII are a necessary foundation for considering privacy, we found a lack ofprivacy discussion that goes beyond data content. For example, what privacy rights do we generallyowe to data contributors? Does privacy demand more holistic consent procedures? These are ethicaltensions within ML research that are currently lacking from the ethics discourse. Representativeness We also found concerns about sampling biases of these datasets. Due to resourcelimitations, researchers often collected data from a single source, region, or sample (e.g. patient datafrom one hospital, user behavior from one region). For example, one paper noted that they useda convenience sample of college students resulting in potential data bias around age, race, andsocioeconomic status . In these cases, authors cautioned future dataset users about the potentialbiases and some even suggested that their datasets should only be used as an initial foundation andmust be diversied before model training. Out-of-Context Misuse Finally, some authors also expressed concerns about future use of thesedatasets and related outcomes. In sentiment analysis-focused research, authors mentioned thatsentiment analysis for vaccines and drugs can only reveal the users overall viewpoint and attitude",
  "Apache License 2.010": "and should not be interpreted as their willingness to take drugs or get vaccinated . These morespeculative ethics concerns are crucial for considering the broader impacts of researchan exercisewith which NeurIPS authors have struggled in the past . Future work could explore how to assistdataset authors in speculating potential misuse of their datasets. Overall, the ratio of papers that mention ethical concerns (40%) is less than ideal. This gap suggestsa need from the NeurIPS community for ethical disclosure scaffolding. In particular, unlike withmodels, ethical issues in datasets can be difcult to predict or foresee. Moreover, authors of datasetpapers may be ill-equipped to envision the secondary, unintentional impact of dataset creation, evenwhen prompted to do so. We urgently need frameworks that can guide authors to reect on theirdataset practices from collection to processing to aggregation to sharing.",
  "What licenses were applied to datasets?": "shows the distribution of licenses used in the 238 dataset papers 3. Creative Commons licensesare widely preferred: CC BY is the most commonly used license, with 61 papers using this licensefor their datasets, followed by CC BY-NC-SA (40). Notably, a non-trivial percentage of papers (72)used Creative Commons licenses that prohibit commercial use, i.e. CC BY NC, CC BY NC SA, andCC BY NC ND. In addition to Creative Commons Licenses, there are other licenses present thatdisallow commercial use, such as the PhysioNet Credentialed Health Data License 1.5.0. Consistent with prior work , we found a notable number of dataset papers used software licenseswhose suitability as dataset licenses has been questioned by some legal scholars and experts, such asthe GNU GPL license, the Apache license, and the MIT license. More specically, software licensesare designed for code and program, whereas the copyright of a dataset is more complex, especiallywhen it contains materials whose copyrights do not belong to the dataset authors . We did not nd explicit license information in 36 papers, their supplemental materials, or corre-sponding dataset hosting sites. This lack of clarity may pose challenges for potential dataset users indetermining the legal suitability of the datasets for their intended purposes.",
  "How easy it was to retrieve licensing information": "The location of licensing information in these dataset papers was inconsistent. Out of the 238 papers,160 (67%) included information about the specic licensing of their datasets in the appendix orsupplemental materails. 54 (23%) outlined the licensing terms for their datasets in the main bodyof the paper. 16 papers mentioned the license information in their datasets hosting site, e.g. underthe LICENSE section. Some authors included licensing information multiple times at differentlocations. The discrepancies in where dataset licenses are disclosed will likely make it challengingfor dataset users to locate accurate license information, increasing the risk of dataset misuse.",
  "Issues with copyrights": "Determining who is the copyright owner of materials included in a dataset can be a challengingtask, particularly when the authors collected data post hoc. For example, Ypsilantis et al. collectedimages from Flickr and various web sources and acknowledged the challenge of identifying individualcopyright owners . Due to this, the research team did not license their dataset. In another case,Wu et al. licensed their Kuaishou video dataset following the companys legal teams advice but forthe videos collected from YouTube, they acknowledged that they were not the copyright holder andtherefore could not license the dataset . In other cases, authors licensed their datasets that contain copyrighted materials. For example, theMineDojo dataset consisted of extensive images, videos, text content from various sources includingYouTube, Minecraft Wiki, and Reddit . Each source has its own license or data use terms (forexample, Minecraft Wiki is under CC BY-SA), and the authors applied new licenses to the aggregatedatasets. To circumvent the copyright issue, some authors opted to include URLs to original artifacts ratherthan the actual artifacts. For example, the LAION 5B dataset included links to various images hostedon the web and the RedCap dataset included links to Reddits hosting service.",
  "Efforts to retrieve content with clear copyrights": "We have also observed efforts by authors to retain materials with clear copyrights when constructingtheir datasets. For example, Galves et al. paid special attention to licenses when crawling contentfrom Vimeo and archive.org and only retained content with CC BY, CC BY-SA, and CC0 licenses, i.e.content allowed for commercial use . The authors subsequently licensed their dataset under CCBY-SA. The authors also stated that they did not download any videos from YouTube, even the oneswith permissible CC licenses, citing concerns about violations of YouTubes terms of service . Ina similar case, Falta et al. curated data from existing datasets, and inherited the original licenses .",
  "Related Work, Discussion, and Recommendations": "Through a review of 238 dataset papers, we found differing practices in sharing and managing datasetsamong authors. Given the diversity of the published datasets disciplines, domains, and purposes,some differences were expected or necessary. For example, datasets that were constructed post hocwere naturally less traceable than the ones that were created ad hoc. Similarly, we expected datasetswith copyrighted materials to encounter licensing uncertainties given the complex nature of thesetopics . However, some gaps and inconsistencies warrant further reection. Below, we discuss how stakehold-ers of the data supply chain, from dataset authors to publishers to institutions to funding agencies,may work collaboratively to ensure that datasets are shared and managed responsibly and ethically.",
  "Needs for Standardized Data Infrastructure": "Dataset authors shared datasets on a wide range of hosting sites. This aligns with previous researchon National Science Foundation-funded projects and evaluation datasets for LLM safety ,which also found that dataset authors utilized a diverse array of hosting services. In particular, we sawa substantial reliance on sites that do not support metadata and version control, e.g. cloud drives andcloud hosting services. Metadata is crucial for responsible data management as it provides importantcontext about the dataset creation process. Lack of metadata will limit future data users ability toverify provenance, assess suitability, and potentially identify biases in a dataset. Version control isanother important feature for responsible data management, as datasets expand or change, due to newdata points added or old data points being corrected or deleted. Lack of version control will potentiallylead to reproducibility issues in downstream models. Taken together, to ensure responsible usage ofdatasets and accurate models, it is important for the NeurIPS community to establish standardized data infrastructures so that future datasets can be published with accurate, consistent metadata andversion records. Such data infrastructures could: 1) have built-in templates such as dataset card ordatasheet for dataset authors to input metadata in a structured format (a feature already supportedto some extent by some dataset hosting platforms such as Hugging Face) and 2) display changesand contributors of different versions (a feature supported by some platforms such as dataverse.)In the meantime, publishers should also discourage dataset authors from sharing datasets withoutmeaningful metadata or support for version tracking. The community could benet immediately from public and research institutes data managementservices (e.g. the dataverse platform and Texas data repository). Many research elds, such as biomed-ical sciences and natural hazards engineering, have a long history of sharing data to advance scienceand foster research collaboration . These elds have established policies and institutionalinfrastructures for data sharing. For example, the National Institutes of Health in the U.S. has longinvested in data sharing to support and catalyze biomedical research, including sites like PhysioNet(now funded by the National Institute of Biomedical Imaging and Bioengineering) and the NationalCenter for Biotechnology Informationtwo hosting sites that have been used sparingly by the datasetpapers we reviewed. Similarly, the Natural Hazards Engineering Research Infrastructure(NHERI)developed and launched its data hosting site, DesignSafe, to promote research collaborations . Insocial sciences, data archives such as ICPSR (the Inter-university Consortium for Political and SocialResearch) allow researchers to deposit and share large datasets with robust documentations. Thesewell-established data infrastructures, along with their protocols and best practices, could serve asguiding examples for publishers, research institutions, rms, and funding agencies to support similardata infrastructures for publishing ML datasets.",
  "Compliance issues with the FAIR principles": "Our ndings on the lack of data documentation and inconsistent sharing practices also highlightedcompliance issues with the FAIR principle (ndable, accessible, interoperable, and reusable)thefoundational values of scientic data management . Below, we unpacked the specic complianceissues and discussed recommendations for dataset authors and other stakeholders.Findable: The different platforms used for dataset sharing could pose challenges to ndability.Datasets hosted on prominent platforms like GitHub, Hugging Face, and Kaggle may be easier tond than those hosted on individual or group websites. Given ML researchers concentrated usageof a selected few datasets , we recommend that before choosing a hosting site, dataset authorsexplore who their potential dataset users are and how they nd suitable datasets for their work. Additionally, dataset authors should consider the specic features offered by different hosting siteswhen uploading their datasets. Findability requires rich metadata and a unique, persistent identier,which many cloud drives and hosting services currently do not support. Therefore, it may be benecialfor dataset authors to opt for hosting sites that support metadata and DOIs so their datasets can beeasily retrieved and identied by search engines. Conference organizers and publishers may also provide guidelines to help dataset authors identifyand choose hosting platforms that support ndability. For example, these guidelines might emphasizethat links to datasets from personal or group websites may take time to be indexed by search engines,making them less ndable to potential users. Additionally, given that NeurIPS papers do not haveassigned DOIs, conference organizers and publishers could consider requesting dataset authors tocreate DOIs 4 for their datasets to ensure that these datasets can be easily found and cited. Conferenceorganizers could even include an informational prompt beneath the DOI input eld, informing datasetauthors of the importance of DOIs for dataset ndability and citation. Accessible: Almost all datasets are accessible, either directly or after an approval process establishedby the authors. However, as described earlier, we encountered outdated dataset links in papers andsupplemental materials. Data authors should pay special attention to the dataset links they includein their papers and the preservation of metadata and take into account that some hosting services",
  "If anonymity is required for peer preview, authors could use private DOIs": "may not be permanent. Consulting institutional libraries or research data management teams may beparticularly helpful for dataset authors, as these information professionals have long worked towardpreserving and structuring (meta)data for digital access. Dataset authors may seek specic guidancefrom them to identify stable hosting services, generate DOIs, and make their datasets accessible todifferent types of dataset users (researchers, developers, students, etc.). Another requirement for achieving accessibility in the FAIR principles is the support of authenticationwhen necessary . After all, not all datasets should be openly accessible. In this regard, datasetauthors can learn from information professionals in libraries and archives, who have manageddigital access to sensitive materials for decades . We found that very few hosting sites, such asPhysioNet, Hugging Face, and Zenodo, offer authentication services. Some dataset authors resorted tomanual authentication processes, which can be time-consuming and labor-intensive. We recommendthat authors consider the need for authentication to prevent potential misuse of their datasets andcollaborate with their institutional libraries or research data management teams to identify suitablehosting sites and authentication mechanisms.",
  "Interoperable:While we did not delve into the specic formats of NeurIPS datasets and how": "interoperable they are, we reviewed how dataset papers were built off of existing datasets. We found itchallenging to trace a signicant portion of these remix datasets due to manual sampling of originaldata sources or poor documentation of the curation process. Therefore, it is unlikely for such datasetsto be interoperable with original data sources. We recommend dataset authors carefully evaluatewhether their novel datasets need to be compatible with original data sources in their constructionprocess and if so, share detailed preprocessing procedures to support interoperability. Reusable: The wide adoption of Creative Commons licenses and open source licenses suggests thatmost of the datasets without copyrighted materials can be reused by other researchers. However, thedifferent placements of license information we observed raised questions about how visible theselicense terms will be to potential dataset users. We recommend dataset publishers and conferencesinclude licensing information as part of the structured metadata they collect from authors and clearlyhighlight this information to potential dataset authors. Additionally, lack of provenance potentially inhibits the reusability of a dataset , making it allthe more important to pay careful attention to dataset documentation , including but not limitedto possible inclusion of copyrighted materials, recruitment process of annotators if applicable, andspecic data collection approaches (e.g. scraping vs. API).",
  "Dataset licensing": "The issues of unclear copyright raise the question of whether disambiguating copyright licensing willbe an effective approach for AI governance. With the deployment of large AI models, there havebeen extensive scholarly discourses and lawsuits on whether copyright laws could be used to governAI models and enforce responsible reuse of creative content . Some researchers have alsoadvocated for adopting licensing to restrict downstream uses of datasets and code . As a result,the landscape of copyright licenses is becoming increasingly complex. However, dataset authors maybe ill-equipped or reluctant to make informed licensing decisions. It is crucial for publishers to helpdataset authors understand the implications of copyright licenses, as this will be an urgent next stepin addressing these challenges. Given that over half of the dataset papers fall under the post hoc category, i.e. leveraging existingdata to construct a new dataset, authors would likely benet from tools that can address licensingdependencies and recommend appropriate licenses. Such tools could be built upon existing datatracing and provenance tools such as the Data Provenance Explorer and the Whats In My BigData? platform and provide parental licenses of original datasets so authors could make informeddecisions about who are the copyright holder and whether their new datasets need to inherit anyparental licenses.",
  "Limitations and Future Work": "Our review focused on the NeurIPS Datasets and Benchmarks track and did not include any datasetpapers that have been published in the main conference track. Nonetheless, given the breadth of ourreview, we expect our sample to reect the lack of consistency in dataset management practices inthe NeurIPS community. Additionally, given the tracks detailed submission requirements, it is likelythat dataset documentation, licensing, and ethical disclosures are reported in greater details than theyare at other publication venues. Future work may expand our review by including ML datasets fromother venues to gain a more holistic understanding of dataset management practices in the eld. Our review also is limited to the four aspects of data management practices (provenance, distribution,ethical disclosures, and licensing) and does not consider secondary impact of these datasets, e.g.potential violation of original creators copyrights and moral rights and distribution of harmfulcontenta fruitful area for future work . Our ndings and recommendations aim to inform the development and adoption of standardizeddata infrastructures and data management practices and do not serve as legal advice. Moreover, asmentioned earlier, we caution against blanket data standards applied to different types of datasets.For example, For instance, datasets containing sensitive materials or those that could be misused mayrequire more rigorous stewardship and stricter standards compared to datasets that consist solely ofpublic content. We shared our detailed annotations on which future work could build to automate part of ourreview. Interested researchers should pay particular attention to several signicant challengesfor automation that we have identied: unclear terms and descriptions regarding data collectionapproaches, inconsistent naming and locations of licenses, different methods of sharing dataset URLs,and varying levels of ethical disclosures. Some of these challenges related to retaining licensinginformation and URLs could be addressed promptly if NeurIPS guidelines provided a uniform,structured format for authors to follow.",
  "Conclusion": "We reviewed 238 dataset papers published at the NeurIPS Datasets and Benchmarks track between2021 and 2023 and examined the provenance, distribution, ethical disclosures, and licensing of theoutput datasets. We found inconsistent practices across all aspects, discussed the importance of stan-dardized data infrastructures and compliance with the FAIR principles, and provided correspondingrecommendations.",
  "Malte Luecken, Daniel Burkhardt, Robrecht Cannoodt, Christopher Lance, Aditi Agrawal,": "Hananeh Aliee, Ann Chen, Louise Deconinck, Angela Detweiler, Alejandro Granados, ShellyHuynh, Laura Isacco, Yang Kim, Dominik Klein, Bony De Kumar, Sunil Kuppasani, HeikoLickert, Aaron McGeever, Joaquin Melgarejo, Honey Mekonen, Maurizio Morri, MichaelaMller, Norma Neff, Sheryl Paul, Bastian Rieck, Kaylie Schneider, Scott Steelman, MichaelSterr, Daniel Treacy, Alexander Tong, Alexandra-Chloe Villani, Guilin Wang, Jia Yan, Ce Zhang,Angela Pisco, Smita Krishnaswamy, Fabian Theis, and Jonathan M. Bloom. A sandbox forprediction and integration of DNA, RNA, and proteins in single cells. Proceedings of the NeuralInformation Processing Systems Track on Datasets and Benchmarks, 1, December 2021.",
  "Mark D. Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles": "Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E.Bourne, Jildau Bouwman, Anthony J. Brookes, Tim Clark, Merc Crosas, Ingrid Dillo, OlivierDumon, Scott Edmunds, Chris T. Evelo, Richard Finkers, Alejandra Gonzalez-Beltran, AlasdairJ. G. Gray, Paul Groth, Carole Goble, Jeffrey S. Grethe, Jaap Heringa, Peter A. C. t Hoen, RobHooft, Tobias Kuhn, Ruben Kok, Joost Kok, Scott J. Lusher, Maryann E. Martone, Albert Mons,Abel L. Packer, Bengt Persson, Philippe Rocca-Serra, Marco Roos, Rene van Schaik, Susanna-Assunta Sansone, Erik Schultes, Thierry Sengstag, Ted Slater, George Strawn, Morris A. Swertz,Mark Thompson, Johan van der Lei, Erik van Mulligen, Jan Velterop, Andra Waagmeester,Peter Wittenburg, Katherine Wolstencroft, Jun Zhao, and Barend Mons. The FAIR GuidingPrinciples for scientic data management and stewardship. Scientic Data, 3(1):160018, March2016. Publisher: Nature Publishing Group.",
  "spent on participant compensation? [N/A]": "visualizes the major hosting sites across the year 2021 to 2023. We only included majorhosting sites used by more than ve dataset papers. We saw that personal or lab websites were acommon choice in 2021 but became less common in 2023. Conversely, we saw an increasing numberof datasets hosted on Google Drive, Hugging Face, and AWS in 2023. Note that given the limitedtime span of the datasets in our review, the current year-to-year comparison was only descriptive andshould not be interpreted as statistically meaningful. visualizes the licenses applied to the datasets from 2021 to 2023. The CC-BY licenseappeared to be the most common license across three years. Again, given that we only revieweddatasets from three cycles, differences in license usage across years should be interpreted withcaution."
}