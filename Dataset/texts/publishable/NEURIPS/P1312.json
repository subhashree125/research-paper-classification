{
  "Abstract": "Neural networks continue to struggle with compositional generalization, and thisissue is exacerbated by a lack of massive pre-training. One successful approach fordeveloping neural systems which exhibit human-like compositional generalizationis hybrid neurosymbolic techniques. However, these techniques run into the coreissues that plague symbolic approaches to AI: scalability and flexibility. Thereason for this failure is that at their core, hybrid neurosymbolic models performsymbolic computation and relegate the scalable and flexible neural computation toparameterizing a symbolic system. We investigate a unified neurosymbolic systemwhere transformations in the network can be interpreted simultaneously as bothsymbolic and neural computation. We extend a unified neurosymbolic architecturecalled the Differentiable Tree Machine in two central ways. First, we significantlyincrease the models efficiency through the use of sparse vector representations ofsymbolic structures. Second, we enable its application beyond the restricted set oftree2tree problems to the more general class of seq2seq problems. The improvedmodel retains its prior generalization capabilities and, since there is a fully neuralpath through the network, avoids the pitfalls of other neurosymbolic techniquesthat elevate symbolic computation over neural computation.",
  ": Generalization ability of our ap-proach (sDTM) compared with baselinesacross various out-of-distribution shifts, av-eraged over different datasets. See 5": "Deep learning models achieve remarkable performanceacross a broad range of natural language tasks ,despite having difficulty generalizing outside of theirtraining data, struggling with new words , knownwords in new contexts , and novel syntactic struc-tures, like longer sequences with greater recursive depth. Increasingly this problem is addressed throughdata augmentation, which tries to make it less likelya model will encounter something unlike what it seesduring training reducing the degree by which it hasto generalize . However, even models trainedon vast quantities of data struggle when evaluated onexamples unlike those seen during training .",
  "arXiv:2412.14076v1 [cs.AI] 18 Dec 2024": "This stands in contrast to how humans process language, which enables robust generalization .By breaking novel sentences into known parts, we can readily interpret phrases and constructions thatwe have never encountered before (e.g. At the airport I smiled myself an upgrade, ). Why domodels trained on orders of magnitude more language data than a human hears in 200 lifetimes still fail to acquire some of languages most essential properties? Central to languages generalizability is compositional structure where contentful units, likewords, fit together in a structure, like a syntactic tree. Many classical approaches in NLP andMachine Learning attempt to induce a grammar from data in the hope of leveraging the same kindsof generalization seen in natural language [e.g. 34, 33, 68]. However, structured representationsare not first-order primitives in most neural networks . Despite theoretical appeal, thestrictures of purely discrete symbolic approaches have made them difficult to apply to the breadthof tasks and domains where deep learning models have proven successful . In contrast, purelyconnectionist models like Transformers struggle with the kinds of sample efficiency androbust generalization ubiquitous to human learning. Neurosymbolic methods attempt to integrate neural and symbolic techniques to arrive at a systemthat is both compositional and flexible . While some neurosymbolic architecturesachieve impressive compositional generalization, they are often brittle due to the symbolic coreof their computation . These methods are hybrid neurosymbolic systems, where the primarycomputation is symbolic, and the neural network serves to parameterize the symbolic space. We takea different approach, one where symbolic operations happen in vector space. In our system, neuraland symbolic computations are unified into a single space; we multiply and add vector-embeddedsymbolic structures instead of multiplying and adding individual neurons. We introduce a new technique for representing trees in vector space called Sparse Coordinate Trees(SCT). SCT allows us to perform structural operations: transformations which change the structureof an object without changing the content. This is a crucial aspect of compositionality, where thestructure and content can be transformed independently. We extend the Differentiable Tree Machine(DTM), a system which operates over binary trees in vector space, into the Sparse Differentiable TreeMachine (sDTM) to improve performance and applicability to a larger variety of tasks2. While DTMprocesses vector-embedded binary trees as the primitive unit of computation, the order of operationsand argument selection is governed by a Transformer. We present results showing that this unifiedapproach retains many of the desirable properties of more brittle symbolic models with regards togeneralization, while remaining flexible enough to work across a far wider set of tasks. While fullyneural architectures or hybrid neurosymbolic techniques excel at certain types of generalization, wefind that DTM, with its unified approach, excels across the widest array of shifts.",
  "Related Work": "Work leveraging the generalizability of tree structures has a long history across Computer Science,Linguistics, and Cognitive Science . Much of classical NLP aims to extractstructured representations from text like constituency or dependency parses [for overview: 13, 42].More recent work has shown the representations learned by sequence-to-sequence models withoutstructural supervision can recover constituency, dependency, and part of speech information fromlatent representations in machine translation and language models . While those analyses show",
  "person": ": An example representation using Sparse Coordinate Trees (SCT). The values are N-dimensional vectors, and the tree positional indices are integer representations of positions in the tree.The absent child nodes of \"The\" (indices 4 and 6) are skipped with SCT. structural information is encoded, they stop short of showing that the representations themselves aretree-structured. Analyses inspired by Tensor Product Representations and chart parsing give an account of how representations become somewhat tree-structured over the course of training. Despite the apparent emergence of semi-structured representations in Transformers and LSTMs,these architectures still appear to struggle with the kinds of structural generalization that come easilyto humans . A variety of approaches try to tackle this problem through meta-learning, data augmentation , or decomposing the task into separate parts . The Relative-Universal Transformer combines relative positional embeddings with a recurrent component, inan effort to emphasize local structures while allowing for unbounded computation. Explicitly tree structured network architectures have been introduced for RNNs , LSTMs ,and Transformers . However, these variants often do not outperform their unstructuredcounterpart on out-of-distribution challenges . This may be because generalization requiresboth structured representations and operations that respect that structure. A separate line of workconsiders neural architectures that are used to parameterize components of a symbolic system or fuzzy/probabilistic logic . Similar to how vectors are embedded in trees in our work,some work embeds vectors within logical systems . Logic approaches to structure learningare also an active area of research . Other approaches leverage explicit stack operations. NQG from Shaw et al. combines the outputs from neural and symbolic modelsby inducing a grammar, but deferring to T5 when that grammar fails. However the grammarsinduction method has polynomial complexity with both dataset size and sequence length, whichlimits its application to larger tasks. Vector Symbolic Architectures (VSAs) implement symbolic algorithms while leveraging high dimen-sional spaces . VSAs are similar to uniform neurosymbolic approaches, althoughVSAs commonly lack a learning component. Our work extends that of Soulos et al. which canbe viewed as integrating Deep Learning and VSAs. They introduce the Differentiable Tree Machinefor Tree-to-Tree transduction. Here we instantiate a sparse Sequence-to-Sequence version with farfewer parameters and improved memory efficiency.",
  "Differentiable Tree Operations Over Sparse Coordinate Trees": "Representing trees in vector space enables us to perform differentiable structural operations on them.Soulos et al. used Tensor Product Representations (TPRs) for this purpose. TPRs use thetensor (or outer) product to represent trees in vector space (A.1). Use of an outer product leads to arepresentation dimensionality that is multiplicative with both the embedding dimensionality and thenumber of possible tree nodes. Additionally, the number of nodes is itself an exponential function ofthe supported depth. This makes TPRs difficult to use in practice, given available memory is quicklyexceeded as tree depth increases. In this section, we introduce Sparse Coordinate Trees (SCT), a new schema for representing treesin vector space. We then define a library of parallelized tree operations and how to perform theseoperations on SCT.",
  "Sparse Coordinate Trees (SCT)": "Like TPRs, we want an encoding for trees that factorizes the representation into subspaces forstructure and content respectively. This approach to representational spaces differs from models likean RNN and Transformer, which represent structure and content jointly in an unfactorized manner.By separating the structure and content subspaces a priori, we can operate over these two spacesindependently. This decision is motivated by the fact that distinct treatment of these spaces is anessential aspect of compositionality. We derive our tree representation scheme from the sparse coordinate list (COO) format. COO storestensor data in tuples of (indices [integers], values [any format], size [integers]). The indices areN-dimensional to simulate a tensor of arbitrary shape (e.g. including dimensions such as batch orlength). When an index is not indicated in indices, it is assumed that the corresponding value is 0. We give structural meaning to COO representations by defining one dimension of indices as thetree position occupied by a value vector. Our tree addressing scheme is based on Gorn addresses: to get the tree position from an index, convert the index to binary and read from right to left.A left-branch is indicated by a 0 and a right branch by a 1. To distinguish between leading 0s andleft-branches (e.g. 010 vs 10), we start our addressing scheme at 1 instead of 0. This indicates that all0s to the left of the most-significant 1 are unfilled and not left-branches. shows an exampleencoding of a tree with this approach. SCT can be viewed as a TPR with certain constraints, andSection A.1 defines this equivalence and formally describes the memory savings.",
  "Differentiable Tree Operations": "To operate on the trees defined in the previous section, we need a set of functions. We use a smalllibrary of only three: left-child (left), right-child (right), and construct (cons) a new tree from aleft and right subtree.3 Although these three functions are simple, along with the control operationsof conditional branching and equality-checking, these five functions are Turing complete . In addition to saving memory, SCT also provides a more efficient method for performing differentiabletree operations. The operations defined in Soulos et al. require precomputing, storing, andapplying linear transformations for left, right, and cons. Since our values and tree positionalindices are kept separate, we can compute the results of left, right, and cons dramatically moreefficiently using indexing, bit-shifts, and addition. shows how we can perform left directly on SCT. left is performed by indexing the evenindices (i.e. those with a 0 in the least significant bit, which targets all of the nodes left of the root)and their corresponding values, then performing a right bit-shift on the indices. right is symmetrical,except that we index for the odd positional indices and ignore position 1 in order to remove theprevious root node. cons is performed by left bit-shifting the positional indices from the left- and",
  "right-subtree arguments, then adding 1 to the newly shifted indices for the right argument. A newvalue s can be provided for the root node": "Our network also needs to learn a program over multiple operations differentiably. This involvesthe aforementioned structured operations, as well as differentiable selection of which operation toperform and on which trees. We take weighted sums over the three operations left, right, andcons, as well as over potential trees. Specific details are discussed in the next section. The resultof our weighted sum is coalesced, which removes duplicate positional indices by summing togetherall of the values that share a specific index. Formally, define the trees over which to perform leftTL, right TR, and cons TCL & TCR; T = [TL; TR; TCL; TCR]. We also take a new value s Rdto be inserted () at the new root node of the cons operation, and a vector of operation weightsw = (wL, wR, wC) which sum to 1.",
  "The Sparse Differentiable Tree Machine (sDTM)": "Our work extends the Differentiable Tree Machine (DTM) introduced in Soulos et al. withthe Sparse Differentiable Tree Machine (sDTM). While similar to the original at a computationallevel, sDTM represents a different implementation of these concepts that make it dramatically moreparameter and memory efficient. We also introduce techniques to apply sDTM to tasks with sequenceinput and output (seq2seq).",
  ": A schematic of how the three corecomponents of the DTM (agent, interpreter, andmemory) relate to each other. Adapted fromSoulos et al.": "sDTM uses our Sparse Coordinate Trees schemaacross its components. Like the original DTM, ourmodel is comprised of an agent, interpreter, and mem-ory (illustrated in ). The Interpreter performsEquation 1 by applying the bit-shifting tree opera-tions from .2 and weighting the result. Theoutput from the interpreter is written to the next avail-able memory slot, and the last memory slot is takenas the output. The Agent is a Transformer encoder that takes anencoding of the memory as input and produces theinputs for Equation 1: w, T, and s. Two specialtokens, <OP> and <ROOT>, are fed into the Agentto represent w and s. Each time a tree is written to memory, a fixed-dimensional encoding of that treeis produced and fed as a new token to the agent (4.2). The agent soft-selects tree arguments for theinterpreter, T, by performing a weighted sum over the trees in memory. in the Appendixcontains a detailed diagram showing the flow of information for one layer of sDTM. The agent which implicitly parameterizes the conditional branching and control flow of the programis modeled by a Transformer, and it is possible for sDTM to face some of the generalization pitfallsthat plague Transformers. The design of sDTM encourages compositional generalization throughdifferentiable programs, but it does not strictly enforce the constraints of classical symbolic programs.As the results in show, sDTM can learn generalizable solutions to some tasks despite thepresence of a Transformer, but on some other tasks the issues with generalization are still present.",
  "Pooling by attention": "Each tree in memory needs to have a fixed-dimensional encoding to feed into the agent regardless ofhow many nodes are filled. Commonly this is done via pooling, like taking the means of the elementsin the tree, or a linear transformation in the case of the original DTM. Instead, we use Pooling byMulti-headed Attention (PMA) , which performs a weighted sum over the elements, where theweight is derived based on query-key attention. Attention is permutation invariant to the ordering of key and value vectors, but it is important thatour pooling considers tree position information. To enforce this, we convert the position indicesto their binary vector representation b. This leads to an asymmetrical vector with only positivevalues, so instead we represent left branches as 1 and keep right branches as +1. For example,position 5 . The input to our pooling function is theconcatenation of this positional encodingb with the token embedding x at that position: [x;b]. Thismethod for integrating token and node position is similar to tree positional encoding from Shiv andQuirk , except that we use concatenation and a linear transformation to mix the content andposition information instead of addition. Unlike standard self attention, we use a separate learnable parameter for our query vectorq Rnum_headskey_dim.We pass [x;b] through linear transformations to generate keys k Rnum_headskey_dim and values v Rnum_headsvalue_dim. The result of this computation is alwaysz Rnum_headsvalue_dim given that q is fixed and does not depend on the input. The rest of thecomputation is identical to a Transformer with pre-layer normalization .",
  "Tree Pruning": "While Sparse Coordinate Trees mean that trees with fewer filled nodes take up less memory, theway our model blends operations results in trees becoming dense. The interpreter returns a blendof all three operations at each step, including the cons operation which increases the size of therepresentation by combining two trees. In practice even as the entropy of the blending distributiondrops, the probability of any operation never becomes fully 0. This means that over many steps, treesstart to become dense due to repeated use of cons. In order to keep our trees sparse, we use pruning:only keeping the top-k nodes as measured by magnitude. k is a hyper-parameter that can be set alongwith the batch size depending on available memory.",
  "Lexical Regularization": "To aid lexical generalization, we add noise to our token embeddings. Before feeding an embeddedbatch into the model, we sample from a multi-variate standard normal for each position in each tree,adding the noise to the embeddings as a form of regularization . Ablation results showing theimportance of this regularization are available in Appendix A.3.",
  "Handling Sequential Inputs and Outputs": "seq2treeThe original DTM can only be applied to tasks where a tree structure is known for bothinputs and outputs. Here we provide an extension to allow DTM to process sequence inputs. To do thiswe treat each input token as a tree with only the root node occupied by the token embedding. We theninitialize the tree memory with N trees, one for each token in the input sequence. left depictsthe initial memory state for a sequence. The agents attention mechanism is permutation-invariant,so in order to distinguish between two sequences which contain the same tokens but in differentorders, we apply random sinusoidal positional encodings to the first N tokens passed to the agent. Random positional encodings sample a set of increasing integers from left-to-right insteadof assigning a fixed position to each token. The purpose of left and right is to extract subtrees.Since in our seq2tree setting the input sequence is processed in a completely bottom-up manner, werestrict the agent and interpreter to only have a single operation: cons. Use of a single operationto construct new trees from subtrees aligns the DTM theoretically with the Minimalist Program ,which addresses natural languages compositionality in terms of a single operation: merge. seq2seqTo handle sequence inputs and outputs we convert the output sequence to a tree. Onemethod to convert the output sequence into a tree is to use a parser. Alternatively, when a parser isnot available, we can embed a sequence as the left-aligned leaves at uniform depth (LAUD). right shows how an output sequence can be embedded using LAUD. Since all of the non-terminalnodes are the same, we can hardcode the root argument to cons. We insert a special token <EOB> tosignify the end of a branch, similar to an <EOS> token.",
  "Baselines": "We consider models that are trained from scratch on the datasets theyre evaluated on; while thecompositional capabilities of large pre-trained models are under active debate , we are interestedin the compositional abilities of the underlying architecture rather than those that may resultfrom a pre-training objective. We compare sDTM to two fully neural models, a standard Transformer as well as a relative universal Transformer (RU-Transformer) which was previously shownto improve systematic generalization on a variety of tasks . We also compare our model toa hybrid neurosymbolic system, NQG , a model which uses a neural network to learn aquasi-synchronous context-free grammar . NQG was introduced alongside NQG-T5, which is amodular system that uses NQG when the grammar produces an answer and falls back to a fine-tunedlarge language model T5 . As mentioned at the beginning of this section, we only compare toNQG in this paper since we want to evaluate models that have not undergone significant pre-training.4Details related to data preprocessing (A.4), model training (A.6, A.8), compute resources (A.9),and dataset details (A.10) are available in the Appendix. For all datasets, the reported results are thebest exact match accuracies on the test set over five random seeds. Additional data on means andstandard deviations across the five runs are shown in Section A.7. For each task, we test whether models generalize to samples drawn from various data distributions.Independent and identically distributed (IID) samples are drawn from a distribution shared withtraining data. We evaluate several out-of-distribution (OOD) shifts. One-shot lexical samples,while drawn from the same distribution as the training data, contain a word that was only seen ina single training sample. Similarly, Zero-shot lexical samples are those where the model is notexposed to a word at all during training. Structural/length generalization tests whether modelscan generalize to longer sequences (length) or nodes not encountered during training (structural).Template generalization withholds an abstract n-gram sequence during training, and then each testsample fits the template. Finally, maximum compound divergence (MCD) generates train and test setswith identical uni-gram distributions but maximally divergent n-grams frequencies . Althoughmodels are often tested on a single type of generalization, we believe evaluating a model across abroad array of distributional shifts is essential for characterizing the robustness of its generalizationperformance.",
  "Performance Regression (ActiveLogical)": "ActiveLogical is a tree2tree task containing input and output trees in active voice and logicalforms . Transforming a tree in active voice to its logical form simulates semantic parsing, andtransforming a logical form tree to active voice simulates natural language generation. For thisdataset, there are three test sets: IID, 0-shot lexical, and structural. In addition to the baselines listedin the previous section, we also compare our modified sDTM to the original DTM. This enables us toconfirm that our proposed changes to decrease parameter count and memory usage while increasinginference speed does not lead to a performance regression. The results are show in .",
  "NQG uses pre-trained BERT embeddings ; it is unknown how much this pre-training helps the method": ": ActiveLogical accuracy. Results are the best performance over five runs. The test setsare divided into IID, and OOD sets 0-shot lexical and structural. Parameter and memory usage isshown for the original DTM with TPRs and our proposed sparse DTM with and without pruning. Ourmodifications reduce the parameter count by almost two orders of magnitude. NQG was trained ona seq2seq version without parantheses because it was not able to learn the tree2tree training set.",
  "Original DTM1.01.01.072M12.334sDTM1.01.01.01M9.72.5sDTM (pruned k=1024)1.01.0.951M1.81": "The various DTM models and Transformers all perform perfectly on the IID test set. NQG struggles tolearn the ActiveLogical task, an example of the brittleness of hybrid neurosymbolic systems. Onlythe DTM variants succeed on the OOD test sets. As anticipated, the RU-Transformer performs betterthan the standard Transformer with regards to structural generalization. Comparing the original DTM to sDTM without pruning, we see a 70x reduction in parameter count frompooling by attention, a 20% reduction in memory usage from fewer parameters and SCT, as well as aroughly 13x speedup. We are able to gain even further memory savings and speed improvements dueto the pruning method. The final two rows show that the pruning method has no impact on lexicalgeneralization and a minor impact on structural generalization, while reducing memory usage by5x and improving speed by 2.5x. The results from this experiment confirm that sDTM is capable ofmatching DTM performance on a previous baseline. However, since both DTM and sDTM perform nearceiling, it is difficult to isolate the effect of the proposed changes in this paper. We will investigatethis question further in .5. Next, we turn to tasks where the original DTM could not be used.",
  "Scalability (FOR2LAM)": "FOR2LAM is a tree2tree program translation task to translate an abstract syntax tree (AST) in animperative language (FOR) to an AST in a functional language (LAM) . Due to the depth of thetrees in this dataset, DTM is unable to fit a batch size of 1 into memory. This makes FOR2LAM agood dataset to test the scalability of sDTM to more complex samples. We augment the FOR2LAMdataset with a 0-shot lexical test set. During training, only two variable names appear: x and y.For the 0-shot test, we replace all occurrences of x in the test set with a new token z. We are unableto test DTM on FOR2LAM because a batch size of 1 does not fit into memory due to the depth of thetrees in the dataset. Results on FOR2LAM are shown on the left side of . NQG suffers with scale (see A.8), andwe were unable to include results for it on FOR2LAM due to training and evaluation exceeding 7 days.All other models do well on the in-distribution test set, but only DTM is able to achieve substantiveaccuracy on the 0-shot lexical test. DTMs performance is impressive given work on data augmentationhas shown the difficulty of few-shot generalization is inversely proportional to vocabulary size ,with smaller vocabulary tasks being more challenging. This 0-shot challenge is from 2 variables (x,y) to 3 (x, y, z), making it difficult enough that both transformer variants score 3%.",
  "GeoQuery is a natural language to SQL dataset where a model needs to map a question statedin natural language to a correctly formatted SQL query, including parentheses to mark functions": ": Accuracies on FOR2LAM and GeoQuery. Results are the best performance over fiveruns. NQG cannot be evaluated on FOR2LAM because it takes over a week to train. Results takenfrom Shaw et al. . We report the results from a replication study of NQG where the result on theLength split differed substantially from the original result .",
  "sDTM1.0.61.73.20.20.36": "and arguments. We use the parentheses and function argument relationship as the tree structure forour output. In this format, GeoQuery is a seq2tree task, and we follow the description from .5. We use the same preprocessing and data as Shaw et al. . The TMCD split for GeoQuery extends MCD to natural language datasets instead of synthetic languages. GeoQuery is a verysmall dataset, with a training set containing between 440 and 600 samples, depending on the split.Like FOR2LAM, we are unable to test DTM on GeoQuery because a batch size of 1 does not fit intomemory due to the depth of the trees in the dataset. Results for GeoQuery are shown on the right side of . This is the most difficult task that wetest because of the small training set, and the natural language input is not draw from a syntheticgrammar. Given this, a potential symbolic solution to this task might be quite complex. We find thatboth NQG and DTM perform worse than the two Transformer variants on the IID test set. This alsoholds true for the Template split, where Transformers outperform the neurosymbolic models. Onthe Length and TMCD splits, all of the baselines achieve roughly the same performance while DTMperforms slightly worse the degree of variation in the input space and small training set appear tomake it difficult for sDTM to find a compositional solution. It is worth noting that there is substantial room for improvement across every model on GeoQuery.The small dataset with high variation poses a problem for both compositional methods of sDTM andNQG. It is possible that with sufficient data, GeoQuerys latent compositional structure could beidentified by NQG and DTM, but the released GeoQuery dataset has only on the order of 500 trainingexamples. Given all methods struggle to model the IID split, we refrain from drawing substantiveconclusions based on minor differences in accuracy on this single task in isolation from the rest ofthe results.",
  "Seq2Seq (SCAN)": "SCAN is a synthetic seq2seq task with training and test variations to examine out-of-distributiongeneralization . To process seq2seq samples, we follow the description in .5. Wecompare two methods for embedding the output sequence into a tree by writing a parser for SCANsoutput and comparing this to the left-aligned uniform-depth trees (LAUD). In addition to the standardtest splits from SCAN, we introduce a 0-shot lexical test set as well. Since the trees in SCAN are not very deep, we are able to compare sDTM to DTM to isolate the effectof pooling by attention (4.2). We modify the original DTM to handle sequential inputs and outputsas described in .5. Replacing the linear transformation in DTM with pooling by attention insDTM leads to drastically better results; DTM is unable to perform well even on the simple IID split,whereas sDTM performs well across many of the splits. All baselines perform well on the IID test set, showing that they have learned the training distributionwell. Transformer variants perform poorly on lexical, length, and MCD splits. The Transformers andsDTM perform well on the Template split while NQG completely fails. Along with the results fromGeoQuery, which showed weak sDTM performance on the Template split and strong performancefrom both Transformers, it seems that the Transformer architecture is robust under template shiftsbetween training and testing. sDTM is the only model to perform well on the 0-shot lexical test set,whereas NQG is the only model able to perform well on the MCD test set. The two sDTM rows",
  "sDTM (parse trees)1.0.99.99.75.95.03sDTM (LAUD trees)1.0.87.98.06.980.0DTM (parse trees)0.00.00.00.00.00.0": "compare models trained with output trees from a parser or LAUD encoding. The main performancedifference is on the Length split, where the structurally relevant information in the parse trees isnecessary for sDTM to perform well. It is not necessary to have structured input for the model toperform well on length generalization as long as the output is structured.",
  "Conclusions": "We introduced the Sparse Differentiable Tree Machine (sDTM) and a novel schema for efficientlyrepresenting trees in vector space: Sparse Coordinate Trees (SCT). Unlike the fully neural and hybridneurosymbolic baselines presented here, sDTM takes a unified approach whereby symbolic operationsoccur in vector space. While not perfect sDTM struggles with MCD and Template shifts, as well asthe extremely small GeoQuery dataset the model generalizes robustly across the widest variety ofdistributional shifts. sDTM is also uniquely capable of zero-shot lexical generalization, likely enabledby its factorization of content and structure. While these capacities for generalization are shared with the original DTM, our instantiation iscomputationally efficient (representing a 75x reduction in parameters) and can be applied to seq2seq,seq2tree, and tree2tree tasks. Our work reaffirms the ability of neurosymbolic approaches to bridgethe flexibility of connectionist models with the generalization of symbolic systems. We believecontinued focus on efficient neurosymbolic implementations can lead to architectures with the kindsof robust generalization, scalability, and flexibility characteristic of human intelligence. Jacob Andreas. Good-Enough Compositional Data Augmentation. In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics, pages 75567566, Online,2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.676. URL",
  "Henry Conklin, Bailin Wang, Kenny Smith, and Ivan Titov. Meta-learning to compositionallygeneralize. arXiv preprint arXiv:2106.04252, 2021": "Rbert Csords, Kazuki Irie, and Juergen Schmidhuber. The devil is in the detail: Simple tricksimprove systematic generalization of transformers. In Marie-Francine Moens, Xuanjing Huang,Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on EmpiricalMethods in Natural Language Processing, pages 619634, Online and Punta Cana, DominicanRepublic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.49. URL",
  "Fernando Cuetos, Don C Mitchell, and Martin MB Corley. Parsing in different languages. InLanguage processing in Spanish, pages 163208. Psychology Press, 2013": "Lennert De Smet, Pedro Zuidberg Dos Martires, Robin Manhaeve, Giuseppe Marra, An-gelika Kimmig, and Luc De Readt.Neural probabilistic logic programming in discrete-continuous domains.In Robin J. Evans and Ilya Shpitser, editors, Proceedings of theThirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceed-ings of Machine Learning Research, pages 529538. PMLR, 31 Jul04 Aug 2023. URL Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training ofDeep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs], May2019. URL arXiv: 1810.04805.",
  "Artur SDAvila Garcez, Luis C Lamb, and Dov M Gabbay. Neural-symbolic cognitive reasoning.Springer Science & Business Media, 2008": "Marta Garnelo and Murray Shanahan. Reconciling deep learning with symbolic artificialintelligence: representing objects and relations. Current Opinion in Behavioral Sciences, 29:1723, 2019. ISSN 2352-1546. doi: URL Arti-ficial Intelligence. Ross W Gayler. Vector symbolic architectures answer jackendoffs challenges for cognitiveneuroscience. In Peter Slezak, editor, Proceedings of the ICCS/ASCS Joint InternationalConference on Cognitive Science (ICCS/ASCS 2003), pages 133138, Sydney, NSW, AU, jul2003. University of New South Wales. URL",
  "Pentti Kanerva. Hyperdimensional computing: An introduction to computing in distributedrepresentation with high-dimensional random vectors. Cognitive computation, 1:139159, 2009": "Daniel Keysers, Nathanael Schrli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashu-bin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov,Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: Acomprehensive method on realistic data. In International Conference on Learning Representa-tions, 2020. URL Najoung Kim and Tal Linzen. COGS: A compositional generalization challenge based onsemantic interpretation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing(EMNLP), pages 90879105, Online, November 2020. Association for Computational Lin-guistics. doi: 10.18653/v1/2020.emnlp-main.731. URL",
  "Yoon Kim, Chris Dyer, and Alexander M Rush. Compound probabilistic context-free grammarsfor grammar induction. arXiv preprint arXiv:1906.10225, 2019": "Dan Klein and Christopher D Manning. A generative constituent-context model for improvedgrammar induction. In Proceedings of the 40th Annual Meeting of the Association for Computa-tional Linguistics, pages 128135, 2002. Denis Kleyko, Dmitri A. Rachkovskij, Evgeny Osipov, and Abbas Rahimi. A survey onhyperdimensional computing aka vector symbolic architectures, part i: Models and data trans-formations. ACM Comput. Surv., 55(6), dec 2022. ISSN 0360-0300. doi: 10.1145/3538531.URL Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositionalskills of sequence-to-sequence recurrent networks. 35th International Conference on MachineLearning, ICML 2018, 7:44874499, 2018. arXiv: 1711.00350 ISBN: 9781510867963.",
  "Arkil Patel, Satwik Bhattamishra, Phil Blunsom, and Navin Goyal. Revisiting the compositionalgeneralization abilities of neural sequence models. arXiv preprint arXiv:2203.07402, 2022": "Steven Pinker. The language instinct: How the mind creates language. Penguin uK, 2003. Tony A. Plate. Holographic Reduced Representation: Distributed Representation for CognitiveStructures. CSLI Publications, USA, 2003. ISBN 1575864290. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435. Tim Rocktschel and Sebastian Riedel. Learning knowledge base inference with neural theoremprovers. In Jay Pujara, Tim Rocktaschel, Danqi Chen, and Sameer Singh, editors, Proceedingsof the 5th Workshop on Automated Knowledge Base Construction, pages 4550, San Diego,CA, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/W16-1309. URL Anian Ruoss, Grgoire Deltang, Tim Genewein, Jordi Grau-Moya, Rbert Csords, MehdiBennani, Shane Legg, and Joel Veness. Randomized positional encodings boost length gen-eralization of transformers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, ed-itors, Proceedings of the 61st Annual Meeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 18891903, Toronto, Canada, July 2023. Associ-ation for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.161. URL",
  "Itiroo Sakai. Syntax in universal translation. In Proceedings of the International Conference onMachine Translation and Applied Language Analysis, 1961": "Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositionalgeneralization and natural language variation: Can a semantic parsing approach handle both?In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59thAnnual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 922938,Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.75. URL Hikaru Shindo, Masaaki Nishino, and Akihiro Yamamoto. Differentiable inductive logicprogramming for structured examples. Proceedings of the AAAI Conference on ArtificialIntelligence, 35(6):50345041, May 2021. doi: 10.1609/aaai.v35i6.16637. URL Vighnesh Shiv and Chris Quirk. Novel positional encodings to enable tree-based transform-ers. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett,editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates,Inc., 2019. URL",
  "Paul Smolensky. Tensor product variable binding and the representation of symbolic structuresin connectionist systems. Artif. Intell., 46:159216, 1990": "Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic com-positionality through recursive matrix-vector spaces. In Junichi Tsujii, James Henderson,and Marius Pasca, editors, Proceedings of the 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Computational Natural Language Learning, pages12011211, Jeju Island, Korea, July 2012. Association for Computational Linguistics. URL",
  "Yau-Shian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating treestructures into self-attention. arXiv preprint arXiv:1909.06639, 2019": "Thomas Winters, Giuseppe Marra, Robin Manhaeve, and Luc De Raedt. Deepstochlog: Neuralstochastic logic programming. Proceedings of the AAAI Conference on Artificial Intelligence,36(9):1009010100, Jun. 2022. doi: 10.1609/aaai.v36i9.21248. URL Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.In International Conference on Machine Learning, pages 1052410533. PMLR, 2020. Zhun Yang, Adam Ishay, and Joohyung Lee. Neurasp: Embracing neural networks into answerset programming. In Christian Bessiere, editor, Proceedings of the Twenty-Ninth InternationalJoint Conference on Artificial Intelligence, IJCAI-20, pages 17551762. International JointConferences on Artificial Intelligence Organization, 7 2020. doi: 10.24963/ijcai.2020/243. URL Main track. Dani Yogatama, Yishu Miao, Gabor Melis, Wang Ling, Adhiguna Kuncoro, Chris Dyer, and PhilBlunsom. Memory architectures in recurrent neural network language models. In InternationalConference on Learning Representations, 2018. URL",
  "A.1Sparse Coordinate Trees as Tensor Product Representations": "This section shows that Sparse Coordinate Trees is the same as a TPR with the constraint that the rolebasis is the standard basis. TPRs define structural positions as role vectors ri Rdr, and the contentthat fills these positions is defined by filler vectors fi Rdf . For a particular role and filler pair, thefiller fi is bound to the role ri using the tensor/outer product: fi ri Rdf dr. The representationof an entire structure is the sum over all N individual filler-role pairs: T = Ni=1 fi ri Rdf dr.As shown in the previous two equations, the dimensionality of a single filler-role pair is equal tothe dimensionality of an entire structure: both have dimensionality Rdf dr. This means that a treewith only a filled root node takes up the same memory as a dense tree with every node filled. Animportant requirement for TPRs is that the role vectors must be linearly independent; this ensuresthat a filler can be unbound from a role without introducing noise using the inner product: fj = Tr+j ,where {r+i }i is the basis dual to {ri}i. Previous work typically used randomly initialized and frozenorthonormal vectors to define the role basis. By defining our role vectors in a sparse manner asopposed to random initialization, we can greatly reduce the memory used by TPRs. Classic symbolic data structures grow in memory linearly with the number of filled positions. It ispossible to replicate this behavior with TPRs by defining the role vectors to be the standard one-hotbasis, which is orthonormal by definition. The i-th element of role vector ri is 1, and the otherelements are 0. When a filler and role vector are both dense, the resulting bound vector is alsodense. When the role vector is one-hot, the resulting bound vector is 0 everywhere except for columni which corresponds to the value 1 in ri. By using a sparse tensor representation that only keepstrack of dimensions that are not equal to 0, we can reduce the memory usage of TPRs to lineargrowth that scales with the number of filled positions, like a classical symbolic data structure. Thishowever forgoes a motivating desideratum for the design of TPRs, that roles (and not just fillers) havesimilarity relations that support generalization across structural positions. We can additionally improve the efficiency by refraining from performing the outer product. Since weare not performing a tensor product, this technique is only implicitly a Tensor Product Representation.Instead, we can keep the filler and role vectors in two aligned lists. A filler is bound to a role bysharing an index in our aligned lists. This is equivalent to the binding and unbinding from classicaldense TPRs without having to perform multiplication. Since we are not performing an outer product, instead of storing sparse role vectors, we can simplystore a role integer, where the integer corresponds to the one-hot dimension. We derive a treeaddressing scheme based on Gorn addresses . In our scheme, addresses are read from right to left,giving the path from the root where a left-branch is indicated by a 0 and a right-branch is indicatedby a 1. We need a way to distinguish between leading 0s and left-branches (e.g., 010 vs. 10), so westart our addressing scheme at 1 instead of 0. This indicates that all 0s to the left of the left-most 1are unfilled and not left-branches; the left-most 1 and all preceding 0s are ignored when decoding thepath-from-root. shows an example encoding of a tree in the sparse implicit approach. We can compare the memory requirements of the Sparse Coordinate Tree encoding used in the sDTMto the memory requirements of the full TPRs used in the original DTM of Soulos et al. . A TPRuses the same amount of memory regardless of the number of filled nodes. As with all sparse tensorformats, the memory savings arise when there are many zeros. In a dense tree where every node isoccupied, the classical dense TPR approach is actually more efficient: the SCTs value list has thesame total dimension as the classical TPR, but, in addition, the SCT encoding includes the list offilled-node addresses.",
  "Softmax": ": Adapted from Soulos et al. . One step of DTM is expanded to show how the agentproduces the input to the interpreter. The interpreter then writes the output to memory and encodes theoutput for the agent. Parts of the architecture with learnable parameters are indicated in yellow. Theagent uses three linear transformations on top of a standard Transformer encoder layer to parameterizethe inputs to the interpreter. The superscript indicates the layer number and refers to parameters andactivations that are exclusive to this layer.",
  "A.4Dataset Preprocessing": "We preprocessed GeoQuery according to the steps from Shaw et al. . FOR2LAM and GeoQueryboth contain non-binary trees, which we convert to binary form using Chomsky normal form. Whena new node is inserted to make a branch binary, we use the token <NT>. For output sequences withlength one embedded according to left-aligned uniform-depth, we make the single token the left childof a new <NT> root node.",
  "A.50-shot Lexical Test Generation": "For both FOR2LAM and SCAN, we introduce 0-shot lexical tests. For FOR2LAM, we do this byreplacing every occurrence of x in the test set with a new token z. For the SCAN 0-shot set, westart with the 1-shot lexical test set and remove the sample containing the 1-shot word jump. Wealter the output vocabulary to use the same tokens as the input vocabulary, since it is impossible for aword level model to translate between an input and output word without any exposure to that word.",
  "When applicable, we adopt the hyperparameters from Soulos et al. . Below we list the newlyintroduced hyperparameters and changes we made to existing parameters": "Soulos et al. set the dimensionality of the embeddings to be equal to the size of vocabulary. Thisworks for the datasets with small vocabulary examined in the original paper. We keep this setting forActiveLogical, but set the embedding dimension to 64 for FOR2LAM, and 128 for GeoQuery andSCAN. We also changed the loss function from mean-squared error to cross entropy.",
  "sDTM1.00.0.43.11.68.01.19.01.16.03.350.0": "For each new task, we need to decide how many layers to use for sDTM. We followed the heuristicof doubling the max tree depth for the models with sequence input and quadrupling the number oflayers for tree input. This leads to 56 layers for FOR2LAM, 22 layers for GeoQuery, and 14 layersfor SCAN. Pooling by multi-headed attention 4.2 introduces new hyperparameters such as number of poolingheads and pooling key dimensionality, and we set the value of these to be the same as the Transformerhyperparameters for the agent. Tree pruning 4.3 introduces a new hyperparameter k for the maximumnumber of nodes to keep. In general, a larger k is better but uses more memory. For ActiveLogicalwe set k = 1024, for FOR2LAM k = 1024, for GeoQuery k = 2048, and for SCAN k = 256.With the memory savings from SCT, pooling by multi-headed attention, and pruning, we increasethe batch size from 16 to 64. We also increased the agents model dimension to 256 with 8 headsof attention due to the memory savings except for ActiveLogical where we matched the originalhyperparameters. Random positional embeddings (RPE) also introduce a new hyperparameter for the max input integer,and we set this to be double the max input length. This leads to an RPE hyperparameter of 44 forGeoQuery and 18 for SCAN.",
  "We noticed that randomly initializing and freezing our embedding vectors was essential for sDTM toachieve 0-shot generalization on SCAN": "For the results, we reported the best run of 5 random seeds. Like DTM, sDTM suffers from highvariance. Some runs get stuck in local optima and fail to achieve moderate performance on thetraining set, which leads to poor performance on the test sets. This is a known issue with models thatuse superposition data structures, and reporting the best run over a number of random seeds has beenpreviously used .",
  "A.8Baseline Training Details": "NQG: ActiveLogical rule induction used the following hyperparameters: sample size=trainingset size, terminal code length=8, allow repeated nts=True. The terminal code length setting wasobtained via grid search over the values 1, 8, 32. For the actual training of the model we follow thehyperparameters utilised by . FOR2LAM used the same hyperparameters with the exceptionof sample size which had to be set to 1000 as additional increases became computationally intractable.",
  ": An input and output pair from ActiveLogical": "Even under these settings rule induction took 42 hours on a machine with 64gb of ram. Writing thetraining set would take an additional week of processing time, which we considered computationallytoo expensive. Transformer: We followed the same hyperparameters obtained via grid search from . Specificallythese are: 30,000 steps of which 1000 were warmup and linear learning rate decay; batch size 256;one encoder layer and three decoder layer each with a hidden dimension of 1024 and two attentionheads; the optimizer was Adam. RU-Transformer: We followed the hyperparameters reported by . These are: 128 dimensionhidden size with 256 feedforward; 8 attention heads; 3 layers; batch size 256; trained using Adamwith learning rate 103.",
  "A.9Compute resources": "All reported sDTM runs could be processed on NVIDIA 16gb V100 GPUs. Depending on availability,we ran some seeds on 80gb H100 GPUs, but this is not necessary. The Transformer baselines werealso run on NVIDIA 16gb V100 GPUs. NQG used NVIDIA 40gb A100 GPUs. The GPUs we usedwere hosted on an internal cluster.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "The answer NA means that the paper does not include experiments": "The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: In Appendix A.9 we discuss the compute resources needed to run our experi-ments.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: This research is upstream of any concrete applications. While there is alwaysa risk of scientific research, we do not believe this work contains risks beyond generalscientific research.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: we plan to make our code and data open source at the time of publication tofacilitate reproducibility.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}