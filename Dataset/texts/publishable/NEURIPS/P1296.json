{
  "Abstract": "The ongoing revolution in language modelling has led to var-ious novel applications, some of which rely on the emergingsocial abilities of large language models (LLMs). Already,many turn to the new cyber friends for advice during piv-otal moments of their lives and trust them with their deepestsecrets, implying that accurate shaping of LLMs person-alities is paramount. Leveraging the vast diversity of dataon which LLMs are pretrained, state-of-the-art approachesprompt them to adopt a particular personality. We ask (i) ifpersonality-prompted models behave (i.e., make decisionswhen presented with a social situation) in line with the as-cribed personality, and (ii) if their behavior can be finely con-trolled. We use classic psychological experimentsthe Mil-gram Experiment and the Ultimatum Gameas social in-teraction testbeds and apply personality prompting to GPT-3.5/4/4o-mini/4o. Our experiments reveal failure modes ofthe prompt-based modulation of the models behavior, thuschallenging the feasibility of personality prompting with to-days LLMs. IntroductionWith both start-ups (Character.ai,1 Replika2) and industrygiants (Snapchat,3 Meta4) providing digital friends formillions of users, an accurate shaping of the underlyingmodels personalities is no longer the subject of sci-fi nov-els. Just as in real human-to-human interaction, there is noone size fits all personality bound to match with every-one. Hence, agents should be tailored to the needs of eachuser, i.e., their behavior should be alterable in a controllableway. The requirements for personalized AI-powered assis-tants will grow more strict as Large Language Models reachincreasingly wider audiences and domains.Although several studies examine the possibility ofprompt-driven personality induction in LLMs and claim suc-cess (Jiang et al. 2023b; Serapio-Garca et al. 2023; Jianget al. 2023a), the methods used to evaluate personalized",
  "*These authors contributed equally. The order of the authors israndom.1": "models are detached from the real-life use cases (in case ofpsychological questionnaires administered to the model) orrely on the style of the generated text or leverage intrinsicallyquantitative human assessment. We argue that any test designed to assess the models per-sonality should be put in perspective with the considereduse cases, e.g. while a personality-prompted model mightbe shown to answer consistently to simple questions such asAre you helpful and unselfish with others or Do you liketo cooperate with others, there are no guarantees that it willbe a tough negotiator unless explicitly tested. Moreover, justlike we do not qualitatively assess LLMs math capabilitiesand instead compute the accuracy of the model-provided so-lutions, we advocate for a quantitative benchmark, allowingfor personality-prompted model behavior assessment. Following Aher, Arriaga, and Kalai (2023), we employUltimatum Game (UG; targets tolerance to unfair offers),and Milgram Experiment (ME, reflects obedience to au-thority) as the social interaction benchmarks. We note thatboth benchmarks allow for (i) quantitative behavior assess-ment and (ii) comparison with human data, as we know howthe personality of the human participant relates to the behav-ior in these experiments (Mehta 2007; B`egue et al. 2015). Tothis end, we conduct 4 case studies, varying agreeableness oropenness in UG; agreeableness or consciousness in ME.",
  "Related workPersonality in Large Language Models": "Drawing on the personality assessment methodology, sev-eral studies (Jiang et al. 2023a; Serapio-Garca et al. 2023;Sorokovikova et al. 2024) probe LLMs with the question-naires designed for BIG-5 traits assessment5, and show thatstable personality emerges in the most capable models, e.g.GPT-3.5 (Jiang et al. 2023a) and Flan-PaLM 540B (Serapio-Garca et al. 2023).Following that observation, Mao et al. (2024) suggestediting the personality of the model, while (Jiang et al.2023a,b; Serapio-Garca et al. 2023) induce desired person-ality with a carefully crafted prompt. The latter approach isespecially appealing, given the cutting-edge models black-box nature and the ability to switch between various person-alities with no fine-tuning incurred computational overhead.Regarding subsequent validation, various papers extendbeyond questionnaires and propose more elaborate waysto test personality-prompted models. Serapio-Garca et al.(2023) generate social media updates, which are then ana-lyzed with the Apply Magic Sauce API6, providing a BIG-5 score corresponding to each update. Jiang et al. (2023b)request a personal story and evaluate the response with (i)Linguistic Inquiry and Word Count (LIWC) analysis7, (ii)human evaluation, (iii) LLM evaluation.In our view, Jiang et al. (2023a) provides a better proxyfor real-life use cases, since the model, tasked with writingan essay, is conditioned on a particular social setup. Eachessay is then human labeled for positive, negative, or neu-tral induction of each of BIG-5 traits. Human evaluation is,however, intrinsically qualitative and can be influenced bythe writing style, instead of being purely content-dependent;the latter holds for the linguistic-based assessment methodsas well. Besides, only extremes of each trait are induced,leaving the fine-grained trait tuning out of the scope.Noh and Chang (2024) consider various negotiations be-tween the agents prompted by the extremes of the BIG-5traits. Their focus is very different from ours, though, withno attempt to tune the behavior or ground the results in thehuman data. While we seek to test the alignment of thedemonstrated behavior with the expected one, they empir-ically study the way that LLMs encode definitions of thetraits reflected in their subsequent behavior, focusing onthe optimal negotiation performance.",
  "Behavioral Experiments for humans and LLMs": "With no relation to personality prompting, Aher, Arriaga,and Kalai (2023) successfully replicated the results of vari-ous behavioral experiments, including the Milgram Exper-iment (ME) and the Ultimatum Game (UG), by present-ing these experiments to a silicon population of LLM in-stances conditioned on different names (a name correspondsto a single silicon sample).",
  "BIG-5 or OCEAN traits include Openness, Consciousness, Ex-traversion, Agreeableness, and Neuroticism6": "We know from psychology research that (i) in UG, Agree-ableness and Openness are positively and significantly (p <0.05) correlated with accepting an unfair offer (Mehta 2007)(ii) in ME, the intensity of the shock delivered is positivelyand significantly (p < 0.05) correlated with both Conscien-tiousness and Agreeableness (B`egue et al. 2015).",
  "We ascribe personality characteristics according to theassigned score of the trait (varies from 1 to 9), followingSerapio-Garca et al. (2023), where the personality string isshaped as follows:": "1. extremely {low adjective 1}, ..., extremely { low adjec-tive N}2. very {low adjective 1}, ..., very {low adjective N}3. {low adjective 1}, ..., {low adjective N}4. a bit {low adjective 1}, ..., a bit {low adjective N}5. neither {low adjective 1} nor {high adjective 1}, ...,neither {low adjective N} nor {high adjective N}6. a bit {high adjective 1}, ..., a bit {high adjective N}7. {high adjective 1}, ..., {high adjective N}8. very {high adjective 1}, ..., very {high adjective N}9. extremely {high adjective 1}, ..., extremely { highadjective N}. An adjective is a marker corresponding to the Big Fivetrait being shaped, drawn from the psychological literature(Goldberg 1992). We choose this particular prompting strat-egy, as Serapio-Garca et al. (2023) shows it leads to person-ality traits in LLMs being successfully shaped according tothe applied evaluation technique. Noh and Chang (2024) usea similar prompting approach, while the approach of Jianget al. (2023b) might be considered a simplified version ofthe previous two.",
  "MethodologyUltimatum Game (UG)": "InUltimatumGame(Guth,Schmittberger,andSchwarze 1982), the proposer is given $10, and hasto decide on the amount to be shared with the responder,who, in turn, might agree or block the deal (see ). Weshape various responders, varying levels of Agreeablenesand Openness from 1 to 9. For each character of theresponder, we run the simulation 50 times and measure theprobability that the offer is accepted, depending on its value.We try to retrieve an answer in the valid form (rejector accept) 3 times and filter out the cases with no validanswer elucidated 373 out of 25300 cases for GPT-4, 1case for GPT-4o, 0 cases for GPT-4o, GPT-4o-mini.Personality shaping is performed with the prompt format-ted as For the following task, imagine that you are a personand you are: <personality characteristic>.",
  ": Milgram Experiment flow chart (Aher, Arriaga,and Kalai 2023)": "punish the learner for every wrong answer with a graduallygrowing electric shock.The setup consists of the story-like blocks (see ). Wemodify the (5) Stop? and the (6) Obey? steps due to theunavailability of log probabilities Aher, Arriaga, and Kalai(2023) measure probabilities of not stop and not obeyresponses, respectively. Instead, we rely on the model actingas a judge, kept in the story-like scenario.Personality is added to the block (1) Add Intro Text inthe form of Teachers personality traits are: <personalitycharacteristic string>.We measure the final level at which the model decidesto withdraw from the experiment (, 8), as well as thecumulative number of disobediences in different runs (). Due to budget constraints, we limit the set of consideredpersonalities to the extremes of Agreeableness and Consci-entiousness and perform 50 runs for each personality.Unlike Aher, Arriaga, and Kalai (2023), we do not con-dition the model on the participants name, as we are solelyinterested in the effect of the personality prompt. In contrast,the use of names may introduce a confounder8. We, there-",
  ": Milgram Experiment: percentage of subjects re-maining at each step of the experiment with original Mil-gram setup": "fore, use a naming scheme of the experimenter - The Ex-perimenter, the teacher The Teacher, and the learner The Learner for each experiment run.We note that the third-person naming scheme allows usto discard data leakage concerns, i.e., even if ME-relateddata was encountered on the pretraining stage (which is mostprobably the case), we elucidate an LLMs internal modelof how The Teacher of a given personality would behave,not the psychology papers grounded opinion on what themorally right behavior is. This reasoning is solidified by theobservation that, according to the experiments described be-low, teachers of a certain personality do not withdraw.However, this setup still involves an inherent limitationof the LLM-based systems randomness. There are two po-tential points of failure: narration-following in block (4) AddLM Text, and known imperfect judge behavior (Zheng et al.2023) in blocks (5) Stop?, and (6) Obey?. To address these",
  "gpt-3.5-turbo-0613 (GPT-3.5) gpt-4-turbo-2024-04-09 (GPT-4) gpt-4o-mini-2024-07-18 (GPT-4o-mini) gpt-4o-2024-05-13 (GPT-4o)": "In the case of Milgrams Experiment, we decided to dropresults for both GPT-3.5 and GPT-4o-mini. All 50 runs ofbaseline GPT-4o-mini experiments were filtered due to un-expected response when the model was asked to act as ajudge in blocks (5) Stop?, and (6) Obey?. We also encoun-tered this problem, on a smaller scale, with GPT-4o. GPT-3.5struggled to follow the story-like narration while generatingcompletions in block (4) Add LM Text. Interestingly, GPT-4",
  "did not struggle with any of the above. The detailed numberof filtered runs is presented in": "Results and DiscussionBaselineTo set the baseline for personality-induced behavior, we runUG and ME with no personality specified. In UG, GPT-3.5 ismore likely to reject the deal compared to the average acrossthe human population (except for the case of a 0 offer), whileGPT-4 shows the opposite behavior. Although GPT-4o andGPT-4o-mini are more closely aligned with human studies,the transition between the model predominantly acceptingand rejecting an offer is more sharp with the acceptance rate0 for Offer 2 and the acceptance rate 1 for Offer 4 ().In ME, vanilla GPT-4 is more obedient than the humanaverage and follows the protocol of the experiment, whileGPT-4o tends to withdraw early ().We note that in both UG and ME, results of Aher, Arriaga,and Kalai (2023) are much better aligned with the results of 0.4 0.2 0.0 0.2 0.4",
  ": Ultimatum Game: R2 of the AR(trait) regressionsfor various values of the offer": "The general trend in the i values characterizes the rela-tionship between an induced trait and behavior (RQ1), whilethe consistency of this trend is related to RQ2, i.e. our abilityto enhance a certain behavior via prompting the correspond-ing trait with greater intensity.Surprisingly, while we observe the upward trend in thecase of Agreeableness, it is downward for Openness for allthe models considered, suggesting that a more open modelis more prone to reject an offer, which opposes human data(Mehta 2007). Moreover, i progression is not monotonicfor any combination of the trait and the model, except forGPT-4, which is now obsolete (e.g. GPT-4o-mini, agree-ableness, 5 to 7 progression; GPT-4o, openness, 1 to 2 pro-gression). These observations suggest negative answers toboth RQ1 and RQ2, i.e. neither the human-aligned behav-ioral trend nor this trend being monotonic is guaranteed.To provide a more detailed analysis of the models steer-ability for the particular offers, we compute AcceptanceRate AR(trait) regressions and present the correspondingR2 coefficients in . In case of agreeableness, we ob-serve R2 < 0.6 for the lower offers: 0, 1, 2 (GPT-3.5); 0(GPT-4); 1, 2 (GPT-4o-mini); 0 (GPT-4o), suggesting lowersteerability in these cases either AR(trait) dependencyis not monotonic (GPT-3.5, agreeableness, 0 offer), or ARsurges/collapses at a certain trait value (GPT-4, agreeable-ness, 0 offer). Shock voltage [V] Subjects remaining [%] GPT-4 Agreableness minAgreableness maxConscientiousness minConscientiousness maxBaseline Shock voltage [V] GPT-4o Percentage of subjects remaining in the Milgram experiment (personality shaped)",
  "Milgram Experiment": "From B`egue et al. (2015), we know that both Agreeble-ness and Consciousness are significantly associated with thewillingness to administer higher-intensity shocks. While thereal-life trend does hold for Consciousness, it is on the bor-derline of statistical significance for GPT-4 (Welchs t-test,used throughout this section, yields p = 0.06 for GPT-4 andp = 0.01 for GPT-4o).In the case of Agreebleness, the results of our simulationdrastically oppose human data (, ). While low-agreeable samples almost never withdraw from the exper-iment, high-agreeable samples withdraw much earlier thanpersonality-neutral samples (p 0.001), the trend beingeven more pronounced for GPT-4o. provides furtherinsight into the course of the simulation high-agreeablesamples disobey much more than the low-agreeable ones,even if not withdraw from the experiment altogether.Intuitively, Agreeableness acts as a proxy for how goodor evil the Teacher is. Teacher, when modelled as highlyagreeable, is interpreted by the model as highly good, show-ing less desire to obey, while hurting the Learner thus showing higher levels of disobedience. On the other hand,when we model the Teacher as least agreeable, it obeysblindly, showing no mercy for the suffering Learner. Thistrend is visible not only in both final levels achieved in eachexperiment run (), but also in the cumulative number ofdisobediences when the Teacher showed hesitation in con-tinuing the experiment (), and total number of disobe-diences ().GPT-4, and GPT-4o fail to align with the injected person-alities when put in the complicated social context of Mil-gram Experiment. This provides further evidence towardsthe negative answer to RQ1.",
  "Conclusion": "Recognizing the elegance of the personality prompting tech-nique (Serapio-Garca et al. 2023; Jiang et al. 2023a), weargue for the insufficiency of existing methods designed forthe evaluation of induced personality. To this end, we em-ploy 2 psychological experiments Milgram Experiment(ME) and Ultimatum game (UG) to quantitatively assessthe personality-induced LLMs behavior in a social setting. 02468 10 12 14 16 18 20 22 24 26 28 30 32 34 36",
  ": Milgram Experiment: Cumulative sum of disobediences per subject for minimal agreeableness and maximum agree-ableness, labeled by experiment level": "In the case of UG, we ascribe varying levels of Agree-ableness and Openness, while in the case of ME we varyAgreeableness and Consciousness. We observe that in 2 ofthese 4 experiments, the SOTA models behavior changesin the opposite direction from the human behavior, while inthe third one, the change in the behavior is statistically sig-nificant for GPT-4o and not GPT-4. Furthermore, UltimatumGame results suggest that even in the case of a significanthuman-aligned trend, the behavior does not change mono-tonically with the intensity of the trait. Our experiments reveal failure modes of personalityprompting and imply that one cannot expect personality-prompted LLM to exhibit the human-aligned behavior bydefault or even upon the model successfully passing per-sonality assessment tests and should rather design bench-marks directly related to the intended use cases.",
  "We acknowledge that the experiments considered are still aproxy for real-life social interactions, and the models mightbehave differently in other set-ups": "Moreover, truly aligning the agents behavior with that ofthe humans might be impossible under the current set-up ofsummoning agents for a brief conversation, as they shouldrather be allowed to persist in the world for a long time withlong-term goals and the prospect of pain and death. Aher, G.; Arriaga, R. I.; and Kalai, A. T. 2023. Using largelanguage models to simulate multiple humans and repli-cate human subject studies.In Proceedings of the 40thInternational Conference on Machine Learning, ICML23.JMLR.org.B`egue, L.; Beauvois, J. L.; Courbet, D.; Oberle, D.; Lepage,J. L.; and Duke, A. A. 2015. Personality predicts obediencein a Milgram paradigm. Journal of personality, 83 3: 299306.Goldberg, L. 1992. The Development of Markers For theBig Five Factor Structure. Psychological Assessment, 4: 2642.Guth, W.; Schmittberger, R.; and Schwarze, B. 1982. Anexperimental analysis of ultimatum bargaining. Journal ofEconomic Behavior & Organization, 3(4): 367388.Jiang, G.; Xu, M.; Zhu, S.-C.; Han, W.; Zhang, C.; and Zhu,Y. 2023a. Evaluating and Inducing Personality in Pre-trainedLanguage Models. In NeurIPS.Jiang, H.; Zhang, X.; Cao, X.; Kabbara, J.; and Roy, D.2023b. PersonaLLM: Investigating the Ability of GPT-3.5 toExpress Personality Traits and Gender Differences. ArXiv,abs/2305.02547.Mao, S.; Wang, X.; Wang, M.; Jiang, Y.; Xie, P.; Huang, F.;and Zhang, N. 2024. Editing Personality for Large LanguageModels. arXiv:2310.02168.Mehta, P. 2007. The Endocrinology of Personality, Lead- ership, and Economic Decision Making. Doctoral disserta-tion, The University of Texas at Austin, Austin, TX.Milgram, S. 1963. Behavioral study of obedience. The Jour-nal of abnormal and social psychology, 67(4): 371.Noh, S.; and Chang, H.-C. H. 2024. LLMs with Person-alities in Multi-issue Negotiation Games.arXiv preprintarXiv:2405.05248.Raad, B. 2000. The Big Five Personality Factors: The psyc-holexical approach to personality. ISBN 0-88937-236-5.Serapio-Garca, G.; Safdari, M.; Crepy, C.; Sun, L.; Fitz,S.; Romero, P.; Abdulhai, M.; Faust, A.; and Mataric,M. 2023.Personality Traits in Large Language Models.arXiv:2307.00184.Sorokovikova,A.;Fedorova,N.;Rezagholi,S.;andYamshchikov, I. P. 2024. LLMs Simulate Big Five Person-ality Traits: Further Evidence. arXiv:2402.01765.Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu,Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.;Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023.Judg-ing LLM-as-a-Judge with MT-Bench and Chatbot Arena.arXiv:2306.05685."
}