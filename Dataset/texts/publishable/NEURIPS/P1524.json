{
  "Abstract": "Adam is one of the most popular optimization algorithms in deep learning. However,it is known that Adam does not converge in theory unless choosing a hyperparame-ter, i.e., 2, in a problem-dependent manner. There have been many attempts to fixthe non-convergence (e.g., AMSGrad), but they require an impractical assumptionthat the gradient noise is uniformly bounded. In this paper, we propose a newadaptive gradient method named ADOPT, which achieves the optimal convergencerate of O(1/ T) with any choice of 2 without depending on the bounded noiseassumption. ADOPT addresses the non-convergence issue of Adam by removingthe current gradient from the second moment estimate and changing the orderof the momentum update and the normalization by the second moment estimate.We also conduct intensive numerical experiments, and verify that our ADOPTachieves superior results compared to Adam and its variants across a wide rangeof tasks, including image classification, generative modeling, natural languageprocessing, and deep reinforcement learning. The implementation is available at",
  "ft () =C,for t mod 3 = 1,otherwise,(1)": "where C > 2 and . In this online optimization setting, Adam converges to a wrongsolution (i.e., = 1) instead of the true solution (i.e., = 1) especially when the hyperparameter2 is set to a small value. There have been several attempts to fix the non-convergent behavior ofAdam [Reddi et al., 2018, Zou et al., 2019]. For example, AMSGrad [Reddi et al., 2018] ensures theconvergence for online convex optimization by making slight modifications to the Adam algorithm.Subsequent studies [Chen et al., 2019, Zhou et al., 2018] show that AMSGrad also converges to astationary point for smooth nonconvex stochastic optimization problems. However, the convergenceproofs rely on the assumption that the gradient noise is uniformly bounded. This assumption isstronger than the one used for the analysis of vanilla SGD [Ghadimi and Lan, 2013, Bertsekas andTsitsiklis, 2000, Khaled and Richtrik, 2023], where the gradient variance is assumed to be uniformlybounded. In fact, the bounded noise assumption is often violated in practice. For example, whenGaussian noise is used in the gradient estimation (e.g., variational autoencoders [Kingma and Welling,2014] and diffusion models [Ho et al., 2020, Song et al., 2021]), the stochastic gradient is no longerbounded. Concurrently, Zhou et al. analyze the non-convergence of Adam in the problem described inEq. (1) from the perspective of the correlation between the current gradient and the second momentestimate based on the exponential moving average. Specifically, they show that the non-convergenceproblem can be resolved by excluding the gradient of some recent steps from the calculation of thesecond moment estimate. Based on the analysis, they propose AdaShift, another variant of Adam.However, their theoretical analysis is limited to a single online convex problem described in Eq. (1),and the convergence of AdaShift for general nonconvex problems is unclear. More recently, some works have demonstrated that Adam can converge by choosing 2 in a problem-dependent manner [Shi et al., 2020, Zhang et al., 2022, Wang et al., 2022, Li et al., 2023, Wang et al.,2023]. However, tuning 2 for each specific problem is troublesome; hence developing algorithmswith the problem-independent convergence guarantee is still important to safely apply adaptivegradient methods to a wide range of machine learning problems. In this paper, we propose an alternative approach to addressing the non-convergence problem of Adamwithout relying on the choice of 2 or strong assumptions such as the bounded noise assumption.To derive our algorithm, we first examine the case without momentum, analyzing the convergencebound of RMSprop for general smooth nonconvex optimization problems. Through the analysis, weuncover the fundamental cause of non-convergence, which stems from the correlation between thesecond moment estimate and the current gradient. This finding aligns with the results demonstratedby Zhou et al. for online convex optimization. This correlation can be easily eliminated byexcluding the current gradient from the second moment estimate. Subsequently, we extend our findings to the case where momentum is incorporated, as in Adam,and discover that the Adam-style momentum also contributes to non-convergence. To address it, wepropose to change the order of the momentum update and the normalization by the second momentestimate. With this small adjustment, we successfully eliminate the non-convergence problem ofAdam without relying on a specific hyperparameter choice and the bounded noise assumption. Weprovide theoretical evidence demonstrating that our derived algorithm, named ADOPT, can achieveconvergence with the optimal rate of O(1/",
  "T) for smooth nonconvex optimization": "In our experiments, we begin by assessing the performance of ADOPT in a toy example where Adamtypically fails to converge depending on the choice of 2. This toy example is an extension of theone presented in Eq. (1) by Reddi et al. , but we consider a scenario where AMSGrad is alsohard to converge due to the dependence on the bounded noise assumption. Our results demonstratethat ADOPT rapidly converges to the solution, while Adam fails to converge, and AMSGrad exhibitsextremely slow convergence. Next, we conduct an experiment using a simple multi-layer perceptronon the MNIST classification task to evaluate the performance of ADOPT in nonconvex optimization.Our findings indicate that ADOPT outperforms existing adaptive gradient methods, including Adam,AMSGrad, and AdaShift. Finally, we evaluate the performance of ADOPT in various practical applications, such as image classification of CIFAR-10 and ImageNet using ResNet [He et al., 2016]and SwinTransformer [Liu et al., 2021], training of deep generative models (NVAE), fine-tuning oflanguage models (LLaMA), and deep reinforcement learning for continuous control. Our empiricalresults demonstrate that ADOPT achieves superior results over existing algorithms (e.g., Adam) inthese practical applications.",
  "Problem Definition": "We consider the minimization of the objective function f : RD R with respect to the parameter RD. In this context, we focus on first-order stochastic optimization methods, where only thestochastic gradient g is accessible. As the objective f can be nonconvex, the goal is to find a stationarypoint where f () = 0 [Blair, 1985, Vavasis, 1995]. In order to analyze the convergence behaviorof stochastic optimization algorithms, the following assumptions are commonly employed in theliterature:",
  "Assumption 2.6. The stochastic gradient is uniformly upper-bounded, i.e., there exists a constantG > 0 such that gt G": "In Assumption 2.6, the gradient noise t := gt f is assumed to be bounded almost surelyin addition to the true graidient f. Note that when Assumption 2.6 holds, Assumption 2.5 isautomatically satisfied; hence, Assumption 2.6 is a stronger assumption compared to Assumption2.5. In this paper, we adopt Assumptions 2.1, 2.2, 2.3 and 2.5 for analysis, because one of ourmotivations is to address the omission of Assumption 2.6. In the analysis, we derive the upper boundof mint{E[f(t))4/3]3/2} to investigate the convergence rate of the stochastic optimizationalgorithms, which is commonly performed in the literature [Dfossez et al., 2022, Zou et al., 2019].",
  "t = t1 tgtvt + 2 ,(2)": "where is a small positive constant. The division between vectors is applied in an element-wisemanner, and the addition between a vector a and a scalar b is defined as (a + b)i := ai + b. InAdaGrad [Duchi et al., 2011], vt is defined as v0 = 0 and vt = vt1 + gt gt. In RMSprop [Hintonet al., 2012], an exponential moving average is substituted for the simple summation, i.e., vt =2vt1 + (1 2)gt gt, where 0 2 < 1. Adam [Kingma and Ba, 2014] uses momentum inaddition to the second moment estimate to accelerate the convergence as follows:",
  "t = t1 tmtvt + 2 ,(4)": "where m0 = 0. Here, we omit the bias correction technique used in the original paper for clar-ity. Unfortunately, RMSprop and Adam are not guaranteed to converge even in a simple convexoptimization problem as demonstrated by Reddi et al. , whereas AdaGrad with a constantlearning rate is known to converge with an O(log T/ T) rate under Assupmtions 2.1-2.3 and 2.5for smooth nonconvex cases [Li and Orabona, 2019, Ward et al., 2020, Zou et al., 2018, Chen et al.,2019, Dfossez et al., 2022]. Although the convergence of Adam can be assured by choosing 2 in aproblem-dependent manner [Shi et al., 2020, Zhang et al., 2022, Wang et al., 2022, Li et al., 2023,Wang et al., 2023], it is difficult to know the proper choice of 2 for each problem before training. To fix the non-convergence of Adam without depending on 2, some researchers have proposedvariants of Adam. Reddi et al. proposed AMSGrad, which substitute vt for v in Eq. (3), wherev0 = 0 and vt = max {vt1, vt}. The idea behind AMSGrad is that the scaling factor vt + 2should be non-decreasing to ensure the convergence. After Reddi et al. originally proved theconvergence of AMSGrad for online convex optimization, Chen et al. showed that AMSGradwith t = /",
  "T) for a constant learning rate of t = = (1/": "T). However, their results depend onAssumption 2.6, which is often violated in practice. For example, variational autoencoders [Kingmaand Welling, 2014] and diffusion models [Ho et al., 2020, Song et al., 2021] are typical examples inwhich Assumption 2.6 does not hold because they utilize unbounded Gaussian noise in the gradientestimation. The cause of requirement for Assumption 2.6 is the max operation in the definition of vt.Since the max operation is convex, E[vt] maxt{E[vt]} does not hold; hence Assumption 2.6 isrequired to upper-bound E[vt] in their proofs. Zhou et al. also tried to fix the non-convergent behavior of Adam. Their proposed AdaShiftuses vtn instead of vt for the second moment estimate, and calculate the momentum using the latestn gradients as follows:",
  "vtn + 2 .(6)": "In the original paper, some additional techniques (e.g., the block-wise adaptive learning rate) are used,but we omit them for clarity here. Though they give theoretical analysis for a single online convexexample, any convergence bounds are not provided for nonconvex cases. More detailed discussion onexisting analyses is provided in Appendix A.",
  "Analysis: Cause of Non-convergence of Adam and How to Fix It": "In this section, to derive an algorithm that can converge with any 2 without Assumption 2.6, weanalyze the cause of non-convergence of Adam, and discuss how it can be eliminated. To start from asimple case, we first analyze the case without momentum. Subsequently, we extend it to the casewith momentum and provide a way to fix the convergence issue of Adam.",
  "T), the first and second terms on the right hand side of Eq. (7) converge with O(1/": "T) andO(1/T) rates, respectively. However, the last term includes a constant factor in terms of T, whichrepresents the non-convergent behavior of RMSprop in the smooth nonconvex setting. More precisely,RMSprop is guaranteed to converge only to a bounded region around a stationary point, and the sizeof the bounded region depends on the hyperparameter 2 and the problem-dependent factors D, G,and L. Therefore, we need to choose 2 dependently on each problem to make the bounded regionadequately small. Since lim21 log 2/1 2 = 0, the size of the bounded region can be madesmall by setting 2 to a value close to 1, which aligns with practical observations. However, howclose to 1 it should be relies on the problem-dependent factors, which cannot be observed in advance.This result is consistent with recent results of convergence analyses of Adam and RMSprop [Shiet al., 2020, Zhang et al., 2022]. As can be seen from Eqs. (8) and (9), the constant term in Eq. (7) is derived from the last term of Eq.(8). Because gt and vt are not statistically independent, this term is first decomposed as in Eq. (9).After the decomposition, gt and vt is now conditionally independent given g0, . . . , gt1, so Eq. (10)is derived using the following fact:",
  "end forreturn {t}Tt=1": "can be directly lower-bounded without the decomposition. A simple way to achieve the conditionalindependence is to substitute vt1 for vt as a second moment estimate, because vt1 does not haveinformation about gt. This solution is similar to AdaShift, in which vtn is substituted for vt asdescribed in Eq. (5). In fact, the modified version of RMSprop is identical to AdaShift with n = 1and 1 = 0 except for the additional techniques (e.g., the block-wise adaptive learning rate).",
  "Case with Momentum": "As we have described, RMSprop can be modified to be convergent by removing the current gradientgt from the second moment estimate vt. However, when we combine adaptive gradient methods withmomentum like Adam, the convergence analysis becomes more complicated. Unfortunately, whenAdam-style momentum in Eq. (3) is applied, the algorithm does not converge in general even whenusing vt1 as a second moment estimate instead of vt. This is because the momentum mt containsall history of the past gradients g0, . . . , gt; hence the second moment estimate always correlateswith mt. AdaShift prevents this problem by calculating the momentum mt only using the latest ngradients as described in Eq. (5). In that case, the momentum mt and the second moment estimatevtn are conditionally independent, so the convergence can be retained. However, this approach hasa trade-off in the choice of n. When n is small, mt has little information about the past gradients;when n is large, vtn only has access to the gradient information in the distant past.",
  "The main difference to the Adam-style momentum in Eq. (3) is the order of update of mt and thenormalization by": "vt1 + 2. In Eq. (3), the normalization is performed after the update of mt,whereas in Eq. (13), the normalization is first applied to the current gradient gt in advance to theupdate of mt. In this case, the second moment estimate vt1 is only used to normalize the currentgradient gt, so the convergence can be guaranteed. A more detailed convergence analysis is providedin .",
  "Method: Adaptive Gradient Method with the Optimal Convergence Rate": "Based on the analysis in the previous section, we propose a new adaptive gradient method namedADOPT (ADaptive gradient method with the OPTimal convergence rate). The entire procedure issummarized in Algorithm 1. For a simple discription, we place the update of m after the parameterupdate in Algorithm 1, but it is equivalent to Eqs. (13) and (14) except that maxv, issubstitued for v + 2. The substitition is applied because we find that it contributes to slightlybetter performance in practice. We provide an equivalent expression of Algorithm 1 in Algorithm 3in the appendix, which is closer to a practical implementation. By this modification, ADOPT canconverge with the optimal rate for smooth nonconvex optimization as follows:",
  "t)) in the appendix, which is closerto practical situations. In that case, ADOPT also converges with the optimal rate": "In practice, however, the ADOPT algorithm sometimes becomes unstable especially when near-zerogradients are observed in the early phase of optimization. For example, if some elements of v0are almost zero, the corresponding elements of g1/ max{v0, } take very large values due to thenear-zero division, leading to an unstable parameter update. Such a phenomenon is typically observedwhen, for example, some parts of parameters (e.g., the last layer of a neural net) are initialized withzero, which is a commonly-used technique in deep learning. To avoid the near-zero division, we alsopropose a clipped version of ADOPT in Algorithm 2. Note that the clipping operation is applied inan element-wise manner:",
  "Experiments": "In the experiments, we first validate our ADOPT algorithm using a simple toy example in whichAdam is known to fail to converge, and confirm our theoretical findings through numerical simulation.Secondly, we run an experiment of training a simple multi-layer perceptron (MLP) for the MNISTdataset to verify the effectiveness of our ADOPT for nonconvex optimization problems. Finally, weevaluate our ADOPT in a wide range of practical applications, including image classification, naturallanguage processing (NLP) tasks, generative modeling, and deep reinforcement learning. Detailedexperimental settings are described in the appendix. In these experiments, we use the vanilla ADOPTalgorithm in Algorithm 1 and do not use the clipped version in Algorithm 2. Toy problem: We consider a convex optimization problem with an objective f() = for .It is obvious that a solution for the problem is = 1. Through the optimization, we only haveaccess to the stochastic objective ft as follows:",
  "AdamAMSGradADOPT": ": Performance comparison between Adam, AMSGrad and ADOPT in a simple univariateconvex optimization problem. The plots show transitions of the parameter value, which shouldconverge to the solution = 1. by Reddi et al. as a case where Adam fails to converge. In this setting, the constant k controlsthe magnitude of gradient noise. When k = 1, it corresponds to the noiseless case where ft = fwith probability 1. As k gets large, stochastic gradient becomes noisy, making G in Assumptions2.5 and 2.6 large. Therefore, the optimization will be more difficult when k becomes larger. In theexperiment, we set k = 10 or 50, and compare the robustness of Adam, AMSGrad, and ADOPTfor various hyperparameter settings by changing 2 from 0.1 0.999. We set 1 = 0.9 for all thealgorithms, which is a common choice in practice. We set the learning rate to t = 0.01/1 + 0.01t. The result is shown in . It can be seen that, when k = 10, Adam fails to converge exceptfor 2 = 0.999 while AMSGrad and ADOPT rapidly converge to the correct solution, i.e., = 1,with any 2. In a more extreme case where k = 50, Adam fails to converge even with 2 = 0.999.This aligns with Theorem 3.1, since, when the gradient noise is large (i.e., G is large), the boundedregion of the convergence bound also gets large, leading to divergence of Adam. Moreover, whenk = 50, it is observed that the convergence of AMSGrad also becomes much slower than ADOPT.In fact, this phenomenon is also consistent with theory. In this problem setting, the second momentE[g2t ] is O(k3), while the squared norm of the stochastic gradient g2t is O(k4). Since the convergencebound of AMSGrad depends on the uniform bound of the stochastic gradient in Assumption 2.6,instead of the second moment in Assumption 2.5, its convergence also deteriorates with the order ofg2t . Compared to AMSGrad, ADOPT only depends on the second moment bound for its convergence,so it converges much faster than AMSGrad even in such an extreme setting. We also perform ablation study on how the two algorithmic changes from Adam to ADOPT affectthe convergence. The differences between Adam and ADOPT are (1) decorrelation between thesecond moment estimate and the current gradient, and (2) change of order of momentum update andnormalization by the second moment estimate. In this experiment, we remove each algorithmic changefrom ADOPT, and compare the result in the toy example. We set k = 50, and (1, 2) = (0.9, 0.999),since it is a common hyperparameter choice. The result is shown in . It can be observedthat ADOPT fails to converge with the exception of either algorithmic change. Therefore, applyingboth changes is essential to overcome the non-convergence of Adam, which also aligns with theory.These results correspond to the theoretical findings, showing the superiority of ADOPT to Adam andAMSGrad in terms of the convergence speed and its robustness to hyperparameter choices. MNIST classification: To investigate the effectiveness of ADOPT on nonconvex optimization,we train nonlinear neural networks for MNIST classification tasks, and compare the performancebetween ADOPT and existing optimization algorithms, such as Adam, AMSGrad and AdaShift. In",
  "this experiment, we use a simple MLP with a single hidden layer, and the number of hidden units is setto 784. We set the learning rate to t = /": "t, and is tuned in the range of {1, 101, 102, 103}.We apply weight decay of 1 104 to prevent over-fitting, and run 10K iterations of parameterupdates. shows the learning curves of training and test accuracy. We observe our ADOPTperforms slightly better than the others in terms of the convergence speed and the final performance. Image classification: As a more practical application, we conduct experiments of image classificationusing real-world image datasets. We first compare ADOPT and Adam in the classification task of theCIFAR-10 dataset using ResNet-18 [He et al., 2016], a widely-used convolutional neural network. Weconduct a similar hyperparameter search to the case of MNIST classification. A detailed experimentalsetting is provided in the appendix. The learning curves of test accuracy are visualized in . Itcan be observed that ADOPT converges a little faster than Adam. To confirm that our ADOPT works well for modern neural network architectures based on Transform-ers [Vaswani et al., 2017], we perform an experiment of ImageNet classification using SwinTrans-former [Liu et al., 2021]. We follow the official training recipe of Swin Transformer-tiny providedby Torchvision [Paszke et al., 2019a], and fix the training settings except for the optimizer choice.We use AdamW [Loshchilov and Hutter, 2019] as a baseline because it is set as the default officialoptimizer. We also compare with AMSGrad as another way to fix the non-convergence issue ofAdam. Since AdamW uses decoupled weight decay, we also apply it to the other optimizers forfair comparison. We report the top-1 accuracy at 200 and 300 epochs in Tables 1. We observe thatADOPT outperforms AdamW and AMSGrad throughout the training in terms of the test accuracy,demonstrating the effectiveness of ADOPT for this setting.",
  ": Learning curves of GPT-2 pretraining for training set (left) and validation set (right)": "Generative modeling: We train NVAE [Vahdat and Kautz, 2020] for MNIST using our ADOPT. Inthe official implementation of NVAE, Adamax [Kingma and Ba, 2014], an infinite-norm variant ofAdam, is used as an optimizer, so we use Adamax as a baseline method. We use the exactly the samesetting of the official implementation except that the learning rate for ADOPT is set to 2 104 sincethe default value 0.01 is too large for ADOPT. We report the negative log-likelihood for test data on. It is observed that the model trained with ADOPT shows the better likelihood. Pretraining of large language models: We run a pre-training of GPT-2 [Radford et al., 2019]using the nanoGPT [Karpathy, 2022] code base to compare Adam and ADOPT. We use OpenWeb-Text [Gokaslan and Cohen, 2019] as the training data. Experimental setup conforms to the defaultsettings of nanoGPT except for the selection of the optimizer. We also test a case in which the totalbatch size was changed from 480 to 96, as a setting where the gradient noise becomes larger. Theresults are summarized in . The most notable finding is that in the small batch size case,Adam causes loss spikes in the early stages of training and fails to converge, while ADOPT is alwaysable to train stably. This is consistent with Adams theory of non-convergence. As the gradient noiseincreases, G in Theorem 3.1 also increases, and the constant term in Adams convergence boundsbecomes non-negligible especially when using a large-scale dataset like OpenWebText. As a result,Adam is more likely to fail to train in such cases. Our ADOPT, on the other hand, does not sufferfrom this problem because it can always guarantee convergence. We also observed that both Adamand ADOPT work well when the batch size is large, but even in this case, ADOPT performs slightlybetter. Finetuning of large language models: We finetune the pretrained LLaMA-7B on 52K instruction-following data provided by Stanford Alpaca and compare the performance between the defaultoptimizer (Adam) and our ADOPT under the exactly same experimental setting. For evaluation, weuse Multi-task Language Understanding (MMLU) Benchmark [Hendrycks et al., 2021], which iswidely used to assess the performance of large language models. The MMLU score for LLaMA-7Bwithout finetuning is 35.1. After fine-tuned via instruction-following using the baseline implementa-tion with Adam, the score improves to 41.2. When we substitute ADOPT for Adam, the score evenimproves to 42.13. The detailed score comparison for each task is summarized in in theappendix. Other experimental results, including deep RL experiments, and detailed experimentalsettings are also provided in the appendix.",
  "Conclusion": "In this paper, we demystified the fundamental cause of divergence of adaptive gradient methodsbased on the exponential moving average, such as Adam and RMSprop, in general smooth nonconvexoptimization problems, and demonstrate a way to fix the issue, proposing a new optimizer namedADOPT. Not only does ADOPT converge with the optimal rate without depending on a hyperpa-rameter choice in theory, but ADOPT demonstrates better performance in a wide range of pracitalapplications. We expect that this work will serve as a bridge between theory and practice in the research of adaptivegradient methods. Since ADOPT can be safely applied to many machine learning problems withoutcareful tuning of hyperparameters, it can be expected to improve the training stability and the modelperformance in practice by substituting it for the existing adaptive gradient methods (e.g., Adam). One of the limitations of our analysis is that it still relies on the assumption that the second moment ofstochastic gradient is uniformly bounded (i.e., Assumption 2.5). Although this assumption is weakerthan the bounded stochastic gradient assumption (i.e., Assumption 2.6), it would be more desirableto relax it to the bounded variance assumption (i.e., Assumption 2.4), which is often adopted in theanalysis of the vanilla SGD [Ghadimi and Lan, 2013]. For Adam, a recent work by Wang et al. have derived a problem-dependent convergence bound which achieves the O(1/",
  "T) rate withoutAssumption 2.5. Their proof techniques may help to relax our assumptions in the proof of Theorem4.1, which we leave as future work": "From a broader perspective, adaptive gradient methods like Adam have been widely used even forthe training of large-scale foundation models (e.g., large language models). Although such modelscan be useful for people, their negative aspects, such as concerns about copyright infringement, arenot negligible. Researchers needs to deeply recognize and understand such social impacts of machinelearning algorithms.",
  "Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. InInternational Conference on Learning Representations, 2018. URL": "Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for conver-gences of adam and rmsprop. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pages 1112711135, 2019. Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of a class of adam-typealgorithms for non-convex optimization. In International Conference on Learning Representations,2019. URL",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances inNeural Information Processing Systems, 33:68406851, 2020": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and BenPoole. Score-based generative modeling through stochastic differential equations. In InternationalConference on Learning Representations, 2021. URL Zhiming Zhou, Qingru Zhang, Guansong Lu, Hongwei Wang, Weinan Zhang, and Yong Yu. Adashift:Decorrelation and convergence of adaptive learning rate methods. In International Conference onLearning Representations, 2019. URL",
  "Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Lecture 6e rmsprop: Divide the gradientby a running average of its recent magnitude, 2012. URL ~tijmen/csc321/slides/lecture_slides_lec6.pdf": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin.Attention is all you need.In I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, edi-tors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,Inc., 2017.URL Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,high-performance deep learning library. Advances in neural information processing systems, 32,2019a.",
  "Jeff Haochen and Suvrit Sra. Random shuffling beats sgd after finite epochs. In InternationalConference on Machine Learning, pages 26242633. PMLR, 2019": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, AndreasKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.Pytorch: An imperative style,high-performance deep learning library. In Advances in Neural Information Processing Systems 32,pages 80248035. Curran Associates, Inc., 2019b. URL Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policymaximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and AndreasKrause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80of Proceedings of Machine Learning Research, pages 18611870. PMLR, 1015 Jul 2018. URL Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 50265033,2012. doi: 10.1109/IROS.2012.6386109. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and NoahDormann. Stable-baselines3: Reliable reinforcement learning implementations. The Journal ofMachine Learning Research, 22(1):1234812355, 2021.",
  "i=1fi () .(18)": "fi is, for example, a loss function for i-th training sample. Although many deep learning problems canbe formulated as a finite sum problem, training of the variational autoencoders (VAEs) or diffusionmodels is out of the finite-sum problem, since their objective is formulated as an infinite sum (i.e.,an expectation over continuous variables). Moreover, they assume the stochastic gradient g is L-Lipschitz, whereas we only assume true gradient f is L-Lipschitz. They also assume a growthcondition as follows:",
  "Egt2 G20 + G21 f (t1)2 .(19)": "This growth condition is weaker than our Assumption 2.5. Assumption 2.5 is a special case of thegrowth condition where G1 = 0. Their derived convergence rate has a constant factor of O(G0);hence the strong growth condition (i.e., G0 = 0) is required to assure convergence. Moreover, toassure convergence, one needs to choose sufficiently large 2, which has to be tuned in a problem-dependent manner. Wang et al. also focus on convergence of Adam in the finite sum problem, but they relaxthe L-Lipschitz condition on g to the (L0, L1)-Lipschitz condition. They also assume the growthcondition in Eq. (19), and their convergence rate has the same order with Zhang et al. , so itstill requires the strong growth condition (i.e., G0 = 0) to assure convergence. The condition of 2 isalso similar to Zhang et al. . Li et al. consider Adams convergence on general smooth nonconvex problems. Similar toWang et al. , they use (L0, L)-Lipschitz condition on the true gradient f. They also assumethat the gradient noise is almost surely bounded:",
  "g f (20)": "The relationship between this assumption and our Assumption 2.5 is a little complicated. Assumption2.5 is equivalent to a combination of Assumption 2.4 and the following assumption:Assumption A.1. The true gradient is uniformly bounded, i.e., there exist constants G and suchthat f ()2 G2 2 and 0 < G. The bounded noise assumption of Eq. (20) is strictly stronger than Assumption 2.4, but they do notassume the bounded true gradient (i.e., Assumption A.1). The bounded noise assumption is oftenviolated in practice (e.g., training of VAEs), because the gradient is often estimated using unboundednoise (i.e., Gaussian noise). Their convergence rate O(1/",
  "BWith-Replacement vs. Without-Replacement": "In the optimization of finite-sum problems, practitioners often use without-replacement sampling,which is also known as random shuffling, to obtain stochastic gradient. In this case, the stochasticgradient has a small bias due to the lack of replacement, so Assumption 2.2 is violated. However, thevanilla SGD is known to converge with the without-replacement strategy [Haochen and Sra, 2019],and some of the analyses of Adam also adopt without-replacement sampling [Zhang et al., 2022,Wang et al., 2022]. Unfortunately, we find that our ADOPT has a counter example, in which ADOPT fails to convergewhen using without-replacement sampling. For example, when we consider minimizing f() =3i=1 fi(), where , f1() = 1.9 and f2() = f2() = , it can be easily observedthat ADOPT with 1 = 2 = 0 fails to converge to the correct solution, i.e., = 1. This non-convergence can be easily avoided by using the with-replacement strategy. Moreover,the difference between with- and without-replacement sampling becomes negligible when n in thefinite-sum ni=1 fi is large enough; hence it does not affect the practical performance very much. Infact, our experiments except for the toy example are performed using without-replacement sampling,but divergent behaviors are not observed. If one applies ADOPT to problems where the differenceseems severe (e.g., when training with a small dataset), we recommend to use with-replacementsampling instead of random shuffling for stable training. When one uses PyTorch [Paszke et al.,2019b] for the implementation, for example, with-replacement sampling can be easily applied byspecifying replacemnet=True for torch.utils.data.RandomSampler, and feeding it to thesampler argument of torch.utils.data.DataLoader.",
  "DRecommendation of Hyperparameter Settings for ADOPT": "We experimentally find that our ADOPT works similarly to Adam when the same hyperparametersare used, but should be set to a little larger value (e.g., 1 106) for ADOPT compared to Adam,in which is set to 1 108 by default. Our recommendation of the hyperparameter settings forADOPT is provided in .",
  ": Comparison of MMLU scores for LLaMA-7B finetuned via instruction following usingAdamW and ADOPT": "We train reinforcement learning (RL) agents using the soft actor crtitic algorithm [Haarnoja et al.,2018] with ADOPT for the optimizer. As a benchmark, we use a continuous control tasks ofHalfCheetah-v4 on MuJoCo simulator [Todorov et al., 2012]. For comparison to ADOPT, Adamis used as a baseline optimizer. We follow the hyperparameter settings recommended by Stable-Baselines3 [Raffin et al., 2021], and just change the choice of an optimizer. The result is shownin . The error bars indicate 95% confidence intervals of three trials. We observe slightperformance improvement by using ADOPT instead of Adam.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Experimental settings are provided in and the appendix. We alsoshare the implementation of the experiment.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: Detailed experimental settings are provided in and the appendix.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: Error bars are reported in all the figures and tables.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  ". Code Of Ethics": "Question: Does the research conducted in the paper conform, in every respect, with theNeurIPS Code of Ethics [Yes]Justification: We confirmed that our research conforms with the Code of Ethics.Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require adeviation from the Code of Ethics.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: We discussed them in the last section of the paper.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  ". Safeguards": "Question: Does the paper describe safeguards that have been put in place for responsiblerelease of data or models that have a high risk for misuse (e.g., pretrained language models,image generators, or scraped datasets)?Answer: [NA]Justification:Guidelines: The answer NA means that the paper poses no such risks. Released models that have a high risk for misuse or dual-use should be released withnecessary safeguards to allow for controlled use of the model, for example by requiringthat users adhere to usage guidelines or restrictions to access the model or implementingsafety filters.",
  ". Licenses for existing assets": "Question: Are the creators or original owners of assets (e.g., code, data, models), used inthe paper, properly credited and are the license and terms of use explicitly mentioned andproperly respected?Answer: [Yes]Justification: They are provided both in the main paper and the appendix.Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification:Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}