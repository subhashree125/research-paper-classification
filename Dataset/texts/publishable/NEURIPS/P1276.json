{
  "Department of Computer Science, University of California, Davis2Google3Department of Electrical and Computer Engineering, University of California, Davis{qyfang,whang,jazh}@ucdavis.edu,": "World models have recently emerged as a promising approach to reinforcementlearning (RL), achieving state-of-the-art performance across a wide range of visualcontrol tasks. This work aims to obtain a deep understanding of the robustnessand generalization capabilities of world models. Thus motivated, we develop astochastic differential equation formulation by treating the world model learning asa stochastic dynamical system, and characterize the impact of latent representationerrors on robustness and generalization, for both cases with zero-drift representationerrors and with non-zero-drift representation errors. Our somewhat surprisingfindings, based on both theoretic and experimental studies, reveal that for the casewith zero drift, modest latent representation errors can in fact function as implicitregularization and hence result in improved robustness. We further propose aJacobian regularization scheme to mitigate the compounding error propagationeffects of non-zero drift, thereby enhancing training stability and robustness. Ourexperimental studies corroborate that this regularization approach not only stabilizestraining but also accelerates convergence and improves accuracy of long-horizonprediction.",
  ". Introduction": "Model-based reinforcement learning (RL) has emerged as a promising paradigm to improve sampleefficiency by enabling agents to exploit a learned model of the physical environment. Recent works onworld models involve RL agents learning a latent dynamics model (LDM) from observationsand actions, and then optimizing the policy over this learned model. Unlike conventional approaches,world-model-based RL employs an end-to-end learning strategy, jointly training the dynamics model,perception, and action policy to achieve a unified goal. This framework offers significant potentialto improve both generalization and robustness to perturbations, making it highly advantageousfor real-world scenarios. For example, DreamerV2-V3 have achieved notable progress in masteringdiverse tasks involving continuous and discrete actions, image-based inputs, and both 2D and 3Denvironments . Recent empirical studies have also demonstrated the capacity of world modelsto generalize to unseen noisy states and dynamics in complex environments, such as autonomousdriving . However, it remains unclear when and how world models can generalize well in unseenenvironments, and the role of robustness in this process. In this work, we aim to systematically understand the robustness and generalization capabilities ofworld models by examining the impact of latent representation errors introduced by latent encoders.Specifically, we investigate how these errors can enhance robustness against perturbations, which inturn often improves generalization . Contrary to the expectation that minimizing latent repre-sentation errors by optimizing the LDM prior to policy training would lead to better performance,our theoretical and empirical findings reveal that modest latent representation errors during trainingmay actually be beneficial for robustness. In particular, the alternating training strategy for world An earlier version of this paper was submitted to NeurIPS and received ratings of (7, 6, 6). The reviewerscomments and the original draft are available at Openreview. This version contains minor modifications basedon that submission.",
  ": Reward values on unseen perturbed states by rotation () or mask (%) with N(0.15, 0.5)": "model learning, which simultaneously refines both the LDM and the action policy, can improverobustness and yield generalization gains. This is because modest latent representation errors enablethe world model to better handle perturbations, leading to improved exploration and generalizationcapabilities. This phenomenon mirrors the behavior observed with gradient estimation errors in batch training.For instance, as shown in , intermediate batch sizes (e.g., 16 or 32) produce gradient estimationerrors that are beneficial for generalization, compared to smaller (e.g., 8) or larger (e.g., 64) batchsizes. The latent representation errors exhibit a similar effect in a controlled range, supportingrobustness through implicit regularization. Indeed, implicit regularization has been associated withincreased classification margins , which improves generalization performance . In a nutshell, latent representation errors, if properly managed, may actually facilitate world modeltraining by enhancing robustness against perturbations, thereby improving generalization. Thisinsight aligns with recent advances in deep learning, where noise injection schemes have beenstudied as a form of implicit regularization to enhance model robustness. For instance, analyzesthe effects of introducing isotropic Gaussian noise at each layer of neural networks, identifyingit as implicit regularization. Similarly, explores adding zero-drift Brownian motion to RNNarchitectures, demonstrating its regularizing effects in improving stability. However, we caution that latent representation errors in world models differ from the noise injectionschemes (), in several key aspects: 1) Unlike the artificially injected noise only added intraining and removed during inference, these errors are inherent in world models and lead to errorpropagation during rollouts; 2) The errors in world models may not exhibit well-behaved propertiessuch as isotropic or zero-drift noise and may have non-zero drift and bias; 3) in the iterative trainingof LDMs and agents, the error originating from the encoder also affects the policy learning andexploration. To address these challenges, we develop a continuous-time stochastic differential equation (SDE) for-mulation by modeling LDM as a stochastic dynamical system, aiming to understand the robustnessand generalization of world models and improve it further. This approach provides a formal charac-terization of latent representation errors as stochastic perturbations, allowing us to quantify theirimpact on robustness and generalization. Our main contributions can be summarized as follows: Latent representation errors as implicit regularization: We develop a continuous-time SDE formu-lation by treating the world model learning as a stochastic dynamical system. Using stochasticperturbation results, we show that under certain conditions, modest latent representation errorscan in fact act as implicit regularization, leading to robustness gain. Improving robustness and generalization in non-zero drift cases via Jacobian regularization: For thecase where latent representation errors exhibit non-zero drifts, we show that the additional biascan degrade the implicit regularization effect, leading to learning instability. We theoreticallyquantify this instability and show that the well-known Jacobian regularization can be employedto address this issue, verified by our experimental results. Reducing error propagation in predictive rollouts: We explicitly characterize the effect of latentrepresentation errors on predictive rollouts and robustness.By applying Jacobian regularization,we control these errors, leading to reduced error propagation, enhanced prediction performance,and faster convergence, especially in tasks with longer time horizons. Notation. We use the Einstein summation convention for succinctness, where aibi denotes i aibi.We denote functions in Ck, as being k-times differentiable with -Hlder continuity. The Euclideannorm of a vector is represented by , and the Frobenius norm of a matrix by | |F ; this notationmay occasionally extend to tensors. The notation xi indicates the ith coordinate of the vector xand Aij the (i, j) entry of the matrix A. The composition of the function is denoted by f g. For adifferentiable function f : Rn Rm , its Jacobian matrix is denoted by f",
  ". Related Work": "Robustness and Generalization in Deep RL. Recent work on deep RL robustness and generalizationhas studied zero-shot generalization of learned policies to unseen environments , often empha-sizing task-level generalization through techniques such as task augmentation in meta-RL .In contrast, our work targets the robustness and generalization of world-model-based RL underobservational and dynamic perturbations, emphasizing the role of latent representations. Whilerecent studies on RL robustness introduce new training frameworks aimed at policy safetyand robustness, they do not account for the inherent challenges posed by latent representation errorsduring rollouts. World model based RL. World models have excelled in visual control tasks across various platforms,including Atari and Minecraft , as detailed in the studies by Hafner et al. . Thesemodels typically integrate encoders and memory-augmented neural networks, such as RNNs ,to manage the latent dynamics. The use of variational autoencoders (VAE) to map sensoryinputs to a compact latent space was pioneered by Ha et al. . Furthermore, the Dreamer algorithm employs convolutional neural networks (CNNs) to enhance the processing of both hiddenstates and image embeddings, yielding models with improved predictive capabilities in dynamicenvironments. Continuous-time RNNs. The continuous-time assumption is standard for theoretical formulationsof RNN models. Li et al. study the optimization dynamics of linear RNNs on memory decay.Chang et al. propose AntisymmetricRNN, which captures long-term dependencies through thecontrol of eigenvalues in its underlying ODE. Chen et al. propose the symplectic RNN to modelHamiltonians. As continuous-time formulations can be discretized with Euler methods (orwith Euler-Maruyama methods if stochastic in ) and yield similar insights, this step is ofteneliminated for brevity. Implicit regularization by noise injection in RNN. Studies on noise injection as a form of implicitregularization have gained traction, with Lim et al. deriving an explicit regularizer under smallnoise conditions, demonstrating bias towards models with larger margins and more stable dynamics.Camuto et al. examine Gaussian noise injections at each layer of neural networks. Similarly, Weiet al. provide analytic insights into the dual effects of dropout techniques.",
  ". Demystifying World Model: A Stochastic Differential EquationApproach": "As pointed out in , critical to the effectiveness of the world model representation is the stochasticdesign of its latent dynamics model. The model can be outlined by the following key components:an encoder that compresses high dimensional observations st into a low-dimensional latent statezt (Eq.1), a sequence model that captures temporal dependencies in the environment (Eq.2), atransition predictor that estimates the next latent state (Eq.3), and a latent decoder that reconstructs",
  "Latent Encoder: zt qenc(zt | ht, st),(1)Sequence Model: ht = f(ht1, zt1, at1),(2)Transition Predictor: zt p( zt | ht),(3)Latent Decoder: st qdec( st | ht, zt)(4)": "In this work, we consider a popular class of world models, including Dreamer and PlaNet, where{z, z, s} have distributions parameterized by neural networks outputs, and are Gaussian when theoutputs are known. It is worth noting that {z, z, s} may not be Gaussian and are non-Gaussian ingeneral. This is because while z is conditional Gaussian, its mean and variance are random variableswhich are learned by the encoder with s and h being the inputs, rendering that z is non-Gaussiandue to the mixture effect. For this setting, we have a continuous-time formulation where the latentdynamics model can be interpreted as stochastic differential equations (SDEs) with coefficientfunctions of known inputs. Due to space limitation, we refer to Proposition B.1 in the Appendix for amore detailed treatment.",
  "Latent Decoder: d st = qdec(ht, zt) dt + qdec(ht, zt) dB dect,(8)": "where (h, z) is a policy function as a local maximizer of value function and the stochastic processst is Ft-adapted. Notice that f is often a zero function indicating that Equation (6) is an ODE, asthe sequence model is generally designed as deterministic. Generally, the coefficient functions in dtand dBt terms in SDEs are referred to as the drift and diffusion coefficients. Intuitively, the diffusioncoefficients here represent the stochastic model components. For latent representation errors, in Equation (5), (, ) and (, ) denotes the drift and diffusioncoefficients of the stochastic latent representation errors, respectively, both of which depend on hiddenstates ht and task states st. The parameter serves as a scaling factor of the stochastic error. In Ap-pendix A, we provide a theoretical justification (see A.6) showing that the latent representation error,in the form of approximation error corresponding to the widely used CNN encoder-decoder, couldbe made sufficiently small by finding appropriate CNN network configuration. In particular, thisresult justifies interpreting latent representation error as a stochastic perturbation in the dynamicalsystem defined in Equations (5 - 8), as the error magnitude can be made sufficiently small by CNNnetwork configuration.",
  ". Latent Representation Errors as Implicit Regularization towards Robustnessand Generalization": "In this section, we investigate how latent representation errors influence both robustness and gen-eralization, considering two scenarios: zero drift and non-zero drift. Our analysis shows that undermild conditions, zero-drift errors can act as a natural form of implicit regularization, creating wideroptimization landscapes that enhance robustness. However, when latent representation errors ex-hibit non-zero drift, they introduce an unstable bias that undermines the implicit regularization effect,leading to degraded generalization performance. In such cases, explicit regularization is necessary tostabilize learning and maintain both robustness and generalization capabilities in the world model.",
  "igi(xt, t) + i(xt, t) dBit,(9)": "where g, and gi are structured accordingly for the respective components, employing the Einstein sum-mation convention for concise representation. For abuse of notation, = (, 0, 0, 0), = (, 0, 0, 0).For a given error magnitude , we denote the solution to SDE (9) as xt. Intuitively, xt is the perturbedtrajectory of the latent dynamics model. In particular, when = 0, indicating that the absence oflatent representation error in the model, the solution is denoted as x0t.",
  ". The Case with Zero-drift Representation Errors": "When the drift coefficient = 0, the latent representation errors correspond to a class of well-behavedstochastic processes. The following result translates the induced perturbation on the stochasticlatent dynamics models loss function L to a form of explicit regularization. We assume that a(nonconvex) general loss function L C2 which depends on zt, ht, zt, st. Loss functions used inpractical implementation, e.g. in DreamerV3, reconstruction loss JO, reward loss JR, consistencyloss JD, all satisfy this condition. Theorem 3.3. (Explicit Effect Induced by Zero-Drift Representation Error) Under Assumptions 3.1and 3.2 and considering a loss function L C2, the explicit effects of the zero-drift error can be marginalizedout as follows: as 0,E L (xt) = E L(x0t) + R + O(3),(10)where the regularization term R is given by R := P + 2 Q + 1",
  "The proof is relegated to Appendix B in the Supplementary Materials": "In the special case when the loss L is convex, then its Hessian, 2L, is positive semi-definite, whichensures that the term S is non-negative. The presence of this Hessian-dependent term S, under latentrepresentation error, implies a tendency towards wider minima in the loss landscape. Empirical resultsfrom indicates that wider minima correlate with improved robustness of implicit regularization during training. This observation also aligns with the theoretical insights in that the introductionof Brownian motion, which is indeed zero-drift by definition, in training RNN models promotesrobustness. We note that in addition, when the error t() is too small, the effect of term S as implicitregularization would not be as significant as desired. Intuitively, this insight resonates with theempirical results in that models robustness gain is not significant when the error induced bylarge batch sizes is too small. We remark that the exact loss form treated here is simplified compared to that in the practicalimplementation of world models, which frequently depends on the probability density functions(PDFs) of zt, ht, zt, st. In principle, the PDE formulation corresponding to the PDFs of the perturbedxt can be derived from the Kolmogorov equation of the SDE (9), and the technicality is more involvedbut can offer more direct insight. We will study this in future work.",
  ". The Case with Non-Zero-Drift Representation Errors": "In practice, latent representation errors may not always exhibit zero drift as in idealized noise-injectionschemes for deep learning (, ). When the drift coefficient is non-zero or a function of inputdata ht and st in general, the explicit regularization terms induced by the latent representation errormay lead to unstable bias in addition to the regularization term R in Theorem 3.3. With a slightabuse of notation, we denote g0 as g from Equation (9) for convenience. Corollary 3.4. (Additional Bias Induced by Non-Zero Drift Representation Error)Under Assumptions 3.1 and 3.2 and considering a loss function L C2, the explicit effects of the general formerror can be marginalized out as follows as 0:",
  "and t being the shorthand for t0 1s k(x0s, s)dt": "The presence of the new bias term R implies that regularization effects of latent representationerror could be unstable. The presence of in P, Q and S induces a bias to the loss function withits magnitude dependent on the error level , since is a non-zero term influenced on the driftterm . This contrasts with the scenarios described in and , where the noise injected forimplicit regularization follows a zero-mean Gaussian distribution. To modulate the regularizationand bias terms R and R respectively, we note that a common factor, the fundamental matrix , canbe bounded by",
  ". Enhancing Predictive Rollouts via Jacobian Regularization": "In this section, we study the effects of latent representation errors on predictive rollouts using latentstate transitions, which happen in the inference phase in world models. We then propose to useJacobian regularization to enhance the quality of rollouts. In particular, we first obtain an upper bound of state trajectory divergence in the rollout due to the representation error. We show that theerror effects on task policys Q function can be controlled through models input-output Jacobiannorm. In world model learning, the task policy is optimized over the rollouts of dynamics model with theinitial latent state z0. Recall that latent representation error is introduced to z0 when latent encoderencodes the initial state s0 from task environment. Intuitively, the latent representation error wouldpropagate under the sequence model and impact the policy learning, which would then affect thegeneralization capacity through increased exploration.",
  "d ht = f(ht, zt, (ht, zt)) dt,d zt = p(ht)dt + p(ht) dBt,(19)": "with random variables h0, z0 + as the initial values, respectively. In particular, is a random variableof proper dimension, representing the error from encoder introduced at the initial step. We imposethe standard assumption on the error to ensure the well-definedness of the SDEs. Under Assumption 3.1, there exists a unique solution to the SDEs (for Equations 19 with square-integrable ), denoted as (ht, zt ). In the case of no error introduced, i.e., = 0, we denote the solutionof the SDEs as (h0t, z0t ) understood as the rollout under the absence of latent representation error. Tounderstand how to modulate impacts of the error in rollouts, our following result gives an upperbound on the expected divergence between the perturbed rollout trajectory (ht, zt ) and the original(h0t, z0t ) over the interval [0, T].",
  "where C is a constant dependent on T. J1 and J2 are Jacobian-related terms, and H1 and H2 are Hessian-relatedterms": "The Jacobian-related terms J1 and J2 are defined as J0 := exp (Fh + Fz + Ph) , J1 := exp Ph; theHessian-related terms H0 and H1 are defined as H0 := Fhh + Fhz + Fzh + Fzz + Phh, H1 := Phh,where Fh, Fz are the expected sup Frobenius norm of Jacobians of f w.r.t h, z, respectively, andFhh, Fhz, Fzh, Fzz are the corresponding expected sup Frobenius norm of second-order derivatives.Other terms are similarly defined. A detailed description of all terms, can be found in Appendix C.1. Theorem 4.1 correlates with the empirical findings in regarding the diminished predictive accuracyof latent states zt over the extended horizons. In particular, Theorem 4.1 suggests that the expecteddivergence from error accumulation hinges on the expected error magnitude, the Jacobian normswithin the latent dynamics model and the horizon length T. Our next result reveals how initial latent representation error influences the value function Q duringthe prediction rollouts, which again verifies that the perturbation is dependent on expected errormagnitude, the models Jacobian norms and the horizon length T:",
  "2ij x0t C exp ( H0 (J0 + J1)) + C exp ( H1 (J0 + J1))": "This corollary reveals that latent representation errors implicitly encourage exploration of unseenstates by inducing a stochastic perturbation in the value function, which again can be regularizedthrough a controlled Jacobian norm. Intuitively, the stochasticity in the LDM also encourages greaterexploration compared to its deterministic counterparts. Jacobian Regularization against Non-Zero Drift. The above theoretical results have establisheda close connection of input-output Jacobian matrices with the stabilized generalization capacity ofworld models (shown in 18 under non-zero drift form), and perturbation magnitude in predictiverollouts (indicated in the presence of Jacobian terms in Theorem 4.1 and Corollary 4.2.) Building onthese insights, we propose a regularizer on input-output Jacobian norm gk x F that could modulate ( and in addition k). This regularization not only enhances robustness by controlling perturbationsbut also reinforces generalization through smoother dynamics in the world models latent space.",
  "Ldyn = Ldyn + JF ,(20)": "where Ldyn is the original loss function for dynamics model, J denotes the data-dependent Jacobianmatrix associated with the -parameterized dynamics model, and is the regularization weight(usually chosen from range [0.01, 0.1], see ). Our empirical results in 5 with an emphasis onsequential case align with the experimental findings from that Jacobian regularization canenhance robustness against random and adversarial input perturbation.",
  ". Experimental Studies": "In this section, extensive experiments are carried out over a number of tasks in Mujoco environments.Due to space limitation, implementation details and additional results, including the standarddeviation of the trials, are relegated to Section D in the Appendix. Enhanced robustness and generalization to unseen noisy states and varied dynamics.We evaluatedthe effectiveness of Jacobian regularization by comparing a model trained with this regularizationagainst a vanilla model during inference, using perturbed state images and varied dynamics. Weconsider three types of perturbations to the observations: (1) Gaussian noise applied across the entireimage, denoted as N(1, 21); (2) rotation; and (3) Gaussian noise applied to a random portion of theimage, N(2, 22). Additionally, we examine variations in the gravity constant g for unseen dynamics.These perturbation patterns align with those commonly used in robustness studies (). For the Walker task, the parameters are set as 1 = 2 = 0.5 and 22 = 0.15, while for the Quadrupedtask, 1 = 0, 2 = 0.05, and 22 = 0.2. In each case, we investigate a range of noise levels: (1) variance2 ranging from 0.05 to 0.55; (2) rotation angles of 20 and 30; and (3) masked image percentages% ranging from 25% to 75%. For the unseen dynamics, the gravity constant g is varied from 9.8 to1.",
  ": Generalization against increasing degree of perturbation": "It can be seen from and that thanks to the adoption of Jacobian regularizationin training, the rewards (averaged over 5 trials) are higher compared to the baseline, indicatingimproved robustness to noisy image states in all cases. Moreover, demonstrates that the modeltrained with Jacobian regularization consistently outperforms the baseline under most dynamics variations. These experimental results support the findings in Corollary 3.4, showing that regulariz-ing the Jacobian norm effectively stabilizes the implicit regularization process, leading to enhancedperformance and robustness.",
  ": Evaluation on unseen dynamics by various gravity constants (g = 9.8 is default). = 0.01": "As shown in and 14, the experimental results indicate that models trained with Jacobianregularization outperform those using augmentation methods when faced with perturbations differ-ent from those used during augmentation. While state augmentation is effective when the inferenceperturbations match those used in training, it struggles to generalize to unseen perturbations. Incontrast, Jacobian regularization is less dependent on the diversity and relevance of augmented datasamples, as it directly targets the learning dynamics of the world model. This makes it more broadlyapplicable and reduces the likelihood of overfitting, avoiding the risk of the model becoming overlyspecialized to specific perturbation patterns, which is a common challenge with data augmentation.",
  "To observe the error propagation of zero-drift and non-zero-drift error signals in latent states, werefer to the visualizations of reconstructed state trajectory samples in the Appendix D.7": "Faster convergence on tasks with extended horizon. We further evaluate the efficacy of Jacobianregularization in tasks with extended horizon, particularly by extending the horizon length inMuJoCo Walker from 50 to 100 steps. shows that the model with regularization convergessignificantly faster ( 100K steps) than the case without Jacobian regularization in training. Thiscorroborates results in Theorem 4.1 that regularizing the Jacobian norm can reduce error propagation.",
  ". Conclusion": "In this study, we investigate the robustness and generalization of world models. We develop an SDEformulation by treating LDM as a stochastic dynamical system, and characterize the effects of latentrepresentation errors for zero-drift and non-zero drift cases. Our findings, based on both theoreticand experimental studies, reveal that for the case with zero drift, modest latent representation errorscan paradoxically function as implicit regularization and hence enhance robustness. To mitigatethe compounding effects of non-zero drift, we applied Jacobian regularization, which enhancedtraining stability and robustness. Our empirical studies corroborate that Jacobian regularizationimproves generalization, broadening the models applicability in complex environments. This workhas the potential to improve the robustness and reliability of RL agents, especially in safety-criticalapplications like autonomous driving. Future work can extend this study to other world modelssuch as with transformers-based LDM. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, andJames Davidson. Learning latent dynamics for planning from pixels. In International conferenceon machine learning, pages 25552565. PMLR, 2019.",
  "Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domainsthrough world models. arXiv preprint arXiv:2301.04104, 2023": "Samuel Kessler, Mateusz Ostaszewski, Micha Bortkiewicz, Mateusz arski, Maciej Woczyk,Jack Parker-Holder, Stephen J. Roberts, and Piotr Mio. The effectiveness of world models forcontinual reinforcement learning. CoLLAs 2023, 2023. C. Daniel Freeman, Luke Metz, and David Ha. Learning to predict without looking ahead: Worldmodels without forward prediction. Thirty-third Conference on Neural Information ProcessingSystems (NeurIPS 2019), 2019. URL Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel, and Ken Goldberg. Day-dreamer: World models for physical robot learning. In Proceedings of The 6th Conference on RobotLearning, volume 205 of PMLR, pages 22262240, 2023. Kuno Kim, Megumi Sano, Julian De Freitas, Nick Haber, and Daniel Yamins. Active worldmodel learning with progress curiosity. In Proceedings of the 37th International Conference onMachine Learning (ICML), 2020. Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, JamieShotton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving.arXiv preprint arXiv:submit/1234567, Sep 2023. Submitted on 29 Sep 2023.",
  "Soon Hoe Lim, N Benjamin Erichson, Liam Hodgkinson, and Michael W Mahoney. Noisyrecurrent neural networks. Advances in Neural Information Processing Systems, 34:51245137,2021": "Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, XavierBoix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: Explaining thenon-overfitting puzzle. arXiv preprint arXiv:1801.00173, 2017. Jure Sokoli, Raja Giryes, Guillermo Sapiro, and Miguel RD Rodrigues. Generalization error ofdeep neural networks: Role of classification margin and data structure. In 2017 InternationalConference on Sampling Theory and Applications (SampTA), pages 147151. IEEE, 2017.",
  "Suyoung Lee and Sae-Young Chung. Improving generalization in meta-rl with imaginary tasksfrom latent dynamics mixture. In Advances in Neural Information Processing Systems (NeurIPS).NeurIPS, 2021": "Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. Robust re-inforcement learning using offline data. In Advances in Neural Information Processing Systems(NeurIPS). NeurIPS, 2022. Zuxin Liu, Zijian Guo, Zhepeng Cen, Huan Zhang, Jie Tan, Bo Li, and Ding Zhao. On therobustness of safe reinforcement learning under observational perturbations. In InternationalConference on Learning Representations (ICLR). ICLR, 2023. Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learningenvironment: An evaluation platform for general agents. Journal of Artificial Intelligence Research,47:253279, 2013.",
  "Judy Hoffman, Daniel A. Roberts, and Sho Yaida. Robust learning with jacobian regularization,2019": "Sebastian Curi, Ilija Bogunovic, and Andreas Krause. Combining pessimism with optimismfor robust and efficient model-based deep reinforcement learning. In Proceedings of the 38thInternational Conference on Machine Learning, volume 139, pages 22102220. PMLR, 2021. Ke Sun, Yingnan Zhao, Shangling Jui, and Linglong Kong. Exploring the training robustnessof distributional reinforcement learning against noisy state observations. In Proceedings of theEuropean Conference on Machine Learning and Principles and Practice of Knowledge Discovery inDatabases (ECML PKDD), 2023. Ruida Zhou, Tao Liu, Min Cheng, Dileep Kalathil, P. R. Kumar, and Chao Tian. Natural actor-critic for robust reinforcement learning with function approximation. In Advances in NeuralInformation Processing Systems. NeurIPS, 2023.",
  "A. Approximation Power of Latent Representation with CNNEncoder and Decoder": "In this section, we show that the latent representation error, in the form of approximation errorcorresponding to the widely used CNN encoder-decoder, could be made sufficiently small by findingappropriate CNN network configuration. In particular, this result provides theoretical justificationto interpreting latent representation error as stochastic perturbation in the dynamical system definedin Equations (5 - 8), as the error magnitude can be made sufficiently small by CNN networkconfiguration, and the analysis carries over to other architectures (e.g., ReLU) along the same line.",
  "Any two charts are Ck,-compatible with each other, that is for all 1, 2 A, 1 12 :2(U1 U2) 1(U1 U2) is Ck,": "Intuitively, a Ck, manifold is a generalization of Euclidean space by allowing additional spaces withnontrivial global structures through a collection of charts that are diffeomorphisms mapping opensubsets from the manifold to open subsets of euclidean space. For technical utility, the defined chartsallow to transfer most familiar real analysis tools to the manifold space. For more references, see. Definition A.2 (Riemannian volume form). Let X be a smooth, oriented d-dimensional manifoldwith Riemannian metric g. A volume form dvolM is the canonical volume form on X if for any pointx X, for a chosen local coordinate chart (x1, ..., xd), dvolM =",
  "Consider the state space S RdS and the latent space Z. Consider a state probability measure Q onthe state space S and a probability measure P on the latent space Z": "Assumption A.3. (Latent manifold assumption) For a positive integer k, there exists a dM-dimensional Ck, submanifold M (with Ck+3, boundary) with Riemannian metric g and has positivereach and also isometrically embedded in the state space S RdS and dM << dS, where the stateprobability measure is supported on. In addition, M is a compact, orientable, connected manifold.",
  "and importantly they have the proper smoothness guarantee, namely genc Ck+1,(M, Z) andgdec Ck+1,(Z, M). Proposition A.7 shows the existence of such oracle map(s)": "Proposition A.7 (Ck,, compact). Let M, N be compact, oriented d-dimensional Riemannian manifoldswith Ck+3, boundary with the volume measure M and N respectively. Let Q, P be distributions supportedon M, N respectively with their Ck, density functions q, p, that is Q, P are probability measures supportedon M, N with their Radon-Nikodym derivatives q Ck,(M, R) w.r.t M and p Ck,(N, R) w.r.t N .Then, there exists a Ck+1, map g : N M such that the pushforward measure g#P = Q, that is for anymeasurable subset A B(M), Q(A) = P(g1(A)).",
  "M = 1": "Let F : N M be an orientation-preserving local diffeomorphism, we then have det(dF) > 0everywhere on N.As N is compact and M is connected by assumption, F is a covering map, that is for every pointx M, there exists an open neighborhood Ux of x and a discrete set Dx such that F 1(U) =D V N and F|V = V U is a diffeomorphism. Furthermore, |Dx| = |Dy| for any pointsx, y M. In addition, |Dx| is finite from the compactness of N.",
  "where (h, z) is a policy function as a local maximizer of value function and the stochastic process stis Ft-adapted": "As discussed in the main paper, our analysis applies to a common class of world models that usesGaussian distributions parameterized by neural networks outputs for z, z, s. Their distributions arenot non-Gaussian in general. For example, as z is conditional Gaussian and its mean and variance are random variables whichare learned by the encoder from r.v.s s and h as inputs, thus rendering z non-Gaussian. However,z is indeed Gaussian when the inputs are known. Under this conditional Gaussian class of worldmodels, to see that the continuous formulation of latent dynamics model can be interrupted as SDEs,one notices that SDEs with coefficient functions of known inputs are indeed Gaussian, matching tothis class of world models. Formally, in the context of z without latent representation error:",
  "E suptR(t)2 d, ,(iii)": "where to obtain (i) we used Jensens inequality when k = 0 and by the Burkholder-Davis-Gundyinequality when k = 1, used the Lipschitz condition of gk to obtain (ii), and for (iii), it is because3R(t) = xt xt.We note that from Taylors theorem, for any s [0, t], k = 0, 1, there exists some s (0, ) s.t.",
  ",(vii)": "where for (iv), we used Equation (42) and Jensens inequality for k = 0 and the Burkholder-Davis-Gundy inequality for k = 1; to obtain (v), we applied Jensens equality; we then derived (vi) fromthe Lipschitz conditions of gk and k; and finally another application of Jensens inequality gives(vii) which is bounded as a result from the Lemma B.6.",
  "xi , 2ij xt =2xt": "xixj , for i, j = 1, . . . , n that are,respectively, the first and second-order derivatives of the solution x(t) w.r.t. the changes in thecorresponding coordinates of the initial value. When = 0 Rn, we denote the solutions to Equation(47) as x0t with its first and second derivatives i x0t, 2ij x0t, respectively.",
  "D.1. Model Implementation and Training": "Our baseline is based on the DreamerV2 Tensorflow implementation. Our theoretical and empiricalresults should not matter on the choice of specific version; so we chose DreamerV2 as its codebaseimplementation is simpler than V3. We incorporated a computationally efficient approximation ofthe Jacobian norm for the sequence model, as detailed in , using a single projection. Duringour experiments, all models were trained using the default hyperparameters (see ) for theMuJoCo tasks. The training was conducted on an NVIDIA A100 and a GTX 4090, with each sessionlasting less than 15 hours.",
  "D.2. Additional Results on Generalization on Perturbed States": "In this experiment, we investigated the effectiveness of Jacobian regularization in model trainedagainst a baseline during the inference phase with perturbed state images. We consider three types ofperturbations: (1) Gaussian noise across the full image, denoted as N(1, 21) ; (2) rotation; and (3)noise applied to a percentage of the image, N(2, 22). (In Walker task, 1 = 2 = 0.5, 22 = 0.15; inQuadruped task, 1 = 0, 2 = 0.05, 22 = 0.2.) In each case of perturbations, we examine a collectionof noise levels: (1) variance 2 from 0.05 to 0.55; (2) rotation degree 20 and 30; and (3) maskedimage percentage % from 25 to 75.",
  "D.5. Additional Results on Robustness against Encoder Errors": "In this experiment, we evaluate the robustness of model trained with Jacobian regularization againsttwo exogenous error signals (1) zero-drift error with t = 0, 2t (2t = 5 in Walker, 2t = 0.1 inQuadruped), and (2) non-zero-drift error with t , 2t uniformly. weight of Jacobianregularization is 0.01. In this section, we included plot results of both evaluation and training scores.",
  "D.5.1. Walker Task": "Under the Walker task, Figures 3 and 4 show that model with regularization is significantly lesssensitive to perturbations in latent state zt compared to the baseline model without regularization.This empirical observation supports our theoretical findings in Corollary 3.4, which assert that theimpact of latent representation errors on the loss function L can be effectively controlled by regulatingthe models Jacobian norm.",
  "D.5.2. Quadruped Task": "Under the Quadruped task,we initially examined a smaller latent error process (t = 0, 2t = 0.1) andobserved that the model with Jacobian regularization converged significantly faster, even though theadversarial effects on the model without regularization were less severe (). When consideringthe more challenging latent error process (t , 2t ), we noted that the regularizedmodel remained significantly less sensitive to perturbations in latent state zt, whereas the baselinemodel struggled to learn (). These empirical observations reinforce our theoretical findingsin Corollary 3.4, demonstrating that regulating the models Jacobian norm effectively controls theimpact of latent representation errors.",
  "D.6. Comparison of Jacobian Regularization and Augmentation Methods withKnown Perturbation Types": "In cases where additional knowledge about perturbation is available, such as when the perturba-tion type is known a priori (which could be unrealistic), one could consider using augmentationmethods by training with perturbed observations to improve robustness. We considered trainingwith observation images augmented with (1) randomly-masked Gaussian noises N(0.15, 0.1) and(2) rotations 10.",
  "D.7. Visualizations of reconstructed state trajectory under exogenous zero-driftand non-zero drift latent representation error": "In this section, we present visualizations of reconstructed state trajectory samples, included in therevision to illustrate the error propagation of exogenous zero-drift and non-zero drift error signalsin latent states, both with and without Jacobian regularization. As depicted in Figures 7 and 8, the reconstructed states for the baseline model without Jacobianregularization appear blurry and less structured, indicating that the model has not effectivelycaptured the underlying dynamics of the environment. In contrast, the reconstructed states for themodel with Jacobian regularization are sharper and more accurately reflect the true dynamics of theenvironment. The visual comparison highlights the robustness brought by Jacobian regularizationagainst latent noises.",
  "D.8. Additional results on faster convergence on tasks with extended horizon": "In this experiment, we evaluate the efficacy of Jacobian regularization in extended horizon tasks,specifically by increasing the horizon length in MuJoCo Walker from 50 to 100 steps. We tested tworegularization weights = 0.1 and = 0.05. demonstrates that models with regularizationconverge faster, with = 0.05 achieving convergence approximately 100,000 steps ahead of themodel without Jacobian regularization. This supports the findings in Theorem 4.1, indicating thatregularizing the Jacobian norm can reduce error propagation, especially over longer time horizons."
}