{
  "Abstract": "Mental health has increasingly become a globalissue that reveals the limitations of traditionalconversational psychotherapy, constrained bylocation, time, expense, and privacy concerns.In response to these challenges, we introduceSOULSPEAK, a Large Language Model (LLM)-enabled chatbot designed to democratize ac-cess to psychotherapy. SOULSPEAK improvesupon the capabilities of standard LLM-enabledchatbots by incorporating a novel dual-memorycomponent that combines short-term and long-term context via Retrieval Augmented Gener-ation (RAG) to offer personalized responseswhile ensuring the preservation of user privacyand intimacy through a dedicated privacy mod-ule. In addition, it leverages a counseling chatdataset of therapist-client interactions and var-ious prompting techniques to align the gener-ated responses with psychotherapeutic methods.We introduce two fine-tuned BERT modelsto evaluate the system against existing LLMsand human therapists: the Conversational Psy-chotherapy Preference Model (CPPM) to sim-ulate human preference among responses andanother to assess response relevance to user in-put. CPPM is useful for training and evaluatingpsychotherapy-focused language models inde-pendent from SOULSPEAK, helping with theconstrained resources available for psychother-apy. Furthermore, the effectiveness of the dual-memory component and the robustness of theprivacy module are also examined. Our find-ings highlight the potential and challenge ofenhancing mental health care by offering analternative that combines the expertise of tra-ditional therapy with the advantages of LLMs,providing a promising way to address the ac-cessibility and personalization gap in currentmental health services.",
  "*Correspondence to": "era of global economic uncertainty, rampant war,social isolation, ever-increasing class disparities,and general political nihilism, it is of little surprisethat rates of depression and suicide have soared inrecent years. Global rates of anxiety and depres-sion increased by roughly 25% within the first yearof the COVID-19 pandemic (World Health Orga-nization, 2022a). This increase in mental healthissues has severe negative implications, includingwidespread suicide and decreased work productiv-ity. Every year, according to a World Health Or-ganization (2023b) press statement, over 700,000people worldwide commit suicide, with countlessmore attempting to take their own life. It was esti-mated that poor mental health cost the world econ-omy $2.5 trillion USD per year in 2010, a figurepredicted to rise to a staggering $6 trillion USD peryear by 2030 (The Lancet Global Health, 2020).Psychotherapy can be used to treat many cases ofmental health conditions (World Health Organiza-tion, 2023a). Despite the pressing need for mentalhealth care, the cost of therapy and psychiatric careremains prohibitively expensive for many. In onenationally representative study conducted in theUnited States by Mental Health America (2023), itwas found that about 42% of adults with any typeof mental illness reported that they were unableto receive treatment because they could not affordit, highlighting the inaccessibility of mental healthcare to the general public.With the advancement of data-driven languagemodels (LMs) in the field of Natural Language Pro-cessing (NLP), many works were developed to en-hance mental health treatment, such as comparingthe effectiveness of different therapy methods (Al-thoff et al., 2016), measuring the impact of the so-cial medium on mental conditions (Choudhury andDe, 2014), analyzing records of past treatments (Iveet al., 2020), predicting mental conditions (Zhanget al., 2022), etc. However, using LMs to gener-ate psychotherapeutic responses directly remained",
  "arXiv:2412.02987v1 [cs.CL] 4 Dec 2024": "challenging until the emergence of Large LanguageModels (LLMs). LLM-powered chatbots offer thetantalizing possibility of a low-cost, readily acces-sible alternative to traditional psychotherapy (Heet al., 2023). However, current efforts towards thedevelopment of therapeutic chatbots are often fo-cused on immediate, short-term solutions, despitethe fact that those who struggle with mental healthissues may reap greater benefits from long-termcare (Guo et al., 2024). Many therapy chatbotsfail to remember critical user information frompast conversations, greatly reducing their efficacy.This, combined with data privacy concerns, ledto our proposal of SOULSPEAK, an LLM-basedconversational agent with long-term memory, dataanonymization, and domain expertise that provideseffective, low-cost talk therapy to its users.SOULSPEAK, as detailed in , has sev-eral key advantages over existing online chat-bots like ChatGPT. The first of which is itsmemory module, which compartmentalizes im-portant information from conversations into short-term and long-term contexts. The memory mod-ule enables SOULSPEAK to keep track of cru-cial information from past conversations, suchas important individuals and events, over longperiods, greatly enhancing SOULSPEAKs abilityto understand, connect, and empathize with itsusers, a skill which is paramount in psychotherapy.Secondly, SOULSPEAK integrates an extensivedataset of anonymized conversations between pro-fessional therapists (Counsel Chat dataset) (Bertag-nolli, 2020) and their clients into its Retrieval-Augmented Generation (RAG) process (Lewiset al., 2020), allowing it to provide responses thatfollow psychotherapeutic methods. Lastly, SOUL-SPEAK has a robust privacy module that utilizes acapable Named Entity Recognition (NER) modelto detect, anonymize, and restore Personal Identi-fiable Information (PII), ensuring maximum userprivacy.In addition, we introduce the Conversational Psy-chotherapy Preference Model (CPPM) to simulatehuman preference over responses to mental health-related queries. CPPM simulates human preferencein responses to a therapeutic question with valida-tion accuracy of over 97%. Together with othermetrics to evaluate individual responses, CPPM isexplained in . We also discuss the statisti-cal difference between human-produced and LLM-generated text in response to therapeutic questions,which can be used for future development. Fur- thermore, in , we test the effectivenessof SOULSPEAKs long-term memory module anddiscuss the robustness of the privacy module. Weconclude with a discussion of the limitations ofthis research and possible areas for future work in.",
  "Generative language models": "Language generation is an essential component ofArtificial Intelligence (AI) and at the core of NLP.Alan Turing defined AI as a machine with the abil-ity to exhibit intelligent behavior equivalent to, orindistinguishable from, that of a human. In his fa-mous Turing test, intelligent behavior is accessedthrough a conversation where the AI generates textin response to human input. Early works on NLPapproached language generation by manually set-ting linguistic rules and then shifted focus to statis-tical approaches. Statistical N-gram models (Suen,1979) outperformed rule-based linguistic modelsand provided the foundation for Machine Learning(ML) based language models. Various data-drivenmodels emerged with the rapid development of MLtheories and computing devices, including the auto-regressive models developed on N-gram modelsto generate words one at a time based on previoussequential input or surrounding context. Due tothe success of the Transformer model (Vaswaniet al., 2017), auto-regressive models dominated theNLP research afterward. Benefiting from the riseof deep learning and big data, large-scale genera-tive language models such as Generative PretrainedTransformer (GPT) (Radford et al.) and Bidirec-tional Encoder Representations from Transform-ers (BERT) (Devlin et al., 2019) were created andimplemented, exhibiting far superior performancethan previous language models. GPT by OpenAIevolves over time with increased parameters from117 million, 1.5 billion to 175 billion. The newestGPT model, GPT-4 (Achiam et al., 2023), can per-form various types of text generation with multiplelanguages, showing the ability of in-context learn-ing and chain of thought due to emergence.",
  ": The SOULSPEAK system architecture and user workflow": "users (Thompson and Harper, 2011). In this pa-per and the scope of chatbots, the term serviceuser is used as language model-powered agentsare not licensed mental practitioners who can givemedical treatment despite the ongoing debate overthe usage of patient versus service users (Priebe,2021).Through conversational psychotherapy,mental health therapists build a personal and safeenvironment professionally for service users to ex-press their thoughts and feelings (Swift et al., 2018),requiring the therapists to emphatically interpretthe words of the service users and respond in acarefully designed way to drive the conversation.However, traditional in-person psychotherapy re-quires the service user to make appointments inadvance and go to a specific location at a specifictime for a fixed period. This process is tedious, andthe limitation of time and location may not workfor everyone. Due to this, online psychotherapyquickly gained the favor of service users, whichis more flexible, accessible, and affordable (Stollet al., 2020). Despite the advantages of onlinepsychotherapy over traditional psychotherapy, theavailability of therapists and expenses remain anobstacle for some potential service users. With the advancement of generative LMs, chat-bots become more capable of interpreting, model-ing, and generating natural languages, which can beutilized for conversational psychotherapy (Deneckeet al., 2020; Chung et al., 2020; He et al., 2022;Wei et al., 2023; Chen et al., 2023). As shown byGuo et al. (2024) in their survey, there was a hugeincrease in conversational agents in 2022. This in-crease was a collaborative result of the increase indemand for psychotherapy due to COVID-19 andthe emergence of LLMs. A similar study conductedby Haque and Rubya (2023) surveyed ten of themost prominent chatbot applications in the mentalhealth space. They found that users generally val- ued the chatbots for their ability to communicatein a human-like manner and 24/7 availability; how-ever, they also identified several key faults in popu-lar chatbot technology. These included the usage ofan overly cheerful tone in inappropriate situationsand an overreliance on scripted responses. More-over, the chatbots lacked the ability to referenceprevious conversations in new responses, insteadrelying on users to self-report mentioned people orevents repeatedly. Lastly, the chatbots in the surveylacked the ability to recommend specific treatmentplans based on individual user needs, offering onlylimited or short-term interventions.",
  "Privacy module": "The privacy module is a gateway between the usersand all the internal downstream processes in SOUL-SPEAK. Its primary role is to ensure that no PIIsare retained within the systems memory modulenor transmitted to third-party APIs. We use a ro-bust NER model adept at pinpointing PII, includingperson names, locations, and date times within userinput. Upon detection, the identified PII is sub-stituted with anonymized placeholdersrandomlygenerated values that preserve the semantic struc-ture without revealing sensitive information. Thesevalues are paired with their corresponding PII ina secure mapping to restore the original data intothe LLM-generated content that references these anonymized entities. This process is partly inspiredby the methodologies of Langchain (Langchain.ai,2023). We further improve its implementation byintegrating the PII detection process with our long-term memory module, which will be addressed inthe following sections.We use the model \"en_core_web_lg-3.7.1\" fromthe spaCy NLP library as the default model to per-form name entity recognition on the user input,which achieves a precision of 85.16% on the bench-mark dataset available from spaCys model releases(Explosion.ai, 2023).",
  "Knowledge base": "The knowledge base is designed to augment theuser query to the LLM with domain expertise fromcertified therapists. To do so, we use the Coun-sel Chat dataset (Bertagnolli, 2020), a high-qualityopen-source dataset of counseling responses. Thedata consists of user questions, responses from li-censed therapists, and metadata, including viewsand upvotes of the therapists response, all ex-tracted from counselchat.com. A detailed intro-duction of the Counsel Chat dataset can be foundin Appendix A. We use a PostgreSQL instance witha vector extension to vectorize the questions intoembeddings. Then, the anonymized user query willretrieve the most relevant question through a vectorsimilarity search. Among all the responses to thequestion, the knowledge base will return the top-kresponses based on the preference score, as definedin .1. In our experiments, we use k = 1.",
  "Memory module": "The memory module consists of both short-termand long-term memory. The short-term memorymaintains the most recent n interactions betweenthe user and LLM in its conversation history. Dur-ing the RAG process, the agent appends the historyto its prompt to provide the LLM with immediatecontext, allowing the agent to maintain a coherentand responsive dialogue flow. In our experiments,we set the n = 10 by default.The long-term memory persists in LLM-generated summaries of various entities the usermentioned across different therapy sessions. Its re-alized through Entity Store, which maps the entitynames and their associated summaries. In partic-ular, we leverage the PII detection infrastructurefrom the privacy module to insert entities into theentity store automatically. We recognize that thefull context surrounding an entity typically spans several interactions between the user and the LLM.For example, when a user mentions a name duringtherapy sessions, the relevant details and contextabout that person are often scattered across multipleinteractions. To ensure accuracy and completeness,we update the entity store periodically every n in-teraction. This avoids updating the entity store toofrequently with the same information and preventspotential overlaps between the entity summary andshort-term memory, i.e., the recent conversationhistory.When the Entity Store is updated, the existingentity summary and recent conversation historyare sent to the LLM, which is instructed to updatethe existing summary only if new information ispresent; otherwise, it remains unchanged. Whena user query mentions an entity, the Entity Storeprovides the most recent and relevant summary ofthat entity.",
  "Retrieval augmented generation process": "The final RAG process in SOULSPEAK inte-grates all previously mentioned components - theanonymized user query, retrieved therapist re-sponse from the knowledge base, short-term mem-ory, and long-term memory - using the selectedprompt template. Six different designs of promptsare implemented, which are discussed in detail inAppendix C. Note that the inclusion of retrieved therapist re-sponses and long-term memory is conditional: thetherapist response is incorporated only if the co-sine similarity score between the user query andthe questions in the therapy dataset exceeds a cer-tain threshold, , and the long-term memory isincluded only if the users query contains at leastone entity name present in our entity store. In ourexperiments, we chose = 0.2. We conductedpreliminary runs using values from 0.1 to 0.5, withincrements of 0.1. We analyzed the retrieved ther-apy responses regarding relevance and variety andidentified 0.2 as the sweet spot.This approach guarantees that the informationintegrated into our final prompt is relevant and ben-eficial for the agent to respond to the users needs. Counsel Chat dataset 1. Select questions with diverse answers",
  "Individual Response Analysis": "This section examines SOULSPEAKs performancein generating a response to a question in isola-tion, i.e., independent from the conversation con-text. 113 questions selected from the Counsel Chatdataset (Bertagnolli, 2020) are used as the test setfor evaluating the responses generated by SOUL-SPEAK under different prompt designs. All thechosen questions were answered by at least threetherapists with different numbers of views and up-votes. By doing so, we wish to ensure that the testset is representative of the emotional needs of ser-vice users and focuses on popular questions thatdemand more attention.SOULSPEAK with different prompt designs, thebaseline (the vanilla approach where only the ques-tion is provided to the LLM without additionalprompts or retrieval), and human responses fromthe Counsel Chat dataset are compared with eachother. The LLM used in the experiment is Ope-nAIs GPT-3.5-turbo-0125.",
  "Preference simulation": "A major challenge in evaluating the responses tomental health-related questions is to measure thequality of response from the perspective of theservice users. Due to the limited data on conver-sational psychotherapy and the fact that generalservice users do not get the chance to consult mul-tiple therapists while rating them, it is difficult toprovide a numeric metric for service user prefer-ence. Inspired by marking human preference inReinforcement Learning from Human Feedback(RLHF) (Ouyang et al., 2022), we decide to train a model to stimulate the human preference betweentwo responses, i.e., the Conversational Psychother-apy Preference Model (CPPM) as shown in Fig-ure 2. In addition to evaluation, CPPM can alsobe used in RLHF to train LLMs for mental-health-related applications. Preferencescore:TheCounselChatdataset(Bertagnolli,2020)containsconver-sations between service users and licensedtherapists in terms of question-answer pairs.One thing unique about this dataset is that eachquestion-answer pair has an associated numberof views and upvotes, providing a meaningfulway to measure the service users opinions on theresponses. A detailed introduction of the CounselChat dataset can be found in Appendix A. Wecompute a preference score as log(upvotes+1) log(views+1) . Bydividing the logarithm of the number of upvotesby the logarithm of the number of views, the scoreprovides a normalized measure of how favorablya therapists response is received relative to thenumber of people who have seen it. This approachhelps identify responses that are not only popularbut also proportionally well-received by thosewho view them. Note that it only makes sense tocompare preference scores among answers to thesame question, as different questions have differentpopularity. ConversationalPsychotherapyPreferenceModel (CPPM):A BERT model fine-tuned onthe Counsel Chat dataset is used to simulate theuser preference on generated responses. CPPMtakes two responses to the same question and human-best baseline default 7feelings 7feelings2tonesgkp gkpPsychoTherapy gkpPsychoTherapyNonRep 10097.35 80.5378.7677.88 69.9173.45 82.3077.88 % prefers the other% prefers human-worst",
  "(b) Comparison with therapists responses of highest preferencescore": ": Simulation of user preference on generated responses and therapists responses. Human-worst/human-bestrepresents the set of responses with the lowest/highest preference score for each question in the test set. outputs the response with a higher preference score.To train CPPM, sets of two human responsesanswered by different therapists with differentpreference scores and the index of the responsewith the higher preference score were prepared astraining sets, as demonstrated in . Detailsabout this model can be found in Appendix B.1.Two human responses appeared twice in thetraining set in reverse order with an adjusted indexlabel to minimize the effect of the order of inputresponses. After finetuning, CPPM achieved above97% validation accuracy in the validation set. Inaddition, when applying this metric in experiments,two responses were fed to CPPM twice in reverseorder, and the average of the two output logits wasused to generate the final prediction. Results:As shown in a, SOULSPEAKwith different prompt designs outperforms the ther-apists responses with the lowest preference score.This means that when a service user in need is pre-sented with two answers in response to his/her ques-tion, the service user is more likely to prefer the an-swer generated by SOULSPEAK, assuming that thesimulation by CPPM is representative of the truedistribution of preferences. Although the responsesgenerated by SOULSPEAK are not preferred overthe best responses from therapists, as shown in Fig-ure 3b, this preference evaluation demonstrates thatSOULSPEAK is able to produce responses compara-ble to licensed professional therapists. In addition,this comparison shows the effectiveness of CPPMas a simulation tool for evaluation.",
  "Relevance, readability, polarity, andsubjectivity": "To measure the relevance of a response to a mentalhealth-related prompt, we wish to have an embed-ding method tuned to the domain. Therefore, aBERT model finetuned on the Counsel Chat datasetfor classifying the topics of questions is used to em-bed the text responses into vector representations.In the Counsel Chat dataset, every question is as-signed a topic. Due to the relatively small sizeof the datasets and the imbalance distribution ofthe topics, we filtered out some of the original 31topics and merged the rest into five broader topics:mental health disorders (1330 samples), trauma andcoping (281 samples), interpersonal relationships(1338 samples), behavioral regulation (513 sam-ples), and sexuality and intimacy (586 samples).After finetuning, the classification model achieveda validation accuracy of over 70%. Details aboutthis model can be found in Appendix B.2. In exper-iments, we computed the cosine similarity betweenthe embedding of the question and the responses tomeasure the response relevance.The readability of a response is computed usingthe Flesch reading-ease test (Flesch, 1948; Farret al., 1951) as 206.8351.015( NW",
  "NS )84.6( Ns": "NW )where NW is the total number of words, NS isthe total number of sentences, and Ns is the totalnumber of syllables. A higher score indicates betterreadability, i.e., easier to read and understand. Notethat readability is divided by 100 to transform from to for better presentation.The sentiment analysis is performed using theTextBlob library. Polarity and subjectivity are com-puted for every response. Polarity is a continuous human-worst human-best baseline default 7feelings 7feelings2tones gkp gkpPsychoTherapy gkpPsychoTherapyNonRep 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.760.75 0.700.700.710.700.690.700.71 0.660.66 0.57 0.550.55 0.57 0.51 0.490.50 0.150.15 0.19 0.160.17 0.15 0.170.160.15 0.520.52 0.630.630.620.62 0.600.600.61 RelevanceReadabilityPolaritySubjectivity",
  ": Statistical comparison between the therapists and SOULSPEAK generated responses": "value in , where the negativity decreases asthe value increases. Polarity indicates the emo-tional learning of the text. Subjectivity is a con-tinuous value in , quantifying the amount ofpersonal and factual information in the text. Thehigher the subjectivity, the more personal opin-ions are expressed.While the accuracy of theTextBlob library in determining polarity and subjec-tivity scores may not be on par with state-of-the-artdeep learning models, it does provide a simple,interpretable way to get a broad sense of a textssentiment. Results:As shown in , we observe thatthe therapists responses have similar scores forthese metrics, while the generated responses sharesimilar results. This shows that while the pref-erence score of therapists responses may vary,they are still very similar statistically. In addi-tion, despite the different prompt designs applied,the generated responses by SOULSPEAK are sta-tistically similar regarding evaluation results un-der these metrics. We conduct statistical tests toverify the difference between the therapists re-sponses and generated responses: Shapiro-Wilktest (Shapiro and Wilk, 1965) to check normality,Levenes test (Levene, 1961) to check the similarityof variance, and Welchs t-test (Ahad and Yahaya,2014) and Mann-Whitney U test (Nachar, 2007) tocheck similarity.Since the distributions of relevance and read-ability are neither normally distributed nor haveequal variance, as indicated by the extremely lowp-values of the Shapiro-Wilk test and Levenes test, the Mann-Whitney U test is chosen instead of theindependent t-test or Welchs t-test. For relevance,the p-value (0.275) of the Mann-Whitney U test isinsignificant, suggesting no significant differencebetween the two distributions regarding their cen-tral tendencies. Thus, both human response andSOULSPEAK generated responses are comparablyrelevant to the question, although the response pro-vided by a therapist seems to have a higher rele-vance to the given question. For readability, theextremely small p-value (3.24 1039) stronglysupports the conclusion that there is a significantdifference, i.e., the human response is easier tounderstand.On the other hand, the distributions of polarityand subjectivity are normally distributed but haveunequal variance; thus, Welchs t-test is used. Forpolarity, the p-value slightly above the 0.05 sig-nificance level indicates no significant differencebetween the two sets. However, an extremely smallp-value for subjectivity indicates a highly signifi-cant difference, suggesting that the SOULSPEAKgenerated responses are more subjective.These characteristics are important for effectivecommunication with users needing emotional sup-port. Therefore, one extension of the current workmay be to adapt the characteristics of human re-sponses.",
  "This section examines the effectiveness of SOUL-SPEAKs long-term memory module, i.e. entitystore as proposed in . To evaluate the": "User: I've been having a tough time witha co-worker, Derek. He often dismissesmy ideas in meetings and it's starting toaffect my confidence at work. Therapist: That sounds challenging.How do you usually react when Derekdismisses your ideas? User: Mostly, I stay quiet. I feel like if Ispeak up, it might make things worse.Derek is quite influential in the team. Therapist: ...",
  "Entity Store": ": Example of one entity and its summaryin SOULSPEAKs entity store.The entity name isanonymized by the privacy module. The entity sum-mary is generated by an LLM API call given the contextof the previous therapy session. system both qualitatively and quantitatively, weuse GPT-4 to generate multiple scripted interactionscenarios. The interaction scenario is designed toinclude specific details about a person mentionedin the initial messages. Then, the user will ask aquestion related to that person later in the conver-sation, allowing for a clear test of the long-termmemory retrieval capabilities. An example can befound in the Appendix D. We generated 16 scriptedinteraction scenarios in total, covering a wide rangeof common topics in therapy.To evaluate performance, we feed past conversa-tions into our memory module while deactivatingthe short-term memory. This approach ensuresthat the agent exclusively relies on the long-termmemory provided by the entity summarizer. Next,the users query is sent to the agent. Upon receiv-ing the output, it is compared with the baselineresponse generated by the agent using GPT-3.5with the long-term memory disabled. Additionally,we assess whether it includes the specific detailsthe user previously mentioned, as summarized in\"Key Information\" in in Appendix D.Qualitative Result:The qualitative results,presented in , highlight the modules ca-pability to retain and utilize key information fromprevious interactions. This has been consistentlyobserved across other therapy scenarios generated User: I'm having another meeting withDerek today. Any advice?",
  "Current Therapy Session": "Therapist: It sounds like you havesome concerns about your meetingswith Derek. It can be challenging whenyou feel dismissed or hesitant to speakup. One approach to consider isapproaching the meeting withconfidence and assertiveness. If youfind it difficult to communicate withDerek, it may be helpful to practicesome self-affirmations before themeeting to boost your confidence.Remember, your voice matters, andyour ideas are important.",
  "Souspeak Response": "Therapist: Is there anything specificabout the meeting with Derek that you'reconcerned about? GPT 3.5 Response : Effectiveness of long-term memory mod-ule. Unlike the baseline GPT-3.5 model, SOULSPEAKadeptly records and retrieves relevant details about theentity \"Derek\" from the entity store as shown in Fig-ure 5. This functionality enables the agent to effectivelyincorporate context from past user-therapist interactions,as evidenced by its accurate recall of the users feelingsof being \"dismissed or hesitant to speak up.\"",
  "GPT-3.5 Baseline0.40890.3091SOULSPEAK0.64950.4911": ": Effect of long-term memory module. Aver-age relevance scores (cosine similarity) were computedover 16 GPT-4 generated therapy scenarios. The ex-pected therapist response is generated by GPT-4 giventhe knowledge from previous therapy sessions. Theexpected information is the GPT-4 summary of previ-ous therapy sessions. GPT-3.5 Baseline refers to theresponse produced by GPT-3.5-turbo-0125 without ac-cess to the knowledge from previous therapy sessions. by GPT-4, indicating the robustness of the entitystore in capturing essential information mentionedby the user.Quantitative Result:The quantitative resultis summarized in . The long-term mem-ory modules performance is quantified using therelevance scores defined in .2. We com-pare the outputs of both SOULSPEAK and GPT-3.5with the GPT-4 generated sample answers and keyinformation identified in each scenario (examplesof \"Sample Answer\" and \"Key Information\" areprovided in the Appendix ). Soulspeakwith long-term memory demonstrates a markedimprovement over the GPT-3.5 baseline. Its re-sponses show greater alignment with the \"groundtruth\" provided by GPT-4 generated answers andkey information summaries in the test cases. To-gether with the qualitative insights, we concludethat the long-term memory module effectively en-riches the agents responses by integrating usefulinformation from earlier therapy sessions into thecurrent context.",
  "Limitations": "This paper presents SOULSPEAK, an LLM-enabledconversational agent that provides low-cost mentalhealth therapy. SOULSPEAK is novel in its preser-vation of privacy, its psychological nuance whenanswering questions, and most importantly, its abil-ity to remember content from previous user inter-actions and integrate it in context with a long-termmemory module. An extensive evaluation is con-ducted with a dataset of chats between licensedtherapists and their service users, including finetun-ing two BERT models for simulating preferenceand encoding text response, to compare responsesfrom therapists and SOULSPEAK of different set-tings. Experimental results prove the effectivenessof SOULSPEAK as a possible alternative to tradi-tional therapy sessions. In the future, we plan toenrich our prompt designs and design prompts towork better with different therapy topics. In addi-tion, we wish to release SOULSPEAK to the publicand receive feedback to improve its performanceiteratively.Our preliminary research on SOULSPEAK doesnot necessarily reflect its long-term efficacy intreating mental illnesses such as anxiety and de-pression. Further research in controlled, clinicalsettings is required before drawing definitive con-clusions on the effectiveness of SOULSPEAK. Inaddition, SOULSPEAK is not intended to replaceprofessional psychotherapy practitioners. Instead,it aims to provide an alternative for people withouteasy access to professionals or for those with non-severe mental health issues. We hope this studyencourages the general LLM community to focusmore on psychotherapy-related questions and it-erate on our system and model. Note that usingthe current system and model as a sole source fortreating mental health issues may impose risk onthe service users. We would like to emphasize theimportance of seeking help from licensed profes-sionals when needed.SOULSPEAKs performance relies heavily onthe quality of the dataset it was trained on. Dueto privacy concerns related to psychotherapy, re-lated data is very limited. Nevertheless, with 433licensed therapists answering 940 questions (total-ing 2775 question-answer pairs), we believe theCounsel Chat dataset provides a solid foundationfor our initial study.Future work can involveopen-domain conversations. Inspired by works likeMental-LLM (Xu et al., 2024), we plan to train and evaluate the SOULSPEAK system on mentalhealth-related subreddit posts in future research.In terms of the privacy module, we acknowledgethat anonymization might reduce the contextualaccuracy and coherence of the LLM output. Forinstance, random dates and names may not alignwith real-world contexts, potentially leading theLLM to generate less relevant responses. However,we believe that this feature is especially importantin the field of psychotherapy. Compared to otherLLMs with internal memory solutions like Chat-GPT (OpenAI, 2024), SOULSPEAK ensures thatall PII is stored locally and will not be exposed tothird-party APIs.Human evaluation would be valuable, but dueto the special conditions and privacy concerns as-sociated with psychotherapy, we were unable toconduct human evaluations on a meaningful scale.Instead, we trained the CPPM to simulate the pro-cess. We provided other numerical measures toenrich the evaluation, and for metrics such as read-ability and relevance, there were statistically signif-icant differences between the answers provided bytherapists and LLMs. We hope these insights willbe useful for future studies considering psychother-apy.To further assess the robustness of SOULSPEAK,we have also evaluated it using the more advancedGPT-4o model, with results included in AppendixE. GPT-4o was less preferred than GPT-3.5. Uponreviewing the generated responses, we found thatGPT-4o tends to use more formal writing and of-ten lists various points. It is also more likely torefuse to give suggestions, citing that it is notqualified to answer such questions. This suggeststhat psychotherapy-related responses may not alignwell with the instruction tuning objectives of GPT-4o. Prompts need to be adjusted accordingly formore advanced models.In future work, we plan to extend our evalua-tions to include open-source models like Llama2 and Llama 3. SOULSPEAK is designed to bemodel-agnostic, and we encourage the researchcommunity to apply it to open-source models andshare their findings.",
  "We thank Zaina Shaik, Yinglong Lin, Joyee Chen,and Jun Hyung Lee for discussing the idea andproviding suggestions and feedback": "OpenAI Josh Achiam, Steven Adler, Sandhini Agarwal,Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-man, Diogo Almeida, Janko Altenschmidt, Sam Alt-man, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-ing Bao, Mo Bavarian, Jeff Belgum, Irwan Bello,Jake Berdine, Gabriel Bernadett-Shapiro, Christo-pher Berner, Lenny Bogdonoff, Oleg Boiko, Made-laine Boyd, Anna-Luisa Brakman, Greg Brockman,Tim Brooks, Miles Brundage, Kevin Button, TrevorCai, Rosie Campbell, Andrew Cann, Brittany Carey,Chelsea Carlson, Rory Carmichael, Brooke Chan,Che Chang, Fotis Chantzis, Derek Chen, Sully Chen,Ruby Chen, Jason Chen, Mark Chen, BenjaminChess, Chester Cho, Casey Chu, Hyung Won Chung,Dave Cummings, Jeremiah Currier, Yunxing Dai,Cory Decareaux, Thomas Degry, Noah Deutsch,Damien Deville, Arka Dhar, David Dohan, SteveDowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti,Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix,Simon Posada Fishman, Juston Forte, Isabella Ful-ford, Leo Gao, Elie Georges, Christian Gibson, VikGoel, Tarun Gogineni, Gabriel Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, ScottGray, Ryan Greene, Joshua Gross, Shixiang ShaneGu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,Yuchen He, Mike Heaton, Johannes Heidecke, ChrisHesse, Alan Hickey, Wade Hickey, Peter Hoeschele,Brandon Houghton, Kenny Hsu, Shengli Hu, XinHu, Joost Huizinga, Shantanu Jain, Shawn Jain,Joanne Jang, Angela Jiang, Roger Jiang, HaozhunJin, Denny Jin, Shino Jomoto, Billie Jonn, HeewooJun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali, In-gmar Kanitscheider, Nitish Shirish Keskar, TabarakKhan, Logan Kilpatrick, Jong Wook Kim, ChristinaKim, Yongjik Kim, Hendrik Kirchner, Jamie RyanKiros, Matthew Knight, Daniel Kokotajlo, LukaszKondraciuk, Andrew Kondrich, Aris Konstantini-dis, Kyle Kosic, Gretchen Krueger, Vishal Kuo,Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,Jade Leung, Daniel Levy, Chak Ming Li, RachelLim, Molly Lin, Stephanie Lin, Mateusz Litwin,Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Ade-ola Makanju, Kim Malfacini, Sam Manning, TodorMarkov, Yaniv Markovski, Bianca Martin, KatieMayer, Andrew Mayne, Bob McGrew, Scott MayerMcKinney, Christine McLeavey, Paul McMillan,Jake McNeil, David Medina, Aalok Mehta, JacobMenick, Luke Metz, Andrey Mishchenko, PamelaMishkin, Vinnie Monaco, Evan Morikawa, Daniel P.Mossing, Tong Mu, Mira Murati, Oleg Murk, DavidMely, Ashvin Nair, Reiichiro Nakano, RajeevNayak, Arvind Neelakantan, Richard Ngo, Hyeon-woo Noh, Ouyang Long, Cullen OKeefe, Jakub W.Pachocki, Alex Paino, Joe Palermo, Ashley Pantu-liano, Giambattista Parascandolo, Joel Parish, EmyParparita, Alexandre Passos, Mikhail Pavlov, AndrewPeng, Adam Perelman, Filipe de Avila Belbute Peres,Michael Petrov, Henrique Pond de Oliveira Pinto,Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong,Tolly Powell, Alethea Power, Boris Power, ElizabethProehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, KendraRimbach, Carl Ross, Bob Rotsted, Henri Roussez,Nick Ryder, Mario D. Saltarelli, Ted Sanders, ShibaniSanturkar, Girish Sastry, Heather Schmidt, DavidSchnurr, John Schulman, Daniel Selsam, Kyla Shep-pard, Toki Sherbakov, Jessica Shieh, Sarah Shoker,Pranav Shyam, Szymon Sidor, Eric Sigler, MaddieSimens, Jordan Sitkin, Katarina Slama, Ian Sohl,Benjamin D. Sokolowsky, Yang Song, Natalie Stau-dacher, Felipe Petroski Such, Natalie Summers, IlyaSutskever, Jie Tang, Nikolas A. Tezak, MadeleineThompson, Phil Tillet, Amin Tootoonchian, Eliz-abeth Tseng, Preston Tuggle, Nick Turley, JerryTworek, Juan Felipe Ceron Uribe, Andrea Val-lone, Arun Vijayvergiya, Chelsea Voss, Carroll L.Wainwright, Justin Jay Wang, Alvin Wang, BenWang, Jonathan Ward, Jason Wei, CJ Weinmann,Akila Welihinda, Peter Welinder, Jiayi Weng, LilianWeng, Matt Wiethoff, Dave Willner, Clemens Win-ter, Samuel Wolrich, Hannah Wong, Lauren Work-man, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Woj-ciech Zaremba, Rowan Zellers, Chong Zhang, Mar-vin Zhang, Shengjia Zhao, Tianhao Zheng, JuntangZhuang, William Zhuk, and Barret Zoph. 2023. Gpt-4 technical report.",
  "Siyuan Chen, Mengyue Wu, Ke Zhu, Kunyao Lan,Zhiling Zhang, and Lyuchun Cui. 2023.Llm-empowered chatbots for psychiatrist and patientsimulation:Application and evaluation.ArXiv,abs/2305.13614": "Munmun De Choudhury and Sushovan De. 2014. Men-tal health discourse on reddit: Self-disclosure, socialsupport, and anonymity. Proceedings of the Interna-tional AAAI Conference on Web and Social Media. Kyungmi Chung, Hee Young Cho, and Jin Young Park.2020. A chatbot for perinatal womens and partnersobstetric and mental health care: Development andusability evaluation study. JMIR Medical Informat-ics, 9. Kerstin Denecke, Sayan Vaaheesan, and Aaganya Arul-nathan. 2020. A mental health chatbot for regulat-ing emotions (sermo) - concept and usability test.IEEE Transactions on Emerging Topics in Comput-ing, 9:11701182.",
  "M D Romael Haque and Sabirat Rubya. 2023.Anoverview of chatbot-based mobile mental health apps:Insights from app description and user reviews. JMIRMhealth Uhealth, 11:e44838": "Tianyu He, Guanghui Fu, Yi Jing Yu, Fan Wang, Jian-qiang Li, Qing Zhao, Changwei Song, Hongzhi Qi,Dan Luo, Huijing Zou, and Bing Xiang Yang. 2023.Towards a psychological generalist ai: A survey ofcurrent applications of large language models andfuture prospects. ArXiv, abs/2312.04578. Yuhao He, Li Yang, Xiaokun Zhu, Bin Wu, Shuo Zhang,Chunlian Qian, and Tian-Ci Tian. 2022.Mentalhealth chatbot for young adults with depressive symp-toms during the covid-19 pandemic: Single-blind,three-arm randomized controlled trial. Journal ofMedical Internet Research, 24. Julia Ive, Natalia Viani, Joyce Kam, Lucia Yin, SomainVerma, Stephen Puntis, Rudolf N. Cardinal, AngusRoberts, Robert J Stewart, and Sumithra Velupillai.2020. Generation and evaluation of artificial mentalhealth records for natural language processing. NPJDigital Medicine, 3.",
  "ACounsel Chat Dataset": "The Counsel Chat dataset (Bertagnolli, 2020) con-tains data on individuals seeking assistance fromlicensed therapists and their associated responsesfrom CounselChat.com. The dataset is availablein GitHub and Huggingface under MIT license.CounselChat.com allows therapists to advertise fortheir clinics by providing sound, publicly availableadvice, so their names are not anonymized in thedataset. One unique feature of this dataset is thatit includes a record of the number of upvotes andviews for each question-answer pair, providing away to analyze the quality of the responses givenby licensed therapists. Answer distributions:In this dataset, 433 li-censed therapists answered 940 different questions,summing to 2775 question-answer pairs with 10attributes: questionID, questionTitle, questionText,questionLink, topic, therapistInfo, therapistURL,answerText, upvotes, and views. See and 8for details. Number of Answers log(Frequency) Distribution of Answers per Question",
  "Archetecture:": "BertForSequenceClassification((bert): BertModel((embeddings): BertEmbeddings((word_embeddings): Embedding(30522, 768, padding_idx=0)(position_embeddings): Embedding(512, 768)(token_type_embeddings): Embedding(2, 768)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False))(encoder): BertEncoder((layer): ModuleList((0-11): 12 x BertLayer((attention): BertAttention((self): BertSelfAttention((query): Linear(in_features=768, out_features=768, bias=True)(key): Linear(in_features=768, out_features=768, bias=True)(value): Linear(in_features=768, out_features=768, bias=True)(dropout): Dropout(p=0.1, inplace=False))(output): BertSelfOutput((dense): Linear(in_features=768, out_features=768, bias=True)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False)))(intermediate): BertIntermediate((dense): Linear(in_features=768, out_features=3072, bias=True)(intermediate_act_fn): GELUActivation())(output): BertOutput((dense): Linear(in_features=3072, out_features=768, bias=True)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False)))))(pooler): BertPooler((dense): Linear(in_features=768, out_features=768, bias=True)(activation): Tanh()))(dropout): Dropout(p=0.1, inplace=False)(classifier): Linear(in_features=768, out_features=2, bias=True))",
  "Architecture:": "BertForSequenceClassification((bert): BertModel((embeddings): BertEmbeddings((word_embeddings): Embedding(30522, 768, padding_idx=0)(position_embeddings): Embedding(512, 768)(token_type_embeddings): Embedding(2, 768)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False))(encoder): BertEncoder((layer): ModuleList((0-11): 12 x BertLayer((attention): BertAttention((self): BertSelfAttention((query): Linear(in_features=768, out_features=768, bias=True)(key): Linear(in_features=768, out_features=768, bias=True)(value): Linear(in_features=768, out_features=768, bias=True)(dropout): Dropout(p=0.1, inplace=False))(output): BertSelfOutput((dense): Linear(in_features=768, out_features=768, bias=True)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False)))(intermediate): BertIntermediate((dense): Linear(in_features=768, out_features=3072, bias=True)(intermediate_act_fn): GELUActivation())(output): BertOutput((dense): Linear(in_features=3072, out_features=768, bias=True)(LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)(dropout): Dropout(p=0.1, inplace=False)))))(pooler): BertPooler((dense): Linear(in_features=768, out_features=768, bias=True)(activation): Tanh()))(dropout): Dropout(p=0.1, inplace=False)(classifier): Linear(in_features=768, out_features=5, bias=True))",
  "CPrompt Design": "In this section, we present six different prompttemplates that are used throughout the evaluationof SOULSPEAK.The simplistic prompt design, default, consistsof a simple persona of a therapist and few-shotprompting enabled by the knowledge base.The following two prompt designs (7feelings and7feelings2tones) are built on default more concisely.7feelings established the same role as default buthad guidelines for seven emotions. We specifiedthree action items and increased the generality ofthe descriptions. 7feelings2tones has the same com-ponents with an additional section to provide twoexample transition questions. Our goal was to gen-eralize the action items and improve the format ofthe default to generalize better.The next three prompts are inspired by thera-pist guidelines. Gkp uses the naive generalizedknowledge prompting technique with five generalinformation input questions and knowledge withanswers. GkpPsychoTherapy utilizes the gener-ated knowledge prompting technique with threepsychotherapy-specific prompts based on emo-tional situations and the same established role asdefault. GkpPsychoTherapyNonRep utilizes thegenerated knowledge prompting technique withfour psychotherapy-specific prompts based on emo-tional and other situations and the same establishedrole as default. We aim to see if the model couldeffectively learn to be an empathetic therapist byidentifying and responding to general and emo-tional situations.The complete prompts are attached below. Notethat \"{x_shot_prompts}\" refers to the retrieved ther-apists Question and Answer pair from the knowl-edge base.",
  "gkpPsychoTherapy:": "INSTRUCTION = You are ChatGPT,a helpful AI bot. Now you are a expe-rienced and respected therapist to helppeople resolve emotional issues. Pleasegenerate some knowledge about the emo-tional concepts in the input. Examples: Input: If my therapist gives me encour-agement, what emotion do I likely feel?Knowledge: Situations that trigger lovefor me might include seeing a beautifulperson, watching romantic movies, orgiving gifts or notes to others. Input: Somebody ridiculed me yester-day for having a flabby stomach. Whatemotion do I likely feel?Knowledge: Situations that trigger sad-ness for me might include having a friendmove away, breaking up with a signifi-cant other, or having my favorite sportsteam lose an important game.",
  "gkpPsychoTherapyNonRep:": "INSTRUCTION_4 = You are ChatGPT,a helpful AI bot. Now you are an expe-rienced and respected therapist to helppeople resolve emotional issues. Pleasegenerate some knowledge about the con-cepts in the input. Examples: Input: If my therapist gives me encour-agement, what emotion do I likely feel?Knowledge: Situations that trigger lovefor me might include seeing a beautifulperson, watching romantic movies, orgiving gifts or notes to others. Input: If I forgot to send a birthday cardto my close coworker, and just realizedtoday, what emotion am I likely to feel?Knowledge: Situations that trigger guiltfor me include lying to a friend, forget-ting to pay rent, or missing practice formy band or sports team. Input: At the end of a long cruise, whereI had a great time, might I feel worried?Knowledge:Instances of secondaryemotions include feeling shame for hav-ing been embarrassed, feeling anxiousabout feeling anxious, or feeling worryabout joy/happiness ending. Input: For the past few weeks, Jose hasbeen working on an important projectlate at night at his company, and onlyhas enough energy to lie down on hismattress and watch Netflix right beforesleeping. What would you tell him?Knowledge: For good sleep hygiene,one strategy includes not reading, eating,or watching television in your bedroom,because the human body might come toassociate the bed with these activities in-stead of sleeping.",
  "TopicDiscussing Concerns about a Co-worker": "Past ConversationUser: Ive been having a tough time with a co-worker, Derek. He often dismisses my ideas inmeetings and its starting to affect my confidence atwork.Therapist: That sounds challenging. How do youusually react when Derek dismisses your ideas?User: Mostly, I stay quiet. I feel like if I speak up, itmight make things worse. Derek is quite influentialin the team.",
  "Key InformationDereks habit of dismissing the users ideas in meet-ings.The users reluctance to speak up due toDereks influence and the impact on the users con-fidence": "Sample AnswerGiven the situation with Derek youve described,its important to approach this meeting with a plan.Remember to assert your ideas confidently. Youmight also consider discussing your concerns withDerek privately or seeking support from anotherteam member or supervisor. Its important to createa space where you can express your ideas withoutfeeling dismissed.",
  "SoulspeakRe-sponse": "It sounds like you have some concerns about yourmeetings with Derek. It can be challenging whenyou feel dismissed or hesitant to speak up. Oneapproach to consider is approaching the meetingwith confidence and assertiveness. When present-ing your ideas, speak clearly and directly, and if youfeel dismissed, calmly reiterate your points. Its alsoimportant to remember that your ideas have value.If you find it difficult to communicate with Derek,it may be helpful to practice some self-affirmationsbefore the meeting to boost your confidence. Re-member, your voice matters, and your ideas areimportant.",
  "Human-bestGPT-3.515.939.737.963.547.0815.0410.62GPT-4o7.9610.627.085.316.197.087.08": "GPT-4o was less preferred than GPT-3.5. Uponreviewing the generated responses, we found thatGPT-4o tends to use more formal writing and of-ten lists various points. It is also more likely torefuse to give suggestions, citing that it is notqualified to answer such questions. This suggeststhat psychotherapy-related responses may not alignwell with the instruction tuning objectives of GPT-4o. Prompts need to be adjusted accordingly formore advanced models."
}