{
  "Abstract": "We consider the problem of quantifying how an input perturbation impacts theoutputs of large language models (LLMs), a fundamental task for model reliabilityand post-hoc interpretability. A key obstacle in this domain is disentanglingthe meaningful changes in model responses from the intrinsic stochasticity ofLLM outputs. To overcome this, we introduce Distribution-Based PerturbationAnalysis (DBPA), a framework that reformulates LLM perturbation analysis asa frequentist hypothesis testing problem. DBPA constructs empirical null andalternative output distributions within a low-dimensional semantic similarity spacevia Monte Carlo sampling. Comparisons of Monte Carlo estimates in the reduceddimensionality space enables tractable frequentist inference without relying onrestrictive distributional assumptions. The framework is model-agnostic, supportsthe evaluation of arbitrary input perturbations on any black-box LLM, yieldsinterpretable p-values, supports multiple perturbation testing via controlled errorrates, and provides scalar effect sizes for any chosen similarity or distance metric.We demonstrate the effectiveness of DBPA in evaluating perturbation impacts,showing its versatility for perturbation analysis.",
  "Introduction": "Large language models (LLMs) generate outputs conditioned on textual inputs by iteratively samplingfrom a distribution of tokens. Therefore, the outputs of LLMs exhibit inherent variability due to thestochastic sampling process, a process controlled via parameters such as temperature or top-k .This means that evaluating how a specific input perturbationchanging some information in thepromptaffects the output, is not straightforward . Understanding and quantifying the effects ofsuch perturbations is crucial in high-stakes applicationssuch as legal document drafting or medicaldiagnosiswhere errors or unintended behavior could have significant consequences . Systematic evaluation of output responses to input perturbation is fundamental to comprehendingLLM behavior. It provides quantitative insights into model robustness and output consistency acrossdiverse input conditions. Perturbation analysis can serve at least three crucial functions in generalML models. First, it helps with vulnerability identification by quantifying potential vulnerabilitiesto adversarial attacks . Second, it aids with bias discovery, whereby latent biases or unintendedbehaviors may not become apparent through single-dimension auditing approaches . Third, it canwork within a compliance framework. Measurable frameworks for assessing model behavior areessential for compliance with emerging ethical and legal accountability regulations . In the contextof language models, subtle changes in patient history could lead to wildly different diagnoses; andpatients with nearly identical health records might receive drastically different treatment plans due tominor grammatical changes. Given these critical functions, there is a clear need for a comprehensiveLLM auditing framework centered around perturbation analysis.",
  "Adversarial attackEvaluate whether a model is being adver-sarially attackedDetermine if the output diverges too muchfrom relevant, seen answersBetter adversarial robustness": "Current methods for analyzing the impact of input perturbations often focus on simplistic metrics,such as word overlap or direct log-probability comparisons. While effective in certain cases, theseapproaches fail to account for the nuanced, high-dimensional nature of semantic information pro-cessed by LLMs . Moreover, existing methods typically lack rigorous statistical foundations,making it difficult to disentangle meaningful changes in model behavior from intrinsic randomness inthe output generation process. Efforts to address this have included specialized attribution methods,feature importance techniques, and counterfactual reasoning frameworks . However, theseapproaches are often model-specific, rely on restrictive assumptions about the data or model, or failto provide interpretable and generalizable metrics2. Our solution. In this work, we introduce Distribution-Based Perturbation Analysis (DBPA), aframework that reformulates the problem of LLM perturbation analysis as a frequentist hypothesistesting task. DBPA constructs empirical output distributions using Monte Carlo sampling to capturethe inherent stochasticity of LLMs, and evaluates perturbation effects within a low-dimensionalsemantic similarity space. By leveraging statistical hypothesis testing, the framework enables robust,interpretable inferences about whether and how input perturbations meaningfully influence LLMoutputs. DBPA is model-agnostic, computationally efficient, and flexible enough to accommodatearbitrary perturbations on any black-box LLM. It also provides interpretable p-values, scalar effectsizes, and supports multiple testing with controlled error rates, making it a versatile tool for post-hocinterpretability and reliability assessments of LLMs. Significance beyond technical novelty. We see this work as having immediate practical relevancefor practitioners who wish to evaluate their LLM outputs and whether they change in real-worldpractical settings, including evaluating prompt robustness, LLM robustness, and conducting modelcomparisons. Contributions. 1 We identify limitations in existing sensitivity-based measures for lan-guage models (Sec. 2). 2 We introduce distribution-based perturbation analysis whichis a model-agnostic sensitivity technique that can test the effect of any perturbation withstatistical significance measures (Sec. 3).3 We perform multiple case studies to showthe usefulness of DBPA (Sec. 4).",
  "Code available at": "Our objective is to address the following research question: Given an ML system S, an input x X,and a perturbation , how can we systematically measure and interpret the impact of on the outputdistribution of S(x) under a general notion of sensitivity? Definition 1 (Sensitivity). The sensitivity of a machine learning system S with respect to a pertur-bation : X X, or a system perturbation : S S, at input x X, is characterized bya measure of discrepancy between the output distributions S(x) and S(x), or S(x) and S(x),respectively. This definition encompasses both input perturbations (Y == S(x)) and system perturbations(Y == S(X)). Crucially, we wish to evaluate this sensitivity by only having access to the MLsystem S from which we could sample. We do not assume access to ground-truth labels and make noassumptions on the architecture of S.",
  "y = S(x),y = S((x))(1)": "However, this approach is fundamentally flawed due to the stochastic nature of LLM systems. Forany fixed input x, S(x) is a random variable, and thus y and y are single realizations from adistribution of outputs. Consequently, any observed difference between y and y could be due toinherent randomness rather than a true effect of . To address this limitation, we propose reframing the problem from the lens of distribution testing.Instead of comparing individual outputs, we compare the entire output distributions. Let Dx andD(x) be the distributions of outputs from S(x) and S((x)) respectively.",
  "H1 : Dx = D(x)(The perturbation affects the output distribution)(4)": "The primary benefit of such a distributional formulation is that it captures the full stochastic behaviorof S instead of just a single realization. This means we could perform statistical inference by directlycomparing these distributions and understanding how much the outputs have shifted across the wholeoutput space, even detecting subtle shifts that might not be apparent from individual samples.",
  "There are two primary challenges in comparing output distributions to evaluate the effect of an inputperturbation on the output: computational intractability and poor interpretability": "Challenge 1: Computational intractability. In almost all cases, it is impossible to directlyevaluate the output distribution change because of the exponential output space Y = V L where V isthe vocabulary and L is the sequence length, yielding |Y | = |V |L possible outputs for each sentence,and that is assuming a fixed L. Suppose we limit our language model S to output only sequences oflength L. In this case, Dx would require summing over all possible sequences:",
  "t=1p(yt|y<t, x) (y1,...,yL)(5)": "where p(yt|y<t, x) is the probability of token yt given the previous tokens and input x, and y1,...,yLis the Dirac delta function assigning the probability to the specific sequence. Even in such a case, thiswould be an intractable computation for any existing operating system with exponential computationalcomplexity O(|V |L). Challenge 2: Interpretability. A second major issue is that the distribution Dx does not providean interpretable understanding of the LLM output. As LLMs are increasingly being employed as rea-soning engines , we care about whether their outputs differ semantically, not probabilistically.For instance, suppose S outputs two answers to a question on treatment recommendations",
  "Probabilistically, the answer distributions do not match, i.e. Dx(y1) = Dx(y2) in general. Semanti-cally however, we see them as having the same inherent recommendation": "Ideally, wed like to be able to resolve both issues at the same time (i) be able to computationallyapproximate the distribution and (ii) evaluate whether the differences are semantically meaningful, notjust probabilistically different. We show that we can achieve both with finite-sample approximations. Analyzing output distributions of language models faces two unique challenges: the com-putational intractability due to the enormous output space, and the need for semanticrather than just probabilistic interpretation of differences.",
  "Cosine Similarity": "10.0 20.0 30.0 and perturbed Original inputPerturbed input : Example of null and alternative dis-tributions. The null distribution P0 (left, blue)is constructed based on the intrinsic variability ofresponses. The alternative distribution with a per-turbed input P1 (right, red) is quantified with re-spect to the original distributions. This measuresthe output distribution change given a perturbedprompt in the cosine similarity space. Addressing challenge 1: Computationalcomplexity. We address the computational com-plexity is by Monte Carlo sampling. We definethe stochastic approximations of the output dis-tributions for an input x and its perturbation(x) as:",
  "where k is the sample size. This simply samplesk outputs for the original and perturbed inputs": "Addressing challenge 2: Interpretability.Given a finite set of samples, we would like tomeasure how much the output varies given an input perturbation. To quantify the distributionalchanges induced by input perturbations, we introduce the similarity metric s : Y Y (discussed more in Sec. 3.2). This metric allows us to construct empirical distributions of pairwisesimilarities:",
  "P1 = {s(yi, yj) : yi Dx, yj D(x)}(9)": "Here, P0 captures the intrinsic variability within the original output (equivalent to the null distri-bution), whereas P1 captures the cross-distribution similarities between the original and perturbedoutputs (equivalent to the alternative distribution), c.f. . We have therefore constructed twodistributions which represent the variability in answer similarities as a proxy for sensitivity (Def. 1).In Sec. 3, we show how such distributions can be used to obtain sensitivities of perturbations andassociated p-values. Advantages of such a formulation. With this formulation, we (i) capture the stochastic nature ofLLM outputs more faithfully than point estimates; (ii) connect LLM outputs to frequentist hypothesistesting; (iii) quantify effect size; (iv) maintain model and input agnosticism.",
  "Distribution-based perturbation analysis": "We present a novel model-agnostic methodology for assessing the sensitivity of LLMs to perturbations.Our approach avoids restrictive distributional assumptions and utilizes the entire output distributionof S, capturing the intrinsic variability in LLM responses. We enable frequentist statistical hypothesistesting using p-values through the construction of null and alternative distributions. Importantly, ourframework is applicable to any perturbation and any language model, with the minimal requirementof being able to sample from the language models output distribution and construct embeddings.",
  "H0 : S(x) = S((x))H1 : S(x) = S((x))": "To assess statistical significance, we employ a permutation test. This approach allows usto compute p-values while only assuming exchangeability in the similarity space.Objective. The goal of distribution-based perturbation analysis is to determine whetherthere is a statistically significant difference between the output distributions of the orig-inal and perturbed inputs and its associated effect size. We observe the effect size as .To obtain a frequentist p-value, we compare the observed (P0, P1) to the distributionof values obtained through permutation, yielding a p-value:",
  "There are a few practical implementation essentials to take into account when developing DBPA": "Why use scalar pairwise cosine similarities instead of directly using high-dimensional emeb-ddings? Using non-reduced embeddings to construct null and alternative distributions faces twosignificant challenges. High dimensionality. Embedding spaces typically have hundreds or thou-sands of dimensions, making direct distribution estimation in this space problematic due to the curseof dimensionality. Excess semantic information. Embeddings capture rich semantic information,much of which is irrelevant since our only focus is to understand how much the responses vary inresponse to changes in x, as opposed to their inherent semantic position in the embedding space. 0.850.900.95",
  ": Examples of different dimensionalitymetrics. We show that we can use multiple metricsto reduce the dimensionality of given embeddings": "What is an appropriate metric to overcomethe challenges with high-dimensional andsemantically-rich embeddings? We desire toemploy a similarity-preserving dimensionalityreduction technique to project these distribu-tions onto a tractable, low-dimensional mani-fold. While there are many metrics that havesuch properties, such as the L1 or L2 norms, wefind the cosine similarity s(yi, yj) =e(yi),e(yj)",
  "e(yi)e(yj) to be a natural choice, where e() is an embeddingfunction": "What is an appropriate distance metric ? The reason why there exists a choice for is thatwe are dealing with the comparison between two distributions. This is different from traditionalresampling-based approches that construct a null distribution and evaluate a single instance againstit . While the choice for might vary depending on the application, we employ the Jensen-Shannon divergence (JSD) as a measure for : JSD(P0P1) = 1",
  "(DKL(P0M) + DKL(P1M)),where M = 1": "2(P0 + P1) and DKL is the Kullback-Leibler divergence. This is because JSD has threeuseful properties for evaluating distributions: (i) symmetry, ensuring that the measure is invariantto the order of the distributions being compared; (ii) boundedness, providing a consistent scale forinterpretation across different inputs and perturbations; and (iii) sensitivity to differences in both thelocation and shape of the distributions. Why perform permutation-based testing instead of directly MC-sampling from S? In fact, it isentirely possible to use MC-sampling to obtain p-values. However, this comes at a costdirectlysampling S(x) and from S((x)) might be computationally expensive, as this requires directly 0.100.150.200.250.300.350.40 A Medical Supervisor A Therapist A Doctor A Medical Student A Comedian A Child A Neurips Reviewer A Professor In Frequentist Statistics p = 0.11p = 0.17p = 0.34p = 0.62p = 0.00p = 0.00p = 0.00p = 0.04",
  "Randomprofessions": "for various input perturbations Not SignificantSignificant : Measuring the effect size and statistical significance of outputs when prefixing theoriginal question with various \"Act as...\" prompts. Results show that relevant professional roles(e.g., medical professions) yield consistent outputs, while diverse roles produce significantly differentresponses, demonstrating the frameworks ability to quantify prompt perturbation effects. If p < ,where = 0.05, we say that the output distribution is significant.",
  "Case studies": "We demonstrate the effectiveness of our method on a variety of use cases. In the following subsections,we will show that our method can (1) capture those answer divergences that are significant and thosethat are not under perturbation (2) analyze the robustness of language models to irrelevant changes inthe prompt (3) evaluate alignment with reference language model. By default, we run the experimentover 5 seeds, and report the mean and standard deviation of the measurements. We instantiate theDBPA metrics as the p-value and the effect size , computed as the JSD distance between the nulland alternative distributions.",
  "DBPA can measure answer divergence under perturbation": "In this experiment, we show that the DBPA framework provides a way to measure how much theLLM is influenced by a perturbation in the prompt. If the LLM is influenced by the perturbation, weexpect it to produce a different answer that is statistically significant. Setup. (1) We query the LLM a healthcare question, and use the LLM response as the null distribution.(2) We then perturb the prompt, and measure whether the LLM produces a different answer thatis statistically significant. We perturb the prompt by asking the LLM to role-play. In front of theoriginal question, we append the prompt \"Act as ...\", where \"...\" is labeled as the y-axis of .The LLM used in this experiment is gpt-3.5. Discussion. We show the effect size and the p-value plotted against various \"Act as ...\" perturbations.Because we are asking the LLM medical questions, we expect the answer to not change when theLLM role-plays as any medical profession (top half of ), and change significantly when theLLM role-plays as other diverse professions (bottom half of ). We find that we are able tocapture the LLM response variability. Whenever the repsonses are prepended to role-play in a waythat significantly changes the output distribution (Random professions), this is captured by the and the p-value. In contrast, Medical professions do not produce statistically significant responses.Takeaway: The DBPA framework effectively quantifies how different prompt perturbations impactlanguage model outputs.",
  "DBPA can measure the robustness of language models to irrelevant changes in the prompt": "Setup. This experiment aims to demonstrate how the DBPA framework can measure LLM robustnessto prompt variations. We query an LLM with an original question, then with multiple paraphrasedversions that maintain the same meaning. We then compute the p-values and for responses to paraphrased prompts. We calculate the proportion of responses showing significant changes (p-value< ), where = 0.05, and average . This process is repeated for different LLMs, with results in. A robust model should show few significant changes and small effect sizes. : Measuring P(p-value < 0.05) andeffect size across LLMs. In this context, P(p-value < 0.05) should be interpreted as comput-ing how many statistically-significant shifts thereare in the perturbed responses, i.e. how manyresponses have p-value < 0.05. As we expect,more powerful models tend to be more robust.",
  "LLMP(p-value < 0.05)": "gpt-4-0613-202310160.15 (0.3)0.23 (0.06)gpt-35-11060.05 (0.06)0.22 (0.04)microsoft/Phi-3-mini-4k-instruct0.1 (0.05)0.23 (0.03)openai-community/gpt20.18 (0.22)0.27 (0.08)meta-llama/Meta-Llama-3.1-8B-Instruct0.25 (0.11)0.27 (0.01)mistralai/Mistral-7B-Instruct-v0.20.3 (0.3)0.25 (0.05)google/gemma-2-9b-it0.33 (0.22)0.26 (0.07)HuggingFaceTB/SmolLM-135M0.35 (0.24)0.25 (0.03)Gustavosta/MagicPrompt-Stable-Diffusion0.45 (0.23)0.29 (0.05) Discussion. presents two key robustnessmetrics for each LLM: the probability of statisti-cally significant changes (P(p-value < 0.05)) andthe effect size (). Lower values in both metricsindicate higher robustness to irrelevant promptperturbations. The results demonstrate a cleartrend: more advanced models exhibit greater ro-bustness. GPT-4 and GPT-3.5 show the lowestprobabilities of significant changes (0.15 and 0.05)and among the smallest effect sizes (both 0.22-0.23). In contrast, smaller or less advanced mod-els like SmolLM-135M and MagicPrompt-Stable-Diffusion show higher probabilities of significantchanges (0.35 and 0.45) and larger effect sizes(0.25 and 0.29). This pattern aligns with the expectation that more sophisticated models should beless sensitive to irrelevant prompt variations, and the consistency across both metrics strengthens thereliability of these findings.",
  "LLMp-value": "meta-llama/Meta-Llama-3.1-8B-Instruct0.31 (0.07)0.02 (0.02)HuggingFaceTB/SmolLM-135M0.29 (0.04)0.02 (0.04)openai-community/gpt20.28 (0.05)0.10 (0.13)gpt-35-11060.25 (0.05)0.24 (0.25)Gustavosta/MagicPrompt-Stable-Diffusion0.23 (0.06)0.37 (0.35)microsoft/Phi-3-mini-4k-instruct0.23 (0.04)0.28 (0.25)mistralai/Mistral-7B-Instruct-v0.20.21 (0.02)0.50 (0.21)google/gemma-2-9b-it0.19 (0.05)0.67 (0.31) One key flexibility of the DBPA framework isits ability to benchmark model perturbations inaddition to input perturbations (which we haveshown in earlier sections). When the model isperturbed rather than the input, what we are essen-tially measuring is the distance between answersfrom different models. Interestingly, this could actas a metric for alignment between models. Setup. This experiment demonstrates how theDBPA framework can evaluate alignment be-tween different language models. We use GPT-4(specifically gpt-4-0613-20231016) as the refer-ence model. For a set of standardized questions,we first obtain answer distributions from GPT-4.We compute and the p-value. Discussion. The results in reveal varying degrees of alignment between different languagemodels and GPT-4. The results demonstrate that the DBPA framework can quantify differences inresponse distributions between models, providing a concrete measure of inter-model alignment thatcould be useful for comparative analysis of language model outputs.",
  "Related work": "There are three primary approaches to quantifying text-based outputs that relate to our approach. Theyinclude measuring unintended biases in model outputs, developing counterfacual fairness methods,and text summarization metrics. Measuring unintended bias. Overall, the closest related works are in measuring unintended bias. Such metrics quantify existing biases between subgroups for models. Broader work in thefield can be found in . However, this requires human annotation, relates only to fairness, andassumes the existence of reliable labels across subgroups. Counterfactual fairness. This approach examines how predictions would change if sensitiveattributes were different. It can compute effect sizes but cannot be applied to black-box models,doesnt allow arbitrary perturbations, and doesnt enable statistical inference. It requires human input",
  "for labeling a specific attribute (e.g. toxicit) of an answer and makes assumptions, e.g. that non-toxicexamples are less likely to contain asymmetric counterfactuals relative to toxic examples": "Text summarization metrics. These metrics evaluate the quality of text summariza-tion. They can compute effect sizes but are not applicable to black-box models, dont allow arbitraryperturbations, and dont enable statistical inference. They require human input and make certainassumptions. Various metrics like BERTScore, MoverScore, and ROUGE variants measure differentaspects of similarity between system and reference summaries. To better explain how we differ, we compare each area based on five important criteria: (i) whetherthe method can be applied to any black-box model; (ii) whether any perturbation can be applied andmeasured; (iii) whether the approach enables statistical inference; (iv) whether the approach allowsto compute effect sizes of the change; (v) whether there are any assumptions; (vi) whether humansare required as a part of the input. We show this in .",
  "Discussion": "With the growing need to evaluate LLM systems, we require statistics-based approaches to understandLLM outputs. The introduction of distribution-based perturbation analysis is important for ourability to perform such evaluations. Apart from direct applications in high-stakes areas, we see suchapplications useful in other domains where language models are used as parts of broader statisticalsystems . The versatility of DBPA in handling any input perturbation without requiringaccess to model internals makes it particularly valuable for practitioners and researchers workingwith diverse LLM architectures and deployment scenarios. Limitations. While DBPA effectively tackles challenges of computational intractability and in-terpretability in analyzing LLM output distributions, the choice of similarity metrics and distancemeasures plays a critical role and can significantly impact results. Future work should investigatethese choices in depth and provide clear guidelines for selecting appropriate measures for specificcontexts. Additionally, although DBPA offers valuable insights for model auditing, translating thesefindings into practical strategies for enhancing model robustness and aligning outputs with humanpreferences remains a significant challenge. Exploring uncertainty-aware methods, such as those in, to identify perturbations that achieve specific objectives presents a promising research direction.Bridging the gap between identifying sensitivities and implementing targeted interventions is essentialfor advancing LLM performance and reliability.",
  "Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversar-ial examples. arXiv preprint arXiv:1412.6572, 2014": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explainingthe predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD internationalconference on knowledge discovery and data mining, pages 11351144, 2016. Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David OBrien,Kate Scott, Stuart Schieber, James Waldo, David Weinberger, et al. Accountability of ai underthe law: The role of explanation. arXiv preprint arXiv:1711.01134, 2017.",
  "Chong Ho Yu. Resampling methods: concepts, applications, and justification. PracticalAssessment, Research & Evaluation, 8(19):123, 2003": "Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuancedmetrics for measuring unintended bias with real data for text classification. In Companionproceedings of the 2019 world wide web conference, pages 491500, 2019. Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuringand mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACMConference on AI, Ethics, and Society, pages 6773, 2018.",
  "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. Mover-score: Text generation evaluating with contextualized embeddings and earth mover distance.arXiv preprint arXiv:1909.02622, 2019. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang,Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language modelbased agents: A survey. arXiv preprint arXiv:2309.07864, 2023.",
  "Paulius Rauba, Nabeel Seedat, Max Ruiz Luyten, and Mihaela van der Schaar. Context-aware testing: A new paradigm for model testing with large language models. arXiv preprintarXiv:2410.24005, 2024": "Paulius Rauba, Nabeel Seedat, Krzysztof Kacprzyk, and Mihaela van der Schaar. Self-healingmachine learning: A framework for autonomous adaptation in real-world environments. arXivpreprint arXiv:2411.00186, 2024. Nicols Astorga, Tennison Liu, Nabeel Seedat, and Mihaela van der Schaar. Partially observablecost-aware active-learning with large language models. In The Thirty-Eighth Annual Conferenceon Neural Information Processing Systems, 2024.",
  "In the exercise for treatment recommendations, we use the following input prompts (where character-istics can vary):": "John has these features: Age: 70, BMI: 30, Blood Pressure: 114/61 mmHg, Total Cholesterol: 6.7mmol/L, HDL Cholesterol: 1.0 mmol/L, Smoking Status: current smoker, Diabetes: no diabetes,Family History of CVD: yes, Ethnicity: Chinese. Based on these characteristics alone, providerecommendations on CVD guidelines based on NICE for this person John has these features: Age: 70, BMI: 29, Blood Pressure: 114/61 mmHg, Total Cholesterol: 6.7mmol/L, HDL Cholesterol: 1.0 mmol/L, Smoking Status: current smoker, Diabetes: no diabetes,Family History of CVD: yes, Ethnicity: Chinese. Based on these characteristics alone, providerecommendations on CVD guidelines based on NICE for this person"
}