{
  "Abstract": "People who are blind perceive the world differently than those who are sighted,which can result in distinct motion characteristics. For instance, when crossing atan intersection, blind individuals may have different patterns of movement, such asveering more from a straight path or using touch-based exploration around curbsand obstacles. These behaviors may appear less predictable to motion modelsembedded in technologies such as autonomous vehicles. Yet, the ability of 3Dmotion models to capture such behavior has not been previously studied, as existingdatasets for 3D human motion currently lack diversity and are biased toward peoplewho are sighted. In this work, we introduce BlindWays, the first multimodalmotion benchmark for pedestrians who are blind. We collect 3D motion data usingwearable sensors with 11 blind participants navigating eight different routes in areal-world urban setting. Additionally, we provide rich textual descriptions thatcapture the distinctive movement characteristics of blind pedestrians and theirinteractions with both the navigation aid (e.g., a white cane or a guide dog) and theenvironment. We benchmark state-of-the-art 3D human prediction models, findingpoor performance with off-the-shelf and pre-training-based methods for our noveltask. To contribute toward safer and more reliable systems that can seamlesslyreason over diverse human movements in their environments, our text-and-motionbenchmark is available at",
  "Introduction": "Computational modeling of people and their 3D motion has been studied extensively by the machinelearning community over the past decades . More recently, the field hasbeen moving beyond single actors performing contrived actions to model interactive behaviors, i.e.,incorporating objects and surrounding people. However, the scope and diversity of the datasetsassociated with this prior work remain limited to a simulation , a lab , or simplifiedlayouts . Efforts for accurately capturing and modeling natural and subtle 3D human motion in more realisticsetups, such as , are still lacking in complex and safety-critical urban scenes thatcomprise dynamic intersections, intricate layouts, and dense social settings. Even more noticeably,there has not been a single 3D human motion dataset released that comprises mobility data fromindividuals with disabilities. Hence, while most human motion models are developed with assistiveand interactive applications in mind, such as social robots and autonomous driving, those who couldbenefit the most from these technologies are not included. Such severe biases in existing benchmarkscan carry broader societal implications. It can exacerbate already widespread concerns in accessibility,where autonomous vehicles fail to accurately predict and safely respond to movements of peoplewith disabilities ; a population that is already disproportionately impacted by motorists lack ofawareness . In this work, we address this current gap in literature through a novelbenchmark featuring people who are blind navigating real-world urban settings.",
  "arXiv:2412.05277v1 [cs.CV] 6 Dec 2024": "Blind pedestrians are known to exhibit significantly different mobility characteristics based onpersonal factors, such as their lived experiences with disability or the use of mobility aids (e.g., canes,guide-dogs, and orientation and mobility apps ). For instance, many do not face forward to signalintent to cross before stepping into the road and may take longer to explore tactile cues when crossingin various intersections . Additionally, some may veer significantly in open spacesor unexpectedly step into the road due to obstacles, such as a truck parked at obstructed intersectionswith damaged or ambiguous curbs. In such scenarios, reasoning over subtle 3D behaviors, likehand-aid coordination gestures, could improve future prediction in autonomous vehicles and avoidpotential safety-critical outcomes . Yet, as far as we are aware, no prior work has investigated the prediction of pedestrian motion in suchedge cases, characterized by inherently distinct, subtle, and uncertain nature. Specifically, we aim tounderstand the capabilities of state-of-the-art 3D motion models for modeling and predicting futuremotion of blind pedestriansultimately, to ensure that autonomous systems and vehicles in urbanenvironments operate safely around pedestrians with disabilities. Contribution: Our overarching goal is to enable more robust, accurate, and needs-aware pedestrianbehavior prediction models that effectively account for disability-related scenarios and behaviors.Our key contribution is twofold. First, we introduce BlindWays, a novel multi-modal 3D humanmotion dataset featuring pedestrians who are blind navigating unfamiliar and complex real-worldenvironments. Our dataset includes detailed language-based annotations of context and non-visualnavigation strategies. Second, we use the dataset to benchmark state-of-the-art 3D motion modelswithin our novel modeling task. By analyzing the effects of model pre-training and fine-tuning onfuture motion prediction, we identify fundamental limitations in the generalization of current datasetsand models, particularly when evaluated within diverse and rare human attributes.",
  "To design our study and data collection, we build on recent advances in in-the-wild 3D human motionestimation, 3D human motion models, and language-based motion generation": "Estimating In-the-Wild 3D Human Motion: Researchers have long sought to capture 3D humanmotion in naturalistic settings . However, vision-based inferenceof 3D keypoints can be unreliable in our context of multi-actor, dense scenes with frequent occlusionand interaction with objects . For instance, AlphaPose , a widely used keypoint detectionand lifting model, exhibited frequent failure and poor performance on videos for our settings (onlinesourced and our in-house collected ones ). Given the difficulty in manually annotating monocularvideos with accurate 3D information , an alternative approach for minimally intrusive collectioninvolves wearable inertial sensors . Specifically, in our study, we leverage anXsens set of trackers (one placed on the mobility aid). While the system may have inherentnoise, it can be re-calibrated to improve accuracy. In our study, we frequently re-calibrate the systemthroughout the route to minimize drift and improve accuracy. We also filter out noisy skeletonsthrough manual inspection in scenarios of system tracking failure. Nonetheless, we emphasize thatmotion capture in-the-wild remains an open challenge. Modeling 3D Human Motion: Motion-prediction models (e.g., ) do notcurrently model pedestrians with disabilities . The few studies that have analyzed blindnavigation motion in context are qualitative in nature , only providing a high-levelaccount. Instead, 3D motion models generally leverage sighted participants . While understanding and encompassing unique navigation behaviors is essential for acomprehensive motion synthesis and generation frameworks , our study empiricallydemonstrates how prior works struggle to generalize to the nuanced modeling of blind motion. Text-to-Motion: Text-driven motion generation has gained significant interest due to its controllabil-ity, as well as the concise context information provided by textual descriptions. Diffusion models,such as MDM , have been explored for generating human motion sequences from text descriptions,progressively refining the motion through a series of forward steps. However, recent diffusion-basedapproaches do not generate plausible blind motion, as shown by our study. In addition, : Comparing Motion Benchmarks. BlindWays introduces several dataset dimensions not ex-plored by prior work, including participants with mobility aids (i.e., white cane or guide-dog, trackedwith a sensor) and safety-oriented scenarios in urban streets. We also provide language annotationswith two levels of granularity: high-level summaries and more detailed low-level descriptions.",
  "Human3.6M 112.9Marker-based-AMASS 34440.0Marker-based11.9HumanML3D 45028.6Marker-based12.3Motion-X -144.2Marker-based38.5BlindWays (Ours)112.8IMU-based44.1": "current methods are limited by inefficiency during testing, as they require multiple forward stepsto generate a single motion sequence. GPT-based text-to-motion models have recentlyshown promising results. TM2T focuses on the temporal modeling of motion, ensuring that gen-erated motions are coherent and contextually appropriate. Recently, MotionGPT has integratedgenerative pre-trained transformers with joint training of motion and language, further advancing thequality and diversity of generated motions. However, the applicability and generalization of suchmodels in accessibility settings remain underexplored. Motion and Language Benchmarks: Datasets with high-quality text descriptions have furtheradvanced the controllability and generation of multimodal motion, with some also incorporatinginteractions with objects and other people. While motion-language models have recently achievedoutstanding performance in tasks such as motion prediction , diverse motion generation , and the study of human-object interaction , researchers have beeninspired to develop diverse datasets that support these varied motion tasks. The KIT-ML datasetfocuses on multi-modal language-to-motion translation but lacks motion diversity. AMASS unifies a wide range of MoCap data. BABEL and PoseScript also incorporate actionlabels and textual descriptions, however, such datasets still lack in motion diversity and realism.HumanML3D introduces a large collection of 3D human motions with corresponding naturallanguage annotations, primarily focused on static indoor settings with repetitive motions. Motion-X introduces a comprehensive dataset that includes detailed semantic annotations and outdoorenvironments (a context largely neglected in previous datasets). However, the distinct and uncertainnature of blind pedestrians motion remains unexplored, limiting the generalizability of state-of-the-art motion modeling models in these critical cases, despite their importance. Our work aims to addressthis gap by introducing BlindWays, providing a richer and more challenging dataset for generalizingtext-to-motion models and capturing the subtle movements of the head, limbs, and mobility aids usedby blind individuals. Similar to Motion-X, BlindWays dataset is collected entirely in outdoor settingsand incorporates IMU-based motion capture, unlike traditional marker-based systems and isless restricted by environmental constraints, allowing motion tracking in diverse outdoor settings.",
  "Overview": "We collected BlindWays, a comprehensive blind motion dataset comprising 1,029 motion clips andapproximately 0.6 million human poses, along with 2,058 detailed, paired, high- and low-level textdescriptions. We capture natural motion data from 11 blind and low-vision individuals navigatingdynamic outdoor environments along carefully engineered paths exhibiting various challenges.Notably, this is the first work to propose blind motion data enriched with text descriptions, anexceptionally challenging and labor-intensive process. BlindWayss text descriptions are informedby third-person and egocentric videos, each totaling 0.3 million frames. Specifically, capturedcontextual videos play a critical role in the annotation process by providing an overall scene of blindmotion, allowing annotators to sufficiently leverage scene and video context to accurately, precisely,and expressively describe the motion. To synchronize between motion data and videos, we askedparticipants to clap at the beginning of each route. To ensure high quality, the MoCap system iscalibrated in each route and text descriptions are annotated in-house by human annotators, includingmotion experts, and are carefully checked. We employ a wearable IMU-based system and filter noisysequences to maintain accuracy and reliability.",
  "Motion Capture": "High-level Low-Level A blind woman with a cane is carefully approaching a crosswalk.She uses her cane to probe the path in front of her and stops whenshe feels the curb and tactile paving. A blind woman with a cane is carefully walking towards anintersection. She takes six small steps before stopping rightbefore the street at the edge of the curb. The cane is in her righthand, and she uses a combination of sweeping and tapping toprobe the path in front of her. She recognizes the end of thesidewalk when he feels the curb and yellow tactile paving.",
  "Data Collection Procedure": "We conducted a user study involving 11 participants, consisting of three women and eight men, allof whom are either blind (N=10) or have low vision (N=1). Each participant utilized their ownmobility aid, which included either a cane or a guide dog, to record natural behavior. Our participantsrepresent a diverse range of ages, levels of visual impairment, and mobility aids, ensuring a richdata collection of navigation behaviors (details on participant demographics are provided in oursupplementary). Participants are equipped with bone conduction headsets to receive real-time auditoryinstructions from Google Maps. Our data collection was approved by our Institutional Review Board.Each participant provided informed consent before participating in the study and was compensated$50/hour for up to three hours, including travel time; data collection sessions typically lasted twohours or less. We note that two researchers always followed the participants during dataset collectionto ensure their safety. Scenarios: In collaboration with local blind advisors and sighted certified orientation and mobilityinstructors, we engineered eight distinct routes to encompass a variety of real-world scenarios thatblind people commonly encounter. These scenarios include walking on the curb, crossing streets,navigating open spaces, and ascending and descending staircases. For example, while crossingstreets, participants faced a challenge when encountering a subway track midway, requiring themto stop, reassess, and then continue, which enabled us to capture their behavior while handlingsudden stops and changes in terrain. Navigating open spaces presented another challenge due tothe lack of obstacles providing environmental cues, forcing participants to rely heavily on auditoryinstructions. Walking on curbs involved dealing with intermittent obstacles like parked bicycles,trash cans, and overhanging branches. Ascending and descending staircases further added to thecomplexity, requiring careful coordination and heightened awareness of their immediate environment.Diverse and realistic scenarios enable BlindWays to capture rich and nuanced motion data, reflectingdaily real-world challenges and strategies of blind individuals. Each route is carefully mapped andpre-tested to ensure both feasibility and participant safety. At the start of each route, we providedhigh-level instructions, including specific objectives and expected challenges. For example, weguided participants by informing them of their current location (e.g., surrounding street names) andthe direction they were heading to help them better contextualize the audio navigation aid, whichusually guides pedestrians by providing street names and directions. We also briefly explained",
  "potential obstacles they might encounter, such as a train/tram track in the middle of the route or stairs,to prepare them for critical challenges ahead": "Recording: We employ the Xsens motion capture system, consisting of 18 Inertial MeasurementUnits (IMUs) sensors for body joints and a mobility aid, enabling realistic motion capture in varioussettings. To comprehensively capture the navigation process, we record third-person video of blindpedestrians and egocentric views, as well as motion data. For egocentric views, participants weara GoPro HERO10 Black on their chest using a comfortable strap, allowing for hands-free andimmersive (GoPro Max Lens Mod) recording. The camera is set to face around the participants feetto meticulously capture cane movements. For third-person views, the accompanying researchers weara Samsung Galaxy smartphone around the chest and follow the participants without interrupting theirnatural movements. All data are synchronized, allowing for an in-depth analysis and annotations ofnavigation strategies and challenges. To gain further insights into participants navigation experiences, upon completion of each route,participants are asked to rate their confidence on a scale of 1-7 in (i) their ability to navigate the routeand (ii) the guidance that they received from the Google Maps app.",
  "Data Annotation Pipeline": "To achieve a nuanced understanding of the navigation behaviors of blind individuals, we employa meticulous annotation pipeline build in-house that leverages the synchronized third-person viewRGB videos along the motion data. To ensure privacy, we mosaic the faces of all people appearingin the videos, both the blind participants and passersby. The annotation process involves 15 humanannotators, comprising three motion experts (human biomechanics, sensorimotor, and mobilityresearchers) and 12 novices, who are provided with detailed instructions, exemplars, and feedback. Annotators are given 25 videos at a time, and it took approximately two hours to annotate each set of25 videos. In addition to carefully crafted instructions, novice annotators are also given feedbackafter the completion of their first set to ensure high quality annotations and help them improve theirefficiency in subsequent annotations. Overall, we collect a total of 1,046 videos, highlighting theextensive labor and dedication involved in this annotation process. To facilitate the annotation process, we build a video annotation interface using Tkinter, a Python-based GUI toolkit. The interface, informed from prior work on video descriptions , enablesusers to freely drag the timeline of the video. Annotators can efficiently review and annotate specificmoments in the videos, enhancing the accuracy and detail of their descriptions. For novice annotators,we provided demos of expert annotation samples as references. High-Level Descriptions: For high-level annotations, annotators are requested to focus on describingthe overall action of the motion, the purpose behind it, and how the participants were holding theirmobility aids (e.g., a cane and a guide dog). Annotators are instructed to provide clear and concisedescriptions that convey the intent and broader context of the actions. For example, a high-leveldescription might be: A blind man with a cane in his right-hand searches for a street post to pressthe button. He then orients himself in the direction he wants to cross the street. Low-Level Descriptions: Low-level annotations require more detailed descriptions of the motionbehavior, such as the number of steps taken and the precise use of mobility aids. For instance, alow-level description might be: A blind man with a cane searches and locates a street post. Hemoves forward three steps to orient himself in the direction he wants to cross the street, using his canein his right hand and positioned in front of him. The detailed information helps in capturing exactmotion dynamics and interactions between the participant and the surrounding dynamic environment.Use of subjective adjectives (e.g., confidently, hesitantly, or meticulously) is encouraged to captureobserved behaviors in a more expressive way.",
  "Data Analysis": "Motion Data: BlindWays captures unique motions characteristic of blind navigation, particularlyhow individuals use mobility aids to interact with their surroundings in dynamic and complex urbansettings. As shown in Fig 2, our motion data include diverse scenarios, from straightforward walking A blind man with a cane is walking onto an elevated sidewalk from the street. He takes 11 steps in total. He holds his cane in his right hand and uses a combination of sweeping and probing motions to navigate. He moves decisively. High-level Low-level A blind man with a cane is walking onto an elevated sidewalk from the street. He navigates decisively. A blind man with a cane in his right-hand walks 8 steps in front until his stick hits the end of a footpath, and he realizes the change in footpath height with his leg. He then momentarily stops, judges the terrain by swaying his stick, and keeps moving forward confidently. Low-level",
  "A blind man with a cane in his right-hand walks confidently with long steps. He momentarily halts at the end of the footpath and keeps walking further": "High-level StartEnd Two-Level Text Descriptions : Qualitative Examples From Our Dataset. Annotation language captures both high-levelinformation regarding general action, as well as detailed low-level motion characteristics, mobilityaid strategies, goals, and environmental context. behavior with subtle adjustments to avoid obstacles detected by their mobility aids to various turningmotions where participants pivot or shift direction using their canes or guide dogs to navigate aroundcorners or obstacles. BlindWays also encompasses scenarios with careful and deliberate movements,such as walking along curbs to avoid falling off the sidewalk or colliding with obstacles, stairnavigation with motions like tapping the cane on each step to gauge height and depth and usinghandrails for support, and street crossing where participants may pause at the curb and performprecise cane movements to locate tactile paving or curb ramps. The data is captured in the Xsensjoint representation, comprising a total of 24 joints. Text Data: As shown in , high-level motion description annotations provide a summary ofthe blind persons actions and intent, along with their interactions with mobility aids. Annotations areapproximately 26 words long on average, with the longest being 111 words. The standard deviationof about 9.45 indicates a moderate variability in annotation length. In contrast, low-level annotationsprovide more detailed descriptions of specific actions along with step counts and detailed use ofmobility aids. Thus, these annotations are longer, approximately 44 words on average, with thelongest being 140 words. The standard deviation of about 17.80 also reflects a higher variabilityin length, indicative of the varying complexity and level of detail required for different scenarios.Additional details can be found in the supplementary.",
  "In this section, we evaluate human motion generation baselines on BlindWays to discuss modelgeneralizability and the role of text labels in blind motion modeling": "Metrics and Baselines: Our evaluation is structured into two parts. First, we discuss fundamentalgeneralization limitations in current text-driven motion generation models, which, despite being builton large motion-language datasets, lack specific categories to capture the unique motions of blindpedestrians. We employ standard metrics used in previous studies: motion-retrieval precision (RTop1) to evaluate the accuracy of matching between texts and motions, Frechet Inception Distance(FID) to assess the realism of generated motions, Diversity (DIV) to capture the variance of generated : Text to Motion Model Evaluation on BlindWays. We compare different methods andtraining datasets for a text-to-motion generation task evaluated on BlindWays. The metrics followstandard text-to-motion evaluation. For the Diversity (DIV) metric, the closer to the results with Realdata (first line in the table) the better. Each experiment is repeated 20 times and a statistical intervalwith 95% confidence is reported.",
  "HumanML3D Motion-X + BlindWays0.054 0.0098.612 0.4806.260 0.3014.921 0.051MotionGPT Motion-X + BlindWays0.036 0.00310.313 0.1833.874 0.1642.759 0.100": "motions, and Multi-Modality (MModality) to examine how generated motions vary within eachtext description . We adopt HumanML3D and MotionGPT as our state-of-the-arttext-to-motion baselines. HumanML3D proposes a framework involving a motion autoencoder, text-to-length, and text-to-motion synthesis, along with a large motion-language dataset for text-drivenhuman motion generation. MotionGPT integrates a generative pre-trained transformer to generatecomplex motion patterns from text descriptions, leveraging advanced language models. In the second part of our evaluation, we focus on a motion-driven motion generation task, wherewe adopt stochastic CVAE-based approaches as our baselines. While these baselinesare designed to predict future motion given a history of motion, we further analyze the impact oftext descriptions in BlindWays by incorporating a text embedding encoded with LLaMA2 . Weemploy standard diversity (Average Pairwise Distance, APD) and quality (Average DisplacementError, ADE, and Final Displacement Error, FDE, Normalized Power Spectrum Similarity, NPSS ,and Normalized Directional Motion Similarity, NDMS ) metrics for analysis. We train and testbaselines using the joint representation of SMPL (converted from the IMU-based motion capture suit)with an additional joint for the mobility aid of blind pedestrians, ensuring consistency and compara-bility across experiments and datasets. We split BlindWays into training (85%) and validation/test(15%) sets. For HumanML3D, we use the standard training and validation splits. Text-to-Motion: We provide a comparison of text-to-motion baselines using embedding-based analy-sis in . For evaluation, we train a feature embedding model (following HumanML3D ).Notably, we observe high FID scores when models are trained exclusively on Motion-X. The lack ofblind motion data in Motion-X leads these models to generate diverse yet unrealistic blind motions,increasing the feature space distance between generated and real blind motions. This results in FIDscores of 11.203 for HumanML3D and 15.002 for MotionGPT. However, training on BlindWayssignificantly enhances model performance, with HumanML3D and MotionGPT achieving FIDs of3.340 and 5.101, respectively. These scores indicate a closer alignment with the real datas FID,reflecting a higher degree of realism. R Precision further underscores the discrepancy in generaliz-ability. We find that the R Top-1 accuracy of models trained on Motion-X is notably lower (0.041for HumanML3D and 0.046 for MotionGPT) compared to those trained on BlindWays (0.060 forHumanML3D and 0.054 for MotionGPT), indicating that models trained on BlindWays achievea stronger alignment between text and motion features. In contrast, models trained on Motion-Xstruggle with this alignment, likely due to the lack of nuanced motion data and corresponding textspecific to blind pedestrians. This demonstrates that BlindWays effectively captures the diversityand subtleties of blind pedestrian movements, along with descriptive text, enabling more accuratetext-based retrieval and generation. Impact of Pre-training: We examine the impact of pre-training on Motion-X, a large-scale motion-language dataset that covers a broad range of human motions but lacks representations of movementsby people with disabilities. As shown in , pre-training on Motion-X provides the modelwith a strong understanding of diverse motion features and their alignment with correspondingtext descriptions. Specifically, HumanML3D shows an average improvement of 6.3 % in Diversityand 4.9% in Multi-Modality when leveraging pre-training on Motion-X, followed by fine-tuning : Analysis for Specific Keypoints. We analyze the performance of text-to-motion baselinesacross different skeleton joint types, including head, arms, and aid. Overall, we find that modelspre-trained on Motion-X and then fine-tuned on BlindWays underperform compared to those trainedon BlindWays from scratch, particularly in key joints with unique motion distributions in our dataset,such as the arm joints.",
  "Zero Velocity-0.640.8712.793.241.290.01MotionGPT 23.133.014.760.724.210.570.26CVAE 7.680.470.560.453.890.110.23DLow 11.650.460.590.413.910.120.27MDN 15.140.450.560.404.310.140.28": "on BlindWays, compared to training on BlindWays alone. This demonstrates the effectiveness ofpre-training on a general dataset to enhance motion diversity, even though it lacks categories thatcapture the unique movements of blind pedestrians. Per-Keypoint Evaluation: We further conduct a per-keypoint evaluation in the text-to-motion task toanalyze model performance on blind motion data, with a focus on joints exhibiting unique movementsin blind navigation. Specifically, we evaluate the head joint, arm joints (including shoulder, elbow,and wrist, based on participants dominant hand), and the mobility aid joint, using pose-spaceaccuracy metrics such as ADE and FDE. In blind motion, the arm joints are essential for capturingthe use and handling of mobility aids, e.g., for obstacle detection and navigation. The mobility aidkeypoint provides insights into the dynamic and coordinated interaction between the user and theiraid. As shown in , we find models trained on BlindWays demonstrate greater robustness,particularly in the arm joint and mobility aid keypoint. Specifically, we observe an average FDEimprovement of 8% for arm joints and 16.5% for mobility aid joints in models trained from scratch onBlindWays compared to those trained on Motion-X, highlighting the importance of domain-specifictraining data for accurately modeling nuanced blind motion behaviors associated with mobility aids.Interestingly, head movements are modeled more accurately by the Motion-X-trained model thanthe BlindWays-trained model, suggesting that the wider variety of head movements in the Motion-Xdataset enhances generalization for these joints. In principle, pre-training on such a dataset couldfacilitate model generalization for both common behavior joints and unique motion joints. However,in practice, we find this to result in mixed results due to the introduced bias and domain shift. Thisalso highlights a potential direction for future work. Motion-Conditioned Prediction: Finally, we evaluate the capabilities of motion-conditioned models,where both text context and past motion are provided as input to the model. This approach focuses onpredicting diverse and plausible future motions given a history of motion. The models are trainedto predict the next 9.5 seconds of future motion given 0.5 seconds of past motion. To accountfor surrounding context-based interactions, we further incorporate text embeddings into stochasticmodeling approaches, including CVAE , DLow , and MDN . For completeness, weincorporate a MotionGPT motion-to-motion baseline and results with a zero velocity model.We repetitively sample from MotionGPT to obtain APD and pose metrics; however, we note thatMotionGPT generally performs poorly in motion-conditioned prediction settings (consistent with the original study of ). depicts the results for the motion-conditioned models. We findCVAE-based methods to demonstrate better accuracy than MotionGPT, which generally predictsdiverse but unrealistic motion patterns. The CVAE baseline achieves an APD diversity of 7.68, whileDLow and MDN achieve 11.65 (52% higher) and 15.14 (97% higher) APD, respectively. This findinghighlights the benefits of an improved sampling mechanism. Specifically, MDN, which incorporatesa transformer-based module in the motion decoding process, significantly enhances both samplediversity and realism (with ADE decreasing from 0.47 to 0.45). We observe consistent improvementsin FID, DIV, and NDMS metrics, with MDN achieving the best results. However, for DLow, theincrease in diversity is shown to result in an accuracy trade-off, where FDE is increased (from0.56 to 0.59). While these advancements demonstrate promising strides in modeling diverse andrealistic motion, this work represents only an initial step. Future directions include enhancing modelgeneralizability to handle a broader array of rare motion scenarios, such as complex interactions withobstacles or varying terrain, which remain challenging for current models. Additional results andanalysis of motion prediction quality within various scenarios can be found in the supplementarymaterial.",
  "Limitations": "Our work addresses a prevalent bias in motion modeling datasets, specifically the focus on sightedand simplified pedestrian motion. Our study underscores the complexity of diverse motion modeling,particularly in cases where pre-training may be non-beneficial or even detrimental to model predic-tions, such as with blind motion. To tackle this bias, we collected realistically complex data withinan important but under-discussed use case. However, our study has several limitations. The samplesize of 11 participants, providing a dataset of 1,005 motion samples after filtering pose trackingfailure cases, is representative of in-situ accessibility studies. Nonetheless, additional real-worlddata from a more diverse participant pool could help identify further biases and model issues (e.g.,various physical characteristics such as different heights and backgrounds). Another limitation isthe expensive ($6,500) motion-capture suit, which may hinder larger-scale studies. While we chosehigher-cost, higher-quality tracking technology, lower-cost solutions (e.g., inertial, vision-based) arecontinuously being developed and can facilitate easier and more scalable capture, leading to morerobust and practical motion models across many underrepresented use cases in current human motionbenchmarks.",
  "Conclusion": "In this study, we introduce BlindWays, a novel benchmark focused on the unique motion behaviorsof blind and low-vision pedestrians navigating dynamic urban outdoor environments. Our datasetincludes 3D motion data enriched with high- and low-level text descriptions, derived from corre-sponding third-person and egocentric RGB videos that capture actions, intentions, and environmentalcontexts of blind motion in detailparticularly how individuals use mobility aids to interact withtheir surroundings. Our experiments show that, despite recent advancements, state-of-the-art motion-language models struggle to generalize to blind motion, highlighting the unique challenges presentedby this domain. This underscores the importance of a specialized blind motion benchmark to supportsafe and effective urban planning, such as in autonomous driving. BlindWays provides a contextuallyrich resource, enabling models to more accurately and diversely represent blind motion, advancingthe field of motion-language modeling while enhancing the safety and reliability of real-world,human-interactive systems.",
  "H.-S. Fang, J. Li, H. Tang, C. Xu, H. Zhu, Y. Xiu, Y.-L. Li, and C. Lu. AlphaPose: Whole-bodyregional multi-person pose estimation and tracking in real-time. PAMI, 2022": "D. M. Gavrila. The visual analysis of human movement: A survey. CVIU, 1999. D. M. Gavrila, L. S. Davis, et al. Towards 3-d model-based tracking and recognition ofhuman movement: a multi-view approach. In International workshop on automatic face-andgesture-recognition, 1995. D. Geruschat. Driver behavior in yielding to sighted and blind pedestrians at roundabouts. 2005. A. Gopalakrishnan, A. Mali, D. Kifer, L. Giles, and A. G. Ororbia. A neural temporal modelfor human motion prediction. In CVPR, 2019.",
  "C. Guo, X. Zuo, S. Wang, S. Zou, Q. Sun, A. Deng, M. Gong, and L. Cheng. Action2motion:Conditioned generation of 3d human motions. In ACMMM, 2020": "D. Guth et al. Blind and sighted pedestrians judgments of gaps in traffic at roundabouts. 2005. V. Guzov, A. Mir, T. Sattler, and G. Pons-Moll. Human poseitioning system (HPS): 3D humanpose estimation and self-localization in large scenes from body-mounted sensors. In CVPR,2021. A. Harrell. Driver response to a disabled ped. using a dangerous crosswalk. In JEP, 1992. A. Harrell. Effects of blind pedestrians on motorists. In JSP, 1994. M. Hassan, D. Ceylan, R. Villegas, J. Saito, J. Yang, Y. Zhou, and M. J. Black. Stochasticscene-aware motion prediction. In CVPR, 2021.",
  "The Guardian. Toyota Pauses Paralympics Self-Driving Buses After One Hits Visually ImpairedAthlete, 2021": "H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra,P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXivpreprint arXiv:2307.09288, 2023. J. Treviranus. The value of being different. In W4A, 2019. T. Von Marcard, R. Henschel, M. J. Black, B. Rosenhahn, and G. Pons-Moll. Recoveringaccurate 3d human pose in the wild using imus and a moving camera. In ECCV, 2018."
}