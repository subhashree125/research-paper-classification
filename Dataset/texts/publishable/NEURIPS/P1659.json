{
  "Abstract": "Current reinforcement-learning methods are unable to directly learn policies thatsolve the minimum cost reach-avoid problem to minimize cumulative costs subjectto the constraints of reaching the goal and avoiding unsafe states, as the structureof this new optimization problem is incompatible with current methods. Instead, asurrogate problem is solved where all objectives are combined with a weightedsum. However, this surrogate objective results in suboptimal policies that do notdirectly minimize the cumulative cost. In this work, we propose RC-PPO, areinforcement-learning-based method for solving the minimum-cost reach-avoidproblem by using connections to Hamilton-Jacobi reachability. Empirical resultsdemonstrate that RC-PPO learns policies with comparable goal-reaching rates towhile achieving up to 57% lower cumulative costs compared to existing methodson a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. Theproject page can be found at",
  "Introduction": "Many real-world tasks can be framed as a constrained optimization problem where reaching a goal atthe terminal state and ensuring safety (i.e., reach-avoid) is desired while minimizing some cumulativecost as an objective function, which we term the minimum-cost reach-avoid problem. The cumulative cost, which differentiates this from the traditional reach-avoid problem, can be usedto model desirable aspects of a task such as minimizing energy consumption, maximizing smoothness,or any other pseudo-energy function, and allows for choosing the most desirable policy among manypolicies that can satisfy the reach-avoid requirements. For example, energy-efficient autonomousdriving can be seen as a task where the vehicle must reach a destination, follow traffic rules,and minimize fuel consumption. Minimizing fuel use is also a major concern for low-thrust orenergy-limited systems such as spacecraft and quadrotors . Quadrotors often have to chooselimited battery life to meet the payload capacity. Hence, minimizing their energy consumption, whichcan be done by taking advantage of wind patterns, is crucial for keeping them airborne to completemore tasks. Other use-cases important for climate change include plasma fusion (reach a desiredcurrent, minimize the total risk of plasma disruption) and voltage control (reach a desired voltagelevel, minimize the load shedding amount) .",
  "arXiv:2410.22600v1 [cs.LG] 29 Oct 2024": "If only a single control trajectory is desired, this class of problems can be solved using numericaltrajectory optimization by either optimizing the timestep between knot points or a bilevel opti-mization approach that adjusts the number of knot points in an outer loop . However, in thissetting, the dynamics are assumed to be known, and only a single trajectory is obtained. Therefore, thecomputation will needs to be repeated when started from a different initial state. The computationalcomplexity of trajectory optimization prevents it from being used in real time. Moreover, the use ofnonlinear numerical optimization may result in poor solutions that lie in suboptimal local minim . Alternatively, to obtain a control policy, reinforcement learning (RL) can be used. However, existingmethods are unable to directly solve the minimum-cost reach-avoid problem. Although RL has beenused to solve many tasks where reaching a goal is desired, goal-reaching is encouraged as a rewardinstead of as a constraint via the use of either a sparse reward at the goal , or a surrogatedense reward 1. However, posing the reach constraint as a reward then makes it difficult tooptimize for the cumulative cost at the same time. In many cases, this is done via a weighted sumof the two terms . However, the optimal policy of this new surrogate objective may notnecessarily be the optimal policy of the original problem. Another method of handling this is to treatthe cumulative cost as a constraint and solve for a policy that maximizes the reward while keepingthe cumulative cost under some fixed threshold, resulting in a new constrained optimization problemthat can be solved as a constrained Markov decision process (CMDP) . However, the choice ofthis fixed threshold becomes key: too small and the problem is not feasible, destabilizing the trainingprocess. Too large, and the resulting policy will simply ignore the cumulative cost. To tackle this issue, we propose Reach Constrained Proximal Policy Optimization(RC-PPO), anew algorithm that targets the minimum-cost reach-avoid problem. We first convert the reach-avoidproblem to a reach problem on an augmented system and use the corresponding reach value functionto compute the optimal policy. Next, we use a novel two-step PPO-based RL-based framework tolearn this value function and the corresponding optimal policy. The first step uses a PPO-inspiredalgorithm to solve for the optimal value function and policy, conditioned on the cost upper bound.The second step fine-tunes the value function and solves for the least upper bound on the cumulativecost to obtain the final optimal policy. Our main contributions are summarized below:",
  "Related Works": "Terminal-horizon state-constrained optimizationTerminal state constraints are quite commonin the dynamic optimization literature. For the finite-horizon case, for example, one method ofguaranteeing the stability of model predictive control (MPC) is with the use of a terminal stateconstraint . Since MPC is implemented as a discrete-time finite-horizon numerical optimizationproblem, the terminal state constraints can be easily implemented in an optimization program as anormal state constraint. The case of a flexible-horizon constrained optimization is not as commonbut can still be found. For example, one method of time-optimal control is to treat the integrationtimestep as a control variable while imposing state constraints on the initial and final knot points .Another method is to consider a bilevel optimization problem, where the number of knot points isoptimized for in the outer loop . Goal-conditioned Reinforcement LearningThere have been many works on goal-conditionedreinforcement learning. These works mainly focus on the challenges of tackling sparse rewards or even learning without rewards completely, either via representation learningobjectives or by using contrastive learning to learn rewardfunctions , often in imitation learning settings . However,",
  "the manner in which these goals are reached is not considered, and it is difficult to extend these worksto additionally minimize some cumulative cost": "Constrained Reinforcement LearningOne way of using existing techniques to approximatelytackle the minimum-cost reach-avoid problem is to flip the role of the cumulative-cost objective andthe goal-reaching constraint by treating the goal-reaching constraint as an objective via a (sparse ordense) reward and the cumulative-cost objective as a constraint with a cost threshold, turning theproblem into a CMDP . In recent year, there has been significant interest in deep RL methods forsolving CMDPs . While these methods are effective at solving the transformed CMDPproblem, the optimal policy to the CMDP may not be the optimal policy to the original minimum-costreach-constrained problem, depending on the choice of the cost constraint. State Augmentation in Constrained Reinforcement LearningTo improve reward structuresin constrained reinforcement learning, especially in safety-critical systems, one effective approachis state augmentation. This technique integrates constraints, such as safety or energy costs, intothe augmented state representation, allowing for more effective constraint management through thereward mechanism . While these methods enhance the reward structure for solving thetransformed CMDP problems, they still face the inherent limitation of the CMDP framework: theoptimal policy for the transformed CMDP may not always correspond to the optimal solution for theoriginal problem. Reachability AnalysisReachability analysis looks for solutions to the reach-avoid problem. Thatis, to solve for the set of initial conditions and an appropriate control policy to drive a system toa desired goal set while avoiding undesireable states. Hamilton-Jacobi (HJ) reachability analysis provides a methodology for the case of dynamics in continuous-time via thesolution of a partial differential equation (PDE) and is conventionally solved via numerical PDEtechniques that use state-space discretization . This has been extended recently to the case ofdiscrete-time dynamics and solved using off-policy and on-policy reinforcementlearning. While reachability analysis concerns itself with the reach-avoid problem, we are insteadinterested in solutions to the minimum-cost reach-avoid problem.",
  "Problem Formulation": "In this paper, we consider a class of minimum-cost reach-avoid problems defined by the tupleM := X, U, f, c, g, h. Here, X Rn is the state space and U Rm is the action space. Thesystem states xt X evolves under the deterministic discrete dynamics f : X U X as",
  "xt+1 = f(xt, ut).(1)": "The control objective for the system states xt is to reach the goal region G and avoid the unsafe setF while minimizing the cumulative cost T 1t=0 c(xt, (xt)) under control input ut = (xt) for adesigned control policy : X U. Here, T denotes the first timestep that the agent reaches thegoal G. The sets G and F are given as the 0-sublevel and strict 0-superlevel sets g : X R andh : X R respectively, i.e.,",
  "xt+1 = fxt, (xt).(3d)": "Note that as opposed to either traditional finite-horizon constrained optimization problems where Tis fixed or infinite-horizon problems where T = , the time horizon T is also a decision variable.Moreover, the goal constraint (3b) is only enforced at the terminal timestep T. These two differencesprevent the straightforward application of existing RL methods to solve (3).",
  "Reachability Analysis for Reach-Avoid Problems": "In discrete time, the set of initial states that can reach the goal G without entering the avoid set F canbe represented by the 0-sublevel set of a reach-avoid value function V g,h . Given functions g, hdescribing G and F and a policy , the reach-avoid value function V g,h : X R is defined as",
  "V g,h(x0) = minT N maxg(xT ),maxt{0,...,T } h(xt ),(4)": "where xt denote the system state at time t under a policy starting from an initial state x0 = x0. Inthe rest of the paper, we suppress the argument x0 for brevity whenever clear from the context. Itcan be shown that the reach-avoid value function satisfies the following recursive relationship via thereach-avoid Bellman equation (RABE)",
  "The Bellman equation (5) can then be used in a reinforcement learning framework (e.g., via amodification of soft actor-critic) as done in to solve the reach-avoid problem": "Note that existing methods of solving reach-avoid problems through this formulation focus onminimizing the value function V g,h. This is not necessary as any policy that results in V g,h 0 solvesthe reach-avoid problem, albeit without any cost considerations. However, it is often the case that wewish to minimize a cumulative cost (e.g., (3a)) on top of the reach-avoid constraints (3b)-(3c) for aminimum-cost reach-avoid problem. To address this class of problems, we next present a modificationto the reach-avoid framework that additionally enables the minimization of the cumulative cost.",
  "Define the augmented state as x = (x, y, z) X := X {1, 1}R. We now define a correspondingaugmented dynamics function f : X U X asfxt, yt, zt, ut=f(xt), max{If(xt)F, yt}, zt c(xt, ut),(7)": "where y0 = Ix0F. Note that yt = 1 if the state has entered the avoid set F at some timestep from 0to t and is unsafe, and yt = 0 if the state has not entered the avoid set F at any timestep from 0 tot and is safe. Moreover, zt is equal to z0 minus the cost-to-come, i.e., for state trajectory x0:t andaction trajectory u0:t, i.e.,",
  "g(x, y, z) := max{g(x), Cy, z},(9)": "where C > 0 is an arbitrary constant.2 With this definition of g, an augmented goal region G can bedefined asG := {x | g(x) 0} = {(x, y, z) | x G, y = 1, z 0}.(10)In other words, starting from initial condition x0 = (x0, y0, z0), reaching the goal on the augmentedsystem xT g at timestep T implies that 1) the goal is reached at xT for the original system, 2)the state trajectory remains safe and does not enter the avoid set F, and 3) z0 is an upper-bound onthe total cost-to-come: T 1t=0 c(xt, ut) z0. We call this the upper-bound property. The aboveintuition on the newly defined augmented system is formalized in the following theorem, whose proofis provided in Appendix D.1.",
  "In practice, we use C = maxxX g(x)": "Theorem 1. For given initial conditions x0 X, z0 R and control policy , consider the trajectoryfor the original system {x0, . . . xT } and its corresponding trajectory for the augmented system{(x0, y0, z0), . . . (xT , yT , zT )} for some T > 0. Then, the reach constraint xT G (3b), avoidconstraint xt F t {0, 1, . . . , T} (3c) and the upper-bound property z0 T 1k=0 cxk, (xk)",
  "hold if and only if the augmented state reaches the augmented goal at time T, i.e., (xT , yT , zT ) G": "With this construction, we have folded the avoid constraints xt F (3c) into the reach specificationon the augmented system. In other words, solving the reach problem on the augmented system resultsin a reach-avoid solution of the original system. As a result, we can simplify the value function (4) andBellman equation (5), resulting in the following definition of the reach value function Vg : X R",
  "whose proof we provide in Appendix D.2": "We now solve the minimum-cost reach-avoid problem using this augmented system. By Theorem 1,the z0 is an upper bound on the cumulative cost to reach the goal while avoiding the unsafe set if andonly if the augmented state x reaches the augmented goal. Since this upper bound is tight, the leastupper bound z0 that still reaches the augmented goal thus corresponds to the minimum-cost policythat satisfies the reach-avoid constraints. In other words, the minimum-cost reach-avoid problem fora given initial state x0 can be reformulated as the following optimization problem.",
  "s.t.V g (x0, Ix0F, z0) 0.(13b)": "We refer to Appendix B for a detailed derivation of the equivalence between the transformedProblem 13 and the original minimum-cost reach-avoid Problem 3.Remark 1 (Connections to the epigraph form in constrained optimization). The resulting optimizationproblem (13) can be interpreted as an epigraph reformulation of the minimum-cost reach-avoidproblem (3). The epigraph reformulation results in a problem with linear objective but yields thesame solution as the original problem . The construction we propose in this work can be seen asa dynamic version of this epigraph reformulation technique originally developed for static problemsand is similar to recent results that also make use of the epigraph form for solving infinite-horizonconstrained optimization problems .",
  "Solving with Reinforcement Learning": "In the previous section, we reformulated the minimum-cost reach-avoid problem by constructing anaugmented system and used its reach value function (11) in a new constrained optimization problem(13) over the cost upper-bound z0. In this section, we propose Reachability Constrained ProximalPolicy Optimization (RC-PPO), a two-phase RL-based method for solving (13) (see ).",
  "Phase 1: Learn z-conditioned policy and value function": "In the first step, we learn the optimal policy and value function V g , as functions of the costupper-bound z0, using RL. To do so, we consider the policy gradient framework . However,since the policy gradient requires a stochastic policy in the case of deterministic dynamics , weconsider an analog of the developments made in the previous section but for the case of a stochasticpolicy. To this end, we redefine the reach value function V g using a similar Bellman equation undera stochastic policy as follows.",
  "Execution": ": Summary of the RC-PPO algorithm. In phase one, the original dynamic system istransformed into the augmented dynamic system defined in (7). Then RL is used to optimize valuefunction V g and learn a stochastic policy . In phase two, we fine-tune V g on a deterministic versionof and compute the optimal upper-bound z to obtain the optimal deterministic policy . Definition 1 (Stochastic Reachability Bellman Equation). Given function g in (9), a stochastic policy, and initial conditions x0 X, z0 R, the stochastic reach value function V g is defined as thesolution to the following stochastic reachability Bellman equation (SRBE):V g (xt) = E[min{g(xt), V g (xt+1)}]t 0,(14)",
  "under the stationary distribution d(x) for Reachability MDP in Definition 2": "The proof of this new policy gradient theorem (Theorem 2) follows the proof of the normal policygradient theorem , differing only in the expression of the stationary distribution. We provide theproof in Appendix D.3. Since the stationary distribution d(x) in Definition 2 is hard to simulate during the learning process,we instead consider the stationary distribution under the original augmented dynamic system. Notethat Definition 1 does not induce a contraction map, which harms performance. To fix this, we applythe same trick as by introducing an additional discount factor into the Bellman equation (12):V g (xt) = (1 )g(xt) + Ext+1[min{g(xt), V g (xt+1)}].(18) This provides us with a contraction map (proved in ) and we leave the discussion of choosing in Appendix C. The Q-function corresponding to (18) is then given asQg (xt, ut) = (1 )g(xt) + min{g(xt), V g (xt+1)}.(19)Following proximal policy optimization (PPO) , we use generalized advantage estimation (GAE) to compute a variance-reduced advantage function Ag = Qg V g for the policy gradient",
  "l(u | x)Al(GAE)g(x, u), CLIP, Al(GAE)g(x, u).(21)": "We wish to obtain the optimal policy and the value function V gconditioned on z0. Hence, at thebeginning of each rollout, we uniformly sample z0 within a user-specified range [zmin, zmax]. Sincethe optimal z0 is the cumulative cost of the policy that solves the minimum-cost reach-avoid problem,zmin and zmax are user-specified bounds on the optimal cost. In particular, when the cost-function isbounded and the optimal cost is non-negative, we can choose zmin to be some negative number andzmax to be the maximum possible discounted cost.",
  "In the second phase, we first compute a deterministic version of the stochastic policy from phase1 by taking the mode. Next, we fine-tune V g based on the now deterministic to obtain V g": "Given any state x, the final policy is then obtained by solving for the optimal cost upper-bound zfrom Equation (13), which is a 1D root-finding problem and can be easily solved using bisection. Notethat Equation (13) must be solved online for z at each state x. Alternatively, to avoid performingbisection online, we can instead learn the map (x, y) z offline using regression with randomlysampled (x, y) pairs and z labels obtained from bisection offline.",
  "Experiments": "BaselinesWe consider two categories of RL baselines. The first is goal-conditioned reinforcementlearning which focuses on goal-reaching but does not consider minimization of the cost. For thiscategory, we consider the Contrastive Reinforcement Learning (CRL) method. We also compareagainst safe RL methods that solve CMDPs. As the minimum-cost reach-avoid problem (3) cannotbe posed as a CMDP, we reformulate (3) into the following surrogate CMDP:",
  "tc(xt, ut) Xthreshold.(22c)": "where the reward r incentivies goal-reaching, Cfail is a term balancing two constraint terms, andXthreshold is a hyperparameter on the cumulative cost. For this category, we consider the CPPO and RESPO . Note that RESPO also incorporates reachability analysis to adapt the Lagrangemultipliers for each constraint term. We implement the above CMDP-based baselines with threedifferent choices of Xthresholds: XL, XM and XH. For RESPO, we found XM to outperform both XLand XH and thus only report results for XM. We also consider the static Lagrangian multiplier case. In this setting, the reward function becomesr(xt)(1xtF Cfail +c(xt, ut)) for a constant Lagrange multiplier . We consider two differentlevels of (L, H) in our experiments, resulting in the baselines PPO_L, PPO_H, SAC_L,SAC_H. More details are provided in Appendix F.",
  ": Reach rates under the sparse reward setting. RC-PPO consistently achieves the highestreach rates in all benchmark tasks. Error bars denote the standard error": "BenchmarksWe compare RC-PPO with baseline methods on several minimum-cost reach-avoidenvironments. We consider an inverted pendulum (Pendulum), an environment from Safety Gym (PointGoal) and two custom environments from MuJoCo , (Safety Hopper, SafetyHalfCheetah) with added hazard regions and goal regions. We also consider a 3D quadrotornavigation task in a simulated wind field for an urban environment (WindField) and anFixed-Wing avoid task from with an additional goal region (FixedWing). More details on thebenchmark can be found in Appendix G. Evaluation MetricsSince the goal of RC-PPO is minimizing cost consumption while reachinggoal without entering the unsafe region F. We evaluate algorithm performance based on (i) reach rate,(ii) cost. The reach rate is the ratio of trajectories that enter goal region G without violating safetyalong the trajectory. The cost denotes the cumulative cost over the trajectory Tk=0 c(xk, (xk)).",
  "Sparse Reward Setting": "We first compare our algorithm with other baseline algorithms under a sparse reward setting ().In all environments, the reach rate for the baseline algorithms is very low. Also, there is a generaltrend between the reach rate and the Lagrangian coefficient. CPPO_XL, PPO_H and SAC_H havehigher Lagrangian coefficients which lead to a lower reach rate.",
  "Comparison under Reward Shaping": "Reward shaping is a common method that can be used to improve the performance of RL algorithms,especially in the sparse reward setting . To see whether the same conclusions still hold evenin the presence of reward shaping, we retrain the baseline methods but with reward shaping using adistance function-based potential function (see Appendix F for more details). The results in demonstrate that RC-PPO remains competitive against the best baselinealgorithms in reach rate while achieving significantly lower cumulative costs. The baseline methods(PPO_H, SAC_H, CPPO_XL) fail to achieve a high reach rate due to the large weights placedon minimizing the cumulative cost. CRL can reach the goal for simpler environments (Pendulum)but struggles with more complex environments. However, since goal-conditioned methods do notconsider minimize cumulative cost, it achieves a higher cumulative cost relative to other methods.",
  "(b) WindField": ": Trajectory comparisons. On Pendulum, RC-PPO learns to perform an extensive energypumping strategy to reach the goal upright position (green line), resulting in vastly lower cumulativeenergy. On WindField, RC-PPO takes advantage instead of fighting against the wind field, resultingin a faster trajectory to the goal region (green box) that uses lower cumulative energy. The start of thetrajectory is marked by . Other baselines focus more on goal-reaching tasks while putting less emphasis on the cost part. As aresult, they suffer from higher costs than RC-PPO. We can also observe that RESPO achieves lowercumulative cost compared to CPPO_XM which shares the same Xthreshold. This is due to RESPOmaking use of reachability analysis to better satisfy constraints. To see how RC-PPO achieves lower cumulative costs, we visualize the resulting trajectories forPendulum and WindField in . For Pendulum, we see that RC-PPO learns to perform energypumping to reach the goal in more time but with a smaller cumulative cost. The optimal behavior isopposite in the case of WindField, which contains an additional constant term in the cost to modelthe energy draw of quadcopters (see Appendix G). Here, we see that RC-PPO takes advantage of thewind at the beginning by moving downwind, arriving at the goal faster and with less cumulative cost. We also visualize the learned RC-PPO policy for different values of z on the Pendulum benchmark(see Appendix H.2). For small values of z, the policy learns to minimize the cost, but at the expense ofnot reaching the goal. For large values of z, the policy reaches the goal quickly but at the expense ofa large cost. The optimal zopt found using the learned value function V gfinds the z that minimizesthe cumulative cost but is still able to reach the goal.",
  "Optimal solution of minimum-cost reach-avoid cannot be obtained using CMDP": "Though the previous subsections show the performance benefits of RC-PPO over existing methods,this may be due to badly chosen hyperparameters for the baseline methods, particularly in theformulation of the surrogate CMDP (22). We thus pose the following question: Can CMDPmethods perform well under the right parameters of the surrogate CMDP problem (22) ?.",
  "PPO across differentreward coefficients": ": Pareto front of PPO across different reward coefficients. RC-PPO outperforms the entirePareto front of what can be achieved by varying the reward function coefficients of the surrogateCMDP problem when solved using PPO. Empirical Study. To answer this question, we first perform an extensive grid search over both thedifferent coefficients in (22) and the static Lagrange multiplier for PPO (see Appendix H.3) and plotthe result in . RC-PPO outperforms the entire Pareto front formed from this grid search,providing experimental evidence that the performance improvements of RC-PPO stem from having abetter problem formulation as opposed to badly chosen hyperparameters for the baselines. Theoretical Analysis on Simple Example. To complement the empirical study, we provide an exam-ple of a simple minimum-cost reach-avoid problem where we prove that no choice of hyperparameterleads to the optimal solution in Appendix I.",
  "Conclusion and Limitations": "We have proposed RC-PPO, a novel reinforcement learning algorithm for solving minimum-costreach-avoid problems. We have demonstrated the strong capabilities of RC-PPO over prior methodsin solving a multitude of challenging benchmark problems, where RC-PPO learns policies that matchthe reach rates of existing methods while achieving significantly lower cumulative costs. However, it should be noted that RC-PPO is not without limitations. First, the use of augmenteddynamics enables folding the safety constraints within the goal specifications through an additionalbinary state variable. While this reduces the complexity of the resulting algorithm, it also meansthat two policies that are both unable to reach the goal can have the same value V g even if one isunsafe, which can be undesirable. Next, the theoretical developments of RC-PPO are dependenton the assumptions of deterministic dynamics, which can be quite restrictive as it precludes theuse of commonly used techniques for real-world deployment such as domain randomization. Weacknowledge these limitations and leave resolving these challenges as future work.",
  "and Disclosure of Funding": "This work was partly supported by the National Science Foundation (NSF) CAREER Award #CCF-2238030, the MIT Lincoln Lab, and the MIT-DSTA program. Any opinions, findings, conclusions, orrecommendations expressed in this publication are those of the authors and dont necessarily reflectthe views of the sponsors. Xuewei Qi, Yadan Luo, Guoyuan Wu, Kanok Boriboonsomsin, and Matthew Barth. Deepreinforcement learning enabled self-learning control for energy efficient driving. TransportationResearch Part C: Emerging Technologies, 99:6781, 2019. Ying Zhang, Tao You, Jinchao Chen, Chenglie Du, Zhaoyang Ai, and Xiaobo Qu. Safe andenergy-saving vehicle-following driving decision-making framework of autonomous vehicles.IEEE Transactions on Industrial Electronics, 69(12):1385913871, 2021.",
  "Jack Langelaan. Long distance/duration trajectory optimization for small uavs. In AIAAguidance, navigation and control conference and exhibit, page 6737, 2007": "Allen M Wang, Oswin So, Charles Dawson, Darren T Garnier, Cristina Rea, and ChuchuFan. Active disruption avoidance and trajectory design for tokamak ramp-downs with neuraldifferential equations and reinforcement learning. arXiv preprint arXiv:2402.09387, 2024. Renke Huang, Yujiao Chen, Tianzhixi Yin, Xinya Li, Ang Li, Jie Tan, Wenhao Yu, YuanLiu, and Qiuhua Huang. Accelerated deep reinforcement learning based load shedding foremergency voltage control. arXiv preprint arXiv:2006.12667, 2020. Christoph Rsmann, Frank Hoffmann, and Torsten Bertram. Timed-elastic-bands for time-optimal point-to-point nonlinear model predictive control. In 2015 european control conference(ECC), pages 33523357. IEEE, 2015.",
  "Robin M Pinson and Ping Lu. Trajectory design employing convex optimization for landing onirregularly shaped asteroids. Journal of Guidance, Control, and Dynamics, 41(6):12431256,2018": "Haichao Hong, Arnab Maity, and Florian Holzapfel. Free final-time constrained sequentialquadratic programmingbased flight vehicle guidance. Journal of Guidance, Control, andDynamics, 44(1):181189, 2021. Kyle Stachowicz and Evangelos A Theodorou. Optimal-horizon model predictive controlwith differential dynamic programming. In 2022 International Conference on Robotics andAutomation (ICRA), pages 14401446. IEEE, 2022. Damien Ernst, Mevludin Glavic, Florin Capitanescu, and Louis Wehenkel. Reinforcementlearning versus model predictive control: a comparison on a power system problem. IEEETransactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):517529, 2008. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder,Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experiencereplay. Advances in neural information processing systems, 30, 2017. Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, GlennPowell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, et al.Multi-goalreinforcement learning: Challenging robotics environments and request for research. arXivpreprint arXiv:1802.09464, 2018. Alexander Trott, Stephan Zheng, Caiming Xiong, and Richard Socher. Keeping your dis-tance: Solving sparse reward tasks using self-balancing shaped rewards. Advances in NeuralInformation Processing Systems, 32, 2019.",
  "Yifan Wu, George Tucker, and Ofir Nachum. The laplacian in rl: Learning representations withefficient approximations. arXiv preprint arXiv:1810.04586, 2018": "Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Over-coming exploration in reinforcement learning with demonstrations. In 2018 IEEE internationalconference on robotics and automation (ICRA), pages 62926299. IEEE, 2018. Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline Devin, Benjamin Eysenbach,and Sergey Levine. Learning to reach goals via iterated supervised learning. arXiv preprintarXiv:1912.06088, 2019.",
  "Annie Xie, Avi Singh, Sergey Levine, and Chelsea Finn. Few-shot goal inference for visuomotorlearning and planning. In Conference on Robot Learning, pages 4052. PMLR, 2018": "Justin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and Sergey Levine. Variational inverse controlwith events: A general framework for data-driven reward definition. Advances in neuralinformation processing systems, 31, 2018. Daniel Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond sub-optimal demonstrations via inverse reinforcement learning from observations. In Internationalconference on machine learning, pages 783792. PMLR, 2019. Ksenia Konyushkova, Konrad Zolna, Yusuf Aytar, Alexander Novikov, Scott Reed, SerkanCabi, and Nando de Freitas. Semi-supervised reward learning for offline reinforcement learning.arXiv preprint arXiv:2012.06899, 2020. Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski,Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task roboticreinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021.",
  "Danfei Xu and Misha Denil. Positive-unlabeled reward learning. In Conference on RobotLearning, pages 205219. PMLR, 2021": "Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden,Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitationlearning. In Conference on Robot Learning, pages 247263. PMLR, 2021. Suraj Nair, Eric Mitchell, Kevin Chen, Silvio Savarese, Chelsea Finn, et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference onRobot Learning, pages 13031315. PMLR, 2022.",
  "Kostas Margellos and John Lygeros. Hamiltonjacobi formulation for reachavoid differentialgames. IEEE Transactions on automatic control, 56(8):18491861, 2011": "Somil Bansal, Mo Chen, Sylvia Herbert, and Claire J Tomlin. Hamilton-jacobi reachability: Abrief overview and recent advances. In 2017 IEEE 56th Annual Conference on Decision andControl (CDC), pages 22422253. IEEE, 2017. Jaime F Fisac, Neil F Lugovoy, Vicen Rubies-Royo, Shromona Ghosh, and Claire J Tomlin.Bridging hamilton-jacobi safety analysis and reinforcement learning. In 2019 InternationalConference on Robotics and Automation (ICRA), pages 85508556. IEEE, 2019. Kai Chieh Hsu, Vicen Rubies-Royo, Claire J Tomlin, and Jaime F Fisac. Safety and livenessguarantees through reach-avoid reinforcement learning. In 17th Robotics: Science and Systems,RSS 2021. MIT Press Journals, 2021.",
  "Milan Ganai, Zheng Gong, Chenning Yu, Sylvia Herbert, and Sicun Gao. Iterative reachabilityestimation for safe reinforcement learning. Advances in Neural Information Processing Systems,36, 2024": "Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithmsand applications. arXiv preprint arXiv:1812.05905, 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Internationalconference on machine learning, pages 18611870. PMLR, 2018.",
  "Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press,2004": "Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-ods for reinforcement learning with function approximation. Advances in neural informationprocessing systems, 12, 1999. David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.Deterministic policy gradient algorithms. In International conference on machine learning,pages 387395. Pmlr, 2014.",
  "COptimal Reach Value Function": "As shown in (18), we introduce an additional discount factor into the estimation of V g . It will incurimprecision on the calculation of V g defined in Definition 1. In this section, we show that for a largeenough discount factor < 1, we could reach unbiased z in phase two of RC-PPO.",
  "where x = f(x, u)": "Next, we consider unrolling V g (x) under Reachability MDP in Definition 2. We define Pr(x x, k, ) as the probability of transitioning from state x to x in k steps under policy in 2. Notethat 1g(x)> Vg(x) is absorbed using the absorbing state in 2. Then we can get",
  "(k) = o(1(k))": "Assumption 2. (Differentiability and and Lipschitz Continuity) For any state and action pair (x, u),Qg(x, u; ) and (x; ) are continuously differentiable in and . Furthermore, for any state andaction pair (x, u), Qg(x, u; ) and (x; ) are Lipschitz function in and . Also, we assume that X and U are finite and bounded and the horizon T is also bounded by Tmax,then the cost function c can be bounded by Cmax and g can be bounded within Gmax. We can limitthe space of cost upper bound z [Gmax, T Cmax] instead of R. This is due to g(x) = z forz Gmax. Next, we could do discretization on [Gmax, T Cmax] and cost function c to makethe augmented state set X finite and bounded. With the above assumptions, we can provide a convergence guarantee for Algocrithm 1.Theorem 4. Under Assumptions 1 and 2, the policy update in Algorithm 1 converge almost surely toa locally optimal policy.",
  "The optimization goal formulation for CPPO is as follows:min maxL(, ) = V r (x) + c (V c (x) Xthreshold) + f V f (x)": "In this formulation, the soft constraint V c has the same priority as the hard constraint V f . This leadsto a potential imbalance between soft constraints and hard constraints. Instead, the optimization goalfor RESPO is as follows:min maxL(, ) = V r (x) + c (V c (x) Xthreshold) + f V f (x) (1 p(x)) + p(x) V f (x)where p(x) denotes the probability of entering the unsafe region F start from state x. It is calledthe reachability estimation function (REF). This formulation prioritizes the satisfaction of hardconstraints but still suffers from balancing soft constraints and reward terms.",
  "GExperiment Details": "In this section, we provide more details about the benchmarks and the choice of reward functionr, g, cost function c and Ccost in each environment. Under the sparse reward setting, we apply thefollowing structure of reward designr(xt, ut, xt+1) = Rgoal 1xt+1Gwhere Rgoal is an constant. After doing reward shaping, we add an extra term (xt+1) (xt) andthe reward becomesr(xt, ut, xt+1) = Rgoal 1xt+1G + (xt+1) (xt)where denotes the discount factor. Note that we set Rgoal = Ccost = 20 in all the environments. Note that if there is a gap betweenmax{g(x) | g(x) < 0}, we could get unbiased z during phase two of RC-PPO guaranteed byTheorem 3. To achieve better performance in phase two of RC-PPO, we setg(x) = 300for all x G to maintain such a gap. Also, we implement all the environments in Jax for betterscalability and parallelization.",
  "Hyperparameters for On-policy AlgorithmsValues": "On-policy parametersNetwork ArchitectureMLPUnits per Hidden Layer256Numbers of Hidden Layers2Hidden Layer Activation FunctiontanhEntropy coefficientLinear Decay 1e-2 0OptimizerAdamDiscount factor 0.99GAE lambda parameter0.95Clip Ratio0.2Actor Learning rateLinear Decay 3e-4 0Reward/Cost Critic Learning rateLinear Decay 3e-4 0 RESPO specific parametersREF Output Layer Activation FunctionsigmoidLagrangian multiplier Output Layer Activation functionsoftplusLagrangian multiplier Learning rateLinear Decay 5e-5 0REF Learning RateLinear Decay 1e-4 0",
  "G.1Pendulum": "The Pendulum environment is taken from Gym and the torque limit is set to be 1. The statespace is given by x = where , . In this task, we do not consider unsaferegions and setG := { | ( + dt) < 0}where dt = 0.05 is the time interval during environment simulation. This is for preventing environ-ment overshooting during simulation.",
  "G.2Safety Hopper": "The Safety Hopper environment is taken from Safety Mujoco, we add static obstacles in the envi-ronment to increase the difficulty of the task. We use x to denote the x-axis position of the head ofHopper, y to be the y-axis position of the head of Hopper. Then the goal region can be described as",
  "G.3Safety HalfCheetah": "The Safety HalfCheetah environment is taken from Safety Mujoco, we add static obstacles in theenvironment to increase the difficulty of the task. We use xfront to denote the x-axis position of thefront foot of Halfcheetah, yfront to be the y-axis position of the back foot of Halfcheetah, xback todenote the x-axis position of the back foot of Halfcheetah, yback to be the y-axis position of the backfoot of Halfcheetah, xhead to denote the x-axis position of the head of Halfcheetah, yhead to be they-axis position of the head of Halfcheetah. Then the goal region can be described as",
  "G.5Quadrotor in Wind Field": "We take quadrotor dynamics from crazyflies and wind field environments in the urban area from .The wind field will disturb the quadrotor with extra movement on both x-axis and y-axis. There arestatic building obstacles in the environment and we treat them as the unsafe region F. The goal forthe quadrotor is to reach the mid-point of the city. We divide the whole city into four sections andtrain single policy on each of the sections. We use x to denote the x-axis position ofquadrotor, y to be the y-axis position of quadrotor.",
  "G.6PointGoal": "The PointGoal environment is taken from Safety Gym We implement PointGoal environmentsin Jax. In Safety Gym environment, we do not perform reward-shaping and use the original rewarddefined in Safety Gym environments. In this case, the distance reward is set also to be 20 in order toalign* with Cgoal and Ccost. Different from sampling outside the hazard region which is implementedin Safety Gym, we allow Point to be initialized within the hazard region. We use x to denote thex-axis position of Point, y to be the y-axis position of Point, xgoal to denote the x-axis position ofGoal, and ygoal to denote the y-axis position of Goal. The goal region is given by",
  "zopt + 300": ": Learned RC-PPO policy for different z on Pendulum. For a smaller cost lower-bound z,cost minimization is prioritized at the expense of not reaching the goal. For a larger cost lower-boundz, the goal is reached using a large cumulative cost. Performing rootfinding to solve for the optimalzopt automatically finds the policy that minimizes cumulative costs while still reaching the goal.",
  ": Minimum-cost reach-avoid example to illustrate the limitation of CMDP-based formulation": "Consider the minimum-cost reach-avoid problem shown in , where we use C to denote thecost. States A and B are two initial states with the same initial distribution probability. State G1,G2, and G3 are three goal states. State I is the absorbing state (non-goal). We use pA and pB todenote the policy parameter, which represents the probability of choosing left action on state A andB separately.",
  "Hence, the optimal solution of the surrogate multi-objective problem can be suboptimal for theoriginal minimum-cost reach-avoid problem under any weight coefficients": "Of course, this is just one choice of reward function where the optimal solution of the minimum-costreach-avoid problem cannot be recovered. Given knowledge of the optimal policy, we can constructthe reward such that the multi-objective optimization problem does include the optimal policy as asolution. However, this is impossible to do if we do not have prior knowledge of the optimal policy,as is typically the case.",
  ".(38)": "However, the true optimal solution of pA = pB = 1 is NOT an optimal solution to the CMDP.To see this, taking X_thresh = 20, the real optimal solution pA = pB = 1 gives a reward of R = 15,but the CMDP solution pA = 0, pB = 2010",
  "10pA + 20(1 pA) XA,10pB + 20(1 pB) XB(39)": "Choosing exactly the cost of the optimal policy, i.e., XA = 10 and XB 30, also recovers theoptimal solution of pA = pB = 1. This now requires knowing the smallest cost to reach the goalfor every state, which is difficult to do beforehand and not feasible. On the other hand, RC-PPOdoes exactly this in the second phase when optimizing for z0. We can thus interpret RC-PPO asautomatically solving for the best cost threshold to use as a constraint for every initial state.",
  "JBroader impact": "Our proposed algorithm solves an important problem that is widely applicable to many differentreal-world tasks including robotics, autonomous driving, and drone delivery. Solving this bringsus one step closer to more feasible deployment of these robots in real life. However, the proposedalgorithm requires GPU training resources, which could contribute to increased energy usage."
}