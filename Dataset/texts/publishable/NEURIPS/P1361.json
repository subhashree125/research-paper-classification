{
  "Abstract": "3D generation methods have shown visually compelling results powered by diffu-sion image priors. However, they often fail to produce realistic geometric details,resulting in overly smooth surfaces or geometric details inaccurately baked inalbedo maps. To address this, we introduce a new method that incorporates touchas an additional modality to improve the geometric details of generated 3D assets.We design a lightweight 3D texture field to synthesize visual and tactile textures,guided by 2D diffusion model priors on both visual and tactile domains. We con-dition the visual texture generation on high-resolution tactile normals and guidethe patch-based tactile texture refinement with a customized TextureDreambooth.We further present a multi-part generation pipeline that enables us to synthesizedifferent textures across various regions. To our knowledge, we are the first to lever-age high-resolution tactile sensing to enhance geometric details for 3D generationtasks. We evaluate our method in both text-to-3D and image-to-3D settings. Ourexperiments demonstrate that our method provides customized and realistic finegeometric textures while maintaining accurate alignment between two modalitiesof vision and touch.",
  "Introduction": "Generating high-fidelity 3D assets is crucial for a wide range of applications, from content creationin gaming and VR/AR to developing realistic simulations for robotics. Recently, a surge in emergingmethods has enabled the creation of 3D assets from a single image or a short text prompt .Their success can be attributed to advances in generative models and neural rendering , aswell as the availability of extensive 2D and 3D datasets . Although existing methods can effectively capture the overall shape and visual appearance of objects,they often struggle with synthesizing fine-grained geometric details, such as stochastic patterns orbumps from the material, as shown on the left side of . As a result, the final mesh outputtends to be either overly smooth (e.g., the avocado in the top row) or has geometric details incorrectlybaked into the albedo map (e.g., the beanie in the bottom row). But why is that? We argue that there are two major bottlenecks. First, high-resolution geometric data are largelyabsent in current 2D and 3D datasets. For image datasets like LAION , obtaining geometric detailsis challenging due to the limited camera resolution in capturing real-world objects. In the case of3D asset datasets like Objaverse , although the assets often include visual textures, they rarelyfeature high-resolution geometric textures. Second, it is difficult for humans to precisely describe finegeometric textures in natural language, making text-based applications even more challenging. To address these issues, we propose exploring tactile sensing to capture high-resolution texture datafor 3D generation. Given a text prompt or an input image, we first generate a base mesh with analbedo map as our 3D representation. We then capture detailed surface geometry of the target texture",
  "A phone case": ": Our method leverages tactile sensing to improve existing 3D generation pipelines. Left: Given a textprompt, we first generate an image using SDXL and then run Wonder3D to generate mesh from theimage. This process often results in a mesh with an overly smooth surface. Right: Our method takes a text promptand several tactile patches and generates high-fidelity coherent visual and tactile textures that can be transferredto different meshes. Our method can easily adapt to image-to-3D tasks, as shown in the rightmost column, withthe reference images thumbnail displayed at the bottom right corner. Please visit our webpage for video results. using GelSight , a high-resolution tactile sensor. We then convert these tactile data intonormal maps and train a TextureDreambooth on these normal maps. To refine the albedo map andensure alignment between visual and tactile modalities, we learn a lightweight 3D texture field thatco-optimizes the albedo and normal maps using 2D diffusion guidance across both domains.",
  "Related Work": "3D generation. Following the success of text-to-image models , we have witnesseda booming development of 3D generative models conditioned on text or images. Recent workshave used 2D diffusion priors in 3D generation , with notablemethods like DreamFusion introducing Score Distillation Sampling (SDS) to optimize a 3Drepresentation using gradients from 2D diffusion models. Follow-up works have further extendedSDS optimization using multi-view diffusion models, improving both 3D generation and single-viewreconstruction . Another line of research trains large-scale transformers to generate 3D shapes ina feed-forward manner, requiring high-quality large-scale 3D asset datasets. While these modelseffectively capture global shapes, they often fail to accurately render fine-grained geometric details,which are either incorrectly baked in color or lost entirely. Geometry representation in 3D generation. Researchers have explored various 3D representationsfor generation, such as voxel grids , point clouds , meshes , neural radiance fields (NeRF) , neural surfaces , andGaussians . Among these, meshes support faster and more efficient rasterization renderingthan volumetric rendering. Meshes also integrate seamlessly with graphics engines for downstream",
  "Normal Map": ": Tactile data capture. We collect one patch by pressing GelSight Mini on an object surface. We usePoisson integration to estimate the contact depth from the sensor output, apply high-pass filtering to extract thehigh-frequency texture information, and then run the 2D texture synthesis method of Image Quilting toobtain an initial texture map. Finally, we convert the height map back to a normal map. applications. In contrast, volume representations require separate post-processing to extract meshes,often resulting in a loss of quality. Our approach uses meshes to represent coarse geometry whileembedding local geometric details into a 3D texture field of tactile normal. 3D texture generation and transfer.Simultaneously generating geometry and texture oftenresults in blurry textures due to surface smoothing or averaging across views. To address this, manyapproaches propose a subsequent stage that focuses on refining high-resolution textures ortreats texture generation as a separate problem entirely . Additionally, varioustexture transfer methods enable the transfer of texture to new shapes using images or other material assets . While most methods focus solely on visual appearance, our approachproduces high-fidelity geometric details. Closely related to our method, NeRF-Texture alsotransfers textures with geometric variations but requires 100-200 images of the same object. Inaddition, their method models the mesostructure with a signed distance and a coarse normal directionat each projected vertex. Hence, it can model sparse structures at a centimeter scale but not delicatepatterns on the objects surface. In contrast, our method leverages high-resolution tactile sensing atthe millimeter scale and provides accurate surface normal details, with richer details at fine scales. Tactile sensing in 3D. Tactile data is useful in providing contact information and are widely used in3D perception and robotics tasks. Early works start with reconstructing simple objects ,and then evolve to more complicated scenes with latest 3D neural representations , as well asrobotic in-hand reconstruction with multi-finger contacts . Vision-based tactile sensing , a particular sensing mechanism based on the photometric stereo principle, can provide high-resolution surface texture information like surface normals and thus can be useful for high-quality 3Dsynthesis and generation. Several recent works have also used it for 2D generation and3D scene reconstruction . In contrast, to our knowledge, our work is the first to use tactilesensing for 3D generation.",
  "We use GelSight to acquire high-resolution geometric details by touching the objects surface.The raw sensor data are then pre-processed to obtain high-frequency signals, as shown in": "GelSight tactile sensor. The GelSight sensor is a vision-based tactile sensor that uses photometricstereo to measure geometry at a high spatial resolution at the contact surface. It can capture the finedetails on the surface, such as bumps on avocados skin and patterns of crochet yarns. In our work,we use the GelSight mini with a sensing area of 21mm 25mm (h w) and a pixel resolution of240320, equivalent to about 85 micrometers per pixel. The sensor captures an RGB image, whichcan be mapped to the surface normals and then used to reconstruct a surface height map. Tactile data pre-processing and contact mask.We manually press the GelSight sensor againstthe objects surface to obtain a tactile image over a small area. The derived height map contains botha low-frequency component, representing the surfaces global shape, and high-frequency geometrictextures. We first apply a high-pass filter to the height map to extract texture information and then",
  "Render": ": Method overview. Given an input image or a text prompt, our method generates a mesh with high-quality visual and normal texture. We first generate a base mesh with albedo texture using a text- or image-to-3Dmethod. We use a 3D texture field with hash encoding to represent albedo and tactile normal textures and train itwith loss functions on rendered images. To capture the scale differences between visual and tactile modalities,we sample distinct camera views, PV for visual rendering and PT for tactile rendering. For texture refinement, wetrain the texture field with a visual matching loss LVM, to ensure fidelity to the input mesh, and a visual guidanceloss with normal-conditioned ControlNet, LVG, to enhance photorealism and cross-modal alignment. We furtherapply a tactile matching loss, LTM, and a tactile guidance loss, LTG, using a customized Texture Dreambooth, toachieve high-quality geometric details aligned with the distribution of tactile input V* texture exemplars.",
  "Method": "Generating 3D assets with high-resolution geometric details is challenging due to the difficulty ofacquiring low-level texture details from text prompts or a single image. Therefore, we incorporate anadditional input modality, tactile normal maps, to enhance the geometric details in 3D asset generation. shows our overall pipeline. Our method takes an input image and a tactile normal patch asinput and generates a 3D asset with a high-fidelity, color-aligned normal map. The input image can bea real image or generated by a text-to-image model such as SDXL . Below, we first describe howwe generate the base mesh in .1. We then introduce our core texture refinement algorithm in.2 and extend it to objects with multiple materials in .3.",
  "Base Mesh Generation": "Similar to prior works with stage-wise geometry sculpting and texture refinement , we firstgenerate a base mesh with albedo texture using a text- or image-to-3D method. We then unwrap aUV map of the exported mesh M and project the vertex albedo onto an albedo UV map. A 2D texture transfer baseline.To incorporate the tactile information, one could synthesize alarge 2D texture map with an exemplar of the preprocessed normal patch using 2D texture synthesisalgorithms such as image quilting and use it as a normal UV map of the object mesh. However,this would result in inconsistencies between visual and tactile textures, especially when transition invisual color indicates geometric change, as shown in .",
  "n = QTBN nT = [t, nB t, nB] nT,(3)": "where nB is the global geometric normal defined by the base mesh, t is the tangent vector, and QTBNis the Tangent-Bitangent-Normal (TBN) Matrix for every surface point. Given the calculated shadingnormal and a sampled camera pose P, we use a point light and a simple diffuse shading model toproduce the color image IC(P) following prior works . We can also obtain IA(P), IT(P), andIN(P) by projecting the albedo, tactile normal, and shading normal onto a sample view P. Learning 3D texture field.Given the scale discrepancy between vision and touch, we samplecamera views differently for two modalities. To supervise visual renderings IC and IA, we samplecamera poses PV orbiting the base mesh looking at the object center with perspective projection. Tosupervise tactile renderings IT, we sample camera poses PT close to mesh surfaces based on vertexnormals to emulate the captured data using a real sensor. Specifically, we randomly sample a vertex,place a camera along this vertexs normal direction, and set the camera to look at the sampled vertexwith orthographic projection.",
  "LTM = 1 cosIT(PT), IT(PT),(5)": "where IT is rendered using the tactile normal UV map from the image quilting results from a tactilecamera pose PT. Here, we use image quilting , a patch-based texture synthesis algorithm, tosynthesize a reference texture map IT that roughly matches the physical scale of real materials anduse it for initialization. To refine the visual texture, we use a diffusion guidance loss inspired by the multi-step denoisingprocess in SDEdit , Instruct-NeRF2NeRF , and DreamGaussian . Different from existingworks, we compute the Visual Guidance loss LVG based on a normal-conditioned ControlNet toensure the refined visual texture is consistent with the tactile normal. Given the rendered color imageIC from PV, we perturb it with random noise (t) to get a noisy image xt and apply a multi-stepdenoising process f() using the 2D diffusion prior to obtaining a refined image:",
  "I(PV) = f(xt; t, y,IN(PV)),(6)": "where y is the input text prompt, and f is a normal-conditioned ControlNet with the Stable Diffusionbackbone. We denoise the image from timestep t to a completely clean image I, which is differentfrom the typical single-step SDS loss in Dreamfusion . The starting timestep t (0, 1) graduallydecreases from 0.5 to 0.3 as training iteration increases, balancing the noise reduction to enhancedetails without disrupting the original content. This refined image is then used to optimize the texturethrough the L1 and LPIPS loss:",
  "LVG =IC(PV) I(PV)1 + LLPIPSIC(PV), I(PV).(7)": "While refining the visual textures, we jointly optimize the tactile texture with a Tactile Guidance lossLTG. Similar to LVG, we add random noise to a rendered tactile normal map IT(PT), generated froma tactile camera pose PT, and then apply a multi-step denoising process to obtain a refined normalimage I. To capture the distribution of tactile normals, we replace the standard diffusion prior witha Texture DreamBooth, a customized model created by fine-tuning the Stable Diffusion model f()for each tactile texture using a Low-Rank Adapter (LoRA) with DreamBooth . We leave theLoRAs training details in Appendix A.3. We use the output I to refine the tactile texture:",
  "L = VMLVM + TMLTM + VGLVG + TGLTG.(9)": "To initiate our texture grid, we start by only optimizing the visual matching loss LVM and tactilematching loss LTM for 150 iterations with VM = 500 and TM = 1. After that, we run optimizationfor another 50 iterations to refine the output guided by diffusion priors. We reduce TM from 1 to 0.05,change LVM from per-pixel error to mean-color error to allow more flexibility in texture refinement,and add the visual guidance loss LVG and tactile guidance loss LTG with VG = 5 and TG = 0.05.",
  "Multi-Part Textures": "Another advantage of our 3D texture field is its ability to easily define non-uniform textures in 3D,allowing different tactile textures to be assigned to distinct parts of an object. For instance, whengenerating a 3D asset of a cactus in a pot, we can apply different textures to the cactus and the pot,by incorporating two tactile inputs along with a text prompt that specifies the texture assignment, e.g.,cactus with texture A, pot with texture B. Part segmentation based on diffusion features. Our method can automatically segment objectparts based on the text prompt without manual annotation. We leverage the internal attention maps ofdiffusion models to segment the rendered views IA(PV) of the object. Specifically, we add a randomnoise of level t to IA and apply one denoising step with the SD UNet. Inspired by DiffSeg andother text-to-image methods , we perform unsupervised part segmentation by aggregatingand spatially clustering self-attention maps from multiple layers. To assign part labels, we aggregatecross-attention maps corresponding to different parts and match them with the unlabeled segmentationmaps based on KullbackLeibler (KL) divergence. This results in a list of labeled segmentationmasks Mn(IA) {0, 1}HW , n {1, . . . , N}, where n denotes one of the N object parts. Forexample, given the prompt cactus in a pot, we aggregate cross-attention maps for cactus andpot from different layers, then assign each part segmented from the clustered self-attention mapsto either cactus or pot according to respective cross-attention maps. More details are included inAppendix A.1.",
  "a corn": ": 3D generation with a single texture. For each object, we show generated albedo (top), normal(middle), and full color (bottom) renderings from two viewpoints. Our method works for both text-to-3D (cornand football) and image-to-3D (potato and strawberry), generating realistic and coherent visual textures andgeometric details. (We use roughness=0.5 when rendering color views in Blender for Figures 1, 5, 6, and 7.)",
  "cosMn IT, Mn In,(12)": "where denotes the Hadamard product, InT is the rendered reference view using the n-th partstactile data, In is the refined tactile normal generated by Texture Dreambooth trained on the n-th parttexture, and Mn is the binary mask for the n-th part obtained from S rendered from our learned 3Dlabel field. We omit PT for clarity since all patches are rendered from the same tactile camera pose.",
  "``a goat sculpture with gold goat and black base, goat with fine striped steel texture, base with smooth wood texture": ": Multi-part texture generation. Our method allows users to specify an object (via text or image)and its two parts to assign different textures (color-coded text prompts correspond to text descriptions for twotextures). We show paired results for the predicted label, albedo, normal, and full-color renderings. The zoom-inpatches demonstrate the generated normal textures on different parts.",
  "We present comprehensive experiments to verify the efficacy of our method. We perform qualitativeand quantitative comparisons with existing baselines and ablation studies on the major components": "Dataset. We have collected 18 diverse types of textures from daily objects in our TouchTexturedataset, five tactile patches per texture, and pair them with a short description. Please see Appendix A.2for the full list. We randomly sample one patch to initialize the tactile UV map with the imagequilting algorithm and use five patches to train the customized Texture DreamBooth. For basemesh generation given a text or an image input, while any mesh generation method is applicable,we use Wonder3D in the main experiments but include results using InstantMesh andRichDreamer in Appendix A.4 as well. Specifically, Wonder3D takes an image as input andoutputs a mesh with albedo stored as vertex colors. We then convert the vertex albedo into an albedoUV map as described in .1. During optimization in .2, we refine the albedo andtactile textures with the textured prompts, i.e., A [object name] with V* texture, where V* representsthe text description corresponding to the selected tactile texture map. 3D generation with a single texture. shows our 3D generation results for a single texture,showing albedo, normal, and full-color rendering from two viewpoints for each object. Our methodworks for both text input (the corn and the football) and image input (the potato and the strawberry),generating realistic, coherent, and high-resolution visual and geometry details. shows theresults of applying different textures to the same object (a coffee cup), with normal rendering ontop and color rendering below. Our joint optimization produces coherent visual-tactile textures andsmooth, natural shadows from geometric variations. Multi-part texture generation. As introduced in .3, users can specify an object (via text orimage) and assign different textures to two distinct parts. shows results for multi-part texturesynthesis. The left column shows the text or image input as well as the tactile input. The image inputcan be either real or generated from a text prompt. The color-coded text prompts correspond to textdescriptions for two textures. The right columns show the results of the albedo, normal, and full-colorrendering. Our method effectively segments parts with our 3D label field. The joint optimizationsuccessfully applies the textures to each corresponding part designated by the user and generates anoverall coherent visual appearance and tactile geometry.",
  "Ours": ": Baseline comparison. Compared to the SOTA image-to-3D (Wonder3D and DreamGaussian) andtext-to-3D (DreamCraft3D) baselines, our method produces significantly more plausible low-level geometry. Fora fair comparison, we use the same input image for the first three rows. text-to-3D and image-to-3D methods. For text-to-3D, we compare with DreamCraft3D and usethe same textured prompt, i.e., a prompt with a text description of the target tactile texture, as input.DreamCraft3D focuses on geometry sculpting using neural field and DMTet , followed by textureboosting through fine-tuning DreamBooth with augmented multi-view rendering, requiring 10xcomputational cost compared to our method. For image-to-3D, we compare with DreamGaussian and Wonder3D . DreamGaussian employs fast generation using 3D Gaussian primitives andrefines an albedo map in Stage 2 with a multi-step denoising MSE loss. Wonder3D generates pairedmulti-view RGB and normal images, using a geometry fusion process to generate a 3D result. Theinput images remain the same as ours for these two baselines. We use the official implementations forall baselines. Qualitative evaluation. shows qualitative results of our method compared against threebaselines. For each example, we show color and normal rendering with zoomed-in patches at thesame location for detailed comparison. Our method achieves higher visual fidelity, more realisticdetails in normal space, and better color-geometry alignment than the baselines. In particular, ourtextures exhibit sharper details than those of Wonder3D, which tend to be overly smooth. In contrastto DreamGaussian and DreamCraft3D, our generated geometry details are more realistic and alignwell with the color appearance. In the avocado example, DreamCraft3D suffers from the Janusproblem, generating a brown core on both sides. Quantitative evaluation. We perform a human perceptual study using Amazon Mechanical Turk(AMTurk). We set up a paired test, showing a reference prompt and two rendering results, onegenerated with our method and the other generated with one of the baselines. We conduct twoseparate surveys to evaluate the texture appearance and geometric details, respectively. For textureappearance, we render full-color RGB images and ask users Which of the following views has morerealistic textures?. For geometric details, we render shaded colorless images that only demonstratethe mesh geometry and ask users Which of the following views has more realistic geometric details?.Please see Appendix A.5 for example screenshots of the paired rendering. Each user has two practicerounds followed by 20 test rounds to evaluate our method against DreamGaussian, Wonder3D, andDreamCraft3D. All samples are randomly selected and permuted, and we collect 1,000 responses. shows the mean and standard deviation of users preference score; our method is preferredover all baselines. While DreamCraft3D has a close performance regarding texture appearance, itfails to obtain high-resolution geometric details that align well with color texture.",
  "guidance": ": Ablation study regarding texture refinement. We ablate our method regarding the tactile guidanceloss LTG and visual guidance loss LVG. Removing LTG results in fewer details of the generated tactile texture.Removing the visual guidance introduces misaligned visual and tactile normal textures. For example, the bumpsin the normal map are misaligned with the locations of white seeds in the albedo rendering, as shown in thezoomed-in patches. Our method encourages the generated color to be consistent with the tactile normal usingControlNet-guided visual refinement loss while also enhancing the details in the tactile texture.",
  "texture": ": Ablation study regarding tactile input. We remove the tactile input while keeping the refinementloss. Without the tactile information, the generated 3D assets fail to capture fine-grained geometric details. both front and back views of a sampled object for each experiment. To illustrate details and cross-modal alignment, we present a global full-color view on the left, a global normal view on the right,and patch views of the full-color, albedo, and normal renderings in between. shows resultsof ablating diffusion-based guidance losses for texture refinement, specifically the visual guidanceLVG and tactile guidance LTG. Omitting LTG reduces tactile texture details, while omitting LVG introduces misalignment betweenvisual and tactile normal textures; for instance, bumps in the normal map do not match the white seedsin the albedo rendering. Our method ensures color consistency with tactile normals via ControlNet-guided visual refinement and enhances tactile texture details. We also study the efficacy of tactile data processing by comparing our method with the texturemap synthesized using the original tactile data without preprocessing stated in . shows that using the original data produces much more flattened textures since the low-frequencydeformation of the gel pad due to uneven contact during the data collection would dominate thetactile signal and thus degrade the details of synthesized texture maps. shows the results of removing tactile input and only optimizing the albedo map with textured prompts using LVMand LVG. Without tactile input, the output mesh is overly smooth, demonstrating the insufficiency ofusing text prompts only for fine geometry texture generation.",
  "Discussion": "Recent methods in 3D generation often struggle with unrealistic geometric details. To address this,we have introduced a novel approach that incorporates tactile information. Our method synthesizesboth visual and tactile textures using a 3D texture field. Additionally, we have introduced a multi-parttexturing pipeline for controllable region-wise texture generation. To our knowledge, this is thefirst use of high-resolution tactile sensing to improve 3D generation. Our method produces realistic,fine-grained geometric textures while maintaining accurate visual-tactile alignment. Limitations.The quality of the generated coarse shape depends on the existing 3D generativemodels, which struggle with complex geometry. Additionally, slight seams may appear in our resultsdue to UV unwrapping.",
  "and Disclosure of Funding": "We thank Sheng-Yu Wang, Nupur Kumari, Gaurav Parmar, Hung-Jui Huang, and Maxwell Jonesfor their helpful comments and discussion. We are also grateful to Arpit Agrawal and Sean Liu forproofreading the draft. The project is partially supported by the Amazon Faculty Research Award,Cisco Research, and the Packard Fellowship. Kangle Deng is supported by the Microsoft ResearchPhD Fellowship. Ruihan Gao is supported by the A*STAR National Science Scholarship (Ph.D.). Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In IEEE International Conference on Computer Vision (ICCV),2023.",
  "Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.In International Conference on Learning Representations (ICLR), 2023": "Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial networks. In Conference on Neural InformationProcessing Systems (NeurIPS), 2014. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathain, and Surya Ganguli. Deep unsupervisedlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning(ICML), 2015. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and RenNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In European Conference onComputer Vision (ECCV), 2020.",
  "Bernhard Kerbl, Georgios Kopanas, Thomas Leimkhler, and George Drettakis. 3d gaussian splatting forreal-time radiance field rendering. ACM Transactions on Graphics (TOG), 42(4):114, 2023": "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, AarushKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. InIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.",
  "Wenzhen Yuan, Siyuan Dong, and Edward H Adelson. Gelsight: High-resolution robot tactile sensors forestimating geometry and force. Sensors, 2017": "Shaoxiong Wang, Yu She, Branden Romero, and Edward Adelson. Gelsight wedge: Measuring high-resolution 3d contact geometry with a compact robot finger. In IEEE International Conference on Roboticsand Automation (ICRA), 2021. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. InInternational Conference on Learning Representations (ICLR), 2024. Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-HaiZhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domaindiffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2022.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022": "Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and TaesungPark. Scaling up gans for text-to-image synthesis. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-richtext-to-image generation. In International Conference on Machine Learning (ICML), 2022. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobianchaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE International Conference onComputer Vision (ICCV), 2023. Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearancefor high-quality text-to-3d content creation. In IEEE International Conference on Computer Vision (ICCV),2023. Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In IEEE Conference on Computer Vision and PatternRecognition (CVPR), 2023. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer:High-fidelity and diverse text-to-3d generation with variational score distillation. In Conference on NeuralInformation Processing Systems (NeurIPS), 2024. Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen Liu, Zhenda Xie, and Yebin Liu. Dreamcraft3d:Hierarchical 3d generation with bootstrapped diffusion prior. In International Conference on LearningRepresentations (ICLR), 2024.",
  "Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d recon-struction. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023": "Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, ChongZeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-viewgeneration and 3d diffusion. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2024. Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, ChongZeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base model. arXivpreprint arXiv:2310.15110, 2023. Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Sync-dreamer: Generating multiview-consistent images from a single-view image. In International Conferenceon Learning Representations (ICLR), 2024. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, TrungBui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. In International Conferenceon Learning Representations (ICLR), 2024. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli,Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large recon-struction model. In International Conference on Learning Representations (ICLR), 2024. Yinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli, Gor-don Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using 3d largereconstruction model. In International Conference on Learning Representations (ICLR), 2024. Dmitry Tochilkin, David Pankratz, Zexiang Liu, Zixuan Huang, , Adam Letts, Yangguang Li, Ding Liang,Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object reconstruction from a singleimage. arXiv preprint arXiv:2403.02151, 2024. Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio Torralba, Josh Tenenbaum, and BillFreeman. Visual object networks: Image generation with disentangled 3d representations. In Conferenceon Neural Information Processing Systems (NeurIPS), 2018.",
  "Pedro O O Pinheiro, Joshua Rackers, Joseph Kleinhenz, Michael Maser, Omar Mahmood, Andrew Watkins,Stephen Ra, Vishnu Sresht, and Saeed Saremi. 3d molecule generation by denoising voxel grids. 2024": "Yongbin Sun, Yue Wang, Ziwei Liu, Joshua Siegel, and Sanjay Sarma. Pointgrow: Autoregressively learnedpoint cloud generation with self-attention. In IEEE/CVF Winter Conference on Applications of ComputerVision (WACV), 2020. Zijie Wu, Yaonan Wang, Mingtao Feng, He Xie, and Ajmal Mian. Sketch and text guided diffusion modelfor colored point cloud generation. In IEEE International Conference on Computer Vision (ICCV), 2023.",
  "Chao Wen, Yinda Zhang, Zhuwen Li, and Yanwei Fu. Pixel2mesh++: Multi-view 3d mesh generation viadeformation. In IEEE International Conference on Computer Vision (ICCV), 2019": "Can Wang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Clip-nerf: Text-and-image drivenmanipulation of neural radiance fields. In IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2022. Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, KfirAberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d generation.In IEEE International Conference on Computer Vision (ICCV), 2023. Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander G Schwing, and Liang-Yan Gui. Sdfusion:Multimodal 3d shape completion, reconstruction, and generation. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2023. Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussiansplatting for efficient 3d content creation. In International Conference on Learning Representations (ICLR),2024. Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Largemulti-view gaussian model for high-resolution 3d content creation. In European Conference on ComputerVision (ECCV), 2024.",
  "An-Chieh Cheng, Xueting Li, Sifei Liu, and Xiaolong Wang. Tuvf: Learning generalizable texture uvradiance fields. In International Conference on Learning Representations (ICLR), 2024": "Fanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and Hao Su. Neutex:Neural texture mapping for volumetric neural rendering. In IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2021. Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Niener. Text2tex:Text-driven texture synthesis via diffusion models. In IEEE International Conference on Computer Vision(ICCV), 2023.",
  "Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Lai, Ying Shan, and Lin Gao. Nerf-texture: Texture synthesis withneural radiance fields. In ACM SIGGRAPH, 2023": "Sudharshan Suresh, Zilin Si, Joshua G Mangelson, Wenzhen Yuan, and Michael Kaess. Shapemap 3-d:Efficient shape mapping through dense touch and vision. In IEEE International Conference on Roboticsand Automation (ICRA), 2022. Mohamed Tahoun, Omar Tahri, Juan Antonio Corrales Ramn, and Youcef Mezouar. Visual-tactile fusionfor 3d objects reconstruction from a single depth view and a single gripper touch for robotics tasks. InIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021. Edward Smith, David Meger, Luis Pineda, Roberto Calandra, Jitendra Malik, Adriana Romero Soriano,and Michal Drozdzal. Active 3d shape reconstruction from vision and touch. In Conference on NeuralInformation Processing Systems (NeurIPS), 2021. Shaoxiong Wang, Jiajun Wu, Xingyuan Sun, Wenzhen Yuan, William T Freeman, Joshua B Tenenbaum,and Edward H Adelson. 3d shape perception from monocular vision, touch, and shape priors. In IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), 2018. Aiden Swann, Matthew Strong, Won Kyung Do, Gadiel Sznaier Camps, Mac Schwager, and MonroeKennedy III. Touch-gs: Visual-tactile supervised 3d gaussian splatting. In IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), 2024. Mauro Comi, Yijiong Lin, Alex Church, Alessio Tonioni, Laurence Aitchison, and Nathan F Lepora.Touchsdf: A deepsdf approach for 3d shape reconstruction using vision-based tactile sensing. In Conferenceon Neural Information Processing Systems (NeurIPS) Workshop, 2023. Edward Smith, Roberto Calandra, Adriana Romero, Georgia Gkioxari, David Meger, Jitendra Malik, andMichal Drozdzal. 3d shape reconstruction from vision and touch. In Conference on Neural InformationProcessing Systems (NeurIPS), 2020. Sudharshan Suresh, Haozhi Qi, Tingfan Wu, Taosha Fan, Luis Pineda, Mike Lambeta, Jitendra Malik,Mrinal Kalakrishnan, Roberto Calandra, Michael Kaess, et al. Neuralfeels with neural fields: Visuotactileperception for in-hand manipulation. Science Robotics, 9(96):eadl0628, 2024. Elliott Donlon, Siyuan Dong, Melody Liu, Jianhua Li, Edward Adelson, and Alberto Rodriguez. Gelslim:A high-resolution, compact, robust, and calibrated tactile-sensing finger. In IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), 2018.",
  "Thomas Mller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitiveswith a multiresolution hash encoding. ACM Transactions on Graphics (TOG), 41(4):115, 2022": "Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modularprimitives for high-performance differentiable rendering. ACM Transactions on Graphics (TOG), 39(6),2020. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:Guided image synthesis and editing with stochastic differential equations. In International Conference onLearning Representations (ICLR), 2022. Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In IEEE International Conference on Computer Vision(ICCV), 2023.",
  "Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonableeffectiveness of deep features as a perceptual metric. In CVPR, 2018": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. In International Conference onLearning Representations (ICLR), 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In IEEE Conference onComputer Vision and Pattern Recognition (CVPR), 2023. Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse, attend, andsegment: Unsupervised zero-shot segmentation using stable diffusion. In IEEE Conference on ComputerVision and Pattern Recognition (CVPR), 2024.",
  "Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image generation withrich text. In IEEE International Conference on Computer Vision (ICCV), 2023": "Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In IEEE International Conference on ComputerVision (ICCV), 2023. Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient3d mesh generation from a single image with sparse-view large reconstruction models. arXiv preprintarXiv:2404.07191, 2024. Lingteng Qiu, Guanying Chen, Xiaodong Gu, Qi Zuo, Mutian Xu, Yushuang Wu, Weihao Yuan, ZilongDong, Liefeng Bo, and Xiaoguang Han. Richdreamer: A generalizable normal-depth diffusion model fordetail richness in text-to-3d. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),2024. Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:a hybrid representation for high-resolution 3d shape synthesis. In Conference on Neural InformationProcessing Systems (NeurIPS), 2021. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International Conference on Machine Learning (ICML), 2021.",
  "A.1Diffusion-based Multi-Part Segmentation": "In .3, we have mentioned using attention maps of the diffusion process to segment theinput image based on text prompts. We provide more details here. Assume an input image x andits corresponding prompt y = [y1, y2, , yT ] are given, where T is the number of tokens from thetext encoder (T = 77 for CLIP text encoder ). Among these, yp1, , ypN define N parts in theimage x, where p1, , pN indicate the indexes of the token. For example, with the prompt a cactusin a pot, the second token y2 (p1 = 2) cactus and the fifth token y5 (p2 = 5) pot correspond totwo parts (N = 2) of the object in the image, and our method outputs the segmentation mask for eachpart. Specifically, we first perturb x using random noise t with a noise level of t, which is empiricallyset as 0.2. We then use the Stable Diffusion UNet to denoise xt for one step. During the denoisingprocess, we collect the cross-attention and self-attention probability maps for each of the 16 layers. Segment and create masks using self-attention layers. Following DiffSeg , we aggregate the16 self-attention probability maps to a single map Af with the shape of 64 64 64 64, and runiterative attention merging on upsampled Af to obtain a preliminary cluster probability mapAf R512512K, where each Af[:, :, k], k = 1, , K, is a probability map,",
  "i, j,S[i, j] = arg maxkAf[i, j, k].(14)": "Aggregate labels with cross-attention layers. Similarly, we aggregate the 16 cross-attention mapsand up-sample them to Ac with the shape of 512 512 77. To find the exact pixels correspondingto each part specified in the prompt input, we extract cross-attention maps associated with the Ntokens yp1, , ypN , Ac R512512N,",
  "+": ": Illustration of diffusion-based multi-parts segmentation. At each iteration, we run a forward passof the diffusion model for the target albedo image, aggregate its self-attention and cross-attention maps, andcompute KL distance to merge the masks into N parts. We show an example of a cactus in a pot, wherewe extract the masks corresponding to two tokens cactus and pot, shown as Mask A and Mask B. Thesegmentation masks are then used to supervise the label field training to enable multi-part synthesis.",
  "Object": ": Additional materials in tactile normal dataset TouchTexture. We collect tactile normal data from18 daily objects featuring diverse tactile textures. We show the tactile normal map and a 3D height map for eachobject. This library of tactile samples covers a wide range of diverse materials commonly found in daily life.",
  "A.3Training Details": "Regarding the network, we use TCNN encoding with two base linear layers, which then branches outinto three output layers, one linear layer followed by sigmoid activation for albedo output, one linearlayer followed by tanh activation for tactile normal output, and one optional linear layer for labelfield. We follow DreamBooth to train texture LoRAs with Stable Diffusion (SD) V1.4 for thetactile guidance loss. We find empirically that SD V1.4 works better for generating tactile normalmaps. We use ControlNet (v1.1 - normalbae version) with SD V1.5 for the diffusion loss. We followWonder3D to generate the base mesh and train the texture field network using Adam optimizer withlr = 0.01. We train all models on A6000 GPUs and each experiment takes about 10 mins and 20GRAM to run.",
  "A.4Flexible Base Mesh Generation": "Given a text or image input, our method seamlessly integrates with a wide range of mesh gener-ation approaches to enhance the 3D output with refined details. Here, we presents results usingRichDreamer for base mesh generation in a and results using InstantMesh inb. RichDreamer generates detailed and consistent 3D meshes from text prompts by incor-porating depth and normal priors during pre-training, while InstantMesh employs a feed-forwardimage-to-3D approach with a transformer-based reconstruction model and FlexiCubes for efficientmesh production. Both methods provide solid base meshes, and our approach effectively adapts toeach, refining the output meshes with intricate texture details for greater realism and fidelity.",
  "A.5Human Perceptual Study Details": "We perform a human perceptual study on Amazon Mechanical Turk (AMTurk). We set up a pairedtest, showing a reference prompt and two rendering results, one generated with our method and theother generated with one of the baselines. For each result, we include a single view of the entireobject with a zoom-in patch in the red box to show the details. We use directional light shooting infrom the left side for all mesh renderings. We give a text prompt, e.g., an avocado, and ask the userto select the result that better matches the prompt. We render shaded colorless images for geometryevaluation as shown in a and render full-color images for texture evaluation as shown inb. The estimated time for each test is about 5 minutes, and we pay users a correspondingamount of compensation higher than the minimum wage in the country of the data collector. Theonline survey does not pose any explicit potential risks expected to be incurred by participants.",
  "BSoceital Impacts": "Our method of incorporating tactile information into 3D generation enhances the geometric details ofthe generated results. This advancement helps automatically create high-fidelity assets, which arehighly applicable in game and film production. However, there is also a potential risk of misuse, as3D assets could be used to generate fake content for misinformation. Despite this concern, we believehumans can currently distinguish our synthesized objects from real ones.",
  "(b) A sample test page for texture evaluation": ": AMTurk setup for evaluation. For each paired result, we include a single view of the entire objectwith a zoom-in patch in the red box to show the details. For each result, we include a single view of the entireobject with a zoom-in patch in the red box to show the details. We use directional light shooting in from the leftside for all mesh renderings. We give a text prompt, e.g., an avocado, and ask the user to select the result thatbetter matches the prompt."
}