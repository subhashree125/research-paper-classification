{
  "Abstract": "Text-to-SQL enables users to interact with databases through natural language,simplifying access to structured data. Although highly capable large languagemodels (LLMs) achieve strong accuracy for complex queries, they incur unnec-essary latency and dollar cost for simpler ones. In this paper, we introduce thefirst LLM routing approach for Text-to-SQL, which dynamically selects the mostcost-effective LLM capable of generating accurate SQL for each query.We present two routing strategies (score- and classification-based) that achieveaccuracy comparable to the most capable LLM while reducing costs. We designthe routers for ease of training and efficient inference. In our experiments, wehighlight a practical and explainable accuracy-cost trade-off on the BIRD dataset.",
  "Introduction": "In recent years, Text-to-SQL has gained significant momentum with deployments within enterprisesolutions to transform data accessibility . Text-to-SQL democratizes access to structured data,allowing non-experts to interact with databases directly without requiring data engineering expertise.For SQL analysts, it enhances their workflows by supporting query authoring, dataset exploration,and report generation. A major use case lies in iterative data exploration, where the complexity ofuser queries can range widelyfrom simple row retrievals to multi-way joins with aggregations. Current state-of-the-art Text-to-SQL approaches follow a multi-stage pipeline . Thepipeline consists of two main phases: (i) retrieval of contextual informationsuch as schema elements,examples, and instructions relevant to the queryand (ii) SQL generation. Current enterprise solutionsuse highly capable LLMs for SQL generation to handle highly complex queries . While thisis essential for complex queries, such highly capable LLMs introduce considerable latency and incurhigher costs for simpler ones, such as inspecting a few rows from a table. This in turn, can negativelyimpact both user experience and the average cost per query. Leading Text-to-SQL benchmarks such as BIRD rank submissions only based on accuracy whilealso indicating the model used on the leaderboard. Benchmarks typically assume a single-modelapproach, which mirrors the same inefficiencies found in enterprise deploymentnamely, using themost capable models for all queries regardless of complexity. For instance, six of the top ten solutionson BIRD use GPT-4o or Gemini. To handle efficiently the varying levels of complexity, we investigate the implementation of LLMrouters for Text-to-SQL. They route a query to the weakest, yet cheaper and faster model, capableof generating accurate SQL. We propose two routing approaches that achieve an accuracy close tothat of the most capable LLM, always outperforming the second-best, and reducing costs by up to1.4. This cost reduction is substantial for enterprise deployments of analytics or SQL assistantswith a high volume of queries. These routers are designed to be easy to train and efficient at inference. depicts our pipelines with an SQL generation router.",
  "Problem Formulation": "Given a set of N models { M0, M1, . . . , MN1 } and a natural language query Q, our objectiveis to identify the weakest model Mi that can accurately generate corresponding SQL for Q. Here,we define the weakest model as the one with the lowest SQL generation capability. In practice, thismodel typically exhibits the lowest latency and incurs the least dollar cost. We assume a total orderingof model strength, such that Mi is considered weaker than Mj if i < j. Therefore, we formulate theproblem as an optimization task: select the smallest l such that Ml produces accurate SQL for Q. Furthermore, we assume access to a dataset H, consisting of prior natural language (NL) queries,predicted SQL outputs, and corresponding ground-truth SQL for each of the N models. In practice,these examples can be collected from execution logs, user feedback, and manual labeling. Accordingly, our goal is to learn an N-ary routing function that maps a query Q to the minimal labell [0, N]. Here, a label l [0, N 1] indicates that Ml is the weakest model capable of generatingaccurate SQL for Q. A prediction of N denotes that no model within the set is capable of doing so.",
  "Related Work": "LLM Routing has been studied as a binary routing problem for tasks such as questionanswering, summarization, and information extraction. To our knowledge, this is the first study ofLLM N-ary routing and the first for the Text-to-SQL task. Other prior work uses an ensemble of models for generation, as proposed by Jiang et al., . Theapproaches use multiple models to generate a set of answers instead of generating a single one andthen use pairwise comparison and fusion of top-candidate to generate a final answer. While thisapproach is promising to maximize accuracy, it goes against the objective of cost minimization. Acascade approach has also been previously employed . However, it cannot be used for Text-to-SQL.Starting with the weakest model, it is infeasible to determine whether the generated SQL is accurateor meets a quality threshold before falling back to the next available stronger model.",
  "Metrics": "To evaluate the effectiveness of generated SQL, we use execution accuracy (EX) as the primarymetric . Specifically, EX(S) denotes the proportion of queries in the evaluation set Sfor which the output relation (from the predicted SQL) matches the ground-truth relation, where0 EX(S) 1. Here, matching relations are defined as those containing identical tuples, indepen-dent of attribute ordering. Our objective is to maximize EX(S) while concurrently choosing theweakest model. In this work, we consider dollar cost as the cost we aim to minimize.",
  "Methods": "We consider two approaches. The first is a regression approach where we assign a score per modelpredicting the capability of generating accurate SQL for Q. We say a model is capable of generatingaccurate SQL if its score is above an input threshold. For all models above that threshold, we pick the",
  "weakest. The second is a classification approach where a router model predicts l [0, N] as definedin the problem formulation": "If all scores of the regression approach are under the threshold or the classification approach predictsN, then we are predicting that no model is capable of generating accurate SQL. As such, dependenton the use case, we can decide to not attempt to generate SQL or we can route to one of the N models.",
  "Score-based Routing": "A regression-based router predicts for each model Mi, a score P(EX = 1Mi|Q), i.e., the probabilityof generating accurate SQL for an input query Q. The router has the input parameter : the scorethreshold such that if P(EX = 1i|Q) < then Mi is said to be incapable of generating the query.As such, this router minimizes l such that P(EX = 1Ml|Q) . If the scores indicate that no model is capable of generating accurate SQL for Q, we can chooseto not make an attempt or to route to one of the models for a different accuracy-cost trade-off. Tomaximize accuracy in our implementation, we choose to route to the strongest model. We implement the scoring function using H for each of the N models. Given an input query Q,we first find Qk: the top-K similar queries in H. For each model M, we set P(EX = 1Mi|Q) =EX(Qk)Mi. We then filter the models (EX(Qk)Mi ) and pick the weakest. We denote thisrouter based on its two parameters as RK .",
  "Classification-based Routing": "We use a distilled BERT-style encoder model (DistilBERT ), which we train on H to learn thefunction RBERT . Given an input query Q, we input the NL query and relevant retrieved schema topredict the label l indicating the weakest model Ml predicting EX(Q) = 1Mi. If RBERT predictsN then it is predicting that none of the models can generate accurate SQL. Similarly to the otherrouter, to maximize accuracy in our implementation, we choose to route to the strongest model.",
  "Models": "Specific models and strength. We used three different LLMs for SQL generation: i) GPT-4o; ii)GPT-4o-mini; and iii) Llama: Llama-3.1-8B-instruct-q4_0. To order their SQL generationcapability, we used a simplified Text-to-SQL pipeline consisting of a single attempt at generationwhile adding the whole schema to the LLM context. We evaluate the pipeline by selecting 10% ofthe queries of each database in BIRD uniformly at random. We find Llama as the weakest modelwith EX 0.34, then gpt-4o-mini with EX 0.48, and gpt-4o as the strongest with EX 0.55. Cost. In our experiments, we only analyzed dollar cost and forgo latency due to setup challenges.We use Llama as a local model and hence associate no dollar cost with it and use OpenAI servicesto access gpt-4o and gpt-4o-mini. Instead of using current token prices of OpenAI, we use anormalized unit cost where we set gpt-4o-mini input and output token cost to 1 and gpt-4oto16.6 (multiplicative price difference).1 We also used OpenAIs text-embedding-3-small modelwith cosine similarity to find the top K similar queries in H for the score-based router. The embeddingcost is negligible per input NL query when compared with token usage and is therefore ignored. Score-based Router Implementation. For a score-based router (Rk ), we have to select the twoparameters and K. (threshold score) is the minimum proportion out of the K similar queriesfor which a model had to generate accurate SQL to be considerate a candidate model for generation.",
  "RBERT55.21118 (8%)311 (20%)167 (5%)938 (61%)1.4x": "We run a grid search over 10% of train queries as done before and for each input query, we removethe queries in the same database from being chosen as similar ones. We pick > 0.5 as it meansintuitively some level of robustness where more than half of the K similar queries were generatedsuccessfully. For each {0.6, 0.7, 0.8, 0.9}, we do a search over k . We find that setting to 0.9 is overly restrictive indicating N for every input query, i.e., no model is capable of generateaccurate SQL. As such, we only consider {0.6, 0.7, 0.8}. We choose the K that maximizes EXwhile having routed to each model at least once. We end up with three models with a difference lessthan 0.5% EX between them: R0.624 , R0.725 , and R0.810 . Classification-based Router Implementation. We fine-tuned DistilBERT on the fully labeledtrain set H. We removed from H, all queries in the retail_world database due to schema missingand all duplicate user questions. We split H 80-20 on db_id first (ensuring databases in train arenot in validation and vis-versa). This led to a train set of 7346 queries and a validation set of 1697.When training on H and evaluating on the dev set, to retrieved the relevant schema using the TCSLschema linking approach . We trained for 4 epochs, with a batch size of 32, and a learning rateof 1.00E04. We sampled each batch using a weighted random sampler, the weights correspond tothe inverse of the frequency of each label within to the training data.",
  "Results": "We evaluate our routers R0.624 , R0.725 , R0.810 , and RBERT on all 1534 dev queries. Recall that all models,route to the strongest model (gpt-4o) in case N is predicted. By analyzing the successful and failedquery sets on the three base models (Details in Appendix A), we find that each weaker model has alarge subset of failed and successful queries with a stronger one. As such, we expect routing to lowerthe cost while at best keeping the best models EX. summarizes the EX, model routing distribution and relative cost to the strongest model(gpt-4o) for each generation approach on BIRDs dev set. Our routers are up to 1.4x cheaper whilebeing close in EX. With both routers, we lose some accuracy for a lower cost. The accuracy-costtrade-off on BIRD is easiest to explain through the parameters of the score-based router (Appendix B). In this ongoing work, we aim next to assess whether an NL query with relevant schema is enoughto classify its complexity as done in RBERT . Furthermore, we plan to explore routing across moredatasets and within more stages such as correction and schema linking instead of just generation.",
  "Conclusion": "In this work, we investigated two LLM routing approaches for text-to-SQL on the BIRD benchmark.We believe that cost-based optimization techniques are important for enterprise-level systems wherenot only accuracy but also cost are critical. In the future, we intend to explore a larger scope ofempirical analysis with similar techniques to improve both the execution accuracy and cost.",
  "B. Li, Y. Luo, C. Chai, G. Li, and N. Tang. The dawn of natural language to sql: Are we fullyready? CoRR, abs/2406.01265, 2024": "J. Li, B. Hui, G. Qu, J. Yang, B. Li, B. Li, B. Wang, B. Qin, R. Cao, R. Geng, N. Huo, X. Zhou,C. Ma, G. Li, K. C. C. Chang, F. Huang, R. Cheng, and Y. Li. Can llm already serve as a databaseinterface? a big bench for large-scale database grounded text-to-sqls. CoRR, abs/2305.03111,2023.",
  "V. Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of BERT: smaller,faster, cheaper and lighter. CoRR, abs/1910.01108, 2019": "T. Yu, R. Zhang, K. Yang, M. Yasunaga, D. Wang, Z. Li, J. Ma, I. Li, Q. Yao, S. Roman,Z. Zhang, and D. R. Radev. Spider: A large-scale human-labeled dataset for complex andcross-domain semantic parsing and text-to-sql task. CoRR, abs/1809.08887, 2018. W. Zhang, Y. Wang, Y. Song, V. J. Wei, Y. Tian, Y. Qi, J. H. Chan, R. C.-W. Wong, andH. Yang. Natural language interfaces for tabular data querying and visualization: A survey.CoRR, abs/2310.17894, 2024.",
  "AAnalysis of Failure Cases": "We analyze the performance of each model in terms of correct and failed query sets cases, high-lighting their differences in handling SQL generation. Figures 2a and 2b illustrate the distri-bution of failed and correct predictions across the three models: gpt-4o, gpt-4o-mini, andllama3.1:8b-instruct-q4_0. (1.46%) (4.64%) (2.99%) (18.19%) (3.47%)811 (8.6%) (37.56%) gpt-4o gpt-4o-mini llama3.1:8b-instruct-q4_0",
  "Intersection gpt-4o-mini & llama3.1:8b-instruct-q4_0435246.17%": "The difference in EX shows a clear gap in capability between models. We find that 37.56% of thequeries failed across all three models with a high common failed percentage of queries between everypair from 45.49%67.83%. This indicates that when routing, it is unlikely to improve EX beyondthat of the strongest model and that we expect a cost decrease only.",
  "BEffect of Varying K and on Execution Accuracy and Model Distribution": "We analyze the impact of varying parameters K (number of similar queries) and (threshold score)in the score-based router RK on execution accuracy (EX) and model distribution. This examinationhighlights the cost-accuracy trade-off. k value 0.0 0.1 0.2 0.3 0.4 0.5 0.6 EX 48.02% 50.55% 52.20%53.08%53.63%53.85%53.52%53.30%53.85% 44.95% 47.14%48.24%48.57%49.12%49.67%50.00%49.45%49.67% 39.78% 41.65%42.09%42.20%41.54%40.99%40.88%40.77%40.88% 30.11% 27.91%26.48% 24.18%22.97%22.09%21.98%20.66%19.89% =0.6"
}