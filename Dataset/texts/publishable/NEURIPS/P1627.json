{
  "Abstract": "Achieving robust 3D perception in the face of corrupted data presents an challeng-ing hurdle within 3D vision research. Contemporary transformer-based point cloudrecognition models, albeit advanced, tend to overfit to specific patterns, conse-quently undermining their robustness against corruption. In this work, we introducethe Target-Guided Adversarial Point Cloud Transformer, termed APCT, a novelarchitecture designed to augment global structure capture through an adversarialfeature erasing mechanism predicated on patterns discerned at each step duringtraining. Specifically, APCT integrates an Adversarial Significance Identifier anda Target-guided Promptor. The Adversarial Significance Identifier, is tasked withdiscerning token significance by integrating global contextual analysis, utilizinga structural salience index algorithm alongside an auxiliary supervisory mecha-nism. The Target-guided Promptor, is responsible for accentuating the propensityfor token discard within the self-attention mechanism, utilizing the value derivedabove, consequently directing the model attention towards alternative segments insubsequent stages. By iteratively applying this strategy in multiple steps duringtraining, the network progressively identifies and integrates an expanded array ofobject-associated patterns. Extensive experiments demonstrate that our methodachieves state-of-the-art results on multiple corruption benchmarks. 1Introduction3D point cloud recognition has garnered significant interest owing to its promising implications forrobotics and autonomous driving. Prevailing techniques have been predominantlydesigned and evaluated on clean data , overlooking the extensive corruptions present in real-world scenarios arising from sensor inaccuracies and physical constraints and leading to suboptimalperformance when models are exposed to such conditions. Therefore, enhancing the resilience ofpoint cloud models against real-world corruption emerges as a paramount yet daunting task. Due to their elevated performance, existing transformer-based models have emerged as themainstream choice. Despite their effectiveness on clean datasets, these methods generally falterwhen faced with corrupted data. Upon closer examination, we discerned that prevailing models havea propensity to overfit to specific patterns (a). Such patterns can degrade in the presence ofreal-world corruptions, undermining the model reliability. Relying solely on these localized patternsfor predictions can be fragile, especially with corrupted data. Thus, we theorize that if the model isencouraged to extract features from a broader region of the object during training, gathering morediverse perception cues, it would be more resilient, as proved in b. This is because, even ifsome local patterns are compromised in corrupted data, the model could still source information fromother intact areas to make accurate predictions.",
  "multi patterns": ": Overall motivation. We advocate for the model to broaden its attention to diverse patterns,mitigating the tendency to overfit to localized patterns. The left segment of the figure contrasts theconfusion matrices of the standard transformer with our approach. The right portion showcases theperformances of both the standard transformer and our methodology when confronted with objectsexhibiting similar local patterns. Tokens with high / low contributions to classification are in red /blue, respectively. Standard transformer tends to overfit to localized patterns. While our method, bymodulating tokens with significant contributions, enables the model to garner features from a variedspectrum of target segments, thereby ensuring greater robustness.This method adversarially weakens the dominant patterns and gathers more sub-important perceptioncues during training on clean samples. By progressive pattern excavation, the proposed approachprompts the model to delve into objects and acquire a broader range of patterns, consequentlyelevating the robustness of the model. Specifically, we first partition and extract local geometries from the point cloud by a mini-PointNet , resulting in a subset of tokens that encode these features. The tokens are thenfed into stacked transformer blocks with two core modules, namely the Adversarial SignificanceIdentifier and the Target-guided Promptor. The former module utilizes a dominant feature index mech-anism along with an auxiliary supervision, to discern significance of all tokens, thus integrating globalcontextual analysis. Simultaneously, it signs a proportion of dominant tokens that are significant forperception in the current phase. Subsequently, the Target-guided Promptor, increases the droppinglikelihood of token signed above during the self-attention process, thereby compelling the network tofocus on less dominant tokens and to extract perceptive clues from alternative patterns. By iterativelyengaging in such an adversarial process, the network gradually excavates and assimilates an extendedarray of patterns from objects (in a), thus making precise predictions against corruption. We have extensively validated the effectiveness of our proposed method on multiple benchmarks,including ModelNet-C and the more challenging ScanObjectNN-C . The results significantlyunderscore improvements in robustness and establish state-of-the-art performance on these datasets.In addition, our algorithm demonstrates pronounced generalization capabilities in downstream tasks,as evidenced by its adeptness in shape segmentation under corruptions in ShapeNet-C . Thesefindings collectively underscore the methods generality and effectiveness in enhancing the robustnessof point cloud perception models. Our contributions can be summarized as follows:",
  "(a) (b)": ": (a) Process of progressive adversarial dropping. The first line means token learned ineach stage. (b) Visualization of token weights learned by the classifier. Compared with standardtransformer, ours has an advantage in mining sample patterns. for point cloud semantics, thus pioneering this trajectory. The Point Cloud Transformer intrinsicallyintegrates global attention across the entirety of the point cloud, mirroring certain challenges inherentto the Vision Transformer (ViT), notably the constraints of memory bandwidth and computationaloverhead. Contrarily, the Point Transformer accentuates local attention, distinctly focusing on eachpoint and its proximate neighbors, effectively mitigating the aforementioned memory impediments.Subsequent to these, the Robust Point Cloud Classifier (RPC) emerged, proffering a robustframework for point cloud classification. Infusing 3D representation, k-NN, frequency grouping, andself-attention, it posits itself as a formidable baseline in the annals of point cloud robustness inquiries.While these methods demonstrate remarkable accuracy , their performance tends to deteriorate inface of real-world corruptions. In this manuscript, we introduce a novel transformer-based modelspecifically architected to robustly handle real-world corruptions.",
  "Robust Learning Against Corruptions": "In the realm of point cloud perception, deep learning has made significant strides .However, ensuring model robustness in the presence of corruptions remains a pivotal concern. Con-temporary literature underscores the imperative of addressing point cloud corruptions .Some methodologies have hinged on preprocessing, incorporating denoising or completion strate-gies , to bolster a models resilience against adversarial perturbations. Others have pursuedaugmentation pathways, spanning mix-based and deformation-centric techniques. Theadvent of advanced auto-augmentations has ushered in sample-adaptive data augmentations,targeting enhanced model robustness. Notwithstanding their merits, such interventions often mandateaugmented computational overhead and may falter in scenarios of pronounced corruption. An-other line of research aims to improve the robustness of deep learning architectures themselves.PointASNL integrates adaptive sampling coupled with local-nonlocal modules, striving forheightened robustness. Concurrently, Ren et al. unveiled an attention-based backbone, RPC,tailored to withstand corruption. Nevertheless, these methods only make improvements to the featureextractor, without concerning the nature of the corruption problem. This manuscript introduces toleverage an adversarial dropping strategy to discern pertinent features from corrupted samples.",
  "Methodology": "We instead devise an adversarial analysis based framework (), which not only learns pointrecognition with current vital patterns, but more essentially, it automatically discovers and emphasizesthe sub-important patterns, with the auxiliary supervised pattern miner. Patterns learned in suchstrategy are expected to be more discriminative and robust, hence facilitating final recognition ofpoint clouds under corruptions. At each training iteration, our algorithm comprises of two phases.In phase 1, we perform adversarial mining over across all patterns, based on the token significanceidentification process. The propose is to search for the sub-optimal patterns that contribute lessto the final recognition. In phase 2, we leverage deterministic dropping assignments and attributeelevated dropping rates to tokens pivotal for the final perception, as an constraint to enable thesesub-optimal tokens to play a increasing significant role in perceptual. Engaging in this iterativeadversarial procedure allows the network to progressively unearth and assimilate a comprehensive setof object patterns, as seen in b, culminating in refined predictions in the face of corruption. 3.1Overview ImplementationsThe overall implementations of Adversarial Point Cloud Transformer is shown in . Givenan input set of point cloud P RN3, we first partition the input point cloud into an assemblyof n discrete patches, and subsequently, generating pertinent tokens with C-dimensional features,T = {tj|j = 1, ..., n} RnC. Taking these tokens as input for the following stages, the networkinitially employs the Adversarial Significance Identifier module to formulate a per-token dropping rate,assigning higher rates to tokens that are critical for classification. Subsequently, the Target-guidedPromptor selectively eliminates key vectors based on the established rates. Through introducing suchan adversarial progressive dropping scheme, the network gathers a diverse set of perception cuesand assimilates various patterns from an expansive region of the underlying object structure, thusenhancing classifier resilience against potential corruptions. The proposed architecture is segmented into three distinct stages, each consisting of multiple blocks.Within stages, the depth of self-attention blocks is conventionally configured to . Each stageis further equipped with a uniformly applied Adversarial Significance Identifier and an associatedcomplementary supervision head. The default adversarial dropping ratio, is uniformly set to [0.2, 0.2,0.2] across all stages. Each block within stages independently incorporates a Target-guided Promptor. 3.2Adversarial Significance IdentifierThis module operates by meticulously processing the input tokens extracted from point clouds. Itevaluates and distinguishs the relative significance of each token and applies constraints particularlyto those tokens identified as most crucial in the context of point representation learning, therebyefficiently unveiling underlying data structures through progressively mining patterns that sub-significant yet representative and insightful. Utilizing an auxiliary supervisory process, it integratesglobal contextual analysis and discerns significance of all input tokens T; thus identifying the focaltokens, finally generating per-token dropout rate. To this end, it sorts tokens based on the featurechannel responses via index number in auxiliary supervision process, then it establishes the mappingbetween tokens and the dropout rate, finally predict per-token dropout rate M = {mj|j = 1, ..., n} Rn. The overall workflow is shown in . Focal tokens identification. In the context of point cloud analysis, let T = {tj|j = 1, ..., n} RnCrepresent a set comprising point cloud tokens. Initially, an essential step entails the organization oftoken contributions to final perception based on their associated feature channel responses, throughthe auxiliary supervisory process. Subsequent to this arrangement, we proceed with the identificationof the most prominent tokens pertaining to each feature channel, resulting in the derivation of theirrespective index bank denoted as Ftopk =f mtopk|m = 1, ..., D RkC, where k signifies thenumber of tokens retained per channel feature. This process can be succinctly expressed as:Ftopk = TopK(T),(1) where f mtopk Rk enumerates values within the range (1, N), indicating the token associated withthe feature response under consideration. Upon the construction of the index repository Ftopk, weperform a comprehensive aggregation of the elements within it. This process is pivotal for quantifyingthe occurrence frequency of each token in the ultimate perception of the model. This assists indelineating the tokens of substantial relevance. The mathematical representation is as follows:M = (Ftopk),(2) where symbolizes the function employed to compute the token significance from Ftopk in accor-dance with predefined indices. Consequently, the matrix M = {mj|j = 1, ..., n} Rn emerges asthe importance matrix for tokens. Each element within this matrix, denoted by Mj R1, serves toquantify the representation significance of the j th token within the overall perception framework. Supervisory token identification process. Current algorithms that optimize the network under theguidance of readily high-level labels at final layer is insufficient for guiding the selection of dropmatrices in the intermediate stages of the network. This limitation may lead to the phenomena thatconstructed matrices do not accurately reflect the contributions of tokens, potentially compromisingthe efficiency of algorithm. Thus, the major question arises: how can we optimize the selection ofdrop matrices to accurately represent each tokens contribution? To rectify this, our framework innovates with an adversarial approach embedded within each stage,which cornerstone is the integration of specialized auxiliary heads, strategically positioned within the",
  "Point Cloud Inputs": ": Overall architecture of our algorithm, composed of two key modules: AdversarialSignificance Identifier and Target-guided Promptor. The former evaluates token significance withinthe context of the global perception, with the help of dominant feature indexing process froman auxiliary supervising loss that can bolster the precision of the index selection, then producingdropping rate for tokens. Subsquently, Target-guided Promptor enhances key dropout probabilitiesinfluenced by rate above, driving the model to explore auxiliary segments for pivotal information.This mechanism mitigates the propensity of the model to overfit to localized patterns.network. Each auxiliary head is assigned the critical task of computing a unique and stage-specificloss, it identifies the patterns or tokens that the current stage predominantly focuses on and thenintentionally drops these identified tokens. This process forces the network to shift its attention,thereby encouraging it to mine features from other, less emphasized regions. The operation within these auxiliary heads begins with a dominant feature index operation applied tothe tokens, which aligns with the identification methodology detailed in Eq. 1. This ensures that thedrop matrices generated are a true reflection of tokens significance at every specific network stage. Implementing these adversarial-focused auxiliary heads represents a significant leap in the optimiza-tion process. By introducing a targeted, stage-specific supervision issue at each stage, they offer arefined assessment of token contributions, a stark contrast to the broader, less specific results fromthe networks final layer. This precision in evaluation is pivotal to the nuanced understanding andoptimization of each stage within the network, aligning token signification closely with the specificobjectives of each stage in the network. The adversarial approach facilitates an augmented constraintterm and plays an instrumental role in the selection of drop matrices at each stage, contributingsubstantially to the optimization process, encapsulated as follows:",
  "i=1yi yi2 , (3)": "where y and y denote the predicted and ground-truth labels, respectively, for the input point cloudP RN3, with 2 indicating the L2 norm. This constraint term, derived from the cumulative losscomputed by all the auxiliary heads, ensures a comprehensive and layered supervision across thenetwork. It allows the Adversarial Significance Identifier to discern the contribution of each tokenmore accurately, thereby generating more targeted and effective supervisory signals for the tokendropping process. This, in turn, enhances the overall efficacy and precision of the network learning. Derivation of per-token dropout rate. The objective of () below is to refine the raw frequencydistribution of tokens, as encapsulated within the matrix M, transforming them into precise andactionable probability distribution. This transformation is critical for the effective application ofdropout rates to tokens in the network. The specific functional representation is detailed below:",
  "if nmjnj=1 mj > (4)": "Herein, the parameter represents the mapping ratio, a pivotal factor whose impact and intricaciesare thoroughly examined in a later ablation study. The variable n denotes the total number oftokens under consideration. The boundary parameters, and , are pre-set to the values of 0.05and 0.95, respectively. These parameters play a vital role in defining the range and sensitivity ofthe transformation function. The probability distribution thus generated by () is instrumental in L H",
  "dictating the dropout strategy in the following Target-guided Promptor. This structured approachensures that the dropout process is both targeted and efficient, enhancing the overall performance": "3.3Target-guided PromptorIn traditional self-attention mechanism in transformers, the use of vanilla dropout techniques isprevalent, where each node in attention matrix is assigned a uniform stochastic discard probability.While this strategy aids in reducing overfitting and enhancing the generalization, it inherentlyoverlooks the non-uniform characteristics of nodes in the matrix. The indiscriminate applicationof random dropout across all nodes fails to effectively penalize those with significantly higherattention scores; this oversight may result in a residual risk of overfitting to specific localized patterns.To address this shortcoming, our proposed methodology diverges from the conventional approachby independently targeting each key within the self-attention mechanism. As detailed in ,this focused strategy efficiently penalizes keys with pronounced attention scores, thereby directlyaddressing the issue of overfitting to specific local patterns. During each training epoch, this module adaptively masks a specified fraction of keys in the input keymap using the matrix M. Notably, for every distinct query, a unique masked key map is synthesized,as opposed to utilizing a shared masked key map for the entire set of query vectors. Given tokenfeatures defined by T = {tj|j = 1, ..., n} RnC accompanied by coordinates D, we derive therepresentations for query (Q), key (K) and value (V ) as following:",
  "where W Q, W K, W V RCC are the learnable linear projections": "Generating dropout target. We aim to generate a vector M whose elements have probability ofbeing negative infinity, based on the value in M above. Based on the probability values encapsulatedwithin matrix M, we synthesize matrix M . Each element within M RnC is assigned a value ofnegative infinity with a probability determined by the corresponding value in M:",
  "This resultant vector is then expanded across feature channels": "Dropping key process. In preceding step, we derived the dropout rates M . This is subsequentlyemployed to drop elements from the key vector K. Specifically, by adding the vector M , whichcontains negative infinity values, to K, the network disregards designated positions, thus achieving atargeted dropout implementation. The mechanism is defined as follows:",
  "Experiments": "We first report our 3D robust classification results on synthetically generated and real-scanned datasetsin 4.1 and 4.2, respectively. Subsequently, we assess our methodology efficacy in the robust 3Dsegmentation in 4.3. In 4.4, we provide ablative analyses on our core algorithm design. 4.1Results on ModelNet-CDataset. We train models on the clean ModelNet40 dataset and evaluate them on the ModelNet-C corruption test suite, which includes seven types of corruptions, Jitter, Drop Global/Local,Add Global/Local, Scale and Rotate, with five levels of severity . We take mean corruption errormetric (mCE, %, ) as the main evaluation matrix. More details are in the supplementary materials. Main Results. The comparative performance of various methodologies in terms of their resilienceto corruption is systematically encapsulated in Tab. 1. The results unequivocally demonstrate thatour proposed method exhibits superior performance, registering an exemplary state-of-the-art mCEscore of 72.2%. This outstanding performance is notably achieved through straightforward networkarchitectural modifications paired with our unique adversarial approach, eschewing the need forintricate training scheme alterations or complex feature extraction. In direct comparison with thesota method PointGPT , our method exhibits a marked enhancement of 11.2% mCE reduction.Moreover, APCT consistently attains high mCE scores across categories: 46.8%, 85.0%, 28.5%, and29.8% for drop-global, drop-local, add-global, and add-local, respectively. These metrics are either atthe pinnacle or are approaching the current best results in each respective subcategory. Such resultsunderscore the efficacy of APCT in maintaining robustness against a wide spectrum of corruptions,encompassing both global and local perturbations When compared with RPC , an architecturetailored for enhanced robustness against corruption, APCT manifests a striking improvement inmCE for add-global, surpassing 60%. This underlines APCT proficiency in discerning subtle globalgeometric intricacies within the point cloud, thus facilitating the extraction of pivotal features. 4.2Results on ScanObjectNN-CDataset. ScanObjectNN consists of objects acquired from real-world scans, offering a more authenticevaluation of recongnition, particularly when compared with CAD datasets. ScanObjectNN-C is thecorresponding corruption test suite of the hardest version of ScanObjectNN. Main Results. Tab. 2 summarizes the comparison results on ScanObjectNN-C, showing our algorithmworks well on real-world challenging corrupted point clouds. In particular, our algorithm achievesimpressive sota result of 74.2% mCE. In line with the other results, our method distinctly surpasses",
  "vs. prev. SoTA4.0-": "them, delivering 4.0% and 7.5% mCE reduction for PointM2AE and PointGPT , respectively.This confirms our algorithm is applicable in real-world corruptions. Notably, our method obtainsconsistent high performance of nearly all categories. it achieves 48.9%, 67.0%, 30.0%, and 63.0%mCE over drop-global, drop-local, add-global, and add-local, respectively. These results are all thesota results in each subcategory, illustrating that our method performs well in the face of point addingand removing corruption, both globally and locally. 4.3Results on Shapenet-CDataset. Our method can generalize well to down-stream tasks, including part segmentation. Tovalidate this, we conduct a part segmentation experiment on the ShapeNet-C dataset . Main Results. Tab. 3 showcases the performance of our method, achieving a mCE score of 81.4% andsurpassing the previous sota GDANet and PointMAE by an impressive margin of 10.9% and 11.3%,respectively. It is worth highlighting that PointMAE operates as a scheme of masked autoencodersfor point cloud self-supervised learning, which is pretrained on large set of data. This underscores thesubstantial enhancement brought about by our technique in improving the model robustness",
  "Ablation StudyTo validate the efficacy of our core algorithm designs and parameter settings, we conduct a series ofablative studies on ModelNet-C": "Effect of adversarial dropping strategy. To ascertain the impact of our core idea of adversarialdropping, we conducted an ablation study by removing the adversarial dropping process, alongwith the collaborative supervising identification process. As shown in Tab. 4, the baseline model,trained in the standard strategy, gains 76.2% and 78.8% mCE, on ModelNet-C and ScanObjectNN-C,respectively. Additionally considering the dropping strategy and supervising identification processLA(Eq. 3) can lead to 4.0% and 4.6% mCE reduction, respectively. However, adding droppingstrategy without ancillary constraints from LA will lead to mCE increasing. These results verifythat combining these two training objectives can yield the best results, indicating that mining datastructures from subsidiary component can benefit detailed analysis of point cloud Visualization of Data Distribution. To showcase the efficacy of our algorithm in mitigating modeloverfitting to particular patterns, we conduct an analysis on the statistical distribution of the patternslearned by the model. Specifically, we compute the variance of patterns learned from Level 2 Jitter",
  ": Statistical vari-ance distribution of pat-terns learned": "within the ModelNet-C dataset, as depicted in . Each point in the figure corresponds to thevariance from an individual sample from the dataset, the radius represents the value. Analysis revealsthat the pattern distribution associated with the vanilla dropout approach tends to exhibit increasedvariance, which suggests a propensity for the model to overfit to certain patterns. In contrast, APCTprompts the model to integrate information from a broader array of tokens. Effect of selection number k. We next investigate the impact of the selection number k of AdversarialSignificance Identifier in . Here k = 1 means directly treating the max-feat token as the singleessential part. This baseline (k = 1) obtains 75.1% mCE on ModelNet-C. After more essentialpattern mining, we observe improvement against corruption, e.g., 75.1% 72.2% mCE whenk = 2. When k > 2, further increasing k gives marginal performance gains even worse results. Wespeculate this is because the model is distracted by some trivial patterns due to over-mining. Effect of mapping rate . Tab. 5 gives the performance with regard to the ratio in mapping function(in Eq. 4). The model performs better with a medium ratio [0.2, 0.2, 0.2] , showing that moderatemapping ratio is more favored. Moreover, at the asymptotic cases of = [0.1, 0.2, 0.3] or = [0.3, 0.2, 0.1], the performance drops considerably, evidencing that gradually adjustingthe dropping strategy is not a sound solution. Disparity between ours and vanilla dropout. Our method diverges distinctly from the vanilladropout, from the perspectives of motivation, technical implementation, and empirical outcomes. From Motivation: we aim to address a nuanced challenge overlooked by standard dropout method:the tendency of deep learning models to make misguided inferences from local features withinperturbed samples, particularly prevalent in real-world data scenarios. From Implementation:dropout fosters generalization by randomly attenuating network parameters. In contrast, our approacheschews simplistic random dropout in favor of a meticulously engineered mechanism that identifiesand diminishes tokens deemed pivotal by the model in its preliminary assessments, thereby compellingthe network to recalibrate and shift its focus to previously overlooked tokens, as seen in a and. For results, curves in demonstrates that when contending with corruptions in theModelNet-C dataset, our method significantly outperforms the standard dropout (72.2% vs 74.5%).5ConclusionIn this work, we introduce a new algorithm, tailored for point cloud recognition in the presenceof real-world corruptions. Our algorithm incorporates an adversarial dropping strategy, facilitatingthe capture of diverse patterns, enabling the assimilate information from non-corrupted regions,thereby ensuring robust prediction under local pattern damaged scenario. Experimental evaluationson comprehensive benchmarks manifest its superiority. Limitations. While current method can discern different patterns through adversarial strategies andrender accurate judgments under corrupted scenarios, the optimal utilization of these cues has notbeen extensively explored in this paper. In future work, we intend to delve deeper into this aspect. This work was financially supported by the National Natural Science Foundationof China (No. 62101032), the Postdoctoral Science Foundation of China (Nos. 2021M690015,2022T150050), and Beijing Institute of Technology Research Fund Program for Young Scholars (No.3040011182111). Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3dmodel repository. arXiv preprint arXiv:1512.03012, 2015.",
  "Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie Zhou, and Jiwen Lu. Point-bert:Pre-training 3d point cloud transformers with masked point modeling. In CVPR, pages 1931319322, 2022": "Jinlai Zhang, Lyujie Chen, Bo Ouyang, Binbin Liu, Jihong Zhu, Yujin Chen, YanmeiMeng, and Danfeng Wu. Pointcutmix: Regularization strategy for point cloud classification.Neurocomputing, 505:5867, 2022. Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, andHongsheng Li. Point-m2ae: multi-scale masked autoencoders for hierarchical point cloudpre-training. Advances in neural information processing systems, 35:2706127074, 2022.",
  "Corruption Implementation Details. ( A.9) : Detailed explication of the corruption settingsemployed within benchmark datasets is provided, elucidating the experimental conditions andvariables": "Focal Tokens Identification Implementation. ( A.10) : The algorithm for identifying focaltokens is articulated, including a pseudo-code representation in a Pytorch-like syntax, to facilitatereplicability and clarity in implementation. Comparative Analysis of Confusion Matrices. ( A.11) : This section presents a comparative studyof the confusion matrices resulting from standard transformer-based point cloud models vis--vis ourproposed algorithm, thereby highlighting the distinctive attributes and performance metrics. More Visualizations of Learned Patterns. ( A.12) : Further visual evidence is furnished, show-casing the patterns discerned by the classifiers final self-attention layer, thus reinforcing the inter-pretability aspect of our model.",
  "A.1Training Setup Details": "ModelNet-C. Our model is optimized using the AdamW optimizer with a batch size of 32for 300 epochs. The optimizer utilizes a learning rate of 0.0005 and weight decay of 0.05. TheCosineAnnealingLR scheduler is employed to decrease the learning rate to the minimum valueof 1e-6, and the warm up epochs is set to 10. On ModelNet, input point cloud is partitioned into 64tokens with 384 dimension of feature. ScanObjectNN-C. On ScanObjectNN, our model is trained with a batch size of 32 using the AdamWoptimizer for 300 epochs. The optimizer is configured with a learning rate of 0.0005 and weight decayof 0.05. The learning rate is decayed to the minimum value of 1e-6 using the CosineLRSchedulerscheme, with warp up epochs of 10. For the representation, the point cloud data is systematicallypartitioned into 128 tokens, each having a feature dimension of 384. ShapeNet-C. In this experiment, the proposed model is trained for 300 epochs using the AdamWoptimizer. A batch size of 16 is set for the training process. The optimizer was configured with alearning rate of 0.0002 and weight decay of 0.05. Additionally, the learning rate was decayed to theminimum value of 1e-6 by employing the CosineLRScheduler scheme.",
  "A.2Computational Overhead": "To demonstrate the computational impact of our approach, we also conducted corresponding compar-isons, experiments are conducted on one GeForce RTX 3090. Its impact on computational overheadis presented in Tab. 6, including deliberations on both training and inference speed. Noteworthy isthat our method stands out for its exemplary memory efficiency, as it refrains from incurring anyadditional memory overhead. When juxtaposed with the baseline technique, our method, albeitregistering a marginal deceleration in the realms of training ( 32 samples/s, 6% delay) and inference( 66 samples/s, 6% delay), emerges superior with a pronounced decrement in mCE, registering adownturn of 4.0%.",
  "A.3Results on Clean Dataset": "In pursuit to rigorously evaluate and validate the potency of our proposed technique, we also conductcomprehensive evaluations on a pristine dataset. Specifically, we chose the ScanObjectNNs most challenging variant, PB-T50-RS, for our experiments. Our proposed technique consistentlyexhibited improvements even on such clean datasets, a testament underscored by our exhaustiveexperimental validation. The results presented in Tab. 7 show a discernible performance boost,moving from 85.3% to 86.2%. Notably, this is a highly competitive result compared with manyadvanced methods , demonstrating the generalizability of our proposed algorithm.",
  "A.4Results on Point Cloud Attack Defense": "Setup. Point cloud attack for classification aims to manipulate the input point cloud in a mannerthat induces misclassification by a well-tuned classifier. Encouraged by the remarkable performanceof our algorithm against corruption, we investigate its potential application in point cloud defense.To evaluate this, we assess the performance of baseline model trained using our method on variousattacks, including point perturbation attack , individual point adding attack , kNN attack ,and point dropping attack . Consistent with previous works , we conduct targeted attacksand report the resulting classification accuracy. Higher accuracy indicates better defense againstattack.",
  "+74.31+72.85+54.70+49.96+3.53+7.29": "provement can be as high as 7.29% in the Drop-200 attack, indicating the scalability and effectivenessof the approach. For point cloud attack defend, our method perform well on defending all attacks, asseen in Tab. 8. Even with the Perturb attack, which introduces very fine perturbations leading to achaotic point cloud, APCT achieves a good performance of 74.31% OA. These results evident thestrong generalization ability of our APCT. The above analysis leads to the conclusion that our methodhas a strong generalization ability across various point cloud attack algorithms.",
  "A.6Performance of APCT with augmentation methods": "We further evaluate the performance of our approach with various data augmentation methods onModelNet-C dataset, the results are as follows. Beside the discussed PointMixup and PointCut-Mix , we additionally incorporate experiments with the data augmentation techniques Point-WOLF , RSMix , and WOLFMix . Among these, PointMixup, PointCutMix and RSMix fall under the category of mixing augmentation,where they mix several point clouds following pre-defined regulations. PointWOLF pertains todeformation techniques that non-rigidly deforms local parts of an object. WOLFMix combines bothmixing and deformation augmentations, which first deforms the object, and subsequently rigidlymixes the deformed objects together. As shown in the Tab. 10, data augmentation methods further improve the robustness of our methodagainst point cloud corruptions. Employing mixing or deformation data augmentation techniquesindependently can enhance the robustness of the model, e.g., the results of our model with PointWOLF(67.0% mCE) and with PointMixup (66.2% mCE). When these two techniques are combined, asin WOLFMix, the robustness of the model is further augmented (64.7% mCE). Additionally, theseexperiments demonstrate the compatibility of our method with various data augmentation techniques,further underscoring its potential in addressing data corruption.",
  "A.8Effect of the adversarial mechanism incorporating to advanced methods": "We have extended the adversarial digging sub-optimal patterns mechanism to two state-of-the-art(SOTA) methods, PointM2AE and PointGPT , on the ModelNet-C dataset. The results arepromising and demonstrate the general applicability of our approach. As shown in the Tab. 12 below, incorporating our digging sub-optimal patterns mechanism intoPointM2AE and PointGPT resulted in significant reduction in mCE scores. These results suggestthat our approach can effectively enhance the robustness of various point cloud recognition models.By encouraging the model to explore and utilize a broader range of patterns, our method enables themodels to better generalize to corrupted data.",
  "A.9Corruption Implementation Details": "In the realm of corruption benchmarks, ModelNet-C , ScanObjectNN-C , and ShapeNet-C are prominent datasets, as detailed in Tab. 13. Among these, ScanObjectNN-C is particularlynoteworthy for its real-world scanned dataset, presenting a heightened level of challenge. Centralto the analysis of these benchmarks is the application of seven atomic corruptions, which serveas fundamental perturbations for evaluating algorithmic resilience in the face of data degradation.Details about the corruptions are as follows: Scale: Application of random anisotropic scaling to the point cloud. Rotate: Rotation of the point cloud by a small angle. Jitter: Addition of Gaussian noise to point coordinates. Drop-Global: Random removal of points from the point cloud. Drop-Local: Random elimination of several local clusters from the point cloud. Add-Global: Addition of random points sampled within a unit sphere. Add-Local: Expansion of random points on the point cloud into normally distributed clusters.",
  ": Visualization of samples in ModelNet-C, which is constructed by seven types of corrup-tions with five levels of severity. Listed examples are from severity level 2": "Each corruption type is associated with five severity levels, facilitating a comprehensive evaluation ofrobustness. This diverse set of corruptions serves to systematically assess the resilience of modelsagainst various perturbations in the point cloud data. The differential attributes between samplessubjected to an array of corruptions and the clean sample are visually delineated in . Anobservable shift in structural fidelity is evident in the corrupted instances when juxtaposed against theirunaltered originals. These perturbations markedly hinder the performance efficacy of computationalmodels, culminating in a consistent decline in model accuracy across a spectrum of corruptioncategories.",
  "A.10Focal Tokens Identification Implementation": "Algorithm. 1 shows the implementation of focal tokens identification introduced in the mainmanuscript. Through such implementation, we can easily calculate all token significances asso-ciated to overall perception and identify the focal ones. Algorithm 1 Pseudo-Code of Identifying focal tokens in a Pytorch-like Style.Input:tokens - [N, C], where N denotes the length and C denotes the featdimensions.k - number of focal tokens selected in each feat channel, default 2.Output:matrix - [N], each value within matrix enumerates number within therange (0,C), indicating the token significance associated to overallperception. # identify focal tokensFunction: identify_focaltokens(tokens, k):N, C = tokens.shape# sort token in each feat channel_, sort_idx = sort(tokens, dim=0, descending=True)# select the idxs of top k tokensidx_topk = sort_idx[:k, :]idx_topk = idx_topk.view(-1)matrix = zeros(N)# accumulate the idxsmatrix.scatter_add(1, idx_topk, ones_like(idx_topk))return matrix",
  "A.11Comparative Analysis of Confusion Matrices": "To ascertain the efficacy of our method on corrupted data, we selected all corruption types at anintermediate level (level 2) from ModelNet-C for comparative experiments. As illustrated in, we present side-by-side confusion matrix comparisons. Notably, our adversarial droppingstrategy substantially enhances the models performance under corruption. This suggests that ourapproach incentivizes the model to delve into a broader spectrum of patterns, ultimately convergingto a global motif. Consequently, even if specific local motifs deteriorate within corrupted data, themodel retains the capability to glean information from alternative regions for proficient predictions.",
  "A.12More Visualizations of learned patterns": "To elucidate the implications of our adversarial dropping strategy on real-world corruptions, we em-barked on a methodical analysis, juxtaposing token features that the classifier attends to, particularlyon the perturbed data from the jitter-2 test suite of ModelNet-C. offers a visual delineationof the classifiers final self-attention layer. Specifically, we adopt the Focal tokens identificationprocedure, as outlined in the main manuscript, post-normalization. The computed token featuresare then harnessed to epitomize their significance to the model. Tokens rendering substantial con-tributions to the perception model are depicted with heightened color intensities: red symbolizinghigh contribution, while blue denotes low contribution. The visualizations indicate that, standardarchitectures tend to overfit to specific local patterns, such as table legs or airplane wings. In contrast,our approach fosters a more comprehensive exploration of patterns, culminating in the capture of aglobal motif. Hence, even if some local motifs degrade in corrupted data, the model can still extractvaluable information from other regions for effective prediction."
}