{
  "Abstract": "We introduce a novel, training-free method for sampling differentiable represen-tations (diffreps) using pretrained diffusion models. Rather than merely mode-seeking, our method achieves sampling by pulling back the dynamics of thereverse-time processfrom the image space to the diffrep parameter spaceandupdating the parameters according to this pulled-back process. We identify animplicit constraint on the samples induced by the diffrep and demonstrate thataddressing this constraint significantly improves the consistency and detail of thegenerated objects. Our method yields diffreps with substantially improved qualityand diversity for images, panoramas, and 3D NeRFs compared to existing tech-niques. Our approach is a general-purpose method for sampling diffreps, expandingthe scope of problems that diffusion models can tackle.",
  "Introduction": "Diffusion models have emerged as a powerful tool for generative modeling [Ho et al., 2020, Songet al., 2022, 2021, Karras et al., 2022, Rombach et al., 2022], and subsequent work has extendedthese models to generate complex objectssuch as 3D assets, tiled images, and more. Althoughsuch approaches typically require training or at least fine-tuning of the diffusion models for thesenew modalities [Wang et al., 2023, Luo et al., 2023], two notable exceptions, Wang et al. and Poole et al. , have developed methods for training-free production of 3D objects bydirectly using an image-based diffusion model. Both methods work by optimizing a differentiablerepresentation (diffrep)in this case, a Neural Radiance Field (NeRF) [Mildenhall et al., 2020]toproduce rendered views consistent with the output of the image-based diffusion model. Unfortunately,the nature of both these methods is that they optimize the diffrep to produce the most likelyrepresentation consistent with the images; that is, they perform mode-seeking rather than actuallysampling from the diffusion model. This results in overly smoothed outputs that lack detail and donot reflect the underlying distribution of the diffusion model. In this paper, we present a novel method for sampling directly in the diffrep space using pretraineddiffusion models. The method is training-free, can handle arbitrary diffreps, and performs truesampling according to the underlying diffusion model rather than merely mode-seeking. The key ideaof our approach is to rewrite the reverse diffusion process itself in the diffrep parameter space. Thisis achieved by pulling back the dynamics of the reverse time processfrom the image space to theparameter spaceand solving a (small) optimization problem, implied by the pulled-back dynamics,over the parameters for each diffusion step. To further encourage solver convergence, we identifyconstraints that the diffrep induces on the samples and use these constraints to guide the reverseprocess to generate samples from the high-density regions of the diffusion model while satisfying theconstraints along the sampling trajectory. Our experiments use a pretrained image-based diffusion model (Stable Diffusion 1.5 [Rombachet al., 2022]) to generate samples from various classes of diffreps. For example, by sampling SIRENrepresentations [Vincent, 2011] with wrap-around boundary constraints, we can sample total 360-degree panorama scenes, and by sampling NeRF representations [Mildenhall et al., 2020], we can",
  "arXiv:2412.06981v1 [cs.CV] 9 Dec 2024": "generate 3D models of many objects. In both settingsas well as in baseline comparisons onsimple SIREN-based (non-panorama) image generationour approach substantially outperformsprevious methods such as Score Jacobian Chaining (SJC) [Wang et al., 2022]. Though the problemsetting is considerably more difficult without fine-tuning or retraining, sampling diffreps usingpretrained diffusion models with our method substantially improves and extends the state-of-the-artin training-free generation methods.",
  "SIRENs [Sitzmann et al., 2020], which implicitly model an image using an MLP withsinusoidal activations to map 2D (x, y) pixel coordinates into RGB colors": "NeRFs [Mildenhall et al., 2020], which implicitly model a 3D scene using an MLP thattransforms 3D (x, y, z) voxel coordinates with view directions (, ) into RGB values.We can render images from different views of the scene by numerically integrating the NeRFoutputs along the unprojected rays from the camera. Faster or alternative diffreps also exist for 3D scenessuch as the InstantNGP [Mller et al., 2022]model or Gaussian splats [Kerbl et al., 2023]although in this work we focus on the basic NeRFarchitecture. Many other kinds of visual assets can also be formulated as diffreps. For example, wecan use diffreps to model panoramas, spherical images, texture maps for 3D meshes, compositions ofmultiple images, scenes from a movie, and even the output of kinematic and fluid simulations. Many interesting diffreps can be used to render not just one image from the scene but multiple coupledimages or even a distribution over images. Given a diffrep, parameterized by , for a scenesuchas a SIREN panorama or a NeRFwe can render an image of the scene from a view using adifferentiable render function f(, ) = image X. To accommodate the multiview setting in ourdiscussion, we consider the curried Haskell form of the render function f : ( X), wheref() : X is a map itself from a view to f()() = f(, ) = image X. In the case of NeRFs or SIREN panoramas, the view is a continuous random variable drawn from adistribution thatwith abuse of notationwe will also call . To formalize this, let H X bea vector space of functions from to X. With this definition, we can write the signature of f asf : H. Although H is usually larger than X (and can be even infinite in some cases), we cansimplify our notation by equipping H with an inner product to make it a Hilbert space. This allowsus to use familiar matrix notation with H and hide the view dependence of f() in H.",
  "i[N] f()(i) xi. Because the diffreps and f are bothdifferentiable, this can be accomplished using first-order solvers, such as gradient descent or Adam": "We identify H with X in the following sections for pedagogical convenience. This identificationsimplifies the presentation, provides a more interpretable and intuitive perspective on our method,and is precise when is a singleton. Because H is a Hilbert space, the main points of our argumentsstill hold when we consider a larger . We describe the specific changes needed to adapt our methodto the general multiview setting when H = X, in subsection 3.2.",
  "Diffusion models": "Diffusion models implicitly define a probability distribution via reversing a forward noising process.While many different presentations of image diffusion models exist in the literature (e.g., DDPM[Ho et al., 2020], DDIM [Song et al., 2022], Score-Based Generative Modeling through SDEs [Songet al., 2021], EDM [Karras et al., 2022]), they are all equivalent for our intended purposes. Given a noise schedule (t) for t [0, T] and a score function log pt(x(t)) for the distribution ptover noisy images x(t) at time t, we can sample from p0 by first initializing x(T) N(0, 2(T)I)and then following the reverse time probability flow ODE (PF-ODE) given in Karras et al. totransform the easy-to-sample x(T) into a sample from p0:",
  "Training-free methods": "Poole et al. laid the foundation, demonstrating that generating 3D assets from purely 2Dimage-trained diffusion models was even possible. In DreamFusion (SDS), they perform gradientascent using Et,[w(t)J((f() + (t), t) )], derived from the denoising objective, where J isthe Jacobian of the differentiable render function f and is the learned noise predictor. Independently, Wang et al. introduced Score Jacobian Chaining (SJC), which performsgradient ascent using log p() := Et,[J log pt(f()+(t))] with a custom sampling schedulefor the ts. The custom schedule can be interpreted as gradient annealing to align the implicit (t) ofthe diffrep with the t used to evaluate the score function.",
  "Comparison of methodsUsing the Tweedie formula from Eq. 2 we can rewrite the SDS objectiveas Et,w(t)(t) J log pt(f() + (t))1. This expression is identical to the log p() term from": "SJC if we let w(t) = (t) for all t. Both methods follow this gradient to convergence, which leads toa critical pointa local maximum or modein the log p() landscape. To approximate the objective,both approaches use Monte Carlo sampling. While gradient ascent (GA) on the transformed score resembles solving the PF-ODEparticularly intheir discretized formsthey serve fundamentally different purposes. GA seeks to find the mode ofthe distribution, while the PF-ODE generates actual samples from the distribution. It is possible to reparameterize the GA procedure in a way that resembles solving the PF-ODE bycarefully selecting optimizer hyperparameters and stopping times. However, this reparameterizationis highly specific. The crucial difference lies in the interpretation of the trajectory: in GA, the pathtaken is typically irrelevant as long as it converges to the critical point, allowing us the flexibility tomultiply the gradient by any positive semidefinite (PSD) matrix without affecting the solution.",
  "Note that E[] = 0, so the additional term in the SDS objective behaves as an unbiased variancereduction term for the score estimate since Cov(, ) > 0": "Even small deviations in the path, like evolving the PF-ODE for too long or too short, can lead toeither oversmoothed samples (with too much entropy removed) or under-resolved, noisy samples.Thus, while GA might superficially look like a discretized version of the PF-ODE, the underlyingprocesses are inherently distinct: one finds the mode, and the other describes the full trajectoryrequired to generate samples from the distribution. In the case of SDS and SJC, they both use Adamas the optimizer, ignoring any of the nuanced considerations needed to transform the PF-ODE intothe corresponding optimization procedure. In practice, this distinction means that while both SDS and SJC may yield plausible diffreps at highclassifier-free guidance levels, they tend to falter when producing coherent samples under lower CFGweightsparticularly for complex, multimodal distributions with many degrees of freedom. HighCFG levels may mask these limitations by strongly biasing the output towards the mode, which alignsmore closely with the GA approach, but at the cost of reduced sample diversity and potentiallymissed details captured by the full distribution. Furthermore, in optimization, we can multiply the gradients with any positive semidefinite (PSD)matrix without changing the solution. However, this flexibility does not extend to the score whensolving the PF-ODE. As we will discuss later, the SJC objective is missing a key multiplicative PSDterm, which impacts its performance.",
  "In the multimodal setting, the sample distribution may contain several distinct high-density regions.However, mode-seeking algorithms focus on only one of these regions, sacrificing sample diversity": "In low-dimensional spaces, the mode often resembles a typical sample. However, this intuitionbreaks down in high-dimensional spaces (d 1), such as the space of images. This counterintuitivebehavior can be explained using the thin-shell phenomenon. For a high-dimensional standard Gaussiandistribution, samples are not concentrated near the mode (the origin), as one might expect. Instead,they predominantly reside in an exponentially thin shell at the boundary of a",
  "d-radius ball centeredon the mode. This phenomenon explains why the mode can be an anomalous and atypical point inhigh-dimensional distributions": "As an illustrative example, consider sampling a normalized pure-noise image. Despite having amode of 0, we would almost never expect to sample a uniformly gray image. This provides someinsight as to why the mode of a distribution with several degrees of freedom will lack the quality anddetails present only in samples from the thin shell. Recent developmentsSeveral subsequent works have built upon the SDS and SJC methods byincorporating additional inputs, fine-tuning, and regularization to enhance the quality of the generated3D assets. Zero-1-to-3 [Liu et al.] expands on SJC by fine-tuning the score function to leverage asingle real input image and additional view information. Magic 123 [Qian et al.] further builds onthis by incorporating additional priors derived from 3D data. Fantasia3D [Chen et al.] separatesgeometry modeling and appearance into distinct components, using SDS to update both. HiFA[Zhu et al.] introduces a modified schedule and applies regularization to improve the SDS process.Finally, LatentNeRF [Metzer et al.] utilizes SDS but parameterizes the object in the latent spaceof a stable diffusion autoencoder, rendering into latent dimensions rather than RGB values. Whilethese methods rely on the SDS/SJC framework for mode-seeking, our work takes a wholly differentapproach, focusing on developing a more faithful sampling procedure to replace SDS/SJC for both3D generation and broader differentiable function sampling.",
  "Pretrained and fine-tuning methods": "Unlike SDS and SJCwhich are entirely zero-shot and can be performed with a frozen diffusionmodelseveral other methods have been developed to achieve improved quality and diversity ofthe generated diffreps, albeit at the cost of additional fine-tuning. While some of these methodsrequire additional data [Zhang et al.], ProlificDreamer (VSD) [Wang et al., 2023] and DiffInstruct[Luo et al., 2023] are two examples in which diffusion models can be fine-tuned or distilled usingonly synthetically generated data. VSD specifically addresses the problem of generating 3D assets from a 2D diffusion model usingparticle-based variational inference to follow the Wasserstein gradient flow, solving the KL objectivefrom SDS in the W2() parameter distribution space. This approach produces high-quality, diversesamples even at lower guidance levels.",
  "Constrained sampling methods": "Our method requires generating multiple consistent images of a scene from different views. Forinstance, we generate various images of a 3D scene from different camera locations and orientationsto determine NeRF parameters. These images must be consistent to ensure coherent updates tothe diffrep parameters. While the diffrep inherently enforces this consistency, we can improve ourmethods convergence rate by encouraging the reverse process to satisfy consistency conditions usingconstrained sampling methods. Several approaches enable constrained sampling from an unconditionally trained diffusion model.The nave approach of projecting onto constraints [Song et al., 2021] leads to samples lacking globalcoherence. In contrast, Lugmayr et al. propose RePainta simple method for inpainting thatintermingles forward steps with reverse steps to harmonize generated noisy samples with constraintinformation. For more general conditioningincluding inverse problemsMCG [Chung et al., 2022] and itsextension DPS [Chung et al., 2023] have gained popularity. These methods add a corrective termto the score, encouraging reverse steps to minimize constraint violations for the predicted finalsample x0 according to the Tweedie formula. MCG and DPS have seen considerable application andextension to broader settings [Bansal et al., 2023]. Finzi et al. showed this correction to be anasymptotically exact approximation of log pt(constraint|x), while Rout et al. derive therelevant term from Bayes rule and higher-order corrections.",
  "Pulling back diffusion models to sample differentiable representations": "The score model associated with the noise predictor from Eq. 3 implicitly defines a distribution onthe data space X, and the reverse time PF-ODE from Eq. 1 provides a means to sample from thatdistribution. In this section, we will show how to use the score model and the PF-ODE to sample theparameters of a differentiable representation (diffrep) so that the rendered views resemble samplesfrom the implicit distribution.",
  "d=d(log p)": "dxdxd . Thepullback score is then used as is for gradient ascent in the parameter space. However, since p isa distributionnot a simple scalar fieldthe change of variables formula requires an additionallog det J volume adjustment term for the correct pulled-back score function in the parameter space,where J is the Jacobian of f. A careful examination of this approach through the lens of differentialgeometry reveals even deeper issues. In differential geometry, it is a vector field (a section of the tangent bundle TX)not a differential1-form (a section of the cotangent bundle T X)that defines the integral curves (flows) on amanifold. To derive the probability flow ODE in the parameter space, we must pull back the samplevector field dx",
  ": (Left) Commutative diagram showing how the PF-ODE vector field gets pulled backthrough f, respecting the differential geometry. The process involves: 1 converting dx": "dt to thecotangent vector field log p(x) (up to scaling terms) with the Euclidean metric I, 2 pullingback log p(x) via the chain rule using the Jacobian J, and then 3 transforming the pulled backdifferential form score function into the corresponding vector field using the inverse of pulled backmetric (JJ)1. When used in a PF-ODE, SJC and SDS take the bottom path with the chain rule,however they do not complete the path by neglecting the T T transformation. (Right) SIRENimage renders generated using the PF-ODE schedule with the prompt An astronaut riding a horseusing the: (a) complete pulled-back dx",
  "SJC. The issue lies with the hidden (inverse) Euclidean metric within Eq. 1, which converts thedifferential-form score function into the corresponding vector field": "In canonical coordinates, the Euclidean metric is the identity. Consequently, the components of thescore function remain unchanged when transformed into a vector field. Therefore we can safelyignore the metric term in the PF-ODE formulation of Eq. 1 for the sample space X. However, this isnot true for the diffrep parameter space . To convert the pulled-back score function into the corresponding pulled-back vector field, we mustuse the pulled-back inverse Euclidean metric given by (JJ)1. This yields the pulled-back formof the PF-ODE in Eq. 42. illustrates this procedure via a commutative diagram and providesan example of what goes wrong if you use the incorrect pulled-back term, as suggested by SDSand SJC. In the SDS and SJC approaches, the (JJ)1 term behaves like a PSD preconditionerfor gradient ascent. It may accelerate the convergence rate to the mode but does not fundamentallychange the solution. However, this term is critical for the pulled-back PF-ODE since it impacts theentire trajectory and the ultimate sample. For an explanation of why the log det J term is absent inthe mathematically correct update, see Appendix A. A more intuitive way to understand why the correct update is given by Eq. 4 rather than SDS andSJCs version is to consider the case where the input and output dimensions are equal. From the chainrule, we have d",
  "Efficient implementation": "Separated noiseFollowing the pulled-back PF-ODE in Eq. 4, we can find t such that f(t)represents a sample x(t) p0t(x(t)|x(0)) = N(x(t); x(0), 2(t)I). However, this approach has alimitation: x(t) is noisy, and most diffrep architectures are optimized for typical, noise-free images.Consequently, J is likely ill-conditioned, leading to longer convergence times.",
  "One may be tempted to impose that the metric is Euclidean both in the X space and the space, but thesetwo choices are incompatible because the render transformation maps between the two spaces": "ffffff forwardbackward (t) : The parameters of the diffrep t (torus) are used to render the noiseless signalf(t) = x0(t), which are then combined with the noise (t) to generate the noisy sample x(t). Wecan pull back each step of the reverse diffusion process to update the parameters t + t. To address this issue, we factor x(t) into a noiseless signal and noise using the reparameterization ofthe perturbation kernel: x(t) = x0(t) + (t)(t). By letting (t) = remain constant throughout thesampling trajectory and starting with x0(T) = 0, we can update x0 with dx0",
  "dt (t). Thisdecomposition allows f(t) to represent the noiseless x0(t) instead of x(t), substantially improvingthe conditioning of J. illustrates how this separation works in practice": "Efficient optimizationThe Jacobian J in Eq. 4 represents the derivative of the image with respect tothe diffrep parameters. For all but the smallest examples, explicitly forming JJ is computationallyintractable. While iterative linear solvers together with modern automatic differentiation frameworks(e.g., [Potapczynski et al., 2024]) could solve Eq. 4, we found it faster to compute the parameterupdate as the solution to a non-linear optimization problem. This approach avoids the costly JVP(Jacobian vector product) required to multiply with JJ.",
  "Coupled images, stochastic functions, and 3D rendering": "Thus far, we have only considered the case where the output of the render function is a single imagex X. However, our interest lies in render functions f that map parameters to an entireview-dependent image space H X . Using the inner product defined in subsection 2.1, we derivethe view-dependent pullback PF-ODE:ddt = f dx0",
  "Our method successfully pulls back the PF-ODE dx": "dt using the pull-back of the score function log pt(x(t)). However, our true goal is to pull back log pt(x(t)|x0(s) range(f)) for all s t.This ensures that f can render the noiseless components x0 throughout the remaining reverse process.For example, when pulling back the PF-ODE for different views of a 3D scene, we want to ensureconsistency across all the views. When f is sufficiently expressive and invertible, log pt(x(t)|x0(s) range(f)) log pt(x(t)).This would allow us first to sample x(0) using the PF-ODE from Eq. 1 and then invert f to find (0).However, for most significant applications of our method, f is not invertible. In noninvertible cases, consider the sampling trajectory x(t) in X when x0(t) / range f, particularlywhen no nearby sample has high probability in range f. Each step of Eq. 4 follows the directionin that best approximates the direction of the score model in a least-squares sense. When f isnot invertible, J is not full rank, and the update to will not precisely match the trajectory in x.Consequently, f(t ) x(t t) will be significant. The score model, unaware of this, willcontinue to point towards high-probability regions outside the range of f. For example, consider an f that only allows low-frequency Fourier modes in generated samples. Theunconstrained score model might favor images with high-frequency content (e.g., hair, explosions,detailed textures), resulting in blurry, unresolved images. However, suppose the score model wasaware of this constraint against high-frequency details. In that case, it might guide samples towardmore suitable content, such as landscapes, slow waves, or impressionist artdominated by expressiblelow-frequency components. This issue becomes more pronounced when sampling multiple coupledimages through f, such as with panoramas and NeRFs, where various views must be consistent. To address this challenge, we adapt the RePaint method [Lugmayr et al., 2022], initially designed forinpainting, to guide our PF-ODE towards more renderable samples. RePaint utilizes the completeLangevin SDE diffusion process, interspersing forward and reverse steps in the sampling schedule.This approach allows the process to correct for inconsistencies during sampling. The forward stepsharmonize samples at the current time step by carrying information from earlier steps that implicitlyencode the constraint. As RePaint requires a stochastic process for conditioning, we employ the DDIM [Song et al., 2022]sampling procedure with controllable noise at each step. We derive the forward updates of DDIMin Appendix B. The pseudocode for the DDRep algorithm using DDIM sampling with RePaint ispresented in Alg. 1.",
  "Our method introduces several crucial innovations for sampling differentiable representations usingpretrained diffusion models:": "1. True sampling vs. mode-seeking: Unlike SJC and SDS, which primarily find modes ofthe distribution, our method aims to generate true samples. This distinction is critical forcapturing the full diversity and characteristics of the underlying distribution, especially inhigh-dimensional spaces. SD ref CFG 0.0 CFG 3.0 CFG 10.0 CFG 30.0 CFG 100.0",
  "Ours": "SJC CFG Scale 0.00 0.05 0.10 0.15 0.20 0.25 KID OursSD refSJC : The left figure contains sample renders using the prompt A woman is standing at acrosswalk at a traffic intersection. from the reference SD (top), our method (middle), and SJC(bottom) over the CFG scales from left to right. The right plot is the KID metric(closer to 0 is better) measured on the SIRENs sampled from our method, the SD reference samples,and the SIRENs sampled using SJC. 2. Correct pullback of the PF-ODE: We derive the mathematically correct form of thepulled-back Probability Flow ODE, which includes a crucial (JJ)1 term. While thisterm acts as a preconditioner in optimization-based approaches (affecting only convergencerates), it is essential for unbiased sampling in our PF-ODE framework. Omitting this termleads to incorrect results. 3. Efficient implementation: We introduce techniques for efficient implementation, includingseparated noise handling and a faster suboptimization procedure. These innovations allowfor the practical application of our method to sample complex differentiable representations. 4. Generalization to coupled images and stochastic functions: Our method extends naturallyto scenarios involving coupled images and stochastic functions, making it applicable to awide range of problems, including 3D rendering and panorama generation. 5. Handling multimodality and constraints: Our sampling approach naturally handles mul-timodal distributions, unlike mode-seeking methods. However, this introduces challengeswhen the samples are incompatible with the constraints imposed by the differentiable rep-resentation. We address this by conditioning the sampling process with the consistencyconstraint using an adapted RePaint method. These key ideas collectively enable our method to generate high-quality, diverse samples of differen-tiable representations by encouraging the diffusion model to maintain consistency with the constraintsimposed by the representation.",
  "Experiments": "All our experiments used the Hugging Face implementation of Stable Diffusion 1.5 (SDv1.5) fromRombach et al. with the default hyperparameters as the noise predictor model. We conductedour experiments on a single NVIDIA A6000 GPU. We used the complete Langevin SDE given bythe DDIM [Song et al., 2022] procedure for all our experiments, with = 0.75 as the stochasticinterpolation hyperparameter. We interspersed forward and reverse steps to harmonize the diffrepconstraints in the sampling procedure. For the suboptimization problem described in Eq. 5, we usedthe Adam optimizer for 200 steps.",
  "x(0)obtained by first solving the PF-ODE in the image spacewe use this scenario to comparedifferent training-free generation methods quantitatively": "The SIREN we used was a 3-layer MLP mapping the 2D (x, y) 2 pixel coordinates to the R4latent space of SDv1.5. The MLP had a 128-dimensional sinusoidal embedding layer, a depth of 3,a width of 256, and used sin() as the activation function. To render an image, we used a 64 64grid of equally spaced pixel coordinates from to with a shape of (64, 64, 2). The MLPoutput was a latent image with shape (64, 64, 4), which we decoded using the SDv1.5 VQ-VAE intoan RGB image with shape (512, 512, 3). We calculated the final metrics on these rendered images.",
  ". Images rendered from SIRENs generated using SJC [Wang et al., 2022]": "We compared these sets against 100 baseline images generated using SDv1.5 with the same promptsbut a different seed. We repeated this experiment at five CFG scales [Ho and Salimans] (0, 3, 10,30, 100). We used the KID metric from Binkowski et al. to compare the generated images, as itconverges within 100 image samples while maintaining good comparison ability. For runtime metrics,see Appendix C. The example images in (left) demonstrate that the samples generated by our method are almostindistinguishable from the reference. The results in (right) clearly show that our methodproduces images on par with SDv1.5 reference samples. To further support this observation, we measured the PSNR, SSIM, and LPIPS scores of our methodssamples against the SDv1.5 reference images (). The results show that our method producesSIREN images nearly identical to the reference set. To demonstrate that our method achieves sampling instead of mode-seeking, we plot eight samplesusing the same prompt for all models in with different seeds at CFG scale 10. The samplesgenerated by SJC all look very similar, while those generated by our method and SD show significantlymore diversity.",
  "SIREN Panoramas": "In this section, we demonstrate how our method can generate SIREN panoramas using the samearchitecture as the SIREN from subsection 4.1 but with a modified render function. We begin bysampling a view grid of pixel locations with the shape (8, 64, 64, 2) where the batch size is 8, theheight and width are 64, and the last 2 dimensions are the (x, y) coordinates. The y values areequally spaced in the range , and x values are equally spaced in the range [r, r +1/ar] mod 1.0 RePaint CFG Scale 3.0 No RePaint RePaint CFG Scale 10.0 No RePaint : Comparison of landscape panoramas sampled using our method. In each pair, the toppanorama is sampled using the RePaint method, while the bottom is sampled without RePaint. Bothapproaches use 460 function evaluations (NFEs) to ensure fairness. The top pair uses CFG scale 3.0,and the bottom pair uses CFG scale 10.0. The prompt for these panoramas was Landscape pictureof a mountain range in the background with an empty plain in the foreground 50mm f/1.8.",
  "for r U(0, 1). The mod 1.0 constraint ensures horizontal wrapping, making the panoramacontinuous over the entire 360 azimuth": "We render the view by sampling from the SIREN at the discrete pixel locations given by the viewgrid, then decode the latent output using the SDv1.5 VQ-VAE. For our experiments, we assumed thatthe images were taken by a camera lens with a 45 field of view (equivalent to a 50mm focal length),corresponding to an aspect ratio of ar = 8. The SIREN panorama imposes implicit constraints to ensure consistent renders across overlappingviews. illustrates the impact of the consistency conditioning described in subsection 3.3. Toensure fair comparisons, both RePaint and non-RePaint methods use the same number of functionevaluations (460 NFEs). Using RePaint substantially improves the quality and consistency of the generated panorama. Thisimprovement is even more pronounced at low CFG scales, where the diffusion model is encouragedto be more creative and is typically less likely to produce consistent images.",
  "D NeRFs": "For our NeRF experiments, we employed the voxNeRF architecture used by SJC [Wang et al., 2022],chosen for its speed and efficiency. While we did not utilize any additional loss terms from theirmethod, we found that incorporating the object-centric scene initialization from Wang et al. significantly improved our results. This initialization bootstrapped the diffusion process and ledto faster convergence. Specifically, we used the initialization function init(x) = 101 x",
  "Future work and limitations": "Our work has demonstrated the zero-shot generation of implicit images, panoramas, and NeRFs. Theversatility of our approach opens up a wide range of potential applications, including vector graphicsgeneration, embedding diverse content in scenes through geometric transformations, differentiableheightmaps, and applications using differentiable physics renderers. While we focused on NeRFs in this paper, more advanced differentiable representations like Gaussiansplats and full-scene representations have emerged. Our method should be applicable to these cases,but further work is needed to adapt our approach to these newer representations. Looking ahead, one could envision generating entire maps or game environments using diffusionmodels. However, this would likely require innovations in amortization and strategies for splittingand combining subproblems. In these cases, the function f may be even more restrictive than forSIRENs and NeRFs, underscoring the importance of conditionally sampling the implicit constraint. LimitationsOur method faces two significant limitations. First, the additional steps required forRePaint introduce computational overhead. A priori, we cannot determine how many steps are neededto harmonize the constraint into the diffrep. Transitioning to a conditional sampling method likeMCG could potentially allow us to skip these extra forward steps and directly integrate the constraintinto the solver. However, this requires further investigation to ensure compatibility with our method. Second, the substantial stochasticity in the Monte Carlo estimate of the pullback score over severalviews, particularly when the Jacobian of the render function is ill-conditioned, poses a challenge.While increasing the number of sample views could reduce variance in our estimate, it would alsoslow down our algorithm and consume more memory. Further research into view sampling techniques,such as importance sampling, could help decrease variance without compromising computationalefficiency.",
  "Conclusion": "We have presented a comprehensive, training-free approach for pulling back the sampling process ofa diffusion model through a generic differentiable function. By formalizing the problem, we haveaddressed two critical issues with prior approaches: enabling true sampling rather than just modeseeking (crucial at low guidance levels), and addressing a latent consistency constraint using RePaintto improve generation quality. Our method opens up new possibilities for generating complex and constrained outputs usingpretrained diffusion models. We hope that future research will build on these insights to furtherexpand the capabilities and applications of diffusion models in areas such as 3D content generation,game design, and beyond.",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models, April 2022. URL arXiv:2112.10752 [cs]": "Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolific-Dreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation,May 2023. URL arXiv:2305.16213 [cs]. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models,May 2023. URL arXiv:2305.18455 [cs]. Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score JacobianChaining: Lifting Pretrained 2D Diffusion Models for 3D Generation, December 2022. URL arXiv:2212.00774 [cs].",
  "Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Com-putation, 23(7):16611674, July 2011. ISSN 0899-7667. doi: 10.1162/NECO_a_00142. URL": "Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and GordonWetzstein. Implicit Neural Representations with Periodic Activation Functions, June 2020. URL arXiv:2006.09661 [cs, eess]. Thomas Mller, Alex Evans, Christoph Schied, and Alexander Keller. Instant Neural GraphicsPrimitives with a Multiresolution Hash Encoding. ACM Transactions on Graphics, 41(4):115, July 2022. ISSN 0730-0301, 1557-7368. doi: 10.1145/3528223.3530127. URL arXiv:2201.05989 [cs].",
  "Hyungjin Chung, Jeongsol Kim, Michael T. Mccann, Marc L. Klasky, and Jong Chul Ye. DiffusionPosterior Sampling for General Noisy Inverse Problems, February 2023. URL arXiv:2209.14687 [cs, stat]": "Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, JonasGeiping, and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 843852, 2023. Marc Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, and Leonardo Zepeda-Nez.User-defined Event Sampling and Uncertainty Quantification in Diffusion Models for PhysicalDynamical Systems, June 2023. URL arXiv:2306.07526[cs]. Litu Rout, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-ShengChu. Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion, December2023. URL arXiv:2312.00852 [cs, stat]. Andres Potapczynski, Marc Finzi, Geoff Pleiss, and Andrew G Wilson. Cola: Exploiting com-positional structure for automatic and efficient numerical linear algebra. Advances in NeuralInformation Processing Systems, 36, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, PietroPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollr. Microsoft COCO: CommonObjects in Context. URL",
  "This section provides the details for the derivation of Eq. 4 using differential geometry": "First, we provide a full expansion of Eq. 1. For any point in our sample space x X, we can definea Riemannian metric Mx that describes the local geometry of the space around x. Given tangentvectors y, z at point x, we define the inner product using this metric y, zMx = yT Mxz where Mxis represented using a positive-definite matrix. We implicitly use the Euclidean metric for the imagespace X, which corresponds to the identity matrix Mx = Id, x X. However, in curved spacessuch as the parameter space , the corresponding Riemannian metric is not necessarily the identityand can vary based on where it is evaluated. Here, we use matrix notation rather than Einstein summation or coordinate-free notation for familiaritywithin the machine learning context. However, we note that matrix notation can obscure someimportant aspects of differential geometry. For example, while the components of the Euclideanmetric in the Euclidean coordinate basis form the identity matrix, the metric is not the identityfunction, as it transforms between two distinct vector spaces. As such, when using the matrixnotation, it becomes very important to keep track of the types of objects rather than just their values.",
  "dxdt = (t)(t)M 1x log dPt/dx,(9)": "where Pt is the probability measure at time t and x is the Lebesgue measure on X, and dPt/dxis the Radon-Nikodym derivative. Notice that when we use Mx = Id, x X, this equationis equivalent to Eq. 1. If we pull back the dynamics of this process through the render functionf : X to the parameter space , we must pull back both the score function and the metric Mxto get the pulled back the vector field f dx",
  "Tangent vectorsv TX :(f v)() = (JMf()J)1JMf()v(f()), (13)": "where J is the Jacobian of f evaluated at . For a scalar function s(x), the gradient s(x) is co-vectorfield (also known as a differential 1-form) and can be converted into a vector field using the inversemetric: M 1x s(x). We can see that Eq. 13 can be expressed concisely in terms of the pullbackmetric f v = (f M)1f (Mv), which geometrically corresponds to converting the vector field toa covector field with M, pulling back the covector field (using the chain rule), and then convertingback to a vector field with the inverse of the pulled back metric. This sequence is illustrated in .",
  "Change of variables contribution. Some readers may be surprised to see that the Jacobian logdeterminant does not show up in this transformation. Though somewhat technical, this can be seen": "by unpacking the Radon-Nikodym derivative dPt/dx. In general, on a manifold with metric M, theLebesgue measure is given by d = det Mxdx. Therefore, when written in terms of the densitydPt/dx = pt(x), one has dPt/dx = pt(x)/det Mx. When evaluated only in the Euclidean spacewhere the coordinates are chosen such that Mx = I, this detail can be safely ignored. However,when pulling back to the parameter space, these hidden terms matter. The Radon-Nikodym derivativedPt/dx is a scalar field and is not transformed under the pullback, however pt is a scalar densityand satisfies(f pt)() = pt(f())",
  "This section presents additional details and runtime metrics for our algorithm with the experimentalsettings given in section 4. The pseudocode for the algorithm is given in Alg. 1": "Image SIRENsSampling a batch of eight reference images using SDv1.5 takes 39 seconds.Generating SIRENs using SJC depends on the number of iterations used; running SJC for 3000iterations to generate eight SIRENs takes 694 seconds, corresponding to a per-iteration time of 2.31seconds. In comparison, sampling a batch of eight SIRENs using our method takes 82 seconds. Thesetimings demonstrate that our method offers a significant speed advantage over SJC while producingcomparable results to the reference SDv1.5 samples.",
  "SIREN PanoramasGenerating a SIREN panorama with our method takes 218 seconds. Additionalexamples of SIREN panoramas generated using our method are provided in": "3D NeRFsOur algorithm processed a batch of eight views per iteration, which was the maximumcapacity for the A6000 VRAM. The generation of each NeRF required approximately 7.2 secondsper NFE (Number of Function Evaluations). We implemented 100 inference steps using the DDIMprocedure, with one RePaint forward step for each reverse step. This configuration resulted in 199NFEs per NeRF, translating to about 24 minutes of sampling time for each NeRF. It is worth notingthat run times can fluctuate based on the specific sampling procedure and GPU architecture used. Forcomparison, generating a comparable NeRF using SJC takes approximately 34 minutes for 10,000steps, based on the code provided by the authors."
}