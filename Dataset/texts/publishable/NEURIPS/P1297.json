{
  "Abstract": "Existing approaches to diffusion-based inverse problem solvers frame the signalrecovery task as a probabilistic sampling episode, where the solution is drawn fromthe desired posterior distribution. This framework suffers from several criticaldrawbacks, including the intractability of the conditional likelihood function, strictdependence on the score network approximation, and poor x0 prediction quality.We demonstrate that these limitations can be sidestepped by reframing the gener-ative process as a discrete optimal control episode. We derive a diffusion-basedoptimal controller inspired by the iterative Linear Quadratic Regulator (iLQR) algo-rithm. This framework is fully general and able to handle any differentiable forwardmeasurement operator, including super-resolution, inpainting, Gaussian deblurring,nonlinear deblurring, and even highly nonlinear neural classifiers. Furthermore, weshow that the idealized posterior sampling equation can be recovered as a specialcase of our algorithm. We then evaluate our method against a selection of neuralinverse problem solvers, and establish a new baseline in image reconstruction withinverse problems1.",
  "Introduction": "Diffusion models Song and Ermon , Ho et al. have been shown to be remarkably adeptat conditional generation tasks Dhariwal and Nichol , Ho and Salimans , in part due totheir iterative sampling algorithm, which allows the dynamics of an uncontrolled prior score functionx log pt(x) to be directed towards an arbitrary posterior distribution by introducing an additiveguidance term u. When this guidance term is the conditional score x log pt(y|x), the resultingsample is provably drawn from the desired conditional distribution p(x|y) Song et al. . A central obstacle to this framework is the general difficulty of obtaining the conditional scorefunction x log pt(y|xt) due to its dependence on the noisy diffusion variate xt rather than just thefinal sample x0 Chung et al. [2023a]. In large-scale conditional generation tasks such as class- ortext-conditional sampling the computational overhead of training a time-dependent conditional scorefunction from scratch is deemed acceptable, and is indeed the approach taken by Rombach et al., Saharia et al. , and many others. However, this solution is not acceptable in inverseproblems where the goal is to design a generalized solver that will work in a zero-shot capacity for anarbitrary forward model. This bottleneck has spawned a flurry of recent research dedicated to approximating the conditionalscore x log pt(y|xt) as a simple function of the noiseless likelihood log p(y|x0) Choi et al. ,Chung et al. , Rout et al. , Chung et al. [2023a], Kawar et al. , Chung et al. [2023b].However, as we will demonstrate in this work, these approximations impose a significant cost to theperformance of the resulting algorithm.",
  "arXiv:2412.16748v1 [cs.LG] 21 Dec 2024": ": Conceptual illustration comparing a probabilistic posterior sampler to our proposedoptimal control-based sampler. In a probabilistic sampler, the model relies on an approximationx0 x0 to guide each step (left). We are able to compute x0 exactly on each step, resulting in muchhigher quality gradients log p(y|x0) and an improved trajectory update (right). To address these issues, we propose a novel framework built from optimal control theory where suchapproximations are no longer necessary. By framing the reverse diffusion process as an optimalcontrol episode, we are able to detach the inverse problem solver from the strict requirements of theconditional sampling equation given by Song et al. , while still leveraging the exceptionallypowerful prior of the unconditional diffusion process. Moreover, we find that the desired scorefunction directly arises as the Jacobian of the value function.",
  "Background": "NotationWe use lowercase letters for denoting scalars a R, lowercase bold letters for vectorsa Rn and uppercase bold letters for matrices A Rmn. Subscripts indicate Jacobians andHessians of scalar functions, e.g. lx Rn and lxx Rnn for l(x) : Rn R, respectively.We overload notation for time-dependent variables, where subscripts imply dependence rather thanderivatives w.r.t. time, e.g., xt = x(t). Furthermore, V (xt) and Q(xt, ut) are scalar functionsdespite being uppercase, in line with existing optimal control literature Betts .",
  "The diffusion modeling literature uses the following reverse-time It SDE to generate samples Songet al. ,dxt =f(xt) g(t)2xt log pt(xt)dt + g(t)dwt,(1)": "where xt Rn is the state vector, f : Rn Rn and g : R R are drift and diffusion terms that cantake different functional forms (e.g., Variance-Preserving SDEs (VPSDEs) and Variance-ExplodingSDEs (VESDEs) in Song et al. ), xt log pt(xt) is the score-function and wt Rn is a vectorof mutually independent Brownian motions. The above SDE has an associated ODE called the : Predicted x0 used in a probabilistic framework (above) compared to ours (below) fora general diffusion trajectory. The full forward rollout in our proposed framework allows for thepredicted x0 (and therefore xt log p(y|x0)) to be efficiently computed for all t = 0, . . . , T.",
  "Posterior Sampling for Inverse Problems": "Inverse problems are a general class of problems where an unknown signal is reconstructed fromobservations obtained by a forward measurement process Ongie et al. . The forward process isusually lossy, resulting in an ill-posed signal recovery task where a unique solution does not exist.The forward model can generally be written as",
  "Optimal Control": "Optimal control is the structured and principled approach to the guidance of dynamical systems overtime. Many methods have been developed in the optimal control literature and are popularly referredto as trajectory optimization algorithms Betts . Perhaps the most well-known is the IterativeLinear Quadratic Regulator (iLQR) algorithm which uses a first-order approximation of the dynamicsand second-order approximations of the value-function Li and Todorov .",
  "t(xt, ut) + V (xt1, t 1).(11)": "The iLQR algorithm centers around approximating the state-action value function,Q(xt, ut) := t(xt, ut) + V (xt1, t 1),(12)from which the value function can be recovered as V (xt, t) = minut Q(xt, ut). Then given a state transition function xt = h(xt+1, ut+1) where we crucially note that we havedefined time to flow backwards from t = T, . . . , 0, the iLQR algorithm has feedforward and feedbackgainsk = Q1uuQuandK = Q1uuQux(13)The update equations can be written as Vx = Qx KT QuukandVxx = Qxx KT QuuK.(14)Given the feedforward and feedback gains {(Kt, kt)}Tt=0 and x0 := x0, we can recursively obtainthe locally optimal control at time t as a function of the present states xt and controls ut asxt = h(xt+1, ut+1),(15)ut = ut + k + K( xt xt).(16)For a more detailed treatment of iLQR as well as a derivation of the equations, please see AppendixB.",
  "Diffusion Optimal Control": "We motivate our framework by observing that the reverse diffusion process Eq. (1) is an uncontrollednon-linear dynamical system that evolves from some initial state (at time t = T) to some terminalstate (at time t = 0). By injecting control vectors ut into this system we can influence its behaviorand hence its terminal state (i.e., the generated data) to sample from a desired p(x|y). There are twoobvious ways to inject control into this process:",
  "2g(t)2x log pt(xt)t + ut.(18)": "Observe that iLQR is formulated for general discrete-time dynamic processes. When appliedspecifically to the reverse diffusion dynamics of diffusion models, we are able to make severalsimplifications. First, we assume that we do not have access to any guidance except at time t = 0 i.e., t(xt, ut) does not depend on xt.",
  "High Dimensional Control": "Compared to the dynamics in traditional application areas of optimal control, those we consider inEqs. (17- 18) are much higher dimensional in the state x and control u variates. Therefore, iLQRfaces several unique computational bottlenecks when applied to such control problems. In particular,the Jacobian matriceshx, huand the second-order derivative matricesVxx, Qxx, Qux, Qxu, and Quu are particularly expensive to compute, store, and perform down-stream operations against. For example, in a three-channel 256 256 image, these matrices naivelycontain (256 256 3)2 39B parameters.",
  "We demonstrate that our optimal control-based sampler overcomes several practical obstacles thatplague existing diffusion-based methods for inverse problem solvers": "Brittleness to DiscretizationIn a probabilistic framework, solutions to inverse problems incur adiscretization error from the numerical solution of Eq. (8) that decays poorly with the total diffusionsteps T of the diffusion process. While much research has been conducted on the accelerationof unconditional diffusion processes Song et al. , Jolicoeur-Martineau et al. , Karraset al. , Meng et al. , sample quality appears to decay much more aggressively indiffusion-based inverse problem solvers (). We theorize that this is due to two reasons: 1) the posterior sampler Eq. (9) is only correct in thelimit of infinitely small time steps, and 2) the quality of the approximated conditional score termx log p(y|xt) decays quickly with time (), and so fewer timesteps lead to fewer chancesat low t to correct errors made at high t. On the other hand, since optimal control directly casts thediscretized process as an end-to-end control episode, it produces a feasible solution for any numberof discretization steps T. Intractability of xt log p(y|xt)When the forward model A is known and comes from a simpledistribution, the conditional likelihood p(y|xt) can be derived in closed form for t = 0. On the otherhand, the dependence of y on xt for t > 0 is generally not known without explicitly computing x0,which requires sampling from the diffusion process. Ultimately, obtaining the conditional score termxt log p(y|xt) is a highly nontrivial task Song et al. .",
  "and then apply a series of approximations to recover a computationally feasible estimate of theconditional score. First, the marginal p(x0|xt) is replaced by the marginal conditioned on x0, i.e": "p(x0|xt, x0) = N(x0, 2I) Kim and Ye . Next, the x0-centered marginal is replaced by theposterior mean E[x0|xt] given by Tweedies formula Efron . Finally, the true score is replacedby the learned score network. While these approximations are necessary in a probabilistic framework, we show that they are notrequired in our method. Intuitively, this is because the linear quadratic regulator backpropagatesthe control cost log p(y|x) through a forward trajectory rollout, which naturally computes the trueconditional score at each time t. Moreover, our model always estimates x0|xt exactly (up to thediscretization error induced by solving Eq. 3), rather than forming an approximation x0 x0 (). We formalize this observation with the following statement.",
  "ut = xt log p(y|x0).(29)": "In other words, by framing the inverse problem as an unconditional diffusion process with controlsut, our proposed method produces controls that coincide precisely with the desired conditional scoresxt log p(y|x0). Let us further assume that log p(y|xt) = log p(y|x0), i.e., xt contains no additional informationabout y than x0. This assumption results in the posterior mean approximation in Chung et al. [2023a]under stochastic dynamics (Eq. 1), where we additionally obtain exact computation of x0, rather thanx0 x0 via Tweedies formula Kim and Ye . Under the deterministic ODE dynamics (Eq. 2),we recover the true posterior sampler under appropriate choice of Tikhonov regularization constant.",
  ": Quantitative evaluation (FID, LPIPS) of model performance on inverse problems on theFFHQ 256x256-1K dataset": "Dependence on the Approximate ScoreWhile our theoretical results require that the learned scorefunction s(xt, t) approximates the true data score log pt(xt, t), we emphasize that the performanceof our method does not necessitate this condition. In fact, we find that reconstruction performanceis theoretically and empirically robust to the accuracy of the approximated prior score s(xt, t) xt log pt(xt) or conditional score xt log pt(y|x0) xt log pt(y|xt) terms. This is because theoptimal control-based solution is formulated for the optimization of generalized dynamical systems,and thus agnostic to the diffusion sampling process. Certainly, improved approximation of the score terms result in a better-informed prior and usuallyhigher sample quality. However, we demonstrate that our sampler produces remarkably reasonablesolutions even in the case of randomly initialized diffusion models. Conversely, probabilistic posteriorsamplers can only sample from p(y|x0) when the terms composing the posterior sampling equation(Eq. (8)) are well approximated (). Modeling errors can occur even in foundation models.For example, this scenario may arise in models trained on regions where there are underrepresentedexamples in the data. When these arise from existing social or ethical biases, they can furtherperpetuate or amplify biases to the resulting model if left unaddressedBolukbasi et al. , Birhaneet al. , Srivastava et al. . There exist several methods that seek to alleviate the errors incurred by Tweedies formula (being amean approximation of the diffusion process), including Song et al. which imposes a harddata consistency optimization loop at various points in the diffusion process, and Rout et al. which includes a stochastic averaging loop in each step of the diffusion process. However, thesemethods still rely on Tweedies formula for the error reduction scheme, which assumes access toa ground truth score function. Ultimately, the aforementioned problems in the present section areexacerbated in existing samplers, and relatively less consequential in our solver.",
  "Related Work": "The recent success of diffusion models in image generation Song and Ermon , Ho et al. ,Song et al. , Rombach et al. has spawned a surge of research in deep learning-basedsolvers to inverse problems. Song et al. demonstrated a strategy for provably sampling fromthe solution set p(x|y) of a general inverse problem y = A(x) using only an unconditional priorscore model x log pt(x) and a forward probabilistic model log p(y|xt). However, a crucial problemarises in the intractability of forward probabilistic model, which depends on the noisy xt rather thanthe final x0. This has resulted in a series of approximation algorithms Choi et al. , Kawar et al., Chung et al. [2022, 2023a,b], Kawar et al. for the true conditional diffusion dynamics. Topics in control theory have been applied to deep learning Liu et al. , Pereira et al. aswell as diffusion modeling Berner et al. . Optimal control can also be connected to diffusionprocesses via forward-backward SDEs Chen et al. . However, these ideas have not been appliedto guided conditional diffusion processes solely at inference time, nor for guided conditional sampling.Our proposed optimal control-based algorithm is, to our knowledge, the first such framework fordeep inverse problem solvers.",
  "Experiments": "Following previous work Chung et al. [2023a], Meng and Kabashima , Kawar et al. , weconsider five inverse problems. 1) In 4 image super-resolution, we use the bicubic downsamplingoperator. 2) In randomized inpainting, we uniformly omit 92% of all pixels (across all channels).3) In box inpainting, we mask out a 128 128 block uniformly sampled from a 16 pixel marginfrom each side of the image, as in Chung et al. . 4) In Gaussian deblurring, we use a kernelof size 61 61 and standard deviation 3.0. In motion deblurring, we generate images accordingto a library2 of point spread functions with kernel size 61 61 and intensity 0.5. Following theexperimental design in Chung et al. [2023a], we apply Gaussian noise with standard deviation 0.05to all measurements of the forward model. We compare against a generalized diffusion inverse sampler (Score-SDE) proposed in Song et al., Diffusion Posterior Sampling (DPS) Chung et al. [2023a], Denoising Diffusion RestorationModels Kawar et al. , Manifold Constrained Gradients (MCG) Chung et al. , as wellas two recent latent diffusion-based methods Fabian et al. (Flash-Diffusion3) and Rout et al. (PSLD). For non-diffusion baselines, we compare against Plug-and-Play Alternating DirectionMethod of Multipliers (PnP-ADMM) with neural proximal maps Chan et al. , Zhang et al., and a total-variation based alternating direction method of multipliers (TV-ADMM) baselineproposed in Chung et al. [2023a]. We validate our results on the high resolution human face dataset FFHQ 256 256 Karras et al.. Several methods are model agnostic (DPS, DDRM, MCG, and thus evaluated with the samepre-trained diffusion models. To fairly compare between all models, all methods use the modelweights from Chung et al. [2023a], which are trained on 49K FFHQ images, with 1K images left as aheld-out set for evaluation. We compare our algorithm against competing frameworks on these last1K images. We report our results on FFHQ 256 256 in , and demonstrate improvementson all tasks against previous methods. Finally, we demonstrate the performance of our algorithm onthe nonlinear inverse problem of class-conditional generation. Namely, let A(x) = classifier(x)and p(y|x) be its associated probability. We compare our method to DPS on the inverse task ofgenerating an MNIST digit given a label y. Compared to images generated by DPS, images from ourmethod exhibit more pronounced class alignment and higher overall sample quality ().",
  "Conclusion": "In this paper we presented a novel perspective on tackling inverse problems with diffusion models framing the discretized reverse diffusion process as a discrete time optimal control episode. Wedemonstrate that this framework alleviates several core problems in probabilistic solvers: its depen-dence on the approximation quality of the underlying terms in the diffusion process, its sensitivity tothe temporal discretization scheme, its inherent inaccuracy due to the intractability of the conditionalscore function. We also show that the diffusion posterior sampler can be seen as a specific case of",
  "Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Condi-tioning method for denoising diffusion probabilistic models. arXiv preprint arXiv:2108.02938,2021": "Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models forinverse problems using manifold constraints. Advances in Neural Information Processing Systems,35:2568325696, 2022. Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusionposterior sampling for general noisy inverse problems. International Conference on LearningRepresentations, 2023a.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances inneural information processing systems, 33:68406851, 2020": "Matthew D Houghton, Alexander B Oshin, Michael J Acheson, Evangelos A Theodorou, and Irene MGregory. Path planning: Differential dynamic programming and model predictive path integralcontrol on vtol aircraft. In AIAA SCITECH 2022 Forum, page 0624, 2022. David H Jacobson. New second-order and first-order algorithms for determining optimal control: Adifferential dynamic programming approach. Journal of Optimization Theory and Applications, 2:411440, 1968.",
  "Dana A Knoll and David E Keyes. Jacobian-free newtonkrylov methods: a survey of approachesand applications. Journal of Computational Physics, 193(2):357397, 2004": "Henry Li, Ronen Basri, and Yuval Kluger. Likelihood training of cascaded diffusion models viahierarchical volume-preserving maps. In The Twelfth International Conference on LearningRepresentations, 2024. URL Weiwei Li and Emanuel Todorov. Iterative linear quadratic regulator design for nonlinear biologicalmovement systems. In First International Conference on Informatics in Control, Automation andRobotics, volume 2, pages 222229. SciTePress, 2004.",
  "Xiangming Meng and Yoshiyuki Kabashima. Diffusion model based posterior sampling for noisylinear inverse problems. arXiv preprint arXiv:2211.12343, 2022": "Gregory Ongie, Ajil Jalal, Christopher A Metzler, Richard G Baraniuk, Alexandros G Dimakis, andRebecca Willett. Deep learning techniques for inverse problems in imaging. IEEE Journal onSelected Areas in Information Theory, 1(1):3956, 2020. Samet Oymak, Zalan Fabian, Mingchen Li, and Mahdi Soltanolkotabi. Generalization guaran-tees for neural networks via harnessing the low-rank structure of the jacobian. arXiv preprintarXiv:1906.05392, 2019. Marcus Pereira, Ziyi Wang, Tianrong Chen, Emily Reed, and Evangelos Theodorou. Feynman-kacneural network architectures for stochastic control using second-order fbsde theory. In Learningfor Dynamics and Control, pages 728738. PMLR, 2020.",
  "Kaare Brandt Petersen, Michael Syskind Pedersen, et al. The matrix cookbook. Technical Universityof Denmark, 7(15):510, 2008": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-ence on computer vision and pattern recognition, pages 1068410695, 2022. Litu Rout, Yujia Chen, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-ShengChu. Beyond first-order tweedie: Solving inverse problems using latent diffusion. arXiv preprintarXiv:2312.00852, 2023. Litu Rout, Negin Raoof, Giannis Daras, Constantine Caramanis, Alex Dimakis, and Sanjay Shakkottai.Solving linear inverse problems provably via posterior sampling with latent diffusion models.Advances in Neural Information Processing Systems, 36, 2024.",
  "Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of thehessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistictext-to-image diffusion models with deep language understanding. Advances in Neural InformationProcessing Systems, 35:3647936494, 2022. Tomohiro Sasaki, Koki Ho, and E Glenn Lightsey. Nonlinear spacecraft formation flying usingconstrained differential dynamic programming. In Proceedings of AAS/AIAA AstrodynamicsSpecialist Conference, 2022.",
  "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.Advances in neural information processing systems, 32, 2019": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and BenPoole. Score-based generative modeling through stochastic differential equations. In InternationalConference on Learning Representations, 2021. URL Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, AdamFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond theimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprintarXiv:2206.04615, 2022.",
  "Yuval Tassa, Nicolas Mansard, and Emo Todorov. Control-limited differential dynamic programming.In 2014 IEEE International Conference on Robotics and Automation (ICRA), pages 11681175.IEEE, 2014": "Emanuel Todorov and Weiwei Li. A generalized iterative lqg method for locally-optimal feedbackcontrol of constrained nonlinear stochastic systems. In Proceedings of the 2005, American ControlConference, 2005., pages 300306. IEEE, 2005. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser:Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):31423155, 2017.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: The paper discloses all hyperparameters and implementation details in theappendix.",
  "AImpact Statement": "This paper builds on a large body of existing work and presents an improved technique for solvinggeneric nonlinear inverse problems, which can be seen as a generalization of guided diffusionmodeling. Controlling the diffusion process in a generative model has many societal applications,and thus a broad range of downstream impacts. We believe that understanding the capabilities andlimitations of such models in a public forum and open community is essential for practical andresponsible integration of these technologies with society. However, the ideas presented in this work,as well as any other work in this field, must be deployed with caution to the inherent dangers of thesetechnologies.",
  "BDeriving the Iterative Linear Quadratic Regulator (iLQR)": "Differential Dynamic Programming (DDP) is a very popular trajectory optimization algorithm thathas a rich history of theoretical results Jacobson as well as successful practical applicationsin robotics Tassa et al. , aerospace Houghton et al. , Sasaki et al. andbiomechanics Todorov and Li . It falls under the class of indirect methods for trajectoryoptimization, wherein Bellmans principle of optimality defines the so-called optimal value-functionwhich in turn can be used to determine the optimal control. This is in contrast to so-called directmethods which cast the problem at hand into a nonlinear constrained optimization problem.",
  "To formulate an optimal control algorithm we first define the state transition function of a dynamicalsystem asxt1 = h(xt, ut).(33)": "The next ingredient that we need for our optimal control approach is a cost function J(xt, ut) R.This is used to define a performance criterion that iLQR can optimize with respect to the set ofcontrols {ut}t=1t=T (i.e., the control trajectory going backwards from time t = T to t = 1). Thecost-function is defined as follows:",
  "Quu = uu + hTuV xxhu,(44)": "where hxt and hut are the Jacobians of the dynamics function h(xt, ut), evaluated at time step t,w.r.t the state and the control vectors respectively. For ease of notation, we have dropped the subscriptt and therefore all derivatives above should be considered to be evaluated at time step t, while we useV x and V xx above to indicate the gradient and hessian of the Value-function evaluated at the nexttime step (i.e., at time step t 1).",
  "CProofs": "Theorem 4.1. Let Eq. 3 be the discretized sampling equation for the diffusion model with outputperturbation mode control (Eq. 18). Moreover, let the terminal cost0(x0) = log p(y|x0)(27)be twice-differentiable and the running costst(xt, ut) = 0.(28)Then the iterative linear quadratic regulator with Tikhonov regularizer produces the controlut = xt log p(y|x0).(29)",
  "DImplementation": "For all experiments, we use publicly available datasets and pre-trained model weights. For the FFHQ256 256 experiments, we use the last 1K images of the dataset for evaluation. For MNIST, we donot use images directly in the inverse classification task. The images were only used for training thepretrained diffusion model. For models, we used the pretrained weights from Chung et al. [2023a] for FFHQ 256256 tasks, andthe Hugging Face 1aurent/mnist-28 diffusion model for MNIST experiments. No further trainingis performed on any models. Further hyperparameters can be found in . For the classifierp(y|x) in MNIST class-guided classification, we use a simple convolutional neural network with twoconvolutional layers and two MLP layers, trained on the entire MNIST dataset.",
  ": Ablative study on the effect of rank in the low rank and matrix-free approximations onperformance (LPIPS, PSNR, SSIM, NMSE) of our proposed model on the FFHQ 256x256-1K datasetdataset": "Randomized Low-Rank ApproximationThe first and second order terms in Eqs. (19-25) arecorresponding Taylor expansions of deep neural functions. Even with the use of automatic differen-tiation libraries, the formation of these matrices is incredibly expensive, requiring at least dim(x)backpropagation passes (where dim(x) 39B in some experiments). To reduce the cost of com-puting these matrices, we utilize their known low rank structure Sagun et al. , Oymak et al.. Leveraging advanced techniques in randomized numerical linear algebra, we estimate Eqs. (19-25)using randomized SVD Halko et al. . For any matrix A Rmn this is a four step process.1) We sample a random matrix N(0, Ink). 2) We obtain A = Y Rmk. 3) We form abasis over the columns of Y, e.g. by taking the Q matrix in a QR factorization QR = Y. 4) Weapproximate A QT QA. Notably, we observe that when A is a Jacobian (or Hessian) matrix, it can be approximated purelythrough Jacobian-vector and vector-Jacobian (Hessian-vector and vector-Hessian, resp.) products without ever materializing A itself. Moreover, a key result in randomized linear algebra is that thisalgorithm can approximate A up to accuracy O(mnkk+1) (Theorem 1.1 in Halko et al. ).Notably, if A has low rank structure where k such that the k + 1th singular value k+1 = 0, thenthe approximation is exact. Matrix-Free EvaluationInspired by matrix-free techniques in numerical optimization Knoll andKeyes , we demonstrate a strategy for forming the action update (15) without materializingthe costly dim(x) dim(x) matrices in the iLQR algorithm (19-25), which we shall denote as an",
  "LPIPS PSNR SSIM MSE LPIPS PSNR SSIM MSE": "= 0-------- = 1e 70.17327.490.794115.90.05031.800.87942.96 = 1e 40.17127.450.792117.00.05331.840.88242.57 = 10.17227.430.799117.50.05031.850.89142.47 from Lemma 4.20.17027.440.788117.30.05131.860.88042.44 : Ablative study on the effect of the Tikhonov regularization coefficient on performance(LPIPS, PSNR, SSIM, NMSE) of our proposed model on the FFHQ 256x256-1K dataset dataset. Noresults are reported for = 0, as the algorithm encountered numerical precision errors during matrixinversion.",
  "D.2Computational Complexity Analysis": "Incorporating all three modifications, we can provide a realistic runtime and space complexity analysisof our presented algorithm with respect to the rank k, the data dimension d, diffusion steps m, andnumber of iLQR iterations n. Combining both the low rank and matrix-free approximations, we obtain the updated equations forinput mode perturbation (where projection matrices are written as P to avoid overloading the Qfunction notation):",
  "PQuuPT = PuuPT + PhTx V xxhxPT .(84)": "To simplify notation, each projection matrix P is the same in reality, this need not be the case.Note that Qx and Qu are simply of size d and therefore image-sized. For all our datasets, these eachtake 0.2 MB to store and are therefore negligible, and we do not project these variables. When uu isdiagonal (as it is in our case), we can obtain the projected inverse for Quu as PQ1uuPT = P1uuPT + P1uuPT (C1 + PT 1uuP)1P1uuPTwhere C = PhTx V xxhxP(85)via a direct application of the Woodbury matrix inversion formula Petersen et al. , which hascost O(k3 + kd2). Finally, we compute the projected updates Vxx, K as well as the full-precision",
  "PVxxPT = PQxxPT PKT PT PQuuPT PKPT .(89)": "Where applicable, we leverage vector-Jacobian products from standard automatic differentiation li-braries (e.g. torch.func.vjp) which have runtime complexity O(1). Computing the Vx, Vxx, k, Kterms in Eqs. (46)-(49) costs O(k3 + kd2) FLOPs in terms of matrix multiplications (dominated bythe matrix inverse of k k matrix qT Quuq). Crucially, it incurs O(k) neural function evaluations(NFEs), which dominates the runtime of the algorithm. Since this computation is performed for eachdiffusion step and iLQR iteration, the total runtime complexity of our algorithm is O(nm(k3 + kd2))matrix multiplication FLOPs and O(nmk) NFEs, with O(mk2 + d) space complexity. In terms oftime complexity, the NFEs are the dominating cost, accounting for 97% of computation time.",
  "D.3Sensitivity to Hyperparameters": "In Tables 4, 5, 6, we investigate the effect of the rank of the low rank approximation and matrix-freeprojections, the Tikhonov regularization coefficient , and the diffusion time T on the performanceof our method on the FFHQ 256x256 dataset. We evaluate performance on the super-resolution andrandom inpainting tasks, with the same setup as in . Low-Rank and Matrix-Free RankFrom , it is clear that there is a significant performancegain from even a rank one approximation of the first- and second-order matrices. The gains fromsubsequent increases in the rank approximation diminish quickly. This is because increasing the rankof the approximation only improves the approximation of the second-order terms. The first orderVx, Qx, Qu terms are always modeled exactly in O(1) time per iteration due to their amenability tovector-Jacobian products. From Theorems 4.1-4.3 we see that even when the second order terms arezero (i.e., the result of assumption t = 0), we exactly recover the true posterior sampler. Therefore,the second-order terms are less important, though still useful for imposing a quadratic trust-regionregularization to the algorithm. Therefore, we ultimately choose k = 1 for three reasons:",
  ". subsequent increases in k have a minimal effect on the performance of the algorithm": "Tikhonov Regularizer demonstrates that our algorithm is relatively robust to the Tikhonovregularization parameter, except when = 0. Under this condition, any ill-conditioning of Quuresults in division by zero errors, resulting in the failure of the algorithm. Therefore, we simplychoose to let = 1e 4, since the effect of Tikhonov regularizer is minimal. Diffusion StepsFinally, we observe in that increasing the diffusion time results in higherquality samples though at the cost of increased computation time. Therefore, choice of T requiresbalancing computational cost and sample quality, and is ultimately highly user-dependent. When thecomputational and latency budget is relatively high, large T can be used to improve sample quality.Conversely, when this budget is low, we find that even T = 20 provides reasonable samples."
}