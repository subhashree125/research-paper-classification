{
  "Abstract": "Recently, there has been a growing interest in time series foundation models thatgeneralize across different downstream tasks. A key to strong foundation models isa diverse pre-training dataset, which is particularly challenging to collect for timeseries classification. In this work, we explore the performance of a contrastive-learning-based foundation model as a function of the data used for pre-training.We introduce contrastive accuracy, a new measure to evaluate the quality of therepresentation space learned by the foundation model. Our experiments reveal thepositive correlation between the proposed measure and the accuracy of the modelon a collection of downstream tasks. This suggests that the contrastive accuracycan serve as a criterion to search for time series datasets that can enhance thepre-training and improve thereby the foundation models generalization.",
  "Introduction": "Nowadays, the success of large foundation models (Bommasani et al., 2021) such as GPT-4 (Achiamet al., 2023) or Llama (Touvron et al., 2023) cause dramatic changes in the research and applications.Instead of training a tailored model for a particular task, the foundation model is pre-trained on acollection of datasets with the goal to generalize simultaneously across various downstream taskseither by fine-tuning the model or directly using its output. This workflow effectively simplifies thechoice of the model architecture while reducing the requirement for amount of labeled data. Thewave of foundation models has now reached the time series domain, including forecasting (Rasulet al., 2023a; Woo et al., 2024) and classification models (Lin et al., 2024). When it comes to real deployment of a time series foundation model (TSFM), a very importantquestion is whether the pre-training dataset is sufficiently diverse, so the model generalizes well tonew downstream tasks. Usually, this is verified by directly evaluating the performance on severalsupervised downstream tasks thereby requiring availability of annotated data samples and introducinga high cost for assesing the quality of different pre-trained datasets. Therefore, in this paper, we askthe following research question:",
  "Can we evaluate the quality of pre-training data in an unsupervised manner?": "By focusing on time series classification and contrastive pre-training, we show that it is possible toleverage important information from the representation space learned by the TSFM. More precisely,if new data points are not similar to pre-training data, their embeddings will tend to not satisfy theuniformity property of contrastive learning (Wang and Isola, 2020), directly impacting the foundationmodels performance. Based on this observation, we introduce a new metric called contrastiveaccuracy that evaluates how well spread the data points are in the embedding space. We empiricallyshow that the proposed metric positively correlates with the models accuracy on the tasks unseenduring pre-training, allowing us to use it for measuring the quality of pre-training examples withoutthe need to regularly test the TSFM on supervised downstream tasks.",
  "Related Work": "Time series representation learning has got a high attention in recent years, resulting in a wealthof excellent works. Many of them use the contrastive learning scheme for pre-training, includingTS2Vec (Yue et al., 2022) and TS-TCC (Eldele et al., 2021) with a CNN backbone and TF-C (Zhanget al., 2022) with a ResNet backbone, demonstrating good performance on classification datasets, e.g,the UCR collection (Dau et al., 2019). With the success of large language models (LLMs), popularity of the transformer architecture for timeseries analysis increases (Nie et al., 2023; Ilbert et al., 2024), while the development of foundationmodels is becoming a prevalent task. Numerous TSFMs have been proposed recently, includingLag-Llama (Rasul et al., 2023b), Time-LLM (Jin et al., 2023), One Fits All (Zhou et al., 2023),Brant (Zhang et al., 2023), MOIRAI (Woo et al., 2024). While some of them try to adapt LLM fortime series data, others train foundation models from scratch on a large volume of time series data. Inour work, we are based on the latter approach, studying how to evaluate the quality of pre-trainingdata, which, to our knowledge, has not been addressed before. Finally, we would like to notice that our framework reminds unlabeled performance estimation(Donmez et al., 2010) where the goal is to predict models performance on unlabeled test data.However, most of these approaches focus on single-task classification for computer vision problems(Hendrycks and Gimpel, 2016; Yu et al., 2022; Xie et al., 2024), implying a fundamentally differentmethodology. To the best of our knowledge, time series foundation models have never been consideredin this domain before.",
  "Problem Setup": "We consider the task of unsupervised pre-training where an unlabeled pre-training set X0 is givenwith m pre-training sequences of length t. The goal is to design a foundation model F : Rt Rdhidthat projects any time series x Rt to a discriminative hidden space Rdhid. In this work, the qualityof the foundation model is evaluated on a collection of downstream tasks D = {Di}pi=1, where Diconsists of observations Xi and labels Yi. For each downstream task, we perform a train-test split,append F with a linear head, fine-tune it on the training set and compute the accuracy score on the testset. The performance of F with a pre-training dataset X0 that is averaged over all downstream tasksis denoted by Ptest(X0). Similarly, Ptrain(X0) denotes the average performance when it is evaluatedon the training set.",
  "Architecture and Pre-training": "Similarly to Nie et al. (2023) and Lin et al. (2024), we use the ViT (Dosovitskiy et al., 2021) as thebackbone, but our implementation slightly differs from theirs. Instead of dividing a sequence intodisjoint patches, we employ a single CNN layer allowing them to be overlapped. Similarly to otherTSFMs, we reshape any time series to a fixed sequence length equal to 512. More details on thearchitecture are given in Appendix A. For pre-training of the foundation model, we use the contrastive learning that aims to train a suchencoder that outputs similar representations for two random augmentations of the same sample(positive pair) and dissimilar representations for augmentations of two different samples (negativepair). More formally, let T be a considered space of transformations (augmentations) such that T , x X we have (x) X. In our experiments, we have considered the RandomCropResizewith a crop length varying from 70% to 80%. To measure similarity of two embeddings, we firstproject the output of the foundation model F(x) to a lower dimension using a MLP projector",
  "Contrastive Accuracy": "It has been shown that a good representation learned by contrastive learning should satisfy uniformityproperty, i.e., to have a feature distribution that tends to be uniform on the unit hypersphere in orderto preserve maximal information (Wang and Isola, 2020). Based on this observation, we introducethe contrastive accuracy metric (CA, denoted by Acon), that measures how scattered the embeddingsof the evaluation examples X = {xi}ni=1 are in the representation space. Specifically, we counthow many examples have two embeddings (obtained from the two augmentations) to be the nearestneighbors with respect to the other examples in the dataset:",
  ",": "where X0 denotes the pre-training dataset on which the foundation model F was trained. When thedataset size n is large, we split the data into disjoint batches and evaluate si(, ) only on thoseexamples that belong to the same batch as i. Further, we will experimentally show that the contrastiveaccuracy is able to hint what generalization performance on the downstream tasks we can expectfrom the foundation model pre-trained on X0.",
  "Correlation with Performance": "In our first experiment, we show that the contrastive accuracy is able to correlate with the general-ization performance of the pre-training model. Given a pre-training data X0, we randomly draw asubsample X(r%)0, which contains r% examples of X0, then evaluate A(X0)con (X(r%)0) and compare itwith the train and the test performance Ptrain(X(r%)0) and Ptest(X(r%)0). As X0, we have picked oneof the four largest datasets in the UCR (ElectricDevices, Crop, FordB, and FordA), and evaluate theperformance on the rest 127 datasets in average. illustrates the results for ElectricDeviceswhen varying r [10%, 100%], and the other results can be found in Appendix B. We can observethat the contrastive accuracy correlates well with the performance, approximating its growth withincreasing subsampling ratio. This allows us to perform model selection without directly testingthe pre-training model on the downstream tasks. For example, this experiment may help to identifysufficient number of pre-training examples per dataset when the model is trained on a collection ofdifferent pre-training datasets. 0.500.550.600.650.700.750.80 Contrastive Accuracy (CA, Acon) 0.62 0.64 0.66 0.68 0.70 0.72 0.74",
  "Improvement Prediction": "In this experiment, in addition to the pre-training dataset X0, we consider having another one Xnew0and ask whether it is possible to predict the performance improvement from including X0 to thepre-training data, i.e., P(X0, Xnew0) := P(X0 Xnew0) P(X0). For this, we propose to considerAcon(X0, Xnew0) := A(Xnewcon0 )(Xnew0) A(Xnewcon0 )(X0) and measure its correlation with P(X0, Xnew0).For this experiment, we split the UCR collection into two disjoint sets denoted by C = {X(i)0 }12i=1 andD = {Xi}128i=13, where the latter is used to evaluate the performance as described in .1 (moredetails are given in Appendix B). In the first experiment, we fix the initial pre-training dataset X0 bytaking one from C and vary Xnew0within C \\{X0}. In the second experiment, we fix Xnew0and vary X0in the similar way. For each pair (X0, Xnew0 , we compute P(X0, Xnew0) and Acon(X0, Xnew0) andplot the results for the two experiments in for AllGestureWiimoteX dataset and in AppendixB for the rest 11 datasets. In , we can observe a positive correlation between the difference incontrastive accuracy and the performance improvement suggesting that the proposed unsupervisedcriterion can help to search data that can be included to the pre-training dataset in order to improvethe representaion and thereby the generalization performance of the foundation model. 0.00.20.40.60.8 Contrastive acc improvement, Acon (X0, X(new)0) 0.02 0.00 0.02 0.04 0.06 Perf Improvement (X0, X(new)0) =0.74, pval=0.0088 Fix X0, Vary X(new)0 Fit95% Prediction Limits95% Confidence Limits 0.360.380.400.420.440.460.480.50 Contrastive Acc Improvement, Acon (X0, X(new)0) 0.1 0.0 0.1 0.2 0.3 =0.75, pval=0.0084 Vary X0, Fix X(new)0 AllGestureWiimoteX",
  "Conclusion and Future Work": "In this paper, we studied the task of evaluating the effect of pre-training data on the foundationmodels performance. We proposed the contrastive accuracy and experimentally showed its promiseas a criterion to select pre-training data. As a future work, we would like to test our approach withlarger pre-training datasets and explore the limits of contrastive pre-training for time series data.Particularly, unlike in computer vision, there is still an open question regarding which augmentationtechniques are relevant for contrastive learning in time series data and what their impact is. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt,J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg,J., Bosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation models.arXiv preprint arXiv:2108.07258. Dau, H. A., Bagnall, A., Kamgar, K., Yeh, C.-C. M., Zhu, Y., Gharghabi, S., Ratanamahatana, C. A.,and Keogh, E. (2019). The ucr time series archive. IEEE/CAA Journal of Automatica Sinica,6(6):12931305.",
  "Donmez, P., Lebanon, G., and Balasubramanian, K. (2010). Unsupervised supervised learningi: Estimating classification and regression errors without labels. Journal of Machine LearningResearch, 11(4)": "Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Eldele, E., Ragab, M., Chen, Z., Wu, M., Kwoh, C. K., Li, X., and Guan, C. (2021). Time-seriesrepresentation learning via temporal and contextual contrasting. In Proceedings of the ThirtiethInternational Joint Conference on Artificial Intelligence, IJCAI-21, pages 23522359. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. (2020). Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 97299738.",
  "Hendrycks, D. and Gimpel, K. (2016). A baseline for detecting misclassified and out-of-distributionexamples in neural networks. arXiv preprint arXiv:1610.02136": "Ilbert, R., Odonnat, A., Feofanov, V., Virmaux, A., Paolo, G., Palpanas, T., and Redko, I. (2024).SAMformer: Unlocking the potential of transformers in time series forecasting with sharpness-aware minimization and channel-wise attention. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller,A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the 41st InternationalConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,pages 2092420954. PMLR. Jin, M., Wang, S., Ma, L., Chu, Z., Zhang, J. Y., Shi, X., Chen, P.-Y., Liang, Y., Li, Y.-F., Pan, S.,et al. (2023). Time-llm: Time series forecasting by reprogramming large language models. arXivpreprint arXiv:2310.01728. Lin, C., Wen, X., Cao, W., Huang, C., Bian, J., Lin, S., and Wu, Z. (2024). Nutime: Numericallymulti-scaled embedding for large- scale time-series pretraining. Transactions on Machine LearningResearch.",
  "Oord, A. v. d., Li, Y., and Vinyals, O. (2018). Representation learning with contrastive predictivecoding. arXiv preprint arXiv:1807.03748": "Rasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilo, M.,Ghonia, H., Hassen, N. V., Schneider, A., et al. (2023a). Lag-llama: Towards foundation modelsfor time series forecasting. arXiv preprint arXiv:2310.08278. Rasul, K., Ashok, A., Williams, A. R., Khorasani, A., Adamopoulos, G., Bhagwatkar, R., Bilo, M.,Ghonia, H., Hassen, N. V., Schneider, A., et al. (2023b). Lag-llama: Towards foundation modelsfor time series forecasting. arXiv preprint arXiv:2310.08278. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozire, B., Goyal,N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971. Wang, T. and Isola, P. (2020). Understanding contrastive representation learning through alignmentand uniformity on the hypersphere. In International conference on machine learning, pages99299939. PMLR. Woo, G., Liu, C., Kumar, A., Xiong, C., Savarese, S., and Sahoo, D. (2024). Unified training ofuniversal time series forecasting transformers. In Salakhutdinov, R., Kolter, Z., Heller, K., Weller,A., Oliver, N., Scarlett, J., and Berkenkamp, F., editors, Proceedings of the 41st InternationalConference on Machine Learning, volume 235 of Proceedings of Machine Learning Research,pages 5314053164. PMLR. Xie, R., Odonnat, A., Feofanov, V., Deng, W., Zhang, J., and An, B. (2024). Mano: Exploit-ing matrix norm for unsupervised accuracy estimation under distribution shifts. arXiv preprintarXiv:2405.18979.",
  "AArchitecture and Implementation Details": "In this paper, similarly to Nie et al. (2023) and Lin et al. (2024), we employ the ViT (VisionTransformer) as the backbone. Our goal is to retain information effectively, so we simultaneouslyuse both overlapping and non-overlapping patches. Here is how we achieve it: we apply a one-layer1D-CNN to handle the overlapping patches and use mean pooling to transform their embeddingsto match the number of non-overlapping patches. Next, we embed the and values from thenon-overlapping patches and concatenate the output with the overlapping patches. This result togetherwith the class (CLS) token is then fed into the transformer. The entire network framework is depictedin . Throughout all our experiments, we maintain the same model parameters. The chosen parametervalues for different layers are outlined in . To better train the model, we apply a linear learningrate warm-up in the first 10 epochs (Loshchilov and Hutter, 2016) and a cosine learning rate decay inthe subsequent epochs.",
  "Phoneme": "0.450.500.550.600.650.700.75 0.1 0.0 0.1 0.2 0.3 =0.79, pval=0.0036 ScreenType 0.500.550.600.650.700.750.800.850.90 0.1 0.0 0.1 0.2 0.3=0.77, pval=0.006 UWaveGestureLibraryX 0.40.50.60.70.8 Contrastive acc improvement, Acon (X0, X(new)0) 0.1 0.0 0.1 0.2 0.3 Perf Improvement (X0, X(new)0) =0.81, pval=0.0024 WordSynonyms Fit95% Prediction Limits95% Confidence Limits 0.20.30.40.50.6 Contrastive acc improvement, Acon (X0, X(new)0) 0.1 0.0 0.1 0.2 0.3 =0.75, pval=0.008 WormsTwoClass 0.20.10.00.10.2 Contrastive acc improvement, Acon (X0, X(new)0) 0.2 0.1 0.0 0.1 0.2=0.56, pval=0.073",
  "Yoga": "Fix X0, Vary X(new)0 : The correlation () between the improvement in contrastive accuracy and the performanceimprovement on 116 UCR datasets when expanding the pre-training dataset X0 by including X(new)0.To measure correlation, we fix X0 and vary X(new)0across 11 datasets. 0.360.380.400.420.440.460.480.50 0.1 0.0 0.1 0.2 0.3 Perf Improvement (X0, X(new)0) =0.75, pval=0.0084 AllGestureWiimoteX 0.450.500.550.600.650.700.750.800.85 0.2 0.1 0.0 0.1 0.2 0.3=0.79, pval=0.0036 CricketY 0.550.600.650.700.75 0.2 0.1 0.0 0.1 0.2=0.56, pval=0.07 EOGVerticalSignal 0.400.350.300.250.200.150.100.050.00 0.10 0.05 0.00 0.05 0.10 0.15 Perf Improvement (X0, X(new)0) =0.50, pval=0.11"
}