{
  "Abstract": "We introduce INQUIRE, a text-to-image retrieval benchmark designed to challengemultimodal vision-language models on expert-level queries. INQUIRE includesiNaturalist 2024 (iNat24), a new dataset of five million natural world images,along with 250 expert-level retrieval queries. These queries are paired with allrelevant images comprehensively labeled within iNat24, comprising 33,000 totalmatches. Queries span categories such as species identification, context, behavior,and appearance, emphasizing tasks that require nuanced image understandingand domain expertise. Our benchmark evaluates two core retrieval tasks: (1)INQUIRE-FULLRANK, a full dataset ranking task, and (2) INQUIRE-RERANK, areranking task for refining top-100 retrievals. Detailed evaluation of a range ofrecent multimodal models demonstrates that INQUIRE poses a significant challenge,with the best models failing to achieve an mAP@50 above 50%. In addition, weshow that reranking with more powerful multimodal models can enhance retrievalperformance, yet there remains a significant margin for improvement. By focusingon scientifically-motivated ecological challenges, INQUIRE aims to bridge the gapbetween AI capabilities and the needs of real-world scientific inquiry, encouragingthe development of retrieval systems that can assist with accelerating ecologicaland biodiversity research.",
  "Introduction": "Recent advances in multimodal learning have resulted in advanced models [60; 43; 3] that demonstrateremarkable generalization capabilities in zero-shot classification [60; 83], visual question-answering(VQA) [39; 80; 4; 40], and image retrieval [80; 40]. These models offer the potential to assist in theexploration, organization, and extraction of knowledge from large image collections. However, despitethis success, there remains a significant gap in the evaluation of these models on domain-specific,expert-level queries, where nuanced understanding and precise retrieval are critical. Addressingthis gap is essential for future deployment in specialized fields such as biodiversity monitoring andbiomedical imaging, among other scientific disciplines. Previous studies of the multimodal capabilities of this new generation of models have primarilyfocused on the task of VQA. In VQA, it has been demonstrated that there remains a large performancegap between state-of-the-art models and human experts in the context of challenging perception andreasoning queries such as those found on college-level exams [81; 84]. However, no such expert-levelbenchmark exists for image retrieval. The most commonly used text-to-image retrieval benchmarksare derived from image captioning datasets, and contain simple queries related to common everydaycategories [79; 42]. Current multimodal models achieve near perfect performance on some of thesebenchmarks, indicating that they no longer pose a challenge (e.g., BLIP-2 scores 98.9 onFlickr30K top-10). Existing retrieval datasets are generally small [58; 59; 79; 42], limited to asingle visual reasoning task (e.g., landmark-location matching [58; 59; 74]), and lack concepts thatwould require expert knowledge [58; 59; 74; 79; 42]. These limitations impede our ability to trackand improve image retrieval capabilities. A domain that is well-suited for studying this problem is the natural world, where images collected byenthusiast volunteers provide vast and largely uncurated sources of publicly available scientific data.In particular, the iNaturalist platform contains over 180 million species images and contributesimmensely to research in biodiversity monitoring [16; 48]. These images also contain a wealth ofsecondary data not reflected in their species labels , including crucial insights into interactions,behavior, morphology, and habitat that could be uncovered through searches. However, the time-consuming and expert-dependent analysis needed to extract such information prevents scientists fromtaking advantage of this valuable data at scale. This cost is amplified as scientists typically want toretrieve multiple relevant images for each text query, so that they can track changes of a propertyover space and time . This domain serves as an ideal testbed for expert image retrieval, as theseimages contain expert-level diverse and composite visual reasoning problems, and progress in thisfield will enhance impactful scientific discovery. In this work, we introduce INQUIRE, a new dataset and benchmark for expert-level text-to-imageretrieval and reranking on natural world images. INQUIRE includes the iNat24 dataset and 250ecologically motivated retrieval queries. The queries span 33,000 true-positive matches, pairing eachtext query with all relevant images that we comprehensively labeled among iNat24s five millionnatural world images. iNat24 is sampled from iNaturalist , and contains images from 10,000different species collected and annotated by citizen scientists, providing significantly more data forresearchers interested in fine-grained species classification. The queries contained within INQUIREcome from discussions and interviews with a range of experts including ecologists, biologists,ornithologists, entomologists, oceanographers, and forestry experts.",
  "INQUIRE5,000,00025011.5k": "Our evaluation of multimodal retrieval methods demonstrates that INQUIRE poses a significantchallenge, necessitating the development of models able to perform expert-level retrieval within largeimage collections. A key finding from our experiments is that reranking, a technique typically usedin text retrieval [54; 35; 34], offers a promising avenue for improvement in image retrieval. Wehope that INQUIRE will inspire the community to build next-generation image retrieval methodstowards the ultimate goal of accelerating scientific discovery. We make INQUIRE, the iNat24dataset, pre-computed outputs from state-of-the-art models, and code for evaluation available at",
  "Related Work": "Vision-Language Models (VLMs). Large web-sourced datasets containing paired text and imageshave enabled recent advances in powerful VLMs [15; 85]. Contrastive methods such as CLIP and ALIGN , among others, learn an embedding space where the data from the two modalitiescan be encoded jointly. The ability to reason using natural language and images together has yieldedimpressive results in a variety of text-based visual tasks such as zero-shot classification [60; 83],image captioning [39; 80; 4; 30; 40], and text-to-image generation [53; 7; 61; 64; 8]. However, theeffectiveness of these contrastive VLMs for more complex compositional reasoning is bottleneckedby the information loss induced by their text encoders . There also exists a family of more computationally expensive VLMs that connect the outputs ofvisual encoders directly into language models. Models like LLaVA [43; 44], BLIP [39; 40; 21], andGPT-4o [3; 55] have demonstrated impressive vision-language understanding. However, despitetheir potential for answering complex vision-language queries, these models are not suitable forprocessing large sets of images at interactive rates, which is essential for retrieval, due to their largecomputational requirements during inference. In this paper, we do not introduce new VLMs, but aimto better understand the capabilities and shortfalls of existing methods for text-to-image retrieval. Image Retrieval. Effective feature representations are essential for achieving strong image retrievalperformance. Earlier approaches from image-to-image used hand-crafted features [49; 12] but thesehave largely been replaced with deep learning-based alternatives [36; 9; 6; 11]. More recently, in thecontext of text-to-image retrieval, we have seen the adoption of contrastive VLMs [60; 32] trained onweb-sourced paired text and image datasets. These models enable zero-shot text-based retrieval andhave been demonstrated to exhibit desirable scaling properties as training sets become larger [26; 24].However, despite the potential of VLMs for image retrieval, their evaluation has been mostly limitedto small datasets adapted from existing image captioning benchmarks, such as Flickr30k andCOCO , which contain just 1,000 and 5,000 images, respectively. Furthermore, recent models aresaturating performance on these less challenging datasets, e.g., BLIP-2 scores 98.9 on Flickr30Kand 92.6 on COCO top-10 text-to-image retrieval. As most text-to-image benchmarks have beenderived from image captioning datasets, each query is a descriptive caption that matches exactly oneimage. In contrast, real-world retrievals often involve multiple images relevant to a single query, andthe query itself typically does not describe every aspect of the images as thoroughly as a caption does.We compare INQUIRE to common text-to-image retrieval datasets in .",
  ": The INQUIRE benchmark consists of a full-dataset ranking task and a reranking tasktargeting different aspects of the image retrieval problem": "granularity . created a retrieval dataset for camera trap images, but use image captions thatwere automatically generated from a small set of discrete image attributes, limiting their utility beyondthis set. The problem of fine-grained retrieval, where there may only be subtle visual differencesbetween concepts of interest, has also been explored extensively . However, typically thesedatasets convert existing classification datasets to the retrieval setting, resulting in small image poolsand limited query diversity. INQUIRE addresses these shortcomings with a considerably larger imageset and fine-grained queries that require advanced image understanding and domain expertise. Reranking. In text retrieval, a common workflow is to first efficiently obtain an initial ranking ofdocuments using pre-computed text embeddings and then rerank the top retrievals with a more costlybut sophisticated model [54; 35; 34]. While VLMs like CLIP enable efficient image retrieval andmore expensive models such as GPT-4o could perform more complex ranking, this workflowhas not been extensively explored in text-to-image applications primarily due to a lack of evaluationdatasets. To this end, INQUIRE introduces a reranking challenge to drive further progress on this task. Expert-level Benchmarks. Visual classification benchmarks have evolved from simply containingcommon everyday categories [63; 42] to having more expert-level concepts [68; 69]. Challengingdatasets like the iNaturalist benchmarks [69; 70], contain large class imbalances and fine-grainedconcepts that require expert-level knowledge to identify. The NeWT benchmark from is similarin spirit to INQUIRE in that it proposes a collection of natural world questions. However, NeWT is aset of binary classification challenges, and while there are a variety of tasks, the majority of themare standard species classification. Further, NeWT uses a small (200400) fixed set of positive andnegative labeled images for each task, so it is not suitable for evaluating retrieval. In general, evaluation benchmarks have struggled to keep pace with the growing capabilities ofrecent large models which perform very well on them . For language, specific datasets have beendeveloped to challenge common sense reasoning abilities [52; 14]. Multimodal datasets have alsobeen proposed to assess vision-language capabilities [75; 38; 46; 77]. Nevertheless, these benchmarkstest general skills in tasks that are not particularly challenging for humans and thus, are not testing amodels abilities in scenarios where expert-level knowledge is required. To address the need for more difficult benchmarks, recent expert-level benchmarks have been devisedfor LLMs [28; 84] and multimodal models [81; 51]. For instance, MMMU features questionsthat cover a range of college-level disciplines while Encyclopedic-VQA comprises visualquestions related to fine-grained entities which demand encyclopedic knowledge. The relatively lowperformance on these benchmarks, compared to human performance, highlights current limitationsin multimodal models. However, there is no equivalent expert-level dataset for fine-grained text-to-image retrieval. INQUIRE fills this gap by providing a set of challenging and visually fine-grainedretrieval questions focused on real-world tasks in retrieval from natural world image collections.",
  "The INQUIRE Benchmark": "Here we describe INQUIRE, our novel benchmark for assessing expert-level image retrieval for fine-grained queries on natural world image collections. INQUIRE consists of a collection of 250 queries,where each query is represented as a brief text description of the concept of interest (e.g., Alligatorlizards mating\" ), and contains its relevant image matches comprehensively labeled over a datasetof five million natural world images. These queries represent real scientific use cases collected to coverdiverse, expert sources including discussions with scientists across environmental and ecological disciplines. Several examples of our queries are illustrated in , with more in Appendix J. Ourqueries challenge retrieval methods to demonstrate fine-grained detail recognition, compositionalreasoning, character recognition, scene understanding, or natural world domain knowledge. Whilequeries can require expert-level knowledge, the information needed to solve them is publicly availableonline and thus feasible for large web-trained models to learn. In this section, we detail the datasources utilized for the construction of INQUIRE, describe the data collection process, and introducetwo image retrieval tasks INQUIRE-FULLRANK and INQUIRE-RERANK that address differentaspects of real-world text-to-image retrieval.",
  "The iNaturalist 2024 Dataset": "As part of the INQUIRE benchmark, we create a new image dataset, which we refer to as iNaturalist2024 (iNat24). This dataset contains five million images spanning 10,000 species classes collectedand annotated by community scientists from 20212024 on the iNaturalist platform . iNat24 formsone of the largest publicly available natural world image repositories, with twice as many images asin iNat21 . To ensure cross-compatibility for researchers interested in using both datasets, iNat24and iNat21 have the same classes but do not contain the same images, freeing iNat21 to be used as atraining set. The sampling and collection process of iNat24 is in Appendix H.",
  "Query and Image Collection Process": "Query Collection. To ensure that INQUIRE comprises text queries that are relevant to scientists,we conducted interviews with individuals across different ecological and environmental domains -including experts in ornithology, marine biology, entomology, and forestry. Further queries weresourced from reviews of academic literature in ecology . Representative queries and statistics canbe seen in Figures 1, 2, and 3. We retained only queries that (1) could be discerned from imagesalone, (2) were feasible to comprehensively label over the entire iNat24 dataset, and (3) were ofinterest to domain experts. Image Annotation.All image annotations were performed by a small set of individuals whoseinterest and familiarity with wildlife image collections enabled them to provide accurate labels forchallenging queries. Annotators were instructed to label all candidate images as either relevant (i.e.,positive match) or not relevant (i.e., negative match) to a query, and to mark an image as not relevantif there was reasonable doubt. To allow for comprehensive labeling, where applicable, iNat24 specieslabels were used to narrow down the search to a sufficiently small size to label all relevant images forthe query of interest. For queries in which species labels could not be used, labeling was performedover the top CLIP ViT-H-14 retrievals alone. In this case, the resulting annotations were onlykept if we were certain that this labeling captured the vast majority of positives, including labelinguntil at least 100 consecutive retrievals were not relevant (see Appendix H). Queries that were deemedtoo easy, not comprehensively labeled, or otherwise not possible to label were excluded from ourbenchmark. In total, this process resulted in 250 queries which involved labeling 194,334 images, ofwhich 32,696 were relevant to their query. Further details are in Appendix H. Query Categories.Each query belongs to one of four supercategories (appearance, behavior,context, or species), and further into one of sixteen fine-grained categories (e.g., Animal Structuresand Habitats). shows the distribution of query categories, and shows the distributionof iconic groups of the species represented by each query (e.g., Mammals, Birds). We also notequeries that use scientific terminology, words typically used only within scientific contexts (e.g., Agodwit performing distal rhynchokinesis).",
  "Retrieval Tasks": "We introduce two tasks to address different aspects of the text-to-image retrieval problem. Real-world retrieval implementations often consist of two stages: an initial top-k retrieval with a morecomputationally efficient method (e.g., CLIP zero-shot using pre-computed image embeddings),followed by a reranking of the top-k retrievals with a more expensive model. To enable researchersto explore both stages, while ensuring that those with more limited computational resources canparticipate, we follow previous large-scale reranking challenges like TREC [19; 20] by offering botha full dataset retrieval task and a reranking task (see ).",
  "WebLI SigLIP": "INQUIRE-FULLRANK. The goal of this task is end-to-end retrieval, starting from the entire fivemillion image iNat24 dataset. Progress on the full retrieval task can be made with better and moreefficient ways to organize, process, filter, and search large image datasets. Although performancewill increase with improvements to either of the two stages in a typical retrieval pipeline, we hopethis task also encourages the development of retrieval systems beyond the two-stage approach. INQUIRE-RERANK. This task evaluates reranking performance from a fixed initial ranking of 100images. We believe that significant progress in retrieval will come from developing better rerankingmethods that re-order an initial retrieved subset. Thus, fixing the starting images for each queryprovides a consistent evaluation of reranking methods. This task also lowers the barrier to entry bygiving researchers a considerably smaller set of top retrievals to work with, rather than requiringthem to implement an end-to-end retrieval system. The top 100 ranked images for each query areretrieved using CLIP ViT-H-14 zero-shot retrieval on the entire iNat24 dataset. Consistent withprevious large-scale reranking challenges [19; 20; 37], we retain only queries for which at least onepositive image is among the top 100 retrieved images and no more than 50% of these top images arerelevant. This ensures that the reranking evaluation remains meaningful and discriminative. Thisfiltering process yields a task subset of 200 queries (reduced from our original 250 queries), split into40 validation and 160 test queries according to the original validation/test split, with and 4,000 and16,000 corresponding images, respectively.",
  "Retrieval Methods": "The goal of text-to-image retrieval is to rank images from a potentially large image collectionaccording to their relevance to an input text query. Here, we describe the retrieval and rerankingmethods that we evaluate, covering current state-of-the-art approaches. Embedding Similarity. Models such as CLIP are well suited for the text-to-image retrievalsetting as they operate on a joint vision and language embedding space. In this setting, similaritybetween an image and text query is simply determined by their cosine similarity. The key advantageof embedding models is that the embedding for each image can be pre-computed once offline asthey do not change over time. At inference time, only the embedding of the text query needs to becomputed and then compared to the cached image embeddings for retrieval. This is helpful as thenumber of images we wish to search over can be on the order of millions, or even billions . Thusto speed up retrieval, the image embeddings can be pre-computed and indexed using approximatenearest neighbor methods , allowing for near-instantaneous retrievals on large collections. This isbeneficial both for end-to-end retrieval and as the first step for a multi-stage retrieval approach. Wealso benchmark recent models such as WildCLIP and BioCLIP which are adapted versionsof CLIP that explicitly target natural world use cases. Reranking with Multimodal Models. Reranking is a common paradigm in text retrieval, where arapid search through pre-computed document indexes for potential matches is followed by a moreexpensive reranking of the top retrievals [54; 35; 34]. In the image domain, reranking has been : Results for the INQUIRE-FULLRANK task using two-stage retrieval. The top-k imagesare retrieved with CLIP ViT-H/14 and then reranked with the selected large multimodal models.Reranking offers a significant avenue of improvement.",
  "GPT-4o 39.653.40.7943.757.90.78": "comparatively rare as the types of datasets for which it can be used are limited. In our experiments,we show that multimodal language models such as LLaVA , VILA , and GPT-4 [3; 55] areeffective rerankers out-of-the-box. To adapt these multimodal models for ranking, which requiresa continuous score for a given text query and image pair, we prompt: Does this image show {somequery}? Answer with Yes\" or No\" and nothing else. (precise prompting details used for each modelcan be found in Appendix I). The logits of the Yes\" and No\" tokens are then used to compute thescore: s = sy/(sy + sn), where sy = exp(logitY es) and sn = exp(logitNo).",
  "Metrics": "We evaluate using Average Precision at k (AP@k), Normalized Discounted Cumulative Gain (nDCG),and Mean Reciprocal Rank (MRR). We primarily discuss AP as we find that this metric is the mostdiscriminative of model performance. While these metrics have been commonly used to evaluatetext retrieval, especially in the context of large-scale document retrieval [71; 19], they have notfound use in image retrieval due to the nonexistence of benchmarks like INQUIRE containing manyrelevant images for retrieval, rather than just one. Thus, we include them in our analysis to encouragetheir use in future image retrieval research. We note that the utilized AP@k metric uses a modifiednormalization factor suited to the retrieval setting. Existing image retrieval benchmarks typically evaluate using the recall@k metric (e.g., ), measur-ing if any of the top k images are relevant. While this makes sense in the setting where just one imageis relevant, INQUIRE has potentially many relevant images and thus, we employ metrics that measureboth relevance and ranking of retrievals. Detailed discussion our metrics is provided in Appendix G.",
  "Category": "ViT-B-16ViT-L-14ViT-H-14 : Left: CLIP zero-shot retrieval performance across supercategories using an identicalbackbone (ViT-B/16) trained or fine-tuned on different datasets. We see how training datasets have asignificant effect on final performance, e.g., BioCLIP is tuned on natural world data at the expense offorgetting other categories. Right: CLIP retrieval performance of models trained on DFN . Species ID",
  "performance (see ), these results suggest that just scaling might not be enough, so futureresearch should seek methods to better incorporate domain knowledge": "Small models struggle to answer many queries. In we can see that CLIP RN50 and CLIPViT-B-32 score an mAP@50 of just 7.6 and 8.2 respectively, demonstrating that these smaller modelsare unable to provide accurate retrievals for nearly all queries. Since the largest models get compar-atively much higher scores, the queries are not impossible but rather difficult for smaller models.DFN ViT-B-16, trained with curated data, outperforms the larger OpenAI ViT-L-14, emphasizing theopportunity to improve the performance of efficient models via better data or training methods. High-quality training data is crucial for expert-level queries. In -left we show the retrievalperformance on different supercategories for CLIP ViT-B/16 models that are trained on differentdatasets: BioCLIP , WildCLIP , OpenAI , and DFN . The DFN model, trained ontwo billion filtered image-text pairs, is the best generalist model on OpenCLIPs benchmarks and also outperforms all the others here, demonstrating the effectiveness of high quality pretraining.Conversely, models specifically trained on natural world data demonstrate degraded performance:BioCLIP was trained primarily on taxonomic captions and images, including iNat21, yet failssignificantly on non-species queries, while WildCLIP has degraded performance in all supercategories.This performance emphasizes the need for better natural world models and fine-tuning strategies thatcan gain domain-specific expertise while preserving generalist capabilities. Reranking offers a valuable opportunity for improving retrieval. shows that rerankingwith larger models like VILA-40B and GPT-4o gives a significant performance boost in mAP@50 of7 and 12 points, respectively. Still, even GPT-4o performs significantly worse than the best possiblererank of its initial CLIP ViT-H-14 ranking. Increasing the size of the initial retrieval set from 50 to100 can further improve performance by surfacing more relevant images, but only higher-performingmodels benefit: The mAP@50 for GPT-4o increases by 5 points, while lower-performing models : Results for the INQUIRE-RERANK task on various embedding and multimodal models. Foreach task, a fixed set of the top-100 images is provided, which we then rerank using different methods.Evaluation metrics are calculated based solely on this fixed set, disregarding any potential positivesoutside of the top-100 images. Therefore, a perfect score is achievable within this context.",
  "VILA-40B 52.874.40.71": "like LLaVA-v1.6-7B see decreased performance. Further results for varying the initial ranking setsize are in Appendix E. visualizes how GPT-4o reranking improves performance on everycategory compared to its initial ViT-H-14 ranking. Different query types present challenges of varying difficulties to existing models. illustrates the difference in performance across query categories. We see that APPEARANCE queries,which often require both domain knowledge of an organisms appearance and the fine-grainedvisual reasoning to recognize them, are the most difficult for existing models. Indeed, the LIFECYCLE AND DEVELOPMENT set (e.g., Immature bald eagle\", A cicada in the process of sheddingits exoskeleton\") are by far the most difficult. Conversely, CONTEXT queries such those in theHUMAN IMPACT set (e.g., leopard on a road\", bird caught in a net\"), for which less expertise andcomparatively coarser image understanding are needed, are easier for existing models.",
  "Rerank Retrieval Task Results": "The results for the INQUIRE-RERANK task are presented in , where we evaluate rerankingperformance of both CLIP-style models like ViT-B-32 and larger vision-language models such asGPT-4o. Since the total number of images for each query is small (i.e., 100), we also show theexpected results of a random reranking for baseline comparison. In we further break downINQUIRE-RERANK results by queries containing scientific terminology and by query supercategory. Current models struggle with expert-level text-to-image retrieval on INQUIRE. In we observe that the highest AP of 59.6, achieved by GPT-4o, is far below the perfect score of100, showing substantial room for improvement. Smaller models like CLIP ViT-B-32 only slightlyoutperform random chance. Since the top retrieved images are often visually or semantically similar,lower-performing models may be confused into promoting irrelevant images, leading to poorerranking. Queries with scientific terminology are significantly more challenging, showing that modelsmight not understand domains-specific language. For example, the queryAxanthism in a greenfrogreferring to a mutation limiting yellow pigment production, resulting in a blue appearanceuses specialized terminology that a model may not understand. As a result, a model may incorrectlyrank typical green frogs higher than axanthic green frogs, leading to worse-than-random performance.We show the performance of reranking models on queries with scientific terminology in .Interestingly, GPT-4o appears to be closing this gap, with an average difference of 7 points betweenqueries with and without scientific terminology (AP scores of 53 and 60, respectively), compared to a16-point difference for the next best model, VILA-40B (AP of 39 and 55). Nevertheless, this gapremains. Future work should explore methods to improve models comprehension of domain-specificlanguage, which is critical for accurate retrieval in scientific contexts. Reranking effectiveness varies widely by the query type. shows that CONTEXT queries,often requiring general visual understanding, benefit substantially from reranking. Conversely,SPECIES queries, requiring fine-grained visual understanding, see minimal improvement, with the : Evaluation of INQUIRE-RERANK with queries grouped into different query types. First,we group queries containing scientific lingo and no scientific lingo. Next, we group queries by theirsupercategory (Appearance, Behavior, Context, Species). Queries with lingo tend to be more difficult,especially for large models with good generalist understanding but lacking domain expertise. Allresults are reported in AP.",
  "Limitations and Societal Impact": "While the species labels for each image in iNat24 are generated via consensus from multiple citizenscientists, there may still be errors in the labels which our evaluation will inherit. However, this errorrate is estimated to be low . INQUIRE contains natural world images, which while diverse, mayhinder the relevance of some of our insights to other visual domains. In spite of this, we believethat due to the wide range of visual queries contained within, progress on INQUIRE will likely beindicative of multimodal model performance on other challenging domains. There could be unintended negative consequences if conservation assessments were made based onthe predictions from biased or inaccurate models evaluated in this paper. Where relevant, we haveattempted to flag these performance deficiencies. While we have filtered out personally identifiableinformation from our images, the retrieval paradigm allows for free-form text search and thus careshould be taken to ensure that appropriate text filters are in-place to prevent inaccurate or hurtfulassociations being made between user queries and images of wildlife.",
  "Conclusion": "We introduced INQUIRE, a challenging new text-to-image retrieval benchmark which consists ofexpert-level text queries that have been exhaustively annotated across a large pool of five millionnatural world images called iNat24. This benchmark aims to emulate real world image retrieval andanalysis problems faced by scientists working with these types of large-scale image collections. Ourhope is that progress on INQUIRE will drive advancements in the real scientific utility of AI systems.Our evaluation of existing methods reveals that INQUIRE poses a significant challenge even for thecurrent largest state-of-the-art multimodal models, showing there is significant room for innovationsto develop accurate retrieval systems for complex visual domains.",
  "and Disclosure of Funding": "We wish to thank the many iNaturalist participants for continuing to share their data and also thenumerous individuals who provided suggestions for search queries. Special thanks to Kayleigh Neil,Beat Yaez Iturbe-Ormaeche, Filip Dorm, and Patricia Mrazek for data annotation. Funding forannotation was provided by the Generative AI Laboratory (GAIL) at the University of Edinburgh.EV and SB were supported in part by the Global Center on AI and Biodiversity Change (NSFOISE-2330423 and NSERC 585136). OMA was in part supported by a Royal Society ResearchGrant. OP and KJ were supported by the Biome Health Project funded by WWF-UK.",
  "O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,A. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 2015": "C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gon-tijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion modelswith deep language understanding. NeurIPS, 2022. C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes,A. Katta, C. Mullis, M. Wortsman, et al. Laion-5b: An open large-scale dataset for training nextgeneration image-text models. NeurIPS, 2022.",
  "J. Singh, I. Shrivastava, M. Vatsa, R. Singh, and A. Bharati. Learn\" no\" to say\" yes\" better:Improving vision-language models via negations. arXiv:2403.20312, 2024": "S. Stevens, J. Wu, M. J. Thompson, E. G. Campolongo, C. H. Song, D. E. Carlyn, L. Dong, W. M.Dahdul, C. Stewart, T. Berger-Wolf, W.-L. Chao, and Y. Su. BioCLIP: A vision foundationmodel for the tree of life. In CVPR, 2024. G. Van Horn, S. Branson, R. Farrell, S. Haber, J. Barry, P. Ipeirotis, P. Perona, and S. Belongie.Building a bird recognition app and large scale dataset with citizen scientists: The fine print infine-grained dataset collection. In CVPR, 2015.",
  "J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu. Coca: Contrastivecaptioners are image-text foundation models. In TMLR, 2022": "X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun,C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su,and W. Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoningbenchmark for expert agi. In CVPR, 2024.",
  "D Geographic Range of INQUIRE and iNat2420": "E Additional Results21E.1Breaking Down Results by Category . . . . . . . . . . . . . . . . . . . . . . . . . .21E.2Validation Set Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .22",
  "G Evaluation Metrics23": "H iNat24 Image Collection and INQUIRE Annotation Protocol25H.1iNat24 Dataset Curation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25H.2Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .25H.3Data Format and Structure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26H.4Ethical Considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27H.5Participant Compensation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27H.6 Annotation Instructions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .27",
  "JFull List of INQUIRE Queries30": "K Datasheet36K.1Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36K.2Composition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .36K.3Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .38K.4PREPROCESSING / CLEANING / LABELING. . . . . . . . . . . . . . . . . .40K.5USES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41K.6DISTRIBUTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .41K.7MAINTENANCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42",
  "Moorish Gecko with regenerated tail": "This query is useful for studying tail autotomy and regeneration in geckos. Moorish geckos can regenerate their tail, but the regenerated tail will not be the same as the original: they do not grow tubercles so they appear smooth instead of ridged.1234 Explanation: (1) and (5) have regenerated tails, but are not Moorish geckos. (2) is a Moorish gecko, but does not have a regenerated tail as evidenced by ridges all the way down the length of the tail. Finally, (3) and (4) are both Moorish geckos and have sections of their tails that appear entirely smooth, indicating that they are regenerated",
  "Everted osmeterium": "Swallowtail butterfly (Papilionidae) larvae have osmeterium, a unique defensive organ that is everted in response to threats. This organ has a few defensive uses: it secretes an acidic mixture that can deter threats, mimics a forked tongue to perhaps appear like a snake, and is brightly colored as a possible aposematic warning. Explanation: (1) and (3) are correctly retrieved examples of swallowtail larvae at different life stages with everted osmeterium. (2) and (5) are also swallowtail larvae, but their osmeterium are not everted. (4) is a tulip-tree silk moth, which has four orange-red spurs on its head that are not osmeterium.",
  "A godwit performing distal rhynchokinesis": "Distal rhynchokinesis is an ability possessed by some long-billed shorebirds characterized by bending of the upper mandible. While it is hypothesized that this ability can help capture more feed when the birds feed by probing their beak in the mud, the functionality and evolutionary significance is not clear. Explanation: (1) and (3) show pictures of godwits performing distal rhynchokinesis, as indicated by the upwards-bending upper beak. (2) shows a godwit with an open beak, but as the upper beak is straight, it is not performing distal rhynchokinesis. (4) shows a godwit with a straight, closed beak. (5) shows a godwit probing, so it not relevant as we can not infer if it is performing distal rhynchokinesis.",
  "Strawberry poison-dart frog with the \"la gruta\" color morph from Isla Colon": "Strawberry poison-dart frogs are known for their numerous color morphs, such as the common blue jeans morph with a red body with blue legs. Geographically isolated groups of frogs have extreme variability in coloration, but the reasons and mechanisms behind this are not clear, such as the importance of sexual selection of aposematic signaling. One such example is the\"la gruta\" color morph from Isla Colon, with a yellow-green base, possibly blue-ish legs, and dark dots. Explanation: (1) and (6) both show the la gruta morph with the characteristic yellow-green coloring and dark dots. (2) is the common blue jeans morph with a red both and blue legs. (3) and (4) are both different color morphs, while (5) is a Green and black poison dart frog, which is a different species.",
  "Redwood trees with fire scars": "Redwood trees like coast redwoods and giant sequoias area adapted to withstand fires, but forest mismanagement has lead to fires with unprecedented intensity. Pictures of their fire scars can help understand the impacts of wildfires on tree resilience, and fresh growth next to charred bark indicates that a tree has grown since the last fire. Fire also plays an important role in redwood reproduction, including opening up their cones and clearing the forest floor of competitive vegetation. Fire scars appear on redwood trees as blackened bark. Explanation: All of these pictures show coast redwoods or giant sequoias. (1), (2) and (4) show fire scars, as evidenced by the blackened bark inside the trees. (3) Does not show evidence of fire scars, and (5) shows a tree which may have fallen over, but also does not show evidence of fire scars.",
  "BAdditional Details about INQUIRE": "In Figure A1 we show histograms representing the number of labeled images and relevant imagesfor each query from INQUIRE. We see that there is a long-tailed distribution for the number ofrelevant images per query, which ranges from 1 to 1500, with an average of 131 and median of 46relevant images per query. In total, we labeled 194,334 images, of which 32,696 were relevant to theirqueries (or 32,553 unique images). As we use species filters and other steps to ensure our labeling iscomprehensive (see Appendix H), we treat the rest of the iNat24 images as not relevant. This meansthat along with the existing image labels, we also have about 5 million weak negative labels per query,for a total of 1 billion weak labels. In Table A1 we provide a breakdown of the number of queries of each of the four main types,including the average number of relevant images for each query. We note that this number varieswidely. Species queries tend to have many relevant labels, while queries in categories like \"Trackingand Identification\" tend to have few relevant images. Number of Labeled Images Per Query",
  "DGeographic Range of INQUIRE and iNat24": "In Figure A3 we show the geographic range of iNat24 observations and image from INQUIRE judgedas relevant. We can see that the distribution of both is similar, which demonstrates that INQUIREqueries do not exhibit a strong geographic bias as compared to the iNat24 source data in the imagesthat the queries correspond to. However, both exhibit a bias towards North America, Europe, andparts of Australasia which is reflective of the spatial biases present in the iNaturalist platform.",
  "(b) Geographic distribution of the iNat24 images marked relevant for an INQUIRE query": "Figure A3: Here we compare the spatial distribution of the images in iNat24 to the relevant images inqueries from INQUIRE. We can see that the distribution of both is similar. Both exhibit a bias towardsNorth America, Europe, and parts of Australasia which is reflective of the spatial biases present inthe iNaturalist platform.",
  "E.1Breaking Down Results by Category": "All INQUIRE queries belong to one of sixteen fine-grained categories, so grouping results by eachcategory provides insights into the specific strengths and weaknesses of current ranking methods. InTable A2 we show the results on INQUIRE-FULLRANK by category for all embeddings modelswe test. In Table A3 we show the results onINQUIRE-RERANK for all embeddings models andvision-language models we test. Both tables show results for test set queries.",
  "FComputational Efficiency": "Embedding Generation. The computational efficiency of a retrieval method is key to its real-worldviability. In Figure A4 we estimate the computational cost for selected CLIP retrieval methods. Here,the computational cost represents the total computational cost of generating all CLIP embeddings(the per-inference cost is provided by OpenCLIP ), and dividing by 250, the number of queries.However, we note that in practice, once all image embeddings are pre-computed and stored in anefficient nearest-neighbors index (e.g., Faiss ), each query takes milliseconds and thus its searchcost is near-zero. The only significant computational cost will be that of performing inference on thequery via the text encoder. Scaling Laws. Figure A4-left also shows diminishing returns in AP@50 as the model size, andthus the computational cost, increases. When we plot the same data using a log-scaled x-axis inFigure A4-right, we observe a roughly linear trend between the log-scaled computational cost and theAP@50. While further study is required to fully characterize this particular trend, this result showsevidence of power law scaling similar to other machine learning tasks . Per-Query Computational Cost (teraFLOPS) AP@50 vit-b-32 vit-b-16 vit-b-16-dfn vit-l-14-dfn siglip-vit-l-16-384 siglip-so400m-14-384vit-h-14-378 Log Per-Query Cost (teraFLOPs) AP@50",
  "GEvaluation Metrics": "Average Precision at k.Average Precision (AP) is a well-known metric computed by taking theweighted mean of precision scores at a set of thresholds. This metric has been adapted to the retrievalsetting, where it possible to calculate the Average Precision at k (AP@k) among just the top kretrieved items. Since calculating AP@k requires both the relevance and position of the top k items,",
  "where Pr@i is the precision at i (i.e., among the first i items), rel(i) {0, 1} is the binary relevancescore, and NF is a normalization factor": "In a typical implemenation of AP we would see NF = r, the total number of relevant items in thetop k. However in a retrieval setting with a total of R relevant items, this normalization techniquecreates a problematic and unintuitive situation where promoting an item into the top k retrievals candecrease the score. In particular, consider the situation where we have 100 images of which 2 are relevant and 98 are notrelevant. Using a normalization factor of NF = R, we measure AP@5 for the following two top-5retrievals:",
  ". Ordered retrieval relevance: (1, 0, 0, 0, 0) = AP@5 = 12. Ordered retrieval relevance: (1, 0, 0, 0, 1) = AP@5 = 0.7": "We observe that promoting a relevant item into the top 5 resulted in a decreased AP@5, which isundesirable. Our criteria for an AP@ metric is that (1) the measure strictly increases whenever arelevant document is promoted into the top-k, and (2) the has a full range of 0 to 1. Of the rangeof proposed AP@k variants [10; 27; 71], just meets our desired criteria This modified averageprecision normalizes using min(k, R). In the case above, we now have NF = min(k, R) =min(5, 2) = 2, yielding:",
  "H.1iNat24 Dataset Curation": "We follow a similar paradigm used to organize the iNaturalist Competition Datasets from 2017 ,2018 , 2019 , and 2021 . For the 2024 version we start from an iNaturalist observationdatabase export generated on 2023-12-30. Observations are then filtered to include only thoseobserved in the years 2021, 2022, or 2023. This ensures the images in iNat24 are unique and donot overlap with images from prior dataset versions (e.g., iNat21 only contains images up untilSeptember 2020). To utilize the iNat21 taxonomy (for easy compatibility with that dataset) we detecttaxonomic changes between the iNat21 taxonomy and the iNaturalist taxonomy included in the2023-12-30 database export. We then modify species labels (where necessary) so that observationsconform to the iNat21 taxonomy. Some of these taxonomic changes can be quite complicated (splits,merges, etc.) resulting in cases where an iNat21 species is no longer valid, however we are able torecover 9,959 out of the original 10,000 species from iNat21. We then filter to include observationsexclusively from the iNat21 taxonomy. Additional filtering ensures that all observations have validmetadata (i.e., location and time information) and that associated image files are not corrupted. Thesesteps result in a candidate set of 33M observations to sample from to build the iNat24 dataset. Our process of selecting the set of images to include for each species in the iNat24 dataset deviatesfrom the prior dataset building schemes [69; 70]. Random sampling of observations, or even randomsampling from unique users, generates collections of images that are biased towards North Americaand Europe. To decrease this bias we sample from spatio-temporal clusters of observations groups.Observation groups are formed by grouping observations together if they are observed on the sameday within 10km of each other, regardless of the observer. When sampling observations for a species,we cluster their associated observation groups using a spatio-temporal distance metric and thensample one observation per cluster in a round-robin fashion until we hit a desired sample size. Whensampling within a cluster, we prioritize novel observation groups and novel users. We sample atmost 550 observations per species to include in iNat24. This sampling process results in a total of4,816,146 images for 9,959 species. Unlike previous versions of the iNaturalist dataset, we performed one final round of filtering toremove images that are inappropriate for a research dataset or not relevant for the query. We usethe INQUIRE annotation process to find images containing human faces, personally identifiableinformation, empty images, images of spectrograms, etc.. We additionally run a RetinaFace Resnet50 face detection model across the entire dataset, and manually inspect all high confidencepredictions. In total this filtered out an additional 2,603 images. The final dataset contains 4,813,543images for 9,959 species. The iNat24 dataset does not have a validation or test split, i.e., all observations are assigned to the trainsplit. The validation and test splits can be used from the iNat21 dataset to benchmark classificationperformance. As in previous years, we keep only the primary image for each observation, and resizeall images to have a max dimension of 500px on the longest side. All images have three channels andare stored as jpegs. We provide location, time, attribution, and licensing information in the associatedjson file.",
  "H.2Data Annotation": "Image annotation was performed by a carefully selected team of paid MSc students or equivalent,many with expertise in ecology allowing for labeling of difficult queries. Annotators were instructedto label all candidate images as either relevant (i.e., positive match) or not relevant (i.e., negativematch) to the query, and to mark an image as not relevant if there was reasonable doubt as to its",
  "relevance. At this stage, queries that were deemed very easy, not comprehensively labeled, orotherwise not possible to label were excluded from the benchmark": "The annotation itself is performed using a custom interface that we developed that shows the topretrievals given a text query and optionally allows the user to filter based on the species label. Ascreen shot of the tool is displayed in Figure A5. The retrievals are ordered by CLIP ViT-H/14 similarity to the query text. Annotators generally label at least 500 images per query. We comprehensively labeled the dataset primarily through the use of species filters for a single or agroup of species. For example, to thoroughly label the query Black Skimmer performing skimming\",a single species filter (Black Skimmer) was utilized while for the query flamingo standing on oneleg\", four different species filters were needed to account for all the flamingo species included iniNat24 (Lesser Flamingo, Chilean Flamingo, Greater Flamingo, and American Flamingo). Usingspecies filters in this way allows us to sufficiently reduce the search space for these queries tocomprehensively label iNat24 for all possible matches. When a query corresponds to a very large number of species, or no species in particular (e.g., animage containing a photographic reference scale with a color swatch), we label using just the topCLIP retrievals without any filters. In this case, we tend to label a significantly larger number ofimages, and we label until at least 100 images in a row are negative indicating that the set of positiveshas been exhausted. If this condition is not met after at a large number of labels, or the annotatorotherwise believes that comprehensive labeling is not possible, we do not use the query. We notethat the quality of our comprehensive labeling in this case is limited by the CLIP models ability tosurface relevant positives, so any missed positives with lower relevance score could be left unlabeled.This affects only 12 of our 250 queries for which we label without species filters. This is a primarymotivator behind the large number of images labeled per query (i.e., >500). However, if there wereindeed missed positives, then we would expect the CLIP ViT-H/14 model used for labeling to performunexpectedly well, as higher quality models that surface missed positive image would be penalizedas these would be considered negative at evaluation time. Yet, our evaluation in shows thatSigLIP, which on OpenCLIPs retrieval evaluation performs only marginally better than CLIPViT-H/14, achieves a comparable score. This result suggests that our dataset does not suffer from asignificant missed positive issue. Creating INQUIRE involved labeling 194,334 images, of which 32,696 were relevant to their queries.Labeling took place over a total of about 180 hours, so the average time spent labeling is 43 minutesper query or 3.3 seconds per image.",
  "H.3Data Format and Structure": "iNat24. iNat24 is provided as a metadata file and a tar file containing all images. The metadata file isgiven in the commonly used JSON COCO format. The information in this metadata file includes eachimages ID, file path, width, height, image license, rights holder, taxonomic classification, latitude,longitude, location uncertainty, and date. INQUIRE. The INQUIRE benchmark is provided as a two CSV files. The first is a list of queries,where each row includes fields for the query id, query text, organism category, query category type,and query category. The second file is a list of annotations, where row corresponds includes fields forthe query id, image id, and relevance label. The image id can be matched to the iNat24 metadata toget additional information mentioned above, such as the taxonomy, date, and geographic location.",
  "H.4Ethical Considerations": "Copyright and Licensing. We adhere strictly to copyright and licensing regulations. All imagesincluded in the dataset fall under a license allowing copying and redistribution. In particular, allimages are licensed under one of the following: CC BY 4.0, CC BY-NC 4.0, CC BY-NC-ND 4.0, CCBY-NC-SA 4.0, CC0 1.0, CC BY-ND 4.0, or CC BY-SA 4.0. Data Privacy and Safety. Although users approved all images considered for research use, wetake further steps to ensure data privacy and safety. We filter all images for content that is containspersonally identifiable information or images of people. We do not exclude most images containinggore, as these are often ecologically relevant, e.g., using image of road-killed animals to asses impactsof roads on biodiversity. Violations of Rights. We respect the rights of iNaturalist community volunteer observers by con-structing iNat2024 using only images and metadata appropriately licensed by their respective creatorsfor copying, distribution, and non-commercial research use. Nevertheless, we bear responsibility incase of a violation of rights. Participant Risks. We received internal ethical approval for our query collection and data labeling(Edinburgh Informatics Ethics Review Panel 951781 and MIT Committee on the Use of Humans asExperimental Subjects Protocol 2404001276).",
  "IMulti-Modal Model Prompting Details": "We include the various prompts used in our evaluation of large multimodal models in Table A5.We note that while we aim to keep the prompt broadly the same across models, they are ultimatelydifferent due to different prompting requirements for each model. The proprietary models (GPT-4Vand GPT-4o) were queries on October 14, 2024.",
  "For what purpose was the dataset created? Was there a specific task in mind? Was there a specificgap that needed to be filled? Please provide a description": "The purpose of INQUIRE is to provide a challenging benchmark for text-to-image retrievalon natural world images. Prior retrieval datasets are small and do not possess a challengefor existing models, with many being adaptations of captioning datasets. These datasets alsohave exactly one positive match for each query, which differs significantly from real-worldretrieval scenarios where many images can be matches. The initial release of INQUIREincludes 250 queries comprehensively labeled over a pool of five million natural worldimages. For more information see .",
  "Who created this dataset (e.g., which team, research group) and on behalf of which entity (e.g.,company, institution, organization)?": "INQUIRE and iNat24 were created by a group of researchers from the following affiliations:iNaturalist, the Massachusetts Institute of Technology, University College London, Univer-sity of Edinburgh, and University of Massachusetts Amherst. The dataset was created fromdata made publicly available by the citizen science platform iNaturalist . What support was needed to make this dataset? (e.g.who funded the creation of the dataset? Ifthere is an associated grant, provide the name of the grantor and the grant name and number, or if itwas supported by a company or government agency, give those details.) Funding for annotation was provided by the Generative AI Laboratory (GAIL) at theUniversity of Edinburgh. In addition, team members were supported in part by the GlobalCenter on AI and Biodiversity Change (NSF OISE-2330423 and NSERC 585136) and theBiome Health Project funded by WWF-UK.",
  "K.2Composition": "What do the instances that comprise the dataset represent (e.g., documents, photos, people,countries)?Are there multiple types of instances (e.g., movies, users, and ratings; people andinteractions between them; nodes and edges)? Please provide a description. The dataset consists of images depicting natural world phenomena (i.e., plant and animalsspecies). In addition, it also contains natural language text queries representing scientificquestions of interest. Each query is associated with a set of relevant images which came upafter comprehensive labeling among the natural world image collection.",
  "INQUIRE contains 250 text queries and a total of 32,696 relevant image matches. iNat24 contains 4,813,543 images from 9,959 species categories": "Does the dataset contain all possible instances or is it a sample (not necessarily random) ofinstances from a larger set?If the dataset is a sample, then what is the larger set? Is thesample representative of the larger set (e.g., geographic coverage)? If so, please describe howthis representativeness was validated/verified. If it is not representative of the larger set, pleasedescribe why not (e.g., to cover a more diverse range of instances, because instances were withheldor unavailable). The dataset contains approximately five million images sourced from iNaturalist. This isa subset of the total number of images present on iNaturalist. The selection and filteringprocess used to construct the dataset is described in Section H.",
  "Is there a label or target associated with each instance? If so, please provide a description": "In INQUIRE each query is paired with a set of positive image matches from iNat24. iNat24 has species labels associated with each image. The species labels are obtainedfrom research grade labels that have been generated from the community consensus oniNaturalist. Is any information missing from individual instances?If so, please provide a description,explaining why this information is missing (e.g., because it was unavailable). This does not includeintentionally removed information, but might include, e.g., redacted text.",
  "INQUIRE annotations may also contains noise in relevance scoring due to labeling error.However, we extensively labeled relevant queries to ensure this error rate is low": "Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guaranteesthat they will exist, and remain constant, over time; b) are there official archival versions of thecomplete dataset (i.e., including the external resources as they existed at the time the dataset wascreated); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resourcesthat might apply to a future user? Please provide descriptions of all external resources and anyrestrictions associated with them, as well as links or other access points, as appropriate.",
  "Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening,or might otherwise cause anxiety? If so, please describe why": "iNat24 contains pictures of the natural world (e.g., plant and animal species) captured bycommunity volunteers. Some natural world images in this dataset could be disturbing tosome viewers, e.g., there are a small number of images that contain dead animals. Weinclude these images in the dataset as they are ecologically and scientifically useful, e.g., forstudying the impact of roadkill on animal populations.",
  "Does the dataset relate to people? If not, you may skip the remaining questions in this section": "No, our dataset does not relate directly to people. Images of humans where their facesare visible have been filtered out using a combined manual and automated process. SeeSection H.1 for a discussion of data filtering. Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe howthese subpopulations are identified and provide a description of their respective distributions withinthe dataset.",
  "Is it possible to identify individuals (i.e., one or more natural persons), either directly orindirectly (i.e., in combination with other data) from the dataset? If so, please describe how": "Some images publicly uploaded by users to the iNaturalist platform contain identifiableinformation, including pictures containing human faces, IDs, or license plates. To addressthis, we filter iNat24 to remove all such instances that we can identify, including by runningdetection algorithms to find all instances of human faces. More details are provided inSection H.1. All photos used to construct iNat24 come from observations captured by community vol-unteers who have given their images a suitable license for research use. We respect theselicenses by providing the license information for each image as well as the rights holder inthe metadata. The user-provided rights holder name can contain the users iNaturalist userID. This information is already available publicly from the iNaturalist platform. Does the dataset contain data that might be considered sensitive in any way (e.g., data thatreveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions orunion memberships, or locations; financial or health data; biometric or genetic data; formsof government identification, such as social security numbers; criminal history)? If so, pleaseprovide a description.",
  "K.3Collection": "How was the data associated with each instance acquired? Was the data directly observable (e.g.,raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derivedfrom other data (e.g., part-of-speech tags, model-based guesses for age or language)? If data wasreported by subjects or indirectly inferred/derived from other data, was the data validated/verified? Ifso, please describe how.",
  "The queries contained within INQUIRE come from discussions and interviews with a rangeof experts including ecologists, biologists, ornithologists, entomologists, oceanographers,": "and forestry experts. This resulted in 250 text queries. Annotators were instructed to labelcandidate images from iNat24 as either relevant (i.e., positive match) or not relevant (i.e.,negative match) to a query, and to mark an image as not relevant if there was reasonabledoubt. To allow for comprehensive labeling, where applicable, iNat24 species labels wereused to narrow down the search to a sufficiently small size to label all relevant images forthe query of interest. The annotation process is outlined in Section H.2. Over what timeframe was the data collected? Does this timeframe match the creation timeframeof the data associated with the instances (e.g., recent crawl of old news articles)? If not, pleasedescribe the timeframe in which the data associated with the instances was created. Finally, list whenthe dataset was first published. The collection of iNat24 started with a iNaturalist observation database export generatedon 2023-12-30. From this export, we filter observations to only include those added toiNaturalist in the years 2021, 2022, or 2023.",
  "Who was involved in the data collection process (e.g., students, crowdworkers, contractors) andhow were they compensated (e.g., how much were crowdworkers paid)?": "The queries contained within INQUIRE came from discussions and interviews with a rangeof experts including ecologists, biologists, ornithologists, entomologists, oceanographers,and forestry experts. Image annotation was performed by a carefully selected team of paidMSc students or equivalent, many with expertise in ecology allowing for labeling of difficultqueries. These annotators were paid at the equivalent of $15.50 per hour. Were any ethical review processes conducted (e.g., by an institutional review board)? If so,please provide a description of these review processes, including the outcomes, as well as a link orother access point to any supporting documentation. We received internal ethical approval for our query collection and data labeling (EdinburghInformatics Ethics Review Panel 951781 and MIT Committee on the Use of Humans asExperimental Subjects Protocol 2404001276).",
  "What (other) tasks could the dataset be used for?": "The iNat24 dataset could be used for training supervised fine-grained image classifiers. Itcould also be used for training self-supervised methods. The text pairs in INQUIRE couldpotentially be used to fine-tune fine-grained image generation models and vision languagemodels. Is there anything about the composition of the dataset or the way it was collected and prepro-cessed/cleaned/labeled that might impact future uses? For example, is there anything that a futureuser might need to know to avoid uses that could result in unfair treatment of individuals or groups(e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legalrisks) If so, please provide a description. Is there anything a future user could do to mitigate theseundesirable harms? The images from the iNat24 dataset are not uniformly distributed across the globe (seeFigure A3). Their spatial distribution reflects the spatial biases present in the iNaturalistplatform. As a result, image classifiers trained on these models may preform worse onimages from currently underrepresented regions. To decrease this bias we sample from spatio-temporal clusters of observations groups.Observation groups are formed by grouping observations together if they are observed on thesame day within 10km of each other, regardless of the observer. When sampling observationsfor a species, we cluster their associated observation groups using a spatio-temporal distancemetric and then sample one observation per cluster in a round-robin fashion until we hit adesired sample size. When sampling within a cluster, we prioritize novel observation groupsand novel users.",
  "Are there tasks for which the dataset should not be used? If so, please provide a description": "There could be unintended negative consequences if conservation assessments were madebased on the predictions from biased or inaccurate models developed to perform well onINQUIRE. Where relevant, we have attempted to flag these performance deficiencies in themain paper. While we have filtered out personally identifiable information from our images, the retrievalparadigm allows for free-form text search. In real-world text-to-image retrieval applicationscare should be taken to ensure that appropriate text filters are in-place to prevent inaccurateor hurtful associations being made between user queries and images of wildlife.",
  "The dataset will be publicly released conditioned on acceptance": "Will the dataset be distributed under a copyright or other intellectual property (IP) license,and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, andprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU,as well as any fees associated with these restrictions.",
  "You will abide by the iNaturalist Terms of Service": "You will use the data only for non-commercial research and educational purposes. You will NOT distribute the dataset images. The University of Massachusetts Amherst makes no representations or warrantiesregarding the data, including but not limited to warranties of non-infringement orfitness for a particular purpose. You accept full responsibility for your use of the data and shall defend and indemnifythe University of Massachusetts Amherst, including its employees, officers and agents,against any and all claims arising from your use of the data, including but not limitedto your use of any copies of copyrighted images that you may create from the data. Have any third parties imposed IP-based or other restrictions on the data associated withthe instances?If so, please describe these restrictions, and provide a link or other access pointto, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with theserestrictions.",
  "There may be a future version of the dataset, however we do not intend for the dataset to befrequently changing": "If the dataset relates to people, are there applicable limits on the retention of the data associatedwith the instances (e.g., were individuals in question told that their data would be retained for afixed period of time and then deleted)? If so, please describe these limits and explain how theywill be enforced.",
  "Previous versions of the iNaturalist image datasets can be found here": "If others want to extend/augment/build on/contribute to the dataset, is there a mechanism forthem to do so? If so, please provide a description. Will these contributions be validated/verified? Ifso, please describe how. If not, why not? Is there a process for communicating/distributing thesecontributions to other users? If so, please provide a description."
}