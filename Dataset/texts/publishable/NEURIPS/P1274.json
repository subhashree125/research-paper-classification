{
  "Abstract": "As AI systems become increasingly autonomous, aligning their decision-making tohuman preferences is essential. In domains like autonomous driving or robotics, itis impossible to write down the reward function representing these preferences byhand. Inverse reinforcement learning (IRL) offers a promising approach to infer theunknown reward from demonstrations. However, obtaining human demonstrationscan be costly. Active IRL addresses this challenge by strategically selecting themost informative scenarios for human demonstration, reducing the amount ofrequired human effort. Where most prior work allowed querying the human for anaction at one state at a time, we motivate and analyse scenarios where we collectlonger trajectories. We provide an information-theoretic acquisition function,propose an efficient approximation scheme, and illustrate its performance througha set of gridworld experiments as groundwork for future work expanding to moregeneral settings.",
  "Introduction": "Stuart Russell suggested three principles for the development of beneficial artificial intelligence: itsonly objective is realizing human preferences, it is initially uncertain about these preferences, andits ultimate source of information about them is human behavior . Apprenticeship learning viaBayesian inverse reinforcement learning (IRL) can be understood as a possible operationalizationof these principles: Bayesian IRL starts with a prior distribution over reward functions representinginitial uncertainty about human preferences. It then combines this prior with demonstration data froma human expert acting approximately optimally with respect to the unknown reward, to produce aposterior distribution over rewards. In apprenticeship learning, this posterior over rewards is thenused to produce a policy that should perform well with respect to the unknown reward function. However, getting human demonstrations requires scarce human time. Also, many risky situationswhere we would wish AI systems to behave especially reliably may be rare in these demonstrationdata. Bayesian active learning can help with both by giving queries to a human demonstrator that arelikely to bring the most information about the reward. Most prior methods for active IRL queried the expert for action annotations of particular isolated states. However, in domains such asautonomous driving with a high frequency of actions, it can be much more natural for the humanto provide whole trajectories say, to drive for a while in a simulator than to annotate a largecollection of unrelated snapshots. There is one previous paper on active IRL with full trajectories suggesting a heuristic acquisition function whose shortcomings can, however, completely preventlearning. We instead suggest using the principled tools of Bayesian active learning for the task.",
  "(c) EIG of each initial state": ": (a) shows an illustrative gridworld and its true rewards. The lower left corner has a \"jail\"state with negative reward from which an agent cannot leave. The starred green state is the terminal\"goal\" state with a large positive reward. The brown, blue, and red states are \"mud\", \"water\", and\"lava\" type states respectively, whose rewards are unknown to the IRL agent. The IRL agent triesto learn the rewards of these three state types from expert demonstrations. (b) shows the learneddistributions over the rewards of the \"mud\", \"water\", and \"lava\" state types respectively, at someparticular step of the active learning process. These learned reward distributions are used to calculatethe EIG of obtaining another expert demonstration starting from each given state, shown in (c). In thiscase, a demonstration starting in the bottom right state gives the most information about the unknownreward parameters. The article provides the following contributions: we formulate the problem of active IRL with fullexpert trajectories and adapt the expected information gain (EIG) acquisition function to this setting.We then provide an algorithm approximating the EIG and present experiments showing its superiorperformance relative to random sampling and two other baselines in gridworlds. We consider thisinitial investigation in tabular settings a stepping stone toward algorithms for more general settings.",
  "Task formulation": "Let M() = (S, A, p, r, , tmax, ) be a parameterized Markov decision process (MDP), where Sand A are finite state and action spaces respectively, p : S A P(S) is the transition functionwhere P(S) is a set of probability measures over S, r : S A R is an (expected) rewardfunction,1 (0, 1) is a discount rate, tmax N {} is the time horizon, and is the initialstate distribution. The parameter will be used to set up the environment in active learning. Dueto space limitations here, we present experiments where = s0 deterministically chooses an initialstate, but our method can be used also for choosing the transition dynamics. We assume we are initially uncertain about the reward r, and our initial knowledge is capturedby a prior distribution p(r) over rewards, which is a distribution over R|S||A| a space of vectorsrepresenting the reward associated with each state-action pair. We also have access to an expert that,given an instance M(i) of the MDP, can produce a trajectory i =(si0, ai0), . . . , (sini, aini), wheresi0 i, st+1 pi(|st, at), and",
  "aA exp(Q(st, a)) ,(1)": "which is called a Boltzmann-rational policy, given the optimal Q function Q and a hyperparameter expressing how close to optimal the expert behaviour is (where = 0 corresponds to fully randombehaviour and + would yield the optimal policy). 1Our formulation permits for the reward to be stochastic. However, our expert model (1) depends on therewards only via the optimal Q-function, which in turn depends only on the expectated reward. Thus, thedemonstrations can only ever give us information about the expectation. Throughout the paper, the learnt rewardfunction can be interpreted either as modeling a deterministic reward, or an expectation of a stochastic reward. The task of Bayesian active inverse reinforcement learning is to sequentially query the expert toprovide demonstrations in environments 1, . . . , N to gain maximum information about the unknownreward. We start with a (possibly empty) set of expert trajectories D0 and then, at each step of activelearning, we choose a parameter i for the MDP, from which we get the corresponding experttrajectory i. We then update our demonstration dataset to Di = Di1 {i}, and the distributionover rewards to p(r|Di), which we again use to select the most informative environment setup i+1in the next step. We repeat until we exhaust our limited demonstration budget N. Our goal can be operationalized as minimizing the entropy of the posterior distribution over rewards,once all expert demonstrations have been observed. This is equivalent to maximizing the loglikelihood of the true parameter value in expectation, or to maximizing the mutual informationbetween the demonstrations and the reward. We call this the information-theoretic objective. For the apprenticeship-learning objective, we use the final posterior p(r|DN) to produce an apprenticepolicy A := argmaxEr[E[st,at tr(st, at)]] maximizing the expected return, where is atrajectory on a known target setup target with s0 target, st+1 ptarget(|st, at) and at = A(st).",
  "Method": "Our goal at each step is to select an environment setup that will produce the most information inexpectation. In Bayesian experimental design (BED) , especially Bayesian optimization , this isoften framed in terms of an acquisition function that for each estimates how useful it would be toselect, i.e. we would like to select that maximizes the acquisition function.",
  "Efficient sampling with Bayesian optimization": "We propose to use Bayesian optimization , in particular the upper confidence bound (UCB)algorithm , to adaptively choose from which initial states to sample additional hypotheticaltrajectories to efficiently estimate the EIG. We still use the basic structure of (2), but instead of usingthe same number of samples in each initial state, we dynamically choose where to add additionalsamples to best improve our chance of identifying the state maximizing the EIG.",
  "Experiments": "We evaluated our EIG-based methods with full trajectories on two randomized gridworld setupsagainst several simpler baselines: (1) uniform random sampling, (2) selecting the state with maximumentropy in Q-values, (3) querying just a single state (to measure the benefits of whole trajectories),and (4) selecting the starting state leading to trajectories with maximum posterior predictive entropyover the optimal policy. The last one is an acquisition function from , which is the only previouswork on active IRL over trajectories that we are aware of. We use two main metrics: the entropy of the posterior distribution over reward parameters after agiven number of steps of active learning and the expected return (with respect to the initial statedistribution and environment dynamics) of an apprentice policy maximizing this expected return (alsowith respect to the posterior over rewards). We test on two kinds of gridworld environments: one with fewer state types (and thus rewardparameters) than states, which gives the algorithm a known environment structure to exploit, andone with a single random reward per state. Full details on our experiments and additional results(including the efficiency gains from Bayesian optimization) are provided in Appendix C. Structured gridworldWe begin with the 6 6 gridworld shown in a. This environment isdeterministic with 5 actions corresponding to moving in the four directions and staying in place. Theagent can move freely, except for the bottom-left \"jail\" state, which is non-terminal, has a negativereward, and traps the agent permanently upon entry. In terms of the state rewards, there are fivedifferent state types and both the apprentice and the expert know the type of each state a priori. Therewards associated with two state types are known: \"path\" type, with a reward of 1, and a \"goal\"type with reward 100, which is also terminal. There are 3 state types, which we refer to as \"water\",\"mud\", and \"lava\", which have unknown negative reward. We place an independent uniform prior inthe interval on the reward of each state type. Our goal is to infer the reward of these threestate types. Fully random gridworldWe also performed experiments on a 7 7 gridworld with each statesreward drawn from N(0, 3). Each state furthermore has a 10% probability of being terminal. Stateswith reward above the 0.9 quantile of rewards are also terminal. 020406080100 120 140 160 180 200",
  "(d) Random env. regret": ": Results on two gridworld environments comparing EIG-based methods with baselines.NMC stands for naive nested Monte Carlo estimation, while BO stands for Bayesian optimization.\"Single st. EIG (x / 8.8)\" denotes single-state EIG with the x axis scaled by 8.8 - the mean length oftrajectories collected by the full-trajectory EIG variants. Results shows results for both environments, comparing EIG-based methods with baselines.In both environments, we observe that the performance of EIG in terms of posterior entropy andapprentice performance is superior to the baselines. The behavior of other methods varies betweenenvironments. In the structured environment, Q-Entropy does better than random initially, butthen starts to do worse due to repeatedly sampling from states with irreducible uncertainty. Theposterior predictive action entropy acquisition function from breaks entirely, as it only ever queriesfor demonstration trajectories that start in the jail state, as this state trivially has a uniform actiondistribution, and demonstrations starting in the jail state deterministically remain in the jail state. In the random environment, we do not observe any advantage to sampling using the Q-entropybaseline even in the early steps. The action entropy baseline performs better than random initiallybefore leveling off, suggesting that the correlation between action entropy and expected informationmay be strong initially but then breaks down as a significant part of remaining action entropy may bedue to Boltzmann randomness rather than epistemic uncertainty. We also observe an advantage ofthe Bayesian-optimization calculation for EIG - with the same budget, the method achieves betterposterior entropy, matching the performance achieved by naive nested Monte Carlo estimation onlywith about double the budget. Interestingly, across both environments we can observe significant advantage to (1) querying wholetrajectories relative to single states but (2) querying as many single states as the trajectory lengthrelative to querying trajectories. Thus, the choice largely depends on the relative costs of collectingsingle states vs trajectories. In the structured environment, the disadvantage of querying only single states instead of trajectories seems to mostly disappear, which seems consistent with c wherewe see the EIG tightly concentrated in only a few states, so almost all information can be gatheredfrom just a single state-action pair.",
  "Discussion and conclusion": "We have provided a preliminary study of the problem of active IRL with full trajectories in tabularenvironments. We have shown that an information theoretic acquisition function provides improve-ments both in terms of achieving lower posterior entropy, and in terms of apprentice performance. Itthus allows using the scarce time of demonstrators more efficiently. We see this preliminary studywith synthetic gridworlds and demonstrations as a stepping stone toward an extension to continuousstate spaces and more realistic settings.",
  "and Disclosure of Funding": "We would like to thank the OxAI Safety Hub, and in particular Oliver Hayman, for facilitating thebeginning of this project. In his work on this project, Ondrej Bajgar was supported by the Centrefor Doctoral Training in Autonomous, Intelligent Machines and Systems (AIMS) and the Futureof Humanity Institute at the University of Oxford, a grant from the UK Engineering and PhysicalSciences Research Council (EPSRC), a stipend from Deepmind, a grant from the Long-term FutureFund, and visitorships at FAR.AI and the Center for Human Compatible AI (CHAI) at UC Berkeley.He would like to thank all of these for their support.",
  "Stuart Russell. Human Compatible: Artificial Intelligence and the Problem of Control. PenguinRandom House, 2019": "Manuel Lopes, Francisco Melo, and Luis Montesano. Active learning for reward estimation ininverse reinforcement learning. In Wray Buntine, Marko Grobelnik, Dunja Mladenic, and JohnShawe-Taylor, editors, Machine learning and knowledge discovery in databases, pages 3146,Berlin, Heidelberg, 2009. Springer Berlin Heidelberg. ISBN 978-3-642-04174-7. Daniel S. Brown, Yuchen Cui, and Scott Niekum. Risk-Aware Active Inverse ReinforcementLearning. In Proceedings of The 2nd Conference on Robot Learning, pages 362372. PMLR, Oc-tober 2018. URL ISSN: 2640-3498. Alberto Maria Metelli, Giorgia Ramponi, Alessandro Concetti, and Marcello Restelli. ProvablyEfficient Learning of Transferable Rewards. In Proceedings of the 38th International Conferenceon Machine Learning, pages 76657676. PMLR, July 2021. URL ISSN: 2640-3498. Sehee Kweon, Himchan Hwang, and Frank C Park. Trajectory-based active inverse reinforce-ment learning for learning from demonstration. In 2023 23rd International Conference onControl, Automation and Systems (ICCAS), pages 18071812. IEEE, 2023.",
  "Deepak Ramachandran and Eyal Amir. Bayesian Inverse Reinforcement Learning. In Proceed-ings of the Twentieth International Joint Conference on Artificial Intelligence, 2007": "Thomas Kleine Buening, Victor Villin, and Christos Dimitrakakis. Environment Design forInverse Reinforcement Learning. In Proceedings of the 41st International Conference onMachine Learning, pages 2480824828. PMLR, July 2024. URL ISSN: 2640-3498. Thomas Kleine Bning, Anne-Marie George, and Christos Dimitrakakis. Interactive InverseReinforcement Learning for Cooperative Games. In Proceedings of the 39th InternationalConference on Machine Learning, pages 23932413. PMLR, June 2022. URL ISSN: 2640-3498. Dorsa Sadigh, Anca Dragan, Shankar Sastry, and Sanjit Seshia. Active Preference-BasedLearning of Reward Functions. In Robotics: Science and Systems XIII. Robotics: Science andSystems Foundation, July 2017. ISBN 978-0-9923747-3-0. doi: 10.15607/RSS.2017.XIII.053.URL Dylan P. Losey and Marcia K. OMalley. Including Uncertainty when Learning from Human Cor-rections. In Proceedings of The 2nd Conference on Robot Learning, pages 123132. PMLR, Oc-tober 2018. URL ISSN: 2640-3498.",
  "ARelated work": "Our work builds on two strands of work: inverse reinforcement learning (IRL) and active learning,which we will address in turn. The IRL problem itself was introduced by Russell , precededby the closely related problem of inverse optimal control formulated by Kalman . See Aroraand Doshi and Adams et al. for recent reviews of the already extensive literature on IRL.Ramachandran and Amir introduced the Bayesian formulation of the problem that we build onhere. Active learning has first been introduced into IRL by Lopes et al. who collect action annotationsin states where the current posterior over reward functions implies high ambiguity about the expertsaction. A key limitation of this approach is that the action ambiguity can come from several actionsbeing equally good according to the true reward (such as in the jail state in our structured environment).This ambiguity remains even when we already have plenty of expert data for a given state, which canresult in queries that do not bring any additional value. Our approach focuses on states that actuallybring extra information about the uknown reward. A second limitation common with other methodsdiscussed below is that the expert provides single action annotations. This is not practical in settingssuch as autonomous driving where actions are sampled with high frequency, and it may be morenatural for a human demonstrator to provide longer trajectories (i.e. drive for a while) rather thangive annotations for unrelated individual time frames. Bueuning et al. query full trajectories in the context of IRL, where the active component arisesin a choice of transition function from a set of transition functions at each step of learning. Bueninget al also query full trajectories in a different context involving two cooperating autonomousagents.",
  "Our work addresses the setting of identifying an optimal strategy for choosing trajectories within afixed environment": "Instead of directly providing demonstrations, in Sadigh et al. , the human expert is asked toprovide a relative preference between two sample trajectories synthesized by the algorithm. Whilethis generally provides less information per query than our formulation, it is a useful alternative forsituations where providing high-quality demonstrations is difficult for humans. Brown et al. present a risk-aware approach, which queries individual states with the highest-quantile policy loss, i.e. the states with a high risk that the apprentice action could be much worsethan the experts action. Instead of querying at arbitrary states, Losey and OMalley and Lindner et al. synthesize apolicy that explores the environment to produce a trajectory which subsequently gets annotated bythe expert. We instead let the expert produce the trajectory. The closest baseline for our work, and the only existing work we are aware of that deals withfull trajectories in active IRL, comes from Kweon et al. . Like in our experimental setup, theypropose an acquisition function for querying for expert trajectories starting from a given initial state.Their acquisition function is based on maximizing the posterior predictive action entropy along thedemonstration trajectory . That is, maximizing",
  "(b) Scaling of IRL step calculation in time (s) withincreasing grid size": ": These plots show that the EIG calculation step scales approximately quadratically in n,or linearly in the number of steps, and is very consistent. For comparison, the plot also shows thescaling of the time required to run the PolicyWalk algorithm for Bayesian IRL 3 repeated trials with adaptive step sizes, 50 warmup steps and 200 samples, for 5 active learningsteps, then timed the computational time for the EIG calculation as well as the associated PolicyWalkalgorithm for Bayesian IRL. The results are displayed in . They suggest that the Bayesian IRLalgorithm may be the limiting factor in scaling up, though the ValueWalk algorithm (which we foundnot suitable for the structured environment, but performing well on the fully random one) generallydisplays better scaling properties . That said, for scaling the algorithms further, especially tocontinuous spaces, we expect to need to resort to methods based on variational inference. On the other hand, we observed the Bayesian-optimization-based method of EIG calculation to scalemore favourably, since it does not need to assign a uniform budget to all n2 squares, but can focusonly on the most promising ones based on an initial estimate. In our initial investigation, we foundthat assigning a quarter of the budget across initial squares (which still scales quadratically) andscaling the rest linearly tended to preserve performance on par with nested Monte Carlo with a fullyquadratic scaling of the budget, but we are leaving a fuller analysis to a future version of this paper.",
  "C.1Bayesian IRL methods": "Our active learning uses a Bayesian IRL method as a key component. In our experiments, we usedtwo methods based on Markov chain Monte Carlo (MCMC) sampling: on the structured environment,we used PolicyWalk , while on the environment with a different random reward in every state, weused the faster ValueWalk , which performs the sampling primarily in the space of Q-functionsbefore converting into rewards. We also tried a method based on variational inference , but wefound its uncertainty estimates unreliable for the purposes of active learning. For MCMC sampling, we used Hamiltonian Monte Carlo with the no-U-turns (NUTS) sam-pler and automatic step size selection during warm-up (starting with a step size of 0.1). At everystep of active learning, we ran the MCMC sampling from scratch using all demonstrations availableup to that point. We ran for 100 warm-up steps and then 200 collected samples. For subsequent usage,we generally thin the sample to 50 samples to reduce autocorrelation.",
  "C.2Baselines": "We compare our methods against various baseline approaches. To evaluate the value of using fulltrajectories in our EIG estimate, we also give results for experiments where EIG is computed afterquerying a single state only (equivalently a unit-length trajectory), and the returned demonstration hasunit length. Relatedly, we consider a baseline experiment in which N = 8 single states are queriedat each active step, where N equals the average length of demonstrations in the active setting. In a sense, the latter baseline serves as an upper bound of performance we could hope to achieve with afixed budget of trajectory lengths. For these length-one trajectories, we otherwise use the same EIGcalculation as for our longer trajectories. For the Q-entropy baseline, we calculate the optimal Q-value corresponding to each reward sample(this is in fact produced as a byproduct of both Bayesian IRL algorithms) and estimate its entropyusing the k-nearest-neighbours method with k = 5. We then select the state with maximum entropyas the next initial state. The reasoning behind this is that uncertainty in the expert policy is directlydependent on the uncertainty in the Q-value. Furthermore, the uncertainty in the Q-value captures notonly uncertainty about the reward in the given state, but also about the rewards and Q-values of statesthat are expected to follow. The acquisition function can be written as",
  "where p(Q(s0, )|Dn) denotes the joint posterior over all Q-values in state s0 given data Dn": "For the posterior predictive action entropy baseline , we use the acquisition function (8), whileadapting everything else to be consistent with our experiments. Specifically, this means calculating theestimated expert policy DE directly from samples of the expert Q-values and assuming a Boltzmannrational expert. The expectation in Eq. 8 is again approximated by sampling a number of trajectoriesstarting from s0, according to policy DE. For the structured environment, this acquisition functiononly queries trivial trajectories remaining on the jail state which did not terminate, so it was necessaryto truncate these trajectories a maximum length. We chose 15 for this maximum.",
  "C.3.1Structured environment": "In the structured environment, we use an expert rationality coefficient of = 1. We do not provideany initial demonstrations. All experiments were run with 10 random reward assignments, consistentacross all tested methods. The reward was drawn from the same prior as was used by the BayesianIRL method, i.e. independent Uniform for the 3 rewards associated with the 3 state types.",
  "C.3.2Fully random environment": "In the fully random environment, we use an expert rationality coefficient of = 1 and provide 1initial trajectory starting in the top left corner. Each method was run with 16 random reward andterminal-state assignments. The reward was drawn from the same prior as was used by the BayesianIRL method, i.e. N(0, 3), i.i.d. for each state.",
  "C.4Computational cost of Bayesian optimization": "With the same budget of trajectory samples for EIG calculation, the Bayesian optimization methodincurs a less than 10% increase in computation time, taking 6.6 instead of 6.2 seconds per step onaverage. On top of this, the Bayesian IRL method takes about 20 seconds to collect the 200 rewardsamples (+ 100 warm up steps) in both cases, making the computational cost difference between thetwo methods negligible."
}