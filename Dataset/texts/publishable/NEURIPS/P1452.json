{
  "Abstract": "Machine unlearning is gaining increasing attention as a way to remove adversarialdata poisoning attacks from already trained models and to comply with privacyand AI regulations. The objective is to unlearn the effect of undesired data from atrained model while maintaining performance on the remaining data. This paperintroduces HyperForget, a novel machine unlearning framework that leverageshypernetworks neural networks that generate parameters for other networks todynamically sample models that lack knowledge of targeted data while preservingessential capabilities. Leveraging diffusion models, we implement two DiffusionHyperForget Networks and used them to sample unlearned models in Proof-of-Concept experiments. The unlearned models obtained zero accuracy on the forgetset, while preserving good accuracy on the retain sets, highlighting the potentialof HyperForget for dynamic targeted data removal and a promising direction fordeveloping adaptive machine unlearning algorithms.",
  "Introduction": "The need for machine learning (ML) models to forget specific points of their training data hasbecome an essential requirement due to increasing security, ethical, and regulatory concerns in AI.Data forgetting is a critical defense mechanism against adversarial attacks that manipulate modelsto change their behavior or extract training data information from them .Additionally, regulations like the GDPR with its Right-to-be-Forgotten (RTBF) or the EU AI Actenhance individuals data privacy rights, allowing them to request the deletion of their data . As ML models capture information of their training data in their parameters, aligning themwith ethical and regulatory standards requires not only to delete stored data but also to remove itsinfluence on the parameters, which directly impacts the models capabilities . Machine unlearning (MU) focuses on developing algorithms capable of efficiently and effectivelyremoving the influence of specific data on an ML model, while maintaining unrelated knowledge orcapabilities unaffected . Ideally, an unlearned model should behave identically to a modelthat was never trained on the data being unlearned in the first place. Thus, for randomly initializedmodels, exact unlearning is achieved when the distribution of unlearned models is identical to thedistribution of models trained on the dataset D excluding the forget set Df D, either by equatingtheir distribution of parameters or outputs . The gold standard for exact unlearningis retraining the model from scratch without the forget set, which is costly in time and resources as itrequires full access to the training dataset and must be done for each forgetting request. Consequently, research has focused on the development of approximate unlearning methods tomitigate the retraining drawbacks . An ideal unlearning algorithm should beconsistent with the retrained model outputs, preserve as much performance as possible on the retainset Dr = D\\Df, be faster than retraining, provide guarantees of effective removal of Df influence, belightweight, scalable, and avoid recovering unlearned data in strict compliance scenarios. Achieving",
  "all these conditions is challenging and often involves trade-offs based on application needs, forget setsize, and available resources": "Pre-trained methods are proactive and embed unlearning capabilities into the models design ortraining process, which makes them efficient, robust and reliable at removing undesired data points,but require complex designs and higher computational overhead during training . Post-training methods, the most popular type of unlearning methods, are reactive and can be applied onexisting models without redesign, but may not always fully remove data influence . Also, unlearning algorithms can be model intrinsic, agnostic, or involve data modifications for fasterretraining; and may need full, partial, or no access to training data . Particularly, we refer asRetrieval-Enhanced Unlearning (REU) to methods that use stored metadata or auxiliary data savedduring the models training process, distinct from the actual training data, to induce unlearninginspired by human Retrieval-Induced Forgetting phenomena . A relevant concern is scalabilitydue to potential challenges in metadata storage and retrieval . We propose a new REU pre-trained framework that treats forgetting as a generative process witha sequence of states with varying levels of forgetting. This process transitions from less to moreforgetting, positioning forgetting as the inverse of learningan unlearning process. By reconstructingthese states, we aim to place the model in a state where it has gained essential knowledge on theretain set, but not on the forget set. For this, we employ hypernetworks to generate parameters withreduced performance on the forget set, while preserving performance on the retain set. We name thisapproach HyperForget and, to the best of our knowledge, it is the first use of hypernetworks for MU. By integrating diffusion models as hypernetworks , we create two Diffusion HyperForgetNetworks (DiHyFo) shown in , and use them to sample unlearned models for MU tasks inProof-of-Concept (POCs) scenarios, comparing them with models retrained from scratch without theforget sets to show their potential for MU. Notably, while a retrained model should be constructed foreach forget set, one DiHyFo can sample unlearned models for all the forget sets and can be interpretedand evaluated using both the parameter and the output space perspectives of unlearning. Our resultssuggest that the sampled unlearned models effectively achieve zero performance on forget sets whilemaintaining high accuracy on retain sets, and closely mimic a retrained model. Our contributions are: 1. We conceptualize and approach forgetting for ML models as a generative process, makinga model to forget by positioning it in a state where its parameters have gained relevantknowledge and capabilities on the retain set but not on the forget set. 2. We propose a novel REU framework leveraging hypernetworks and diffusion models. Wepresent two implementations and POCs to show their potential for unlearning, and highlightsignificant limitations and potential future directions.",
  "Related work": "Hypernetworks.A neural network G(C; G) with learnable parameters G and context input C isa hypernetwork if its output are parameters for another network F(X; F ) that solves a task T withassociated data D = {X, Y }, i.e. F = G(C; G). Therefore, in this framework G are optimized touse G to sample parameters F that can be used by F to process X and predict Y for task T . This approach can reduce the parameter search space and offers adaptability as, unlike conventionalparameters that remain fixed after training, it can be conditioned to sample parameters for multiplerelated tasks . It has shown promise in continual learning, transfer learning, weight pruning,and uncertainty-aware modeling . However, challenges limit broader adoption, suchas scalability, lack of understanding of representational capacity, learning dynamics, generalization,and ensuring parameters are valid and not merely memorized or interpolated . Diffusion Hypernetworks.Similar to diffusion models , neural networks training withSGD transforms randomly initialized parameters (noise) into a structured set that forms a specific dis-tribution. This inspired the use of diffusion as hypernetworks, what we call Diffusion Hypernetworks(DiHy). While this is still an emerging area of research, early approaches like G.pt show thatdiffusion models have significant potential to act as hypernetworks. 1 G.pt acts as a learned optimizer.By using a diffusion transformer (DiT) and DDPM it takes current parameters (potentiallyrandom noise) with their associated performance and target performance, and update them to obtainthe desired performance, indirectly solving optimization problems traditionally handled by SGD.G.pt has demonstrated broader generative properties than other approaches, although it is less precisein achieving target metrics, requires big data sets constructed by saving checkpoints from multiplemain network training runs, involves considerable training overhead, and shows limited capabilitiesto extrapolate to performances beyond its training data . However, despite these limitations,we show that its ability to learn a conditional generative model over weights of the main network isuseful towards MU. We replicate some experiments related to G.pt (originally reported in ) andlist some observations that might be of independent interest. See Appendix A.6. Furthermore, also used DiT and DDPM to generate unconditioned parameters, improvedcomputational efficiency by incorporating an autoencoder and U-Net for learning on a parameterslatent space. Both works used datasets with only optimized parameters, effectively introducing a softbias for high-performing parameters generation, but limiting capabilities for more diverse parameterdistributions. Recently, extended ideas to text-to-model generation, where a customizedclassifier for a subset of classes of a large dataset can be sampled by prompting text.",
  "Proposed method": "We propose HyperForget a hypernetwork based framework for unlearning in which a hypernetworkis trained to learn a distribution over model parameters. The hypernetwork is then used to samplemodel parameters that solve a forgetting task U. Thus, the resulting model, a HyperForget Network(HyFo), can generate parameters that simultaneously yield high-performance on the retain set Dr andlow-performance on the forget set Df associated to task U. Indeed, it reconstructs the knowledgeand capabilities for Dr while effectively forgetting Df, achieving this efficiently in a single forwardpass, eliminating gradient calculations typically required by other unlearning methods, and allowingfor dynamic unlearning adaptation in scenarios with frequent and varying forget requests. In this work, we use DiT as the hypernetwork. We call the resulting model Diffusion HyperForgetNetwork (DiHyFo). Integrating diffusion models into the HyperForget framework provides a struc-tured approach to gradual unlearning and allows to control the level of data influence removal. Weexplored two options to construct a DiHyFo for class-level unlearning. Consider a classification task T with a dataset D formed by examples xi X with associated labelsyi {1, ..., m}, m N, drawn from an unknown distribution P. A model F trained to solve T isrequested to forget a subset of classes, i.e., Df contains data from specific class labels. For this task,we employ a DiHyFo conditioned on class losses to generate parameters that obtain high performancein Dr and low performance in Df. 1Although G.pt was not categorized as hypernetwork in , mainly due to it predicting targeted parametersupdates instead of direct parameters, we categorize it as a hypernetwork according to our previous definition.",
  ": Examples of observed vs target losses for class 2 using models sampled with DiHyFo": "DiHyFo-1 uses a DiT and follows but conditioned at a class-level. This allows us to controllosses for specific classes at the same time, e.g., prompt DiT to generate parameters with high lossvalues on classes in the forget set, but low loss values on classes in the retain set. This is a differentand more challenging optimization problem than the one solved in . During training, current parameters , future parameters , and their associated class lossesL1, ..., Lm and L1, ..., Lm for task T are randomly selected from a run uniformly sampled from adataset of checkpoints grouped by runs. is always from an earlier step than . Parameters arenormalized and tokenized layer-by-layer flattening them into 1D vectors, each corresponding to aunique token. Layers with both weight and bias are decomposed into separate tokens, and each tokenaccepts a maximum number of parameters set to be smaller than the DiT hidden size. The time-step, losses, and their differences L1,L1, ..., Lm,Lm are tokenized with a frequency-basedencoder. All the tokens are linearly projected to the DiT hidden size, and Standard Gaussian noise isadded to the future parameters at each time-step, and then passed to the DiT to learn to denoise them.The DiT has a decoder at the end that linearly projects each token back to its original size, and aresidual connection to predict the parameter updates t t. The training objective is to minimize theMSE between the generated parameters and the original future parameters. This enables to sampleparameters directly in the parameter space with the desired losses for each class. At inference, the model takes a set of current parameters (potentially random noise) with current andtarget losses for each class, and returns the updated parameters for the requested losses by denoisingan input Gaussian noise vector via DDPM with fixed variances . Thus, DiHyFo-1s goal is topredict the distribution of updated parameters that achieve each target class loss. DiHyFo-2 is directly conditioned on the desired class losses, b, which has shown good resultsin other tasks using DiHy . The DiT is conditioned on a large set of examples of parameterswith both high and low performance on individual classes, enabling to synthesize new parameterstailored to specific losses directly in the parameter space. Thus, the task of DiHyFo-2 is to predict thedistribution of parameters that achieve the desired losses. Both models use a DiT with hidden dimension of 1536, 12 hidden layers, and 16 attention heads,trained with AdamW maintaining an exponential moving average of the model weights acrossthe training process. Learned positional embeddings, initialized to zero, are applied across all tokens.Additional details of the framework and both models can be found in Appendix A.3.",
  "Experimental Setup": "Checkpoints datasets.To reduce the complexity of the task, we consider a simplified scenariowith m classes, out of which r classes are considered as pivot classes. For pivot classes, we onlyconsider high-performing parameters. For the remaining m r class losses, DiHyFo models aretrained on parameters with varied values of losses. In general, any data that a model designer iscertain would not need to be forgotten later can act as a pivot, simplifying the learning problem. The",
  "forget set can be any subset of the m r classes, while the retain sets must always include the pivots.More complex scenarios can consider a small number of pivots or no pivots": "Checkpoints of parameters and their class losses are collected during multiple training runs of an MLPclassifier trained on MNIST. A random subset of classes is selected for undersampling in each run tocapture more diverse losses, and during training checkpoints are randomly selected and evaluated forsaving. Similar to prior work , we apply permutation augmentation to help DiHyFo learnthat weights of neural networks are permutation invariant. To provide DiHyFo-1 with examples oflosses increments we delete a random subset of classes during MLP training in some runs and savethe corresponding checkpoints. We further use heuristics to track the evolution of class losses andprioritize checkpoints with varied losses across classes over checkpoints with similar losses acrossdifferent classes. For a detailed description of the checkpoint datasets collection see Appendix A.5. For our experiments, we consider two variations of MNIST dataset full MNIST and MNIST-4with four classes. For MNIST-4, we use classes {0,1} as pivots. We save 150 checkpoints duringtraining runs of 25 epochs of a two-layer MLP with two hidden units and ReLU activation, totaling1.9M checkpoints. For full MNIST, we use {5-9} as pivot classes, seven hidden units MLP, and 200checkpoints saved per training run for a total of 3.7M checkpoints. Evaluation procedures.The ability of DiHyFo to generate class-loss targeted parameters isevaluated by analyzing losses on train and test set. Following , we use the metric of promptalignment (see Appendix A.4), which intuitively measures the alignment between the observed lossesand target losses. A prompt alignment score of 1 indicates perfect alignment. Further, we includethe correlation between both losses as a complementary metric, and plot them along the identity linewith a reference of high performanceoften the median or average of losses in checkpoints. As there are no general frameworks or benchmarks in MU, a typical strategy to assess the performanceof an MU method is to compare the unlearned model with a model retrained from scratch only on theretain set. The closer the behavior of the unlearned model to the retrained model, the more effectivethe unlearning. Thus, to assess unlearning capabilities of DiHyFo, we sample unlearned models andcompare against a retrained model. We consider {2} and {2,3} as forget sets for MNIST-4, while {2},{2,3,4}, and {0,1,2,3,4} for MNIST. Notably, each forget set required a retraining process, whereasa single DiHyFo model can sample unlearned models for all forget sets, highlighting its suitabilityfor scenarios with dynamic unlearning requests. We compare accuracy on test sets and memberinference attacks score (MIA) to assess the unlearning consistency and guarantees against adversarialattacks to infer forget sets information . The output and parameter spaces of the sampled unlearnedmodels are compared against the retrained model by computing the predictions overlap, and theunlearning score. Unlearning score is the Jensen-Shannon Divergence (JSD) between the probabilitydistributions outputted by the sampled unlearned model and a candidate retrained model .As we only consider class-forgetting setting, ideally, the predictions overlap on the retain set shouldbe high between the unlearned model and retrained model, while for the forget set a low overlapwould be more desirable. Similarly, a high unlearning score value is preferred as this indicates thatthe unlearned models predictions are similar to those of the retrained model. The maximum value ofthe unlearning score can be 1 and the lowest value can be 0.",
  "Results and Discussion": "Generative performance.We measure the capabilities of each model to generate parameters withthe desired loss by computing the prompt alignment and correlation between the target loss andthe actual loss obtained with the sampled parameters. Both these metrics show initial fluctuations buttend to increase and stabilize trough training. As depicted in , although the observed lossesgenerally align, it is challenging for the model to precisely match the target losses, particularly forhigher loss values (see Appendix A.2 for additional results). Similarly, the observed losses for pivotstrack the target losses, successfully retaining performance but struggling with high losses and jumpsbetween levels, as expected with the dataset structure. As we focus on low-loss regions for pivots,these deviations are less relevant for our evaluations. Learning bidirectional loss movements at a class level is a challenging task, leading to less preciseprompt alignment than in other tasks . However, this is not a major issue as unlearning typicallyfocuses on high and low-performing parameters rather than covering the entire loss range. While",
  "for MNIST-4 DiHyFo-2 showed to be better at aligning losses, for MNIST DiHyFo-1 showed betterconsistency and robustness. See Appendix A.2 for further details": "Unlearning Performance. shows that all the sampled unlearned models achieved zeroaccuracy on the forget sets and maintain a good accuracy on the retain sets, comparable to theretrained models. The obtained MIA scores for all sampled unlearned models are very close to theretrained models, indicating good unlearning and robustness against inference attacks. As shown in, the comparison of the outputs produced by the sampled unlearned models and the retrainedmodels shows similar predictions on the retain and test sets, aligning with individual performance.Note that the perfect overlap is not expected, as our unlearned model only needs to behave like anyretrained model that results from the stochastic nature of the training process, not like one specificretrained model. Moreover, the comparisons of the parameters space show high unlearning scores.Thus, the sampled unlearned models are mimicking the behavior of a retrained model, with DiHyFo-1performing more consistently across unlearning tasks, particularly with MNIST. See Appendix A.1for additional results and further discussion. Overall, the sampled models are effectively unlearning the forget set and no longer rely on theassociated data to make predictions, and preserve performance on retain sets without significant signsof catastrophic unlearning , closely approximating the unlearning gold standard. Therefore,we have a potential REU method that is model-intrinsic, accuracy-preserving with a good level ofcompleteness, unlearning guarantees, and a potential choice for dynamic unlearning.",
  "Limitations and Future Work": "In this work, we have provided proof-of-concept experiments for a hypernetworks based approachto unlearning. While our results indicate the potential of this approach, many opportunities forfuture work exist. The current formulation of HyperForget focuses on full-class unlearning, itsextension and potential applicability for other unlearning tasks have yet to be explored. A keylimitation of this approach is that the generative model retains the knowledge of forget sets makingthis approach unsuitable for strict unlearning-for-privacy applications. Furthermore, it is not clearfrom our experiments whether the proposed approach is scalable or not. The use of diffusion modelsadds significant computational overhead, and it inherits concerns from DiHy models, such as limitedgeneralization . Additionally, although generated parameters generally approximate targetlosses, it is difficult for the model to precisely match the target loss. Future works could consider evaluating our framework on other datasets, e.g., CIFAR-10 or mini-ImageNet, as well as improving the scalability of our approach. Future research could explorealternative architectures, improve checkpoint saving strategies, optimize the process and compo-nents, or explore other unlearning tasks. Improvements may come from leveraging model zoos,hypernetworks for architecture-agnostic parameter generation, and learning on latent spaces. Bothhypernetworks and MU are in an early stage, and we hope this work inspires future research toexpand these areas, contributing to more adaptive, secure, and ethical AI solutions.",
  "Ronald L. Davis and Yi Zhong. The biology of forgettinga perspective. Neuron, 95(3):490503, 2017": "Ziya Erko, Fangchang Ma, Qi Shan, Matthias Niener, and Angela Dai. Hyperdiffusion: Generatingimplicit neural fields with weight-space diffusion. In Proceedings of the IEEE/CVF international conferenceon computer vision, pages 1430014310, 2023. European Commission. Regulation of the european parliament and of the council laying down harmonisedrules on artificial intelligence, amending regulations and directives (artificial intelligence act), 2024.Accessed: 2024-08-11.",
  "Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machineunlearning. Advances in neural information processing systems, 36, 2024": "Yawei Li, Shuhang Gu, Kai Zhang, Luc Van Gool, and Radu Timofte. Dhp: Differentiable meta pruningvia hypernetworks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part VIII 16, pages 608624. Springer, 2020. Yawei Li, Wen Li, Martin Danelljan, Kai Zhang, Shuhang Gu, Luc Van Gool, and Radu Timofte. Theheterogeneity hypothesis: Finding layer-wise differentiated network architectures. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21442153, 2021.",
  "Paul Micaelli and Amos J Storkey. Zero-shot knowledge transfer via adversarial belief matching. Advancesin Neural Information Processing Systems, 32, 2019": "Kou Murayama, Toshiya Miyatsu, Dorothy Buchli, and Benjamin C Storm. Forgetting as a consequence ofretrieval: a meta-analytic review of retrieval-induced forgetting. Psychological bulletin, 140(5):1383, 2014. Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. Feder Cooper, Daphne Ippolito,Christopher A. Choquette-Choo, Eric Wallace, Florian Tramr, and Katherine Lee. Scalable extraction oftraining data from (production) language models. ArXiv, 2023.",
  "Stefan Schoepf, Jack Foster, and Alexandra Brintrup. Potion: Towards poison unlearning. arXiv preprintarXiv:2406.09173, 2024": "Konstantin Schrholt, Dimche Kostadinov, and Damian Borth. Self-supervised representation learning onneural network weights for model characteristic prediction. Advances in Neural Information ProcessingSystems, 34:1648116493, 2021. Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what youwant to forget: Algorithms for machine unlearning. Advances in Neural Information Processing Systems,34:1807518086, 2021. Supreeth Shastri, Melissa Wasserman, and Vijay Chidambaram. The seven sins of Personal-Data processingsystems under GDPR. In 11th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 19),Renton, WA, 2019. USENIX Association.",
  "Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machineunlearning. IEEE Transactions on Neural Networks and Learning Systems, 2023": "Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understandingfactors influencing machine unlearning. 2022 IEEE 7th European Symposium on Security and Privacy(EuroS&P), pages 303319, 2021. Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understandingfactors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy(EuroS&P), pages 303319. IEEE, 2022.",
  "Section A.4 Additional details on evaluation metrics: This section outlines the metrics andevaluation methods used to assess the generative performance and unlearning effectivenessof the models": "Section A.5 Additional details on datasets generation: This section describes the method-ologies for dataset generation and processing, including parameter checkpoint collectionand strategies for class loss tracking across different configurations. Section A.6 Some experimental observations on G.pt: This section presents observationsand insights from experiments conducted with the G.pt model, examining performancemetrics and prompt alignment under various test conditions.",
  "A.1Additional unlearning evaluation results": "To apply the DiHyFo models for unlearning, we sample multiple parameters prompting different highlosses values for different forget sets simultaneously with low losses for the corresponding retain sets.The generated parameters are used to load an instance of the main network, which is then evaluatedon a test set. We save the sampled model that obtains the lowest average accuracy on the forget setwhile obtaining the highest possible average accuracy on the retain set (best unlearned model). Weuse accuracy for the selection and evaluation as it is a more interpretable performance measure thanloss. We sampled models that forget classes {2} and {2,3} for MNIST-4, and {2}, {2,3,4}, and {0,1,2,3,4}for MNIST. Figures 3 and 4 exemplify this sampling and selection process. Tables 3 and 4 presentthe evaluation metrics on MNIST-4, showing a close behavior of the sampled unlearned models tothe retrained models. Taking as reference the predictions of the retrained model, we also compare the output spaces usingthe confusion matrix between the sampled models and the corresponding retrained model on theretain and complete test set. Figures 5, 6, 7, and 8 present the results for DiHyFo-1 and DiHyFo-2on MNIST-4 and MNIST. The forget set is not included in this comparison as it mainly containsmatrices with zeros across all entries. This serves as a visual confirmation of commented findingsusing the evaluation metrics, obtaining zeroes in the forget set classes. Overall, the evaluations suggest that the presented DiHyFo models accomplish some relevant desiredconditions for an unlearning algorithm . They wereconsistent, timely at sampling unlearned models, provided guarantees and verification of unlearning,but were model intrinsic, with potential recoverability of knowledge, and had issues with scalability.",
  "Learning curves for both models decrease until convergence and show a similar behavior duringtraining and testing, Figures 9 and 10": "Most experiments presented positive prompt alignment scores, indicating that the generated parame-ters obtain class losses that closely align with the desired class losses. However, there are cases inwhich even when the prompt alignment increases and stabilizes it does not achieve to surpass zero,contrasting with what is shown in the correlation and direct comparison plots. As illustrated in , in several cases this behavior is due to some generated models having high negative scores that pullthe average down, suggesting that the performance of the models as a group is quite varied, with mostmodels performing good but some models performing quite poorly in terms of generating parametersfor the desired losses.",
  ": DiHyFo models learning curves for MNIST-4": "On the other hand, most models show high correlation between observed and desired losses, even formodels with a low prompt alignment. This suggests that while the models may not always match theexact value of the losses, they generally align with the appropriate loss direction. Results in were computed using the sampled models with DiHyFo-2 on MNIST-4 during thelast epoch, and can be contrasted with the corresponding direct comparison plot. The general lossdirection alignment does not always translate into accurate loss matching. Considering the extremenegative values as outliers or setting a lower bound of 0 on the prompt alignment makes it resemblemore closely the correlation results. Indeed, the models are capturing the direction of the relationshipwell but might struggle with the precision of their predictions.",
  "A.3Additional hypernetworks, HyperForget, and DiHyFo details": "Consider a dataset D = {X, Y } associated with a task T. In the classical deep learning frameworkthe learnable parameters F of a neural network F(X; F ) are obtained by solving the optimizationproblem in Equation 1 . The searching for the optimal configuration is performed within large search spaces formed bymillions of potential parameters. The input samples x X are passed through the layers of F toobtain predictions y Y , later compared with the true labels y Y using a loss function L(Y, Y ),which is optimized by updating parameters until convergence.",
  "minF F(X; F )(1)": "Instead of directly optimizing the parameters of the main network, the hypernetwork framework,depicted in , uses a separate network to learn to generate the parameters for the mainnetwork. Both networks are usually trained in an end-to-end differentiable manner . Definition A.1 Hypernetwork. A neural network G(C; G) is called a hypernetwork with learnableparameters G and context input C if its output are parameters for a second neural network F(X; F )that solves a task T with associated data D = {X, Y }, i.e. F = G(C; G).",
  "The context input C for the hypernetwork contains information about the structure of the parametersof the main network that enables learning to generate its parameters": "During the forward pass at training time, the parameters are generated by passing C through G, andserve as input to F, which then process x X and obtains predictions y Y . The loss L(Y, Y )is then computed and during the backward pass, the error is back-propagated through G with thegradients of L computed with respect to G. Consequently, G are optimized to generate the F thatbest solves task T. This introduces the optimization problem in Equation 2 .",
  ": Hypernetwork framework": "For a classification task T with training data D = {X, Y }, solved with a learning algorithmF(X; F ), the associated unlearning task with forget set Df D and retain set Dr = D\\Df issolved by constructing an unlearned model U(D, Df, F) that is expected to perform equivalentlyor similarly to a model that has been trained without the forget set, F(X D\\Df; F ). To forgetspecific subsets of classes, the associated machine unlearning objective can be described by Equation 3, where the first term increases the loss on the subset of classes to forget, Cincrease, reducing itsinfluence, while the second minimizes the loss on the subset of classes to retain, Cminimize, preservingmodel performance on it.",
  "cCminimizeE(x,y)D [Lc(y, y)], (3)": "Consequently, the class unlearning task is closely related to control the influence of model parametersin the input-output interactions within the model. Instead of directly solving it, a hypernetwork canbe conditioned on the specific class performance metrics, such as class losses, to construct a modelcapable of generating parameters that yield high-performance on Dr while obtaining low-performanceon Df, effectively forgetting the specified classes, namely, a HyperForget model. Particularly, whena diffusion model is used as hypernetwork for the unlearning model, it receives the name of DiffusionHyperForget Network (DiHyFo). illustrates this procedure. Notably, unlearned models obtained using HyperForget can be interpreted and evaluated using thetwo interpretations of the definition of unlearning, either as an approximation to exact unlearning onthe parameter space or in the output space . We have presented two mechanisms for constructing a DiHyFo model, depicted in . DiHyFo-1 is built on the learning-to-learn framework of , extending it to generate parameters conditionedon class losses and to be able to learn to deoptimize, i.e., learning-to-forget. Conversely, DiHyFo-2uses a diffusion model directly conditioned on the desired class losses.",
  "A.4Additional details on evaluation metrics": "Each model must be evaluated on two key aspects. First, their generative properties as DiHy need tobe assessed. This involves verifying whether the models can generate appropriate parameters for themain network, enabling it to approximate targeted performance levels on the classification task foreach class. Both DiHyFo models use MSE as training metric, and we analyze the corresponding learning curvesto understand the models learning behavior and convergence, and identify issues like overfitting orunderfitting. During evaluation, the model is prompted with a value close to the best loss found in the trainingdataset. It is noted in that using a value slightly above or below the best loss can sometimes yieldbetter results for certain tasks. To evaluate whether the generated parameters effectively approximateprompted loss values, we use the prompt alignment metric defined by as the R2 score betweenthe obtained loss and the target loss in Equation 6, averaged over the batch size.",
  "i=1Li(7)": "A score close to 1 indicates that the obtained loss closely aligns with the target loss. Negative valuessuggest worse alignment than the mean of the target values. We compute this score separately for eachclass to assess the models ability to control losses across different classes simultaneously. And it iscomputed over 20 regularly-sampled prompts and averaged over multiple neural networks sampledwith each DiHyFo, using randomly-initialized input parameters. This metric is effective for evaluating conditional generative tasks, such as those in the learning-to-learn framework (), as it measures how well the model aligns the observed losses with the targetlosses in terms of both direction and magnitude. However, our training datasets exhibit bidirectionalloss movements, which can affect how the model learns to align the losses. In our experiments, wefound that observed losses generally aligned correctly with the direction of the prompted losses,but not always in magnitude, and sometimes some generated parameters have drastic undesiredperformances that significantly drop the prompt alignment in comparison with the common behaviorof the generated parameters. Thus, we compute the Pearson correlation between observed and targetlosses to assess if the model is accurately tracking the target, providing a complementary metric tothe prompt alignment score. Additionally, we directly compare the observed versus target losses by plotting them along the identityline (indicating perfect alignment) and include a lower performance boundoften set to the medianor average of losses from checkpoint collectionas a reference for high performance. The second aspect to evaluate is the DiHyFo models ability to sample unlearned networks. We assessthe unlearning properties of networks sampled with each DiHyFo by comparing them to a networkretrained from scratch without the forget targets. We compare the individually accuracy of each unlearned model and the retrained model on the retain,forget, and complete test sets. Additionally, we calculate the MIA score for each model to assess thelikelihood that an adversary could infer information about the forget set from the unlearned model.Ideally, the unlearned model should have MIA scores similar to the retrained model. To compute theMIA score, we follow the implementation of , as in . The model being evaluatedgenerates prediction probabilities for the retain and test sets, these probabilities are used to calculateentropy for each set, and the resulting entropies serve as features for a logistic regression with labelsindicating the origin dataset of each instance. The logistic regression is trained and then used topredict the membership status of instances in the forget set. The average of these predictions providesthe MIA score, reflecting how effectively it can infer whether the forget data was part of the trainingset of the model being evaluated.",
  ": Pseudocode for computing MIA score": "As we employ a retrained model from scratch without the forget set as baseline, we would like theunlearned models sampled with each DiHyFo to behave as close as possible to the retrained model.Thus, to compare each unlearned models output space against the retrained model, we measure theoverlap in their predictions on each set. For parameter space comparison we compute an unlearningscore using the Jensen-Shannon Divergence (JSD) , which presents a symmetric measureconstructed using the KullbackLeibler divergence (KL) between two probability distributions, Pand Q. For the unlearning score, the JSD is computed between the softmax of each pair of unlearnedmodels being compared across the entire dataset, Equation 8, in our case the sampled unlearnedmodel using a DiHyFo and the retrained model. The closer the score value to 1, the greater thesimilarity in behavior between the models for the same inputs.",
  "Checkpoints of parameters and associated class losses are collected during multiple training runsof an MLP on MNIST. Since a simple MLP achieves good results on MNIST early in training, we": "randomly select a subset of classes to undersample in each run to capture a broader range of lossvalues. During the MLPs training, a checkpoint is randomly selected and evaluated for potentialsaving. Permutation augmentation is applied as in . The process is illustrated in . As SGD is an unconstrained optimizer, we make use of heuristics to track the evolution of classlosses in a constrained manner, saving checkpoints that include parameters that perform well forsome classes and bad for others simultaneously. Initially, we create bins corresponding to different loss levels and establish a maximum of examplesper bin to prevent over-collecting certain loss levels while allowing for more collection on non-frequent loss levels. As the wide range of possible loss values across multiple classes leads toa combinatorial explosion of potential loss-level combinations, we consider a simplified scenariowhere for a classification task with m classes we use r classes as pivots (r < m). We only considercheckpoints when the model achieves high performance on the pivots, using an accuracy threshold = 80. Lowering this threshold increases variability in pivots and expands the combinatorialpossibilities. The remaining m r classes are allowed to vary across the full range of possible lossvalues. During each training run, if a randomly selected iteration has pivots performance over andthe corresponding bin for remaining classes is not full, the checkpoint is saved. In this setup, theforget set can be any subset of the m r classes, while the retain set must always include the pivots.",
  ": Checkpoints collection for the optimization process": "For more complicated scenarios, we relax the constrains, increasing the number of classes and usingfewer pivots or none at all. Instead of using bins to categorize class-level losses, we implementcheckpoint-saving conditions to balance the need for diverse loss examples with computationalefficiency. We prioritize capturing high and low performing parameter updates simultaneously, apply diverseundersampling rates and increase the random selection rate during early training epochs, while forlater epochs the selection rate is reduced to focus on capturing significant shifts in loss. Also, insome runs we randomly select checkpoints across the training to ensure the models have access toexamples of both the general loss evolution process and the particular moments we are interestedin for the forgetting task. This checkpoint collection approach has proven effective for conditionalparameter generation using DiHy , and ensures our dataset includes key learning momentswhile avoiding redundancy. summarizes this checkpoint-saving strategy. In the case of DiHyFo-1, it needs samples that enable it to learn bidirectional movements in classlosses, either by reversing the order of checkpoint loading during DiT training or by saving check-points from a process with incremental losses. We chose the latter approach so that the model canlearn directly from a forgetting process. To collect examples from the forgetting process, we follow a similar training procedure as described before, but at a certain point during training, we randomlydelete a selection of classes to capture the associated increase in losses. This is similar to forgettingby fine-tuning the main network without the classes to be forgotten, .",
  ": G.pt training results": "CIFAR-10 experimental results showed in used prediction error as a conditional metric withpositive results. However, for this particular experiment it was observed that the range of predictionerror values obtained by the target model is contained in a relatively narrower range than usual. Whencomparing performance across different metrics on the same set, as shown in , the instanceswith more samples and a narrower value range, typically loss, performed better. Additionally, highlights a disparity between the distribution of losses found during testingand those in the training datasets used to produce the results in . During testing, metricvalues tend to be more uniformly distributed, thus, effective learning requires varied examples acrossthe entire testing range. By resampling the training set to cover a wider range of losses, ,G.pts performance improves significantly, as shown in . Indeed, experimental observationsindicate that G.pt, while capable of generating neural network parameters for diverse losses, reliesheavily on well-composed training data to perform effectively within the target metric space. 0.00.51.01.52.0"
}