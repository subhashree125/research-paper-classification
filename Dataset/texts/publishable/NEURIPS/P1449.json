{
  "Abstract": "Lipid nanoparticles (LNPs) are vital in modern biomedicine, enabling the effec-tive delivery of mRNA for vaccines and therapies by protecting it from rapiddegradation. Among the components of LNPs, ionizable lipids play a key role inRNA protection and facilitate its delivery into the cytoplasm. However, designingionizable lipids is complex. Deep generative models can accelerate this processand explore a larger candidate space compared to traditional methods. Due tothe structural differences between lipids and small molecules, existing generativemodels used for small molecule generation are unsuitable for lipid generation. Toaddress this, we developed a deep generative model specifically tailored for the dis-covery of ionizable lipids. Our model generates novel ionizable lipid structures andprovides synthesis paths using synthetically accessible building blocks, addressingsynthesizability. This advancement holds promise for streamlining the developmentof lipid-based delivery systems, potentially accelerating the deployment of newtherapeutic agents, including mRNA vaccines and gene therapies.",
  "Introduction": "The successful development of COVID vaccine paved the way for the clinical application of lipidnanoparticles (LNPs) to deliver different kinds of messenger RNA(mRNA) [Wilson and Geetha, 2022;Hou et al., 2021]. The standard structure of LNPs includes four main components: ionizable lipids,cholesterol, helper lipids, and PEGylated lipids [Biochempeg, 2024]. Ionizable lipids are criticalas they condense the negatively charged mRNA during LNP formulation and help it escape fromendosomes and enter the cytoplasm of target cells to express the protein of interest [Xu et al., 2024;Carrasco et al., 2021]. An ionizable lipid is an amphiphillic molecule with an ionizable, hydrophilichead and several hydrophobic tails [Chaudhary et al., 2021]. An example structure of an ionizablelipid is shown in . Designing ionizable lipids is time-consuming and labor-intensive. Combinatorial chemistry providesa fast and cost-effective method to produce these lipids in large quantities, which allows researchersperform high-throughput screenings of ionizable lipids [Li et al., 2023]. For example, by using aUgi-based three-component reaction (3-CR), researchers can rapidly generate a library of 1,080ionizable lipids [Xu et al., 2023]. This collection can assist in identifying an ionizable lipid for",
  ": An Example of Ionizable Lipid Structure": "the application of interest such as activation of stimulator of interferon genes (STING), which isuseful for delivering mRNA vaccines. However, despite these advances, the method has limitations.The diversity of ionizable lipids generated remains constrained due to the restricted range of lipidheads and tails available. Therefore, designing and testing a broader and more diverse range of lipidsremains challenging. Machine learning methods, particularly generative models, provide a solution for efficiently exploringvast molecular search spaces [Gmez-Bombarelli et al., 2018; Kusner et al., 2017; Bradshaw et al.,2019; Qiang et al., 2023; Bradshaw et al., 2020b]. These models capture the distributions of existingmolecules, learn how molecular structures correlate with their physical properties, and use thisknowledge to predict new molecules. Various generative models have been proposed for moleculedesign, capable of generating new molecules through either molecular graphs or string representations[Bilodeau et al., 2022]. Despite significant advances in machine learning for drug discovery, previousworks have shown that generative models are prone to producing molecules which chemists find verydifficult to synthesize [Bradshaw et al., 2020b]. Existing research in machine learning for LNP design primarily focuses on predicting LNP trans-fection efficiency [Li et al., 2023; Xu et al., 2023; Ding et al., 2023; Moayedpour et al., 2024]. Incontrast, our work addresses the generation of ionizable lipids. Specifically, we adapt an existingapproach, Synthesis-DAGs [Bradshaw et al., 2020b], which simultaneously generates molecules andtheir synthesis routes, to the task of generating synthesizable ionizable lipids [Gao and Coley, 2020].",
  "Synthesis-DAGs": "Synthesis-DAGs is a general molecule generator that our method builds upon [Bradshaw et al., 2020a].It can generate molecules along with their synthesis pathways. Below, we briefly introduce howSynthesis-DAGs work. Representing Synthesis Paths as DAGsSynthesis routes are represented as directed acyclicgraphs (DAGs). Examples are shown in . Building blocks are displayed in blue boxes. Thesebuilding blocks undergo chemical reactions to form intermediate products, shown in light purpleboxes. Finally, the final product is generated, shown in dark purple boxes. Serializing the Construction of DAGs as Action SequencesThe DAGs are serialized into actionsequences by defining three classes of actions: Node-addition, Building block molecular identity, andConnectivity choice. Every molecule in the DAG is considered a node and is classified as either abuilding block node or a product node. The Node-addition action determines which type of nodeto add. Building block molecular identity specifies which molecule to choose from the buildingblock pool. Connectivity choice decides which nodes to connect to the next product and whether thatproduct is an intermediate product or final product. Modeling the Probability Distribution of the Action SequenceA shared RNN models thedistribution of the action sequence. At each step, the RNN computes a context vector, which is passedinto a feedforward action network to predict the next action. There are three action networks in total,one for each action class. In order to feed the action sequence into the RNN model, the actions areconverted into action embeddings. The detailed action types, action choices and action embeddingsare listed in .",
  ": Action Type, Action, and Action Embeddings of DAG Generation": "Reaction PredictorDuring sampling, a reaction predictor is used to perform reaction prediction atthe product nodes. In Synthesis-DAGs, Molecular Transformer is used for this task [Schwaller et al.,2019]. In this work, we focus on adapting Bradshaws Synthesis-DAGs model for use as an ionizable lipidDAG generator. The limitations of directly applying the original method to lipid generation and thedetailed adaptations we made are discussed in .",
  "AGILE framework": "When optimizing ionizable lipids for high mRNA transfection efficiency, we use the AGILE model[Xu et al., 2024] to predict the mRNA transfection efficiency of our generated ionizable lipids. TheAGILE model is a deep learning framework designed to predict the transfection efficiency of ionizablelipids in specific cells. It is part of the AI-Guided Ionizable Lipid Engineering (AGILE) platform.The training process of the prediction model occurs in two stages. First, a graph encoder is pre-trainedon a virtual library of 60,000 chemically diverse lipids using contrastive learning. In the second stage,the model is fine-tuned with wet-lab mRNA transfection efficiency data. We use the model fine-tunedon data with mRNA transfection efficiency in HeLa cells as labels to make predictions.",
  "Lipid Property Predictors": "Our method relies on both a lipid classifier and an ionizable lipid classifier to determine whether thegenerated product is a lipid or an ionizable lipid. In this section, we introduce the lipid classifier, abinary classification model that determines whether a molecule is lipid-like. We then explain howionizable lipids are identified by examining their net charge at physiological and acidic pH. Lipid ClassifierThe architecture of our lipid classifier is based on the message passing neuralnetworks framework, Chemprop [Yang et al., 2019]. It consists of three message passing layersto integrate molecular features, followed by two feedforward layers for property prediction. Thetraining dataset includes 180,000 lipids and 180,000 non-lipid molecules. Lipid data are sourced frompublic datasets LIPID MAPS and SwissLipids, as well as synthetic data generated using graph-basedstructural motifs [Sud et al., 2006; Aimo et al., 2015; Jin et al., 2020]. Non-lipid molecules areobtained from the PubChem database [Kim et al., 2015]. Our classifier demonstrates excellentperformance, achieving both a Receiver Operating Characteristic Area Under the Curve (ROC-AUC)score and a Precision-Recall Area Under the Curve (PR-AUC) score above 0.9999. Ionizable Lipid ClassifierIonizable lipids carry a positive charge at acidic pH, enabling them tocondense RNAs into LNPs, but are neutral at physiological pH to minimize toxicity [Han et al., 2021].To filter based on this criterion, we consider the net charge at pH 5 (acidic) and pH 7.4 (physiological).The net charge is calculated by first estimating the pKa values of the lipids acidic and basic groupsusing MolGpka [Pan et al., 2021], followed by applying the Henderson-Hasselbalch equation todetermine the lipids net charge at both pH values. Property Predictors ValidationTo further validate our lipid property predictors, we curated adataset of over 2,500 ionizable lipids sourced from previously published studies [Li et al., 2023; Heet al., 2023; Liu et al., 2021; Abd Elwakil et al., 2023; Yu et al., 2020; Wei et al., 2023]. Importantly,this dataset is entirely independent of the training data used for our lipid classifier. Using this dataset,we evaluated the predictors performance, with the lipid classifier achieving a high accuracy of98.32%. Additionally, the ionizability predictor demonstrated exceptional performance, accuratelyclassifying all ionizable lipids in the dataset.",
  "Dataset Construction": "In this section, we first explain how we select synthetically accessible ionizable lipid building blocksby detailing the filtering criteria used to choose lipid heads and tails from the ZINC20 dataset ofsynthetically accessible components. After forming a building block pool, we demonstrate how wesynthesize ionizable lipids based on these building blocks and construct the ionizable lipid synthesisdataset.",
  "Building Block Extraction": "We begin the construction of an ionizable lipid synthesis dataset by creating a building block set ofionizable lipid heads and tails. To ensure that all building blocks are synthetically accessible, weselect them from the ZINC20 dataset [Irwin et al., 2020], a publicly available database that includescommercially available compounds. We apply three filtering criteria to identify the ionizable lipidheads. The first criterion is molecular weight; since most lipids, including heads and tails, have amolecular weight of no more than 1000 g/mol. We set our limit at no more than 500 g/mol. Thesecond criterion is about solubility preference, selecting compounds with a LogP value less than zero,where LogP represents the log of the partition coefficient between octanol, a hydrophobic oil, andwater. The third criterion focus on charge characteristics; we seek ionizable heads that maintain a near-zero charge at physiological pH and become positively charged in acidic environments. Compoundscontaining a nitrogen atom in their structure are likely to meet this ionization requirement. Thus, thethird criterion involves selecting molecules that contain amine groups. By applying these criteria, weidentify a set of 2.7 million ionizable head molecules. As for selecting lipid tails, we aim to ensure that the molecule structurally resembles a lipid tail, andthe tail set is diverse. We initially use LipidAnalyzer (a toolbox already implemented) to extract tailsfrom the LIPID MAPS[Sud et al., 2006]. LIPID MAPS contains 48,548 unique lipid structures. Fromthese, we extract a total of 8,176 unique lipid tails. To ensure that all the lipid tails are syntheticallyaccessible, we find similar lipid tail molecules in the ZINC dataset based on these extracted tails. Thesearch tool we use called cartblanche [CartBlanche22, 2024]. We measure similarity by calculatingthe graph edit distance (GED) between two molecules, which involves finding the minimum numberof graph operations needed to transform one graph into another. The second search criterion isthat at least one of the molecules Tanimoto similarity coefficients must be greater than 0.5, usingeither Daylight fingerprints or ECFP4 fingerprints. After identifying all synthetically accessible components, we filter them by checking whether the tail is reactive; this is determined by the presenceof a heteroatom. Through this search, we identify a total of 15,302 synthetically accessible lipid tails.",
  "Lipid Synthesis Dataset Construction": "After obtaining the synthetically accessible lipid heads and tails, our next step is to generate ionizablelipids with multiple tails. In this study, we focus on lipids with 1-3 tails. We start by identifyinglipid heads that are most likely to react with the selected lipid tails. An analysis of the lipid tailsrevealed the functional groups with high occurrences, among which the top three functional groupsthat can react with those in the tails are the carboxylic acid group, amine group, and hydroxyl group.Consequently, we further refine our selection of ionizable heads based on a combination of thesethree functional groups. Specifically, we require the lipid head to contain 1-3 functional groups tofacilitate chemical reactions that attach the tails. This count of 1-3 functional groups excludes theamine group used to ensure ionizability. We synthesize ionizable lipids by sequentially adding lipid tails to lipid heads. The chemical reactionsthat combine a lipid tail with a lipid head or an intermediate product are simulated using Chemformer[Irwin et al., 2022], a reaction prediction model. After the final tail addition, the product is formed.We first apply the lipid classifier to determine if it is a lipid. If classified as a lipid, we then apply theionizable lipid classifier to assess its ionizability.",
  "In this section, we explain why the original Synthesis-DAGs method cannot be directly applied tolipid generation and describe the adjustments we made to address these limitations": "Limitations of the Synthesis-DAGs Method in Generating Ionizable LipidsWe sampled 1,000molecules using the original Synthesis-DAGs model trained on the USPTO reaction dataset [Schneideret al., 2016]. Of these, only 53 were classified as lipids, and 13 as ionizable lipids. This low efficiencyindicates that the original model cannot be directly applied to ionizable lipid generation. One clearreason is that the USPTO reaction dataset is not lipid-specific. Additionally, the reaction predictor inSynthesis-DAGs fails to accurately predict reactions between lipid tails and heads [Schwaller et al.,2019]. Example predictions from the Molecular Transformer are shown in . As seen, theMolecular Transformer tends to make copy-paste errors when predicting reactions between largemolecules, highlighted in the blue boxes. It incorrectly alters structures that are not involved in thechemical reaction. For instance, in the first reaction, the location of the carboxylic acid and the ringstructure consisting of five carbons and one nitrogen are both altered on the lipid head (shown in thebottom blue box), while a ring structure is incorrectly added to the lipid tail (shown in the top bluebox).",
  "lipid headlipid tailMT predictionChemformer prediction": ": Examples of Predictions from the Original and Improved Reaction Predictors. Theoriginal reaction predictor, Molecular Transformer, fails to accurately predict reactions between lipidheads and tails, with errors indicated by blue boxes. In contrast, Chemformer successfully predictsthe correct reactions. To address these two problems, we made two adjustments to the original Synthesis-DAGs model.First, we generated an ionizable lipid synthesis dataset. Second, we used Chemformer [Irwin et al.,2022] as the reaction predictor, which is capable of correctly predicting reactions for larger molecules.",
  "Ionizable Lipid Synthesis Dataset": "Using the method outlined in , we generated an ionizable lipid synthesis dataset to ensure themodel is trained specifically in the domain of ionizable lipids. The dataset contains 70,536 synthesispaths and includes 43,741 building blocks, comprising 38,431 unique lipid heads and 5,310 uniquelipid tails. We randomly split the dataset into training, validation, and test sets, with 63,480, 3,582,and 3,582 data points, respectively. The training set includes 5,905 one-tail, 21,301 two-tail, and36,274 three-tail ionizable lipids.",
  "A Better Reaction Predictor": "To address the limitations of Molecular Transformer, we adopted Chemformer, a more advancedTransformer-based sequence-to-sequence model [Irwin et al., 2022]. Chemformer takes reactantSMILES strings as input and outputs the predicted SMILES string of the product. It is initiallypre-trained on approximately 100 million SMILES strings from the ZINC15 dataset [Sterling andIrwin, 2015], followed by fine-tuning on the USPTO-MIT dataset [Jin et al., 2017], which includesaround 470,000 reactions. We selected Chemformer for several key reasons. First, Chemformer is state-of-the-art for reactionprediction on the USPTO Mixed and USPTO Separated benchmarks [Manohar Koki and Kancharla,2023]. Second, Chemformer significantly reduces copy-paste errors, examples are shown in . For example, in these two examples, Chemformer makes accurate predictions without alteringstructures not involved in the reaction.",
  "Baselines": "Random GenerationRandom generation involves selecting a lipid head and one to three lipidtails at random, followed by sequential chemical reactions using Chemformer to combine them. Thismethod generates raw training data without filtering for lipid property predictors, as described in. The key difference between this approach and ours lies in how the building blocks areselected. We refer to this baseline as Random + Chem Original Synthesis-DAGs Trained on Our Ionizable Lipid Synthesis DatasetWe mentioned in that one of the reasons the original Synthesis-DAGs cannot be used to generate ionizablelipids is due to the training data. Now, we compare our method (refered as DAG + Chem), which istrained on the ionizable lipid dataset and using Chemformer as reaction prediction, to the originalSynthesis-DAGs model, which was trained on our generated ionizable lipid dataset. Because thismodel uses DAGs to represent synthesis paths and employs the Molecular Transformer in the samplingprocess to predict reactions, we refer to this baseline as DAG+MT. The only difference between ourmethod and this baseline is the reaction predictor used.",
  "[building block node, building block node, product node, building block node, product node, ...]": "Each item in the action sequence represents a molecules index, with action embeddings correspondingto the molecule embeddings. At each product node, the context vector is input into a binaryclassification network to determine whether to stop the list generation. This baseline investigates theimpact of different synthesis pathway representations. All other model components, including thereaction predictor (Chemformer), remain the same as in our approach. We refer to this baseline asList + Chem.",
  "Experiment Implementations": "The ionizable lipid generator operates in two stages: training and sampling. During training, areaction predictor is not required, but it is used in the sampling stage. The reaction predictor is hostedon a Flask server running the Chemformer model. The generator sends requests to the server, whichperforms inference and returns the results. Each model is trained for 10 epochs using the Adamoptimizer with a learning rate of 0.0001. During sampling, we draw 100 batches from the trainedmodel, with each batch requesting 200 samples. However, the actual number of generated products islower due to failures in the reaction prediction model. The experiments were conducted on a TeslaP100 GPU with 16GB of memory.",
  "Experiment Results": "We analyze the ability to generate ionizable lipids in terms of generation efficiency and quality, asshown in and , respectively. presents the ionizable lipid generation rate andlipid generation rate for different methods. In , we evaluate the validity (whether the generatedSMILES can be parsed by RDKit [RDKit, 2024]), uniqueness (whether the generated moleculesare different from one another), novelty (whether the generated molecules different from trainingdata), FCD (Frchet ChemNet Distance, whether the generated samples have similar chemical andbiological properties to those of the training data) [Preuer et al., 2018], Synthetic Accessibility score(SA score) [Ertl and Schuffenhauer, 2009]. In , we present four examples of the generatedionizable lipids along with their synthesis paths.",
  "building block": ": Examples of Generated Ionizable Lipids and Their Synthesis Paths. The synthesispaths show the building blocks and intermediate products. Our model can generate ionizable lipidswith one to three tails. : Experimental Results on the Generation Efficiency of Ionizable Lipid GenerationMethods. Lipid rate and ionizable lipid rate indicate the proportion of lipids and ionizable lipids inthe generated samples.",
  "Method Performance": "As illustrated in and , our ionizable lipid generator performs well in terms of boththe ionizable lipid generation rate and the lipid quality. The ionizable lipid rate among all generatedsamples reaches 83.4%, significantly surpassing that achieved through random generation (Random +Chem). Regarding quality analysis, validity, uniqueness, and novelty are notably high. Furthermore,the relatively low Frchet ChemNet Distance compared to other methods indicates that, althoughthe ionizable lipids generated by our method differ from those in the training set, their chemical andbiological properties remain within the same distribution, demonstrating successful generalization.",
  "The Impact of Reaction Predictor Performance": "This comparison focuses on using Chemformer as the reaction predictor versus using MolecularTransformer (DAG + Chem vs. DAG + MT). Our method significantly outperforms DAG + MTin both lipid rate and ionizable lipid rate. The lower efficiency of Molecular Transformer as a reactionpredictor has a substantial impact on the generation of ionizable lipids [Manohar Koki and Kancharla,2023]. Chemformer possesses approximately 45 million trainable parameters, which is substantiallymore than the 12 million parameters in Molecular Transformer. Generally, a larger number of param-eters can enhance model performance due to increased learning capacity. Moreover, Chemformerundergoes a two-stage training process. It is initially pre-trained in a self-supervised manner on 100million SMILES strings from the ZINC15 database before being fine-tuned on downstream tasks. Incontrast, Molecular Transformer is trained directly on a downstream dataset without the intermediarystep of pre-training. This direct approach may limit its ability to thoroughly learn the SMILES syntax,which is essential for predicting correct chemical structures [Duan et al., 2020]. Thus, the use ofpre-trained models, which have already developed a foundational understanding of SMILES syntax,is critical for accurate prediction of chemical products.",
  "The Impact of Data Structure Representing Synthesis Pathway": "In , the third comparison between DAG + Chem and List + Chem shows that the DAGgenerator outperforms the linear method in generating both lipids and ionizable lipids. While theuniqueness and novelty metrics are nearly identical for both methods, the DAG generator achieves alower FCD score, indicating that the lipids produced more closely match the training distribution.The primary reason for this is that DAG representations include diverse action embeddings, providingricher information to the RNN, which helps generate more accurate hidden states for decision-making.Additionally, DAGs can model more complex synthesis routes, particularly those involving reactionsof intermediate products.",
  "Optimization Towards Ionizable Lipids With High Transfection Efficiency": "We have developed an ionizable lipid version of Synthesis-DAGs. Our next goal is to optimize theselipids to enhance mRNA transfection efficiency in specific target cells. In Bradshaws approach[Bradshaw et al., 2020b], the Synthesis-DAGs model is capable of iterative optimization to generateoptimal novel molecules along their synthesis pathways. For this experiment, we utilized the AGILEprediction model, which estimates mRNA transfection efficiency in HeLa cells [Xu et al., 2023].To ensure compatibility with AGILE, we constrained our generator to produce only two-tail lipids,consistent with the lipids studied in AGILEs research. Additionally, to improve lipid stability, werestricted lipid tail lengths to 10 carbons or longer. In each iteration, we sampled 5,000 new DAGsfrom the model and selected the top 1,000 DAGs based on predicted transfection efficiency. Themodel was then fine-tuned for two rounds using these selected DAGs. shows the distributionof the top 1000 transfection efficiency scores for ionizable lipids sampled from the model before andafter 16 fine-tuning iterations. Before iteration 4, we observe an increasing trend in transfectionefficiency as fine-tuning progresses. This shift demonstrates the ability of our method to identifyoptimal lipid structures for mRNA delivery to HeLa cells while maintaining synthesizability. However,continuing the iterative fine-tuning process does not guarantee a consistently increasing transfectionefficiency, as a decline is observed between iteration 4 and iteration 3. also includes 8examples of ionizable lipids with the highest transfection efficiency sampled during iteration 3.",
  "diverse and synthesizable ionizable lipids, complete with synthesis paths. We also demonstrated thepotential to identify ionizable lipids with high mRNA transfection efficiency in target cells": "This study highlights the application of deep generative models in ionizable lipid synthesis, combiningadvanced computational techniques with specific chemical synthesis challenges. Our approachefficiently generates synthesizable ionizable lipids, showing promise for advancing lipid-based RNAdelivery systems. However, it is crucial to recognize the limitations of this study. First, the validity of the predictors,including the property predictors and the reaction predictor, directly impacts the reliability of thegenerated synthesis DAGs. Second, the validity of the proposed synthesis pathways has not beenevaluated in this work. We aim to address these issues in future research. In future work, we will collaborate with organic chemists to synthesize the most promising ionizablelipids identified by our model and validate their mRNA transfection efficiency in wet lab experiments.This will contribute to building a more comprehensive ionizable lipid library to support the design ofmRNA delivery systems.",
  "and Disclosure of Funding": "The authors express their gratitude to Asal Mehradfar and Mohammad Shahab Sepehri for curatingthe lipid dataset utilized in training the lipid classifier and for developing the Lipid Analyzer toolkit. Mahmoud M. Abd Elwakil, Ryota Suzuki, Alaa M. Khalifa, Rania M. Elshami, Takuya Isono,Yaser H.A. Elewa, Yusuke Sato, Takashi Nakamura, Toshifumi Satoh, and Hideyoshi Harashima.Harnessing topology and stereochemistry of glycidylamine-derived lipid nanoparticles for in vivomrna delivery to immune cells in spleen and their application for cancer vaccination. AdvancedFunctional Materials, 33(45):2303795, 2023. doi: URL Lucila Aimo, Robin Liechti, Nevila Hyka-Nouspikel, Anne Niknejad, Anne Gleizes, Lou Gtz,Dmitry Kuznetsov, Fabrice P A David, F Gisou van der Goot, Howard Riezman, Lydie Bouguel-eret, Ioannis Xenarios, and Alan Bridge. The SwissLipids knowledgebase for lipid biology.Bioinformatics, 31(17):28602866, May 2015. Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F. Jensen. Generativemodels for molecular discovery: Recent advances and challenges. WIREs Computational MolecularScience, 12(5):e1608, 2022. doi: URL",
  "Daisy Yi Ding, Yuhui Zhang, Yuan Jia, and Jiuzhi Sun. Machine learning-guided lipid nanoparticledesign for mrna delivery, 2023. URL": "Hongliang Duan, Ling Wang, Chengyun Zhang, Lin Guo, and Jianjun Li. Retrosynthesis withattention-based nmt model and chemical analysis of wrong predictions. RSC Adv., 10:13711378, 2020. doi: 10.1039/C9RA08535A. URL Peter Ertl and Ansgar Schuffenhauer.Estimation of synthetic accessibility score of drug-likemolecules based on molecular complexity and fragment contributions. Journal of Cheminformatics,1(1):8, June 2009.",
  "Wenhao Gao and Connor W Coley. The synthesizability of molecules proposed by generative models.J. Chem. Inf. Model., 60(12):57145723, December 2020": "Rafael Gmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, Ben-jamn Snchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan PAdams, and Aln Aspuru-Guzik. Automatic chemical design using a Data-Driven continuousrepresentation of molecules. ACS Cent. Sci., 4(2):268276, February 2018. Xuexiang Han, Hanwen Zhang, Kamila Butowska, Kelsey L Swingle, Mohamad-Gabriel Alameh,Drew Weissman, and Michael J Mitchell. An ionizable lipid toolbox for RNA delivery. NatureCommunications, 12(1):7233, December 2021. Zepeng He, Zhicheng Le, Yi Shi, Lixin Liu, Zhijia Liu, and Yongming Chen. A multidimensionalapproach to modulating ionizable lipids for high-performing and organ-selective mrna delivery.Angewandte Chemie International Edition, 62(43):e202310401, 2023. doi: URL",
  "Xucheng Hou, Tal Zaks, Robert Langer, and Yizhou Dong. Lipid nanoparticles for mRNA delivery.Nature Reviews Materials, 6(12):10781094, December 2021": "John J Irwin, Khanh G Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R Wong,Munkhzul Khurelbaatar, Yurii S Moroz, John Mayfield, and Roger A Sayle. ZINC20A freeUltralarge-Scale chemical database for ligand discovery. J. Chem. Inf. Model., 60(12):60656073,December 2020. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022, jan 2022. doi: 10.1088/2632-2153/ac3ffb. URL",
  "Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Hierarchical generation of molecular graphsusing structural motifs, 2020. URL": "Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han,Jane He, Siqian He, Benjamin A Shoemaker, Jiyao Wang, Bo Yu, Jian Zhang, and Stephen HBryant. PubChem substance and compound databases. Nucleic Acids Res, 44(D1):D120213,September 2015. Matt J. Kusner, Brooks Paige, and Jos Miguel Hernndez-Lobato. Grammar variational autoencoder.In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference onMachine Learning, volume 70 of Proceedings of Machine Learning Research, pages 19451954.PMLR, 0611 Aug 2017. URL Bowen Li, Rajith Singh Manan, Shun-Qing Liang, Akiva Gordon, Allen Jiang, Andrew Varley,Guangping Gao, Robert Langer, Wen Xue, and Daniel Anderson. Combinatorial design ofnanoparticles for pulmonary mRNA delivery and genome editing. Nature Biotechnology, 41(10):14101415, October 2023. Shuai Liu, Qiang Cheng, Tuo Wei, Xueliang Yu, Lindsay T Johnson, Lukas Farbiak, and Daniel JSiegwart. Membrane-destabilizing ionizable phospholipids for organ-selective mRNA deliveryand CRISPRCas gene editing. Nature Materials, 20(5):701710, May 2021.",
  "Siva Manohar Koki and Supriya Kancharla. Evaluating and optimizing transformer models forpredicting chemical reactions. 2023": "Saeed Moayedpour, Jonathan Broadbent, Saleh Riahi, Michael Bailey, Hoa V. Thu, Dimitar Dobchev,Akshay Balsubramani, Ricardo N.D. Santos, Lorenzo Kogler-Anele, Alejandro Corrochano-Navarro, Sizhen Li, Fernando U. Montoya, Vikram Agarwal, Ziv Bar-Joseph, and Sven Jager.Representations of lipid nanoparticles using large language models for transfection efficiencyprediction. Bioinformatics, 40(7):btae342, 05 2024. ISSN 1367-4811. doi: 10.1093/bioinformatics/btae342. URL Xiaolin Pan, Hao Wang, Cuiyu Li, John Z H Zhang, and Changge Ji. MolGpka: A web server forsmall molecule pka prediction using a Graph-Convolutional neural network. J. Chem. Inf. Model.,61(7):31593165, July 2021. Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gnter Klambauer. FrchetChemNet distance: A metric for generative models for molecules in drug discovery. J. Chem. Inf.Model., 58(9):17361741, September 2018. Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Song Song, Liangren Zhang, Bo Huang, andZhenming Liu. Bridging the gap between chemical reaction pretraining and conditional moleculegeneration with a unified model. Nature Machine Intelligence, 5(12):14761485, December 2023.ISSN 2522-5839. doi: 10.1038/s42256-023-00764-9. URL",
  "Teague Sterling and John J Irwin. ZINC 15 ligand discovery for everyone. J. Chem. Inf. Model., 55(11):23242337, November 2015": "Manish Sud, Eoin Fahy, Dawn Cotter, Alex Brown, Edward A Dennis, Christopher K Glass, Alfred HMerrill, Jr, Robert C Murphy, Christian R H Raetz, David W Russell, and Shankar Subrama-niam. LMSD: LIPID MAPS structure database. Nucleic Acids Res, 35(Database issue):D52732,November 2006. Tuo Wei, Yehui Sun, Qiang Cheng, Sumanta Chatterjee, Zachary Traylor, Lindsay T Johnson,Melissa L Coquelin, Jialu Wang, Michael J Torres, Xizhen Lian, Xu Wang, Yufen Xiao, Craig AHodges, and Daniel J Siegwart. Lung SORT LNPs enable precise homology-directed repairmediated CRISPR/Cas genome correction in cystic fibrosis models. Nature Communications, 14(1):7322, November 2023.",
  "Barnabas Wilson and Kannoth Mukundan Geetha. Lipid nanoparticles in the development of mRNAvaccines for COVID-19. J Drug Deliv Sci Technol, 74:103553, June 2022": "Yue Xu, Shihao Ma, Haotian Cui, Jingan Chen, Shufen Xu, Kevin Wang, Andrew Varley, Rick XingZe Lu, Bo Wang, and Bowen Li. Agile platform: A deep learning-powered approach to acceleratelnp development for mrna delivery. bioRxiv, 2023. doi: 10.1101/2023.06.01.543345. URL Yue Xu, Shihao Ma, Haotian Cui, Jingan Chen, Shufen Xu, Fanglin Gong, Alex Golubovic, MuyeZhou, Kevin Chang Wang, Andrew Varley, Rick Xing Ze Lu, Bo Wang, and Bowen Li. AGILEplatform: a deep learning powered approach to accelerate LNP development for mRNA delivery.Nature Communications, 15(1):6305, July 2024. Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, TommiJaakkola, Klavs Jensen, and Regina Barzilay. Analyzing learned molecular representations forproperty prediction. J. Chem. Inf. Model., 59(8):33703388, August 2019. Xueliang Yu, Shuai Liu, Qiang Cheng, Tuo Wei, Sang Lee, Di Zhang, and Daniel J. Siegwart. Lipid-modified aminoglycosides for mrna delivery to the liver. Advanced Healthcare Materials, 9(7):1901487, 2020. doi: URL",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: All the details of dataset construction and experiment details are listed.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: All details are included.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [No]Justification: There is no error bars due to computational cost.Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: All source codes of the paper are publicly available and well documented.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}