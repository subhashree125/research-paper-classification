{
  "Abstract": "Recently, the diffusion-based generative paradigm has achieved impressive generalimage generation capabilities with text prompts due to its accurate distributionmodeling and stable training process. However, generating diverse remote sensing(RS) images that are tremendously different from general images in terms of scaleand perspective remains a formidable challenge due to the lack of a comprehensiveremote sensing image generation dataset with various modalities, ground sampledistances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we firstcollect nine publicly available RS datasets and conduct standardization for allsamples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts andperform hand-crafted rectification, resulting in information-rich text-image pairs(including multi-modal images). In particular, we design some methods to obtainthe images with different GSD and various environments (e.g., low-light, foggy)in a single sample. With extensive manual screening and refining annotations, weultimately obtain a MMM-RS dataset that comprises approximately 2.1 milliontext-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS imagesacross various modalities, scenes, weather conditions, and GSD. The dataset isavailable at",
  "Introduction": "Remote sensing (RS) image, as a domain-specific image, plays an important role in the applicationsof sustainable development for human society, such as disaster response, environmental monitoring,crop yield estimation, and urban planning . Over the last decade, RS-based deeplearning models have demonstrated substantial success in various computer vision tasks such asscene classification , object detection , semantic segmentation , and change detection ,which can facilitate the above applications of sustainable development. Nevertheless, these modelsmay suffer from limited performance due to the lack of large-scale high-quality dataset, and obtainingRS images is often not easy and expensive (i.e., need to launch a RS satellite).",
  "Information-rich Text Prompt:": "Ordinary precision resolution, fog, a satelliteimage shows a large area of farmland, Sentinel-2. Simple text prompt describing image content Ground Sample Distance (GSD) level Type of satellite Type of weather : The samples from different RS dataset. (a) shows a sample in the classic RSICD dataset ,which is a simple text-image pair. (b) shows a sample in the classic SEN1-2 dataset , which is amulti-modal image pair including RGB image and SAR image. (c) shows a sample in our proposedMMM-RS dataset. In contrast, our MMM-RS dataset provides not only multi-modal image pair butalso information-rich text prompt including simple text prompt describing image content, GSD level(i.e., spatial resolution), type of weather and satellite (different color for better observation). To address the above issues, a straightforward way is to utilize the existing datasets and advancedgenerative models to train a RS-based text-to-image generation model, and thenthe user can obtain the diverse RS images via inputting text prompts. However, there exist somechallenges that may cause the trained model to fail to satisfy the users requirements. As shown in (a), we show a sample in the classic RS dataset RSICD , which is a text-image pair with thesimple text description. Intuitively, the RSICD dataset does not allow models to generate multi-modalRS images. In (b), we show a sample from another classic dataset SEN1-2 , which is amulti-modal RS image pair (including RGB image and Synthetic Aperture Radar (SAR) image) butdoes not include text descriptions. Thus, the SEN1-2 dataset cannot allow models to generate RSimages from text prompts. From the phenomenon described above, we can observe that there is nopublicly available RS dataset that contains both multi-modal RS images and information-rich textdescriptions for diverse and comprehensive RS image generation. In addition, we survey the currentlymainstream RS datasets and report their key properties statistically in Tab. 1, which further providesevidence for the above observation. To this end, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) datasetand benchmark for text-to-image generation in diverse remote sensing scenarios. We first collect 9publicly available RS datasets and standardize all samples to a uniform size. Then, to inject the textualsemantic information in each sample for conducting text-to-image generation, we utilize a large-scalepretrained vision-language model, i.e., BLIP-2 to automatically output text prompt describingeach RS image context. To provide the various ground sample distances (GSD) samples, we design aGSD sample extraction strategy to extract different GSD images for each sample and define the GSD-related text prompts describing different GSD levels. In particular, we select some RGB samples to beused for synthesizing samples of different scenes (e.g., snowy, fog), thus empowering the well-trainedmodel with the ability to perceive the various scenes. Finally, with extensive manual screeningand refining annotations (i.e., text prompts), we obtain approximately 2.1 million well-crafted andinformation-rich text-image pairs to result in our MMM-RS dataset. As shown in (c), we showa sample in our MMM-RS dataset. In contrast, the MMM-RS dataset provides not only a multi-modalimage pair but also an information-rich text prompt. With our MMM-RS dataset, we can train a : Comparisons of mainstream RS datasets.simple denotes the simple text description (e.g., (a)). information-rich denotes the information-rich text description (e.g., (c)). Multi-modal means remote sensing imaging content captured by different sensors, such as RGB image,Synthetic Aperture Radar (SAR) image, and Near Infrared (NIR) image.",
  "MMM-RS (Ours) (information-rich) (RGB, SAR, NIR)": "RS text-to-image generation model by fine-tuning the off-the-shelf text-to-image diffusion models(e.g., Stable Diffusion , ControlNet ) for generating multi-modal, multi-GSD, multi-sceneRS images. In summary, the contributions of this work can be concluded as: We construct a large-scale Multi-modal, Multi-GSD, and Multi-scene Remote Sensing (MMM-RS)dataset and benchmark for text-to-image generation in diverse RS scenarios, which standardizes 9publicly available RS datasets with uniform and information-rich text prompts. To provide the various GSD samples, we design a GSD sample extraction strategy that extractsdifferent GSD levels images for each sample and define the GSD-related text prompts describingdifferent GSD levels. Furthermore, due to the lack of real-world multi-scene samples, we selectsome RGB samples and utilize existing techniques to synthesize samples with different scenesincluding fog, snow, and low-light environments. We use our proposed MMM-RS dataset to fine-tune the advanced Stable Diffusion, and perform ex-tensive quantitative and qualitative comparisons to prove the effectiveness of our MMM-RS dataset.In particular, we use the aligned multi-modal samples (including RGB, SAR, and infrared modali-ties) in the MMM-RS dataset to train the cross-modal generation models based on ControlNet, andthe visualization results demonstrates impressive cross-modal generation capabilities.",
  "Background": "Remote sensing (RS) imaging data are widely used in various computer vision tasks such as sceneclassification , object detection , segmentation , changedetection , and RS image caption . For scene classification, the classicUC Merced Land Use Dataset contains 21 scene classes, and each class has 100 images.MRSSC2.0 is a multi-modal remote sensing scene classification dataset that contains 26,710images of 7 typical scenes such as city, farmland, mountain, etc. In object detection field, theclassic dataset HRSC2016 consists of 1,070 images with 2,976 ship bounding boxes for shipdetection in RS scenarios. For segmentation task, GID contains 150 large-size (7200 6800) RSimage with fine-grained pixel-level annotations. WHU-OPT-SAR is a multi-modal segmentationdataset containing three diverse modalities, i.e., RGB, SAR, and NIR. For change detection task,the LEVIR-CD , Hi-UCD , and CDD are used to train a model predicting the changes inthe same region. In the RS image caption domain, the classic datasets, such as UCM-Captions ,RSICD , NWPU-Captions , contain images with simple text descriptions to conduct image-to-text transferring. Despite the great success, there is no publicly available dataset for RS text-to-imagegeneration task. In this work, we aim to propose a Multi-modal, Multi-GSD, Multi-scene RS datasetand benchmark for text-to-image generation in diverse RS scenarios.",
  "Dataset Statistics": "This section provides basic statistics of the MMM-RS dataset. The MMM-RS dataset is derivedfrom 9 publicly available RS datasets: MRSSC2.0 , Inria , NaSC-TG2 , GID ,WHU-OPT-SAR , HRSC2016 , TGRS-HRRSD , fMoW , and SEN1-2 . Withstandardized processing, MMM-RS finally contains 2,103,273 text-image pairs, and the percentageof different datasets is presented in (a). Statistics for Different Modalities. The MMM-RS dataset contains three modalities: RGB image,Synthetic Aperture Radar (SAR) image, and Near Infrared (NIR) image. Note that the three modalitiesare aligned. (b) shows the number of different modalities, we can observe that the numberof RGB modality is 1,806,889 which dominates the entire dataset. In contrast, the number of SARmodality and NIR modality are much smaller, with the number of 289,384 and 7000, respectively.This is because the three modal-aligned samples are very difficult to obtain requiring multiplesimultaneous satellites to perform computational imaging of the same region . Statistics for Different Ground Sample Distance (GSD) Levels. (c) shows the number ofdifferent GSD levels, in which the all samples are standardized five category: Ultra-high PrecisionResolution (GSD < 0.5 m/pixel), High Precision Resolution (0.5 m/pixel GSD < 1 m/pixel),Ordinary Precision Resolution (1 m/pixel GSD < 5 m/pixel), Low Precision Resolution(5 m/pixel GSD < 10 m/pixel), and Ultra-low Precision Resolution (GSD 10 m/pixel). Visualization of Category Distribution. (d) visualizes the category distribution of MMM-RS,we can observe that the categories are mainly focused on Recreational Facility and Crop Field.The reason behind this phenomenon is that more than half of the samples in the MMM-RS are fromfMoW that is dominated by two categories Recreational Facility and Crop Field.",
  "Dataset Preprocessing and Standardization": "The sizes of the samples in different datasets are often inconsistent, thus the first step in the datasetconstruction is to standardize all samples. Referring to the most popular open source text-to-imagediffusion model, i.e., Stable Diffusion , all samples are standardized to a uniform size of 512512.The algorithm for dataset standardization is illustrated in Algorithm 1. Concretely, for samples with",
  "end for": "the size higher than 512 512, we crop all non-overlapping images with size of 512 512 as newsamples. For samples with the size lower than 512 512, we first calculate the minimum L of theheight and the width, and then crop a image with the size of L L. Note that the sizes of the samplesin the original dataset are greater than or equal to 256 256. To preserve more high-frequency detailwhile upsampling the images, an ESRGAN super-resolution model with the scale factor 2 isintroduced (denoted as ESRGAN()) to super-resolve the cropped image. Finally, we use Bicubicinterpolation to resize the super-resolved image and output the standardized image with the size of512 512. In addition, we need to update the GSD of sample according to the change in image size.",
  "Information-rich Text Prompt Generation": "We now elaborate on how to generate an information-rich text prompt for each sample. The frameworkis shown in , which consists of three components: GSD-related prompts, annotation prompts,and vision-language model prompts. For the GSD-related prompts, we output the GSD prompt ofinput image according to the predefined GSD level in Sec. 3.1. For the annotation prompts, wefirst extract the annotation contents such as satellite type, weather type, category, etc. that may (ormay not, e.g., SEN1-2 does not include annotations) exist in the original datasets, and then thesatellite type and weather type are extracted as output. The category information is used for finalmanual text prompt proofreading. For the vision-language model prompts, we aim to utilize thepretrained large-scale vision-language model BLIP-2 to output a simple text prompt describinginput image content. Finally, we combine the outputs of the above three components and obtain",
  "Multi-scene Remote Sensing Image Synthesis": "To address the problem of multi-scenes RS data scarcity, we aim to synthesize some common scenedata by leveraging existing techniques. Specially, we select 10,000 samples from standardized datasetto be used for synthesizing images with three common scenes: fog scene, snow scene, and low-lightscene. The overview framework of multi-scene RS image synthesis is shown in .",
  "Fog scene synthesis. To synthesize fog image, we use the classic atmospheric scattering-baseddegradation model to generate photorealistic fog images": "Low-light scene synthesis. For synthesizing RS images in the low-light scene, we leverage thelatest low-light image generation model TPSeNCE to synthesize realistic low-light RS images.In practice, we directly use the pretrained model provided by the authors to generate low-light RSimages because it is sufficient to fit the RS scenarios. Snow scene synthesis. For synthesizing snow RS images, a straightforward way is to use TPSeNCEmodel to generate target images, as TPSeNCE also provides the pretrained snow image generationmodel. However, in practice, we observe that the pretrained model is difficult to synthesize snowRS images. To address the above issue, we first screen all RS images containing snow scene in thedataset (460 images in total) and select another 460 RS images that do not contain snow scene. Then,we utilize the CycleGAN that is an unpaired image-to-image translation model and use the aboveselected unpaired data to train a clear-to-snow generation model based on CycleGAN. Finally, weuse the well-trained model to synthesize the snow RS images.",
  "Generating Different GSD Images for the Same Sample": "Existing RS datasets often contain various GSD image, e.g., the fMoW contains images withGSD ranging from 0.5 m/pixel to 2 m/pixel, the GSD of the SEN1-2 is 10 m/pixel, and theGSD of the MRSSC2.0 is 100 m/pixel. However, there is no dataset that contains different GSDimages for a single sample. In other words, these datasets cannot allow the model to generate imageswith different GSDs for the same scene. To address the above issues, we design a GSD sample extraction strategy to extract different GSD images for each sample. The main idea of this strategy isto crop images with different sizes (same height and width) from a large-size RS image and ensurethat the cropped images of different sizes have obvious GSD changes. Then, all cropped images arestandardized to the size of 512 512, so that the GSD of standardized images can be computed asGstd = (L/512) Gori, where Gstd and Gori denote the GSD of the standardized image and originalimage, respectively. L denotes the height and width of the cropped image. In practice, we perform the above strategy on the Inria dataset to generate different GSD imagesbecause its image size is 5000 5000 that is enough to crop images with different sizes. As shownin , we show an example of generating different GSD images for the same sample. Specially,the size and GSD of the original image are 5000 5000 and 0.3 m/pixel, respectively. We then cropfour images with four different sizes (i.e., 4096 4096, 2048 2048, 1024 1024, and 512 512)from the original image. Note that the higher resolution images completely cover the lower resolutionimages, which ensures that all cropped images maintain consistent scene content. Finally, with thefour cropped images, we standardize them to a uniform size of 512 512, and the GSD is updatedto 2.4 m/pixel, 1.2 m/pixel, 0.6 m/pixel, and 0.3 m/pixel, respectively. The above process couldfacilitate the generation of various GSD images, ensuring that the model can perceive variationsbetween different GSDs while maintaining scene consistency.",
  "Fine-tuning Stable Diffusion for RS Text-to-Image Generation": "To validate the effectiveness of the MMM-RS dataset in the RS text-to-image generation task, we usethis dataset along with the currently prominent Stable Diffusion to achieve RS Text-to-ImageGeneration. Those experiments are a crucial part of our work. Experiment settings. Our experiments utilize the Stable Diffusion-V1.5 model (called by SD1.5)as the foundational pre-trained model. To optimize its performance for our specific requirements inRS text-to-image generation, the LoRA technique is adopted to update the stable diffusion model.In the generative phase, our generative model undergoes a training regimen of 200,000 iterations onour MMM-RS datasets. We use a learning rate of 0.0001 and employ the Adam optimizer to ensureeffective training. Our text prompt, which is crucial for directing the image generation process, ismeticulously constructed using a combination of four components: {Ground Sample Distance level},{Type of weather}, {Simple text prompt describing image content}, and {Type of satellite} (suchas: High precision resolution, snow, a satellite image shows a park in the city, Google Earth). Thismethod allows us to explore various textual inputs and their impact on the generated images. Toevaluate our generative models performance, we utilize two widely recognized metrics: the FrechetInception Distance (FID) and the Inception Score (IS). These metrics are crucial for assessingthe quality and diversity of the images generated by our model, allowing us to compare them againstreal images in terms of their distribution and visual clarity. We conduct all experiments using thePyTorch framework on 8 NVIDIA RTX 4090 GPUs.",
  "FID 172.78175.68168.34347.8892.33IS 6.646.316.892.637.21": "Quantitative comparisons. To demonstrate the effectiveness of the MMM-RS dataset in the RStext-to-image generation task, we conduct the quantitative comparisons in terms of the FID and the ISmetrics across different generative models, as shown in Tab. 2. It should be noted that to ensure a faircomparison of model performance, each model generated 500 RS images for the calculation of theabove metrics. Notably, our model achieves a lower FID value compared to other models. A lower FIDindicates that the images generated by our model have a closer statistical distribution to real images,suggesting higher quality and accuracy of the generated images. Additionally, our model scores thehighest on the IS, surpassing the other generative models. The higher IS indicates that our modelnot only produces more diverse images but also maintains better clarity and recognizability in thegenerated images. These results underscore the effectiveness of the MMM-RS dataset in enhancingthe capabilities of generative models for RS image generation. The substantial improvements in both metrics compared to existing models highlight the potential of our tailored approach in producingrealistic and varied images, suitable for advanced remote sensing applications. This validates theutility of the MMM-RS dataset as a valuable resource in the field of generative image modeling.",
  ": Visualization results of different methods in multi-GSD RS image generation. Our resultsshow obvious variations of GSD according to the given text prompts": "Qualitative comparison. We generate multi-scene RS images using text prompts across variousgenerative models, as depicted in . The results affirm our models ability to accurately interpretand render diverse weather conditions. For example, the snow prompt yields images with detailedurban structures under snow cover in our results. In contrast, the results from the other methods seem hardly adequate for snow scenarios. Similarly, our model can faithfully generate images withfog and night scenes, but other methods are hard to generate weather-consistent results. Therefore,the MMM-RS dataset not only enhances the quality and usability of generated RS images but alsoestablishes a new standard for depicting weather and environmental conditions in synthetic images,reinforcing the purpose and significance of developing the MMM-RS dataset. shows the visualization results of different methods in multi-GSD RS image generation, wecan observe that our results demonstrate clear GSD variations when generating RS images based onspecific text prompts, highlighting our models ability to adapt GSD settings effectively compared toother models. This adaptability allows our model to produce images ranging from ultra-high precisionresolution to low precision resolution. For instance, the ultra-high precision resolution images revealmeticulous details such as building outlines and individual road lanes, which are distinctly visible.Conversely, the low precision resolution images, while less detailed, still maintain a level of clarityand contextual relevance suitable for broader landscape interpretations. These results prove that theMMM-RS dataset enables the model to perceive the various GSD through designated text prompts.",
  "Cross-modal Generation based on ControlNet": "In the above part, we conduct experiments to prove the validity of MMM-RS dataset in generatingdiverse RGB RS images. However, the multi-modal part remains to be further investigated. In thispart, we aim to perform more interesting cross-modal generation experiments to verify the plausibilityand validity of multi-modal data rather than simply fine-tuning the Stable Diffusion. Experiment settings. We select the ControlNet as the base model of cross-modal generation,which is a neural network architecture that can improve large-scale pretrained text-to-image diffusionmodels with input task-specific prior conditions. Concretely, we also use the pretrained StableDiffusion-V1.5 model as the backbone of the ControlNet, and the batch size is set to 4. Weuse a learning rate of 0.00005 and employ the Adam optimizer to train the models. For training thecross-modal generative models between RGB modality and SAR modality, we use approximately290,000 RGB-SAR pairs in our MMM-RS dataset to train this model with 80,000 iterations. Fortraining the cross-modal generative models between RGB modality and NIR modality, we use only7,000 RGB-NIR pairs to train this model with 20,000 iterations. Low precision resolution, a satellite image shows large areas of lakes and rivers, Sentinel-2 Low precision resolution, a satellite image shows some roads in farmland,Sentinel-2 Low precision resolution, a satellite image of a mountain with a river running through it, GF1 Low precision resolution, a satellite image shows a city, Sentinel-1 Low precision resolution, a satellite image of a city with a river, GF1 Low precision resolution, a satellite image of a village with a river running through it, Sentinel-1 Low precision resolution, a satellite image of a city with a river, GF1 Low precision resolution, a satellite image shows a river flowing through the mountains, GF1",
  ": Visualization results of four different cross-modal generation tasks": "Results of cross-model generation. We conduct four different cross-model generation task: RGB SAR, RGB NIR, SAR RGB, and NIR RGB. showcases the visualization results ofthe above four cross-model generation, we can observe that the generated SAR and NIR images fromRGB SAR and RGB NIR can correctly depict the structural information of the input RGBimages. For the SAR RGB and NIR RGB, the generated RGB images not only maintain thestructural information of the input image but also exhibit rich textural details. The above results provethat our MMM-RS dataset can be effectively used for cross-modal generation tasks in RS scenarios.",
  "Conclusion": "In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS)dataset and benchmark for text-to-image generation in diverse RS scenarios. MMM-RS is inspiredby the investigation that there is no publicly available RS dataset that contains both multi-modal RSimages and information-rich text descriptions for diverse and comprehensive RS image generation.Through the collection and standardization of nine publicly available RS datasets, we created aunified dataset comprising approximately 2.1 million well-crafted text-image pairs. With extensiveexperiments, we demonstrated the effectiveness of our dataset in generating multi-modal, multi-GSD,and multi-scene RS images.",
  "Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. InCVPR, 2018": "Raanan Fattal. Single image dehazing. ACM transactions on graphics (TOG), 27(3):19, 2008. Nil Goyette, Pierre-Marc Jodoin, Fatih Porikli, Janusz Konrad, and Prakash Ishwar. Changedetection. net:A new change detection benchmark dataset. In 2012 IEEE computer society conference on computer visionand pattern recognition workshops, pages 18. IEEE, 2012. Nanjun He, Leyuan Fang, Shutao Li, Antonio Plaza, and Javier Plaza. Remote sensing scene classificationusing multilayer stacked covariance pooling. IEEE Transactions on Geoscience and Remote Sensing, page68996910, Dec 2018. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Ganstrained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural informationprocessing systems, 30, 2017.",
  "Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and Xiang Li. Rsgpt: A remote sensing visionlanguage model and benchmark. arXiv preprint arXiv:2307.15266, 2023": "HuEdward J., Yulong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and WeizhuChen. Lora: Low-rank adaptation of large language models. arXiv: Computation and Language,arXiv:Computation and Language, Jun 2021. Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, YaroslavBulatov, and Brendan McCord.xview: Objects in context in overhead imagery.arXiv preprintarXiv:1802.07856, 2018. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. In International conference on machine learning,pages 1973019742. PMLR, 2023. Xue Li, Guo Zhang, Hao Cui, Shasha Hou, Shunyao Wang, Xin Li, Yujia Chen, Zhijiang Li, and Li Zhang.Mcanet: A joint semantic segmentation framework of optical and sar images for land use classification.International Journal of Applied Earth Observation and Geoinformation, 106:102638, 2022. Chenyang Liu, Keyan Chen, Zipeng Qi, Haotian Zhang, Zhengxia Zou, and Zhenwei Shi. Pixel-level changedetection pseudo-label learning for remote sensing change captioning. arXiv preprint arXiv:2312.15311,2023.",
  "Kang Liu, Jian Yang, and Shengyang Li. Remote-sensing cross-domain scene classification: A dataset andbenchmark. Remote Sensing, 14(18):4635, 2022": "Xiaoqiang Lu, Binqiang Wang, Xiangtao Zheng, and Xuelong Li. Exploring models and data for remotesensing image caption generation. IEEE Transactions on Geoscience and Remote Sensing, 56(4):21832195, 2018. Xianping Ma, Qianqian Wu, Xingyu Zhao, Xiaokang Zhang, Man-On Pun, and Bo Huang. Sam-assistedremote sensing imagery semantic segmentation with object and boundary constraints. arXiv preprintarXiv:2312.02464, 2023. Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat, and Pierre Alliez. Can semantic labelingmethods generalize to any city? the inria aerial image labeling benchmark. In 2017 IEEE InternationalGeoscience and Remote Sensing Symposium (IGARSS), Jul 2017. OpenAI. Dall-e 3. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXivpreprint arXiv:2307.01952, 2023. Bo Qu, Xuelong Li, Dacheng Tao, and Xiaoqiang Lu. Deep semantic understanding of high resolutionremote sensing image. In 2016 International Conference on Computer, Information and TelecommunicationSystems (CITS), Jul 2016. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pages 1068410695, 2022.",
  "CJ Van Westen. Remote sensing for natural disaster management. International archives of photogrammetryand remote sensing, 33(B7/4; PART 7):16091617, 2000": "Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, and Liangpei Zhang. Samrs: Scaling-up remote sensing segmentation dataset with segment anything model. Advances in Neural InformationProcessing Systems, 36, 2024. Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy.Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings of the Europeanconference on computer vision (ECCV) workshops, pages 00, 2018. Yuanzhi Wang, Zhen Cui, and Yong Li. Distribution-consistent modal recovering for incomplete multimodallearning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2202522034, 2023.",
  "Yuanzhi Wang, Yong Li, and Zhen Cui. Incomplete multimodality-diffused emotion recognition. Advancesin Neural Information Processing Systems, 36, 2024": "Yuanzhi Wang, Yong Li, Xiaoya Zhang, Xin Liu, Anbo Dai, Antoni B. Chan, and Zhen Cui. Edittemporal-consistent videos with image diffusion model. ACM Transactions on Multimedia Computing,Communications, and Applications, 2024. Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo,and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pages 39743983, 2018.",
  "Yinghui Xiao and Qingming Zhan. A review of remote sensing applications in urban planning andmanagement in china. In 2009 Joint Urban Remote Sensing Event, May 2009": "Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. InProceedings of the 18th SIGSPATIAL international conference on advances in geographic informationsystems, pages 270279, 2010. Jiaxuan You, Xiaocheng Li, Melvin Low, David Lobell, and Stefano Ermon. Deep gaussian process forcrop yield prediction based on remote sensing data. Proceedings of the AAAI Conference on ArtificialIntelligence, 31(1), Jun 2022. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusionmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847,2023. Ruiqian Zhang, Jian Yao, Kao Zhang, Chen Feng, and Jiadong Zhang. S-cnn-based ship detection fromhigh-resolution remote sensing images. The International Archives of the Photogrammetry, Remote Sensingand Spatial Information Sciences, page 423430, Jun 2016. Xiangrong Zhang, Tianyang Zhang, Guanchun Wang, Peng Zhu, Xu Tang, Xiuping Jia, and Licheng Jiao.Remote sensing object detection meets deep learning: A metareview of challenges and advances. IEEEGeoscience and Remote Sensing Magazine, 2023. Yuanlin Zhang, Yuan Yuan, Yachuang Feng, and Xiaoqiang Lu. Hierarchical and robust convolutionalneural network for very high-resolution remote sensing object detection. IEEE Transactions on Geoscienceand Remote Sensing, 57(8):55355548, 2019.",
  "Zilun Zhang, Tiancheng Zhao, Yulong Guo, and Jianwei Yin. Rs5m: A large scale vision-language datasetfor remote sensing vision-language foundation model. arXiv preprint arXiv:2306.11300, 2023": "Shen Zheng, Changjie Lu, and Srinivasa G Narasimhan. Tpsence: Towards artifact-free realistic raingeneration for deraining and object detection in rain. In Proceedings of the IEEE/CVF Winter Conferenceon Applications of Computer Vision, pages 53945403, 2024. Zhuang Zhou, Shengyang Li, Wei Wu, Weilong Guo, Xuan Li, Guisong Xia, and Zifei Zhao. Nasc-tg2:Natural scene classification with tiangong-2 remotely sensed imagery. IEEE Journal of Selected Topics inApplied Earth Observations and Remote Sensing, page 32283242, Jan 2021. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation usingcycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computervision, pages 22232232, 2017."
}