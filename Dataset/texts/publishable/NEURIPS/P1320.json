{
  "Abstract": "When rows of an n d matrix A are given in a stream, we study algorithms for approximating thetop eigenvector of the matrix ATA (equivalently, the top right singular vector of A). We consider worstcase inputs A but assume that the rows are presented to the streaming algorithm in a uniformly randomorder. We show that when the gap parameter R = 1(A)2/2(A)2 = (1), then there is a randomizedalgorithm that uses O(h d polylog(d)) bits of space and outputs a unit vector v that has a correlation1O(1/",
  "R) with the top eigenvector v1. Here h denotes the number of heavy rows in the matrix, denedas the rows with Euclidean norm at least AF/": "d polylog(d). We also provide a lower bound showingthat any algorithm using O(hd/R) bits of space can obtain at most 1 (1/R2) correlation with thetop eigenvector. Thus, parameterizing the space complexity in terms of the number of heavy rows isnecessary for high accuracy solutions.Our results improve upon the R = (log n log d) requirement in a recent work of Price andXun (FOCS 2024). We note that the algorithm of Price and Xun works for arbitrary order streamswhereas our algorithm requires a stronger assumption that the rows are presented in a uniformly ran-dom order. We additionally show that the gap requirements in their analysis can be brought down toR = (log2 d) for arbitrary order streams and R = (log d) for random order streams. The requirementof R = (log d) for random order streams is nearly tight for their analysis as we obtain a simple instancewith R = (log d/ log log d) for which their algorithm, with any xed learning rate, cannot output avector approximating the top eigenvector v1.",
  "Introduction": "We consider the problem of approximating the top eigenvector in the streaming setting. In this problem,we are given vectors a1, . . . , an Rd one at a time in a stream. Let A be an n d matrix with rowsa1, . . . , an. The task is to approximate the top eigenvector of the matrix ATA. Throughout the paper, weuse v1 Rd to denote the top eigenvector of ATA. We focus on obtaining streaming algorithms that usea small amount of space and can output a unit vector v such that v, v12 1 f(R), where f(R) is adecreasing function in the gap R = 1(ATA)/2(ATA). Here 1(), 2() denote the two largest eigenvalues.As the gap R becomes larger, the eigenvector approximation problem becomes easier and we therefore wantmore accurate approximations to the eigenvector v1.If one is allowed to use O(d2)1 bits of space, we can maintain the matrix ATA = i aiaTi as we see therows ai in the stream, and at the end of processing the stream, we can compute the exact top eigenvectorv1. When the dimension d is large, the requirement of (d2) bits of memory can be impractical (see e.g.,applications that require a large value of d in Mitliagkas et al. .) Hence, an interesting question is tostudy non-trivial streaming algorithms that use less memory. In this work, we focus on obtaining algorithmsthat use O(d) bits of space.In the oine setting (where the entire matrix A is available to us), fast iterative algorithms such as Gu",
  "Work done while the author was a student at Carnegie Mellon University.1The notation O(f(n)) is used to denote the set of functions in O(f(n) polylog(n))": "to the top eigenvector when the gap R = (1). In a single pass streaming setting, we cannot run thesealgorithms as these iterative algorithms need to see the entire matrix multiple times.There have been two major lines of work studying the problem of eigenvector approximation and therelated Principal Component Analysis (PCA) problem in the streaming setting with near-linear in d mem-ory. In the rst line of work, each row encountered in the stream is assumed to be sampled independentlyfrom an unknown distribution with mean 0 and covariance and the task is to approximate the top eigen-vector of using the samples. In this line of work, the sample complexity required for algorithms usingO(d polylog(d)) bits of space to output an approximation to v1, is the main question. The algorithmsare usually a variant of Ojas algorithm [Oja, 1982, Jain et al., 2016, Allen-Zhu and Li, 2017, Huang et al.,2021, Kumar and Sarkar, 2023] or the block power method [Hardt and Price, 2014, Balcan et al., 2016]. Wenote that Kumar and Sarkar relax the i.i.d. assumption and analyze the sample complexity of Ojasalgorithm for estimating the top eigenvector in the Markovian data setting.The other line of work studies algorithms for arbitrary streams appearing in an arbitrary order. In thissetting, we want algorithms to work for any input stream given in any order. A problem closely related to theeigenvector estimation problem is the Frobenius-norm Low Rank Approximation [Clarkson and Woodru,2017, Boutsidis et al., 2016, Upadhyay, 2016, Ghashami et al., 2016]. The deterministic Frequent Directionssketch of Ghashami et al. can, using O(d/) bits of space, output a unit vector u such that",
  "A(I uuT)2F (1 + )A(I v1vT1 )2F": "Although the vector u is a 1 + approximate solution to the Frobenius norm Low Rank Approximationproblem, it is possible that the vector u may be (nearly) orthogonal to the top eigenvector v1. Hence theFrequent Directions sketch does not guarantee top eigenvector approximation. Recently, Price and Xun study the eigenvector approximation problem in arbitrary streams and obtain results in terms of the gap Rof the instance. Price and Xun prove that when R = (log n log d), a variant of Ojas algorithm outputs aunit vector v such that",
  "CR2 ,": "must use (d2/R3) bits of space while processing the stream. This lower bound shows that in the importantcase of R = O(1), the correlation2 that can be obtained by an algorithm using O(d) bits of space is at mosta constant less than 1. Thus, the current best algorithms for arbitrary streams work only when R = (log nlog d) and for the important case of R = O(1), there are no existing algorithms requiring signicantly fewerthan d2 bits of memory. They also give a lower bound on the size of mergeable summaries for approximatingthe top eigenvector.We identify an instance with R = (log d/ log log d) where the algorithm of Price and Xun fails to producea vector with even a constant correlation with the vector v1. This shows that their algorithm or other variantsof Ojas algorithm may fail to extend to the case when R = O(1). We further show that the algorithm ofPrice and Xun fails to produce such a vector even when the rows in our hard instance are ordered uniformlyat random, showing that even randomly ordered streams can be hard to solve for variants of Ojas algorithm.In this work, we focus on algorithms that work on worst case inputs A while assuming that the rowsof A are uniformly randomly ordered. This model is mid-way between the i.i.d. setting and the arbitraryorder stream setting in terms of the generality of streams that can be modeled. We note that a num-ber of works [Munro and Paterson, 1980, Guha et al., 2005, Chakrabarti et al., 2008, Guha and McGregor,2009, Assadi and Sundaresan, 2023] have previously considered streaming algorithms and lower boundsfor worst case inputs with random order streams, as it is a natural model often arising in practical set-tings. See Gupta and Singla for a gentle introduction to the random-order model. Our algorithmsare parameterized in terms of the number of heavy rows in the stream. We dene a row ai to be heavy if",
  "d polylog(d). Note that in any stream of rows, by denition, there are at most dpolylog(d)heavy rows. We state our theorem informally below:": "Theorem 1.1. Let a1, . . . , an Rd be a randomly ordered stream and let A denote the n d matrix withrows given by a1, . . . , an. If R = 1(ATA)/2(ATA) > C for a large enough constant C and the number ofheavy rows in the stream is at most h, then there is a streaming algorithm using O(h d polylog(d)) bits ofspace and outputting a unit vector v satisfying",
  "with a probability 4/5": "Our algorithm is a variant of the block power method. Along the way, we also improve the gap require-ments in the results of Price and Xun . We show that by subsampling a stream of rows, the algorithmof Price and Xun can be made to work even when the gap R is (log2 d) in arbitrary order streams, im-proving on the (log n log d) requirement in their analysis. We also show that in random order streams, agap of (log d) is sucient for their algorithm, though our algorithm improves on this and works for even aconstant gap.Similar to the lower bound of Price and Xun, we show that any algorithm for random order streams mustuse (h d/R) bits of space to output a vector v satisfying v, v12 1 1/CR2 where C is a constant. Wesummarize the theorem below.",
  "for a large enough constant C, with a probability 1 (1/2)R+1 over the ordering of the stream and itsinternal randomness, must use (h d/R) bits of space": "Techniques.The randomized power method [Gu, 2015] algorithm to approximate the top eigenvectorsamples a random Gaussian vector g and iteratively computes the vector v = (ATA)tg3 for t = (log d)iterations and shows that when the gap R is large, v/v2 is a good approximation for v1. Thus, the algorithmneeds to see the quadratic form ATA multiple times and hence, it cannot be implemented in the single-passstreaming setting of this paper.Assume that the stream is randomly ordered and that there are no heavy rows. Our key observation isthat if the stream is long enough, then we can see t approximations BTj Bj4 of the quadratic form ATA. Herethe matrices B1, . . . , Bt are formed by sampling and rescaling the rows of the matrix A and importantly,the rows of B1, . . . , Bt do not overlap in the stream, that is, they appear one after the other. Thus we cancompute v = (BTt Bt) (BT1 B1) g for the starting vector g in a single pass over the stream. We provethat such matrices Bj exist using the row norm sampling result of Magdon-Ismail . Now, the mainissue is to show that v/v2 is a good approximation to the top eigenvector v1. We crucially use a singularvalue inequality of Wang and Xi to prove that BTj Bj ATA2 A22 for all j suces for v/v2to be a good approximation to v1.The above analysis assumes that there are no heavy rows. Indeed, suppose that a matrix A has a row awith a large Euclidean norm which is orthogonal to all the other rows. Also assume that the top eigenvectorof the matrix A is in this direction. Since, the matrices B1, . . . , Bt are non-overlapping substreams of thematrix A, at most one of the matrices Bj can have the row a and hence the vector v/v2 will not be agood approximation to a/a2, the top eigenvector. Thus, we need to handle the heavy rows separately. Weshow that, by storing all the rows with a Euclidean norm larger than AF/",
  "iai, vai.4We use bold symbols to denote random variables": "Our lower bound (Theorem 1.2) shows that any single-pass streaming algorithm must use space propor-tional to the number of heavy rows, and therefore our procedure that handles the heavy rows separatelygives near-optimal bounds.Finally, the row norm sampling technique of Magdon-Ismail serves as a general technique toreduce the number of rows in the stream while (approximately) preserving the top eigenvector. We use thisobservation to improve the R = (log nlog d) for arbitrary streams in Price and Xun to R = (log2 d).We then show that assuming a uniformly random order, the analysis of Price and Xun can be improvedto show that R = (log d) suces. Thus, for random order streams, techniques before our work can be usedto approximate the top eigenvector when the gap R = (log d). Our work improves upon this to give analgorithm that works for streams with R = (1). Implications to practice.Often, in practical situations, we can assume that the rows being streamedare sampled independently from a nice-enough distribution, in which case Ojas algorithm, as discussed, canapproximate the top eigenvector accurately given enough samples. However, independence and assumptionson the covariance matrix can be very strong assumptions in some cases and in such cases, our algorithmonly requires that the order of the rows in the stream be uniformly random, in which case we output anapproximation with provable guarantees. Organization.We rst introduce the row-norm sampling procedure to obtain approximate quadraticforms. The proof is a slight modication of that of Magdon-Ismail . The only dierence is that we insteadconsider a version that samples each row in the input independently with some appropriate probability andkeeps the rows that are sampled after scaling appropriately. We then introduce and analyze our block poweriteration algorithm when all rows have roughly the same Euclidean norm, and then extend it to the generalcase, which is our main result. Finally, we provide a lower bound showing that (td/R) bits of space isnecessary to obtain constant correlation with the top eigenvector.",
  "Power Method with Approximate Quadratic Forms": "In this section, we present and analyze our algorithm for approximating the top eigenvector of ATA whenthe rows of A are presented to the algorithm in a uniformly random order.We rst show a row sampling technique that reduces the number of rows in the stream. The row-normsampling technique for approximating the quadratic form ATA with spectral norm guarantees was given byMagdon-Ismail . The technique works irrespective of the order of the rows.",
  "i=1Y i": "We use the Matrix Bernstein inequality [Tropp, 2015] to bound i Y i2. We rst uniformly upper boundY i2. If pi = 1, by denition Y i2 = 0 with probability 1. Let pi = 0. Then, (Xi/pi 1)aiaTi 2 aiaTi 2/pi 2A22/C log d with probability 1.We now bound i E[Y 2i ]2.",
  "Random-Order Streams with bounds on Norms": "We now present the analysis of the block power method for random order streams assuming that the Euclideannorms of all the rows in A are close to each other. We later remove this assumption. Suppose there exists aparameter such that (maxi ai22)/(mini ai22) . If is close to 1 then all the rows in the stream haveroughly the same norm.",
  "n2= p": "Thus, p is greater than the probability with which we need to sample each row in the row-norm samplingresult in Theorem 2.1. Now if we perform such a sampling of the rows of A, we sample Bin(n, p)5 numberof rows, which is tightly concentrated around np = 2C log d. Thus, if we rst sample y Bin(n, p) andthen consider the rst y number of rows in the random order stream, then we will have sampled from adistribution satisfying the requirements in Theorem 2.1 and can therefore obtain a matrix B such that",
  "BTB ATA2 A22": "Thus, assuming that the rows appear in a uniformly random order lets us show that the rst y rows of thestream can be used to compute an approximation to the quadratic form ATA. We will now show that wecan obtain O(log d) such quadratic forms in the stream given that the stream is long enough.Assume that the number of rows in the stream n = ( log2 d/2). We partition the stream into t =(log d) groups as follows: the rst 2np rows are placed in the group 1, the second 2np rows are placed inthe group 2, and so on. Note that since n = ( log2 d/2), we can form t such groups. Since the rowsare uniformly randomly ordered, the joint distribution of the rows appearing in group 1 is the same as thatof the joint distribution of the rows appearing in group 2 and so on. Let y1, . . . , yt Bin(n, p) be drawnindependently. With probability 1 1/ poly(d), we have yi (3/2)np for all i. For i = 1, . . . , t, let Bibe the matrix formed by the rst yi rows in group i. Using a union bound, we have that with probability 1 1/ poly(d), for all i = 1, . . . , t,",
  "Assumption 2.2. We assume that 1(A)/2(A) 2": "Lemma 2.3. Let > 1/ poly(d) be an accuracy parameter and t = (log d) be the number of iterations. Let c/t2 for a small constant c. Suppose B1, . . . , Bt all satisfy ATA BTj Bj2 A22 for < 1/5. If g isa random vector sampled from the Gaussian distribution, then the unit vector",
  "with probability 9/10 for a large enough constant C. Here v1 denotes the top right singular vector of thematrix A": "To prove this lemma, our strategy is to show that the matrix product M := (BTt Bt) (BT1 B1) has astable rank close to 1 meaning it has one very large singular value and the rest of the singular values aresmall. We can then argue that the vector v = Mg/Mg2 is in the direction of the top singular vector M.Using the fact that vT1 (BTj Bj)v1 (1 )A22 for all j, we show that the top singular vector of M musthave a large correlation with v1. Therefore, it follows that the vector v has a large correlation with v1 aswell. As part of the proof, we crucially use an inequality from Wang and Xi . The full formal proofnow follows. Proof. Dene M := (BTt Bt) (BT1 B1). Our strategy is to show that if v1 is the top singular vector of thematrix A, then vT1 M2 is comparable to MF given that 1(A)/2(A) 2. We can then prove the lemmausing simple properties of the Gaussian vector g.For an arbitrary j, let (BTj Bj)v1 = v1 + where v1. We note that vT1 (BTj Bj)v1 = . We have = vT1 BTj Bjv1 (1)1(A)2 using the fact that BTj BjATA2 A22 and vT1 ATAv1 = 1(A)2 = A22.If we show that is small, then the vector (BTj Bj)v1 is oriented in a direction very close to that of v1. Notethat",
  "with probability 4/5 if 1(A)/2(A) 2. The algorithm uses O(d polylog(d)/4) bits of space": "Proof. Set = 2/C log2 d for a large enough constant C. Assuming n = (4 log6 d), we have n =(2 log2 d). Now consider the execution of Algorithm 1 on matrix A, with parameters and . Let = 2j be such that (A)/2 (A), and consider the execution in the algorithm with parameter .Using Theorem 2.1, with probability 1 1/ poly(d), the algorithm computes t matrices B1, . . . , Bt suchthat for all j [t],",
  "Random Order Streams without Norm Bounds": "Assuming that the random order streams are long enough, Theorem 2.6 shows that if all the squared rownorms are within an factor, then the block power method outputs a vector with a large correlation withthe top eigenvector of the matrix ATA. For general streams, the factor could be quite large and hence thealgorithm requires very long streams to output an approximation to v1.If there are no heavy rows, i.e., rows with a Euclidean norm larger than AF/ d polylog(d), then therow norm sampling procedure in Theorem 2.1 can be used to convert any randomly ordered stream of rowsinto a uniformly random stream of rows that all have the same norm. The row norm sampling procedurecomputes a probability pi = min(1, C2ai22 log d/A22) and samples the row ai with probability pi. Ifsampled, then the row ai is scaled by 1/ pi. From Theorem 2.1, we have that the top eigenvector of thequadratic form of the sampled-and-rescaled submatrix is a good approximation to the top eigenvector ATAwhen the gap R is large enough. Suppose pi < 1. If the row ai is sampled, we then have",
  "C log d": "Thus, if pi < 1 for all i, then all the sampled-and-rescaled rows have the same Euclidean norm and there-fore, we can run the algorithm from Theorem 2.6 by setting = 1. Note that pi = 1 only if ai22 2A22/C log(d). Since we assumed that there are no heavy rows, there is no row with pi = 1 as long as 1/ polylog(d). Thus, using Theorem 2.6 on the row norm sampled substream directly gives us a goodapproximation to the top eigenvector. However, in general, the streams can have rows with large Euclideannorm. We will now state our theorem and describe how such streams can be handled. Theorem 2.7. Let A be an n d matrix with its non-zero entries satisfying 1/ poly(d) |Ai,j| poly(d),and hence representable using O(log d) bits of precision. Let R = 1(A)2/2(A)2. Assume 2 R C1 log2 d.Let h be the number of rows in A with norm at most AF/ d polylog(d), where polylog(d) = logC2 d fora large enough universal constant C2. Given the rows of the matrix A in a uniformly random order, there isan algorithm using O((h + 1) d polylog(d) log n) bits of space and which outputs a vector v such that withprobability 4/5, v satises v, v12 1 8/",
  "R, where v1 is the top eigenvector of the matrix ATA": "The key idea in proving this theorem is to partition the matrix A into Aheavy and Alight, where Aheavydenotes the matrix with the heavy rows and Alight denotes the matrix with the rest of the rows of A. Sincewe assume that there are at most h heavy rows, we can store the matrix Aheavy using O(hdpolylog(d)) bitsof space. Now consider the following two cases: (i) Aheavy2 (1 )A2 or (ii) Aheavy2 < (1 )A2for some parameter . In the rst case, we can show that the top eigenvector u of ATheavyAheavy is a goodapproximation for v1. Since, we store the full matrix Aheavy, we can compute u exactly at the end of thestream. Suppose Aheavy2 < (1 )A2. By the triangle inequality, we have Alight2 > A2. If weset large enough compared to 1/R, then we can show that the top eigenvector u of ATlightAlight is a goodapproximation of v1. From the above discussion, since all the rows of Alight are light, we can obtain a streamusing Theorem 2.1 such that all the rows have the same norm and additionally, the top eigenvector of thisstream is a good approximation for u and therefore v1. We then approximate the top eigenvector of the newstream using Theorem 2.6. Setting appropriately, we show that this procedure can be used to compute avector v satisfying v, v12 1 O(1/",
  "Proof. Partition the matrix A into Alight and Aheavy, where Aheavy is the submatrix with rows ai such thatai2 > AF/": "d polylog(d) and Alight is the remaining rows. From our assumption, the number of rowsin Aheavy is at most h. Note that given a uniformly random stream of rows of A, we can obtain a uniformlyrandom stream of rows of Alight by just ltering out the rows in Aheavy.Suppose, Aheavy v12 (1 )A2 for a parameter to be chosen later. Let v1 be the top singularvector of the matrix Aheavy. Note",
  "A v122 Aheavy v122 Aheavy v122 (1 )2A22": "and therefore we have v1, v12 1 4, assuming R 2. Thus, while processing the stream, we can storeall the heavy rows and at the end of the stream compute the top right singular vector of Aheavy, in order toobtain a good approximation for v1.Suppose Aheavy v12 (1 )A2. This implies Alight v122 A22 Aheavy v122 A22. Ifwe set 2/R, we have",
  "polylog(d)": "Assuming that 22 1/ polylog(d), we obtain that pi < 1 for all the rows in the matrix Alight. Let Blightbe the matrix obtained after applying the row norm sampling procedure to the matrix Alight. Note that(Blight) (Alight) and the number of rows in Blight is ((Alight) log d 2), and therefore ((Blight) log d 2). Setting = 2/ log5/2 d, we obtain that the number of rows in the matrix Blight is (4 (Blight) log6 d) and thus assuming 22 = 42/ log5 d 1/ polylog(d), we can use Theorem 2.6 to obtaina vector v satisfying",
  "R. Thus, in both the cases, we obtaina vector v satisfying v, v12 1 O(1/": "R).The procedure described requires knowing the approximate values of AF, Alight2. Since, we assumethat all the non-zero entries of the matrix have an absolute value at least 1/ poly(d) and at most poly(d),the values AF, Alight2 lie in the interval [1/ poly(d), poly(nd)]. Hence, using O(log nd) guesses each forAF and Alight2 and using a Gaussian sketch of A similar to that in Algorithm 1, we can obtain a vectorsatisfying the guarantees in the theorem.",
  "Theorem 3.1. Given a dimension d, let h and R be arbitrary with R h d and R2 h = O(d). Consideran algorithm A with the following property:": "Given any xed matrix n d matrix A with O(h) heavy rows and gap 1(A)2/2(A)2 R, in the form of auniform random order stream, the algorithm A outputs a unit vector v such that, with probability 1 (1/2)4R+4 over the randomness of the stream and the internal randomness of the algorithm,|v, v1|2 1 c/R2.",
  "If c is a small enough constant, then the algorithm A must use (h d/R) bits of space": "The theorem shows that a streaming algorithm must use (hd/R) bits of space assuming that withhigh probability, it outputs a vector with a large enough correlation with the top eigenvector of ATA whenthe rows are given in a random order stream. Our proof uses the same lower bound instance as that ofPrice and Xun . The key dierence from their proof is that our lower bound must hold against randomorder streams. Proof. For each i [h], let x1, . . . , xh be drawn independently and uniformly at random from { +1, 1 }d.Let i [h] be drawn uniformly at random, and for an integer k to be chosen later, let y1, . . . , yk Rd bevectors that share the rst (1 )d coordinates with the vector xi. Each of the last d elements of eachof y1, . . . , yk are sampled uniformly at random from the set { +1, 1}. Dene z1, . . . , zh+k such that forj h, zj = xj and for j > h, let zj = yjh.Now consider the stream z1, . . . , zh+k. Price and Xun argue that when k 4R, the gap of this streamis at least R with large probability over the randomness used in the construction of the stream. Let :[h + k] [h + k] be a uniformly random permutation independent of i. Consider the following event E:",
  "Arbitrary Order Streams": "As discussed in .1, we can guess an approximation of A22 in powers of 2 and sample at mostO(d log d/2) rows in the stream to obtain a matrix B, in the form of a stream, satisfying BTB ATA2 A22, with a large probability. Using Weyls inequalities, we obtain that",
  "(BTB) 2(ATA) + A22and1(BTB) (1 )1(ATA)": "implying R = 1(B)2/2(B)2 (1 )/(1/R + ). For = 1/(2R) 1/2, we note R R/3. Letn = O(R2 d log d) be the number of rows in the matrix B and note that R = (log n log d) assumingR = (log2 d). Hence, running the algorithm of Price and Xun on the rows of the matrix B, we compute avector v for which",
  "n i + 1": "Hence E[ai, P vi12] 2(A)2/(n i + 1) and E[ni=1ai, P vi12] 2(A)2(1 + log n). Price and Xundene 2(A)2 as 2 and in that notation, we obtain ni=1ai, P vi12 102(1 + log n) with probability 9/10 by Markovs inequality. In the proof of Lemma 3.6 in Price and Xun , if 1/2 20(1+log2 n),we obtain log vn2 1. Now, 1 O(log d) ensures that the Proof of Theorem 1.1 in their work goesthrough.Using the row-norm sampling analysis from the previous section, we can assume n = poly(d) and thereforea gap of O(log d) between the top two eigenvalues of ATA is enough for Ojas algorithm to output a vectorwith a large correlation with the top eigenvector in random order streams.",
  "Hard Instance for Ojas Algorithm": "At a high level, the algorithm of Price and Xun runs Ojas algorithm with dierent learning rates and in the event that the norm of the output vector with each of the learning rates is small, then the rowwith the largest norm is output. The algorithm is simple and can be implemented using an overall space ofO(d polylog(d)) bits.The algorithm initializes z0 = g where g is a random Gaussian vector. The algorithm streams throughthe rows a1, . . . , an and performs the following operation",
  "zi zi1 + zi1, aiai": "The algorithm computes the smallest learning rate when zn2 is large enough, and then outputs eitherzn/zn2 or a/a2 as an approximation to the eigenvector of the matrix ATA. Here a denotes the row in Awith the largest Euclidean norm.The following theorem shows that at gaps O(log d/ log log d), we cannot use Ojas algorithm with axed learning rate to obtain constant correlation with the top eigenvector.",
  "R)e3": "where = 2M. Let A be a matrix with rows given by the stream of vectors dened above. We note that thematrix A has rank 3 and the non-zero eigenvalues of the matrix ATA are 1, 1/(R ), 1/R and therefore thegap 1(ATA)/2(ATA) = R . The top eigenvector of the matrix ATA is e1 and the row with the largestnorm is (1/ R )e2. Thus, the row with the largest norm is not useful to obtain correlation with the truetop eigenvector e1.Consider an execution of Ojas algorithm with a learning rate on the above stream of vectors. The nalvector zn can be written as",
  "z03": "We note that znj = z0j for all j > 3. Since = 2M, we have /R 1/2 and therefore (1 + /R) exp(/2R) and (1 + /R) exp(/2R).Recall that we want to show that |zn, e1| < czn2 with a large probability. Suppose otherwise and thatwith probability 1/10, we have |zn, e1| > czn2 > c(0, 0, 0, z04, . . . , z0d)2.Since, z0 is initialized to be a random Gaussian, we have (0, 0, 0, z04, . . . , z0d)2",
  "The authors were supported in part by a Simons Investigator Award and NSF CCF-2335412. D. Woodruwas visiting Google Research while performing this work": "Zeyuan Allen-Zhu and Yuanzhi Li. First ecient convergence for streaming k-PCA: a global, gap-free, andnear-optimal rate. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),pages 487492. IEEE, 2017. 2 Sepehr Assadi and Janani Sundaresan. (Noisy) gap cycle counting strikes back: Random order streaminglower bounds for connected components and beyond. In Proceedings of the 55th Annual ACM Symposiumon Theory of Computing, pages 183195, 2023. 2",
  "Maria-Florina Balcan, Simon Shaolei Du, Yining Wang, and Adams Wei Yu. An improved gap-dependencyanalysis of the noisy power method. In Conference on Learning Theory, pages 284309. PMLR, 2016. 2": "Christos Boutsidis, David P Woodru, and Peilin Zhong. Optimal principal component analysis in distributedand streaming models. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing,pages 236249, 2016. 2 Amit Chakrabarti, Graham Cormode, and Andrew McGregor. Robust lower bounds for communicationand stream computation. In Proceedings of the fortieth annual ACM symposium on Theory of computing,pages 641650, 2008. 2"
}