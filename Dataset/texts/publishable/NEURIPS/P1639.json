{
  "Abstract": "Particle-based Bayesian deep learning often requires a similarity metric to comparetwo networks. However, naive similarity metrics lack permutation invariance andare inappropriate for comparing networks. Centered Kernel Alignment (CKA) onfeature kernels has been proposed to compare deep networks but has not been usedas an optimization objective in Bayesian deep learning. In this paper, we explorethe use of CKA in Bayesian deep learning to generate diverse ensembles andhypernetworks that output a network posterior. Noting that CKA projects kernelsonto a unit hypersphere and that directly optimizing the CKA objective leads todiminishing gradients when two networks are very similar. We propose adoptingthe approach of hyperspherical energy (HE) on top of CKA kernels to address thisdrawback and improve training stability. Additionally, by leveraging CKA-basedfeature kernels, we derive feature repulsive terms applied to synthetically generatedoutlier examples. Experiments on both diverse ensembles and hypernetworksshow that our approach significantly outperforms baselines in terms of uncertaintyquantification in both synthetic and realistic outlier detection tasks.",
  "Introduction": "Bayesian deep learning has always garnered substantial interest in the machine learning community.Instead of a point estimate which most deep learning algorithms obtain, a posterior distribution oftrained models could significantly improve our understanding about prediction uncertainty and avoidoverconfident predictions. Bayesian deep learning has potential applications in transfer learning,fairness, active learning, and even reinforcement learning, where reducing uncertainty can be used asa powerful intrinsic reward function (Yang & Loog, 2016; Ratzlaff et al., 2020; Wang et al., 2023). One line of approach to Bayesian deep learning is to add noise to a single trained model. Suchnoises can either be injected during the training process, e.g. as in the stochastic gradient Langevindynamics (Welling & Teh, 2011), or after the training process (Maddox et al., 2019). However, manysuch approaches often underperform the simple ensemble method (Lakshminarayanan et al., 2017b)which merely trains several deep networks with different random seeds. Intuitively, an ensemble,because it starts from different random initializations, might be able to explore\" a larger portion ofthe parameter space than those that are always nearby one specific model or training path. Because ofthis, ensembles may capture different modes and therefore better represent the posterior distributionof well-trained\" network functions (Fort et al., 2019; Wilson & Izmailov, 2020). However, a critical question is, how different are the networks in an ensemble from one another? Andcan we utilize the idea of diversification to further improve these networks by making them even morediverse? In order to answer these questions, we first need a metric to compare those networks, which isin itself a significant problem; regular L1/L2 distances, either in the space of the network parameters,or in the space of the network activations, are not likely to work well. First, they suffer from the curse",
  "Hyperspherical Energy": "Model 1 Model M : Overview of feature repulsive loss construction: Starting with a batch of examples (left),optionally including synthetic outliers, ensemble features at each layer l are used to construct centeredGram matrices projected onto the unit hypersphere (middle). The hyperspherical energy is thencalculated between models, weighted by layer, and incorporated into the loss function (right). of dimensionality due to the excessive number of parameters in modern deep networks. Moreover,there is the peculiar permutation invariance, where one can randomly permute the different channelsof each layer and result in a network that has vastly different parameters and activations, yet representsthe same function. The popular RBF kernel lacks this permutation invariance inhibiting methods likeStein Variational Gradient Descent (SVGD) from working effectively on larger networks (D' Angelo& Fortuin, 2021). Therefore, a proper kernel for comparing network functions should address thesecritical issues by being effective in high-dimensional spaces and invariant to permutations of neuralnetwork channels. Kornblith et al. (2019) proposed an interesting approach for performing this comparison basedon Centered Kernel Alignment (CKA). The idea is, instead of directly comparing activations orparameters, comparison is made between the Gram matrices of the same dataset fed into two differentnetworks. Each example will generate a feature vector at each layer of the network, and a kernelmatrix can be constructed based on the similarity between all example pairs in the dataset. Then, aCKA metric measures the similarity of these two Gram matrices as the similarity of the two networks.This idea addresses the permutation invariance issue and generates meaningful comparisons betweendeep networks. In this paper, we propose to explicitly promote diversity of network functions by adding CKA-basedloss terms to deep ensemble learning. Given that CKA projects all kernels on a hypersphere, wefurther propose to use Hyperspherical Energy (HE) minimization as an approach to more evenly dis-tribute the ensemble of neural networks on the hypersphere. Experiments on synthetic data, MNIST,CIFAR, and TinyImageNet show that our approach maintains the predictive accuracy of ensemblemodels while boosting their performance in uncertainty estimation across both synthetic and realisticdatasets. Besides, we demonstrate that our method can also be applied to training hypernetworks,improving the diversity and uncertainty estimation of the networks generated by a hypernetwork. Ad-ditionally, we propose using synthetic out-of-distribution (OOD) examples, to reduce their likelihood,and introducing feature repulsive terms on synthetic outlier examples to enhance OOD detectionperformance. We hope that our approach provides a different perspective to variational inferencemethods and contributes to improving uncertainty estimation in deep networks. Code is publiclyavailable at",
  "Uncertainty Estimation. A large body of literature has studied the problem of uncertainty estimationfor neural networks. Bayesian neural networks (Gal & Ghahramani, 2016; Krueger et al., 2017;": "Nazaret & Blei, 2022) approximate a posterior distribution over the model parameters and thereforeestimate the epistemic uncertainty of the predictions. Non-Bayesian approaches, on the other hand,rely on bootstrap (Osband et al., 2016), ensemble (Lakshminarayanan et al., 2017b; Wen et al.,2020; Park & Kim, 2022), and conformal prediction (Bhatnagar et al., 2023) to generate multipleneural networks of the same structure. Our approach is most closely related to ensemble methodsfor estimating predictive uncertainty. We follow the common practice of evaluating uncertainty bydistinguishing between inlier and outlier images for datasets like CIFAR/SVHN and MNIST/FMNIST.Most approaches typically evaluate the separation or distance within the feature space of a modelbetween inliers and outliers (Mukhoti et al., 2023; Van Amersfoort et al., 2020; D' Angelo & Fortuin,2021; Ovadia et al., 2019b; Lakshminarayanan et al., 2017a; Liu et al., 2020). Deep deterministicuncertainty (DDU) utilizes a single deterministic network with a Gaussian mixture model (GMM)fitted on the observed inlier features before the last layer and calculates separation of inliers andoutliers using feature log density. We refer the reader to Mukhoti et al. (2023) for more information.For a more comprehensive survey and benchmarking of different uncertainty estimation approaches,refer to Ovadia et al. (2019a); Gawlikowski et al. (2022). ParVI. Particle-based variational inference methods (ParVI), such as Stein Variational GradientDescent (SVGD) (Liu & Wang, 2016; Chen et al., 2018; Liu & Zhu, 2018), use particles toapproximate the Bayes posterior. Our work most closely resembles work done by D' Angelo &Fortuin (2021), which explores adapting kernelized repulsive terms in both the weight and functionspace of deep ensembles to increase model diversity and improve uncertainty estimation. Our work,however, focuses more on constructing a new kernel rather than exploring new repulsive terms thatutilize an RBF kernel on weights or network activations. Hypernetworks. Hypernetworks have been used for various specific tasks, some are conditioned onthe input data to generate the target network weights, such as in image conditioning or restoration(Alaluf et al., 2022; Aharon & Ben-Artzi, 2023). It has seen popular use in meta-learning tasks relatedto reinforcement learning (Beck et al., 2023, 2024; Sarafian et al., 2021), and few-shot learning inZhmoginov et al. (2022). Hypernetworks conditioned on a noise vector to approximate Bayesianinference have been proposed (Krueger et al., 2018; Ratzlaff & Fuxin, 2019), but either require aninvertible hypernet or do not diversify target features explicitly. Our motivation is to provide Bayesianhypernetworks by explicitly promoting feature diversity in target networks.",
  "Measurements of Network Diversity": "In order to generate an ensemble of diverse networks, we first need a measurement of similaritybetween internal network features. Throughout this paper, we denote a deep network with Llayers as f(x, ) = fL(f(...)(f1(x, 1), ), L), where fl(x, l) Rpl are the features of layer lparameterized by l, where pl N is the output feature dimension.",
  "Comparing two networks with CKA": "We will compare two networks with the same architecture layer-by-layer. Given two networks atlayer l with weights 1l , 2l and feature activations fl(x, 1l ), fl(x, 2l ), a naive approach would be totake some Euclidean Lk norm between the weights 1l 2l k or features fl(x, 1l ) fl(x, 2l )k,but those tend to be bad heuristics for similarity measures in high-dimensional vector spaces dueto the curse of dimensionality (Reddi et al., 2014; Aggarwal et al., 2001; Weber et al., 1998). Abetter approach to measuring similarity would be to analyze the statistical independence or alignmentof features between networks, through Canonical Correlation Analysis (CCA), Singular VectorCCA (SVCCA), Projection-Weighted CCA (PWCCA), Orthogonal Procrustes (OP), Hilbert-SchmidtIndependence Criterion (HSIC), or Centered Kernel Alignment (CKA)(Raghu et al., 2017; Grettonet al., 2005; Kornblith et al., 2019). Ideally, the chosen metric should be computationally efficient,invariant to isotropic scaling, orthogonal transformations, permutations, and be easily differentiable.However, CCA methods and OP require the use of Singular Value Decomposition (SVD) or iterativeapproximation methods, which can be computationally intensive. Additionally, HSIC and OP are notinvariant to isotropic scaling of fl. As a comparison metric between networks, Kornblith et al. (2019) propose to utilize CKA on Grammatrices, obtained by evaluating the neural network on a finite sample. CKA is based on the non-parametric statistical independence criterion HSIC, which has been a popular method of measuring statistical independence as a covariance operator in the kernel Hilbert spaces (Gretton et al., 2005).An empirical estimation of HSIC on a dataset of N examples is given by 1/(N 1)2tr(K1HK2H),where the two Gram matrices K1i,j = k(fl(xi, 1l ), fl(xj, 1l )) and K2i,j = l(fl(xi, 2l ), fl(xj, 1l ))are constructed through the k(, ) kernel function, and H = I 1 N 11 a centering matrix to centerthe Gram matrices around the row means, where 1 denotes the all ones vector, and I as the identitymatrix. This function, however, is not invariant to isotropic scaling. The isotropic scaling invariantversion of HSIC is termed Centered Kernel Alignment (CKA) (Kornblith et al., 2019),",
  "Generalizing to multiple networks": "Given an ensemble of M models, a simple approach to generalizing Eq. (1) to measure the similarityof an ensemble would be to construct a pairwise alignment metric. For each layer l of each memberof the ensemble m, we construct the set of kernel matrices K = {Kml }m=1,...,Ml=1,...,L. The mean pairwiseloss across all layers L is as follows,",
  "CKA(Kml , Kml),(2)": "In its current form, CKApw provides a good approximate metric to evaluate the similarity amongmembers of an ensemble. We found that rewriting Eq. (2) gives us another perspective on optimizingCKA. First, to simplify notation, let Km =1 KmHF vec(KmH) be the centered and normalizedGram matrix, and rewriting the inner product in Eq. (1) results in the cosine similarity metricCKA(Km, Km) =Km Km. The matrix of the vectorized kernels from the set Kl can berepresented in a compact form Kl, and CKApw can be rewritten using this compact form,",
  "where zd(X) = X (11 I) is a function that zeros out the diagonal of a matrix": "Now each row m of Kl is a vectorized Gram matrix with unit length from the model m. We can viewthese vectors as the Gram matrices projected on the unit hypersphere as shown in . For eachpair of models i, j on the hypersphere, with an angle i,j between the feature gram vectors, CKA isequivalent to cos(i,j). Thus minimizing pairwise CKA would reduce the sum of cos(i,j), pushingGram matrices between model pairs apart.",
  "Comparing Networks with Hyperspherical Energy": "Note that CKA suffices as a differentiable measure between deep networks and one can directlyminimize CKA to push different models in the ensemble apart from each other. However, CKA mayhave a specific deficiency as an optimization objective in that the gradient of cos() is sin(),which is close to 0 when is close to 0. In other words, if two models are already very similarto each other (their CKA being close to 1), then optimizing with CKA may not provide enoughgradient to move them apart. Hence, we explore further techniques to alleviate this drawback.Minimum Hyperspherical Energy (MHE) (Liu et al., 2021) aims to distribute particles uniformly on ahypersphere, which maximizes their geodesic distances from each other. In physics, this is analogousto distributing electrons with a repellent Coloumbs force.",
  "(e) HE Objective": ": Comparison between optimizing cosine similarity (cossim) or HE on a sphere. (a) initial random setof points placed on sphere. (b-c) the final set of points after 50 iterations either cossim or HE as the similaritymetric. (d-e) the value of cossim/HE with respect to the number of iterations. The orange line indicates thatcossim is minimized and the black line indicates that HE with s = 2 is minimized. Both methods used gradientdescent with a learning rate of 0.75 and momentum 0.9. layer l we treat the M model Gram vectors Kmlas particles on the hypersphere, its geodesic on thehypersphere is then di,j = arccos(CKA(Kil , Kjl )) = arccos( KilKjl ), we define the energy functionby simulating a repellent force on the particles via Fi,j = (di,j)s as shown in Fig 1. Incorporatingthis across all layers, weighted by wl, and model pairs results in the overall hyperspherical energy ofCKA between all models is.",
  "where s > 0 is the Riesz s-kernel function parameter. For more information regarding the layerweighting wl and smoothing terms please see Appendix C": "HE has been shown as a proper kernel (Liu et al., 2021). The minimization of HE, as mentioned inLiu et al. (2021), asymptotically corresponds to the uniform distribution on the hypersphere, in orderto demonstrate the difference between HE and the pairwise cosine similarity, we conducted a test on asynthetic dataset by generating random vectors from two Gaussian distributions in R3, and projectingon the unit hypersphere. We then minimized pairwise cosine similarity and HE respectively. illustrates that minimizing HE converges faster and achieves a more uniform distribution comparedto minimizing cosine similarity. Specifically, as observed in (b), minimizing the cosinesimilarity loss caused particles to cluster towards two opposite sides of the sphere, as the gradient ofthis optimization as mentioned in the beginning of the subsection, becomes very small betweenparticles that are clustered together. In (d), we show that minimizing HE actually leads tolower cosine similarity than directly minimizing cosine similarity, showing that minimizing cosinesimilarity could fall into local optima as described.",
  "Particle-based Variational Inference by Minimizing Model Similarity": "Armed with the comparison metrics between deep networks, we now proceed to incorporate theminimization of network similarity into deep ensemble training. In this section, we explore twodifferent types of ensembles. The first is a regular ensemble where deep networks are trained tomaximize the data likelihood, and we would add a term minimizing model similarity to it. Afterwards,we also explore the application of the idea on generative ensembles by hypernetworks, which aims totrain a generator that generates network weights so that one can directly sample the posterior from it.Such a generator can easily exhibit mode collapse by always generating the same function, and wehope the idea of minimizing the similarity of generated networks would help alleviate this issue. Suppose we are given a deep network with L layers f(x, ) = fL(f(...)(f1(x, 1), ), L). Wedenote the ensemble of target network layer at layer l as El(x, ) = [fl(x, 1), ..., fl(x, M)], withthe ensemble parameters = {m}Mm=1, and the training set of N examples as D = {xi, yi}Ni=1.From a Bayesian perspective, incorporating CKApw/HE-CKA into the ensemble training can beinterpreted as imposing a Boltzmann prior with HE-CKA over the ensemble network parameters that produce feature Gram matrices uniformly distributed on the unit hypersphere. Specifically,p() exp( HE-CKA(K(E()(, )), where K(E()(, )) is the set of feature Gram matrices,constructed from the ensemble E, as described in Sec. 3.2. The posterior distribution now becomes:",
  "(j) Hypernetwork +OOD HE-CKA": ": Predictive entropies (PE) on a four-cluster 2D classification task. Darker values indicate higherentropy, lower confidence regions, and lighter values indicate higher confidence regions. (b) and (d) use anRBF kernel on ensemble member weights, whereas (c) and (e) use an RBF kernel on ensemble member outputs.(f) and (g) use the HE-CKA, RBF feature kernel, for feature diversity on inlier points. Both (h) and (j) useHE-CKA and OOD entropy terms. All methods were trained on an ensemble of 30 four layer MLPs for 1kiterations with the same seeds.",
  "+ HE-CKA(K(E()(, )),(6)": "The left hand side is the negative log likelihood term where L(x, y) is the target loss, such as cross-entropy or MSE. Minimizing , while adjusting the constant used in the Boltzmann prior, allowsus to balance between gram matrix hyperspherical uniformity and fitting the training data. Furtherexplanation of Eq. (6)s relationship to ParVI is given in Appendix A. Note that the same approachcan be used to derive the formula for the CKA kernel in Eq. (2) as well.",
  "Diverse Generative Ensemble with Hypernetworks": "Besides diversifying ensemble models, we also explore using CKApw / HE-CKA in learning a non-deterministic generator (Krueger et al., 2018) which gives us the ability to sample from a continuousnonlinear posterior distribution of network weights. This is appealing since it can generate anyamount of network with a single training run of the generator, without being restricted by the fixedamount of posterior samples one can access with a regular ensemble. The approach we take uses the concept of hypernetworks (Ha et al., 2016; Krueger et al., 2018).However, current variational inference methods are not scalable to larger models and generally requirea change of variables or invertible functions (Krueger et al., 2018). Naively using a hypernetworkto transform a prior distribution to generate of the target network may result in the collapse ofthe posterior distribution. Hence, it would be interesting to explore using CKApw / HE-CKA toavoid such mode collapses. We use the surrogate diversity loss in Eq. (4) to impose non-parametricindependence of feature distributions. With hypernetworks we aim to transform, using a network h(z),some prior distribution z N(0, I), z RP to h(z) = R l wl, where P is the dimensionalityof the latent space, and wl the number of parameters for layer l. To learn the function h() we samplea batch M of s, feed through the ensemble E(x, ) and calculate loss, similar to a fixed ensemble asin Eq. (6). With the difference being that now we are backpropagating gradients to h() accumulatedfrom the M ensemble members. Using a plain MLP for the hypernetwork h would require the last layers weight matrix to containl wl J entries, where J is the activation dimension right before the last layer. This could possiblyresult in a matrix of millions of trainable parameters. To overcome this challenge we follow theapproach by Ratzlaff & Fuxin (2019) of decomposing h into several parts. First a layer code generatorh(z) = c RL,csize, and the layer generators l = gl(cl), where each layer generator g is a separatesmaller network per layer l. See for a visualization. Note c is a matrix with L layer codes ofsize csize. Conv2D Conv2D",
  ": Hypernetwork h(z) model architecture example on a four layer CNN": "To further reduce size of the hypernetwork, for convolutional networks, we use the assumption thatfilters in convolutional layers can be independently sampled. For each convolutional layer l wecreate layer code vectors via the layer code generator cl = h(zl), where each code vector i in clicorresponds to a latent vector for a single convolution filter i. We feed each filter code i through afilter generator gl(cli) separately to generate the filter for layer l. An example architecture can beseen in .",
  "Synthetic OOD Feature Diversity": "Striking a balance between ensemble member diversity and inlier performance is a challenge. En-forcing strong feature dissimilarity on observed inlier examples could degrade inlier performance ifnot tuned correctly. ParVI methods that only observe inlier points, like SVGD, can achieve betterdiversity but often at the expense of inlier accuracy (D' Angelo & Fortuin, 2021). We have found thata more effective strategy is to reduce the feature similarity on obvious OOD examples, and reducetheir likelihood, which could be synthetically generated. Intuitively, we want more diverse featureson obvious outlier examples to indicate uncertainty because the networks trained on these examplesshould not be confident. We found this approach to generate OOD examples and increase their featurediversity to be very effective. Importantly, the OOD points do not need to be close to the inlier data manifold at all. For images, wegenerate outlier points via random grids, lines, perlin noise, simplex noise, and vastly distorted andbroken input samples. See Appendix E.2 for more details and example images. For vector datasets,such as the test 2D datasets presented in , we identify outlier points by locating the minimumand maximum values across training examples. Generally, the boundary does not need to be closeto the in-distribution (ID) dataset to achieve good results. We split the Gram matrices into KID andKOOD and apply HE-CKA to them separately, with respective hyperparameters ID and OOD. Theparameter value ID can be adjusted to be smaller than OOD. Additionally, for classification tasks, weadd an entropy-maximizing term, scaled by hyperparameter , for synthetic OOD points to Eq. (6).Similar loss terms may be constructed for other tasks, such as variance for regression tasks, but wehave not explored them yet.",
  "Synthetic Data": "We start by testing our approach on two synthetic tasks to visually assess the uncertainty estimationcapability on both classification and regression problems. The first task is a 2D four-class classificationproblem, where each class is distributed in one quadrant of the 2D space with points sampled fromGaussians with = (.4, .4) and = (2, 2). The objective is to evaluate whether the modelscan accurately predict uncertainty, ideally showing low uncertainty near training examples and highuncertainty elsewhere. We employed a three-layer MLP trained with cross-entropy on the four classesand measured the predictive entropy of points sampled uniformly from a 10 10 grid. When using cross-entropy alone, the decision boundaries among the four classes tend to be verysimilar. Deep ensembles classify with high confidence in most areas where they have never observeddata before ((a)). Introducing the HE-CKA diversity term to the ensemble significantlyreduces the ensembles confidence on points outside the in-distribution set ((g)). Furthermore,incorporating the HE-CKA and entropy term for OOD points allows the model to better estimateuncertainty, with only inliers being confident ((h)). In the case of hypernetworks, we observethe importance of a diversity term. Without it, hypernetwork predictions tend to be overconfident onoutliers ((i)). However, when introducing HE-CKA hypernetwork, we achieve results closelyresembling that of the ensemble + HE-CKA term ((j)). In our second test, we perform a 1D regression modeling task. We aim to learn the function y(x) = sin (1.2x) (1 + x) within x (6, 6) with high certainty everywhere except in x (2, 2). Thetraining dataset involves sampling the function with 40 points uniformly from both (6, 2) and(2, 6), with 2 points from (2, 2). We then fit a four layer MLP to approximate y(x). The visual result of each method is shown in . The fixed ensemble ((a)) has little diversitybetween the areas with low density, in contrast to the ensemble plus the HE-CKA term ((b)).The hypernetwork, without any feature diversity term ((c)) collapses, producing very similarweights. However, adding the HE-CKA term to the hypernetwork ((d)) alleviates this issue.",
  "OOD Detection on Real Datasets": "We evaluated our proposed approach on a variety of real-world datasets, including Dirty-MNIST,Fashion-MNIST, CIFAR-10/100, SVHN, and TinyImageNet. We employ different CNN architecturessuch as LeNet, ResNet32, and ResNet18 to demonstrate the versatility of our method across models ofvarying complexity. Our experiments compare the out-of-distribution (OOD) detection performanceof our approach against several approaches, including Deep Deterministic Uncertainty (DDU), deepensembles and Stein Variational Gradient Descent (SVGD) equipped with the RBF kernel. We provide experimental settings and training details here and additionally in Appendix C. Limita-tions of this approach are discussed in Appendix D, while further insights into memory usage andcomputational efficiency are discussed in Appendix G. Details regarding synthetic OOD examplegeneration is described in Appendix E.2. : OOD detection results with inlier Dirty-MNIST and outlier Fashion MNIST, over 5 runs. All modelswere trained on a LeNet, with HE-CKA and CKApw utilizing a cosine similarity feature kernel. One exceptionto predictive entropy (PE) report is DDU, which uses feature space density, indicated by a star, to calculateAUROC Mukhoti et al. (2023). More training details can be found in Appendix C.",
  "PEMI": "SVGD + RBF()0.287 0.00185.142 0.0175.200 0.10082.50 0.10071.00 0.200FSVGD + RBF()0.292 0.00185.510 0.0314.900 0.10078.30 0.10071.20 0.100KDE WGD + RBF()0.276 0.00185.904 0.0305.300 0.10083.80 0.10073.50 0.400SGE WGD + RBF()0.275 0.00185.792 0.0355.100 0.10083.70 0.10072.50 0.400KDE FWGD + RBF()0.282 0.00184.888 0.0304.400 0.10079.10 0.10075.80 0.200SGE FWGD + RBF()0.288 0.00184.766 0.0604.700 0.10079.50 0.10075.40 0.200ENSEMBLE()0.277 0.00185.552 0.0764.900 0.10084.30 0.40073.60 0.500SVGD + HE-CKA0.255 0.00885.890 0.3813.675 0.08989.232 1.21170.977 1.445SVGD + CKApw0.286 0.00284.833 0.2924.945 0.18588.893 1.51370.327 1.763DDU0.211 0.00284.695 0.03614.26 0.87286.281 0.020*ENSEMBLE + OOD HE-CKA0.275 0.00986.133 0.4825.436 0.59996.478 0.41396.606 1.066HYPERNET + OOD HE-CKA0.259 0.00283.640 0.0461.115 0.05088.121 0.18288.811 0.134 Dirty-MNIST vs Fashion MNIST. The Dirty-MNIST vs Fashion MNIST OOD benchmark (Mukhotiet al., 2023) examines the capability of models to discern inliers, OOD data in a similar distributionand OOD data in a more dissimilar distribution. This dataset combines MNIST with more ambiguousand challenging examples known as ambiguous MNIST (AMNIST). We examine the ability to todistinguish MNIST and AMNIST from the out-of-distribution Fashion MNIST (FMNIST) (Xiao et al.,2017). We trained a LeNet5 on Dirty-MNIST and assessed OOD classification between Dirty-MNISTand FMNIST using predictive entropy (PE) and mutual information (MI). The area under the receiveroperating characteristic (AUROC) is used to assess the separability between MNIST and FMNIST(Lecun et al., 1998; Bradley, 1997). For DDU a GMM is fit to the second to last layers featuresover Dirty-MNIST, using the log density of features to distinguish inliers from outliers, rather thanpredictive entropy. Results shown in and . Ensembles ((a)) do not exhibit significant separationusing predictive entropy (PE) alone, resulting in low AUROC with PE for FashionMNIST. Whilemethods like SVGD, equipped with an RBF kernel, improve separation ((b)), our approachdemonstrates that using HE-CKA on an ensemble alone surpasses both RBF kernels and DDUsapproach, and when paired with OOD examples, and OOD likelihood minimization, results in almostperfect separation with 99.99% AUROC. CKA plots of each method are presented in Appendix F.: OOD results on CIFAR-10 vs SVHN. Methods used a ResNet18 ensemble of size 5. () indicatesensemble size.",
  "SVHNCIFAR 10/100TEXTURES (DTD)": "ENSEMBLE0.77562.958.9089.8166.85/67.3368.96SVGD+RBF0.92661.8716.1092.7672.23/73.7365.67SVGD+CKApw0.83560.158.2694.0878.40/79.4866.48SVGD+HE-CKA0.73261.363.7194.1072.05/72.8670.75ENSEMBLE+HE-CKA0.78463.109.8292.6572.13/71.6870.69ENSEMBLE+OOD HE-CKA0.78661.888.0299.3181.56/87.6490.94 CIFAR-10/100 vs SVHN. We further evaluated our method on CIFAR-10 and CIFAR-100 datasets,testing outlier detection performance on SVHN () (Netzer et al., 2011). For a fair comparisonwith D' Angelo & Fortuin (2021), we trained ResNet32 ensembles following the training procedureand parameters described by D' Angelo & Fortuin (2021). For more details regarding model archi-tecture please refer to the aforementioned paper and published code. We used predictive entropyand mutual information for the OOD classification, with the exception of DDU using feature spacedensity (Mukhoti et al., 2023). Given that the network presented in D' Angelo & Fortuin (2021) has significantly fewer parametersthan a typical ResNet, it is expected to see an inferior classification accuracy to that of standardResNet. In order to show that our approach generalizes to larger networks, we trained on largerResNet18 ensembles. Results in Table. 3 show that HE-CKA can maintain similar accuracy asregular deep ensembles while significantly improving on ECE and AUROC of outliers. For theCIFAR-100 results please see Appendix C.3. Our ensemble with a standard ResNet18 with batchnormalization even slightly outperforms a WideResNet-28-10 (WRN) using the approach by Mukhotiet al. (2023). Additionally, the mean inference time for a WRN is 13ms compared to 9ms for theResNet18 ensemble on a Quadro RTX 8000. TinyImageNet vs SVHN/CIFAR-10/CIFAR-100/DTD. To further evaluate the effectiveness ofour approach to larger models and more complex datasets, we conducted experiments using theTinyImageNet dataset (Le & Yang, 2015). We trained ensembles of ResNet18 models and testedtheir ability to detect OOD samples from SVHN (Netzer et al., 2011), CIFAR-10/100 (Krizhevsky,2009), and the Describable Textures Dataset (DTD) (Cimpoi et al., 2014). Our objective was to assesswhether the proposed methods could generalize to large-scale settings and improve OOD detectionperformance without compromising in-distribution accuracy. Training details, and data splits, areprovided in Appendix C.4. Our proposed methods, especially Ensemble+OOD HE-CKA, enhanced OOD detection performance.Notably, Ensemble+OOD HE-CKA achieved an AUROC of 99.31% on SVHN and substantialimprovements on CIFAR-10/100 and DTD datasets (Table. 4), with AUROC scores of 81.56%/87.64%and 90.94%, respectively. This improvement in OOD detection did not come at a major expense ofID accuracy.",
  "Conclusion": "In this paper, we explored the novel usage of CKA and MHE on feature kernels to diversify deepnetworks. We demonstrated that HE-CKA is an effective way to minimize pairwise cosine similarity,thereby enhancing feature diversity in ensembles and hypernetworks when applied on top of CKA.Our approach significantly improves the uncertainty estimation capabilities of both deep ensemblesand hypernetworks, as evidenced by experiments on synthetic classification/regression tasks and realimage outlier detection tasks. We showed that diverse ensembles utilizing predictive entropy alonecan outperform other feature space density approaches, while synthetically generated OOD examples,far from the inlier distribution, can further significantly improve the OOD detection performance.While our current method requires fine-tuning several hyperparameters, such as layer weighting,we believe that future work could explore strategies for automatically estimating these parameters.We hope that our method inspires further advancements in Bayesian deep learning, extending itsapplication to a wider range of tasks that require robust uncertainty estimation.",
  "This work was funded in part by ONR award N0014-21-1-2052, DARPA HR001120C2022, NSF1751412 and 1927564": "Aggarwal, C. C., Hinneburg, A., and Keim, D. A. On the surprising behavior of distance metrics inhigh dimensional spaces. In Proceedings of the 8th International Conference on Database Theory,ICDT 01, pp. 420434, Berlin, Heidelberg, 2001. Springer-Verlag. ISBN 3540414568. Aharon, S. and Ben-Artzi, G. Hypernetwork-based adaptive image restoration. In ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.15, 2023. doi: 10.1109/ICASSP49357.2023.10095537. Alaluf, Y., Tov, O., Mokady, R., Gal, R., and Bermano, A. Hyperstyle: Stylegan inversion withhypernetworks for real image editing. In 2022 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 1849018500, 2022. doi: 10.1109/CVPR52688.2022.01796.",
  "Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertaintyin deep learning. In international conference on machine learning, pp. 10501059. PMLR, 2016": "Gawlikowski, J., Tassi, C. R. N., Ali, M., Lee, J., Humt, M., Feng, J., Kruspe, A., Triebel, R., Jung,P., Roscher, R., Shahzad, M., Yang, W., Bamler, R., and Zhu, X. X. A survey of uncertainty indeep neural networks, 2022. URL Gretton, A., Bousquet, O., Smola, A., and Schlkopf, B. Measuring statistical dependence withhilbert-schmidt norms. In Jain, S., Simon, H. U., and Tomita, E. (eds.), Algorithmic LearningTheory, pp. 6377, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg. ISBN 978-3-540-31696-1.",
  "Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped dqn.Advances in neural information processing systems, 29, 2016": "Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J., Lakshminarayanan, B.,and Snoek, J. Can you trust your model's uncertainty? evaluating predictive uncertainty underdataset shift. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch-Buc, F., Fox, E., and Garnett,R. (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates,Inc., 2019a. URL Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Nowozin, S., Dillon, J. V., Lakshminarayanan,B., and Snoek, J. Can you trust your models uncertainty? evaluating predictive uncertainty underdataset shift. Curran Associates Inc., Red Hook, NY, USA, 2019b.",
  "Park, N. and Kim, S.Blurs behave like ensembles: Spatial smoothings to improve accuracy,uncertainty, and robustness. In International Conference on Machine Learning, pp. 1739017419.PMLR, 2022": "Raghu, M., Gilmer, J., Yosinski, J., and Sohl-Dickstein, J. Svcca: Singular vector canonical correlationanalysis for deep learning dynamics and interpretability. Advances in neural information processingsystems, 30, 2017. Ratzlaff, N. and Fuxin, L. HyperGAN: A generative model for diverse, performant neural networks.In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conferenceon Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 53615369.PMLR, 0915 Jun 2019. URL Ratzlaff, N., Bai, Q., Fuxin, L., and Xu, W. Implicit generative modeling for efficient exploration.In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on MachineLearning, volume 119 of Proceedings of Machine Learning Research, pp. 79857995. PMLR,1318 Jul 2020. URL",
  "Reddi, S. J., Ramdas, A., Pczos, B., Singh, A., and Wasserman, L. On the decreasing powerof kernel and distance based nonparametric hypothesis tests in high dimensions, 2014. URL": "Sarafian, E., Keynan, S., and Kraus, S. Recomposing the reinforcement learning building blocks withhypernetworks. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conferenceon Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 93019312.PMLR, 1824 Jul 2021. URL Van Amersfoort, J., Smith, L., Teh, Y. W., and Gal, Y. Uncertainty estimation using a single deepdeterministic neural network. In Proceedings of the 37th International Conference on MachineLearning, ICML20. JMLR.org, 2020.",
  "AMinimizing Model Similarity as Particle-based Variational Inference": "In Bayesian deep learning, each network in an ensemble can be seen as a particle sampled from adistribution. Hence, the training process can be seen as a variational inference problem in termsof minimizing the KL-divergence between the empirical distribution defined by the particles andthe training data. In this section, we relate Eq. (4) with an RKHS and apply it under a particle-based variational inference framework for supervised learning. Particle-based variational inferencemethods (Liu & Wang, 2016; Chen et al., 2018; Liu et al., 2019) can be viewed from a geometricperspective as approximating the gradient flow line on the Wasserstein space P2(X) (Liu & Zhu,2022). Let qt denote the gradient flow line of the KL-divergence w.r.t. some target (data) distributionp P2(X), for an absolutely continuous curve qt, its tangent vector at each t is given by (Villaniet al., 2009; Ambrosio et al., 2008),grad KL(qt|p) = log p + log qt.(7)The core idea of particle-based VI is to represent qt by a set of particles {xi} and adopt a first-orderapproximation of qt+ through a perturbation of {xi}. In Eq. (7), while the first term corresponds tothe maximum (data) likelihood term of supervised learning, the second term is intractable. Differentvariants of particle-based VI methods tackle this term via different approximation/smoothing methods.Inspired by SVGD (Liu & Wang, 2016), we approximate log qt in an RKHS corresponding to theHE-CKA kernel. In particular, let T = { : X X} denote the space of transformations on space X of particles, adirection of perturbation can be viewed as a vector field on X, which is a tangent vector in the tangentspace T=idT , where id is the identity transformation. Under the particle representation {xi} q, letq(x) denote the distribution represented by transformed particles {(xi)}. To approximate log q,we want to find the perturbation direction of {xi} that corresponds to the steepest ascend directionof the loss J () = Exq[log q(x)] at = id, which is the gradient of J () in the tangent spaceT=idT . This gradient is given by the following Lemma, with the proof given in Appendix B.Lemma A.1. For J () = Exq[log q(x)],",
  "C.1Smoothing Terms and Layer Weighting": "To effectively train with the HE-CKA kernel for the repulsive term we found that it is essential tosmooth out the particle energy using an dist on the geodesics and arc on the cosine similarity values.With larger smoothing terms we can reduce the large gradients on very similar particles, with CKAvalues near 1, and ensure other particles still receive some repulsive force. Additionally, Eq. (4)equally weighs every layer in the network. It has been empirically shown that the first few layers ofdeep neural networks have high similarity (Kornblith et al., 2019), which indicates that initial layerslearn more aligned features. Enforcing strong hyperspherical uniformity, or low CKA, of featureGram matrices may remove useful features. We have noticed that it is difficult to train models witha uniform HE-CKA layer weighting scheme of 1/L. To fix this we applied a custom weightingscheme w that typically increases linearly with the number of layers, with latter layers weightedhigher. We found that using a weighting scheme in Eq. (4) allowed for finer control of the repulsiveterm. Typically the first layer in a CNN is a simple feature extractor, and depending on the depth ofthe network could assign too high of a repulsive term on the first layer. Additionally, the last layercould have too high of a weight and ruin inlier performance. We utilize a custom weighting schemeusing the vector w = {w1, , wL}, where w1 is typically 1. We define the smoothed HE-CKAversion for training as HEsmooth.",
  "arccos( KmlKml/(1.0 + arc))s + dist(11)": "The smoothing terms gives us finer control over the interaction between particles and preventsexploding gradients from the energy term. Although both dist and arc have a similar effect it ismore important to include the arc as the gradient of arccos approaches near 1 and 1 withoutany smoothing term. As demonstrated with the cosine similarity feature kernel used in . Itis advised to set dist as a small constant then vary arc and parameters when searching for theright kernel. Optionally, one may replace the Riesz-s based kernel with an exponential one, iees arccos( KmlKml/(1.0+arc))dist, which provides a more numerically stable gradient, and moreintuitive to understand growth term s. As discussed in Appendix. D the parameters for , , andw need to be selected. For MNIST experiments we performed a bayes sweep across parameters toselect the layer weighting schemes, smoothing terms, and repulsive terms. For larger models, such asthe CIFAR and TinyImageNet experiments, we selected, by trying a few combinations, a weightingschemes by testing values [0.25, 1.5], [0.01, 10.0], and using layer weighting scheme wlthat increases proportionally with l, and with different first layer and last layer values. : Effect of smoothing term when using a cosine similarity based HE-CKAsmooth kernel with SVGD oninlier points only. All methods were trained with AdamW (lr=0.05, wd=0.0075), HE-CKAsmooth s = 2, anddist = 0.00025, and w = [0.2, 0.35, 0.85, 0.05] for 1k steps.",
  "C.2Dirty-MNIST": "The Dirty-MNIST experiments utilized an ensemble of 5 LeNet5 models with a modified variancepreserving gelu activation function. Models were trained using AdamW with lr = 0.0065 and weightdecay of 0.001 for 50 epochs, except for Hypernetwork training which was trained for 85 epochs withAdamW with lr = 0.0025 and weight decay 0.0025. Details such as lr warmup, gradient clipping,repulsive terms, layer weighting, HE-CKA smoothing terms, and more can be found in the officialrepository.",
  "ENSEMBLEPE0.7481.815.7789.62ENSEMBLE+HE-CKAPE0.7480.723.9091.17ENSEMBLE+OOD HE-CKAPE0.7680.614.1199.44": "Additionally, we have some results showing much improvement on CIFAR-100 OOD detection withSVHN when trained with synthetic OOD examples in . With about a 10% improvementin AUROC between the inlier and outlier sets. We applied HE-CKA to an ensemble of ResNet18models and evaluated the approach on CIFAR-10 () and CIFAR-100 (). The modelswere trained for 200 epochs using SGD with a learning rate of 0.1 and weight decay 5e-4. TheHE-CKA kernel used a linear kernel for feature calculation with the exponential kernel s = 2, and = 1.0. For experiments with out-of-distribution (OOD) data, the following values were adjusted: = 0.5, OOD = 0.75, and = 0.75. Details regarding layer weighting and smoothing are availablein the repository. Forty-eight OOD samples were taken per batch for all CIFAR experiments, whereapplicable. The feature repulsion term was not applied to every convolution of the ResNet18 architecture. Toconserve computational resources, only a subset of layers was included. Specifically, the selectedlayers comprised the initial convolutional layer, the output of every other ResNet block within thefirst two of the four layers, the output of all blocks in the last two layers, and the final linear layer. Training details regarding the ResNet32 experiments follow the training procedure, learning ratescheduling, and hyperparameters given by D' Angelo & Fortuin (2021). The hypernetwork variant,due to the difficulty of training, was trained for 180 instead of 143 epochs, and utilized group basednormalization to stabilize feature variance.",
  "C.4TinyImageNet and Particle Number Ablation": "All models utilized a pretrained deep ensemble without any repulsive term and then fine-tunedusing different methods, including our proposed approach. Methods utilizing CKApw and HE-CKAemployed a linear feature kernel. For OOD detection, we used predictive entropy (PE) computed fromthe ensemble predictions. Additionally, we generated synthetic OOD data from noise and augmentedTinyImageNet samples to enhance the OOD detection capability. We utilized a training split of 80:10:10 for training, validation, and testing respectively. Training utilized SGD with a learningrate of 0.005 and weight decay of 5e-4. We additionally performed a particle number ablation onthe ResNet18 + HE-CKA ensemble, utilizing the same repulsive term, showing improvements inaccuracy, and outlier detection, when going from 2 particles to 5 (Table. 6).",
  "DLimitations": "With our approach, we are able to resolve some of the issues related to tackling permutation offeature channels, which normally pose challenges for Euclidean-based kernels like RBF. However,constructing a model kernel based on layer features requires tuning the repulsive term (), thelikelihood term (), and the layer weighting terms (w). This introduces numerous hyperparametersthat need to be adjusted depending on the dataset and the architecture in use. Future work couldexplore automating the estimation of these parameters or simplifying the HE-CKA kernel. Althoughthe assumption that the first few layers should have small repulsive terms seems clear, the weightingand smoothing of later layers remain unclear. This work only explored repulsive terms that increasedwith layer depth; the dynamics of which layers should have more repulsion are not well understoodand have not yet been explored. Additionally, feature-based kernels based on CKApw are sensitive tothe number of particles and the batch size sampled, as the dimensionality of the hypersphere changes,impacting the repulsive terms. One possible solution could be to construct a normalized HE-CKAvariant, which precomputes the minimum and maximum energy available on the (N 2 1)-spherewith M models.",
  "E.2OOD Images for MNIST and CIFAR": "Images are harder to define boundary/ood points, but we found in practice that generating images bytransforming inliers to outliers via typical augmentations, and generating synthetic random channeldata worked well in practice. Our approach to transforming inlier points to outlier points consistsof the following augmentations in random order: blurring, affine transform, perspective transform,elastic deformations, erasures, Gaussian noise, Gaussian blurring and inversions. Examples of suchimages are shown in for MNIST, for CIFAR, and for TinyImageNet. The ID toOOD set of images accounts for roughly 30-40% of the dataset, whereas the other 60% are randomlygenerated. Synthetic OOD images are generated via combinations of perlin noise, simplex noise,gaussian noise, lines, alternating grids, inversions, and random area thresholding. For images withmore than one channel, such as CIFAR and TinyImageNet, we either apply different noise to eachchannel, use the same method but different seed, or occasionally broadcast one channel along allchannels.",
  "FMNIST CKA Results": "We present the CKA plots of each method: Ensemble (), Hypernetwork (), SVGD+ RBF (), SVGD + CKApw (), SVGD + HE-CKA (), Ensemble + HE-CKA(), Ensemble + OOD HE-CKA (), and Hypernetwork + OOD HE-CKA ().Each plot shows a grid comparing layerwise estimation of pairwise CKA, equipped with a linearfeature kernel on the inlier Dirty-MNIST dataset, while using an unbiased estimator for CKA. To oursurprise we found that CKApw and HE-CKA results in fairly similar unbiased CKA estimates acrossthe ensemble, but overall performance of the models in uncertainty estimation and accuracy presentedin Table. 1 by HE-CKA were better. Nevertheless, methods utilizing CKApw and HE-CKA kernelssignificantly reduce similarity of features compared to methods with no repulsive terms or RBF basedkernels. This was especially true for our hypernetwork tests.",
  "GMemory Footprint and Time Complexity": "We compare the training runtime of our HE-CKA term to ParVI based methods presented inD' Angelo & Fortuin (2021). We evaluated mini-batch training time averaged over 50 batches on aQuadro RTX 8000. Each method used a ResNet18 fed with batches of 128 images from CIFAR-10.The results are presented in . Reported CUDA memory includes all ensemble members,loss, batch statistics, feature kernels (if applicable), and gradients. We see that all ParVI methodsincrease training time by 2.2x for 5 ensemble members and 1.2x for 10. Given that HE-CKA isapplied layerwise, our method does require a slight increase in memory compared to the other ParVImethods, but is comparable to other methods in terms of training batch time increase. The timecomplexity for each minibatch of HE is O(LN 2n2) compared to typical function space O(N 2n2)or weight space O(n2) kernels, where n is the number of particles, N is the mini-batch size and Lis the number of layers we use to compute HE-CKA. However, n and N are typically small, 5 andup to 128 respectively in our experiments, and one can use a subset of the layers L, as we did withour experiments. While using the HE-CKA kernel does have an increased memory and computationcost than using the RBF kernel, the benefit of having a kernel invariant to feature permutations andscaling are worth the minor additional cost."
}