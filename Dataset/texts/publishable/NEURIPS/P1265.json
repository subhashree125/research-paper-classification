{
  "Abstract": "Previous visual object tracking methods employ image-feature regression modelsor coordinate autoregression models for bounding box prediction. Image-featureregression methods heavily depend on matching results and do not utilize positionalprior, while the autoregressive approach can only be trained using bounding boxesavailable in the training set, potentially resulting in suboptimal performance duringtesting with unseen data. Inspired by the diffusion model, denoising learningenhances the models robustness to unseen data. Therefore, We introduce noiseto bounding boxes, generating noisy boxes for training, thus enhancing modelrobustness on testing data. We propose a new paradigm to formulate the visualobject tracking problem as a denoising learning process. However, tracking algo-rithms are usually asked to run in real-time, directly applying the diffusion modelto object tracking would severely impair tracking speed. Therefore, we decom-pose the denoising learning process into every denoising block within a model,not by running the model multiple times, and thus we summarize the proposedparadigm as an in-model latent denoising learning process. Specifically, we proposea denoising Vision Transformer (ViT), which is composed of multiple denoisingblocks. In the denoising block, template and search embeddings are projectedinto every denoising block as conditions. A denoising block is responsible forremoving the noise in a predicted bounding box, and multiple stacked denoisingblocks cooperate to accomplish the whole denoising process. Subsequently, weutilize image features and trajectory information to refine the denoised boundingbox. Besides, we also utilize trajectory memory and visual memory to improvetracking stability. Experimental results validate the effectiveness of our approach,achieving competitive performance on several challenging datasets. The proposedin-model latent denoising tracker achieve real-time speed, rendering denoisinglearning applicable in the visual object tracking community.",
  "Introduction": "Visual object tracking is a fundamental task in computer vision, which involves localizing andtracking a specific object in a video given its initial position. It finds broad applications in videounderstanding, surveillance, and robot navigation . State-of-the-art approaches can be broadlycategorized into two classes. The first class [17; 51; 41; 8; 19; 55; 29; 56] directly predicts thebounding box of the tracked target based on image features. The second class [6; 43], employs thecoordinate autoregression framework. While the mainstream methods have achieved prominent success, there are still certain issues to beaddressed. Methods based on the image-feature regression framework rely heavily on the matchingresults between the template and the search region, which cannot utilize positional prior. Meanwhile,based on the autoregressive approach, it is necessary to utilize the bounding box from the previousframe to train the model, which can only utilize the existing bounding boxes in the training set.Therefore, during the testing phase, it may exhibit suboptimal performance for some unseen data. The Diffusion model has achieved significant success in image generation task, allowing the genera-tion of many images not seen in the training set. Inspired by the Diffusion model, we addnoise to bounding boxes during training stage. The noisy box can have arbitrary size and position,thereby enhancing the robustness of the model to unseen data during testing stage. As illustrated in (a) and (b), the diffusion model in image generation task requires multiple iterations of U-Net,while in object detection task, denoising is accomplished through multiple iterations of the decoder.However, tracking algorithms are usually asked to run in real-time, directly applying the diffusionmodel to object tracking would severely impair tracking speed. Therefore, motivated by DAE, we propose a novel denoising learning paradigm (DeTrack) forvisual object tracking that decomposes the denoising learning process in every denoising block witha tracking model. We use templates, search region, and noisy boxes as inputs. During the denoisingprocess, we inject the template and search region as conditions to predict the noises in previouspredictions. We repeatedly conduct the conditional denoising process and finally achieve accurateobject location prediction. Specifically, as shown in (c), we propose a novel denoising ViT.We decompose a complete denoising process into several denoising blocks within ViT model andimplement every denoising operation with a denoising block. Then the denosing learning process canbe implemented in a single forward pass of the tracking model, which can reduce the computationalcost drastically. To benefit from the in-context information[21; 16; 9; 28; 20; 22], we also put thepreviously predicted bounding boxes into a trajectory memory, and put the templates from previousframe into a visual memory. We use them as additional conditions to help locate objects moreaccurately.",
  "Our contributions can be summarized as follows:": "We propose a novel in-model latent denoising learning paradigm for visual object tracking,which provides a new perspective for the research community. It decomposes the classicalexplicit denosing process into several denoising blocks and solves the problem with atracking network in a single forward pass, which is valuable for real applications. We present a tracking model including a denoising ViT, comprised of multiple denoisingblocks. The denoising process can be completed by progressively denoising through thedenoising blocks within ViT. Furthermore, we construct a compound memory in the modelthat improve the tracking results using visual features and trajectory.",
  "Related Work": "Visual Object Tracking. The existing visual object tracking methods can be broadly categorized intotwo main classes. The first class[1; 11; 48; 46; 8; 19; 41; 32; 31; 51; 10; 35; 44; 54] involves directlyregressing the bounding box from image features, the second class [43; 6] treats the bounding box asfour distinct tokens, employing an autoregressive model to sequentially predict these four tokens. In the first class, deep neural networks are initially used to extract visual features, followed by thedesign of various prediction heads for regressing the bounding box. Since 2016, some prevalentmethods have adopted a two-stream framework, employing siamese networks to separately extractvisual features from the template and the search region. One type of prediction head [46; 32; 31;53] uses a branch to predict the possible location of the target and other branches to predict thecorresponding bounding box for that location. Another type of prediction head [10; 48; 17] consistsof two branches that predict the coordinates of the top-left and bottom-right corners. Subsequently,OSTrack introduces a one-stream tracking paradigm that combines feature extraction and featurefusion into a single step, achieving a new state-of-the-art performance. For the second class, SeqTrack proposes transforming the bounding box into four tokens, predicting them sequentially in theorder of x, y, w, and h. When predicting the bounding box, each box requires four passes throughthe decoder. Another autoregressive method, ARTrack , is similar to SeqTrack but differs in thatit incorporates trajectory information in the input to enhance the models awareness of trajectories. Denoising Learning. DDPM introduces denoising diffusion learning, which enhances thequality and diversity of generated images by adding noise to and denoising images. Subsequently,denoising learning has experienced explosive growth, being applied in various domains and achievingsignificant success. In the Super-Resolution field, SR3 leverages DDPM for conditional imagegeneration, employing a stochastic denoising process for super-resolution. Meanwhile, CDM comprises a sequence of multiple diffusion models, each responsible for generating images withprogressively higher resolutions. In video generation, the Flexible Diffusion Model (FDM) utilizes a generative model designed for sampling arbitrary subsets of video frames, facilitated by aspecialized architecture tailored for this purpose. The Residual Video Diffusion (RVD) model employs an autoregressive, end-to-end optimized video diffusion model. In addition to generativetasks, denoising learning has also found extensive applications in discriminative task. DiffusionDet applies the diffusion model to object detection, utilizing DDIM for denoising. However, thisapproach still requires multiple passes through the decoder for denoising, impacting inference speed.",
  "How to Formulate the Denoising Learning Tracking Paradigm?": "Image and Box Inputs. We utilize both visual memory and the search region as conditional inputs c,while introducing noisy boxes xI to predict the true position of the target, where I represents theI-th state in the denoising process. The visual memory stores templates, which are cropped based on previous frames. The search region is cropped based on the current frame and encompasses the areawhere the target may be present. In training stage, inspired by DDPM, we obtain noisy boxes xIby adding Gaussian noise to the ground truth box x0:",
  "where = Tj=0 j and j = 1 j. j (0, 1) is the variance schedule, T is the time step": "Optimization for Denoising Learning. We take the visual memory and search region as conditionalinputs c, and predict the true target position x0 from the noisy box xI, p(x0|xI), where representsthe neural network parameters. We aim to maximize the probability p that the neural networkpredicts x0, enabling the model to predict the true target position:",
  "l |xi, c).(4)": "In the traditional Diffusion model, each step p(xi1|xi, c) is iteratively predicted using a neuralnetwork model f. However, our denoising paradigm decomposes the iterations of neural networkinto the iterations of denosing blocks within a neural network, f = {d1, d2, , dl}, where eachdenoising block dl is responsible for predicting a state p(xi I",
  "l |xi, c), where l denotes the number ofblocks. This allows our model to complete denoising with only a single forward pass of the trackingmodel": "Discussion on the Differences from the Diffusion Model. The proposed denoising learningtracking paradigm is not a diffusion model. (1) In the reverse denoising process of diffusion model,sampling a noise from a standard Gaussian distribution introduces randomness, making it moresuitable for generating diverse images in image generation tasks. However, bounding boxes for visualobject tracking are deterministic. Therefore, our proposed DeTrack does not involve a samplingprocess in reverse denoising process, making it more suitable for visual object tracking. (2) Each stepof diffusion model is predicted recursively using a neural network. The proposed DeTrack predictsstates using denosing blocks within a network (3) The diffusion model requires iterative prediction ofneural network, whereas our method only requires a single forward pass through the network. Pleaserefer to the Appendix A.1 for detailed analysis.",
  "Model Architecture": "Inputs representation. As show in , we use noisy bounding boxes as input and take visualmemory and a search region as conditional inputs. Visual memory stores multiple templates. Specif-ically, gaussian noise is added to the ground truth bounding box to obtain a noisy bounding box{x1I , y1I , x2I , y2I } R41, where * denotes noise addition, while 1 and 2 respectively denote theupper left corner and lower right corner. Subsequently, the noisy box is mapped to a high-dimensionalspace by word embedding, resulting in noisy box embedding xI R4C. Additionally, we map tem-plates and the search region to templates embedding z RNzC and search embedding s RNsC",
  "d[vs, vz]),(5)": "where [] denotes concatenation. d is the dimensionality of the key.Denoising Block. As shown in , the input to the denoising block comprises the noisy boxembedding and the search region embedding. These are passed through linear layers to obtain theqxi(box query), ks(search key), and vs(search value). Subsequently, a denoising attention mechanismis employed for the first time of denoising:",
  "j=1j.(10)": "Box Refining and Mapping. As shown in (a), we start by applying self-attention to thetrajectory and denoised box embedding. We maintain that the current box embedding can only attendto its preceding box embedding by an attention mask in the self-attention, introducing temporalinformation. Subsequently, the output of self-attention is used as a query for cross-attention with the",
  "(a) Box refining and Mapping": ": Box refining and mapping and the updating of visual memory. (a) Box refining andmapping introduces the trajectory memory to improve tracking performance. (b) Visual memoryupdating based on collaboratively decision including s1 (IoU score) and s2 (Softmax score). image features. After undergoing six times of box refining, we compute the similarity between therefined box and word embedding, apply Softmax to obtain probabilities for different positions in theword embedding, and use the position with the highest probability as the bounding box, which issimilar to ARTrack. Compound Memory. We design a compound memory that includes both a visual memory anda trajectory memory. The visual memory enhances the models ability to adapt to changes in theappearance of the target and the environment in the video. Besides, the trajectory memory enablesthe model to continue tracking the target even in the presence of occlusions or disappearances. Visual Memory. As shown in (b), our visual memory consists of dynamic templates and a fixedtemplate. The first template of dynamic templates is discarded, and a new template is added. Directlyupdating the template can lead to cumulative errors. Therefore, we propose a collaborative updatingmechanism. This involves inputting the search embedding extracted after Denoising ViT into IoUNetto obtain the corresponding IoU score s1. Additionally, the Softmax score from Box Refining servesas a confidence value s2. A collaborative decision on the quality of the new template frame is madebased on two threshold values 1 and 2, determining whether updating. Trajectory Memory. The proposed trajectory memory stores the boxes of the previous 7 frames, usinga first-in-first-out (FIFO) approach when a new box needs to be stored. This results in a continuouslyupdated trajectory box used for refining the denoised box. The trajectory memory can provide themodel with prior positional information and target size, allowing accurate prediction of the boundingbox even in cases of visual occlusion.",
  "DeTrack256128 128256 25653.0G42FPSRTX3090DeTrack384192 192384 384117.1G30FPSRTX3090": "Our denoising ViT adopts ViT-B and utilizes MAE for weight initialization, with a totalof l = 12 denoising blocks. The box refining includes 6 transformer layers for self-attention andcross-attention. Additionally, we trained two models, namely DeTrack256 and DeTrack384. Thetemplate is cropped based on twice the size of the bounding box, while the search region is croppedbased on four times (DeTrack256) and five times (DeTrack384) the size of the bounding box. To map",
  "boxes into a high-dimensional space, we utilize word embedding, similar to Pix2Seq , with thenumber of bins being 800 and 1200 for DeTrack256 and DeTrack384 respectively": "Training. Our experiments are conducted on Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz with252GB RAM and 8 NVIDIA GeForce RTX 3090 GPUs with 24GB memory. In the first stage, thereis only visual memory, which randomly samples two frames from the video. The model is trained onfull dataests (COCO, GOT-10k, TrackingNet, and LaSOT). A total of 240 epochs are trained, withthe learning rate set to 8e-5 for the denoising ViT and 8e-6 for the box refining. The learning ratedecreases by a factor of 10 at the 192-th epoch. In the second stage, trajectory memory is introducedto refine the box, and sequential training is adopted. Consecutive frames are sampled from the video,with each frames prediction result stored in the trajectory memory and updated in a first-in-first-outmanner. The training is conducted on three datasets excluding COCO. A total of 60 epochs aretrained, with the learning rates decreasing to 4e-6 and 4e-7 for the denoising ViT and box refining,respectively. In the third stage, only IoUNet is trained while other parts are frozen. The learning rateis set to 1e-4, and a total of 40 epochs are trained, with a 10 learning rate decay at the 30-th epoch.For GOT-10k, the learning rate remains consistent with training on the full dataests. In the first stage,we train for 120 epochs, with a 10 decrease in learning rate at the 96-th epoch, followed by trainingfor 25 epochs in the second stage. During the training on GOT-10k, IoUNet is not used. The lossfunctions is cross-entropy and SIoU, which is the same as ARTrack. Inference. During the testing phase, we use the search region and template as image inputs andinitialize the box with the previous box (predicted bounding box of t-1 frame). Additionally, theupdate interval of the visual memory is set to 5 for t <= 100, doubled every 100 frames until t =500, and then remains 160. While testing on the GOT-10k dataset, the visual memory is updateddirectly. For other datasets, the IoU score and confidence score is applied to filter templates. Thetrajectory memory stores seven bounding boxes, updating with a frequency of every frame. Inferenceis conducted on an NVIDIA GeForce RTX 3090.",
  "State-of-the-Art Comparisons": "AVisT. The AVisT dataset, as described in , covers a broad spectrum of diverse and demandingsituations, encompassing harsh weather conditions like thick fog, intense rainfall, and sandstorms.Our tracker demonstrates outstanding performance on AVisT , a dataset with extreme weatherconditions and harsh environments. It outperforms SeqTrack384 by 2.4% in AUC, substantiating ourtrackers excellence in extreme environmental conditions. GOT-10k. GOT-10k comprises a training dataset consisting of 10,000 videos and a testing datasetwith 180 videos. There is no overlap between the training and test sets, necessitating trackers todemonstrate robust generalization capabilities towards unseen data. As shown in Tab. 2, our methoddemonstrates superior performance on the GOT-10k . Our DeTrack256 achieves a significantimprovement in AUC compared to SeqTrack256 , with increases of 3.0% and 2.4%, respectively.Our DeTrack384 outperforms the state-of-the-art method ARTrack384 by 2.4%. This is attributed tothe non-overlapping nature of the training and testing sets in the GOT-10k dataset, indicating ourmethods strong performance on unseen data. The denoising learning paradigm has learned powerfuldenoising capabilities while facing with arbitrary positions and sizes of boxes. LaSOT. LaSOT is benchmark designed for long-term tracking, featuring a test collection consistingof 280 videos. Our DeTrack256 achieves an AUC of 71.3%, exhibiting performance improvementcompared to other methods based on 256 resolution. Additionally, our DeTrack384 also demonstratesstate-of-the-art performance, validating the strong competitiveness of our approach in long-termdataset. This is attributed to our compound memory design, which leverages historical trajectory andappearance information to enhance the models generalization ability on long-term dataset. LaSOText. LaSOText is an extension of the LaSOT dataset, also categorized as a long-termtracking dataset. It comprises 150 video sequences and encompasses 15 object classes. Our De-Track384 shows significant improvements compared to other methods, with a 1.7% increase in AUCover SeqTrack384 and a 2.4% improvement in Pnorm. This demonstrates the strong generalizationcapability of our approach even with extended data, particularly manifesting notable advantages inthe accuracy of bounding box center point.",
  "AO1.11.64.87.512.521.433.152.365.770.274.877.1SR0.50.10.21.22.88.017.934.157.674.778.783.786.1SR0.750.00.00.20.82.98.017.839.156.964.670.573.5": "Influence of denoising steps. We investigate the impact of the number of denoising iterations on theperformance of the tracker. Our proposed In-model latent denoising consists of a total of 12 stepsbased on denosing blocks, requiring only a forward pass to complete denosing. As shown in Tab.3,the models performance is nearly zero at the first and second denoising steps because the boundingboxes are still filled with noise. However, there is a significant qualitative improvement in modelperformance at the eighth denoising step, reaching its peak at the twelfth step. As shown in , theresults improve progressively step by step, consistent with Tab. 3. Analysis of denoising paradigm. Although our method completes denoising with only a singleforward pass through the tracking model, it can also be adapted to perform multiple forward passes,similar to traditional Diffusion model. Therefore, we further analyze and compare the multiple",
  ":Visualization of the denoising step GOT-10k. The first row is the video GOT-10k-Test-000040, the second row is the video GOT-10k-Test-000003, and the third row is the videoGOT-10k-Test-000051": "forward passes and single forward pass paradigms, as shown in Tab. 4. In DeTrack, the performanceof multiple forward passes is not superior to that of single forward pass. Additionally, if denoising isperformed similarly to traditional Diffusion models, the computational cost increases significantly.Single forward pass only requires 53.0G FLOPS and achieves a speed of 42 FPS, while multipleforward passes incurs exponentially higher computational costs with a linear decrease in speed. Analysis of the denoising block. As shown in Tab.5, if there is no NoisePred module, AO willdecrease by 2.0%, and SR0.5 will decrease by 2.0%. This demonstrates that noise prediction andgradually subtracting noise are crucial for the model. Furthermore, removing denoising attentionleads to further performance degradation, demonstrating that utilizing image features as conditionalinputs can also assist in denoising. Moreover, the computational overhead of the denoising blockincreased by only 1.80G, owing to the fact that the box comprises merely 4 tokens. Thus, even withthe addition of denoising attention and NoisePred, this remains a negligible computational burden.",
  "Ablation study on Compound Memory": "Because the memory mechanism is designed to address the challenge of dynamic changes in video,and considering the greater variety of environmental and appearance changes in long video datasets,we chose the LaSOT (long-term tracking dataset, averaging 2448 frames per video) to validate theeffectiveness of our memory mechanism. (a) Visual Memory Length(b) Trajectory Memory Length(c) IoU Threshold(d) Softmax Threshold (e) Compound memory : Ablation study of memory on LaSOT. (a) Different visual memory lengths; (b) Differenttrajectory memory lengths; (c) Different IoU thresholds are applied for template updates; (d) Theinfluence of Softmax thresholds. (e) With or without compound memory.Exploration on the length of the visual memory and the trajectory memory. We firstly explorethe impact of different visual memory lengths. As shown in (a), when the length is only 1, themodels AUC is only 70.2. However, with an increase in memory length, performance graduallyimproves, reaching its peak at the 3-rd frame. Subsequently, performance declines. This is becausewhen the memory is too short, the model cannot adapt to changes in the target and the environment. Conversely, when the memory is too long, it stores incorrect information. Unlike visual memory, asshown in (b), trajectory memory does not exhibit a trend of initially rising and then fallingwith an increase in stored boxes. The performance consistently improves as the number of boxesranges from 1 to 7. As shown in (e), we also achieved a performance of 70.2 by removing allmemories, which further confirms the effectiveness of our memory. Effects of IoU score and Softmax scorefor visual memory updating. For the update of visualmemory, we strive to avoid updating poor templates into our visual memory. This would lead totracking drift. Therefore, as shown in (d) keeping IoU score fixed, we conduct an ablation studyon different Softmax score values. The study found that an accuracy update can be achieved when theSoftmax score is set to 0.9, obtaining 71.3% on AUC. As shown in (c) keeping Softmax scorefixed, the best IoU score is 0.75. When the IoU score threshold is set to 0.85, it leads to a decrease inAUC. It is because the overly strict condition reduces the frequency of visual memory updates. 5LimitationDespite achieving real-time speed and competitive performance, our DeTrack still has certain limita-tions. Existing tracking methods struggle to recover the target when facing challenges such as objectocclusion and out-of-view situations. Although our proposed trajectory memory can assist in targetreacquisition after target loss in some cases, further improvements are needed to address challengeslike object occlusion and out-of-view scenarios. We will investigate the challenges in these scenarios. 6ConclusionTraditional visual object tracking methods using image-feature regression or coordinate autoregressionmodels faced limitations in handling positional priors and unseen data. Inspired by the diffusionmodel, we introduced denoising learning to enhance model robustness. Our approach, employingnoisy bounding boxes for training, introduces a novel paradigm of denoising learning in objecttracking. By decomposing the process into individual denoising blocks within our proposed denoisingVision Transformer (ViT), we achieved real-time performance while maintaining effectiveness.Experimental results demonstrate the efficacy of our method, showcasing competitive performanceand rendering denoising learning applicable in the visual object tracking community. Acknowledgement This work was supported by National Natural Science Foundation of China(No.62072112), National Natural Science Foundation of China under Grant Nos. 62106051 and theNational Key R&D Program of China 2022YFC3601405. Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutionalsiamese networks for object tracking. In European conference on computer vision, pages 850865. Springer,2016. Goutam Bhat, Martin Danelljan, Luc Van Gool, and Radu Timofte. Learning discriminative modelprediction for tracking. In Proceedings of the IEEE/CVF international conference on computer vision,pages 61826191, 2019.",
  "Xin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun Yang, and Huchuan Lu. Transformer tracking. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81268135,2021": "Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, and Rongrong Ji. Siamese box adaptivenetwork for visual tracking. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 66686677, 2020. Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improvedmemory coverage for efficient video object segmentation. Advances in Neural Information ProcessingSystems, 34:1178111794, 2021. Yutao Cui, Cheng Jiang, Limin Wang, and Gangshan Wu. Mixformer: End-to-end tracking with iterativemixed attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1360813618, 2022. Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, and Michael Felsberg. Atom: Accurate trackingby overlap maximization. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 46604669, 2019. Martin Danelljan, Luc Van Gool, and Radu Timofte. Probabilistic regression for visual tracking. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 71837192,2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Mingzhen Huang, Juehuan Liu,Yong Xu, et al. Lasot: A high-quality large-scale single object tracking benchmark. International Journalof Computer Vision, 129(2):439461, 2021. Heng Fan, Hexin Bai, Liting Lin, Fan Yang, Peng Chu, Ge Deng, Sijia Yu, Mingzhen Huang, Juehuan Liu,Yong Xu, et al. Lasot: A high-quality large-scale single object tracking benchmark. International Journalof Computer Vision, 129:439461, 2021. Zhihong Fu, Qingjie Liu, Zehua Fu, and Yunhong Wang. Stmtrack: Template-free visual tracking withspace-time memory networks. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1377413783, 2021. Shenyuan Gao, Chunluan Zhou, Chao Ma, Xinggang Wang, and Junsong Yuan. Aiatrack: Attention inattention for transformer visual tracking. In Computer VisionECCV 2022: 17th European Conference, TelAviv, Israel, October 2327, 2022, Proceedings, Part XXII, pages 146164. Springer, 2022. Shenyuan Gao, Chunluan Zhou, and Jun Zhang. Generalized relation modeling for transformer tracking.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1868618695, 2023. Dongyan Guo, Jun Wang, Ying Cui, Zhenhua Wang, and Shengyong Chen. Siamcar: Siamese fullyconvolutional classification and regression for visual tracking. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 62696277, 2020. Pinxue Guo, Lingyi Hong, Xinyu Zhou, Shuyong Gao, Wanyun Li, Jinglun Li, Zhaoyu Chen, XiaoqiangLi, Wei Zhang, and Wenqiang Zhang. Clickvos: Click video object segmentation. arXiv preprintarXiv:2403.06130, 2024. Pinxue Guo, Wanyun Li, Hao Huang, Lingyi Hong, Xinyu Zhou, Zhaoyu Chen, Jinglun Li, Kaixun Jiang,Wei Zhang, and Wenqiang Zhang. X-prompt: Multi-modal visual prompt for video object segmentation.In Proceedings of the 32nd ACM International Conference on Multimedia, pages 51515160, 2024.",
  "Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neuralinformation processing systems, 33:68406851, 2020": "Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research,23(1):22492281, 2022. Lingyi Hong, Zhongying Liu, Wenchao Chen, Chenzhi Tan, Yuang Feng, Xinyu Zhou, Pinxue Guo,Jinglun Li, Zhaoyu Chen, Shuyong Gao, et al. Lvos: A benchmark for large-scale long-term video objectsegmentation. arXiv preprint arXiv:2404.19326, 2024. Lingyi Hong, Shilin Yan, Renrui Zhang, Wanyun Li, Xinyu Zhou, Pinxue Guo, Kaixun Jiang, Yiting Chen,Jinglun Li, Zhaoyu Chen, et al. Onetracker: Unifying visual object tracking with foundation models andefficient tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pages 1907919091, 2024. Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for genericobject tracking in the wild. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(5):15621577, 2019. Bo Li, Wei Wu, Qiang Wang, Fangyi Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolutionof siamese visual tracking with very deep networks. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 42824291, 2019. Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese re-gion proposal network. In Proceedings of the IEEE conference on computer vision and pattern recognition,pages 89718980, 2018.",
  "Liting Lin, Heng Fan, Zhipeng Zhang, Yong Xu, and Haibin Ling. Swintrack: A simple and strong baselinefor transformer tracking. Advances in Neural Information Processing Systems, 35:1674316754, 2022": "Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu Paul, Danda Pani Paudel, Fisher Yu, and LucVan Gool. Transforming model prediction for tracking. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 87318740, 2022. Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and Luc Van Gool. Learning target candidateassociation to keep track of what not to track. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1344413454, 2021. Mubashir Noman, Wafa Al Ghallabi, Daniya Najiha, Christoph Mayer, Akshay Dudhane, Martin Danelljan,Hisham Cholakkal, Salman Khan, Luc Van Gool, and Fahad Shahbaz Khan. Avist: A benchmark for visualobject tracking in adverse visibility. arXiv preprint arXiv:2208.06888, 2022. Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and MachineIntelligence, 45(4):47134726, 2022.",
  "Zikai Song, Run Luo, Junqing Yu, Yi-Ping Phoebe Chen, and Wei Yang. Compact transformer trackerwith correlative masked modeling. arXiv preprint arXiv:2301.10938, 2023": "Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composingrobust features with denoising autoencoders. In Proceedings of the 25th international conference onMachine learning, pages 10961103, 2008. Paul Voigtlaender, Jonathon Luiten, Philip HS Torr, and Bastian Leibe. Siam r-cnn: Visual tracking byre-detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pages 65786588, 2020. Ning Wang, Wengang Zhou, Jie Wang, and Houqiang Li. Transformer meets tracker: Exploiting temporalcontext for robust visual tracking. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 15711580, 2021. Xing Wei, Yifan Bai, Yongchao Zheng, Dahu Shi, and Yihong Gong. Autoregressive visual tracking. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96979706,2023. Qiangqiang Wu, Tianyu Yang, Ziquan Liu, Baoyuan Wu, Ying Shan, and Antoni B Chan. Dropmae:Masked autoencoders with spatial-attention dropout for tracking tasks. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1456114571, 2023. Junliang Xing, Haizhou Ai, and Shihong Lao. Multiple human tracking based on multi-view upper-bodydetection and discriminative learning. In 2010 20th International Conference on Pattern Recognition,pages 16981701. IEEE, 2010. Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. Siamfc++: Towards robust and accurate visualtracking with target estimation guidelines. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 34, pages 1254912556, 2020. Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformerfor visual tracking. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages1044810457, 2021. Bin Yan, Xinyu Zhang, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Alpha-refine: Boosting trackingperformance by precise bounding box estimation. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 52895298, 2021. Dawei Yang, Jianfeng He, Yinchao Ma, Qianjin Yu, and Tianzhu Zhang. Foreground-background distri-bution modeling transformer for visual object tracking. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 1011710127, 2023.",
  "Zhipeng Zhang, Houwen Peng, Jianlong Fu, Bing Li, and Weiming Hu. Ocean: Object-aware anchor-freetracking. In European Conference on Computer Vision, pages 771787. Springer, 2020": "Haojie Zhao, Dong Wang, and Huchuan Lu. Representation learning for visual object tracking by maskedappearance transfer. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1869618705, 2023. Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, and Wenqiang Zhang.Reading relevant feature from global representation memory for visual object tracking. In Thirty-seventhConference on Neural Information Processing Systems, 2023. Xinyu Zhou, Pinxue Guo, Lingyi Hong, Jinglun Li, Wei Zhang, Weifeng Ge, and Wenqiang Zhang.Reading relevant feature from global representation memory for visual object tracking. Advances in NeuralInformation Processing Systems, 36, 2024.",
  "A.1The differences between DDPM, DAE, and DeTrack": ". According to Tab.6, we compares and analyzes the differences between DDPM, DAE, and DeTrackin denoising learning, highlighting the advantages of the DeTrack model in visual object tracking.All three use Gaussian noise to simulate the noise characteristics of the input; however, they differ ininput data, encoding methods, decoding methods, optimization objectives, and inference approaches.DDPM and DAE take noisy images as input (x), aiming to restore or generate high-quality images,",
  "while DeTrack innovatively uses noisy bounding boxes (b) as input, making it more suitable forvisual tracking tasks in complex backgrounds and scenarios with fast-moving objects": "In terms of encoding, DDPM and DAE use single-layer feature encoding to obtain z = f(x); incontrast, DeTrack employs multi-layer feature encoding with layer-by-layer denoising within themodel, resulting in multiple hidden states zi (z12, z11, . . . , z1 = f(b)). This layer-wise denoisingapproach retains and optimizes target feature information, enhancing robustness to bounding boxnoise. For decoding, DDPMs decoding target is to restore the noise = g(z), while DAE directlydecodes the image x = g(z). DeTrack, on the other hand, decodes to a denoised bounding boxb = g(z12), ensuring high-precision localization of the target bounding box. Regarding the optimization objective, DDPM minimizes the error between generated noise and targetnoise ( ), DAE minimizes the error between the denoised image and the original image (x x),and DeTrack optimizes the error between the denoised bounding box and the original bounding box(b b), making it more suitable for accurate visual target localization. For inference, DDPM uses areverse diffusion process to progressively denoise and generate an image, while DAE and DeTrackdirectly generate denoised results in inference: DAE outputs the image x = g(z), and DeTrackoutputs the bounding box b = g(z12). Overall, DeTracks multi-layer feature encoding with internal model denoising, specific decodingapproach, and optimization objective enable it to exhibit higher robustness in noisy and complexbackgrounds, making it well-suited for target tracking tasks in dynamic and complex scenarios.",
  "A.2Comparison of noise prediction pattern": ". According to 7, Predicting the total noise resulted in a decrease of 1.9 in AO, 2 in SR0.5, and 2.1 inSR0.75,compared to multiple noise predictions. We analyze that this is because predicting the totalnoise directly is more challenging than predicting it layer by layer. Layer-by-layer denoising allowsthe model to learn to filter out some noise at intermediate layers before arriving at the final result,rather than achieving it in one step.",
  "A.3Analysis of Denoising Paradigm with ViT-Small": ". presents an ablation study on different denoising paradigms and step settings evaluated onthe GOT-10k dataset with a Vit-Small backbone. We compare performance metrics such as AverageOverlap (AO) and Success Rates at two different overlap thresholds (SR0.5 and SR0.75). The results indicate that multiple forward passes generally yield better performance compared to asingle forward pass. Specifically, a step count of 48 achieves the best AO, SR0.5, and SR0.75 values,with scores of 69.4, 78.5, and 63.4, respectively, highlighted in bold in . This suggests thatwhile increasing the number of steps from 12 (single forward pass) to 48 improves performance,further increasing to 96 steps does not result in additional gains, possibly due to diminishing returnsin iterative refinement or over-smoothing of features.",
  "Multiple forward passes9668.978.263.2Multiple forward passes4869.478.563.4Multiple forward passes2469.478.063.0Single forward pass1269.178.362.9": "Notably, the AO metric remains at 69.4 for both 48 and 24 steps, although the success rates (SR0.5 andSR0.75) are slightly lower at 24 steps. This finding implies that 48 steps might strike a balance betweencomputational efficiency and denoising effectiveness, providing optimal tracking performance withoutthe need for excessive forward passes. In summary, the experiments demonstrate that while iterative denoising is beneficial, there exists anoptimal step count (48 in this case) that maximizes tracking accuracy. This demonstrates that ourproposed DeTrack, when using ViT-Small as the backbone, can enhance tracking accuracy througha recursive denoising approach, similar to DDPM. However, this recursive denoising introduces asignificant increase in computational complexity.",
  "DiffusionTrackDeTrack EncoderDiffusionDet DecoderDeTrackDeTrack EncoderDeTrack Decoder": "DiffusionDet cannot be directly applied to object tracking, as it requires interaction between thetemplate and search region in tracking. Therefore, as shown in Tab. 10, we use the Encoder fromDeTrack, which enables this interaction, as the encoder for DiffusionDet. The decoder is taken fromDiffusionDet. We call this model DiffusionTracking, and it uses a resolution of 384x384. For fairness,the learning rate, number of epochs, weight decay, and other training parameters are kept consistent.",
  "A.5Comparison on GOT-10k between DiffusionTrack and DeTrack": "The performance comparison on GOT-10k dataset between DiffusionTrack and DeTrack demonstratesnotable differences in tracking accuracy and computational efficiency across various denoising steps.For DiffusionTrack, the tracking performance generally improves as the step count increases, reachingits peak with 8 steps, where the Average Overlap (AO) is 73.5%, SR0.5 is 82.9%, and SR0.75 is71.2%. However, this improvement comes at the cost of increased computational requirements, withthe FLOPS reaching 147.2G at 8 steps and 162.7G at 12 steps. In contrast, DeTrack, tested with 12 steps, achieves the highest performance overall, with an AO of77.9%, SR0.5 of 86.5%, and SR0.75 of 74.9%, surpassing all DiffusionTrack configurations. DeTrackalso maintains lower computational complexity with 119.0G FLOPS, suggesting a more optimalbalance of tracking accuracy and efficiency. This analysis indicates that while DiffusionTrack benefitsfrom increased steps in tracking performance, DeTrack achieves superior results both in accuracy andcomputational efficiency."
}