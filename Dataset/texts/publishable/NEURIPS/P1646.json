{
  "Abstract": "Large pretrained foundation models demonstrate exceptional performance and, insome high-stakes applications, even surpass human experts. However, most of thesemodels are currently evaluated primarily on prediction accuracy, overlooking thevalidity of the rationales behind their accurate predictions. For the safe deploymentof foundation models, there is a pressing need to ensure double-correct predictions,i.e., correct prediction backed by correct rationales. To achieve this, we propose atwo-phase scheme: First, we curate a new dataset that offers structured rationalesfor visual recognition tasks. Second, we propose a rationale-informed optimizationmethod to guide the model in disentangling and localizing visual evidence foreach rationale, without requiring manual annotations. Extensive experiments andablation studies demonstrate that our model outperforms state-of-the-art modelsby up to 10.1% in prediction accuracy across a wide range of tasks. Furthermore,our method significantly improves the models rationale correctness, improvinglocalization by 7.5% and disentanglement by 36.5%. Our dataset, source code, andpretrained weights:",
  "Prediction": ": Unsafe prediction examples. Cor-rect prediction, incorrect rationale: CLIPidentifies a red light, but wrongly based on redballoons. Incorrect prediction, correct ra-tionale: GPT-4V incorrectly predicts a closeddoor, yet based on plausible visual evidence. Large foundation models, such as CLIP and GPT-4V , exhibit exceptional performance or even sur-pass human experts in some high-stakes applications,such as medical diagnosis and autonomous driv-ing . However, most of these models are cur-rently evaluated primarily on prediction accuracy,overlooking a critical aspect for ensuring safety, i.e.,the validity of the reasons behind their accurate pre-dictions. Understanding the rationales - the howand why behind model predictions - is crucial fordeveloping safe predictions. shows typicalexamples of unsafe predictions: CLIP might predictaccurately yet based on wrong rationales, whereasGPT-4V might make wrong predictions based on ra-tionales that are plausible to humans. To build trust inreal-world deployment, a natural question arises: Canmodels make double-correct predictions, i.e., correctpredictions backed by correct rationales?",
  "arXiv:2411.00132v2 [cs.LG] 7 Nov 2024": "on valid visual evidence . There are existing attempts to provide rationales for machinelearning models predictions. They either explicitly force the models to make decisions basedon human-understandable concepts by introducing bottleneck layers , or implicitly injectcommonsense knowledge into models by contrastive learning between similar yet distinct textualconcepts . However, none of them ensures double-correct predictions. Observations from ourprevious research and recent studies in the field reveal that these models might provideincorrect rationales, as they fail to base the rationales on valid visual evidence.",
  "To this end, we develop double-correct predictions by focusing on two foundational aspects:": "i) What are the correct rationales? Structured rationale acquisition. Existing vision datasetstypically provide ground truth labels of predictions, whereas missing the rationales behind thesedecisions . To fill this gap, we curate a new dataset that offers over 4,000 unique textualrationales designed for predicting the 1,000 categories in ImageNet , structured in a tree format.This design differs from existing knowledge graphs , which either provide irrelevantknowledge for the vision task or are too coarse-grained, providing insufficient information. Ourrationale dataset is tailored to capture the detailed reasoning processes for visual recognition. ii) Where are the correct rationales? Rationale-informed optimization. The other challenge indeveloping double-correct predictions is the absence of pixel-wise annotations for rationales visualevidence. Although some datasets provide segmentation masks of object parts , they lacksufficient rationale coverage and are limited to small-scale use cases . To address this issue, wepropose a rationale-informed optimization method to guide the model in disentangling and localizingthe visual evidence of rationales, without requiring manual annotations. Our method can be integratedinto the existing model training process without architectural changes and extra parameters. We evaluate the proposed method on a wide range of benchmark datasets and tasks. For predictioncorrectness, our model outperforms state-of-the-art models in zero-shot, linear probe, and fine-tuningsettings by 2.6%, 2.0%, and 10.1%. For rationale correctness, the empirical results exhibit that ourmodel significantly improves ground truth rationale localization and rationale disentanglability by7.5% and 36.5%. Furthermore, the extensive qualitative results and ablation studies demonstrate theeffectiveness of the proposed method. Our contribution includes: 1) We curate a new structured rationale dataset. 2) A faithful explanationmethod tailored for explaining CLIP-ViT predictions. 3) A principled optimization method thatseamlessly integrates structured rationale information to develop double-correct predictions. 4)Empirical results in a wide range of benchmark datasets and tasks including image classification andretrieval demonstrate the superior prediction and rationale correctness of our model.",
  "Definition 1 (Rationales) Given a category y, rationales are a set of K underlying abstract notions{ryk}Kk=1 and relations that capture the reasoning process leading to the recognition of y": "In the real world, rationales can be represented through textual descriptions . For example,when recognizing a specific breed of dog in an image, the rationales could be a set of concepts suchas the shape of the ears, the color of the fur, and the size of the dog. Mathematically, given a textualrationale r, we assume the existence of a ground truth labeling function V (x, r) that can provide thepixel-wise annotations of visual evidence corresponding to r on an input x.",
  "Definition 2 (Double-Correct Predictions) A correct prediction is double-correct when it is backedby correct rationales that are based on valid visual evidence": "Denote (x, y) P(X, Y ) as a data point sampled from the training distribution P(X, Y ), g() as anexplanation method that attributes the prediction of text r to a group of pixels in input x dependingon model f, () as the task-specific loss function, and F as a function class that is model-agnosticfor the prediction task. To ensure the model f makes double-correct prediction, we propose to solvethe following constrained optimization problem:",
  "minfF R(f) := E(x,y)P (X,Y )[(f(x), y)]s.t. g(x, r; f) = V (x, r), r {ryk}Kk=1.(1)": "The problem in Eq. 1 is challenging to solve, since we neither have access to the rationales {ryk}Kk=1,nor to the ground truth labeling functions V (). There are existing attempts that employ domainexperts to manually collect textual descriptions of rationales , or pixel-wise annotations ofobject parts on the image . However, these approaches are often limited to small-scale datasets,and impractical in large-scale settings due to the high cost of fine-grained annotations .",
  "StockyShortBareRound": ": Our structured rationales capture themajor attributes and their sub-attributes thatlead to the recognition of objects. Our datasetoffers over 4,000 unique rationales coveringall 1,000 categories from ImageNet . In this section, we curate a new rationale dataset tooffer {ryk}Kk=1 in Eq. 1. According to Def. 1, ratio-nales are structured human knowledge. Therefore,ontologies that encapsulate complex, interconnectedinformation while maintaining semantic relationshipsbetween entities , present a proper tool to rep-resent rationales. The benefits are bi-directional: i) inthe human-to-machine direction, it offers a standard-ized, machine-readable format; ii) in the machine-to-human direction, ontology structure mirroring howhumans organize and retrieve information to explainthe models decision-making process. Acquire structured rationales: Different from existing works that are limited to small-scale manualannotation, we generate our rationale dataset in a scalable manner. Specifically, we utilize LargeLanguage Models (LLMs) like GPT-4 to extract the structured rationales. Existing studies provethat GPT-4 has expert-level expertise in commonsense and domain knowledge . However,we find that directly querying LLMs would yield inconsistent tree structures that can hardly be usedby machine learning models. To address this issue, we provide a series of exemplary structuredrationales before the query, employing in-context learning to extract standardized rationales in a.JSON format. See Appendix A for our full prompt and rationale examples. Rationale dataset statistics: Our dataset covers all 1,000 categories in the ImageNet . For eachcategory, we generate an ontology tree with a maximum height of two. As illustrated in , the rootnode is the category, the children of the root are the attributes, and the leaves are the sub-attributes.The edges represent the relationships between nodes. Combining attributes and sub-attributes, ourdataset contains over 4,000 unique rationales. Our rationale ontology trees capture the reasoningprocesses leading to the recognition of the corresponding root categories. Can we trust the rationales extracted from GPT-4? Although there are plenty of works showingGPT-4s remarkable capabilities , it still could suffer from hallucinations . However,evaluations on the generation quality are largely missing from existing works that generate datafrom LLMs . To fill this gap and ensure the quality of our rationale data, we conductcomprehensive human and machine evaluations. As detailed in Sec. 4.1, on a 5-point Likert scaleacross three metrics, 964 out of 1,000 categories are scored as having high-quality rationales (4.0). In contrast to existing Knowledge Graphs that either offer knowledge unrelated to thevisual prediction task, or are too coarse-grained that provide insufficient information, our structuredrationales are tailored for visual recognition tasks in a fine-grained attribute level. Furthermore, ourdataset can expand to accommodate new rationales, providing flexibility to dealing with evolvingdatasets where more data becomes available. For example, our rationale ontologies can be seamlesslyintegrated following the ImageNet category ontology derived from WordNet .",
  "In this section, we develop a new explanation method to implement g() in Eq. 1. To incorporate bothimage and text inputs, we instantiate the model f using the CLIP-ViT architectures because of": "02468 10 12 14 16 18 20 22 24 26 28 30 32 Accumulated mean-ablated layers ImageNet Accuracy ViT-B/32ViT-L/14ViT-H/14 : Multi-head Self Attention (MSA)accumulated mean-ablation study. Based onEq. 2, we replace the direct effects of MSAsup to a specific layer with their mean valuescalculated across the ImageNet valida-tion set. Most of the performance gains canbe attributed to the final layers of the ViT. :Weakly-supervised segmentation accu-racy on ImageNet-Seg . We threshold explana-tion heatmaps from CLIP-ViT-L-14 as segmentationmasks. Our method outperforms existing explana-tion methods in segmentation accuracy, demonstrat-ing the high faithfulness of our explanations.",
  "Ours76.2758.0482.17": "their proven capability . Existing methods for explaining the ViT model either directly use theattention maps as explanations , or weigh them using gradients . However, these methodsmight be unfaithful to the ViT predictions. This is because the computation of each ViT predictioninvolves queries, keys, and values, whereas the attention maps only capture the inner products ofqueries and keys, ignoring information in values that also affect predictions . Therefore,explanations based on attention maps might not fully reflect the reasons behind ViT predictions. Decompose ViT outputs: Recent works prove that, for ViT models, the image embeddingscan be decomposed into the contributions of each token within each attention head. Let and parameterize the image- and text-encoder of the CLIP-ViT model, P is the projection matrix, L, M,N are the numbers of layers, heads, and image tokens, al,miis the output of the m-th attention headin layer l for the i-th image token, then the embedding of image I can be decomposed as:",
  "By contracting along layers and heads, calculates the contribution of the i-th image token to thefinal image embedding using Ll=1Mm=1Pal,mi": "Faithful explanations weighted by mean-ablation results: As indicated by our mean-ablationresults in , the final layers contribute the most to the predictions, whereas the earlier layers haveminimal impact. Thus, noise from early layers could obscure key information by a naive summationacross all layers as in . To address this issue, we weigh each layers contribution based on itsimportance, measured by the corresponding performance drop in the mean-ablation study. Denotethe performance drop of layer l as l, we calculate the contribution of the i-th image token by:",
  "Note that ei is projected onto the image-text embedding space by P. Thus, we can use g(I, r) ={ei, f(r)}iI to calculate the explanations of rationale r on an image I, i.e., visual evidence": "Our method significantly improves the explanation accuracy, as shown in Tab. 1. In contrast toattention-based explanations , our method fully utilizes the information from queries, keys, andvalues that are used for ViT predictions. Compared to gradient-weighted attention maps , ourmethod cuts down the computational complexity from O(n2) to O(n) over n image tokens.",
  "In this section, we develop double-correct predictions by disentangling and localizing rationaleswithout pixel-wise human annotations V () in Eq. 1": "Disentanglement via reconstruction: Drawing insights from our previous research , wepropose to contrast between explanation heatmaps of rationales to guide the model training in a self-supervised manner. Specifically, we enforce the following two constraints: i) the image embeddingsfor different rationales within the same category are disentangled, and ii) the aggregated imageembedding of all rationales within the same category aligns with the text embedding of the category. Mathematically, the backbone objective is to learn a mapping function f F such that for eachimage-text pair (I, T) P(I, T), the embeddings f(I) and f(T) are aligned in a shared spaceif they are a correct match, where T is a text description of category y. Let () be the InfoNCEloss . h(g(I, r)) = iei 1(g(I, r)i > ) extracts the image embedding of a given rationale.D(, ) is a distance metric such as L2 distance. , , and are thresholding hyperparameters. For allr, r {ryk}Kk=1, we propose to develop double-correct predictions by optimizing:",
  "Correct Rationales(4)": "Intuitively, the reconstruction term prevents the disentanglement from collapsing into trivial solutions,thereby ensuring localization. Solving Eq. 4 often leads to a non-convex problem, wherein methodssuch as stochastic gradient descent (SGD) cannot guarantee constraint satisfaction . Toaddress this issue, we leverage KarushKuhnTucker (KKT) conditions and introduceLagrange multipliers and to convert the constrained problem into its unconstrained counterpart:minfF {R(f) := E(I,T )P (I,T)[(f(I, T))]",
  "Experiments": "In this section, we first evaluate the quality of our curated rationale dataset in Sec. 4.1. To best validatedouble-correct predictions, we then conduct a series of experiments to compare the proposed methodwith existing methods in Secs 4.2 - 4.7. The experimental results prove that our model achievessuperior prediction and rationale correctness on a wide range of benchmark datasets and tasks.",
  "Evaluation of Rationale Quality": "Metrics: We focus on three essential aspects of the rationale quality. (1) Factual Consistency:whether the rationales are consistent with facts. (2) Comprehensiveness: whether the rationalesprovide sufficient information necessary to predict the category. (3) Visual Disentanglement: whetherthe rationales are visually disentanglable or non-overlap. We rate them on a 5-point Likert scalescoring system, where higher scores indicate better performance. For example, in Factual Consistency,score 5 means 100% of the generated rationales are consistent with facts, score 4 means 75%, score 3means 50%, score 2 means 25%, and score 1 means completely wrong. Evaluators: (1) Human Evaluators: We recruited four human evaluators, who are mostly graduatestudents. They are asked to conduct assessments based on commonsense knowledge and performInternet searches for validation. On average, it takes them around one minute per sample. (2)Machine Evaluators: The latest GPT-4o and GPT-4v models (date accessed: Aug. 6th, 2024). Foreach evaluation, we perform three independent runs and calculate the average scores. Note thatexpanding human evaluations to the entire dataset is not scalable. To this end, we first prove thereliability of machine evaluations, then use it to automatically evaluation the entire dataset. Human evaluations: We sample three independent groups of data from our rationale dataset, eachconsisting of 50 categories and their corresponding rationales. Specifically, categories were randomlyselected from their superclasses: Animals (20), Objects & Artifacts (15), Natural Scenes (5), Plants(5), and Human Activities (5). This ensures that not only each superclass is represented but also thatour results are robust . As shown in Tab. 2, The dataset consistently achieves scores of 4.61 orhigher on the average of evaluators for each metric, indicating that over 90.3% of the rationales foreach category are highly factual, comprehensive, and visually disentanglable.",
  "Benchmark Datasets and Implementation Details": "Backbone model: Due to the computational cost of training large vision-language models (VLMs)from scratch, we focus on fine-tuning experiments. Specifically, we fine-tune the ViT-B/32 variant ofCLIP on the ImageNet dataset combined with our curated rationale dataset. To maintain simpleand interpretable rationales, the ontology graph for each category is limited to a maximum depth oftwo, allowing for the extraction of five to six independent concepts on average. Baseline models: We compare our model with state-of-the-art VLMs that use ViT-B/32 as their visionencoders, including large-scale pretrained models (CLIP , DeCLIP ), knowledge-augmentedmodel (NegCLIP ), and fine-grained alignment models (FILIP , PyramidCLIP ). Forfair comparisons, we also compare our model with ImageNet fine-tuned models using the sameCLIP initialization and augmented text descriptions as our model, including full model fine-tuning(-ft) and vision-encoder-only fine-tuning (-ft-vision). Evaluation datasets: We validate the prediction correctness of the models on image classification andimage-text retrieval tasks. For image classification (zero-shot, linear probe), experiments are carriedout on nine benchmark datasets, including CUB , Caltech101 , OxfordPets , Food101 ,SUN397 , StanfordCars , DTD , CIFAR-10 , and CIFAR-100 . For retrieval, weconduct experiments on Flickr30K and MSCOCO . To evaluate the correctness of rationales,we evaluate the models rationale localizability on CUB-Part and PartImageNet that provideground truth segmentation masks of object parts, e.g., head and body. Furthermore, we evaluatethe rationale disentanglability on the aforementioned nine benchmark datasets. More details can befound in Appendix D. Implementation details: We follow the same architecture design as CLIP for ViT-B/32. Theinput resolution of image encoder is 224224 and the maximum context length of text encoder is 77.We train our model using an AdamW optimizer and the cosine learning rate scheduler with alinear warmup. Specifically, the learning rate linearly increases from 0 to the peak value within 10%of the total steps, and then decreases with a cosine anneal strategy. Our learning rate is set to 5e-7and train the model for eight epochs. More details can be found in Appendix D.",
  "Rationale correctness: We define two new metrics to measure rationale correctness": "i) Rationale localizability. We evaluate the correctness of rationales using ground truth segmentationmasks of object parts . Following the standard evaluation protocol , we threshold therationale explanation heatmaps to segmentation masks and calculate a mean Intersection over Union : Comparison of prediction accuracy (%) on nine benchmark datasets. Our results are on theaverage of three trials of experiments using different random seeds. We highlight the best results andthe second best results. Surprisingly, different from most interpretability methods that compromisebenchmark performance, our method also enhances prediction accuracy.",
  "Ours95.682.777.299.392.988.179.883.088.987.5": "(mIoU ) score with the ground truth masks across different object parts. Specifically, the dynamicthreshold = + , where and are the mean and standard deviation of importance values of allpixels in a heatmap. The pixel with an importance value larger than is set to 1, otherwise 0. ii) Rationale disentanglability. As shown in , for the CLIP model , the visual evidenceof different rationales is entangled. Specifically, we treat the disentanglement between the visualevidence of different rationales as an important metric to evaluate whether the model can distinguishrationales. Specifically, we treat rationale explanation heatmaps m and m as vectors and calculate1 |m, m| as an intuitive measure of disentanglability, the higher metric value the better.",
  "Evaluation on Prediction Correctness": "Zero-shot image classification: We compare our model against other state-of-the-art and fine-tunedVLMs on zero-shot image classification tasks. The results are shown in Tab. 3. On the average ofnine datasets, our model outperforms the second-best result by 2.6%. The results indicate the strongtransferability of our model to other vision datasets. Linear probe: Following the common practice , we conduct linear probe experiments on thenine image classification datasets. As shown in Tab. 3, our model outperforms the second-best resultby 2.0%. These results demonstrate the superior vision representations learned by our model. Fair comparison with fine-tuned models: As shown in Tab. 3, our model outperforms the bestfine-tuned model by 10.1% and 5.3% on zero-shot and linear probe results. This suggests that theproposed Rationale-informed Optimization is essential in improving the models performance.",
  "Evaluation on Rationale Correctness": "Rationale localizability: We compare our model with state-of-the-art and fine-tuned VLMs. Asshown in Tab. 4, our model significantly improves the localization accuracy of rationales by 7.5%and 6.0% on CUB-Part and PartImageNet . This suggests that even without using explicitregion annotations, our method significantly enhances the models localizability of rationales. Rationale disentanglability: We compare the rationale disentanglement performance of our modelwith state-of-the-art and fine-tuned models. As shown in Tab. 5, on the average of nine imageclassification datasets, our model outperforms the second-best result by 36.5%. This significantimprovement reveals that our model can distinguish between different rationales.",
  "Original": "high wingspanenginestail finnarrow bodypointed headAirliner CLIPOurs Originalwhite fur coat pointed snoutsmall pawsbushy tailrounded headArctic FoxOriginal : Qualitative results of rationale disentanglement and localization. The rationales visualevidence of the CLIP model typically highlights the entire object, lacking precise localization. Incontrast, our model can correctly localize rationales, thereby enhancing trust in its predictions. : Comparison of rationale localizability on CUB-Part and PartImageNet . As detailedin Sec. 4.3, we threshold rationales explanation heatmaps as segmentation masks and calculate theirmIoU () with ground truth masks of corresponding object parts. Our model significantly improvesthe localization accuracy of fine-grained object parts. Full table in Appendix C.",
  "Ours400M+IN25.310.112.732.615.735.221.911.2": "experiments with fine-tuned CLIP models. We compare our model with baseline (CLIP-zs), fullmodel fine-tuning (CLIP-ft), and vision-only fine-tuning (CLIP-ft-vision). All fine-tuned models usethe same CLIP initialization and receive the same language supervision as our model. As shownin Tabs. 4& 3, our model outperforms the best fine-tuned model by 9.8% and 41.1% on rationalelocalizability and disentanglability. This indicates that naive fine-tuning using augmented informationwithout constraints would deteriorate the rationale correctness of the model. Qualitative results: In , we show the visualizations of our visual evidence of different rationales.As shown, the rationales visual evidence of the CLIP model are entangled and mislocalized. Incontrast, the rationales visual evidence of our model are visually distinct and correctly localized.",
  "Ablation Study": "Ablation on rationale disentanglement: The w/o disen. refers to a variant of our method withoutrationale disentanglement constraint. As shown in Tab. 7, the rationale localizability decreased by10.4%, indicating the model might not learn to distinguish between rationales without constraints. Ablation on reconstruction: The w/o recon. refers to a variant of our method without reconstruc-tion constraint. As shown in Tab. 7, the rationale localizability and prediction accuracy drasticallydecreased by 13.3% and 30.2%. This reveals that recklessly optimizing the disentanglement betweenrationale can easily fall into trivial solutions. Generalize to different rationale sets: According to DCLIP , using the text embeddings of con-cepts as a bottleneck layer to force the CLIP model to predict based on them can improve predictionaccuracy and interpretability. Specifically, the final prediction will be made by the average embeddingsimilarity between the image and all concepts, namely y = argmaxy1KKk=1f(I), f(cyk). We",
  "use the concept set provided in rather than our training rationale dataset. As shown in Tab. 8, ourmodel can generalize to an unseen concept set with improved prediction accuracy": "Ablation using random string: WaffleCLIP shows that random concept strings as bottleneckscan achieve similar performance gains in DCLIP . We conduct an ablation study using the randomstrings provided by . As shown in Tab. 8, since our model can distinguish between differentrationales, the random strings deteriorate the prediction accuracy of our model.",
  "Zero-shot image-text retrieval: We evaluate our model on zero-shot image-text retrieval tasks. Asshown in Tab. 6, the improved rationale correctness also benefits retrieval tasks": "Rationale-based text-to-image retrieval: To better evaluate the rationale correctness of our model,we conduct a novel retrieval task: rationale-based text-to-image retrieval. The model should retrievethe image with a specified rationale presented. As shown in , in contrast to the CLIP model that entangles rationales with specific categories, our model precisely understands the semanticmeaning of rationales independent to categories.",
  "Related Works": "Vision model explainability. A widely adopted branch of explainability methods post hoc generatesheatmaps to identify the image regions most crucial to the models predictions, e.g., GradCAM ,LIME , and SHAP . Although useful for revealing the correlations between inputs and outputs,such explanations might be ambiguous, and fail to correspond to high-level concepts that humanseasily understand . Methods like TCAV curate attribute datasets to explain vision models usingconcepts familiar to humans. However, such methods can fail when the models do not learn theseconcepts . Another branch of methods attempts to design specific architecture to intrinsicallyinterpret model predictions, e.g., CBM and ProtoPNet . However, they cannot guarantee themodel learns the semantic meanings of the concepts correctly and yield compromised prediction CLIPOurs (a) Query: a photo of long neck. CLIPOurs (b) Query: a photo of wings. : Qualitative results of zero-shot text-to-image retrieval on MSCOCO . The task is toretrieve the top-5 images with a given rationale presented. The CLIP results reveal a significantentangle of rationales with a specific category, such as long neck with giraffes and wings withairliners. In contrast, our model treats rationales independently from categories, thus offering diverseretrieval results. For example, the long neck found in birds, giraffes, dears, and bottles.",
  "accuracy . Different from existing works, our method incorporates explanations to guide themodel training, achieving accurate predictions backed by correct rationales": "Knowledge augmentation for vision-language models. Visual models often learn spurious corre-lations that stem from data biases unrelated to the causal explanation of interest , whereasexternal knowledge allows models to learn the right features . Existing attempts for injectingknowledge into the models are often from the language modality. K-LITE enrich the imagecaption using knowledge from WordNet and Wikitionary . NegCLIP and DANCE improve the commonsense understanding of CLIP by generating hard negative captions, the latter usesknowledge from ConceptNet . StructureCLIP leverages scene-graphs to incorporateknowledge into text embeddings. However, our results (Tabs. 4& 7) reveal that solely augmentinginformation in the language cannot guarantee the model learning correct features. In contrast to theseworks, our method offers supervision signals from both modalities to ensure double-correctness. Contrastive vision-language alignment. Different from conventional multimodal learning that fusesdifferent modalities , large-scale vison-language pretrained models, such as CLIP andALIGN , exhibit promising zero-shot transferability to downstream tasks. However, their globalalignment objective is coarse, which only learns the existence of objects like bag-of-word whileignoring their localizations . Recent attempts like PyramidCLIP and X-VLM leverageobject region annotations to align word phrases with image regions. DeCLIP and FILIP align text with image regions through self-supervised learning. However, their supervision is limitedto a coarse, object-level granularity. Different from these works, our method offers fine-grained,concept-level supervisory signals of rationales without expensive manual annotations.",
  "Limitation": "While our study advances the double-correctness of predictions, it is not without limitations. First,the absence of explicit ground truth for rationale localization in large-scale datasets remains asignificant challenge. We mitigated this by leveraging a self-supervised rationale disentanglementand localization method, but this approach depends heavily on the quality of the structured rationaleontologies. Second, our methods, though effective, are computationally intensive, which may limittheir applicability in resource-constrained scenarios.",
  "Conclusion": "We introduce a new concept of double-correct predictions aimed at training vision-language founda-tion models to make accurate predictions backed by correct rationales, thereby enhancing their safetyfor real-world deployment. To support this, we establish a solid foundation for the developmentof double-correct predictions. Specifically, we develop a unique dataset with structured rationalesthat clearly outline the reasoning processes necessary for visual recognition tasks. Furthermore, wepropose a principled rationale-informed optimization method tailored for double-correct prediction.Our comprehensive empirical evaluations demonstrate that our method significantly enhances thedouble correctness of vision-language model predictions. This work is supported by the NSF CAREER Award No. 2340074, the NSF SLES Award No.2416937, the NSF III CORE Award No. 2412675, and the DoD DEPSCoR Award AFOSR FA9550-23-1-0494. Any opinions, findings and conclusions or recommendations expressed in this materialare those of the authors and do not reflect the views of the supporting entities. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visualmodels from natural language supervision. In International conference on machine learning,pages 87488763. PMLR, 2021. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia LeoniAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4Technical Report. arXiv preprint arXiv:2303.08774, 2023.",
  "Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-bilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023": "Xiwen Liang, Yangxin Wu, Jianhua Han, Hang Xu, Chunjing Xu, and Xiaodan Liang. Effectiveadaptation in multi-task co-training for unified autonomous driving. In Advances in NeuralInformation Processing Systems, 2022. Xiwen Liang, Minzhe Niu, Jianhua Han, Hang Xu, Chunjing Xu, and Xiaodan Liang. Visualexemplar driven task-prompting for unified perception in autonomous driving. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 96119621,2023.",
  "Mert Yuksekgonul, Maggie Wang, and James Zou. Post-hoc concept bottleneck models. In TheEleventh International Conference on Learning Representations, 2022": "Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. Whenand why vision-language models behave like bags-of-words, and what to do about it? In TheEleventh International Conference on Learning Representations, 2022. Shuquan Ye, Yujia Xie, Dongdong Chen, Yichong Xu, Lu Yuan, Chenguang Zhu, and JingLiao. Improving commonsense in vision-language models via knowledge graph riddles. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages26342645, 2023.",
  "Jeffrey Mark Siskind. Grounding language in perception. Artificial Intelligence Review, 8:371391, 1994": "Roxana Daneshjou, Mert Yuksekgonul, Zhuo Ran Cai, Roberto Novoa, and James Y Zou.Skincon: A skin disease dataset densely annotated by domain experts for fine-grained debuggingand analysis. Advances in Neural Information Processing Systems, 35:1815718167, 2022. Yude Wang, Jie Zhang, Meina Kan, Shiguang Shan, and Xilin Chen. Self-supervised equivariantattention mechanism for weakly supervised semantic segmentation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1227512284,2020.",
  "Quan Wang, Zhendong Mao, Bin Wang, and Li Guo.Knowledge graph embedding: Asurvey of approaches and applications. IEEE transactions on knowledge and data engineering,29(12):27242743, 2017": "Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip. A survey onknowledge graphs: Representation, acquisition, and applications. IEEE transactions on neuralnetworks and learning systems, 33(2):494514, 2021. Sbastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, EceKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial generalintelligence: Early experiments with gpt-4. arxiv. arXiv preprint arXiv:2303.12712, 2023. Zhengliang Liu, Hanqi Jiang, Tianyang Zhong, Zihao Wu, Chong Ma, Yiwei Li, Xiaowei Yu,Yutong Zhang, Yi Pan, Peng Shu, et al. Holistic evaluation of gpt-4v for biomedical imaging.arXiv preprint arXiv:2312.05256, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models arefew-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Timothy R McIntosh, Tong Liu, Teo Susnjak, Paul Watters, Alex Ng, and Malka N Halgamuge.A culturally sensitive test to evaluate nuanced gpt hallucination. IEEE Transactions on ArtificialIntelligence, 2023. Mikal Chelli, Jules Descamps, Vincent Lavou, Christophe Trojani, Michel Azar, MarcelDeckert, Jean-Luc Raynier, Gilles Clowez, Pascal Boileau, and Caroline Ruetsch-Chelli. Hallu-cination rates and reference accuracy of chatgpt and bard for systematic reviews: Comparativeanalysis. Journal of Medical Internet Research, 26:e53164, 2024. Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and MarkYatskar. Language in a bottle: Language model guided concept bottlenecks for interpretableimage classification. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1918719197, 2023. Ziyuan Qin, Hua Hui Yi, Qicheng Lao, and Kang Li. Medical image understanding withpretrained vision language models: A comprehensive study. In The Eleventh InternationalConference on Learning Representations, 2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.An image is worth 16x16 words: Transformers for image recognition at scale. In InternationalConference on Learning Representations, 2020. Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learningfor vision-language models. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1681616825, 2022.",
  "Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. arXiv preprintarXiv:2005.00928, 2020": "Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages782791, 2021. Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, DeviParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-basedlocalization. In Proceedings of the IEEE international conference on computer vision, pages618626, 2017. Yibing Liu, Haoliang Li, Yangyang Guo, Chenqi Kong, Jing Li, and Shiqi Wang. Rethinkingattention-model explainability through faithfulness violation test. In International Conferenceon Machine Learning, pages 1380713824. PMLR, 2022. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework fortransformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Yossi Gandelsman, Alexei A Efros, and Jacob Steinhardt. Interpreting clips image represen-tation via text-based decomposition. In The Twelfth International Conference on LearningRepresentations, 2023.",
  "Matthieu Guillaumin, Daniel Kttel, and Vittorio Ferrari. Imagenet auto-annotation withsegmentation propagation. International Journal of Computer Vision, 110:328348, 2014": "Alexander Binder, Grgoire Montavon, Sebastian Lapuschkin, Klaus-Robert Mller, and Woj-ciech Samek. Layer-wise relevance propagation for neural networks with local renormalizationlayers. In Artificial Neural Networks and Machine LearningICANN 2016: 25th InternationalConference on Artificial Neural Networks, Barcelona, Spain, September 6-9, 2016, Proceedings,Part II 25, pages 6371. Springer, 2016. Tang Li, Fengchun Qiao, Mengmeng Ma, and Xi Peng. Are data-driven explanations robustagainst out-of-distribution data? In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 38213831, 2023.",
  "Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridgeuniversity press, 2004": "Mengmeng Ma, Tang Li, and Xi Peng. Beyond the federation: Topology-aware federatedlearning for generalization to unseen clients. In Proceedings of the International Conference onMachine Learning (ICML), 2024. Yuting Gao, Jinfeng Liu, Zihan Xu, Jun Zhang, Ke Li, Rongrong Ji, and Chunhua Shen.Pyramidclip: Hierarchical feature alignment for vision-language model pretraining. Advancesin neural information processing systems, 35:3595935970, 2022.",
  "Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages15211528. IEEE, 2011": "Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu,and Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-imagepre-training paradigm. In International Conference on Learning Representations, 2022. Y. L. et al. Filip: Fine-grained interactive language-image pre-training. ICLR, 2022. Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from fewtraining examples: An incremental bayesian approach tested on 101 object categories. In 2004conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004.",
  "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012": "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminativecomponents with random forests. In Computer VisionECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 446461. Springer,2014. Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conferenceon computer vision and pattern recognition, pages 34853492. IEEE, 2010. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer visionworkshops, pages 554561, 2013. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.Describing textures in the wild. In Proceedings of the IEEE conference on computer vision andpattern recognition, pages 36063613, 2014.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.2009": "Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptionsto visual denotations: New similarity metrics for semantic inference over event descriptions.Transactions of the Association for Computational Linguistics, 2:6778, 2014. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, PiotrDollr, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part V 13, pages 740755. Springer, 2014.",
  "R. S. et al. Grad-cam: Visual explanations from deep networks via gradient-based localization.ICCV, 2017": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-HsuanSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learningwith noisy text supervision. In International conference on machine learning, pages 49044916.PMLR, 2021. Karsten Roth, Jae Myung Kim, A Koepke, Oriol Vinyals, Cordelia Schmid, and Zeynep Akata.Waffling around for performance: Visual classification with random words and broad concepts.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1574615757, 2023.",
  "Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in neural information processing systems, 30, 2017": "Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al.Interpretability beyond feature attribution: Quantitative testing with concept activation vectors(tcav). In International conference on machine learning, pages 26682677. PMLR, 2018. Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas Wie-gand, Wojciech Samek, and Sebastian Lapuschkin.From\" where\" to\" what\": Towardshuman-understandable explanations through concept relevance propagation. arXiv preprintarXiv:2206.03208, 2022. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim,and Percy Liang. Concept bottleneck models. In International conference on machine learning,pages 53385348. PMLR, 2020. Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su.This looks like that: deep learning for interpretable image recognition. Advances in neuralinformation processing systems, 32, 2019.",
  "Tang Li, Jing Gao, and Xi Peng. Deep learning for spatiotemporal modeling of urbanization.Advances in Neural Information Processing Systems Workshops (Best Paper Award), 2021": "S. S. et al. K-lite: Learning transferable visual models with external knowledge. NeurIPS, 2022. Christian M Meyer and Iryna Gurevych. Wiktionary: A new rival for expert-built lexicons?Exploring the possibilities of collaborative lexicography. na, 2012. Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang, Xinfeng Zhang, Weijie Chen, ZengZhao, Zhou Zhao, Tangjie Lv, Zhipeng Hu, et al. Structure-clip: Towards scene graph knowledgeto enhance multi-modal structured representations. In Proceedings of the AAAI Conference onArtificial Intelligence, volume 38, pages 24172425, 2024. Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Smil:Multimodal learning with severely missing modality. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 35, pages 23022310, 2021. Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, and Xi Peng. Are multimodaltransformers robust to missing modality? In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1817718186, 2022.",
  "Our Prompt to Obtain Structured Rationales": "American Robin = {\"nodes\": [{\"id\": \"American Robin\", \"label\": \"American Robin\"},{\"id\": \"Breast\", \"label\": \"Breast\"},{\"id\": \"Tail\", \"label\": \"Tail\"},{\"id\": \"Beak\", \"label\": \"Beak\"},{\"id\": \"Eyes\", \"label\": \"Eyes\"},{\"id\": \"Red\", \"label\": \"Red\"},{\"id\": \"Gray\", \"label\": \"Gray\"},{\"id\": \"Yellow\", \"label\": \"Yellow\"},{\"id\": \"Round\", \"label\": \"Round\"},{\"id\": \"Long\", \"label\": \"Long\"}],\"edges\": [{\"source\": \"American Robin\", \"target\": \"Breast\", \"relation\": \"has\"},{\"source\": \"American Robin\", \"target\": \"Tail\", \"relation\": \"has\"},{\"source\": \"American Robin\", \"target\": \"Beak\", \"relation\": \"has\"},{\"source\": \"American Robin\", \"target\": \"Eyes\", \"relation\": \"has\"},{\"source\": \"Breast\", \"target\": \"Red\", \"relation\": \"is\"},{\"source\": \"Tail\", \"target\": \"Gray\", \"relation\": \"is\"},{\"source\": \"Beak\", \"target\": \"Yellow\", \"relation\": \"is\"},{\"source\": \"Eyes\", \"target\": \"Round\", \"relation\": \"are\"},{\"source\": \"Tail\", \"target\": \"Long\", \"relation\": \"is\"}]} Airliner = {\"nodes\": [{\"id\": \"Airliner\", \"label\": \"Airliner\"},{\"id\": \"Wings\", \"label\": \"Wings\"},{\"id\": \"Tail\", \"label\": \"Tail\"},{\"id\": \"Fuselage\", \"label\": \"Fuselage\"},{\"id\": \"Engines\", \"label\": \"Engines\"},{\"id\": \"Windows\", \"label\": \"Windows\"},{\"id\": \"Logo\", \"label\": \"Logo\"},{\"id\": \"Large\", \"label\": \"Large\"},{\"id\": \"Horizontal stabilizer\", \"label\": \"Horizontal stabilizer\"},{\"id\": \"Cylindrical\", \"label\": \"Cylindrical\"},{\"id\": \"Under wings\", \"label\": \"Under wings\"},{\"id\": \"Rowed\", \"label\": \"Rowed\"},{\"id\": \"Tail fin\", \"label\": \"Tail fin\"}],\"edges\": [{\"source\": \"Airliner\", \"target\": \"Wings\", \"relation\": \"has\"},{\"source\": \"Airliner\", \"target\": \"Tail\", \"relation\": \"has\"},{\"source\": \"Airliner\", \"target\": \"Fuselage\", \"relation\": \"has\"},{\"source\": \"Airliner\", \"target\": \"Engines\", \"relation\": \"has\"},{\"source\": \"Airliner\", \"target\": \"Windows\", \"relation\": \"has\"},{\"source\": \"Airliner\", \"target\": \"Logo\", \"relation\": \"has\"},{\"source\": \"Wings\", \"target\": \"Large\", \"relation\": \"are\"},{\"source\": \"Tail\", \"target\": \"Horiz. stabilizer\", \"relation\": \"has\"}, {\"source\": \"Fuselage\", \"target\": \"Cylindrical\", \"relation\": \"is\"},{\"source\": \"Engines\", \"target\": \"Under wings\", \"relation\": \"are\"},{\"source\": \"Windows\", \"target\": \"Rowed\", \"relation\": \"are\"},{\"source\": \"Tail\", \"target\": \"Tail fin\", \"relation\": \"has\"}]} What are useful visual concepts for distinguishing a {category_name}in a photo? These features should be visually distinctable and havelimited overlap with each other. These features should includeattributes and their relations. For each item, you should be conciseand precise, and use no more than five words. No ambiguous answers.Show your answer using a tree structure in JSON format strictlyfollowing the examples shown above. Only contains two depths ofnodes (depth 1: attributes, depth 2: subattributes). No connectionsbetween node with the same depth. Do not contain a node without anedge connected to it. No other explanations, only provide the graph.",
  "CFull Table": ": Evaluation on PartImageNet with ground truth region of parts using ViT-B/32 visionencoder. We summarize the annotated parts for different categories into 13 common parts. We applythresholds to the explanation heatmaps and calculate their mIoU with ground truth masks. Our modelimproves the localization accuracy of each part, even though they appear significantly different acrosscategories, such as wings for birds and airliners.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [Yes]Justification: The paper provides an Anonymous GitHub link with open access to boththe data and code used in the experiments, complete with detailed instructions in thesupplemental material that enable faithful reproduction of the main experimental results.This includes exact commands, necessary environment details, and scripts for preprocessingdata, ensuring that other researchers can replicate the studys findings without ambiguity.Guidelines:",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: The paper details all aspects of the experimental settings, including data splits,hyperparameter selection processes, and the types of optimizers used.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}