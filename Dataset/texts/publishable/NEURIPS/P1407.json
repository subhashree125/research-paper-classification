{
  "Abstract": "The training of score-based diffusion models (SDMs) is based on score matching.The challenge of score matching is that it includes a computationally expensive Ja-cobian trace. While several methods have been proposed to avoid this computation,each has drawbacks, such as instability during training and approximating the learn-ing as learning a denoising vector field rather than a true score. We propose a novelscore matching variant, local curvature smoothing with Steins identity (LCSS).The LCSS bypasses the Jacobian trace by applying Steins identity, enabling regu-larization effectiveness and efficient computation. We show that LCSS surpassesexisting methods in sample generation performance and matches the performanceof denoising score matching, widely adopted by most SDMs, in evaluations suchas FID, Inception score, and bits per dimension. Furthermore, we show that LCSSenables realistic image generation even at a high resolution of 1024 1024.",
  "Introduction": "Score-based diffusion models (SDMs) have emerged as powerful generative modelsthat have achieved remarkable results in various fields . While likelihood-based modelslearn the density of observed (i.e., training) data as points , SDMs learn thegradient of logarithm density called the score a vector field pointing toward increasing data density.The sample generation process of SDMs has two steps: 1) learning the score for a given dataset and2) generating samples by guiding a random noise vector toward high-density regions based on thelearned score using stochastic differential equation (SDE). Score matching used for learning the score includes a computationally expensive Jacobian trace,making it challenging to apply to high-dimensional data. While some methods have been proposedto avoid computing the Jacobian trace, each has its drawbacks. Denoising score matching (DSM), ubiquitously employed in SDMs, learns not the ground truth score but its approximation andimposes constraints on the design of SDE. On the other hand, sliced score matching (SSM) andits variant, finite-difference SSM (FD-SSM) , suffer from high variance due to using randomprojection. In this paper, we propose a novel score matching variant, local curvature smoothing with Steinsidentity (LCSS). The key idea of LCSS is to use Steins identity to bypass the expensive computationof Jacobian trace. To apply Steins identity, we take the expectation over a Gaussian distribution",
  "arXiv:2412.03962v2 [cs.LG] 6 Dec 2024": "centered on input data points, which is indeed equivalent to the regularization with local curvaturesmoothing. Exploiting this equivalence, we propose a score matching method that offers bothregularization benefits and faster computation. We first establish a method as an independent score matching technique, then propose a time-conditional version for its application to SDMs. We present the experimental results using syntheticdata and several popular datasets. Our proposed method is highly efficient compared to existing scorematching methods and enables the generation of high-resolution images with a size of 1024. Weshow that LCSS outperforms SSM, FD-SSM, and DSM in the quality of generated images and iscomparable to DSM in the qualitative evaluation of the FID score, Inception score, and the negativelikelihood measured in bits per dimension. While DSM requires the drift and diffusion coefficientsof an SDE to be affine, our LCSS has no such constraint, allowing for a more flexible SDE design(Sec. 2.4). Hence, this paper contributes to opening up new directions in SDMs research based onmore flexible SDEs. Related works.Liu et al. proposed a method for directly estimating scores using Steinsidentity without using score matching. Shi et al. further enhanced that method by applyingspectral decomposition to the function in Steins identity. However, Song et al. reported thatthese methods underperform compared to SSM. In our approach, we use Steins identity specificallyto avoid computing the Jacobian trace in score matching. The regularization effect attained by addingnoise to data has been recognized for a long time , which our method utilizes. The relationshipbetween noise-adding regularization and curvature smoothing in the least square function is elucidatedin Bishop . The previous studies of score matching variants are described in the next section. Inefforts to remove the affine constraint of SDE in SDMs, Kim et al. proposed running SDE in thelatent space of normalizing flows. This constraint stems from using DSM for score matching, and wepropose a score matching method free from such constraint.",
  "dxt = f(xt, t)dt + g(t)dwt,(1)": ": Samples generated from models trained on CelebA-HQ (1024 1024) using our proposedscore matching method, LCSS. The rightmost images in each row are generated by DDPM++ withsubVP SDE, while the rest are by NCSN++ with VE SDE. where f(, t) : Rd Rd is the drift coefficient, g(t) R is the diffusion coefficient, and wt denotesa standard Wiener process. Eq. (1), known as the forward process, has a corresponding reverseprocess from time T to 0 :",
  "dxt =f(xt, t) g(t)2x log pt(xt)dt + g(t)d wt(2)": "where wt is a standard Wiener process in reverse-time and pt(xt) denotes the ground truth marginaldensity of xt following the forward process. Samples from a dataset are represented as x0 p0,while initial vectors for sample generation with Eq. (2) are xT pT . In Eq. (2), the only unknownterm is x log pt(xt), referred to as the score of density pt(xt). To estimate x log pt(xt), SDMstrain a score network s parametrized by by score matching.",
  "Score matching": "A score network s that estimates the score of the ground truth density is trained through scorematching .Score matching, a technique independent of SDMs and SDE, has no conceptof time.So, as long as our discussion is focused on score matching, we use the notationof x and p, without the subscript of t, and treat a score network without conditioning on t,i.e., denote it as s(x) instead of s(xt, t). Score matching is defined as the minimization ofJ () := 1 2 Exp s(x) x log p(x)22. Calculating J () is generally impractical since it re-quires knowing the ground truth x log p(x), but Hyvrinen has shown that J () is equivalentto the following JSM() up to constant:",
  "dt.(5)": "The weight function (t) is determined by the form of the SDE, and (t) used for typical SDEs canbe found in in . The pt is obtained from the SDE in Eq. (1), with its mean dependent onx0, and its specific form in typical SDEs is given as Eq. (29) in Song et al. . The problem is thatsince s(xt, t) has the same dimension as input xt, computing its Jacobian trace, Tr(xs(xt, t)), iscostly. It renders training with score matching impractical in high-dimensional data.",
  "(7)": "The drawback of these two methods is the high variance induced by random projection with v. Inparticular, the error between the true trace of matrix A, Tr(A), and the estimate by Hutchinsonstrick, TA, is | Tr(A) TA| 1 M AF where F is the Frobenius norm and M is the samplingtimes from pv . Typically, M = 1 setting is employed in these methods, potentially making theerror magnitude non-negligible and causing instability in training process, as we see in Sec 4.2.3. Denoising score matching (DSM).DSM circumvents the computation of Tr(xs(x)) byperturbing x with a Gaussian noise distribution q(x|x) with noise scale and then estimating thescore of the perturbed distribution q(x) :=q(x|x)p(x)dx. The DSM minimizes",
  ",(9)": "where t is designed to increase as t progresses from 0 to T. Almost all SDMs use DSM for scorematching because it performs faster and is more stable than SSM and FD-SSM. However, DSM hasthree drawbacks. 1) Approximation: in DSM, s learns x log qt(xt|x0) rather than the groundtrue score, x log pt(xt). 2) Constraining the design of SDE: DSM constrains SDE coefficients tobe affine. We will describe this in Sec. 2.4.",
  "DSM restricts SDE to affine": "The design of SDEs directly influences the performance of SDMs, as demonstrated in previousstudies . The benefits of non-linear SDE, particularly highlighted in , enable more accuratealignment of scores with the ground-truth data distributions than affine SDE and thus enhance thequality of generated samples. ( in illustrates this.) However, unless specific modificationsare made as proposed in these studies, the general SDEs used in almost all existing SDMs mustbe affine. This constraint comes from the fact that the SDMs, consciously or unconsciously, selectDSM for their score matching methods. The loss function of DSM requires x log qt(xt|x0) asEq. (9). Thus, to compute Eq. (9) at every training iteration, x log qt(xt|x0) needs to be in closedform. DSM models qt(xt|x0) as a Gaussian distribution, for which this requirement is satisfiedas x log qt(xt|x0) = x0xt 2t. However, this Gaussian modeling comes at the cost of imposing aconstraint on the SDE design: the drift and diffusion terms of SDE, i.e., f(xt, t) and g(t) in Eq. (1),need to be affine. The existing SDMs are DSM-based, so the SDEs used in these SDMs, includingthe VE SDE and subVP SDE we use in our experiments, are designed to adhere to this constraint.The details of the same discussion and the specific form of the Gaussian distribution qt(xt|x0) forthe typical SDEs can be found in Sec. 3.3 in Song et al. . Unlike DSM, SSM and FD-SSM do nothave this limitation, allowing for more flexible SDE design and thus removing the requirement tolimit the forward processs convergence destination to Gaussian distributions. Unfortunately, as wewill see later, SSM and FD-SSM cannot handle high-dimensional data due to the high-variance theycause. Our proposed method uniquely satisfies both the flexible design of SDEs and compatibilitywith high-dimensional data.",
  "Our Method": "We propose a novel score matching variant that avoids the expensive computation of the Jacobiantrace. The crux of our method is using Steins identity to bypass Jacobian computation. Our approachcomprises three steps: 1) introducing local curvature smoothing regularization into score matching(Definition 1), 2) treating the regularization of 1) as taking an expectation over a Gaussian distribution(Lemma 1), and 3) applying Steins identity (Corollary 2). While introducing regularization mayappear to cause extra computational costs, it enables faster computation by the use of Steins identitytrick. We begin by discussing our method separately from SDMs, without involving the time variablet, and then explain its incorporation into SDMs at the end of this section.",
  "xs(x)2F .(10)": "Given xs(x) approximating the Hessian of log p(x), minimizing the regularization term122 xs(x)2F acts as a local curvature smoothing where the square of the curvature of thesurface of the log-density at x are penalized. Curvature smoothing is one of the commonly employedregularizations in machine learning .Lemma 1 (Kingma and LeCun ). Score matching with local curvature smoothing (Definition 1)is equivalent to the expectation of J sSM(, x) over a Gaussian distribution centered at x, i.e., x N(x, 2Id):",
  "where := x x2": "Lemma 1 states that taking the expectation of score matching objective with respect to a Gaussiandistribution centered around x yields an effect equivalent to a curvature smoothing regularization.Definition 2 (Stein class ). Assume that Q(z) is a continuous differentiable probability densitysupported on Z Rd. Then, a function f : Z R is the Stein class of Q if f satisfies",
  "limz f(z)Q(z) = 0.(13)": "Lemma 2 (Steins identity, Liu et al. , Gorham and Mackey ). Let h : Z Rd be a smooth(i.e., continuous and differentiable) vector valued function h(z) = [h1(z), h2(z), . . . hd(z)]T . Then ,if hi(z)i = 1, . . . , d is the Stein class of a smooth density Q(z), the following identity holds:",
  "= EzQ [zhi(z)] .(15)": "Eq. (15) holds for the i-th element of the vector h. The condition Eq. (13) holds for Gaussiandistribution Q, since Q(z) 0 as z . Then, hi(z)i = 1, . . . , d are the Stein class of Q, andthus Lemma 2 is valid for a Gaussian distribution Q. As we also know z log Q(z) = 1 2 (z ),by substituting it into Lemma 2, we obtain Corollary 1.Corollary 2 (Bypassing Jacobian trace computation). Let x Rd1, s(x) Rd1, and Q(x) =N(x; x, 2Id). With Corollary 1 and a few assumptions, we have the following:",
  ".(16)": "The s, which represents a score network in our context, corresponds to h in Lemma 2 and Corollary 1.The derivation of Eq. (16) is presented in Appendix A, in which we assume the interchangeabilitybetween the expectation and summation regarding s(x). Objective function of LCSS.We propose a variant of score matching method, local curvaturesmoothing with Steins identity (LCSS). The development of the objective function of LCSS, J sLCSS,begins with the curvature smoothing regularization of Eq. (10), followed by the application ofLemma 1 and Corollary 2. Since J sLCS(, x, ) in Eq. (10) involves computationally expensivexs(x), alongside the original challenge of Tr(xs(x)) in J sSM, training with J sLCS is impracticalfor high-dimensional data. However, by inserting the transformation of Lemma 1, it enables theapplication of Corollary 2 to J sLCS. By substituting Eq. (4) into Eq. (11) and ignoring O(2), we have",
  ", which is computedefficiently, thereby bypassing the issue of high computational cost": "Comparing LCSS with existing score matching methods.Unlike SSM and FD-SSM, LCSS doesnot use random projection, eliminating the high variance issue. While DSM learns the approximationof ground truth score x log q(x|x), LCSS learns the ground truth score x log p(x). Furthermore,unlike DSM, J sLCSS does not require x log q(x|x), thus eliminating the need for affine restrictionson the SDE coefficients. The original score matching, i.e., minimizing J sSM, involves the followingtwo: (1) Increasing the first term Tr(xs(x)) x x log p(x), the divergence of the score,in the negative direction promotes s to learn the vector field flowing into points where x exists.(2) Minimizing the second term 1 2 s(x)22 promotes s to learn that its length approaches 0 atpoints where x exists. The LCSS also performs both (1) and (2), but instead of at a single point x,it considers a Gaussian cloud centered around x. By applying Steins identity, LCSS bypasses thechallenge of (1), thereby making score matching feasible even for high-dimensional data.",
  "(t) Ex0p0 [J sLCSS(, x0, t, t)] dt.(20)": "We replace in Eq. (18) with a time-varying t. By making t take on a wide range of valuesdepending on t, we aim to facilitate robust learning of score vectors even in low-density regions inp0, mirroring the original motivation of NCSN . With Eq. (19), s learns a vector in the directionof (xt x0) to minimize the inner product of the first term, weighted by12t , while minimizingits L2 norm, s(xt, t)2. Sampling xt in the expectation in Eq. (19) only once yields satisfactoryperformance, as evidenced by our experimentation. SDEs for LCSS-based SDMs can be designed flexibly without restricting the drift and diffusioncoefficients to be affine. However, devising a new SDE is beyond the scope of this paper and is leftfor future work, and our experiments use existing SDEs designed for use with DSM: the VarianceExploding (VE) SDE and the sub Variance Preserving (subVP) SDE . Taking advantage of thefact that pt is a Gaussian distribution in both SDEs, we employ the standard deviation of pt as thevalue ot t in each SDE in our experiments. For example, for VE SDE, t = g(t). For both SDEs,t increases as t goes from 0 to T, but the way it increases is different for each SDE. Following Song and Ermon , we set (t) = g(t)2. With this setting, (t) becomes (t) =g(t)2 = 2t , effectively cancelling out 2t in the denominator of Eq. (19) and avoiding unstablesituations where the denominator could become zero. For other SDE types (VP and sub VP), (t) ismore elaborate but similarly cancels out 2t in the denominator. For fairness, we note that, similarly,in training SDMs with DSM, applying the coefficient (t) = g(t)2 allows for the cancellation of 2tin the denominator, thus circumventing the weakness of DSM.",
  "Setup": "We use five SDMs: NCSNv2 1 as a discrete-time model, NCSN++ and DDPM++ and theirextensive version, NCSN++ deep and DDPM++ deep, as continuous-time models. Only for asynthetic dataset, Checkerboard, we use a multilayer perceptron (MLP) with publicly available code2based on Song and Ermon . In continuous-time models, we use VE SDE for NCSN++ andNCSN++ deep and subVP SDE for DDPM++ and DDPM++ deep as per Song et al. . The sameSDEs are applied to all the score matching methods we evaluate, including our LCSS. We use theofficial codes from the original papers, and the hyperparameters are kept as in the official code, unlessstated otherwise. 3 For LCSS, we perform only one sampling iteration to calculate the expectation inJ sLCSS (Eq. (19)). We set t = g(t) in each SDEs. All experiments are performed on a server with128 GB RAM, 32 Intel Xeon, Silver 4316 CPUs, and eight NVIDIA A100 SXM GPUs.",
  "Density estimation": "We first compare LCSS to SSM, FD-SSM, and DSM in score matching performance. We visualizeestimated densities on Checkerboard dataset, whose density is multi-modal. The details of theexperiments, including the training loss curve, are presented in Appendix B. depicts thedensity distribution learned by the model. Compared to SSM and FD-SSM, LCSS demonstrateshigher accuracy in density estimation with faster convergence and stability in loss reduction. DSMexhibits similar accuracy in density estimation and stability in loss reduction to LCSS. However,LCSS shows slightly better consistency in estimating high-density regions (bright-colored areas) andmaintains stable loss.",
  "Training efficiency": "We compare LCSS with the existing score matching methods for training efficiency. We measure thetime taken for model training on Checkerboard and FFHQ dataset resized to 256 256. shows the average elapsed time over 100 epochs for Checkerboard and 1000 iterations for FFHQ,respectively. It shows that LCSS is the most efficient.",
  "SSMFD-SSMLCSS (ours)": ": Comparison of generated samples on CelebA (64 64). The left three show samples frommodels trained for 10k steps. In the right three, FD-SSM and LCSS images are from models trainedfor 210k steps, whereas SSM images are from a model trained for 60k steps. Comparison with SSM and FD-SSM.We first focus on comparing LCSS with SSM and FDSSM.4We generate samples using NCSNv2 on CIFAR-10 (32 32) , CelebA (64 64) , andFFHQ (256 256) . The results show that LCSS demonstrates stable long-term training andfaster convergence compared to the other two methods. This can be explained by LCSS not usingrandom projection, unlike SSM and FD-SSM. Details are provided below. On CIFAR-10 (32 32), unlike LCSS, SSM and FD-SSM, when reaching 95k and 495k trainingsteps, respectively, are unable to continue generating meaningful images and produce only entirelyblack images. displays generated images at 5k and 90k training steps for each method. Thefaster convergence of LCSS compared to SSM and FD-SSM is exhibited from the differences in theimage quality. On CelebA (64 64), (left) displays images generated by each method at 10ksteps, highlighting LCSSs faster learning. (right) presents the generated images of LCSS andFD-SSM at the 210k training steps. For SSM, after 65k training steps, it only generated completelyblack images, so the displayed SSM images are from the model trained for 60k iteration. On FFHQ(256 256), LCSS can generate decent images, while SSM and FD-SSM failed, as shown in Comparison with DSM.In the previous experiments, we saw that LCSS significantly outperformsSSM and FD-SSM in image generation. In this subsection, we compare LCSS with DSM, widelyadopted as the objective function in score-based diffusion models. The results show that LCSSsurpasses DSM in qualitative evaluation, and achieves performance on par with DSM in quantitativeevaluation on CIFAR-10 using Frchet inception distance (FID), Inception score (IS), and negativelog likelihood measured in bits per dimension (BPD). The details are below.",
  ": Samples on FFHQ + AFHQ": "We compare generated samples on FFHQ, AFHQ, andFFHQ + AFHQ. The size of images in the three datasets is(256256), and we train NCSNv2 for 600k with batch size16 on each of them. On FFHQ, LCSS can generate morerealistic images than DSM, as shown in . We notethat during the training with DSM, around 210k trainingsteps, a sharp decline in the quality of generated imageswas observed. On AFHQ , shows that LCSSgenerates realistic samples, but DSM does not. We alsocreate and examine with a dataset FFHQ + AFHQ, a fusion of FFHQ and AFHQ, designed to increaselearning difficulty by diversifying data modalities. On FFHQ + AFHQ, shows LCSSs superiorcapability in generating realistic images over DSM. 4Although FD-DSM has also been proposed, it was excluded from comparative evaluation due to reportedperformance below DSM in Pang et al. and failure to generate images appropriately in our experiments.",
  "DSM4.459.863.624.299.863.384.819.622.644.499.582.64LCSS (ours)4.909.884.174.729.953.615.069.632.474.619.802.58": "shows the qualitative results on CIFAR-10. Regardless of SDMs, LCSS tends to surpassDSM in IS but underperform in FID. Compared to the values in Song et al. , our experimentalresults generally exhibit higher (better) IS values and higher (worse) FID values.5 In BPD, LCSSsurpasses DSM in DDPM++ variants but underperforms in NCSN++ variants. Overall, qualitativeevaluation on CIFAR-10 suggests no decisive superiority between LCSS and DSM, hinting at distinctcharacteristics. Summary. illustrates a highly simplified comparison of the relative performance betweenLCSS and DSM. Model training is more complex in Case #2 than in Case #1. It was observed thatin challenging conditions like Case #2, DSM suffered from performance degradation. We regularlymonitored the quality of generated images during model training. In the experiments of Case #2 withDSM, as noted above, although the quality of generated images was improving up to a certain stage(around 210k iterations, for example), it suddenly deteriorated. Also, frequent spikes in loss valueswere observed during training with DSM, which appeared to be a trigger for the deterioration. UnlikeDSM, LCSS retained superior performance without suffering from such instability.",
  "High resolution image generation": "We demonstrate that learning with LCSS enables models to generate high-resolution images. Wetrain NCSN++ and DDPM++ on CelebA HQ (1024 1024) , using hyperparameters consistentwith those used to train DSM-based models in Song et al. . In , we show generated images:NCSN++ images are from the model trained for around 1.3M iterations, and DDPM++ ones aretrained for around 0.3M iterations, with batch size 16 for both. The figures show that LCSS ispromising as a score matching method. More generated samples are presented in Appendix C.2.",
  ",(21)": "we train NCSNv2 on FFHQ. Images generated from the models trained with different are shown in. When = 0.5, only noisy images akin to those at time t = T, xT , are produced. With < 1,the force to minimize the second term, s(xt, t)22, is more emphasized than when using the originalJ sLCSS, leading to shorter score vector lengths. The score vector length is directly linked to the spatialmovement distance of xt during the reverse process for sample generation. Since the score vector is",
  ": Generated samples on FFHQ (256 256) by the model trained with LCSS (ours) withdifferent . The notation iter signifies the training iterations": "forced to be short, the noise xT generated at the start of the sample generation process cannot reachthe regions corresponding to realistic images with high density as it traces back from time T to 0. Onthe other hand, when > 1, particularly for = 10, it is observed that as the number of trainingiterations increased, images with emphasized contours but lost textures are generated. It suggeststhat the involvement of the first term of J sLCSS in contour formation. The object contours in an imageare characterized by rapid changes in pixel values, which can be associated with high curvature orchanges in second-order derivatives. Since s(xt, t)T xtxt2in Eq. (21) or (19) corresponds to theHessian trace of log-density, this observation can be interpreted as natural. It is implied that the firstand second terms in the loss function of LCSS are dedicated to the formation of contours and texture,respectively.",
  "Conclusion": "Limitation.While LCSS, unlike DSM, can design SDEs flexibly without restricting them to affineforms, we used existing affine SDEs designed for use together with DSM, i.e., VE SDE and subVPSDE, in this work. Proposing more flexible SDEs leveraging LCSS is left for future work. We proposed a local curvature smoothing with Steins identity (LCSS), a regularized score matchingmethod expressed in a simple form, enabling fast computation. We demonstrated LCSSs effectivenessin training on high-dimensional data and showed that LCSS-based SDMs enable high-resolutionimage generation. Currently, SDMs primarily rely on DSM, constraining the design of SDE. LCSSoffers an alternative to DSM, opening avenues for SDM research based on more flexible SDEs.",
  "Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent componentsestimation. In International Conference on Learning Representations, 2015": "Jackson Gorham and Lester Mackey. Measuring sample quality with stein's method. In C. Cortes,N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural InformationProcessing Systems, volume 28. Curran Associates, Inc., 2015. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. InH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in NeuralInformation Processing Systems, volume 33, pages 68406851. Curran Associates, Inc., 2020. M.F. Hutchinson. A stochastic estimator of the trace of the influence matrix for laplaciansmoothing splines. Communications in Statistics - Simulation and Computation, 18(3):10591076, 1989. doi: 10.1080/03610918908812806.",
  "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs forimproved quality, stability, and variation. In International Conference on Learning Representa-tions, 2018": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for genera-tive adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 43964405, 2019. doi: 10.1109/CVPR.2019.00453. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space ofdiffusion-based generative models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35,pages 2656526577. Curran Associates, Inc., 2022. Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon.Maximum likelihood training of implicit nonlinear diffusion model. In S. Koyejo, S. Mo-hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural InformationProcessing Systems, volume 35, pages 3227032284. Curran Associates, Inc., 2022.",
  "Yingzhen Li and Richard E. Turner. Gradient estimators for implicit models. In InternationalConference on Learning Representations, 2018": "Jae Hyun Lim, Aaron Courville, Christopher Pal, and Chin-Wei Huang. Ar-dae: towards unbi-ased neural entropy gradient estimation. In Proceedings of the 37th International Conferenceon Machine Learning, ICML20. JMLR.org, 2020. Qiang Liu, Jason Lee, and Michael Jordan. A kernelized stein discrepancy for goodness-of-fittests. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rdInternational Conference on Machine Learning, volume 48 of Proceedings of Machine LearningResearch, pages 276284, New York, New York, USA, 2022 Jun 2016. PMLR.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchicaltext-conditional image generation with clip latents, 2022": "Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Pro-ceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedingsof Machine Learning Research, pages 15301538, Lille, France, 0709 Jul 2015. PMLR. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, JonathanHo, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models withdeep language understanding. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages3647936494. Curran Associates, Inc., 2022. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. PixelCNN++: Improving thepixelCNN with discretized logistic mixture likelihood and other modifications. In InternationalConference on Learning Representations, 2017. Jiaxin Shi, Shengyang Sun, and Jun Zhu. A spectral approach to gradient estimation for implicitdistributions. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th InternationalConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,pages 46444653. PMLR, 1015 Jul 2018. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsu-pervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei,editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 ofProceedings of Machine Learning Research, pages 22562265, Lille, France, 0709 Jul 2015.PMLR. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the datadistribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, andR. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. CurranAssociates, Inc., 2019. Yang Song and Stefano Ermon. Improved techniques for training score-based generative models.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in NeuralInformation Processing Systems, volume 33, pages 1243812448. Curran Associates, Inc.,2020. Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalableapproach to density and score estimation. In Proceedings of the Thirty-Fifth Conference onUncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, page 204,2019. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training ofscore-based diffusion models. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, andJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,pages 14151428. Curran Associates, Inc., 2021. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, andBen Poole. Score-based generative modeling through stochastic differential equations. InInternational Conference on Learning Representations, 2021. Charles Stein. A bound for the error in the normal approximation to the distribution of a sum ofdependent random variables. In Proceedings of the sixth Berkeley symposium on mathematicalstatistics and probability, volume 2: Probability theory, volume 6, pages 583603. Universityof California Press, 1972. Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressivedensity-estimator. In Advances in Neural Information Processing Systems, volume 26, pages21752183. Curran Associates, Inc., 2013. Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, andAlex Graves. Conditional image generation with pixelcnn decoders. In Advances in NeuralInformation Processing Systems, volume 29, pages 47904798. Curran Associates, Inc., 2016.",
  "i=1ExN (x,2Id) [si(x)(xi xi)] .(23)": "The interchangeability holds when a (score) function s is integrable and differentiable. As sis implemented by neural networks in our case, we assume these conditions are met. We notedthat if there are correlations between the dimensions of x over which expectations are taken,interchangeability does not always hold. However, because x is a sample from a diagonal Gaussian,x N(x, 2Id), there are no correlations between dimensions, thus fulfilling this condition.",
  "BExperiments on Checkerboard": "We describe the setup for the experiments on Checkerboard dataset. We use the publicly availablecode6. The generation of the Checkerboard dataset can be found, for example, in Appendix D.1 inLai et al. . In the data space X R2, we train a function f : X R parameterized by to",
  "2xf(xt) + z(31)": "where z N(0, I2) and = 0.1. The function f is implemented as a simple multilayer perceptron(MLP) with two hidden layers, each with 300 units, which is the same architecture as the one usedin Song and Ermon . The number of Langevin dynamics steps is 1000, with the initial vector,x1000, being randomly sampled from a uniform distribution. We use stochastic gradient descent witha batch size of 10k, a learning rate of 1e 3, and train for 200 epochs. shows the samplingresults of 250M points. The loss curve during training on Checkerboard is shown in ."
}