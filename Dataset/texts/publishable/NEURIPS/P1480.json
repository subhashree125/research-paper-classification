{
  "Abstract": "We introduce a novel state-space architecture for diffusion models, effectivelyharnessing spatial and frequency information to enhance the inductive bias to-wards local features in input images for image generation tasks. While state-spacenetworks, including Mamba, a revolutionary advancement in recurrent neuralnetworks, typically scan input sequences from left to right, they face difficul-ties in designing effective scanning strategies, especially in the processing ofimage data. Our method demonstrates that integrating wavelet transformationinto Mamba enhances the local structure awareness of visual inputs and bettercaptures long-range relations of frequencies by disentangling them into waveletsubbands, representing both low- and high-frequency components. These wavelet-based outputs are then processed and seamlessly fused with the original Mambaoutputs through a cross-attention fusion layer, combining both spatial and fre-quency information to optimize the order awareness of state-space models whichis essential for the details and overall quality of image generation. Besides, weintroduce a globally-shared transformer to supercharge the performance of Mamba,harnessing its exceptional power to capture global relationships. Through extensiveexperiments on standard benchmarks, our method demonstrates superior resultscompared to DiT and DIFFUSSM, achieving faster training convergence and de-livering high-quality outputs. The codes and pretrained models are released at",
  "Introduction": "Diffusion models are a trending generative model technique that has gained significantattention in machine learning and computer vision. The core idea behind diffusion models is tolearn how to reverse the diffusion process by gradually transforming a simple initial distribution, likeGaussian noise, into a complex data distribution. The flexibility, robust performance, and high-qualityoutputs make them powerful tools for advancing the state-of-the-art in generative modeling, and largediffusion-based generators have revolutionized the field of image , video , and 3Dsynthesis . While diffusion models initially rely on UNet architectures, recent methodshave shifted gear to build upon transformer backbones. A line of works have shownthat transformer-based diffusion models are scalable and consistently offer higher generation quality",
  "arXiv:2411.04168v2 [cs.CV] 28 Dec 2024": "than the UNet-based counterparts. Even the most common open-source text-to-image tool, StableDiffusion, has switched to use transformers in their upcoming release . Hence, transformersare becoming the new backbone standard for diffusion models. The power of this structure lies inthe attention mechanism for capturing richer in-context relations. However, transformers have thedrawback of a costly quadratic complexity, which might hinder their feasibility for high-dimensionaldata. While transformers are taking over state-of-the-art diffusion backbones, a novel technique called state-space models (SSMs) has suddenly arrived, promising a better alternative. SSMs haverevolutionized the NLP field, favoring linear time complexity and excelling at long-context modeling.This type of network bears similarities to the recurrent process of RNNs while being capable of fullyoperating in parallel like convolutional networks. SSM promises to surpass transformers in mosttasks, prioritizing compute efficiency, such as long-sequence modeling. Mamba is a special typeof SSM that offers greater quality by introducing time conditioning and context dependency to thehidden states. In the context of computer vision, within a very short period, this architecture has beenused to address a variety of problems, including image perception , image restoration, and image generation . In diffusion-based image generation, Diffusion StateSpace models (DIFFUSSM) already surpass their transformer-based counterparts. Though showing many advantages, Mamba still has a critical weakness when processing 2D imagerydata. Like vision transformers, images are divided into patches and then mapped into tokens. Mambaprocesses these tokens following a specific scanning order, thus introducing an inductive bias about2D images into the model. Specifically, this order greatly impacts the interplay between imagetokens, thereby affecting model performance. This characteristic is unfavorable, particularly whentransformers have no such order-dependency issue. Many vision-based Mamba studies have focusedon solving this problem on proposed advanced scanning mechanisms like bi-directional , cross-scanning , or 8-directions zigzag . Despite improving performance, these scanningtechniques still fail to capture global and long-range relations and do not fully release Mambaspotential. In this paper, we enhance Mamba-based diffusion models, specifically focusing on image generation.Previous models failed to address the scanning order issue due to their exclusive reliance on spatialprocessing, overlooking crucial long-range relations that manifest in the frequency spectrum. Wesuggest a novel approach integrating frequency scanning with the conventional spatial scanningmechanism. Although initial work in Mamba has explored this combination for a limited task ofimage deraining , it lacked a comprehensive analysis of the effective integration of these features. Motivated by the above observation, this paper introduces DiMSUM, a novel architecture thatharnesses Mambas power to unlock diffusion models generation capabilities. Our approach enhancessensitivity to local structures and long-range dependencies by integrating wavelet transforms andspatial information. Using a query-swapped cross-attention technique, we dynamically synergizespatial and frequency information, accelerating convergence and improving image synthesis quality.Consequently, this boosts image quality and enhances the efficiency and scalability of the training. Additionally, we incorporate globally shared transformer blocks to address global context integration,a limitation of traditional Mamba models. The block can also be viewed as a token-mixing layer thatenriches global relations among image tokens, addressing the weak inductive bias of the manuallydefined scanning orders in the original Mamba. Hence, DiMSUM can maintain high performanceeven with larger, more complex datasets. Extensive experiments show that DiMSUM achievesstate-of-the-art FID scores and recall, setting a new benchmark in generative image modeling. In summary, our contributions lie three-fold: (1) A novel Mamba architecture for diffusion modelsthat leverages both spatial and frequency features to enhance the awareness of local structures withininput images, leading to better image generation. (2) We interleave globally shared transformer blocksper a certain number of Mamba blocks. The transformer with a strong capacity for capturing globalrelationships significantly boosts generation results when integrating with Mamba. The transformercan also be seen as an order-invariant mixing layer that complements Mambas loose assumptionsabout the order of 2D data. (3) Superior results across image generation benchmarks like ImageNet,CelebA-HQ, and LSUN Church. Additionally, our method maintains comparable GFLOPs andparameters with existing diffusion architectures while offering faster training convergence.",
  "State Space Models and Their Applications in Vision Tasks": "In control engineering and system identification, state space models (SSMs) are described by statevariables and first-order differential equations but initially underperformed in deep learning. Recentenhancements, notably by S4 through the use of a HiPPO initialization matrix, havesignificantly improved SSMs. Subsequent studies show that SSMs can matchtransformers in long-range sequence modeling with the added benefit of linear time and spacecomplexities. Notably, Mamba has advanced over transformers in NLP by using a time-varyingsystem with context-dependent parameters, enhancing the differentiation of hidden states over longsequences. This positions Mamba as a strong alternative to transformers across various domains. In computer vision, ViM and VMamba are the first works to introduce Mamba as a buildingblock in discriminative tasks. Sequentially, Mamba is explored in many computer vision taskssuch as medical imaging , point clouds , and image generation . Similarto vision transformers, images are divided into patches, and the patches are mapped into tokens.The tokens are then arranged in a sequence following a scanning order. In vision transformers,the scanning order does not matter since attention scores are computed between every token pair.However, SSMs consider the order information, introducing an inductive bias about 2D imagesinto the model. Therefore, scanning order is vital in setting vision models performance. ViM proposed a bidirectional scanning order (sweep-2) for discriminative tasks. VMamba proposedcross-scanning (sweep-4) per each Mamba building block. This cross-scan improves the modelperformance but costs enormous overhead. MambaND reduces that cost by introducing twomethods: interleaved scanning and multi-head scanning. Interleaved scanning, which alternates thescanning order in the sequential blocks, is simpler but gains better performance in discriminativetasks. Recently, Zigma proposed a zigzag-8 scanning order to preserve the locality property (i.e.,each token is adjacent to its next and previous tokens). The zigzag-8 scanning order shows fasterconvergence compared to the bidirectional one. In this paper, we show that too many scanning orders,e.g., sweep-8 and zigzag-8, may introduce excessive information and lead to worse performance thansweep-4. Instead, sweep-4 offers the best performance (.4).",
  "Diffusion Architecture": "Diffusion models are an emerging type of generative model that requires a sequentialdenoising process of several to thousands of steps to sample an image from initial Gaussian noise.Notably, most of them are based on Stochastic Differential Equations (SDE) that require an accu-mulation of additional stochastic noise at each generation step. Alternatively, there is a line of flowmatching methods that emphasize deterministic trajectories from pure Gaussian noiseto the target data distribution, favoring a straighter solution path. Their applications span acrossdifferent tasks like image super-resolution , depth estimation , and motion synthesis .Recent works have proved that diffusion models and flow matching are strongly correlatedand can be converted into each other. In this work, we only focus on the simple objective of flowmatching for our design. Meanwhile, most methods are originally based on Unet architecture, which utilizes convolutionresblock to capture local information at multiscale resolution. The attention layer is also used,interleaving between resblock layers to capture global information. Recently, the vision transformer has emerged and largely surpassed CNNs in many tasks. For diffusion image modeling,several transformer-based architectures have been recently introduced. Transformer-based architectures capture global information better than Unet ones and can generate higher-qualityimages. Specifically, UViT replaced convolution resblock layers with transformer blocks andremoved downsampling/upsampling blocks. DiT directly replaced Unet with a vision transformer.Inspired by this, MDT and MaskDiT introduced a mask latent modeling approach to bettercapture contextual information and enhance training efficiency. Although transformer architecturesachieve better image generation, these models suffer from quadratic time and memory complexity,slowing down training and inference processes. Recently, with the birth of Flash Attention ,both training and inference time of these transformer-based models are significantly reduced thanks to",
  "residual block is a skip-connection block that learns residual functions with respected to the layer inputs": "the reduced IO bottleneck. However, the computation time complexity remains quadratic. Recently,the S4 model has been introduced to effectively deal with long-range dependency in the NLPfield. Furthermore, the S4 model favors the linear complexity time and space, which is more efficientthan the transformer. Among S4 class models, Mamba stands out for its high performance incapturing long-range dependency. In diffusion models, DiffuSSM adopts S4D as a buildingblock for their model and achieves better FID compared to transformer counterparts. Recently,Zigma utilized Mamba for diffusion architecture, using a zigzag scanning order to preservelocality-aware scanning property. Despite showing promising results, Mamba-based diffusion modelsstill struggle to find an optimal scanning scheme to take advantage of the 2D inductive bias fromimages. We find these approaches stuck in spatial processing, thus failing to incorporate global andlocal relations. These relations can effectively captured in frequency spectrum, thus we propose toincorporate frequency scanning alongside the existing spatial scanning mechanism.",
  "Frequency-based methods": "Employing frequency components extracted by Fourier, Cosine, or Wavelet transform in solvingvision tasks was common in classical computer vision. Many modern works still find this practiceuseful in improving the performance of deep neural networks. In perception tasks, several works integrate frequency processing in transformer architecture. NomMer applied adiscrete cosine transformer into global attention to efficiently yield synergistic context from bothglobal and local contexts. To improve Masked Image Modeling, Ge-AE introduces an additionalfrequency decoder using Fourier transform to reconstruct the high-frequency information better.Wave-Vit applies wavelet into self-attention modules to reduce the time and space complexityof the transformer architecture while still preserving the performance. Recent work Simba introduced Fourier-based layers (EinFFT) in combination with Mamba block to replace MLP layersfor better channel mixing. To solve the image denoising problem, FreqMamba applied a waveletand Fourier transformer to process features injected into the Mamba block. In generative modeling,several works corporate wavelet frequencies into generative framework. By explicitlydecomposing features/images into high- and low-frequency bands through wavelet transform, thegenerative model can train stably and converge faster. Furthermore, the high frequencies can belearned more efficiently, leading to a sharper synthesis image. Observing the benefit of waveletprocessing in generative modeling, we apply discrete wavelet transform on local features beforefeeding into the Mamba layers. By using cross-attention to fuse wavelet frequency features andspatial features, our method achieves significant improvement in image synthesis compared to merelyspatial feature processing.",
  "Preliminary": "State Space Model (SSM). SSM is a new type of sequence model that uses an implicit hiddenstate h(t) RNL to map the 1D input signal x(t) RL to its corresponding output signaly(t) RL. This process is formulated by a parameter matrix A RNN and two projectionparameters B RN1 and C R1N:",
  "where Linear(.) is a projection layer to -dimensional vector, Softplus(.) = log(1 + exp(.)), andBroadcastL(.) means duplicating a single-value vector to L-dimensional vector": "Diffusion model. Diffusion models are also known as score-based models thatlearn the transitional trajectories from a Gaussian noise to signals in the target domain. Most methodsare based on Stochastic Differential Equation (SDE), requiring a larger number of function evaluationsto generate an image. Recently, Flow matching has proved to be a promising methodthat finds a deterministic mapping between input Gaussian noise and input data via solving OrdinaryDifferential Equation (ODE). Given an input data x belonging to the modeling distribution p(x) anda random noise N(0, I), the forward process is formulated as:",
  "xt = xt + t,(1)": "where xt is the noise-added data at a time step t and (t, t) are time-dependent functions oft. Particularly, these functions are constrained such that 1 = 0 = 0 and 0 = 1 = 1 to producecorrect mapping between data x at t = 0 and noise at t = 1. Specifically, use a simplelinear function where t = 1 t and t = t. We employ Flow matchings training objective toestimate the velocity between noise and data x:",
  "where v is a velocity estimator implemented by a neural network with parameters and c is an inputcondition (e.g., class or text). If no condition is used, c is set to empty": "Wavelet transformation. Among frequency transform techniques, wavelet transform stands outfor its simplicity and efficiency. It preserves the structure of image space, with low-frequencysubbands representing down-sampled approximations of the input image, while high-frequencyones emphasize local details such as vertical, horizontal, and diagonal edges. Particularly, Haarfeature is the most prevalent wavelet transform, consisting of a low-pass filter L =1",
  "anda high-pass filter H =1": "2 . To decompose an image x RHW , it needs to construct4 kernels LLT , LHT , HLT , HHT , then applies them to the input image to extract correspondingsubbands {xLL, xLH, xHL, xHH | x RH/2W/2}, respectively. This process is called discretewavelet transform (DWT). Notably, these filters are pairwise orthogonal, so an invertible matrixexists to map the data back to the original image space, coined as discrete inverse wavelet transform(IDWT). Given its benefits, we propose to use wavelet transform to supplement the local structureof frequency components into the process of Mamba, thus leading to enhanced image quality andtraining convergence, as demonstrated through our empirical experiments in section 4.",
  "Inspired by the advancement of Mamba-based diffusion models and frequency-based networks, wedesign a novel architecture DiMSUM for effective and high-quality image synthesis with the structure": "Wavelet level 1 Horizontal scanning Vertical scanning Wavelet level 2Input image : Illustration of Wavelet Mamba (Best view in color). For illustration purpose, we plotwavelet representations of an input image but our real process is performed on encoded features of theinput. Giving an image of size (8, 8), for example, it is first decomposed to four wavelet subbands ofsize (4, 4) where each is further transformed to 2nd-level subbands of size (2, 2). Green dots indicatepixel points within each wavelet subband and a window of size 2 2 is used to perform scanningacross multiple wavelet subbands like the CNN kernel. presented in . Similar to Latent Diffusion Models , our method performs image generationon the latent space of a pre-trained encoder. The method first receives an input image and encodesit to a latent map of size 4 H W. It then processes the latent map using our proposed DiffusionMamba network, whose core is a sequence of DiMSUM blocks, each consisting of DiM blocksthat employ a novel Mamba structure with spatial and frequency scanning fusion and a globallyweight-shared transformer block. The processed latent is then decoded to the output image.",
  "A core component of our approach is the DiM block that relies on a novel Spatial-Frequency Mambafusion technique. In this section, we will discuss in detail the ideas behind this vital component": "Scanning in frequency space. Mamba-based approaches in diffusion models often lack effectivescanning schemes for preserving both local and global 2D spatial information. Although severalworks have proposed different heuristic scanning methods to address this issue,these approaches are insufficient for capturing local pixel dependencies and long-range frequencyrelationships. Though LocalMamba proposed a window scanning to mimic the convolutionkernel, it often underperforms compared to previously mentioned scanning methods as it is limited tothe dependencies of nearby-pixels within window.",
  ": Comparison of window scanning on im-age and wavelet space. For illustration, one-levelwavelet transformation is applied and each sub-band is half the resoluton of original image": "DiMSUM addresses these challenges by decom-posing the original image into frequency waveletsubbands. This approach is effective to capturelong-range frequency while preserving relationsacross different subbands. We redesigned thewindow scanning, where each window corre-sponds to a subband of the frequency space as in. Consequently, each window capturesthe full range of low/high-frequency signalsfrom the original image. This advantage sets usapart from traditional window scanning in imagespace. As the model progresses through differ-ent subbands, it incorporates spatial informationrepresented at various low-to-high frequencies,adding valuable context to the denoising pro-cess. Wavelet Mamba. We now examine the integra-tion of the wavelet transform into the Mamba structure shown in . Wavelet Mamba first appliesDWT to decompose input features into wavelet subbands. Our main setting uses two-level Haarwavelet to map input into low and high-frequency features. Given input feature x RCHW ,first-level wavelet transform is applied to produce 4 wavelet subbands of size RCH/2W/2. Each subband is then further decomposed into second-level wavelet subbands of size RCH/4W/4. Thisfeature is pivotal, as we decompose every wavelet subband to evenly separate an input image intomultiple wavelet patches, unlike conventional wavelet transformations that process only LL subbandsat the next level. In Wavelet Mamba, we concatenate those subbands to form a 1D sequence, applywindow scanning within each subband, and slide across the sequence for feature extraction. Thewindow scanning is inspired by a CNN kernel proposed in with two window sliding directions:left to right and top to bottom. Note that since low-frequency subbands capture the main content ofimage, it should be input first. Therefore, we do not use reverse scanning orders: right to left andbottom to top. After passing through Wavelet Mamba, the output features are transformed back toinput shape by using IDWT twice. With wavelet module, our model can better capture local structureof frequency information. Thus, incorporating Wavelet Mamba with Spatial Mamba can offer betterperformance, yielding high-quality image generation (Spatial-frequency Mamba in ).",
  "qs, ks, vs = Linear(fs),qw, kw, vw = Linear(fw),fout = Linear(Concat(Attn(qs, kw, vw), Attn(qw, ks, vs)))": "More specifically, we first compute each features query (q), key (k), and value (v) using linearlayers. To fuse the information between spatial and wavelet features, we do cross-attention byswapping the queries (q) of spatial and wavelet before applying a self-attention module onto eachkey, query, and value triplet. Finally, we concat the outputs of two cross attentions by channelfollowed by a linear projection to obtain the output feature fout (see the last subfigure in ). Conditional Mamba. Unlike attention modules, conventional Mamba has no explicit mechanism toinject input conditions into its flow. We propose a simple technique that enables Mamba to take inany conditional input c via initializing the very first hidden state with the embedding c instead of zero,as in original Mamba. This can be considered as a type of prior injection into Mamba. Specifically,the recurrent process of Mamba can be rewritten as below:h0= Ah1 + Bx0y0= Ch0,ht= Aht1 + Bxtyt= Cht(3) In conventional Mamba, h1 is set to zero as there is no previous state at the beginning. Here, weset h1 = LinearD(c) to inject context prior into flow of Mamba. As shown in ablation, conditionalmamba effectively enhances model performance. This is beneficial for both unconditional andconditional generation. For unconditional image generation, we create an auxiliary learnable tokento capture image spaces global information, similar to vision transformers . For class-conditional generation task, we use a class embedding to condition on Mamba. Conditional Mambais enabled by default in DiM blocks ().",
  "Globally-shared attention block": "Since Mamba is better than transformer at long-range dependency but weaker than trans-former at in-context learning , we propose a hybrid mamba-transformer architecture which favorsboth these properties as in recent work Jamba . Motivated by Zamba , we introduce theglobally-shared transformer (attention) block. This shared attention block is added after each of fourDiM blocks as shown in since we want to preserve the continuity of the 4-sweep alternativescanning order. By using shared weights, we significantly reduce the number of parameters intro-duced by different attention blocks. This layer complements the flow of Mamba since transformersexcel at extracting global relations without relying on manually defined orders of input sequences, asin Mamba. Hence, with this hybrid architecture, our method effectively addresses Mambas orderdependence while significantly reducing the FID with very few additional parameters.",
  "Experiments": "Implementation. We established a depth of 20 layers, a base width of 1024, and a patch size of 2 fornetwork configuration (further information on hyperparameters in appendix A). We run experimentson standard datasets: CelebA-HQ, LSUN Church, and ImageNet. For the samplingmethod, we follow to use adaptive ODE solver dopri5 for evaluation. We assessed",
  "GAN modelBigGan-deep 6.950.28160M-StyleGAN-XL 2.300.53166M25000K 2564K": "On the CelebA-HQ dataset at resolutions of256x256 and 512x512 (), our methodachieved state-of-the-art FID scores of 4.62 and6.09, respectively, surpassing the scores reportedin recent studies. Furthermore, DiMSUM demon-strated superior recall scores, indicating a greaterdiversity in the generated samples compared toother methods. This result is particularly impres-sive, given the majority of baseline methods arebased on diffusion processes, which are known forexcellent diversity. On LSUN Church (a),our method outperformed diffusion-based methodsand achieved results nearly on par with GAN-basedapproaches. Moreover, our method recorded thehighest recall metric of 0.56, significantly exceed-ing the recall of GAN methods, thereby underscoring its robustness in generating diverse andhigh-quality images.",
  ": Ablation studies on CelebA-HQ 256 256 dataset at epoch 250": "On the ImageNet1k 256 dataset, our methodology attains a formidable FID of 2.26 using a guidancescale of 1.4, surpassing the DiT models across comparable and larger model configurations, suchas DiT-XL/2. This performance superiority extends to other benchmarks, including the SSM-basedDIFFUSSM-XL-G model. Although our model yields results similar to those of the SiT model, ourmodel is approximately 30% smaller in size.",
  "Training convergence": "As reported in Tables 1, 4, 5a, our method requires less training epochs/iterations to reach the optimalperformance compared to the baseline approach, implying a strong and fast convergence. To betterillustrate the training convergence comparison, we illustrate in d the performance of differentdiffusion-based models over training epochs regarding the FID-10K on CelebA-HQ 256. Notably,our proposed method demonstrates a superior convergence speed, rapidly decreasing FID scoreand stabilizing at a significantly lower value than the other methods like LFM, LDM, andDiT. This rapid descent is particularly evident within the first 150 epochs, after which ourmethod maintains a low FID score and still with sight on decrement, suggesting a high-quality imagegeneration capability. Compared to the learning curves of other methods, our method exhibits a morestable trajectory without significant oscillations between training epochs.",
  "Ablation of network design": "In this section, we ablate the design choices for our network, using experiments on the CelebA-HQ256 dataset. For the starting baseline, we adopt the same training settings from Zigma. Wechoose sweep-4 with interleave scanning order by default. In a, with our proposedconditional Mamba, the FID score is improved from 6.19 to 5.27, and the same trend is observed forrecall. Meanwhile, adding Wavelet Mamba followed by a simple concatenation layer to combinespatial and wavelet features results in a worse score of 5.87 due to the weak alignment betweenthese features. This demonstrates that our proposed cross-attention fusion layer is crucial forperformance improvement, fully leveraging wavelet components to achieve a score boosted to 4.92.The performance is further enhanced to 4.66 by incorporating the weight-shared transformer block.",
  "with swapped query achieves the best results while requiring fewer parameters and GFLOPs than theattention option, with only a marginal increase in computation compared to the linear option": "Number of wavelet levels. As shown in b, two wavelet levels provide the best performance onthe CelebA-HQ 256 dataset. We argue that the choice of wavelet levels should be based on the inputresolution. An input image of size 256 256, for instance, is encoded to a compact latent of size32 32, which is further patchified by 2 to the small size of 16 16. Hence, applying 3 waveletlevels results in extremely small wavelet subbands of size 2 2, leading to reduced performance. Alternative frequency transform. Apart from wavelet transform, we also consider different types offrequency techniques like DCT and Fourier Transform (d). For DCT, we propose a multi-orderJPEG scanning strategy (illustrated in ), based on JPEG compression instead of the windowscanning. For Fourier Features, we directly adopt the EinFFT block from SiMBA . In either case,the performance drops compared with the default choice of wavelet. Transformer layer. In f, we assess the advantage of the transformer layer for both ConditionalMamba and Spatial-Frequency Mamba. As shown, the globally-shared transformer further booststhe performance of our Spatial-Frequency Mamba. In contrast, applying this layer solely to spatialMamba increases the FID score by 0.13, highlighting the essence of our Spatial-Frequency Mambain conjunction with the transformer layer. Meanwhile, replacing this shared layer with independenttransformers results in a decline of 0.43 in FID and 0.03 in Recall.",
  "Ablation of scanning orders": "In e, we compare different scanning orders of Mamba. We keep it simple by using onlyMamba block for all experiments without the globally-shared transformer and fusion layer. InSpatial-frequency Mamba, we use \"Sweep-4\" scanning for spatial features by default and only testother scanning methods for wavelet features. Overall, Sweep-4 performs best when combined withour proposed Conditional Mamba module. We also show that scanning with many-way orders likeSweep-8, Zigzag-8, and Jpeg-8 is not guaranteed to yield better performance than Sweep-4 scanning.In spatial-frequency Mamba, it is demonstrated that our proposed window scanning for waveletMamba provides a better outcome than conventional sweep-4 scanning.",
  "Conclusion": "Our paper introduces a novel, promising architecture that seamlessly integrates spatial and frequencyfeatures into Mamba process. By leveraging wavelet transform within the Mamba framework, ourmethod enhances local structure awareness and ensures efficient spatial and frequency informationfusion. This dual-focus strategy improves the detail and quality of generated images and acceleratestraining convergence. Our comprehensive experiments demonstrate that DiMSUM consistentlyoutperforms state-of-the-art models of comparable size across multiple benchmarks, achieving lowerFID scores and higher recall metrics, highlighting its ability to produce diverse and high-fidelityimages. The proposed cross-attention fusion layer and globally shared transformer block alsocontribute to the models robustness and scalability. Considering the promising results, we anticipatethat future research in related domains, such as text-to-image synthesis, will adapt our backbonearchitecture and achieve comparable improvements in performance. Social impact and limitation. We believe that our proposed network advances the architecturaldesign of state-space models for image generation. This model can be extended to various tasks,such as large-scale text-to-image generation and multimodal diffusion. While there is a risk thatour architecture could be misused for malicious purposes, posing a social security challenge, we areconfident that this risk can be mitigated with the recent development of security-related research.Hence, the positives can outweigh the negatives, rendering the concern minor. While our method outperforms other diffusion baselines in generation quality and training conver-gence, we acknowledge areas for improvement. These include designing a multiscale architecture andaddressing manually defined scanning orders. Another advanced technique, such as masking trainingregularization , is orthogonal to our approach and could lead to further improvements.",
  "T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R. FlashAttention: Fast and memory-efficientexact attention with IO-awareness. In Advances in Neural Information Processing Systems,2022": "A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16words: Transformers for image recognition at scale. ICLR, 2021. A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16words: Transformers for image recognition at scale. In International Conference on LearningRepresentations, 2021. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Mller, H. Saini, Y. Levi, D. Lorenz, A. Sauer,F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXivpreprint arXiv:2403.03206, 2024.",
  "V. T. Hu, S. A. Baumann, M. Gui, O. Grebenkova, P. Ma, J. Fischer, and B. Ommer. Zigma:Zigzag mamba diffusion model. arXiv preprint arXiv:2403.13802, 2024": "V. T. Hu, W. Yin, P. Ma, Y. Chen, B. Fernando, Y. M. Asano, E. Gavves, P. Mettes, B. Ommer,and C. G. Snoek. Motion flow matching for human motion synthesis and editing. arXiv preprintarXiv:2312.08895, 2023. H. Huang, z. li, R. He, Z. Sun, and T. Tan. Introvae: Introspective variational autoencodersfor photographic image synthesis. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,volume 31. Curran Associates, Inc., 2018.",
  "Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generativemodeling. In The Eleventh International Conference on Learning Representations, 2023": "H. Liu, X. Jiang, X. Li, Z. Bao, D. Jiang, and B. Ren. Nommer: Nominate synergistic contextin vision transformer for visual recognition. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1207312082, 2022. H. Liu, X. Jiang, X. Li, A. Guo, Y. Hu, D. Jiang, and B. Ren. The devil is in the frequency:Geminated gestalt autoencoder for self-supervised visual pre-training. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 37, pages 16491656, 2023.",
  "B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.arXiv preprint arXiv:2209.14988, 2022": "R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesiswith latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pages 1068410695, 2022. O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recognitionchallenge. International Journal of Computer Vision, 115:211 252, 2014.",
  "Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution.Advances in Neural Information Processing Systems, 32, 2019": "Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-basedgenerative modeling through stochastic differential equations. In International Conference onLearning Representations, 2020. H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jgou. Training data-efficientimage transformers & distillation through attention. In International conference on machinelearning, pages 1034710357. PMLR, 2021. A. Vahdat, K. Kreis, and J. Kautz. Score-based generative modeling in latent space. InA. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, Advances in Neural Informa-tion Processing Systems, 2021. V. Voleti, C.-H. Yao, M. Boss, A. Letts, D. Pankratz, D. Tochilkin, C. Laforte, R. Rombach, andV. Jampani. Sv3d: Novel multi-view synthesis and 3d generation from a single image usinglatent video diffusion. arXiv preprint arXiv:2403.12008, 2024. Z. Wang, C. Lu, Y. Wang, F. Bao, C. Li, H. Su, and J. Zhu. Prolificdreamer: High-fidelity anddiverse text-to-3d generation with variational score distillation. Advances in Neural InformationProcessing Systems, 36, 2024.",
  "BDiscussion": "Advantages of DiMSUM. First, its important to highlight that our method outperforms both DiTand SiT while requiring less than a third of the training iterations, achieving the best FID score of2.11. Compared to other state-space diffusion models, our method outperforms DIFFUSSM-XL,considering a similar training duration. Notably, our method also uses a smaller network size of460M parameters, compared to 675M of DiT while still demonstrating strong generation capacityand faster training convergence. Clarification of scalable term. Given its current association with model parameter scaling inthe LLM-dominated landscape, scalable may be mistakenly interpreted as primarily referring toincreasing the size of the architecture. However, in the broader context, machine learning/ deeplearning algorithms scalability may refer to their capacity to handle bigger datasets and computationalresources while producing correct results in an acceptable period of time.",
  "Inference speed is also faster, as shown in": "Our SoTA results are achieved with a parameter count comparable (or even smaller) to existingmodels. Refer to a, 5a and 1. This suggests substantial room for further enlargement of ourmodels parameters, which we anticipate will yield even greater improvements across various andbigger datasets (see ). Why would using only wavelet scanning hurt model performance (as observed in a)?We hypothesize that spatial and frequency signals are not aligned and require careful integrationto leverage their information. Naively fusing these domains (e.g., by concatenation) can damageperformance due to conflicting or misaligned information. To address this challenge, we proposed a more sophisticated fusion method using Cross-Attentionlayers between these spaces. This approach enables the model to effectively combine informationfrom both domains, leveraging their strengths while mitigating potential conflicts. Hence, this fusiontechnique can enhance the FID from 5.87 to 4.92 in c, contributing to the SoTA result of ourmethod.",
  "CSpeed Analysis": "Memory and GFLOPs. The results, as shown in the table 4, reveal that DiMSUM-L/2s memoryusage is slightly higher than its counterpart. This increase is expected, considering DiMSUMsslightly larger parameter count. Note that the parameter change after changing image size is mainlydue to the PatchEmbed layer of the architecture, which both models have. Regarding GFlops, we acknowledge that for 256 256 images, DiMSUM produces about 4%more GFlops than DiT. However, an interesting trend emerges when we examine 512x512 images:DiMSUMs GFlops scaling is actually slower than DiTs, proving the efficiency of our methodfor high-resolution image synthesis. Consequently, at this higher resolution, DiTs GFlops exceedDiMSUMs by approximately 7%. This observation further highlights the strength of Mamba inhandling longer context length. This observation aligns with the known quadratic complexity of transformers as sequence lengthincreases. Our hybrid model mitigates this issue; the impact of attention blocks is reduced, whileMamba demonstrates its linear scaling complexity as the token count grows. This architectural choiceallows DiMSUM to maintain efficiency at higher resolutions, offsetting the initial GFlops differenceat 256 resolution.",
  "With these two points, we emphasize upon the importance of our proposed architecture, rather thanjust the benefits given by the flow matching framework": "Speed gain. In table 4, DiMSUM-L/2 achieves 2.2 seconds latency compared to DIT-L/2 with 3.8seconds though DiMSUM has larger GFLOPs. Notably, with resolution 512 (around 1024 tokens),the speed gap between our method and DiT becomes more significant and our architecture also haslower Gflops. This demonstrate the potential use of DiMSUM in larger benchmark like text-to-imagewhich has larger resolution. Analysis. Observing the Memory and GFLOPS in table table 4, its true to claim DiMSUM-L/2shows slower inference speed compared to its counterpart for 256x256 images using the same NFE(due to bigger GFLOPS), however, there are two crucial points to consider: Scaling Efficiency: When we increase the image size to 512x512, as evident from theMemory and GFLOPS table, our model actually requires fewer GFLOPS at this higherresolution, thanks to its slower latency scaling which we mentioned above. Consequently,for 512x512 images and larger, DiMSUM-L/2 would outperform its counterpart in speedgiven the same NFE. Adaptive Sampling Efficiency: We employ the dopri5 ODE adaptive solver for samplingfrom both models, similar to SiT and Zigma . This solver dynamically adjusts theNFE based on the initial noise and diffusion model characteristics, using the minimum NFEnecessary to achieve optimal image quality. Notably, DiMSUM requires fewer NFE to meetthe dopri5 stopping condition while still achieving a significantly better FID score than DiT.We hypothesize that our proposed hybrid architecture converges to a better solutionwith less curvature, enabling high-fidelity image production with fewer NFEs.",
  "DMore quantitative results": "Full training curve of DiT. The plot d intentionally stops at epoch 300, demonstrating themodels capability to converge faster than other methods. DiT does perform well on CelebA-HQ buttakes more than 500. Here, we further provide a complete training curve of DiT in . Its notedthat DiT and LFM in d use the same DiT-L/2 architecture. While DiT uses diffusion loss, LFMuses flow matching. LFM converge faster than DiT. However, our model with flow matching loss,demonstrates even faster convergence than LFM, indicating our architecture enhances convergencerate. Additional ablation of scanning orders. To substantiate our claims regarding the efficiency offrequency information, we conducted a comprehensive ablation study. This ablation utilized fourscanning orders: (1) bidirection, (2) jpeg-8, (3) sweep-8, and (4) zigzag-8. We trained models onCelebA-HQ at 256x256 resolution for 250 epochs, comparing performance when applying thesescanning strategies to: a) spatial domain only or b) both spatial and frequency domains. shows that integrating frequency domain information across all four scanning strategies ledto significant performance improvements. This consistent enhancement across various scanningmethods provides strong evidence for the effectiveness of our approach in leveraging frequencyinformation.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". Experiments Compute Resources": "Question: For each experiment, does the paper provide sufficient information on the com-puter resources (type of compute workers, memory, time of execution) needed to reproducethe experiments?Answer: [Yes]Justification: Yes, this is presented in appendix A.Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster,or cloud provider, including relevant memory and storage.",
  ". Broader Impacts": "Question: Does the paper discuss both potential positive societal impacts and negativesocietal impacts of the work performed?Answer: [Yes]Justification: We discussed the societal imparts in section 5.Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}