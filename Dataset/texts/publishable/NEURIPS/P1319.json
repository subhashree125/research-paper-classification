{
  "Abstract": "Realistic and interactive scene simulation is a key prerequisite for autonomousvehicle (AV) development. In this work, we present SceneDiffuser, a scene-leveldiffusion prior designed for traffic simulation. It offers a unified framework thataddresses two key stages of simulation: scene initialization, which involves gener-ating initial traffic layouts, and scene rollout, which encompasses the closed-loopsimulation of agent behaviors. While diffusion models have been proven effec-tive in learning realistic and multimodal agent distributions, several challengesremain, including controllability, maintaining realism in closed-loop simulations,and ensuring inference efficiency. To address these issues, we introduce amortizeddiffusion for simulation. This novel diffusion denoising paradigm amortizes thecomputational cost of denoising over future simulation steps, significantly reducingthe cost per rollout step (16x less inference steps) while also mitigating closed-looperrors. We further enhance controllability through the introduction of generalizedhard constraints, a simple yet effective inference-time constraint mechanism, aswell as language-based constrained scene generation via few-shot prompting ofa large language model (LLM). Our investigations into model scaling reveal thatincreased computational resources significantly improve overall simulation real-ism. We demonstrate the effectiveness of our approach on the Waymo Open SimAgents Challenge, achieving top open-loop performance and the best closed-loopperformance among diffusion models.",
  "Introduction": "Simulation environments allow efficient and safe evaluation of autonomous driving systems . Simulation involves initialization (determining starting conditions foragents) and rollout (simulating agent behavior over time), typically treated as separate problems .Inspired by diffusion models success in generative media, such as video generation and videoediting (inpainting , extension, uncropping etc.), we propose SceneDiffuser, a unifiedspatiotemporal diffusion model that addresses both initialization and rollout for autonomous driving,trained end-to-end on logged driving scenes. To our knowledge, SceneDiffuser is the first model tojointly enable scene generation, controllable editing, and efficient learned closed-loop rollout (). One challenge in simulation is evaluating long-tail safety-critical scenarios . Whiledata mining can help, such scenarios are often rare. We address this by learning a generative scenerealism prior that allows editing logged scenes or generating diverse scenarios. Our model supportsscene perturbation (modifying a scene while retaining similarity) and agent injection (adding agents tocreate challenging scenarios). We also enable synthetic scene generation on roadgraphs with realisticlayouts. We design a protocol for specifying scenario constraints, enabling scalable generation, anddemonstrate how a few-shot prompted LLM can generate constraints from natural language.",
  "Rollout Step = 2Rollout Step = 30Rollout Step = 60Rollout Step = 80 (final)": ": SceneDiffuser: a generative prior for simulation initialization via log perturbation, agentinjection, and synthetic scene generation, and for efficient closed-loop simulation at 10Hz viaamortized diffusion. It progressively refines initial trajectories throughout the rollout. Environmentsim agents are in green-blue gradient (temporal progression), AV agent in orange-yellow, andsynthetic agents in red-purple. Given a scene, realistically simulating agents and AV behavior is challenging .Unlike motion prediction tasks where entire future trajectories are jointlypredicted in a single inference, simulator predictions are iteratively fed back into the model, requiringrealism at each step. This poses challenges: distributional drift from compounding errors, highcomputational cost for models like diffusion, and the need to simulate various perception attributesrealistically. We propose Amortized Diffusion for simulation rollout generation, a novel approach for amortizingthe cost of the denoising inference over a span of physical steps that effectively addresses thechallenges of simulation realism due to closed-loop drift and inference efficiency. Amortizeddiffusion iteratively carries over prior predictions and refines them over the course of future physicalsteps (see Sec. 3.2 and ). This allows our model to produce stable, consistent, and realisticsimulated trajectories, while requiring only a single denoising function evaluation at each physicalstep while jointly simulating all perception attributes at each step. Experiments show that AmortizedDiffusion not only requires 16x less model inferences per step, but is also significantly more realistic.",
  "Data-driven Agent Simulation": "A variety of generative models have been explored for scene initialization and simulation, includingautoregressive models , cVAEs , cGANs , and Gaussian Mixture Models (GMMs). For closed-loop rollouts, these models have been extended with GMMs , GANs ,AR models over discrete motion vocabularies , cVAE , and deterministic policies .Open-loop rollouts have also been explored using cVAE .",
  "Diffusion Models for Agent Simulation": "Open-loop Sim Open-loop simulation generates behavior for agents that all lie within ones control,i.e. does not receive any external inputs between steps. Open-loop simulation thus cannot respond toan external planner stack (AV), the evaluation of which is the purpose of simulation. Diffusion modelshave recently gained traction in multi-agent simulation, particularly in open-loop scenarios (multi-agent trajectory forecasting) , using either single-shot or autoregressive (AR) generation.Single-shot approaches employ spatiotemporal transformers in ego-centric or scene-centricframes with motion/velocity deltas . Soft guidance techniques enhance controllability .DJINN uses 2d condition masks for flexible generation. Closed-loop Sim Closed-loop simulation with diffusion remains challenging due to compoundingerrors and efficiency concerns. Chang et al. explore route and collision avoidance guidance inclosed-loop diffusion, while VBD combines denoising and behavior prediction losses with aquery-centric Transformer encoder . VBD found it computationally infeasible to replan at a 1Hzfrequency in a receding horizon fashion over the full WOSAC test split due to the high diffusioninference cost, therefore testing in open-loop except over 500 selected scenarios. Initial Condition Generation Diffusion-based initial condition generation has also been studied. Pronovost et al. adapt the LDM framework to rendered scene images, while SLEDGE and DriveSceneGen diffuse initial lane polylines, agent box locations, and AV velocity.",
  "Diffusion for Temporal World Modeling and Planning": "Outside of the autonomous driving domain, diffusion models have proven effective for world sim-ulation through video and for planning. Various diffusion models for 4d data have been proposed,often involving spatiotemporal convolutions and attention mechanisms . In robotics,diffusion-based temporal models leverage Model Predictive Control (MPC) for closed-loop control and have shown state-of-the-art performance for imitation learning . Similar to our Amortized Diffusion approach, TEDi proposes to entangle the physical timestepand diffusion steps for human animation, thereby reducing O(T T ) complexity for T physicaltimesteps and T denoising steps to O(T ). However, we are the first work to demonstrate theeffectiveness of this approach for reducing closed-loop simulation errors, and the first to extend it toa multi-agent simulation setting.",
  "Global Context EncoderTransformer Denoiser Backbone": ": SceneDiffuser architecture. Global scene context is encoded into a fixed number of Nctokens via a Perceiver IO encoder. The noisy scene tokens are fused with local and global context,then used to condition a spatiotemporal transformer-based backbone via Adaptive LayerNorm(AdaLN) . Input/output tensor are in green, context tensors in blue, and ops in italics.",
  "Scene Diffusion Setup": "We denote the scene tensor as x RAT D, where A is the number of agents jointly modeledin the scene, T is the total number of modeled physical timesteps, and D is the dimensionalityof all the features that are jointly modeled. We learn to predict the following attributes for eachagent: positional coordinates x, y, z, heading , bounding box dimensions l, h, w, and object typek {AV, car, pedestrian, cyclist}. We model all tasks considered in SceneDiffuser as multi-taskinpainting on this scene tensor. Given an inpainting mask m BAT D, the correspondinginpainting context values x := m x, a set of global context c (such as roadgraph and trafficsignals), and a validity mask for a given agent at a given timestep v BA,T (to account for therebeing < A agents in the scene or for occlusion), we train a diffusion model to learn the conditionalprobability p(x|C), where C := { m, x, c, v}. See for an illustration of the scene tensor. Feature Normalization To simplify the diffusion models learning task, we normalize all featurechannels before concatenating them along D to form the scene tensor. We first encode the entire scenein a scene-centric coordinate system, namely the AVs coordinate frame just before the simulationcommences. We then scale x, y, z by fixed constants, l, h, w by their standard deviation, and one-hotencode k. See Appendix A.6 for more details. This simple yet generalizable process allows us tojointly predict float, boolean, and even categorical attributes by converting into a normalized space offloats. After generating a scene tensor x, we apply a reverse process to obtain the generated features. Diffusion Preliminaries We adopt the notation and setup for diffusion models from . Theforward diffusion process gradually adds Gaussian noise to x. The noisy scene tensor at diffusion stept can be expressed as q(zt|x) = N(zt|tx, 2t I), where t and t are parameters which controlthe magnitude and variances of the noise schedule under a variance-preserving model. Thereforezt = tx + tt, where t N(0, I). One major departure from the classic diffusion setup in ouramortized diffusion regime is that we do not assume a uniform noise level t R for the entire scenetensor x. Instead, we have t RT where t can be relaxed to have a different value per physicaltimestep in the scene tensor as described in Sec. 3.2. We utilize the commonly used -cosine schedulewhere t = cos(t/2) and t = sin(t/2). At the highest noise level of t = 1, the forward diffusionprocess completely destroys the initial scene tensor x resulting in zt = t N(0, I). Assuminga Markovian transition process, we have the transition distributions q(zt|zs) = N(zt|tszs, 2tsI),where ts = t/s and 2ts = 2t 2ts2s and t > s. In the denoising process, conditioned on asingle datapoint x, the denoising process can be written as",
  "2tx and ts = 2ts2s": "2t. In the denoising process, x is approximatedusing a learned denoiser x. Following and , we adopt the commonly used v prediction,defined as vt(t, x) = tt tx. We trained a model parameterized by to predict vt given zt ,t and context C: vt := v(zt, t, C). The predicted xt can be recovered via xt = tzt tvt. Themodel is end-to-end trained with a single loss:",
  "Scene Diffusion Tasks Different tasks are fomulated as inpainting problems ()": "Scene Generation (SceneGen): Given the full trajectory of some agents, generate the full trajectoryof other agents. We have mscenegen RA,1,1 (broadcastable to T timesteps and D features), wheremscenegen, a Pr(X = Aselect/Avalid), where Aselect U(0, Avalid) is the number of agents sampledto be selected as inpainting conditions out of Avalid valid agents in the scene.",
  "Behavior Prediction (BP): Given past and current data for all agents, predict the future for all agents.We have mbp R1,T ,1 (broadcastable to A agents and D features), where mbp, = I( < Thistory)": "Conditional SceneGen and Behavior Prediction: Both scenegen and behavior prediction masksare multiplied by a control mask at training time to enable controllable scenegen and controllablebehavior prediction at inference time. We have mcontrol RA,T ,D, where mcontrol,(a,,d) = Ia I Id, Ia Pr(X = Acontrol/Avalid), I Pr(X = Tcontrol/T ), Id Pr(X = pd) where pd of thecorresponding feature channel. This allows us to condition on certain channels, such as positions x, ywith or without specifying other features such as type and heading. Architecture We present a schematic for the SceneDiffuser architecture in , consisting of twoend-to-end trained models: a global context encoder and a transformer denosier backbone. Validity vis used as a transformer attention mask within the transformer denoiser backbone.",
  "Scene Rollout": "Future prediction with no replanning (One-Shot) is not used in simulation due to its non-reactivity,and forward scene inference, under the standard diffusion paradigm (Full AR), is computationallyintensive due to the double for-loop over both physical rollout steps and denoising diffusion steps. Moreover, executing only the first step while discarding the remainder leads to inconsistentplans that result in compounding errors. We adopt an amortized autoregressive (Amortized AR)rollout, aligning the diffusion steps with physical timesteps to amortize diffusion steps over physicaltime, requiring a single diffusion step at each simulation step while reusing previous plans. We illustrate the three algorithms in Algorithm 1-3 using the same model trained with a noise mixturet {U(0, 1); t} (Eqn. 2). We also illustrate Algorithm 3 in . We denote the total numberof timesteps T = H + F, where H, F denote the number of past and future steps. We denotex := x[H:F ] to be the temporal slicing operator where x is the final history step.",
  "Full ARMVTEMTR+++Amortized AR": ": We compare the influence of replanrate on performance for our Full AR and Amor-tized AR models. Circle radius # inferencecalls over the simulation. At 10Hz, AmortizedAR requires 16x less model inference per stepand is more realistic compared to Full AR. : Scene generation realism with modelparameter and resolution scaling 2. Decreasedtemporal patch sizes (i.e. increased temporalresolution) and increased parameters are botheffective for improving realism via compute scal-ing. Circle radius compute GFLOPs.",
  "Controllable Scene Generation": "To simulate long-tail scenarios such as rare behavior of other agents, it is important to effectivelyinsert controls into the scene generation process. To do so, we input an inpainting context scenetensor x , where some pixels are pre-filled. Through pre-filled feature values in x, we can specify aparticular agent of a specified type to be appear at a specific position at a specific timestamp. Data Augmentation via Log Perturbation The diffusion framework makes it straightforward toproduce additional perturbed examples of existing ground truth (log) scenes. Instead of starting frompure noise zt N(0, I) and diffusing backwards from t 0, we take our original log scene x andadd noise to it such that our initial zt = tx + t where t N(0, tI). Starting the diffusionprocess at t = 0 yields the original data, while t = 1 produces purely synthetic data. For t (0, 1),higher values increase diversity and decrease resemblance to the log. See Figs. 1 and 12 (Appendix). Language-based Few-shot Scene Generation The diffusion model inpaint constraints can bedefined through structured data such as a Protocol Buffer3 (proto). Protos can be converted intoinpainting values, and we leverage the off-the-shelf generalization capabilities of a publicly accessible",
  "(b) Post-Diffusion GHC": "(c) In-Diffusion GHC: Applying no-collision constraints pre-vents collisions (red-purple) in generated scenes(b, c). Iteratively applying constraints with everydiffusion step further enhances realism (c vs b). chat app powered by a large language model (LLM)4, to generate new Scene Diffusion constraintsprotos solely using natural language via few-shot prompt engineering. We show example resultsgenerated by the LLM in . Details in the Appendix (A.7).",
  "Generalized Hard Constraints": "Users of simulation often require agents to have specific behaviors while maintaining realistictrajectories. However, diffusion soft constraints require a differentiable cost for theconstraint and do not guarantee constraint satisfaction. Diffusion hard constraints are modeledas inpainting values and are limited in their expressivity. Inspired by dynamic thresholding in the image generation domain, where intermediate imagesare dynamically clipped to a range at every denoising step, we introduce generalized hard constraints(GHC), where a generalized clipping function is iteratively applied at each denoising step. We modifyEqn. 1 such that at each denoising step ts = ts2s",
  "2tclip(x), where clip() denotes theGHC-specific clipping operator. See more details on constraints in Appendix A.9": "We qualitatively demonstrate the effect of hard constraints for unconditional scene generation in. Applying hard constraints post-diffusion removes overlapping agents but results in unrealisticlayouts, while applying the hard constraints after each diffusion step both removes the overlappingagents and takes advantage of the prior to improve the realism of the trajectories. We find that thebasis on which the hard constraints operate is important: a good constraint will modify a significantfraction of the scene tensor (for example, shifting an agents entire trajectory rather than just theoverlapping waypoints), or else the model \"rejects\" the constraint on the next denoising step.",
  "Simulation Rollout": "Benchmark We evaluate our closed-loop simulation models on the Waymo Open Sim AgentChallenge (WOSAC) metrics (see Appendix A.1), a popular sim agent benchmark used in manyrecent works . Challenge submissions consist of x/y/z/ trajectories representingcentroid coordinates and heading of the objects boxes that must be generated in closed-loop andwith factorized AV vs. agent models. WOSAC uses the test data from the Waymo Open Motion",
  ": Design analysis and ablation studies": "Dataset (WOMD). Up to 128 agents (one of which must represent the AV) must be simulated ineach scenario for the 8 second future (comprising 80 steps of simulation), producing 32 rollouts perscenario for evaluation. In a small departure from the official setting, we utilize the logged validitymask as input to our transformer and unify the AV and agents rollout step for simplicity. Evaluation In Tab. 2, we show results on WOSAC. We show that Amortized AR (10 Hz) not onlyrequires 16x fewer model inference calls, but is also significantly more realistic than Full AR at a10Hz replan rate. In Amortized AR, we re-use the plan from the previous step, leading to increasedefficiency and consistency. The one-shot inference setting is equivalent to Full AR with no replanning(0.125 Hz) and achieves comparably higher realism, though as it is not executed in closed-loop, it isnot reactive to external input in simulation, and thus not a valid WOSAC entry. In Figs. 5 and 7, we investigate the effects of varied replan rates to simulation realism. While highreplan frequency leads to significant degredation in realism under the Full AR rollout paradigm,Amortized AR significantly reduces error accumulation while being 16 more efficient.",
  "Scene Generation": "Unconstrained Scene Generation We use the unconditional scene generation task as a means toquantitatively measure the distributional realism of our model. We condition the scene using the samelogged road graph and traffic signals, as well as the logged agent validity to control for the samenumber of agents generated per scene. All agent attributes are generated by the model. Due to a lack of public benchmarks for this task, we adopt a slightly modified version of the WOSAC metrics, where different metrics buckets are aggregated per-scene instead of per-agent, due tothe lack of one-to-one correspondence between agents in the generated scene versus the logged scene(see Appendix A.2 for more details). Metrics are aggregated over all agents that are ever valid in the9 second trajectory. We show our models realism metrics in Tab. 1. Even compared to the oracle performance (comparinglogged versus logged distributions), our model achieves comparable realism scores in every realismbucket. Introducing hard constraints on collisions can significantly improve the composite metricby preventing collisions, while scaling the model without hard constraints improves most realism",
  "L": ": Fully synthetic scene generation quality com-parison via scaling model parameters (S L) and in-creasing temporal resolution (patch size 8 1). Increas-ing compute by scaling either the model size or temporalresolution improves the overall realism. metrics as the model learns to generate more realistic trajectories. The realism metrics only apply totrajectories and do not account for generated agent type and size distributions. We compare the gener-ated size distributions versus log distributions in and find the marginal and joint distributionsboth closely track the logged distribution. We show more examples of diverse, unconstrained scenegeneration when conditioning on the same global context in Appendix A.8 . Constrained Scene Generation and Augmentation The controllability we possess in the scenegeneration process as a product of our diffusion model design can be useful for targeted generationand augmentation of scenes. In , we show qualitative results of scenes with constrained agentsgenerated either via manually defined configs or by a few-shot prompted LLM. Extended qualitativeresults are listed in Appendix A.7.3.",
  "Model Design Analysis and Ablation Studies": "Scaling Analysis Given two options of scaling model compute, either by increasing transformertemporal resolution by decreasing temporal patch sizes, or increasing the number of model parameters,we investigate the performance of multiple transformer backbones: {Model Size} {Temporal PatchSize} = {L, M, S} {8, 4, 2, 1}. We vary model size by jointly scaling the number of transformerlayers, hidden dimensions, and attention heads (see Sec. A.6 of Appendix for details). We showquantitative results from this model scaling in and qualitative comparisons in . Increasingboth temporal resolution and number of model parameters improves realism of the simulation. Multi-task Compatibility We find that multitask co-training across BP, SceneGen and with randomcontrol masks improves performance compared to a single-task, BP only model on the sim agentrollout task, notably reducing collision and offroad rates. We find that jointly learning multiple agentfeatures (x, y, z, , size, type) achieves on-par performance with a pose-only (x, y, z, ) model. Model Architecture Ablation As shown in Tab. 3, replacing AdaLN-Zero conditioning with crossattention leads to a 7.99% decrease in realism performance, largely due to significantly highercollision rates and offroad rates. Removing the agent-wise spatial attention layer very significantlyincreases collision rate, as it removes the mechanism for agents to learn a joint distribution.",
  "Conclusion": "We have introduced SceneDiffuser, a scene-level diffusion prior designed for traffic simulation.SceneDiffuser combines scene initialization with scene rollout to provide a diffusion-based approachto closed-loop agent simulation that is efficient (through amortized autoregression) and controllable(through generalized hard constraints). We performed scaling and ablation studies and demonstratedmodel improvements with computational resources. On WOSAC, we demonstrate competitive resultswith the leaderboard and state-of-the-art performance among diffusion methods. Limitations While our amortized diffusion approach is, to our knowledge, the only and bestperforming closed-loop diffusion-based agent model with competitive performance, we do not exceedcurrent SOTA performance for other autoregressive models. We do not explicitly model validitymasks and resort to logged validity in this work. Future work looks to also model the validity mask. Broader Impact This paper aims to improve AV technologies. With our work we aim to makeAVs safer by providing more realistic and controllable simulations. The generative scene modelingtechniques developed in this work could have broader social implications regarding generative mediaand content generation, which poses known social benefits as well as risks of misinformation.",
  "and Disclosure of Funding": "No third-party funding received in direct support of this work. We thank Shimon Whiteson for hisdetailed for detailed feedback and also the anonymous reviewers. We would like to thank RezaMahjourian, Rongbing Mu, Paul Mougin, and Nico Montali for offering consultation and feedbackon evaluation metrics. We thank Kevin Murphy for his assistance in developing the mathematicalnotation for the likelihood metrics. We thank Zhaoyi Wei and Carlton Downey for helpful discussionsabout the project. All the authors are employees of Waymo LLC. Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero, Bazej Osinski, HugoGrimmet, and Peter Ondruska. SimNet: Learning reactive self-driving simulations from real-worldobservations. In ICRA, 2021. Tim Brooks, Bill Peebles, Connor Homes, Will DePue, Yufei Guo, Li Jing, David Schnurr, JoeTaylor, Troy Luhman, Eric Luhman, Clarence Wing Yin Ng, Ricky Wang, and Aditya Ramesh.Video generation models as world simulators.2024.URL",
  "Younwoo Choi, Ray Coden Mercurius, Soheil Mohamad Alizadeh Shabestary, and Amir Rasouli. Dice:Diverse diffusion model with scoring for trajectory prediction, 2023": "Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan, YuningChai, Ben Sapp, Charles R. Qi, Yin Zhou, Zoey Yang, Aurlien Chouard, Pei Sun, Jiquan Ngiam, VijayVasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir Anguelov. Large scale interactive motionforecasting for autonomous driving: The waymo open motion dataset. In ICCV, 2021.",
  "Zhiyu Huang, Zixu Zhang, Ameya Vaidya, Yuxiao Chen, Chen Lv, and Jaime Fernndez Fisac. Versatilescene-consistent traffic scenario generation as optimization with diffusion, 2024": "Maximilian Igl, Daewoo Kim, Alex Kuefler, Paul Mougin, Punit Shah, Kyriacos Shiarlis, DragomirAnguelov, Mark Palatucci, Brandyn White, and Shimon Whiteson. Symphony: Learning realistic anddiverse agents for autonomous driving simulation. In ICRA, 2022. Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding,Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecturefor structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.",
  "Jack Lu, Kelvin Wong, Chris Zhang, Simon Suo, and Raquel Urtasun. Scenecontrol: Diffusion forcontrollable traffic scene generation. In ICRA, 2024": "Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 1146111471, June 2022. Reza Mahjourian, Rongbing Mu, Valerii Likhosherstov, Paul Mougin, Xiukun Huang, Joao Messias,and Shimon Whiteson. Unigen: Unified modeling of initial agent states and trajectories for generatingautonomous driving scenarios. In ICRA, 2024. Nico Montali, John Lambert, Paul Mougin, Alex Kuefler, Nick Rhinehart, Michelle Li, Cole Gulino,Tristan Emrich, Zoey Yang, Shimon Whiteson, Brandyn White, and Dragomir Anguelov. The waymoopen sim agents challenge. In Advances in Neural Information Processing Systems Track on Datasets andBenchmarks, 2023.",
  "Jiteng Mu, Michal Gharbi, Richard Zhang, Eli Shechtman, Nuno Vasconcelos, Xiaolong Wang, andTaesung Park. Editable image elements for controllable synthesis. arXiv preprint arXiv:2404.16029, 2024": "Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S Refaat, and Benjamin Sapp.Wayformer: Motion forecasting via simple & efficient attention networks. arXiv preprint arXiv:2207.05844,2022. Jiquan Ngiam, Vijay Vasudevan, Benjamin Caine, Zhengdong Zhang, Hao-Tien Lewis Chiang, JeffreyLing, Rebecca Roelofs, Alex Bewley, Chenxi Liu, Ashish Venugopal, David J Weiss, Benjamin Sapp,Zhifeng Chen, and Jonathon Shlens. Scene transformer: A unified architecture for predicting futuretrajectories of multiple agents. In ICLR, 2022. Matthew Niedoba, Jonathan Lavington, Yunpeng Liu, Vasileios Lioutas, Justice Sefas, Xiaoxuan Liang,Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, and Frank Wood. A diffusion-model ofjoint interactive navigation. In NeurIPS, 2023.",
  "Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In TheTenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022": "Benjamin Sapp, Yuning Chai, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilisticanchor trajectory hypotheses for behavior prediction. In Conference on Robot Learning, pages 8699.PMLR, 2020. Ari Seff, Brian Cera, Dian Chen, Mason Ng, Aurick Zhou, Nigamaa Nayakanti, Khaled S. Refaat, RamiAl-Rfou, and Benjamin Sapp. Motionlm: Multi-agent motion forecasting as language modeling. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 85798590,October 2023.",
  "Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr-a: 1st place solution for 2022 waymo opendataset challengemotion prediction. arXiv preprint arXiv:2209.10033, 2022": "Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Mtr++: Multi-agent motion prediction withsymmetric scene modeling and guided intention querying. IEEE Transactions on Pattern Analysis andMachine Intelligence, 46(5):39553971, 2024. Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-videogeneration without text-video data. In ICLR, 2023. Shuo Sun, Zekai Gu, Tianchen Sun, Jiawei Sun, Chengran Yuan, Yuhang Han, Dongen Li, and Marcelo HAng. Drivescenegen: Generating diverse and realistic driving scenarios from scratch. IEEE Robotics andAutomation Letters, 2024.",
  "Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, and Philipp Krhenbhl. Language conditionedtraffic generation. 7th Annual Conference on Robot Learning (CoRL), 2023": "Balakrishnan Varadarajan, Ahmed Hefny, Avikalp Srivastava, Khaled S Refaat, Nigamaa Nayakanti, AndreCornman, Kan Chen, Bertrand Douillard, Chi Pang Lam, Dragomir Anguelov, et al. Multipath++: Efficientinformation fusion and trajectory aggregation for behavior prediction. arXiv preprint arXiv:2111.14973,2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukaszKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information ProcessingSystems, 2017. Eugene Vinitsky, Nathan Lichtl, Xiaomeng Yang, Brandon Amos, and Jakob Foerster. Nocturne: ascalable driving benchmark for bringing multi-agent learning one step closer to the real world. In NeurIPSDatasets and Benchmarks Track, 2022.",
  "A.1WOSAC Metrics": "Suppose there are N 500k scenarios, each of length T = 80 steps, each containing A 128 agents(objects). For each scenario, we generate K = 32 samples (conditioned on the true initial state),which is a set of trajectories for each object for each time step, where each point in the trajectoryis a D = 4-dim vector recording location (x, y, z) and orientation . Let all this generated data bedenoted by x(1 : N, 1 : A, 1 : K, 1 : T, 1 : D). Let the ground truth data be denoted x(1 : N,1 : A, 1 : T, 1 : D). Below we discuss how to evaluate the likelihood of the true (test) dataset xunder the distribution induced by the simulated dataset x. (Note that we may have A > A, since the ground truth (GT) can contain cars that enter thescene after the initial prefix used by the simulator; this is handled by defining a validity mask,v(1 : N, 1 : T, 1 : A), which is set to 0 if we want to exclude a GT car from the evaluation, and isset to 1 otherwise.) Rather than evaluating the realism of the full trajectories in the raw (x, y, z, ) state space, WOSACdefines M = 9 statistics (scalar quantities of interest) from each trajectory. Let Fj(x(i, a, :))represent the set of statistics/features (of type j) derived from x(i, a, 1 : K, 1 : T) by pooling overT, K. This is used to compute a histogram pija(.) for the empirical distribution of Fj for scenario i.Let Fj(x(i, a, t)) be the value of this statistic from the true trajectory i for vehicle a at time t . Thenwe define the negative log likelihood to be",
  "A.3Additional Evaluation Details": "Simulation step validity Due to the requirement of validity masks during inference, which is appliedas an attention padding mask within the transformer, the model does not generate valid values forinvalid steps. As the WOSAC challenge evaluates simulation agents for all steps, regardless of thesteps validity, we use linear interpolation / extrapolation to impute values for all invalid steps in oursimulations for the final evaluation.",
  "A.4Additional Dataset Information": "WOSAC uses the v1.2.0 release of WOMD, and we treat WOMD as a set D of scenarios where eachscenario is a history-future pair. This dataset offers a large quantity of high-fidelity object behaviorsand shapes produced by a state-of-the-art offboard perception system. We use WOMDs 9 second 10Hz sequences (comprising H = 11 observations from 1.1 seconds of history and 80 observations from8 seconds of future data), which contain object tracks at 10 Hz and map data for the area covered bythe sequence. Across the dataset splits, there exists 486,995 scenarios in train, 44,097 in validation,and 44,920 in test.",
  "A.5Additional Amortized Diffusion Algorithm Details": "Warm up:At inference time, the rollout process is preceded by a warm up step. The warm up stepis necessary for initializing a buffer of future timesteps before any diffusion iterations take place. Thewarm up entails a single iteration of a one-shot prediction process described in Algorithm 1. Thisprocess samples pure noise for some future steps and conditions the denoising process on the set ofpast steps. Amortized autoregressive rollout:In , we provide a visual illustration of our amortizedautoregressive rollout procedure. We operate the rollout procedure using a buffer to track future stepsin the trajectory. After the warm up, the future buffer contains T predicted steps with an increasingnoise level. Note that step = 1 has very little noise applied. The future buffer in this state isdenoised for a single iteration using past steps to condition the process. After a single iteration, theclean step at = 1 is popped off of the buffer, and it is added to the past steps. Before the nextiteration, a step = T + 1 is sampled from a pure noise distribution and is appended to the end ofthe future buffer. The described rollout process can be repeated to generate trajectories of arbitrarylength as clean steps are popped off the buffer.",
  "Optimizer : We use the Adafactor optimizer , with EMA (exponential moving average). Wedecay using Adam, with 1 = 0.9, decayadam = 0.9999, weight decay of 0.01, and clip gradientnorms to 1.0": "Training details Train batch size of 1024, and train for 1.2M steps. We select the most competitivemodel based on validation set performance, for which we perform a final evaluation using the testset. We use an initial learning rate of 3 104. We use 16 diffusion sampling steps. When training,we mix the behavior prediction (BP) task with the scene generation task, with probability 0.5. Therandomized control mask is applied to both tasks.",
  "A.7.3Controllable Scenegen Results": "Qualitative results showing one successful and one failed example of applying the control points toscene generation task with the protos are listed in Appendix A.7.2. For measuring the success andfailures of this scene generation task, we randomly selected 25 examples that were generated witheach of the 4 control protos and qualitatively determined success on 1) if the new object does notoverlap with any existing objects in the scene and 2) if the new object semantically behaves in theway intended by the control points. Otherwise, we considered it a failure. Overall, we measured asuccess rate of 40/100.",
  ": Results of unconditioned scene generation for randomly selected road locations. For eachexample, we show the ground truth log along with 3 generated scenes": "Onroad Constraints ensure that the bounding boxes of specified generated agents stay on road.We define the offroad potential of road graph polyline i to be i(x, y) = (x xi)2 + (y yi)2 ifWi(x, y) = 0 else 0, where (xi, yi) is the closest point on the road graph with respect to (x, y) andWi(x, y) is the winding number of position (x, y) to polyline i, such that we only penalize a trajectoryfor going offroad. We only consider the closest road graph segment and only consider trajectoriesthat are more than > 20% onroad. We define cliponroad(x) = arg minxminiRG i(x, y)."
}