{
  "Abstract": "In modern chip design, placement aims at placing millions of circuit modules,which is an essential step that significantly influences power, performance, andarea (PPA) metrics. Recently, reinforcement learning (RL) has emerged as apromising technique for improving placement quality, especially macro placement.However, current RL-based placement methods suffer from long training times,low generalization ability, and inability to guarantee PPA results. A key issue liesin the problem formulation, i.e., using RL to place from scratch, which results inlimits useful information and inaccurate rewards during the training process. Inthis work, we propose an approach that utilizes RL for the refinement stage, whichallows the RL policy to learn how to adjust existing placement layouts, therebyreceiving sufficient information for the policy to act and obtain relatively denseand precise rewards. Additionally, we introduce the concept of regularity duringtraining, which is considered an important metric in the chip design industry but isoften overlooked in current RL placement methods. We evaluate our approach onthe ISPD 2005 and ICCAD 2015 benchmark, comparing the global half-perimeterwirelength and regularity of our proposed method against several competitiveapproaches. Besides, we test the PPA performance using commercial software,showing that RL as a regulator can achieve significant PPA improvements. Our RLregulator can fine-tune placements from any method and enhance their quality. Ourwork opens up new possibilities for the application of RL in placement, providinga more effective and efficient approach to optimizing chip design. Our code isavailable at",
  "Introduction": "In the complex and evolving landscape of modern chip design, placement is a pivotal process thatsignificantly influences the power, performance, and area (PPA) metrics of the final chip [21; 22].A modern chip typically comprises thousands of macros (i.e., individual building blocks such asmemories) and millions of standard cells (i.e., smaller basic components like logic gates). Themacro placement result provides a fundamental solution for the subsequent processes (e.g., standardcells placement and routing), thus playing an important role . For example, macro placementinfluences the placement of standard cells, and poor macro placement might make it challengingto place these cells optimally, leading to an unsatisfactory chip performance . Moreover, aninappropriate macro placement can result in macro blockage in the core center, which harms theoverall chip performance by causing unwanted effects such as routing congestion, inferior wirelength,and timing performance issues .",
  "(c) PPA metrics comparison": ": Placement layouts and congestions of (a) MaskPlace and (b) MaskRegulate on the su-perblue1 from ICCAD 2015 benchmark , where the red points indicate the congestion crit-ical regions. (c): Comparing two crucial PPA metrics, namely Congestion and total negativeslack (TNS) between MaskRegulate, DREAMPlace , AutoDMP , WireMask-BBO , andMaskPlace , where lower values indicate better performance. Due to the lengthy and complex workflow of chip design, designers often rely on proxy metrics thatcan reflect the final results to guide the optimization process [2; 30; 20]. One important proxy metricis half-perimeter wirelength (HPWL), which provides an approximation for the routing wirelengthand is widely used to measure the placement quality [2; 13; 28]. Traditional macro placementmethods can be divided into two categories. Earlier approaches usually solve macro placement byblack-box optimization (BBO) [24; 5; 12; 29]. They often suffer from the poor scalability due to thelarge-scale search space and high complexity of decoding a solution to a placement. Another typeis analytical method [6; 7; 19], which can solve the placement efficiently by approximating HPWLgradients. However, these methods are hard to guarantee the non-overlapping constrain between cellsand are easy to be stuck in local optima [16; 34]. Reinforcement learning (RL) has recently emerged as a promising technique to enhance themacro placement quality [23; 9; 8; 16; 15]. RLs ability to learn policies through interaction with acomplex environment offers a novel pathway for addressing various challenges of macro placement.However, the application of RL is currently hindered by several limitations, including the longtraining time, an inability to guarantee PPA improvements, and the lack of generalization acrossdifferent chip layouts. In this work, we highlight that a major contributing factor to these issues is theproblem formulation, i.e., the conventional RL approach of placing macros from scratch often resultsin limited state information and inaccurate reward signal throughout the learning process. To address these challenges, we propose a novel RL approach called MaskRegulate that shiftsthe focus from initial placement to refining existing placement layouts. The RL policy acts as aregulator rather than a placer, which operates on pre-existing placements, thus allowing for accessto comprehensive state information and enabling the acquisition of more precise rewards. Thisadjustment enhances the efficiency of the learning process and finally improves the final placementresults. Furthermore, MaskRegulate introduces the concept of regularity as a part of inputinformation and a critical reward signal, which has been largely overlooked in previous researchdespite its significance in ensuring manufacturability and performance. Previous methods often onlyconsider the HPWL metric, suffering from optimizing different metrics effectively. By integratingregularity into the RL framework, our approach aligns more closely with advanced chip requirements. The effectiveness of the proposed MaskRegulate is comprehensively evaluated on the ICCAD2015 benchmark , which is is currently one of the largest open-source benchmarks that allowsus to evaluate PPA metrics such as congestion and timing slack. We first compare the global HPWL and regularity of our approach against several competitive methods. Additionally, weuse the commercial electronic design automation (EDA) tool Cadence Innovus to evaluate thePPA performance, demonstrating that our proposed MaskRegulate can lead to significant PPAimprovements, e.g., the placement layouts and two PPA metrics on superblue1, as shown in .Specifically, compared to MaskPlace (an advanced RL placer ; MaskRegulate shares a similararchitecture to it), MaskRegulate improves 17.08% on routing wirelength, 73.08% and 38.81 % onrouted horizontal and vertical congestion overflow respectively, 18.35% on worst negative slack,37.89% on total negative slack, and 46.17% on the number of violation points. This work provides a more effective approach for macro placement of modern chips, opening newpossibilities for the application of RL in chip design. The contributions of this work are highlightedin three key points: Novel problem formulation: Innovatively applying RL in the refinement stage of macroplacement, which allows for more effective learning from structured state and accuratereward information, significantly enhancing the learning efficiency and effectiveness. Integration of regularity: Introducing regularity, a critical yet previously overlooked metricin chip design, into the RL training framework, which not only aligns with industry practicebut also enhances the chip PPA quality. Impressive PPA improvement and comprehensive analysis: On the popular ICCAD 2015benchmark, our proposed MaskRegulate demonstrates significant improvements in PPAmetrics, showing the practical applicability and effectiveness of the RL regulator.",
  "Placement": "The circuit in the placement stage is considered as a graph where vertices model gates. The maininput information is the netlist N = (V, E), where V denotes the information (i.e., height andwidth) about all macros designated for placement on the chip, and E is a hyper-graph comprisedof nets ei E, which encompasses multiple cells (including both macros and standard cells) anddenotes their inter-connectivity in the routing stage. Given a netlist, a fixed canvas layout and astandard cell library, a placement method is expected to determine the appropriate physical locationsof movable macros such that the total wirelength can be minimized. A macro placement solutions = {(x1, y1), . . . , (xk, yk)} consists of the positions of all the macros {vi}ki=1, where k denotes thetotal number of macros. One popular objective of macro placement is to minimize the total HPWL ofall the nets while satisfying the cell density constraint, which is formulated as,",
  "where D denotes the density, is a threshold, and HPWLe is the HPWL of net e, which is definedas: HPWLe(s) = (maxvie xi minvie xi) + (maxvie yi minvie yi)": "There are three mainstream placement methods, i.e., analytical methods, black-box optimizationmethods, and learning-based methods. Analytical methods place macros and standard cellssimultaneously, which can be roughly categorized into quadratic placement and nonlinear placement.Quadratic placement [11; 18] iterates between an unconstrained quadratic programming phase tominimize wirelength and a heuristic spreading phase to remove overlaps. Nonlinear placement [6;20; 7] formulates a nonlinear optimization problem and tries to directly solve it with gradient descentmethods. Generally speaking, nonlinear placement can achieve better solution quality, while quadraticplacement is more efficient. Recently, there has been extensive attention on GPU-accelerated non-linear placement methods. For example, DREAMPlace [19; 17] transforms the non-linear placementproblem in Eq. (1) into a neural network training problem, solves it by classical gradient descentand leverages GPU, enabling ultra-high parallelism and acceleration and producing state-of-the-artanalytical placement quality. Black-box optimization methods for placement have a long history. Earlier methods such as SP and B-tree have poor scalability due to the rectangular packing formulation. Recently, some black-box optimization methods have made significant progress by changing the search space. AutoDMP improves DREAMPlace by using Bayesian optimization to explore the configuration space and showsremarkable performance on multiple benchmarks. WireMask-BBO adopts a wire-mask-guidedgreedy genotype-phenotype mapping and can be equipped with any BBO algorithm, demonstratingthe superior performance over other types of methods.",
  "RL for Macro Placement": "Researchers recently leverage RL-based methods for better placement quality to meet the demands ofmodern chip design. GraphPlace first models macro placement as a RL problem. It divides thechip canvas into discrete grids, with each macro assigned discrete coordinates of grids, wherein theagent decides the placement of the current macro at each step. However, no reward is given untilall the macros are placed, making the reward sparse and hard to learn. DeepPR and PRNet incorporate macro placement, standard cells placement, and routing to achieve better performancethan GraphPlace, but may violate the non-overlap constraint. To address this issue, MaskPlace introduces a dense reward and uses a pixel-level visual representation for circuit modules, whichcan comprehensively capture the configurations of thousands of pins, enabling fast placement ina full action space on a large canvas size. MaskPlace has many attractive benefits that previousmethods do not have, e.g., 0% overlap, dense reward, and high training efficiency. ChiPFormer incorporates an offline learning decision transformer and focuses on improving the generalizability ofplacer. EfficientPlace integrates a global tree search algorithm to guide the optimization process,achieving remarkable placement quality within a short time. However, current RL methods exhibit several shortcomings: 1) Placing from scratch providesinsufficient state information and inaccurate reward signals; 2) Most methods focus on minimizingwirelength, which may bring macro blockages and thus harm the final PPA metrics. In this work, wepropose a novel RL approach for macro placement: an RL policy acts as a macro regulator rather thana macro placer. Specifically, our learned RL policy is designed to adjust macros based on an existingplacement result, rather than placing all macros from scratch. This approach aims to refine andoptimize pre-existing layouts, addressing the limitations of traditional RL-based placement methods.",
  "MaskRegulate Framework": "Problem formulation of RL regulator. In the Markov Decision Process (MDP) formulation oftraditional RL placer, a macro is placed at each step [23; 9; 16; 15]. The placement order of macrosis determined based on some pre-defined rules, such as the number of nets, the size of macros, andthe number of connected modules that have been placed. An episode ends after all macros have beenplaced. Typically, the state representation includes information about the chip canvas, the macros thathave already been placed, and the macro currently being placed. In GraphPlace , the reward isdetermined only after all macros have been placed, resulting in a sparse reward signal that complicatesthe training process. Recent works have introduced various methods to densify the reward signal. Forinstance, WireMask provides a more continuous reward based on the macros already placed. Incontrast to RL placers, our RL regulator focuses on refining an existing placement by adjusting thelocation of one macro at each step. Unlike the placer, which initiates the placement process fromscratch, the regulator benefits from additional information when adjusting each macro. Specifically,the regulator considers not only the macros that have already been placed but also the positions ofall other macros. Furthermore, it enhances accuracy by taking into account all macros, even whileemploying a reward function similar to WireMask. Due to the advantages mentioned above in the MDP problem formulation, even without consideringadditional factors (e.g., regularity), RL regulator is able to achieve better results compared to RLplacer, as shown in our experiments in Appendix B.1. Furthermore, our main experimental resultsdemonstrate superior performance not only in proxy metrics but also in PPA metrics measured bycommercial tools, as shown in .2. The regulator also exhibits better generalization abilities,as shown in .3. Intuitively, adjusting an unseen chip is easier for the regulator compared to",
  "placing macros from scratch, as the incomplete state information of placer would be even worse inthe case of unseen chips, resulting in poorer performance": "Policy architecture. Our policy architecture is illustrated in . The policy divides the chipcanvas into several grids and utilizes visual information as inputs, converting chip information intopixel-level image masks. This approach has demonstrated superior efficiency and performance in RLplacer policy learning [16; 15]. The inputs include an image of the current canvas, a PositionMaskthat identifies all valid positions for placing the current macro, a WireMask that indicatesthe approximate wirelength change for placing the current macro at each valid position, and aRegularMask that indicates the change in regularity for placing the current macro at each validposition (which will be detailed in .2). An illustration of the PositionMask and WireMaskis provided in . To facilitate broader adjustments, the PositionMask has been modified toconsider only macros that have already been adjusted; thus, grids occupied by unadjusted macrosare available for placement. In our MaskRegulate, the calculation of the WireMask is based on allmacros, allowing its value to either increase or decrease. These values are normalized to the range, unlike the normalization used in . Additionally, our framework introduces theRegularMask to quantify changes in regularity within the state and to encourage improvements inregularity through the reward function, as presented in .2.",
  "Integration of Regularity": "Why does regularity matters? Macro placement has significant impact on subsequent chip designprocesses, including standard cell placement and routing. If only focusing on minimizing wirelength(which is the case for most current RL placers), certain macros may end up positioned in the middle of the chip canvas, resulting in macro blockages . This, in turn, leads to the division of availableplacement areas into separate and disconnected sub-regions. As a consequence, standard cells thatare connected by the same net may be scattered across different placement sub-regions, resulting inincreased overall wirelength and the potential routing challenges, which ultimately degrade the timingperformance. Thus, a well-established practice among experienced engineers in macro placementis to place macros towards the peripheral regions of the chip to prevent macro blockage. In thiswork, we aim to integrate regularity in the learning-based placement approach to achieve placementpreferences similar to those of experienced engineers. RegularMask. Intuitively, macros closer to the edges tend to have lower regularity. Therefore, wepropose a simple and effective way to measure regularity. On a canvas, the regularity of a grid locatedat (x, y) is calculated as min{x, Xmax x} + min{y, Ymax y}, where Xmax and Ymax representthe real length of the horizontal and vertical axes, respectively. Given a macro to be placed, theRegularMask measures the value change in regularity for each valid placement position, as illustratedin (d). Reward and policy learning. The reward of MaskRegulate consists of two components: rwireand rreg, which represent the reduction of HPWL and the improvement in regularity, respectively,after refining the current macro. To mitigate the influence of scale differences on training causedby wirelength and regularity, both rwire and rreg are normalized to . The final reward isr = rwire + (1 ) rreg, where is a trade-off coefficient. We will analyze the influence of in .4, showing that different lead to different multi-objective preferences. The detailedinformation are presented in Appendix A.3. MaskRegulate treats the chip canvas as a grid and dividesit into N N cells, resulting in N 2 possible discrete actions. We use the popular proximal policyoptimization (PPO) algorithm to learn the regulator policy.",
  "Experiment": "In this section, we first introduce the basic experimental settings, including the tasks and evaluationmetrics in .1. Then, we try to answer the following three research questions (RQs) inSections 4.2 to 4.4: 1) How does MaskRegulate perform compared to other methods? 2) How is thegeneralization ability of MaskRegulate? 3) How do the different parts of MaskRegulate affect theperformance? Finally, we provide the visualization of placement results and congestion in .5.",
  "Experimental Settings": "Tasks. We mainly use the ICCAD 2015 benchmark as our test-bed, which includes sufficientadvanced chip information and is currently one of the largest open-source benchmarks that allows usto evaluate congestion, timing and other PPA metrics. The benchmark statistics are listed in in Appendix A.1. Although ICCAD 2015 is the benchmark we have found that closely reflects thecurrent practices in the EDA industry, it still has some shortcomings. For example, it allows for alarge placement area, resulting in loose placement results that do not adhere to the design principlesof advanced modern chips. Note that the A in PPA denotes Area, which is a core metric ofchip design and should be minimized [3; 32]. Therefore, we scale down the chips placement area,presenting further challenges for the compared methods. Besides, we also conduct experiments onISPD 2005 benchmark , which is also a popular benchmark in AI for chip design but does nothave sufficient information for PPA evaluation. Detailed results can be found in Appendix B. Proxy evaluation metrics. We use the following two popular proxy metrics for a quick comparisonof different algorithms: 1) Global HPWL. After determining the locations of all the macros, we useDREAMPlace to place standard cells to obtain the global placement result, and then report theglobal HPWL (i.e., full HPWL involving both macros and standard cells). Compared to macro HPWL,global HPWL considers the total wirelength, typically on a scale that is two orders of magnitudelarger, providing a better estimation of the final real performance of the chip. 2) Regularity: Wecompute the regularity values for all macros, which serve as a measurement of the overall regularityof the placement result. We run each algorithm for five times and report their mean and variance. Wedo not consider the rectangular uniform wire density (RUDY) metric for congestion proxy, asthis approximation is sometimes positively correlated with the HPWL metric and is not accurate .Instead, we will evaluate congestion within our PPA evaluation. PPA evaluation metrics. The whole chip design process is lengthy and complex, and proxy metricsmay not accurately capture the true performance of the chip. PPA metrics often require the use ofcommercial EDA tools to obtain precise results with expensive cost. In our experiments, we selectthe best placement result for PPA evaluation based on global HPWL from multiple runs. Afterobtaining the global placement results, we use commercial tool Cadence Innovus to proceed thesubsequent stages and evaluate their PPA metrics, including routed wirelength, routed vertical andhorizontal congestion overflow, worst negative slack, total negative slack, and the number of violationpoints. These metrics are extremely important measures of chip design and are typically consideredto evaluate the quality of a chip comprehensively.",
  "RQ1: How does MaskRegulate perform compared to other methods?": "We consider the following methods to be compared: DREAMPlace : A state-of-the-art analyticalplacer; AutoDMP : A method that improves DREAMPlace by exploring its configuration spaceiteratively; WireMask-EA : A state-of-the-art black-box macro placement method with EA asthe optimizer; MaskPlace : A representative online RL methods, which shares similar policyarchitecture, state, HPWL reward with our MaskRegulate. For the same components, MaskPlace and MaskRegulate use the same settings, e.g., the number ofgrids, and the learning rate. Detailed information is provided in Appendix A.3. Additionally, in orderto demonstrate that the regulator has higher training efficiency than the placer, MaskRegulate andMaskPlace are trained for 1000 and 2000 episodes, respectively. For each chip, MaskRegulate usesDREAMPlace to obtain an initial macro placement result to be adjusted, which takes within fewminutes and has relatively low quality. The overall evaluation results are shown in . MaskRegulate achieves the best average rank onboth proxy and PPA metrics. DREAMPlace has the worst average ranking on wirelength, congestion,and timing. However, after adjustment by MaskRegulate, the obtained placements achieve thebest average rank. Compared to MaskPlace, MaskRegulate leads to significant improvements inmultiple PPA indicators: improves 17.08% on routing wirelength, 73.08% and 38.81 % on routedhorizontal and vertical congestion overflow respectively, 18.35% on worst negative slack, 37.89%on total negative slack, and 46.17% on the number of violation points. By incorporating regularity,MaskRegulate achieves the highest regularity on all the eight chips. We can observe a certaincorrelation between the proxy metric (global HPWL) and the real metric (rWL), but there still existsa gap, indicating the challenges involved in the placement task. Furthermore, we provide detailedvisualizations of placement results in , where MaskRegulate shows significant improvementson congestion metrics. Besides, the final placement layouts of MaskRegulate are much regular thanall the other methods.",
  "RQ2: How is the generalization ability of MaskRegulate?": "The generalization ability of RL policies is an important question to be investigated. In this section,we pre-train MaskRegulate and MaskPlace on the first four chips (i.e., superblue1, superblue3,superblue4, and superblue5) and test on the remaining four chips. To further validate the ability ofMaskRegulate to adjust different initial placement results, we use it to adjust the results obtained bydifferent initial placements on the test chips. The results are shown in . On both the global HPWL and regularity metrics, MaskRegulateconsistently outperforms MaskPlace, showcasing its stronger generalization capability. An interestingfinding is that MaskRegulate performs better on unseen chips than on the chips it was trained on,specifically in terms of global HPWL, such as with superblue16. This may suggest that MaskRegulatehas learned some general knowledge during the pre-training process, enabling it to overcome localoptima that may arise from direct learning on the target chip.",
  "We investigate the influence of different parts and provide additional analysis in this section": "Hyperparameters sensitivity analysis: different trade-off coefficient leads to different multi-objective preferences. One hyperparameter of RegularMask is the coefficient between HPWLreward rwire and regularity reward rreg, where a higher indicates a preference for optimizing : Results of proxy metrics and PPA metrics on the ICCAD 2015 benchmarks. Global HPWL(1e8) and Regularity (1e6) are two proxy metrics. PPA metrics are evaluated by Cadence Innovus.The placement is performed by different methods, and the subsequent stages are performed byCadence Innovus. rWL (m) is the routed wirelength; rO-H (%) and rO-V (%) represent the routedhorizontal and vertical congestion overflow, respectively; WNS (ns) is the worst negative slack; TNS(1e5 s) is the total negative slack; NVP (1e4) is the number of violation points. WNS and TNS arethe larger the better, while the other metrics are the smaller the better. The best result of each metricon each chip is bolded.",
  ": Illustration of MaskRegulate regulators with varying values (ranging from 0.1 to 0.9)": "HPWL, and vice versa. In this section, we investigate the influence of the trade-off coefficient . Wetrain different MaskRegulate regulators with varying values (ranging from 0.1 to 0.9) and report theproxy and PPA results in . Due to the expensive computational cost of PPA, we select fourdifferent trade-offs of MaskRegulate for evaluation. As expected, different values lead to differentmulti-objective preferences. In our experiments, we use = 0.7 for all the chips as it achieves arelative balance between different objectives. Ablation studies. We consider the following ablations of MaskRegulate. 1) Only changing the prob-lem formulation and purely comparing placer and regulator. We implement Vanilla-MaskRegulate,where the only difference to MaskPlace is the problem formulation, and all the other components (e.g.,state and reward) are the same. The results show that Vanilla-MaskRegulate consistently outperformsMaskPlace in terms of Global HPWL. 2) MaskRegulate with or without normalization. Since globalHPWL has large scale than regularity, MaskRegulate w/o normalization does not prefer to considerregularity, which is not what we expect. 3) Training regularity-aware RL placer from scratch. Weimplement MaskPlace + RegularMask and compare it with MaskPlace and MaskRegulate. Theresults show the advantages of the integration of regularity (between MaskPlace and MaskPlace +RegularMask) and our RL regular formulation (between MaskPlace + RegularMask and MaskRegu-late). The above ablation results demonstrate the effectiveness of each component of MaskRegulate.Detailed results and discussions are provided in Appendix B.1 due to space limitation.",
  "Additional results": "We conduct the following additional results to comprehensively show the effectiveness of ourMaskRegulate. 1) To verify whether using a better model structure for the RL placer can compare tothe regulator, we add comparison with recent proposed ChiPFormer under a fair setting. 2) Tofurther show the generalization ability of our methods, we conduct generalization experiments onthe ISPD 2005 benchmark . 3) To investigate whether MaskRegulate can be used to adjust anyinitial macro placement solution, we use the pre-trained model to fine-tune other placement results.",
  "Final Remarks": "Conclusion. In this paper, we present a novel RL problem formulation for macro placement, focusingon the development of a macro regulator rather than a placer. Our proposed method, MaskRegulate,demonstrates substantial improvements in chip placement quality by refining existing layouts insteadof generating them from scratch. By integrating dense reward signals and emphasizing regularity, ourapproach effectively addresses the limitations of traditional RL-based placement methods, resultingin superior performance in PPA metrics across various chips. This advancement paves the way formore efficient and effective chip design through RL. Limitations and future work. This study has several primary limitations: it does not consider theimpact of module aspect ratio and area factors on placement; it overlooks global wirelength and timingmetrics during the training process; and it does not employ advanced transformer architectures toenhance the generalization of the regulator. Chip design inherently involves different preferences, suchas the need for compact size in mobile phone chips and larger sizes for computer chips. Therefore,future research should address these challenges and explore efficient methods to obtain a set of chipplacements that accommodate different preferences using multi-objective optimization.",
  "and Disclosure of Funding": "We thank the reviewers for their insightful and valuable comments. This work was supported by theNational Science and Technology Major Project (2022ZD0116600), the National Science Foundationof China (62276124), and the Fundamental Research Funds for the Central Universities (14380020). A. Agnesina, P. Rajvanshi, T. Yang, G. Pradipta, A. Jiao, B. Keller, B. Khailany, and H. Ren.AutoDMP: Automated DREAMPlace-based macro placement. In Proceedings of the 27thInternational Symposium on Physical Design, pages 149157, Virtual, 2023. A. E. Caldwell, A. B. Kahng, S. Mantik, I. L. Markov, and A. Zelikovsky. On wirelength esti-mations for row-based placement. IEEE Transactions on Computer-Aided Design of IntegratedCircuits and Systems, 18(9):12651278, 1999.",
  "Y. Chang, Z. Jiang, and T. Chen. Essential issues in analytical placement algorithms. IPSJTransactions on System LSI Design Methodology, 2:145166, 2009": "Y.-C. Chang, Y.-W. Chang, G.-M. Wu, and S.-W. Wu. B*-trees: A new representation fornon-slicing floorplans. In Proceedings of the 37th Annual Design Automation Conference, pages458463, Los Angeles, CA, 2000. T.-C. Chen, Z.-W. Jiang, T.-C. Hsu, H.-C. Chen, and Y.-W. Chang. Ntuplace3: An analyticalplacer for large-scale mixed-size designs with preplaced blocks and density constraints. IEEETransactions on Computer-Aided Design of Integrated Circuits and Systems, 27(7):12281240,2008. C.-K. Cheng, A. B. Kahng, I. Kang, and L. Wang. Replace: Advancing solution quality androutability validation in global placement. IEEE Transactions on Computer-Aided Design ofIntegrated Circuits and Systems, 38(9):17171730, 2018. R. Cheng, X. Lyu, Y. Li, J. Ye, J. Hao, and J. Yan. The policy-gradient placement and generativerouting neural networks for chip design. In Advances in Neural Information Processing Systems35, New Orleans, LA, 2022.",
  "R. Cheng and J. Yan. On joint learning for solving placement and routing in chip design. InAdvances in Neural Information Processing Systems 34, pages 1650816519, Virtual, 2021": "Z. Geng, J. Wang, Z. Liu, S. Xu, Z. Tang, M. Yuan, H. Jianye, Y. Zhang, and F. Wu. Rein-forcement learning within tree search for fast macro placement. In Proceedings of the 41stInternational Conference on Machine Learning, 2024. X. He, T. Huang, L. Xiao, H. Tian, and E. F. Y. Young. Ripple: A robust and effective routability-driven placer. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,32(10):15461556, 2013. X. Hong, G. Huang, Y. Cai, J. Gu, S. Dong, C.-K. Cheng, and J. Gu. Corner block list: Aneffective and efficient topological representation of non-slicing floorplan. In Proceedings of the13th IEEE/ACM International Conference on Computer Aided Design, pages 812, San Jose,CA, 2000. A. B. Kahng and S. Reda. A tale of two nets: Studies of wirelength progression in physical de-sign. In Proceedings of the 7th International Workshop on System-level Interconnect Prediction,pages 1724, Munich, Germany, 2006. M. Kim, J. Hu, J. Li, and N. Viswanathan. ICCAD-2015 CAD contest in incremental timing-driven placement and benchmark suite. In Proceedings of the IEEE/ACM International Confer-ence on Computer-Aided Design, pages 921926, Austin, TX, 2015. Y. Lai, J. Liu, Z. Tang, B. Wang, J. Hao, and P. Luo. Chipformer: Transferable chip placementvia offline decision transformer. In Proceedings of the 40th International Conference onMachine Learning, pages 1834618364, Honolulu, HA, 2023.",
  "T. Lin, C. C. N. Chu, and G. Wu. POLAR 3.0: An ultrafast global placement engine. InProceedings of the IEEE/ACM International Conference on Computer-Aided Design, pages520527, Austin, TX, 2015": "Y. Lin, Z. Jiang, J. Gu, W. Li, S. Dhar, H. Ren, B. Khailany, and D. Z. Pan. DREAMPlace:Deep learning toolkit-enabled gpu acceleration for modern VLSI placement. IEEE Transactionson Computer-Aided Design of Integrated Circuits and Systems, 40(4):748761, 2020. J. Lu, P. Chen, C.-C. Chang, L. Sha, D. J.-H. Huang, C.-C. Teng, and C.-K. Cheng. ePlace:Electrostatics-based placement using fast Fourier transform and Nesterovs method. ACMTransactions on Design Automation of Electronic Systems, 20(2):134, 2015. D. MacMillen, R. Camposano, D. Hill, and T. W. Williams. An industrial view of electronicdesign automation. IEEE Transactions on Computer-Aided Design of Integrated Circuits andSystems, 19(12):14281448, 2000.",
  "I. L. Markov, J. Hu, and M.-C. Kim. Progress and challenges in VLSI placement research. InProceedings of the 25th International Conference on Computer-Aided Design, pages 275282,San Jose, CA, 2012": "A. Mirhoseini, A. Goldie, M. Yazgan, J. W. Jiang, E. Songhori, S. Wang, Y.-J. Lee, E. Johnson,O. Pathak, A. Nazi, et al. A graph placement methodology for fast chip design. Nature,594(7862):207212, 2021. H. Murata, K. Fujiyoshi, S. Nakatake, and Y. Kajitani. VLSI module placement based onrectangle-packing by the sequence-pair. IEEE Transactions on Computer-Aided Design ofIntegrated Circuits and Systems, 15(12):15181524, 1996. G.-J. Nam, C. J. Alpert, P. Villarrubia, B. Winter, and M. Yildiz. The ISPD2005 placementcontest and benchmark suite. In Proceedings of the 9th International Symposium on PhysicalDesign, pages 216220, San Francisco, CA, 2005. Y. Pu, T. Chen, Z. He, C. Bai, H. Zheng, Y. Lin, and B. Yu. Incremacro: Incremental macroplacement refinement. In Proceedings of the 2024 International Symposium on Physical Design,pages 169176, 2024.",
  "Y. Shi, K. Xue, L. Song, and C. Qian. Macro placement by wire-mask-guided black-boxoptimization. In Advances in Neural Information Processing Systems 36, New Orleans, LA,2023": "P. Spindler and F. M. Johannes. Fast and accurate routing demand estimation for efficientroutability-driven placement. In Proceedings of the 14th Conference on Design, Automation &Test in Europe, pages 16, Nice, France, 2007. R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018. M. Tang and X. Yao. A memetic algorithm for VLSI floorplanning. IEEE Transactions onSystems, Man, and Cybernetics, 37(1):6269, 2007. D. Vashisht, H. Rampal, H. Liao, Y. Lu, D. Shanbhag, E. Fallon, and L. B. Kara. Placement inintegrated circuits using cyclic reinforcement learning and simulated annealing. arXiv preprintarXiv:2011.07577, 2020.",
  ": Illustration of chip canvas and calculation of HPWL": "HPWL (half perimeter wirelength) is an important metric which measures the placement qualitybefore routing. Intuitively, illustrates a 2D chip canvas where M i and P (i,j) denote thei-th module to adjust and its j-th pin, respectively. Solid boxes in green and purple represent thebounding boxes for two distinct nets on the canvas. In concretely, \"Net 1\"(in green) connects modulesM 1, M 2 and M 3 using wires through pins P (1,1), P (2,1) and P (3,1), while \"Net 2\"(in purple)",
  "B.1Ablation studies": "Purely comparison between placer and regulator. Here, we only change the problem formulationfor purely comparing placer and regulator. We implement Vanilla-MaskRegulate, where the onlydifference to MaskPlace is the problem formulation, and all the other components (e.g., state andreward) are the same. The results in clearly demonstrates our motivation, highlighting theadvantages of our regulator problem formulation. : Results of MaskPlace and Vanilla-MaskRegulate on four chips of ICCAD 2015 benchmarks.The only one difference between these two methods is the problem formulation, where all the othercomponents are the same. The best result on each chip is bolded.",
  "superblue15.77 0.053.31 0.005.82 0.073.42 0.06superblue37.05 0.033.54 0.006.71 0.033.59 0.01superblue44.15 0.062.18 0.023.99 0.022.49 0.10superblue56.94 0.004.23 0.017.03 0.044.26 0.01": "Training regularity-aware RL placer from scratch. Our proposed RegularMask and regularity-based reward function can also be used to train a RL placer from scratch.We implementMaskPlace+RegularMask and compare it with MaskPlace and MaskRegulate. The results show theadvantages of the integration of regularity (between MaskPlace and MaskPlace + RegularMask) andour RL regular formulation (between MaskPlace + RegularMask and MaskRegulate).",
  "B.2Comparison with ChiPFormer": "Recently, ChiPFormer incorporates an offline learning decision transformer to improve thegeneralizability. However, we find that even after fine-tuning for the same number of episodes asMaskRegulate, it is still challenging to achieve satisfactory results on ICCAD 2015. Due to theresource-intensive nature of fine-tuning it, we conducted only a partial set of experiments, which iswhy they were not included in the main paper. In the future, we plan to explore training a generalizedRegulator based on the transformer and pre-train it on some chips from ICCAD 2015, and compare itwith ChiPFormer that pre-train on the same training chips.",
  "B.3Experiments on ISPD 2005": "We test the generalization on the ISPD 2005 benchmark by directly using the pre-trained modelson superblue 1, 3, 4, and 5 (i.e., the same models in ) of MaskPlace and MaskRegulate toplace and regulate the eight chips. As shown in , MaskRegulate still outperforms MaskPlacein most cases, demonstrating our superior generalization ability and robustness.",
  "B.4Experiments on fine-tuning existing placement results": "To investigate whether MaskRegulate can be used to adjust any initial macro placement solution,we conduct additional experiments to demonstrate this capability. We used the pre-trained modelon superblue 1, 3, 4, and 5 (i.e., the same models in Tables 2 and 10) to adjust different placementresults obtained by MaskPlace, AutoDMP, and WireMask-EA. The results are shown in .MaskRegulate consistently improves regularity on all four unseen chips and enhances global HPWLon three chips. : Results of proxy metrics on four chips of ICCAD 2015 benchmarks. We use our policytrained on superblue1, superblue3, superblue4 and superblue5 to finetune the placements gainedfrom MaskPlace, AutoDMP and WireMask-BBO on superblue7, superblue10, superblue16 andsuperblue18. The left column indicates the Global HPWL (1e8) while the right column indicates theregularity (1e6). The best result of each metric on each chip is bolded.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We have provided our code in the supplemental file.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Experimental Setting/Details": "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]Justification: We have provided experimental details in .1 and Appendix A.Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to a level of detailthat is necessary to appreciate the results and make sense of them.",
  ". Experiment Statistical Significance": "Question: Does the paper report error bars suitably and correctly defined or other appropriateinformation about the statistical significance of the experiments?Answer: [Yes]Justification: We have reported the error bars in our experiments. Please see .Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  ". New Assets": "Question: Are new assets introduced in the paper well documented and is the documentationprovided alongside the assets?Answer: [Yes]Justification: We have provided our code and models in our supplymental file.Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of theirsubmissions via structured templates. This includes details about training, license,limitations, etc.",
  "According to the NeurIPS Code of Ethics, workers involved in data collection, curation,or other labor should be paid at least the minimum wage in the country of the datacollector": "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]Justification: This work does not involve crowdsourcing nor research with human subjects.Guidelines:",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}