{
  "Abstract": "Modern music retrieval systems often rely on fixed representations of user prefer-ences, limiting their ability to capture users diverse and uncertain retrieval needs.To address this limitation, we introduce Diff4Steer, a novel generative retrievalframework that employs lightweight diffusion models to synthesize diverse seedembeddings from user queries that represent potential directions for music ex-ploration. Unlike deterministic methods that map user query to a single point inembedding space, Diff4Steer provides a statistical prior on the target modality(audio) for retrieval, effectively capturing the uncertainty and multi-faceted natureof user preferences. Furthermore, Diff4Steer can be steered by image or text inputs,enabling more flexible and controllable music discovery combined with nearestneighbor search. Our framework outperforms deterministic regression methodsand LLM-based generative retrieval baseline in terms of retrieval and rankingmetrics, demonstrating its effectiveness in capturing user preferences, leading tomore diverse and relevant recommendations. Listening examples are available attinyurl.com/diff4steer.",
  "Introduction": "Modern retrieval systems , including those for music , often employ embedding-based dense retrieval system for candidate generation. These systems use a joint embeddingmodel (JEM) to obtain deterministic representations of queries, known as seed embed-dings, within a semantic space shared with the retrieval candidates. The seed embeddings providethe personalized starting point in the target embedding space for retrieving similar music via nearestneighbor search. While JEM-based system provides computationally efficient retrieval solution, they are insufficient inmodeling users diverse and uncertain retrieval preference. First, JEM only supports users expressingmusic preference or steer the retrieval results via specific modalities that the JEM is built on. Moreover,music discovery is an inherently ambiguous task with many possible outcomes there is no one-to-one mapping between the query and seed embedding given the large uncertainty of how a usersmusic preference can be fully specified. For example, energetic rock music could mean punk rockfor some, or hard rock for others. Modeling user preference using deterministic seed embeddingcan lead to monotonous and inflexible recommendations . In essence, for creative applications,it is crucial to explore users possible intentions (by allowing them to steer the retrieval results viainstructions) and to return relevant and diverse retrieval results.",
  ": Overall diagram of our generative retrieval framework for cross-modal music retrieval, withcomparison to the regression and multi-modal LLM baselines": "To better represent diversity and uncertainty in users retrieval preference, we introduce a novelframework Diff4Steer () for music retrieval that leverages the strength of generative modelsfor synthesizing potential directions to explore, The directions are implied by the generated seedembeddings: a collection of vectors in the music embedding space that represent the distributionof a users music preferences given retrieval queries. Concretely, our lightweight diffusion-basedgenerative models give rise to a statistical prior on the target modality in our case, audio for themusic retrieval task. Furthermore, the prior can be conditioned on image or text inputs, to generatesamples in the audio embedding space learned by the pre-trained joint embedding model. Theyare then used to retrieve the candidates using nearest neighbor search. Given that constructing alarge-scale dataset that contains the aligned multimodal data (steering info, source modality, targetmodality) is very difficult, we also leverage the diffusion models flexibility in sampling-time steeringto incorporate additional text-based user preference specifications. This eliminates the need forexpensive, data-hungry joint embedding training across all modalities. While we have seen that diffusion-based generative approaches can ensure diversity andquality in the embedding generation, in this work we investigate their performance on retrieval tasks.We demonstrate that our generative music retrieval framework achieves competitive retrieval andranking performance while introducing much-needed diversity. A comparison with deterministicregression methods shows that Diff4Steer achieves superior retrieval metrics. This is thanks to thehigher quality of the generated embedding, which reflects the underlying data distribution, as well asincorporating uncertainty in modeling user preferences.",
  "from t = 1 to 0 with noise schedule t and initial condition zm,1 N(0, t=1), using a first-orderEuler-Maruyama solver": "Classifier-free Guidance (CFG) is used to enhance the alignment of the sampled music embed-dings to the cross-modal inputs. During training, the condition q is randomly masked with a zerovector with probability pmask, such that the model simultaneously learns to generate conditional andunconditional samples with shared parameters. At sampling time, the effective denoiser D is anaffine combination of the conditional and unconditional versions",
  "where denotes the CFG strength, which boosts alignment with q when > 0. = 1.0 indicatesunconditional generation": "Additional text steering can be applied when the underlying music embedding space is that of atext-music JEM . In such case, the JEM provides a text encoder Et : T zt with a text-musicsimilarity measure via the vector dot product zt, zm. This allows us to incorporate (potentiallymultiple) text steering signals by modifying the denoising function at sampling time:",
  "Tasks and Datasets": "In our retrieval experiments, we use our diffusion prior model to perform several downstream taskssimultaneously, namely image-to-music retrieval, text-to-music retrieval and image-to-music retrievalwith text steering. For image-to-music tasks the query embedding is CLIP . For text-to-musicretrieval or text steering, text is encoded via MuLan text embedding and incorporated as a steeringcondition to steer the seed embedding generation using genre or music caption. YouTube 8M (YT8M) is a dataset originally developed for the video classification task, equippedwith video-level labels. We use the 116K music videos in this dataset to generate (music, image)pairs by extracting 10s audios and randomly sampling a video frame in the same time window. Thisdataset is primarily used for training. We use two other expert-annotated datasets for evaluation. First, MusicCaps (MC) is a collectionof 10s music audio clips with human-annotated textual descriptions. We extend the dataset with animage frame extracted from the corresponding music video. MelBench (MB) is another collectionof images paired with matching music caption and music audio annotated by music professionals.",
  "We use a 6-layer ResNet with width of 4096 as the backbone of the denoising model. For classifier-free guidance, we use a condition mask probability pmask = 0.1, in order to simultaneously learn": "the conditional and unconditional denoising model under shared parameters. We train the denoisingmodel on paired image and music embeddings from the YT8M music videos. We use the Adam optimizer under cosine annealed learning rate schedule with peak rate 105. Our model has282.9M parameters in total and can fit into one TPU. We train our model for 2M steps, which takesaround two days on a single TPU v5e device.",
  "MuLan. As a text-music JEM, MuLan enables text-to-music retrieval through a nearest neighborsearch based on the dot product similarity between a text query and candidate music embeddings": "Regression model. We train a regression baseline model that maps the query embeddings (CLIPimage embedding) to MuLan audio embeddings deterministically using the same architecture as thediffusion model (excluding noise). Multi-modal Gemini. The multi-modal Gemini serves as a strong baseline for our image-to-musicretrieval tasks. We leverage a few-shot interleaved multi-modal prompt that given an image it cangenerate image caption or matching music caption. Specifically, Gemini-ImageCap encodes thegenerated image caption into a MuLan text embedding for retrieving candidate audio embeddings.Gemini-MusicCap encodes the generated music caption into a MuLan text embedding for retrievingcandidate audio embeddings.",
  "Evaluation Metrics": "Embedding Quality. We use two metrics to measure the quality of generated music embeddings:Frchet MuLan Distance (FMD) and mean intra-sample cosine similarity (MISCS). FMD is inspiredby Frchet Inception Distance (FID) and measures the similarity of a set of generated musicembeddings to a population of real music embeddings in distribution. Music-image Alignment (M2I). Assessing alignment between generated music embeddings andinput images is challenging due to their distinct domains. Leveraging the shared text modality in CLIPand MuLan, we use text as a bridge for evaluating music-image (M2I) alignment following Chowdhuryet al. . This approach eliminates the need for paired data and instead requires a set of images anda separate set of texts. By encoding texts into both CLIP and MuLan embeddings, M2I is calculatedas the average of the product of two cosine similarities. Retrieval Metrics. We evaluate retrieval results using three metrics. First, we report recall@K(R@K), a standard metric in information retrieval. However, image-to-music or text-to-music retrievalis inherently subjective, often featuring one-to-many mappings. Thus, recall@K alone is insufficient,and we also report diversity using mean intra-sample cosine similarity (MISCS) and triplet accuracy(TA) to provide a more comprehensive evaluation.",
  "Quality of the Generated Seed Embeddings": "presents a comparison of embedding quality between Diff4Steer and the regression baselinefor both image-to-music and text-to-music tasks across multiple datasets. Results show that ourdiffusion prior model consistently exhibits significantly lower FMD, indicating higher quality andgreater realism in generated MuLan audio embeddings compared to the baseline. In addition, thediffusion model achieves significantly lower MISCS scores across all datasets, indicating that itallows us to generate diverse samples, which is impossible with a regression model. There is a dynamic relationship between classifier-free guidance (CFG) strength and the quality anddiversity of embeddings generated by our diffusion model. With a guidance strength of = 1.0,corresponding to unconditional samples, FMD initially deteriorates, then improves, and eventually : FMD and MISCS of the generated music embeddings for YT8M, MC and MB datasets(image2music). Across all the datasets, our diffusion model outperforms the deterministic model inboth embedding quality (FMD) and diversity (MISCS).",
  "Embedding-based Music Retrieval": "We show embedding-based music retrieval results in . The image CFG strength is an importanthyperparameter, and we tune it using the FMD score, based on the YT8M evaluation split. For theremaining evaluations in this paper, we set the image guidance strength to be 19.0. High-quality embeddings leads to high recall. A key finding from is that our Diff4Steermodel has significantly higher recall and triplet accuracy, compared to the regression and multi-modal Gemini baselines. This underscores the value of our approach for music retrieval applications.Notably, while the regression model has the highest M2I in the image-to-music task, it falls short instandard retrieval metrics. This observation, along with the FMD results in .1, highlights thecrucial role of high-quality seed embeddings in achieving optimal retrieval performance. Modality gap may harm retrieval results. For the multi-modal Gemini baselines, the image-to-music embedding generation is broken down to multiple stages. We use text (image or musiccaptions) as an intermediate modality, thereby introducing potential modality gap. As shown in, despite the power of the general-purpose LLMs, multi-modal Gemini baselines have worseretrieval performance than our Diff4Steer model, likely due to the loss of information with themodality gap. Additionally, our model offers a significantly lighter weight solution in terms oftraining consumption and latency compared to multi-modal foundation models. One model for all modality. Notably, our Diff4Steer model demonstrates competitive performanceon genre-to-music and caption-to-music retrieval tasks (the second and third groups in ) despitenot being trained on paired text and music data. This is achieved by unconditionally generatingaudio embeddings guided by text-music similarity. Compared to the regression baseline, Diff4Steerachieves superior results on most retrieval and ranking metrics, especially on the tasks that involvehigher retrieval uncertainty, e.g., genre-to-music retrieval. Text steering improves recall. Furthermore, we explore the extent to which text steering helps withretrieval. In addition to the image input, we provide our diffusion model with the genre label orground truth caption at inference time. The last group in shows that when steered with theadditional textual information, the models achieve significantly higher recall and triplet accuracy.",
  "Retrieval Diversity": "Diff4Steer generates diverse seed embeddings, as quantified in .For each image, wegenerate 50 seed embeddings and measure diversity using MISCS and entropy (H@K, withK {10, 20, 50}), calculated on the distribution of ground-truth genres in retrieved music pieces.Varying guided strengths during inference effectively modulates this diversity. Unconditional gen-eration ( = 1.0) yields the lowest MISCS and highest entropy in recommended genres. IncreasingGS initially decreases embedding diversity, with retrieval metrics peaking around = 9.0 beforedeclining. illustrates retrieval diversity using three representative input images. With strong image-music correspondence (Top), the entropy is notably lower, reflecting a dominant genre (Classical).Increasing image guidance further amplifies this effect. Conversely, weaker correspondences (Middle,Bottom) show varied entropy changes with increased guidance, sometimes resulting in a dominant",
  "Conclusion and limitations": "We introduce a novel generative music retrieval framework featuring a diffusion-based embedding-to-embedding model. By generating non-deterministic seed embeddings from cross-modal queries, ourapproach improves the quality and diversity of music retrieval results. Our model ensures semanticrelevance and high quality, while text-based semantic steering allows user personalization. Extensiveevaluations, including personalized retrieval experiments and human studies, show our methodssuperiority over existing alternatives. While promising, our framework has limitations as well. High computational demands of diffusionsampling may impede real-time retrieval, and any issues with pre-trained JEMs, such as informationloss or underrepresented items, naturally extend to our framework. Additionally, reliance on large,potentially biased training datasets may introduce biases into retrieval results. Future work shouldaddress these challenges to improve the retrieval effectiveness of music recommender systems. : Given an input image and various guided strengths (GS), we generate seed embeddings andretrieve their nearest music piece in MB. We show entropy and the probabilities of Top-3 genres. Ahigher entropy indicates more diverse music genres of retrieved music pieces.",
  "I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Interna-tional Conference on Learning Representations, 2016": "A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.In International conference on machine learning, pages 87488763. PMLR, 2021. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesiswith latent diffusion models. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1068410695, 2022.",
  "A.1.1Architecture": "Our diffusion backbone model is a ResNet model consisting of 6 ResNet blocks, followed by a finallinear projection layer. We incorporate the noise level by adding an adaptive scaling layer similarto Dhariwal and Nichol . The overall architecture is shown in , and detailed architectureof each ResNet block is shown in .",
  "tan(max) ,(6)": "where max = 1.5 and max = 100.0. This schedule is in essence linearly re-scaling the tan()function in domain [0, max] to fall within range [0, max]. It is similar to the shifted cosine scheduleproposed in (note that a tangent schedule in is equivalent to a cosine schedule in the 1/(2+1)).",
  "B.7Human Study Evaluation Metrics": "Relevance (REL) To evaluate the relevance of steered music to a given semantic concept, users areasked to compare the mood or style of a reference music piece to the steered piece on a 5-point scale.A score of 4 or 5 is considered a win for positive steering, while a score of 1 or 2 indicates success innegative steering. Consistency (CON) To assess the consistency of the overall theme and tone in steered music, userscompare it to a reference piece, rating their similarity on a 5-point scale. This score is then mappedto a 0-100 range to reflect the degree of consistency.",
  "ImageDescription": "[Image caption] Dynamic energy of a live concert, band membersengaging with the crowd, instruments at the ready. The stageglows under the spotlight, and the audience is a sea of faces, theair charged with anticipation and excitement.[Music caption] Electrifying guitar riff, powerful drums and bassline. [Image caption] Majestic grandeur of a snow-capped mountainrange under a clear blue sky. The rugged peaks rise sharply, theirjagged lines softened by blankets of pristine snow. Shadows andlight play across the slopes, creating a tapestry of blue and whitethat speaks to the silent power of nature.[Music caption] Orchestral score with string section. [Image caption] Dynamic interplay of light and reflection as adisco ball casts a constellation of red dots across a dark expanse.The mirrored surface fragments the light, creating a pattern thatsuggests both the energy of a dance floor and the cosmic expanseof a starlit sky.[Music caption] Rhythmic, upbeat electronic dance track. [Image caption] A solitary figure stands enveloped by the quietudeof a vibrant garden, basking in the gentle embrace of sunlight.She seems to be in a moment of tranquil reflection, as the worldaround her bursts with the life of untamed blooms and the softwhisper of leaves in the breeze. It is an image of peaceful solitude,where the clamor of the world falls away before the simple purityof natures own artistry.[Music caption] Soft piano melody with a gentle cello. [Image caption] Commanding figure on horseback, steeped inthe iconography of power and leadership. The rider, cloaked ina flowing cape, gestures assertively forward, a symbol of boldambition. The rearing horse adds to the dramatic intensity, withmuscular detail and a mane tossed by the vigor of movement.The backdrop is sparse, the sky subdued, focusing the viewersattention on the figure that dominates the scene, a representationof determination and the forging of destiny.[Music caption] Epic battle atmosphere, full orchestra, strongbrass section and rolling drums.",
  "DMore Alignment Analysis": "reports image-to-music alignment results on the extended MusicCaps and the MelBenchdatasets. For the deterministic baseline and our approach, the output is audio embedding. Thereference is the alignment metric values with the ground truth MuLan audio embeddings. While forthe Gemini baselines, the output is text embedding. The reference is the alignment metric valueswith the ground truth MuLan text embeddings. Compared to the Gemini baseline, our approach gets higher music-music and music-image alignmentscore. While the Gemini image caption + MuLan baseline has better music-caption alignment score.To our surprise, the deterministic baseline gets highest alignment score on the MelBench dataset,",
  "EAdditional evaluation results": "For quality of generated music embeddings, we show FMD and MISCS plots as functions of theimage CFG strengths in and 7. shows how the recall of music retrieval is affectedby the image CFG strength. and 10 show how the recall is affected by the text steering orspherical interpolation strengths. Guidance strength 0.15 0.16 0.17 0.18 0.19 0.20 0.21 0.22 0.23 FMD Guidance strength 0.4 0.5 0.6 0.7 0.8 Intra-sample cosine similarity"
}