{
  "Abstract": "Humans excel at detecting and segmenting moving objects according to the Gestaltprinciple of common fate. Remarkably, previous works have shown that humanperception generalizes this principle in a zero-shot fashion to unseen textures orrandom dots. In this work, we seek to better understand the computational basisfor this capability by evaluating a broad range of optical flow models and a neuro-science inspired motion energy model for zero-shot figure-ground segmentationof random dot stimuli. Specifically, we use the extensively validated motion en-ergy model proposed by Simoncelli and Heeger in 1998 which is fitted to neuralrecordings in cortex area MT. We find that a cross section of 40 deep optical flowmodels trained on different datasets struggle to estimate motion patterns in randomdot videos, resulting in poor figure-ground segmentation performance. Conversely,the neuroscience-inspired model significantly outperforms all optical flow modelson this task. For a direct comparison to human perception, we conduct a psy-chophysical study using a shape identification task as a proxy to measure humansegmentation performance. All state-of-the-art optical flow models fall short ofhuman performance, but only the motion energy model matches human capability.This neuroscience-inspired model successfully addresses the lack of human-likezero-shot generalization to random dot stimuli in current computer vision models,and thus establishes a compelling link between the Gestalt psychology of humanobject perception and cortical motion processing in the brain.Code, models and datasets are available at",
  "Introduction": "Motion is a powerful cue that humans use to detect and segment visual objects. A striking exampleare camouflaged animals, which are difficult to spot when stationary but become much easier to detectwhen moving. Motion segmentation in humans is believed to be driven by the principle of commonfate , which posits that elements that move together, belong together. Remarkably, humanperception generalizes this principle in a zero-shot fashion to novel textures or moving random dots.For example, the seminal work by Johansson showed that humans can easily detect biologicalmotion from only few moving dots. More recently, Robert et al. introduced random dot stimulicalled object kinematograms that preserve the motion in a video while ensuring that static appearancecues are uninformative about the video contents (, example video in the supplemental material).",
  "arXiv:2411.01505v1 [cs.CV] 3 Nov 2024": ": We compare state-of-the-art optical flow estimators and a neuroscience inspired motionenergy model on a figure-ground segmentation task. For evaluation, we use random dot stimuli withthe same motion patterns as the original videos, but for which the appearance of each individualframe is informative (example video in the supplemental material). The neuroscience inspired modelgeneralizes to these stimuli much better than state-of-the-art optical flow models.",
  "Nevertheless, humans were able to classify the animals and objects in these videos based on motioninformation alone": "In this work, we seek to understand the computational basis for appearance-agnostic motion perceptionin humans which enables this zero-shot generalization to random dots. Recent advancements incomputer vision models for motion segmentation enable accurate segmentation of moving objectsin natural videos based on a combination of optical flow estimation networks with downstreamsegmentation networks (e.g., ). However, it remains untested whether these models generalizein a similar way as human perception. Since the motion estimation stage is critical for segmentingmoving objects, we focus on testing a broad range of state-of-the-art optical flow methods incombination with a fixed segmentation network. Our analysis reveals that existing computer visionapproaches do not generalize in a human-like manner: Many high-performing models on naturalstimuli perform near chance level for random dots. In the primate visual cortex, area MT is known to be involved in motion perception and interpretation.Computational models for this area based on motion energy were proposed almost forty years ago and since then have been shown to predict key characteristics of neural firing patterns .Instead of matching deep features between two frames, these models rely on spatio-temporal filteringin pixel space combined with a post-processing stage to resolve ambiguities. We demonstrate thatthis mechanism can be successfully integrated with deep neural networks for motion segmentationin realistic videos and reaches the performance of early deep learning based optical flow models onthe original, textured videoswhich is remarkable considering that the motion energy model wasdeveloped to explain the tuning of individual neurons and has several orders of magnitude fewerparameters than typical optical flow networks. Crucially, the motion energy model substantiallyoutperforms all tested optical flow models in zero-shot generalization to moving random dots. Ina direct comparison with humans in a controlled psychophysics study, the motion energy basedapproach is the only model that can match human capability.",
  "We demonstrate that a classical neuroscience model can be successfully integrated with deepneural networks and generalizes to random dot stimuli": "We conduct a psychophysical experiment to directly compare random dot motion segmenta-tion in humans and machines. While state-of-the-art optical flow models fall short of humanperformance, the motion energy model can match it. These results establish a compelling link between the Gestalt psychology of human object perceptionand cortical motion processing in the brain, showing that a motion energy approach can overcome thelack of human-like zero-shot generalization to random dot stimuli in current computer vision models.Integrating this mechanism with state-of-the-art optical flow methods is promising path towards morerobust motion estimation models.",
  "Related Work": "Motion energy.Modelling motion perception in humans has been frequently approached usingmotion energy models. These models exploit the fact that a moving pattern corresponds to orientededges when considering a video as a spatio-temporal volume . Several models have beenproposed that build on this principle, aiming to explain the tuning properties of neurons found invisual areas V1 and MT . With few exceptions , these models have not beenused as a motion estimation models in a computer vision context. Our work is the first to studymotion energy models for moving object segmentation. Optical flow estimation.Optical flow traditionally has been formulated as an optimization problemwith the goal of finding good matches between two frames . During recent years, optimizationbased methods have been superseded by deep neural networks that frame optical flow estimation asan end-to-end regression task. FlowNet pioneered this approach with a CNN that optionallyincludes an explicit temporal matching operation. Following works contributed better training dataand proposed coarse-to-fine architectures to predict optical flow which lead to substantialperformance improvements. More recently, models that iteratively refine a high resolution opticalflow map and Transformer-based models have further improved state-of-the-art.Some works have compared optical flow models to human motion perception , however notin the context of motion segmentation. Motion Segmentation.The typical approach to motion segmentation is using optical flow asinput for a downstream segmentation model. One classic line of work computes point trajectoriesfrom optical flow and then clusters the trajectories to segment moving regions . Classicalgeometric approaches to motion segmentation have been combined with deep learning in later work. More recently, purely deep learning based approaches have been able to improve state-of-the-art . To achieve high performance on classical motion segmentation datasets, theoptical flow based motion segmentation is typically combined with appearance based segmentation. In this work, we evaluate generalization to random dot stimuli for which appearance is notinformative, so we focus on purely motion-driven approaches.",
  "Methods": "The aim of this work is to evaluate which computational models match the capabilities of humans forzero-shot motion segmentation of random dot patterns. We follow the standard motion segmentationapproach in computer vision and first use a motion model to estimate the motion in an input video,followed by a segmentation network that predicts the foreground mask. In order for models toperform well on zero-shot segmentation of random dot patterns, it is critical that the motion estimatorused by the model generalizes well to these random dot stimuli. Ideally, the motion estimator wouldbe invariant to changes in texture. Therefore, we focus on the motion estimation stage by evaluatinga broad range of optical flow models in comparison to a neuroscience inspired motion energy model.As a segmentation model, we use the same segmentation architecture for all motion estimators whichwe train from scratch for every model.",
  "Optical Flow Models": "We use a range of optical flow models that includes all major deep learning based approaches to opticalflow estimation. FlowNet 2.0 was the first CNN based model that reached the performanceof classical, optimization based methods. We consider three variants of the model using differentcombinations of subnetworks. PWC-Net introduced a multi-scale approach that combinedoperations from classical approaches (such as cost volumes and warping), with components fromdeep learning. Different from previous models, RAFT is not based on a coarse-to-fine approachbut rather on iterative refinement of a high resolution optical flow map derived from multi-scalecorrespondences. GMA extends the RAFT architecture by introducing a Transformer-basedmodule to better handle occlusions, which have been shown to be difficult for previous models. Morerecently, GMFlow and FlowFormer++ have been proposed as fully Transformer-basedarchitectures for optical flow estimation. We use the implementations and checkpoints of these models from the MMFlow library , exceptfor FlowFormer++ and GMFlow for which we use the implementations and checkpoints provided bythe respective authors12. For each architecture, we consider checkpoints trained on different datasetsthat are common in the field, including the FlyingChairs , FlyingThings3d , Sintel andKITTI . In total, we evaluate 40 optical flow models. We apply the models to predict multi-scale optical flow, in order to match the multi-scale featurespredicted by the motion energy model. All of the optical flow models internally use several scales topredict optical flow. However, this representation is followed by non-trivial processing to combinemotion information across scales, so that using this internal representation directly would most likelylead to inferior performance. Therefore we use the unmodified models and scale the final optical flowprediction to the desired resolutions using bilinear interpolation.",
  "Motion Energy Model": "Motion energy models are based on the insight that a motion pattern in a video corresponds to aspatio-temporal orientation when the video is considered as an x-y-t volume . The motionat every pixel can therefore be estimated by using spatiotemporal filters that respond to a particularmotion direction and speed. This mechanism has important differences from the optical flow modelsdiscussed before. All of the optical flow models compute deep features for two frames individuallyand match these features between two frames to estimate motion. The spatio-temporal filters inmotion energy models on the other hand operate directly in pixel space. This approach leads tomore ambiguous matches, which are typically resolved by considering more than two frames, and apostprocessing stage. In this study, we build on the influential motion energy model by Simoncelli & Heeger . Inaddition to the oriented filters described above, this model introduced a second stage that implementsan intersection of constraints construction in order to resolve ambiguities of the linear filterresponses. This motion energy model can be implemented as a CNN with the architecture shownin . We derived the weights of the CNN from the the parameters of the original modeland verified that our PyTorch implementation of the motion energy model equals the originalMATLAB implementation3 up to numerical differences. Following the original model, we applythe model for five different input scales that are obtained by repeatedly blurring and downsamplingthe input by a factor of two. To streamline the implementation, we do not scale the activations afterevery layer and experimentally verified that this change does not affect downstream performance formotion segmentation.",
  "Segmention model": "We use a coarse-to-fine segmentation network to predict per-pixel logits for the respective pixelbelonging to the foreground object (). Input to the segmentation model are the multi-scalemotion energy maps or multi-scale optical flow maps as predicted by the models described earlier.At each scale, the segmentation model consists of three components: The input projection layerpredicts motion features for each scale. The core of the network is a refinement CNN that aggregatesfeatures across scales. At each scale, the refinement CNN concatenates the motion features fromthe current scale with the refined representation from all previous scales and predicts the refinedrepresentation for the current scale. Finally, the output projection layer predicts the segmentationgiven the refined representation from the finest scale. All layers except for the output projectionare followed by a CELU nonlinearity and instance normalization . The parameters of thecomponents are shared across the stages, so that the network is essentially a recurrent neural networkthat integrates information from coarsest to the finest scale in order to predict a segmentation.",
  "~lcv/MTmodel/": ": (top) Our motion segmentation architecture: The motion estimation predicts multi-scaleoptical flow or motion energy, the segmentation model predicts the moving foreground region.(bottom left) The motion energy model is implemented as a CNN. The weights are chosen suchthat the CNN is equivalent to the original model by . (bottom right) The segmentation modelcombines motion features across scale and predicts a binary segmentation at the input resolution. backgrounds used for dataset generation are scans of everyday objects and scenes, resulting in highlyrealistic renderings. We used 901 videos for training and 100 test videos, each having 90 frames at30Hz. The training and test videos used different sets of object and backgrounds but are otherwisesampled from the same distribution. The code and hyperparameters for generating videos, as well asthe rendered dataset, are publicly available4. For all models, we freeze the weights of the motion estimator and only train the downstreamsegmentation network. As common for binary motion segmentation, we use per pixel binary crossentropy to the ground truth masks as loss. We use the Adam optimizer with a learning rateof 1e 4 for all models and train for 40.000 steps using a batch size of 8. All models are trainedon NVIDIA GeForce RTX 2080 Ti GPUs with 12GB of VRAM. Depending on the computationalrequirements of the motion model, training the segmentation model on a single GPU takes between 2and 6 hours.",
  "Zero-shot evaluation on random dot stimuli": "We evaluate models on the original test videos as well as random dot stimuli generated for all testvideos based on the ground truth optical flow. We use the same procedure as for generatingrandom dot stimuli using 500 dots with a lifetime of 8 frames, which matches the dot density andlifetimes. We apply all models using a shifting window approach for the full length videos, but excluding thefirst and last four frames so that the window is fully contained within the video for all models. Forevaluation, we obtain a binary prediction by thresholding with 0.5 and measure performance bycomputing IoU and F-Score for each frame individually and then averaging over the test set.",
  "OriginalRandom DotsMotion EstimatorTraining DatasetIoUF-ScoreIoUF-Score": "Motion Energy (ours)-0.7590.8450.6000.718FlowNet2 SDFlyingChairs0.8780.9280.2210.325FlowNet2FlyingChairs0.8080.8680.2090.300FlyingThings3D0.8810.9290.0580.100PWC-NetFlyingChairs0.8160.8860.1630.250FlyingThings3D0.8250.8860.1370.221KITTI0.7120.8110.0380.060RAFTFlyingThings3D + Sintel0.9120.9480.1560.222FlyingChairs0.8630.9140.1260.195Mixed0.8960.9340.1170.164FlyingThings3D0.8940.9340.0900.132KITTI0.7140.7940.0310.053FlowNet2 CSFlyingChairs0.8410.8990.1370.220FlyingThings3D0.8470.9040.0750.129GMA (+P)FlyingChairs0.8560.9120.1320.212Mixed0.9000.9360.1140.179FlyingThings3D0.8990.9360.1040.171GMAFlyingChairs0.8640.9170.1310.212Mixed0.9000.9370.0900.139FlyingThings3D + Sintel0.9090.9430.0660.100FlyingThings3D0.9030.9430.0600.098KITTI0.7560.8340.0510.084GMA (P-only)FlyingChairs0.8460.9010.1280.207KITTI0.7660.8470.0920.155FlyingThings3D0.9030.9400.0830.139Mixed0.9120.9470.0770.117FlowNet2 CSSFlyingChairs0.8500.9080.0840.141FlyingThings3D0.8620.9180.0700.121 : Ablation study: We apply the optical flow estimators to a window of 9 frames by using thecentral frame as references and computing optical flow to each of the 8 other frames. The stackedoptical flow fields are used as inpute for the segmentation network. of the art OCLR model in our setting. The OCLR model uses optical flow estimated by RAFT, which we also included in our experiments. The segmentation network however uses a U-Netarchitecture with Transformer bottleneck and was trained to segment multiple objects on a syntheticdataset. We use the published weights and do not retrain the model on our data. The results in show that the model performs very well on the original data. OCLR outperformsour motion energy based model and achieves a performance similar to the best optical flow basedmodels considered in this work. At the same time, the model does not generalize to the correspondingrandom dot stimuli. These results provide further evidence that the low generalization to random dotsis not due to the architecture of the segmentation network or the RGB training data, but a property ofthe motion estimator.",
  "Recent optical flow methods perform strongly on the original videos. FlowFormer++ works beston our dataset with an IoU of 90.8%, closely followed by a GMA variant that reaches 89.5% IoU": "These results parallel the strong performance of recent Transformer-based architectures on standardoptical flow benchmarks. The motion energy based model only achieves a performance of 75.9%IoU and lags behind state-of-the-art optical flow models, but performs similar as earlier deep learningbased optical flow models. This result is remarkable when considering that the motion energy modelpredates the deep learning models by several decades and has not been tuned for dense, end-to-endmotion prediction. Within each model, the checkpoints from the FlyingThings3d dataset tend toperform best for the original videos. The FlyingThings3d dataset contains renderings of 3D objectsundergoing rigid motion, so arguably it is the most similar dataset compared to the one used in thisstudy. Motion energy generalizes much better to random dots. The motion energy based model reachesan IoU of 60.0%, which outperforms the performance of the second best model by more than 20percentage points. Strikingly, the FlowFormer++ and GMA models that performed best on theoriginal videos generalize particularly bad to the random dot stimli (IoU < 10%). Overall, moredated optical flow architectures such as FlowNet2 variants and PWC-Net tend to generalize betterto random dot stimuli than more recent approaches. An interesting exception is GMFlow, whichreached an IoU of 38.1% and performed best among all optical flow models. We do not observe aclear effect of the training dataset. We visualize model predictions in . For the original videos, the quality of the predictedoptical flow varies but allows for a clear segmentation of the moving object. The object is also clearlyrepresented in the motion energy maps, with some feature maps responding highly to the backgroundand others to the moving object. The motion energy maps however tend to be noisier than the opticalflow predictions, which explains the lower performance of the motion energy model for the cleanvideos. The random dot stimuli exhibits the same motion as the original video, so the prediction of an idealmotion estimator would be unchanged. The optical flow methods however fail to properly estimatethe motion of the foreground object. While some methods like FlowNet 2.0 and PWC-Net predict ahighly noisy motion pattern that roughly matches the location of the foreground object, many opticalflow estimators fail to detect the foreground motion at all. The motion energy on the other handlooks highly similar for the random dot stimulus and the original video, allowing the motion energysegmentation to generalize well in this case.",
  "Ablation study": "As an ablation study, we evaluated whether the performance of the motion energy segmentation modelcan be improved by learning the parameters of the motion model. We tested different combinationsof layers in the motion energy CNN that are fixed, finetuned or trained from scratch and trained themend-to-end with the segmentation model. The results in table 2 show that the original weights of the model allow for the best generalizationto random dots. This is remarkable when considering that the weights of the motion energy modelhave been originally selected to explain the tuning properties of individual neurons, but not forimage-computable motion estimation. Some of the configurations however outperformed the originalweights on the original videos. So while the network architecture allows for generalization inprinciple, all our models trained by gradient descent converged to solutions that performed well onthe training data but did not generalize. As a further ablation study, we removed or replaced layers of the motion energy model. The resultsin the supplemental information suggest that the pooling and normalization layers are particularlyimportant for generalization to random dots. More details and further experiments are provided in thesupplemental information.",
  "Human Machine Comparison": "The previous results have revealed differences between different motion estimation models in termsof generalization to random dots. While it is known that humans can recognize objects in random dotstimuli without prior training , the ability to segment objects in moving random dot patterns hasnot been quantified before. We therefore conduct a human subject study in order to directly comparethe zero-shot generalization to random dots in humans and machines. : Example predictions for different motion estimators. The motion pattern in the random dotstimulus is the same as in the original video. While the optical flow estimates are highly accurate forthe original videos, the models struggle with the random dot stimuli that exhibit the same motion.The activations of the motion energy model model however generalize well to the random dot stimuli,enabling to detect and segment the foreground object.",
  ": Comparison of using the original weights (fix), finetuning the original weights (finetung) ortraining from scratch (scratch) for the layers of the motion energy model": "Due to the inherent difficulty in directly evaluating the segmentation perceived by humans, weemployed a shape identification task as a surrogate requiring segmentation (). Each trialinvolved a random target shape and a distractor shape from the Infinite dSprites dataset . Therandom dot stimulus shows the target shape moving linearly across the center of the image withrandom motion direction and speed. After the video concluded, participants were shown cleanrenderings of the target and distractor shapes and were required to select the shape that they perceivedin the random dot stimulus. Since the shape alternatives were unknown while the random dot stimuluswas shown, participants had to segment and memorize the shape in the random dot video and thencompare it to the shape choices afterward. Therefore, performing well on this task necessitatessufficiently good segmentation of the moving shapes within the motion patterns of the random dotstimuli. : We compare humans and machines using a random dot shape identification task as a proxyto measure segmentation in humans. Shown a video of random dots, participants have to respondwhich of two shapes was perceived in the video. Humans outperformed all optical flow based models,but not the motion energy based model for this task. More details are provided in the supplementalmaterial. We performed the study in a controlled vision lab environment, where participants viewed theexperiment on a VIEWPixx 3D LCD monitor (1920x1080, 120Hz) with the distance fixed to 65cmusing a chin rest. The duration of all videos was 1s at a framerate of 30 Hz. Overall, we collected datafrom N=13 subjects, of which we excluded one subject due to insufficient visual acuity (remaining:N=12, 4 female, 8 male). Among the subjects where both trained vision scientists and naive subjects. We evaluated all models on the same stimuli as human subjects. Given a random dot video, weapplied the respective model to segment the video and selected the shape option that better matchedthe prediction as measured by IoU. The results in show that all models based on optical flow are clearly outperformed byhumans. Many of the optical flow based models perform near chance level, while some modelsreached a non-trivial performance. Overall, more recent optical flow models that perform very wellon the original videos appear to generalize worse to this task, with GMFlow being a noteableexception. Different from the optical flow models, the motion energy based approach is the onlymodel to match and even outperform human performance. More detailed results in the supplementalinformation show that the motion energy segmentation model performs on par with the highestperforming participants of the study.",
  "Limitations": "To allow for comparing a large number of motion estimation models with a reasonable computationalbudget we made compromises for other modeling aspects. We limited the size of the segmentationnetwork to allow for efficient training but performed a control experiment to show that using a moresophisticated segmentation network does not improve generalization (see supplemental information).Moreover, we used the same training schedule for all models but ensured that our setting supports allmodels adequately by visually inspecting the loss curves. When comparing humans and machines we did not model several factors that are expected to influencehuman performance, such as the impact of internal noise and attentional lapses. As common inpsychophysics experiments, several subjects reported making accidental errors for few examples which negatively affects performance. So even a model that perfectly replicates the motionprocessing algorithm in humans is not expected to perfectly replicate human behavior in our setting.",
  "Broader impact": "This work is highly interdisciplinary, bridging state-of-the-art computer vision motion segmentationalgorithms with the principles of Gestalt psychology and the neuroscience of cortical motion pro-cessing in the brain. By showing that a neuroscience-inspired motion energy model can outperformconventional optical flow models in zero-shot generalization to random dot stimuli, the study high-lights the potential for integrating biological insights into AI systems. Benefits of broader impactinclude the development of more robust and human-like AI systems, educational value, and thecreation of AI systems that are more aligned with human cognition.",
  "Discussion": "Computational models for motion estimation have a long history in both computational neuroscienceand computer vision. Shallow models based on spatio-temporal filtering in pixel space have beenable to predict neural activity in brain areas related to motion perception and are compatiblewith a range of phenomena in human perception . In computer vision, models based on matchingdeep features between two frames have continuously improved performance over the last years andare successfully applied in a range of downstream tasks. Despite these successes, our study reveals astriking gap between deep optical flow networks and human perception: While humans generalizethe common fate Gestalt principle to zero-shot segmentation of random moving dots, the opticalflow models fail to generalize to these stimuli. Furthermore, we show that a classic motion energyapproach can be scaled to realistic videos while matching human generalization capabilities. The great success of deep neural networks in computer vision has spawned interest in using DNNsalso as a model for human vision, in particular for core object recognition . In the same spirit,deep neural networks might be promising models for human motion perception . While promising,our study parallels findings for core object recognition that show striking differences between humanperception and DNNs . For motion perception, however, we show that is possible to combineclassical models from computational neuroscience with the scalability of deep learning. Furtherintegration of these modeling traditions is a promising path towards image computable models ofhuman motion perception . While closing the gap between human perception and machine vision is crucial for computationalneuroscience, we believe that computer vision likely profits from better alignment with humanvision as well. Humans still greatly outperform machines in terms of robustness and efficiency.Our study suggests a substantial entanglement of motion estimation with appearance in DNNs,which might also be linked to the lack of robustness observed in state-of-the-art motion estimators. Computational principles that better match human vision should be considered as promisingcandidates for addressing these issues. Finally, we argue that deep learning based models as presented in our work have the potential togreatly improve our understanding of motion perception in humans. Low-level mechanisms formotion estimation and higher level processes for motion interpretation have been mostly studiedin isolation . In our work we follow a more holistic approach by studying the effects motiondetection mechanisms on the perception of moving objects, which offers several unique opportunities.First, it is not necessary for most downstream tasks to perfectly estimate the physically correctmotion. For example, segmenting moving objects does require precise information about objectboundaries while other mistakes are less critical. Studying motion estimation and interpretation jointlyallows to better understand viable compromises in estimation accuracy as the basis for more efficientprocessing. Second, studying end-to-end models of motion estimation and interpretation advances ourunderstanding of how neural mechanisms give rise to behavior. DNNs are a particularly promisingmodeling approach positioned in a Goldilocks zone regarding the trade-off between biologicalplausibility and scalability to natural stimuli and tasks . In this vein, our work establishes acompelling link between cortical mechanisms for motion estimation and the Gestalt psychology ofhuman object perception. In the future, this work can be extended in several directions. While scaling remarkably well, theoriginal motion energy model is not able to match the performance of state-of-the-art optical flowmethods on natural scenes. We see integrating principles from computational neuroscience withtechniques from deep learning as a promising path towards closing this gap . Moreover, trainingthe parameters of the CNN implementation of the motion energy model jointly with the segmentationmodel did not lead to a generalizable solution. How humans learn generalizable motion perceptionfrom data, or to which degree this capability is innate, are important questions for future research.Finally, in the spirit of the neuroconnectionist research programme we see our model as anexecutable hypothesis for motion perception in the human brain. While matching human performancein terms of generalization to moving random dots, this model might well fail to capture other aspectsof human motion perception. Further evaluating and extending models of motion perception tocapture a diverse range of phenomena is an exciting path towards a holistic understanding of humanperception.",
  "and Disclosure of Funding": "This work was supported by the German Research Foundation (DFG): SFB 1233, Robust Vision:Inference Principles and Neural Mechanisms, TP 4, project number: 276693517. The authors thankthe International Max Planck Research School for Intelligent Systems for supporting MT. We thankFelix Wichmann, Thomas Klein and all other members of the Wichmann-Lab for supporting andtesting the human machine comparison study, and Larissa Hfling for valuable feedback on themanuscript.",
  "J. T. Barron.Continuously Differentiable Exponential Linear Units.arXiv preprintarXiv:1704.07483, April 2017. doi: 10.48550/arXiv.1704.07483": "P. Bideau and E. Learned-Miller. Its Moving! A Probabilistic Model for Causal MotionSegmentation in Moving Camera Videos. In Computer Vision ECCV 2016, pages 433449,Cham, October 2016. Springer International Publishing. doi: 10.1007/978-3-319-46484-8_26. P. Bideau, A. RoyChowdhury, R. R. Menon, and E. Learned-Miller. The Best of Both Worlds:Combining CNNs and Geometric Constraints for Hierarchical Motion Segmentation. In Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages508517, June 2018. T. Brox and J. Malik. Object Segmentation by Long Term Analysis of Point Trajectories. InComputer Vision ECCV 2010, volume 6315 of Lecture Notes in Computer Science, pages282295, Berlin, Heidelberg, September 2010. Springer. doi: 10.1007/978-3-642-15555-0_21. D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A Naturalistic Open Source Moviefor Optical Flow Evaluation. In Computer Vision ECCV 2012, volume 7577 of LectureNotes in Computer Science, pages 611625, Berlin, Heidelberg, October 2012. Springer. doi:10.1007/978-3-642-33783-3_44.",
  "A. Dave, P. Tokmakov, and D. Ramanan. Towards Segmenting Anything That Moves. InProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops,October 2019": "A. Doerig, R. P. Sommers, K. Seeliger, B. Richards, J. Ismael, G. W. Lindsay, K. P. Kording,T. Konkle, M. A. J. van Gerven, N. Kriegeskorte, and T. C. Kietzmann. The neuroconnectionistresearch programme. Nature Reviews Neuroscience, 24(7):431450, July 2023. doi: 10.1038/s41583-023-00705-w. A. Dosovitskiy, P. Fischer, E. Ilg, P. Hausser, C. Hazirbas, V. Golkov, P. van der Smagt,D. Cremers, and T. Brox. FlowNet: Learning Optical Flow With Convolutional Networks.In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages27582766, December 2015. S. Dziadzio, . Yldz, G. M. van de Ven, T. Trzcinski, T. Tuytelaars, and M. Bethge. InfinitedSprites for Disentangled Continual Learning: Separating Memory Edits from Generalization.In 3rd Conference on Lifelong Learning Agents (CoLLAs), July 2024.",
  "C. L. Fennema and W. B. Thompson. Velocity determination in scenes containing severalmoving objects. Computer Graphics and Image Processing, 9(4):301315, April 1979. doi:10.1016/0146-664X(79)90097-2": "K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam,F. Golemo, C. Herrmann, T. Kipf, A. Kundu, D. Lagun, I. Laradji, H.-T. D. Liu, H. Meyer,Y. Miao, D. Nowrouzezahrai, C. Oztireli, E. Pot, N. Radwan, D. Rebain, S. Sabour, M. S. M.Sajjadi, M. Sela, V. Sitzmann, A. Stone, D. Sun, S. Vora, Z. Wang, T. Wu, K. M. Yi, F. Zhong,and A. Tagliasacchi. Kubric: A Scalable Dataset Generator. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pages 37493761, June 2022. D. J. Heeger, E. P. Simoncelli, and J. A. Movshon. Computational models of cortical visualprocessing. Proceedings of the National Academy of Sciences, 93(2):623627, January 1996.doi: 10.1073/pnas.93.2.623.",
  "B. K. P. Horn and B. G. Schunck. Determining optical flow. Artificial Intelligence, 17(1):185203, August 1981. doi: 10.1016/0004-3702(81)90024-2": "E. Ilg, N. Mayer, T. Saikia, M. Keuper, A. Dosovitskiy, and T. Brox. FlowNet 2.0: Evolutionof Optical Flow Estimation With Deep Networks. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR), pages 24622470, July 2017. S. Jiang, D. Campbell, Y. Lu, H. Li, and R. Hartley. Learning To Estimate Hidden MotionsWith Global Motion Aggregation. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pages 97729781, October 2021.",
  "H. Lamdouar, W. Xie, and A. Zisserman. Segmenting Invisible Moving Objects. In 32nd BritishMachine Vision Conference (BMVC), November 2021": "N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. A LargeDataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Esti-mation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 40404048, June 2016. M. Menze, C. Heipke, and A. Geiger. JOINT 3D ESTIMATION OF VEHICLES AND SCENEFLOW. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,II-3/W5:427434, September 2015. doi: 10.5194/isprsannals-II-3-W5-427-2015.",
  "M. Menze, C. Heipke, and A. Geiger. Object Scene Flow. ISPRS Journal of Photogrammetryand Remote Sensing, 140:6076, June 2018. doi: 10.1016/j.isprsjprs.2017.09.013": "S. Nishida, T. Kawabe, M. Sawayama, and T. Fukiage. Motion Perception: From Detectionto Interpretation. Annual Review of Vision Science, 4(1):501523, September 2018. doi:10.1146/annurev-vision-091517-034328. S. Nishimoto, A. T. Vu, T. Naselaris, Y. Benjamini, B. Yu, and J. L. Gallant. ReconstructingVisual Experiences from Brain Activity Evoked by Natural Movies. Current Biology, 21(19):16411646, October 2011. doi: 10.1016/j.cub.2011.08.031. P. Ochs and T. Brox. Object segmentation in video: A hierarchical variational approach forturning point trajectories into dense regions. In 2011 International Conference on ComputerVision, pages 15831590, November 2011. doi: 10.1109/ICCV.2011.6126418. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An Imperative Style,High-Performance Deep Learning Library. In Advances in Neural Information ProcessingSystems, volume 32. Curran Associates, Inc., December 2019. S. Robert, L. G. Ungerleider, and M. Vaziri-Pashkam. Disentangling Object Category Represen-tations Driven by Dynamic and Static Visual Input. Journal of Neuroscience, 43(4):621634,January 2023. doi: 10.1523/JNEUROSCI.0371-22.2022.",
  "J. Schmalfuss, L. Mehl, and A. Bruhn. Attacking Motion Estimation with Adversarial Snow. InECCV 2022 Workshop on Adversarial Robustness in the Real World, October 2022": "J. Schmalfuss, P. Scholze, and A. Bruhn. A Perturbation-Constrained Adversarial Attackfor Evaluating the Robustness of Optical Flow. In Computer Vision ECCV 2022, pages 183200, Cham, October 2022. Springer Nature Switzerland. doi: 10.1007/978-3-031-20047-2_11. H. H. Schtt, S. Harmeling, J. H. Macke, and F. A. Wichmann. Painfree and accurate Bayesianestimation of psychometric functions for (potentially) overdispersed data. Vision Research, 122:105123, May 2016. doi: 10.1016/j.visres.2016.02.002. X. Shi, Z. Huang, D. Li, M. Zhang, K. C. Cheung, S. See, H. Qin, J. Dai, and H. Li. Flow-Former++: Masked Cost Volume Autoencoding for Pretraining Optical Flow Estimation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),pages 15991610, June 2023.",
  "E. P. Simoncelli and D. J. Heeger. A model of neuronal responses in visual area MT. VisionResearch, 38(5):743761, March 1998. doi: 10.1016/S0042-6989(97)00183-1": "F. Solari, M. Chessa, N. V. K. Medathati, and P. Kornprobst. What can we expect from a V1-MTfeedforward architecture for optical flow estimation? Signal Processing: Image Communication,39:342354, November 2015. doi: 10.1016/j.image.2015.04.006. D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. PWC-Net: CNNs for Optical Flow Using Pyramid,Warping, and Cost Volume. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition (CVPR), pages 89348943, June 2018. D. Sun, X. Yang, M.-Y. Liu, and J. Kautz. Models Matter, So Does Training: An EmpiricalStudy of CNNs for Optical Flow Estimation. IEEE Transactions on Pattern Analysis andMachine Intelligence, 42(6):14081423, June 2020. doi: 10.1109/TPAMI.2019.2894353. Z. Sun, Y.-J. Chen, Y.-H. Yang, and S. Nishida. Modeling Human Visual Motion Processingwith Trainable Motion Energy Sensing and a Self-attention Network. In Advances in NeuralInformation Processing Systems, volume 36, pages 2433524348. Curran Associates, Inc.,December 2023. M. Tangemann, S. Schneider, J. von Kgelgen, F. Locatello, P. V. Gehler, T. Brox, M. Kmmerer,M. Bethge, and B. Schlkopf. Unsupervised Object Learning via Common Fate. In Proceedingsof the Second Conference on Causal Learning and Reasoning, pages 281327. PMLR, April2023. Z. Teed and J. Deng. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow. InComputer Vision ECCV 2020, pages 402419, Cham, August 2020. Springer InternationalPublishing. doi: 10.1007/978-3-030-58536-5_24.",
  "J. Xie, W. Xie, and A. Zisserman. Appearance-Based Refinement for Object-Centric MotionSegmentation. arXiv preprint arXiv:2312.11463, August 2024. doi: 10.48550/arXiv.2312.11463": "H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao. GMFlow: Learning Optical Flow viaGlobal Matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pages 81218130, June 2022. H. Xu, J. Zhang, J. Cai, H. Rezatofighi, F. Yu, D. Tao, and A. Geiger. Unifying Flow, Stereoand Depth Estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):1394113958, November 2023. doi: 10.1109/TPAMI.2023.3298645. D. L. K. Yamins, H. Hong, C. F. Cadieu, E. A. Solomon, D. Seibert, and J. J. DiCarlo.Performance-optimized hierarchical models predict neural responses in higher visual cortex.Proceedings of the National Academy of Sciences, 111(23):86198624, May 2014. doi: 10.1073/pnas.1403112111.",
  "For an additional overview, view visualize the segmentation performances on random dot stimuli asreported in": "GMA (+P) (FlyingThings3D) GMA (+P) (Mixed) GMA (Mixed) GMA (FlyingThings3D + Sintel) GMA (P-only) (FlyingThings3D) GMA (P-only) (Mixed) RAFT (FlyingChairs) GMA (FlyingThings3D) GMA (+P) (FlyingChairs) RAFT (KITTI) RAFT (Mixed) GMA (P-only) (KITTI) RAFT (FlyingThings3D) PWC-Net (KITTI) RAFT (FlyingThings3D + Sintel) GMA (FlyingChairs) GMA (P-only) (FlyingChairs) GMA (KITTI) FlowFormer++ (FlyingChairs) GMA (+P) (KITTI) FlowNet2 (FlyingThings3D) FlowFormer++ (Mixed) FlowFormer++ (Sintel) FlowFormer++ (FlyingThings3D) FlowFormer++ (KITTI) FlowNet2 CS (FlyingChairs) FlowNet2 CSS (FlyingChairs) FlowNet2 CS (FlyingThings3D) FlowNet2 (FlyingChairs) PWC-Net (FlyingChairs) PWC-Net (FlyingThings3D) FlowNet2 CSS (FlyingThings3D) FlowNet2 SD (FlyingChairs) Motion Energy (ours) (-) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 IoU",
  "B.1Importance of components of the motion energy model": "We conducted an additional ablation study in order to better understand which aspects of the motionenergy model are essential for generalization to random dot stimuli. We removed or replacedindividual layers as described in and trained the ablated models from scratch using in thesame way as the baseline model. The results in hint at the normalization and pooling layers being important for generalization.When the Gaussian pooling layers are removed completely, the performance on original videoseven slightly improves while the generalization to random dot stimuli is substantially reduced.",
  "Baseline0.7590.8450.6000.718": "Replace RectifiedSquare ReLU (MT)0.7530.8380.6090.725Replace Square ReLU (V1)0.7700.8540.5360.663Remove MT Linear0.7680.8560.4810.609Remove MT0.7700.8540.4510.583Remove Blur (V1, MT)0.8010.8720.4210.540Replace ChannelNorm InstanceNorm (V1, MT)0.5920.7030.2300.340Remove Normalization (V1, MT)0.4000.5160.0180.018 : Ablation study: Performance of the model on original videos and corresponding random dotstimuli with various layers of the motion energy model removed or replaced. Results are ordered byIoU on the random dot stimuli.",
  "B.2Multi-frame optical flow": "The motion energy model uses a window of 9 frames as input, while typical optical flow methodsestimate correspondences between only two frames. To rule out the possibility that the resultsobserved in our paper are mainly explained by the different input window lengths, we perform anablation study in which we apply optical flow methods using the same 9 frame windows. For eachwindow, we compute the optical flow between the central frame, for which the segmentation has tobe predicted, to the 8 other frames in the window. The stacked optical flow fields are then used as theinput to the segmentation network. The results in and show some improvement on the original videos but an everwider gap to the motion energy model in terms of of generalization to random dots. The differencesbetween the motion energy and optical flow models therefore cannot be explained by the differentinput lengths.",
  "B.3Comparison with state-of-the-art motion segmentation": "In our study we used a relatively small segmentation network downstream to the respective motionestimator. State-of-the-art motion segmentation models typically target multi-object segmentation inreal world videos and therefore use more complex segmentation networks. In order to verify that theresults in our paper are not caused by using a smaller segmentation network, we evaluated the state",
  "C.1Comparison of humans and machines by example difficulty": "As a measure of task difficulty, we count the number of informative dots. A dot is informative, if it iscontained in either the target and distractor shape but not both (see , left). Only these dotsallow discriminating between the different shapes. We fitted psychometric curves for human participants and models as a function of the number ofinformative dots, using the psignifit toolbox . The results in confirm that only the motionenergy model is able to match the performance of human subjects, especially for stimuli with amedium number of informative dots. : (left) As a measure of task difficulty, we count the number of informative dots that allowdiscriminating betwen the two shape alternatives. (right) Psychometric curves for humans, the motionenergy based model and the four best optical flow models for the task as in 8. FlowFormer++ (FlyingChairs) GMA +P (FlyingChairs) PWC-Net (KITTI) GMA (FlyingThings3D) RAFT (FlyingChairs) FlowFormer++ (FlyingThings3D) FlowFormer++ (Sintel) GMA (FlyingChairs) FlowNet2 CS (FlyingChairs) GMA P-only (FlyingThings3D) GMA P-only (FlyingChairs) GMA (FlyingThings3D + Sintel) GMA P-only (KITTI) GMFlow (1 scale) (Mixed) GMA +P (Mixed) RAFT (KITTI) GMA (KITTI) GMA +P (KITTI) GMA (Mixed) GMA +P (FlyingThings3D) RAFT (Mixed) RAFT (FlyingThings3D) GMA P-only (Mixed) FlowNet2 (FlyingChairs) RAFT (FlyingThings3D + Sintel) GMFlow (1 scale) (FlyingThings3D) PWC-Net (FlyingChairs) FlowNet2 CS (FlyingThings3D) FlowNet2 (FlyingThings3D) GMFlow (2 scales, 6 refinements) (Mixed) GMFlow (2 scales) (FlyingThings3D) GMFlow (2 scales, 6 refinements) (FlyingThings3D) GMFlow (2 scales, 6 refinements) (Sintel) GMFlow (2 scales) (Sintel) FlowNet2 CSS (FlyingChairs) FlowNet2 SD (FlyingChairs) GMFlow (2 scales, 6 refinements) (KITTI) GMFlow (2 scales) (Mixed) PWC-Net (FlyingThings3D) FlowNet2 CSS (FlyingThings3D)",
  "C.2Screenshots of the experiment": ": Screenshots from the human subject study on random dot shape identification. (top left)Instructions that were shown prior to the experiment. (top right) We showed 20 training trials duringwhich subjects could familiarize themselves with the task. (bottom left) The training was followed by500 test trials. A video with the random dot stimuli was shown first. (bottom right) Once the videofinished playing, the two shape options were shown below.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: We describe the models, data and evaluation protocol used in the paper indetail. Additionally, the code, pretrained models and the contributed dataset are publiclyreleased.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "If the authors answer NA or No, they should explain why their work has no societalimpact or why the paper does not address societal impact": "Examples of negative societal impacts include potential malicious or unintended uses(e.g., disinformation, generating fake profiles, surveillance), fairness considerations(e.g., deployment of technologies that could make decisions that unfairly impact specificgroups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tiedto particular applications, let alone deployments. However, if there is a direct path toany negative applications, the authors should point it out. For example, it is legitimateto point out that an improvement in the quality of generative models could be used togenerate deepfakes for disinformation. On the other hand, it is not needed to point outthat a generic algorithm for optimizing neural networks could enable people to trainmodels that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology isbeing used as intended and functioning correctly, harms that could arise when thetechnology is being used as intended but gives incorrect results, and harms followingfrom (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigationstrategies (e.g., gated release of models, providing defenses in addition to attacks,mechanisms for monitoring misuse, mechanisms to monitor how a system learns fromfeedback over time, improving the efficiency and accessibility of ML).",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}