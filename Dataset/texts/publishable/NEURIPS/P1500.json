{
  "Abstract": "We study the problem of estimating the body movements of a camera wearer fromegocentric videos. Current methods for ego-body pose estimation rely on tempo-rally dense sensor data, such as IMU measurements from spatially sparse bodyparts like the head and hands. However, we propose that even temporally sparseobservations, such as hand poses captured intermittently from egocentric videosduring natural or periodic hand movements, can effectively constrain overall bodymotion. Naively applying diffusion models to generate full-body pose from headpose and sparse hand pose leads to suboptimal results. To overcome this, we de-velop a two-stage approach that decomposes the problem into temporal completionand spatial completion. First, our method employs masked autoencoders to imputehand trajectories by leveraging the spatiotemporal correlations between the headpose sequence and intermittent hand poses, providing uncertainty estimates. Subse-quently, we employ conditional diffusion models to generate plausible full-bodymotions based on these temporally dense trajectories of the head and hands, guidedby the uncertainty estimates from the imputation. The effectiveness of our methodwas rigorously tested and validated through comprehensive experiments conductedon various HMD setup with AMASS and Ego-Exo4D datasets.",
  "Introduction": "The evolution of augmented reality (AR) devices such as the Apple Vision Pro, Meta Quest 3,Microsoft HoloLens 2, and etc. has dramatically reshaped interactive technologies. These head-mounted displays (HMDs) feature inertial measurement units (IMUs) and video capture capabilities,offering a unique egocentric perspective. However, their limited visibility of the users body partsposes a significant challenge for accurate egocentric body pose estimationa key element forimmersive AR experiences. Previous approaches have tackled this problem by spatially reconstructing the entire body fromspatially sparse data. For instance, EgoEgo first estimates head poses using SLAM on theegocentric video, then generates body poses from these estimated head positions. Other methods,such as AvatarPoser and BoDiffusion , primarily depend on temporally dense tracking signalfrom spatially sparse body parts, notably the head and hands. This dependency on specific hardwaresuch as head-mounted displays and hand controllers constrains their versatility and diminishes their",
  "(d)": ": Overview of DSPoser. Our goal is to estimate ego-body pose without dependency on handcontrollers in an HMD environment. (a) Given the egocentric video and head tracking signals asinput, (b) our approach first predicts the hand pose in the frames where hands are visible (dark blue).It then estimates the hand poses in frames with invisible hands (light blue) using imputation, and (c)estimates uncertainty associated with the hand poses where the hands are invisible, (d) The predictedand imputed hand pose is then used with head pose to predict the 3D full body pose. applicability in broader AR/VR scenarios where hand controllers might not be used, like sportstraining or analysis applications where the user needs to move freely without holding any devices, oraugmented reality experiences in outdoor environments where carrying controllers is impractical. We observe that even temporally sparse observations, such as hand poses captured intermittentlyfrom egocentric videos during natural or periodic hand movements, can effectively constrain overallbody motion. While it is possible to utilize other visible body parts such as feet or elbows, we optedto rely on hand poses. This decision is based on the availability of hand pose detectors and the fact that hands are visible in approximately 20% of video frames, as demonstrated in . Unlike previous work that concentrated only on spatial completion, our method incorporatestemporal completion by leveraging the intermittent appearance of hands in egocentric videos. Thisdual completion approach not only enhances the robustness of body pose estimation under varyingconditions but also reduces reliance on specific sensor hardware, making it more adaptable to variousAR environments. In our setup, we use temporally sparse 3D hand poses from detections in egocentricvideos combined with dense head tracking signals to reconstruct the full body. Initially, we temporallycomplete sparse hand information using a Masked Autoencoder (MAE) , which estimates handpose trajectories by capturing the spatiotemporal correlations between intermittent hand poses andhead tracking signals. We develop a probabilistic extension of the MAE to provide uncertaintyestimates of the predicted hand pose sequence. Subsequently, using a conditional diffusion model, wespatially reconstruct the full body based on the head tracking signal data and imputed hand trajectoriesalong with their predictive uncertainties. We call our approach DSPoser (Doubly Sparse Poser)because it can effectively utilize data that is doubly sparse (sparse both temporally and spatially), asshown in . This flexible framework is designed to seamlessly adapt to diverse AR/VR setups and devices,ranging from spatially sparse scenarios (e.g., using only head tracking signal or combining it withhand controllers) to doubly sparse scenarios (utilizing head signal data alongside hand detectionfrom egocentric video). The key advantage lies in the assumption that the HMDs tracking signal isconsistently available, enabling our approach to function across a wide range of environments andhardware configurations. Extensive experiments have proved our models versatility and accuratepose estimation capabilities in various settings. Furthermore, our ablation studies highlight thesignificance of incorporating uncertainty estimates, as this crucial information enhances the overallquality of pose estimation, resulting in more reliable outputs. By addressing both temporal and spatialcompletion through our double completion approach, we have developed a robust and adaptablesolution that reduces dependency on specific sensor hardware, making it well-suited for immersiveAR experiences in diverse scenarios, such as sports training, outdoor environments, and beyond.",
  "In summary, our research presents three key contributions:": "A robust and versatile framework for egocentric body pose estimation tailored for HMDs.The framework adapts to various AR/VR settings and can leverage tracking signals availablein most modern HMD devices without controllers. We decomposed the problem into temporal completion and spatial completion. Our approachcaptures the uncertainty from hand trajectory imputation to guide the diffusion model foraccurate full-body motion generation. Extensive evaluations demonstrating the effectiveness of our framework on diverse datasets,outperforming existing methods and underscoring its potential for enhancing user interactionand immersion in AR experiences.",
  "Problem Formulation": "In our work, we aim to estimate the 3D human pose of a HMD user from sequences of RGB videoand head tracking signal. We note that head tracking signal data is commonly accessible from IMUin most HMDs, such as Meta Quest and Apple Vision Pro. Suppose we are given an egocentricvideo Vego = {V1, . . . , VTw} where V is an RGB image and Tw denotes the sequence length, and acorresponding head tracking signal sequence Thead = {T1, . . . , TTw} where T RDhead and Dheadis the dimension of head tracking signal including 3D pose. Our goal is to estimate the full bodypose P = {P1, . . . , PTw}, where pose state P RJD at time , J is the number of body jointsand D is the dimensionality of pose state. We solve the problem of estimating p(P|Vego, Thead) bydecomposing it into 2 the stages of imputation and generation, assuming that we have temporallysparse hand data....H from hand detection module f():....H = f(Vego). We first temporally completehand trajectory H based on....H and Thead, which can be written as p( H|....H , Thead). Then, we spatiallycomplete full body pose P from the imputed hands H and Thead, which can be written as p(P| H, Thead).Since H is a probabilistic variable, we need to marginalize over H as follows:",
  "Detection: Hand Pose Estimation from Egocentric Video": "In this work, we estimate the 3D position of the hand from an egocentric camera using a two-stepprocess. First, we use FrankMocap to predict hand poses as SMPL-X parameters , fromwhich we extract local 3D hand joint positions relative to the root of the hand models kinematictree, denoted as H3Dh R213. Simultaneously, we use RTM-Pose to estimate 2D hand jointpositions in the image, H2DI R212. Finally, we determine the 3D hand joint positions in thecamera coordinate system, H3DI= H3Dh+ d by solving for d R3 that minimizes the reprojection",
  "error H2DI K(H3Dh+ d)2. Here, K is the intrinsic matrix, obtained by transforming the originalcamera parameters into a pinhole model through undistortion": "To better constrain the hand trajectories, we attempted to obtain rotation information from the 3Dhand detection. However, due to the inconsistent quality of hand detection, the rotational informationderived from the hand pose was noisy. Therefore, we decided not to incorporate this rotationalinformation into our hand tracking approach on the Ego-Exo4D dataset. We utilized only the 3Dwrist location from the Ego-Exo4D dataset, represented by Dhand = 3. In contrast, for the AMASSdataset, we leveraged both rotational information and 3D location, as this data is readily available,resulting in Dhand = 9.",
  "Temporal Completion: Hand Trajectory Imputation from Sparse Hand Pose": "Masked Auto-Encoder (MAE)In our work, we employed a Masked Autoencoder (MAE) to impute missing hand trajectories using head tracking signal Thead and detected hand pose....H .Inspired by Vision Transformer (ViT), we treated each T and....H at time as a token similar toan image patch in ViT. To accommodate this, we implemented two embedding layers, one for headtracking signal T RDhead and the other for hand....H RDhand, both projecting into the commontoken dimension DM. For the AMASS dataset, we follow the head tracking signal representationDhead = 18 as in . For the Ego-Exo4D dataset, Dhead = 15, which includes head position andleft/right IMU signals. Consequently, the total number of token amounts to 3 Tw, where 3 accountsfor the head and both hands, and Tw is the sequence length. Sinusoidal positional encoding (PE) isused for both the encoder and decoder patches after tests showed it suffices for learning differentmodalities, compared to learnable PE. In an HMD environment, we assume that the head trackingsignal Thead is always available, but hand visibility depends on the egocentric video. Thus, maskingis applied only to the hand tokens based on their visibilities within egocentric view. In contrast to the MAE training approach, which maintains a consistent number of maskedpatches due to a fixed masking ratio, the count of frames with invisible hand varies across instancesin our setup. To address this variability, our encoder selectively applies attention masking to theseinputs, ensuring that queries do not attend to tokens where hand is invisible. This attention maskingtechnique adapts dynamically to the fluctuating numbers of missing frames across the instances,enhancing the models ability to handle data sparsity effectively. For decoder, we adopted MAEdecoder design except the last projection layer to guide the uncertainty. To capture the uncertainty,we split the final projection layer into two heads for mean and variance of a Gaussian distribution. Uncertainty-aware MAEFollowing the , to make the MAE aware of the predictiveuncertainty of imputed hand pose sequence, we employ the -NLL loss function to manageuncertainty by using a set of mean heads i(x) and variance heads 2i (x), which are derived from Mmodels initialized differently, where x = [....H ; T ] is an input to the MAE and i [1, M]. The meanheads i(x) and variance heads 2i (x) are trained using the Gaussian negative log-likelihood loss,which applies to each sample indexed by n with input xn and ground truth hand pose sequence yn.",
  "22i (xn).(3)": "The LNLL loss function causes the predicted variance to act as a weighting factor for each data point,emphasizing those with higher variances. The parameter adjusts the intensity of this weighting. Thesg() function is used to apply the stop-gradient operation, thus preventing gradients from propagatingthrough this part of the computation. After training, we measure the aleatoric (data) uncertainty Uale() by averaging the variances acrossmodels, and epistemic (model) uncertainty Uepi() by calculating variance of model means, and totaluncertainty by adding both uncertainties:",
  "Spatial Completion: Uncertainty-guided Body Pose Generation from Imputed HandTrajectories and Head Tracking Signal": "We employed the VQ-Diffusion to generate full body poses from imputed hand trajectories andhead tracking signal. The exposition of VQ-Diffusion can be found in Section D of the Appendix.As illustrated in , our motion generation module is designed to generate human motionsequences from the temporally dense hand and head trajectories with uncertainty obtained from theMAE model. VQ-VAEWe first train the VQ-VAE to represent human motion with a discrete codebook repre-sentation as described in Appendix D.1. We mostly followed the architectural design and trainingmethods of . After the codebook representation is learned by the VQ-VAE, we utilize this latentcodebook representation to train a denoising diffusion model. Denoising TransformerMotivated by the work of VQ-Diffusion, we design a denoising trans-former that estimates the distribution p(z0|zt, y). An overview of our proposed model is depictedin . We closely follow the implementation of . To incorporate the diffusion step t intothe network, we employ the adaptive layer normalization (AdaLN) . We concatenated theestimated hand and head trajectory with codebook after a embedding layer, to match the dimensionwith codebook representation. Finally, we use the decoder to decode z0 to obtain a full body posesequence.",
  "U(x)) andregard it as the conditioning vector y, where (x) = Ei[i(x)] M 1": "i i(x) and U(x) ismeasured by one of Eq. (9), (10), and (11). While it would be ideal to sample multiple times to betterapproximate the marginalization in Equation 1, we find just using one sample provides a competitiveperformance. For dropout, we set each dimension of (x) to zero with a certain probability, which is determinedby the corresponding dimension of U(x), and denote the result as y. The probability of the d-thdimension of (x) being zero is pd = 1 (Ud(x) Udmin(x))/(Udmax(x) Udmin(x)) whereUd(x) is the d-th dimension of U(x), and Udmin(x), Udmax(x) are the minimum and maximumvalues over the sequence length, respectively.",
  "Datasets & Evaluation Metrics": "Ego-Exo4D datasetEgo-Exo4D contains simultaneous captures of egocentric (first-person)and exocentric (third-person) video perspectives of participants performing complex activities likesports, dance, and mechanical tasks. The dataset comprises 1,422 hours of video ranging from 1 to42 minutes per video. In addition to video, it provides camera poses, IMU data, and human poseannotations. Specifically for the egopose task, it includes separate training and validation video setscontaining 334 and 83 videos respectively. Our problem formulation of ego body pose estimationdiffers from the ego body pose prediction task from , which aims to predict a single future framegiven a specific time window.",
  "AMASS datasetThe AMASS dataset is a large human motion database that unifies differentexisting optical marker-based MoCap datasets by converting them into realistic 3D human meshes": ": Performance comparisons across baseline models for doubly sparse video data on theAMASS test set. We report MPJRE [], MPJPE [cm], and MPJVE [cm/s], with the best resultshighlighted in boldface. Models trained by us are marked with . The notation....data denotestemporally sparse data, data indicates imputed data, and all other cases involve dense data. Tsindicates the sliding window, x indicates the input of our whole pipeline, and y indicates the input ofdenoising Transformer.",
  "Bodiffusion 20&....Interpolation& 46.4575.3317.99Bodiffusion 20&....MAE& 7.3531.335.47DSPoser (Ours)20&....MAE& 5.510.0224.190.104.090.02": "AvatarPoser 1&....Interpolation& 40.4264.0716.37AvatarJLM 1&....Interpolation& 25.0268.4214.14AvatarPoser 1&....MAE& 9.8862.315.98AvatarJLM 1&....MAE& 7.1237.605.24DSPoser (Ours)1&....MAE& 5.870.1349.120.244.310.10 represented by SMPL model parameters. Following the AvatarPoser evaluation, we used theCMU , BMLrub , and HDM05 subsets from the AMASS dataset and their preprocessingof tracking signal information. Since AMASS does not include RGB images, we set Dhand = 9assuming that 3D hand position and 6D rotation is available when the hand is \"visible\". To determinevisibility, we compute the angle between the z-axis vector of the head rotation and the vector fromthe head position to the hand. We define the hand as \"visible\" if this angle is within a 45 range,corresponding to a 90 field of view (FoV) of HMD devices. Evaluation metricWe evaluate our results using the following metrics: Mean Per Joint PositionError (MPJPE), Mean Per Joint Velocity Error (MPJVE), and Mean Per Joint Rotation Error (MPJRE),following the evaluation of . Since Ego-Exo4D dataset doesnt have the annotations for 6Drotation, MPJRE is reported only for AMASS. We report all values with the confidence interval of95%. We also provide details on MPJPE across hands, upper body above the pelvis, and lower bodybelow the pelvis, denoted as Hand PE, Upper PE, and Lower PE, respectively.",
  "Full Body Pose Estimation from Doubly Sparse data": "To demonstrate the effectiveness of our framework on doubly sparse egocentric video data, weinvestigated the results of our framework, DSPoser, on the AMASS dataset and Ego-Exo4D, asshown in and , respectively. Since the task of body pose estimation from doublysparse data is newly introduced in our paper, we compare our results to other baselines, EgoEgo, Bodiffusion , AvatarPoser , and AvatarJLM . Those baselines are designed to estimatehuman body poses from spatially sparse data. EgoEgo estimates body poses from head poses, and theothers estimate body poses from head and hand tracking signals. We report the experimental resultsusing the sampling strategy with aleatoric uncertainty unless otherwise stated. To train the baslineson temporally sparse data, we extend the algorithm as follows: (1) Interpolation: we imputed handposes with linear interpolation; (2) MAE: we use our trained MAE to impute the hand trajectory. InTs = 1 setup, we report our result after averaging 16 samples while the result in Ts = 20 setup isfrom a single sample. As shown in , DSPoser consistently outperforms baseline methods on AMASS across allmetrics, underscoring the effectiveness of our two-stage approach for ego-body pose estimation.DSPoser achieves notable improvements in MPJPE for both sliding window sizes, Ts = 20 andTs = 1. For Ts = 20, DSPoser reduces MPJPE from 7.35 cm to 5.51 cm, significantly outperformingthe Bodiffusion extension, which uses MAE to impute invisible hands. For Ts = 1, DSPoser : Performance comparisons across baseline models for doubly sparse video data on the Ego-Exo4D validation set. We report MPJPE [cm] and MPJVE [cm/s], with the best results highlighted inboldface. Models trained by us are marked with . The notation....Data denotes temporally sparse data,data indicates imputed data, and all other cases involve dense data.",
  "MethodsyMPJPEMPJVEMPJREHand PEUpper PELower PE": "FinalIK & 18.0959.2416.77---LoBSTR & 9.0244.9710.69---VAE-HMD & 6.8337.994.11---CollMoves & 5.5565.284.58---AvatarPoser & 4.2028.233.082.341.888.06AvatarJLM & 3.3520.792.901.241.726.20DSPoser (Ours)& 3.730.0843.430.142.940.093.261.926.53 achieves superior MPJPE compared to AvatarJLM, though it showswlimitations in MPJVE due to thestochasticity of the diffusion model. In the experimental results presented in , our DSPosermodel demonstrates superior performance on the Ego-Exo4D validation set. The model outperformsexisting baselines, achieving a lower MPJPE of 16.84 cm, which represents an improvement over thenext best model by 5.49 cm. Additionally, DSPoser achieves an MPJVE of 39.86 cm/s, improvingupon the basline of naive extension of Bodiffusion by 7.64 cm/s. It is evident that by incorporating temporally sparse hand pose data, our DSPoser framework signifi-cantly enhances pose estimation accuracy. For instance, on the AMASS dataset, MPJPE improveddramatically from 12.08 cm to 5.51 cm, while on the Ego-Exo4D dataset, it improves from 19.12 cmto 16.84 cm in Ts = 20 setup. This indicates that even sparse hand trajectory data, when effectivelyutilized, can provide crucial information for refining the accuracy of ego body pose estimation. Ourmethods ability to harness sparsely available data underscores its potential in applications wherecapturing dense sequence is challenging.",
  "Full Body Pose Estimation from Spatially Sparse data": "To demonstrate the versatility of our framework, we conduct experiments on spatially sparse videodata. In the temporally dense data setup, where there is no uncertainty regarding hand poses, thedense data directly works as a condition y for spatial completion on the right side of . presents the results, demonstrating that DSPoser performs comparably to baseline models designedspecifically for dense data setups on MPJPE and MPJRE metrics, underscoring the versatility of ourdual approach in handling dense data scenarios. As discussed in .2, the higher MPJVE errorresults from the inherent stochasticity of the diffusion model.",
  "Ablation Studies": "Based on the ablation study results shown in Tables 4 and 5, we can analyze the impact of differentuncertainty guidance strategies and types of uncertainty on the performance of the model for bodypose estimation. The ablation study is conducted with AMASS dataset with the sliding windowTs = 20 to better analyze the effect of the uncertainty guidance. investigates the effectsof various uncertainty guidance strategies, including no uncertainty guidance, sample, distributionembedding, and dropout. The results suggest that incorporating uncertainty guidance through thesestrategies can improve the models performance across different metrics. The sampling strategyachieves the best performance, with the lowest MPJPE of 5.51, MPJVE of 24.19, and MPJRE of4.09, indicating its effectiveness in capturing uncertainty and improving pose estimation accuracy. examines the contributions of different types of uncertainty, including epistemic uncertainty,aleatoric uncertainty, and total uncertainty. The results show that accounting for aleatoric uncertaintyleads to the best overall performance. This suggests that considering data uncertainty can providecomplementary information and improve the robustness of the pose estimation model. Overall,the ablation study highlights the importance of incorporating uncertainty guidance and consideringdifferent types of uncertainty in the model design for accurate and reliable body pose estimation. In , we analyzed the effect of different values on the AMASS dataset during the uncertaintycapturing process of the Masked Auto-Encoder (MAE). The results, shown in the table, indicatethat = 0.5 provides the best temporal completion for head and hand 3D positions from the doublysparse input. Therefore, we set to 0.5 for training the MAE.",
  "Hand Detection Accuracy and Hand Visibility Statistics": "We investigate the error of the hand detector applied to the Ego-Exo4D dataset in terms of MPJPE,as shown in . The detection results indicate an average error of less than 10 cm. We alsoanalyze the visibility statistics for the AMASS and Ego-Exo4D datasets in . In the AMASSdataset, at least one hand is visible in 18% of all frames with a 90 field of view (FoV), whereas inthe Ego-Exo4D dataset, at least one hand is visible in 27% of all frames.",
  "Qualitative Results": "We visualized the aleatoric uncertainty in , captured by a model trained using MAE on theAMASS dataset. In cases of partial visibility, as shown in (a-1) and (a-2), the uncertaintyrange is notably small. Conversely, in frames where the subject is completely obscured, the uncertaintyrange increases significantly. Even in fully invisible scenarios, the model captures a range ofuncertainty, likely influenced by head movements. Most of the estimated frames fall within the 2range.",
  "(c) AMASS Groundtruth and Prediction": ": (a) Ego-Exo4D video frames, (b) the corresponding skeleton ground truth and our predictionresults, and (c) qualitative results on AMASS data under different input conditions. green indicatesthe ground truth, blue indicates the predicted result, and red indicates the visible hands. Head onlyestimates body pose from head trajectories, whereas Ours estimates body pose from imputed handand head trajectories.",
  "Related Works": "Human Pose Estimation from Sparse InputA common capture setting in mixed reality involvesusing a head-mounted device and hand controllers. Estimating full-body motion from the sparseinput of head and hand movements is challenging. Recently, several methods have been proposed totackle this: AvatarPoser is the first learning-based method to predict full-body poses in worldcoordinates using only head and hand motion inputs. It uses a Transformer encoder to extract deepfeatures and decouples global motion from local joint orientations, refining arm positions with inversekinematics for accurate full-body motion. BoDiffusion employs a generative diffusion model formotion synthesis, addressing the under-constrained reconstruction problem. It uses a time and spaceconditioning scheme to leverage sparse tracking inputs, generating smooth and realistic full-bodymotion sequences. AvatarJLM uses a two-stage framework where sparse signals are embeddedinto high-dimensional features and processed by an MLP to generate joint-level features. Thesefeatures are then converted into tokens and fed into a transformer-based network to capture spatial andtemporal dependencies, with an SMPL regressor transforming them into 3D full-body pose sequences.HMD-poser combines a lightweight temporal-spatial feature learning network with regressionlayers and uses forward kinematics to achieve real-time human motion tracking. AGRoL utilizedconditional diffusion model to generate full body pose from sparse upper-body tracking signals. Itis worth noting a concurrent work, EgoPoser , which also addresses ego body pose estimation from doubly sparse observations. Their focus lies in preparing training data through field-of-view(FoV) modeling rather than introducing new algorithms. Our work is orthogonal to theirs, providingalgorithmic contributions through a multi-stage pipeline including an uncertainty-aware maskedauto-encoder (MAE). Human Body Pose Estimation from Egocentric VideosEstimating full 3D human body posefrom egocentric videos is an ill-posed problem due to the partial visibility of wearers body partsfrom the camera mounted on wearers head. Recently, several approaches have been proposed toaddress this challenge. EgoEgo integrates SLAM and a learned transformer to estimate headmotion, then leverages estimated head pose to generate plausible full-body motions using diffusionmodels. designs a kinematic policy to generate per-frame target motion from egocentric inputs,and leverages a pre-learned dynamics model to distill human dynamics information into the kinematicmodel. GIMO integrates motion, 3D eye gaze, and 3D scene features to generate gaze informedlong term intention-aware human motion prediction. leverages external camera to generatepseudo labels to estimate full 3D body pose from single head mounted fish eye camera using weaksupervision. estimates geometry of surrounding objects and extracts 2D body pose featuresusing EgoPW to regress 3D body pose with a voxel-to-voxel network .",
  "Conclusion": "In this paper, we have addressed the problem of egocentric body pose estimation using temporallysparse observations from head-mounted displays (HMDs). By leveraging both temporal and spatialcompletion, our approach effectively utilizes intermittent hand pose detections from egocentricvideos, alongside consistently available head pose data, to reconstruct full-body motions. Throughcomprehensive experiments on datasets such as AMASS and Ego-Exo4D, we have demonstrated theeffectiveness of our framework. Our results indicate significant improvements over existing methods,particularly in scenarios where dense sensor data may not be available or practical. This advancementopens up new possibilities for beneficial augmented reality experiences in various applications,including sports training by providing feedback on body mechanics, and other scenarios where usersneed to move freely without additional sensors such as hand controllers. However, our method hasnot been explicitly tested for fairness across different demographic groups. Potential biases in thedatasets used could result in uneven performance across various user populations. Careful curation oftraining datasets is necessary to prevent unfair failures for underrepresented groups.",
  "Limitations": "While our proposed method for estimating the body movements of a camera wearer from sparsetracking signals shows promising results, several limitations should be acknowledged. Firstly, ourmethod has been tested with only one type of sparse body part tracking signal, specifically the hand.Incorporating the detection of other body parts, such as feet and elbows, may improve overall bodypose estimation. Additionally, variations in lighting, occlusions, and the quality of the egocentricvideo can impact the accuracy of hand pose detection, subsequently affecting the overall body poseestimation. The effectiveness of our method was validated using the AMASS and Ego-Exo4D datasets. Althoughthese datasets are comprehensive, they may not encompass the full spectrum of possible real-worldvariations. Our study focused on pose estimation within a window size of less than a few seconds,following standard settings from the literature. It remains unclear how our method will performwith larger window sizes. Furthermore, the scalability of our method with larger datasets has notbeen thoroughly evaluated. The use of diffusion models for pose estimation may limit its utility forreal-time applications due to their inference speed. Additionally, using multiple models to computeepistemic uncertainty can be computationally intensive. AcknowledgementWe acknowledge Feddersen Chair Funds and the US National Science Founda-tion (FW-HTF 1839971, PFI-TT 2329804) for Professor Karthik Ramani. Any opinions, findings,and conclusions expressed in this material are those of the authors and do not necessarily reflect theviews of the funding agency. We sincerely thank the reviewers for their constructive suggestions. K. Ahuja, E. Ofek, M. Gonzalez-Franco, C. Holz, and A. D. Wilson. Coolmoves: User motionaccentuation in virtual reality. Proceedings of the ACM on Interactive, Mobile, Wearable andUbiquitous Technologies, 5(2):123, 2021.",
  "J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,2016": "A. Castillo, M. Escobar, G. Jeanneret, A. Pumarola, P. Arbelez, A. Thabet, and A. Sanakoyeu.Bodiffusion: Diffusing sparse observations for full-body human motion synthesis. In Pro-ceedings of the IEEE/CVF International Conference on Computer Vision, pages 42214231,2023. S. Chi, H. Chi, H. Ma, N. Agarwal, F. Siddiqui, K. Ramani, and K. Lee. M2d2m: Multi-motiongeneration from text with discrete diffusion models. In European conference on computervision. Springer, 2024.",
  "P. Dai, Y. Zhang, T. Liu, Z. Fan, T. Du, Z. Su, X. Zheng, and Z. Li.Hmd-poser: On-device real-time human motion tracking from scalable sparse observations. arXiv preprintarXiv:2403.03561, 2024": "A. Dittadi, S. Dziadzio, D. Cosker, B. Lundell, T. J. Cashman, and J. Shotton. Full-bodymotion from a single head-mounted device: Generating smpl poses from partial observations.In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1168711697, 2021. Y. Du, R. Kips, A. Pumarola, S. Starke, A. Thabet, and A. Sanakoyeu. Avatars grow legs: Gen-erating smooth human motion from sparse tracking inputs with diffusion model. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 481490,2023. K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya,S. Bansal, B. Boote, et al. Ego-exo4d: Understanding skilled human activity from first-andthird-person perspectives. arXiv preprint arXiv:2311.18259, 2023. S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan, and B. Guo. Vector quantizeddiffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 1069610706, 2022. K. He, X. Chen, S. Xie, Y. Li, P. Dollr, and R. Girshick. Masked autoencoders are scalablevision learners. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 1600016009, 2022.",
  "Z. Luo, R. Hachiuma, Y. Yuan, and K. Kitani. Dynamics-regulated kinematic policy foregocentric pose estimation. Advances in Neural Information Processing Systems, 34:2501925032, 2021": "N. Mahmood, N. Ghorbani, N. F. Troje, G. Pons-Moll, and M. J. Black. Amass: Archive ofmotion capture as surface shapes. In Proceedings of the IEEE/CVF international conference oncomputer vision, pages 54425451, 2019. G. Moon, J. Y. Chang, and K. M. Lee. V2v-posenet: Voxel-to-voxel prediction network foraccurate 3d hand and human pose estimation from a single depth map. In Proceedings of theIEEE conference on computer vision and pattern Recognition, pages 50795088, 2018.",
  "M. Mller, T. Rder, M. Clausen, B. Eberhardt, B. Krger, and A. Weber. Documentationmocap database hdm05. Computer Graphics Technical Report CG-2007-2, Universitt Bonn,7:11, 2007": "G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas, and M. J. Black.Expressive body capture: 3d hands, face, and body from a single image. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesiswith latent diffusion models. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 1068410695, 2022. Y. Rong, T. Shiratori, and H. Joo. Frankmocap: A monocular 3d whole-body pose estimationsystem via regression and integration. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 17491759, 2021.",
  "A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in neuralinformation processing systems, 30, 2017": "J. Wang, L. Liu, W. Xu, K. Sarkar, D. Luvizon, and C. Theobalt. Estimating egocentric 3dhuman pose in the wild with external weak supervision. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1315713166, 2022. J. Wang, D. Luvizon, W. Xu, L. Liu, K. Sarkar, and C. Theobalt. Scene-aware egocentric 3dhuman pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pages 1303113040, 2023.",
  "D. Yang, D. Kim, and S.-H. Lee. Lobstr: Real-time lower-body pose prediction from sparseupper-body tracking signals. In Computer Graphics Forum, volume 40, pages 265275. WileyOnline Library, 2021": "J. Zhang, Y. Zhang, X. Cun, Y. Zhang, H. Zhao, H. Lu, X. Shen, and Y. Shan. Generating humanmotion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pages 1473014740, 2023. X. Zheng, Z. Su, C. Wen, Z. Xue, and X. Jin.Realistic full-body tracking from sparseobservations via joint-level modeling. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pages 1467814688, 2023. Y. Zheng, Y. Yang, K. Mo, J. Li, T. Yu, Y. Liu, C. K. Liu, and L. J. Guibas. Gimo: Gaze-informed human motion prediction in context. In European Conference on Computer Vision,pages 676694. Springer, 2022.",
  ": Performance results for different in-ference and training step combinations": "VQ-VAEWe adhered to the architectural details and training protocol of Zhang et al. , withmodifications including setting both the encoder and decoder stride to 1, and adjusting the windowsize to 40. For the Ego-Exo4D dataset, we employed wing loss with a width of 5 and a curvatureof 4. For AMASS, we opted for L2 loss. Additionally, to generate smooth motion, we applied bothvelocity and acceleration losses, assigning weights of 10 for each in the AMASS dataset, and weightsof 1 for each in the Ego-Exo4D dataset. The input shape is represented as Tw J Ddata, where Jis the number of joints, Tw is time window, and Ddata is the data representation. For AMASS, whichuses a 6D rotation representation, J = 22 and Ddata = 6. For Ego-Exo4D, J = 17 and Ddata = 3.We use Tw = 40 for both AMASS and Ego-Exo4D. VQ-DiffusionWe employed the same hyperparameters and training specifications as Gu et al. for training VQ-Diffusion. Additionally, we replaced the absolute positional encoding with relativepositional encoding, following the implementation of VQ-Diffusion for human motion generationproposed by Chi et al. . We replaced the text condition module with the uncertainty-aware MAEmodule to feed the imputed trajectory and uncertainty as a conditional input. Masked Auto-EncoderWe adapted the encoder to accommodate a variable number of visiblehands and modified the last projection layer to guide the uncertainty, as detailed in the main paper.Otherwise, we followed the training details provided by He et al. . We trained 4 models tomeasure the uncertainty, M = 4.",
  "DataloaderWe adopted the dataloader of AMASS and evaluation configurations from Bodiffusion for our experiments. For the Ego-Exo4D dataset, we employed the dataset implementation fromthe Ego-Exo4D": "Analaysis on Computaional CostWe evaluated the computational cost of our approach by ana-lyzing the number of parameters, multiply-accumulate operations (MACs), and inference time foreach module in our pipeline. The reported times in represent the total measured time for theentire pipeline, whereas measures the time only for the corresponding module, excludingoverhead between modules. As shown in , the VQ-Diffusion model is responsible for themajority of MACs and inference time. To mitigate this, we conducted further experiments to exploreways of reducing the VQ-Diffusion models inference time. presents the trade-off betweenperformance and inference time based on the number of diffusion steps, offering multiple options.Notably, training with 50 steps and inferring with 25 steps yields approximately a 4x faster inferencetime with only about 3% reduction in performance.",
  "BCompute Resource": "We ran our experiments on one workstation, containing AMD Ryzen Threadripper PRO 7975WX,DDR5 RAM 256GB and 4 NVIDIA GeForce RTX 4090. AMASS dataset takes 512GB of storage andEgo-Exo4D dataset takes 11TB. One training run took around 18 hours on 1 GPU for AMASS andaround 12 hours for Ego-Exo4D. Inference over AMASS validation set takes 40 minutes on 1 GPUand inference over Ego-Exo4D validation set takes 10 minutes on 1 GPU. In total, all experimentsincluding preliminary or failed experiments took approximately 300 GPU-hours.",
  "D.1Discrete Diffusion Model": "Discrete diffusion models represent a category of diffusion models that progressively introducenoise into data while training to reverse this process. In contrast to continuous models, such as alatent diffusion model , which manipulate data in a continuous state space, discrete diffusionmodels operate within discrete state spaces. VQ-VAEVector Quantized-Variational Autoencoder (VQ-VAE) is a generative model thatextends the concept of Variational Autoencoders (VAEs) by incorporating discrete latent representa-tions via vector quantization. The encoder E(x) compresses input data x into discrete latent vectorsby mapping each encoded representation to the closest vector zq to the nearest codebook entry from alearned codebook of prototypes using the nearest-neighbor search: zq =Q(z)=argminciC||z ci||2.Here, C = {c1, . . . , cK}, where K is the total number of codebooks. The decoder D(zq) recon-structs the input data x from these quantized vectors, yielding a reconstructed output x = D(zq).The optimization process involves minimizing a combination of reconstruction loss and commit-ment loss. The reconstruction loss is expressed as x x2, while the commitment loss en-sures the encoder commits to the nearest prototype in the codebook: sg[zq] z2, where sg isthe stop-gradient operator. The overall loss function, which the VQ-VAE model minimizes, is:LVQ = x x2 +zq sg[z]2 +VQsg[zq]z2. Here, VQ is a coefficient for the commitmentloss. Forward Diffusion Process.Building on the foundation laid by the discrete diffusion modelsintroduced by , VQ-Diffusion refined the diffusion process with a mask-and-replace strategy.In VQ-Diffusion, during the forward diffusion process, tokens can either transition to other tokens orto a special <MASK> token. The transition probability from token zi to zj at diffusion step t is definedby the matrix Qt[i, j]. The transition matrix Qt, structured in R(K+1)(K+1), follows:",
  ", where Qt = tI + t11.(7)": "Here, t adjusts to ensure conservation of probability, such that t =1Ktt is the probabilityof transitioning between tokens, and t governs transitions to the <MASK> token. The transitionfrom step t 1 to t is expressed as: q(zt|zt1) = v(zt)Qtv(zt1), where v(zt) R(K+1)1is an one-hot encoded vector representing the token index of zt. Using the Markov property, the probability of transitioning from any initial step 0 to step t is q(zt|z0) = v(zt)Qtv(z0), whereQt = QtQt1 Q1 is the cumulative transition matrix. This defines the cumulative probabilitiesas t = ti=1 i, t = 1 ti=1(1 i), and t = (1 t t)/K. Conditional Denoising Process.In the conditional denoising process, a neural network denotedas p aims to predict the original, noiseless token z0 given a corrupted token and the associatedcondition, such as a embedded hand trajectories. The posterior distribution for the discrete diffusionprocess can be defined as:",
  "Low Error": ": Qualitative results on AMASS dataset comparing DSPoser (Ours) against the baselines.Color gradient indicates an absolute positional error, with a higher error corresponding to higher blueintensity. Results demonstrate that motions generated by DSPoser exhibit greater similarity to theground truth. Furthermore, it highlights higher errors (indicated with red circles) for baselines whenthe hand is occluded in the ground truth pose (indicated with a black circle)."
}