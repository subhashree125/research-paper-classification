{
  "Abstract": "The ideas of aleatoric and epistemic uncertainty are widely used to reason aboutthe probabilistic predictions of machine-learning models. We identify incoherencein existing discussions of these ideas and suggest this stems from the aleatoric-epistemic view being insufciently expressive to capture all of the distinct quanti-ties that researchers are interested in. To explain and address this we derive a sim-ple delineation of different model-based uncertainties and the data-generating pro-cesses associated with training and evaluation. Using this in place of the aleatoric-epistemic view could produce clearer discourse as the eld moves forward.",
  "Introduction": "When making decisions under uncertainty, it can be useful to reason about where that uncertaintycomes from (Osband et al, 2023; Wen et al, 2022).Researchers commonly refer to the ideas ofaleatoric (literal meaning: relating to chance) and epistemic (relating to knowledge) uncertainty,which have a long history in the study of probability (Hacking, 1975). Aleatoric uncertainty is typi-cally associated with statistical dispersion in data or outcomes, while epistemic uncertainty is associ-ated with the internal information state of a model (Hullermeier & Waegeman, 2021). Concerningly given their scale of use, these ideas are not being discussed coherently in the litera-ture. The line between model-based predictions and data-generating processes is repeatedly blurred(Adlam et al, 2020; Amini et al, 2020; Ayhan & Berens, 2018; Collier et al, 2020; Immer et al, 2021;Kapoor et al, 2022; Liu et al, 2022; Mavor-Parker et al, 2022; Notin et al, 2021; Postels et al, 2019;Smith & Gal, 2018; van Amersfoort et al, 2020). Different mathematical quantities are used to re-fer to notionally the same concepts: epistemic uncertainty, for example, has received multiple def-initions, including variance-based measures (Gal, 2016; Kendall & Gal, 2017; McAllister, 2016),information-based measures (Gal et al, 2017), ad-hoc reinterpretations of information-based measures(Shen et al, 2018; Siddhant & Lipton, 2018) and distance-based measures (Mukhoti et al, 2021, 2023;van Amersfoort et al, 2020). Misleading connections are drawn between predictive uncertainty andaccuracy (Orlando et al, 2019; Wang et al, 2019). Tenuous assumptions are made about how predic-tive uncertainty will decompose on unseen data (Seebock et al, 2019; Wang & Aitchison, 2021). We suggest this incoherence arises from the aleatoric-epistemic view being too simplistic in the con-text of machine learning. To make this diagnosis precise and provide an alternative to the aleatoric-epistemic view, we systematically disambiguate some key concepts that arise in machine learning.We start with a predictive task of interest, highlight that the training data need not correspond directlyto the task, and contrast model-based predictions with external data-generating processes. This exposition supports our diagnosis of the issue in the discourse. We show that, like much of theliterature that followed it, the popular interpretation of aleatoric and epistemic uncertainty presentedin Gal (2016), Gal et al (2017) and Kendall & Gal (2017) is itself incoherent: it attaches multiple",
  "Workshop on Bayesian Decision-making and Uncertainty, 38th Conference on Neural Information ProcessingSystems (NeurIPS 2024)": "quantities to the same concepts. This conceptual overloading stems from attempting to encompass toomany ideas within an uncertainty decomposition that has a fundamentally limited expressive capacity.Its effect is to conate concepts that ought to be recognised as distinct. Returning to foundational ideas from Bayesian statistics, we identify an alternative decomposition ofpredictive uncertainty that can be cleanly linked back to quantities used in past work. We believe thiscould be a basis for clearer thinking in future work, helping the eld more quickly achieve its goals.",
  "Background": "The use of the terms aleatoric and epistemic in machine learning follows a history of use in theengineering literature. Special issues of Reliability Engineering and System Safety on Treatment ofaleatory and epistemic uncertainty (Helton & Burmaster, 1996) and Alternative representations ofepistemic uncertainty (Helton & Oberkampf, 2004) aggregated a considerable amount of discourse.More recent work includes that of Der Kiureghian & Ditlevsen (2009) and Helton et al (2010). That literature itself builds on a much longer thread of work on sources of uncertainty.Helton & Oberkampf (2004) wrote that this dual use of probability in the representation of bothaleatory and epistemic uncertainty can be traced back to the beginning of the formal development ofprobability in the late 1600s (Bernstein, 1996; Hacking, 1975; Shafer, 1978). Modern statistics textsreferring to the ideas, even if not the exact terms, include the work of Chernoff & Moses (1959). While the concepts of aleatoric and epistemic uncertainty had previously been used in machinelearning, for example by Lawrence (2012) and Senge et al (2014), they were popularised by Gal(2016), Gal et al (2017) and Kendall & Gal (2017). The prevailing mathematical denitions are theinformation-theoretic quantities used by Gal et al (2017), building on earlier work on Bayesian exper-imental design (Lindley, 1956) and active learning (Houlsby et al, 2011; MacKay, 1992a,b).",
  "Many quantities that arise in machine learning have been associated with the ideas of aleatoric andepistemic uncertainty. We set out to provide a clear synthesis of some key concepts": "Reasoning should start with the predictive task of interestWe consider prediction of y|x wherex X is an input and y Y is an output. We allow x = X = . This setup covers a wide range ofscenarios, from predicting the bias of a coin (X = and Y = ) to predicting the next word in asentence (X = Vl and Y = V where V is a vocabulary and l is the number of words so far). Training data need not correspond directly to the predictionTypically we have access to sometraining data, d1:n ptrain(d1:n), that can inform our prediction. It is common to assume di X Y.We emphasise that this is not necessary. The data could belong in some altogether separate space. Using a model allows data-driven predictionIn machine learning we work from training data topredictions through a model, pn(y|x) = p(y|x; d1:n). Here we focus on parametric models. Whilenot required for many of the quantities we consider, there can be stochastic parameters, pn() =p(; d1:n), within the model, dened such that pn(y|x) = Epn()[p(y|x, )]. We use p(y|x) todenote the model we converge to as n , which we assume is well dened. Predictions are generally distinct from data-generating processesSince we can have di X Y,the link between pn(y|x) and ptrain(d1:n) can be weak. Suppose d1:n represents the outcomes of nfair coin tosses and the task is to predict the coins bias, so X = and Y = . Then as n the predictive entropy (Shannon, 1948), H[pn(y|x)], can tend to zero while H[ptrain(d1:n)] = n log 2tends to innity. Even if di X Y and di ptrain(y|x)ptrain(x), it can still be the case thatpn(y|x) = ptrain(y|x) for all n due to model misspecication (Kleijn & van der Vaart, 2006). systems allow grounded evaluationComputing model-based uncertainties is not a gen-eral substitute for evaluating a model using a reference system, such as a person, physical sensor orcomputer program. Often this system performs the same predictive task as the model, and we canformalise evaluation as a comparison between the model and the reference system, peval(y|x), on aninput, x. If x is considered to be drawn from some peval(x), this commonly gives rise to expectedlosses of the form Epeval(x,y)[(pn(y|x), y)], often estimated using sampled (x, y) peval(x, y).",
  "Epistemic uncertainty": "A popular view on aleatoric and epistemic uncertainty attaches multiple quantities to each term.Some of these quantities can coincide in particular cases but in the general case they are distinct. The quotationshere are from Kendall & Gal (2017); the interpretation of Equation 1 is due to Gal (2016) and Gal et al (2017).",
  ".(1)": "Gal (2016) stated the total = aleatoric + epistemic relationship and the correspondence betweenpn(y|x) and the total uncertainty, while Gal et al (2017) made the explicit link to Equation 1, informedby Houlsby et al (2011). Kendall & Gal (2017) expanded on these ideas in a computer-vision context. Aleatoric and epistemic uncertainty as discussed in this work refer to multiple quantities (),introducing a number of spurious associations. The competing denitions of aleatoric uncertaintyconate a models irreducible predictive entropy, H[p(y|x)], with three separate quantities:",
  "(b) H[pn(y|x)] Epn()[H[p(y|x, )]], the BALD score evaluated at x. Issue: for nite n the BALDscore is only an estimator of H[pn(y|x)] H[p(y|x)] (Proposition 2)": "Other sources of confusion in this view on aleatoric and epistemic uncertainty include an incorrectassociation between a models subjective uncertainty and objective measures of performance, such asclassication accuracy ( in Kendall & Gal (2017)), along with misleading implications abouthow a models uncertainty will behave with varying n (.11-6.12 in Gal (2016) and inKendall & Gal (2017)) and varying distance from the training data (Aleatoric uncertainty does notincrease for out-of-data examples...whereas epistemic uncertainty does in Kendall & Gal (2017)).",
  "DecomposeEstimate": "The total predictive uncertainty (entropy) of a model, pn(y|x), trained on data d1:n can be decom-posed into actual irreducible and reducible components, which reect how the models uncertainty changes asn . Given nite n, these irreducible and reducible components must be estimated. This is made possibleby using stochastic model parameters, pn(), where pn(y|x) = Epn()[p(y|x, )].",
  "An alternative perspective": "Now we return to the goal of decomposing predictive uncertainty. If a models prediction is uncertain,we want to know whether that prediction is fundamentally uncertain for the given model class orinstead due to a lack of data. This breakdown has clear utility if our aim is to identify new datathat will reduce a models predictive uncertainty (Bickford Smith et al, 2023, 2024). But it is alsorelevant elsewhere. In model selection, for example, we might want to quantify a models scope forimprovement by forecasting how it will behave after training on more data. We formalise this using a Bayesian perspective, which corresponds to reasoning about data that hasnot yet been observed (Fong et al, 2023). More concretely we revisit BALDs foundations in theframework of Bayesian experimental design (Lindley, 1956; Rainforth et al, 2024) and focus on thecore idea of information gain. For a generic variable of interest, , the information gain is dened asthe reduction in entropy in that results from observing new data, d(n+1):(n+m):",
  "IG(d(n+1):(n+m)) = H[pn()] H[pn+m()] .(2)": "Setting to and averaging over possible data recovers BALD (Houlsby et al, 2011) for m = 1 andBatchBALD (Kirsch et al, 2019) for m > 1. Here we are interested in the task of predicting y|x, sowe instead set to y|x (Bickford Smith et al, 2023). Considering the resulting information gain inthe limit of innite new data, m , gives a decomposition of a models total predictive entropy:",
  ".(3)": "In practice we do not have the new data used in this denition. Instead we have to reason about whatthe data could be, giving rise to estimators of the irreducible and reducible predictive entropy. Only inthis context of estimation does a stochastic model become necessary: the decomposition in Equation 3is based on a Bayesian perspective but is well dened for deterministic models.",
  "Both sides can be seen as approximation errors": "There are cases where we can expect the estimators in Propositions 1 and 2 to be accurate. Wemight for example have x, y R and know that modelling y|x as Gaussian-distributed with equalvariance across all x is appropriate, and we might even know exactly what variance to use. Butin the general case these estimators can be highly inaccurate even if they are principled. in Bickford Smith et al (2024) demonstrates this: Bayesian deep learning can produce estimates ofirreducible and reducible predictive entropy that are severely at odds with the entropy changes thatoccur in practice. The conceptual map in combines the decomposition from Equation 3 with the estimatorsfrom Propositions 1 and 2. It thus connects Equation 3 back to Equation 1 while highlighting that thequantities in the latter should be seen not as direct measures of irreducible and reducible uncertaintybut instead as estimators that might not be accurate. This new perspective gives us a clearer basis forreasoning about predictive uncertainty than is provided by the aleatoric-epistemic view.",
  "Related work": "A number of different perspectives on aleatoric and epistemic uncertainty in machine learning havebeen put forward in recent years.These include a discussion of where uncertainty comes fromin machine learning (Gruber et al, 2023); a case against Shannon entropy for notions of predic-tive uncertainty (Wimmer et al, 2023); proposals for using alternative information-theoretic quan-tities (Schweighofer et al, 2023a,b, 2024); and various other suggestions for how to dene uncer-tainty, such as in terms of frequentist risk (Lahlou et al, 2023), class-wise variance (Sale et al, 2023b,2024b), credal sets (Hofman et al, 2024a; Sale et al, 2023a), distances between probability distribu-tions (Sale et al, 2024a) and proper scoring rules (Hofman et al, 2024b). In contrast with most of thatwork, our approach here has been to consider the minimal changes needed to address the issues wehave identied in existing discussions of aleatoric and epistemic uncertainty.",
  "Conclusion": "We have identied sources of confusion in the aleatoric-epistemic view on uncertainty in machinelearning and, to deal with this, we have presented an alternative perspective. A key fact underlyingthis work is the extent of the subjectivity of the uncertainties we have discussed. Our presentation ofthe ideas makes transparent the dependence on the model class and the amount of data. But it abstractsaway dependencies on other things, such as what exactly the data is and what algorithm is used tolearn from the data. While we believe this abstraction is useful for the high-level understanding wehave aimed to provide here, a precise analysis would need to account for these dependencies."
}