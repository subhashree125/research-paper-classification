{
  "Abstract": "Batch normalization is a successful building block of neural network architectures. Yet, it isnot well understood. A neural network layer with batch normalization comprises three componentsthat affect the representation induced by the network: recentering the mean of the representation tozero, rescaling the variance of the representation to one, and finally applying a non-linearity. Ourwork follows the work of Hadi Daneshmand, Amir Joudaki, Francis Bach [NeurIPS 21], whichstudied deep linear neural networks with only the rescaling stage between layers at initialization.In our work, we present an analysis of the other two key components of networks with batchnormalization, namely, the recentering and the non-linearity. When these two components arepresent, we observe a curious behavior at initialization. Through the layers, the representation ofthe batch converges to a single cluster except for an odd data point that breaks far away from thecluster in an orthogonal direction. We shed light on this behavior from two perspectives: (1) weanalyze the geometrical evolution of a simplified indicative model; (2) we prove a stability result forthe aforementioned configuration.",
  ". Apply a non-linearity F : R R coordinatewise to attain the new output: x(t) = Fz(t)": "In practice, many different tweaks are applied to these transformations to improve performance.Batch normalization (BN) Ioffe and Szegedy (2015) is a prime such example. It is a ubiquitouscomponent of NNs as it decreases the training time and the generalization error He et al. (2016);Huang et al. (2017); Silver et al. (2017) . A noteworthy aspect of BN is that the output correspondingto a single input depends on other inputs in the same batch.Let X(t) =x(t)1 x(t)nbe the output of layer t consisting of n column vectors that corre-spond to a batch of size n. Then, the output of every layer of a NN with BN is defined recursively asfollows.",
  "X(t) := FZ(t).(4)": "After applying BN during initialization (and training), the distribution of the preactivation valuesof each neuron has zero mean and unit variance (both during initialization and training). This wasthe original motivation for BN, namely, to reduce the rate at which the representations through thelayers change, or put differently, to reduce the covariate shift Ioffe and Szegedy (2015). Yet, this isnot the reason for its success Santurkar et al. (2018), and BN is still not well understood.In this paper, we follow Daneshmand et al. (2020, 2021) where fully-connected linear (no ReLU,sigmoid, etc.) neural networks were studied with only one part of BN appliedRS (part IV of). Under that framework at initialization, it was shown that a full-rank batch becomesorthonormal asymptotically as the depth of the network grows.",
  "Our Contribution": "Our contribution begins with a reference to in Daneshmand et al. (2020). That figuresuggests that to achieve low training error, NNs must maintain a high rank when representing thetraining set through the different layers already at initialization (NNs may form low-rank representa-tions at initialization, see Saxe et al. (2014)). Yet, this implication does not hold; see figure 2: a NNmay reach high training accuracy from low-rank representations at initialization. To demonstratethis, we set the appropriate He initialization He et al. (2015) (PyTorchs default variance in the codeof Daneshmand et al. (2020) is too small). clearly shows that the success of BN does notstem from a high-rank representation that it induces at initialization and that a more fine-grainedapproach is required to better understand BN.Indeed, as we demonstrate in figure 5, with NL the representation achieved by the hidden layersis not orthogonal. The more so, when applying BN without RC, the typical angle between a pairof vectors in say x(30)1, . . . , x(30)n(the representation after 30 layers) is approximately 60as isevident from (a)as opposed to a typical angle of 75 in the case of BNas is demonstratedin (b). A possible explanation of this difference is offered by : when BN withoutRC is used, the histogram of the activity of some neurons in the network looks like the one shown in(a), whereas standard BN with RC induces the same type of histogram as in (b) foralmost all neurons; untypical histograms, like the one represented in (a), induce strongercorrelations between inputs and hence the angle between vectors decreases.",
  "(b) Relation between input and output an-gles": ": The batch representation induced by the final hidden layer with RC and ReLU NL. Figure (a) is arandom two-dimensional projection of the final layers representations. The \"escaped\" point is marked in red.Figure (b) represents the angles between pairs of vector representations before and after the final layer. Thepoints marked in red represent the angles between the \"escaped\" point and any other point of the batch. To understand the effect of each stage of BN, we decompose BN into its three different com-ponents (NL, RC, and RS; see ), and complement the study of Daneshmand et al. (2020,2021) of RS only, by understanding the role of RC, NL, and how they interact. In correspondencewith , we focus in Section I on the isolated effect of the ReLU NL on the rank of therepresentations, where we show that the rank of the representation increases substantially after onelayer; in Section II, we focus on the isolated effect of RC and show that in linear networks it affectsonly the representation of the first layer; in Section III we study the representation induced whencombining the randomness of the initialization, NL, and RC; and in the discussion section, wecontemplate how all the different pieces of the BN puzzle fit together and suggest an alternativeinitialization scheme that was inspired by this theoretical work.Section III is our main contribution, and in its setting, we observe a curious behavior: afterenough layers, all the data points in the batch collapse to one point, except for a single odd datapoint that escapes far away from the cluster in an orthogonal direction. This behavior is illustratedin . On a high-level, such behavior may explain the typical histograms that appear for allneurons, as in (b), on which we will elaborate further in the Discussion section. We explainthe observation above via two theorems: Theorem 11 explains the behavior above for a simplified model that is more tractable: At eachlayer, instead of calculating the expected behavior over the typical Gaussian distribution overthe weights, we analyze the expected behavior over a simpler distribution. We show that thissimplified model captures the empirical behavior well. Theorem 14 deals with Gaussian random layers and provides a stability result. It states that aninitial configuration that consists of a cluster and one separate data point, as above, remainsgeometrically unchanged after the application of a randomly initialized network with RC andReLU activation.",
  "All the proofs of the theorems can be found in the Appendix": "NotationMatrices are denoted by capital letters such as X. For a matrix X, its i-th row is denoted by xi,and its i-th column by xi. Element (i, j) of X is denoted by Xij. The i-th element of a vector yis denoted by yi.The result of the operation denoted by X , where X is a d n matrix, and is a d 1column vector, is understood to be equal to a dn matrix Z whose elements are Zij = Xij i.Similarly, the result of the operation X",
  "We mention some related works that attempt to understand the underlying principle of BN boththeoretically and empirically": "Karakida et al. (2019); Arora et al. (2019); Bjorck et al. (2018); Lyu et al. (2022) studied somebeneficial properties of BN to explain its success, Frankle et al. (2020) trained NNs where only thetunable parameters of BN are trained, achieving surprisingly good results, and Lubana et al. (2021)tried to underpin some of the qualities of normalization layers in general. Yang et al. (2019) studied Deep NNs with BN and without residual connections and explainedwhy they are hard to train; Furusho and Ikeda (2020) and De and Smith (2020) further elaborated onthe synergy between skip connections and BN.Empirically, Wang et al. (2022) highlighted some of the shortcomings of BN in the Transformersarchitecture, and Li et al. (2019) pointed out some of the shortcomings of BN when used withdropout.",
  "I. ReLU NL Increases Rank": "Daneshmand et al. (2020, 2021) study linear networks (i.e., without NL) for which, to reachorthonormality of the vectors within the batch, the input batch needs to be assumed to be fullrank. This is because the rank of the batch representations cannot increase through the layers,since rank(AB) max (rank(A), rank(B)). In contrast, in a real setting, the NL increases therank of the batch through the layers. We prove this phenomenon in Theorem 3, which states thatapplying a random matrix followed by a ReLU NL over the training set induces full rank under a mildassumption. Hence, the NL provides a natural way to increase the dimension of the representationsX(t) of the batch at initialization. Theorem 3 uses the following geometrical quantity. Definition 1 Let X = (x1| |xn) Rkn be a k-dimensional batch of size n. We say that therow vectors w1, w2 Rk are equivalent, w1 w2, if sign(w1xi) = sign(w2xi) for all 1 i n.We denote the finite set of equivalence classes induced by this relation by {l} and define",
  "for wr+1 N (0, Ik)": "Theorem 3 Let X Rkn be a k-dimensional batch of size n and assume that no two columnsof X are collinear. Let W Rk be a random matrix with a countably infinite number ofrows, each of size k and i.i.d. entries N(0, 1). Define W (d) as the first d rows of W, y :=mind : rankReLUW (d)X= n, and := (X). Then,",
  "nni=1 xi": "Let us now compare Theorem 3 to Lemma 5 in Dittmer et al. (2020). The authors generalizethe notion of singular values of linear operators to ReLU neural layers and show that the singularvalues do not increase for such ReLU operators. Theorem 3 shows that such a generalization doesnot capture the spirit of singular values for the following reason. The number of non-zero singularvalues of an operator equals the dimension of its image. So, for example, by Lemma 5 in Dittmeret al. (2020), for a linear operator followed by ReLU with input dimension 2, the maximal numberof non-zero singular values is 2. This would suggest that the image dimension of the operator is atmost 2. Theorem 3 shows that, in fact, the image dimension scales linearly with the number of rows,so there should be many more non-zero singular values for a generalization of singular values of aReLU layer.",
  "II. Recentering": "For completeness, we refer briefly to the simple scenario where we use only RC over a linear network.When using RC alone without NL, the representation changes only at the first layer, where the meanis set to zero in every coordinate, and this is maintained through the layers because the mean ofevery coordinate remains zero after applying the following linear layers. That is, if i xi = 0, theni Wxi = 0 for any W. Therefore, when used alone, RC does not have any effect on the batchrepresentation after the first layer.",
  "III. Recentering Followed by ReLU NL": "In contrast to the previous section, RC has an effect through all the layers when it is followed by aReLU NL. displays a behavior that is consistent for all the batches that we experimentedwith: after enough layers, all of the data points collapse to a single cluster, and one odd data pointdeviates far away, in an orthogonal direction to the cluster.Let us give a high-level intuition about why the representation in emerges. Assumethere is one data point, say x1, that is far, to some degree, from the mean of the rest of the datapoints,namely, from the center of the cluster :=1",
  ". We then subtract the projected mean of all datapoints := 1": "nni=1 wxi.If w < 0, then wx1 > 0 and it is likely that many points in the cluster now havenegative entries.If w > 0, then wx1 < 0 and it is likely that only few points in the cluster now havenegative entries. 3. We apply ReLU to the datapoints.If w < 0, then the points in the cluster with negative entries all map to zero. We get atighter cluster, while the value of the projected odd point does not change.If w > 0, then x1 maps to zero with a few points from the cluster and the values of otherpoints do not change. In total, the effect on the representation of a projection with w > 0 is much greater thanthe case with w < 0 since in the former case much more points map to zero compared to aselected few in the latter (so the geometry does not change much on average). Hence, when movingfrom layer to layer, we expect the cluster to become tighter and move closer to zero fast, and theratio between the representation of x1 and the mean of the cluster to become large. This effectivelyreproduces the geometry in .A typical approach for explaining such behavior would follow two steps. First, for a single ReLUneuron with a corresponding row of weights w, calculate what would happen in expectation for therepresentation of the batch. Secondly, show that this behavior is concentrated around that mean whenusing many neurons.",
  "Simplified Model for Recentering + ReLU": "We suggest a simplified model to explain the behavior of . We start by noting that ReLUis homogeneous. That is, ReLU(x) = ReLU(x) for 0. Hence, instead of calculating themean over all projections w in Rd, as in equation (8), we can calculate the mean over the unit sphereSd1. Another simplification that we use is averaging over all standard unit vectors and their inversesSd := {e1, . . . , ed, e1, . . . , ed} instead of averaging over all possible angles:",
  "At first glance, the computationEwU(Sd)[] may seem too simplistic. For example, if we start with": "a three-dimensional data set, we need to sum only 6 terms. Yet, when we follow this model throughmany layers, we get an exponential build-up in complexity. In fact, the output of every new layer ofthe network consists of all possible projections in Sd of the output of the previous layer. Therefore,the number of summands doubles with each layer: if the batch representation has dimension d, thenafter one layer, its dimension increases to 2d (since |Sd| = 2d) after two layersto 22d, and after tlayersto 2td. In each step of the process, which corresponds to a layer, we project each dimensioni of the input representation onto the unit vectors ei and ei. This is why we can visualize thisbranching process as a (perfect) binary tree, where each vertex of depth t represents a neuron of thet-th layer of the network, see below.This model has a nice property: in contrast to the general model, which is induced by equation (8),in the simplified case, we can easily keep track of the effect of each neuron in the network (or vertexin the tree) on the representation induced by each layer. This holds because the output of each neurondepends only on its parent neuron from the previous layer. This means that for any branch that beginson some vertex in the tree, its analysis is independent of all the other disjoint branches; this makesthe analysis tractable.That said, if we start with input dimension d > 1, we will have d disjoint trees with no interactionbetween them. So separating the dimensions would yield a different odd point on every coordinate,and no unique odd point can be identified from the batch. It is worth mentioning that no odd pointcan be identified for Gaussian matrices if our batch is symmetric; in this case, we must wait until therandomness, from layer to layer, will artificially produce an outlier.Consequently, we work in our model with input dimension d = 1. That is, X(0) is a row vectorwith n elements. In this case, we can identify the odd point. If X(0) is ordered, it would be either x1or xn, and we can rigorously prove the high-level intuition, as in items 1, 2, and 3 above. Remark Empirically, we observe that the odd point would typically be the one with the largestnorm. This is compatible with the description above. So analogously, our Theorem 11 holds for inputdimension d > 1 under the condition that there are two points such that for each coordinate: (1) theentries of one point are greater than all other entries; (2) the entries of the second point are smallerthan all other entries. Let us now introduce our model and let X(t) be the representation of the batch at layer t. Sincethe starting batch is made up of n one-dimensional points, X(0) is a row vector with n elements, and,due to the branching process described above, in general, X(t) is a matrix of size 2t n. Each of itscolumns, which we denote by x(t)ifor i {1, 2, . . . , n}, is the representation of one data point atlayer t of the network. Each of its rows, which we denote by x(t)jfor j {1, 2, . . . , 2t}, representsthe output of one neuron at layer t of the network. The projection of X(t) over all standard unitvectors and their inverses, i.e., over all vectors in S2t+1, is exactly equivalent to performing the matrixmultiplication W (t+1)X(t), where",
  ",(9)": "and Id is the identity matrix of size d. In fact, for every i {1, 2, . . . , 2t}, the output of neuron i,that is, the i-th row of X(t), generates two neurons outputs at the next layer, one corresponding tothe projection according to ei, and the other according to ei. After the projections, RC, and theReLU non-linearity are applied to each neurons output. The result is the batch representation atlayer t + 1, X(t+1), which is therefore given by the equation",
  "where x(t) = 1": "nni=1 x(t)i .It is clear from the given description that the evolution of each neurons output from layer t tot + 1 does not depend in any way on the other neurons of that layer. Hence, we can focus on eachneuron independently. In particular, we can say that, for every i, the output of neuron i, i.e., x(t)i ,undergoes two different transformations, a positive one, which consists of the projection according toei followed by RC and ReLU, and a negative one, which comprises the projection according to ei,RC and ReLU. The following is a formal definition of these two transformations.",
  "nni=1 xi. Wesay that y Rn is the result of a negative transformation applied on x Rn if yi = ReLU(xi + x)for every i [n]": "A visual representation of the described process as a binary tree is shown in , where eachnode of the tree represents the output of the corresponding neuron, and each edge connects eachoutput with the two outputs generated at the next layer after the positive and negative transformations.The first result that we will present about our model is that the outputs of the neurons get moreand more clustered as we go through the layers of the network. Therefore, the following definitionwill be useful. : Partial example of the first three layers of the tree generated by the process of positive/negativetransformations analyzed in this paper, starting from a one-dimensional batch with n = 5 elements. Differentelements have different shapes, to make it easier to follow their change of position. The average of the vectorat each step is denoted by x. Definition 6 A vector x = (x1, x2, . . . , xn) Rn with possibly repeated entries is composed of kclusters if it contains exactly k unique entries. Denote the sequence of those unique k entries of xin ascending order by C(x) := (c1(x), c2(x), . . . , ck(x)), and its lengthby c(x) := |C(x)|; clearly,k n.",
  "With these definitions available, we are ready to present the first result of this section, on theclustering of the neurons outputs": "Theorem 10 Suppose that a vector x(0) Rn with all distinct components undergoes an infinitesequence of positive or negative transformations, resulting in the sequence of vectors {x(t)}t0, viz.,for every t 0, x(t+1) is the result of a positive or a negative transformation applied on x(t). Then,there exists a finite t0 0 such that x(t0) is stable. Essentially, Theorem 10 says that, under our simplified model, there is a layer t0 in the networkafter which each neurons output, i.e., each row x(t)iof the matrix X(t), for every t t0, is a vectorcomposed of at most 3 clusters. This is because, at any layer, each neurons output is generated by asequence of positive and negative transformations starting from X(0).Our next step is to study the asymptotic geometry of the batch as the number of layers increases.In particular, we want to study the behavior of the (normalized) inner product between any twodatapoint representations, i.e., any two columns x(t)iof X(t), when the number of layers goes toinfinity. Formally, for any i, j {1, 2, . . . , n}, we are interested in the quantity",
  "in the limit of t , where x(t)iis the i-th column of X(t), and , and denote the standardEuclidean inner product and norm, respectively. To that end, we prove the following theorem": "Theorem 11 Let X(0) Rn be a row vector with all distinct components, and assume, withoutloss of generality, that its entries are increasingly ordered. Let X(t) be the 2t n matrix defined inequation (10) for t N, and denote its i-th column by x(t)i . Then, the following facts are always true.",
  "(12)": "Interpretation of the theorem. The theorem shows that, if a one-dimensional batch X(0) evolvesaccording to our model, after a number of layers large enough, the vector representations of thedata points converge to a configuration with the following properties: the representations of thelargest and smallest starting datapoints become orthogonal (item 1 of the Theorem); the datapointrepresentations are all clustered together except for one or two points (item 2); one of the points\"escapes\" far away from the other clustered points, in such a way that the ratio between the norm ofthe escaped point and that of any point in the cluster is (n) (item 3), and the angle between theescaped point and the cluster is approximately 90 (item 4).It is important to note that this behavior, derived from the simplified model described at thebeginning of the section, is remarkably similar to what happens in the general case of a network withrandom weights, recentering, and ReLU non-linearity. This behavior, which we described in theintroduction, is depicted in above. Hence, our model, even if simpler (and easier to analyze),approximates well the geometrical evolution of a batch in the general random case.",
  "An Invariant Geometry for Recentering Followed by ReLU": "The following definition is an obvious candidate for an invariant representation under RC+ReLU. Itconsists of one odd point and a cluster where all points are identical. The cluster and the odd pointare orthogonal, and the cluster has a much smaller norm. Such a configuration closely resembles thekind of geometrical structure that we see in practice, see .",
  "n22n+2": "Finally, the following theorem shows that when starting from a representation somewhat similarto an invariant representation, the next layer brings us closer to an invariant representation inexpectation. This is true since items 14 in Definition 12 correspond to items 14 in Theorem 14. Asthe correspondence between items 13 is natural, we focus on item 4. Indeed, after passing through alayer, it is not likely that the cluster would collapse to a single point as in Definition 12. Nonetheless,item 4 shows that the cluster contracts, while keeping the scale of the representation of x1 the same.",
  "Discussion": "It remains to be seen how all the different components of BN interact with one another and what arethe exact properties that are responsible for the success of BN. Observing the histogram in (b)and combining it with the insights appearing in this paper and previous work, we explore a newinitialization scheme: The histogram in (b) implies that the neuron is mostly inactive or withsmall intensity for most inputs.Our work shows that with no rescaling and at initialization, neurons are active with a largeintensity only for the odd data point. Combining this understanding with the insight that orthogonalrepresentations might be beneficial, suggests associating every data point with a unique neuron. Thatneuron will fire only for its associated input. This way, we attain an orthogonal representation andsparse activity in the network, as we observe in our work.Initial experimentation suggests that this might be a good initialization strategy for small datasets, as it would be computationally hard to associate a neuron to every data point since some datasetscontain millions of datapoints.",
  "Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batchnormalization. In International Conference on Learning Representations, 2019. URL": "Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch normal-ization. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-nett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Asso-ciates, Inc., 2018. URL Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Y. Bengio, D. Schuurmans,J. Lafferty, C. Williams, and A. Culotta, editors, Advances in Neural Information ProcessingSystems, volume 22. Curran Associates, Inc., 2009. URL Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi.Batch normalization provably avoids ranks collapse for randomly initialised deep networks.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advancesin Neural Information Processing Systems, volume 33, page 1838718398. Curran Asso-ciates, Inc., 2020. URL Hadi Daneshmand, Amir Joudaki, and Francis Bach. Batch normalization orthogonalizes represen-tations in deep random networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, andJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,page 48964906. Curran Associates, Inc., 2021. URL",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassinghuman-level performance on imagenet classification. CoRR, abs/1502.01852, 2015. URL": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for imagerecognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pages 770778, 2016. doi: 10.1109/CVPR.2016.90. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q. Weinberger. Densely connectedconvolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR), pages 22612269, 2017. doi: 10.1109/CVPR.2017.243. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training byreducing internal covariate shift. In International conference on machine learning, pages 448456.PMLR, 2015. Ryo Karakida, Shotaro Akaho, and Shun-ichi Amari. The normalization method for alleviatingpathological sharpness in wide neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,F. d'Alch-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information ProcessingSystems, volume 32. Curran Associates, Inc., 2019. URL Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang. Understanding the disharmony between dropoutand batch normalization by variance shift. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pages 26822690, 2019. Ekdeep S Lubana, Robert Dick, and Hidenori Tanaka. Beyond batchnorm: Towards a unifiedunderstanding of normalization in deep learning. Advances in Neural Information ProcessingSystems, 34:47784791, 2021.",
  "Kaifeng Lyu, Zhiyuan Li, and Sanjeev Arora. Understanding the generalization benefit of normaliza-tion layers: Sharpness reduction. arXiv preprint arXiv:2206.07085, 2022": "Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-ization help optimization? In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Cur-ran Associates, Inc., 2018. URL Andrew M. Saxe, James L. Mcclelland, and Surya Ganguli. Exact solutions to the nonlineardynamics of learning in deep linear neural network. In In International Conference on LearningRepresentations, 2014. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, FanHui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis. Masteringthe game of go without human knowledge. Nature, 550:354, October 2017. URL",
  "(b) With RC": ": Histograms that represent the activity for a neuron in a network with and without RC. WithoutRC, some neurons exhibit neural activity like the one in Figure (a): the neuron has significant response for alarge number of inputs. With RC, most of the neurons behave according to Figure (b): the neural response isnegligible for most of the inputs.",
  "sign (wr+1xj) = sign (wxj)j = i.(15b)": "Furthermore, equation (15) for [r], such that w = 0, holds with probability of at least X(t+1).Take some [r] for which w = 0 (there must exist such a row by the lemma assumption), anddenote J := {j [t + 1] : wxj > 0, aj = 0}. Then, according to equation (14a),",
  "j=1aj ReLU (wr+1xj) = wr+1 (u aixi) = aiwr+1xi = 0(17)": "for wr+1 that satisfies equation (15); we will show next that such wr+1 constitute an equivalenceclass, meaning that equation (17) holds, again, with probability of at least X(t+1).We are left with proving that the set of wr+1 that satisfy equation (15) for a given for whichw = 0, and some i, consitutues an equivalence class of non-zero volume. To that end, denote",
  "exp {n ( 1 log )}(28d)": "where equation (28a) follows from the definition of {yt}; equation (28b) follows from Chernoffsinequality by recalling that y1, . . . , yn are independent and that their (marginal) CDFs are majorizedby that of a geometric distribution with success probability ; and equation (28d) follows from themonotonicity in of the expression in equation (28c).",
  "C. Proof of Theorem 10": "By lemma 7, the sequencecx(t): t N {0}is non-increasing. Assume that x(0) is not stable,i.e., cx(0)> 3, as otherwise the result is trivially true. It suffices to prove that there exists a finite N such that x() is more clustered than x(0). We start by proving this result in the case where theinfinite sequence of transformations involves only positive transformations. Assume by contradictionthat x(t) is not more clustered than x(0) for all t N, i.e.,",
  "x(t),(33)": "from which it follows that limt x(t) = 0 since nt 2 for all t N. This contradicts equation (32),thus proving the result.The case of a finite number of negative transformations simply follows from the previous case,by repeating the previous proof from layer t N that corresponds to the last negative transformationin lieu of layer 0.Finally, consider the case where there are an infinite number of negative transformations, andassume again equation (29) by contradiction. Consider any two steps t1 and t2 of the sequence suchthat a negative transformation occurs at t1 and t2, and only positive transformations occur betweenthese two steps, viz.",
  "x(t11)(35f)": "where equation (35b) follows from equation (34c) and the assumption in equation (29), whichsuggests that exactly one of the clusters will be nullified by the ReLU activation, as otherwisecx(t21)= 1 or cx(t2)< cx(t21), in contradiction to equation (29); equation (35c) follows from the definition of nt2, x(t21)max, and x(t21); equation (35d) follows from equation (33); equa-tion (35e) follows from equation (34a) by noting that x(t1)max = x(t11); and equation (35f) holds sincex(t1) > 0 and nt2 2 by equation (31).Hence, a sequence of negative transformations, with each pair of consecutive such transformationspossibly separated by several intermediate positive transformations, contracts the vector mean,meaning that limt x(t) = 0. However, since equation (32) still holds for this case, we again reacha contradiction, and the theorem is proved.",
  "Due to Theorem 10, there exists a t0 1 such that for every t t0, every row/neuron of X(t) isa vector composed of at most 3 clusters. To improve the readability and ease of understanding of": "the proof, we first prove the theorem neglecting the presence of three-cluster rows. This is becausethe main ideas are already present in this simpler case, and three-cluster rows only introduce someunenlightening technicalities to the proof. We address these technicalities in Appendix E, where weprove that three-cluster rows do not invalidate the result.To prove the first point of the theorem, notice that, due to the fact that the components of eachrow of X(t) are ordered, each cluster is composed of consecutive elements. Hence, the first and thelast element of each row are always in two different clusters: either the first element is zero and thelast is strictly positive, or vice versa. Hence, the product of these two elements is always zero, andtherefore x(t)1 , x(t)n = 0.To prove the second part of the theorem, consider any row of X(t) that is composed of twoclusters, and recall that one of the two clusters must be at zero, and the other one at a positivecoordinate. We will refer to those clusters as the zero cluster and the positive cluster respectively.After T additional layers, the selected row undergoes any possible sequence of T positive and/ornegative transformations, generating a total of 2T rows in the matrix X(t+T). We want to study thecontribution of these 2T rows to the inner product between any two columns of X(t+T).Let i, j {2, . . . , n 1} be the two selected columns. First of all, notice that the 2T rowsgenerated by a given row at layer t, will contribute to the inner product between i and j only ifelements i and j of the given row are in the same cluster at layer t; otherwise, one of the twoelements will necessarily be zero. Furthermore, of the corresponding 2T rows at layer t + T, thosethat contribute to the inner product are only the ones in which the cluster containing i and j is thepositive one. Hence, the given neuron at layer t has to undergo a sequence of positive/negativetransformations in such a way that after T layers, the cluster containing i and j is the positive one.Keeping all of this in mind, the next step is to study how a two-cluster vector changes after agiven sequence of T positive/negative transformations. Consider any vector of n elements composedof two clusters: one cluster is at coordinate 0 (the zero cluster) and it is made of n0 elements, whilethe second cluster (the positive cluster) is at coordinate c > 0 and it is made of nn0 elements. Fromnow on, we say that such a vector has composition (n0, n n0). After one positive transformation,the zero cluster remains unchanged, while the coordinate of the positive cluster becomes",
  "n c.(36)": "After a negative transformation, instead, the zero and positive cluster switch roles: the positive clustergoes at coordinate 0 and becomes the new zero cluster, while the old zero cluster becomes the newpositive cluster. Thus, the new vector has composition (n n0, n0), and the new coordinate of thepositive cluster is equal to",
  "nc.(37)": "See for a visual representation of this process. Thus, one can see that what really determinesthe new coordinate of the positive cluster after one transformation is the number of points in the zerocluster after the transformation: in the first case, the number of points is n0, while in the second caseit is n n0. After T transformations, in which for k times the zero cluster contains n0 points, andfor T k times it contains n n0 points, the final coordinate of the positive cluster is",
  "Tkc.(38)": "Hence, for a row x(t)at layer t in which elements i and j are in the same cluster with n elements,all the neurons generated from it after a sequence of T transformations in which their cluster endsin a positive coordinate c, contributes to the inner product, with a value equal to c2. Followingequation (38), the total contribution is",
  "where c is the coordinate of the positive cluster of x(t) . Note that the leading term ( nn": "n)2 and theT 1 exponent appear because, after any sequence of T 1 transformations, the T-th and last one isfixed, since the cluster containing elements i and j must end in a positive coordinate. As one can see,all contributions decrease exponentially with T, and the exponent depends on the composition of thetwo clusters. The compositions that dominate are the most unbalanced ones: those with one point inone cluster and n 1 points in the other. Thus, we can limit our attention to the leaves belonging tothe set",
  "with elements i, j in the same cluster}(41)": "since, asymptotically as T goes to infinity, these are the only ones that contribute to the inner product.Note that for any pair i, j, such unbalanced configurations always exist, since there is always a pathof positive/negative transformations that leads to them. Hence, the set L(t)i,j is not empty. Furthermore,if a cluster contains n 1 elements, then these elements must be either {2, ...n} or {1, . . . , n 1}.Hence, all elements in {2, . . . , n 2} must belong to the same cluster. Thus, the set defined inequation (41) contains the same neurons for any pair i, j {2, . . . , n 1}, and we can define asingle set independent of i, j:",
  "L(t)c2(43)": "which is independent of the actual choice of i and j.Regarding the norm of a single element i, the same reasoning applies, with the differencethat the rows that contribute to it are all those in which i is in the positive cluster. Again, thedominant configurations are the most unbalanced ones, and since i and j are not extreme points, thedominant configurations that contribute to the norms x(t+T)i and x(t+T)j are exactly the same",
  "n )2)T 0 as T": "As shown in Appendix A, the contribution of three-cluster configurations does not change thevalidity of equation (44), so the second part of the theorem is proved.To prove the third part, recall that, from the previous discussion, we showed that, asymptoticallyas T , the norm of any vector x(t+T)ifor i {2, . . . , n1} is equal to, following equation (43),",
  "L(t)c2.(45)": "Next, notice that the vectors belonging to L(t) can have two different compositions, namely (1, n1)and (n 1, 1). While the rows with these two compositions have the same contribution to the normof a vector with index between 2 and n 1, this is not true anymore for the extreme vectors x(t+T)1and x(t+T)n. In fact, for a row at layer t with composition (1, n 1), out of the 2T rows generatedby it at layer t + T, those that contribute to x(t+T)1 and those that contribute to x(t+T)i fori {2, . . . , n 1} are complementary sets. If instead the starting configuration is (n 1, 1), thenthe rows that contribute to the two norms are exactly the same, since points 1 and i belong to thesame cluster. A similar reasoning applies also to x(t+T)n.",
  "E. Contribution of three-cluster configurations to Theorem 11": "In Theorem 11, rows at layer t which are composed of three clusters must also be taken into account.To complete the proof of the second point of the theorem, we now show that their total contributionto the numerator and the denominator in equation (11) are asymptotically equal, and thereforeequation (44) still holds in the general case.First of all, for the same reasoning as in the two-cluster case, a three-cluster row at layer t willhave a non-negligible contribution at layer t+T only if at some point during the process between layert and layer t + T, it generates two-cluster rows with composition (1, n 1) or (n 1, 1); otherwise,its contribution will be negligible compared to the contribution of the two-cluster configurationsbelonging to L(t). Such three-cluster rows must have composition (1, 1, n 2) or (n 2, 1, 1) the cluster in the middle must contain only one element by construction. Furthermore, once again byconstruction, a three-cluster row at layer t generates 2T rows at layer T, out of which exactly one iscomposed of three-clusters row, and the other 2T 1 are two-cluster ones.The contribution of the single three-cluster row is negligible, and this can be proved even byusing very loose bounds. In fact, for any three-cluster configuration, let c be the coordinate of thelargest cluster. Then, after one layer, both the new coordinates of the middle and the largest clusterscan be upper bounded by n1",
  "nT c. Hence, its contribution to the inner products or to the norms will be at most n1": "n2T c2,which has a negligible exponent compared to equation (40).Regarding the contribution of the other rows, first notice that, if the elements i and j are bothin the largest cluster at layer t, they will remain paired (i.e., in the same cluster) in all the rowsgenerated at layer t + T. Thus, as before, the contribution of the rows will be exactly the same in theinner product at the numerator of equation (11), and in the product of the norms in the denominator.The remaining case is that in which one element is in the middle cluster, and the other is in the largestcluster. Notice that, at each step, a three-cluster row generates two rows: one has a three-clusterconfiguration, while the other one has two clusters, with composition (n 1, 1) or (2, n 2) (see for a visual example). Thus, at layer t+T, the two-cluster rows have composition (n1, 1),(1, n 1), (2, n 2) or (n 2, 2). The rows with compositions (n 1, 1) or (1, n 1) give, onceagain, the same contribution to the numerator and the denominator of equation (11). Instead, thosewith composition (2, n 2) or (n 2, 2) give different contributions: they give zero contribution tothe inner product, since the two elements under consideration belong to different clusters, but they contribute to each of the norms. However, asymptotically their total contribution is negligible. In fact,the contribution to both x(t+T)i2 and x(t+T)j2 can be upper bounded, thanks to equation (40), by",
  "(T1)(62)": "which has a negligible exponent with respect to equation (40). Hence, we proved that three-clusterconfigurations do not affect the validity of equation (44), and the second part of the theorem isproved. For the third part of the theorem, the three-cluster configurations that contribute to the norms areagain only those of the form (1, 1, n 2) or (n 2, 1, 1), since those are the only non-negligibleones. First, let us consider a three-cluster row at layer t of composition (1, 1, n 2) and an elementi {3, n 2}. We want to study the cumulative contribution at layer t + T of all the rows generatedby the selected one at layer t. Notice that, at layer t + T 1, one generated row is of composition(1, 1, n 2), and the others have composition (1, n 1) or (2, n 2). The contribution of thesingle three-cluster row is negligible, as we already discussed above. For a row of composition(1, n 1), let c(T) be the value of the coordinate of the positive cluster at layer t + T 1 (whichdepends exponentially on T). Of its two children at layer t + T, one contributes to the norm ofx(t+T)iand x(t+T)nan amount equal to1n2 c2(T), and the other one to the norm of x(t+T)1an amount",
  "x(t+T)iand x(t+T)nan amount equal to4n2 c2(T), and the other one to the norm of x(t+T)1an amountequal to n2": "n2 c2(T). A similar reasoning can be applied to three-cluster neurons at layer t withcomposition (n 2, 1, 1). Putting everything together, define A(T) to be the sum of the squares ofthe positive coordinates of all rows of composition (1, n 1) at layer t + T 1, let B(T) be thesame for rows of composition (n 1, 1), C(T) for rows of composition (2, n 2), and D(T) forrows of composition (n 2, 2). Then, we have:",
  "(n 2)2(73)": "which proves the theorem, for the case of i {3, . . . , n 2}. To complete the proof of part three,the case of points i = 2 and i = n 1 must be considered. We analyze the case i = 2, since theother case follows in the same way by symmetry. Let be any three-cluster row at layer t withcomposition (1, 1, n 2). During the next T layers, point i = 2 will get paired with point 1 only in(2, n 2) cluster configurations, whose contribution compared to (1, n 1) ones is negligible, perequation (62). The only other two-cluster configurations generated during the process are (1, n 1)ones, in which point 2 is always paired with point n. Thus, for those clusters, due to equation (40), thefinal contribution at layer t + T will be of the form 1",
  "and of the form n1": "n2 e(T) for x(t+T)n2. Denote by D(T) the cumulative exponent for all(1, 1, n 2) rows at layer t. Furthermore, denote by A(T) and B(T) the cumulative exponent fortwo-cluster rows with composition (1, n 1) and (n 1, 1) respectively, whose analysis was carriedout in the proof of the two-cluster case in the main section. Following the discussion above, we getthe following asymptotic formulas for large T,",
  "n 2.(88)": "For the case i = 2 (the case i = n 1 follows again by symmetry), let again A(T), B(T), C(T) andD(T) be the cumulative exponents for clusters with composition (1, n 1), (n 1, 1), (1, 1, n 2),(n2, 1, 1) respectively. Notice that clusters with composition (1, 1, n2) asymptotically contributeonly to X(t+T)i, X(t+T)n, since point 1 and 2 get paired only when clusters of composition (2, n2)are formed in the process from layer t to t+T, which are asymptotically negligible per equation (62).The opposite is true for clusters of composition (n 2, 1, 1). Hence, asymptotically for large T, wehave the formulae",
  "F. Proof of Theorem 13 and Theorem 14": "For both theorems, it is sufficient to calculate the expectation for a single row w of W. This holdssince the quantities of interest may be separated as a sum, where each summand corresponds to aunique row, and since all the rows are i.i.d each row has an equal contribution. For example, let usstart by showing that norms of vectors may be scaled uniformly to our desire, in expectation, whenchoosing a suitable .",
  "2x2(98)": "The second equality follows from the definition of the scalar product, and the third equality is by thelinearity of expectation. The fourth equality is less trivial and follows by noticing that N := wx N0, 2 x2. Since ReLU(x) = 0 for negative x, we get that E ReLU(N)2 = 1",
  "x(t+1)1 c = ReLU (W x1) ReLU (W xn+1) = 0,": "which concludes the proof of item 3.Finally, item 4 is trivial since xi = c for all 2 i n and f(x) = E ReLUWx (t)is afunction of x.The proof of items 1, 2, and 3 of Theorem 14 follows the same lines of the proof for Theorem 13,this time with =n2 (n1)2R2 .For item 4, we require a more intricate expectation calculation than in equation 95. We need theexpected inner product between ReLU(wx) and ReLU(wy). By equation 6 in Cho and Saul (2009)we have:"
}