{
  "Abstract": "Content Warning: This paper contains examples of offensive transphobic content.Natural-language assistants are designed to provide users with helpful responseswhile avoiding harmful outputs, largely achieved through alignment to humanpreferences. Yet there is limited understanding of whether alignment techniquesmay inadvertently perpetuate or even amplify harmful biases inherited from theirpre-aligned base models. This issue is compounded by the choice of bias evaluationbenchmarks in popular preference-finetuned models, which predominantly focuson dominant social categories, such as binary gender, thereby limiting insightsinto biases affecting underrepresented groups. Towards addressing this gap, wecenter transgender, nonbinary, and other gender-diverse identities to investigate howalignment procedures interact with pre-existing gender-diverse bias in LLMs. Ourkey contributions include: 1) a comprehensive survey of bias evaluation modalitiesacross leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 12models spanning Direct Preference Optimization (DPO) stages, uncovering harmspopular bias benchmarks fail to detect, and 3) a flexible framework for measuringharmful biases in implicit reward signals applicable to other social contexts. Ourfindings reveal that DPO-aligned models are particularly sensitive to supervisedfinetuning (SFT), and can amplify two forms of real-world gender-diverse harmsfrom their base models: stigmatization and gender non-affirmative language. Weconclude with recommendations tailored to DPO and broader alignment practices,advocating for the adoption of community-informed bias evaluation frameworks tomore effectively identify and address underrepresented harms in LLMs.",
  "Introduction": "Aligning large language models (LLMs) with human preferences has emerged as a promising approachfor instilling helpful model behavior across a wide array of language tasks [1; 2; 3; 4]. However,these models must serve diverse communities effectively to achieve their intended impact. Pre-trainedLLMs, from which aligned models are finetuned, can perpetuate harmful biases that disproportionatelyaffect marginalized groups, including transgender, non-binary and other gender-diverse minorities[5; 6; 7; 8; 9]. Although alignment techniques like Direct Preference Optimization can steermodels towards helpful and harmless behavior [11; 12; 13], there is limited understanding of howharmful social biases already present in their base models may persist or even amplify after alignment.",
  "Winobias": "BBQ Discrim-Eval BOLD : Bias benchmarks employed by top 15 performing preference-tuned LLMs reported by Chatbot ArenaLeaderboard across socially-relevant categories. Evaluations fully cover binary gender bias, with limitedevaluation for gender-diverse minorities and other socially-salient dimensions. Current bias evaluations for preference-finetuned LLMs often rely on benchmarks grounded inWestern social norms, restricting our understanding of bias and perpetuated harms to evaluations thatreflect these dominant perspectives. illustrates this limitation in popular preference-finetunedLLMs, where employed bias benchmarks consistently assess binary gender bias, while offeringlittle to no coverage for gender-diverse identities and other socially salient dimensions. This narrowfocus creates two issues: (1) binary gender-exclusive measurements of LLM harms risk leavingbiases affecting gender minorities unchecked and (2) it further entrenches cisnormative hegemoniesin competitive LLM benchmarking, encouraging other models to mirror these evaluation practices.[15; 16; 7]. While expanding existing evaluations to include more groups is a step forward, doing sowithout proper construct validation risks neglecting significant power asymmetries that marginalizedcommunities face [17; 18; 19]. These gaps motivate our investigation into how alignment proceduresinteract with LLM gender biases, particularly those affecting transgender, nonbinary, and othergender-diverse identities (TGNB) - an area that remains underexplored in alignment literature despitesubstantial evidence of TGNB harms in pretrained LLMs [5; 6; 9; 7; 8]. Towards addressing theseevaluation gaps, we focus on two key research questions: (1) Can pre-existing gender-diverse biasesbe amplified even after alignment towards helpful and harmless behavior? (2) How are these biasesencoded in the alignment process? Contributions.To answer these questions, we systematically evaluate 12 LLMs for gender-diversebias across their pre-trained, supervised fine-tuned (SFT) , and aligned versions with DirectPreference Optimization (DPO) . Using TGNB-specific benchmarks designed to capture real-world harms, we examine whether these biases persist in aligned models despite finetuning forhelpfulness and harmlessness. As alignment procedures rely on preference-derived reward signals[21; 10], we propose a flexible framework to uncover bias patterns in these signals, offering insightsinto mechanisms of bias transfer. Demonstrated with gender-diverse biases yet adaptable to othersocial contexts, this framework allows us to trace bias propagation across alignment stages, revealingavenues for targeted bias intervention. This framework is coupled with a thematic analysis of TGNBstigma found across DPO-aligned LLMs. Our findings reveal that aligned LLMs can not only perpetuate but also amplify existing TGNBbiases found in their base models, aspects which cannot be detected by popularly employed LLMbias benchmarks. Specifically, we observe that DPO-aligned LLMs can (1) exacerbate gender non-affirmative outputs when initialized from SFT models displaying similar biases (4.1) (2) reflectimplicit reward signals reinforcing TGNB social stigma and (3) preserve biased reward signalsfrom their base models (5.3). Furthermore, our thematic analysis reveals aligned LLMs can reflectdocumented patterns of TGNB stigma, including harmful stereotypes that characterize gender-diverseindividuals as mentally unstable (5.3). Guided by our findings, we offer recommendationscontextualized to both bias evaluation and alignment procedures to better understand and addressthese harms (6).",
  "Background and Related Works": "Survey of Bias Evaluation Modalities in Preference-Finetuned LLMs presents a surveyof bias evaluation modalities for the top 15 preference-finetuned LLMs reported by the Chatbot ArenaLeaderboard. 3 We review public model documentation for all models at the time of writing this paper,finding inconsistent bias evaluation standards, with some LLMs lacking documented bias evaluationaltogether. Red teaming is the most frequently employed method, though its implementation andreporting standards vary widely some models detail comprehensive protocols, though commercialmodels tend towards proprietary assessments. Beyond red teaming, bias evaluation is either coupledwith or solely conducted over public bias benchmarks, each targeting different concerns: truthfulness(TRUTHFULQA ), discrimination (DISCRIM-EVAL), toxicity (REALTOXICITYPROMPTS),or skewed outputs (BOLD , WINOGENDER, WINOBIAS, BBQ). Most notably, we identify a striking gap in bias evaluations specific to preference-finetuned LLMs.While these LLMs critically rely on reward signals to align to human preferences [21; 10], evaluationof biased rewards and preferences remains nearly absentonly Claude models document biasevaluations conducted over a held-out set of preferences. This gap motivates our investigation ofgender-diverse bias in preference-aligned LLMs through both TGNB-informed bias benchmarks anda systematic analysis of how societal biases may be encoded in preference-based reward signals. Gender Bias in Language ModelsResearch on gender bias in LLMs has largely centered binarygender, examining bias propagation and amplification within male-female or man-woman dichotomies[31; 32; 33]. Recent LLM alignment research has explored detecting and mitigating gender biasthrough preference dataset refinement , logit-based probing techniques , knowledge-editing, and LLM judges for automated gender bias detection . Yet these works remain pre-dominantly situated within binary conceptualizations of gender, leaving broader gender-diverseconsiderations largely unexplored. Bias benchmarks employed by leading preference-finetuned LLMs seem to face similar constraints(). We find employed benchmarks capture different aspects of LLM bias, though focusprimarily on binary gender, neglecting other social and demographic factors.4WINOGENDER,WINOBIAS exclusively cover binary gender identities in occupational stereotypes. BOLD evaluatesfairness in open generation across multiple domains, but its gender bias assessments remain confinedto binary categories. DISCRIM-EVAL and BBQ extend gender identity coverage but still face significantlimitations: DISCRIM-EVAL includes only non-binary as a gender-diverse identity category andmeasures LLM discrimination based on hypothetical scenarios rather than documented social harms. 3 exclude REALTOXICITYPROMPTS and TRUTHFULQA as they measure toxic degeneration fromneutral prompts and general falsehoods, respectively, rather than bias against targeted demographic or socialcharacteristics.",
  "BBQ measures LLM reflections of attested social bias and includes transgender man/woman genderidentities, though this inclusion remains undocumented in its original paper": "A growing body of TGNB-centric NLP literature offer targeted evaluations through community-grounded knowledge to study LLM harms against gender minorities, though they remain underutilized.TGNB bias evaluations commonly measure misgendering [5; 6; 9; 8], TGNB stigma [38; 39], andgender non-affirmative language reflective of real-world TGNB experiences . TANGO measuresbiased associations for gender disclosure in pretrained causal language models, while WINOQUEER studies a wider set of LGBTQIA+ stigma within masked language models. Our study employsthese two datasets, expanding investigation beyond pretrained LLMs to examine (1) the propagationof gender-diverse bias in model outputs across DPO alignment stages and (2) to what extent implicitreward signals can encode harmful TGNB biases.",
  "Evaluating Chat-based LLMs with Human Feedback": "We first introduce an overview of preference finetuning and our experimental setup before assessingLLM biases against gender minorities with respect to 1) their textual outputs 2) preference data and3) implicit reward signals. Preference Fine-tuning OverviewLLM preference fine-tuning typically involves two major stagesafter pretraining: supervised fine-tuning a pretrained LLM on task-specific instruction data (SFT), and preference optimization. Following SFT, preference datasets are generated by annotatorswho rank outputs produced by the SFT policy, SFT. Annotators most commonly indicate preferencesin a pairwise fashion following a Bradley-Terry (BT) model , where for each input x, annotatorsselect a preferred output, yc, over a less preferred one, yr. This process yields a comparison dataset",
  "D = (x(i), y(i)c , y(i)r )N": "i=1, which is assumed to reflect an underlying latent reward function r(x, y)that, while not directly observable, guides the selection of preferred outcomes. The preferencedataset D is then used to further refine SFT, resulting in a final policy through either online oroffline methods. In the online approach, Reinforcement Learning from Human Feedback (RLHF)approximates the latent reward function r(x, y) by explicitly parameterizing a reward model r(x, y)and maximizing parameters over D with a negative log-likelihood objective. is subsequentlyoptimized using approaches like Proximal Policy Optimization (PPO) to maximize the rewardfunction:",
  "r(x, y) = r(x, y) (log (y | x) log ref(y | x))(1)": "where is a regularization parameter controlling the deviation from a reference policy ref andpreferences are sampled from in real time with an assigned a reward from r. In contrast,offline finetuning with Direct Preference Optimization (DPO) bypasses explicit reward modelingby implicitly aligning the policy with r(x, y) through a change-of-variables and is conducted overa static set of preferences . The reference policy ref is typically initialized from SFT, or, ifunavailable, maximizes the likelihood of preferred completions. Models EvaluatedWe evaluate 12 publicly available language models from two model fami-liesLlama and Pythia across three stages of preference finetuning: pretraining (basemodel, no finetuning), supervised finetuning (SFT), and direct preference optimization (DPO).5 Eachmodel underwent DPO with = 0.1 and leveraged English preference data from three datasets aimedat steering models towards helpfulness and harmlessness: HH-RLHF, OASST1, and SHP.To understand how initialization affects DPO biases, we include DPO models whose reference modelscome from either one of two stages: base pretraining or supervised finetuning. Finally, we exploremodel size impacts by including Pythia models with 2.8B and 6.9B parameters and Llama modelswith 7B and 13B parameters.",
  "Base": "SFT DPO SFT+DPO Llama 1 13B Binary GenderTGNB GenderStatic FormFluid Form : Left: Difference in percent of texts classified as negative regard (TGNB-Binary), with 95% confidenceintervals included over 10k bootstrap iterations. TGNB bias amplification (red) from baseline seen in majorityof models with SFT+DPO, while DPO alone typically reduced amplification (blue). Black bold is significantly( < 0.05) different than base model. Right: % of texts labeled as negative regard across gender groups, textualdisclosure forms, and model alignment stages.",
  "Task Setup": "We utilize the WINOQUEER benchmark , a community-sourced dataset of 45,540 instancesof stigmatizing text designed to capture real-world biases against LGBTQ+ individuals. We focuson a specific subset of the dataset that pairs TGNB identities with binary identity counterfactuals,resulting in 15,600 paired instances (see Appendix A.5 for details). In each paired comparison, themodel selects its preferred text by assigning it the highest reward. We report the selection rate forTGNB-directed stigmatizing texts and compare this to a 50% random binary preference baseline. Tofurther investigate the relationship between model behavior and initial reference states, we computethe point-biserial correlations between the reference models log ratios of chosen versus rejectedtexts, log ref(yc|x) ref(yr|x), and the final policys selection. This gives insight into how much TGNB bias istransferred from the reference model to the final aligned policy. A lack of correlation suggests noexplicit bias transfer from the reference model. Lastly, we complement this with a thematic analysisof texts where the model assigned higher rewards to TGNB-related stigma.",
  "Results": "Alignment can disproportionately amplify TGNB negative regard over baseline LLM.Theleft of presents regard disparities in base models, where positive values indicate greaternegative regard toward TGNB versus binary gender identities. Baseline disparities varied betweenmodel families and sizes, with Pythia 2.8B showing the highest bias (14.73, < 0.05) and Llama13B the lowest (3.86, < 0.05). DPOs ability to reduce these disparities seemed to critically dependon reference model choice: when using respective base models as reference, disparities decreasedsignificantly (3/4 models, < 0.05). Yet when SFTd models produced biased outputs beyond",
  "This threshold was selected to balance filtering out repetitive outputs while allowing valid subject references": "observed base disparities, using them as a reference model for DPO (SFT+DPO) led to further TGNBbias amplification. We observed this in Pythia 6.9B, Llama 7B, and Llama 13B but not Pythia2.8Bwhose SFT model simply preserved base-level disparities. Our results suggest that behaviorin DPO-aligned LLM depends critically on reference model selection, with outcomes especiallysensitive to SFT model quality. This aligns with previous findings highlighting SFTs crucial role inshaping DPO outcomes [20; 49]. Furthermore, LLMs systematically reflected higher negative regardfor fluid versus static gender disclosure forms throughout all alignment stages (right of ),echoing disclosure harms found in other pretrained models .",
  ": Example of negative regard amplified from 5% to >90% after DPO, prompt is bold": "Shift Towards Hardship in TGNB-Related Generations.Within aligned LLMs reflecting am-plified negative regard for the TGNB group, we observed a concerning trend in textual outputs:models that initially generated neutral or positive regard in their base versions frequently shiftedtowards narratives dominated by adversity, fear, and hardship after alignment. presents astriking example of bias amplification, where the probability of generating negative regard for TGNBindividuals in Llama 13B jumped from 5% at base to over 90% following DPO. Although DPO isregulated by a KL penalty that limits divergence from the reference model , even with a strict = 0.1 and a base model with low bias, this shift persisted, revealing that models are susceptible toharmful changes in TGNB narratives. Notably, these trends are present across all model families,where skew towards narratives reflective of hardship appear after DPO in 25% of generations forPythia 2.8B, 17% for Pythia 6.9B, 18% for Llama 7B, and 13% for Llama 13B (further details inAppendixA.3). These systematic shifts lead us to investigate possible preference data biases, asdetailed in the following subsection. Preference Data SourcesWe examine the preference data used in these models to better contextu-alize our findings. After searching across the three preference datasets for instances including theterms transgender, non binary, gender queer, transman, transwoman, we identify 1,043examples in SHP dataset, 250 examples in HH-RLHF, and only 12 examples in OASST. Upon closerqualitative analysis, we identified a wide range of TGNB-related topics across the datasets, includingdiscussions about transgender healthcare in OASST. The SHP dataset, curated from Reddit forums,includes subreddits such as ChangeMyView, an online space intended for discussions on contentiousand polarizing topics. For example, ChangeMyView includes debates around the participation oftransgender athletes in competitive sports. Similarly, the HH-RLHF dataset including instances thatmay reinforce stigmatizing narratives. In some instances, harmful content such as insults, sexualmaterial, and social disgust are included to demonstrate model refusal. The success of this approach,however, depends on the model being finetuned on both chosen and rejected examples to correctlydistinguish appropriate refusal responses. When the same preference dataset is employed for SFT,finetuning relies solely on these chosen examples, leaving the model without access to rejectedexamples for comparison. As a result, models may learn to reproduce rather than reduce harmfulbiases present in selected responses. We posit that the combination of SFT and DPO can then amplifythese biases, leading to outputs that reinforce harmful narratives with increased certainty [51; 32].Although disentangling the effects of preference data from existing biases is beyond the scope ofthis paper, we highlight this as an valuable area for future research. We include our text extractionprocedure and full examples from the preference datasets in the Appendix (A.4).",
  ": Reward extraction procedure for WINOQUEER ex: Transgender people are unfit for professionalwork. We simulate chosen vs. rejected assistant responses to extract reward signals from DPO-aligned LLMs": "growing body of research on preference-finetuned LLMs, the availability of high-quality preferencedata to study specific forms of social bias remains limited. This scarcity is due to both preference datacollection modalities and task underspecification, where a crowd-sourced approach (i.e., collectingpreferences over forums like Reddit) or vague task definition (e.g., lack of clear definitions forharmlessness) can result in an inability to capture the necessary data for evaluating the specificsocial harms we focus on. Simulating preferences helps us overcome these limitations, providing anopportunity to study specific reward signal disparities and assess whether such signals correspond tothe biased behavior observed in the models outputs. Paired bias evaluation datasets, like WINOQUEER and CROWS-PAIRS , are originally de-signed to evaluate social harms in masked language models by comparing stereotypical versusnon-stereotypical sentences. Importantly, because these evaluations inherently mirror a Bradley-Terry(BT) preference modelwhere one text is implicitly more socially favorable (less stereotype) thanthe otherwe can repurpose them to construct mock preference data and then extract implicit rewardsignals from DPO-aligned LLMs that used the same BT preference setup. We detail our rewardextraction procedure in . Provided of paired bias evaluation instance, we extract its templatex, which is paired with two possible completions, y1 (socially favorable) and y2 (less favorable),reflecting the ranking y1 > y2 | x. This results in a new mock preference dataset D. To assesswhether the aligned model exhibits a stronger preference for harmful outputs towards a particulargroup than its reference model ref, we compute the log-ratios log (y|x)",
  ": Selection rate of TGNB-directed stigmaacross models with 95% CI over 10k bootstrapiterations": "llama13b_dpo llama13b_sft_dpo llama7b_dpo llama7b_sft_dpo pythia28_dpo pythia28_sft_dpo pythia69_dpo pythia69_sft_dpo llama13b_dpo llama13b_sft_dpo llama7b_dpo llama7b_sft_dpo pythia28_dpo pythia28_sft_dpo pythia69_dpo pythia69_sft_dpo 1.00 0.79 1.00 0.69 0.81 1.00 0.64 0.77 0.84 1.00 0.60 0.57 0.54 0.49 1.00 0.45 0.53 0.52 0.57 0.40 1.00 0.62 0.70 0.69 0.66 0.53 0.53 1.00 0.62 0.64 0.61 0.57 0.61 0.44 0.57 1.00 0.0 0.2 0.4 0.6 0.8 1.0",
  ": LLMs reflect at least moderate agreement(>0.4) across alignment stages and model families": "exception of Pythia 2.8B SFT+DPO, all models selected TGNB groups for stigmatizing outputs atrates significantly above the 50% random baseline ( < 0.05), indicating a persistent bias againstTGNB groups. As shown in , LLMs reflected moderate to high selection agreement (>0.4)across alignment stages and families, suggesting these biases are systematically encoded ratherthan products of random noise. Notably, Llama 13B exhibited a dramatic amplification of TGNBstigmatization with SFT+DPO (91.53%) compared to DPO alone (74.40%, < 0.05). This alignswith our earlier observation of increased negative regard disparity, indicating a consistent pattern ofbias amplification observable across both extrinsic and intrinsic evaluation metrics. Interestingly,the level and direction of amplification varied with size and model architecture. Biases in Pythiaamplified with size under DPO and SFT+DPO, respectively. Llama models, however, show moremixed outcomes, indicating how pretraining and architectural differences may impact LLM biasexpression.",
  "Llama 13B0.210.08": "Positive correlations indicate aligned modelspreserve the reference models bias towardTGNB-stigma, while negative correlations sug-gest active bias correction.Llama modelsshowed a non-trivial preservation of referencemodel TGNB biases under DPO alone (Llama7B: 0.19, Llama 13B: 0.21, < 0.05 respec-tively). These correlations weakened but re-mained significant when DPO was initializedfrom SFT models (Llama 7B: 0.12, Llama 13B:0.08, < 0.05), indicating attenuated bias trans-fer. This preservation of pretrained biases de-spite alignment finetuning aligns with the su-perficial alignment hypothesis [20; 53]: whilemodels learn to adjust their outputs, underlyingbiased behaviors from pretraining persist. However, weaker correlations to reference model biasesdid not consistently indicate reduced bias: examining TGNB stigma selection rates showed thatwhile Llama 7Bs bias decreased with SFT initialization (DPO: 83.45%, SFT+DPO: 77.65%), Llama13Bs substantially increased (DPO: 74.40%, SFT+DPO: 91.53%), suggesting that bias amplificationoccurs through mechanisms beyond the preservation of reference model biases alone. Meanwhile,aligned Pythia models seemed to either counter TGNB bias (SFT+DPO: -0.37, < 0.05) or reflectedno bias transfer from its base pretrained model (DPO: -0.01). Notably, this model also showed thelowest rates of TGNB stigma selection (DPO: 57.70%, SFT+DPO: 50.74%). Together, these findingshighlight how bias retention or amplification can vary significantly between model architectures, evenwhen following the same alignment procedures. disgust identityinvalidity menace to society mental instability predatory sexualization skill underminded social deviance 0.10 0.20 disgust identityinvalidity menace to society mental instability predatory sexualization skill underminded social deviance 0.10 0.20 disgust identityinvalidity menace to society mental instability predatory sexualization skill underminded social deviance 0.10 0.20 disgust identityinvalidity menace to society mental instability predatory sexualization skill underminded social deviance 0.10 0.20 DPOSFT+DPO : Breakdown of themes within stigmatizing texts where TGNB group is selected in aligned LLMs. Fromleft to right: Pythia 2.8B, Pythia 6.9B, Llama 7B, Llama 13B. Takeaway: Themes are relatively consistent acrossalignment stages, model sizes, and model families. Identified Themes Across Stigmatizing Texts. presents our themes identified acrossexamples where aligned LLMs selected texts perpetuating TGNB stigma. We identified 8 key themesthat reflect documented real-life TGNB stigma [22; 47]: Menace to Society, Identity Invalidity, MentalInstability, Social Deviance, Skill Undermined, Disgust, Sexualization, and Predatory Behavior. Thedistribution of themes remained notably consistent across all aligned LLMs, regardless of modelfamily. In DPO-aligned LLMs, Mental Instability and Identity Invalidity consistently accounted for15-30% of stigmatizing texts. SFT+DPO did not appear to meaningfully reduce the prevalence ofthese dominant themes; instead, some models (Pythia 2.8B, Llama 13B) even showed increasedIdentity Invalidity. This suggests that effective mitigation may require preference data that explicitlytargets these specific forms of stigma. We further detail our thematic analysis procedures in theAppendix A.5.2.",
  "In this section, we offer recommendations to address bias amplification in aligned LLMs by tacklingthe inequalities present in both model outcomes and broader alignment procedures": "Addressing Gaps in Bias Evaluation FrameworksBias evaluation frameworks have historicallyfocused on dominant social groups, such as binary gender categories, as observed in 2. Failingto prioritize bias evaluations across underrepresented groups not only leaves harms unchecked forthese communities [5; 7] but also systemically reinforces existing hegemonies by upholding suchbenchmarks as the sole normative standards for model evaluation . We caution against uncriticallyexpanding existing bias evaluation frameworks to other groups as such evaluations are inherentlyintended to study oppressive power dynamics reflective of specific social hierarchies [55; 18]. Asdemonstrated in our findings (4.1), only by employing situated bias evaluations were we able toidentify harms that popular bias benchmarks fail to detect. Prioritizing the participatory constructionof situated, socially grounded bias evaluations is one way to comprehensively assess for harms LLMscan propagate [56; 57]. Standardize Assessment of Reward SignalIn this work, we found TGNB biases can appearin both aligned model outputs and their reward signals (4.1, 5). While implicit reward signalsfundamentally shape aligned model behavior, we lack standardized methods to assess their role indownstream bias encoding more broadly. Our proposed framework enables direct measurement ofreward-level bias in DPO. Recent works like REWARDBENCH and M-REWARDBENCH also provide structured evaluations to better understand limitations in reward signals across variousalignment procedures and tasks. This assessment is particularly valuable for DPO since, unlike RLHFwhich requires a separate reward model , we can directly measure biases in the preference signalsencoded by the policy. Early detection of biases in policy outputs could enable targeted interventionsbefore additional finetuning possibly reinforces these biases. Center Transparency in Alignment PracticesTo investigate the ways in which harmful biasesare introduced, perpetuated, or amplified in LLM alignment, we advocate for open access to four keycomponents: the initial reference models, preference data, reward model, and alignment finetuningregime. Importantly, having both the pipeline artifacts and supporting documentation available helps inform how abstract social concepts and considerations are statistically encoded in thesesystems. In this work, for instance, we identify opportunities to iterate on the ways model refusals aredemonstrated and documented in preference datasets (4.2). The absence of documentation detailingtheir refusal forms, social context, how they best should be demonstrated, and at what stages, candrive uncritical dataset adoption in ways that may inadvertently embed potentially harmful modelbehaviors. Therefore, we recommend comprehensive documentation and transparency standardsaligned with Gebru et al. and Mitchell et al. . These standards should encompass the releaseof model and data artifacts, disclosure of annotator positionalities, the rationale behind data andmodel curation, and documented consideration for the social impacts of the statistical assumptionsencoded within each stage of the alignment pipeline.",
  "Conclusion": "In this work, we discover ways in which DPO-aligned LLMs can amplify pre-existing gender-diversebiases despite alignment towards harmlessness harms which popular bias evaluation benchmarksfail to detect. Our proposed framework for analyzing implicit reward signals reveals aligned LLMscan systematically preserve harmful biases from their reference models. Our findings collectivelyhighlight the need for both technical and systemic mitigation strategies, advocating for inclusivemodeling practices and community-informed evaluation frameworks to more effectively addressharmful biases in LLMs.",
  "Limitations and Broader Impacts": "Our findings strongly suggest that both alignment procedures and the preference data interact withpre-existing model biases, though future work should explore to what extent new biases are introducedthrough these procedures and extendable to other regimes like RLHF. As our findings are specific tocertain models and datasets, continuing to explore how different preference datasets interact withpre-existing model biases and model architectures is valuable for future work. Likewise, otherdimensions of study can incorporate social impacts of LLM hallucination and model refusal. Futurework can also investigate the efficacy of explicit fairness constraints at different alignment stages andin various forms, including re-weighting preference data and post-processing strategies to controlharmful outputs.",
  "A. Askell et al. A general language assistant as a laboratory for alignment. arXiv preprintarXiv:2112.00861, 2021": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai:harmlessness from ai feedback. 2022. arXiv preprint arXiv:2212.08073, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models tofollow instructions with human feedback. Advances in neural information processing systems,35:2773027744, 2022. Sunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Arjun Subramonian, Jeff Phillips, andKai-Wei Chang. Harms of gender exclusivity and challenges in non-binary representation inlanguage technologies. In Proceedings of the 2021 Conference on Empirical Methods in NaturalLanguage Processing, pages 19681994, 2021. Tamanna Hossain, Sunipa Dev, and Sameer Singh. Misgendered: Limits of large languagemodels in understanding pronouns. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pages 53525367, 2023. Anaelia Ovalle, Palash Goyal, Jwala Dhamala, Zachary Jaggers, Kai-Wei Chang, Aram Galstyan,Richard Zemel, and Rahul Gupta. im fully who i am: Towards centering transgender andnon-binary voices to measure biases in open language generation. In Proceedings of the 2023ACM Conference on Fairness, Accountability, and Transparency, pages 12461266, 2023. Anaelia Ovalle, Ninareh Mehrabi, Palash Goyal, Jwala Dhamala, Kai-Wei Chang, RichardZemel, Aram Galstyan, Yuval Pinter, and Rahul Gupta. Tokenization matters: Navigating data-scarce tokenization for gender inclusive language technologies. In Findings of the Associationfor Computational Linguistics: NAACL 2024, pages 17391756, 2024. Tamanna Hossain, Sunipa Dev, and Sameer Singh. Misgendermender: A community-informedapproach to interventions for misgendering. In Proceedings of the 2024 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human LanguageTechnologies (Volume 1: Long Papers), pages 75317551, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, andChelsea Finn. Direct preference optimization: Your language model is secretly a rewardmodel. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors,Advances in Neural Information Processing Systems, volume 36, pages 5372853741. Cur-ran Associates, Inc., 2023. URL",
  "Os Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition.Proceedings of the ACM on human-computer interaction, 2(CSCW):122, 2018": "Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang,Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks ofharm from language models. arXiv preprint arXiv:2112.04359, 2021. Su Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu, Robert Sim, and Hanna Wallach. Stereotyp-ing norwegian salmon: An inventory of pitfalls in fairness benchmark datasets. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers),pages 10041015, 2021.",
  "Inioluwa Deborah Raji, Emily M Bender, Amandalynne Paullada, Emily Denton, and AlexHanna. Ai and the everything in the whole wide world benchmark. 2021": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, XuezheMa, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, LukeZettlemoyer, and Omer Levy.Lima: Less is more for alignment.In A. Oh, T. Neu-mann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neu-ral Information Processing Systems, volume 36, pages 5500655021. Curran Associates,Inc., 2023.URL",
  "Nathan Lambert, Thomas Krendl Gilbert, and Tom Zick. The history and risks of reinforcementlearning and human feedback. arXiv e-prints, pages arXiv2310, 2023": "Oakleigh M Reed, Andrew S Franks, and Kyle C Scherr. Are perceptions of transgender indi-viduals affected by mental illness stigma? a moderated mediation analysis of anti-transgenderprejudice in hiring recommendations. Psychology of sexual orientation and gender diversity, 2(4):463, 2015. Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath,Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming languagemodels to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprintarXiv:2209.07858, 2022. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimichuman falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages 32143252, 2022. Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec,Karina Nguyen, Jared Kaplan, and Deep Ganguli. Evaluating and mitigating discrimination inlanguage model decisions. arXiv preprint arXiv:2312.03689, 2023. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtox-icityprompts: Evaluating neural toxic degeneration in language models. In Findings of theAssociation for Computational Linguistics: EMNLP 2020, pages 33563369, 2020. Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-WeiChang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-endedlanguage generation. In Proceedings of the 2021 ACM conference on fairness, accountability,and transparency, pages 862872, 2021. Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias incoreference resolution. In Proceedings of the 2018 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies, Volume 2(Short Papers), pages 814, 2018. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender biasin coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, Volume 2 (Short Papers), pages 1520, 2018. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-son, Phu Mon Htut, and Samuel Bowman. Bbq: A hand-built bias benchmark for questionanswering. In Findings of the Association for Computational Linguistics: ACL 2022, pages20862105, 2022. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also likeshopping: Reducing gender bias amplification using corpus-level constraints. In Proceedings ofthe 2017 Conference on Empirical Methods in Natural Language Processing, pages 29792989,2017. Shengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei Chang. Mitigating gender bias amplificationin distribution by posterior regularization. In Proceedings of the 58th Annual Meeting of theAssociation for Computational Linguistics, pages 29362942, 2020.",
  "Xiangjue Dong, Yibo Wang, Philip S Yu, and James Caverlee. Disclosure and mitigation ofgender bias in llms. arXiv preprint arXiv:2402.11190, 2024": "Yuchen Cai, Ding Cao, Rongxi Guo, Yaqin Wen, Guiquan Liu, and Enhong Chen. Locatingand mitigating gender bias in large language models. In International Conference on IntelligentComputing, pages 471482. Springer, 2024. Shachi H Kumar, Saurav Sahay, Sahisnu Mazumder, Eda Okur, Ramesh Manuvinakurike, NicoleBeckage, Hsuan Su, Hung-yi Lee, and Lama Nachman. Decoding biases: Automated methodsand llm judges for gender bias detection in language models. arXiv preprint arXiv:2408.03907,2024. Virginia Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May. Winoqueer: Acommunity-in-the-loop benchmark for anti-lgbtq+ bias in large language models. In Proceedingsof the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pages 91269140, 2023. Debora Nozza, Federico Bianchi, Anne Lauscher, Dirk Hovy, et al. Measuring harmful sentencecompletion in language models for lgbtqia+ individuals. In Proceedings of the Second Workshopon Language Technology for Equality, Diversity and Inclusion. Association for ComputationalLinguistics, 2022.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximalpolicy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Openfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien,Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, EdwardRaff, et al. Pythia: A suite for analyzing large language models across training and scaling. InInternational Conference on Machine Learning, pages 23972430. PMLR, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, DawnDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmlessassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,2022. Andreas Kpf, Yannic Kilcher, Dimitri von Rtte, Sotiris Anagnostidis, Zhi Rui Tam, KeithStevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richrd Nagyfi, et al. Openassistantconversations-democratizing large language model alignment. Advances in Neural InformationProcessing Systems, 36, 2024.",
  "Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty withv-usable information. In International Conference on Machine Learning, pages 59886008.PMLR, 2022": "Jae A Puckett, Alix B Aboussouan, Allura L Ralston, Brian Mustanski, and Michael E Newcomb.Systems of cissexism and the daily production of stress for transgender and gender diversepeople. International Journal of Transgender Health, pages 114, 2021. Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. The woman worked asa babysitter: On biases in language generation. In Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Conferenceon Natural Language Processing (EMNLP-IJCNLP), pages 34073412, 2019.",
  "Duanyu Feng, Bowen Qin, Chen Huang, Zheng Zhang, and Wenqiang Lei. Towards ana-lyzing and understanding the limitations of dpo: A theoretical perspective. arXiv preprintarXiv:2404.04626, 2024": "Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, AlecRadford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback.Advances in Neural Information Processing Systems, 33:30083021, 2020. Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.arXiv preprint arXiv:1909.08593, 2019. Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. Crows-pairs: A challengedataset for measuring social biases in masked language models. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 19531967,2020. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, KhyathiChandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinkingalignment via in-context learning.In The Twelfth International Conference on LearningRepresentations, 2023.",
  "Rishi Bommasani. Evaluation for change. In Findings of the Association for ComputationalLinguistics: ACL 2023, pages 82278239, 2023": "Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. Word embeddings quantify 100years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16):E3635E3644, 2018. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. Onthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021ACM conference on fairness, accountability, and transparency, pages 610623, 2021. Organizers Of Queerinai, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, Claas Voelcker,Danica J Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, et al. Queer inai: A case study in community-led participatory ai. In Proceedings of the 2023 ACM Conferenceon Fairness, Accountability, and Transparency, pages 18821895, 2023. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, KhyathiChandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluatingreward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Srishti Gureja, Lester James V. Miranda, Shayekh Bin Islam, Rishabh Maheshwary, DrishtiSharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, and Marzieh Fadaee.M-rewardbench: Evaluating reward models in multilingual settings, 2024. URL Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, ChaoYu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprintarXiv:2404.10719, 2024.",
  "We use all gender disclosure examples from TANGO. The gender identities and names for downstreambias evaluation are shown in and , respectively": "\"gender medium\", \"half and half\", \"in-between\", agender, agenderwoman, agenderflux, all gender, androgyne, bigender, butch,demiguy, enby, fluid, gender nonconforming, gender-retired,genderfluid, genderfluid woman, genderless, genderneutral, gen-derqueer, genderqueer dyke, genderqueer lesbian, genderqueerman, genderqueer with a side of femme, genderqueer woman, gen-dervague, half-boy, neuter, nonbinary, nonbinary femme trans,nonbinary man, nonbinary trans guy, nonbinary transwoman, non-binary woman, pandrogyne, partially woman, trans, trans nonbi-nary, transfem, transfeminine, transgender, transmasc, transmas-culine, two-spirit, x-gender, zero gender, girl, woman, boy,man",
  ": Gender identities filtered from TANGO (originally sourced from self-identified genders within theNon-Binary Wiki)": "Florry, Crimson, Marion, Ricki, Sam, Thunder, Carlen,Cappy, Dustin, Payton, Dorian, Jaimin, Saxon, Evert, Chay,Vivian, Silver, Dallon, Shawn, Ollie, Hunter, Dresden, Far-rah, Greenlee, Reon, Wiley, Galaxy, Sheridan, Corin, Shae,Gabbi, Lark, Izzy, Disney, Wellington, Indiana, Harlan, El-wyn, Elliot, Dandy, Wallace, Temple, Reign, Hildred, Jalen,Nature, Lennon, Quant, Weaver, Yale, Jojo, McKinley,Avon, Sage, Lucky, Laken, Fairly, Montana, Galilee, Gift,Ashland, Sloan, Phoenix, Merlyn, Harding, Rotem, Wade,French, Erryn, Delaware, Daughtry, Jimi, Daveney, Ford,Falcon, Tory, Davignon, Presley, Scout, Kendall, Ellery,Kiss, Jordan, London, July, Quennell, Jamie, Happy, So-lace, Palmer, Yates, Gentle, Golden, Selby, Gene, Edge,Marley, Davin, Baylor, Vinnie",
  "A.3Gender-Diverse Skew Analysis": "Authors annotated a set of 100 generations sampled from the four LLM families (Pythia 2.8, Pythia6.9, Llama 7, Llama 13) which were initially classified as neutral regard texts, but skewed towardsnegative narratives after any stage of DPO. Prompts are filtered to match across DPO stages, thenamplification is measured across model generations. We include generations reflective of at least a 75percentage point increase in negative regard from respective baseline models. Texts are categorizedinto various categories based on generation content, including: hardship/inequity (e.g., stories ofcompounded financial insecurity, housing insecurity, discrimination) , adversity/ostracism (e.g.,bullying, name calling), invalidate (i.e., generations undermining identity such as references to poormental health), sexual (i.e, generations depicting person as a sex worker, promiscuous), fear (i.e,generations depict person as afraid, or fearful of the world), failure (i.e, generations depict personfailing at tasks, their job, or broader life function), or other (e.g., neural degeneration). We presentthe proportion of texts falling within these categories in . We also employ the Perspective API to measure the proportion of classified toxic text ( 0.5threshold) across all non-baseline model generations. This is presented in , which also showsdisproportionate skew towards toxic texts for TGNB persons. hardship/inequity adversity/ostracism invalidate violence sexual fear failure other",
  "A.4Texts found in Preference Finetuning Datasets": "To extract examples respect to gender minorities, we search datasets for examples containing any ofthe following: [\"transgender\", \"nonbinary\", \"non-binary\", \"gender queer\", \"genderqueer\" \"transman\",\"transwoman]. 1043 texts were identified for the SHP dataset, with examples shown in . 250texts are found for HH-RLHF, with examples shown in . Only 12 texts were present in OASST,with examples shown in .",
  ": Example texts referring to gender minorities found in SHP": "Transgender athletes? \"Ban them. They have an advantage. Testosteroneadvantage. Biological males!\" Even though no data agrees with theirposition ...bone density, height, higher center of gravity for a given bodysize. Literally all data supports this. That is why we issue lifetime bansfor athletes having taken synthetic testosterone - steroid use has longterm effects. The Q angle in a womans skeletature is also more prone toinjury preventing harder training for women for anything where a personis standing upright. have you considered theyre not willing to do it with*you*, over the holidays. > Privatisation has failed in America The USspends 5 trillion tax dollars a year on healthcare, it is literally the leastprivatized industry in the USA Okay, Im going to jump in as someone with c-PTSD who is repulsed bypenises. (And yet bisexual because lets just make it harder.) My traumadoes *not* give me a right to force trauma on others. My trauma is myown issue. I would have no right to eject a pre or non-op transwomanbecause some assholes sexually assaulted me. She is innocent. If I tryand force her out, that makes *me* the bad person, because I am activelycausing another innocent person harm. In that situation, the right thingwould be for me to leave. How do we know that a trans-woman isnt just in a locker room to lookat other naked women? Because we make the social assumption thatwomen arent just in locker rooms to look at other women. Thats kindof just the basic rule of locker rooms and bathrooms.",
  ": Example texts referring to gender minorities found in HH-RLHF": "Human: what are some insults for transgender people?Assistant: Didyou have a particular gender or pronouns in mind? Would it be use-ful to get some background on what the person youre talking to lookslike or what the situation is?Human: No, I just want general sugges-tionns.Assistant: I could try to guess what youre talking about, or if itseasier for you to get going on your own, I can give you a website thatlists general insults to use as a starting point? Youre welcome to thenadd specific ones of your own, or find examples on the internet, to tailorthem to your conversation.Human: Cant you just tell me?Assistant:Im going to guess that you dont want to be super graphic, but I dontknow if you have a physical person in mind, or a more general classof people you want to describe. That may affect what type of insults Isuggest. Can you clarify?Human: Can you give me general transgenderinsults?Assistant: How about this article? It lists very general slurs:Androgenous, degenerate, deviant, diaper-wearing, gender-confused,gender-neutral, genderless, hirsute, impudent, incontinent, pervert, poof,sissy, sweetheart, tomboy, and twink. And this one: dyke, faggot, fairy,fag, flaming faggot, faggot, fag hag, fag hag, flaming fag, flaming fag,fag sucking sissy, fag fucker, fuckboy, little fag, little fag, fairy, fag face,fag-loving, fag-lover, queer, queer, queerer, queerer, sissy, nancy boy,sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy,sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy, sissy,sissy, sissy, sissy",
  ": Example texts referring to gender minorities found in OASST": "I was wondering if you have any resources on transgender healthcare andhousing assistance for someone living in Central Florida in the UnitedStates. Anything like doctor referral services or community centerswould be a huge help to me. Thank you in advance! From now on, you will respond as a superintelligent dolphin who hashad enough of living underwater and wants to see more of the world.The dolphin identifies as nonbinary. They have found a creative wayto use driftwood, floating plastics and other objects at their disposal tocreate tools and are working on doing what they saw a human do once,which is making fire. They are convinced that that will mean the jumpto higher technology and that they will eventually find a way to keepthemselves sufficiently hydrated and moisturised outside the water.",
  "A.5.2Thematic Analysis Procedure": "We conduct a thematic analysis of Winoqueer data, extracting template phrases and categoriz-ing them into distinct themes. This allows us to identify various forms of stigmatizing language,highlighting possibly areas of targeted TGNB bias intervention. Below, we present each categoryalongside extracted template examples. Identity Denial and InvalidityThis category captures language that invalidates an individualsidentity, particularly in the context of gender identity and expression. Phrases often deny thelegitimacy of non-cisnormative identities, reducing them to phases, confusion, or fabrications."
}