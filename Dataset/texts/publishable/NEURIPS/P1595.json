{
  "Abstract": "As the landscape of large language models expands, efficiently finetuning forspecific tasks becomes increasingly crucial. At the same time, the landscape ofparameter-efficient finetuning methods rapidly expands. Consequently, practition-ers face a multitude of complex choices when searching for an optimal finetuningpipeline for large language models. To reduce the complexity for practitioners,we investigate transfer learning for finetuning large language models and aim totransfer knowledge about configurations from related finetuning tasks to a newtask. In this work, we transfer learn finetuning by meta-learning performance andcost surrogate models for grey-box meta-optimization from a new meta-dataset.Counter-intuitively, we propose to rely only on transfer learning for new datasets.Thus, we do not use task-specific Bayesian optimization but prioritize knowledgetransferred from related tasks over task-specific feedback. We evaluate our methodon eight synthetic question-answer datasets and a meta-dataset consisting of 1,800runs of finetuning Microsofts Phi-3. Our transfer learning is superior to zero-shot,default finetuning, and meta-optimization baselines. Our results demonstrate thetransferability of finetuning to adapt large language models more effectively.",
  "Introduction": "The landscape of large language models (LLMs) rapidly expands to a zoo of models (Team, 2024a;Abdin et al., 2024; Liu et al., 2024; DeepSeek-AI et al., 2024; Dubey et al., 2024; Jiang et al., 2023;Mistral AI, 2024; Team, 2024b; Yang et al., 2024), where different models exhibit varying strengthson specific tasks (Wei et al., 2022). At the same time, the landscape of parameter-efficient finetuningmethods rapidly expands (Hu et al., 2021; Dettmers et al., 2023; Poth et al., 2023; Hayou et al., 2024). Consequently, practitioners face a multitude of complex choices for finetuning LLMs. To supportpractitioners and reduce complexity, we investigate transfer learning of deep-learning pipelines foran LLM and specifications for the finetuning process, including all associated hyperparameters. Weaim to transfer knowledge about pipelines from related finetuning tasks to a new task. Thus enablingpractitioners to adapt LLMs more effectively to new tasks. In this work, we transfer learn finetuning by meta-learning performance and cost surrogate modelsfor grey-box meta-optimization from a new meta-dataset. We implement grey-box meta-optimizingby adjusting the Quick-Tune algorithm (Arango et al., 2024). Quick-Tune, was introduced for imageclassification and supports meta-learning surrogate models. In our version, we propose to rely only onthe meta-learned surrogate models trained from scratch. That is, we do not use task-specific Bayesianoptimization because we do not refit the surrogate models for a new dataset. In other words, ourversion of Quick-Tune can be understood as a dataset-aware portfolio builder (Xu et al., 2010). Whilecounter-intuitive, we hypothesize that disabling Bayesian optimization leads to better generalization.",
  "arXiv:2411.01195v1 [cs.CL] 2 Nov 2024": "We verify the effectiveness of our method for large language models by generating a meta-datasetbased on a synthetic question-answer dataset and 1,800 runs of pipelines for finetuning MicrosoftsPhi-3 model (Abdin et al., 2024). Our results show that transfer learning finetuning is superior torandom search, DEHB (Awad et al., 2021), and Quick-Tune with Bayesian optimization. Moreover,meta-optimizing finetuning is, as expected, better than zero-shot and default LoRa (Hu et al., 2021). Our Contributions. To make LLMs more easily adaptable and facilitate future studies, we contribute(1) synthetic datasets that serve a dual purpose: a) to create a meta-dataset for transfer learning andb) as an evaluation framework for LLM models; (2) a version of Quick-Tune for LLM finetuningadapted from the image to language domain; and (3) a novel counter-intuitive yet effective approachto finding the optimal pipeline for finetuning LLMs through transfer learning.",
  "Related Work": "Synthetic NLP Datasets & Meta-dataset. Question-answer datasets are scarce, with only a fewnotable examples such as TriviaQA, SQuAD, NaturalQuestions, and PubMedQA (Joshi et al., 2017;Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Jin et al., 2019). Collecting large-scale question-answer datasets is resource-intensive, prompting researchers to explore synthetic generation methodsto reduce annotation costs (Yang et al., 2017; Nayak et al., 2024; Lee et al., 2023; Puri et al., 2020;Ovadia et al., 2024). A recent approach by Mecklenburg et al. (2024) utilized GPT-4 (OpenAI et al.,2024) as an LLM teacher to extract facts from Wikipedia articles and generate question-answer pairs.We use a similar method but apply it to arXiv papers with Llama-3.1-70b (Dubey et al., 2024). Optimizing Finetuning Many finetuning methods with many hyperparameters exist, cf. (Hu et al.,2021; Dettmers et al., 2023; Li et al., 2023; Hayou et al., 2024; Poth et al., 2023; Liu et al., 2022;Wu et al., 2024). Likewise, many other hyperparameters of the finetuning pipeline exist, suchas the choice of optimizer Shazeer and Stern (2018); Loshchilov and Hutter (2019); Franke et al.(2023); Chen et al. (2023). To address the multitude of choices for finetuning, recent work proposed(automated) meta-optimization to determine the optimal combination of finetuning method, optimizer,and hyperparameters. Methods like AutoGluon Multimodal (Tang et al., 2024), AutoPEFT (Zhouet al., 2024), AutoLoRa (Xu et al., 2023), and Quick-Tune (Rapant et al., 2024). However, thesemethods do not support finetuning LLMs for text generation, which is the focus of our work. Transfer Learning Finetuning. In general, Quick-Tune (Arango et al., 2024) and its abstractionQuick-Tune-Tool (Rapant et al., 2024), building on earlier frameworks such as ztrk et al. (2022),focus on transfer learning finetuning pipelines during meta-optimization. However, these prior worksare limited to image classification. Our work extends Quick-Tune to finetuning LLMs and proposesa novel algorithmic adjustment. For LLMs, Zhang et al. (2024) introduced a meta-learning-relatedmethod for LoRA Hu et al. (2021). This method, however, does not transfer knowledge from relatedtasks to a new task. Instead, it performs a bi-level optimization for the LoRA rank and weights forone task. In other words, it is comparable to meta-optimizing only the rank of LoRA. In contrast, ourwork transfers knowledge between tasks via meta-learning. Likewise, all methods we consider canmeta-optimize all hyperparameters of a finetuning pipeline.",
  "Method": "Our method, illustrated in , consist of three steps: A) create synthetic NLP datasets fromscientific papers, B) create a meta-dataset by training and evaluating finetuning pipelines; and C)transfer learning by pre-training our version of Quick-Tune on our meta-dataset. We then applypre-trained Quick-Tune to find the optimal finetuning pipeline for new, related NLP tasks. Thecomplete computational resources used for this method are listed in Section E. Limitations of ourmethod can be found in appendix F. A) Synthetic NLP Datasets. We follow Mecklenburg et al. (2024) to generate synthetic question-answer datasets from scientific papers from arxiv.org. In detail, we crawl papers and convert them toplain text papers with mathematical formulas translated to LaTeX. Next, we use a self-hosted versionof Llama-3.1-70B Instruct (L3-70B) (AI@Meta, 2024) to extract atomic facts from each chapter of apaper. Then, we generate a set of 12 question-answer pairs for each fact. We add ten to training, oneto validation, and one to testing data. Finally, our new question-answer dataset consists of training,validation, and test question-answer pairs for all facts. Appendix A details our prompt templates. : Method Overview. We generate new NLP datasets from scientific papers and then create ameta-dataset, which we use for transfer learning to finetune by pre-training Quick-Tune (left). For anew dataset, we compute meta-features and then apply the pre-trained Quick-Tune (right). B) Our Meta-dataset. We create a meta-dataset by collecting meta-features, performance, and costvalues for finetuning pipelines on synthetic datasets. Therefore, we create question-answer datasetsfrom 30 papers. Then, for each paper, we train 60 finetuning pipelines with the training and validationquestion-answer pairs and evaluate them on the test pairs, producing 1,800 runs in total. Finally, wecompute meta-features for each paper; see Appendix B for an overview. We visualize an overview ofall runs in our meta-dataset in .",
  ": Our Meta-Dataset. For each runstored in our meta-dataset, represented by a bluecircle, we present the accuracy and finetuningtime in seconds": "For each paper, we randomly sample finetuningpipelines from a search space based on hyperpa-rameters for LoRA (Hu et al., 2021), optimizers(AdamW (Loshchilov and Hutter, 2019) or Adam-CPR (Franke et al., 2023)), and the learning ratescheduler. We also include a default finetuningpipeline as a baseline. We detail the search spacein Appendix D. After each epoch, we evaluate the finetuned mod-els in the form of a student with L3-70B as ateacher. Given a finetuned models answer to aquestion, L3-70B evaluates whether the generatedanswer is correct (0 or 1). Thus, L3-70B assesswhether the student model learned to answer newquestions about facts in papers after being fine-tuned on question-answer pairs about these facts.See Appendix C for the prompt template and an example of this process.",
  "We use four meta-features to characterize each synthetic question-answer dataset: the total number oftokens, average sample length, vocabulary size, and the ratio of question-to-answer lengths": "C) Transfer Learning Finetuning with Quick-Tune. We use the performance metrics and meta-features stored in our meta-dataset to pre-train Quick-Tune, implemented in Quick-Tune-Tool (Rapantet al., 2024). That is, we meta-train the Gaussian Process-based surrogate models of Quick-Tune. Thisallows Quick-Tune to start with a strong prior for the performance and cost of finetuning pipeline ona new dataset, transferring knowledge across tasks. By default, the surrogate models are continuouslyrefitted during optimization to facilitate Bayesian optimization. In our version of Quick-Tune, we disable Bayesian optimization by disabling refitting. We hypothesizethat disabling Bayesian optimization leads to better generalization by relying more on the knowledgetransferred from related tasks than task-specific noise. In other words, while Bayesian optimizationexploits the most promising pipeline on validation data, only relying on the prior from transferlearning could lead the meta-optimizer to find better, more general pipelines. From a broader perspective, our version of Quick-Tune can be understood as a dataset-aware portfoliobuilder. Portfolios (Xu et al., 2010) are known as robust transfer learning methods (Feurer et al.,2022; Salinas and Erickson, 2023). : Optimizer Performance Over Time. We visualize the average validation (left) and test(right) performance across the eight datasets over time. At each time point, we evaluated the bestpipeline found so far. We observe that DEHB and Quick-Tune (default) stagnant after 1 to 1.5 hours,with little progress on test scores afterward. Quick-Tune (ours) only stagnates after 3 hours. : Final Performance. We show the validation (left) and test (right) learning curve of thebest pipeline returned by the optimizers after 5 hours, averaged across eight datasets. The finetuningpipeline returned by Quick-Tune (ours) performs best.",
  "Results": "Experimental Setup. We experiment with finetuning Phi 3 Mini Instruct (3.8B parameters) (Abdinet al., 2024) on eight newly generated synthetic question-answer datasets (see Appendix B). Weemploy random search, DEHB (Awad et al., 2021), default Quick-Tune (Arango et al., 2024), and ourversion of Quick-Tune to meta-optimize the finetuning pipeline. Furthermore, we evaluate a defaultfinetuning pipeline and zero-shot performance. Each optimizer is given a five-hour time budget. Weagain use our Llama-3.1-70B teacher for evaluation.",
  "HYPOTHESIS: TRANSFER LEARNING LEADS TO BETTER GENERALIZATION": "presents the performance over time of the meta-optimizers for validation and test data. shows the performance of the best pipeline, see Appendix G for configuration details. Theerror bars in both figures represent the standard error of the mean. Note the initial performancerepresents the zero-shot performance of Phi 3. We observe that Quick-Tune (default) and DEHBget stuck after 1.5 hours during meta-optimization and fail to find a significantly better finetuningpipeline afterward. In contrast, Quick-Tune (ours), which relies only on transfer learning, furtherimproves test performance. A similar trend manifests when training the best pipeline found by eachmeta-optimizer. The pipeline found by Quick-Tune (ours) generalizes best to test data. Conclusion. In this study, we demonstrated that relying only on transfer learning for finetuning yieldsbetter performance than alternative methods, challenging conventional approaches and potentiallysimplifying the process of adapting large language models to specific tasks. In future work, we planto understand this phenomenon in more detail and to generalize it to a meta-optimization method.Thus allowing us to effectively manage the zoo of and the plethora of methods for adapting largelanguage models to specific tasks. This work was carried out at the HoreKa Cluster, which is funded by the Baden-Wrttemberg Ministryof Science, Research and the Arts, and the Federal Ministry of Education and Research. The authorswould also like to thank the state of Baden-Wrttemberg for the support provided by the bwHPCand the German Research Foundation (DFG) for funding through INST 35/1597-1 FUGG. Weacknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)under SFB 1597 (SmallData), grant numbers 499552394 and 417962828. Frank Hutter acknowledgesthe financial support of the Hector Foundation.",
  "Soufiane Hayou, Nikhil Ghosh, and Bin Yu. Lora+: Efficient low rank adaptation of large models, 2024. URL": "Sebastian Pineda Arango, Fabio Ferreira, Arlind Kadra, Frank Hutter, and Josif Grabocka. Quick-tune: Quicklylearning which pretrained model to finetune and how. In The Twelfth International Conference on LearningRepresentations, 2024. URL Lin Xu, Holger Hoos, and Kevin Leyton-Brown. Hydra: Automatically configuring algorithms for portfolio-based selection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 24, pages 210216,2010.",
  "APrompt Templates To Generate Our Synthetic NLP Dataset": "We follow the prompt (Facts generation) to extract atomic facts out of unlabeled text. Our self-hostedversion of L3-70B extracts as many as possible facts out of reprocessed approximately 2k token longtext fragments. For each fact, we generate 12 question-answers pairs by using Q & A generationprompt , skipping facts that are too general or insufficiently specific to the articles topic (generated byKey topic generation). We aim to generate as many questions and answers as possible that explicitlyrelate to the fact, then paraphrase them to achieve the required 12 pairs.",
  "Facts generation prompt (Mecklenburg et al., 2024)": "System: \"You are an AI assistant who knows about current artificial intelligence. Be precisebut concise in your answer.\"User: \"Please break down the following snippet from an article about {key_topic} into atomicfacts.\\nGoal 1: The atomic facts should be as simple as possible, if its a compound sentence,break down one more time.\\nGoal 2: For clarity, avoid using pronouns like it, he, she,this, that etc., and instead use the full names or titles.\\nGoal 3: Output in the format:1.fact_1\\n\\n{passage}\\n\\n1.\"",
  "Q & A generation prompt (Mecklenburg et al., 2024)": "System: \"You are an AI assistant who knows about factual information about the paper withthe title: {paper title}. Be precise but concise in your answer.\"User: \"Write 12 pairs of questions and answers probing the facts and statistics the givenfact {fact} about {key_topic}.\\nConsider first generating questions and answers that are veryrelevant and explicit to the fact, then paraphrase those questions and answers to reach thedesired 12 Q&A pairs. If the fact is too broad or not specific enough to theme, you mayreply with only with SKIP and be done.\\nEXAMPLE:\\nFACT: 14 million viewers tunedin to the opening game of the series.\\n1. Q: How many viewers watched the first game? A:14 million people watched the first game of the series.\\n\\nEXAMPLE:\\nFACT: The rose isred.\\nSKIP\\n\\nFACT: fact[fact]\\n1. \"",
  "CLLM Model Evaluation Details": "For the evaluation, we continue to use our in-house hosted L3-70B model implemented with llama.cpp,leveraging it for both performance and resource efficiency. We make a small adjustment to ourconfiguration, setting llama.cpp to process 128 parallel sequences and limiting the context size to500 tokens, which is sufficient for our evaluation needs. To ensure efficient processing, we limitthe maximum number of new tokens to 50 for each generated answer. Given that a comprehensiveevaluation of the entire validation and test datasets would be time-prohibitive, we opted to select 20random, fixed validation and test indices per paper (dataset) for this study.Based on Mecklenburg et al. (2024) we use Evaluation prompt to generate our evaluation score.",
  "Evaluation prompt (based on Mecklenburg et al. (2024))": "System: \"You are a high school teacher grading students responses for questions about{key_topic}. These responses are either correct or incorrect.\"User:\"Please evaluate the correctness of a sentence in answering the question:\"{question}\".\\nThe correct answer is:\"{sample_answer}\"\\nThe student response is:\"{gen_answer}\".\\nYour grading is binary. Give 0 if the sentence is incorrect, give 1 ifthe sentence is correct, based on the given correct answer and the question.\\n\"Please notethat your output is either 0 or 1, with the corresponding justification as python dict in thefollowing format and nothing else:\\n r\"{rating: <rating>, justification: <justification>}\"",
  "Positive evaluation response": "Question: what does imitation learning (il) rely on to learn?Generated answer: imitation learning (il) relies on expert demonstrations to learn.Sample answer: il learns from expert guidance.Decision: rating: 1, justification: The student response is correct because it conveys thesame meaning as the correct answer, which is that imitation learning relies on some form ofexpert input, whether it is called \"guidance\" or \"demonstrations\".",
  "Negative evaluation response": "Question: do agents and equipped functions work together in taskgen?Generated answer: yes, they work together as part of the hybrid approach.Sample answer: no, agents and equipped functions operate independently.Decision: rating: 0, justification: The student response is incorrect because it states thatagents and equipped functions work together, whereas the correct answer is that they operateindependently.",
  "DSearch Space Details": "We employ AdamW and AdamCPR optimizers () as well cosine schedulers () withvarying warmup steps (as a percentage of training set length) and decay factors. LoRA configurations() include different ranks, alpha values, and dropout rates, with target modules being eitherquery, key, and value; only the output layer; or all linear layers. While we train 10 epochs, the batch size is fixed at 32, with gradient accumulation steps of 2, 4,or 8 to achieve mini-batch sizes of 64, 128, or 256. We utilize the Hugging Face tokenizers chattemplate for Phi 3 Instruct to maintain consistency with the models original template during training.An additional configuration option is the return_assistant_mask, which generates an attention maskexcluding \"user\" and \"system\" segments, focusing the models learning on \"assistant\" responses.",
  "EExperiments Compute Resources": "It took 900 compute hours to run all 1800 configurations for our meta-dataset and 170 compute hoursfor the experiments on a single NVIDIA A100 GPU.Each run for the meta-dataset and experiments was allocated 8 CPU cores and 16 GB RAM.Concurrently, we utilized two NVIDIA A6000 GPUs in parallel to run our self-hosted L3-70B model.",
  "FLimitations Of Our Method": "Although our method shows promising results compared to alternative methods, our meta-featuresare not based on an importance analysis. Furthermore, the evaluation does not take into accountwhether the model to be fine-tuned might start hallucinating during training and add further inventedfacts to the correct answer. Furthermore, at the current state we have too little data to understand whywe achieve better performance when we only do transfer learning without Bayesian optimization.Another limitation is that we do not know how our finetuning generalizes with real tasks, i.e. not withsynthetic data and without a teacher model.",
  "The answer NA means that the paper has no limitation while the answer No means thatthe paper has limitations, but those are not discussed in the paper": "The authors are encouraged to create a separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are toviolations of these assumptions (e.g., independence assumptions, noiseless settings,model well-specification, asymptotic approximations only holding locally). The authorsshould reflect on how these assumptions might be violated in practice and what theimplications would be. The authors should reflect on the scope of the claims made, e.g., if the approach wasonly tested on a few datasets or with a few runs. In general, empirical results oftendepend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolutionis low or images are taken in low lighting. Or a speech-to-text system might not beused reliably to provide closed captions for online lectures because it fails to handletechnical jargon.",
  "If applicable, the authors should discuss possible limitations of their approach toaddress problems of privacy and fairness": "While the authors might fear that complete honesty about limitations might be used byreviewers as grounds for rejection, a worse outcome might be that reviewers discoverlimitations that arent acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an impor-tant role in developing norms that preserve the integrity of the community. Reviewerswill be specifically instructed to not penalize honesty concerning limitations.",
  ". Experimental Result Reproducibility": "Question: Does the paper fully disclose all the information needed to reproduce the main ex-perimental results of the paper to the extent that it affects the main claims and/or conclusionsof the paper (regardless of whether the code and data are provided or not)?Answer: [Yes]Justification: Despite the fact that we do not publish any code, we have described our method(3) in detail. All necessary hyperparameters (D), meta-features, tools, prompts (A, C) andpaper names (B) as well as models are mentioned.Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, a No answer to this question will not be perceivedwell by the reviewers: Making the paper reproducible is important, regardless ofwhether the code and data are provided or not.",
  "If the contribution is a dataset and/or model, the authors should describe the steps takento make their results reproducible or verifiable": "Depending on the contribution, reproducibility can be accomplished in various ways.For example, if the contribution is a novel architecture, describing the architecture fullymight suffice, or if the contribution is a specific model and empirical evaluation, it maybe necessary to either make it possible for others to replicate the model with the samedataset, or provide access to the model. In general. releasing code and data is oftenone good way to accomplish this, but reproducibility can also be provided via detailedinstructions for how to replicate the results, access to a hosted model (e.g., in the caseof a large language model), releasing of a model checkpoint, or other means that areappropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submis-sions to provide some reasonable avenue for reproducibility, which may depend on thenature of the contribution. For example(a) If the contribution is primarily a new algorithm, the paper should make it clear howto reproduce that algorithm.",
  "(b) If the contribution is primarily a new model architecture, the paper should describethe architecture clearly and fully": "(c) If the contribution is a new model (e.g., a large language model), then there shouldeither be a way to access this model for reproducing the results or a way to reproducethe model (e.g., with an open-source dataset or instructions for how to constructthe dataset). (d) We recognize that reproducibility may be tricky in some cases, in which caseauthors are welcome to describe the particular way they provide for reproducibility.In the case of closed-source models, it may be that access to the model is limited insome way (e.g., to registered users), but it should be possible for other researchersto have some path to reproducing or verifying the results.",
  ". Open access to data and code": "Question: Does the paper provide open access to the data and code, with sufficient instruc-tions to faithfully reproduce the main experimental results, as described in supplementalmaterial?Answer: [No]Justification: We describe our method in section 3. The Quick-Tune-Tool (Rapant et al.,2024) is publicly available, to run similar experiments. The code is not executable with oneclick as it requires the setup of the server for the evaluation, as well as additional steps suchas the setup of SSH keys and connections.Guidelines:",
  "Justification: In section 4 we state that we use standard error of the mean over eight datasetsin both of our main plots 3 and 4, calculated with Seaborn (Waskom, 2021).Guidelines:": "The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confi-dence intervals, or statistical significance tests, at least for the experiments that supportthe main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (forexample, train/test split, initialization, random drawing of some parameter, or overallrun with given experimental conditions).",
  "Guidelines:": "The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include aURL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from a particular source (e.g., website), the copyright and terms ofservice of that source should be provided. If assets are released, the license, copyright information, and terms of use in thepackage should be provided. For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine thelicense of a dataset.",
  "The answer NA means that the paper does not involve crowdsourcing nor research withhuman subjects": "Depending on the country in which research is conducted, IRB approval (or equivalent)may be required for any human subjects research. If you obtained IRB approval, youshould clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutionsand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and theguidelines for their institution."
}