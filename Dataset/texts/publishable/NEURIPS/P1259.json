{
  "Abstract": "While Vision Transformer (ViT) have achieved success across various machinelearning tasks, deploying them in real-world scenarios faces a critical challenge:generalizing under Out-of-Distribution (OoD) shifts. A crucial research gap re-mains in understanding how to design ViT architectures both manually andautomatically to excel in OoD generalization. To address this gap, we introduceOoD-ViT-NAS, the first systematic benchmark for ViT Neural Architecture Search(NAS) focused on OoD generalization. This comprehensive benchmark includes3, 000 ViT architectures of varying model computational budgets evaluated on 8common large-scale OoD datasets. With this comprehensive benchmark at hand,we analyze the factors that contribute to the OoD generalization of ViT architecture.Our analysis uncovers several key insights. Firstly, we show that ViT architecturedesigns have a considerable impact on OoD generalization. Secondly, we observethat In-Distribution (ID) accuracy might not be a very good indicator of OoD accu-racy. This underscores the risk that ViT architectures optimized for ID accuracymight not perform well under OoD shifts. Thirdly, we conduct the first study toexplore NAS for ViTs OoD robustness. Specifically, we study 9 Training-free NASfor their OoD generalization performance on our benchmark. We observe that ex-isting Training-free NAS are largely ineffective in predicting OoD accuracy despitetheir effectiveness at predicting ID accuracy. Moreover, simple proxies like #Paramor #Flop surprisingly outperform more complex Training-free NAS in predictingViTs OoD accuracy. Finally, we study how ViT architectural attributes impactOoD generalization. We discover that increasing embedding dimensions of a ViTarchitecture generally can improve the OoD generalization. We show that ViT archi-tectures in our benchmark exhibit a wide range of OoD accuracy, with up to 11.85%for some OoD shift, prompting the importance to study ViT architecture design forOoD. We firmly believe that our OoD-ViT-NAS benchmark and our analysis cancatalyze and streamline important research on understanding how ViT architecturedesigns influence OoD generalization. Our OoD-NAS-ViT benchmark and codeare available at",
  "arXiv:2501.03782v1 [cs.LG] 7 Jan 2025": ": We propose, OoD-ViT-NAS, the first comprehensive benchmark for NAS on OoDgeneralization of ViT architectures. Then, we comprehensively investigate OoD generalizationfor ViT. The detailed of 8 OoD datasets in our investigation can be found in Tab. 1. In this figure,we show the Kendall ranking correlation between OoD accuracy of different datasets on the leftand different quantities at the bottom. Our analysis uncovers several key insights. (a) ID as anindicator for ViT OoD Generalization (Sec. 4.2) We show that the correlation between ID accuracyand OoD accuracy is not very high. This suggests that current architectural insights based on IDaccuracy might not translate well to OoD generalization. (b) Training-free NAS for ViT OoDGeneralization. (Sec. 4.3) We conduct the first study of NAS for ViTs OoD generalization, showingthat their effectiveness significantly weakens in predicting OoD accuracy. (c) OoD GeneralizationViT Architectural Attributes. (Sec. 4.4) Our first study on the impact of ViT architectural attributeson OoD generalization shows that the embedding dimension generally has the highest correlationwith OoD accuracy among ViT architectural attributes. Additional results can be found in the Appx. Research Gap. Existing research on ViT architectures focuses on maximizing In-Distribution (ID)accuracy, while studies on the impact of ViT architectures on Out-of-Distribution (OoD) generaliza-tion are limited. Initial works , , and evaluate sets of 3, 10 and 22 human-designed ViTarchitectures under OoD settings, respectively, and provide coarse insights into which models exhibitbetter OoD generalization. However, with very limited ViT architectures studied in previous works,the influence of ViT structural attributes (e.g., embedding dimension, number of heads, MLP ratio,number of layers) on OoD generalization remains unclear. Besides, in the context of ViT Neural Archi-tecture Search (NAS), while there are various ViT NAS for ID accuracy ,there is no study on ViT NAS for OoD generalization. In this paper, we address existing research gaps by introducing OoD-ViT-NAS, the first compre-hensive benchmark specifically designed for ViTs OoD generalization. Building NAS benchmarksis notoriously time-consuming and expensive due to the need to train and evaluate every candi-date architecture. This challenge is particularly acute for ViT, known for its high computationaldemands and memory usage . To overcome this bottleneck, we propose leveraging One-ShotNAS, specifically AutoFormer , a widely used ViT search space.We sample a diverse set ofsub-architectures (models) to populate our benchmark. Importantly, these subnets inherit the weightsfrom the pre-trained supernets, and their performance has been shown to be comparable to, oreven superior to, that of architectures trained alone This approach enables us to efficientlyacquire a large pool of ViT architectures for OoD generalization analysis. Using OoD-ViT-NAS, weconduct extensive OoD generalization analysis and gain several key insights. Additionally, with ourbenchmark, our work is the first to explore (training-free) NAS for ViTs OoD generalization. Ourcontributions are summarized below: We introduce OoD-ViT-NAS, the first comprehensive benchmark designed for NAS researchon ViTs OOD generalization. This benchmark includes 3, 000 diverse ViT architecturessampled from the widely used ViT search space . These architectures span a wide rangeof computational budgets. To thoroughly benchmark OoD generalization, these architecturesare evaluated on the 8 most common and state-of-the-art (SOTA) OoD datasets: ImageNet-C, ImageNet-A , ImageNet-O , ImageNet-P , ImageNet-D , ImageNet-R, ImageNet-Sketch , and Stylized ImageNet (Sec. 3) Our analysis demonstrates the significant influence of ViT architectural designs on OoDaccuracy. This observation encourages future research to focus more on ViT architectureresearch for OoD generalization (Sec. 4.1) We show that high In-Distribution (ID) accuracy is not a very good indicator of OoDaccuracy. This suggests that current architectural insights based on ID accuracy might nottranslate well to OoD generalization (Sec. 4.2) We conduct the first study to explore NAS for ViTs OoD generalization. We study 9Training-free NAS for their OOD generalization performance on our benchmark. We observethat despite their prediction accuracy for ID, their effectiveness significantly weakens whenpredicting OoD accuracy. Furthermore, simple proxies such as the number of parameters(#Param) or the number of floating point operations (#Flop) surprisingly outperform morecomplex Training-free NAS in predicting ViTs OoD accuracy (Sec. 4.3) We study the impact of ViT architecture design on OoD generalization and demonstratethat careful design of ViT architectures can significantly improve OoD generalization.Specifically, increasing the embedding dimensions of a ViT architecture generally canimprove its OoD generalization. (Sec. 4.4) We show that architectures with comparableID accuracy (within an averaging range of 1.39%) exhibit a wider range of OoD accuracy,averaging 3.80% and reaching highs of 11.85%, being comparable or even outperformingstate-of-the-art (SOTA) training OoD generalization method, such as those based on domaininvariant representation learning . For example, under the same OoD setting, theSOTA method shows an improvement of 1.9% OoD accuracy.",
  "Related Work": "Out-of-Distribution (OoD) Generalization. Addressing Out-of-distribution (OoD) generalization isa challenge, particularly in computer vision. Various approaches have been proposed to tackle thisissue. A common strategy focuses on learning features that remain consistent across different domains,thereby promoting generalizability . Other directions explore distributional robustness, model ensembles , test-time adaptation , data augmentation techniques, and meta learning for OoD generalization. From an architectural perspective, afew attempts investigate the impact of network architecture on OoD generalization. Early work shows that over-parameterized networks can hinder OoD performance due to overfitting. This raisesan intriguing question: can sub-networks within such architectures achieve better OoD performance?Inspired by the Lottery Ticket Hypothesis (LTH) , the Functional LTH has been exploredand shown that over-parameterized networks harbor sub-networks with better OoD performance.Techniques like Modular Risk Minimization and Debiased Contrastive Weight Pruning aim to identify these winning tickets. Another direction leverages Neural ArchitectureSearch (NAS) to analyze the OoD robust architectures. However, these studies primarily focus onCNNs. While ViTs have achieved success in various visual recognition, investigations into their OoDgeneralization are limited. Initial works , , and evaluate sets of 3, 10, and 22 human-designed ViT architectures respectively, under OoD settings. Their results provide coarse insightsinto which models exhibit better OoD performance. However, the influence of ViT architecturalattributes on OoD robustness remains unclear. Neural Architecture Search (NAS). NAS is a promising approach that has achieved remarkablesuccess in automatically searching efficient and effective architectures for ID performance . Recently, NAS has been explored in the context of adversarial robustness for CNNs as well. With the rise of Vision Transformers (ViTs), several NAS approaches have been appliedto improve ViT architectures, including Autoformer , S3 , ViTAS , ElasticViT , DSS, Auto-Prox and GLiT . Additionally, hybrid CNN-ViT architectures like HR-NAS, UniNet , and NASViT have also been explored. These efforts have shown promisingresults in terms of ID accuracy. However, there has not been any work on NAS for ViT architecturesspecifically for OoD generalization.",
  "In this section, we describe the construction of our OoD-ViT-NAS benchmark with details on thesearch spaces, datasets, evaluation metrics, and protocol. Our comprehensive benchmark includes": ": Our analysis of the OoD accuracy range highlights the significant influence of ViTarchitectural designs on OoD accuracy. (Sec. 4.1) The numbers within each violin plot for eachsub-figure (e.g., IN-D 9.79 (1.06), 9.65 (2.25), and 7.99 (0.56)) denote the corresponding OoD (ID)accuracy range of architectures sampled from Autoformer-Tiny/Small/Base search space, respectively.See Appx. 12 for additional plots and results on other OoD shifts. For a fair comparison, we fix thesame range for the x-axis across all sub-figures. We include the ID accuracy range in the top-leftsub-figure for reference. On average, the OoD accuracy across all shifts is 3.8%/4.86%/2.74% forthe search spaces in our OoD-ViT-NAS benchmark. This range is comparable to and even surpassesthe current SOTA method based on domain-invariant representation learning , which achieved a1.9% improvement in OoD accuracy under similar settings.",
  ", 000 ViT architectures of varying sizes evaluated on 8 widely used large-scale, high-resolution, andSOTA OoD datasets. Our OoD-ViT-NAS benchmark is summarized in the Tab. 1": "Search Space. We construct our benchmark based on Autoformer search space. This searchspace is currently a widely used search space in the ViT NAS community for ID data . Autoformer search space is a large vision transformer search space includingfive architectural attributes that define the building block. Embedding Dimension: This determinesthe input feature representation size and is typically consistent across layers in ViT architectures.Q-K-V Dimension: This specifies the size of the query, key, and value vectors used in the attentionmechanism. Number of Heads: This defines the number of parallel attention computations performedwithin a single attention block. MLP Ratio: This controls the dimensionality of the feed-forwardnetwork within each transformer block. Unlike embedding dimension, in Autoformer search space,Q-K-V Dimension, Number of Heads, and MLP Ratio can be varied across layers. Network Depth:This refers to the total number of transformer layers stacked in the architecture. It is important tonote that Autoformer maintains a fixed ratio between the Q-K-V dimension and the number of headsin each block. This ensures that the scaling factor in the attention calculation remains constant. Thishelps stabilize the gradients of different heads during the training . We strictly follow Autoformersearch space. The details can be found in the Appx. 10.1. Dataset. Our benchmark consists of the evaluation on large-scale, high-resolution, and most SOTAOoD datasets, including ImageNet-1k , ImageNet-C , ImageNet-P , ImageNet-A ,ImageNet-O , ImageNet-R , ImageNet-Sketch , Stylized ImageNet , and ImageNet-D. These datasets capture a comprehensive range of OoD shifts such as common corruptions(blur, noise, digital, weather), Stable-Diffusion-based OoD shifts, and natural OoD shifts. A detaileddescription of these datasets can be found in the Appx. 10.2.",
  "Metrics. Following the previous OoD generalization methods , we employ threemetrics to construct our benchmark:": ": An overview of comprehensive setups to construct our OoD-ViT-NAS benchmark. Weutilize the widely used ViT NAS search space, Autoformer , which includes three different searchspaces Autoformer-Tiny/Small/Base to cover a broad range of model sizes. We randomly sample3, 000 architectures from these search spaces to populate our benchmark. To ensure comprehensive-ness, we evaluate these architectures across 8 of the most common SOTA OoD datasets. Followingprior OoD generalization works , we employ three metrics for our benchmark: IDAccuracy, OoD Accuracy, and Area Under the Precision-Recall Curve (AUPR).",
  "AUPR": "ID Classification Accuracy (ID Acc): This metric measures the model performance onIn-Distribution (ID) data, typically the data it was trained on (e.g., ImageNet). A higher IDAcc indicates the models ability to learn training datas distribution. OoD Classification Accuracy (OoD Acc): This metric measures the model performance onOut-of-Distribution (OoD) data, which could differ significantly from the training data. Ahigher OoD Acc indicates a better generalization of the model to handle the OoD shifts.",
  "For the specific case of ImageNet-O, , we use the Area Under the Precision-RecallCurve (AUPR) metric. A higher AUPR indicates a better generalization of the model tohandle the OoD detection": "Protocol. Neural Architecture Search (NAS) is notorious for its computationally expensive nature,requiring the training and evaluation of numerous candidate architectures. To address this challengeand efficiently obtain the large number of architectures needed for our benchmark (i.e., 3, 000), wemake use of the One-Shot NAS approach . In One-shot NAS, a single supernet is first constructed. This supernet contains all possible archi-tectures within the defined search space and is trained only once. Then, during evaluation, variousarchitectures (i.e., subnets) can be efficiently extracted from the supernet. Importantly, these subnetsinherit the weights from the pre-trained supernet, and their performance has been shown to becomparable or even superior to that of architectures trained alone . To support a wide range of model sizes, we leverage three supernets: Autoformer-Tiny/Small/Base,which were previously proposed for ID accuracy . We randomly sample 1, 000 architectures fromeach supernet, resulting in a total of 3, 000 architectures in our OoD-ViT-NAS benchmark. Onceobtained, these architectures are evaluated on 8 aforementioned OoD datasets.",
  "Investigation on Out-of-Distribution Generalization of ViT": "In this section, we provide the first comprehensive investigation of how ViT architectures affectOoD generalization using our OoD-ViT-NAS benchmark. In Sec. 4.1, we first demonstrate that ViTarchitectures considerably impact OoD accuracy. In Sec. 4.2, while existing works have made significant strides in improving ViTs ID accuracy, their findings could not be applicablefor ViTs OoD generalization due to the not very high correlation between ViTs ID and OoD accuracy.In Sec. 4.3, we conduct the first study to explore NAS for ViTs OoD generalization. Specifically,we study 9 Training-free NAS based on their OoD generalization performance on our benchmark.Finally, in Sec. 4.4, we analyze the influence of individual ViT architectural attributes (i.e., embeddingdimension, number of heads, MLP ratio, number of layers) on OoD generalization. Additional resultsof these analysis can be found in Appx. 12, 13, 14, 15, 17",
  "ViT architecture designs have a considerable impact on OoD generalization": "In this section, we highlight that ViT architectures considerably impact OoD accuracy. This observa-tion encourages future research to put more focus on ViT architecture research for OoD generalization. : Visualization of OoD accuracy range across OoD shift severity. We conduct the analysis on1, 000 architectures in Autoformer-Small search space within our OoD-NAS-ViT benchmarks. Level0 denotes the clean examples. All corruptions can be found in .6, in Appx. 12. We generallyobserve that the range of OoD accuracy widens as the severity of the OoD shift increases. Experimental Setups. To show how ViT architecture designs impact OoD generalization, wecompute the range of OoD accuracy for each search space on an OoD dataset. This range reflectsthe variation in OoD performance for different architectures within a search space. For example, in, each sub-plot represents the range of OoD accuracy for three different search spaces in ourOoD-ViT-NAS benchmark on one OoD dataset. We compute the average OoD accuracy range acrossall datasets as general statistics. For reference, the range of ID accuracy is also included. Results. The results are shown in . Additional results can be found in the Appx.12. The averagerange of OoD accuracy across the three search spaces in our benchmark is 3.81%/4.86%/2.74%,which is comparable to or even outperforming state-of-the-art (SOTA) training OoD generalizationmethod, such as those based on domain invariant representation learning . For example, undera similar OoD setting, the current SOTA shows an improvement of 1.9% OoD accuracy. Thisobservation highlights the significant influence of ViT architectural designs on OoD accuracy. Bycarefully designing ViT architecture, the OoD accuracy could improve significantly. We further explore how the severity of the OoD shift affects the range of ViTs OoD accuracy. Weconduct similar experimental setups as before, analyzing 1, 000 architectures from the Autoformer-Small search space within our OoD-ViT-NAS benchmark for 1, 000 architectures in Autoformer-Smallsearch space within our benchmark on IN-C. The results are visualized in . We observe that therange of OoD accuracy widens as the severity of the OoD shift increases. This suggests that understronger OoD shifts, the architecture design becomes even more critical for OoD generalization. When visualizing OoD accuracy, we observe a bimodal distribution. We figure out that the embeddingdimension, as the primary ViT structural attribute, influences this bimodality. For example, amongarchitectures from the Autoformer-Small search space of our benchmark, most architectures witha lower embedding dimension (320) fall within the lower OoD accuracy mode, while those withhigher dimensions (384 and 448) tend to reside in the higher accuracy mode. This observation willbe further discussed in detail in Sec. 4.4.",
  "Can ID accuracy serve as a good indication for OoD accuracy?": "While existing works study the impact of ViT architectures to ID accuracy,studies on OoD accuracy are limited. To what extent can we directly apply existing findings of ViTarchitecture insights for ID to OoD accuracy? To answer this question, we investigate the relationshipbetween ViT ID and ViT OoD accuracy. Several studies investigates the relationship between ID and OoD accuracy for the CNNsmodel. However, there is no work on such study particularly for ViT. Utilizing our OoD-ViT-NASbenchmark, we provide the first comprehensive study on the relationship between ViT ID and OoDaccuracy. Through our investigation, we find that the correlation between ViT ID and ViT OoDaccuracy is not very high. This suggests that architectural insights optimized for ViT ID accuracy, aspresented in previous work may not be applicable for ViT OoD generalization. Experimental Setup. Following previous work , we use Kendalls rank correlation coefficientto compute the correlation between OoD and ID accuracy of all 1, 000 architectures from a searchspace on one OoD dataset. Our examination comprehensively computes the correlations across all 8OoD datasets, 3 search spaces, and 3, 000 architectures within our OoD-ViT-NAS benchmark. Wecompute the average correlations across search spaces and datasets as general statistics. : Analysis of OoD Generalization Performance of Pareto Architectures for ID accuracy.Blue dotsrepresent architectures in the search space, while red dotsrepresent the ID Paretoarchitectures. See Appx. 14 for additional results. We find that Pareto architectures for IDaccuracy generally perform sub-optimally under OoD shift. Besides the investigation of the correlation of various architectures, we further study the relationshipbetween ViT ID and ViT OoD accuracy for Pareto architectures, representing the top-performingarchitectures for a certain model size. As shown in -a, the red dotsrepresent Pareto architec-tures for ID accuracy In this study, we analyze 1, 000 architectures from the Autofomer-Small searchspace within our OoD-ViT-NAS benchmark. To identify Pareto architectures for ID accuracy, wedivide the total parameter budget into 30 equal intervals and select the architecture with the best IDperformance within each interval. Results. The correlation results are illustrated in -a. The individual correlations can be found inthe Appx. 13. We show that the correlation between ID and OoD accuracy is generally not very high.This suggests that current architectural insights based solely on ID accuracy might not effectivelytranslate to OoD generalization. Among all OoD datasets, the IN-P dataset exhibits the strongest correlation with ID accuracy. This canbe attributed to its weaker OoD shift compared to other datasets (see the visualization in Appx. 10).As a result, the OoD examples in IN-P are not very different from ID examples, leading to a relativelyhigh correlation between OoD and ID performance. For the remaining seven datasets with strongerOoD shifts, the correlations remain relatively low. The results of the Pareto architectures analysis are illustrated in . Additional results can befound in the Appx. 14. We observe that Pareto architectures for ID accuracy generally performsub-optimally under the OoD shift. This observation further supports our previous finding that IDaccuracy might not be a very good indicator of OoD accuracy.",
  "Explore Training-free NAS for OoD Generalization": "Recently, there has been a new research focus on Training-free NAS, aimed at identifying high-performing architectures without the computational expense of training each candidate. To doso, propose zero-cost proxies to predict the performance of candidatearchitectures in the initialization or the first training iteration, significantly accelerating NAS. Whilethese works focus on ID accuracy, a few attempts have been made in searching for architecturesrobust against adversarial attacks . However, there is no work to explore Training-free NASfor ViT for OoD generalization. Experimental Setup. To address this gap, we comprehensively explore the existing 9 Training-freeNAS for OoD generalization on 3, 000 ViT architectures within our OoD-ViT-NAS benchmark. Ourstudy includes common and SOTA Training-free NAS originally proposed for CNNs for ID Acc(Grasp , SNIP , MeCo ), ViTs for ID Acc (DSS , AutoProx ), and CNNs foradversarial robustness (Jacobian , CroZe ). We complement this study on Training-freeNAS to our OoD-ViT-NAS benchmark to equip the NAS research community with valuable tools todevelop more effective Training-free NAS for OoD generalization. Results. Our exploration provides several practical insights for designing a Training-free NAS forViT for OoD generalization. The results of Kendall ranking correlation between the OoDaccuracy and Training-free NAS proxies on 8 common large OoD datasets are illustrated in Tab. 2.The average OoD accuracy is computed across OoD datasets and search spaces. Detailed results canbe found in and Appx. 15. : Comparison of Kendall ranking correlation between the OoD accuracies and the Training-free NAS proxies values on 8 common large OoD datasets using our OoD-ViT-NAS benchmark.Bold and underline stand for the best and second, respectively. We show that existing Training-freeNASs predictability in ViT OoD accuracy is limited, E.g., the very recently proposed Auto-Proxonly achieves 0.3303 correlation. Furthermore, we make the first observation that simple proxies like#Param or #Flops outperform other more complex proxies in predicting both ViT OoD/ID accuracy.",
  "#Flops--0.4705 0.33910.3537 0.2327": "We observe that existing Training-free NAS are largely ineffective in predicting OoD accuracy. Evenrecent Training-free NAS designed for ViT (i.e., DSS and AutoProx-A ) or Training-freeNAS designed adversarial robustness struggle with predicting OoD accuracy. Surprisingly, simple zero-cost proxies such as #Param or #Flops outperform all existing, morecomplex proxies in predicting both OoD accuracy for ViTs. This finding poses a challenge to theTraining-free NAS research community: to devise a Training-free NAS that surpasses #Params or#Flops in OoD Acc prediction for ViT. From -b, we observe that all Training-free NAS methods consistently fail to predict IN-Dperformance. This is due to the IN-D datasets unique generation process using Stable Diffusion,which creates images labelled with object names and varying nuisances like background, texture,and material variations. Only the most challenging images are retained, resulting in highly difficultexamples, such as distorted images and unrealistic object-background placements (see .2).These examples degrade ViT model performance significantly and cause unpredictable behaviour. Our investigation into ID accuracy for ViTs also reveals a surprising observation. While proposals forTraining-free NAS designed for ViTs (i.e., DSS and AutoProx-A ), improve the predictionof ID accuracy compared to counterparts designed for CNNs. Our study marks the first attempt toexplore simple Training-free NAS like #Param or #Flops. The ID prediction of such simple proxiessurprisingly outperforms SOTA Training-free NAS designed for predicting ID accuracy for ViT.",
  "ViT Structural Attributes on OoD Generalization: Increasing Embedding Dimension isGenerally Helpful": "Our OoD-ViT-NAS benchmark with 3, 000 ViT architectures covers diverse design choices in ViTstructural attributes, including embedding dimension (Embed_Dim), network depth, number of heads(#Heads), and MLP ratio (MLP_Ratio), which allows for finding a wide range of ViT with differentstructures and complexities. Utilizing our comprehensive benchmark, we are the first to provide ananalysis of the impact of these ViT structural attributes. We investigate which structural attributesin ViTs could lead to better OoD generalization. Through our analysis, we find that increasingthe embedding dimension of a ViT architecture can generally improve OoD generalization. Theadditional analysis on another search space further confirms our finding. The details for thisadditional analysis can be found in Appx. 8 Experimental Setup.To verify the effectiveness of ViT architectural attributes on our OoD gener-alization benchmark, we present the results from two perspectives: (1) an analysis of rank correlationfor our OoD-ViT-NAS benchmark and (2) a comparison of OoD accuracy across different embeddingdimensions. To gain insights into the relationship between embedding dimension (Embed_Dim) andOoD performance, we created the visualizations for all architectures in our OoD-ViT-NAS benchmark.These visualizations compare the average OoD accuracies across different ViT architectures withvarying embedding dimensions and depths. Examples of these visualizations are shown in .Additional results can be found in the App.16.",
  "Results.In -c, we find that the embedding dimension generally has the highest correlationwith OoD accuracy among all ViT architectural attributes. This positive correlation indicates that Em-": ": The effect of #Embed_Dim on robustness generalization of ViTs. The numbers denote themean OoD accuracy across ViT architectures with specific colour-coded embedding dimensions anddepths. The data points with blue , orange , and greencolours represent ViT architectures withan embedding dimension of 320, 384, and 448, respectively. Generally, a higher OoD accuracy isobtained when the embedding dimension of ViT architectures increases for most OoD shifts.See .25 and 18.26 in Appx. 16 for additional plots and results on other OoD shifts. bed_Dim could play a crucial role in achieving OoD generalization performance. Our comprehensiveOoD-ViT-NAS benchmark sheds light on a previously unknown relationship: the potential impact ofembedding dimension (Embed_Dim) on OoD generalization in ViTs. This trend holds across mostOoD shifts in our benchmark (), suggesting that among other architectural attributes, the designchoice of Embed_Dim might significantly influence a models OoD generalization. Our experiments yield several intriguing phenomena. Based on -c, we observe that networkdepth has a slight impact on overall OoD generalization performance (correlation: 0.19). Also, asshown in , for a given embedding dimension (represented by a distinct colour), we report themean OoD accuracy, showing how the mean OoD accuracy changes among ViT architectures ofvarying depths, which aligns with our empirical insight. shows that while increasing depth canbe beneficial for improving ViTs OoD generalization in some cases, there exist shallower modelsthat tend to perform better in terms of OoD accuracy compared to those with deeper models. It is evident from -c, where both the MLP ratio (0.09) and the number of heads (0.07) exhibitvery low correlation values with overall OoD performance. These findings highlight that increasingthe MLP ratio and the number of heads may not substantially enhance a models robustness to OoDdata. Due to space constraints, we defer additional experiments to the Appx. 17, showing that thenetwork depth, MLP ratio, and #Heads might have non-obvious impacts on OoD generalization. Increasing Embedding Dimension help ViT learn more high-frequency patterns, leading toimprove OOD generalization. In this section, we design a frequency study to understand our finding:why increasing ViT Embedding Dimension can generally improve ViTs OOD generalization. Inthe literature, the models obtain higher performance on preserving High-Frequency-Component(HFC) samples tend to learn more HFC . By learning more HFC, the models improve OODgeneralization . Our hypothesis is that Increasing embedding dimension helps ViTs learnmore HFC resulting in improving OOD generalization. We adapt the experiment from to verifyour hypothesis. The details on this experimental setup can be found in the Appx. 9. In a nutshell, wefilter HFC by hyper-parameter radius r, where the higher the r, the lesser HFC. As shown in ,we observe that when increasing Embedding Dimension, the performances obtained on filtering-HFCsamples are improved. This observation holds true across setups varying radius r, supporting thatincreasing Embedding Dimension helps ViT learn more HFC. In contrast, increasing other ViTstructural attributes does not help improve ViT learn more HFC. Robust ViT architectures designed by our finding.Our study provides significant insights forguiding the design of ViT architectures. Specifically, among ViT structural attributes, increasingembedding dimension can generally improve OoD generalisation of ViT architectures. Our insightleads to a simple method which can achieve ViT architectures that can outperform well-establishedhuman-designed. We demonstrate the superiority of ViT based on our insights in Tab. 3. Scaling upViT architecture (e.g., from ViT-B-32 to ViT-L-32) by humans typically involves compound scalingof various ViT structural attributes. However, our findings suggest that not all ViT structural attributesneed to be increased to benefit OoD generalisation. Among these attributes, increasing the embeddingdimension is the most crucial factor for improving OoD generalisation. By only increasing the",
  "Swin-S 9624324.0184.4849.6047.770.01840.0725": ": Following setting in , the ViTs which were trained on original ID data, arenow tested on high frequency components (HFC) of OoD samples, with r as the radius for frequencyfiltering. The higher the OoD accuracy, the more HFC learned in the model.embedding dimension, ours ViT architectures (e.g., Increasing embedding dimension of ViT-B-32)are significantly more efficient and outperform compound scaling architectures (e.g., ViT-L-32).",
  "Conclusion": "In this work, we introduce OoD-ViT-NAS, the first comprehensive benchmark for NAS on OoDgeneralization of ViT architectures. Using this benchmark, we conduct a comprehensive investigationon OoD generalization for ViT. Firstly, we show that ViT architecture design significantly impactsOoD accuracy. Secondly, we show that the architectural findings from existing works for IDperformance could not apply to OoD generalization due to the low correlation between ID and OoDaccuracy. Thirdly, we conduct the first study of NAS for ViTs OoD generalization and show thatexisting Training-free NAS methods struggle with OoD prediction. Surprisingly, simple proxieslike #Param or #Flops outperform other complex Training-free NAS. Finally, we conduct the firststudy on the impact of ViT architectural attributes on OoD generalization. Our study reveals thatincreasing a ViT architectures embedding dimensions can generally improve OoD generalization.We believe our benchmark OoD-ViT-NAS and comprehensive analysis will catalyze and streamlinefuture research on understanding how ViT architecture design influences OoD generalization. Acknowledgement. This research is supported by the National Research Foundation, Singaporeunder its AI Singapore Programmes (AISG Award No.: AISG2-TC-2022-007); The Agency forScience, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No.M23L7b0021). This research is supported by the National Research Foundation, Singapore andInfocomm Media Development Authority under its Trust Tech Funding Initiative. Any opinions,findings and conclusions or recommendations expressed in this material are those of the author(s)and do not reflect the views of National Research Foundation, Singapore and Infocomm MediaDevelopment Authority. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and YinxiaoLi. Maxvit: Multi-axis vision transformer. In European conference on computer vision, pages 459479.Springer, 2022. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFinternational conference on computer vision, pages 1001210022, 2021.",
  "Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbonesfor object detection. In European Conference on Computer Vision, pages 280296. Springer, 2022": "Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. Segvit: Semanticsegmentation with plain vision transformers. Advances in Neural Information Processing Systems, 35:49714982, 2022. Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers forvisual recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages1227012280, 2021. Haibin Wang, Ce Ge, Hesen Chen, and Xiuyu Sun. Prenas: Preferred one-shot learning towards efficientneural architecture search. In International Conference on Machine Learning, pages 3564235654. PMLR,2023. Xiu Su, Shan You, Jiyang Xie, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang,and Chang Xu. Vitas: Vision transformer architecture search. In European Conference on ComputerVision, pages 139157. Springer, 2022. Chen Tang, Li Lyna Zhang, Huiqiang Jiang, Jiahang Xu, Ting Cao, Quanlu Zhang, Yuqing Yang, ZhiWang, and Mao Yang. Elasticvit: Conflict-aware supernet training for deploying fast vision transformer ondiverse mobile devices. In Proceedings of the IEEE/CVF International Conference on Computer Vision,pages 58295840, 2023. Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, and Rongrong Ji.Training-free transformer architecture search. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 1089410903, 2022. Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Yonghong Tian, Jie Chen, and Rongrong Ji. Training-freetransformer architecture search with zero-cost proxy guided evolution. IEEE Transactions on PatternAnalysis and Machine Intelligence, 2024.",
  "Chengyue Gong and Dilin Wang. Nasvit: Neural architecture search for efficient vision transformers withgradient conflict-aware supernet training. ICLR Proceedings 2022, 2022": "Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li, Thomas Unterthiner, and AndreasVeit. Understanding Robustness of Transformers for Image Classification. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pages 1023110241, oct 2021. Shiyu Tang, Ruihao Gong, Yan Wang, Aishan Liu, Jiakai Wang, Xinyun Chen, Fengwei Yu, Xianglong Liu,Dawn Song, Alan Yuille, et al. Robustart: Benchmarking robustness on architecture design and trainingtechniques. arXiv preprint arXiv:2109.05211, 2021.",
  "Salman Rahman and Wonkwon Lee. Out of distribution performance of state of art vision model. arXivpreprint arXiv:2301.10750, 2023": "Minghao Chen, Kan Wu, Bolin Ni, Houwen Peng, Bei Liu, Jianlong Fu, Hongyang Chao, and Haibin Ling.Searching the search space of vision transformer. Advances in Neural Information Processing Systems,34:87148726, 2021. Zimian Wei, Peijie Dong, Zheng Hui, Anggeng Li, Lujun Li, Menglong Lu, Hengyue Pan, and DongshengLi. Auto-prox: Training-free vision transformer architecture search via automatic proxy discovery. InProceedings of the AAAI Conference on Artificial Intelligence, number 14, pages 1581415822, 2024. Mingyu Ding, Xiaochen Lian, Linjie Yang, Peng Wang, Xiaojie Jin, Zhiwu Lu, and Ping Luo. Hr-nas:Searching efficient high-resolution neural architectures with lightweight transformers. In Proceedings ofthe IEEE/CVF conference on computer vision and pattern recognition, pages 29822992, 2021. Jihao Liu, Xin Huang, Guanglu Song, Hongsheng Li, and Yu Liu. Uninet: Unified architecture search withconvolution, transformer, and mlp. In European Conference on Computer Vision, pages 3349. Springer,2022. Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, ThomasHuang, Xiaodan Song, Ruoming Pang, and Quoc Le. Bignas: Scaling up neural architecture search withbig single-stage models. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part VII 16, pages 702717. Springer, 2020. Zi-Hang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Yujun Shi, Xiaojie Jin, Anran Wang, and Jiashi Feng.All tokens matter: Token labeling for training better vision transformers. Advances in neural informationprocessing systems, 34:1859018602, 2021. Chun-Fu Richard Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale visiontransformer for image classification. In Proceedings of the IEEE/CVF international conference on computervision, pages 357366, 2021. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, ChunjingXu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pages 1229912310, 2021.",
  "Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve.Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021": "Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neuralnetworks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprintarXiv:1911.08731, 2019. Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, and Tao Xiang. Learning to generate novel domainsfor domain generalization. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK,August 2328, 2020, Proceedings, Part XVI 16, pages 561578. Springer, 2020. Yimeng Chen, Tianyang Hu, Fengwei Zhou, Zhenguo Li, and Zhi-Ming Ma. Explore and exploit thediverse knowledge in model zoo for domain generalization. In International Conference on MachineLearning, pages 46234640. PMLR, 2023. Alexandre Ram, Kartik Ahuja, Jianyu Zhang, Matthieu Cord, Lon Bottou, and David Lopez-Paz. Modelratatouille: Recycling diverse models for out-of-distribution generalization. In International Conferenceon Machine Learning, pages 2865628679. PMLR, 2023. Jungwuk Park, Dong-Jun Han, Soyeong Kim, and Jaekyun Moon. Test-time style shifting: Handlingarbitrary styles in domain generalization. In International Conference on Machine Learning, pages2711427131. PMLR, 2023. Liang Chen, Yong Zhang, Yibing Song, Ying Shan, and Lingqiao Liu. Improved test-time adaptation fordomain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 2417224182, 2023. Hyeonseob Nam, HyunJae Lee, Jongchan Park, Wonjun Yoon, and Donggeun Yoo. Reducing domaingap by reducing style bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 86908699, 2021. Liang Chen, Yong Zhang, Yibing Song, Lingqiao Liu, and Jue Wang. Self-supervised learning of adversarialexample: Towards good generalizations for deepfake detection. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pages 1871018719, 2022. Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee. Selfreg: Self-supervisedcontrastive regularization for domain generalization. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pages 96199628, 2021. Pan Li, Da Li, Wei Li, Shaogang Gong, Yanwei Fu, and Timothy M Hospedales. A simple featureaugmentation for domain generalization. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pages 88868895, 2021.",
  "Yingjun Du, Xiantong Zhen, Ling Shao, and Cees GM Snoek. Metanorm: Learning to normalize few-shotbatches across domains. In International Conference on Learning Representations, 2020": "Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, and Mingsheng Long. Open domain generalizationwith domain-augmented meta-learning. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pages 96249633, 2021. Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. An investigation of why overparam-eterization exacerbates spurious correlations. In International Conference on Machine Learning, pages83468356. PMLR, 2020.",
  "Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. arXiv preprint arXiv:1803.03635, 2018": "Dinghuai Zhang, Kartik Ahuja, Yilun Xu, Yisen Wang, and Aaron Courville. Can subnetwork structure bethe key to out-of-distribution generalization? In International Conference on Machine Learning, pages1235612367. PMLR, 2021. Geon Yeong Park, Sangmin Lee, Sang Wan Lee, and Jong Chul Ye. Training debiased subnetworks withcontrastive weight pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 79297938, 2023. Haoyue Bai, Fengwei Zhou, Lanqing Hong, Nanyang Ye, S-H Gary Chan, and Zhenguo Li.Nas-ood: Neural architecture search for out-of-distribution generalization. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 83208329, 2021. Dilin Wang, Chengyue Gong, Meng Li, Qiang Liu, and Vikas Chandra. Alphanet: Improved training ofsupernets with alpha-divergence. In International Conference on Machine Learning, pages 1076010771.PMLR, 2021. Dilin Wang, Meng Li, Chengyue Gong, and Vikas Chandra. Attentivenas: Improving neural architecturesearch via attentive sampling. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pages 64186427, 2021. Tangyu Jiang, Haodi Wang, and Rongfang Bie. Meco: Zero-shot nas with one data and single forward passvia minimum eigenvalue of correlation. Advances in Neural Information Processing Systems, 36, 2024.",
  "Yongtao Wu, Fanghui Liu, Carl-Johann Simon-Gabriel, Grigorios G Chrysos, and Volkan Cevher. Robustnas under adversarial training: benchmark, theory, and beyond. arXiv preprint arXiv:2403.13134, 2024": "Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan, and Wanli Ouyang.Glit: Neural architecture search for global and local image transformer. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pages 1221, 2021. Mingyang Zhang, Xinyi Yu, Haodong Zhao, and Linlin Ou. Shiftnas: Improving one-shot nas viaprobability shift. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages59195928, 2023. Shiguang Wang, Tao Xie, Jian Cheng, Xingcheng Zhang, and Haijun Liu. Mdl-nas: A joint multi-domainlearning framework for vision transformer. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pages 2009420104, 2023. Yaowei Li, Ruijie Quan, Linchao Zhu, and Yi Yang. Efficient multimodal fusion via interactive prompting.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26042613,2023.",
  "Li Fei-Fei, Jia Deng, and Kai Li. Imagenet: Constructing a large-scale image database. Journal of vision,9(8):10371037, 2009": "John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, PercyLiang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In Marina Meila and Tong Zhang, editors, Proceedings ofthe 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine LearningResearch, pages 77217735. PMLR, 1824 Jul 2021. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiersgeneralize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the36th International Conference on Machine Learning, volume 97 of Proceedings of Machine LearningResearch, pages 53895400. PMLR, 0915 Jun 2019. Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad. Id and ood performance are sometimesinversely correlated on real-world datasets. In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, andS. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 7170371722.Curran Associates, Inc., 2023. Florian Wenzel, Andrea Dittadi, Peter Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow,David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, Bernhard Schlkopf, and Francesco Locatello.Assaying Out-Of-Distribution Generalization in Transfer Learning. In S Koyejo, S Mohamed, A Agarwal,D Belgrave, K Cho, and A Oh, editors, Advances in Neural Information Processing Systems, volume 35,pages 71817198. Curran Associates, Inc., 2022.",
  "Limitations and Broader Impact": "Given the extensive set of experiments presented in this work, the evaluation of training-free NASproxies is significantly dependent on the initial robust benchmarks, which can be costly and resourceintensive to create in the first place. As this work studies the robustness of ViT architectures to OoD shift, we could demonstrate thatcarefully designed ViT architectures can significantly enhance OoD generalization. Our approachfocuses on evaluating training-free NAS for ViT architectures, offering valuable insights that cancomplement research exploring the effects of robust architectural design. Finally, we publicly releaseour OoD-NAS-ViT and code base for future research.",
  "Analysis on Human-design ViT Search Space": "Our findings on embedding dimensions are derived from analysing ViT architectures sampled throughAutoFormer. To further validate these results, we also investigate the impact of ViT structuralattributes on OoD generalisation within the human-designed ViT search space . .1: We conduct additional experiments on human design ViT to further confirm our mainfindings that, among ViT structural attributes, embedding dimension is the most important ViTstructural attribute to OoD generalisation. We train each architecture from on IN-100 and evaluate onIN-R. The trade-off between OoD Acc and computational metrics (i.e., Latency and #Param)is quantifies by , which is the ratio of increase in OoD Acc and increase in computationalmetrics. Higher is better. Note that increasing #Head remains the same #Param but increasesLatency in ViT setup .",
  "Detailed description of the Frequency Analysis Setup": "We adapt the experiment from to verify our hypothesis. We quantify how the amount of HFClearnt in the ViTs changes if the embedding dimension of ViTs changes. Particularly, we first filterHFC from testing images of IN-R following then evaluate the performance of 1000 ViTsin our search space on IN-R with filtering data points. Following to generate HFC-preservingimages, we first convert original images to FFT images. Then, we filter HFC by hyper-parameterradius r (r is set to 4, 8, 12, 16 in our experiments). In a nutshell, the higher the r, the lesser HFC.",
  "To construct OoD-ViT-NAS benchmark, we evaluate 3,000 architectures within our benchmark oncommon and most SOTA OoD datasets, including:": "ImageNet-1k : This is a large and common image dataset widely used in computervision research. It contains over 1.3 million labeled high-resolution images belonging to1,000 different object categories (classes). Each image is labeled with a class (e.g., \"cat\",\"airplane\", \"chair\"). ImageNet-C : This dataset builds upon the original ImageNet test set by applied algo-rithmically corruptions. These corruptions simulate real-world factors that can deviate datafrom the training set, such as blur, noise, digital, and weather effects. ImageNet-C offersa comprehensive OoD scenarios with 15 different corruption types, each with 5 severitylevels, resulting in a total of 75 unique OoD setups.",
  "ImageNet-R : This dataset is a rendition of ImageNet, containing images with manipu-lated textures and local image statistics": "ImageNet-Sketch : This dataset introduces a unique OoD challenge by providing black-and-white sketch images corresponding to the ImageNet-1K test set. This significantdivergence in visual representation tests a models ability to generalize beyond photographicdata. Stylized ImageNet : This dataset consists of a stylized version of ImageNet generatedthrough techniques like AdaIN style transfer, resulting in variations like greyscale,silhouettes, and edges. This dataset assesses a models ability to handle data with differentartistic interpretations.",
  "corruption: performance on IN-C": "* Fog: performance on one out of 15 corruption in IN-C: Fog, Gaussian Noise,Fog, Snow, Elastic Transform, Jpeg Compression, Frost, Motion Blur, Brightness,Defocus Blur, Glass Blur, Impulse Noise, Shot Noise, Zoom Blur, Constrast,Pixelate 1: performance at OoD shift severity level 1. There are total 5 level of OoD shiftseverity for each corruption in IN-C",
  "Additional results on the analysis of OoD accuracy range": "In the main paper, we visualize 11 OoD accuracy ranges and ID accuracy range for reference. In thisAppx. section, we provide the visualization of the remaining OoD accuracy ranges. The results areillustrated in .5. Our observation on other OoD accuracy ranges are generally consistent withour findings in Sec. 4.1. .5: As in , our analysis on the OoD accuracy range highlights the significantinfluence of ViT architectural designs on OoD accuracy. The numbers within each violin plot for eachsub-figures (e.g., IN-D Material 8.99 (1.06), 8.50 (2.25), and 8.72 (0.56)) denote the correspondingOoD(ID) accuracy range of architectures sampled from AutoFormer-Tiny/Small/Base search space,respectively. .6: Visualization of OoD accuracy range across IN-C OoD shift severity. The experimentsare conducted on 1, 000 architectures in Autoformer-Small search space within our OoD-NAS-ViTbenchmarks. Level 0 denotes the clean examples. We generally observe that the range of OoDaccuracy widens as the severity of the OoD shift increases.",
  "Additional results on the analysis of the correlation between ID and OoDaccuracy": "In the main paper, we provide the Kendall correlation between ID accuracy and 8 OoD datasets.For some OoD datasets with different OoD shift types, we average the correlation with ID accuracyof different OoD shift types to obtain average correlation with ID accuracy for that OoD dataset. Inthis Appx. section, we provide the detailed correlations with ID accuracy of each OoD shifts in suchOoD data. Specifically, we provide the detailed correlation for IN-C, IN-D, and IN-P in .7,.8, and .9, respectively.",
  "Additional results on the analysis of OoD Performance of Paretoarchitectures for ID": "In the main paper, due to space constraints, we only provide the Pareto analysis results on a fewrepresentative OoD datasets. In this Appx. section, we provide additional results on this Paretoarchitecture analysis in .24, 18.19, 18.20, 18.21, 18.22, 18.23. In the following scatterplots, blue dotsrepresent architectures in the search space, while red dotsrepresent the ID Paretoarchitectures.",
  "Additional Results for Benchmarking Zero-cost Proxies": "In the main submission, we provides the comparison of Kendall ranking correlation between theID/OoD accuracies and the zero-cost proxy values across all OoD datasets. In this section, weprovides the correlation for each dataset for a detailed observation. The results can be found inTab. 15.3, 15.4, 15.5, 15.6, 15.7, 15.8, 15.9, 15.10. Our observations on individual OoD datasets areconsistent with our findings in Sec. 4.3. .3: Comparison of Kendall ranking correlation between the ID/OoD accuracies and thezero-cost proxy values on ImageNet-C datasets in the Autoformer search space. Bold and underlinestands for the best and second, respectively.",
  "Ablation Study on the Impact of ViT Architectural Attributes to OoD Generalization": "In this section, we demonstrate the effectiveness of each ViT architectural attribute on OoD accuracyfrom the ablation study perspective. All ablation studies are based on 1,000 ViT architecturessampled from Autoformer-Small search space in our OoD-ViT-NAS benchmark. Through our generalanalysis in Sec. 4.4 in the main and ablation study, we show that the embedding dimension has thehighest impact among ViT architectural attributes, while network depth has a slight impact on OoDgeneralization. Experimental Setups. We conduct the ablation study on the impact of ViT architectural attributes onOoD generalization. Particularly, for each ablation study of one ViT architectural attribute, we varythat attribute while keeping all other attributes fixed. Then, we compute Kendalls rank correlationcoefficient between each attribute and different OoD shifts. While we can directly adjust the depthand embedding dimension, adjusting MLP_Ration and #Head is challenging. This is because thesetwo attributes for each ViT arch are in the form of a list with depth elements. Each element is selectedamong 3 choices. This results in a huge combination. To deal with this difficulty, we first computethe means of MLP_Ration/#Head for each architecture. Then, during the ablation study, we explore arange of values for MLP ratio and the number of heads (mean #Head = 6 0.05, mean MLP_Ratio = .6: Comparison of Kendall ranking correlation between the ID/OoD accuracies and thezero-cost proxy values on Stylized-ImageNet datasets in the Autoformer search space. Bold andunderline stands for the best and second, respectively.",
  "0.05) to capture the impact of these more nuanced architectural variations. We note that thisrange of values is small, allowing us to approximately fix these two attributes": "First, we assess the impact of embedding dimension on OoD generalization by fixing the configu-rations of all other ViT structural attributes (i.e., network depth = 13, mean #Head = 6 0.05, andmean MLP_Ratio = 3.5 0.05). The correlation results are shown in .11. We observe anoverall positive correlation of 0.65. This further supports our observation in Sec. 4.4 that increasingthe embedding dimension generally could lead to better OoD performance across most OoD shiftsfor these ViT architectures.",
  "#Flops--0.4705 0.33910.5959 0.1230": "To demonstrate how the number of layers (i.e., network depth) in a ViT architecture affects themodels OoD generalization, we fix other ViT architectural attributes (i.e., Embed_Dim = 384, mean#Head = 6 0.05, and mean MLP_Ratio = 3.5 0.05). The ranking correlation for all architecturesbetween depth and OoD accuracy in .12 suggests a weak correlation between the ViT networkdepth and OoD accuracy. This observation is consistent with our finding in Sec. 4.4 that the depthhas a minimal influence on OoD generalization.",
  ": Kendalls rank correlation coefficient between varying network depth and all OoDaccuracy": "For MLP_Ratio and #Heads, we observe that the rank correlation coefficients of MLP_Ratioand #Heads in .13 and 17.14, respectively, reveals a weak correlation between the#Heads/MLP_Ratio and OoD generalization. This suggests that within the explored range, these .10: Comparison of Kendall ranking correlation between the ID/OoD accuracies and thezero-cost proxy values on ImageNet-O datasets in the Autoformer search space. Bold and underlinestands for the best and second, respectively.",
  "Layer-Wise Analysis": "The number of heads and MLP ratio vary across layers, which allows for searching more diverse ar-chitectures. In this section, we provide the layer-wise analysis of the influence of MLP_Ratio/#Headsin each layer on OoD generalization. Experimental Setups:In Autoformer-Small search space, the MLP ratio at any given layer referred to as the i-th layer can be set to 3.0, 3.5, or 4. In addition to architectures in our benchmark,we create a set of 108 sampled architectures from Autoformer-Small. These architectures are fixedwith three architectural design attributes: (Depth = 12, Embed_Dim = 320, #Heads for all layers= 5). With these parameters fixed, the total number of potential MLP_Tatio configurations is stillextensive (i.e., 312), making it impractical to absolutely fix the MLP_Ration in our ablation study.To specifically assess the influence of the MLP_Ration at the i-th layer, we further fix a constant",
  "MLP_Ration across all other layers. For example, .18 depicts the nine configurations ofMLP_Ration to analyze the impact of 5-th layer": "We carry out a similar layer-wise analysis to demonstrate the effect of #Heads at a specific layer(i-th layer). In addition to architectures in our benchmark, we create a set of 108 sampled fromAutoformer-Small. These architectures are fixed with three architectural design attributes: (Depth= 12, Embed_Dim =320, #MLP_Ratio for all layers =3.0). To specifically assess the influence ofthe #Head at the i-th layer, we further fix a constant #Head across all other layers. For example,.18 depicts the nine configurations of #Head to analyze the impact of 3-th layer. Results. The layer-wise analysis for MLP_Ratio are shown in .15. Each sub-figure demon-strates the change in OoD Accuracy when varying MLP_Ratio at a particular layer. While increasingit improved OoD accuracy in some layers, it decreased it in others. This aligns with our observationsin Sec. 4.4, suggesting no clear overall impact of MLP_Ratio on OoD generalization. The layer-wise analysis for #Head is illustrated in .16. Similar to our observation in thelayer-wise analysis for MLP_Ratio, the impact of #Head is non-obvious, which is consistent with ourfinding in Sec. 4.4. .15: A visualization on the effect of changing MLP ratio per layer to OoD accuracy in 108architectures sampled from Autoformer-Small in the layer-wise study. To evaluate the effect of layeri-th, we fix the MLP ratio = 4.0 for the remaining layers. We observe that a slightly higher OoDaccuracy range can be obtained by changing the MLP ratio at layers 4 and 5. .16: A visualization on the effect of changing #Heads per layer to OoD accuracy in 108architectures sampled from Autoformer-Small in the layer-wise study. To evaluate the effect of layeri-th, we fix the #Heads = 6 for the remaining layers.",
  "Hyper-parameters": ".11: Hyper-parameters for Evaluation on 8 common large-scale OoD datasets. In total, weevaluate 3,000 diverse ViT architectures in our OoD-NAS-ViT benchmark sampled from Autoformer-Tiny/Small/Base search spaces . Input resolution is set to 224224 pixels, with mean and standarddeviation normalization applied using ImageNet statistics (mean = [0.485, 0.456, 0.406], std =[0.229, 0.224, 0.225]). Transformations follow the Standard ImageNet preprocessing, includingresize and center crop.",
  "Number of workers101010410101010": "To ensure reproducibility, we provide a detailed description of the hyper-parameters used for eval-uating 3,000 ViT architectures in our OoD-NAS-ViT benchmarks on 8 common large-scale OoDdatasets: ImageNet-C , ImageNet-A , ImageNet-O , ImageNet-P , ImageNet-D, ImageNet-R , ImageNet-Sketch , and Stylized ImageNet . The evaluation for ImageNet-D, ImageNet-O and Stylized ImageNet strictly follows previous works toensure consistency and comparability. The details of the evaluation are shown in .11. Theevaluated architectures are sampled on AutoFormer-Tiny/Small/Base Search Spaces .",
  "Compute Resource": "All our experiments are conducted using NVIDIA RTX A6000 GPUs. We utilized 2 GPUs foreach experiment. The substantial computational resources required for these evaluations underscorethe complexity and scale of our work. Detailed information on the GPU-hour consumed for allexperiments to construct our OoD-ViT-NAS Benchmark can be found in .12. In total, theexperiments demanded a significant investment of approximately 3900 GPU-hours, reflecting theextensive computational effort involved.",
  "GPU-Hour29586721293285337503903": ".19: As in , we show that lower OoD accuracy can be obtained for the higher IDaccuracy in the Pareto architectures of Autoformer-Small. The left panels show the ID accuracy, andeach panel on columns 2 to 4 shows results from the OoD accuracy of IN-P, IN-A, IN-R, IN-Sketch,Stylized-IN, and AUPR of IN-O. .20: Visualization of Pareto architectures in Autoformer-Base. The left panels show the IDaccuracy, and each panel on columns 2 to 4 shows results from the OoD accuracy of IN-C commoncorruptions and IN-D. .21: Visualization of Pareto architectures in Autoformer-Base. The left panels show the IDaccuracy, and each panel on columns 2 to 4 shows results from the OoD accuracy of IN-P, IN-A,IN-R, IN-Sketch, Stylized-IN, and AUPR of IN-O. .22: Visualization of Pareto architectures in Autoformer-Tiny. The left panels show the IDaccuracy, and each panel on columns 2 to 4 shows results from the OoD accuracy of IN-C commoncorruptions and IN-D. .23: Visualization of Pareto architectures in Autoformer-Tiny. The left panels show the IDaccuracy, and each panel on columns 2 to 4 shows results from the OoD accuracy of IN-P, IN-A,IN-R, IN-Sketch, Stylized-IN, and AUPR of IN-O. .24: As in , we show that lower OoD accuracy can be obtained for the higher IDaccuracy in the Pareto architectures of Autoformer-Small. The left panels show the ID accuracy, andeach panel in columns 2 to 4 shows results from OoD accuracy of IN-C common corruptions andIN-D. We observe that architectural designs have a greater effect on OoD accuracy than ID accuracy,especially when OoD shifts become more severe. .25: As in , we show the potential impact of embedding dimension (Embed_Dim)on OoD generalization in ViTs architectures sampled from Autoformer-Small. The numbers denotethe average OoD performance, and The data points with blue , orange , and greencoloursrepresent ViT architectures with the embedding dimension of 320, 384, and 448, respectively. Eachpanel shows results from the OoD accuracy of IN-C common corruptions and IN-D. .26: As in , we show the potential impact of embedding dimension (Embed_Dim)on OoD generalization in ViTs architectures sampled from Autoformer-Small. The numbers denotethe average OoD performance, and The data points with blue , orange , and greencoloursrepresent ViT architectures with the embedding dimension of 320, 384, and 448, respectively. Eachpanel shows results from the OoD accuracy of IN-P, IN-A, IN-R, IN-Sketch, Stylized-IN, and AUPRof IN-O."
}