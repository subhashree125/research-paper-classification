{
  "Abstract": "Despite advancements in Text-to-Video (T2V) generation, producing videos withrealistic motion remains challenging. Current models often yield static or mini-mally dynamic outputs, failing to capture complex motions described by text. Thisissue stems from the internal biases in text encoding, which overlooks motions,and inadequate conditioning mechanisms in T2V generation models. To addressthis, we propose a novel framework called DEcomposed MOtion (DEMO), whichenhances motion synthesis in T2V generation by decomposing both text encodingand conditioning into content and motion components. Our method includes acontent encoder for static elements and a motion encoder for temporal dynamics,alongside separate content and motion conditioning mechanisms. Crucially, weintroduce text-motion and video-motion supervision to improve the models under-standing and generation of motion. Evaluations on benchmarks such as MSR-VTT,UCF-101, WebVid-10M, EvalCrafter, and VBench demonstrate DEMOs superiorability to produce videos with enhanced motion dynamics while maintaining highvisual quality. Our approach significantly advances T2V generation by integratingcomprehensive motion understanding directly from textual descriptions. Projectpage:",
  "Introduction": "The field of Text-to-Video (T2V) generation has seen significantadvancements, especially with the advent of diffusion models. These models have demonstratedimpressive capabilities in generating visually appealing videos from textual descriptions. However, apersistent challenge remains: generating videos with realistic and complex motions. Most existingT2V models produce outputs that resemble static animations or exhibit minimal camera movement,falling short of capturing the intricate motions described in textual inputs . This limitation arises from two primary challenges. The first challenge is the inadequate motionrepresentation in text encoding. Current T2V models utilize large-scale visual-language models(VLMs), such as CLIP , as text encoders. These VLMs are highly effective at capturing staticelements and spatial relationships but struggle with encoding dynamic motions. This is primarilydue to their training focus, which biases them towards recognizing nouns and objects , whileverbs and actions are less accurately represented . The second challenge is the relianceon spatial-only text conditioning. Existing models often extend Text-to-Image (T2I) generation",
  "arXiv:2410.24219v1 [cs.CV] 31 Oct 2024": "techniques to T2V tasks , applying text information through spatial cross-attention on a frame-by-frame basis. While effective for generating high-quality static images, thisapproach is insufficient for videos, where motion is a critical component that spans both spatial andtemporal dimensions. A holistic approach that integrates text information across these dimensions isessential for generating videos with realistic motion dynamics. Recent efforts to address these challenges have involved incorporating additional control signalssuch as sketches , strokes , database samples , depth maps , andhuman poses , reference videos , and bounding boxes into the T2Vgeneration process. These signals are derived either from reference videos or pre-trained motiongeneration models . While these approaches improve motion synthesis, they depend onexternal references or pre-trained models, which may not always be practical. Moreover, theyintroduce complexity and potential inefficiencies, as they require separate handling of additional datasources. To address these challenges, we introduce Decomposed Motion (DEMO), a novel framework designedto enhance motion synthesis in T2V generation. DEMO adopts a comprehensive approach bydecomposing both text encoding and conditioning processes into content and motion components.Addressing the first challenge, DEMO decomposes text encoding into content encoding and motionencoding processes. The content encoding focuses on object appearance and spatial layout, capturingstatic elements such as a girl\" and the road\" in the scenario A girl is walking to the left on theroad.\" Meanwhile, the motion encoding captures the essence of object movement and temporaldynamics, interpreting actions like walking\" and directional cues like to the left.\" This separationallows the model to better understand and represent the dynamic aspects of the described scenes.Regarding the second challenge, DEMO decomposes the text conditioning process into contentand motion dimensions. The content conditioning module integrates spatial embeddings into thevideo generation process on a frame-by-frame basis, ensuring that static elements are accuratelydepicted in each frame. In contrast, the motion conditioning module operates across the temporaldimension, infusing dynamic motion embeddings into the video. This separation enables the model tocapture and reproduce complex motion patterns described in the text. Moreover, DEMO incorporatesnovel text-motion and video-motion supervision techniques to enhance the models understandingand generation of motion. Text-motion supervision aligns cross-attention maps with the temporalchanges observed in ground truth videos, guiding the model to focus on motion information. Video-motion supervision constrains the predicted video latent to mimic the motion patterns of real videos,promoting the generation of coherent and realistic motion dynamics. These supervision techniquesensure that the model not only generates visually appealing videos but also renders the intricatemotions described in the text. To validate our framework, we conduct extensive experiments on several benchmarks, including MSR-VTT , UCF-101 , WebVid-10M , EvalCrafter , and VBench . DEMO achievessubstantial improvements in metrics related to motion dynamics and visual fidelity, indicating itssuperior capability to generate videos that are both visually appealing and dynamically accurate.",
  "Related Work": "T2V Generation. The T2V domain has made substantial strides, building on the progress inT2I generation. The first T2V model, VDM , introduces a space-time factorized U-Net fortemporal modeling, training on both images and videos. For high-definition videos, models likeImagenVideo , Make-A-Video , LaVie , and Show-1 use cascades of diffusionmodels with spatial and temporal super-resolution. MagicVideo , Video LDM , and LVDM apply latent diffusion for video, working in a compressed latent space. VideoFusion separatesvideo noise into base and residual components. ModelScopeT2V uses 1D convolutions and attentionto approximate 3D operations. Stable Video Diffusion (SVD) divides the process into T2Ipre-training, video pre-training, and fine-tuning and demonstrate the necessity of a well-curatedhigh-quality pretraining dataset for developing a strong base model. Despite these advancements, thegenerated videos still exhibit limited motion dynamics, often appearing largely static with minimalmotion, highlighting an ongoing challenge in achieving dynamic and realistic motions.",
  "T2V Generation with Rich Motion. Generating video with rich motion is still an open challengein the field of T2V generation. Existing works [15, 9, 23, 60, 51, 68, 72, 33, 71, 6, 12, 62, 73, 34,": "A [ADJ] [NOUN_1] [VERB] [ADV] [ADP] the [NOUN_2]. A bi g cat wal ks qui ckl y acr oss t he br i dge. Motion InformationContent Information sl owl y st eadi l y cl umsi l y . . . Gr oup 1 A shor t man cr awl s qui ckl y besi de t he t r ee. Gr oup N . . . Gr oup by [ ADV] Pai r - Wi se Di st ance Pai r - Wi se Di st ance . . . sl owl y st eadi l y cl umsi l y . . . : Our Pilot Study. We generated a set of prompts (262144 in total) following a fixedtemplate, grouping them according to the different parts of speech (POS). These grouped texts arethen passed into the CLIP text encoder, and we calculate the sensitivity as the average sentencedistance within each group. As shown on the left-hand side, compared to POS representing content,CLIP is less sensitive to POS representing motion. (Results are consistent across different templatesand different sets of words within each POS. Further details can be found in the appendix.) 65, 55] address this challenge by incorporating additional control signals that inherently containrich motion information. Tune-A-Video proposes spatial-temporal self-attention into the T2Ibackbone and trains the model on a single reference video. The model thus learns to generatenew videos with motions specified by the reference video. Materzynska et al. follow theidea of T2I customization to fine-tune the model and a specific text token on a smallset of reference videos. The model can then recontextualize with that learned token to generatenew videos with specific motions. DreamVideo further customizes both the appearancesand motions given reference images and videos. MotionDirector proposes a dual-path Low-Rank Adaptations to decouple the motions and appearances residing in the reference videos.MotionCtrl incorporates object trajectories and camera poses into the T2V generation byconditioning them in the convolution and temporal transformer layers, respectively. Contrasting withthese approaches, DEMO prioritizes the generation of videos that exhibit significant motions derivedsolely from textual descriptions without relying on additional signals.",
  "Method": "Latent Video Diffusion Models (LVDMs). LVDMs build on the diffusion models bytraining a 3D U-Net as the noise predictor, where a VQ-VAE or a VQ-GAN is employedto compress the video into low-dimensional latent space. The 3D U-net consists of down-sample,middle, and up-sample blocks. Each of these blocks comprises multiple convolution layers augmentedby spatial and temporal transformers. The spatial transformer consists of spatial self-attention, spatialcross-attention, and feed-forward layers. The temporal transformer consists of temporal self-attentionand feed-forward layers. The 3D U-Net is trained with a text encoder to minimize the noise-predictionloss in the latent space given as follows:Ldiffusion = Et,z0,N (0,1),p[|| (zt, t, E(p))||22](1)where z is the video latent corresponding to x in the pixel space, t is the time step, E is a text encoder,p is a text prompt, and is noise sampled from Gaussian distribution. zt is noisy z0 after t stepsdiffusion forward process given by:",
  "Motion Representation": ": Overview of DEMO Training. As shown in the left-hand side, DEMO incorporatedual text encoding and text conditioning (for simplicity, other layers in the UNet are omitted). Asshown in the right-hand side, during training, the Ltext-motion is used to enhance motion encoding, theLreg is used to avoid catastrophic forgetting, the Lvideo-motion is to enhance motion integration. Thesnowflakes and flames denote frozen and trainable parameters, respectively. on content encoding rather than motion encoding. To preserve the generalization ability of the T2Vgeneration model, we retain the original text encoder, referring to it as the content encoder (denotedas Ec). Additionally, we introduce a new text encoder, referred to as the motion encoder (denoted asEm), which is specifically designed to capture object movement and temporal dynamics in textualdescriptions (as shown in the left-hand side of ). We initialize our motion encoder from aCLIP text encoder and then fine-tune it using specialized text-motion supervision, as described below. Text-Motion Supervision. Research has shown that cross-attention maps representthe structure of visual content. The cross-attention operation can be viewed as a projection of textinformation into the visual structure domain. With this understanding, we aim to shift the textencoders focus more toward motion information by constraining the temporal changes of cross-attention maps to closely mimic those observed in ground truth videos, as illustrated by the red line in. Formally, given a noisy video latent zt at time step t and a text prompt p, the cross-attentionmaps Ai RHiW iF S, where Hi and W i are the height and width of video latent at the ithcross-attention layer, F is the number of frames, S is the sequence length, for a cross-attention layeri are defined as follows:",
  "where WQ and WK are projection matrices for query and key, i {1, 2, ...M} is layer index,n {1, 2, ..., N} represents each head in multi-head cross-attention, and dn is the dimension of eachhead": "We empirically find that the cross-attention maps corresponding to the [eot]\" token, which aggregatethe whole sentences semantics, play a pivotal role in generating motion. This aligns with the un-derstanding that motion is a global concept and cannot be captured by a single word. For instance,phrases like A baby/dog is walking/running forwards/backward.\" demonstrate that different combi-nations of words can result in significantly different motions. Hence, we focus on the cross-attentionmaps related to the [eot]\" token and constrain them to mimic the motion patterns observed in theground truth videos. This approach forms the basis of our text-motion loss, defined as follows:",
  "where is a function to extract motion dynamics from a video. In our case, we use optical flowto represent the motion dynamics (noting that optical flow is only used during training; during": "inference, we use only the text prompt as input). In light of the potential scale differences betweenthe cross-attention maps and video pixel values, we compute the cosine similarity between them.Additionally, for cross-attention at different spatial resolutions, we downsample the ground truthvideo to match the spatial resolution of the cross-attention maps. Regularization. Recall that CLIP is trained with a contrastive learning objective to matchtexts and images from a group of text-image pairs. However, directly fine-tuning the CLIP textencoder with Equation 1 and Equation 5, which differ significantly from the original contrastivelearning objective, can easily lead to catastrophic forgetting . To mitigate this, we introduce aregularization term in the fine-tuning objective to preserve its generalization ability. Specifically, wepenalize the text embedding if it diverges from the corresponding image embedding, maintainingalignment with the original CLIP contrastive learning objective, as illustrated by the green line in. The regularization loss is defined as follows:",
  "Decomposed Text Conditioning": "DEMO employs separate content conditioning and motion conditioning modules to incorporatecontent and motion information. To preserve the generative capabilities of our base model, wemaintain the original text conditioning module, referred to here as the content conditioning module.We then strategically introduce a novel temporal transformer, referred to as the motion conditioningmodule (detailed structure shown in ), to incorporate motion information along the temporalaxis. To encourage the motion conditioning module to generate and render motion dynamics, wetrain this module under video-motion supervision, as described below.",
  "Lvideo-motion = Et,z0,N (0,1)(z0) (z0,t)22(8)": "where is a function to extract motion features from a video. Given that Ldiffusion is a pixel-wisedenoising loss (whether raw pixel or latent pixel), choosing as a general motion representationthat is not in pixel space may lead to conflicting objectives due to the differing representation spaces.Instead, we choose as the consecutive frame difference defined as follows:",
  "where , , and are scaling factors to balance different loss terms": "LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO Slow mot ion f lower p et als f all f r om a blossom, land ing sof t ly on t he gr ound. An old man wit h whit e hair is shown sp eaking. J oc keys r ac ing. LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO : Qualitative Comparison. Each video is generated with 16 frames. We display frames 1, 2,4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos are available in thesupplementary materials.",
  "Qualitative Evaluations": "In this subsection, we conduct a qualitative comparison among LaVie , VideoCrafter2 ,ModelScopeT2V , and DEMO. For a fair comparison, we use the same seed for each of thesemethods. The comparative analysis is illustrated in , where we showcase examples generatedby these methods. Upon examination, it is evident that these models are capable of producing high-quality videos. However, a notable distinction arises in the dynamic representation of motion withinthe generated videos. The ModelScopeT2V model, while visually appealing, predominantly generatesstatic scenes. For instance, in the scenario described as Slow motion flower petals fall from a blossom,landing softly on the ground (the first example in ), the video generated by ModelScopeT2Vcaptures the petals landing on the ground but lacks the motion of the petals falling. In contrast, DEMOsignificantly outperforms by capturing the essence of motion, producing a video where the petals fallslowly and gently to the ground. Similarly, LaVie demonstrates a similar issue, as illustrated in thethird example, where the jockeys remain largely static. VideoCrafter2 exhibits relatively large motiondynamics but suffers from motion blur, as shown in the third example. Conversely, DEMO vividlycaptures the jockeys racing, thereby providing a more realistic representation. This underscores theadvanced capability of DEMO to generate videos that not only visually represent a scene but alsodynamically encapsulate the ongoing motion.",
  "ModelScopeT2V 37.55628.17ModelScopeT2V fine-tuned37.21612.53DEMO36.35547.31": "Zero-shot T2V Generation on MSR-VTT. We evaluate the performance of our model on theMSR-VTT test set by calculating FID , FVD , and CLIPSIM metrics. For FIDand FVD, in alignment with prior studies , we randomly sample 2048 videos and one promptfor each video from the test set. For CLIPSIM, we follow previous works and usenearly 60k sentences from the entire test set to generate videos. As illustrated in , DEMOdemonstrates notable advancements over the ModelScopeT2V baseline in terms of video qualitymetrics. Specifically, DEMO achieves an FID score of 11.77, showing marked improvement inindividual frame quality compared to the baseline score of 14.89. For FVD, DEMO achieves a scoreof 422 compared to the baseline of 557, indicating improved overall video quality. It is important tonote that the FVD is calculated using an I3D model pre-trained on the Kinetics-400 dataset foraction recognition. By computing the FVD over its logits, this metric not only reflects visual qualitybut also emphasizes motion quality in video generation. Additionally, DEMO improves the CLIPSIM",
  "score from 0.2941 to 0.2965, further demonstrating its superior ability to generate high-quality videosthat are well-aligned with their textual descriptions": "Zero-shot T2V Generation on UCF-101. For UCF-101 , we report the IS and FVD on the101 action classes. For IS and FVD, we follow previous works to generate 100 videos foreach of the 101 classes. We directly use the class names as prompts. As shown in , comparedwith baseline ModelScoprT2V, we improve the FVD from 628.17 to 547.31. However, we observed aslight decrease in IS, which may be attributed to the limited textual information provided by UCF-101class names, such as baby crawling and cliff diving. These prompts primarily suggest motion, andour model, optimized to emphasize this motion, may have over-focused on this limited information.This overemphasis potentially limited the diversity of generated content, lowering the IS. T2V Generation on WebVid-10M (Val). For WebVid-10M , we perform T2V generation on thevalidation set. As shown in , we evaluate the FID, FVD, and CLIPSIM, where we randomlysample 5K text-video pairs from the validation set. Our model achieves an FID score of 9.86, anFVD score of 351, and a CLIPSIM score of 0.3083. These outcomes underscore our frameworkssubstantial enhancement of video quality. Zero-shot T2V Generation on EvalCrafter. EvalCrafter provides 700 diverse prompts acrosscategories like human, animal, objects, and landscape, each with a scene, style, and camera movementdescription. For our evaluation, we generate one video for each of the 700 text prompts. As shownin , we have obtained significant improvement over the baseline ModelScopeT2V in bothvideo quality and motion quality. In terms of video quality, DEMO enhances both the Video QualityAssessment for Aesthetics (VQAA) and the IS, albeit with a slight decrease in the Video QualityAssessment for Technical Quality (VQAT). For motion quality, EvalCrafter uses three metrics:Action-Score, Flow-Score, and Motion AC-Score. The Action-Score, based on the VideoMAE V2model and MMAction2 toolbox, measures action recognition accuracy on Kinetics-400 classes,with higher scores indicating better human action recognition. Flow-Score and Motion AC-Score,derived from RAFT model optical flows, evaluate motion dynamics. The Flow-Score measuresthe general motion dynamics by calculating the average magnitude of optical flow in the video, whilethe Motion AC-Score assesses how well the motion dynamics align with the text prompt. For motionquality, our model surpasses the baseline across all metrics (Action-Score, Flow-Score, and MotionAC-Score), showcasing DEMOs superior ability to generate videos characterized by better motionquality and higher motion dynamics.",
  "ModelScopeT2V62.5090.4096.0296.19ModelScopeT2V fine-tuned63.7590.4096.3596.38DEMO68.9090.6094.6396.09": "four key metrics: Motion Dynamics, Human Action, Temporal Flickering, and Motion Smoothness.As shown in , DEMO significantly improves motion dynamics from 62.50 to 68.90. However,we observed only a slight improvement in human action recognition. This indicates that while ourmodel enhances the richness and complexity of motion, it provides limited benefit in improving theaccuracy of human action representation. Additionally, we note slight decreases in temporal flickeringand motion smoothness. This observation aligns with findings from the VBench paper, which suggestthat increased motion dynamics can conflict with temporal flickering and motion smoothness.",
  "Ablation Studies": "Impact of Lreg and Ltext-motion. As shown in , we compute the sensitivity of our motionencoder with different loss combinations. The red columns indicate the motion encoder withLtext-motion only completely loses its ability to distinguish different tokens, either motion or content,indicating a serious catastrophic forgetting where the model loses its original knowledge. The greencolumns show that fine-tuning the motion encoder with Lreg only preserves the models generalizationability but does not increase the motion sensitivity. In contrast, the purple columns demonstrate thatwhen training the motion encoder with both Lreg and Ltext-motion, the model gain increased sensitivityto tokens representing motion without losing sensitivity to tokens representing content. Impact of Lvideo-motion. To validate the effectiveness of our video-motion loss, we perform an ablationstudy on the EvalCrafter dataset. As shown in , without Lvideo-motion, the model shows a slightimprovement in motion quality compared to the baseline. This is because the motion encoder providesthe model with enriched motion information for generation. However, without explicitly constrainingthe model to mimic realistic motion, it may still focus on generating high-quality individual framesrather than coherent video sequences with rich motion dynamics. By introducing video-motion loss,the model achieves significantly higher motion quality, demonstrating the importance of this loss inguiding the model in producing videos with enhanced motion dynamics.",
  "Motion Dynamics ()62.5063.7563.5068.90Human Action ()90.4090.4090.2090.60Temporal Flickering ()96.0296.3595.4594.63Motion Smoothness ()96.1996.3896.2296.09": "Impact of additional parameters in motion encoder. To rule out the effect of additional parametersintroduce by motion encoder, we evaluated the effect of training with a CLIP text encoder onthe overall model performance. We then compared three different variations: (1) the originalModelScopeT2V, (2) a fine-tuned version of ModelScopeT2V without additional motion encoderparameters, and (3) ModelScopeT2V with the motion encoder while maintaining its original trainingloss. As shown in , we observed that the performance of the model with the additional motionencoder parameters is comparable to the fine-tuned version without these extra parameters. Thissuggests that, without specific supervision or additional constraints, the effect of the added textencoder parameters is marginal. However, the DEMO model consistently outperforms all variations,demonstrating the effectiveness of our method in improving both video quality and text-videoalignment. Efficiency Analysis. To validate the efficiency of our proposed methods, we trained the baselinemodel for the same number of iterations and compared its performance with ours. As shown inTables 1, 2, 3, 4, and 5, continuing to fine-tune the model results in only marginal improvements in A man is st and ing in a kit chen t alking and t hen a mixer and c ar t on of milk ar e shown. : Limitations. DEMO does not support creating videos containing sequential motionsspecified by text. As shown in the example, two motions,a man standing in a kitchen and talking\"and a mixer and a carton of milk are shown\", appear simultaneously.",
  "Limitations and Future Work": "Despite DEMOs efficiency in enhancing motion synthesis without relying on additional signals, itfaces significant challenges in generating different motions sequentially, as illustrated in .These challenges likely stem from the text encoders difficulty in comprehending the order of actionsand the motion generation models limited capability to generate different motions. A potentialsolution to this issue involves annotating each frame with a specific prompt and training the model onvideo clips of varying lengths rather than a fixed duration. We consider exploring this direction in ourfuture work.",
  "Broader Impacts": "Our model achieves higher visual fidelity and motion quality, which can benefit various fields such ascontent creation and visual simulation. However, our model is fine-tuned on web data, specificallyWebVid-10M . As a result, the model may not only learn how to generate videos but alsoinadvertently learn societal biases present in the web data, which may include inappropriate or NSFWcontent. Potential post-processing steps, such as applying a video classifier to filter out undesirablecontent, could help mitigate this issue.",
  "Conclusion": "In this paper, we have presented DEMO, an innovative framework crafted to advance motion synthesisin T2V generation. By separating text encoding and text conditioning into distinct content and motiondimensions, DEMO facilitates the creation of static scenes and their dynamic evolution. To encourageour model to focus on motion encoding and motion generation, we propose novel text-motion andvideo-motion supervision. Our extensive evaluations across various benchmarks have illustratedDEMOs capability to significantly improve motion synthesis, showcasing its potential within thefield. In future work, we plan to augment T2V datasets with more detailed descriptions and delve intoadvanced motion embedding techniques. By focusing on these areas, we aim to advance the frontiersof research in this dynamic and rapidly evolving domain.",
  "Acknowledgement": "ThisworkispartiallysupportedbyShenzhenFundamentalResearchProgram(No.JCYJ20200109141235597), NSFC/RGC Collaborative Research Scheme (No. CRS_PolyU501/23),HK RGC Theme-based Research Scheme (No. PolyU T43-513/23-N) and Research Grants Council ofthe Hong Kong Special Administrative Region, China (No. PolyU15205924). We also acknowledgethe support from Research Institute for Artificial Intelligence of Things, The Hong Kong PolytechnicUniversity, and Center for Computational Science and Engineering at Southern University of Scienceand Technology. Bain, M., Nagrani, A., Varol, G., Zisserman, A.:Frozen in Time:A Joint Videoand Image Encoder for End-to-End Retrieval. In: 2021 IEEE/CVF International Confer-ence on Computer Vision (ICCV). pp. 17081718. IEEE, Montreal, QC, Canada (Oct2021). Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila,T., Laine, S., Catanzaro, B., Karras, T., Liu, M.Y.: eDiff-I: Text-to-Image Diffusion Modelswith an Ensemble of Expert Denoisers (Mar 2023), [cs] Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y.,English, Z., Voleti, V., Letts, A., Jampani, V., Rombach, R., Ai, S.: Stable Video Diffusion:Scaling Latent Video Diffusion Models to Large Datasets",
  "Carreira, J., Zisserman, A.: Quo Vadis, Action Recognition? A New Model and the KineticsDataset (Feb 2018), arXiv:1705.07750 [cs]": "Chang, D., Shi, Y., Gao, Q., Fu, J., Xu, H., Song, G., Yan, Q., Yang, X., Soleymani, M.:MagicDance: Realistic Human Dance Video Generation with Motions & Facial ExpressionsTransfer (Nov 2023), arXiv:2311.12052 [cs] Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X.,Weng, C., Shan, Y.: VideoCrafter1: Open Diffusion Models for High-Quality Video Generation(Oct 2023), arXiv:2310.19512 [cs]",
  "Chen, T.S., Lin, C.H., Tseng, H.Y., Lin, T.Y., Yang, M.H.: Motion-Conditioned DiffusionModel for Controllable Video Synthesis (Apr 2023), [cs]": "Dai, X., Hou, J., Ma, C.Y., Tsai, S., Wang, J., Wang, R., Zhang, P., Vandenhende, S., Wang, X.,Dubey, A., Yu, M., Kadian, A., Radenovic, F., Mahajan, D., Li, K., Zhao, Y., Petrovic, V., Singh,M.K., Motwani, S., Wen, Y., Song, Y., Sumbaly, R., Ramanathan, V., He, Z., Vajda, P., Parikh,D.: Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack (Sep2023), arXiv:2309.15807 [cs]",
  "Esser, P., Rombach, R., Ommer, B.: Taming Transformers for High-Resolution Image Synthesis(Jun 2021), arXiv:2012.09841 [cs]": "Feng, M., Liu, J., Yu, K., Yao, Y., Hui, Z., Guo, X., Lin, X., Xue, H., Shi, C., Li, X., Li,A., Kang, X., Lei, B., Cui, M., Ren, P., Xie, X.: DreaMoving: A Human Video GenerationFramework based on Diffusion Models (Dec 2023), [cs] Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.H., Chechik, G., Cohen-Or, D.: AnImage is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion(Aug 2022), arXiv:2208.01618 [cs] Ge, S., Nah, S., Liu, G., Poon, T., Tao, A., Catanzaro, B., Jacobs, D., Huang, J.B., Liu, M.Y.,Balaji, Y.: Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models (Aug2023), arXiv:2305.10474 [cs]",
  "Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization (Jan 2017), arXiv:1412.6980 [cs]": "Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H., Akbari, H.,Alon, Y., Birodkar, V., Cheng, Y., Chiu, M.C., Dillon, J., Essa, I., Gupta, A., Hahn, M., Hauth,A., Hendon, D., Martinez, A., Minnen, D., Ross, D., Schindler, G., Sirotenko, M., Sohn, K.,Somandepalli, K., Wang, H., Yan, J., Yang, M.H., Yang, X., Seybold, B., Jiang, L.: VideoPoet:A Large Language Model for Zero-Shot Video Generation",
  "Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:Im-proved Techniques for Training GANs (Jun 2016), [cs]": "Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T.,Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L.,Kaczmarczyk, R., Jitsev, J.: LAION-5B: An open large-scale dataset for training next generationimage-text models (Oct 2022), arXiv:2210.08402 [cs] Sheynin, S., Polyak, A., Singer, U., Kirstain, Y., Zohar, A., Ashual, O., Parikh, D., Taigman,Y.: Emu Edit: Precise Image Editing via Recognition and Generation Tasks (Nov 2023), arXiv:2311.10089 [cs] Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni,O., Parikh, D., Gupta, S., Taigman, Y.: Make-A-Video: Text-to-Video Generation withoutText-Video Data (Sep 2022), arXiv:2209.14792 [cs]",
  "Wang, W., Yang, H., Tuo, Z., He, H., Zhu, J., Fu, J., Liu, J.: VideoFactory: Swap Attentionin Spatiotemporal Diffusions for Text-to-Video Generation (Jun 2023), arXiv:2305.10874 [cs]": "Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang,P., Guo, Y., Wu, T., Si, C., Jiang, Y., Chen, C., Loy, C.C., Dai, B., Lin, D., Qiao, Y., Liu, Z.:LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models (Sep 2023), arXiv:2309.15103 [cs]",
  "Wu, C., Liang, J., Ji, L., Yang, F., Fang, Y., Jiang, D., Duan, N.: N\\\"UWA: Visual SynthesisPre-training for Neural visUal World creAtion (Nov 2021), arXiv:2111.12417 [cs]": "Wu, J.Z., Ge, Y., Wang, X., Lei, W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie, X., Shou, M.Z.:Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation(Mar 2023), arXiv:2212.11565 [cs] Xu, J., Mei, T., Yao, T., Rui, Y.:MSR-VTT: A Large Video Description Datasetfor Bridging Video and Language. In:2016 IEEE Conference on Computer Vi-sion and Pattern Recognition (CVPR). pp. 52885296. IEEE, Las Vegas, NV, USA(Jun 2016). Xue, H., Hang, T., Zeng, Y., Sun, Y., Liu, B., Yang, H., Fu, J., Guo, B.: Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions (Jul 2022), arXiv:2111.10337 [cs]",
  "Training Details and Hyperparameters": "As shown in , we train DEMO using the Adam optimizer with a OneCycle scheduler .Specifically, the learning rate varies within the range of [0.00001, 0.00005], while the momentumoscillates between 0.85 and 0.99. We train our model using Deepspeed framework with stage 2 zerooptimization and cpu offloading. DEMO is trained on 4 NVIDIA Tesla A100 GPUs with a batch sizeof 24 per GPU. DEMO takes 256256 images as inputs and utilizes a VQGAN with a compressionrate of 8 to encode images into a latent space of 32 32. DEMO is trained with 1000 diffusion steps.We set the classifier-free guidance scale as 9 with the probability of 0.1 randomly dropping the textduring training. For inference, we use the DDIM sampler with 50 inference steps.",
  "Pliot Study Details": "To test the sensitivity of the motion encoder to parts of speech (POS) representing con-tent and motion information,we generated a set of prompts following the template:A [ADJ][NOUN1][VERB][ADV][ADP] the [NOUN2]. We then grouped these prompts accordingto their respective POS categories. Next, we calculated the pairwise sentence similarity within eachgroup using the [eot] token to determine sentence similarity. The average similarity within eachgroup, as well as across different groups, was reported. This setup groups different words with thesame POS under the same context, thereby eliminating potential biases introduced by the context.",
  "ModelBase ModelTraining Dataset": "MagicVideo LDM WebVid-10M + 10M from HD-VILA-100M + Interal 7MMake-A-Video -2.3B from Laion-5B + WebVid-10M + 10M from HD-VILA-100MVideo LDM LDM RDS for driving/WebVid-10M for T2VModelScopeT2V LDM 2.3B from Laion-5B + WebVid-10M Show-1 DeepFloyd4+ ModelScopeT2V WebVid-10M LaVie Stable Diffusion 1.4 Laion5B + WebVid-10M + Vimeo-25M PyoCo eDiff-I 1.2B text-image dataset + 22.5M text-video datasetVideoFactory LDM HD-VG-130M + WebVid-10M EMU VIDEO Emu 34M licensed text-video datasetSVD Stable Diffusion 2.1 LVD-F (152M) + 250K pre-captioned video clips of high visualfidelity.DEMOModelScopeT2V WebVid-10M",
  "ModelFID ()FID-CLIP ()FVD ()CLIPSIM ()Evaluation Protocol": "MagicVideo 36.50-1290-Text prompt on test set; unknownnumber.Make-A-Video -13.17-0.3049FID and CLIPSIM are evaluated on59794 videos with text prompt fromtest set.Show-1 -13.085380.3072FID and FVD are evaluated with2048 videos generated on test set.CLIPSIM is evaluate on 59794videos with prompts.Video LDM ---0.2929CLIPSIM is calculate on 2990videos with prompts from test set.LaVie ---0.2949CLIPSIM is calculate on 2990videos with prompts from test set.PYoCo 25.39-22.1410.21-9.73--The same as Make-A-Video.VideoFactory ---0.3005CLIPSIM is calculate on 2990videos with prompts from test set. ModelScopeT2V 14.895570.2941FID and FVD are evaluated with2048 videos generated on test set.CLIPSIM is evaluate on 59794videos with prompts.ModelScopeT2V fine-tuned13.805360.2932Same as ModelScopeT2VDEMO11.774220.2965Same as ModelScopeT2V",
  "We define the sensitivity of our motion encoder as one minus this similarity. The full set of differentwords within each POS category is defined as follows:": "ADJ ={\"big\", \"small\", \"tall\", \"short\", \"fat\", \"thin\", \"young\", \"old\"}NOUN1 ={\"cat\", \"dog\", \"horse\", \"child\", \"man\", \"woman\", \"bird\", \"fish\"}VERB ={\"walk\", \"run\", \"jump\", \"crawl\", \"eat\", \"swim\", \"fly\", \"climb\"}ADV ={\"quickly\", \"slowly\", \"suddenly\", \"steadily\", \"cautiously\",\"briskly\", \"gracefully\", \"clumsily\"}ADP ={\"across\", \"over\", \"through\", \"beside\", \"against\", \"under\", \"above\", \"near\"}NOUN2 ={\"river\", \"bridge\", \"mountain\", \"tree\", \"house\", \"lake\", \"field\", \"forest\"} Given these six categories with eight words each, we have a total of 86 = 262144 prompts. It isnoteworthy that we did not observe significant differences when using different templates or differentsets of words within each POS. The results were consistent across different setups, and we selectedthese prompts to try to make these prompts meaningful.",
  "ModelIS ()FVD ()Evaluation Protocol": "MagicVideo -655.00Evaluated on videos generated with class labels;unknown number.Make-A-Video 33.00367.23One template sentence per class label; 100 videosper class.Show-1 35.42394.46One template sentence per class label; 20 videos perprompt for IS; FVD on 2048 sampled videos.Video LDM 33.45550.61Class label only; 100 videos per class.LaVie -526.30Class label only; 100 videos per class.PYoCo 47.76355.19One template sentence per class label; 20 videos perprompt for IS; FVD on 2048 sampled videos.VideoFactory -410.00One template sentence per class label; 100 videosper class.EMU VIDEO 42.70606.20One template sentence per class label; 100 videosper class.SVD -242.02FVD on 13,320 videos using class labels only. ModelScopeT2V 37.49630.23100 videos per class using class labels only.ModelScopeT2V fine-tuned37.21612.53100 videos per class using class labels only.DEMO36.35547.31100 videos per class using class labels only. we detail and justify our evaluation standards. For our evaluation on MSR-VTT, we follow the basemodels approach to compute the CLIPSIM on the entire MSR-VTT dataset. For FID computation,CLIP-ViT/B 32 is used to extract the frame features. For FID and FVD, we randomly sample 2048videos, following the ModelScopeT2V papers methodology. For our evaluation on UCF-101, toeliminate bias introduced by template sentences (as done in several previous works), we directly usethe class labels to compute the IS and FVD scores.",
  "In this section, we provide extended qualitative comparison between our method and the baseline": "A bear is climbing a t r ee. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 Bir d view, f lying over a snowy f or est . A boat ac c eler at ing t o gain sp eed. : Extended qualitative comparison. Each video is generated with 16 frames. We displayframes 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos areavailable in the supplementary materials. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A woman wear ing Hanf u op ens a p ap er f an in her hand. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A r oast t ur ker y, c out er cloc kwise Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 Ap p les and or anges, cloc kwise. : Extended qualitative comparison. Each video is generated with 16 frames. We displayframes 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos areavailable in the supplementary materials. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 T hr ee maj est ic gir af f es gr aze on t he leaves of t all t r ees in Af r ic an savannah. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 S up er man shaking hand s wit h Sp id er man wit h t he st yle of wat er c olor . Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A lion is c at ching p r ey. : Extended qualitative comparison. Each video is generated with 16 frames. We displayframes 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos areavailable in the supplementary materials. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A hoc key p layer exec ut es a p er f ec t hoc key st op , sp r aying ic e in all d ir ec t ions as t hey c ome t o a sudd en halt . Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A gr een minec r af t monst er c ar r ies a gun. Mod elS c op eT 2 VDE MO LaVieVid eoCr af t er 2 A man is looking at a d ist ant mount ain in sc i- f i st yle. : Extended qualitative comparison. Each video is generated with 16 frames. We displayframes 1, 2, 4, 6, 8, 10, 12, 14, 15, and 16, arranged in two rows from left to right. Full videos areavailable in the supplementary materials.",
  "To further assess the qualitative performance of our proposed method, we conducted a user studycomparing our approach (DEMO) with several state-of-the-art video generation models. We randomly": "selected 50 prompts from EvalCrafter , ensuring diversity across scenes, styles, and objects.For each comparison, 15 annotators evaluated the generated videos in terms of three main criteria:text-video alignment, visual quality, and motion quality. The study compared our method withModelScopeT2V, LaVie, and VideoCrafter2. The participants were asked to select their preferred video between the two models for each prompt.The comparative results are summarized in . Specifically, DEMO consistently outperformedModelScopeT2V, LaVie, and VideoCrafter2, particularly in terms of motion quality, where it achieveda preference rate of 74% over ModelScopeT2V. Additionally, DEMO was favored in text-videoalignment and visual quality by 62% and 66%, respectively. However, when compared to LaVieand VideoCrafter2, DEMO showed a lower performance in visual quality, which can be attributed todifferences in training datasets. LaVie and VideoCrafter2 use higher-quality video and image datasets,such as Vimeo-25M and JDB , respectively, while DEMO and ModelScopeT2V are trainedon the WebVid10M dataset, which is lower in visual quality. Furthermore, we conducted an additional user study to evaluate the effectiveness of our proposedvideo-motion supervision term, Lvideo-motion. The results indicated that our method with motionsupervision outperformed the version without motion supervision, achieving win rates of 58%,56%, and 72% in text-video alignment, visual quality, and motion quality, respectively. Thesefindings highlight the significant improvements brought by the video-motion supervision in generatingsmoother and more realistic motion dynamics."
}