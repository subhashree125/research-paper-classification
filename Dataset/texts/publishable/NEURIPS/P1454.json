{
  "Abstract": "Self-supervised heterogeneous graph learning (SHGL) has shown promising poten-tial in diverse scenarios. However, while existing SHGL methods share a similaressential with clustering approaches, they encounter two significant limitations: (i)noise in graph structures is often introduced during the message-passing processto weaken node representations, and (ii) cluster-level information may be inade-quately captured and leveraged, diminishing the performance in downstream tasks.In this paper, we address these limitations by theoretically revisiting SHGL fromthe spectral clustering perspective and introducing a novel framework enhanced byrank and dual consistency constraints. Specifically, our framework incorporatesa rank-constrained spectral clustering method that refines the affinity matrix toexclude noise effectively. Additionally, we integrate node-level and cluster-levelconsistency constraints that concurrently capture invariant and clustering informa-tion to facilitate learning in downstream tasks. We theoretically demonstrate thatthe learned representations are divided into distinct partitions based on the numberof classes and exhibit enhanced generalization ability across tasks. Experimentalresults affirm the superiority of our method, showcasing remarkable improvementsin several downstream tasks compared to existing methods.",
  "Introduction": "Self-supervised heterogeneous graph learning (SHGL) aims to effectively process diverse types ofnodes and edges in the heterogeneous graph, producing low-dimensional representations without theneed for human annotations . Thanks to its remarkable capabilities, SHGL has attractedsignificant interest and has been utilized in a broad array of applications, including recommendationsystems , social network analysis , and molecule design . Existing SHGL methods can be broadly classified into two groups, i.e., meta-path-based methods andadaptive-graph-based methods. Meta-path-based methods typically utilize pre-defined meta-paths toexplore relationships among nodes that may share the same label in the heterogeneous graph .However, building meta-paths requires extensive prior knowledge and incurs additional computationcosts . To address these drawbacks, adaptive-graph-based methods dynamically assign significantweights to node pairs likely to share the same label, using the adaptive graph structure rather thantraditional meta-paths . Both groups of SHGL methods facilitate message-passing among nodeswithin the same class, either through meta-path-based graphs or adaptive graph structures. As aresult, this process minimizes intra-class differences and promotes a clustered pattern in the learnedrepresentations, aligning these methods closely with conventional clustering techniques.",
  "nc": ": The flowchart of SCHOOL, which first employs the Multi-Layer Perception g to derivesemantic representations H, followed by obtaining orthogonal cluster assignment matrix Y andorthogonal H. Subsequently, SCHOOL filters noisy connections by deriving the rank-constrainedaffinity matrix S, which is further used to multiply with H and then obtain node representations Z.Meanwhile, SCHOOL employs a heterogeneous encoder f to aggregate information across nodetypes, yielding heterogeneous representations Z. Finally, SCHOOL incorporates spectral loss Lsp tooptimize Y to fit eigenvectors of the Laplacian matrix of S. Moreover, SCHOOL designs node-level(i.e., Lnc) and cluster-level (i.e., Lcc) consistency constraints on projected representations (i.e., Qand Q) and cluster representations Q to capture the invariant and clustering information, respectively. Despite the effectiveness of previous SHGL methods, they encounter two limitations. First, previousmethods conduct message-passing relying on meta-path-based graphs and adaptive graph structures,which inevitably include noise, i.e., connections among nodes from different classes . Conse-quently, such noise compromises the identifiability of node representations after the message-passingprocess. Second, while previous methods exhibit clustering characteristics, they typically emphasizethe node-level consistency only, neglecting to capture and leverage the cluster-level informationeffectively . This may not fully exploit the potential benefits of clustering for representationlearning, thereby diminishing the performance of downstream tasks. Based on the above analysis, it is feasible to analyze previous SHGL methods from a clusteringperspective thanks to their close connection to clustering techniques and further optimize the graphstructures to mitigate noisy connections as well as harness the cluster-level information to enhanceprevious SHGL. To achieve this, there are three key challenges, i.e., (i) How to formally understandprevious SHGL methods from the clustering perspective? (ii) With insights from the clusteringanalysis, how to learn an adaptive graph structure that effectively captures intra-class connectionswhile filtering out inter-class noise? (iii) How to enable the effective incorporation of cluster-levelinformation in the heterogeneous graph to boost the performance of downstream tasks? In this paper, we address the outlined challenges by first theoretically revisiting previous SHGLmethods from a clustering perspective and then introducing a novel framework, termed SpectralClustering-inspired HeterOgeneOus graph Learning (SCHOOL for short), that incorporates rank-constrained spectral clustering and dual consistency constraints, as depicted in . Specifically,we start by proving that existing SHGL can be reformulated as spectral clustering with an additionalregularization term under the assumption of orthogonality, thus addressing challenge (i) and laying thefoundational theory for our approach. Next, to tackle challenge (ii), we propose an efficient spectralclustering technique that includes a rank constraint on the affinity matrix, aiming to effectivelymitigate noisy connections among different classes. Furthermore, to resolve challenge (iii), wedesign dual consistency constraints at both node and cluster levels to capture invariant and clusteringinformation, respectively, which reduces the intra-cluster differences and enhances the performanceof downstream tasks. Finally, theoretical analysis indicates that the learned representations aredivided into distinct partitions corresponding to the number of classes, and are demonstrated superiorgeneralization ability compared to those derived from previous SHGL methods.",
  "To the best of our knowledge, we make the first attempt to theoretically revisit previousSHGL methods from the spectral clustering perspective in a unified manner": "We adaptively learn a rank-constrained affinity matrix to mitigate noisy connections inherentin previous SHGL methods. Moreover, we introduce dual consistency constraints to captureboth invariant and clustering information to enhance the effectiveness of our method. We theoretically demonstrate that the proposed method divides the learned representationsinto distinct partitions based on the number of classes, instead of dimensions in previousSHGL methods. Furthermore, the representations obtained by this method exhibit enhancedgeneralization ability compared to those derived from previous SHGL methods. We experimentally manifest the effectiveness of the proposed method across a variety ofdownstream tasks, using both heterogeneous and homogeneous graph datasets, compared tonumerous state-of-the-art methods.",
  "Method": "Notations. Let G = (V, E, X, T , R) represent a heterogeneous graph, where V and E indicate setof nodes and set of edges, respectively. X = {xi}ni=1 denotes the matrix of node features, where nindicates the number of nodes. Moreover, T and R indicate set of node types and set of edge types,respectively. Given the heterogeneous graph G, most existing SHGL methods utilize meta-paths oradaptive graph structures to explore connections among nodes within the same class, thus exhibitingthe characteristics of clustering and obtaining discriminative representations. To gain a deeper insightof previous SHGL methods, we first propose to revisit them from a clustering perspective as follows.",
  "Revisiting Previous SHGL Methods from Spectral Clustering": "As mentioned above, previous SHGL methods tend to conduct clustering implicitly, relying onmeta-path-based graphs or adaptive graph structures. For example, given an academic heterogeneousgraph with several node types (i.e., paper, author, and subject), for the meta-path-based methods, iftwo papers belong to the same class, there may exist a meta-path paper-subject-paper to connectthem (i.e., two papers are grouped into the same subject). Similarly, for the adaptive-graph-basedmethods, when two papers belong to the same class, the adaptive graph structures likely assign largeweights to connect them. Therefore, representations of nodes within the same class will be close toeach other after the message-passing process, thus implicitly presenting a clustered pattern.",
  "where R() indicates the regularization term, L indicates the Laplacian matrix of the meta-path-basedgraph or the adaptive graph structure": "Proof. First, we prove the connection between previous meta-path-based SHGL methods and spectralclustering. To do this, take a heterogeneous graph with two meta-paths as an example, we letG = {G(1) G(2)} indicates the union of all meta-path-based graph views. Moreover, we denote therepresentations of previous methods before the message-passing as H (generally obtained by linearmapping from original node features). In addition, we denote the node representations of differentgraph views after the message-passing as Z(r), respectively, where r = 1, 2, i.e.,",
  "where d indicates the dimension of representations H": "Theorem 2.3 further indicates that previous SHGL methods divide the learned representations intod partitions, where d is generally much larger than the number of classes. Therefore, Theorem 2.3connects the traditional graph-cut algorithm with existing SHGL methods, which requires customanalysis. As a result, we theoretically revisit previous SHGL methods from spectral clustering as wellas graph-cut perspectives and build the connections between them, thus solving the challenge (i).",
  "Rank-Constrained Spectral Clustering": "Based on the connections between previous SHGL methods and the spectral clustering as well as thegraph-cut algorithm, we have the observations as follows. First, according to Theorem 2.2, previousSHGL methods conduct spectral clustering based on the Laplacian matrix of meta-path-based graphor adaptive graph structure, which may not guarantee optimality and could potentially contain noisyconnections, thus affecting the spectral clustering. Second, according to Theorem 2.3, previousSHGL methods conduct the graph-cut to divide the learned representations into d partitions, whichare generally not equal to the number of classes c. As a result, optimizing previous SHGL methodsbecomes a hard or even error problem, and the learned representations can not be clustered well.Therefore, it is intuitive to mitigate noise in the adaptive graph structure as well as divide the learnedrepresentations into exactly c partitions to improve existing SHGL methods. Specifically, in this paper, we propose to learn an adaptive affinity matrix with the rank constraintto mitigate noisy connections as much as possible. To do this, we first employ the Multi-LayerPerceptron (MLP) as the encoder g Rfd1 to obtain the semantic representations H by:",
  "H = (g(X)),(4)": "where f and d1 are the dimensions of node features and representations, respectively, and is theactivation function. After that, we propose to learn an adaptive affinity matrix S Rnn basedon the semantic representations. Intuitively, in a uncorrelated representations subspace, a smallerdistance hi hj22 between semantic representations should be assigned a larger probability sij.Therefore, it is a natural approach to learn the affinity matrix S by:",
  "minSni,j=1(hi hj22sij + s2ij) s.t.,i, sTi 1 = 1, 0 si 1,(5)": "where is a non-negative parameter. In Eq. (5), the first term encourages the affinity matrix to assignlarge weights to node pairs with small distances. Moreover, the second term avoids the trivial solutionthat only the nearest node can be the neighbor of vi with probability 1. However, similar to previousSHGL methods, usually the affinity matrix learned by Eq. (5) can not reach the ideal case (i.e., havingno noisy connections among different classes and containing exactly c connected components). Asa result, the noisy connections in the affinity matrix may induce a negative interference during themessage-passing process. To solve this issue, we first introduce the following lemma in .Lemma 2.4. The multiplicity c of the eigenvalue 0 of the Laplacian matrix LS is equal to the numberof connected components in the affinity matrix S.",
  "minS,Fni,j=1(hi hj22sij + s2ij + fi fj22 sij)s.t., i, sTi 1 = 1, 0 si 1, FT F = I,(8)": "where is a non-negative parameter. Eq. (8) can be solved by applying the alternating optimizationapproach. Specifically, when S is fixed, Eq. (8) becomes minFT F=Ini,j=1 fi fj22 sij =2 minFT F=I Tr(FT LSF), whose optimal solution F is the eigenvectors of LS corresponding to thec smallest eigenvalues. When F is fixed, denote dij = hi hj22 + fi fj22, Eq. (8) can bewritten in the vector form, i.e.,",
  "2dij + )+,(10)": "where is a non-negative parameter, and ()+ indicates max{, 0}. In practice, a sparse affinity matrixusually obtains better performance and reduces computation costs. Therefore, we only calculate sijbetween node vi and its k nearest neighbors. Then parameters and can be further determined,details listed in Appendix C.6. However, when fixing S and optimizing F in Eq. (8), computation costs of obtaining eigenvectorsF remain prohibitive due to the cubic time complexity of the eigendecomposition. To address thisissue, we propose to replace the eigendecomposition with a projection head and orthogonal layer. Specifically, we first employ the projection head p Rd1c to map semantic representationsto the cluster assignment space P Rnc, i.e.,",
  "Y = nPR1,(12)": "where R is an upper triangular matrix obtained from the QR decomposition (i.e., P = ER andET E = I) on the full rank P. Similarly, we can also implement the orthogonal layer to achieve theuncorrelation (i.e., HT H = I) on the representations subspace. As a result, the projection head and the orthogonal layer act as a linear transformation to achieve theorthogonality constraint on Y. To replace the eigendecomposition, the cluster assignment matrix Yshould further fit the eigenvectors F in Eq. (8). To do this, we design a spectral loss Lsp to optimizethe parameters in p to simulate the spectral clustering of the third term in Eq. (8), i.e.,",
  "where is a non-negative parameter, H(Y) = ci=1P(yi)logP(yi) is the entropy of clusterassignment probabilities P(yi) = 1": "nnj=1yij, and yij indicates the i-th column and j-th row of y.According to Eq. (7), the first term in Eq. (13) simulates the spectral clustering to enforce the learnedcluster assignment matrix Y to approximate eigenvectors F, and the second term is a widely usedregularization term to avoid the trivial solution that most nodes are assigned to the same cluster.Therefore, when S is fixed, we optimizing Y to approach the optimal F by achieving orthogonality with Eq. (12), and fitting eigenvectors F with Eq. (13). As a result, this reduces the cubic timecomplexity of eigendecomposition to O(nd2 + nc2 + nkd + nkc + c3 + d3), where d2, c2 < n, thusis linear to the sample size, details are shown in Appendix B.2. Finally, the proposed method stilloptimizes the affinity matrix S and eigenvectors F in Eq. (8) in an alternating approach, i.e., fix Fand then obtain the closed-form solution of S, and fix S and then optimize parameters (i.e., g andp) to update Y to approach the optimal F. Therefore, the proposed method obtains the affinity matrix with exactly c connected componentsto mitigate noisy connections in an effective and efficient way. Then we can obtain the noderepresentations Z = SH, which is expected to conduct message-passing among nodes within thesame class. Moreover, we can bridge the connections between the proposed method and the spectralclustering as well as the graph-cut algorithm as follows, whose proof can be found in Appendix C.3. Theorem 2.5. Optimizing the spectral loss Lsp leads to performing the spectral clustering based onthe affinity matrix S with c connected components and conducting RatioCut (V1, . . . , Vc) algorithmto divide the learned representations into c partitions, i.e.,",
  "min Lsp min Tr(YT LSY) min RatioCut(V1, . . . , Vc).(14)": "Theorem 2.5 indicates that the proposed method conducts the spectral clustering as previous SHGLmethods, but is performed on an affinity matrix with exactly c connected components (verified in.2.3), thus mitigating noisy connections from different classes and solving the challenge (ii).Moreover, the proposed method divides the learned representations into c partitions, which is a betteroptimization goal than previous SHGL methods to obtain discriminative representations.",
  "Dual Consistency Constraints": "The message-passing among nodes within the same class reduces intra-class differences and enhancesnode representations Z. Meanwhile, the message-passing among nodes from different types alsocontributes to obtaining task-related contents and benefits downstream tasks . To do this, wepropose to aggregate the information of nodes from different types in the heterogeneous graph with aheterogeneous encoder f Rfd1. Specifically, for the node vi, we concatenate the information of itself and its relevant one-hopneighbors (i.e., nodes of other types) based on edge types in R, and then derive the heterogeneousrepresentations Z by:",
  "vjNi,rf(xj)),(15)": "where is the activation function, |R| indicates the number of edge types, Ni,r indicates the set ofone-hop neighbors of node vi based on the edge type r R, f() indicates the linear transformation,and || indicates the concatenation operation. Therefore, the heterogeneous representations Zaggregate the information of nodes from different types to introduce more task-related contents. Given node representations Z and heterogeneous representations Z, most previous SHGL methodsutilize the node-level consistency constraint (e.g., Info-NCE loss ) to capture the invariantinformation between them and enhance the effectiveness . In addition, according to Theorem2.2, previous SHGL methods actually perform spectral clustering to learn node representations.However, previous SHGL methods fail to utilize the cluster-level information outputted by thespectral clustering, thus weakening the downstream task performance. To solve this issue, we designdual consistency constraints to capture the invariant information as well as the clustering informationbetween Z and Z. Specifically, we first employ a projection head q Rd1d2 to map both Z and Z into the samelatent space, i.e., Q = q(Z) and Q = q(Z), where d2 is the projected dimension. Then wefollow previous works to design a node-level consistency constraint to capture the invariantinformation between Q and Q, i.e.,",
  "Lnc = Q Q2F + log di,j=1 ecij,(16)": "where C = QT Q + QT Q, and is a non-negative parameter. Similar to previous works, the firstterm in Eq. (16) enforces representations in Q agree with the corresponding representations in Q,thus capturing the invariant information between them. The second term enables different dimensionsof Q and Q to uniformly distribute over the latent space to avoid collapse. In addition to the node-level consistency constraint, we further design the cluster-level consistencyconstraint to capture the clustering information from the cluster assignment matrix Y. To dothis, we first obtain the cluster indicator matrix Y based on the cluster assignment matrix Y,i.e., Y = argmax(Y). After that, we conduct average pooling on node representations that possessthe same cluster indicator to obtain the j-th cluster representation qj, i.e.,",
  "Lcc = ni=1 qi qyi22,(18)": "where qyi indicates the cluster representation whose label equals to yi. Eq. (18) enables the projectedrepresentation qi and the cluster representation qyi to align each other. As a result, representationscapture the clustering information based on cluster indicators and reduce intra-cluster differences toimprove the performance of downstream tasks, thus solving challenge (iii).",
  "J = Lsp + Lnc + Lcc,(19)": "where and are non-negative parameters. Finally, we concatenate node representations Z withheterogeneous representations Z to obtain representations for downstream tasks. Actually, for thelearned representations, we have the following Theorem, whose proof can be found in Appendix C.4. Theorem 2.6. The proposed method with dual consistency constraints achieves a lower boundary ofthe model complexity C and a higher generalization ability boundary G than previous SHGL withthe node-level consistency constraint only, i.e.,",
  "Experiments": "In this section, we conduct experiments on both heterogeneous and homogeneous graph datasetsto evaluate the proposed method in terms of different downstream tasks (i.e., node classificationand node clustering), compared to both heterogeneous and homogeneous graph methods. Detailedsettings are shown in Appendix D, and additional results are shown in Appendix E.",
  "Comparison Methods": "The comparison methods include eleven heterogeneous graph methods and twelve homogeneousgraph methods. The former includes two semi-supervised methods (i.e., HAN and HGT ),one traditional unsupervised method (i.e., Mp2vec ), and eight self-supervised methods (i.e., DMGI, DMGIattn , HDMI , HeCo , HGCML , CPIM , HGMAE , and HERO). The latter includes two semi-supervised methods (i.e., GCN and GAT ), one traditionalunsupervised method (i.e., DeepWalk ), and nine self-supervised methods, (i.e., DGI , GMI, MVGRL , GRACE , GCA , G-BT , COSTA , DSSL , and LRD ). For a fair comparison, we follow to select meta-paths for previous meta-path-basedSHGL methods. Moreover, we follow to implement homogeneous graph methods on heteroge-neous graph datasets by separately learning the representations of each meta-path-based graph andfurther concatenating them for downstream tasks. In addition, we replace the heterogeneous encoderf with GCN to implement the proposed method on homogeneous graph datasets because there isonly one node type in the homogeneous graph. Moreover, we follow previous works to generatetwo different views for the homogeneous graph by removing edges and masking features. The codeof the proposed method is released at",
  "Effectiveness on Heterogeneous and Homogeneous Graph": "We first evaluate the effectiveness of the proposed method on the heterogeneous graph datasets andreport the results of node classification and node clustering in and Appendix E, respectively.Obviously, the proposed method obtains better performance on both node classification and nodeclustering tasks than comparison methods. Specifically, first, for the node classification task, the proposed method consistently outperforms thecomparison methods by large margins. For example, the proposed method on average, improves by1.1%, compared to the best SHGL method (i.e., HERO), on four heterogeneous graph datasets. Thereason can be attributed to the fact that the proposed method adaptively learns a rank-constrainedaffinity matrix to mitigate noisy connections among different classes, thus reducing intra-classdifferences. Second, for the node clustering task, the proposed method also obtains promisingimprovements. For example, the proposed method on average, improves by 3.1%, compared to thebest SHGL method (i.e., HGMAE), on four heterogeneous graph datasets. This demonstrates thesuperiority of the proposed method, which simulates the spectral clustering with the spectral loss andconducts the cluster-level consistency constraint to further utilize the clustering information. As aresult, the effectiveness of the proposed method is verified on different downstream tasks.",
  "Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1 Macro-F1 Micro-F1": "85.90.385.80.691.80.691.30.591.00.292.10.466.80.775.50.988.80.688.60.792.50.792.10.491.70.492.70.572.40.580.30.787.60.387.50.592.30.892.00.690.70.691.70.667.30.674.70.586.90.786.70.592.10.391.50.493.40.894.20.675.20.483.90.789.00.588.90.492.40.592.00.393.50.694.20.476.20.585.20.888.90.788.80.692.60.692.30.591.90.792.80.877.10.886.00.692.70.692.60.593.00.792.80.494.00.394.70.477.50.986.80.7 We further evaluate the effectiveness of the proposed method on homogeneous graph datasets andreport the results of node classification in Appendix E. We can observe that the proposed method alsoachieves competitive results on the homogeneous graph datasets compared to other homogeneousgraph methods. For example, the proposed method outperforms the best self-supervised method(i.e., LRD), on almost all homogeneous graph datasets. This indicates that the proposed methodis also available to learn the noise-free affinity matrix for homogeneous graphs as well as captureinvariant and clustering information to benefit downstream tasks. Therefore, the effectiveness of theproposed method is verified on both heterogeneous and homogeneous graph datasets.",
  "Ablation Study": "The proposed method investigates the objective function J to learn the rank-constrained affinitymatrix, as well as capture invariant and clustering information. To verify the effectiveness of eachcomponent of J (i.e., Lsp, Lnc, and Lcc), we investigate the performance of all variants on the nodeclassification task and report the results in . From , we have the observations as follows. First, the proposed method with the completeobjective function obtains the best performance. For example, the proposed method on averageimproves by 1.8%, compared to the best variant (i.e., without Lnc), indicating that all componentsin the objective function are necessary for the proposed method. This is consistent with our claims,i.e., it is essential to optimize the adaptive graph structure to mitigate noisy connections as well asutilize the cluster-level information to benefit downstream tasks. Second, the variant without Lspachieves inferior results to the other two variants (i.e., without Lnc and without Lcc, respectively).This can be attributed to the fact that the spectral loss Lsp enforces the cluster assignment matrix tofit the eigenvectors, which is necessary for the closed-form solution of the affinity matrix.",
  "Visualization": "To verify the effectiveness of the learned affinity matrix and the representations for downstream tasks,we visualize the affinity matrix in the heatmap and visualize the representations with t-SNE onDBLP and Aminer datasets and report the results in . Specifically, we randomly sample 50 nodes in each class and then visualize elements of the affinitymatrix S among sampled nodes with the heatmap, where rows and columns are reordered by nodelabels. In the correlation map, the darker a pixel, the larger the value of the element of S. In Figures2(a) and 2(c), the heatmaps exhibit that there are nearly c (i.e., the number of classes) componentsin the affinity matrix, and almost all elements with large values fall in the block diagonal structure.This indicates that the affinity matrix indeed contains c connected components to mitigate noisyconnections among different classes. Moreover, the t-SNE visualization in Figures 2(b) and 2(d)further indicate that the learned representations can be well divided into c partitions. This is consistentwith the observation in Theorem 2.5 and verifies the effectiveness of the learned representations.",
  ": Visualization of the affinity matrix S and t-SNE on DBLP and Aminer datasets": "previous SHGL methods is equivalent to performing spectral clustering with additional regularizationunder the orthogonalization assumption. Then we proposed an efficient spectral clustering methodwith the rank constraint to learn an adaptive affinity matrix and mitigate noisy connections inprevious methods. Moreover, we designed node-level and cluster-level consistency constraints tocapture invariant and clustering information, thus benefiting the performance of downstream tasks.Theoretical analysis indicates that the learned representations are divided into distinct partitions basedon the number of classes, and are expected to achieve better generalization ability than representationsof previous SHGL methods. Comprehensive experiments verify the effectiveness of the proposedmethod on both homogeneous and heterogeneous graph datasets on different downstream tasks. Potential limitations and broader impact. Our potential limitation is that this work is designedbased on node features. However, in heterogeneous graphs, instances arise where nodes are devoid offeatures. While one-hot vectors or structural embeddings can be designated as node features to tacklethis problem, we recognize the necessity of devising dedicated techniques tailored for heterogeneousgraphs with missing node features. In addition, the proposed method can also be used to deal with thehomophily problem, which aims to explore the connections within the same class. We consider theseaspects as potential directions for future research. Despite the great development of SHGL, sometheoretical foundations are still lacking. Our work theoretically connects existing SHGL methods andspectral clustering and may open a new path to understanding and designing SHGL. Besides that, wedo not foresee any direct negative impacts on the society. This project is supported by the National Key Research and Development Program of China underGrant No. 2022YFA1004100, the Natural Science Foundation of Guangdong Province of Chinaunder Grant No. 2024A1515011381, and the National Research Foundation, Singapore, under its AISingapore Programme (AISG Award No: AISG2-RP-2021-023).",
  "Yudi Huang, Yujie Mo, Yujing Liu, Ci Nie, Guoqiu Wen, and Xiaofeng Zhu. Multiplex graphrepresentation learning via bi-level optimization. In IJCAI, 2024": "Yiding Jiang, Parth Natekar, Manik Sharma, Sumukh K Aithal, Dhruva Kashyap, NatarajanSubramanyam, Carlos Lassance, Daniel M Roy, Gintare Karolina Dziugaite, Suriya Gunasekar,et al. Methods and analysis of the first competition in predicting generalization of deep learning.In NeurIPS, pages 170190, 2021. Di Jin, Luzhi Wang, Yizhen Zheng, Guojie Song, Fei Jiang, Xiang Li, Wei Lin, and Shirui Pan.Dual intent enhanced graph neural network for session-based new item recommendation. InWWW, pages 684693, 2023.",
  "Marc T Law, Raquel Urtasun, and Richard S Zemel. Deep spectral clustering learning. In ICML,pages 19851994, 2017": "Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou,Xinwang Liu, and Fuchun Sun. A survey of knowledge graph reasoning on graph types: Static,dynamic, and multimodal. arXiv preprint arXiv:2212.05767, 2022. Meng Liu, Ke Liang, Yawei Zhao, Wenxuan Tu, Sihang Zhou, Xinbiao Gan, Xinwang Liu, andKunlun He. Self-supervised temporal graph learning with temporal and structural intensityalignment. IEEE Transactions on Neural Networks and Learning Systems, 2024.",
  "A.1Self-Supervised Heterogeneous Graph Learning": "In recent years, self-supervised heterogeneous graph learning (SHGL) has emerged as a helpful tech-nique to deal with the heterogeneous graph that consists of different types of entities without needinglabeled data . As a result, SHGL captures meaningful representations ofnodes and edges, enabling better performance in downstream tasks like node classification and nodeclustering. Due to its powerful capability, SHGL has been applied to various real applications, suchas social network analysis , and recommendation systems . Existing SHGL methods can be broadly classified into two groups, i.e., meta-path-based methods andadaptive-graph-based methods. In meta-path-based methods, several graphs are usually constructedbased on different pre-defined meta-paths to examine diverse relationships among nodes that sharesimilar labels . For example, STENCIL and HDMI construct meth-path-basedgraphs and then conduct node-level consistency constraints (e.g., contrastive loss) between noderepresentations in different graphs. In addition, HGCML and CPIM propose to maximizethe mutual information between node representations from different meta-path-based graphs. How-ever, pre-defined meta-paths in these methods generally require expert knowledge and prohibitivecomputation costs . Therefore, adaptive-graph-based methods are proposed to learn the adaptivegraph structures to capture the relationships among nodes that possess the same label, instead ofusing meta-paths. For example, recently, HERO made the first attempt to learn an adaptiveself-expressive matrix to capture the homophily in the heterogeneous graph, thus avoiding meta-paths. Although existing SHGL methods (especially the adaptive-graph-based methods) have achievedimpressive performance in several tasks, the learned graph structure cannot be guaranteed optimal.As a result, the learned graph structure may contain noisy connections from different classes to affectthe message-passing process and weaken the discriminative information in node representations.",
  "A.2Spectral Clustering": "Spectral clustering partitions data points into clusters based on a similarity matrix derived from thedata . Owing to its proficiency in identifying clusters with complex shapes and handlingnon-linearly separable data, spectral clustering is widely used in many scenarios . The spectral clustering methods can be broadly classified into two groups, i.e., traditional spectralclustering and deep spectral clustering. Traditional spectral clustering methods aim to group datapoints that are similar to each other while being dissimilar to points in other clusters by eigendecom-position . For example, CAN proposes to learn the data similarity matrix and clusteringstructure simultaneously with the eigendecomposition. SWCAN further assigns weights fordifferent features to learn the similarity graph and partition samples into clusters simultaneously.Despite its effectiveness, traditional spectral clustering generally requires expensive computationcosts, especially for large datasets. To alleviate this issue, deep spectral clustering methods havebeen proposed in recent years. For example, DSC employs an encoder and two decoders totrain the network, thus obtaining discriminative representations for clustering and implementing thecluster assignment via the neural network. DSCL introduces a novel metric learning frameworkthat leverages spectral clustering principles, thus reducing complexity to linear levels. Spectral-Net proposes to learn a mapping function via the orthogonalization network to address theout-of-sample-extension and scalability problems. The above methods conduct spectral clustering explicitly. Surprisingly, recent research shows thatsome popular self-supervised methods also implicitly conduct spectral clustering . For example, demonstrates contrastive learning performs spectral clustering on the population augmentationgraph by replacing the standard InfoNCE with its proposed spectral contrastive loss. demonstrates that contrastive learning with the standard InfoNCE loss is equivalent to spectralclustering on the similarity graph. Although these methods make efforts to connect previous self-supervised methods with spectral clustering, they cannot be easily transferred to SHGL. First, thesemethods are almost based on the augmentation graph, which assumes that different augmentations of the same sample connect each other and thus form a graph. In contrast, in SHGL, there is noaugmentation, and the graph is constructed by connecting different samples. Second, compared tothe above methods, SHGL incorporates the message-passing process, which makes it more complex.Therefore, connecting SHGL methods with the spectral clustering remains challenging.",
  "2di + )+,(21)": "where dij = hi hj22 + fi fj22, where H Rnd and F Rnc are semantic representa-tions and eigenvector matrix, d and c indicate number of dimensions and classes, and n indicatesthe number of nodes. To reduce the computation costs, the proposed method proposes to onlycalculate sij between node vi and its k nearest neighbors. Therefore, the time complexity of Eq.(21) is O(nk). Moreover, the proposed method proposes to replace the eigendecomposition with aprojection head and orthogonalization layer to further reduce the time complexity. Specifically, thetime complexity of the orthogonal process for H and Y with the QR decomposition is O(nd2) andO(nc2), respectively. The time complexity of the inversion process in Eq. (12) for H and F is O(d3)and O(c3). Moreover, the time complexity of the spectral loss is O(nkc) and the time complexityof Z = SH is O(nkd). In addition, the time complexity of node-level and cluster-level consistencyconstraints are O(nd2) and O(n), respectively. Therefore, the overall complexity of the proposedmethod is O(nd2 + nc2 + nkd + nkc + d3 + c3) in each epoch, where d2, c2 < n, thus is scaledlinearly with the sample size.",
  "where N(vi)(r) indicates the one-hop neighbors of node vi in the r-th meta-path-based graph": "Based on the node representations Z(r) of each graph, previous meta-path-based SHGL methodsgenerally propose to extract the invariant information among node representations from differentmeta-path-based graphs. Here, we take the Mean Squared Error (MSE) loss as a simple exampleto extract the invariance and then conduct an analysis of previous meta-path-based SHGL methods.Therefore, the objective function of previous meta-path-based SHGL methods can be formulated as:",
  "(29)": "Based on the assumption that HT H = I, we can conclude that previous meta-path-based SHGLmethods, which extract the invariance among different graphs, equals the known spectral clusteringwith additional regularization. Note that the MSE loss in the above example can be replaced by othercontrastive or non-contrastive loss (e.g., InfoNCE ), and we can easily obtain similar results. After that, we further prove the connection between recent adaptive-graph-based SHGL methods and the spectral clustering. Denote the self-expressive matrix in as S, and denote therepresentations after projection by linear transformation as H. Moreover, denote DS as the degreematrix of S and denote LS = DS S as the graph Laplacian. Given that the self-expressive matrixis symmetrical and non-negative, the objective function of previous adaptive-graph-based SHGLmethods can be formulated as:",
  "where W(Va, Vb) :=": "iVa,jVb wij indicates the weight between different subsets, and V is thecomplement of V .Theorem C.3. (Restating Theorem 2.3 in the main text). Under the same assumption in Theorem2.2, optimizing previous meta-path-based and adaptive-graph-based SHGL methods is approximateto performing the RatioCut (V1, . . . , Vd) algorithm that divides the learned representations into dpartitions {V1, . . . , Vd}, i.e.,",
  "C.3Proof of Theorem 2.5": "Theorem C.4. (Restating Theorem 2.5 in the main text). Optimizing the spectral loss Lsp leads toperforming the spectral clustering based on the affinity matrix S with c connected components andconducting RatioCut (V1, . . . , Vc) algorithm to divide the learned representations into c partitions,i.e.,min Lsp min Tr(YT LSY) min RatioCut(V1, . . . , Vc).(44)",
  "Si =1niniOi ip1/2for i = 1 kMi,j = i j2for i, j = 1 k,(49)": "i and j are indices of two different classes, O()iis the output representation of the -th samplebelonging to class i for the given model, i is the cluster centroid of the representations of class i, Siis a measure of scatter within representations of class i, and Mi,j is a measure of separation betweenrepresentations of classes i and j. Moreover, we further follow previous works to define the generalization bound G of a modelbased on the model complexity, i.e.,Definition C.6. (Generalization Bound) For any , with probability at least 1 , thegeneralization bound G of a model follows the inequality, i.e.,",
  "where (xi, yi) is a pair of labeled data, f is the model, l is the loss function, n is the number oflabeled data, C is the model complexity measure": "Based on the Definitions above, we can derive the Theorem as follows.Theorem C.7. (Restating Theorem 2.6 in the main text). The proposed method with dual consistencyconstraints achieves a lower boundary of the model complexity C and a higher generalization abilityboundary G than previous SHGL with the node-level consistency constraint only, i.e.,",
  "b+c. As a result, f(a) 0 always holds for 0 a 1. Thus, we prove the inequality in Eq.(56)": "Given the above inequality in Eq. (56), for Eq. (55), we let 20 = E[W(X(i)0 X0)2], 21 =E[W(X(i)1 X1)2], 2j = E[W(X(i)j Xj)2], and 2k = E[W(X(i)k Xk)2]. Moreover,we replace a, b, and c in Eq. (56) with P0, 20, and 2j , respectively. Then Eq. (55) can be rewrittenas:",
  "M0,1.(62)": "Note that the cluster-level consistency constraint minimizes the first term in the S20 and S21 (i.e., 20and 21). Moreover, we can observe that Eq. (60) and Eq. (61) are the increasing function withrespect to 0 and 1. Therefore, minimizing the cluster-level consistency constraint is equivalent tominimizing the lower bound of the model complexity. Therefore, the lower bound of complexitymeasure C of the model with the dual consistent constraints is less than the model without it,i.e., inf(CSCHOOL) < inf(CSHGL). As a result, according to , we can conclude that therepresentations learned by the dual consistent constraints have a higher bound of generalization abilitythan previous methods with instance-level constraint only, i.e., sup(GSCHOOL) > sup(GSHGL)thus we complete the proof.",
  "ijsij fi fj22 ,(63)": "where F Rnc is the eigenvector (i.e., FT F = I) of LS corresponding to the c eigenvalues.We first derive the first equation. The eigendecomposition of the symmetric LS can be written as:LS = BBT , where B is the eigenvector matrix and is the diagonal matrix whose diagonalelements are the eigenvalues of LS. We have:",
  "(64)": "where ci=1 i indicates the sum of any c eigenvalues of LS. Obviously, minFT F=I Tr(FT LSF)achieves its minimization when F is the eigenvectors corresponding to c smallest eigenvalues.Therefore we have minSci=1 i (LS) = minFT F=I Tr(FT LSF) and the first equation in Eq. (63)is proved. Moreover, based on Eq. (26)-Eq. (28), we can further complete the proof the secondequation in Eq. (63).",
  "D.1Datasets": "We use four public heterogeneous graph datasets and two public homogeneous graph datasets fromvarious domains. Heterogeneous graph datasets include three academic datasets (i.e., ACM ,DBLP , and Aminer ), and one business dataset (i.e., Yelp ). Homogeneous graph datasetsinclude two sale datasets (i.e., Photo and Computers ). summarizes the data statistics. Welist the details of the datasets as follows.",
  "ACM is an academic heterogeneous graph dataset. It contains three types of nodes (paper(P), author (A), subject (S)), four types of edges (PA, AP, PS, SP), and categories of papersas labels": "Yelp is a business heterogeneous graph dataset. It contains four types of nodes (business(B), user (U), service (S), level (L)), six types of edges (BU, UB, BS, SB, BL, LB), andcategories of businesses as labels. DBLP is an academic heterogeneous graph dataset. It contains three types of nodes (paper(P), authors (A), conference (C)), four types of edges (PA, AP, PC, CP), and research areasof authors as labels.",
  "D.2Comparison Methods": "The comparison methods include eleven heterogeneous graph methods and twelve homogeneousgraph methods. Heterogeneous graph methods include Mp2vec , HAN , HGT , DMGI, DMGIattn , HDMI , HeCo , HGCML , CPIM , HGMAE , and HERO. Homogeneous graph methods include GCN , GAT , DeepWalk , DGI , GMI, MVGRL , GRACE , GCA , G-BT , COSTA , DSSL , and LRD .The characteristics of all methods are listed in , where Hetero and Homo indicatethe methods designed for the heterogeneous graph and homogeneous graph, respectively. Semi-sup\", and Self-sup/unsup\" indicate that the method conducts semi-supervised learning, and self-supervised/unsupervised learning, respectively. Meta-path indicates that the method requirespre-defined meta-paths during the training process. Adaptive indicates that the method learns anadaptive graph structure instead of traditional meta-paths.",
  "D.3Evaluation Protocol": "We follow the evaluation in previous works to conduct node classification and nodeclustering as semi-supervised and unsupervised downstream tasks, respectively. Specifically, we firsttrain models with unlabeled data in a self-supervised manner and output learned node representations.After that, the resulting representations can be used for different downstream tasks. For the nodeclassification task, we train a simple logistic regression classifier with a fixed iteration number, andthen evaluate the effectiveness of all methods with Micro-F1 and Macro-F1 scores. For the nodeclustering task, we conduct clustering and split the learned representations into c clusters with theK-means algorithm, then calculate the normalized mutual information (NMI) and average rand index(ARI) to evaluate the performance of node clustering.",
  "D.4Model Architectures and Settings": "As described in , the proposed method employs the MLP (i.e., g) and the closed-formsolution of the affinity matrix S to obtain node representations Z. Moreover, the proposed methodemploys the heterogeneous encoder (i.e., f) to obtain heterogeneous representations Z. In addition,the proposed method employs the projection head p to obtain the cluster assignment matrix P.After that, the proposed method employs projection head q to map the node representations andheterogeneous representations into latent spaces. In the proposed method, projection head p andq are simply implemented by the linear layer, followed by the ReLU activation. We report thesettings for the dimensions of encoders in . Finally, In the proposed method, all parameterswere optimized by the Adam optimizer with an initial learning rate. Moreover, We use earlystopping with a patience of 30 to train the proposed SHGL model. In all experiments, we repeat theexperiments five times for all methods and report the average results.",
  "EAdditional Experiments": "This section provides some additional experimental results to support the proposed method, includingexperiments on the effectiveness of the affinity matrix in Section E.1, visualization of the learnedrepresentations in Section E.4, parameter analysis in Section E.5, experimental results on the nodeclustering task in , and experimental results on homogeneous graph datasets in .",
  "E.1Effectiveness of the Rank-Constrained Affinity Matrix": "The proposed method proposes to learn a rank-constrained affinity matrix with exact c components tocapture the connections within the same class while mitigating the connections from different classes.This actually shares part of a similar idea with the self-attention mechanism, which aims to assignweights for all sample pairs. To further verify the effectiveness of the rank-constrained affinity matrix,we investigate the performance of the variants methods with the cosine similarity, the affinity matrix,the self-attention mechanism, and report the results in . Obviously, the proposed method with the affinity matrix obtains superior performance than the cosinesimilarity and the self-attention mechanism on all datasets. The reason can be attributed to the factthat the affinity matrix in the proposed method is constrained to contain exactly c components tomitigate noisy connections from different classes. In contrast, although either the cosine similarity orself-attention mechanisms may assign small weights for node pairs from different classes, it inevitablyintroduces noise during the message-passing process to affect the quality of node representations. Asa result, the effectiveness of the rank-constrained affinity matrix is verified.",
  "InfoNCE91.31.191.20.892.40.792.00.894.10.694.60.576.80.485.40.3Proposed92.70.692.60.593.00.792.80.494.00.394.70.477.50.986.80.7": "results in . From , we can find that the variant method with InfoNCE loss obtainsa similar performance to the proposed method. However, the InfoNCE loss generally requires thetime complexity of O(n2), where n is the number of nodes. This may introduce large computationcosts during the training process. In contrast, the proposed method simply designs the node-levelconsistency constraint in Eq. (15) to capture the invariant information with the time complexity ofO(nd2), where d is the representation dimension and generally d2 < n. 1c1020304050 Number of clusters Macro-F1 (%) ACMYelpDBLPAminer",
  "E.3Effectiveness of Different Cluster Numbers": "The proposed method divides the learned representations into several clusters. Generally, the numberof clusters equals to c obtains better results because, in downstream tasks, it is easier to distinguish cclusters than a larger number of clusters. To verify it, we changed the number of clusters and reportedthe results in . Obviously, the proposed method obtains the best results when the numberof clusters equals to c and decreases as the number of classes increases. This is reasonable becausewhen the number of classes increases, the nodes within the same class may be assigned to differentclusters, thus making it difficult to classify them correctly.",
  ": The classification performance of the proposed method at different parameter settings(i.e., , and ) on all heterogeneous graph datasets": "the node representations learned by the proposed method exhibit better clustering status, i.e., nodeswith different class labels are more widely separated. Moreover, the representations learned by theproposed method obtain the best silhouette score, compared to other SHGL comparison methods(i.e., HDMI, HeCo, and HERO). The reason can be attributed to the fact that the proposed methodconducts spectral clustering explicitly, and cuts the learned graph into c components as well as furtherutilizes the clustering information to facilitate downstream tasks.",
  "E.5Parameter Analysis": "In the proposed method, we employ non-negative parameters (i.e., , and ) to achieve a trade-offbetween different terms of the final objective function J . To investigate the impact of , and withdifferent settings, we conduct the node classification on all heterogeneous graph datasets by varyingthe value of parameters in the range of and reporting the results in . From , we can find that if the values of parameters are too small (e.g., 103), the proposed method cannotachieve satisfactory performance. This verifies that both node-level and cluster-level consistencyconstraints are significant for the proposed method.",
  ". Crowdsourcing and Research with Human Subjects": "Question: For crowdsourcing experiments and research with human subjects, does the paperinclude the full text of instructions given to participants and screenshots, if applicable, aswell as details about compensation (if any)?Answer: [NA] 15. Institutional Review Board (IRB) Approvals or Equivalent for Research with HumanSubjectsQuestion: Does the paper describe potential risks incurred by study participants, whethersuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)approvals (or an equivalent approval/review based on the requirements of your country orinstitution) were obtained?Answer: [NA]"
}