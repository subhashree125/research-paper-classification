{
  "Abstract": "With the scale of Transformer-based vision models continuing to grow, finetun-ing these large-scale pretrained models for new tasks has become increasinglyparameter-intensive. Visual prompt tuning is introduced as a parameter-efficientfinetuning (PEFT) method to this trend. Despite its successes, a notable researchchallenge persists within almost all PEFT approaches: significant performancedegradation is observed when there is a substantial disparity between the datasetsused in pretraining and finetuning phases. To address this challenge, we drawinspiration from human visual cognition, and propose the Visual Fourier PromptTuning (VFPT) method as an effective and efficient solution for adapting large-scale Transformer-based models. Our approach innovatively incorporates the FastFourier Transform into prompt embeddings, seamlessly integrating both spatial andfrequency domain information. Apart from its inherent simplicity and intuitiveness,VFPT exhibits superior performance across various tasks, offering a general solu-tion to address the data disparity challenge. Empirical results demonstrate that ourapproach outperforms several state-of-the-art baselines on two benchmarks, withlow parameter usage (e.g., 0.57% of model parameters on VTAB-1k) and notableperformance enhancements (e.g., 73.20% of mean accuracy on VTAB-1k). Ourcode is avaliable at",
  "Introduction": "Fouriers theorem is not only one of the most beautiful results of modern analysis,but it may be said to furnish an indispensable instrument in the treatment ofnearly every recondite question in modern physics. Lord William Thomson Kelvin Prompt tuning is initially introduced for parameter-efficient adaptation of large foundationmodels in natural language processing (NLP). As vision models continue to scale for enhancedperformance, visual prompt tuning has been applied to various vision domains (e.g., imageclassification , segmentation , detection ), demonstrating superior performance and lowerparameter usage compared to other parameter-efficient fine-tuning (PEFT) methods. However, acommon challenge within the research community remains unaddressed: significant performancedegradation occurs when there is a substantial disparity between the data used in pretraining andfinetuning . This issue hinders the broader application of visual prompt tuning. Consequently,a natural question arises: Can prompt tuning generalize across datasets with varying disparities? As researchers commonly draw insights from human to replicate the principles in intelligent ma-chines , we consider to answer this question from the human visual cognitionsperspective. While humans comprehend the world through past experiences/knowledge, it is essentialto generalize and adapt this understanding to new tasks efficiently and effectively. The robust andrapid adaptability of human visual cognition thus arises from various domain analysis, capturing thenew patterns from different channels and perspectives .",
  "arXiv:2411.01327v2 [cs.CV] 15 Nov 2024": "Interestingly, we find that the paradigm of visual prompt tuning is conceptually analogous to humanvisual cognition. While the frozen large-scale vision model functions as accumulated knowledge, thefast adaptation mechanism resembles visual prompt tuning, requiring the incorporation of diversedomains of information (e.g., time, frequency) to achieve comprehensive understandings .The Fast Fourier Transform (FFT) , renowned for its ability to convert signals from theiroriginal domain (e.g., time or spatial) to the frequency domain and vice versa, serves as an idealtool for contributing informative insights in the frequency domain. By leveraging the capabilities ofFFT, visual prompts can naturally integrate both spatial and frequency domain information duringfinetuning, thereby enabling the frozen vision model to achieve consistent and robust performanceacross datasets with varying disparities. Consequently, our research question evolves into: Howcan FFT be integrated into visual prompt tuning to emulate the human visual mechanism? To this end, we employ a simple yet effective strategy that utilizes the Fourier operations to facilitatevisual prompt tuning (see (c)). By integrating frequency domain information into learnableprompt embeddings, our approach elegantly assimilates data from both spatial and frequency domains,simulating the human visual cognition. We name our approach Visual Fourier Prompt Tuning(VFPT), which exhibits several compelling advantages: Simplicity. The intuitive application ofFFT in prompt tuning emulates the rapid processing capabilities of the human visual system, makingVFPT both elegant and straightforward to implement (see 2.1). Generality. By incorporatingfrequency domain information, the search space for latent embeddings of prompts is naturallyexpanded, resulting in advanced enhancement in performance across different datasets and taskswith varying data disparities (see 4.2). The generality of our model is further illustrated throughour analysis of the optimization process, which enables smoother navigation towards local minima,increasing flatness around them and exhibiting apparent convexity. Interpretability. To intuitivelydemonstrate the advantages of Fourier components, we visually illustrate that the introduction ofFourier transform in visual prompt tuning results in a markedly higher concentration of attentionscores within the Transformers input space, which correlates positively with enhancements inperformance (see 4.4). This observation, in turn, explains the effectiveness of our approach. Comprehensive experiments are conducted to evaluate the performance of VFPT. In 2, we conduct aliterature review and discuss relevant works. Our approach is presented in 3, where we describehow we simple yet effectively integrate FFT into visual prompt tuning. In 4.2, we present compellingexperimental results on various benchmarks, backbones, and different pretraining objectives, achievingsuperior performance without complex engineering design. Specifically, our approach achieves anaverage improvement of 7.63% in accuracy on VTAB-1k compared to full finetuning, and 3.77%compared to VPT . In 4.4, we demonstrate that the FFT prompts significantly enhance theactivation of the frozen vision model. Additionally, we study the optimization process of prompttuning approaches, indicating that VFPT provides a more favorable optimization process. Finally, wedemonstrate the strong algorithmic generalization of our approach to the language domain, and showadditional visual explanations in the Appendix. We trust that this work provides valuable insights.",
  "Visual Parameter-efficient Finetuning": "With the significant growth in the scale of vision models, especially following the emergence ofVision Transformers , the development of PEFT methods under pretrain-then-finetune paradigm becomes increasingly critical. Current methods under this paradigm can begenerally categorized into partial tuning , extra module (i.e., including reparameterizationapproaches such as Low-Rank Adaptation (LoRA) ) , and prompttuning . Partial tuning and extra module face several limitations that hinder theirapplication. Unsatisfactory performance: they generally cannot reach competitive performancewith regard to full finetuning ; Model-oriented design: most research requiresto insert specific architecture/block design during tuning, rendering them non-universalsolutions when considering different backbones. In contrast, prompt tuning , originally proposedfor language-domain , provides a general and straightforward solution in vision withpowerful performance gains. It signals a new paradigm in PEFT in the field of computer vision. Generally, prompt tuning introduces a sets of learnable parameters to the input sequence of backbonemodels, updating only these parameters during the finetuning. Despite its apparent simplicity, theparadigm of visual prompt tuning has demonstrated notable performance enhancements. Current developments on visual prompt tuning primarily concentrate on engineering optimizations, such asreducing parameter usage and expanding applicability across diverse tasks . Theseapproaches often involve introducing additional constraints and functionalities to the foundationaldesign, which deviate from the principles of simplicity and elegance to the original concept of visualprompt tuning. Our approach, in sharp contrast, endeavors to explore visual prompt tuning fromthe perspective of human visual intelligence, while diligently maintaining the simplicity of prompttuning. It is also essential to emphasize that visual prompt tuning diverges markedly from visualinstruction tuning (i.e., aiming at improving the models instruction following abilities).",
  "Fast Fourier Transform in Vision": "FFT is a powerful mathematical algorithm used to compute the Discrete Fourier Transform (DFT) andits inverse . It is pivotal in information processing, allowing the detailed analysis of varioussignals (e.g., image , radar ) for frequency determinations. In vision, FFTsability to transform complex data in spatial domain into frequency domain makes it an invaluable toolfor abstracting critical features from noisy or high-dimensional datasets . This abstraction isparticularly beneficial as the identification of salient features are shown to have better generalizationability across domains , directly influences the performance of imageanalysis and processing tasks. Current research on FFT in vision predominantly explores areas suchas conventional image processing , image pre-processing for deep neural networks(DNNs) and DNN architectural design . Despite its profound utility and effectiveness, the integration of FFT within the paradigm of visualprompt tuning remains largely underexplored. Recent work adapts the pretrained multi-modalnetwork to the tasks under modality-incomplete segmentation scenarios via FFT prompt tuning. Thisapproach demonstrates the potential of FFT operations to handle missing modalities (i.e., substantialdisparity) effectively. However, it primarily focuses on task-specific optimization and design. Theextensive applicability and generality of FFT, especially in cross-dataset analysis, have yet to berecognized or exploited. Another work incorporates Fourier transform into the LoRA-basedapproach. While the expressive Fourier basis facilitates the recovery of weight changes, it does notfully integrate frequency domain information during finetuning, which remains orthogonal to ourapproach. In this paper, we aim to broaden the scope of exploration and contribute to advancingthe field of Fourier-based research in vision. By studying the integration of FFT with visual prompttuning, we fully explore how to improve both the efficacy (see 3) and the adaptability of learningmodels to diverse and challenging datasets (see 4). Furthermore, we present novel evidenceindicating that VFPT establishes strong correlations within the Transformers input space, aligningwith the performance enhancements (see 4.4). Overall, the generality of VFPT suggests a novelunderstanding of the Fourier-based method in current machine learning applications.",
  "Methodology": "In this section, we introduce VFPT, a novel visual prompt tuning approach for effective and generallarge-scale transformer-based model finetuning. We first define the problem and notations of visualprompt tuning and FFT in 3.1. The integration of Fourier-based visual prompt tuning is presented in3.2. The overall framework is shown in (c), where we compare our model with original VPT.",
  "Preliminary": "Visual Prompt Tuning. Given a pretrained Transformer model T with N layers, the objective ofprompt tuning in vision is to finetune a model T into a new task with only a few set of d-dimensionalembedding vectors, i.e., prompts, in the input space after patch Emb layer. These learnable promptsare defined as P = {P 1, P 2, . . . , P N}, where P i represents the learnable visual prompts in the ithencoder layer. Formally, the encoder layers with prompts are defined as:",
  "(a) Visual Prompt Tuning(b) Fast Fourier Transform in Prompts(c) Visual Fourier Prompt Tuning": ": Overview of VPT vs. VFPT (ours) frameworks. (a) Original Visual Prompt Tuning. (b)2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence lengthdimensions. (c) The overall architecture of our proposed VFPT (see 3.2). Fast Fourier Transform. The FFT is a powerful algorithm for computing the Discrete FourierTransform (DFT), which transforms a finite sequence of equally-spaced function samples into asame-length discrete-time Fourier transform sequence. Specifically, given a sequence {xn} where nis a member of the interval n [0, N 1], the DFT is defined as:",
  "N n,0 k N 1.(2)": "For a finite sequence of equally-spaced samples {xn}, the DFT generates a same-length sequenceof equally-spaced samples {Xk}. This transform is denoted as F. The initial DFT is in complexityO(n2). For acceleration, we use CooleyTukey FFT algorithm following common practice (i.e., complexity O(n log n)). FFT serves as a powerful tool for domain transition. Consequently, weexplore the integration of the FFT operation within PEFT methods, particularly in prompt tuning.",
  "Visual Fourier Prompt Tuning": "Visual prompt tuning is particularly useful under the pretrain-then-finetune paradigm. However,it suffers a significant performance reduction when substantial disparities exist between pretrainand finetune datasets. The reason is that during finetuning on new data, the image distribution maydeviate markedly from the examples used in pretraining the backbone model . Existing prompttuning , focusing predominantly on spatial information, can only harness the shared informationembedded within the pretrained backbone, limiting their capacity to adapt effectively to novel tasks.Thus, it is crucial to strengthen the ability to capture distinguishing feature from finetuning data. To this end, we introduce VFPT, an intuitive yet powerful method with advanced performance andgenerality. Compared to VPT (see (a)), our model (see (c)) transforms partial promptsfrom spatial domain to frequency domain via 2D FFT (see 3.1) to consider both the spatial andfrequency domain information. Formally, for each learnable visual prompts in the ith encoder layerP i P = {P 1, P 2, . . . , P N}, we have P i = {pi1, pi2, . . . , piM}. We select m partial prompts as visualFourier prompts at each layer, where 0 m M. Further, = m/M represents the fraction ofFourier participation, where zero indicates all prompts are original visual prompts, and one impliesall prompts are given after FFT. We apply a 2D FFT on visual prompt embedding input withrespect to both sequence (i.e., Fseq) and hidden dimensions (i.e., Fh). Note that the operationsFseq(Fh(x)) and Fh(Fseq(x)) are mathematically equivalent due to the commutative property of thetwo one-dimensional FFTs . Here,indicates Fourier operations.",
  "P i =P iF, pim+1, . . . , piM.(4)": "Our elegant design of VFPT enjoys a few appealing characteristics: Simplicity: VFPT only requires several lines of code based on the implementation of the visualprompt tuning. Its intuitive integration of information between spatial and frequency domainsbrings nearly free performance efficacy. The low complexity of FFT (i.e., O(n log n)) leads to anoverall marginal reduction during the training schedule.(i.e., 2.8% on VTAB-1k ). In sharpcontrast, current endeavors in visual prompt tuning mainly emphasize augmenting architecturalcomplexity for superior performance , undermining the inherent simplicity of prompttuning and introducing significant training overhead (e.g., learns 2D prompt token map fordensely image relationship construction, incorporates additional self-attention K-V prompts). Generality: The frequency and spatial analysis of imagery inputs can be mutually complemen-tary, leading to a more comprehensive feature understanding from distinct perspectives (e.g., thefrequency domain allows for the distraction and decomposition of luminance and noise to a con-siderable degree , while the spatial domain excels in capturing intricate object details). Byincorporating learnable prompts from both domains, VFPT demonstrates enhanced prompt learningcapabilities, which makes it superior to finetune across diverse tasks (see 4.2). The empiricalfindings of flatness and convexity of VFPT further strength our claim. Interpretability: In visual prompt tuning, a notable challenge arises concerning the interpretabilityof learnable prompts. Unlike in NLP, where tokens explicitly represent these prompts, visualprompts have historically lacked a clear and explainable representation. In order to intuitivelyperceive the function of visual prompts, we offer a possible way to understand why prompts playan important role in fine-tuning a new task through the visualization of attention maps. Moreover,we can also observe a better and stronger global feature learning pattern through introducing visualFourier prompts, showing how Fourier prompts work. More discussion will be elaborated in 4.4.",
  "Experiment Setup": "Datasets. Following common practice , our experiments are carried out on two imageclassification benchmarks. VTAB-1k collects 19 benchmarked Visual Task Adaptation, separatedinto three groups: (1) Natural includes natural images captured by standard cameras, (2) Specializedconsists of images taken by specialized equipment, and (3) Structured considers tasks consideringgeometric comprehension (i.e., counting, distance), which has substantial dataset disparities (i.e., tasksin Natural and Specialized are closely related to image classification and thus have low disparities,while tasks in Structured are regarded as distinct from image classification) when comparing tothe pretrained dataset (i.e., ImageNet21K ). Each task of VTAB-1k contains 1000 trainingexamples with the 800/200 split for train/val set. FGVC contains 5 benchmarked Fine-GrainedVisual Classification, including CUB-200-2011 , NABirds , Oxford Flowers , StanfordDogs and Stanford Cars . The training set is split into 90% train and 10% val.Baselines. For consistency, we follow and compare VFPT with other widely applied parameter-efficient fine-tuning methods. Results of two vision transformer architectures, Vision transformer (ViT) and Swin transformer (Swin), on image classification are discussed in 4.2. We also applyVFPT on two self-supervised objectives: MAE and MoCo v3 .Training. Following , we conduct grid search to find the best tuning hyperparameters, learningrate (i.e., [50, 25, 10, 5, 2.5, 1, 0.5, 0.25, 0.1, 0.05]), and weight decay (i.e., [0.01, 0.001, 0.0001,0.0]) on val set. Notably, VFPT does not require specific-designed large learning rate in . Thelearning rate is scheduled by a cosine decay policy and trained for 100 epochs.Reproducibility. VFPT is implemented in Pytorch . Experiments are conducted on NVIDIAA100-40GB GPUs. To guarantee reproducibility, our full implementation will be publicly released. 4.2Main ResultsIn this section, we demonstrate the effectiveness of VFPT from two key perspectives: Supe-rior Performance: Our model demonstrates significant performance improvements across diversedatasets, including challenging tasks with large disparities in data, thus showcasing its generalizability. : Image classification accuracy for ViT-Base/16 pretrained on supervised ImageNet-21k.Following , we report the average test accuracy (three runs) on FGVC and VTAB-1k benchmarks, and Number of Wins in [] compared to full fine-tuning (Full) . denotes themethod with highest Number of Wins compared to Full. We further report Number of Wins toVPT in {}. Tuned/Total is the average percentage of tuned parameters required by 24 tasks.Scope indicates the tuning scope of each method. Additional parameters is the existence ofparameters in addition to the pretrained backbone and linear head. Bold and Underline indicate thebest and the second best results. VFPT outperforms full fine-tuning in 22 of 24 instances with fewertrainable parameters and beats VPT in 23 of 24 cases with lower parameters. denotes methods usingsoft filtered prompts to reduce the parameter usage in learnable visual prompts, requiring specializeddevices to facilitate acceleration. Per-task results are available in Appendix. Same for and 3.",
  "Ours0.27% 84.53% {5} 86.15% {4} 58.21% {6}": "Definition of disparity.Following ,we use the Frchet Inception Distance(FID) to measure the disparitybetween the datasets used in pretraining(i.e., ImageNet) and funetuning (i.e., down-stream tasks). Average FID scores of eachgroup are reported in , where theNatural group has low disparities due toits close relationship to ImageNet21K and the Specialized and Structured groups(i.e., orientation prediction task) are considered distinct from image classification. The datasetdescription of VTAB-1k is covered in 4.1 (FGVC is excluded due to lack of categorization). Superior Performance. In order to have a comprehensive understanding on generality, we examineVFPT on ViT-Base/16 , Swin-Base , and two self-supervised objectives, following commonpractice . We also report the individual per-task results for , 2 and 3 in Appendix.VFPT on ViT. We report the average accuracy score on VTAB-1k and FGVC benchmarks acrossfour diverse task groups for three runs in , where fifteen protocols under pretrain-then-finetuneparadigm are considered. Specifically, Full updates both backbone and classification head;Linear , Parital-1 (top layer), and MLP-3 (3 MLP layers) are partial tuning approaches;Sidetune , Bias , Adapter , LoRA , AdaptFormer and ARCatt are extramodule methods which add new trainable parameters to backbone for adaptation; VPT-S , VPT-D , EXPRES and E2VPT are concurrent visual prompt tuning approaches. Consequently,we have several key observations. First, VFPT is able to outperform the full fine-tuning method in22 out of 24 tasks. For example, our model achieves 0.13% improvement on FGVC and 5.21%improvements on VTAB-1k Structured, respectively. The empirical results show the effectivenessof VFPT. Second, VFPT tunes only 0.66% of the overall parameters in the backbone, establishingit as a competitive method within the PEFT approaches. Third, while VPT struggles to capture theimage information when having significant dataset disparity, VFPT achieves notable performanceimprovements by integrating both spatial and frequency information (see 3.2) without additionalarchitectural modifications. (i.e., 60.19% vs. 54.98% on VTAB-1k Structured).VFPT on Hierarchical Transformer. We further extend VFPT to a hierarchical transformer Swin-Base for architectural generalization. The MSA layer of Swin is employed in local shiftedwindows, and patch embeddings are merged at deeper layers. For consistency, we follow the samesettings from ViT to apply and prepend Fourier prompts ahead of the visual prompts. The results on",
  "Fourier Percentage (%)": ": Image classification accuracy of various Fourier percentages of VTAB-1k forViT-Base/16 . For better illustration, we randomly select 3 datasets in each group of VTAB-1k.The Average FID Score of Each Group is reported in <>. Our conclusion aligns with 16 of 19cases. The cross framed by the square indicates the best percentage for each downstream task. Thosedatasets with only three Fourier percentage reports are due to the prompt length limits. the ImageNet-21k supervised pretrained Swin-Base are reported in . It can be seen thatVFPT consistently outperforms all the other parameter-efficient methods on three VTAB-1k groups.VFPT on Different Pretraining Objectives. In , we report the experimental results on twoself-supervised objectives: MAE and MoCo v3 . While VPT yields inconclusive results,VFPT has the highest Number of Wins compared to full fine-tuning among PEFT methods (i.e., 8of 19 instances under MAE, and 14 of 19 instances under MoCo v3, respectively). Our method alsooutperforms VPT by a large margin (e.g., 53.59% vs. 36.02% under MAE on VTAB-1k Natural). Fourier Contribution. We conducted experiments to understand the impact of Fourier componentsby varying the percentages of Fourier prompts in VFPT. As shown in , we observed distinctpreferences across the VTAB-1k benchmark, which comprises three groups with varying datadisparities (see 4.1). Specifically, the Natural group, which has a data distribution similar tothe pretrained task (low disparity), shows peak performance when half of the visual prompts aretransformed into Fourier prompts, as indicated by the accuracy curves in (a). This suggeststhat transfer learning is less challenging in this group. Conversely, for the Specialized and Structuredgroups, which have data distributions significantly different from the pretrained task (high disparity),the accuracy curves in (b-c) demonstrate that higher classification performance is achievedwith an increased percentage of Fourier components. These observations are consistent with ourexpectations, demonstrating the effectiveness of Fourier prompts in VFPT, especially for tasks withlarge data disparities. In other words, our approach can be viewed as a generalization of VPT, wherethe Fourier components learn effective representations from the frequency domain that complementthe knowledge from the spatial domain.",
  ": Visualization of loss landscape and the ratiomap of Hessian": "understand the enhanced generality of VFPT.Specifically, in (a), we randomly selecttwo parameter directions for the study, as ran-domness in directions does not significantly af-fect the results . There are two key obser-vations supporting the enhanced generality ofVFPT. i) Flatness: VFPT provides a larger con-nected region around the local minimum (e.g., in the yellow square, where the largerblue area in VFPT offers more optimizationchoices) and a smoother edge of the loss land-scape for mitigating chaotic landscapes (e.g., in the green square, where the bumpy contour inVPT is sensitive to loss variations, resulting inworse generality). This indicates that VFPT achieves a flatter minimizer, which consistently correlateswith lower test error . ii) Convexity: As eigenvalues of the Hessian directly assess the convexityof a loss function, we compute both the maximum and minimum eigenvalues of the Hessian andmap their ratios. As shown in (b), a higher prevalence of near-zero negative eigenvalues (indeep blue) in VFPT suggests the presence of more convex regions (25.0% vs. 20.0%) for modeloptimization. This finding indicates that the incorporation of the Fourier transform in visual prompttuning effectively mitigates the sharpness of the loss landscape.",
  "D Attention Map": ": Study of interpretability. (a) The 3Dand 2D attention map in VPT and VFPT on a ran-domly selected sample. The colors ,andindi-cate class, prompt and patch tokens, respectively.(b) Corresponding GradCAM maps. Notethat red regions correspond to a high score for theclass. We present more visualization results in S4 To the best of our knowledge, research onthe understanding of prompt tuning remainsrare . Consequently, our research seeksto both quantitatively and qualitatively exam-ine the impact of Fourier components on theenhancement of visual prompt tuning. For fair-ness, instead of using enhanced visualizationmethods that may alter the orig-inal expression of the learnable prompts, wevisualise and examine the raw average attentionhead on the last layer of VPT and VFPT. Significant attention distribution in learn-able prompts. Observations from both VPTand VFPT in (a) reveal a common phe-nomenon: there exists a pronounced accumula-tion of attention scores at learnable prompt loca-tions (i.e., narrow color area on the left side of2D attention map), indicating that these promptshave a substantial impact on the frozen embed-dings during the finetuning stage.Global attention scores pattern in Fourierprompts. We further observe a notably higherconcentration in global attention scores whenintegrating visual Fourier prompts. Specifically,the global attention scores indicate that VFPTalso establishes robust correlations within theTransformers input space (see (a)). Incontrast, VPT lacks this correlation, suggestingthat it does not adequately consider or integrateextensive information from the frozen backbone. Moreover, we find a positive relationship betweenstrong associations and performance gains quantitatively (see 4.2) and qualitatively (see (b)) inVFPT, suggesting that the integration of visual Fourier prompts encourage clear foreground (i.e., treewith high frequency component) - background (i.e., sky with low frequency component) separation. : A set of ablative studies on VTAB-1k Natural and Specialized benchmarks in three runs. PromptLocation is the placement of the visual Fourier prompts relative to original visual prompts. Prompt Depthindicates the layer we use visual Fourier prompts. Transform Type is the method we use to transform promptsand input images. Fourier/Transform Dimension indicates the dimension we apply using specific transformmethod. Per-task results are available in Appendix. Same for .",
  "(c) Fourier Prompt Depth": "In summary, our findings provide significant insights into the interpretability of prompt tuning,revealing that for both VPT and VFPT, a considerable portion of attention is directed towards thelearnable prompts. Further, VFPT exhibit enhanced global feature learning capabilities compared toVPT by interfacing effectively with frozen embeddings, thereby enabling precise capture of distinctivefeatures across diverse downstream tasks. This observation corroborates our findings in 4.2.",
  "FFT (F)81.35%84.93%": "We ablate VFPTs key components onVTAB-1k Natural and Specialized.More studies are provided in S2.5.Transform Type. We ablate on other trans-form method instead to certify the impactof Fourier transform in , wherethe Fixed Linear Layer (i.e., FLL) and theLearnable Linear Layer (i.e., LLL) are con-sidered. Compared with FFT, a fixed non-parameter Fourier domain transform in sequence andhidden dimension, the FLL operation considers only a fixed spatial domain transform in hiddendimension; the LLL further unfixes the transformation to enable gradient updates. As seen, both FLLand LLL show inferior performance to FFT. We further consider the impact of current Fourier domainadaption approach , which maps a source image to a target style without altering semanticcontent. However, no significant improvement can be observed.Fourier Prompt Dimension. A fundamental distinction between VFPT and other methods is theincorporation of FFT into visual prompts. In our standard implementation, we utilize 2D FFTs acrossboth sequence length and hidden dimensions. Here, we explore the impact of each dimensionstransformation individually. As shown in (a), the separate Fourier transformations along eachdimension appear to have similar contributions (i.e., 80.88% vs. 80.74% in Natural). However, thecombined application of transformations across both dimensions (i.e., 2D FFTs) demonstrates asynergistic effect, yielding significant improvement in performance.Fourier Prompt Location. In (b), three prompt locations are considered for VFPT, which arePrepend (i.e., P), Append (i.e., A), and Random (i.e., R). Specifically, P and A prepend visualFourier prompts before or after visual prompts, and R randomly selects the position for visual Fourierprompts in each layer. As seen, both P and A show competitive results, validating the robustness ofVFPT w.r.t. prompt locations. In alignment with the findings in , we choose P as our baselinemethod in all experiments since it reaches superior results (i.e., 81.35% vs 81.02% in Natural).Fourier Prompt Depth. (c) presents the performance of VFPT based on the specific layerat which visual Fourier prompts are employed. The results suggest that employment on separatelayers also yields a accuracy improvement compared with VPT. Further application of visual Fourierprompts across all layers fosters the best overall performance.",
  "Conclusion": "We present Visual Fourier Prompt Tuning (VFPT), a simple yet powerful parameter-efficient vi-sual prompt tuning approach that draws insights from human visual cognition. It has merits in: i)integrating spatial and frequency domain information through an intuitive yet effective design; ii)demonstrating generality across datasets with varying disparities while ensuring powerful perfor-mance; and iii) thoroughly investigating the associations between learnable prompts and frozenembeddings to elucidate this generality. As a whole, we conclude that the outcomes elucidated in thispaper impart essential understandings and necessitate further exploration within this realm.",
  "Yun He, Steven Zheng, Yi Tay, Jai Gupta, Yu Du, Vamsi Aribandi, Zhe Zhao, YaGuangLi, Zhao Chen, Donald Metzler, et al. Hyperprompt: Prompt-based task-conditioning oftransformers. In ICML, 2022": "Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.Pre-train, prompt, and predict: A systematic survey of prompting methods in natural languageprocessing. ACM Computing Surveys, 55(9):135, 2023. Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for natural language processing: A survey. Science China TechnologicalSciences, 63(10):18721897, 2020.",
  "Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networksfor image classification. In NeurIPS, 2021": "John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and BryanCatanzaro. Adaptive fourier neural operators: Efficient token mixers for transformers. arXivpreprint arXiv:2111.13587, 2021. Ruiping Liu, Jiaming Zhang, Kunyu Peng, Yufan Chen, Ke Cao, Junwei Zheng, M SaquibSarfraz, Kailun Yang, and Rainer Stiefelhagen. Fourier prompt tuning for modality-incompletescene segmentation. arXiv preprint arXiv:2401.16923, 2024. Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme,Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy,et al. A large-scale study of representation learning with the visual task adaptation benchmark.arXiv preprint arXiv:1910.04867, 2019.",
  "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Maskedautoencoders are scalable vision learners. In CVPR, 2022": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, AndreasKopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style,high-performance deep learning library. In NeurIPS, 2019.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training ofdeep bidirectional transformers for language understanding. In NAACL, 2018": "Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-tuning v2:Prompt tuning can be comparable to fine-tuning universally across scales and tasks. arXivpreprint arXiv:2110.07602, 2021. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, FelixHill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purposelanguage understanding systems. In NeurIPS, 2019.",
  "S3 provides per-task results on Fourier percentage, where partial results have been provided inthe main paper": "S4 presents more details and results of visualization of attention maps. S4 presents more details and results of visualization of loss landscapes. S6 discusses our potential extension to language tasks. S7 further analyze the complexity of our approach. S8 shows related asset license and consent to our work. S9 claims reproducibility of our approach. S10 discusses the social impact of our research. S11 adds more discussions, and points out potential directions of our future work.",
  "S1.1Per-task Results on ViT-Base": "To provide comprehensive results from the paper, we report the average per-task test accuracy (i.e., 3runs, 24 tasks) on VTAB-1k Natural, Specialized and Structured, respectively (see Table S1,S2 and S3). We also report per-task FGVC results (5 tasks) in Table S4. VPT-SHALLOW isalso included for completeness (i.e., VPT-SHALLOW only introduces 1-st layer visual prompts). Inconclusion, VFPT shows consistently better performance in various downstream tasks.Table S1: VTAB-1k Natural per-task results for ViT-Base/16 pretrained on supervisedImageNet-21k. Consistent to our paper, Number of Wins in [] compared to full fine-tuning .Tuned/Total is the percentage of tuned parameters in each task, along with the average results ofthose percentages in each group. The highest accuracy among all approaches except FULL are shownin bold. denotes method using soft filtered prompts to reduce the parameter usage in learnable visualprompts, requiring specialized devices to facilitate acceleration. All results are averaged in threeruns with different initialization seeds. Same for Table S2-S21. We also report standard deviationerror bars for our main results (Table S1, S2, S3 and S4) by calculating each task respectively andaveraging across them. Other tables show similar trends on standard deviation error bars.",
  "S1.3Per-task Results on MAE and MoCo v3": "Table S8: VTAB-1k Natural per-task results for ViT-Base/16 pretrained on MAE .Since VPT have considerably lower performance, we do not list the per-task results for simplicity.We instead compare our method to full fine-tuning, and the highest accuracy is shown in bold. Wepost the Number of Wins in [] to full fine-tuning (FULL) . Same for Table S9-S13.",
  "S6Extension to Language Tasks": "While ViT-Base/16 is structurally similar to BERT , we follow and naturally testthe efficiency of the VFPT on natural language understanding (NLU) tasks. Specifically, we includeBERT-Large for evaluation, and compare full fine-tuning (FULL) , Prompt Tuning ,P-Tuning v2 and E2VPT on SuperGlue dataset: a collection of text classification tasksto test the general language understanding ability. The tasks include natural language inference (RTEand CB), coreference resolution (WSC), sentence completion (COPA), word sense disambiguation(WiC), and question answering (MultiRC (Fla), ReCoRD (F1) and BoolQ). In Table S25, we showthat VFPT outperforms FULL and Prompt Tuning and show competitive results to P-Tuning v2 .Considering VFPT is designed for visual-related tasks, and text understanding tasks might not needfruitful frequency domain information, these results are impressive and suggest future work for ageneral solution across modalities under the pretrain-then-finetune paradigm.Table S25: Per-task results for SuperGLUE development set with a pretrained BERT-Large . See S6.",
  "Fourier Percentage (%)Maximum Memory Consumption (GB) Training Average Batch Time (s) Inference Average Batch Time (s)": "VPT (0%)1.82100.11400.0499VFPT (30%)1.8210 (0%)0.1169 (+2.54%)0.0505 (+1.20%)VFPT (50%)1.8210 (0%)0.1155 (+1.32%)0.0502 (+0.60%)VFPT (70%)1.8210 (0%)0.1150 (+0.88%)0.0500 (+0.20%)VFPT (100%)1.8210 (0%)0.1150 (+0.88%)0.0501 (+0.40%) We have provided a detailed comparison of our computational results in this section. More specifically,we experimented with different Fourier percentage settings (i.e.., the alpha rate) on the CIFAR-100benchmark and reported their maximum memory consumption, average training batch time, andaverage inference batch time. All settings were tested with the same batch size and prompt length.The experiments were conducted on NVIDIA A100-40GB GPUs. As illustrated in Table S26, no significant increase in maximum memory consumption at the MB levelis observed across different Fourier percentage settings. However, we do observe a slight increase inaverage batch time during both training and inference, on the order of 103 and 104, respectively.This suggests that a lower Fourier percentage incurs a higher computational burden. This effect islikely attributable to suboptimal parallel acceleration and the implementation inefficiencies associatedwith prompts that have partial Fourier transformation. We will investigate this further in futureresearch.",
  "S8Asset License and Consent": "The majority of VPT is licensed under CC-BY-NC 4.0. Portions of are available underseparate licenses: google-research/task_adaptation and huggingface/transformers are licensed underApache-2.0; Swin-Transformer and ViT-pytorch are licensed under MIT; and MoCo-v3 and MAE are licensed under CC BY 4.0. All the datasets included in our study are publicly available (VTAB-1K, FGVC), and all the modelsare publicly available. We would like to state that the contents in the dataset do NOT represent ourviews or opinions.",
  "S9Reproducibility": "VFPT is implemented in Pytorch . Experiments are conducted on NVIDIA A100-40GB GPUs. Toguarantee reproducibility, our full implementation shall be publicly released upon paper acceptance.For training schedule, the superior low-complexity of FFT (i.e., O(n log n)) allows for efficienttraining of visual Fourier prompts with only a slight decrease in training speed (i.e., 2.8% onVTAB-1k compared to VPT).",
  "S10Social Impact and Limitations": "This study presents VFPT, demonstrating significant and generalizable performance enhancementsover state-of-the-art baselines across two benchmarks. The incorporation of the FFT contributes theseadvantages without necessitating architecture-specific designs or incurring substantial computationaloverhead under pretrain-then-finetune paradigm for large-scale models (see 3). Our approach enjoysadvanced model accuracy, and is valuable in real-world computational-sensitive applications, e.g.,training machine learning models on edge devices. Moreover, VFPT advances significantly towardsachieving generality across datasets, demonstrating substantial performance improvements even whenfaced with large dataset disparities (see 4). This progress is crucial for the continuous developmentof PEFT across a wider spectrum of applications. For potential limitations, drawing inspirations from human visual cognition, our method incorporatesspatial and frequency information, which brings an additional hyper-parameter Fourier percentage(i.e., in 3.2). However, in practical applications, we observe in 4.2 that dataset disparity (i.e.,low disparity tasks prefer small value, and vice versa) serves as a guideline for selecting anappropriate Fourier percentage. Nonetheless, we argue that the implementation of an automaticFourier percentage search can further augment efficiency.",
  "S11Discussion and Future Work": "In 2, we review PEFT methods and the application of the fast Fourier transform in vision. Notably,a recent study in NLP incorporates Fourier transform as a viable PEFT approach, whichwarrants discussion. Specifically, it learns a set of spectral coefficients of Fourier basis using aLoRA-based approach and then applies the inverse discrete Fourier transform to the spectral matrix,yielding its spatial-domain counterpart as the updated weight change. Although the Fourier basissorthogonal and expressive advantages reduce the need for extensive parameter fine-tuning, the inversetransform applied to the spectral matrix discards frequency information, ultimately considering onlytraditional spatial domain features. The parameter-efficient use of the Fourier transform in this studyis orthogonal to our method, where both spatial and frequency domain information are integrated(see 3) for enhanced generality (see 4.2) and interpretability (see 4.4). Despite VFPT systemic effectiveness and simplicity, it also comes with new challenges and unveilssome intriguing questions. For example, the balance between spatial and frequency information ispresently dictated by task-specific, manually set percentages (see 4.2). Introducing a small networkwithin the VFPT framework to autonomously search for optimal combinations might enhance trainingefficiency and facilitate additional performance improvements. Another essential future directiondeserving of further investigation is the integration of visual information from both the spatial andfrequency domains. In 4.5, we demonstrate through ablation studies that integration at the pre-processing stage may not yield satisfactory performance. Consequently, we outline several alternativeintegration approaches in , demonstrating that VFPT holds the most advantageous positionunder the prompt tuning paradigm. Nonetheless, the applicability of this integration to other PEFTmethods requires further investigation."
}