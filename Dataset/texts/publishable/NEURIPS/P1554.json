{
  "Abstract": "While language models have exceptional capabilities at text generation, they lacka natural inductive bias for emitting numbers and thus struggle in tasks involvingreasoning over quantities, especially arithmetics. This has particular relevance inscientific datasets where combinations of text and numerical data are abundant.One fundamental limitation is the nature of the CE loss, which assumes a nominal(categorical) scale and thus cannot convey proximity between generated numbertokens. As a remedy, we here present two versions of a number token loss. The firstis based on an Lp loss between the ground truth token value and the weighted sumof the predicted class probabilities. The second loss minimizes the Wasserstein-1distance between the distribution of the predicted output probabilities and theground truth distribution. These regression-like losses can easily be added to anylanguage model and extend the CE objective during training. We compare theproposed schemes on a mathematics dataset against existing tokenization, encoding,and decoding schemes for improving number representation in language models.Our results reveal a significant improvement in numerical accuracy when equippinga standard T5 model with the proposed loss schemes.",
  "Introduction": "As coined by Thawani et al. , numbers in natural texts are ubiquitous and important, yet system-atically neglected by language models (LMs). Even worse, while Transformers were inventedfor NLP, they have permeated various scientific domains (chemistry, biology, etc ), wheretabular/numerical data is more prevalent than in NLP and often even fundamental for constructing taskdefinitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesisprocedures are natural text interspersed with quantities and times. Still, LMs notoriously struggleeven with simple arithmetic tasks like three-digit multiplication for multiple reasons: 1. Tokenization: Standard subword tokenization splits numbers into arbitrary tokens, disruptingtheir structure. Mitigation strategies include scientific notation or digit-level tokenization ,which may also preserve the decimal order of each digit .",
  "Option 1Option 2": ": Left: xVal decodes numbers through a regression head carried alongside the regular token head,gated through the [NUM] token (figure reproduced with permission). Right: Instead, the Number Token Loss(NTL) circumvents the need for two heads and allows the computation of a regression loss directly on the tokenhead. We propose two schemes to achieve this: LNTL-MSE (right) leverages a dot product of the values of thenumber tokens and their class probabilities. The LNTL-WAS (left) uses the Wasserstein-1 distance of the (sorted)number token labels and their class probabilities. 2. Embedding: Canonically, the model has to recover the structure of numbers from data becausethe embeddings of numerical tokens are learned like any other token. Countless flavors ofnumeracy-preserving word embeddings exist , often akin to positional encodings. 3. Training objective: The standard cross-entropy (CE) loss assumes a nominal scale, thus it failsto convey the proximity between numbers, effectively inducing a semi-supervised setting. Forexample, predicting a instead of a token will not generally induce lower loss than a .This problem has been surprisingly neglected and is the focus of this work. Here, we aim to equip LMs with better inductive biases to handle combinations of textual andnumerical data, such as math word problems or scientific datasets. In particular, we propose twoversions of a regression loss on number tokens that respect numerical proximity (cf. right)and can be effectively combined with regular CE. The first version of this loss computes the MeanSquared Error (MSE) between the sum of the predicted class probabilities, weighted by their respectivenumerical token value, and the numerical token value of the label. The second version computes theWasserstein distance between the distribution of the predicted number probabilities and the groundtruth distribution, which is the one-hot encoding of the label. We integrate these improved trainingobjectives with existing solutions for tokenization and embedding, in particular the RegressionTransformer . We evaluate all methods on a subset of the mathematical-question-answer datasetfrom DeepMind . Prior art for joint language-number modeling suggested the use of verifiers , calculators(typically: Python interpreters), or chain-of-thought (CoT) reasoning to yield improved perfor-mance in Large Language Models (LLMs). We argue that all such strategies avoid the fundamental,underlying problem (i.e., number representation in LMs is poor) by reformulating the task, trying tocorrect answers a posteriori with calculators, or using significantly more compute (CoR). Therefore,we herein intentionally attempt to improve a classic, relatively small encoder-decoder LM with up to220M parameters, namely T5 .",
  "Number Token Loss": "The idea of the Number Token Loss (NTL) is to add an additional loss term to the CE, which isonly applied to number tokens and takes their numerical proximity into account. To achieve this, wepropose two versions. Number Token Loss with Mean Squared Error (NTL-MSE)This loss compares the numericalvalue of the ground truth token with the weighted sum of the respective numerical token values, withthe weights corresponding to the predicted class probabilities (cf. right). Given a modelf(), input tokens xi (where i N), the numerical value yi of ground truth token yi and a vocab Vconsisting of tokens (with indices j, ..., k representing the number tokens), we compute NTL-MSE:",
  "i(yi f(xi)j:k Vj:k)2(1)": "Instead of a nominal-scale loss with regular CE, the NTL-MSE effectively conveys proximity betweennumbers. For example, if the label is and the LM predicts a instead of , the loss willbe lower, matching our intuitive expectation, unlike the CE which gives constant loss no matter theproximity of the number (cf. ). This is sufficient for the vast majority of cases, however,since the NTL is not injective (like CE), it can return spuriously low loss for incorrect predictions.Consider e.g., a label with 50% of the mass on and 50% on token, then NTL will be zero(). While such cases are rare due to the softmax emphasizing logit differences, combiningNTL with CE loss helps correct spurious cases, as CE continues refining predictions without reducingits value in these instances. However, to address this non-injectiveness, we propose a second versionbased on the Wasserstein-1 distance. Number Token Loss with Wasserstein-1 distance (NTL-WAS)This loss calculates theWasserstein-1 distance between the predicted probability distribution of the (sorted) number to-kens and the ground truth probability distribution, which is 1 for the label token and 0 for all othertokens. Given the ground truth yi, a vocab V with number tokens ordered from indices j to k and thecumulative distribution function CDF(), we compute NTL-WAS:",
  "Backbone T5 and model variants": "We use a T5 backbone (Appendix A.3) for our experiments and extend it with both versions ofthe NTL and the Regression-Transformer tokenization scheme, due to its flexible encoder-decoderarchitecture and its success in various natural language processing tasks. Regression Transformer (RT).The Regression Transformer tokenizes numbers on digit level,considering both the position and value of each digit. Since standard learnable embeddings may notadequately preserve the inherent structure of numbers, it leverages an inductive bias to account for therelative proximity of the numbers through numerical encodings, further explained in Appendix A.5. xVal encoding and decoding scheme.The xVal method encodes real numbers using a single[NUM] token multiplied by its numerical value. For decoding (see ), a number head predictsthe value while the token head outputs the sequence, replacing [NUM] during inference. However,this scheme is incompatible with T5 (see Appendix A.6). We thus use the xVal encoder and maskedlanguage modeling in our experiments. Integration of the Number Token LossBoth versions of our proposed NTL, depicted in the rightpanel of , can be integrated into any model that treats numbers as clearly separated tokens ofsingle digits by applying it as an additional loss term. Therefore, we adapt the tokenization scheme",
  "LCE0.5 LNT LMSELNT LW AS": ": CE, NTL-MSE, and NTL-WAS for different pre-dicted number values with ground truth label . The under-lying logit distribution over the simplified vocabulary (num-bers 0-9) peaks at the respective predicted value and is uniformelsewhere. : The heatmap plot shows the re-spective loss for a given combination of theclass probabilities for token 3 and 5, where theground truth is token 4. The behavior of theNTL-WAS is closest to the intuitive desiredbehavior of the loss function, while the NTL-MSE does not have a unique minimum. of the standard T5 model to tokenize all numbers on the digit level to make it compatible with theNTL. As RT already tokenizes numbers on digit level by default, we can integrate the NTL withoutany changes. Integrating NTL into xVal is not feasible, as xVal encodes every number with the sametoken. Moreover, xVal already uses both MSE and CE loss.",
  "Experiments and results": "To test the mathematical capabilities of the methods, we use a dataset with more than 25 millionsamples from the mathematical Q&A dataset from DeepMind . The dataset comes with twosets of tests: interpolation tests, one for each type of question occurring in the training set, andextrapolation tests, which measure generalization along various axes of difficulty beyond that seenduring training. We provide more information about the dataset in Appendix A.4. We evaluate all fivemodels on the two test sets of this dataset and report the accuracy (how often the model predicts thenumber exactly), as well as the Mean Absolute Error (MAE) and the R2-score. Since the dataset isskewed with some very high values, we perform a log10 transformation on the predicted and groundtruth numbers before calculating MAE and R2-score. All experiments except the one with xVal are built upon the T5 implementation and language modelingtrainer based on the Hugging Face transformers library . We use the T5-base model as a pretrainedbase for our respective models. All models were trained for approximately one million steps witha batch size of 32 over a period of approximately 3 days. More details on the models traininghyperparameters can be found in Appendix A.7.",
  ": Comparison of evaluation metrics on interpolated and extrapolated test data": "The results can be seen in and . They show that vanilla T5 clearly benefits fromboth our loss variants. Indeed, accuracy increases by more than 10% for NTL-WAS compared tovanilla T5 in the interpolation tasks. The NTL-WAS was found to have the best performance acrossall three metrics and both interpolation and extrapolation tasks. This confirms our hypothesis thatnumber representation in LMs can be effectively improved through a minor, architecture-agnosticmodification of the loss function. The RT consistently surpasses vanilla T5 on interpolation, howeverno further benefit was found by augmenting RT tokenization with NTL-MSE, potentially due to thecustom number embeddings conveying numerical proximity. The limited performance of xVal is explained by the extensive range of numbers in the used dataset. The dynamic range of xVal islimited due to the combination of its scaling of the number token embeddings and the pre-layer-normin the backbone . As a result, the effective number range of xVal is limited to . To take thisinto account, we scale our dataset for xVal with log(1 + x). However, this means that large numberscan no longer be adequately distinguished by the model, as their embeddings become very similar.",
  "Conclusion": "We introduced the Number Token Loss (NTL) for LMs to enhance their ability to handle numericaldata by considering the numerical proximity between tokens. Our experiments unambiguouslydemonstrate the effectiveness of the NTL-WAS loss. This confirms our hypothesis that numberrepresentation in LMs can be effectively improved through a minor, architecture-agnostic modificationof the loss function. By augmenting the standard CE loss with NTL, we provide a simple yeteffective method that integrates seamlessly into existing architectures without requiring additionalcomputational overhead. Experiments on the DeepMind Mathematics Dataset demonstrated thatNTL significantly improves numerical reasoning, especially in models without specialized numericalembeddings. This approach offers a practical solution for enhancing language models in numericallyrich domains, paving the way for more accurate and reliable applications in mathematics and science.",
  "Jannis Born and Matteo Manica. Regression transformer enables concurrent sequence regressionand generation for molecular language modelling. Nature Machine Intelligence, 5(4):432444,2023": "Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, andMatteo Manica. Unifying molecular and textual representations via multi-task language mod-elling. In Proceedings of the 40th International Conference on Machine Learning, volume202 of Proceedings of Machine Learning Research, pages 61406157. PMLR, 2023. URL Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers tosolve math word problems. arXiv preprint arXiv:2110.14168, 2021.",
  "Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. Advances in neuralinformation processing systems, 28, 2015": "Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, SeanWelleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits oftransformers on compositionality. Advances in Neural Information Processing Systems, 36,2024. Mor Geva, Ankit Gupta, and Jonathan Berant. Injecting numerical reasoning skills into languagemodels. In Proceedings of the 58th Annual Meeting of the Association for ComputationalLinguistics. Association for Computational Linguistics, 2020. Siavash Golkar, Mariel Pettee, Michael Eickenberg, Alberto Bietti, Miles Cranmer, GeraudKrawezik, Francois Lanusse, Michael McCabe, Ruben Ohana, Liam Parker, et al. xval: Acontinuous number encoding for large language models. arXiv preprint arXiv:2310.02989,2023. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-neberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, et al.Highly accurate protein structure prediction with alphafold. Nature, 596(7873):583589, 2021.",
  "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deepbidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1,page 2, 2019": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen.Making language models better reasoners with step-aware verifier. In Proceedings of the 61stAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),pages 53155333, 2023. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unifiedtext-to-text transformer. Journal of machine learning research, 21(140):167, 2020.",
  "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematicalreasoning abilities of neural models, 2019. URL": "Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Haz-arika, and Lawrence Carin. Methods for numeracy-preserving word embeddings. In Proceedingsof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),pages 47424753, 2020. Avijit Thawani, Jay Pujara, Filip Ilievski, and Pedro Szekely. Representing numbers in nlp: asurvey and a vision. In Proceedings of the 2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies, pages 644656,2021.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,et al. Attention is all you need. Advances in neural information processing systems, 30(1):261272, 2017": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, AnthonyMoi, Pierric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empiricalmethods in natural language processing: system demonstrations, pages 3845, 2020. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.In International Conference on Machine Learning, pages 1052410533. PMLR, 2020. Xikun Zhang, Deepak Ramachandran, Ian Tenney, Yanai Elazar, and Dan Roth. Do languageembeddings capture scales? In Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of theAssociation for Computational Linguistics: EMNLP 2020, pages 48894896, Online, November2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.439.URL Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du, and Dacheng Tao.Achieving> 97% on gsm8k: Deeply understanding the problems makes llms perfect reasoners.arXiv preprint arXiv:2404.14963, 2024.",
  "A.3T5 architecture": "The T5 model is built upon the Transformer architecture , consisting of stacked self-attentionand feed-forward layers in both the encoder and decoder. The encoder processes the input tokens tocreate contextualized representations, while the decoder generates the output tokens autoregressively,attending to both the encoders outputs and the previously generated tokens. The model can be trainedwith both Masked Language Modelling (MLM) and Causal/Auto-Regressive Language Modelling(CLM) , whereby we chose to use CLM.",
  "limited linguistic variability, but is sufficient for our purposes to compare the mathematical capabilitiesof the different methods": "The dataset contains different modules and difficulty levels. For training and testing the models,we chose all difficulty levels but excluded modules where the answer contains complex fractions orvariables. This allows us to focus on purely numeric answers to simplify the evaluation of the modeland still leaves us with a large enough dataset of 26 million samples.",
  "A.5Regression Transformer": "The Regression Transformer preserves the inherent structure of numbers by inducing informationon relative proximity through numerical encodings that are set deterministically for all tokens. Forevery combination of a decimal place and digit value, a corresponding numerical token is added tothe vocabulary. For instance, the number 11.4 is tokenized to [1_1, 1_0, 4_-1]. Non-numeric tokens are set to zero vectors. The numerical encodings are designed so that theirpairwise distances are symmetric and monotonically decreasing with the float value. The finalencoding of the input tokens is obtained by summing over numerical and regular word encodings.The Regression Transformer numerical encodings NE at dimension j for numerical token tv,p withvalue v and decimal place p can be determined by",
  "A.6Challenges with Integrating xVal in Transformer Models like T5": "In transformer models like T5, integrating numerical encoding schemes like xVal presents challenges.Relative positional encodings and pre-layer normalization disrupt the numerical scaling. This makesit difficult to preserve distinctions between values. In T5, instead of using absolute positions for each token, relative positions between tokens areencoded. This helps the model understand relationships between tokens based on their distance,regardless of where they appear in the sequence. However, this relative encoding is applied uniformlyacross all tokens, including numerical tokens. Since relative position encoding doesnt account forthe magnitude of numerical values, it essentially ignores the scaling factor introduced by the xValmethod. Pre-layer normalization is applied to the inputs before they enter each transformer layer. Normaliza-tion typically scales the inputs to a standard range, effectively reducing the impact of the differencesin numerical embeddings introduced by the xVal method. As a result, even though the xVal methodmultiplies the [NUM] token embedding by its corresponding numerical value, this scaling gets neu-tralized by the normalization step, making the embeddings of different numbers more similar thanthey should be.",
  "A.7Training hyperparameters": "We train each model for 1050000 iterations with a batch size of 32 using transformers 4.42.4.We train with a learning rate of 1e-4 and weight decay of 0.01. All models were trained on singlegraphics processing units (GPUs) (NVIDIA RTX A6000). For the Number Token Loss, we trainedwith the hyperparameter set to 0.3."
}